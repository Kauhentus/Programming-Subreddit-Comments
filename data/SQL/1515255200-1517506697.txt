As far as I know you can use the where clause to limit the rows that are counted. You use the HAVING clause to limit the displayed results. So you can continue to use the JOIN in the from statement .
&gt;If you try to memorize something off a couple off tutorials and try to boast that on the interview, you will get shredded cause you won't really know what you are talking about. I've been guilty of this a few times in the past when I didn't really think through an answer before "parroting" information back that I'd heard. I'll definitely make sure I'm conscious of this, thank you!
Awesome, I appreciate these. Several I hadn't considered and a couple I realized when trying to answer in my head that I couldn't explain them as clearly as I would want/should be to be able to.
&gt;If your GROUP BY has 30 columns in it, you probably could change a couple of your joins into a sub-query with a 5 column GROUP BY, and eliminate the massive outer GROUP BY entirely. 1 and 3 I'm definitely on board with, but I have a question about 2. Does having a larger group by statement introduce performance degradation or is it simply a matter of "best practice"? This is the first time I've heard anyone say this.
You are using both double and single quotes. Maybe Oracle is ok with that but SQL server isn't.
I didn't realize + for concatenation wasn't standard, I only use sql server
It's not specifically that big GROUP BYs are bad, but they are a red flag that something else in the query has gone amiss. GROUP BY is a way to define the grain of the result set, and it's rare that the grain of a query is legitimately the combination of 30 distinct attributes. I see this a lot in two main scenarios. --Scenario 1 SELECT c.CustomerID , c.FirstName , c.LastName , c.Address1 , c.Address2 , c.City , c.State , c.ZipCode , c.DateOfBirth , SUM(o.SalesAmount) AS SalesAmount , SUM(o.Cost) AS Cost , SUM(o.SalesAmount) - SUM(o.Cost) AS Profit FROM Customer AS c INNER JOIN [Order] AS o ON o.CustomerID = c.CustomerID GROUP BY c.CustomerID , c.FirstName , c.LastName , c.Address1 , c.Address2 , c.City , c.State , c.ZipCode , c.DateOfBirth; --Can be improved to: SELECT c.CustomerID , c.FirstName , c.LastName , c.Address1 , c.Address2 , c.City , c.State , c.ZipCode , c.DateOfBirth , o.SalesAmount , o.Cost , o.Profit FROM Customer AS c INNER JOIN (SELECT o.CustomerID , SUM(o.SalesAmount) AS SalesAmount , SUM(o.Cost) AS Cost , SUM(o.SalesAmount) - SUM(o.Cost) AS Profit FROM [Order] AS o GROUP BY o.CustomerID) AS o ON o.CustomerID = c.CustomerID; --Scenario 2 SELECT da.AgentName , da.ManagerName , da.CallCenterLocation , dd.[Date] AS CallDate , dd.[Month] AS CallMonth , dd.[Quarter] AS CallYear , dq.QueueName , dq.DNIS , dq.LineOfBusiness , dd.DispositionName , dd.DispositionType , dd.IsPositiveResult , SUM(fc.Duration) AS Duration , COUNT(*) AS TotalCalls , SUM(fc.IsInbound) AS InboundCalls , COUNT(*) - SUM(fc.IsInbound) AS OutboundCalls FROM Fact.Call AS fc INNER JOIN Dim.Agent AS da ON da.DimAgentKey = fc.DimAgentKey INNER JOIN Dim.[Date] AS dd ON dd.DimDateKey = fc.CallDateKey INNER JOIN Dim.Queue AS dq ON dq.DimQueueKey = fc.DimQueueKey INNER JOIN Dim.Disposition AS dd.DimDispositionKey = fc.DimDispositionKey GROUP BY da.AgentName , da.ManagerName , da.CallCenterLocation , dd.[Date] AS CallDate , dd.[Month] AS CallMonth , dd.[Quarter] AS CallYear , dq.QueueName , dq.DNIS , dq.LineOfBusiness , dd.DispositionName , dd.DispositionType , dd.IsPositiveResult; --Can be improved to: SELECT da.AgentName , da.ManagerName , da.CallCenterLocation , dd.[Date] AS CallDate , dd.[Month] AS CallMonth , dd.[Quarter] AS CallYear , dq.QueueName , dq.DNIS , dq.LineOfBusiness , dd.DispositionName , dd.DispositionType , dd.IsPositiveResult , fc.Duration , fc.TotalCalls , fc.InboundCalls , fc.OutboundCalls FROM (SELECT fc.DimAgentKey , fc.DimDateKey , fc.DimQueueKey , fc.DimDispositionKey , SUM(fc.Duration) AS Duration , COUNT(*) AS TotalCalls , SUM(fc.IsInbound) AS InboundCalls , COUNT(*) - SUM(fc.IsInbound) AS OutboundCalls FROM Fact.Call AS fc GROUP BY fc.DimAgentKey , fc.DimDateKey , fc.DimQueueKey , fc.DimDispositionKey) AS fc INNER JOIN Dim.Agent AS da ON da.DimAgentKey = fc.DimAgentKey INNER JOIN Dim.[Date] AS dd ON dd.DimDateKey = fc.CallDateKey INNER JOIN Dim.Queue AS dq ON dq.DimQueueKey = fc.DimQueueKey INNER JOIN Dim.Disposition AS dd.DimDispositionKey = fc.DimDispositionKey; The main reason why this is a good idea is it limits the size of the intermediate dataset. Let's say in scenario 1 we have 1 million Customers and 10 million Orders. The average size of a Customer columns is 250 bytes and the Order columns 30 bytes. With the big group by we aggregate a 10 million * (250 + 30) = 2,670 MB dataset. With the subquery we aggregate Orders 10 million * 30 = 290 MB and join it to Customers 1 million * 250 = 238 MB for a total of 528 MB. Let's say in scenario 2 we have 10 million fact records, and a 3 million record result set. In the fact table the DimensionKeys are 16 bytes and the measures are 10 bytes. From the Dimensions the attributes are 200 bytes. With the big group by we aggregate 10 million * (200 + 10) = 2,003 MB. With the sub query we aggregate 10 million * (16 + 10) = 248 MB and join to the dimensions 3 million * (200 + 10) = 601 MB. By reducing the data set earlier in the query, where it's handling fewer and/or less bulky columns, we gain a lot of efficiency. Seeing a gigantic GROUP BY at the end is a red flag that the query could have reduced its result set much earlier.
Ideally, all your data would be in one table since it seems the structure is the same across years. Then you'd simply add something like a year column. If that's not possible, another option would be to instead use `UNION` to create a single dataset. Something along the lines of: SELECT '2005' AS YEAR, ACCT_ID FROM [2005] UNION SELECT '2006' AS YEAR, ACCT_ID FROM [2006] ...
I would suggest that you assign a date to every row in each year table, just pick a date in that year (all rows in 2005 have a date of 2005-01-01 for example) then join all year tables on acct_id and select distinct date from &lt;Combined Table/view with all Years&gt;. That will get you purchase years per account. I would suggest keeping a single fact table for season ticket purchases with actual purchase dates if they are available. But Im not sure how good or bad all your data is. Good luck! 
The &amp;variables get substituted *before* the code begins to run. It doesn't depend on the execution path. You have three &amp;, so it will ask you for three values before the code starts even if only 2 of them will ever be used when the code runs. You should put your &amp;ssn assignment before the case statement so you only need one. 
There is a GROUPING function to determine if a value is NULL or is it is NULL because of groups: https://docs.oracle.com/cd/B28359_01/server.111/b28286/functions064.htm#SQLRF00647
Here combination of all 4 columns which are part of key should be unique. e.g lets assume a key with 2 columns a and b. So data combination pqr,xyz and pqr,qrt is allowed. You cannot have two rows with same combination. 
Unfortunately the data is not that good. The newest table (2008) has more information. I think it will be best to make a new column in that table with the number of years the acct_id has had tickets. making that one the "single fact table like you said. Just need to figure out the best way to add the column after each query I due over the 4 year span. Any ideas? I am sure this is not the most efficient way to do this! . SELECT DISTINCT [2008].[price_code], [2007].[acct_id], [2008].[acct_id], [2008].[block_purchase_price] FROM 2008, 2007 WHERE [2007].[acct_id] = [2008].[acct_id] ORDER BY [2008].[block_purchase_price] DESC; 
Thanks a lot Mate! I will try that way :)
What happens when you have to load 2009+ data? then, the number of years in the fact table based on 2008 is innacurate. i would add data from all years into one table, and a purchase year column to thst table. then, you could write a query to select acct_id,(whatever other columns constitute a purchase for that year),count(1) AS NumPurchasesOverYears from newtableWithPurchaseYear GROUP BY acct_id,(other columns). Sorry for the shorthand as I am on mobile. Does thst make sense? Your method is fine if we never get more data. I think the method I outlined is more sustainable. But as an ETL devops guy, I may be overthinking this for your particular problem.
Pretty sure the two results aren't randomly selected. They're the first two row_ids from your query results. And you're ordering, so it will always be the same first two.
PL/SQL? T-SQL? * Try reading the documentation on your language * Take a look at query tuning * Look at the performance of your queries * Find a better way to write your own queries Challenges: * Can you return only the 2nd highest row from a query? * Can you find out the first order of a client from only their latest order? * What is the average amount of days between purchases over all clients? * Can you write a query that will return column names if you input a tablename? * Can you write a recursive CTE? A table-valued function? * Have you used a lateral join or cross apply? * Have you used XQuery before?
I learnt SQL on codecademy, was enough to get a data analyst job and have learnt the rest on the job building select queries in SQL server management studio and building databases in MS Access. If you use the drag and drop features on Access it writes SQL for you which you can learn from and edit. Combining VBA and SQL can be very powerful if you're using Microsoft stack like most big companies are.
Most important thing you can do is to get comfortable with JOINs. Know the difference between LEFT JOIN and INNER JOIN. Another important thing is to understand how GROUP BY works. /u/GenConsensus has some good suggestions. I'd also suggest learning how to pivot, unpivot and join a table to its self (like an HR table that has EmployeeID and SupervisorID on it). I gather http://sqlfiddle.com/ is a sandbox you can play in, but if you have access to your DBs I'd suggest trying to write SELECT statements off what you do as a web developer. You're already familiar with the Schema. It'll make more sense to you.
Thanks a ton. Finally got time to test it out. First I tested switching order in the time join so tables a and b where on the same side (with datediff). That didn't work very well and actually slowed the query. It might be due to the functions confusing the execution plan rather than comparing the raw datetimes. I tried this first cause it was so very simple. Second I tried your suggested solution and the results were mindblowing. My testquery on one months data and with just 2 timestamps per session (my actual data has more timestamps I need to use for splitting the time in different categories), went from ~4 minutes to 2 seconds. I'll go complete the query and that'll mean I can load full data every night rather than deltaload and also that the load will still run significantly faster. I'll try to implement this crossapply method a few other places where we do similar stuff. First I have to go fix a corruption I just made to our SSAS cubes :)
Your response inpired me to continue studying for a data analyst role. I have good experience with VBA and sql from data management, so I've been trying to study r/python and stats to get a better job but I wasn't sure if itd ever happen 
1. SQL is necessary to gather the data you need for models. It's also preferable in a lot of cases for feature generation. SQL environments are generally better for handling significant amounts of data than R. 2. I suppose it depends on where the data lies. SSMS for MSSQL; SQL Developer for Oracle; I don't know for MySql. 3. [kaggle.com](https://www.kaggle.com/) Alternatively, any predictive model that interests you. Check out these sites for inspirational datasets. https://www.kaggle.com/datasets https://cloud.google.com/bigquery/public-data https://aws.amazon.com/datasets
Found it: Array.isArray
Honestly, I'd recommend T-SQL Fundamentals the book. It's a very nice intro to MS SQL. It's not free, it's about ~$30, but if someone had told me to use that to learn MS SQL, I feel I would have started lightyears above the curve. If you want the free route, just google around on Brent Ozar's site, you'll eventually find more resources and branch out.
SPARQL and CouchDB
Thank you for your answer! I'll take a look on access, seems very helpful!
Select two engineers with less than two shifts in the past two weeks, and no shifts in the past day. Prefer engineers with less shifts in the past two weeks, otherwise select randomly.
Thank you for your answer! It seems like SQL is more of a tool just to "extract" the data from relational databases. Do you think using SQL inside Rnotebook (using the SQL package) would be enough for recruiters?
Recruiters aren't going to recognize R modules. I would install and start using MSSQL or MySql. You're going to be neck deep in a proper database eventually. No reason to put it off.
Please be sure to tag your database system as noted in the sidebar.
&gt;use SSMS such that I can save tables locally like you could in Access SSMS is a client only, so no. &gt; I can't create a link from it to our company's SQL Server in order to join tables between databases. You don't want to join between servers or desktop and server. I mean, you *can*, but it sucks. Have you asked your DBA about creating views in SQL Server that cover what you're trying to do? * Develop a query that pulls together the tables/logic you need * Make a view from that query * Query the view from MS Access (just adding the appropriate `WHERE` criteria)
Could you expand on what you do at your job and how code a academy helped you learn those skills? I'm trying to transition into a data analyst role and this sounds like quite an endorsement.
Check out Brent Ozar's scripts, sometimes they have bugs and they credit you. SP_Whoisactive and the Ola scripts are also very large and interesting to look through.
Link?
I worked through their two modules on SQL and saved the projects into a presentation pack that I showed in my interview. They were impressed I'd learnt it on my own (I didn't think it was very impressive). The best thing you can show an interviewer is your enthusiasm to learn. I use SQL everyday to pull reports (queries) from our large databases and come up with ways to automate it, visualise it and tell stories to management. Once you learn these skills most office workers consider you magic. 
You're right, SQL isn't for visualising. But what do you do with the output? An important related skill is interpreting, analysing and visualising the data. Most companies live off excel dashboards so feeding your SQL queries into automated dashboards through power query or access is just as important. 
&gt; I can't set foreign key on both. sure you can
Sorry, my mistake. I cant delete cascade both
me, i would DELETE CASCADE the employeeuserid -- if an employee leaves, that's one fewer employee for this manager, but if a manager leaves, you don't want to delete the list of employees that used to report to that manager, you would want to re-assign the employees to a new manager, most likely by updating the manageruserid in these rows, and you'd do it one by one, because not every employee will get re-assigned to the same new manager
Looks like you’re trying to toggle a column grouping by using a member of a row grouping. So you’re out of scope. What does TG3 represent, test grades or something similar? Perhaps reconsider and toggle the visibility through a parameter selection at the top of the report (multivalued would be my choice). It’ll be a bit more work on the front end, as you’ll probably need to write a stored procedure to return/populate the parameter values as well as modify your existing stored produce to accept the parameter for the test(s) you want to see. But in the end it will be a much more polished and user friendly solution. 
biggest mess for me would be 65k lines in a stored procedures, once inlining all the sub proc, function view calls. Ain't gonna find that one on git hub thou.... the rabbit hole is deep is all i'm saying.
I'm and ETL/DW dev and we have a data scientist that calls someone on our team a few times a week needing help with his queries (how to do an "if", how to join three tables together, why is my query producing duplicates, etc) Don't. Be. That. Guy.
Quick background... created this for a way to create DDL statements for tables / views. Feed in a parameter of a table/view name and the SPROC will return the DDL of that object. If you feed in no parameter, then the sproc will return DDL for all tables / views in the database (except sys/information schema tables). I used the AdventureWorks2012 database here, but this should work on any DB. One issue I am having is that the NVARCHAR(MAX) is not always big enough to hold all of the DDL (part of it gets cut off). Looking for feedback or anything that can be improved. Thanks!
For free : http://www.studybyyourself.com/seminar/sql/course/?lang=eng. The course is broken down into a basic course and an advanced one. You can submit exercises online too.
Depends. In my opinion a 6 hours boot camp is not really long enough to cover the basics. Therefore, I recommend you to study on your own. When you actually do things by yourself you can better retain it.
&gt;* generate SSIS ETL packages Interesting.. How did you guys do that if you don't mind me asking? 
Yeah you're right, think this is what's causing the scope error.
Unique suggestion, will have to look into this as well. Thanks for the help.
We're not here to do you homework for you, read the sidebar: &gt;**Help posts** &gt;If you are a student or just looking for help on your code please do not just post your questions and expect the community to do all the work for you. We will gladly help where we can as long as you post the work you have already done or show that you have attempted to figure it out on your own. So please post what you have now and we might give you pointers from there on.
&gt; We're not here to do you homework for you Speak for yourself.
* What is the use case? SSMS has script tools Can you explain this one? I am not aware of this. * Strips off the last character from the column list, but should strip the first character. Not following you here. I put the comma at the end and the STUFF function is removing it.
How do you do this in SSMS?
with END it works perfectly as expected Thx
Why would you print things if you want to access it programatically? Not as an output parameter or something. Also, https://docs.microsoft.com/en-us/sql/relational-databases/server-management-objects-smo/sql-server-management-objects-smo-programming-guide
Thank you, I will check out Brent's site and take a look at that book. 
A CASE statement might work here
Here's a few examples: https://www.sqlservercentral.com/Forums/474921/How-to-print-to-a-specified-text-file?PageIndex=1 http://www.sqlservercentral.com/articles/Export/147145/
 select d = xxx.Assignee , count(*) , sum(case when datediff(DAY,created_on,getdate())&gt;7 THEN 1 ELSE 0 END) , sum(case when datediff(DAY,created_on,getdate())&gt;30 THEN 1 ELSE 0 END)
Good point. Thanks for your help! So many improvements that I can make.
Case statement worked.. I was way overthinking this.. Thank you. 
Just a FYI, you can script multiple items of the same type. In SSMS if you hit F7 it should open up 'Object Explorer Details'. In the main Explorer if you navigate to 'Tables' it will list all the tables in the DB. You can then select the items you want and script them as jc4hokies mentioned.
As you work more with SQL you will start to think of your problems and solutions in terms of sets of data. I often ask myself how can I transform this set of data to be more useful to my goal? In this case it would be really helpful if the dataset told me which rows are the most important. When you solved the problem you achieved this goal but the difference is that you had to go back and query the same table multiple times. Generally speaking SQL developers should try to avoid selecting from base tables as much as possible. So the advantage here is that we are selecting from the table once, holding on to that dataset, and then filtering it based on the new computed dense_rank column. So I am taking you down the path of analytic functions. Think of them like group by but instead of squishing the grouped rows together, instead the result is returned for every row. In this case the order by calc_status works just fine because it just so happens the ascending order is the order that we want. but what if there is a calc_status of 1? then we have an issue because 1 will be prioritized before 2. In this case we can create our own ordering by using a case statement like so: case calc_status when 0 then 1 when 2 then 2 else 3 end This says 0 is most important, 2 is second most important, and anything else has equal importance. the final query would look something like this: select bunk.*, dense_rank() over ( partition by voy, ship_code, grade, bunk_line_type order by case calc_status when 0 then 1 when 2 then 2 else 3 end) ranking from bunker_calc_line bunk where (bunk.VOY = 201717 and bunk.SHIP_CODE = 427 or bunk.VOY = 201801 and bunk.SHIP_CODE = 351) and bunk.GRADE = 'LS-380' and bunk.BUNK_LINE_TYPE = '80' order by bunk.VOY desc, bunk.SHIP_CODE asc 
Yeah sorry, SSIS is what I am using. I have been looking into redirecting a little, unsure how to get it to work exactly though, feels like I'd have to redirect a lot to get where I'd want to be able to do 1 record at a time though. I have been using the SSIS package on a server, no indexes on this particular package either. (We are working on putting indexes on everything, but we need SQL2016 for that, and this package hasn't been able to load on the server we have for that either) Thanks for the input!
I've used a couple styles of redirect (if indeed you are encountering data related failures). 1. Redirect the failed batch (drag the red arrow from the destination) to a table with no constraints and generous data types. A lax table that should accommodate sub par data. 2. Redirect the failed batch from a fast-load destination to a non-fast-load destination. Redirect individual failed records from the non-fast-load destination to a log table (maybe just an ID or something). Some things you're saying don't make sense to me, so I'm not sure if I'm interpreting the scenario correctly.
I honestly am not sure what the issue is, haven't been working on the field long (under a year) so this is all new to me. I feel like the data types MAY be the issue, but I've had this job running at nights for a couple months and no issues... I updated the ODBC connection because it was changed from UNICODE to ANSI (Job stopped working then)... Then I had to wait on the ODBC be loaded into production, so tonight will be the first test overnight -- but I haven't gotten the thing to work during the day since first creation of it 2 or so months ago... (We delete the table and reload it every night) Please let me know if I am unclear about anything, I'm sure I was somewhere (or everywhere!)
Thank you for educating me! So performance is directly related to how many times I query the base tables? this is very good advise because i'm querying an operational system directly and limiting the load i put on that system is a good idea for people that are using the front end of that system. thx very much for your time! great community on reddit :)
The package should be outputting error messages. Is there a reason you're not using OLEDB connections? --- &gt; Please let me know if I am unclear about anything See below. --- &gt; no indexes on this particular package either Packages don't have indexes. Tables do. --- &gt; putting indexes on everything, but we need SQL2016 for that Every database since 1990 has had indexes. --- &gt; using Microsoft Visual studio | SSIS package on a server | this job I guess it's just a job. You're not running Visual Studio on a server, are you? --- &gt; We delete the table Delete the data, or drop the table? Also, this could be an issue if it's in use in the middle of the day.
There are no error messages at all, just the counts don't add up when looking at the data (Original table has 28m and I've gotten anywhere from 14-22m rows in my created table) **Is there a reason you're not using OLEDB connections?** We are moving data from MySql to SQL Server so ODBC is our only option. **no indexes on this particular package either** Ah yeah, my technical vocab here is seriously lacking, the table has no indexes. **putting indexes on everything, but we need SQL2016 for that** Clarification: We need Clustered Columnstore Indexes **using Microsoft Visual studio | SSIS package on a server | this job** I'm running microsoft visual studio on a Dev server then saving the SSIS package to run the job on our production server **We delete the table** Dropping the table I suppose, I use the Delete from [tablename] command. It isn't an issue, because it should only be loaded once at night -- I am currently working on it during the day, when the data is messed up anyway.
There is an option at the top of the menu, to select how you would like the output: text, column, or file. 
&gt;Dropping the table I suppose, I use the Delete from [tablename] command. Delete from tablename is not the same as drop tablename nor truncate tablename... Delete is a DML command that removes rows from a table. It gets logged, so rollbacks are available. It also takes more resources than a drop or truncate. Drop and truncate are DDL. Drop completely removes the table from the schema, including all data. Truncate removes the data from the table, but the table definition remains. If you are clearing the entire table nightly I would recommend truncate. Unless you need to rollback but it doesn't appear so. I'd like to see your control flow and SQL statements you are using. In another post here I see that you have left joins and case statements. Without seeing your code and flow it's pretty difficult to identify the problem as it could be with your SQL or your SSIS package but we're currently guessing. If you are having success running your package in the middle of the night, but not during business hours, then I have a hunch you may be running into a timeout, resource or locking issue on the MySQL source side. I think /u/jc4hokies is also noticing the same thing.
From the information at hand I think you're onto something in that the source is probably failing at some point. If the load failed he would have received an error. OP, you could offload to an intermediate csv file and bulk insert from there to be sure. With only one try per 24 hrs I would seek different solutions in parallel. 
this article is useful: https://blog.jooq.org/2017/04/20/how-to-calculate-multiple-aggregate-functions-in-a-single-query/
trust me, do not use your script. You are missing so god damn many case statements in there its not even funny. Just to handle datatypes which you currently don't... bah. Decimal and numeric for example, you have to have case when decimal/numeric then concat('decimal(', length, ',', presicion, ')'). You have to wrap another case around all that, to handle length = -1 (varchar(max) and there such), you are missing all foreign keys, primary keys, indexes, collations, datalength of char&amp;varchar vs nchar&amp;nvarchar, all the depricated legacy crap as text and image, computed columns, I could go on quite a bit. Your script does nothing remotely resembling printing out a generic object definition, and you are gonna spend weeks writing 50x the code to actually do that, and it will fail for months until you fixed all the "whops forgot about that one" errors. Trust me, I've done stuff like that in the past, it is seriously not fun. If you want to clone an object inside of the DB, just to select top 0 * into clone from object. If you need the meta data inside of an application, the meta data of a record set should be quite available inside of something like ADO.Net for example, including mapping to the correct .Net datatypes in this example. So there really really should not be a need for that script. Ps. select top 0 * will not give you any constraints, index defintions, foreign keys, filegroup stuff, computed columns, etc.... but it at the very least will give you reliable datatypes, which in itself is a LOT of very error prone coding to do yourself
I am just going to add in data types, null/not null, and indexes. I am mostly doing this to use at work, and this will serve my purposes well. I have already written most of the code, just need to commit it on github. You raise a good point with the -1 for NVARCHAR(MAX), and I can't deny that nested case statements are a mess. Thanks for the opinions though.
Please add in a disclaimer to that git hub project, that your script is only going to work for your very specific use case, with very very limited range of supported objects. You are not going to get anywhere near getting it to a point where it is truly generic. I'd estimate at least 1-2k lines of SQL on a script, coming close to the be a complete "dump out definition" procedure. Just reading out index definitions is a good 150 if formatted
Holy crap, I could have did a rollback on the table? Dang! I'll look into this for future cases. (Almost all of our Jobs we have some form of Delete From statement beforehand) Generally a rollback on the table I am mentioning would not be that helpful, we need a full update of all the information on the table on a daily (or near daily) basis. Truncate table, I'll be sure to check this out more. Control flow: Execute SQL statement (Delete from Table) --&gt; Data Flow Task Data Flow Task: ODBC Source (Select a,b,c,d, case when datefromtable between '1900-01-01' and curdate() then datefromtable else '1900-01-01 00:00:00' end as datefromtable, e From Table t Left Join Mysqltable on t.unique_key = mysqltable.unique_key where t.e &lt;&gt; 'something' or t.e is null --&gt; OLE DB Destination That is the basis of the control flow. Pretty sure I cannot post the actual information online! I have been thinking it could've been a timeout issue for a while, but since I am so new at this it is hard to have any solid proof of this.
'sql' is a bit of a misnomer here. sql is just a language (syntax). SQL is implemented on specific platforms and these perform somewhat differently in different circumstances. Anywho, I don't know of any single implementation that would allow 98000 columns. Furthermore, while SQL works with sets (or multi-sets), it's not necessarily a good fit syntactically for matrix operations per se. You might want to describe the original problem that led you to seek this solution.
I don't have specifics for you and hoping someone will, but I can offer you experience. I have been working with Microsoft SQL Server for 18+ years professionally and currently manage a team of database developers and architects at a fortune 200 company, with a special focus on OLTP performance tuning. Never ever had SUM being a performance bottleneck, including against tables many magnitudes larger than those you list. That said, "it depends". I don't know your hardware, what's running on the server, the software version, the query specifics, what expression is being summed, etc... A 98k table is nothing, but based on your wording, it's unclear if that is all. A 98k table involved in a Cartesian product with another 98k table gets pretty massive. Do the math. Any query is going to run relatively slow against 10 billion rows. But SUM by itself is pretty efficient from my experience. 
It’s basically [this implementation here](https://notes.mindprince.in/2013/06/07/sparse-matrix-multiplication-using-sql.html) that treats each item in a matrix as a row and does a join between two tables that represent matrices.
Personally I would approach this as two steps 1) "Combine the data" into a single recordset 2) Analyze it to get the year of membership To combine the data I see several options 1) Creating a new table of distinct customerID &amp; year 2) Create a view to combine the existing tables 3) Use a CTE (Common Table Expression) Once you have the data in one grouping, you then need to figure out what year of season ticket holder they are. Personally, I would use the SQL ROW_NUMBER function (if you don't cleanup duplicates - dense rank). Here's an example ;WITH AcctSeasons AS ( SELECT DISTINCT 2005 AS Season, Acct_ID FROM 2005 UNION ALL SELECT DISTINCT 2006 AS Season, Acct_ID FROM 2006 UNION ALL SELECT DISTINCT 2007 AS Season, Acct_ID FROM 2007 ) SELECT Season, Acct_ID, ROW_NUMBER() OVER(Partition BY Acct_ID ORDER BY Season) AS MemberYear FROM AcctSeasons
Yea, these were the kind of questions I was getting for my DBA interviews. SSRS ones than maybe: Do you know how to set a variable in a report? Do you know how to set a parameter in a report? Have you ever made a map chart? Query making ones: Have you used windowed functions? How do you do a rolling total? Have you used GROUP BY with ROLLUP, CUBE, and GROUPING SETS? Let's say you want all data from 2017 in an indexed column, what's better: WHERE DATEPART(yyyy, DateColumn = 2017; or WHERE DateColumn between '2017-01-01' and '2017-12-31';
Commenting to follow along...
Interested in this also
What rbdm platform? Whichever one you choose, they probably have a respectable and solid cert program. SQL Server, oracle, and my SQL should have them. They are also relatively difficult. 
If you're going to get a certification, get one from the vendor of the RDBMS that you're working with - Oracle or Microsoft, I don't know if there's one for Postgres.
While I certainly am not privy to the source code of the SQL engine... addition is one of the quickest (or the quickest) operations the processor can perform. If you are summing 98000 values, you might want to be concerned that you'll hit the integer limits. Sounds like in your scenario you could easily test the speed of the SUM function vs multiplication or addition in your matrix to determine the most efficient method.
Spectre is a range of vulnerabilities, some of which may require re-engineering the chip itself, some possibly yet undiscovered or untried. There is no complete fix yet for spectre, aside from using a chip that is not vulnerable. https://spectreattack.com/spectre.pdf &gt; “While makeshift processor-specific countermeasures are possible in some cases, sound solutions will require fixes to processor designs as well as updates to instruction set architectures (ISAs) to give hardware architects and software developers a common understanding as to what computation state CPU implementations are (and are not) permitted to leak.” – Page 1 &gt; “Unlike Meltdown, the Spectre attack works on nonIntel processors, including AMD and ARM processors. Furthermore, the KAISER patch [19], which has been widely applied as a mitigation to the Meltdown attack, does not protect against Spectre” – Page 3 https://www.raspberrypi.org/blog/why-raspberry-pi-isnt-vulnerable-to-spectre-or-meltdown/ &gt; “The lack of speculation in the ARM1176, Cortex-A7, and Cortex-A53 cores used in Raspberry Pi render us immune to attacks of the sort.” https://twitter.com/securelyfitz/status/949370010652196864 &gt; “Let's start with a standard software product many are familiar with (Visual Studio) and work off that. First, every time you hit 'build' it's called a 'stepping', costs millions of dollars &amp; takes several months. If you want a profitable product, you may only get 10 chances to press 'build'… “ &gt; “But really, even a small fix means building, testing, fine tuning, building, testing, characterizing, and then releasing. So count on 2-3 steppings and we quickly get to 6 months to a year… “ &gt; “The reality is that new features are risky when you only get a few revisions. LOTS of features exist in silicon for generations before they're fully vetted and 'enabled'. So most features end up being more like 5 years to availability on a product with software supporting it.” The fact that the CEO of Intel sold $30m in stock when they found out about this vulnerability is a dead canary in the coal mine here.
how much of a performance hit does the server take when applying these patches ?
Interesting. I am still trying to wrap my head around it. Can you share any information about how this will be useful just so that I can get some context? I am trying to determine the expected maximum number of rows that will be effectively joined between tables A and B. It's hard to tell, but if you are going to taking 1 value from table B and multiplying it with up to 98k from table A, as long as you have proper indexing on your join columns it seems like you can get good performance. Is the data expected to be static, infrequently updated or will it change frequently? You could get even better performance in SQL Server at least by making your database read only. 
Having a presence online can be helpful too. A blog that has periodic posts displaying your SQL knowledge or projects you're working on. Stack overflow is nice, you can see how they interact with people and how their attention to detail is. Any front facing report dashboard with an online data set is pretty slick too. (Power BI + the Brent Ozar survey would probably be a great front facing slice and dice project you can show off to anyone reading.)
We use Amazon I think. I should probably just talk to our database folks and ask them haha
It’s basically an artificial neural network. If you have time here are two videos on how they work. [Video #1](https://youtu.be/aircAruvnKk) [Video #2](https://youtu.be/IHZwWFHWa-w) But I plan on storing image pixel numbers in a table and storing other information regarding artificial neural networks such as weights and biases. The image pixel numbers never change for an image but the weights and biases change when the artificial neural network improves it’s system to adapt to the image preferences of users. So those 1.5 million numbers representing the weights and biases will change whenever a user likes or dislikes an image. Each user will have a set of about 1.5 million weights and biases.
That is your host, the RDBM platform will be the software you want to become certified in. Example, I have a laptop and want to be certified on Excel. Laptop is the Amazon host, Excel is the platform I want certification on. You can get certified for Amazon services too though, which is a good cert to have.
Do you have a company or a job you are shooting for? Look at what they use, a company might use Mongo DB or Sql server or tableau or power bi, all of which are different in their own, I'd recommend learning what they use and get some specific training with the platform This is assuming you know general sql practices 
I'm not sure why it doesn't work for you, but it works for me with the example you provided. Run my code and see, there's something else missing here that you haven't provided, otherwise it should work. Can you give us any more insight? DECLARE @tablea TABLE ( ColumnA VARCHAR(10) ,ColumnB VARCHAR(10) ,ColumnC VARCHAR(10) ) DECLARE @tableb TABLE ( ColumnA VARCHAR(10) ,ColumnB VARCHAR(10) ) INSERT INTO @tablea ( ColumnA ,ColumnB ,ColumnC ) VALUES ('Value1','A',NULL) ,('Value1','B',NULL) ,('Value1','C',NULL) INSERT INTO @tableb ( ColumnA ,ColumnB ) VALUES ('Value1','Test') UPDATE @TABLEA SET ColumnC = b.ColumnB FROM @TABLEA a LEFT JOIN @TABLEB b ON a.ColumnA = b.ColumnA SELECT * FROM @tablea SELECT * FROM @tableb
I found that this issue was not caused by the UPDATE statement, it was caused by a trigger. I disabled the trigger and the update ran fine. Pesky triggers!
Game data analyst is what I'm aiming for. So analyzing game data in the video game industry:)
Gotcha. Probably good idea to chat with our core team to ask what they suggest :P
I would google database performance tools, run a few of their demos or trials either in your environment or on a vm box just to see what it's like. Then, try to make your own free version if your company won't buy it. I've made some metadata and analysis type stuff and logged things before, but it's not a small feat and there's a good reason why lots of tools out there exist for this. Warning, I'm tool biased. I would recommend to check out Idera, Apex, Solarwinds, SQL Sentry, and Redgate to start. 
Thanks 
I'm just going through threads learning, but what is the "explicit join" here? 
Ya probably. http://www.sqlservercentral.com/articles/Reporting+Services+(SSRS)/145092/ [More goog-le results](https://www.google.com/search?q=ssrs+data+driven+subscription+work+around&amp;oq=ssrs+data+driven+subscription+work+around&amp;aqs=chrome..69i57.6159j0j7&amp;sourceid=chrome&amp;ie=UTF-8)
Very cool. Busy day today, so I will check out those videos and get back to you. Regarding your application, that seems like a steep volume curve. How many users do you expect to serve? If this is intended to be for the public with some form of organic growth, my fear is that a traditional rdbms would quickly hit a performance bottleneck. You may want to consider a horizontally scalable data platform like Elastic Search (although there may be better options) to account for growth. You can't do joins per se, but I think you can account for that in your middle tier logic and you will end up with a more scalable solution. If you will have a relatively constrained user pool, this should work in an rdbms up to a point, depending on your performance requirements. 
No one really knows yet. I would highly recommend against patching for now. You may see little to no performance hit. You could end up with a brick.
Install SSMS 2017?
thought of bumping up to 2016 or 2017, but I'm on Windows 7...not supported
2014 you assume correctly - I mentioned the VS version only because the error references it
Generally you would allow the parameter to be null and then have a where clause like (abc.customer_no IN ($cCUSTOMER) OR $cCUSTOMER IS NULL) But the details can vary depending on platform and how you're building/passing the query from your app.
What rdbms are you using? I dont how performance will be you can try something like this. Where ($cCustomer is not null and abc.cumster_no = $cCustomer) This way if you pass null into the parameter it will not applied that filter. Can they pass more than 1 customer at a time? If not I would change the in clause to a equals. Does this help? 
Are you able to connect to a database? Is it a remote server or are you running a local instance? If you can create a database can you see all the tables? What happens if you try to click and select top 1000 from a table?
I know the rules of the sub, this is one question out of 20 SQL questions we need to answer about a database we built. So I don't think this is too much to ask for help.. Using a plain "**WHERE** Item_ID **NOT** 4" doesn't work since you still get the names of people who have purchased other stuff including 4, but I only want the people who have not purchased 4 at all.. help please!
You can left join the table with a subquery selecting those names who bought item b and only select those with no match.
 SELECT Name FROM Table GROUP BY Name HAVING SUM(CASE WHEN Item_ID = 4 THEN 1 ELSE 0 END) = 0
Why didn't I think of this.. I guess I need to get some air and go back to working on this... it seems so clear now lmao.
Thanks, I applied this to the database in hand and it worked. !!
[Here's an example.](http://sqlfiddle.com/#!9/4bd2d5/1)
Each platform behaves differently, but if you could tell us the flavor of database platform, we can probably give you a better answer. To talk about the language for a minute, it's meant to be declaritive. Unlike programming where you would say, "Walk to the other end of the room 14 feet away using small footsteps in the order of left and then right while drawing breath in motion" you would say "Walk to the other end of the room" in SQL. The engine figures everything out in the between. It's written in a way that is similar to english. Go to the other room and bring me the blue book. SELECT Book FROM ROOM WHERE Book = 'Blue' The engine figures out how to handle all of the in between. Again, this is flavor dependent. Now to break it down further, it's architecturally dependent. I'm not going to go too deep into this because I'm going to butcher it. I just got done reading ~150 pages of detailed information on the physical and logical architecture of data and how the platform grabs that data. T-SQL Querying contains that ridiciulously delicious amount of detail and in a better all in one place than any other resource I know of. This page may help give you a little insight for SQL Server though. https://www.mssqltips.com/sqlservertip/4345/understanding-how-sql-server-stores-data-in-data-files/ This is another good one: http://www.itprotoday.com/microsoft-sql-server/logical-query-processing-what-it-and-what-it-means-you
A lot of these methods will probably work. https://www.red-gate.com/simple-talk/sql/t-sql-programming/calculating-values-within-a-rolling-window-in-transact-sql/
Probably not, but here's a quick guess: C:\Program Files\Microsoft SQL Server\MSRS12.MSSQLSERVER\Reporting Services\RSTempFiles Save often, use source control, live happy.
I'm thinking lag and lead window functions and possibly cross apply and top solutions with correlated subqueries. Pretty sure that'd utilize the row numbers. Depends on table architecture due to the query plan and how it was all designed
Yup, lost it. Thanks for help anyway.
Windows 7 (SP-1, 64-bit) is supported. I'm using 17.4 at work right now. https://docs.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms#supported-operating-systems
for row based databases, essentially, yes. b+ trees are most common, but merkel trees and other exotics also happen. for column based databases, more like a collection of trees. the selects are a lot more than a tree search, because of foreign keys, knowing how to bound off things, etc but "ranged tree searches" isn't *that* wrong [this is a quick way to get started](https://blog.jcole.us/2013/01/10/btree-index-structures-in-innodb/) if you're interested in learning more
I work with oracle, but my interest is just general so I have a better idea of what's going on behind the scenes.
&gt; SELECT Book FROM ROOM WHERE Book = 'Blue' not to take anything away from your response, which was great, but any competent optimizer will realize it doesn't have to access the database to prepare the response, which is 'Blue' except if Book isn't a key (which is implied by your use of "*the* blue book"), in which case it has to figure out how to return the correct number of rows of 'Blue'
Shit is that a real gig? I've been a Data/Testing Analyst (think QA stuff mostly) for an insurance software provider for about 2.5 years, would love to jump on board with a video game company.
When you say alias do you mean the table alias as assigned in the FROM clause?
I would imagine you could just use a find and replace so long as the aliases are actually being used. So alias.column do a replace on "alias." with "tablename." and you should be good. Then remove the alias itself.
Are you referring to a synonym? 
People use Red-gate product for that. At the time I tried to use Visual Studio Data Tools but it was very inconvenient. With just ssms you are forced to manualy manage script files and there is no automatic integration with target db. 
Thanks for the reply! I’m planning on trying to use TFS, but i guess my issue is when i look online it says to go to tools options source control, but i don’t see that in 17.4 while it seems to have been in previous versions. Did something change? 
We’re using Visual Studio with SSDT at the office. You can use Schema Compare to update your project to changes in the working database(es). It can take a few minutes. The biggest issues we ran into were some weird user permissioning objects and cross-database references. We had to remove the former, but the later requires creating DACPAC files which had to be added to each Database project.
We use this method as well. Another issue we encountered is that when you delete objects in SSMS, the Schema Compare update doesn't delete the source members in the solution. It leaves them with empty lines with a couple of GO statements in between. We have to manually remove them. I'll look into the DACPAC files. Thanks for the mention.
Yes
Yeah, that's the long way, or just use the option in red gate that lets you rename an alias and all the places that it appears. I've been doing that, but when there are a lot of them it gets tedious. Is like something I can just add to the ctrl+ky routine, or even better an option that does just the aliases
I have been told that aliases are evil, and it's no longer my decision.
let me guess, a developer?
Definitely. Designers requested analytical data from analysts to balance games and make adjustments. For instance how much premium currency spent the past week or how much is in the wild so they can create a currency sink. Or in premium games how often a character is picked and play time or things like that. It's a critical job highly linked to monetization in mobile. 
Awesome thanks :) will do!
I think that you will have to use VS not just SSMS.
Sorry, not the same thing. A synonym, though an alias for a table, is saved globally and can always be referenced across all queries. Sounds like yours is a plain old alias. Wouldn't have the slightest clue on how to deal with that. 
In an AMA they did I believe the SSMS product crew said that it was removed due to their re-write of SSMS, and that it would likely be added back at a future date - but they had no idea when. There was also this bit in which someone from MS provided a bit of a hack: https://www.reddit.com/r/SQLServer/comments/5dijyg/ama_microsoft_sql_server_1118/da4z4t4/ ...I have no idea if that will still work in the newest SSMS, but worth a shot if you're not interested in VS/SSDT.
It’s kinda crazy that they just do that, but I’ll take a look at these thanks. I guess i can always fall back a few versions too. 
I'll have to check. Should it be on/off? It's on my C drive, and I haven't done anything to the drive settings.
Yes, can connect to the database Yes, can even run queries through code while I'm testing If I click and select top 1000, I get the error...same one that I get if I run regular queries
Use Find and Replace and use the Replace All function.
If it is locally hosted then I would almost guarantee that the problem is on the database side not the client side. Permissions or something weird.
Are you using the red gate tool or the built-in ssdt source control/ssdt solution 
Sounds like this is the start of an Expanding Brain meme
I can assert that Red gate source control works for ssms 17.4 and tfs.
GROUP_CONCAT and FIND_IN_SET are non-standard MySQL functions if this is a theoretical question, why MySQL?
it's from a SQL test that was given to me.
Not redgate yet, but that’s starting to seem like the pragmatic option. Any have experience with the tool belt they offer?
My sincerest apologies, that sounds awful. 
Apex makes a source control add in as well
It should be off.
Instead of thinking in terms of classes, you should understand the idea of blocks (Oracle) or pages (MSSQL). They are fixed size (generally 4kB or 8kB) chunks of data. They contain data in binary format which may or may not be compressed. Blocks can accommodate raw data (leaf level) or tree structures. It's sort of like like serialized memory buffers, and pointers to other serialized memory buffers if I were to use programming terms.
If you're referring to circumstances where the same table is joined in 2 or more different places to different things, in those cases aliases are allowed, because there is literally no way around it. But that happens infrequently.
Sounds like a good tip, definitely will start delving deeper into this.
Thirding the SSDT with Schema Compare option. It's a little quirky (you have to set build= none sometimes to avoid intellisense issues). 
Maybe I'm way out of line (I'll admit to just a cursory glance as this mostly seems to have moved off of strictly-SQL territory), but isn't that join syntax retired for every flavor of SQL? I know I see a copyright 2017 at the bottom, but I would do a double check that this site's method is doing what you want and expect it to do, because it seems very possibly outdated. I just saw that you linked it twice, and wanted to try to offer a note of caution - hope you get what you need working and good luck.
We use Visual Studio and dacpacs - I prefer the way you can compile the project and find errors that you wouldn't when only using SSMS. And answering the original question - Source Control is built in to VS.
We use Git by enabling it in SSMS. This process is a little obtuse and I'm not excusing the lack of built in source control that you don't have to edit a config file to enable but it works well. https://blogs.technet.microsoft.com/dataplatforminsider/2016/11/21/source-control-in-sql-server-management-studio-ssms/
&gt; i.e. switching to INNER JOIN's makes the query take 0.2 seconds, but only includes rows that exist in all three tables Why don't you run the query with an inner join between table 1 and table 2, then between table 1 and table 3, and union the results?
I've tried that as well, the trouble I have with it is it returns two rows for each entry that exists in all three tables. (They contain a timestamp which is slightly different in each by a couple ms, but enough that they aren't shown as duplicates and trimmed.) I've tried to get around this by wrapping the whole thing in a select Distinct on the fields that do match, but it feels kinda hacky. Is this a better solution?
Haha honestly, I'm too lazy to try to figure out what the most pristine solution is, especially since I can't look at the tables directly. But I don't see why wrapping it in a select wouldn't work. You can also go back to the original query and try using "select * from table1 where exists (select 1 from table2 where field1 = a.field1 and field2 = a.field2 . . .) or exists (select 1 from table3 where field1 = a.field1 . . .)"
Ok appreciated. I'll keep playing around with it, see what happens.
full disclaimer - I work for redgate The toolbelt has quite a bit that people find useful - it seems like everyone has a favorite tool - but SQL Prompt is generally the fan fav - I call it redgate crack. The DLM Automation and ReadyRoll stuff is currently really popular too - as companies are beginning Continuous Integration and Continuous Development pipelines - and DLM Automation just plugs in with SQL Source Control's repository (the redgate one) and can build a nuget package and release from that.
Damn that’s pretty cool! Can you get me a job? 😃
Do you use tfs ? 
Give us some sample data to look at. Use the format below to generate a few examples of where your buckets are coming from, and what your debit table looks like, etc. | col1 | col2 | col3 | | :--- | :--- | :--- | | put | the | sample | | data | here | so | | we | can | see | | what | it | looks | | like | NULL | NULL | And it will come out looking like this when you post: | col1 | col2 | col3 | | :--- | :--- | :--- | | put | the | sample | | data | here | so | | we | can | see | | what | it | looks | | like | NULL | NULL | 
That is a complicated problem. Generally speaking, if you find yourself needing to use a loop in SQL you're either coming at the problem from the wrong approach, though, or SQL is not the technology you want. SQL is meant for working with sets of data. Try to rethink the challenge as "how do I apply this action to an entire set of data?" rather than "how do I loop through a set of data taking these steps in each iteration?"
In which tables is field1 unique?
MSSQL 2014
For the purposes of the simplified query field1 and field2 are the Pkey, and will appear identically across all three tables.
Update: The nightly run is failing due to 0 buffers available (I think our server may have been getting too full... not sure) Also when I get it to run during the day now, I end up getting MORE data than I should -- I run the same exact code on MySql as I do in Visual studio for the SSIS. The data is not full of dupes, it is unique records that don't seem to exist when I query in MySql. Basically for the data I get: MySQL | SQL Server | Difference | Date Range ---------|----------|----------|---------- 1,894,022 | 1,893,981 | 41(MySql +)|Jan-March 2,568,204| 2,567,972| 232(MySql +)|April-July 3,035,966| 3,289,782| -253,816(SqlServer +)|Aug-Dec This makes NO sense to me when none of it is duplicate values either. Any ideas on what this could be attributed to? 
Okay, so for clarification... The records in the "Bucket" table have an amount of money that is credited towards purchases of items in that bucket. However, you want buckets to apply to products in a specific order, correct? " A ranking order of the different fruits are established." Where is this ranking established? Is it applied in order of BucketID? edit. nvm, saw your edit
the client establishes it but it is not stored anywhere in the DB. For the sake of this example, let's just assume that the order will always be the same for every client. So i think we'd have to somehow establish the order in the SQL in a temp table of sorts. 
Are you using no lock in your queries? It's pretty hard to troubleshoot here without the query or having access to test.
I have Table Lock checked in the Destination, otherwise I'm not using any kind of nolock for my queries.
Yup, almost always.
TFS and git integration is in latest version of SSDT(the one that comes with the Visual Studio 2017 shell - v15.5.1) . Finally! 
 just updated the statement please check the new statement
 i just updated the statement please check this one may be it makes more sense to you
show tables it the sub query here. And it makes a lot of sense if you just read it show tables return something like +-------------+ | names | +-------------+ | some Name| ---------------- so what i am trying to do is to count these names 
Okay so "show tables" is a subquery that returns table names? You can't SELECT a value from multiple tables without a join. You also can't join based on the name of the column. Only based on data within. You're still very unclear and I think I am done here. 
I have no clue what is unclear to you just type show tables to mysql prompt and see what is the response
SHOW TABLES shows you the tables that exist on your database. Again, a basic search would [show you the MySQL syntax](https://dev.mysql.com/doc/refman/5.7/en/show-tables.html). We're not here to google things for you. I haven't used MySQL in a while and I might be out-dated, but I'm pretty confident in saying that you cannot use that output dynamically in a SELECT. Are you just trying to get a count of how many tables exist on your database?
Here is a pretty good starter for the internals of sql server disk based storage: https://www.red-gate.com/simple-talk/sql/database-administration/sql-server-storage-internals-101/
Your misunderstanding of SQL is so deeply profound I can't even imagine where to begin correcting your misapprehensions. I'd suggest starting with a basic SQL 101 primer.
i have nothing to do with tables themselves . `show tables` command returns the list of the tablenames and then i want to use this returned data as my table and query it in this case count it
So you work for an idiot or group of idiots. Is the pay worth it? 
In my experience there are a lot of legitimate reasons to use LOOPs and SQL is many times exactly the technology you want. I do agree with the general sentiment of what you're saying, and have often found a much simpler way to accomplish something without a LOOP, but there are other times where it is the only possible way to interpret large data sets and get the data you want from them. You could use other technologies but if that data is restricted in a production database that might be impossible.
Yes. You're correct that it does that. BUT, your cannot say SELECT COUNT(name) FROM (multiple tables).
It's not a design flaw, it's an understanding flaw. Your use case makes no sense in the context of a relational dataset. If you want to know what is in a table, you include the table in your SQL. The dataset is already designed so all like data items are grouped into tables, so why would your use case be "look through every single object in the database and then count up every occurrence of the name field regardless of its relationship to the rest of the data"? Actually it's not even an understanding flaw, judging by your snippy replies to answers to your dumb question, it's a humility problem - who, with no knowledge at all of a language or its uses, comes up with a nonsensical use case and decides that this decades old widely used and established language is *wrong* and they are right? e: I see your edit, do you mean select count(*) as n from (show tables); i.e., you're looking to count the number of tables?
SHOW TABLES doesn't *return data*, it **lists the non-TEMPORARY tables in a given database**, again from the [MySQL documentation](https://dev.mysql.com/doc/refman/5.7/en/show-tables.html). Look, I get it, you're trying to get something you need done finished. You don't understand what it is that you're trying to do, and you're getting frustrated with what seems like a potential language barrier (more than just SQL). But everyone here is taking time out of their day to try to help you solve your problem. For nothing other than a couple internet points and paying forward for the people that helped them when they needed it. Don't get attitude with us. Short answer is - you can't do what you're trying to do. You cannot use SHOW TABLES as a means of returning data into a larger query. It's not that we don't understand what you're trying to do, it's that you ***CANNOT DO IT***. I'm sorry, I know that's not what you wanna hear, but that's not how MySQL works. Now, let's start over and try again taking some *guesses* at what you want. I have to guess because instead of answering our questions about what you're trying to do, you just get hostile that we should totally know what it is that you're trying to do, and it's not "super super difficult to understand". When you use SHOW TABLES; you somewhere see "names" that you want a count of. You gave a little example of what you see, and it seems like "names" is a table, with a column "some Name." Okay, so you want a count of the number of rows in your table "names." Give this a try: SELECT COUNT(*) AS namesCount FROM names Again, for the 3rd time, I'll link the [function syntax of count](https://www.techonthenet.com/mysql/functions/count.php) where this is almost a word-for-word example that they give.
So to clarify, the ShoppingCartID is the transaction, and ShoppingCartProductID is simply an order of all transactions you have? How are the BucketFundsApplied being distributed for ShoppingCardID 2? I see a total of $20 applied but looking at your buckets am not sure how that would be calculated. Also how are you getting $20 applied for Celery?
 i see so my question was how can i treat the data returned from `(whatever here inside the parethesis)` as a data source like a table
 you dont have any clue who i am as well as i dont have any clue about you yet you can judge me thats one great feature of yours. Secondly languages are `designed` by some person just like designing a building or drawing a portrait and the reason why there are so many languages out there while core functionality of those languages are mostly identical is mostly about the design of the language. As you see some people like some particular design and some don't. Since when things stopped evolving and since when we measure the beauty or functionality of things by how old the thing is or how widely it is implemented ? 
&gt;you dont have any clue who i am as well as i dont have any clue about you yet you can judge me thats one great feature of yours. Isn't it amazing that when you communicate like a wanker people assume you're a wanker? If I see a building and I can't understand why it has been built the way it has, I consider my level of understanding of the construction trade and weigh up the possibilities: have I in the five seconds I've looked at this building arrived at an insight that a team of architects and builders with centuries of collective experience have missed, or is it more likely they know some important fact that I don't which makes it make sense? Because I'm not an egotist I usually take the latter view.
ShoppingCartID identifies a given grouping of products that the consumer purchased at a point in time. If consumerID 12345 was to leave the store and return to purchase more, they'd receive a new ShoppingCartID for this transaction. The ShoppingCartProductID is simply a primary key on the table. Gah - math! Celery should have $10 from the all veggies bucket and $5 from all products bucket. I'll update. 
**Declarative programming** In computer science, declarative programming is a programming paradigm—a style of building the structure and elements of computer programs—that expresses the logic of a computation without describing its control flow. Many languages that apply this style attempt to minimize or eliminate side effects by describing what the program must accomplish in terms of the problem domain, rather than describe how to accomplish it as a sequence of the programming language primitives (the how being left up to the language's implementation). This is in contrast with imperative programming, which implements algorithms in explicit steps. Declarative programming often considers programs as theories of a formal logic, and computations as deductions in that logic space. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/SQL/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
Thank you slingalot for voting on WikiTextBot. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). *** ^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!
 I just cannot understand what is the reason of this much anger. I happened to ask if there is any way to use something as a table structure which was returned from another query. I still think this is a really simple question (possible or not just simple to see the point) 
If LOOP is not the solution here then I'm open to other ideas. That said, SQL is the only technology in my stack right now to work with. 
**Code smell** In computer programming, code smell, (or bad smell) is any symptom in the source code of a program that possibly indicates a deeper problem. According to Martin Fowler, "a code smell is a surface indication that usually corresponds to a deeper problem in the system". Another way to look at smells is with respect to principles and quality: "smells are certain structures in the code that indicate violation of fundamental design principles and negatively impact design quality". Code smells are usually not bugs—they are not technically incorrect and do not currently prevent the program from functioning. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/SQL/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
I've already answered your question in another thread. From your example in the original question you just want a COUNT of the number of rows in a specific table. If that's not true, then start over asking the question that you *actually* want answered. And there's no anger in any of my posts. But you have consistently been nothing but condescending and rude to knowledgeable people trying to help you understand something you've made abundantly clear that you don't. Take a few minutes and read up on what MySQL (and more broadly, SQL) is and what you are trying to use it for. 
Oh I see, sausage has the same bucket as bread. So it gets 10+5 to make 15, and that is applied to the first item in the list, then apples gets the +5. Let me ask you this. Lets say SCID also had the other fruit beneath bread. Would you expect the All Fruit bucket to be applied to the Apple line?
You have to resolve your subquery before you can do that. Post entire query please.
the ordering of the table has no impact to the order the funds are applied. The funds are applied in an order specified by the client. In this case, lets just say the universal order is as follows: Apple Orange Pear Celery Brocolli Bread Sausage So, every consumer's bucket gets applied to their corresponding product(s) in that order. Bucket order would also need to be defined. For the sake of example, lets just go by the ID in the table for the bucket. 
I guess I'm missing somewhere here. Order isn't per se important or part of my question except in reference to where you want to see the credit applied in the output (first row, etc.) What I am not understanding are the rules for how the buckets are applied. What I took "all apples" to mean is that you have apples in your order, not that it your entire order is only apples. What I took "all vegetables" to mean is that your order has all of the items that are defined as vegetables, and what I took "all products" to mean is that the purchaser has at least one of each product type. You need to better define how these are applied in order to start seeing how this might work.
Here's my take on a solution. Let me know if you have questions. I think it could be a bit cleaner. http://sqlfiddle.com/#!6/ec50ff/56
Here do this, take your desired output and group it at the transaction level to sum the totals up, i.e.: | ShoppingCartID | TotProductCost | TotConsumerCost | TotBucketFundsApplied | | :--- | :--- | :--- | :--- | | 1 | NULL | NULL | NULL | | 2 | NULL | NULL | NULL | | 3 | NULL | NULL | NULL | It may be helpful to expand your sample data so that you have (1) ShoppingCartID for each individual case you expect to encounter, i.e. (all apples only, all apples + all fruit, all apples + all fruit + all products, etc.) Once you do that I think this will be a little simpler to approach.
this is really good. Now assume that the number of buckets could be infinite. Here, your SQL limits it to three buckets. Could I just loop through each available bucket until there are no buckets? For example, one client may have only one bucket, but another may have 10.
Yes, although I need to report on each individual product. So I need to know how many funds were applied to specific products. u/jc4hokies is close. I just need his approach to be applied to a potentially infinite number of buckets. 
I'm not worried about backing into reporting on individual products, I'm still trying to understand how you're applying each bucket and what the totals need to be based on that logic. After that it is arbitrary to give you individual product reporting and you can make a variety of decisions where and how you want to see the bucket applied (distributed across all rows, on the top row, etc.) 
Hm, I'll try and walk you through an example. Let's look at consumer 12345. This consumer has purchased an Apple. Now, let's look at the bucket table. This consumer has all those buckets available to them to be applied. So, if we apply those buckets over what they purchased by taking the bucket with the lowest bucketid first (123) then the next (124) then the next (125) etc... we then compute 1234's responsibility towards the apple. So in 12345's case, they are leave a bunch of money on the table. Now, let's take 45678. Again, this consumer has the same buckets available to them as 12345 did. But they purchased Celery, Apple, Bread. We then take the first bucket (Apple Only Bucket) and apply it accordingly based on the bucketprodcuttype table and in the order that I previously established in the above comment - to apple (since the consumer did not purchase any other fruits, it stops there). Then, we take the All Fruit bucket and apply it based that bucket's bucketproducttype properties - in this case just apple, which is already exhausted. Then we take the veggies bucket, and apply it - just to celery. Then we take the all products bucket and apply it to all the products in the consumer's shopping cart. By this step, apple is already covered, celery is already covered, so the only product that this bucket can be applied to is some delicious sausage. 
I think I'm starting to understand. Let me play around.
I'd honestly like this person to answer why they even have READ access to the database. If I were their manager, I'd cut that out real quick. 
What would happen in the first case if the customer bought (2) rows of apples? What would the expected case be?
You might need to be more specific? &gt; What are the options available to store user defined PL/SQL? You can store PL/SQL as triggers, views, functions, stored procedures, or in varchar fields in tables? &gt; How triggers can be used to pull data from other tables into non- base-table fields on a form Forms aren't part of PL/SQL. That's a question better asked about whatever it is you're writing the form in. You might need to explain your question better to get a better answer.
products are distinct to a consumer. So a consumer can only purchase one apple, one bread, one sausage etc... good question!
&gt; Then we take the all products bucket and apply it to all the products in the consumer's shopping cart. By this step, apple is already covered, celery is already covered, so the only product that this bucket can be applied to is some delicious sausage. Lets say there was an additional redundant item on this order. Would this bucket apply only to sausage, or be split between both rows?
Good question. It would apply to the additional products based on that order I outlined a few comments ago. Give me an example. It might be easier to illustrate that way. 
Makes sense, I wonder if most of these are full-time or contract based positions.
I gave you an example. Lets say after sausage there was some other random item on the order (doesn't matter what) -- your "all product" would be applied where? All on sausage, doesn't matter, 50/50 split, alphabetically ordered where it gets applied on the top, bottom, etc.?
It would depend where the other product is ranked. A few comments ago I ranked all of the possible products. So the bucket would be applied in that order. 
So if the other item is ranked lower it would receive the total all products bucket and the next (n) items would just show as a 0?
Did the same thing with a recursive cte. http://sqlfiddle.com/#!6/ec50ff/88 I might see if there's a way to do the same with a normal aggregate.
Sorry. I realize what your question is now. Assuming the next product is ranked lower, sausage would receive the funds. If any are left over they would then be applied to the next product. 
So in this case sausage would receive it because sausage is ranked higher? And then naturally if the next item were higher than sausage it would receive it and sausage would receive a 0 (providing sausage didn't qualify for a bucket of its own.)
I'm not sure I understand his needs completely but you seem to have an understanding. When he talks about needing this for an 'infinite' number of buckets I'm seeing a problem with this: ORDER BY CASE WHEN scp.ProductName = 'Apple' THEN 1 WHEN scp.ProductName = 'Orange' THEN 2 WHEN scp.ProductName = 'Pear' THEN 3 WHEN scp.ProductName = 'Celery' THEN 4 WHEN scp.ProductName = 'Bread' THEN 5 WHEN scp.ProductName = 'Sausage' THEN 6 WHEN scp.ProductName = 'Brocolli' THEN 7 It would seem he's saying this would need to be dynamically generated? Or am I misreading him?
&gt; Currently, the order for each bucket differs per client so the order would need to be variable. For illustrative purposes, set the order as: Apple, Orange, Pear, Celery, Bread, Sausage. That code is based on this spec. It would need to be set somehow, but OP indicated it is taken care of. &gt; When he talks about needing this for an 'infinite' number of buckets This bit handles this spec. Every product is CROSS JOINED with every potential tier. Something has to define which buckets have priority, so whatever that is can be dynamically joined to the list of products. FROM (VALUES(1),(2),(3)) AS t(BucketTier) CROSS JOIN (VALUES('Apple'),('Orange'),('Pear'),('Brocolli'),('Celery'),('Bread'),('Sausage')) AS p(ProductType)
Full time. For sure. It's a critical role in the industry. It's how the evaluate how changes impact in game economies, player retention and DAU, as well as spending behaviours. Data analyst are super valuable in the gaming industry. 
It would work applying buckets alphabetically, but there are benefits to using Tiers. Tiers defines the order in which buckets are applied, so at the very least you want your "All Products" applied last. You also gain some efficiency having multiple Buckets can share the same tier (like "All Fruits" and "All Vegetables"), but can't have a product in a single tier more than once. I've worked up a solution to dynamically assign Tiers to Buckets in a logical way, while avoiding the pitfalls. http://sqlfiddle.com/#!6/ee9a8/5 ^ _Tiers would normally be the temp table #Tiers, but that's not allowed in sqlfiddle.
Your normalization is solid, but naming could use some work. It will help from a readability and understanding perspective in the future, and is just good practice. Keep table names plural. Users, Stocks, UserStocks would be how this looks usually Keep your primary keys separate from your data. UserID should not be tied to the data at all. I am guessing your intent would be for this to be paperclip300 for you, and your email as the email. Normally you would want to see UserID, Username, Email, where UserID is an integer. Most RDBMS will have a way to auto increment this. Do the same with Stocks. StockID, Ticker rather than just Ticker. &gt; That means I'd (I guess) a join [Tracked] to [Stocks] PER USER, then figure out how to return all this back to the actual code and fire off emails that way, but that's really dirty. One way to do this is using a stored procedure, with UserID as a parameter. select s.Ticker ,s.Price ,case when s.Price &gt; us.AbovePrice then 'Sell' when s.Price &lt; us.BelowPrice then 'Buy' else 'This is just good to have for testing' end as "Action" from Stocks s join UserStocks us on s.StockID = us.StockID join Users u on us.UserID = u.UserID where (s.price &gt; us.AbovePrice OR s.price &lt; us.BelowPrice) and u.UserID = @user &gt; Additionally, I'd likely need to run another query every day to check for entries in [Stocks] where there's no user tracking the stock. pseudocode: select s.tracker, s.price from stocks s where s.stockID not in(select distinct us.stockID from UserStocks us) Basically, you are selecting stockIDs from the stock table that do not exist in a subquery that is pulling a list of distinct stockIDs from the UserStocks table. NOT EXISTS could be used as well, and I believe both ways will use the same plan in modern databases.
[I have a rough outline here](https://puu.sh/yZAxZ/f1776d914b.png). This is before the changes you suggested which I'll do. You say seperate primary key from data - can you clarify this? I know not to use email as Pkey because in a relationship table a string comparison is slower than an int comparison. So Pkey should be an int. I'm letting userId be some auto incremented #. Again, a little confused on that separation. Those SP's are pretty clean. However, wouldn't this mean running it from the server for every user, every X interval? The goals are: Every X interval, hit an API and update stock prices. Easily doable. Then, grab every user+stock info where they have gone above/below their price threshold. If it runs once per user, it's going to mean every X minutes, we hit the DB a shit ton. I'm sure I can come up with a query to do it for the whole DB and just return something in bulk. (I'd have to figure this out with language specifics). But then, I'd need to group those by user and generate a report (so if 2 stocks went above Y price we don't send 2 emails). I know C# has linq (this would be in Python), but I don't know if bulk pulling ALL that data, then running calcs in code is better than whatever alternative. This is more of a high level implementation question. I'm trying to figure out if there's a better way to do it. Regarding that stocks not being tracked question - That query works. I'm trying to figure out if I can do something with referential integrity. I know I can do it the other way - if I delete userId/StockTracked row, delete stock. But if there's some kind of only delete stocks not being tracked in that table I don't know. Also, **Thank you so much for your help.** It means a lot. Your advice has been really helpful; I've only done DB stuff in a non-DB focused class, so I'm new to all of this! 
I think if I do it my way with 1 query, I can return back objects representing each row (Basically what an ORM does(?)). Then I can go through and hash each record in O(N) and aggregate that way. Storing the data once hashed will be interesting - whatever the Python equivalent of a List is. I guess that works?
Compare this query on source and target to see if there's a max length, and data's being truncated. SELECT CHAR_LENGTH(TextField) AS FieldLength, COUNT(*) AS RecordCount FROM MyTable GROUP BY CHAR_LENGTH(TextField) ORDER BY FieldLength DESC;
Maybe just use a case statement in the select? Something like SUM(CASE WHEN X = WEEK 14 THEN 1 ELSE 0 END) AS 'WEEK 14'
Typically, this is done on the visualization side of things. Otherwise, you're stuck doing dynamic SQL.
That is how I have been handling things like this, but had hoped there was a way to do it in SQL. Guess I'll stick to my old ways. Thanks!
If you gave us a sample data set and a sample outcome needed data set, I may be able to help more. But dynamic SQL is a given, but you can probably shorten it. It also sounds like pivot may help. https://social.msdn.microsoft.com/Forums/sqlserver/en-US/bf4252eb-1422-4049-82a0-353c84011e90/how-to-pivot-a-table-with-unknown-number-of-rows?forum=sqlgetstarted
&gt;Because this report is pulling data directly from the system, there are no targets in the database. How are you getting them then? Do you insert their goals manually?
I am trying to enter goals. I’m figuring out a way because our application db does not have fields for target.
You can enter them into a @ or #table in SSRS and join to it in your query?
`RAND() AS Target` You can make your own dataset like this: SELECT * FROM (VALUES('John Smith','Expense',0.08), ('Betty White','Revenue',0.13)) AS v(Name,Type,Goal);
Parameters could work, but wouldn’t it be limited? Help me understand because I may not be thinking through.
Bam! This could do. Let me try
Right but for production wouldn't you do something like CROSS JOIN (SELECT DISTINCT FROM) so that it works without hardcoding the values in? That was what I meant.
Happy cake day!
Some dbms have a "pivot" function that might do something like this for you.
I'd create a table in my reporting db to pull the values from. Reason being is that they'd be more easily adapted and added to what with not having to go into SSRS and modify the report structure everytime.
Is this ms sql server?
Yes
Haven't heard of using a pivot (outside of Excel) before. I'll look into this - thanks!
does it look terrible? yes, the formatting is awful should it be on your portfolio? maybe, if it works really well, performed more efficiently than previous versions, solved a problem nobody else could, etc. -- these are the things that are worth mentioning
A basic insert didn’t work, threw a few errors around the XML bit
Do you have any experience with CROSS APPLY / OUTER APPLY? I think you could use this to improve your subqueries. Also, consider a better formatting practice and stay consistent with it.
Need more info to answer this question. It depends on how you are able to get the information from Aldelo. Do they provide flat files? 
Yeah unfortunately you're going to need to provide more info. Is Aldeo SQL based, can it export csv? Also SQL MMS is probably not the best tool for you in this situation. Excel with kutools can do more than enough to help you analyze data. If this Aldelo thing is SQL based look up power query. If not see if you can output files to something flat and simple like CSV and use pivot tables.
Is that get xml a custom function? Looks like you're passing a query to it. I'd guess the engine might not be able to handle that syntax. Maybe make your select a view and then insert using the view?
 UNION SELECT NULL, NULL , NULL , NULL , NULL , NULL , NULL , NULL UNION
It seems Adelo has an export function. You would need to start by seeing what the capabilities are and what the structure of the files are when exported, then from there import them into a database in order to start exploring the data. https://www.youtube.com/watch?v=t3jbdJrkJfc
[removed]
Your point?
[removed]
Comon now, that formatting ain't bad. At least there is formatting. Strictly speaking if we were interviewing candidates out of school this would singularly let me know the applicant had a basic 'mastery' of SQL. I'd want to hear from them how they come to places like this trying to improve, and ask them questions about how they might improve it... why they made certain choices, or what certain components do in order to establish they actually know what they're doing. Overall it's a very complex query that is a good addition to a portfolio, but the thing with portfolios is they need to be geared towards the front end reporting. Not one job I've ever applied to has ever asked to see queries I've written. My portfolio shows them what the results look like when they're all dressed up for user consumption, and then I speak to writing queries as a secondary thing... even though it is the primary skill.
So i can back up my database to an external hard drive and it gives me a .mdb file. 
Does this topic describe what you're trying to do: https://social.msdn.microsoft.com/Forums/sqlserver/en-US/388885f0-d5dd-4087-84d0-da1e87423666/how-to-import-mdb-files-into-sql-server-2008?forum=sqlserversamples
I suggest using some CTEs. 
What's the point of unioning those nulls in?
No CTE's in MYSQl.
https://dev.mysql.com/doc/refman/8.0/en/with.html Says here it's supported.
Current version is 5.7 you are looking at 8.0 which is still a Release Candidate, A+ Google skills though. Changes in MySQL 8.0.5 (Not yet released, Release Candidate) Changes in MySQL 8.0.4 (Not yet released, Release Candidate) Changes in MySQL 8.0.3 (2017-09-21, Release Candidate) Changes in MySQL 8.0.2 (2017-07-17, Development Milestone) Changes in MySQL 8.0.1 (2017-04-10, Development Milestone) Changes in MySQL 8.0.0 (2016-09-12, Development Milestone)
.mdb is a Microsoft Access database file. Do you have MS Access or can you get it?
Seems like you may be having a Datatype issue if the other suggestions didn’t work. In this case, I usually create a table using the query. CREATE TABLE MySchema.tableNamr AS SELECT ... FROM sourcetable WHERE ... This creates a new table with the correct data types that will hold the data, and I adjust the destination table I’m trying to import to accordingly, or I add the appropriate CAST/CONVERT in order to get the data to fit.
&gt; I do not think MYSQL allows cross apply. i doesn't but you forgot to mention which platform you're using, so you wasted this kind person's time
60 is fine ,at least give a hint to the reader is what I would suggest and don't elaborate much
Not a MySQL guy, but I'd think you could replace all those subqueries with a single query using case statements in the aggregation. If also suggest looking into Group By Rollup so you don't have to essentially redo the same query to get the overall totals. Then it's my preference never to alias tables with t1, t2, t3, a,b,c etc because it becomes harder for someone to read later. Always alias with something that actually describes what the data is. If I saw this, I'd consider you entry level with SQL
[ACID Principle](https://en.wikipedia.org/wiki/ACID). One transaction will inevitably come after another and will be processed in order as long as you're using a standardized database system.
I'm using MS SQL
Not yet but looks like I will be getting it. Thank you very much. 
The short answer is; SQL Server will handle them separately. "The exact same time" from the perspective of a human clicking the submit button and a CPU cycle are vastly different. 
I understand what you say about visualization but this goes on a start page so I write it how they want, they want students with totals, a blank line and team totals for the teacher to see, here is an example of the start page. https://imgur.com/a/JkUqW. Ahh yea it comes form SC table, thank you missed that. MYSQL does not have the same grouping functions as SQL server, where you have to group everything that is non aggregate. I can group solely by student number and get results for each student number. Because as far as I know it is determinate for the other fields. MYSQL does not require each non aggregate to be grouped as SQL server does. 
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/pLH80MI.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dsmof0h) 
My problem with group by roll up is that it will not allow the blank line, that line will at min need a studentID or Name in the column for ordering. You also lose the ability to order by since they are mutually exclusive, and it never goes in alphabetical order. I did not even think of a case statement, thank you. 
Thank you very much, this really cleared a lot up for me. So I take it. kind of create a story behind why you needed this query, why you took and certain steps and what you did to overcome issues and needs. Really appreciate the reply!
Awesome, appreciate the advice, ty! 
Yep. You can always attach the query at the end so they can see it. I would advise making your formatting more consistent. More often than not you aren't going to have someone ask to see it and you just learn to talk your way through the interview and demonstrate a competency with SQL. You might find someone who wants it, but often you will be interviewed by people who are not SQL developers.
we don't do homework, *especially* without the original assignment what have you got so far? 
Use a UNION? If I understand your question correctly, it sounds like you want a UNION I'm assuming that the 2nd field in your 2nd query is comparable to "balance" So try this? SELECT PROJECT.PROJ_NUM, PROJ_NAME, PROJ_VALUE, PROJ_BALANCE, (PROJ_VALUE - PROJ_BALANCE) AS PROJ_REMAINDER FROM PROJECT UNION select PROJECT.PROJ_NUM, NULL as proj_name, NULL as proj_value, sum(ASSIGN_CHG_HR * ASSIGN_HOURS) AS CURRENT_CHARGES, NULL as proj_remainder from ASSIGNMENT, PROJECT GROUP BY ASSIGNMENT.PROJ_NUM
the operator you are looking for is INNER JOIN
Thanks 
Nothing happens at the same time. One will be before the other. 
Here is the startpage, all of that is SQL that we can change in and out and it automatically displays in the format you coded. https://i.imgur.com/pLH80MI.png I do not have access to editing any php/jscript/html/css or even the database. I do all in house reports/PowerBI and the developers overseas do the rest. They have so much going on, getting them to add a line or something small is at the bottom of their list so I just code. I even code html into my queries instead of getting colors added etc. Here is an example of adding colors based on level. https://imgur.com/a/IC0rR 
Oof. That's a rough situation, but I get why you are doing it now. I was picturing this going into power bi since you had mentioned that, not straight to a webpage much less one no one will edit the code of
Pivot would handle this, the bad part is that you have to explicitly list every column you want to pivot. Great when you have a few values but not so great even you have 52.
My approach would be to search google for "*swap values in SQL*".
a=a+b b=a-b a=a-b
 update &lt;table&gt; set a = 2, b = 1;
This gave me a good chuckle. 
Probably decimal. Avoid floating point data types like double.
&gt; Avoid floating point data types like double. floating point is **not precise**, which is crucial if you have like 6 digits to the right of the decimal point
Right. Never use floating point to store currency. I don't know a lot about bitcoin but the smallest unit appears to be a Satoshi (0.00000001) so you'll need to use 8 scale and precision thar accounts for your max expected value. Using omething like Decimal (18, 8) would allow you to store values up to 9,999,999,999.99999999.
I'd do something like ```` SELECT date, SUM(value) FILTER(WHERE fact = 'Cash') as Cash, SUM(value) FILTER(WHERE fact = 'Liabilities') as Liabilities, SUM(value) FILTER(WHERE fact = 'Assets') as Assets FROM some_table GROUP BY date ```` Take a look at [this](https://blog.jooq.org/tag/pivot/) blog post.
Technically correct, the best kind of correct.
Is the column you're comparing against a varchar field or a datetime field? You'll have to do a conversion one way or the other (hopefully not on the field in the table) [This article explains PL/SQL date formats and how to convert a string to a datetime](http://www.oracle.com/technetwork/issue-archive/2012/12-jan/o12plsql-1408561.html)
And it seems like that article is for writing functions? They are talking about "Create", "Begin" and "End"?.... I am just trying to query rows.
It explains parsing dates &amp; times. [Here's something a bit more explicit](http://www.sqlines.com/oracle-to-sql-server/to_date)
To figure out the position of the "@" character in the email string, you can do select charindex('@', email) from users Then, you want to be able to get just the part to the left of that character, so you could do select left(email, charindex('@', email) - 1) from users Now that you have the portion of the email to the left of the "@", you can replace it with "****" or whatever you want to use select replace(email, left(email, charindex('@', email)-1), '****') from users [Link to SQL Fiddle](http://sqlfiddle.com/#!6/3bd3c1/11)
You could use REPLACE and SUBSTRING to do it. `SELECT [Masked email] = REPLACE([email], SUBSTRING([email], 0, (CHARINDEX(‘@‘, [email], 0)), REPLICATE(‘*’, LEN(SUBSTRING([email], 0, (CHARINDEX(‘@‘, [email], 0))))) FROM [table]` I may have screwed up the brackets but that should work. You could use LEFT instead of SUBSTRING so you don’t have to use a start point argument. You would have to do this with each field in the selection. You could also create a scalar function with a string parameter. `CREATE FUNCTION [fnMaskEmail] (@email nVarchar(MAX)) RETURNS nVarchar(MAX) AS BEGIN DECLARE @masked nVarchar(MAX) SET @masked = REPLACE(@email, SUBSTRING(@email, 0, (CHARINDEX(‘@‘, @email, 0)), REPLICATE(‘*’, LEN(SUBSTRING(@email, 0, (CHARINDEX(‘@‘, @email, 0))))) RETURN @masked END ` Then you could call `SELECT [Masked email] = [fnMaskEmail] ([email]) FROM [table]` Hope that helps. Please double check my syntax as I’m typing this on a phone.
Use to_date: `Where search_date = to_date('01/12/2018 12:00','DD/MM/YYYY HH:MM')` ETA: don't be a heathen tho, use ISO date formats like God intended.
That's going to do a string comparison, not a timestamp comparison, no? /u/jred250 you should use the [to_timestamp](https://docs.oracle.com/cd/B19306_01/server.102/b14200/functions193.htm) function
Yeah, I rarely use times and was going from memory
What are your needs? Do you have excel? Are you running a Microsoft SQL server? 
Look into Microsoft power bi, super nice tool that syncs directly with you sql server
&gt;open source
Take a look at airbnbs superset
Whoops my bad, don't think there are many options then 
I've been working with Pentaho for a while now developing BI solutions for SMEs. I suggest you take a look at Pentaho Dashboard Designer for visualisations with D3js although it might not be as dynamic as you would like. For analytics there's a bunch of pivot table plugins you can install. Saiku is a good one. I've been using Excel Power Pivot connected to Pentaho since most businesses already have Excel and know how to use it.
You can use the Ace OLEDB driver to read from and write to Excel spreadsheets. Though I haven't found a way to set formatting, meaning integer columns might import as text.
JasperReports is amazing! I just don't know if it's user friendly :/
What did you not like about it?
As someone who is migrating from Pentaho (enterprise licensing) I don’t recommend them anymore. Their support is poor, their licensing has become expensive for what they provide. 
Is this kinda what you're looking for? --Today select b.batchNo ,b.ExpDate ,j.JuiceName ,j.NicLevel ,j.Size from batch b join juice j on j.juicekey = b.juiceKey where b.[ExpDate] = convert(date, getdate()) --Tomorrow select b.batchNo ,b.ExpDate ,j.JuiceName ,j.NicLevel ,j.Size from batch b join juice j on j.juicekey = b.juiceKey where b.[ExpDate] = convert(date, dateadd(day, 1, getdate())) 
I know of two ways to accomplish this and have done both -- 1. Upload the Excel spreadsheet into the database, basically creating a temporary table, then join the requested columns to your table. It's a bit cumbersome to upload the Excel spreadsheet each time because you have to manually create the table before uploading the spreadsheet. There are a few IDEs that can semi-automate this process so it's not always too time consuming 2. Another way, more automated way, is to use python + pandas. You can upload the file as a pandas dataframe and then perform a SQL query to grab the columns you're interested in. Then do a join in python. The advantage here is that the whole script is semi-automated and you're not creating any database tables. The downside is that you need to learn python. 
how are you relating juice and batch?
Specific courses you recommend? I've browsed the Essential SQL w/ the XAMPP software. Relational DB Fundamentals..
1. SQL is important for data science mainly to pull and clean data. Where using a SQL IDE, python, or R, you'll usually be writing SQL to pull data before manipulating and cleaning it up. 2. Are you asking for learning tools? The tutorials like sqlzoo and courses like coursera are good to learn how to construct and understand queries. But I never found them to be great for practice because there was no sandbox or live datasets I can play around with. There aren't many options around for an open database. You can try a startup called stratascratch.com. They have datasets you can use to practice SQL and will be adding a feature that allows you to upload a CSV and use SQL to manipulate your data. 3. You don't need to be a specialist in SQL to become a data scientist. You just need to be good enough to pull data -- sometimes it's as easy as a select *, then clean it up using R. Any data science project requires a lot of data cleaning. So you can try a kaggle competition if you just want to get your hands dirty. 
You can try stratascratch.com. They have live databases and a web-browser IDE so you don't need to install anything. I heard they'll be adding the ability to import Excel files and convert them to db tables so you can run SQL queries on them. That might help you practice SQL on data that's similar to your work.
It looks like it would run but I've had problems with the GETDATE function which is why I'm using DATEPART. I've tried running your code a couple different ways, including raw, modifying the field calls and calling the fields my old way and just using your WHERE clause but its giving me an "undefined function convert" error Sorry, It's probably something I'm doing. I took a class in college years ago and haven't touched this stuff since then. Any idea where I went wrong? 
Are you always joining on the same column? If you're comfortable with SQL just load what you need to join in into a script in ssms using ' where X in ('x ', 'x ') then dump the output back into a new excel sheet and vlookup back onto your original data. Clumsy but simple and works. I'd personally load into access and join to a pass through query. 
What sql server version are you using? 
I'm sorry for taking so long to respond. Work has been so crazy I haven't had a chance to even look at this until today. I gave it a shot and it works!!!!! You're a miracle worker, thank you so much!
Do you have a MySQL server running on localhost?
If you installed the workbench only, you need to install MySQL server or point the connection to a machine that is running MySQL server.
So, how would I go about installing the MySQL Server through the command line? Edit: Does this work? https://www.digitalocean.com/community/tutorials/how-to-install-mysql-on-ubuntu-16-04 Edit2: I already seem to have it alongside with Client and Workbench.
Access 2007 I got the tag wrong didn't I? 
Ah, yea. I thought your example code looked a little weird ;) Give this a shot. --Today select batch.batchNo ,batch.ExpDate ,juice.JuiceName ,juice.NicLevel ,juice.Size from batch join juice on juice.juicekey = batch.juiceKey where batch.[ExpDate] = DateValue(now()) --Tomorrow select batch.batchNo ,batch.ExpDate ,juice.JuiceName ,juice.NicLevel ,juice.Size from batch join juice on juice.juicekey = batch.juiceKey where batch.[ExpDate] = DateValue(dateadd(d, 1, now()))
I stopped at Step 1 because I already have Server installed.
YES!! Thank you, after a minor tweak it ran and came back perfectly &gt;select batchID.batchNo &gt; ,batchID.ExpDate &gt; ,juiceID.JuiceName &gt; ,juiceID.NicLevel &gt; ,juiceID.Size &gt;FROM JuiceID INNER JOIN BatchID ON JuiceID.JuiceKey=BatchID.JuiceKey &gt;where batchID.[ExpDate] = DateValue(now()) Mind helping me understand what the WHERE clause is doing? Thats where your and my code differ the most. Why the brackets on [ExpDate] I can google the datevalue operator easily enough Why does your code not display PK and FK? Those are the values I was looking to omit. I'd love a link just as much as an explanation, I just want to be sure I know why the code is working But again THANK YOU 
I'm away from my machine for a little bit but I believe this is what you're trying to do: https://stackoverflow.com/questions/1237068/pivot-in-sqlite 
Do step 2.
I get the following: isaac@isaac-HP-Laptop-15-bs0xx:~$ mysql_secure_installation Securing the MySQL server deployment. Enter password for user root: Error: Can't connect to local MySQL server through socket '/var/run/mysqld/mysqld.sock' (2) 
I'd start with a query that does a left outer join from table 1 -&gt; 2, and work out your logic for figuring out the parent, e.g. case when. Second step would then make a hierarchical query on top of your "inline view". You'd use "connect by" on your computed parent.
If you don't have many columns you can do something like this: Select cluster_name ,sum(case when annotation_type = 'miRNA' then 1 else 0 end) miRNA ,sum(case when annotation_type = 'antisense_RNA' then 1 else 0 end) antisense_RNA ,sum(case when annotation_type = 'sense_overlap' then 1 else 0 end) sense_overlap from cluster left join annotation_type on annotationtype.cluster_id = cluster.cluster_id left join annotation on annotation on annotation.annotation_type_id = annotationtype.annotation_type_id group by cluster.cluster_name Otherwise you'll have to do a pivot as mentioned by /u/mkingsbu. Unfortunately this can be kind of a pain in the ass to get working correctly. 
The `GROUP BY` clause is the trick. It does a different aggregate for each date, and outputs a row for each one. It actually doesn't matter that you use `SUM` as long as you only have one cash entry per date. I just needed an aggregate function that returns its input when it only gets one row. Max or min would have worked just as well.
[removed]
In console: sudo netstat -lpn and you will see information about all IPs, ports and services. Now you can check, is mysql use 127.0.0.1:111 or not.
I actually started doing these types of request with R, which I highly recommend you use if you do a lot of reports. R makes this kind of stuff really easy, and there's a library called RODBC which allows you to query your SQL server and make a data table from the query. I made a little C# forms app, they provide a spreadsheet, maybe some dates, and it queries our server, merges the data tables (a join in R), and runs some analysis on the 2 data tables, spits out a csv. Now I don't get those requests. It's great!
For example, from one of my servers: tcp 0 0 0.0.0.0:443 0.0.0.0:* LISTEN 1577/nginx my nginx listen tcp port 443 on all local interfaces. Maybe your mysql server listen only local IP, for example 192.168.0.1, but not listen 127.0.0.1
&gt; sudo service mysql status I get the following result: isaac@isaac-HP-Laptop-15-bs0xx:~$ sudo service mysql status [sudo] password for isaac: ● mysql.service - MySQL Community Server Loaded: loaded (/lib/systemd/system/mysql.service; enabled; vendor preset: enabled) Active: inactive (dead) (Result: exit-code) since Mon 2018-01-15 23:36:00 EST; 8h ago Process: 32674 ExecStartPre=/usr/share/mysql/mysql-systemd-start pre (code=exited, status=1/FAILURE) Jan 15 23:35:59 isaac-HP-Laptop-15-bs0xx systemd[1]: mysql.service: Control process exited, code=exited status=1 Jan 15 23:35:59 isaac-HP-Laptop-15-bs0xx systemd[1]: Failed to start MySQL Community Server. Jan 15 23:35:59 isaac-HP-Laptop-15-bs0xx systemd[1]: mysql.service: Unit entered failed state. Jan 15 23:35:59 isaac-HP-Laptop-15-bs0xx systemd[1]: mysql.service: Failed with result 'exit-code'. Jan 15 23:36:00 isaac-HP-Laptop-15-bs0xx systemd[1]: mysql.service: Service hold-off time over, scheduling restart. Jan 15 23:36:00 isaac-HP-Laptop-15-bs0xx systemd[1]: Stopped MySQL Community Server. Jan 15 23:36:00 isaac-HP-Laptop-15-bs0xx systemd[1]: mysql.service: Start request repeated too quickly. Jan 15 23:36:00 isaac-HP-Laptop-15-bs0xx systemd[1]: Failed to start MySQL Community Server. 
Something wrong with mysql server. sudo journalctl -xe and only read, read... Too many cases, why service can't start.
Here is what I get from sudo journalctl -xe: isaac@isaac-HP-Laptop-15-bs0xx:~$ sudo journalctl -xe -- Subject: Unit mysql.service has finished shutting down -- Defined-By: systemd -- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel -- -- Unit mysql.service has finished shutting down. Jan 16 08:35:46 isaac-HP-Laptop-15-bs0xx systemd[1]: Starting MySQL Community Server... -- Subject: Unit mysql.service has begun start-up -- Defined-By: systemd -- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel -- -- Unit mysql.service has begun starting up. Jan 16 08:35:46 isaac-HP-Laptop-15-bs0xx mysql-systemd-start[5734]: MySQL system database not found in /var/lib/mysql. Please run mysqld --init Jan 16 08:35:46 isaac-HP-Laptop-15-bs0xx systemd[1]: mysql.service: Control process exited, code=exited status=1 Jan 16 08:35:46 isaac-HP-Laptop-15-bs0xx systemd[1]: Failed to start MySQL Community Server. -- Subject: Unit mysql.service has failed -- Defined-By: systemd -- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel -- -- Unit mysql.service has failed. -- -- The result is failed. Jan 16 08:35:46 isaac-HP-Laptop-15-bs0xx systemd[1]: mysql.service: Unit entered failed state. Jan 16 08:35:46 isaac-HP-Laptop-15-bs0xx systemd[1]: mysql.service: Failed with result 'exit-code'. Jan 16 08:35:46 isaac-HP-Laptop-15-bs0xx systemd[1]: mysql.service: Service hold-off time over, scheduling restart. Jan 16 08:35:46 isaac-HP-Laptop-15-bs0xx systemd[1]: Stopped MySQL Community Server. -- Subject: Unit mysql.service has finished shutting down -- Defined-By: systemd -- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel -- -- Unit mysql.service has finished shutting down. Jan 16 08:35:46 isaac-HP-Laptop-15-bs0xx systemd[1]: mysql.service: Start request repeated too quickly. Jan 16 08:35:46 isaac-HP-Laptop-15-bs0xx systemd[1]: Failed to start MySQL Community Server. -- Subject: Unit mysql.service has failed -- Defined-By: systemd -- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel -- -- Unit mysql.service has failed. -- -- The result is failed. Jan 16 08:40:45 isaac-HP-Laptop-15-bs0xx NetworkManager[951]: &lt;warn&gt; [1516110045.6595] device (wlo1): activation-stage: schedule activate_stag Jan 16 08:41:34 isaac-HP-Laptop-15-bs0xx sudo[5825]: isaac : TTY=pts/2 ; PWD=/home/isaac ; USER=root ; COMMAND=/bin/journalctl -xe Jan 16 08:41:34 isaac-HP-Laptop-15-bs0xx sudo[5825]: pam_unix(sudo:session): session opened for user root by (uid=0) lines 1703-1744/1744 (END) 
Yeah, that's not going to work. First, I'm using Fulltext searches and not the far-slower like substring search, and group_concat would be making very large strings of arbitrary sizes per logged day.
Definitely the middle option, then. Just fulltext search one parameter to get an initial list of possible dates, and then pare down the rows you need, by removing dates that aren't in the result sets of fulltext searches on the other parameters.
I just got done installing and troubleshooting mysql on debian last night. (Mariadb really.) When I ran hostname -l, it gave me 127.0.1.1 because on the same server I'm running MySQL I'm running apache2. You can use ifconfig to check your ip of the box. In MySQL workbench for a connection, I had to plug in the username for an account on the server plus the ip and port of the server. Then I had to plug in the 127.0.1.1 for the host and plug in a MySQL username and it was on the default port of 3306(I believe that's the default?). Afterwords I was able to connect via workbench.
Have you done any real work yet on mysql? If not I'd try an uninstall and reinstall following the document. 
So you successfully uninstalled MySQL, then you reinstall MySQL repeating the steps in the document. When you get to step 2 though, the same error you posted as before persists?
Yes
You said it worked well on your windows 10 machine. How is the setup currently? Do you have a tower with ubuntu running mysql and then you have a windows 10 device that can use workbench to connect to it? Or are you trying to run workbench on ubuntu that's on a tower? Is ubuntu a vm on your windows 10 device?
Before I try what you stated above, is there any way to just work locally? The reason why I am asking is that my wifi does not work on my Ubuntu machine.
You should be able to connect to MySQL with workbench on your Ubuntu running MySQL without wifi I believe. It's a local connection setup, so internet should not be needed. From what I saw in previous comments, MySQL won't even start up or let you into it, that to me means there's a configuration issue with the MySQL install or a weirdness to the Ubuntu machine with the MySQL setup. I just got MySQL (mariadb really) working on my Debian last night. I was running into localhost issues too. Something you should try really quickly too, is run hostname -l. My MySQL was actually on 127.0.1.1 instead of 127.0.0.1.
It is 127.0.1.1. How do I change it to 127.0.0.1?
SQL reports and open source, you're gonna have a bad time. Why do you need Open Source ? Power BI is free if you use it correctly.
That is a really good question and I'm honestly not sure. For workbench, you'll plug in the ip of the server, the computer username, and the computer password. (What you'd use to log into Ubuntu to begin with.) You should be able to run an ifconfig to figure out the ip. Then you'll configure the port 3306 and ip for the mysql instance, which is 127.0.1.1 and a username and password for the instance. Since you're having issues restarting MySQL, I think you'll still have an issue and may have to go through the troubleshooting still, but it's worth hitting test connection to verify. Note: It failed the first test connection for me and a subsequent second attempt worked. 
https://www.edx.org/ you can buy a certification of completion after taking the free courses.
So, what exactly in terms of steps should I do?
Alrighty, will do.
Even the last step of how they uninstalled and reinstalled? It's providing you with the same errors as before?
I thought doing a sequence of intersections like that would be a possibility, but I couldn't help but wonder if that was the hard way of doing things, as I'm not precisely an expert in this field. I'm sure everyone gets that feeling when working with something they aren't intimately experienced with.
Very true indeed. I'd be delighted to see if somebody does come up with anything more elegant.
1. Left join table1 and table2 in a subquery so you don't have to deal with it anymore. 2. Build a normal [hierarchical query](https://docs.oracle.com/cd/B19306_01/server.102/b14200/queries003.htm) with `START WITH` and `CONNECT BY` 3. Use `SYS_CONNECT_BY_PATH` to create a delimited chain of the values you want 4. Use `REGEXP_REPLACE` (or some other method) to get the last populated value from the delimited string. This will be the most-related value (self over parent over grandparent... over root) example: --test tables t1 and t2 with t1 as ( select 1 ID, cast(null as number) PID from dual union all select 2 ID, 1 from dual union all select 3 ID, 2 from dual union all select 4 ID, 2 from dual union all select 5 ID, 2 from dual union all select 6 ID, 3 from dual union all select 7 ID, 3 from dual union all select 8 ID, 4 from dual union all select 9 ID, 5 from dual union all select 10 ID, 6 from dual union all select 11 ID, 7 from dual union all select 12 ID, 9 from dual union all select 13 ID, cast(null as number) from dual union all select 14 ID, 13 from dual union all select 15 ID, 14 from dual ), t2 as ( select 1 ID, 'AAAA' ATTR from dual union all select 7 ID, 'BBBB' from dual union all select 12 ID, 'CCCC' from dual union all select 3 ID, 'DDDD' from dual ) --Code starts here. Review from the inside out select ID , PID , OWN_ATTR , ATTR_CHAIN , nullif( regexp_replace(ATTR_CHAIN , '.*\^([^\^]+)\^*$' --anything, followed by delimiter, followed by one or more NOT delimiters, followed by only delimiters until the end. , '\1' --Get only the "one or more NOT delimiters" part ) --if the result of the regexp is the same as ATTR_CHAIN, nothing was found. So null it out. , ATTR_CHAIN) as DERIVED_ATTR from ( select ID , PID --for testing, go ahead and carry the PID , ATTR as OWN_ATTR --and ATTR if this ID is directly related to one. , sys_connect_by_path(ATTR, '^') as ATTR_CHAIN --create a chain of attribute values --pick any delimiter you want that doesn't collide with your data -- (even multicharacter if needed, but the more complex, the more -- work it will be to parse out in the next step) from ( --this part is simple: get the join to T2 out of the way now. select T1.ID, T1.PID, T2.ATTR from T1 left join T2 on T1.ID = T2.ID ) --normal hierarchical query start with PID is null connect by PID = prior ID ) 
Good catch, that solved my number duplication problem. Any thoughts on the other parts of the query? Edit after your edit - I just added "AND dt.`dues_package_id` = pk.`id`", see edit to my OP. That worked, but yours also provides the correct sum.
well, it looks like you want sum by dt.id since each dt.id has only one pk.id, the first two columns in your SELECT clause are okay if you are summing payments, then you do ~not~ want each p.id finally, the join to payments should be LEFT OUTER, not INNER SELECT dt.id , dt.dues_package_id , COALESCE(SUM(p.amount),0) AS 'sum' , pk.cost FROM dues_transactions AS dt LEFT OUTER JOIN payments AS p ON p.payable_id = dt.id AND p.payable_type = 'App\DuesTransaction' AND p.deleted_at IS NULL INNER JOIN dues_packages AS pk ON pk.id = dt.dues_package_id GROUP BY dt.id , dt.dues_package_id , pk.cost
I DID IT! OH MY GOD! I DID IT! Alright, WE DID IT! Here is what I did: 1) https://askubuntu.com/questions/172514/how-do-i-uninstall-mysql 2) https://www.digitalocean.com/community/tutorials/how-to-install-mysql-on-ubuntu-16-04#step-1-%E2%80%94-installing-mysql 3) https://linode.com/docs/databases/mysql/install-and-configure-mysql-workbench-on-ubuntu/ So, what I should have initially done is configure the mysql server and only then install the mysql workbench.
Hurray! I recommend to update your primary post with as much as you can, especially any error messages and steps you did to resolve it. Then some poor internet sap can google and find it later like us.
Something like this will probably work. https://stackoverflow.com/questions/176964/select-top-10-records-for-each-category
Assuming you have your items split into 3 tables you can do something like this: select i.ID ,i.Name ,st.storeName ,sa.saleDate from item i join sales sa on sa.saleID = (select top 2 saleID from sales where itemID = i.itemID order by saleDate desc) join stores st on st.storeID = sa.storeID
Thanks, /u/Rehd ! That worked perfectly. Query came at as this... SELECT rs.* FROM (SELECT *, Rank() OVER (PARTITION BY Item ORDER BY RecordDate DESC ) AS Rank FROM Table) rs WHERE Rank &lt;= 2
[removed]
&gt; RecordDate As an FYI, if you have duplicate RecordDate's then you could have more than 2 results returned because they would be ranked the same. You have two options to deal with that, easiest is to just to ROW_NUMBER() rather than RANK() 
It's actually a datetime, but thank you. That's a great suggestion. 
Check these. https://www.reddit.com/r/SQLServer/comments/7j2xwt/passed_70761/ https://www.reddit.com/r/SQLServer/comments/7qfuth/achievement_unlocked_mcsa_sql_2016_database/ &gt; For anyone that has taken these exams, are there specific items that caught you off guard or sections I should spend more time on than others? Just a heads up, while some people may be willing to share what was on their tests or details about it, it is against the NDA you sign when you take the tests. So others and myself will explicitly talk about what we did to study and how we felt about the test, but we all may not give you more detail about the test itself. 
When you insert into a table, the records persist until you delete them. (Or remove them, but for simplicity sake, I'm going to forgo this conversation.) You can update current existing records as well. So when you insert records into a table and then try to insert new records into a table, are you explicitly deleting them before you insert new values? 
Is there anything you can think of that would affect the behavior of this query in my local environment (MySQL 5.7) compared to MySQL 5.6 on SQLFiddle? The sums work perfectly there, but I'm getting straight zeros locally. +----+-----------------+------+-------+ | id | dues_package_id | sum | cost | +----+-----------------+------+-------+ | 1 | 1 | 0.00 | 55.00 | | 3 | 1 | 0.00 | 55.00 | | 4 | 1 | 0.00 | 55.00 | | 5 | 1 | 0.00 | 55.00 | | 6 | 1 | 0.00 | 55.00 | | 7 | 1 | 0.00 | 55.00 | | 9 | 1 | 0.00 | 55.00 | | 10 | 1 | 0.00 | 55.00 | | 11 | 1 | 0.00 | 55.00 | | 12 | 1 | 0.00 | 55.00 | | 13 | 1 | 0.00 | 55.00 | | 14 | 1 | 0.00 | 55.00 | | 15 | 2 | 0.00 | 55.00 | | 16 | 2 | 0.00 | 55.00 | | 17 | 2 | 0.00 | 55.00 | | 18 | 2 | 0.00 | 55.00 | | 19 | 2 | 0.00 | 55.00 | | 20 | 2 | 0.00 | 55.00 | +----+-----------------+------+-------+ 18 rows in set (0.02 sec)
I appreciate it! It looks like I may have been searching the wrong subreddit. 
So, what I am doing is changing values in a script, save, run the script, and get the table. When I run the script, I expect the table to be updated with new values. Hold on, I was not deleting values in a table per se. Perhaps I should do that.
You'll find most of the same people post and browse both subs.
&gt; So, what I am doing is changing values in a script, save, run the script, and get the table. When I run the script, I expect the table to be updated with new values. I would expect the behavior to be one of two things. Either 1) You have constraints in place to enforce data integrity so the second insert cannot occur. Or 2) You now have two similar sets of data in the table. If I wanted to update data in a table, I would explicitly use an UPDATE statement. If I wanted to refresh the data within the table, I would DELETE (you can also remove data with truncate, partition swapping, etc. Too deep for this post.) data from the table first and then run the second insert statement. What it appears is that you are running two similar INSERT statements. 
it's a left outer join, so my guess is, these transactions have no payment rows based on the join conditions p.payable_type = 'App\DuesTransaction' AND p.deleted_at IS NULL
Would the same ID and year always show the same name when not missing? 
It's possible that some of them don't have payments, but almost all of them definitely do. For example, #20 has a $58 payment (`payment_id` 17). Same with 18, 17, and 15.
 A separate database won't help you if you're using foreign keys. I've seen schemas implemented that made no sense and could have all be done in the dbo schema, prefixing the tables with "sales\_", "product\_", or "customer\_" instead of creating separate schemas for each. The logic is usually "it makes securing the database easier", but the "easier" part is debatable, and the security should probably happen at the application level anyway. It might make sense for ETL / data warehousing purposes. That's the only place I tend to see schemas used frequently. Fun trivia: MySQL doesn't differentiate between a schema and database--they're the same thing and the terms can be used interchangeably in the DDL.
How would I use Update?
Everything appears to match in the fiddle and my local server. I exported the create statements from Workbench to try to prevent that problem. +----------------+------------------+------+-----+---------+----------------+ | Field | Type | Null | Key | Default | Extra | +----------------+------------------+------+-----+---------+----------------+ | payable_id | int(11) | NO | | NULL | | | payable_type | varchar(255) | NO | | NULL | | | amount | decimal(8,2) | NO | | NULL | | +----------------+------------------+------+-----+---------+----------------+
You didn't say what platform you're on. If your platform supports CTEs: with t1 as (select a , b from t2 group by a, b having count(*)&gt;1) select t2.* from t1 inner join t2 on t2.a=t1.a and t2.b=t1.b where c is not null with ur; This syntax works on Db2. If there are values for a and b that have a different value for c, then you'll get more than one row for them. It's then easy to add in the count: with t1 as (select a , b , count(*) as count from t2 group by a, b having count(*)&gt;1) select t2.*, t1.count from t1 inner join t2 on t2.a=t1.a and t2.b=t1.b where c is not null with ur;
Let's say we have a (awfully created, I know. I'm just making something simple off the top of my head.) table that holds office supplies for sale. We had a problem where we accidentally updated the price for everything in the table from it's normal value, to $10.50. Here's what the data looks like. Table | Chair | Desk| Price| InStock ---|---|----|----|---- Yes| No| No| 10.50| Yes No| Yes| No| 10.50| No No| No| Yes| 5.50| Yes No| Yes| No| 10.50| Yes No| Yes| No | 0.50| No We want to see what was impacted, so we would run this query: SELECT Table ,Chair ,Desk ,Price ,InStock FROM OfficeSupplies WHERE Price = '10.50' and InStock = 'Yes' You should get this result set back: Table | Chair | Desk| Price| InStock ---|---|----|----|---- Yes| No| No| 10.50| Yes No| Yes| No| 10.50| Yes Now if we wanted to set all of those particular objects to out of stock so no associates accidentally sells one at an incorrect price, we need to update that column to meet a new value. The SELECT forms the list of things you need, the FROM is where you are getting the list, and the WHERE predicate is your criteria for what should be returned from the SELECT list. It's very similar to English. "What tables, chairs, and desks are in stock and set to the incorrect price of 10.50?" That essentially translates to the SQL above. So we can take that same logic and say&gt;: "Let's update the price for office supplies that were incorrectly set to $10.50 and are in stock." Something to notice here is that I did not say update the price for Tables, Chairs, and Desks. I said office supplies. This translates to: UPDATE OfficeSupplies SET InStock = 'No' WHERE Price &gt; '10.50' and InStock = 'Yes' Here is the result set after: Table | Chair | Desk| Price| InStock ---|---|----|----|---- Yes| No| No| 10.50| No No| Yes| No| 10.50| No Now if I wanted to limit it to Tables, Chairs, and Desks, I'd need to append the WHERE predicate. WHERE Price &gt; '10.50' and InStock = 'Yes' AND (Table = 'Yes' OR Chair = 'Yes' OR Desk = 'Yes') This is important, because what if there was another column on the table and it was for Couches? You would be manipulating rows of data that may have been couches but the data set was limited to tables, chairs, and desks. ALWAYS WATCH YOUR WHERE CLAUSE. TERRIBLE THINGS WILL HAPPEN TO YOU SOME DAY. Like that time I accidentally deleted millions of dollars of data when I was three-ish weeks into my first SQL job and I had to spend back to back 80 hour weeks to piece together bits of data because all of our backups were corrupted or turned out to not be working. 
Figured it out after like three hours of banging my head. Turns out it was an issue in the query with how the `\` in `App\DuesTransaction` was being escaped. I needed to do `'App\\DuesTransaction'` and now it's working!
Is your script returning any errors. Once you create a table you cannot call the Create Table command with the same table name, as it already exists. You should look into some free online courses on SQL to get a better understanding on how SQL works. Feel free to ask me questions. I'm not an expert but I work with SQL every day.
Hey there, So, I am currently following: https://www.khanacademy.org/computing/computer-programming/sql/sql-basics/p/aggregating-data In any case, when I update an amount of bananas after "Bananas", why doesn't table update automatically with the new value? For example: "INSERT INTO store VALUES (1, "Bananas", 56, 7)" results in "'1', 'Bananas', '56', '7'" Now, when I change the values of bananas to, let's say 14, "INSERT INTO store VALUES (1, "Bananas", 14, 7)", I expect the table to be updated to "'1', 'Bananas', '14', '7'", BUT I still get "'1', 'Bananas', '56', '7'". Why does 56 still exist in a table when I have changed the value to 14 in the script?
Maybe bug. That query should work. Try swapping your arguments or buying your integer in single quotes (may be a varchar on the back end or it doesn’t actually query, site may just do a string comparison)
Nothing is impossible unless the data isn't there. If you have a table without zip codes of sales data, and another table of population data organized by zip code then you cannot possibly give a break down of the top 10 zip codes for sales. If you do have the data then you can do anything, it only becomes a matter of whether it is practical, efficient, etc. Another example is that if you have a table of garbage data and someone wants something... it might not be practical to write a query to give you the answer you're asked for. In the real world you might have to export that data to Excel to manipulate and cleanse, or seek new data, etc. -- I mean you might be able to do it, but do you really want to have a WHERE statement that is ten thousand lines long? The only thing that is not possible assuming you have the data would be something so systematically intense and complex that it cannot be performed on the system you're executing it, or possibly on any existence known to man, or even possibly will ever theoretically could be built. For example is it possible to calculate Pi to the Googol (1.0 × 10^100)? Sure, in fact it would be a pretty simple query. *Could you do it?* No, that isn't possible. You could probably estimate the amount it would take to run, and you could probably estimate the size of the storage it would take up...but if the question was, "What is the digiti of Pi that is 1.0 x 10^100 then the only realistic answer to that question is, "that isn't possible to answer." You could also probably answer with, "This job doesn't pay nearly well enough for me to tell you that." Or, "If I could do this I would be seeking a better job with a company that has real interview questions." 
Can you explain how by using both methods?
thanks for the update... makes sense... and it was staring us in the face the whole time, eh
You might have to do a unload reload. Do you know if you power PC is big or little endian? Your power PC to Intel is going to cause most of your issues, as your are moving across platforms. You might find it easier to more to Intel 9.7, than move to 11.1 as your biggest issue is going to be moving your data to your Intel platform. Unfortunately I have no experience on DB2 but my father does, and he laughed, and he does ZOS not mid frames.
&gt; the security should probably happen at the application level anyway. Security should happen at *all* levels. As with foreign keys, I don't trust the application to always properly enforce - and I have to remember that I can't assume that the application is the only way into the database.
The SO post takes that into account. The first answer is correct. The person appears to be taking the same class you are even. The question looks identical.
Thank you and alright I'll give it another look, but I would rather a fresh explanation since I have been stuck for a while
Great word choice
Thanks for your help! Aside from the question being confusing, I believe it wants me to solve for the maximum that a customer spent on the product. The product table has a RecommendedPrice column and the purchase table has PurchasedPrice column so that should clear that up!
This sounds like a fun project. I hope you are billing by the hour.
I think you would want to do this as a before trigger. It looks like you're only updating the inserted data. Something along these lines: DELIMITER $ CREATE TRIGGER OK BEFORE INSERT ON ORDERS FOR EACH ROW BEGIN SET @A = 0; IF NEW.or_key IS NOT NULL THEN set @prize = (SELECT price FROM CPU WHERE `name`= NEW.`or_cpu`); SET @A=@A+@prize; END IF; set NEW.or_sum=@A; END$ DELIMITER ; I'm not a MYSQL person, so the code may not be exactly correct, but I hope the general idea helps
Linux on P-series is big-endian. Linux on Intel is little endian. So you'll have to unload/reload the database. db2look and db2move are your friends. Might as well do the 9.7 jump to 11.1 at the same time since it's a complete re-load anyway. It will take a while, and data size will make a huge difference in how long.
Most of my interview questions are dynamic. Here's a few recent examples: Explain a time when you made a schema or architectural change to a high availability DB that could have 0% downtime? Have you ever worked closely with developers creating applications that interact with the DB and have you had to help troubleshoot their code and how did your interactions with the developers go during the development process? How do you handle large DB OLTP environments for adjusting or migrating data during business hours? What are the most common wait types you have encountered and what have you done to fix them? A large ETL process is taking 10-14 hours, what are some things you would look at and where would you tune it? *Shows picture of query with UDF's, incorrect syntax, and subqueries. How would you tune this query? What is wrong with this query? What are things that you notice or what would you do differently?
Thanks for your help! The way the names work is based on a system where names were associated with a period or 'year', and some years there were no people associated. This is 'normal'. One lingering issue after applying the max on name -- it returns values where name is blank or ' '. The data was pulled from a legacy system that had people entering values such as a single space. I tried adding where name != '' or &lt;&gt; '' but then nothing gets returned. I believe this is due to the fact that my results are doing a count of &gt; 1, and in most cases the 'name' field is only present one time. In my second example in the post, 2011 would not get returned. Since I am filtering out where name is blank, my count would only be 1. So I think what I need is to do a count based on the amount of time the same year shows up, but **at least one** name must be present. 
Yes! Thank you for suggesting this. The endians might be it and I have to do perform the unload and reload! Power PC is apparently big endian. And intel is little endian. I saw a comment on my notifications but couldn't see it on my post! Whoever you are thank you. You changed my life forever haha
First thing you should do is clean up the names. If you can't update the source data then load them into a second table or temp table with something like: NULLIF(NULLIF(name, ' '), '') If they're NULL the default behaviour for MAX() would be to select a populated value over the NULL. Then you can change your query to only show rows which have a name: SELECT ID, Year, MAX(Name), COUNT(*) FROM #x WHERE Name IS NOT NULL GROUP BY ID, Year HAVING COUNT(*) &gt; 1 AND MAX(Name) IS NOT NULL This only works on the assumption that you won't have more than one name (once you've cleaned up the empty strings/blanks).
Thank you so much. So pretty much, the only way to validate the name column is with cleaned up data, right? Because ' ' is just a valid name as 'john' is, and max() will simply return anything other than NULL. max() doesnt know the difference between ' ' or 'john' either, yes? Edit: Alternatively, what I was trying to do was add a where name &lt;&gt; ' ' and where name &lt;&gt; ' ' and where name &lt;&gt; ' ' etc.. trying to catch all the different values of space that may be in the data. Could this potentially work if I found all the different values? Thanks again!
 --Standard RowNumber Method SELECT x.Category , x.ProductName , x.PurchasePrice FROM (SELECT pr.Category , pr.ProductName , pu.PurchasePrice , ROW_NUMBER() OVER (PARTITION BY pr.Category ORDER BY pu.PurchasePrice DESC) AS RowNumber FROM product AS pr INNER JOIN (SELECT pu.ProductID, SUM(pu.PurchasePrice) AS PurchasePrice FROM purchase AS pu GROUP BY pu.ProductID) AS pu ON pu.ProductID = pr.ProductID) AS x WHERE x.RowNumber = 1; --String Sorted Aggregate Method (more efficient;doesn't need to sort) SELECT pr.Category , STUFF(MAX(RIGHT('0000000000'+CONVERT(NVARCHAR(10),pu.PurchasePrice),10)+pr.ProductName),1,10,'') as ProductName , MAX(pu.PurchasePrice) AS PurchasePrice FROM product AS pr INNER JOIN (SELECT pu.ProductID, SUM(pu.PurchasePrice) AS PurchasePrice FROM purchase AS pu GROUP BY pu.ProductID) AS pu ON pu.ProductID = pr.ProductID GROUP BY pr.Category; http://sqlfiddle.com/#!6/9870b/21
You can also ask on /r/Db2 - it is not very active, but there are several experts who keep an eye on it.
No problem! The issue you'll have is that using where name &lt;&gt; '' will exclude rows. This would mean they won't be included in the count. And yes using MAX() feels a bit arbitrary in relation to picking from a list of strings - meaning I wouldn't trust it 100% unless it was tested on all the different name values. You can use a subquery or CTE to clean the data up as part of a single query if you really want to: SELECT ID, Year, MAX(Name), COUNT(*) FROM ( SELECT ID, Year, NULLIF(NULLIF(Name, ' '), '') Name FROM #x) x WHERE Name IS NOT NULL GROUP BY ID, Year HAVING COUNT(*) &gt; 1 AND MAX(Name) IS NOT NULL; Just wrap more NULLIFs or REPLACE functions until you've satisfied all possibilities. You could use this to find the main culprits: SELECT Name, LEN(name) NameLen, COUNT(*) FROM #x GROUP BY Name, LEN(name) ORDER BY 2 I should add that none of this really accounts for performance issues but doing the above should be fast enough as long as your rows aren't in the millions.
Why are you avoiding FULL OUTER JOIN?
It's an assignment from school
You could inner join the tables as long as every value in the join column is represented in every table. If that is not the case, join them as derived tables and union on the rows that need to be added. For example if you have a table called “Pets” and a table called “Owners” and there exists a “Bob” in the owners table but he doesn’t have any pets. You would need to add a row into “Pets” where the join field of owner equals Bob and all other columns would be NULL. I hope I’m explaining that clearly. I’m on mobile so the easiest for me to type out.
"If that is not the case". That is exactly what I'm looking for. However, I'm not familiar with UNIONS. Can you give some sort of example of this?
1. A URI is just a Unicode string. Size it appropriately. There is no SQL Server data type for URLs specifically. 2. My preference is "don't, store it somewhere else and then put a URI or filesystem path to that location". Whatever application is interacting with your database and images will need to manage that. If you must put the image in the database itself, use `VARBINARY(MAX)` or, for more advanced usage, `FILESTREAM`. But to get the data into a `VARBINARY` field via a query in SSMS you'll need to turn it into hex first.
I FUCKING HATE QUESTIONS LIKE THIS! How do you do something without doing the right thing? What are you looking for? I don't know? If you are looking for a correct solution, I can figure out what you want, but not this. What do you (teacher not OP) want??? Do you want OUTER APPLY , UNION , OUTER APPLY? Do you want old join syntax? Do you want CROSS JOIN with a filter? Do you want inline sub queries? Do you want a fucking cursor or cursors within cursors? Do you want INNER JOIN , UNION ALL , NOT EXISTS? What is the fucking point of this question? What is it meant to demonstrate?
Not having the context of your goals makes #2 a little difficult but #1 is pretty straight forward regardless of application. 1) As /u/alinroc stated, capture your URL in your preferred type of sting field and make sure the size constraints are appropriate. I will add to this by saying decide upfront how you want to use the URLs and how you will be outputting them in the future. Decide now if you want to capture the http(s):// at the front or if you will be adding that in programatically when you read the url out through your application. I prefer to not to do sting manipulation inside SQL so I strip out all prefix data before inserting the actual URL - but different use cases could call for different methods - just make it consistent and you should avoid any problems. (edit: for instance if http or https in the URL varies or matters to your application you will need to capture that some how either in the URL itself or in a flag field - in my use cases, when I know they will always be of one type I don't store that info) 2) I would like know more about binary data storage and performance inside DBs as well. In most web CRUDs I have developed I got around this by having a file (directory) structure mimicking the IDs of the entities that needed "attached" files and just scanned their directories within the application to determine if they had any attached files. It is an easy option when you control all aspects of your environment, but it is not suited for more complex solutions or ones when you can not be sure of what server the data may be on in the future.
Sounds like they want a big steaming pile of select * from winefarm, wine, contains, grape and a hearty good luck on your way out the door. 
Oh boy.. This link contains the tables used in this assingment and result table what I need: https://pastebin.com/3i5BCwhP I think that i'm allowed to use unions in this case. Do you know a way to achieve result table like that wihout outer joins
 SELECT DISTINCT CASE WHEN MAX(CASE WHEN winefarm.wf_id = wine.wf_id AND wine.w_id = contains.w_id AND contains.g_id = grape.g_id THEN 1 ELSE 0 END) OVER (PARTITION BY winefarm.wf_id) = 1 AND winefarm.wf_id = wine.wf_id AND wine.w_id = contains.w_id AND contains.g_id = grape.g_id THEN winefarm.name WHEN MAX(CASE WHEN winefarm.wf_id = wine.wf_id AND wine.w_id = contains.w_id AND contains.g_id = grape.g_id THEN 1 ELSE 0 END) OVER (PARTITION BY winefarm.wf_id) = 0 AND MAX(CASE WHEN winefarm.wf_id = wine.wf_id THEN 1 ELSE 0 END) OVER (PARTITION BY winefarm.wf_id) = 1 AND winefarm.wf_id = wine.wf_id THEN winefarm.name WHEN MAX(CASE WHEN winefarm.wf_id = wine.wf_id AND wine.w_id = contains.w_id AND contains.g_id = grape.g_id THEN 1 ELSE 0 END) OVER (PARTITION BY winefarm.wf_id) = 0 AND MAX(CASE WHEN winefarm.wf_id = wine.wf_id THEN 1 ELSE 0 END) OVER (PARTITION BY winefarm.wf_id) = 0 AND ROW_NUMBER() OVER (PARTITION BY winefarm.wf_id ORDER BY wine.w_id DESC, contains.g_id, grape.g_id) = 1 THEN winefarm.name ELSE NULL END AS winefarm_name , CASE WHEN MAX(CASE WHEN winefarm.wf_id = wine.wf_id AND wine.w_id = contains.w_id AND contains.g_id = grape.g_id THEN 1 ELSE 0 END) OVER (PARTITION BY wine.w_id) = 1 AND winefarm.wf_id = wine.wf_id AND wine.w_id = contains.w_id AND contains.g_id = grape.g_id THEN wine.name WHEN MAX(CASE WHEN winefarm.wf_id = wine.wf_id AND wine.w_id = contains.w_id AND contains.g_id = grape.g_id THEN 1 ELSE 0 END) OVER (PARTITION BY wine.w_id) = 0 AND MAX(CASE WHEN winefarm.wf_id = wine.wf_id THEN 1 ELSE 0 END) OVER (PARTITION BY wine.w_id) = 1 AND winefarm.wf_id = wine.wf_id THEN wine.name WHEN MAX(CASE WHEN winefarm.wf_id = wine.wf_id AND wine.w_id = contains.w_id AND contains.g_id = grape.g_id THEN 1 ELSE 0 END) OVER (PARTITION BY wine.w_id) = 0 AND MAX(CASE WHEN wine.w_id = contains.w_id AND contains.g_id = grape.g_id THEN 1 ELSE 0 END) OVER (PARTITION BY wine.w_id) = 1 AND wine.w_id = contains.w_id AND contains.g_id = grape.g_id THEN wine.name WHEN MAX(CASE WHEN winefarm.wf_id = wine.wf_id AND wine.w_id = contains.w_id AND contains.g_id = grape.g_id THEN 1 ELSE 0 END) OVER (PARTITION BY wine.w_id) = 0 AND MAX(CASE WHEN winefarm.wf_id = wine.wf_id THEN 1 ELSE 0 END) OVER (PARTITION BY wine.w_id) = 0 AND MAX(CASE WHEN wine.w_id = contains.w_id AND contains.g_id = grape.g_id THEN 1 ELSE 0 END) OVER (PARTITION BY wine.w_id) = 0 AND ROW_NUMBER() OVER (PARTITION BY wine.w_id ORDER BY winefarm.wf_id DESC, contains.g_id, grape.g_id) = 1 THEN wine.name ELSE NULL END AS wine_name , CASE WHEN MAX(CASE WHEN winefarm.wf_id = wine.wf_id AND wine.w_id = contains.w_id AND contains.g_id = grape.g_id THEN 1 ELSE 0 END) OVER (PARTITION BY grape.g_id) = 1 AND winefarm.wf_id = wine.wf_id AND wine.w_id = contains.w_id AND contains.g_id = grape.g_id THEN grape.name WHEN MAX(CASE WHEN winefarm.wf_id = wine.wf_id AND wine.w_id = contains.w_id AND contains.g_id = grape.g_id THEN 1 ELSE 0 END) OVER (PARTITION BY grape.g_id) = 0 AND MAX(CASE WHEN wine.w_id = contains.w_id AND contains.g_id = grape.g_id THEN 1 ELSE 0 END) OVER (PARTITION BY grape.g_id) = 1 AND wine.w_id = contains.w_id AND contains.g_id = grape.g_id THEN grape.name WHEN MAX(CASE WHEN winefarm.wf_id = wine.wf_id AND wine.w_id = contains.w_id AND contains.g_id = grape.g_id THEN 1 ELSE 0 END) OVER (PARTITION BY grape.g_id) = 0 AND MAX(CASE WHEN wine.w_id = contains.w_id AND contains.g_id = grape.g_id THEN 1 ELSE 0 END) OVER (PARTITION BY grape.g_id) = 0 AND ROW_NUMBER() OVER (PARTITION BY grape.g_id ORDER BY winefarm.wf_id, wine.w_id, contains.g_id DESC) = 1 THEN grape.name ELSE NULL END AS grape_name FROM winefarm CROSS JOIN wine CROSS JOIN contains CROSS JOIN grape; 
in addition to file data, you'll want/need to know the file name/type... is it a TIF, GIF, PNG? depending on how it's being used, you'll likely need to know.
Oh sweet jesus... My co-workers are asking my what I'm cackling at here at my desk. 
Are you sure that this is the only way to achieve this :D ? I still appreciate alot ur effort to help me!
You can load the excel file into a table in SQL, or you can use BIGINSERT and create a temp table. You can also set up a linked server, but you probably dont want to do that. Here is an overview https://www.mssqltips.com/sqlservertip/2018/using-a-sql-server-linked-server-to-query-excel-files/
As far as I know, no. If there is a way I'd love to hear it. (also kinda new to sql)
An Excel file is a spreadsheet, not a database that can be queried with SQL. You will not be able to query this data via SQL queries without first importing it into a database.
If you have ms SQL 2014 or later they introduced a new feature on top of file stream called file table which allows you to manage files from the windows operating system or SQL side and maintain linkages automatically. 
No. It a creatively insane way to achieve this. Some like this is more logical, but still stupid. SELECT wfname, wname, gname FROM winefarm INNER JOIN wine INNER JOIN contains INNER JOIN grape UNION ALL SELECT wfname, wname, NULL AS gname FROM winefarm INNER JOIN wine WHERE NOT EXISTS (SELECT * FROM contains INNER JOIN grape) UNION ALL SELECT wfname, NULL AS wname, NULL AS gname FROM winefarm INNER JOIN wine WHERE NOT EXISTS (SELECT * FROM wine) UNION ALL SELECT NULL AS wfname, wname, gname FROM wine INNER JOIN contains INNER JOIN grape WHERE NOT EXISTS (SELECT * FROM winefarm) UNION ALL SELECT NULL AS wfname, wname, NULL AS gname FROM wine WHERE NOT EXISTS (SELECT * FROM winefarm) AND NOT EXISTS (SELECT * FROM contains INNER JOIN grape) UNION ALL SELECT NULL AS wfname, NULL AS wname, gname FROM grape WHERE NOT EXISTS (SELECT * FROM wine INNER JOIN contains) 
Holy shit.. I appreciate your help man. Then again, is there any shorter way to do this :D
I've been stuck on this for ages.. xD Getting really desperate. Do you know a way to achieve this result table: https://pastebin.com/3i5BCwhP Without using outer joins
ssms is a tool to work with sql server - so unless you have a server it's kinda pointless. You can use sql-like interface to your data stored in excel though - you will need a client that can get data from ODBC - excel's query will work, for example, or you can use a reporting/analytics tool (so, in effect, you'll be getting data in excel workbook from another excel file). 
get the first time it's _not_ in the status 'A' and get the minimum (or the next) date that's over that.
To update an existing record we will use the "Update" statement. UPDATE grocery SET quantity = 14 WHERE id = 1 The above statement says: update the table grocery, change the quantity to 14 for the record with an id of 1. To insert an new record: INSERT INTO grocery VALUES (8, "Bananas", 56, 7)" All that does is insert a new records. That means you will have 2 records with the name Bananas. 
LEADING COMMAS TWICE AS PRODUCTIVE!!! EAT IT, TRAILING COMMA PEOPLE !!!
You could use a table valued function that returns 3 columns. You call it with CROSS APPLY. SELECT t.*, f.ChoseDate, f.OpsStartTime, f.OpsEndTime FROM Table AS t CROSS APPLY Function(t.UserDateInput) AS f;
Minor correction, but you used `MIN()` for MaxDate instead of `MAX()`: http://sqlfiddle.com/#!6/b73c3/10
Sorry I missed something, there where clause above doesn't need to be there. Here's a working example, the same ID and year 4 times with a null name 3 times but the count still returns 4: CREATE TABLE #x ( Id INT, Year SMALLINT, Name VARCHAR(10) ); INSERT INTO #x (Id, Year, Name) VALUES (1, 2011, 'name'), (1, 2011, NULL), (1, 2011, NULL), (1, 2011, NULL); SELECT ID, Year, MAX(Name), COUNT(*) FROM ( SELECT ID, Year, NULLIF(NULLIF(Name, ' '), '') Name FROM #x) x GROUP BY ID, Year HAVING COUNT(*) &gt; 1 AND MAX(Name) IS NOT NULL; 
Thanks. Fixed.
This script would update NULL names to their correct value (assuming each ID/Year should have the same name): UPDATE x2 SET x2.Name = x.Name FROM #x x INNER JOIN #x x2 ON x2.Id = x.Id AND x2.Year = x.Year WHERE x.Name IS NOT NULL AND x2.Name IS NULL; You can run it on the sample script above to see it in action.
&gt; UPDATE grocery SET quantity = 14 WHERE id = 1 So, where exactly in the code should I include the udpate line? I inserted it right after the SELECT line and I still have 56 bananas.
Thanks! But for question no. 2: won't storing the images themselves in the database be quite heavy on the database? 
https://pythonhosted.org/querycsv/
Yes, it can be. That’s why it’s not preferred. 
I’ve done it but only to extract data to insert into a real table. It’s not enjoyable. 
Oh sorry, I completely misread it haha. Thanks again!
In the actual tables every table has column "name". I tried to do this like: SELECT winefarm.name, wine.name, grape.name That one didin't work. Is there some other way?
It's pseudo code. I left it for you to fill in the column names, column aliases, join conditions, and not exists conditions.
Update actually works. Cheers mate.
Can anyone please help.
I once managed to just copy-paste the data from Excel straight into the table. Just open the table in edit-mode (right click table and select "edit rows" or something, don't remember excactly) and then just copy-paste from Excel into the empty row. Make sure you select the full row and not a single cell. Edit: probably not the best idea for very large files :)
You can create table and then essentially paste everything in there. Should also be able to do this only using the object explorer, aka you do not have to write a query... just make sure to choose the best data types for your columns. 
Does it work without your IF statement? 
please read it again WHEN PRIMAPPTNO = RELAPPTNO THEN 0 ELSE 1 dude, when they are not equal, it's ~supposed~ to return 1 
It looks fine. Can you show how you're calling the function?
Are you sure for your Canadian zip codes that it's actually a space, and not a non-ascii character? I recommend highlighting the space in the output, copy the value to the clipboard, and do another replace statement in the function pasting the value from the clipboard between the ticks (instead of using a space).
Did not understand you?
Just go for the import man. It’s pretty easy and in the long run is really helpful. 
You’ll need to identify the old value as well as the new value if you plan to use this as part of a SELECT statement and not part of a PL/ SQL block. SELECT oldValue, NEWValue, CASE WHEN oldValue &lt;&gt; newvalue THEN 1 ELSE 0 END AS columnAlias FROM Table If you don’t have the old and new values available within the same query, this will be much more difficult.
select itemno, status, min(date) from table group by 1,2 where status = 'A'
Assuming you need to compare to the previous or next value, you could do a subquery and use rownum to find the previous row, or use the LAG function. Quite ineffective for large tables, though.
You can do it using Ace ODBC driver. 
Define "spiral out of control." An auto increment int column starting at 0 can hold 2,147,483,648 values. (If you started at the minimum int value, you could hold twice as many.) If you're really scared of hitting 2.2 (4.4) billion transient entries, use a bigint (max value is 9,223,372,036,854,775,807!) There's no performance penalty for seeking value 68631390 versus seeking value 7 - they take up the same space in the index. And since there's no duplicates and relationship between value 7 and 8, having large gaps or sparsity also has no impact. You will of course have to update stats and rebuild your index more frequently but that is irrespective of the keys or indexes; that's due to your frequent updates and deletes to your table. 
Also there's no real need to hard delete rows unless you have either a **very** stringent retention/removal policy or a **very** stringent storage limit. Just add a filtered index with a condition expression that the permission is not expired (however you define that, I assume with the expires field...) and the index will automatically maintain itself as you expire permissions, giving you the same seek and search performance as removing the rows while maintaining the old records for auditing or analysis. Just a thought.
&gt; may be easier to have it keep A through Z, 0 though 9 instead. Is there a way to do that without using a loop (and tanking performance)?
Tabs vs. spaces, emacs vs. vi, the wars rage on. Do whatever, just be consistent with it.
see my other post
try putting the REPLACE statement between brackets: (REPLACE(MultiValueMessages , '; 20' ,'&lt;br /&gt;20')) that should do the trick
To be fair, there are times where an increment can skip and never actually assign a record an ID. In SQL Server starting with 2012, this occurs when you restart the service. In other versions, it can occur on transaction rollback too.
Never assign a record an ID? I’ve never seen that happen. That sounds like data corruption to me, which is really out of scope of the conversation. Do you mean that it sometimes assigns records an ID higher than normal? If so, that doesn’t really matter. All that really matters with a primary key is that it’s unique.
&gt; try putting the REPLACE statement between brackets: (REPLACE(MultiValueMessages , '; 20' ,'&lt;br /&gt;20')) &gt; &gt; that should do the trick That worked perfect, thank you!
I see what you mean now, your phrasing is just strange. You said "never actually assign a record an ID" which made me think you were saying that there were records with no value for their primary key. Anyway, I'm curious why you dislike it so much. In my eyes, if you think you'll have enough records for the gaps to make a difference, you should be using a larger datatype for your primary key. 
&gt; I see what you mean now, your phrasing is just strange. You said "never actually assign a record an ID" which made me think you were saying that there were records with no value for their primary key. That's my bad. It's been a long day already starting early with deployments and no coffee. Z_Z &gt; Anyway, I'm curious why you dislike it so much. We had an application that could not handle skips in the keys. Guess who had to be the guy to fix it every time it broke? Not a thing I could really do for it either, all black boxed. Thankfully I was able to make a semi-permanent fix by removing the ~5000-20,000 deadlocks we received daily on that database, but if it restarts, it's pain town again. 
I personally think this is a perfect case for using SQL. SQL is used for databases, their existence and purpose is to hold data. You have data you need to store, access, and manipulate. Technically your spreadsheets are databases, SQL is just a more powerful tool along with a... database engine I guess?
use BigInt. start at the lowest NEGATIVE Number. 
Thanks! I'll give that a try.
That sounds like a bad idea. In my life I have never witnessed a negative identity.
it looks weird, but then again, using Identity as a PrimaryKey is typically wrong or just unique for the sake of unique. (surragate key) if its something that going to be used for a customer ID, then no. but then again, i still wouldn't use a Identity at that level. if 2 billion numbers aren't enough, the 4 billion won't be either. just thoughts for development and modeling. 
Is there any particular sites or tutorials you'd recommend which might put me in the right direction? I've attempted to recreate it in LibreOffice Base because the interface feels a bit more familiar and easy to configure, so far I've managed to list the team names and assign them their primary key and I've then linked that into the league table by assigning them the foreign key, but I've got no idea how to add my fixtures properly and then have that information translate to the table. For example, in my [spreadsheet here](https://docs.google.com/spreadsheets/d/1eweu11SNBC98GWRpL4ZI3xuE9iyemU50YYEgnG42fz4/edit?usp=sharing) on the "FIXTURES - CURRENT" tab I've got it set up so I just put the results in the blue part at the top and that will automatically figure out who won and add points to the league table based on that (All the working out can be seen directly below it, where I've got the formulae set up) I'd be looking to get functionality like that recreated in my SQL set up but honestly I've got no idea where to start with it all. I understand the basics of SQL as my job involves me looking into databases regularly and using SELECT statements quite a lot, but my knowledge pretty much ends there.
I don't have time now to dive into this but I'm going to put it on my list. Just sounds like a fun thing to play with and I'm sure there are a few things I could learn / get more experience with like triggers. PSA: For everyone asking "how do I learn SQL?" this is the answer right here. You do some shit with data that you *know*, and that you can *do* in a program like Excel (i.e. you know the answer, the expected output, etc.) and then you just dive in and start figuring shit out. A small mini-project like this will involve making a new database, building tables, indexing, keys, queries, stored procedures, potentially views, triggers likely, etc. You can take an online course until you're blue in the face, but until you do those things in a meaningful way that you understand on a level that lives outside of SQL... then you will never "learn" SQL. The first step before you even write a query is to figure out what the hell someone wants, then what the hell the data looks like... then if it is even possible... then the query (no matter how complex) starts to architecturally take shape in an organic way. &gt;Ideally it doesn't need to integrate with Python as much as a small web application would more helpful, I'm thinking SQL would work best with this as I'm only getting away with it in Python due to me being able to import tools which work with Google Sheets into Python and it's probably better to move it towards something more "web friendly" One of the things I like about your post so much is that it covers so many areas where SQL impacts, and many of these areas are not areas where I have an educated opinion. I could probably set something up but I don't know whether its good, or there are better options, etc. So I'm looking forward to seeing other people adding opinions.
I'd say the first step is design how the database is going to look. If you download world wide importers or adventureworks for SQL Server, you can get a good view to how to structure your data. Getting your tables set up so it will be fluid will take some work for you. You don't even have to normalize the data to rid redundancy, you just need it so your data can go in and out without too much effort. The second step is to configure stored procedures or functions that allow you to manipulate the data. It's basically just scheduling your SQL to run. You can use triggers to automatically perform some tasks, but triggers are best left alone if you can avoid it. It's a stored procedure essentially that you don't schedule, it executes when a "thing" happens. Such as, you inserted a record with an invalid ID, so it deletes the record or instead of writing that record, it would stub in a new result. Now, there's a TON more you can do to make this nicer. If you are dabbling in the world of DB, I think this is a fine starting point. I would recommend to use these technologies: Cloud - Azure / AWS / Aurora Home Shop - PostGres / MySQL / Access(I would stay away here, but that's personal preference. It is still semi popular and could achieve what you want.) /POSSIBLY SQL Server / Oracle. Those last two have paid versions, free versions, and SQL Server has a "developer" edition. I don't know if I could recommend that, it gets into gray area for this use because I don't know if this would count as a production environment. I personally have a little raspberry pi that runs a MySQL (MariaDB really) DB that has automated jobs that imports race data and I have other projects on and I use it as a data source for PowerBI dashboards. To get the basics, I'd say any Udemy / we3schools / video streaming service like lynda or pluralsight or any beginner SQL book will suffice (SQL in 10 minutes). You really just need to understand how to lay out the data so it works well in a table format and how to do basic selects / updates / inserts / deletes and setting up procedures. That's a relatively large but decent task to start with. Since you get SELECT and know a little by looking at DB's, I think this is a good starting point.
Ideally your environment and application are based on stored procedures executed from code and the code handles the if then else scenario, sorting out all of that business logic crap we don't want in the DB. Then it executes the proper procedure based on that chain of events. If it needs info first, pull it from the DB and then apply logic to execute the next procedure. If there's no application handling that logic and you need it to be instantaneous, triggers are pretty much the way to go. I loathe triggers, but don't get me wrong, I have used them frequently. Right tool for the job approach, even if you hate the tool.
I was assuming one person would have to "initiate" a trade and that when another player "accepted" a trade that a trigger would fire, but I see what you're saying about the code "triggering" individual sprocs that handle the workflow as opposed to "triggers" in SQL. This is an area so far removed from anything I have specific expertise with that I don't have an opinion.
I'd have to re-read his idea again, but as an example. If you were on fantasy football and hit trade, it would actually execute a procedure to place that data inside a table, your trade data if you will. Then when the player accepts or declines, each action has a procedure. Then the procedure does stuff with the trade data from there. If he wants it more automagic and there is no app, it's all contained within the DB. It would probably be triggers. If you want a fun data set based on an application, Ozar has Stack Overflow DB on a torrent you can grab. It's pretty cool, I don't remember if it has procedures and such to it though. I think it did though.
No, no, I totally get what you're proposing. I'm just saying I honestly am so far removed from anything I have personally built or tested that I have no opinion whether that would be better or worse than a SQL trigger. Your approach seems cleaner and less reliant on SQL, which in and of itself seems attractive.
well, you could do this if it works for you, I suppose. Or you can combine it with other string functions to get that specific (fv.value-th) substring/segment.
I ain't helping you, I'm learning.
Since you are developing on this, you can use SQL Server Dev as well. Express is more limited and I believe some people recommend that over dev because then you have less things to worry or deal with. On the flip side, you also have more features and things that can aid you in the development process. Check services (cmd prompt services.msc) and you can view if SQL Server is running. If not, you gotta install the engine as suggested already.
@axismgt, thanks that was helpful. I ended up doing this and it works. CASE WHEN {systemnotes.oldvalue} &lt;&gt; {systemnotes.newvalue} THEN 1 ELSE 0 END The &lt;&gt; was the main thing I was missing. Also learned about SELECT function. So thanks.
I have often applied html to sql strings before sending it to an email queue. I don't know of a better way.
Couple of things when running SQL server on your local machine that you may want to do other things. You will need both ssms and SQL server. You can get developer edition for free and won't have the 10gb database cap. Use the SQL server configuration manager to start and stop the database services when you don't need them. Don't forget to run your database in simple mode. Don't forget to go into instance settings and cap your memory usage. You probably don't want SQL gobbling up all your memory. 
This youtube class made me feel like a pro at SQL. It's a little boring and perhaps *too comprehensive* for your needs, but it's damn informative. https://www.youtube.com/playlist?list=PLroEs25KGvwzmvIxYHRhoGTz9w8LeXek0
 SELECT COUNT(*) FROM places WHERE name &gt;= 'PE' and name &lt; 'ZP' note that `BETWEEN 'PE' and 'ZO'` is another approach, but doesn't work correctly because it excludes names such as 'ZOMBIE' and 'ZOO'
Two minutes into her video on XML and already hooked. Seems like a great series.
There are two types of SQL people in this world: * Those that use leading commas * Those that haven't written complex queries to see why they're necessary.
What is churn?
SQL is the right answer to everything.
Ok, so this mostly worked. But when the only bucket could be applied to all products, it failed. It reported all ER funds to each product. For example, say the consumer only had an All Products bucket and then a Sausage Only bucket (buckets applied in that order). 
http://i.imgur.com/VP0Es19.jpg
you sure this is sargable?
Calculating churn for every month ... Select paidmonth, users, terminatingUsers, case when users = 0 then 0 else terminatingUsers / users as churn from ( Select paidmonth, count(1) as users, sum(case when paidmonth = lastmonth then 1.0 else 0 end) as terminatingUsers from ( Select paidmonth, max(paidmonth) over (partition by customerkey) as last month from basetable ) terminal Group by paidmonth ) monthstats 
You're missing out on quite a bit! /r/Dadjokes To be clear it only sounds like a bad idea to our puny human non negative number handling brains; the database system absolutely doesn't care and will happily find equality between -137799 and -137799 in a join condition. Why walk into a fight with your gun half loaded? An int's an int an int.
 -- one TableValue per record SELECT r.RuleID , CASE WHEN r.TableName = 'A' THEN r.TableValue ELSE NULL END AS A , CASE WHEN r.TableName = 'B' THEN r.TableValue ELSE NULL END AS B FROM Rules AS r; -- arbitrarily consolidates as many TableValues as will fit in a record, then makes a new record SELECT RuleID , A , B FROM (SELECT * , ROW_NUMBER() OVER (PARTITION BY RuleID, TableName ORDER BY (SELECT NULL)) AS RowNumber FROM Rules) AS r PIVOT (MAX(TableValue) FOR TableName IN (A,B)) AS p;
 SELECT company , metric , value , date , has_dimension , dimension_name FROM (SELECT * , ROW_NUMBER() OVER (PARTITION BY company, date ORDER BY CASE WHEN has_dimension = 1 AND dimension_name = 'CommonClass' THEN 1 ELSE 2 END) AS RowNumber FROM table) AS t WHERE t.RowNumber = 1;
I've just looked at the derived column and that appears to be what I'm after, thank you very much for your help. Out of interest what is an update statement? My knowledge is pretty slim and I couldn't spot a tool by that name.
Thanks. This one was driving me nuts. 
There are four basic SQL statements. `SELECT` - query data from a table `INSERT` - add new data to a table `UPDATE` - change data already in a table `DELETE` - remove data from a table
ProTip: Write your expression in Notepad
I agree with you, I find it easiest to do minor stuff to get the file to load properly. Then after it's in the table, manipulate the data however you want. 
Before I saw this, I thought to case the numerator and denominator as decimal also, then the final product also, and it worked 
Before I saw this, I thought to case the numerator and denominator as decimal also, then the final product also, and it worked 
Break the file into segments?
You could write a python script to read and import the file line by line, to not run out of memory.
I'm currently struggling to import a file into my RPI with MySQL. I had to go into PHP and edit the PHP.INI file to adjust the memory used, max upload size, max upload quantity, etc. Now I'm at the point where the data is dirty and I have to finagle with the file to get it to fit. It looks like you can adjust some settings [here](https://www.sqlite.org/c3ref/c_config_covering_index_scan.html). I don't know how helpful it will be though. I'd recommend to maybe give MySQL a try instead. I looked around for other ways to throttle SQLlite and I couldn't find any. 
So what is the final result you are looking for? The number of days an employee was employed, regardless if they are still there or not? Something like this can get you what you need: DECLARE @EndDate as date DECLARE @BeginDate as date DECLARE @CurrentDate as date SET @CurrentDate = GetDate() SET @BeginDate = '06/01/2005' SET @EndDate = '12/31/9999' SELECT DATEDIFF(Day,@BeginDate,DATEADD(Year,DATEDIFF(YEAR,@EndDate, CASE WHEN YEAR(@EndDate)=9999 THEN GetDate() ELSE @EndDate END),@EndDate)) This will check to see if the end date has 9999 in it. If it does, it simply converts the end date to todays date then returns the difference between their start date and end date. If it doesn't contain 9999, then it simply returns the days between when they started and when they ended. You can change the datediff to Months, weeks, years (or even seconds) or whatever you want.
Thanks. I could convert the excel files to .csv. That might make it easier. 
I'll assume employees always start on the 1st and end on the last day of month. If that's not the case you'll need to modify the query a little. ;WITH CTE AS ( SELECT *, DATEDIFF(MONTH,Emp_Start,CASE WHEN Emp_End='99991231' THEN GETDATE() ELSE Emp_End END) + 1 AS MonthsEmployed FROM Employees E ), /* Create Numbers table */ /* https://stackoverflow.com/questions/1393951/what-is-the-best-way-to-create-and-populate-a-numbers-table */ Pass0 as (select 1 as C union all select 1), --2 rows Pass1 as (select 1 as C from Pass0 as A, Pass0 as B),--4 rows Pass2 as (select 1 as C from Pass1 as A, Pass1 as B),--16 rows Pass3 as (select 1 as C from Pass2 as A, Pass2 as B),--256 rows Pass4 as (select 1 as C from Pass3 as A, Pass3 as B),--65536 rows Numbers as (select row_number() over(order by C) - 1 as Number from Pass4) /* Starts at 0 */ SELECT Emp_Id, Emp_Fname, Emp_Lname, Emp_Start, Emp_End, RIGHT('0' + CONVERT(VARCHAR(2),(MONTH(Emp_Start) + Number - 1) % 12 + 1),2) + CONVERT(VARCHAR(4),YEAR(Emp_Start) + (Number + MONTH(Emp_Start) - 1)/12) Month FROM CTE INNER JOIN Numbers ON Number &lt; MonthsEmployed ORDER BY Emp_Id, Number
`CASE WHEN X THEN Y ELSE Z END` Given a table `Table1`: Column1| :--: 1| 2| 3| 4| 5| 6| SELECT Column1, CASE WHEN Column1 % 2 = 0 THEN 'Even' ELSE 'Odd' END AS Column2 FROM Table1 Will return: Column1|Column2 :--:|:-- 1|Odd 2|Even 3|Odd 4|Even 5|Odd 6|Even http://sqlfiddle.com/#!9/4024d/2
I will try that out, since I'm sure I will using it a lot in the future. Thanks again! 
How are you loading that data into SQlite? What kind of tool, what command? 
Good idea I’ll try that
I’ll do this just to practice python!
SQLite commands: .mode csv .import FILENAME DBNAME
This just happened to me too. I noticed that a few days ago i got an update from inTune. Since then it hasn't worked, I'm getting the same message and haven't yet figured out how to fix it.
This got me on the right track and will help so much. Thank you so much, is there is a SOLUTION VERIFIED, SOLVED for /r/SQL let me know! 
So if I'm reading this right, given those three columns, you want to find out which customer shops at which store the most and only display customer and store. So, choose each customer's favorite store based on counts. So here I have a table with Bob and Jane and they shop at various stores but it looks like Bob like Best Buy a lot and Jane like to eat at Subway. So the goal is to produce a query that shows each customer's favorite store based on row counts. I'm using a CTE to create the table, then the 2nd part of the CTE is where the meat of it is, looking at customers and counts of visits. I'm also partitioning the data into customers by customer ID and I'm ordering it by the count descending and aliasing it as "r". In the final selection, I'm only interested in the top "r", so it is limited to r=1. And what I'm selecting every customers top "r", which is where they shopped the most. with mytable as ( select 123 cust_id, 'Bob' name, 'Macys' store from dual union all select 123, 'Bob', 'Best Buy' from dual union all select 123, 'Bob', 'Best Buy' from dual union all select 123, 'Bob', 'Best Buy' from dual union all select 123, 'Bob', 'McDonalds' from dual union all select 123, 'Bob', 'McDonalds' from dual union all select 345, 'Jane', 'Burger King' from dual union all select 345, 'Jane', 'Burger King' from dual union all select 345, 'Jane', 'Subway' from dual union all select 345, 'Jane', 'Subway' from dual union all select 345, 'Jane', 'Subway' from dual union all select 345, 'Jane', 'Macys' from dual union all select 345, 'Jane', 'Smoothie King' from dual union all select 345, 'Jane', 'Sofa Kingdom' from dual ), mytable2 as ( select distinct cust_id, name, store, count(*), row_number() over( partition by cust_id order by count(*) desc) r from mytable group by cust_id, name, store order by r ) select cust_id, name, store from mytable2 where r=1; You can try this yourself in Oracle Live. I'm not sure what platform you're using because you didn't say. 
Honestly I don’t know. I installed SQLite and I’m running the import on the command line. I don’t know where it’s loading the data. The table is not created, it’ll be created after (or during) the import.
I have both, I saved a copy of the original files to CSV. I will read through that documentation and give it a try and see how it goes. Thanks for the info!
What would that look like in PL/SQL?
&gt; The data was just split because of the row limits in Excel Your solution won't scale to OP's requirements. 
Do it twice?
Row limit in excel is 1,048,576. I don't think copying and pasting is the appropriate solution.
Sorry maybe i'm not reading the post right, but doing a CTE with every specific name sounds lengthy. Why not just do something like this... so i created a table with those columns named CustNameStore with CteIdStoreCount as (select id, store, count(id) as storecount from CustNameStore group by id, store), CteCustFav as ( select id, store, ROW_NUMBER() OVER(Partition by id ORDER BY storecount desc) AS Ranking from CteIdStoreCount ) select cf.id, cf.store, cic.storecount as Qty_of_Visits from CteCustFav cf join CteIdStoreCount cic on cf.id = cic.id where cf.ranking = 1 If you want to see how i setup the data i did this... create table CustNameStore (ID int, Name nvarchar(100), Store nvarchar(100)) insert into CustNameStore (id, Name, Store) values ('1','CustomerA','StoreA'), ('2','CustomerB','StoreB'), ('3','CustomerC','StoreC'), ('4','CustomerD','StoreA'), ('5','CustomerE','StoreB'), ('6','CustomerF','StoreC'), ('7','CustomerG','StoreA'), ('8','CustomerH','StoreB'), ('9','CustomerI','StoreC'), ('10','CustomerJ','StoreA'), ('11','CustomerK','StoreB'), ('12','CustomerL','StoreC'), ('1','CustomerA','StoreA'), ('2','CustomerB','StoreB'), ('3','CustomerC','StoreC'), ('4','CustomerD','StoreA'), ('5','CustomerE','StoreB'), ('6','CustomerF','StoreC'), ('7','CustomerG','StoreA'), ('8','CustomerH','StoreB'), ('9','CustomerI','StoreC'), ('10','CustomerJ','StoreA'), ('11','CustomerK','StoreB'), ('12','CustomerL','StoreC'), ('1','CustomerA','StoreA'), ('2','CustomerB','StoreB'), ('3','CustomerC','StoreC'), ('4','CustomerD','StoreA'), ('5','CustomerE','StoreB'), ('6','CustomerF','StoreC'), ('7','CustomerG','StoreA'), ('8','CustomerH','StoreB'), ('9','CustomerI','StoreC'), ('10','CustomerJ','StoreA'), ('11','CustomerK','StoreB'), ('12','CustomerL','StoreC'), ('1','CustomerA','StoreA'), ('2','CustomerB','StoreB'), ('3','CustomerC','StoreC'), ('4','CustomerD','StoreA'), ('5','CustomerE','StoreB'), ('6','CustomerF','StoreC'), ('7','CustomerG','StoreA'), ('8','CustomerH','StoreB'), ('9','CustomerI','StoreC'), ('10','CustomerJ','StoreA'), ('11','CustomerK','StoreB'), ('12','CustomerL','StoreC'), ('1','CustomerA','StoreA'), ('2','CustomerB','StoreB'), ('3','CustomerC','StoreC'), ('4','CustomerD','StoreA'), ('5','CustomerE','StoreB'), ('6','CustomerF','StoreC'), ('7','CustomerG','StoreA'), ('8','CustomerH','StoreB'), ('9','CustomerI','StoreC'), ('10','CustomerJ','StoreA'), ('11','CustomerK','StoreB'), ('12','CustomerL','StoreC'), ('1','CustomerA','StoreA'), ('2','CustomerB','StoreB'), ('3','CustomerC','StoreC'), ('4','CustomerD','StoreA'), ('5','CustomerE','StoreB'), ('6','CustomerF','StoreC'), ('7','CustomerG','StoreA'), ('8','CustomerH','StoreB'), ('9','CustomerI','StoreC'), ('10','CustomerJ','StoreA'), ('11','CustomerK','StoreB'), ('12','CustomerL','StoreC'), ('1','CustomerA','StoreA'), ('2','CustomerB','StoreB'), ('3','CustomerC','StoreC'), ('4','CustomerD','StoreA'), ('5','CustomerE','StoreB'), ('6','CustomerF','StoreC'), ('7','CustomerG','StoreA'), ('8','CustomerH','StoreB'), ('9','CustomerI','StoreC'), ('10','CustomerJ','StoreA'), ('11','CustomerK','StoreB'), ('12','CustomerL','StoreC'), ('1','CustomerA','StoreA'), ('2','CustomerB','StoreB'), ('3','CustomerC','StoreC'), ('4','CustomerD','StoreA'), ('5','CustomerE','StoreB'), ('6','CustomerF','StoreC'), ('7','CustomerG','StoreA'), ('8','CustomerH','StoreB'), ('9','CustomerI','StoreC'), ('10','CustomerJ','StoreA'), ('11','CustomerK','StoreB'), ('12','CustomerL','StoreC'), ('1','CustomerA','StoreA'), ('2','CustomerB','StoreB'), ('3','CustomerC','StoreC'), ('4','CustomerD','StoreA'), ('1','CustomerA','StoreA'), ('2','CustomerB','StoreA'), ('3','CustomerC','StoreA'), ('4','CustomerD','StoreA') and then filled up the table with more random data insert into CustNameStore (id, Name, Store) values ('1','CustomerA','StoreC'), ('2','CustomerB','StoreD'), ('3','CustomerC','StoreA'), ('4','CustomerD','StoreB'), ('5','CustomerE','StoreC'), ('6','CustomerF','StoreD'), ('7','CustomerG','StoreA'), ('8','CustomerH','StoreB'), ('9','CustomerI','StoreC'), ('10','CustomerJ','StoreD'), ('11','CustomerK','StoreA'), ('12','CustomerL','StoreB'), ('1','CustomerA','StoreC'), ('2','CustomerB','StoreD'), ('3','CustomerC','StoreA'), ('4','CustomerD','StoreB'), ('5','CustomerE','StoreC'), ('6','CustomerF','StoreD'), ('7','CustomerG','StoreA'), ('8','CustomerH','StoreB'), ('9','CustomerI','StoreC'), ('10','CustomerJ','StoreD'), ('11','CustomerK','StoreA'), ('12','CustomerL','StoreB'), ('1','CustomerA','StoreC'), ('2','CustomerB','StoreD'), ('3','CustomerC','StoreA'), ('4','CustomerD','StoreB'), ('5','CustomerE','StoreC'), ('6','CustomerF','StoreD'), ('7','CustomerG','StoreA'), ('8','CustomerH','StoreB'), ('9','CustomerI','StoreC'), ('10','CustomerJ','StoreD'), ('11','CustomerK','StoreA'), ('12','CustomerL','StoreB'), ('1','CustomerA','StoreC'), ('2','CustomerB','StoreD'), ('3','CustomerC','StoreA'), ('4','CustomerD','StoreB'), ('5','CustomerE','StoreC'), ('6','CustomerF','StoreD'), ('7','CustomerG','StoreA'), ('8','CustomerH','StoreB'), ('9','CustomerI','StoreC'), ('10','CustomerJ','StoreD'), ('11','CustomerK','StoreA'), ('12','CustomerL','StoreB'), ('1','CustomerA','StoreC'), ('2','CustomerB','StoreD'), ('3','CustomerC','StoreA'), ('4','CustomerD','StoreB'), ('5','CustomerE','StoreC'), ('6','CustomerF','StoreD'), ('7','CustomerG','StoreA'), ('8','CustomerH','StoreB'), ('9','CustomerI','StoreC'), ('10','CustomerJ','StoreD'), ('11','CustomerK','StoreA'), ('12','CustomerL','StoreB'), ('1','CustomerA','StoreC'), ('2','CustomerB','StoreD'), ('3','CustomerC','StoreA'), ('4','CustomerD','StoreB'), ('5','CustomerE','StoreC'), ('6','CustomerF','StoreD'), ('7','CustomerG','StoreA'), ('8','CustomerH','StoreB'), ('9','CustomerI','StoreC'), ('10','CustomerJ','StoreD'), ('11','CustomerK','StoreA'), ('12','CustomerL','StoreB'), ('1','CustomerA','StoreC'), ('2','CustomerB','StoreD'), ('3','CustomerC','StoreA'), ('4','CustomerD','StoreB'), ('5','CustomerE','StoreC'), ('6','CustomerF','StoreD'), ('7','CustomerG','StoreA'), ('8','CustomerH','StoreB'), ('9','CustomerI','StoreC'), ('10','CustomerJ','StoreD'), ('11','CustomerK','StoreA'), ('12','CustomerL','StoreB'), ('1','CustomerA','StoreC'), ('2','CustomerB','StoreD'), ('3','CustomerC','StoreA'), ('4','CustomerD','StoreB'), ('5','CustomerE','StoreC'), ('6','CustomerF','StoreD'), ('7','CustomerG','StoreA'), ('8','CustomerH','StoreB'), ('9','CustomerI','StoreC'), ('10','CustomerJ','StoreD'), ('11','CustomerK','StoreA'), ('12','CustomerL','StoreB'), ('1','CustomerA','StoreC'), ('2','CustomerB','StoreD'), ('3','CustomerC','StoreA'), ('4','CustomerD','StoreB'), ('1','CustomerA','StoreC'), ('2','CustomerB','StoreD'), ('3','CustomerC','StoreA'), ('4','CustomerD','StoreB') go 10
If you only downloaded Sql Server Management Studio, then still need the actual database. SSMS is essentially just an IDE for working on SQL server databases. Also screenshots are a million times better than descriptions
Oh wow. I didn't realize it was that large. Yup, that would take days. 
Try downloading SQL server developer edition or express edition. https://www.microsoft.com/en-us/sql-server/sql-server-downloads. Both are free. You can use SQL server management studio to connect after you have setup and configured your database. You need to install and create an instance of SQL server before you can connect to it to create databases. 
Agreed. get developer edition. Check out BrentOzar’s walkthrough if you need a helping hand, but the wizard is mostly straight forward. https://www.brentozar.com/archive/2016/03/sql-server-2016-installation-screenshot-tour/amp/
Well go developer edition if you are just learning or coding against. If this is on a box at work that you will be using for actual work in production then developer edition can get your company in trouble... Basically if just you will eventually be using it then dev edition is good. 
Definitely. Good point. Dev edition is only for non-production. Regardless I’d always recommend setting up a development version of any instance to get going and then migrating to production once it’s ready to go.
Are you looking for the column to stay in the table or are you looking for it to be just within a query execution? Because you can create calculated columns that will stay with the table for others to use. These can be persisted (written to the data pages) or re calculated at query run time...
Even I want to start preparing for these exams. Did you already take them? If so which materials did help you with the exam?
your HAVING clause is incorrect. Take a look here: [HAVING clause](https://www.w3schools.com/sql/sql_having.asp)
Which of the three SELECT queries are you running that gives you that output? If you're running this altogether, it looks like SQL Server is just throwing out the output of the first SELECT query in your code. 
I don't think think any of the queries would return the data that is shown. In the queries, there are only 2 columns being selected, but in the result all columns are shown. Not sure what query OP is actually running. 
His first query is simply Selecting all from exercise_logs.
here is your solution: https://github.com/dotnet/announcements/issues/53 
The first one.
just remove the AVG function and don't forget to multiply by 100.0 to get a percentage SELECT CalendarQuarter , ArrivalYear , 100.0 * SUM(CASE ItemType when 'A' then 1 else 0 end ) / COUNT(*) AS percent 
So as far as the duplication of insertions into your database table, the database isn't going to insert any rows into your exercise_logs that you didn't otherwise tell it to. You can TRUNCATE exercise_logs; and it will remove any rows in your table without deleting the table itself. Helpful while you're still working on your table creations in the database. Note though, that TRUNCATE won't reset your sequences - so the next row you insert will be 21 instead of 1 even after the TRUNCATE.
Thank you very very very very much!!!! this was it...i now can connect and have the options i was after to get started. 
Depending on how your Schedule_Items table is laid out, it may be able to handle this. Anyways, I'd have something like this. start_date - Date in which the advertisement should begin running on expiration_date - Date in which the advertisement should stop running on complete_status - Signifies whether the scheduled advertisements for whomever has been fulfilled. NOTE: If the dates between first run and last run can be interrupted, then you'll just make multiple entries into the table.
&gt; Any ideas? take another look at ` avg( sum( `
Is this for an actual work project? 
You can't just comment out code that you don't understand and see if it works. You need to know WHY it's not right or you're never going to get it. Have you solved this yet? This is the most obvious answer to your issue. 
You're right. I didn't at first realize between didn't need exact fields. Not sure why I thought that. You could technically do either way, but mine is not correct for the question at hand. 
but... but... `WHERE LEFT(name,2) BETWEEN 'PE' and 'ZO'` ~is~ correct sargability relates only to performance
I don't know that term. Will definitely look it up. 
Hey there, Yes, you are right. I did solve it by renaming exercise_logs to exercise_log.
I had no idea Ken M wrote SQL. This made me laugh out softly and I enjoyed it. :) 
Most definitely. Another individual on here suggested being brief with the other topics in the book but to elaborate and really drill into functions and best practice. That’s where the revision part comes in and not quite at that step yet. But I appreciate the feedback!
It is my pleasure dear philamander. But still, why did this solve my issue?
I just started the course too and everything worked as it should've. You mention that he has a workspace but at that point in the course you should also have one! 
turns out i had to go to the old c9 platform through his invite. now im trying the $ show database; but thats giving me error too. im in the workspace no
SHOW DATABASES;
I admire your persistance and I hope that you have enthusiasm and a willingness to help you along your path to advancing SQL skills for your career. So, I'm going to take you seriously now because I thought you were joking when you responded. I can assure you that I'm 100% positive renaming the table didn't do anything. Something else occurred. Either the data got corrected when you created a new table and inserted new data or maybe your query changed and you didn't realize it. Bottom line, the recreation of the table happened at the same time as the query getting fixed, but it was surely not what fixed the issues you were having. The biggest issue here is that someone pointed out an error you had with your HAVING statement and you responded by commenting it out, rather than trying to understand what was wrong with it. I don't want you to walk away from this without having learned what was wrong in your query. You used a HAVING statement like it was a WHERE clause. HAVING statements are applied to aggregate function results like COUNT() or SUM() &gt;= something. That was the issue here. Commenting it out would have just ran the type, sum() results without any filter. Just would have returned all types. You also pasted in a bunch of SQL that included bits that we didn't need, which isn't heinous, but it's something you should notice for next time. We don't need the SELECT * FROM line in there. Maybe you just forgot about that though. Not a big deal. It just makes me think you're still learning SQL and are at an early level. Seeking out help here and online is a really really good place to start. Just make sure you pay attention to the solutions people give and you should understand what fixes the issue so you can apply that information to another query you write and not just only see it in this one context. 
Can you post table structure? I'd do a select, and in the where clause have a statement where len(code_type) = 2... I'm not the most familiar with Oracle 10g, but I'm sure there's some length function that you can stick in the where clause after you do the join. Check the docs
can you put two values in join? "join t2 on t1.code = t2.code and t2.code_type = 2"
I think you have a java problem, not a sql problem;) Can you pull data from cmd?
Are you going to be doing something that is Java or mysql-specific? I mean, are you really into some specific Java package or mysql-specific stuff like user management etc? If you're just creating a console app and you're not too concerned with the language, I'd recommend Python + sqlite. The setup is minimal and you can think more about the mechanics of the database design than the specifics of making java talk to your local mysql server. Sqlite makes it much easier to play around with db design without worrying about setting up 'under the hood.'
Why are you calling mysql from your command prompt if you want to build a Java app? Just call it from Java... 
Well the issue was I couldn't get them connected per say. I wasn't sure how it was supposed to work, but I copied a simple Java file from a tutorial which just checked to see if their was a connection and it couldn't compile as a keyword in the file namely "Connection" wasn't recognized.
No not language specific in any way(at least I think). What I wanna do is have some program which I can enter information into and that gets stored permanently and can be accessed by the program. If the program is no longer running the information is still there and would not need to be put back in again. I only say Java because I'm most comfortable with it as a language, but if another language works better then absolutely I'm open to working and learning whatever it may be. Just a project to help me become a better programmer and developer so any learning is solid.
That sounds like a question about java more than sql. I help people with select queries here ;)
Fair enough! Well if I ever get this figured out and I need help with select queries I know where to go!
I'm a python programmer so I would just use python with sqlalchemy to create database objects. I would then map my input fields in the gui to the database objects. 
Alright I'll look into that, thanks!
Alright I'll look into that, thanks!!
 SELECT customers.state_abbreviation, code_table.code_name FROM customers LEFT JOIN code_table ON code_table.code = customers.state_abbreviation AND code_table.code_type =2 
Python will be easier. To tack on to what someone else mentioned, try to avoid using a database abstraction layer like sqlalchemy before you get the hang of using raw sql. Knowing your needs, if it were me I would use python and sqlite. I was more comfortable in Java before I started using Python and the transition was pretty quick. It's very user friendly.
On Mobile, its just the picture. I was very confused and amused.
Have you tried select into?
Read the second line, both select and into clauses are in there.
Interesting. Would you be able to send me a screenshot and maybe browser version?
Do you have any SQL code that you've tried thus far? 
These somehow look more like homework than interview questions...
If you don't know where to begin with these questions, you're not qualified for the job.
These are practice questions from an android app
It's for an internship
Dude, it's for an internsh interview. I'm practicing SQL through an android app and i have 4 wrong answers out of 120. Thanks for the helpful judgement
Yes, import the first excel into table 1 and then import the second excel into the existsing table
If you do know where to begin these questions, then post your solutions. You'll find the sub is pretty helpful when the post has more than copy/paste questions. From the sidebar: &gt; #Help posts &gt; If you are a student or just looking for help on your code please do not just post your questions and expect the community to do all the work for you. We will gladly help where we can as long as you post the work you have already done or show that you have attempted to figure it out on your own.
I have just added the queries in the description under the questions. Thanks
Do you know the primary key of the table. That could change things. I'm assuming it's at the invoice level. &gt; SELECT city, country, AVG(InvoiceTotal) as 'Avg Revenue' FROM Customer GROUP BY country, city This displays the average revenue per invoice. To display the average revenue per customer, you SUM revenue over the customer first, then take the AVG over city/country. &gt; SELECT country, SUM( InvoiceTotal) FROM customer GROUP BY countrt HAVING COUNT(email LIKE '%yahoo%') &gt; 1 Again this shows at least 2 invoices from yahoo. I think you want the yahoo logic in the where clause, and COUNT(DISTINCT CustomerID) in the having clause. &gt; I have no answer for this It's not worded super clearly, but I think they want three fields: Company, CompanyCustomerCount, CustomerGrandTotal &gt; SELECT country FROM customer WHERE COUNT (state) &gt; 2 ORDER BY Country COUNT(state) will count the number of records with a state value. You need COUNT(DISTINCT to count the number of different states.
Thanks for all of the help. I tried bulk insert, but was unable due to rights restrictions. I was able to add the first one then load another my manually selecting it to go into the first table with an append option. 
Personally, i'd go with a prebuilt web-based online order site. How much is he going to pay you? Does he want to spend money on hardware, maintenance, backups and restore testing, etc? 
&gt; Max number of order is ~50 per day Then the DB server does not really matter. Use Postgres (my personal preference) or a recent version of MariaDB (a bit more tooling) and you're good. Given that you ask this question, and how standard this is going to be, using a well known ORM might be the way to go, and will make it easy to switch to a different DB server later if you need to.
You're welcome. There isn't any need for this attitude, this sub sees untold amounts of lazy and/or last second homework students who ask questions exactly like this on a regular basis. 
Is the mask format externally given or did you make it up?
I'd agree with another poster here, look for an off the shelf web shop platform. Don't reinvent the wheel here, too much opportunity for security issues if you don't know what you're doing.
I think you want to use `NOT IN` or `NOT EXISTS` instead of a join. WHERE p.PersonID NOT IN (SELECT e.PersonID FROM Eduction AS e WHERE e.Education = 'Diploma')
Unfortunately I dont think we even had NOT IN or even Selects on a where statements in class :/ Also I forgot to mention this is a Example from the JOIN category so I dont think that the main focus of it would be sth that isnt a join :/ Maybe a self-join? I dont know. However your solution works tho I dont think I can use it in the test :(
&gt; Unfortunately I dont think we even had NOT IN or even Selects on a where statements in class :/ &gt; &gt; &gt; &gt; Also I forgot to mention this is a Example from the JOIN category so I dont think that the main focus of it would be sth that isnt a join :/ &gt; &gt; &gt; &gt; Maybe a self-join? I dont know. However your solution works tho I dont think I can use it in the test :( Unfortunately I dont think we even had NOT IN or even Selects on a where statements in class :/ Also I forgot to mention this is a Example from the JOIN category so I dont think that the main focus of it would be sth that isnt a join :/ Maybe a self-join? I dont know. However your solution works tho I dont think I can use it in the test :( ----- Sorry for copy- paste answer but there really isnt anything to add:/ If there is no solution without these I guess im just gonna skip that example?
What's the help you need? Looks like you got it.
The join method goes like this: LEFT OUTER JOIN Education e ON e.EducationID = p.PersonID AND e.Education = 'Diploma' WHERE e.EducationID IS NULL Meaning you try to join to diploma records, and filter results only when the join fails to find a match.
It's giving me an error about numeric
Can you post the whole error?
Wow! This works? I dont get how but it does! Thanks! (im gonna figure out why this works. Eventually)
ERROR: invalid input syntax for type numeric: "Unknown"
Agreed. Squarespace is the one I get advertised to most.
Much like you saw, I'm guessing there's data in that column that cannot be converted to numeric and you need to pluck the strings out or handle them / ignore them. 
appreciate the help
Just use PostgreSQL. It's kind of like MySQL but better (more features, more reliable) and not owned by Oracle.
Access. Built in 10 minutes and is fairly simple for nubs
If credit card info is included, definitely go off the shelf.
The one table, three column approach is nice for **reporting.** This is a good EDW dimension structure. All your filters are in one place, and it's not too much work to make a single join to your financial data. A two table approach, FinRepGrp (FinRepGrp, FrcstGrp) ; FrcstGrp (FrcstGrp, PriceCode), is good for **tracking.** You get the normalization benefits of avoiding insert/update anomalies. An extra join is the cost, but may not be a big deal. I don't advise a hierarchal table. It's flexible for things like employees (when you have an unspecified number of levels), but you describe exactly two well defined levels. So define them well. Recursive ctes are not something you want to use unless absolutely necessary (they can't use more efficient join algorithms). They work reasonably well in multi-dimensional databases (with dedicated hierarchal functions), but they're not as nice in relational databases.
Thanks for the insight. This table will basically be the control table for all financial reporting for the company. I think because of that, I'm going to stick with the one table, three column approach. I doubt that there would ever be need for a third layer. At least not in the lifespan of the ERP. Im still going to add an Xref table for all the codes, so we can have better descriptions than G1 and F1. Or should it be 3 Xref tables? That way I can FK the whole main table to ensure the group exists before adding it to a relationship. Thoughts? That way it only has to change one place if a group changes name/spelling. Any other gotchas to look out for?
&gt; You get the normalization benefits of avoiding insert/update anomalies. &gt; That way it only has to change one place if a group changes name/spelling. This is the reason to put FinancialReportingGroup, ForcastGroup, and PriceCode in separate tables. If you do this, there isn't much reason to have a single table with all three references. This table would be redundant.
There's nothing indicating the relationship between your two subqueries and ts.itemno. The two subqueries are just going to sum everything and be returned for each row. There's honestly no reason you'd need to join or do a subquery here because it's all on the same table Try something like SELECT ts.itemno, SUM(CASE WHEN ts.saledate &gt; '2017-01-01' AND ts.saledate &lt;'2017-12-31' THEN sales END) as fifty_Two_Week, SUM(CASE WHEN ts.saledate &gt; '2017-10-09' AND ts.saledate &lt;'2017-12-31' as '12 week sales' THEN sales END) as twelve_Week FROM total.salesdata GROUP BY ts.itemno 
&gt;One of my co-workers suggested I use an EDW trick he learned at his last job making a relationship table It seems to me like he saw this once and didn't really understand why it was useful for the problem and hand and is now just trying to apply it to everything. For a reporting structure I would prefer your initial inclination of a three column table. Simple. Effective.
In SQL there are many ways to skin a cat. /u/theduckspants idea should work too. IMO it'd be more correct to have the FROM Products as a, but it's all good for simple things. SELECT a.ProductID, b.Sales as [12 week sales], c.Sales as [52 Week sales] FROM TotalSales a LEFT JOIN ( SELECT ProductID SUM(Sales) as Sales FROM TotalSales WHERE Sales in last 12 weeks GROUP BY ProductID ) as b ON a.ProductID = b.ProductID LEFT JOIN ( SELECT ProductID SUM(Sales) as Sales FROM TotalSales WHERE Sales in last 52 weeks GROUP BY ProductID ) as c ON a.ProductID = c.ProductID 
Agreed all those hits to one table are less than ideal. Conversely CASE statements are rough on performance so I try to avoid them. In terms of overall performance I'd be curious to see what's better. For 95%+ of use cases either of our code would work fine. Just to clarify, I was saying in my code I should have table 'a' be Product. When I reread my comment it seemed like I was addressing yours.
The hierarchy table would be redundant, yes, with the codes that define the relationships. But the Xref tables (3) for readable names wouldn't. What do I gain by splitting out the relationships into three tables? I guess I'm having trouble visualizing how joins would work when splitting it out into three tables.
It would ultimately be the most flexible, right? But it makes it needlessly complicated. I'm on board with the first table approach now.
You either want: (an typical reporting structure) CREATE TABLE PriceCode ( PriceCode, PriceCodeName, ForcastGroupCode, ForcastGroupName, FinancialReportingGroupCode, FinancialReportingGroupName ); or: (a typical normalized structure) CREATE TABLE FinancialGroup ( FinancialReportingGroupCode, FinancialReportingGroupName ); CREATE TABLE ForcastGroup ( ForcastGroupCode, ForcastGroupName, FinancialReportingGroupCode ); CREATE TABLE PriceCode ( PriceCode, PriceCodeName, ForcastGroupCode ); 
I've been in my reporting position for about 4 months, and coming from onsite IT support at the same place, I would never go back. But it really depends on what you're passionate about. As far as programming languages go, SQL basics aren't all too difficult to pick up, and it gets a lot easier as you start working on real requests on a daily basis. I would stick with it if you enjoy it and you see a reliable career-path ahead.
Your case statement is operating at the same level as the rest of your query. Either remove the charges and credits Freon the select portion, or also wrap them in a sum. Your group by level is dictating that the query break out each credit and charge. You need to remove those from the group by so that they can be rolled up to the customer level.
Flexible? Sure. Useful? That's debatable. These sound like three attributes of an account. I'm not even sure how the hierarchy would work given that G2 goes to F3 for two different PCs. How would know which one to use?
I self taught and knew enough to get hired as a report writer in a couple months of working on projects on my own. Probably took me about a six months to a year to become on par with or better than the other folks on my team at SQL. Once you start doing it every day it's pretty easy to learn. I went from entry level report writer to senior business intelligence developer with two steps in between, so there is a nice career path to take. But from here it is stay roughly where I am forever, become a manger, or go get more education to become a data scientist type.
Just a lil explanation. Imagine that G2 is "United States". F3 is "Other car dealers" and the PCs are "Ford Dealership" and "Chevy Dealers". Ford and Chevy buy our products at two different prices - hence price codes-. But they both do only a small amount of business with us each year, so our Forecaster want to plan at the "Other Dealers" level, so he isn't magnifying the volatility of the sales history from either Chevy or Ford. This is very common in forecasting. We aggregate unit sales for both, forecast based on the variability of the combined sales, and multiply by an average selling price of Chevy and Ford. Then assume we have an account with Dodge, but they are 90% of our business, so we want to forecast them separately. So we give Dodge F4 as a forecast level code, and still G2 as financial reporting code. When we forecast, we have enough information to be accurate about Dodge, and we can more accurately forecast the sales dollar value because we can multiply out by Dodges exact pricing. Now we get to the Board Room. And they don't care if we sold to Ford, Chevy or Dodge. So we roll up everything with G2, to show total sales for the United States. And we add up all the Forecasts for the accounts with a G2 in the financial reporting column, to compare it to. In essence, forecasting at a level higher than price code in our industry results in more accurate forecasting.
You could always become an analytics solution architect. Wish I had about a dozen more on my team right now ...
Most likely that means the same thing as what I do now with a different title. I do data analysis, control flow into our data lake, ETL to create data marts, create and monitor sever jobs, application scripting both for dashboards and web applications, front end visualizations, tool integrations, and whatever else it takes to provide an analytic solution to the customer.
What a pretentious bot
Sort of but also not? My role's a lot less hands on, usually involves hybrid technologies (streaming analytics connected to a MLLib job, or pushing Azure SQL Data Warehouse dimension data through an MDM service) and a lot more discussions around commercial deal shaping and cost, resources, change management, cloud security and compliance, cost, managed services, DevOps .. did I mention cost? Sounds like you've got pretty strong end to end experience, that was my background about 5 years ago. The only pivot I made was to go work in sales as a SME for a bit. Really makes you appreciate the decisionmaking criteria that go into analytics.
loc1 is never going to equal loc2, so use UNION ALL instead of UNION, because UNION sorts the entire result set to look for duplicates, of which there won't be any
Depending on how big the table is now or in the future, I'd rather use more CPU with the CASE statement than the extra I/O from scanning 3 times.
Interesting. I can't imagine being less hands on and having to deal with the financial things. Under our architecture only the team's director deals with that aspect of things
I most likely don't have access to a (non-business) Linux machine, so I'd still be open to Windows solutions.
I was fortunate to get hired into a mid range business intelligence/reporting position with the understanding that I would pick up the SQL in about 3-6 months. I was able to do it to meet my customer and supervisors expectations, but looking back I feel like I knew nothing during that time. 5 years in and I am still learning better and more efficient ways to accomplish things. Things that took me a couple of hours to work through in my first year now take just a few minutes. So I think you can pick it up pretty quick if you have a knack for logic and understand how a relational database works, but you will not “arrive” in a short amount of time. Like many things, it’s not hard to do, but it is hard to be great at. (Still shooting for great)
To UNPIVOT two columns, you need to do it by hand with a CROSS APPLY. Internally UNPIVOT and CROSS APPLY without a from clause are the same thing. SELECT i.Type, u.N, u.Loc, u.Qty FROM #Items AS i CROSS APPLY (VALUES (1, Loc1, Qty1) , (2, Loc2, Qty2)) AS u(N,Loc,Qty) or SELECT i.Type, u.N, u.Loc, u.Qty FROM #Items AS i CROSS APPLY (SELECT 1 AS N, Loc1 AS Loc, Qty1 AS Qty UNION ALL SELECT 2 AS N, Loc2 AS Loc, Qty2 AS Qty) AS u
X-Post referenced from [/r/libraries](http://np.reddit.com/r/libraries) by /u/verpeteren [I needed a SQL table with 'Dewey Decimal Classification' and documented it maybe it is useful to somebody.](http://np.reddit.com/r/Libraries/comments/7rp0sr/i_needed_a_sql_table_with_dewey_decimal/) ***** ^^I ^^am ^^a ^^bot. ^^I ^^delete ^^my ^^negative ^^comments. ^^[Contact](https://www.reddit.com/message/compose/?to=OriginalPostSearcher) ^^| ^^[Code](https://github.com/papernotes/Reddit-OriginalPostSearcher) ^^| ^^[FAQ](https://github.com/papernotes/Reddit-OriginalPostSearcher#faq)
This is a great answer. I’d probably use common table expressions like so: WITH cteProducts AS ( SELECT * FROM Products ), cte52Weeks AS ( SELECT * FROM cteProducts WHERE in 52 weeks ), cte12Weeks AS ( SELECT * FROM cteProducts WHERE in 12 weeks ) SELECT * FROM cte52Weeks FULL JOIN cte12Weeks on ProductIDs
 tblUser AS recTestuser RIGHT OUTER JOIN (HireDate RIGHT OUTER JOIN (tblDetails AS cc_dd INNER JOIN tblAllocation AS dda ON cc_dd.RefNo = dda.RefNo LEFT OUTER JOIN tblUser AS RecUser ON dda.Alloc = RecUser.UserID ) ON HireDate.RefNo = cc_dd.RefNo ) ON RecTestuser.UserID =cc_dd.Staff This is a valid technique, and can be very important if FORCE ORDER is being used. Most LEFT OUTER JOINS are actually performed as RIGHT OUTER JOINS, and with FORCE ORDER getting hash joins on the correct side makes a big difference. 
I was able to obtain an entry level position using SQL without any IT / SQL experience or a degree through diligience, luck, and promotions. I felt "competent" after three years and definitely capable after a year. I feel extremely confident and adept at seven years, although I know enough to know I'm still at the surface of a very deep rabbit hole when I compare to people who have done this for 15-20 years.
Thank you! What about this one? Trying it on my own, but want to make sure it's correct. RIGHT OUTER JOIN tblExchMatch AS em INNER JOIN tblExchInvoices As ei ON em.MainRef = ei.ExchRef INNER JOIN ExchPay AS ep ON em.MatchRef = ep.ExcRef ON ccl_dd.RefNo = ei.RefNo LEFT OUTER JOIN tblCustomer AS c ON ep.AccountCode = c.acCode 
Thank you! What about this one? Trying it on my own, but want to make sure it's correct. RIGHT OUTER JOIN tblExchMatch AS em INNER JOIN tblExchInvoices As ei ON em.MainRef = ei.ExchRef INNER JOIN ExchPay AS ep ON em.MatchRef = ep.ExcRef ON ccl_dd.RefNo = ei.RefNo LEFT OUTER JOIN tblCustomer AS c ON ep.AccountCode = c.acCode
 Something AS ccl_dd RIGHT OUTER JOIN (tblExchMatch AS em INNER JOIN tblExchInvoices As ei ON em.MainRef = ei.ExchRef INNER JOIN ExchPay AS ep ON em.MatchRef = ep.ExcRef) ON ccl_dd.RefNo = ei.RefNo LEFT OUTER JOIN tblCustomer AS c ON ep.AccountCode = c.acCode
Mine was all self study. I spent a few months using tools like sqlbolt, sqlzoo, w3schools, etc, and installed ssms, sql server express and adventureworks on my home and work PC. You'll want to play with something like SSRS too. Our data team caught word that I might be interested and occasionally gave me some projects to work on. A few months later the position opened up and I got the job. 
For sure. I had an interest in baseball statistical analysis, so I built my own application on top of the Lahman database that's out there and ran all sorts of fun data from there in my spare time
&gt; Question: Can I put ArtistId as a foreign key in the tracks table? the answer is yes, do it now one day, you'll want to store data about compilation albums, where the tracks are all by different artists when that happens, you'll be removing ArtistId from the albums table anyway
So, if I'm reading this right, cc_dd inner joins with dda and creates DT1. DT1 left outer joins with RecUser and creates DT2. HireDate right outer joins with DT2 and creates DT3. recTestuser right outer joins with DT3 and creates DT4?
Yes. It's also logically the same as: tblDetails AS cc_dd INNER JOIN tblAllocation AS dda ON cc_dd.RefNo = dda.RefNo LEFT OUTER JOIN tblUser AS RecUser ON dda.Alloc = RecUser.UserID LEFT OUTER JOIN HireDate ON HireDate.RefNo = cc_dd.RefNo LEFT OUTER JOIN tblUser AS recTestuser ON RecTestuser.UserID =cc_dd.Staff Just rewritten to use RIGHT OUTER JOIN, which can sometimes be useful for performance. LEFT OUTER JOIN and RIGHT OUTER JOIN are the same thing, just written in a different order.
im still getting an error. It says there is an error near create table anime. SQL ERROR[1064][42000]:(conn=10)
Try this: CREATE TABLE anime ( `name` CHAR(20) ,episodes INT ,airing VARCHAR(3) );
the same thing pops up 
wait, wait, you literally submitted this? use mysql create table anime ( ... drop a semicolon in front of `create` to terminate the `use` command 
That's a big assumption. If he goes down that route. Also linking songs to multiple artists is something to consider
never heard of dbeaver just run this as a query -- SHOW CREATE TABLE tablename 
I wanted to delete duplicate records in my database, but it's a little more in depth than just one field. Status|ID| :--|:--| P|123| X|123| P|456| If status = R, the record needs to be removed. If the status = X (this is a reversal), both the X record and the corresponding P (paid) record (both will have the same ID) need to be removed. Can someone please help out a newbie? Thank you! 
 DELETE table1 FROM table AS table1 LEFT OUTER JOIN table AS table2 ON table2.ID = table1.ID AND table2.Status = 'X' WHERE table1.Status &lt;&gt; 'P' OR ( table1.Status = 'P' AND table2.Status IS NULL ) you neglected to mention your platform this works in MySQL, your syntax may vary
Thank you! I am using Microsoft SQL Server Management Studio.
This is what I had so far, I just didn't know how to take into account the 'X' claims with the corresponding 'P' claim. create table XYZ_paid as select * from XYZ where CLAIMSTS = 'P' and (not sure where to go from here)
You need an AbstractArtist, which is either an individual or a group of individuals. To do so you need to understand Table Inheritance. So, read up on that then come back
For regular querying the difference is not major. The structure of the query is mostly the same, for any keyword differences you can just google the equivalent in TSQL and it will pop right up. I had to do this a few times, mostly with date/time conversions (but the from TSQL to Oracle).
give my sql a try, it should work
`create table` ??? i thought you wanted to delete
The union is just adding one blank line to the output per record in docReport. The reason for the differing methods is probably to match the datatypes of the fields in the first part of the union: if the values of the second union statement can't be converted to match the data types of the first statement it will error. Why you've got dates that apparently want empty strings (''), and integers (0) is a bit of a mystery though but looking at the design of tbldoc will probably explain this.
I'm assuming you're correct? If somehow junk data got put into the table it *might* error out, and therefore tell you that there is a problem? Kind of a weird way to go about it? Perhaps there are other sprocs/views that would break if this little snippet were to produce an error because they join on conditions such as, cast(field as date)?
okay, let's take that in steps first, delete rows in original table why do you need a new one? 
After thinking about it, I really don't think I actually need the raw original data. This should work. That makes more sense to me. Thank you so much. I really appreciate it.
 I made a bad example because I didn't want to copy the actual query; in the actual query there are no dates as integers or strings. I just wanted to have 3 examples to illustrate my question on what '', 0, and null mean after the union all. So is this an exercise in data validation against the data before the union, i.e. validating elements with '' are strings, 0 are integers, and null are null? There actually are results in the output that aren't null for the corresponding element that should be null (if my understanding on how this works is accurate, which it probably isn't.) Also, what's telling you it's adding a blank line for each record in docReport? 
Not sure what you mean - the select clause after the union isn't actually returning anything from the table, so it doesn't matter what data is in docReport. 
What would be the point of this query then?
To add blank line(s) to the recordset I guess. 
&gt; I need to keep the original raw claim file. that would've been helpful to know at the outset SELECT table1.* FROM table AS table1 LEFT OUTER JOIN table AS table2 ON table2.ID = table1.ID AND table2.Status = 'X' WHERE table1.Status = 'P' AND table2.Status IS NULL 
&gt; so this is very simplified; over-simplified, and you've blown up the whole basis for asking a legitimate question perhaps you might want to rephrase what you're after here &gt; Also, what's telling you it's adding a blank line for each record in docReport? a very deep understanding of how sql works
Possibly as a bad workaround to add a blank row to a report or export.
&gt; a very deep understanding of how sql works If I may offer a much worse but slightly more illuminating answer for the OP: When you SELECT '', 0, null FROM table since you're selecting from the table you still get as many rows as there are in the table, even though you're not actually displaying anything from within the table. But yeah this has probably been simplified so much it's missed whatever the original point is. 
 SELECT Item , SUM(CASE WHEN SaleDate BETWEEN @52Start AND @52End THEN 1 ELSE 0 END) AS [52 Weeks] , SUM(CASE WHEN SaleDate BETWEEN @52YAGStart AND @52YAGEnd THEN 1 ELSE 0 END) AS [52 Weeks YAG] -- etc FROM Sales GROUP BY Item
Not bad at all. I began with T-SQL and Noe administer/develop both platforms daily and having to switch back/forth comes naturally. It’s like Spanish/Portuguese. The syntax is a bit different. I have to remember which one I’m in when I’m writing things like CREATE TABLE X AS, UPDATE FROM with Joins, SYSDATE/ GETDATE, etc. Common functions are sometimes unavailable in once vs the other. TRUNC (SYSDATE,’DD’) for example is done by casting GETDATE() AS DATE (to remove time) and then casting BACK to DATETIME to add midnight (if you want to display time). TRUNC MM, WI, YY, etc has to be done by combining DATEADD and DATEDIFF but once you have those established you’ll be okay. I have samples of type conversions and string/date Manipulation I’ve gathered over the years if you want to PM me, I’ll send them over to you. Docs.microsoft.com (BooksOnline) has pretty much everything you’ll want. It’s basically docs.oracle.com for T-SQL. BrentOzar.com has a ton of great resources for T-SQL, and (as I’m sure you know) DBA.StackExchange.com is a great resource. All the best!
CREATE TABLE AS with a query will not work in MSSQL. You’ll need to use INTO [NewTableName] right before your FROM to create a table using a select statement (google: “t-SQL SELECT INTO” for more info). You can also use DELETE INTO to delete records from one table into another table.
Wrapping the charges and queries up in a sum did the trick. Thank you!
Use something like SYSDATE-364... etc. 
Build a date table and leave it in your database, permanently. Use it to do all of your reporting. 
In my area, Oracle is definitely more uncommon than T-SQL. 
Which database are you working in?
Now I don't have a database in front of me, but you probably don't need either of those casts. GETDATE() already returns a DATETIME.
So, there are a few things here GETDATE() should always be DATE or DATETIME, so your first clause could just be WHERE Einddatum &gt; GETDATE() If for some reason, GETDATE() is a VARCHAR, first, see your DBA and yell at him. Secondly, you should only need to CAST it once WHERE Einddatum &gt; CAST(GETDATE() AS DATE) That DATE could also be DATETIME, depending on the format of Einddatum
I think the double cast was introduced to compare DATETIME field Einddatum with another DATETIME field. The double cast ensures the expression to be rounded off neatly to the beginning of the day. Hence, the query will return all records with an Einddatum later than latest midnight. 
--Today select getdate() --12 weeks ago select dateadd(ww,-12,getdate()) --Today last year select dateadd(yy,-1,getdate()) --12 weeks ago last year select dateadd(yy,-1,dateadd(ww,-12,getdate())) --Beginning of this year select datefromparts (datepart(yy,getdate()),1,1) --Beginning of the year last year select datefromparts ( datepart(yy, dateadd(yy,-1,getdate())) ,1,1) 
But what it does do is returns midnight of “today” and not the current date and time 
You forgot one possibility that doesn't involve yelling at the DBA- MS SQL, for instance, doesn't have a way to get the current date without the time as well. If you want to compare "before today", not "before right now", you have to cast GETDATE() to the DATE type (or some other, similar, trickery). Otherwise you may be including things you did not want. The second cast back to DATETIME is probably unnecessary, but based on the giant chart of default casts SQL Server makes I've started to be more specific myself in these situations. Default casting can confuse people easily enough.
Can you explain how to utilize this? I've seen it mentioned before, but I'm not sure how it works. 
Well, this gets a little difficult because that code shouldn't work if it is run on a Sunday and relies on some logical shortcuts that don't always work..... However, for most cases the important thing to understand is that when doing date functions SQL will convert an integer to a date, with 0 being "1900-1-1" (a Monday) and 6 being "1900-1-7" a Sunday". This is trying to be smart/ run into a bug I think because SQL sees Sunday a 1 and Saturday as 7, so it is trying to do a shortcut to basically give you the "Monday of last week, and the first Sunday of THIS week". Sorry if this is getting more confusing, but we will start with this code: DATEDIFF(wk, 6, GETDATE()) = DATEDIFF(wk, "1900-1-7", GETDATE()). "1900-1-7" is the first day of essentially the 2nd week. So if there 6160 weeks between now and the "first week" (1899-12-31 to 1900-1-6, Sunday to Saturday) then this will return one less, 6159. So with 6159 it is doing two things, it is adding that in weeks to the first Monday of the first week (1900-1-1) which gives you the Monday of last week. And adding it to the 2nd Sunday (1900-1-7) to give you the Sunday of this week. Again this code breaks if you actually run it on Sunday due to logic issues. 
*Yes, the Monday and Sunday as per your edit. There may be some issues with running it on a Sunday as you alluded and per the link, but it works fine on other days when is needs to.* I get it thanks! **Because '1900-01-01' is a Monday in SQL**, the 6th day, '1900-01-07' is the next Sunday, so the DATEDIFF is giving you one less than the number of weeks that you are going to add back to 0 (1900-01-01) and 6 (1900-01-07), so you get last week's dates. Changing the "6" in DATEDIFF to -1 get's me the current week's, and using 13 gets me two weeks ago's. Cheers, thanks. 
This is so helpful! Thank you. What would I use if I wanted to use a specific date as the date rather than today? We do our reporting on week ending sundays, so if I wanted to pull a report on Monday, what could I use to set the date to the sunday before?
briefly. Seems a bit in depth but might be our best option 
Thanks! I'll give this a watch later.
I know within SSIS you can open the advanced editor, change the data type to Unicode Text stream (DT_NTEXT) and it'll import all the text... you could try move one of rows with a long text character to the first row of data... it usually samples the data to determine the datatype to use and defaults to 255 characters, so if it sees a long row it might pull it in properly (not sure how the import/export wizard works regarding this). Otherwise just save the Excel document as an ASCII tab delimited file or CSV or something else and it should import without issue.
I figured out declaring a variable, but I can't get the YTD to work. This is the line that is erroring out: SUM(CASE WHEN sd.saledate &gt;= select datefromparts (datepart(yy,getdate()),1,1) AND sd.saledate &lt;= @MyDate THEN CaseSales END) as 'YTD' I have @MyDate set for 12/31/17
Not Oracle-savvy, but maybe something along the lines of: Select substr(col,position (‘.’ In col)+1,length(col)-position (‘.’ In col)) From table; You will need to look up Oracle’s alts for position and scalar length. 
You would probably want to do your exec(query) into a temp table, cte, or table variable of some sort then select from that with your partition.
I get nervous when tables are referenced multiple times and not aliased. I have no idea how the database engine is expected to guess which `TERM_CODE_KEY` from which `AS_STUDENT_ENROLLMENT_SUMMARY` to use on which side of which condition. I'd start with cleaning that up.
Create an IDENTITY column and increment by 1: CREATE TABLE TABLE_A( COL_A INT IDENTITY(1, 1) PRIMARY KEY NOT NULL, COL_B VARCHAR(10) ...
Please be aware that if you delete all records from the table, when you insert a new record it starts where the last one left off. If you delete an individual record and add a new one, so like if you have record number 3 then you're like "oh wait that's all wrong" and you delete record #3 then add a new record, the new identity for that new record will be 4. So your data will go 1,2,4,5,etc To manually add an identity number, you have to use SET IDENTITY_INSERT OFF. Be sure to set identity insert back on when you're done. To blast all data from the table, and set your key back to 1 for all new records, use the TRUNCATE command
You don't need to use &gt;=, use between instead. You don't need to use select to call the datefromparts function when you are already in a select statement. Don't but your field name in '' use []. Select sum(case when sd.saledate between datefromparts (datepart(yy,getdate()),1,1) and @MyDate then CaseSales else 0 end) as [ytd]
So I'm not the only one who adds NOT NULL to their primary keys.
Look into your 'select styp_code' subquery. How is it different from your other ones?
You can enable only encrypted communication to an instance or you can use keys to encrypt data in the DB. What kind of penetration vector are you trying to secure - network sniffing?
Can you even specify NULL for a pk?
No, but you can a unique clustered index.
This guy subqueries.
Hi I'm not an expert so hopefully someone more skilled will help out too. Do you have to keep it in this structure? The best performance gain IMO would be to at the very least set it up as a table: CREATE TABLE [reddit].[example]( [Id] [int] NOT NULL, [parameter] [varchar](50) NOT NULL, [value] [varchar](200) NULL, ) That way you can index it a lot more efficiently. If you can go one step further - you say some of the "paths" are defined for every id. Then you should probably do it more like: CREATE TABLE [reddit].[example]( [Id] [int] NOT NULL, [mandatoryColumn1] [varchar](200) NOT NULL, [mandatoryColumn2] [varchar](200) NOT NULL, [mandatoryColumn2] [varchar](200) NOT NULL, PRIMARY KEY (Id) ) And use it in combination with the other table I mentioned above for the non-mandatory columns. Then with indexing on the mandatory columns you will have stellar performance when querying with filters on this mandatory data. In case you have to adhere to a structure similar to what you have now: probably use a format like xml, json or similar instead for the "map" column and then use a DBMS that has integrated support for that datatype. SQL server 2016 and up have support for xml and json, but it's still not very fast compared to the solution I offered above.
Sure, it's got another nested subquery in it. I'm pretty new here and just out of college. I was told by my senior that I can nest as many subqueries within each other as I need to, is that wrong?
this worked. thank you. 
This worked perfectly, thank you. 
I've used `SPARSE` columns with an xml `COLUMN SET` in MSSQL that might be useful for this. Sparse columns are designed for optional / rare columns, and are very efficient at storing NULLs. Column Sets are a way to read or write just the populated sparse columns without having to reference all hundred. Filtered indexes on the individual columns might be nice too. CREATE TABLE dbo.Map ( ID int NOT NULL IDENTITY(1,1) CONSTRAINT PK_Map PRIMARY KEY CLUSTERED, Map xml COLUMN_SET FOR ALL_SPARSE_COLUMNS, Age int SPARSE NULL, BirthDate date SPARSE NULL, FavoriteToy varchar(10) SPARSE NULL, HairColor varchar(10) SPARSE NULL, HasNiceVoice bit SPARSE NULL ); CREATE INDEX IX_Map_Age ON dbo.Map (Age) WHERE Age IS NOT NULL; CREATE INDEX IX_Map_BirthDate ON dbo.Map (BirthDate) WHERE BirthDate IS NOT NULL; CREATE INDEX IX_Map_FavoriteToy ON dbo.Map (FavoriteToy) WHERE FavoriteToy IS NOT NULL; CREATE INDEX IX_Map_HairColor ON dbo.Map (HairColor) WHERE HairColor IS NOT NULL; CREATE INDEX IX_Map_HasNiceVoice ON dbo.Map (HasNiceVoice) WHERE HasNiceVoice IS NOT NULL; TRUNCATE TABLE dbo.Map; INSERT INTO dbo.Map (Map) VALUES ('&lt;Age&gt;12&lt;/Age&gt;&lt;BirthDate&gt;2017-01-01&lt;/BirthDate&gt;&lt;FavoriteToy&gt;dog&lt;/FavoriteToy&gt;') , ('&lt;Age&gt;32&lt;/Age&gt;&lt;BirthDate&gt;2015-01-01&lt;/BirthDate&gt;&lt;HairColor&gt;black&lt;/HairColor&gt;') , ('&lt;Age&gt;42&lt;/Age&gt;&lt;BirthDate&gt;2012-01-01&lt;/BirthDate&gt;&lt;HasNiceVoice&gt;false&lt;/HasNiceVoice&gt;'); SELECT ID, Map FROM dbo.Map; SELECT ID, Age, BirthDate, FavoriteToy, HairColor, HasNiceVoice FROM dbo.Map;
Do you run a blog by chance?
I do not.
Well I'd read it if you did. 
r/Sql is my blog for now.
Very interesting! We actually built a dwh for similar data. On mobile now, I'll try to get back as soon as I'm able. 
no prob, good luck
Not sure exactly what you're saying, but if it simplifies things just dump your queries out into #tables and then run your update statement on that, e.g.: select blahid, sum(case when blah = 1 then blah else 0 end) as blah into #table from table group by blahid update x set x.blah = y.blah from newtable x inner join #table y on y.blahid = x.blahid
Well, he/she is not wrong - you can nest them quite deep. As a side note - usually that's not the preferred practice since this sometimes leads to optimizer inefficiently reading same table multiple times. Anywho, to shortcut the whole 'find x differences' line of questioning here, in other subqueries you select an aggregate value (min or max), guaranteeing that at most a single row is returned. Stepping back to the general principles here, SQL cares very much about data types and overall metadata. You can think about a subquery as of a function of a kind that returns you a "value" of a result set. If a result set has at the most one row and exactly one column it is a 'scalar' the data type of that column and it can be used pretty much anywhere an expression (not just a literal) of the same data type is allowed. Every value in the list of selected columns/expressions is such a place - that's why you are able to use subqueries there, but each of these subqueries must return a 'scalar' result set. 
You could used postgres, you could dump the MAP part into a json type: https://www.postgresql.org/docs/9.4/static/datatype-json.html
I would fo it this way too.
see if you ca read this entire goddamn sentence out loud without chuckling, would ya? &gt; It allows to manage the relationship between the data element and entities normalization help to process to data records in the simplest format and allow for manipulation is to covered the data records into simplest formal and reduces the residency form the database records are easy updates, manipulation etc database allow us to manage the normalization with different method.
and there is not problem in having 7000 columns?
[It should work.](https://stackoverflow.com/questions/922707/how-do-you-create-a-wide-table-in-sql-server-2008-and-what-are-its-limitations) &gt; Bytes per row : 8,060 Columns per wide table : 30,000 [^^Maximum ^^Capacity ^^Specifications ^^for ^^SQL ^^Server](https://docs.microsoft.com/en-us/sql/sql-server/maximum-capacity-specifications-for-sql-server) [Sparse Columns](https://docs.microsoft.com/en-us/sql/relational-databases/tables/use-sparse-columns) [Column Sets](https://docs.microsoft.com/en-us/sql/relational-databases/tables/use-column-sets) The advantages of using this approach are: * Data is stored in it's native format, not bulky strings (storage savings) * Columns are described by meta-data, not key value pairs (storage savings) * Columns can be indexed * Data can still be managed using convenient xml structures rather than specifying 7000 columns
It’s all nonsense, but maybe English isn’t OPs first language. 
Yeah I've been working with json in SQL server for the past few days. In a naive implementation it is very slow even though it's built into SQL Server now. I can see that u/jc4hokies has some suggestions for making it work so try those out. Either way - if you want to test in another RDMS that postgres (though I love postgres too) you can just spin up a quick Azure DB on a trial account. The trial account has around $200 to spend free first and it supports all the features used in u/jc4hokies solution as well as mine.
How do you use an original example for 1NF and 2NF and then just totally throw away that example by posting a picture of 3NF in an unrelated example? How is that useful, whatsoever?
Does this give you what you need? SELECT name , is_policy_checked , is_expiration_checked , LOGINPROPERTY(name, 'IsMustChange') AS is_must_change , LOGINPROPERTY(name, 'IsLocked') AS [Account Locked] , LOGINPROPERTY(name, 'LockoutTime') AS LockoutTime , LOGINPROPERTY(name, 'PasswordLastSetTime') AS PasswordLastSetTime , LOGINPROPERTY(name, 'IsExpired') AS IsExpired , LOGINPROPERTY(name, 'IsLocked') AS IsLocked, LOGINPROPERTY(name, 'BadPasswordCount') AS BadPasswordCount , LOGINPROPERTY(name, 'BadPasswordTime') AS BadPasswordTime , LOGINPROPERTY(name, 'HistoryLength') AS HistoryLength , LOGINPROPERTY(name, 'DaysUntilExpiration') AS ExpirationDays, modify_date FROM sys.sql_logins -- ORDER BY Name 
Sorry for the late response. I have an application that needs to connect to a database and the password is currently stored in plaintext on a literal txt file. 
Bit of a vague one drink your initial description, but if you set a column in your target table to be an identity column you can pick up the latest id after the initial insert to link to your updates. So, you'd need to declare a variable at the top of your query to use, something like this: Declare @ID int Insert into table (F1,F2,F3) Select query with first case statements Set @ID = (select max(id) from table) Update table Set F4 = statement, F5 = statement Where ID = @ID This would be best handled in a stored procedure as if you're wanting to process more than one row you can implement loops. Instead of 14 queries doing separate statements can you not join them to create one query performing one insert?? If they're related there should be a field you can join on, no? Would be easier to answer with a look at the existing queries 
Just as an addition to this, if someone was to delete all records rather than truncate, to restart the identity count you can use DBCC CHECKIDENT('schema.table', reseed,0) 
If the OP is still unsure, this is the answer. Anything to do with dates or date periods I use my calendar table. I have a stored procedure that takes a start and end date as parameters and loops through from start to finish calculating different date groupings. I then have update procedures that run each day and update columns like last 4 weeks, last 8 weeks, current month, previous month for this year and previous years. Then having a look at what the last 4 weeks figures look like compared to what the last 4 weeks looked like at this point in previous years is as simple as adding Where cal.last4weeks = 1 To my query and grouping on year 
&gt; The insert works on its own, but I dont want a new record, I want to add the data to an existing record. You want to use an UPDATE statement then. It changes the values in the columns and rows based on your criteria. UPDATE TABLE SET ColumnA = 'Cat' WHERE ColumnC = 'AnimalNoises' Or if it's new data completely. ALTER TABLE ADD PandaAdventures Varchar(255) UPDATE TABLE SET PandaAdventures = 'FuzzyTown' WHERE ColumnQ = 'RandomRhinocerous'
If you go this route you can use [expression indexes](https://www.postgresql.org/docs/9.6/static/indexes-expressional.html) on arbitrary functions to get the performance you need for the specific queries you need.
It's better than hardcoded into the application. Still, what's your particular worry? Is the password file stored in a secure enough location/instance? Is it shared with anyone other than sysadmins? Is it in violation of some of your security standards?
Multiple users are going to use the application, each with their own credentials. But I think it just sounds wrong. Storing plaintext passwords is never a good thing. I wish some kind of those darkweb sites that require PGP decryption. Thought it was really cool the way they did it.
Yep!
On mssql, try ## instead of #. 
Sorry for not being too explicit but as I mentioned I'm new to this and don't have the appropriate knowledge yet to be able to explain. The sample data that I provided is being just a small piece from the table that I have (around 25 columns) but I am only interested in highlighting the columns where a change has appeared that is why I provided the example with the address because if I run the query the results will be something like: ColumnName|NewValue|OldValue :--:--:-- Address|Address 2 - changed|Address 2 In order to achieve this result I had to manually insert the Sequence_ID in the query where I put "declare @ID1 int = 2". I Am looking for a way around this, mainly to not insert this ID manually and be able to check all the Sequence_ID automatically and display if any changes appear in the columns. If you have any spare time please provide a sample of you suggestion. Thank you! 
speed things up? try a UNION SELECT database1.rn , database2.carrier , database2.dn FROM database1 INNER JOIN database2 ON database2.dn = database1.dn AND database1.rn &lt;&gt; CONCAT(database2.carrier, database2.dn) UNION ALL SELECT database1.rn , database2.carrier , database2.dn FROM database1 LEFT OUTER JOIN database2 ON database2.dn = database1.dn WHERE LEFT(database1.rn,1) = 'd' AND database2.dn IS NULL 
okay, that really helped. I've never heard of UNION before, you learn something new every day. the query is now down to 4.6 sec, which is kinda amzing. Thank you so much for your help!
Does Postgres materialize CTEs like Oracle does? Or is it just syntactic sugar like in SQL Server? It can make a big difference in terms of performance.
Yes, it does materialize them. [The documentation](https://www.postgresql.org/docs/current/static/queries-with.html) says: &gt; A useful property of WITH queries is that they are evaluated only once per execution of the parent query, even if they are referred to more than once by the parent query or sibling WITH queries. Thus, expensive calculations that are needed in multiple places can be placed within a WITH query to avoid redundant work. Another possible application is to prevent unwanted multiple evaluations of functions with side-effects. However, the other side of this coin is that the optimizer is less able to push restrictions from the parent query down into a WITH query than an ordinary subquery. The WITH query will generally be evaluated as written, without suppression of rows that the parent query might discard afterwards. (But, as mentioned above, evaluation might stop early if the reference(s) to the query demand only a limited number of rows.)" You're right that that can have a big impact on the performance. But, I didn't want to talk about performance in my post at all, because it's a topic for another article. But maybe I should've mention this one thing. 
&gt; You're right that this can have a big impact on the performance. I didn't want to talk about performance in my post at all, because it's a topic for another article. But maybe I should've mention this one thing. My point really was from the SQL Server side of things and understanding the potential pitfalls (or benefits) of using a CTE. CTEs aren't materialized on SQL Server, and using one in place of a subquery won't make any difference for performance. Once I learned that, I developed a somewhat unhealthy affinity for temp tables :) With Oracle &amp; Postgres, you apparently *can* gain performance with a CTE (but not always).
&gt; Thus, expensive calculations that are needed in multiple places can be placed within a WITH query to avoid redundant work. Another possible application is to prevent unwanted multiple evaluations of functions with side-effects. Figured I'd comment with my own finding of where I've found CTEs particularly useful: Anyone who has had to work with the consumption or generation of various EDI formats (X12, HL7, EDIFACT, etc.) can appreciate the complexities of mapping all of these various formats to fit nicely in a relational model. Instead of building out an entire subsection of a database for our main communication-hub application, we decided it'd be much simpler to separate these messages by their various formatted fields, with each field getting written into a single row of the database - linked together by a common EDI Message Sequence number and the position it was found in the file. The consumption and generation of these EDI formats now results in some pretty fun pivot queries from this table, which we wrap up into numerous CTEs in our various EDI format packages. This allows us to keep track of the file prior to it being fully generated or fully consumed by way of each CTE being a line in the file/segment/etc. It makes the final file generation or consumption sooo much easier since we'd be iterating over each one of these CTEs data multiple times and significantly reduces the complexity of these queries as a whole. Not sure if I've explained that well. 
A ##table is a global temp table, which means it can be queries in other windows/instances of SSMS, whereas an #table can only be queried in the window you've created it in. Both are reusable and will remain "real" until dropped, or you disconnect from the server. A @table is also a temp table but it only exists as long as a query is running and then drops immediately afterwards. 
Here's a demo on sqlfiddle that should do what you want: http://sqlfiddle.com/#!18/17493/1 For your purposes, you can add as many "old" and "new" columns to the history table as you need, and then add an additional UNION ALL query in the CTE to check for updates in those particular columns. Hopefully this is enough to get you going in the right direction. Good luck!
What flavor of sql are you using? one option in ms sql is to use a cache table to load the new data and then use a stores procedure to [merge](https://docs.microsoft.com/en-us/sql/t-sql/statements/merge-transact-sql) the new data with the current.
Ideally SQLite... but I could stand up a Mysql or Postgres. It's a side project, so ideally I'd like to stick to open source for the engine.
Postgres 9.5+ has something similar: `INSERT ... ON CONFLICT UPDATE (and ON CONFLICT DO NOTHING`
I tend to use ##tables for debugging, where I will open a process and do a CTRL+F, then replace # with ## so I can pop into another window to start playing with code before migrating it into the main process and changing everything back to #tables. I have been consistently told that we cannot and should not use ##tables for sprocs, or any other type of production type code. I tend to agree with this but was wondering if you had any thoughts to add on that? Also any process that has an # must have a corresponding IF not null then drop #table piece of code so that the entire process can be run sequentially without error.
&gt; I have been consistently told that we cannot and should not use ##tables for sprocs, or any other type of production type code. I tend to agree with this but was wondering if you had any thoughts to add on that? There is a time and place for every tool. I don't know where or what it would be in this circumstance, but I think it should definitely be used assuming the solution calls for it.
Oh, I agree. If I had a legitimate use for it I would push for it, but I can't imagine a solution that would need a ##table over a #table unless it's two independently executed sprocs -- but then why wouldn't you just have the data in a real table if it was being used that way.
What version of SQL are you using? I'm guessing SQL Server. Here is an [example on stack overflow](https://stackoverflow.com/questions/19055902/unpivot-with-column-name) of an unpivot to move columns to rows which should accomplish what you are trying to do.
I actually find really interesting what you said, because I've got quite the opposite point of view ;) To me having materialized CTEs is a drawback because it leads to the optimization fence. In the cases that I worked with I'd rather let the database do the optimization magic on its own, than force it to use the precomputed results. Having said that I think that this behavior should just be configurable. Then everyone would be happy ;) 
&gt; To me having materialized CTEs is a drawback because it leads to the optimization fence I don't think I was making an argument for or against materialized/not materialized but I think I see what you're getting at - if the CTEs were never materialized, you could make your own decision about whether or not to do it yourself (by putting the data into a real temp table or not). But with PG/Oracle, you don't have that choice.
I was more thinking about giving user an option to materialize or not a particular CTE. For example by adding an optional keyword "MATERIALIZED" to CTE definition.
&gt; Preferably help with 4, 5, 6! Key topics to know: **For 4** * [SUM](https://www.postgresql.org/docs/10/static/functions-aggregate.html) * [OUTER JOIN](https://w3resource.com/PostgreSQL/postgresql-left-join.php) **For 5** * [CASE](https://www.postgresql.org/docs/current/static/functions-conditional.html) * [AVG](https://www.postgresql.org/docs/current/static/functions-aggregate.html) * SUM (See Above) **For 6** * [COUNT](https://www.postgresql.org/docs/current/static/functions-aggregate.html) * SUM (See Above) * [MAX](https://www.postgresql.org/docs/current/static/functions-aggregate.html)
Is the second statement supposed to have the update at the end? When I run into problems like this i simplify the statement so that it works and then add the other elements in to see which one is making it fail.
I may be wrong, but in the second statement, it doesn't look like there's any parameter specific to the name of the course that was just added? Like you just have the registrants and course tables joining, and then a mass-update of all the seats regardless of course name. I'm not super-familiar with MySQL so sorry if not it. 
Huh. You might be right. I never thought about that. I'll try and find out how to reduce the seats of only the particular course.
I think you may need a semi colon at the end of the first statement to separate it from the second. It may also matter how you are running the query ex. [multi_query](http://php.net/manual/en/mysqli.quickstart.multiple-statement.php)
Looks like SQL 2014 is your highest version possible https://www.microsoft.com/en-us/download/details.aspx?id=42299 Or, if you're just doing this for experimentation sign up for an Azure Free Trial and run an Azure SQL instance. https://azure.microsoft.com/en-us/services/sql-database/ 
Maybe 2014?
The error message speaks for itself I reckon.
2016 definitely does not work on Windows 7. I had to get my workstation upgraded from 7 to 10 (yes, I know Windows 8 will run it but that was the upgrade the company was doing) to run it.
If it's just your desktop for dev work, it's not a big deal. Never as a server though; it's bad enough that they 8-ified the start menu/screen on Windows Server.
Agreed. server 2016 is a bit annoying after working with 03/09/12/14 for so many years.
2014
Either check/change the [default date format](https://support.microsoft.com/en-us/help/173907/inf-how-to-set-the-day-month-year-date-format-in-sql-server) in your instance of SQL, or use yyyy-mm-dd instead.
ISO 8601 is the one true date format. 
It's not an answer to your question, but your code is vulnerable to SQL injection or just an error if someone has an unusual name. Think about what would happen if you tried to insert someone named `Patrick O'Reilly` or, more sinisterly, `Patrick '; DROP TABLE tbregistrants;--'`. I'm not familiar with php, but you should use prepared statements with parameters.
Agreed. Only one I use for inserts and for file storage (backup BAK and TRN files)
That should be done as part of a single transaction *or* as a trigger on the registration table. There's no reason to use that `SELECT` statement there if you're not using the output. Or, don't directly track the number of seats available. `tbRegistrants` has a FK relationship to `tbcourse`, and `tbcourse` to a table holding your rooms (which includes capacity). Then you'll compare the count of registrants to the room capacity. IOW, this would be part of a larger "add registrant" stored procedure that does all this logic. Also: * Please stop using SQL89-style joins (the "comma join") and use the proper `INNER JOIN` syntax when joining (SQL92). * Stop using Hungarian Notation for your table names. * As /u/dXIgbW9t &amp; I both pointed out below, your code is wide open to SQL injection attach. Use PDO &amp; Prepared Statements.
What’s ISO 8601
https://en.wikipedia.org/wiki/ISO_8601
these 2 videos explain it very wel with simple examples. Rollup: https://www.youtube.com/watch?v=HLTdfCtfIJs&amp;t= Grouping Sets: https://www.youtube.com/watch?v=Tc6-X6mNv7k
ROLLUP lets you add a Total row to the bottom of your query result set. The benefit is that it saves you keystrokes because you do not need to do a UNION ALL. It improves performance because you do not need to query the same data twice. GROUPING SETS does the same thing as rollup, but allows you to do Totals and Subtotals on any combination of grouped-by columns. CUBE does the same thing as grouping sets, but will return all possible combinations of grouped-by column subtotal and totals. Cube: https://www.youtube.com/watch?v=9d7fjBgtxos
Is it correct to say your tables look kind of like this: PERSON ID NAME POSTAL_CODE_ID fk POSTAL_CODE_REF POSTAL_CODE_ID pk POSTAL_CODE If so, then whatever process inserts new data will have be modified to, instead of directly adding new records, check first if a postal code already exists. Depending on what your process looks like to insert data, this could be as simple as: IF NOT EXISTS ( SELECT * FROM POSTAL_CODES WHERE POSTAL_CODE = @POSTAL_CODE ) BEGIN --insert data Obviously, the particulars depend on your database and end-to-end process, but that's the gist of it. 
Thank you for your answer! My current tables look like this: USERS USER_ID NAME ETC POSTALCODE POSTALCODE_ID POSTALCODE USERS_POSTALCODE USERS_ID POSTALCODE_ID But do you think your way is more efficient? Because then I'll change my database tables. 
My way isn't necessarily more efficient, it was just a guess what your current schema looked like. You can abstract it further if you have other address information: PERSON PERSON_ID pk NAME ADDRESS ADDRESS_ID pk PERSON_ID fk ADDRESS_LINE_1 ... POSTAL_CODE_ID fk POSTAL_CODE POSTAL_CODE_ID pk ... Regardless, the gist is still the same. Once you start normalizing your database, you're going to have to be more judicious when inserting and updating data.
Try downloading a data set. Microsoft has adventureworks and world wide importers, you can also download stack overflow. Then you can use cross apply to generate more records. Now you can write a complex analysis query. You can write out your steps to make it sub second. I'd recommend to Google Solarwinds performance tuning SQL 12 step pdf and using that as a rough Skeleton instruction sheet. 
It was a joke about a nonexistent 3DNF.
ah... okay
Normalization through BCNF is determined by functional dependencies. You show none of them. From which textbook are you supposed to be learning normalization?
A guide to SQL 9th Edition.
It's a bit confusing. Where I had patient ID it should reflect a patient for that day on which service was received. It should be something like AppointmentID and not actually duplicating patient id numbers.
Read the chapter on database design fundamentals again. That's chapter 2 in the 8th edition.
Sorry, bad joke.
Normalization is just a big fancy word that is, at it's root, very simple when it comes to businesses, and you are (in my non-expert opinion) already seeing the truth.. which is that you can easily over think something and go too far. I mean a simple way to look at this is that data takes up space, and you wouldn't want to include every piece of data in every single table, when you can easily achieve your desired results by having linked tables. Where and when to do this is important. Sometimes you don't want to have the best design possible because the application you are designing has other goals where "normalization" is not as important as front end use. It seems like you're getting the basics, so just keep at it. You need to ask yourself the questions that other people are asking you, because those are the questions your professor should/will be asking you. There is no magic bullet here. Normalization is a concept, not a formula. As the architect you decide where and when to implement it as a concept, but be prepared to defend your decision and be open to those ideas being changed/challenged. To me normalization is more of a process than anything. Sometimes you want to avoid it because of a legitimate reason (ease of use, reporting purposes, etc.) -- and that is OK. Just my $0.02
Thanks for the input. Normalization is a brand new term for me and I understand the concept, mostly, and some of the method. I think I'm working in the right direction. At least if my assignment is incorrect I'll be able to see where I messed up instead of just guessing. 
Tickets
It's like the word, *schema*. First time I encountered it I was confused as hell. It can be used in so many general ways... but after awhile I just got comfortable with it being used as a way to describe organization. Do you name all your tables `dbo.tables` or something more descriptive like `params.zipcodes`? What is the schema of table A as it compares to table B (column structure)? Etc. They're more nebulous terms that imply broader concepts that *should* be implemented in order to make everyone's lives easier. But there is no magic bullet of formula for them... what works for you in one environment may not work in another. It all largely depends on what you're trying to achieve with your data. A simple example here is when you're building some kind of table for a reporting technology to consume. Do you want to normalize that data and have a bunch of ID's that are linked together, then create a view and point the technology towards the view? Maybe, but if it's a large dataset and a complex query then your performance is going to suffer as it comes to how those technologies consume your data. If you avoid normalization and end up taking up an extra 1GB of storage... what does that cost, and does the cost out weigh the gains you get? Another example would be if you were trying to create a datasource in order to run complex calculations against/modeling/etc. Again, could use you simply generate a view using a complex query, and then join views together, etc.? Sure... but instead of a model taking 4 hours to run it might take 12. In the same breath you could prepare permanent tables, run your model/save the results to a table, and then drop your tables. So these concepts (to me, ask others for their opinions) simply refer to organizational techniques and best practices for **long term data housing.** So once you normalize your data and take great pains to break it out into separate tables you might find yourself breaking normalizing and dumping things into tables. The difference being is that your nice pretty normalized data might have 10 years worth of records whereas your non-normalized tables only have a rolling 12 months. So again, normalization/schema to me imply long term organization that considers growth of your data. Take an extreme example and say you wanted to create a database that will only house (1) row of data which is (100) columns wide. Would you want to normalize it? What would be the benefit? In this example normalization would take up more data storage than simply having a single table with (1) row. On the other hand if you expect that (1) row to grow into a billion rows... you can see how much space would be saved instead of having `Los Angeles, California` repeated tens of millions of times in text format versus a simply ID of `1`. Same thing for schema... why have 5 custom schemas for 5 tables if you never plan to have more than 5 tables in your database? On the other hand if you expect to eventually have a few hundred tables it might make a lot of organizational sense to have a prefix other than `dbo.` &gt;At least if my assignment is incorrect I'll be able to see where I messed up instead of just guessing. If you have a good professor then that should be the goal of the assignment, and your grade should more be based on your intellectual interaction with the concept than what you produce... assuming you do produce something and it isn't total shit.
&gt; is it all done through email https://media.giphy.com/media/xU9TT471DTGJq/giphy.gif 
Chat first; if it's a clarification of behavior or possible issue, we try to diagnose on-the-fly. If it looks too big or we don't have an answer within 3-5 minutes, we ask the counterparty to open a ticket and we amend it. For everything else, we ask them to open a ticket directly. Because we have multiple interlinked projects, we have a special project set up in the bugtracker for incoming requests. For bigger tasks we first communicate with the requestor directly for scoping and clarifications, and it will end up as a ticket + documentation about expected APIs.
JIRA or Confluence tickets
We have a SharePoint site where they submit requests. After that its email, IM, OR Phone calls
Fuck SharePoint
Spiceworks. They have a free version
&gt; full outer join Never heard of that before, but yeah, that's pretty much what I want, thanks. 
&gt; and still in 3rd normal form if you simply store the postal codes in the Person table and completely do away with a PostalCodes table and that horrible PostalCodeID idea Er, not really. The table might be more efficient to query, but postal code is not an attribute of person unless it's the only address-related column that they store.
Preach!
first name is not an attribute of person unless it's the only name-related column that they store
You should probably be more specific.
Just a SQL book a long the lines of SQL Cookbook.
Teach yourself SQL in 10 minutes is pretty handy. Very basics for beginners, but goes well beyond into other aspects.
Thanks! That one of the books I was considering. Didn't really know if it was too basic. SQL Cookbook is another on the list, though, I noticed SQL Cookbook is kind of old.
“SQL Queries for Mere Mortals” By John L. Viescas and Michael J. Hernandez “Learning SQL” By Alan Beaulieu “SQL Cookbook” By Anthony Molinaro Just thought this one would be hilarious to throw in. https://www.amazon.com/Manga-Guide-Databases-Mana-Takahashi/dp/1593271905
[SQL pocket guide](https://www.amazon.com/SQL-Pocket-Guide-Usage/dp/1449394094) is a really good one
ROLLUP and CUBE are just special cases of grouping sets. So if you understand grouping sets rollup and cube should be easy to get.
I loathe and hate ticketing systems. If you want to implement one, then make it incumbent on the person who will be doing the work (or someone above them) to fill the ticket out. Do not make non-expert users do it. Half the time if something breaks, or it's a minor enhancement, I can get the work done in less time than it takes to fill out a ticket / discuss it with my team on our weekly call. We have a project tracker for large projects, but I hate individual tickets, and I hate filling them out for other groups because they are constantly being sent back for being incorrectly filled out, or someone needs more detail, etc.
If you figured out the solution you should add an edit with the steps that it took to get to the answer, that way if someone comes across the same problem they have an understanding of what you did. It's one of those teachable moments that takes only a few minutes to share! 
I'm not exactly sure what you're looking for but I can think of a good real world example that you might be able to make work. Let's talk about marketing attribution. Everyone gets all worked up trying to figure out where a sale came from in order to see which marketing campaigns are working, and which are not. Many companies stick with the first contact, many companies go with the last contact... and then good companies try to weight and value each interaction in order to look at the life cycle of a customers interaction with their multiple marketing campaigns to see exactly how much it cost out of pocket to get the sale. Simple example here would be you get a phone call on a number assigned to a radio marketing campaign from Bob Smith, and then Bob Smith engages in 5 paid search ads before finally being sold from an email remarketing campaign. So the email was free, and you might be tempted to give the credit to radio... but each click in paid search cost you money. At a minimum you would want to be able to remove this cost from your paid search "bucket" in order to get a better understanding of how it functions. This becomes rather sticky, and in a lot of companies you will find a datasource where "leads" are stored, but you will quickly find out there are duplicate leads. I.e., a person named Bob Smith, B Smith, Robert Smith, and Bobby Smith will have a unique lead for each interaction with the company, and often times which one gets the sale attached to it is arbitrary and up to the salesperson. Lets imagine a table that looks like this: | LeadID | FirstName | LastName | Address | City | State | ZipCode | HomePhone | CellPhone | SecPhone | OpportunityID | AccountID | SalesID | WebID | SourceCode | | 1x92 | Bob | Smith | 1234 Happy Lane | NY | NY | 12345 | 555-555-5555 | (555) 555-1234 | 5552816721 ext 21 | 6bahwtyz | 92jbazu | NULL | 196baqoituy | 67 | | 1x93 | Robert | Smith | 2356 Parents House | NY | NY | 12347 | 555-555-1234 | NULL | NULL | NULL | NULL | 6720gjazh | NULL | 93 | | 1x94 | B | Smith | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | 6710gajt | 45 | Now, you can see that this is the same person, with one sale, who has had multiple touch points across multiple sources, and we want to analyze this, so we start out with a framework like this: select dbo.phone(HomePhone) AS 'PersonID', LeadID from table union select LastName + cast(dbo.Zip(ZipCode) as varchar), LeadID from table union select webID, LeadID from table union ... So we start by preprocessing the data, in the case of phone number we need some kind of function that will clean up and take the first 10 digits and put them into a standard such as `XXXXXXXXXX` or `1XXXXXXXXXX` or `1 (XXX) XXX-XXXX`. You might concatenate Zip &amp; Last name and assume if two people in the same zip code have the same last name that they are the same person. You can get creative. Anywho you get a nice long list this way and you insert it into a #table or dbo.table and assign a primary key. Now you can do something like this: select Pkey, stuff from keytable a inner join table b on a.personid = dbo.phone(b.HomePhone) or a.personid = b.webid or a.personid = b.salesid ... I'm being very general here but you can see how you start to stitch things together, and soon you will be able to look at every sale, and then count the number of leadID's across each individual source. You could look to see how many times on average (or median) a person creates more than one lead before a sale is made, what the average time it takes from first contact until a sale is made, etc. -- All valuable insights. So you should probably be able to pick this framework up and push it over the goal line, but the problem you're going to run into when you try this on a big table (millions) is that joining with OR's is a horrible idea. At this point you have demonstrated preprocessing. You need to get all the values into a standard or you can't expect them to join. Now time to optimize: select Pkey, stuff into #table from keytable a inner join table b on a.personid = dbo.phone(b.HomePhone) inner join table c on a.personid = b.webid inner join table d on a.personid = b.salesid ... select stuff from #table where col1 is not null union select stuff from #table where col2 is not null union select stuff from #table where col3 is not null ... If necessary I have the full code for when I built this on a live system in case you get stuck but it shouldn't be too hard to weave this together and then show how making a simple tweak like this will massively optimize your query and give you an identical data set as using the OR's, which is a more naturally intuitive choice. 
If that works for you then it sounds good. I work in analytics, so most people with an understanding of analytics work in our department and it causes a nightmare.
I believe, add another where with NOT EXISTS(). In parenthesis basically turn your update into a SELECT. This will ignore rows where the update is not needed. 
I brought this to an interview and got laughed at. When I got hired they gave me the big blue Microsoft training kit books. 
This is not coming out easy to read... https://drive.google.com/file/d/1O_kuvaRujo_srexEhB-Bbxa7u--KM2bq/view?usp=sharing
you forgot the platform... always identify your platform here's a beautiful solution that works in MySQL -- INSERT INTO user ( ... ) VALUES ( ... ) ON DUPLICATE KEY UPDATE ... 
I have never worked with this before, it was simply handed to me to figure out so I do not even know what version of SQL we use. Thank you so much for your help. I will test this out right away.
So I did another test and got this. https://drive.google.com/file/d/1ZRF6poAoP3TQOnGCgxjJlQtBRbsew2Lf/view?usp=sharing https://drive.google.com/file/d/1JTupmSYsbMyzm8ooNhlf6s-_0izBNfD7/view?usp=sharing
Have a google.of database engines, there are many, all with their own flavours. You're gonna need to know which one you're using. Afraid I've never heard of DBsys, so can't offer you much advice. 
DBISAM? if it barfed on the word ON then you're not running MySQL in any case, you got the ON DUPLICATE KEY syntax wrong, there's no table name 
yes, but you don't mention the table in the ON DUPLICATE KEY clause, because it already knows which table you're inserting into, because it's in the INSERT part of the statement but like i said, you don't seem to be running MySQL, so all this is moot
I wish I knew how to find what I'm using. It does not connect to any server, we use it to edit data on a program. I have .dat and .idx files if that means anything.
Been away from SQL and Banner for a little while, but this might be something along what you're looking for (CTEs with some correlated subqueries): WITH RETURNED_STUDE AS ( SELECT a.pidm_key AS "CURR_PIDM" ,a.term_code_key AS "CURR_TERM" ,a.stype_code AS "CURR_STYPE" FROM baninst1.as_student_enrollment_summary a WHERE a.term_code_key = '201730' AND a.styp_code = 'R' ) MOST_RECENT AS ( SELECT b.pidm_key AS "RECENT_PIDM" ,b.term_code_key AS "RECENT_TERM" FROM returned_stude a INNER JOIN baninst1.as_student_enrollment_summary b ON (a.pidm_key = b.pidm_key) WHERE b.term_code_key = (SELECT MAX(term_code_key) FROM baninst1.as_student_enrollment_summary c WHERE b.pidm_key = c.pidm_key AND c.term_code_key &lt; 201730) AND b.enrolled_ind = 'Y' AND b.registered_ind = 'Y' ) FIRST_SEMESTER AS ( SELECT d.pidm_key AS "FIRST_SEM_PIDM" ,d.term_code_key AS "FIRST_SEM_TERM" ,d.stype_code AS "FIRST_SEM_STYPE" FROM returned_stude a INNER JOIN baninst1.as_student_enrollment_summary b ON (a.pidm_key = d.pidm_key) WHERE b.term_code_key = (SELECT MIN(term_code_key) FROM baninst1.as_student_enrollment_summary e WHERE d.pidm_key = e.pidm_key AND e.term_code_key &lt; 201730) AND b.enrolled_ind = 'Y' AND b.registered_ind = 'Y' ) SELECT a.curr_pidm ,a.curr_term ,a.curr_stype ,b.recent_term ,d.first_sem_term ,d.first_sem_stype ,d.first_sem_term FROM returned_stude a INNER JOIN most_recent b ON (a.pidm_key = b.pidm_key) INNER JOIN first_semester d ON (b.pidm_key = d.pidm_key); 
some quick googling suggests that .dat and .idx might be **dBase IV** files 
Can you provide a sample of what your desired output looks like for the above?
that will be possible only if `b.tractFIPS` is first declared UNIQUE, assuming you've already declared `b.id` as PRIMARY KEY
I've not used Postgres, so I'm trying based on syntax I'm finding online, but the command will be something along the lines of &gt; alter table a add constraint FK_somethingthatmakessense foreign key(tractce) references b(tractfips) More info on [Stack Overflow](https://stackoverflow.com/questions/35676149/adding-foreign-key-column-to-postgresql-table) and [PostgreSQL docs](https://www.postgresql.org/docs/8.2/static/sql-altertable.html)
[Invalid Characters in XML](https://stackoverflow.com/questions/730133/invalid-characters-in-xml) You'll probably want to replace all "&amp;" characters with "&amp;amp" instead. How you do this is up to you. Either in SSIS or before the file is accessed by SSIS with a PowerShell script, maybe.
[This should help](http://www.postgresqltutorial.com/postgresql-foreign-key/)
Select a.accountid, b.firstname, max(whatever the date column is called) From table a Inner join table b On a.accountid = b.accountid Group by a.accountid, b.firstname 
If I'm interpreting what you want correctly, I think this should get you there, though not in a dynamic way. You would need to know the max # of items in a single order and hard code that many columns. This assumes 3: SELECT Order , MAX(a.CustName) , MAX(b.Date) , MAX(case when b.rank = 1 then DocType else Null end) as DocType1 , MAX(case when b.rank = 2 then DocType else Null end) as DocType2 , MAX(case when b.rank = 3 then DocType else Null end) as DocType3 , MAX(case when b.rank = 1 then DocAmt else Null end) as DocType1 , MAX(case when b.rank = 2 then DocAmt else Null end) as DocType2 , MAX(case when b.rank = 3 then DocAmt else Null end) as DocType3 , MAX(case when b.rank = 1 then ItemNumber else Null end) as ItemNumber1 , MAX(case when b.rank = 2 then ItemNumber else Null end) as ItemNumber2 , MAX(case when b.rank = 3 then ItemNumber else Null end) as ItemNumber3 , MAX(case when b.rank = 1 then ItemDesc else Null end) as ItemDesc1 , MAX(case when b.rank = 2 then ItemDesc else Null end) as ItemDesc2 , MAX(case when b.rank = 3 then ItemDesc else Null end) as ItemDesc3 FROM Table_A a JOIN (SELECT Order , DocType , DocAmt , ItemNumber , ItemDesc , ROW_NUMBER() OVER (PARTITION BY Order ORDER BY DocAmt DESC) AS rank FROM table b GROUP BY 1,2,3,4,5 ) b on a.Order = b.Order GROUP BY 1 
ROW_NUMBER is the window function you want. 1) Isolate the text without parentheses first. 2) Use that in a ROW_NUMBER() function 3) wrap in outer SELECT statement using the result of the ROW_NUMBER() column in the where clause 4) ??? 5) Profit https://docs.microsoft.com/en-us/sql/t-sql/functions/row-number-transact-sql
I don't know what platform you're on, so I can give specific syntax. A general approach would be to use regular expressions to keep the non-parentheses-ed text and select distinct from the resulting column. Rough outline: select distinct stripped_text from ( select regex_that_gets_rid_of_stuff_in_parentheses as stripped_text from ... ) 
Thanks man! I'll look into it!
I'll edit. Its PostgreSQL
Welcome! Added Oracle alternatives if you’re in that RDBMS. Left me know if you get stuck and I’ll give more hints. Just wanted to let you figure it out on your own for that eureka moment :)
But why? I don't understand the value of this information. Regardless, no need to use a CTE, just use the regular pivot function with a join and a subquery. 
update tableename set userID=, password=, fullname=, pass=, userskin= where userID=X
Do you mean the order of records in the #temp table doesn't follow the row_number, or the row_number values are different than expected? If the former, retrieval order isn't guaranteed unless an order by is specified. If the second, it shouldn't be possible, and I'd need more details to try an figure out what's happening.
Download a dataset you are interested in. Install a database on your computer. Put the dataset in a table. Start asking the data questions. Here are some places to find inspirational datasets: https://www.kaggle.com/datasets https://aws.amazon.com/datasets https://cloud.google.com/bigquery/public-data 
Don't save passwords. Instead save the salt and salted hash of a password. When the user types in their password, you salt it (with the saved salt), hash it, and compare the resulting hash with the saved hash. Like this: Suppose my user sets their password to "password". I generate an arbitrary salt "f+-%0\^/". I then hash the string "f+-%0\^/password" and get "c6aef109875242849d11943edc6bec0b9c067133" (using SHA1; I think there are better options). I save "f+-%0\^/" and "c6aef109875242849d11943edc6bec0b9c067133" with their user record. Next time they log in, they type "Password". I retrieve their salt "f+-%0\^/" and hash "f+-%0\^/Password" getting "43fd9c3e00e4f5ac43b7b446adf62185ac8cd359". This doesn't match their saved hash, so their login fails. They try again using "password". This time I hash "f+-%0\^/password" and get "c6aef109875242849d11943edc6bec0b9c067133" matching their saved hash so they can log in.
Thanks a lot for your advice!!
This is lovely. The project topic is vague and the professor has given us enough room to select a topic, be it research or non-research
Ahhh, thanks. I think I have figured it out, I was just wayyy too tired earlier 
Good point. I'll edit it now.
Don't forget that WebID, AccountID, etc., can all join to other tables where you can pick up even more potential ContactID's to weave things together. Picking up married couples, relatives, etc. in the sales process. What I think would be a good strategy for you is this: 1. Decide the full scope of your table structure. The SQL analysis is already largely written but you'll need to generate or find data sets that actually let you run your queries. Which aren't finished in my examples, so you have that work to do, too. 2. Come up with like 5-10 actual cases and create those rows of data by hand. Now the nice thing about this is that you'll be able to check your queries because you'll know what the right answers should be for whatever you decide you want to, "show." 3. Come up with some way to populate your tables with some junk data so that you can show the performance differences, look at the other poster's advice about tuning, etc. I don't really know what depth you need to go into here but you can use that example of multiple OR's converted into multiple LEFT JOINS and then UNIONED as the basis of your, "optimization piece," and you'll need a function to normalize phone numbers, or zip-codes which will be the basis of your, "pre-processing piece." So you just need to get creative and build the data. Open Excel and use the table schema from the example and just start making 5-6 rows per test case (5-10 test cases) and weaving it together so your queries will pick things up. You can probably find dbo.phone() functions, or dbo.zip() functions or use a LEFT(). It doesn't have to be perfect, just consistent and explained. You got this.
Annoyingly, I can't seem to find the final draft. But [this](https://pastebin.com/tGTEj0eQ) was the very first draft of the "finished" process in terms of dataset, before a lot of pre-processing and optimizing that you can see in the other examples. Cheers.
&gt;What's the query for it? Seems to be pretty basic stuff but i've spend a good few hours already... And yet you show none of that work so we can tell you if you are in the right direction or not so I'm going to go with my gut here and call bullshit.
Well, by work I mean googling. Did post to StackOverflow too, no luck. Maybe have something to do with my subpar english, idk. Here's an image: https://imgur.com/a/vSbGR I need SW_Data to suggest me only those values with corresponding SW_Class, ID_sub and ID_Counter. Maybe has something to do with JOIN? looking into it now
Again you're giving basically 0 information. What's the database in? Mysql, oracle, mssql, postgre? access? databases themselves will not have drop downs so what's the layer on top? web page? phone app? windows form? toaster oven? Anyway the other reason I call bullshit is regardless of all of that this looks like 100% standard where clause which is SQL 101
Well, I guess I'm just stupid. I don't understand how to tie two cells together, SW_Data and SW_Class. How do I use WHERE here? How to point out SW_Class to corresponding SW_Data? To me it looks like I have to point the value to itself, WHERE SW.SW_Class = SW_Class (the value I selected) I did looked up WHERE clause, but all examples showcase either a string or a number, which doesn't work in my case. It's not a homework
While the NAV-db is mssql you very rarely interact with it through sql. Instead you use Navs own language: C/AL. It’s pascal based. But understanding relational databases is important. Temp tables are your friend when it comes to NAV. 
I think you're misunderstanding the nature of SQL. The feature of 'Cascading dropdowns' where sub-entities are displayed depending on an already selected value is something that is usually seen in an application that reads a database, rather than the database itself. You provided a screenshot but haven't mentioned the software you are using in that screenshot. You also haven't provided a sample of the total data you have and the expected result. There's a whole host of really helpful people here, but you need to help them to help you.
When you say "you have to escape it as (no space) &amp; amp", can you explain that a bit more? Sorry, i'm still learning this stuff, after the SQL guy left early last year. It's a miracle we've gotten this far :P
"Oh, thinks looks interesting! I should play around with it and see if it works." **Deletes all data in Prod**
Thanks, I am a beginner. So, it will take some time for me to understand this process. Appreciate your help!!!
What if there are triggers on the tables that act on delete operations? I almost think looping a truncate table command instead of delete without a where clause would work better with less transaction logging...
You can use a script task in SSIS to read your input file as raw text and replace the invalid characters with the correct ones. If you don't know .NET this is a great opportunity to start, as the task is straightforward and there's a lot of sample code out there on reading files and replacing content. 
What do you store in the cookie and/or database that the client can't spoof? A session guid?
You encrypt the cookie with a secret key, otherwise, yes, they could poison or spoof the cookie. I use a random alpha-numeric string generator from Sublime Text. Probably beyond the scope of OP's project but you can set the key as an environment variable and then call it with an os.env call (depending on your language) so it's not exposed in your source code.
Cheers for that, i'll take a loot at some .NET tutorials and see what i can do.
How do you want to decide which one to keep? For example why did you keep "text one (asd)" and not "text one (qwe)"?
To me, I find the engineer level does all the lifting and building. Kind of like the people who make legos, we make all those legos and put them in a bin. Then the BI developers take those legos and do things with them. That's somewhat how I feel doing engineering work, I also find more often than not engineering jobs want you to know of or have minor experience with big data and machine learning and an emphasis on some sort of programming or scripting language besides SQL. Bi developers may have something to that degree, but it's for analyzing, not building. There's a lot of grey area and this is all subjective. I'm a "data engineer" and it feels exactly like my last job as a "data generalist" which feels exactly like "DBA Technician" and that felt exactly like "Database Application Admin". At this point, I tell people just give me "data" work to do and I'll go do it. 
If you can alter a table to turn off constraint checking, presumably you can truncate it as well. Truncate would be a million times faster.
Always the first one.
Hey Im a BI developer, I do everything from back end to front end. It really depends on the company, im a consultant and we don’t have engineers so thats our model. Most large companies will have engineers to lift and shift the data and bi developers to build reports for it. I think it is important to be good at both phases.
One engineers data the other has sex with 1s and 0s
One goes both ways.
Just thought of something to help you. So once this is all done and you have a 'results' table that you union together... you can then query the Lead table, take the leadID to look up the BazookaID, and then using that see how many other leads it matched to. Then you can use this to pull up *all* of those leads... you could segment this to only look at the leads with sales attached and count how many leads per sale, etc. If you know the cost of the lead you can start going further. Really this is all going to come down to how big of a project this is, and how important the class is to you. If it's small you probably don't need to show multiple tables and put together a conceptual presentation / business case (which would be your report). Do some Googling for "marketing attribution" and get some bullshit "industry stats" to make your argument more compelling, etc.
Do you work with big data as an Data Engineer?
Hi Mr. Darcy, Thank you for the reply and all your help. We discrovered that there were duplicates and that they need to be addressed. got to love data clean up. Again thank you, jamkgrif
[UNPIVOT](https://technet.microsoft.com/en-us/library/ms177410(v=sql.105).aspx)
Sounds like you're looking for UNPIVOT
What is big data *for you*? 
If you're working with SQL Server, get yourself to [SQL Saturday NYC](http://www.sqlsaturday.com/716/eventhome.aspx) on May 19th in Times Square. Best part: it's free!
I believe General Assembly has a NYC location. They offer a course on data analytics that covers Excel, SQL and Tableau.
Could be a constraint issue? If a constraint is violated, it's logged in merge replication, but it doesn't stop it like it would if you were running transactional replication.
I don't have a ton of experience with replication, but it may be related to bulk logged operations. Replication is typically based on transaction logs, and inserts can be bulk logged and updates never are.
Hahaha funny you say that. First guy I dealt with was this guy.
varchar(max)
Are the data engineers parsing text from API response files in JSON/XML/etc. using object oriented programming? Meaning, they might pull data from web based data application with this API.
SQL Saturday is a global event. Minnesota was October 7 last year so it'll probably be around the same time this year. Keep your eyes open for Wisconsin (Wausau was September last year, I think there's one or two others), Chicago is coming up in mid-March, and there's probably one in Iowa as well.
I would recommend using the ID field within all tables to reference back to the user table. If you set the ID field up with auto_increment, it will increment by 1 for each new entry (new users in your case) 
Ok that us a lot more rules. By first one I assume you mean lowest ID? First one u s dangerous termonology in RDBMs, just because you visualize it like that doesn't mean it is actually stored like that.
I don't even go for NAV positions any more. Every interview is some drawn out process where the current "NAV EXPERT" spends half the time talking about their experience and credentials. Like, who is actually applying here? Once you get a bit of CAL down and a few SQL concepts you're pretty much on a fast track to a solid career. NAV devs aren't easy to come by. Places like Nigel Frank will be messaging you out of the blue constantly. Good luck and don't let it go to your head! :)
yeah, you are right. But lets assume its the first one by id. 
That site literally has the answer on it. It is a trick question because the exists piece references "id" which doesn't exist on the docs table, so it is comparing envelope.id to envelope.id.
i don't think it's a trick question. i think it's just a badly written question 
I don’t do much with Hadoop or spark. Most of the work is in Redshift. So maybe in terms of quantity, but not in terms of what’s considered “big data” technology.
I don’t do much with Hadoop or spark. Most of the work is in Redshift. So maybe in terms of quantity, but not in terms of what’s considered “big data” technology.
This was a helpful thread. Thanks OP for asking and those who answered
Can you use DARTPART in Your where clause to exclude Saturday’s and Sunday’s?
Piggybacking off this, you can you dw in your datepart for weekdays only
&gt; Thanks, what would the logic be for it? I am a visual guy so it helps for me to see it. &gt; &gt; DECLARE @StartDate DateTime, &gt; &gt; @EndDate DateTime &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; set @StartDate = DATEADD(DAY, DATEDIFF(DAY, 0, GETDATE()), 1); &gt; &gt; &gt; &gt; set @EndDate = DATEADD(DAY, DATEDIFF(DAY, 0, GETDATE()), 4); 
thank you, you beautiful son of a bitch 
Hold on, doesn't seem to work. I need to play with it
yup I just tried it. So you saw what I declared and then what I set. So I would do this as start date between @StartDate and @EndDate so it is getting the date today but looking 4 days in advance. Not sure what the logic would be in the where clause. If I would do start date &gt; @endDate to get it to look at Thursday on Monday. 
I'm sure there's a better way than this, but off the top of my head it could be done with a case statement: CASE WHEN datename(dw,@startdate) = 'Monday' then DATEADD(day, 4, @startdate) WHEN datename(dw,@startdate) = 'Tuesday' then DATEADD(day, 6, @startdate) WHEN datename(dw,@startdate) = 'Wednesday' then DATEADD(day, 6, @startdate) WHEN datename(dw,@startdate) = 'Thursday' then DATEADD(day, 6, @startdate) WHEN datename(dw,@startdate) = 'Friday' then DATEADD(day, 6, @startdate) END 
Datename(weekday, getdate() ) should help with getting the specific day of the week and then you can use datediff and dateadd to skip the four days ahead piece. 
how would I look in my where clause to pull the next 4 days? so servicedatetime between @start and @end would give me everything in between. Sorry for the questions, I am struggling tonight. 
What about something like this DECLARE @startDate DATE = GETDATE(); -- Today DECLARE @daysToAdd INT = 4; -- Days to add DECLARE @endDate DATE = DATEADD(DAY, @daysToAdd, @startDate); -- Calculate new date SELECT (CASE WHEN DATEPART(WEEKDAY, @endDate) = 6 THEN -- Is new date Saturday, DATEADD(DAY, 2, @endDate) -- then add 2 days WHEN DATEPART(WEEKDAY, @endDate) = 7 THEN -- Is new date Sunday, DATEADD(DAY, 1, @endDate) -- then add 1 day ELSE @endDate -- Just return unmodified new date END ) ;
The code above should do it. It sets your @end based on the day of week that is the @start. So if @start is a Monday, the CASE statement will add 4 days. If the @start is Tuesday thru Friday, the statement will add 6 days. Your WHERE clause will BETWEEN @start (current date) and @end (current date + 4 or 6 days) as you have it already.
/r/PowerBi is a great resource, as is the PowerBI website and community forums. The PowerBI YouTube channel is amazing (mainly Guy in a Cube) and they have a bunch of free resources to use. As for sql Server, it really depends if you want to go with DirectQuery or Import data retrieval. If you are using DirectQuery, I’d request some hands on training for you and whoever else is working in the realm. If it’s Import, you can just use CSV, excel, or simple table imports to get your data. I am leading a venture into PowerBI at my organization (using a Data Mart we also built) and so far it’s been a ton of fun. We love the tool and are finding it more useful every day. Microsoft seems to be supporting it very intently (updates come once a week). Get yourself some SQL server (T-SQL) training first. The PowerBI tool is pretty easy to learn with some YouTube videos.
 DECLARE @startDate DATETIME = GETDATE() DECLARE @endDate DATETIME = CASE WHEN DATENAME(dw,@ startDate) = 'Monday' THEN DATEADD(day, 4, @ startDate) WHEN DATENAME(dw,@ startDate) IN ('Tuesday','Wednesday','Thursday','Friday') THEN DATEADD(day, 6, @ startDate) END SELECT * FROM mytable WHERE servicedatetime BETWEEN @startDate AND @endDate
Following. Would like the answer to this too! 
Yes, the results are immediate.
The best reference book you can buy right now is [Introduction to SQL](https://books.google.com/books?id=T29ZQAAACAAJ) by van der Lans. Don't be fooled by the name though, its more of a reference than a tutorial book. Writing is a bit dry but the content is solid.
Awesome! thanks :)
Yes.
Yeah I know it has the answer but I tried doing it on and the query wouldn't go through. I have to agree with StoneCypher.
Regarding your edit: That's intentional, and it's the source of the index miss. I need to get the lowest value of `pk_column1` (and the corresponding row of data) for each value of `pk_column2`.
SUM(CASE WHEN date = ‘2018-01-01’ THEN quantity ELSE NULL END) And SUM(CASE WHEN date = ‘2018-01-02’ THEN quantity ELSE NULL END) Sum the quantities when the condition is met.
you can put a function in a function .. like square(square(2)) 
I don't know if I understood your question correctly, but that looks like an XML tag and MySQL has support for extracting attributes from tags. select ExtractValue(xml_request, '//claim/@PresentinIlnessCode') from @tablename; should give you the value of the PresentingIlnessCode attribute from the claim tag. XML f support in MySQL is described [here](https://dev.mysql.com/doc/refman/5.7/en/xml-functions.html)
Ummm... you just need the current date? [Try this](http://lmgtfy.com/?q=sql+get+current+date)
thanks brother. 
getdate()
This couldn't be anything other than a copy-pasted homework assignment, and from somebody too lazy to spend 5 seconds to google it. I'm all for helping people that need help, but as the Posting rules state &gt;If you are a student or just looking for help on your code please do not just post your questions and expect the community to do all the work for you. We will gladly help where we can as long as you post the work you have already done or show that you have attempted to figure it out on your own. Look at what he posted. Tell me where, other than in a homework assignment, we'd have comment blocks, case statements, null checks and string entries combined with DateTimes, but OP is asking how to get the current date? And not only the simplicity of the question (and finding the answer) in the context of a *mildly* advanced select, but then literally has "NEED todays date" in parenthesis? This is either a joke or an overwhelmingly lazy homework post.
Totally agree. This is something that anyone should be able to google in under a minute.
Is is possible but if I update any column, even one that doesn't have a constraint, then the row that I updated will replicate. I am also not getting a error about constraint violations.
You can use a CASE statement in the SELECT clause. SELECT CASE WHEN Credit = 'open' THEN something WHEN Credit = 'closed' THEN something_else END [column_name]
Would that be efficient? 
that's not SQL
what platform? also, what are the two queries?
sql server. 
okay, then you get to use T-SQL, a superset of SQL
I'm not sure, but I've never had problems with them slowing down my queries. I deleted the code since I realized I wasn't sure if that's what you were looking for.
You don't want to specify the same table more than once if you don't need to. In this case I think you are selecting a number of records based on the filter, then picking a single record based on the order of `vernum` and `defvern`. You can identify the correctly ordered record with the `ROW_NUMBER` ranking function. SELECT * FROM (SELECT * , ROW_NUMBER() OVER (ORDER BY vernum DESC, defvern DESC) AS RowNumber FROM MyTable INNER JOIN AnotherTable ON MyTable.id = AnotherTable.dataid WHERE MyTable.defid = 123456 AND MyTable.attrid = 10) AS a WHERE a.RowNumber = 1;
There are several ways this can be done. Personally I would have 2 stored procs, one with each query. Then a third parent proc that calls the appropriate proc. Another way would be using dynamic sql statements in one stored proc.
if there's an index on the pair of columns { `itemId, effectiveDate` } then it's as efficient as you can make it
[Yeah, that will work in SQL.](https://docs.microsoft.com/en-us/sql/t-sql/language-elements/if-else-transact-sql)
I'm partial to the aggregate method. SELECT i.id , i.name , p.price FROM item AS i INNER JOIN (SELECT itemId , CAST(SUBSTRING(MAX(CONCAT(DATE_FORMAT(effectiveDate,'%Y%m%d%H%i%s%f'),CAST(price AS CHAR(20)))) FROM 21) AS DECIMAL(11,2)) AS price FROM price GROUP BY itemId) AS p ON i.id = p.itemId;
&gt;by the way, your join between I and SUBP is missing the join condition Thanks, that was an omission in the example, I usually do have a join condition. &gt;if there's an index on the pair of columns By that, do you mean one index on both columns or two separate indexes, one for each? And is there a difference? Thanks for the feedback.
what an ugly hack... well, at least it works any idea if it's sargable? i'm going to guess not
It's sargable and typically performs better than alternatives. It only accesses the grouped table once, has less restrictions to benefit from an index, but also doesn't need an index to do well. And it can do MAX and MIN at the same time in some scenarios.
You should be doing reporting off a data warehouse, not your operational database
This.
You want to have production data (in this case a clients) brought down into a reporting database (on another server) at some kind of interval which will depend on business requirements. For example if they have 10 years of data and data only in the last 3 months can ever change... you bring down 3 months of data every month, week, day, or minute depending on client needs. Then you let them hammer away at the reporting database until they're blue in the face while your prod server hums along.
It runs but it's failing at: &gt; AS a &gt;WHERE a.RowNumber = 1; With this error: ORA-00933: SQL command not properly ended 00933. 00000 - "SQL command not properly ended" *Cause: *Action: Error at Line: 6 Column: 42
Hi Riduclly, Thank you helping, but I am getting an error message 'ExtractValue' is not a recognized built-in function name. when I googled it, it's saying that I need to put in [dbo].[@tablename] format, I have tried that. Same message occurred. 
I don't see a typo, it doesn't seem to like the assignment `AS a`. 
How about a window function for each quiz type like ROW_NUMBER() OVER (PARTITION BY quiz_id ORDER BY Score desc) 
MySQL 5.6 doesn't have ROW_NUMBER() - i think i have to use a variable.
Are you missing a join condition between Profile and table_for_id, by chance? Could be a good reason to switch to the modern join syntax.
I was using load data infile to get data from a csv file in my table but after it puts in the data i want sql continues to try to put data from rows that have no data on them. I was wondering if I could set the table i made to only hold 43 rows
TIL
The problem is the file. Delete the empty rows from the file. Empty rows look like this: ,,,,,,,,,,
the files open in excel so how can i delete the empty files from there?
See my edit about the function. The function checks the security to see if the user has permission to view ssn. If not, it will display asterisks.. And I'm not really sure, I'm still learning somewhat. What Join would be, outer? So everything I would want to query on is on one table? 
alright thanks. My last question is how would i search for things in a column that use a certain word
 WHERE Column LIKE '%word%'
Without a join condition, you're returning every record in the first table for each record in the second. 30 rows in each suddenly turns into 900 records returned.
do i need the %?
% is a wild card. similar to "\*.\*"
I see so using this [https://docs.oracle.com/javadb/10.8.3.0/ref/rrefsqlj18922.html](https://docs.oracle.com/javadb/10.8.3.0/ref/rrefsqlj18922.html) im not sure what to use ON or USING with though. select securityscramblefunc(ssn, id) as ssn, lname, fname, mname, dept, hiredate, codedesc, code from Profile left outer join Table_for_id where hiredate &gt;= :start_date and hiredate &lt;= :end_date and code = :selectvar_code
You're missing another where clause that should be something like: and profile.id_key = table_for_id.id_key
SQL is the declarative language used to access data in a RDBM (Relational Database Management System). That being said, do you know which database system you want to use. There are probably sample data you can download for which ever system you want to use. For example, SQL Server has a database called AdventureWorks that you can download and practice with.
Load Data Infile only takes a Comma Separated Value File (.csv), so if you want to use an Excel table then you need to save the table as an .csv Here is an article on how to use Load Data Infile [Load Csv data into MySql](http://kedar.nitty-witty.com/blog/load-delimited-data-csv-excel-into-mysql-server) 
That doesn't look like a MySQL error. What database are you using?
there are no CTEs in MySQL (yet) 
It is a small project for a course. The professor hasn't defined her expectations and so we have the independence to act on our own in this project. After reading a few research papers and articles on this topic, I have realized that this is not going to be a trivial topic and can actually lead to a good paper if I am able to work on it properly. Also, I have only 1 year left in college. :). Thanks!!!
Microsoft published a lot of blog posts around SQL Server 2016 under the tagline "it just runs faster". /u/sqlrockstar compiled a list [on his blog](https://thomaslarock.com/2016/06/sql-server-2016-just-runs-faster) but I'm not sure if they cover backups. I thought I'd heard Bob Ward on a podcast talking about how they improved I/O (and other) performance but I can't find it now. He did talk about it in a [Group By session](https://www.youtube.com/watch?v=pTEDfmQnpzA) though.
Are you just trying to find the player with the Xth highest score? If the 'score' means the aggregate of all that player's scores in the table, make a table/subquery with player, sum(score). Then another with a windowed function that orders them based on score (eg player, rank() over (order by score) as rank). You can query that for rank=X.
I like between on this one SELECT EXTRACT(YEAR FROM period) AS year , AVG(data) AS yearly_unemp_rate FROM fred.unemployment_rate_stg WHERE period BETWEEN '1980-01-01' and '2015-01-01' GROUP BY year ORDER BY year;
right off the bact, your example is wrong because it includes '2015-01-01' what if `period` is a DATETIME? you can't use BETWEEN easily because then you'd have to use a string which includes the very last microsecond of 2014-12-31 clumsy **&gt;=** on the lower bound, **&lt;** on the upper is best practice
Thats why I asked about the viability :)
 I don't have to use JOIN if RANK() is used. But your suggestion led me to learn what windowed function is. Thank you.
 SELECT offer ,expirationdate ,count(discounturl) as "Code Count" ,COUNT(CASE WHEN USED = 'True' THEN 1 ELSE 0 END) as Used ,COUNT(CASE WHEN USED = 'False' THEN 1 ELSE 0 END) as Available FROM DiscountCodes GROUP BY offer, expirationdate ORDER BY expirationdate DESC 
I think you need to know more about the requirements. How often does the data need to be pulled down? How often and how far back does the data change? Etc.
Just curious, do you guys store SSN's in plain text and rely on the whoever to querying the database to use the securityscramble function?
 SELECT offer ,expirationdate ,count(discounturl) as "Code Count" ,(Select DISTINCT COUNT(Used) from DiscountCodes as D2 Where Used = 'True' and D1.Offer = D2.Offer GROUP BY offer, expirationdate) as Used ,(Select DISTINCT COUNT(Used) from DiscountCodes as D2 Where Used = 'False' and D1.Offer = D2.Offer GROUP BY offer, expirationdate) as Available FROM DiscountCodes as D1 GROUP BY offer, expirationdate ORDER BY expirationdate DESC This may be more in line with what you want, increase the sample diversity to confirm.
It's not the destination man, it's the journey. Just start. I started in 2012 with no knowledge at all and passed the 70-761 yesterday, although learning SQL has not always been my primary focus.
Since the TYPECHECK is something declared in your select statement, and not values stored in your db, you won't be able to directly count them. I would store them in a table variable and use that. DECLARE @typecheck table (id INT, typecheck varchar(12)) INSERT INTO @typecheck (id, typcheck) Select [some id to join timecardaudit to person, your CASE statements] FROM VP_TIMECARDAUDIT TA SELECT [column names...,] COUNT(DISTINCT typecheck) FROM [JOIN, WHERE, etc.] GROUP BY [whatever column names you have besides the count] 
Kind of confused by your question, but I think you want something like this SELECT P.HOMELABORLEVELNM3, P.HOMELABORLEVELNM4, COUNT(TYPECHECK) FROM yourtables WHERE whatever GROUP BY P.HOMELABORLEVELNM3, P.HOMELABORLEVELNM4