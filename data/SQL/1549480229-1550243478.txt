A lateral join is at least pretty much the same thing as an outer apply, isn't it? Natural joins are neither here nor there because IMHO they should never appear in prod code. But yes, there are big differences in provided functions.
Just ask for perms on lower environments - dev and test. We all break stuff during development sometimes, which is why we don't dev in production environments.
PowerBI Report server is included with a SQL Enterprise w/Software Assurance license. It can be used for both PowerBI dashboards and good old SSRS rdl reports. Basically looks like SSRS web front end with the addition of the dashboards.
Look into PowerBI Report Server. It's included with a SQL Server Enterprise w/Software Assurance license.
There is no "per user" fee for using PowerBI Report Server. You just need the aforementioned SQL Server license.
Yeah, but give someone who doesnt know Sql the ability to use it and they will cry. 
Can you post the code youre trying to run? 
Would you be willing to state.. a LATERAL JOIN is the same thing as an OUTER APPLY.. and not ask it? thats my only issue. doesnt matter whats not in production, welcome to the wild.. anything is possible. therefore, you should learn MSSQL using MSSQL items, PG using PG, My using My, and PL using PL.
select distinct TABLE1.ID ,Case When Table2.Event Like '%Activity1%' AND Table2.EventOccured = 'True' Then 'Yes' Else 'No' End as EventActivity1 ,Case When Table2.Event Like '%Activity2%' AND Table2.EventOccured = 'True' Then 'Yes' Else 'No' End as EventActivity2 ,Case When Table2.Event Like '%Activity3%' AND Table2.EventOccured = 'True' Then 'Yes' Else 'No' End as EventActivity3 &amp;#x200B; FROM TABLE AS TABLE1 JOIN TABLE2 as TABLE2 on TABLE1.ID = TABLE2.ID;
I thought you had to be a pro user to be able to view the dashboards or reports? 
No indexes on the PK? Like, it's a heap table? That's medium bad. Poor performance, but data integrity is maintained so things could be much worse. MERGE is fine if it's what's required. If you need to update changing attribute values, and you can't truncate and reload, then it is what's required. If you can truncate and reload, then do that (unless you have a huge dataset with relatively small deltas). The ideal, if you can get a delta (e.g. source.UPDATE\_DATETIME &gt; @LAST\_UPDATED) from the source system, is to merge that.
This is the exact approach I'm taking to the rewrite, which is currently taking about 6 hours. Hoping to get it down to about 30 minutes.
Not really, because I only know T-SQL. Not disagreeing one should learn the flavour they'll be working with - just nitpicking. Imagining inheriting a codebase full of natural joins now... fair point.
Without actually reviewing the code [https://sqlwithmanoj.com/tag/option-maxrecursion/](https://sqlwithmanoj.com/tag/option-maxrecursion/)
What about GOTOs... everywhere.... True story, witnessed the code.
On something as simple as your example? Almost certainly not. However, in the case of some monster query that joins many tables together, SQL can "time out" when creating the execution plan for the statement, meaning that it hits it's internal timeout before it was able to fully optimize the query. If that happens, it gives you the best plan it's come up with so far, which might be complete garbage. In such a case, I've seen that rearranging the table sequence can have a dramatic improvement in performance. If I had to guess: changing the sequence that the tables were joined brought a more optimal execution plan sooner into the costing process. It probably still timed out, but at least it got to a better plan before it did. Should you have queries like that in your DB? Of course not. We don't always get to choose the code we inherit though. 
Try this link if it will help you, it goes through how to install SQL Server locally and set up management studio to connect to it. http://www.straightforwardsql.com/database-playground/ 
Truncate deletes all data from the table. Do you want to delete all or just the data from the previous select? You can DELETE with a join or with a CTE or you have SQL 2008+ you can also use the MERGE statement to do both insert and delete. https://docs.microsoft.com/en-us/sql/t-sql/statements/merge-transact-sql?view=sql-server-2017 
Get SQL Server Management Studio installed to be able to manage the database. SQL Operation Studio is a newer product but personally I still prefer SSMS at the moment. That way you can fullymanage the DBS, query data, create maintenance plans, look at performance, etc.. Also FYI, you can get a full version of SQL Server for free if it's just for development/practice purposes. But SQL Express is fine to learn with too.
This is not SARGable and might prevent a relevant index from being used.
I can't think of an example when I would use that. Someone editing my query directly is actually the stuff of the nightmares. I would just go for a coffee and have my code riddled with NOLOCK hints, square brackets around everything, every predicate wrapped in extra parenthesis and useless comments everywhere.
I have never actually collaborated, but often we will ask each other for help, or in a previous team we had to peer review eqch others SQL code.
Thanks! These kind of fears are exactly what I'm looking to know about! 
Can I ask how you peer reviewed? Did you use a tool like Github, or was it more side-by-side review? 
How would one be able to get a free copy of SQL server for development purposes?
We just used had an in-house sharepoint system that controlled work flow. It was garbage but it got the job done.
[https://www.microsoft.com/en-gb/sql-server/sql-server-downloads](https://www.microsoft.com/en-gb/sql-server/sql-server-downloads) SQL Server 2017 Developer edition
I think formatting would be an issue, too. I think just about everyone has their own definition of the SQL standard for formatting. If anyone reformatted my scripts I don't think I'd be able to get anything done until I "corrected" it. Queue Benny Hill as we all spend all day undoing each other's changes in circles.
No, if you've got the server licence there's no licencing required for viewers.
The sticker price for a using Power BI service is $10/user/month, and if you're government you'll be paying less than that. 
I've used this paid solution before ($600/year), and it worked for what we needed at the time. I also created an interface and system to create tables for maintaining data types and lengths for columns on the dynamic tables, but separate from this add-on functionality. The way cozyroc used to work, it may have changed, is basically creating nvarchar(max) columns, iirc. https://www.cozyroc.com/
I always found SSIS packages and CSV to be a PITA. So i went and looked for alternatives and i found that Powershell can do the job very easily. I have script that can grab and load a csv into a SQL and create the table while it is at it. only problem is since CSV does not have datatypes, it all just goes into a nvarchar datatype column. &amp;#x200B; [https://blog.netnerds.net/2015/09/import-csvtosql-super-fast-csv-to-sql-server-import-powershell-module/](https://blog.netnerds.net/2015/09/import-csvtosql-super-fast-csv-to-sql-server-import-powershell-module/) [https://www.sqlteam.com/articles/fast-csv-import-in-powershell-to-sql-server](https://www.sqlteam.com/articles/fast-csv-import-in-powershell-to-sql-server) &amp;#x200B; &amp;#x200B; you can schedule these with a SQL job or if it doesn't bug on you create a script task in VS for SSIS.
&gt;KEEP DOWNLOADED COPIES OF THE UPDATES Also keep downloaded copies of the PBI desktop tool. The report server version lags behind main releases, and there was at least one instance where there wasn't backwards compatibility and there wasn't an official download source for the version of the desktop publisher compatible with the then-latest PBI RS. Report Server is clearly at the moment the red-headed step child, MS would much rather all of us used their cloud.
I can't imagine a need for a tool like this. I've been writing SQL code for 25 years and have never wondered if such a tool existed. If I've ever needed to have someone help me with a query, or someone needing my help, we simply exchanged the SQL code. I can't see how having multiple people working on the same query ends up well. Heck, even in software development, you generally don't want more than one person working on the same piece of code at the same time, if possible. Dealing with merge conflicts can be a royal pain. 
Distinct is all or nothing, you can’t distinct one column and not the rest of them. So there’s no point it having it anywhere but the front. 
I can't watch the video because I am at work.. but if you want to become a data scientist, you'll need more than just SQL. ^^inb4 ^^downvotes
Got it. My error was thinking I can use distinct on specific columns. Thank you!
At least you mentioned that you haven’t watched the video so people can ignore this comment lol
You would use a group by, then: select name, number from my_table group by name, number 
I was recently handed a ~12,000 line transformation script. Two days later, my boss asks where I'm at with it, and I have to explain to him that I'm still trying to re-format everything, as I absolutely cannot read it in its existing format.
What is that you are trying to get? In other words, if that would have been a valid syntax, would would be the expected results?
If you're just looking to build relationships between tables for reporting purposes, you can do that using PowerQuery within Excel. It's basically the same thing as an SQL Server analysis services instance, running within Excel. A crazy powerful tool, and one of Excel's best kept secrets.
I'm referring to the on-site version that you can install, not MS's hosted PowerBI solution.
Honestly, when it comes to collaboration, I tend to agree with the consensus here in that actual sharing code outside of a code review tends to be a nightmare for disaster. Most of the time it's more of a white-boarding session about concepts, rather than actual code. For actually QA/code review, we use bitbucket in my organization and it works quite well.
You make a valid point. I assume that the video mentions learning more than just SQL though. At the very least R, Python, etc.. with a rudimentary understanding of statistics or data modeling/structures. 
The video doesn't mention any of that. It just says you can go to a good university, do a boot camp, or teach yourself. I watched the video and I'm not really sure what exactly a data scientist is. 
Thanks! This is helpful framing and also good to know about the white-boarding! 
Thanks for the insights! When you did exchange the code to help someone, how did you send back your comments to them? 
Sometimes email, sometimes via conversation and sometimes via IM or text. 
Thanks, appreciate the context! 
Dont worry, SQL 2017 added R and Python functionality.
SELECT \* FROM OPENDATASOURCE('Microsoft.ACE.OLEDB.12.0' ,'Data Source=C:\\folder\\file.xls; Extended Properties="Excel 12.0;HDR=YES;IMEX=1"')...\[Sheet1$\] I dont have the CSV one handy.. but this is the XLS version. (SQL 2014)
Don’t forget SAS..very important 
 CREATE PROCEDURE AS BEGIN SET NOCOUNT ON; declare @sql varchar(1000) declare @name varchar(70) set @name = 'C:\test\test.txt' set @sql = 'bcp "SELECT Column FROM Table" queryout '+@name+' -c -t"" -T -S'+ @@SERVERNAME exec master..xp_cmdshell @sql END GO 
Standard exporting from a table CREATE PROCEDURE NameOfProcedure AS BEGIN SET NOCOUNT ON; declare @sql varchar(1000) declare @name varchar(70) set @name = 'C:\test\test.txt' set @sql = 'bcp Database.dbo.TableName out '+@name+' -c -t"" -T -S'+ @@SERVERNAME exec master..xp_cmdshell @sql END GO 
I was working with a table that included joins. The the table was duplicating rows due to having different addresses for the same customer. I was thinking up using distinct on a specific row which I now understand is not possible. Decided to use max on the address and do a group by
They're queued up and put in waiting state. Just a semi-educated guess.
A lot of statistics and modeling too. Plus a good deal of maths I think
My bad - your right I didn’t explain what data science is very well more so how to become a data scientist. I should’ve done that. 
What in Codds name is maths.
If you're getting duplicate addresses, couldn't you just put that into the join clause then? 
No problem, it's good information. It just seems like a general video that could apply to learning almost any new technology skill. If you want to talk classes and self-learning, a lot of that sounds like discipline and time management. Maybe talk about the stresses and difficulties of learning while working and ways to overcome those challenges. You could also talk about the goal of a data scientist and what their role is in a company. Maybe talk about what sort of decisions they make and what thought processes and instincts a data scientist develops.
It’s when you know more than one math
&gt; riddled with NOLOCK hints Ah yes, the magic go-faster command.
Thanks - very good points. I think the how you mention the skill set of a data scientist could be a better video. 
Git my dude.
Setting up a SQL Server is one thing. Making it accessible to others is another. Which is kind of the point of having a DB server to begin with. Otherwise just use Access. But assuming SQL Server is necessary, you might find it difficult to make this a shared resource unless you and your colleague are planning to share the same workstation. Do you have a Windows server to host this on? If that's not an option you might want to look into AWS as a hosting platform. Spin up an instance of SQL Server there and then connect to it with SSMS. If you decide to start by setting something up locally then you can migrate what you've built into an AWS instance later.
SAS is really only important in in a couple industries (health, fintech...) and really shouldn’t be anyone’s goal to learn. Recently got a new data analysis/sci type job and maybe 15% of postings had SAS down as a requirement. I’m fine with applying for just 85% of jobs 😉
&gt;a SQL code editor that will allow for multiple authors of a single query There's an extension for Visual Studio Code that does this already, but it's not limited to SQL. https://marketplace.visualstudio.com/items?itemName=MS-vsliveshare.vsliveshare Anytime I'm collaborating with someone on a query, it's in-person with one person "driving" and and ongoing conversation. Lots of hand gestures &amp; whiteboarding. Running intermediate queries to test ideas and verify assumptions. I can't see using such a tool when working with someone in the same location. We'd just walk all over each others' work and make negative progress.
Can't you use a formatting plugin for your editor?
I just take my boss' chair away and write in the edits in front of him.
Care to explain i'm not sure
Sounds similar to [http://sqlfiddle.com/](http://sqlfiddle.com/) People on stack overflow use tools like sqlfiddle to collaborate and discuss problems/solutions. &amp;#x200B;
So you have your join statement where you're joining the two tables on the primary key... can't you then join on the columns that contain the addresses as well?
The default functions for manipulating and evaluating date / time fields in most RDBMS's have always given me a hard time. It seems like implementations of common functions can vary between different products and versions of the same product, there's a lot of automatic formatting / casting / manipulation under the hood, and date / time comparison functions can render your queries non-sargable if you're not careful. I would suggest you consider storing dates / times as unix time stamps in a bigint field. This way, you can use standard numeric operations to compare and manipulate this information, avoiding these sorts of issues that arise from quirks in how an RDBMS implements a common, but not totally standard, function. I know this doesn't fix your problem, but it's the best advice I can offer you considering I haven't touched MySQL in five years or so.
Ok. Well the question wasn't asking for the most efficient method. This is probably the easiest way to go about it. I could have said make a CTE to store dates in 7 day increments starting on a Sunday then join with your query, or declare variables for your date range blah blah blah. Get a life. 
I would like to run the query so that it "updates" the table by deleting all the current values within it and replacing it with the new ones.
 CASE yourcolumn WHEN 1 THEN 'Apple' WHEN 2 THEN 'Banana' END
You're looking for a CASE in your SELECT statement. E.g. CASE WHEN 1 THEN 'Apple' WHEN 2 THEN 'Banana' ELSE 'Pineapple' END AS 'FRUIT_CAT'
I've never found one that consistently formats how I like it. There are some that get close, but especially once you get into complicated, nested queries, they get weird.
This is the simplest answer. When you get a big record set &gt; 100,000 you'll want to load those values into a #Table and join to that table to get the value. It'll be more performant.
Do you need nicely formatted forms and reports? Or are you and your team comfortable writing some queries and working with a fairly bare-bones interface? If you sign up for a run of the mill shared web hosting plan with a reputable provider you should be able to quickly and easily set up a MySQL database. Many, but not all, providers will allow you to set up a Postgres DB as well. Just pick a recognizable provider. I'd suggest Namecheap or Bluehost or Hostgator. Just don't pick 1&amp;1. There's a second provider that shocked and appalled me but I can't recall it's name right now.
Case is pretty performant anyway, wouldn't a join from a non-indexed field to a non-indexed temp table be worse than what amounts to a simple replace?
No offense but this is absolutely false! Users just need to be on the same network. You can install SQL Server on any computer on the network, setup permissions and it will work just fine. 
If you’re executing in MS SQL implementation then you need to add the keyword GO after each of your statements so that they can be executed in separate and serial batches. Otherwise they’ll be executed in one batch and the transaction will fail because the database wouldn’t have been created yet. 
Understandable, I've seen that as well. Poor Man's T-SQL Formatter does screwy things to `create or alter` and `drop table if exists`. It's kind of a question of how much quirkiness can one tolerate.
* Why is this posted to r/sql? * Why is this a link post to a screenshot of text instead of a text post?
I am really sorry I wasn't sure where to post it. I posted it already in chrome help forum but got not reply so I shared the picture here. Any help ?
For a 4-record lookup table, the difference will likely be negligible. However, consider normalization and future maintainability. * Need to change the meaning of 2? With a table, it's a simple `update` statement. The `CASE` in the query means it's a code change. * Need to add new lookups? `Insert` into the lookup table vs. a code change. Need to do the same lookup in multiple places? Copy/paste code vs. `join`ing to a shared lookup table. * Need to do internationalization? Again, table-driven will work much better in the long run.
You could just change your passwords.
You can't imagine how many account I have .. But thank you for the advice I will start doing it.
Sure, but I was trying to find out how it would increase performance as per OP's comment. And, to go out on a limb, someone who is asking for help from reddit because they don't know what a case statement isn't likely to need to worry about maintaining their code base or internationalisation.....
Yeah, I get it but honestly nobody here is going to give you a solid answer since it’s not really a sql question, it’s more of a question of how the scanning service works. If you’re concerned about it at all, there’s no harm in changing your passwords 🙂. 
Yeah, I get it but honestly nobody here is going to give you a solid answer since it’s not really a sql question, it’s more of a question of how the scanning service works. If you’re concerned about it at all, there’s no harm in changing your passwords 🙂. 
Eh, I remember in college we had collaborative projects on complex queries. Honestly that was one of the worst experiences of my college career and I wouldn’t wish it on anyone. I don’t think that collaboration works very well for query writing. It’s more of a thing where if you can’t figure out how to get a certain value, you can ask someone their opinion, and then take that and implement it how you will. 
Im on mobile, but it should be something like this SELECT * FROM TABLE WHERE ID IN (SELECT ID From table Group by I'd Having count (*) &gt;1)
This video gets really good around 11:19.
How most people become a data scientist in 2019: Create a freelancer.com account and call yourself a data scientist.
Good point, I am a healthcare data analyst (well basically just a report developer) and all we use is SAS and SQL
Yep, I agree on the case statement. I'm showing that there's more than 1 correct answer. For op's use yours is the best and likely what op's instructor is looking for.
Unless you utilize it within an aggregation.
After your union, that first column should be from the CTE, not the temp. You dont have a connection to the first half of the cte.
Probably not as relevant, but in Canada when applying for data analyst co-op jobs, only 1/50 that I applied for used SAS in their work environment
You dont need a where clause unless you have cyclical references A to b to c to d to b
Forgot about u/gggg8 's point. If your tables form a circle.. you might want to include a Where cte.parent &lt;&gt; original.child This will avoid a to b to c to d to b to c to d... etc
Try it and find out. SQL Server does a much better job parsing and executing SQL than random redditors.
PBIRS explicitly requires sql EE *core* licenses... have any old server CALs? Too bad.
Everyone saying PowerBI is expensive doesn't know what Tableau charges. PowerBI is an alternative for Tableau, not SSRS.
You can probably find an answer, but it won't be SQL
What is duplicated? The table1.id? Do group by id and sum(case.. THEN 1 ELSE 0) or learn to pivot
You can build a clustered index on the fly for a Temp table. Which is what you should be doing anyways if you’re working with datasets that big.
You can build a clustered index on the fly for a Temp table. Which is what you should be doing anyways if you’re working with datasets that big.
@name varchar(50) If @name is null Begin Set @name = ‘whatever’ End 
I'm getting some kind a syntax Not sure where I should add the above statement 
2 ways of doing this. First, by defining the default on the parameter: create procedure youNameIt @name varchar(50) = 'boss' as SELECT FirstName,MiddleName,LastName FROM Person.Person WHERE FirstName=@Name; Second, as /u/tkepongo said, by addind a bit of logic in the proc create procedure youNameIt @name varchar(50) as if @name is null begin; set @name='boss' end; SELECT FirstName,MiddleName,LastName FROM Person.Person WHERE FirstName=@Name; or more elegantly in the query itself create procedure youNameIt @name varchar(50) as SELECT FirstName,MiddleName,LastName FROM Person.Person WHERE FirstName=isnull(@Name, 'boss'); &amp;#x200B;
&gt; You can build a clustered index on the fly for a Temp table. How? What do you mean on the fly? Can you give example? Do you mean CREATE INDEX ... ON #TempTable (col1)?
We use SQLyog and an SVN, multiple hands in the pot. Usually if the query is being written from scratch it will have 1 writer who may or may not seek input from others. It'll be reviewed and updated until 'complete' and can be modified by anyone during and after the process. We'll reuse queries or parts of queries to save time if they already perform a function we need again. A program that does that would be 'bad'... but it's not really 'needed' and would be quite niche beyond existing solutions that work fine.
Damn I work with sql at work and I didn’t even know about this shit 
 TIMESTAMPDIFF(YEAR,new.gebdat,CURDATE()) &gt;= 18 [Holy crap do I ever hate the post delay timer, it's midnight thirty and I just want to answer Q's and go to bed...]
Thanks that kinda works guess I'll need to create a new table with default values cuz the above one will just replace the @name with the default value But for the question I need to display these default values 
VPN will work for the short-term. Long-term, you'll need to move the DB to the cloud or figure out how to make your app work asynchronously and for it to work off a local cached DB (on the phone itself) when not in range of your network.
The ApexSQL suite is quite nice to work with. Refactor is the one you want to tidy things up (Complete is also great when writing queries). They’re both free. With Refactor, you can set up your style guide in options. It also expands wildcards which I find really handy. 
Thanks
You'll need to check this value on both inserts (new records) and updates (to existing records). It'll go something like ... (Psudeo SQL) CREATE TRIGGER [TriggerName] ON [TableName] For INSERT AS DECLARE @GreaterThanPercent VarChar(1) SET @GreaterThanPercent = (SELECT TOP 1 CASE WHEN I.Charge * 0.015 &lt; P.Charge * 0.015 THEN 'Y' ELSE 'N' END FROM Products P INNER JOIN INSERTED I on P.ProductID = I.ProductID ORDER BY P.BornOn DESC) BEGIN TRANSACTION IF @GreaterThanPercent = 'Y' THEN ROLLBACK END IF @GreaterThanPercent = 'N' THEN COMMIT END END I'm sure the syntax is jacked up somewhere (I'm doing this on my mobile and from memory) but I think you get the idea. The UPDATE trigger will be basically the same except you'll swap out INSERT with UPDATE.
No, all of the above will put a default if null is present in the @Name variable. If @name contains a value, it's that value that will be used. But yeah, your table should have a default constraint if that's what you want. But you can use the isnull() function in your select too. SELECT isnull(FirstName, 'boss') as FirstName,MiddleName,LastName FROM Person.Person Take a look at the reference to understand what isnull() does: [https://docs.microsoft.com/en-us/sql/t-sql/functions/isnull-transact-sql?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/t-sql/functions/isnull-transact-sql?view=sql-server-2017) &amp;#x200B;
It's exactly what I need! Thank you
Ok Sorry but I don't quite understand it suppose I issue this update query UPDATE Production.Product SET ListPrice = 900 WHERE ProductID = 1; and the previous ListPrice was 0.00 this should execute my trigger now how will I know inside the trigger what the new values and the old values are? 
To understand triggers you have to have an understanding of how SQL Server handles records at a slightly deeper level than what you're used to. UPDATE operations are really two operations in one. Behind the scenes SQL Server deletes the record and then inserts the record all over again but with the updated values. Check out this MS doc on triggers - https://docs.microsoft.com/en-us/sql/t-sql/statements/create-trigger-transact-sql?view=sql-server-2017 In a trigger you can address newly insert records (INSERTED I) and deleted records (DELETED D). Since you clarified what you're looking to do, then what you'll want to do is something like this. When you create a trigger you define on what action you want it to execute (INSERT/UPDATE/DELETE) CREATE TRIGGER [TriggerName] ON [TableName] For INSERT/UPDATE/DELETE In this case you'll want to choose UPDATE You'll then compare the newly inserted value (INSERTED) with the old value (DELETED). You'll INNER JOIN the two (virtual?) tables and then a comparison as you normally would. After the comparison, you'll need a mechanism for denying the change, hence why I included the BEGIN TRANSACTION and ROLLBACK/COMMIT in my above comment. SELECT CASE WHEN I.Charge * 0.015 &lt; D.Charge THEN 'Y' ELSE 'N' END FROM Inserted I INNER JOIN Deleted D ON I.ProductID = D.ProductID You're literally comparing *what you want to insert* (INSERTED) with the *old value* (DELETED). Wrap it in a TRY CATCH or a BEGIN TRANS and you're done.
&gt;In a trigger you can address newly insert records (INSERTED I) and deleted records (DELETED D). Oh I see Thanks That really helps
Git gud
You’ll want to use an aggregate function like COUNT with a HAVING clause to identify anyone with multiple/duplicate addresses. You’ll want to use and analytic/windowing function like Row_Number or Rank to identify the actual multiple addresses for the same person. https://docs.oracle.com/cd/B19306_01/server.102/b14200/functions137.htm
In this case, DISTINCT / MAX is probably the wrong approach. I would recommend using a windowing function to rank order your dupes and the pick the first one (ROW_NUMBER is the SQL Server windowing function for this if that’s your DB). By using this approach, you can retain some control over how you pick a record, use multiple fields to define uniqueness, do some data scrubbing on the fly, and also avoid surprise changes in your data that break your logic later.
What duplicates are the activities. Since it runs on a like if Activity one is No and two is Yes it will show up like this. Also in my real world work the ID in the beginning of the query is not what I joined on because it is only in table. I should have made that clear I am joining on a different ID that is in all tables. ID Activity One Activity Two Activity Three 1 0 0 0 1 0 1 0
This is a neat solution I've used windowed functions but not in this way. I will give you're approach a go. Thank you!
which database are you using? If its oracle you have to use it something like this: WHERE date = to\_date('09-02-2019', 'dd-mm-yyyy')
MS Access as specified in the title
It will depend on your platform but basically get the closest jobPersonAssignment. In MS SQL you can use outer apply (select top 1... order by dateOfStart desc)
hm, try this if it works " SELECT Name FROM Appointments WHERE Date = DateValue("9-2-2019") "
i would simply do lower(FirstName) LIKE '%'||lower(@Name)||'%';
Access wants dates formatted with `#` around the values. Because you've used a reserved word for your field name (`Date`), you need square brackets around it so that Access knows it's a field. Or, you could rename the field to something more sensible that isn't a reserved word, like `AppointmentDate`. **Always** use ISO8601 format for your dates and be sure to use a `date` data type (not a string). Access does a stupid thing in mixing data and presentation (format). As you've written the date here, it's ambiguous to the user. Is it September 2 or February 9? You'll only know if you look at the table definition. `where Date = #2019-02-09#`
rownumber?
First thought is to partition the table on date with a sliding window. Then if you really wanted to lighten backups, do file or filegroup backups of just the active / operational data.
I've used count(*) and having &gt;1 to identify duplicates. I've assigned a rank using over (order by unique_column). How do I pick the first one so I can remove duplicates?
I wanted that first link to work so bad, and I eventually got to the point where I found out the script does not support CSVs that are double quote text qualified (which is about every single CSV I work with)... The second link you posted said that the "fast load" supports double quote text qualification so I'll try that next, thanks for the links!
That actually worked in SQL Developer if I manually highlight everything, vs just using Ctrl-Enter to execute. Shame it doesn’t work in DBeaver. Is there a more universally supported statement I could use in other clients?
Idk I don't use DBeaver haha. To my knowledge that is universally accepted PL/SQL. It may be a limitation of DBeaver. 
Yup, exactly. You can create indexes on temporary tables - when the temporary table is GC'd when the session ends, the indexes will be dropped as well. If I have large amounts of data to pull, that needs to be manipulated in multiple stages, my go to is temp tables for the stages, each indexed. 
Sounds like a good idea. Might be just what I need for a project I'm doing now. Thanks for the tip!
Thank you for your explanation, but i'm not sure i'm actually fully grasping it. What is the difference between child grandchild and a parent grandchild?
This is a great insight, thanks man
Yes, I plan on extending accessibility to about 5 to 10 other colleagues. I currently have a laptop that I take with me to travel for work, take it home, etc. If I create a SQL server and use my computer as a host, I'm assuming that this is something that I would then have to keep docked and powered at all times? &amp;#x200B; My colleagues and I are all running off the same local network, so I imagine it will be fairly easy for them to be able to access the database which I will be creating. &amp;#x200B; Since I've posted this I have also looked at some digital server solutions like Snowflake. I think you're correct that I would need to look at server hosting to support this initiative, but I'd like to get some input from fellow redditors who have a better understanding on this, &amp;#x200B; &amp;#x200B;
Ok I finally got it! I used the second link, but since it did not create the table for you, I had to code powershell to do that. Never coded in powershell before so that was a "fun" experience... sometimes the syntax choices that are made when creating a language baffle me. Anyway, in case anyone wants the code see here: https://pastebin.com/NTuF1Hnt All you need to do is put in the server/database/file path and it will load every CSV file in that folder into SQL. It also creates a field called "rowid" to uniquely identify the fields that are loaded. The tables will be named "Import20190207_CSVFileName" or similarly to that as you'll see. 
Use the query designer. Writing SQL in access is a nightmare. You also can't use the word date unless that's the column name. Is it? Otherwise you would do SELECT [Desired_Column] FROM [Table_Containing_Column] WHERE [Date_Column_Name]=9/2/2018
Dude I just realized you're asking for a date in the future. 9-2 means sep second. 
I manage 6 VM Servers, each has a SQL Server instance. They work ~seamlessly. We also have two desktop workstations that stay on the network that we use for dev and testing. They also work great. We can connect to them as long as we are LAN connected or VPN connected. I would also say that we started creating applications using only desktop workstations and SQL Server Express. Once we proved the value of doing thing it was pretty easy to convince the organization to buy full licenses and migrate to VMs. If this is similar to your journey, feel free to contact me anytime with questions. I would be glad to share my experiences. 
Let's work with 2 columns Parent, child... if I have.. 1,2 2,3 2,4 4,5 5,2 And i want to use a recursive CTE this is how it would work (let's call the above Original) CTE as ( Select parent, child, 0 as nest from original ) My data set would be 1,2,0 2,3,0 2,4,0 4,5,0 5,2,0 It's the union all that changes everything... so again. CTE as ( Select parent, child, 0 as nest from original Union all Select o. Parent, c.child, o.nest + 1 from original o Inner join cte c on o.child = c.parent ) The first pass produces 1,2,0 2,3,0 2,4,0 4,5,0 5,2,0 But the union all keeps the data going (the actual recursion) so this set is joined in the bottom half 1,2,0 2,3,0 2,4,0 4,5,0 5,2,0 -- pass 2 (union all, nest + 1) 1,3,1 1,4,1 2,5,1 4,2,1 5,3,1 5,4,1 The second pass is where things get out of control. I'm not stuck in an endless loop, because of the 2,4 4,5 5,2 circle. If I modified my cte with an AND CTE as ( Select parent, child, 0 as nest from original Union all Select o. Parent, c.child, o.nest + 1 from original o Inner join cte c on o.child = c.parent AND o.parent &lt;&gt; c.child ) Because my CTEs parent is 'repeating' it will stop before the 'loop' recorded forever My output 1,2,0 2,3,0 2,4,0 4,5,0 5,2,0 -- pass 2 (union all, nest + 1) 1,3,1 1,4,1 2,5,1 4,2,1 5,3,1 5,4,1 -- pass 3 1,5,2 -- 4,4,2 removed because of the AND (I think, or this shows and the nest 3 is gone) You query the cte as Select * from cte where parent = 1 And you get all descendants of the parent with a nest representing how far away they are. 1,2,0 1,3,1 1,4,1 1,5,2
No, it means February 9th. Actually, it's ambiguous and interpretation is dependent upon your locale. [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) or bust!
**ISO 8601** ISO 8601 Data elements and interchange formats – Information interchange – Representation of dates and times is an international standard covering the exchange of date- and time-related data. It was issued by the International Organization for Standardization (ISO) and was first published in 1988. The purpose of this standard is to provide an unambiguous and well-defined method of representing dates and times, so as to avoid misinterpretation of numeric representations of dates and times, particularly when data are transferred between countries with different conventions for writing numeric dates and times. In general, ISO 8601 applies to representations and formats of dates in the Gregorian (and potentially proleptic Gregorian) calendar, of times based on the 24-hour timekeeping system (with optional UTC offset), of time intervals, and combinations thereof. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/SQL/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
The problem is that we have to use sql for the exercise... Hahaha
I'll try that once I get to it! Thanks
Yeah the column name is Date (but then in Dutch, I translated it for my question here)
Yeah in Europe we use dd/mm/yyyy
So you understand the confusion here and why I'm such an enthusiastic supporter of ISO 8601 then?
Oh yes definitely, however when putting the data in the table it had no issues with dates like 28-3-2019, which wouldn't exist in a mm-dd-yyyy format, so I assumed it was using the European format.
Just grab the developer edition of SQL Server and use that to convert your backup: https://go.microsoft.com/fwlink/?linkid=853016
The best answer would be to download and install SQL Developer and restore it there, not sql express.
The exact error your new host support team is experiencing might help. You could also sign up for the free Developer Edition of SQL Server, install it, restore, then send them a traditional backup.
Thanks, this is helpful context!
this would be so much easier to explain in a GIF... let me know if that helped any more. I did this all on my phone
ApexSQL Refactor is awesome - one of the first things I install after SSMS on a new machine. 
That's because, as I said in my other reply, Access does screwball things mixing data storage and formatting. And then on top of that, it's interpreting the date based on your PC's region/locale settings
Thanks! Appreciate the tools referenced and description of your process!
If you've got SQL Server Enterprise with software assurance, you should have access to PowerBI Report Server. You can use the instructions under the heading "Purchased Software Assurance agreement" to look: https://docs.microsoft.com/en-us/power-bi/report-server/find-product-key
It is in Access which as far as I know infers your location but nonetheless the fact it's in the future is why he gets no results.
It is in Access which as far as I know infers your location but nonetheless the fact it's in the future is why he gets no results.
Unless there are appointments scheduled in the future which he's trying to see.
Yeah it sounds like shrinking the database would be the best approach to this. If you right click on the database and go to tasks. Then shrink and select database. Then it will show you how much of the actual database data file is actually being used. Then when you shrink it, it will remove all of the unused space - making the database file size way smaller. 
Good point
You are using the semi colon correctly. In apex, are these all in one window? If you can, get connected to the database using an IDae. I'd suggest oracles SQL Developer. It will help you with highlighting but also avoid weird apex quirks.
I think you can do similar stuff with sharing your database on azure
Also look at the `IN` operator... `WHERE Id IN ('t004', 't005')`
 WHERE pd.AccountingDate &gt;= CURRENT_DATE - INTERVAL 3 MONTH AND pd.AccountingDate &lt; CURRENT_DATE
https://www.sqlskills.com/blogs/paul/why-you-should-not-shrink-your-data-files/ The above link is more informational on dbcc shrinkfile. In this specific case, shrinkfile with truncateonly. 
its probably your log file. ship your log or switch Recovery Mode to SIMPLE then shrink your log file.
Yes, you can define the start date as a variable, and derive the end date from the start date. Something like this: declare @startDate date, @endDate date; set dateformat ymd; select @startDate='20180101'; select stuff from table where pn.PlayerID IN ( select playerid from playerday pd where pd.AccountingDate between @startDate and dateadd(month, 3, @startDate) and TripNumber &gt;= 1 ); Remember just that "between" is inclusive, and that the date will represent the day at 00:00:00, meaning that the end date will not include the specified day (00:00:00 is the first millisecond of the day)
Change CREATE PROCEDURE To ALTER PROCEDURE Change the parameter to @name varchar(50) = 'John smith' 
I'll check that out, thanks. I'm assuming by window you mean one SQL script file? I was told to update each SQL Select statement to replace this: SELECT Id as "Team ID" ,Name as "Team Name" ,Number_of_players as "# of Players" ,Discount as "Discount" FROM Teams; **To this:** SELECT Id as "Team ID" ,Name as "Team Name" ,Number_of_players as "# of Players" ,Discount as "Discount" FROM Teams WHERE Id = 't888' OR Id = 't999';
We spent a month debating with the ms acct rep. Tldr: when you go to download and install, it lists core licensing specifically. 
In Oracle (SQL Developer), I was able to use your code exactly as written, with exception to the CTE, and it is valid SQL. Example: with teams as ( select 't005' id, 'The Bobcats' as name, 23 as number_of_players, 5 as discount from dual union all select 't1000', 'The Teminators', 25, 7 from dual union all select 't004', 'The Mavericks', 26, 4 from dual ) SELECT Id AS "Team ID" ,Name AS "Team Name" ,Number_of_players AS "# of Players" ,Discount AS "Discount" FROM Teams WHERE Id = 't004' OR Id = 't005'; With some programs that allow custom SQL statements, I often come across a few common road blocks, none of which seem to present themselves here, but have a double check on this list of things. 1) Smart quotes rather than dumb quotes. Oracle doesn't like smart quotes. 2) Try getting rid of the semicolon completely 3) In some situations, if you put a comment (-- comment) on the last line after the semicolon, the statement will not execute. (SQL*Plus is an example of this, not allowing anything after the statement terminator) 4) This is far fetched but there is a "greek question mark" that looks exactly like a semicolon but it's a completely different unicode character. Commonly used for pranks on co-workers, aka those you hate. 5) Try getting rid of line breaks, or replace the line breaks with new breaks. 6) Use of double quotes where single quotes are required, or vice versa. 7) Use of characters that are not in the defined NLS character set. 8) Unix vs Windows issues: Lines breaking in &lt;cr&gt;&lt;lf&gt; vs. those that only have &lt;cr&gt;. 
And after you do the shrink, rebuild every index in the database, these will have been shredded by the shrink...
Ah, I didn't realize they had Developer Edition for free. Thought SQL Express was my only option. Thanks, I'll give it a shot.
Thanks, I'm going to try that now. 
Thanks, I'm going to try with Developer Edition now, will post error details if I get them. 
Your first method is the winner. It assigns a default value at the initial parameter level. The 2nd method is a bit clunky, but essentially does the same thing. The 3rd method is not a sargable query, so indexes won't used because there's a function on the left side of the where condition. It will work, but it'll be slow and invoke high i/o if the table is large. 
You need something to break the tie on records that are otherwise identical in your output. The Id identity column,for example: -- with Id sort. Using CTE because you can't put a window function in a where clause ;with Ranked_Results as ( SELECT DENSE_RANK() OVER(PARTITION BY REDDIT_KEY ORDER BY REDDIT_USER DESC, Id desc) AS [RANK], REDDIT_KEY, REDDIT_USER, KARMA FROM #Reddit ) select * from Ranked_Results where [Rank] = 1
\+1 for index rebuilds.
I tried those and I'm still getting the errors..thanks though. Do you happen to know if I should be running other script files before running this one? Probably a stupid question but all of this is still so foreign to me at the moment. My professor said something about running script files in a particular order to avoid statement errors.
Are you talking about the isnull? It'll still use indexes with that isnull.
I tried this and it worked, despite having tried using the hashtags (but probably in the wrong way). Thanks!
If I remember correctly, this is a Mysql syntax. It won't work in sql server. 
You can shrink the db, I'd suggested rebuilding/reorg-ing indexes afterwards. But if you have scripts that transfer the data, you could just drop the db and recreate, then run import again. No need to worry about the fragmentation from shrinking. Shrinking in general should be avoided unless you have an understanding of how that creates potential performance problems down the road. 
Thanks! Going to test this out in the lab.
Like... this crazy nonsense? Just meant to give you an idea.. seems you need all dates persons first, then LEFT the jobs.. then pivot it out into a week. with CartesianDatePerson AS ( Select PersonID, DoWeek From Calendar, Person Where Date between @start and @finish) Select PersonName , ISNULL(m,'X'),ISNULL(t,'X')....,w,th,f,s,su FROM ( &amp;#x200B; SELECT pp.PersonId, DoWeek, JobName from CartesianDatePerson LEFT JOIN ( Select p.PersonID, j.JobName, [c.D](https://c.Date)oWeek From Person Left join PersonAssignment left join Job INNER join Calendar on convert(AssignmentDateOfStart, date) = Date AND Date between @start and @finish ) On PersonId and DAte Left Join Person pp )x Pivot ( MAX(JobName) /\*like i said.. stuff before this step for comma delimited job) For DoWeek IN (\[M\],\[T\]....) ) pvt
So for the completely automated one if i run that on say the 1st of April at 00:01 its going to count April as one of the months? My plan was to set up a job to run this and have it run at on the first of each 3rd month. 
no. It will take from the 1st of January 00:01:00 until the 1st of April 00:01:00 I just reworked a little bit the query. If you run this anytime during the month of April, it will return data from the 1st of January 00:00:00 to the 1st of April 00:00:00 declare @startDate date, @endDate date; set dateformat ymd; --this compute the 1st day of 3 month ago, relative to the current date select @startDate=DATEADD(MONTH, -3, DATEADD(DAY, -DATEPART(DAY, CURRENT_TIMESTAMP) + 1, CURRENT_TIMESTAMP)); --Add 3 month to the start date select @endDate=DATEADD(MONTH, 3, @startDate); select stuff from table where pn.PlayerID IN ( select playerid from playerday pd where pd.AccountingDate between @startDate and @endDate and TripNumber &gt;= 1 ); Running this on my local instance now yields this as variables values: @startDate | @endDate \-------------------+---------------- 2018-11-01 | 2019-02-01 &amp;#x200B;
I totally misread it, sorry! Was walking down the street on my phone and didn't process it properly. For some reason I thought the isnull was on the left side of the =, in which case it wouldn't use indexes. But yeah you're right, now that I've reread it I see it's on the other side, so it's fine. I'd still go with option one though. I'd need to look at the execution plan to be 100%, but using the isnull there would still possibly be slower, as your using a function that's not really required. 
I saw in the screenshot that it shows under the Core version. Do you see that in your account?
[SQL Server Stretch DB](https://azure.microsoft.com/en-us/services/sql-server-stretch-database/) (requires Azure) fits your description almost perfectly.
I'll grab screenshots tomorrow when I'm at work
Why do they need to be on completely separate instances?
This is wonderful. Thanks for the help and your time. I ran it in the lab and its working!!! 
Is there like, an Oracle equivalent to a MSSQL 'GO' statement? Might that help here when running multiple statements in the same window? Alternatively, will running one statement work? I think the semicolon is optional.
If you inserted a single record, is there an Oracle version of @scope_identity and is that what your prof is trying to teach about here? Scope_identity variable gets the most recent ID inserted. Folks use that to quickly confirm the data indeed did get inserted, or when doing parent-child inserts where the child table wants to have the parent's ID as a foreign_key so you can join it back up later.
Thanks for the input and suggestion. I think you're right. Talked to my boss a bit and he made some general suggestions. I was trying to prevent anther game being added, but for simplicity I've created a calendarDayJobAssignment table, so each absent day can be directly matched with a job number. Thanks again! 
The semi colon is the equivalent. In both Oracle and MS, you can set this 'delimiter'. (This is what that concept is called.) The semicolon isn't optional. Another in Oracles PlSql is the backslash. This can seperate scripts and is used to end blocks.
In SSMS (and I think SQL developer and toad?) You can also highlight a section if you just want to isolate a single query.
You can indeed. And it can ignore semi colons and run more than one query. However in SQL dev youcan 'run statement' which would run everything top to bottom in your statement *unless* you have backslashes. It will stop there. 
No worries mate, the group by specifies how you're splitting the COUNT function by (so grouping by months \* salary means that you'll be returning the count for every distinct value of months \* salary ). &amp;#x200B; The example you've posted is a good one: `SELECT Salary, COUNT(*) FROM Employee WHERE salary = 5;` &amp;#x200B; &amp;#x200B; If we focus on the basic COUNT statement, this example will return the count of every row in the Employee table where the salary = 5 `SELECT COUNT(*) FROM Employee WHERE salary = 5;` &amp;#x200B; Now this example will return the count for every distinct Salary entry in the table (including Salary 5). That's what the group by is doing, it's saying split the count for every unique value in the salary column. `SELECT Salary, COUNT(*) FROM Employee GROUP BY Salary;` &amp;#x200B; Without the group by it wouldn't know what to do with the Salary column, because you've not told it to split the count by Salary with a group by but you're still trying to show both the count(\*) and the salary column. &amp;#x200B; Does that make sense?
If you're grouping by id, you should not have duplicates anymore. When I get to my machine I'll write a query. If it doesn't work then you're probably omitting a crucial detail.
A temp table will persist for the duration of your connection. A drive table exists for a brief moment while query is run.
What have you tried? 
what happened when you tested it? ™
Thanks, this is a little clearer now. I guess just practicing using each in different ways should do the trick. Thanks again, much appreciated.
I would put a subquery in the select ordered by date, desc, and top 1, that returns the key field, then join that back to the base table for the details. 
I think the “#” column alias might be messing up the quotes. It’s commenting out everything else on the line. Also it’s good practice to avoid spaces and almost all special characters in the names you assign to any database object. You’ll save yourself from some headache later down the road. 
 CREATE TABLE #activity ( id INT, event_name varchar(25) ) INSERT INTO #activity (id, event_name) VALUES (55, 'Activity1'), (55, 'Activity1'), (55, 'Activity1'), (55, 'Activity2'), (55, 'Activity2'), (55, 'Activity2'), (123, 'Activity2'), (123, 'Activity2'), (123, 'Activity2'), (123, 'Activity3'), (123, 'Activity3'), (123, 'Activity3') WITH cte AS ( SELECT id, sum(CASE WHEN event_name like '%Activity1%' THEN 1 ELSE 0 END) AS [Activity One], sum(CASE WHEN event_name like '%Activity2%' THEN 1 ELSE 0 END) AS [Activity Two], sum(CASE WHEN event_name like '%Activity3%' THEN 1 ELSE 0 END) AS [Activity Three] FROM #activity GROUP BY id ) SELECT id, CASE WHEN [Activity One] &gt; 0 THEN 'Yes' ELSE 'No' END AS [Activity One], CASE WHEN [Activity Two] &gt; 0 THEN 'Yes' ELSE 'No' END AS [Activity Two], CASE WHEN [Activity Three] &gt; 0 THEN 'Yes' ELSE 'No' END AS [Activity Three] FROM cte 
Not very performant.
Imgur is blocked at work but the pattern for doing this in a single table would be like: SELECT * FROM (SELECT *, row_number() over (partition by PolicyNumber order by ChangeDate desc) AS ranking FROM yourtable) x WHERE x.ranking = 1 The other mistake people often make is tilting the head backwards. You want to pinch the bridge of your nose and tilt your head *forwards*.
Doubt it matters in this case. What would you do?
See my other comment.
Like it
I usually get this error when I use double quote marks. Try apostrophies and see what happens.
Did this in MSSMS: &amp;#x200B; \--Create temp table containing sample data create table #policies ( CustID int, PolID int, Veh Varchar(4), Make Varchar (4), Model Varchar (2), Coverage Varchar (9), Limit int, DateEntered SmallDateTime) insert into \[#policies\] values ('1','1','2002','BMW','M3','Liability','300000','2/1/2018'), ('1','1','2005','BMW','M5','Liability','300000','1/31/2019'), ('2','1','2010','Audi','S4','Liability','100000','1/15/2019'), ('2','1','2010','Audi','S4','Liability','25000','1/20/2019'); \--Retrieve max value (most recent record) for each CustID select \* from #policies p where p.DateEntered=(select max(DateEntered) from #policies where CustID=p.CustID)
If you're brand new I recommend Access. It's limited, but let's beginners get to creating tables and queries in a short amount of time
Forgot to warn about the fact that truncateonly only takes care of the end of the data file. Good catch
Does your table actually have a GUID as the primary key? I'm not sure whether you know what a GUID really is, or whether you are just using it to say a unique id. It would probably help to script out the full table schema, including all existing indices, and post that. 
This is a simplified version of 7 tables I’m pulling. CustomerID is a GUID, PolicyID is a GUID. I wish there was some kind of TranID for when the policy changes. I’m going to try some of the solutions posted. I think the partition will work, I had something really close to that but was partitioning the wrong field so my row numbers were 1-100000 and didn’t start over with each policy.
Ok cool, I was just making sure you knew what a GUID referred to, but clearly you do. I'd go for the partition technique too, I don't see any reason it would not be performant with the right indices. 
Drop table if exists #MostRecentPolicies Select PolicyId ,Max(DateEntered) as DateEntered Into #MostRecentPolicies From OriginalTable Group by PolicyId Select * From OriginalTable a Join #MostRecentPolicies b On a.policyId = b.policyId And a.DateEntered =b.DateEntered This is the simplest IMO. No need to partition or write a subquery. 
Ok, I got SQLDeveloper Edition working locally. Here's the error I get when trying to import the bacpac Could not import package. Warning SQL72012: The object [NewDatabaseImport_Data] exists in the target, but it will not be dropped even though you selected the 'Generate drop statements for objects that are in the target database but that are not in the source' check box. Warning SQL72012: The object [NewDatabaseImport_Log] exists in the target, but it will not be dropped even though you selected the 'Generate drop statements for objects that are in the target database but that are not in the source' check box. Error SQL72014: .Net SqlClient Data Provider: Msg 12824, Level 16, State 1, Line 5 The sp_configure value 'contained database authentication' must be set to 1 in order to alter a contained database. You may need to use RECONFIGURE to set the value_in_use. Error SQL72045: Script execution error. The executed script: IF EXISTS (SELECT 1 FROM [master].[dbo].[sysdatabases] WHERE [name] = N'$(DatabaseName)') BEGIN ALTER DATABASE [$(DatabaseName)] SET CONTAINMENT = PARTIAL WITH ROLLBACK IMMEDIATE; END Error SQL72014: .Net SqlClient Data Provider: Msg 5069, Level 16, State 1, Line 5 ALTER DATABASE statement failed. Error SQL72045: Script execution error. The executed script: IF EXISTS (SELECT 1 FROM [master].[dbo].[sysdatabases] WHERE [name] = N'$(DatabaseName)') BEGIN ALTER DATABASE [$(DatabaseName)] SET CONTAINMENT = PARTIAL WITH ROLLBACK IMMEDIATE; END (Microsoft.SqlServer.Dac)
&gt; I get the sense you are not entirely free to do this though. You are correct. Should have never put this into the software co’s data center.
I've been doing SQL dev for like 15 years and never once uttered the phrase "derived table". I looked it up and seems to be a subquery you're referring to right? Select * from (select something)? The fact that you're comparing to temp tables had me thinking in memory tables aka table variable. But like the other poster said a temp table (defined as #tablename) is a physical table that lives in tempdb till you're session ends. A subquery lives in memory and exists only during the execution of the statement. Now of course the query engine will make use of tempdb as well if needed. 
I think the key is here... The sp\_configure value 'contained database authentication' must be set to 1 in order to alter a contained database. You may need to use RECONFIGURE to set the value\_in\_use. &amp;#x200B; Open SSMS and connect to the instance you installed. Launch a new query window.... &amp;#x200B; \`\`\`\` sp\_configure 'contained database authentication',1 go reconfigure go \`\`\`\` Try the restore again.
yea that did it. The "reconfigure" is key too. at first i just tried running the configure SPROC.
Now you can throw shade at your new hosting support team... they should have known how to do that.
https://www.microsoft.com/en-us/licensing/product-licensing/products and download the preferred language search "Power BI" (page 28/112 in the English download) "4.6 Use of Power BI Report Server – SQL Server Enterprise Edition Customer may run Power BI Report Server software on the Licensed Server. Customer may run the software on a maximum numbers of cores equal to the number of SQL Server Enterprise Edition Core Licenses with active SA assigned to that Server, subject to a minimum of four cores per OSE. Use is additionally subject to the applicable terms of Customer’s volume license agreement. A Power BI Pro User SL is required to publish shared Power BI reports using the Power BI Report Server. This right expires upon expiration of Customer’s SA coverage. "
You forgot &gt; WITH (NOLOCK)
Good point! I haven't tested this: be careful because it might be too fast for your hardware to handle: SELECT * WITH (NOLOCK) FROM WITH (NOLOCK) (SELECT *, WITH (NOLOCK) row_number() over (partition by PolicyNumber order by ChangeDate desc) AS ranking WITH (NOLOCK) FROM yourtable) x WITH (NOLOCK) WHERE x.ranking = 1 WITH (NOLOCK)
they did tell me to do that, but I didnt know I could get the Developer Edition of SQL to do it locally. Still getting errors when i try to import so we'll have to see how that goes now.
You're hired. 
This ended up working, thank you so much! My grandfather always told me to put a nickle under my lip and shove it up between my gum line and my lip. Coins have to be the dirtiest thing you could ever put in your mouth.
what would the recorded_entry field store in your 2nd example? My guess is you should have a user table (user_id, email, hashed_password) Then another table to store any expiring apikeys (user_id, datetime, apikey) You could then prune the apikeys table once a month or whatever, removing anything older than 12 months. 
Sounds like homework maybe so I will hint rather than solve. Window function, partition by, qualify ... &lt;=10 That's how I'd start looking at it. 
Good question. The recorded value would be a three-digit integer. One recorded by the user at their whim, various times per day. Yeah, thinking about it, I could see doing a different table for the api keys. I wasn’t planning on making them expire, though. At least, not by a time duration; I was planning to give the user the option to generate a new one, though.
Good question. The recorded value would be a three-digit integer. One recorded by the user at their whim, various times per day. Yeah, thinking about it, I could see doing a different table for the api keys. I wasn’t planning on making them expire, though. At least, not by a time duration; I was planning to give the user the option to generate a new one, though.
Good question. The recorded value would be a three-digit integer. One recorded by the user at their whim, various times per day. Yeah, thinking about it, I could see doing a different table for the api keys. I wasn’t planning on making them expire, though. At least, not by a time duration; I was planning to give the user the option to generate a new one, though.
Oh oh, if the apikey is just attached to use and doesn't change and its just one apikey per user, you could just stick it in the user table. For the three digit value, store that in a seperate table, along with the user_id and datetime. Then you can see the full history of 3 digit values per user, and store many values per day, without having to replicate the email and password in each row.
This should be flared as MariaDB or MySQL: I thought I could add them as flair and not as [brackets] in the post title, oops.
Post the execution plan for each query... EXPLAIN SELECT yadda yadda OR criteria can often perform poorly, but 15000 rows is really a tiny amount of data. My guess is you just don't yet have the right covering indices for this query to perform well 
Thanks for your contribution man
I can think of four ways to skin this cat, but I'm betting the answer heavily relies on what the lesson plan was you recently covered.
I found a clue! Asking for the execution plan for the non-terminating query also seems to timeout, so it’s a problem with assembling a plan instead of the planner picking something that doesn’t terminate. 
SELECT orde.user_id, (CASE WHEN orde.order_date BETWEEN firsts.FirstDate AND firsts.FirstDate + INTERVAL 90 DAY THEN orde.subtotal ELSE 0 END) AS '90DaySubTot' FROM orders orde LEFT JOIN ( SELECT ords.user_id, MIN(ords.order_date) AS 'FirstDate' FROM orders ords GROUP BY ords.user_id ) firsts ON orde.user_id = firsts.user_id WHERE orde.order_status = 'complete' GROUP BY orde.user_id
It's not homework actually, just my coworker who gives me practice queries. I only learned the basics of SQL on udemy so far, so I'm wondering which method would be simplest to go with at a beginner level. I'd appreciate hearing your take on it, if it can be explained to a beginner.
Sub query and derived table are actually different. One is in the where clause and the other the from. https://community.teradata.com/t5/Database/Subquery-VS-Derived-Table/td-p/28084 I personally call them the same thing, as to me it is another select it is pulling from. Depending on what you are doing, I normally use temp tables a lot. There are multiple reasons, but the biggest is allowing myself and others to run through it easier to see if there are problems. Working in my system, maybe something changed, something is wrong, or someone entered something wrong. It is easier to go through temp tables with a person to find the problem exactly, almost like writing out a math problem in full steps. This has saved me hours of time, where I have stored procedures I inherited that is one block, with derived, inside derived, with sub selects and 200 lines. Now find why x is not working...a lot longer to test and break apart. I also like to build everything, then use a final temp table to do all my converting as possible. Changing a column of customerNumber to [Customer Number], or dates, or case statements. That way through the main code it is a pure as possible. I also at the end of my code, drop my temp tables.
It's not homework (though I'm thinking of taking a course), just my coworker who gives me practice queries for fun. I only learned basic SQL, so I was wondering if there's a simpler method for someone at a beginner level like myself. I'd appreciate it if you'd like to solve or explain from your perspective.
I normally add my comments in the code. I comment everything, because I have a bad memory. So I tend to comment, due to writing myself notes. I’m have had guys share with me code, and I comment out their lines and add mine, or extreme block comment out their code and add mine.
Source control for stored procedures is something I have wanted to do. I tried BitBucket at a previous job, but each was a folder and after 20 it got cluttered. Any suggestions on Git and a DB? Book or videos.
Ah yes I see now... Subquery vs subselect and yeah I totally know what you mean with a large procedures having intermediate result sets can really help clarify what's going on and make troubleshooting easier. There is however the downside of extra IO, but like anything it really just depends on what you're doing. I've learned over the years the best way to do something is to choose the best way to do something. 
This! Everyone has different forms, hence it is like art. I actually can go in and see who did what by capitalization and etc. There is one that I swear it is the rainbow, converts, copied and pasted on the same column a dozen times throughout. I came up with the idea why not do an editor that we all agree on, nope. I have a new guy who does some and sends it to me, and I reformat it just to read it.
Yea they told me to set it locally again... As far as I can tell this is ran on the Server not the Database correct? They seem to think it's done on the database. I feel like they should have at least known that..
We did something similar in terms of what we call a Lake. We get multiple imports, Ranging from cav to pipe to fixed width. We don’t need all the columns, but had the problem where five years later they wanted a column in the data warehouse and importing again. So we have a generic table called import processed with columns such as col001. Another table actually is the layout, so we have a sap page setup that use these to upload. I been wanting a faster way, as it was designed to do loops of one row at a time, building dynamic sql statements. I rather do step one, then two, etc...
If you want all to be removed, and you insert clean I would truncate. If you want to only update new data and have old data stay there, I would merge. The merge can do an insert of brand new data that does not exist, update of existing, and deleting if there is nothing now.
I always viewed SQL as the same language between with just enough differences to sue each other and claim they are different. Easy analogy would be it is a English, but different accents.
I know you can do this with window functions, though I'm used to not having them available in my 'real world scenarios' The simplest... SELECT acc.Country, SUBSTRING_INDEX(GROUP_CONCAT(acc.AccountName ORDER BY acc.Sales DESC),',',10) FROM accounts acc GROUP BY acc.Country A little more complex (overkill for this scenario) SET @Cnt=0,@LastCountry=''; SELECT b.Country, b.AccountName FROM ( SELECT IF(@LastCountry=a.Country,@Cnt := @Cnt + 1,SUBSTRING_INDEX(CONCAT(@Cnt := 0,';',@LastCountry := a.Country),';',1)) AS 'Weight', a.Country, a.AccountName FROM ( SELECT acc.Country, acc.AccountName FROM accounts acc ORDER BY acc.Country, acc.Sales )a )b WHERE b.Weight &lt; 10 With a window... I'm not real used to these but I googled a little, I think it could be better than this though SELECT a.Country, a.AccountName FROM ( SELECT DENSE_RANK() OVER w AS 'DRank', acc.Country, acc.AccountName FROM accounts acc WINDOW w AS (PARTITION BY acc.Country ORDER BY acc.Sales) )a WHERE a.DRank &lt;= 10
This was done with MySQL 8 using the following for testing: SET @Name='Bob',@Country='Asia'; INSERT INTO accounts (AccountName,Country,Sales) VALUES (CONCAT(@Name,'1'),@Country,RAND()*10000); INSERT INTO accounts (AccountName,Country,Sales) VALUES (CONCAT(@Name,'2'),@Country,RAND()*10000); INSERT INTO accounts (AccountName,Country,Sales) VALUES (CONCAT(@Name,'3'),@Country,RAND()*10000); INSERT INTO accounts (AccountName,Country,Sales) VALUES (CONCAT(@Name,'4'),@Country,RAND()*10000); INSERT INTO accounts (AccountName,Country,Sales) VALUES (CONCAT(@Name,'5'),@Country,RAND()*10000); INSERT INTO accounts (AccountName,Country,Sales) VALUES (CONCAT(@Name,'6'),@Country,RAND()*10000); INSERT INTO accounts (AccountName,Country,Sales) VALUES (CONCAT(@Name,'7'),@Country,RAND()*10000); INSERT INTO accounts (AccountName,Country,Sales) VALUES (CONCAT(@Name,'8'),@Country,RAND()*10000); INSERT INTO accounts (AccountName,Country,Sales) VALUES (CONCAT(@Name,'9'),@Country,RAND()*10000); INSERT INTO accounts (AccountName,Country,Sales) VALUES (CONCAT(@Name,'0'),@Country,RAND()*10000); INSERT INTO accounts (AccountName,Country,Sales) VALUES (CONCAT(@Name,'10'),@Country,RAND()*10000); INSERT INTO accounts (AccountName,Country,Sales) VALUES (CONCAT(@Name,'11'),@Country,RAND()*10000); INSERT INTO accounts (AccountName,Country,Sales) VALUES (CONCAT(@Name,'12'),@Country,RAND()*10000); with the table being: CREATE TABLE `accounts` ( `AccountID` bigint(20) NOT NULL AUTO_INCREMENT, `AccountName` varchar(20) DEFAULT NULL, `Country` varchar(20) DEFAULT NULL, `Sales` bigint(20) DEFAULT NULL, PRIMARY KEY (`AccountID`) ) ENGINE=MyISAM AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; 
##GlobalTempTable 
 SELECT * FROM highly_derived_view WHERE col1 &lt; 1 AND col2 &lt; 1 AND col3 &lt; 1 AND col4 &lt; 1 Does the same thing? I'm not sure why you would tell it to AND and keep the parens like that. Does it work without them?
After some practice i figures it out. Thanks again for the great suggestion I learned something new.
Can you do start-date + 90 in the select? Looks odd to me, but I have no experience with postgre
Ye. The query works perfectly fine minus the WHERE clause. Date + int works just fine in Postgres thankfully 
P.s. can you do +90 to a date datatype? Also seems odd... 90 of what? Days, months, years?
Wow, that’s pretty cool!
That’s pretty cool
Close, but dense rank will return multiple results per rank if there are ties, meaning more than 10 rows per partition. row_number would be better. It's also not clear from OPs description whether Accountnames are unique in OP's table, by the looks of their original query possibly not.
No, that sp_configure sets the option at the instance level, not the database level.
One possibility is to write both clauses as complete queries and UNION them together. Then add an additional AND clause to each that requires the new param to be true or false respectively 
This is how I would do it.
That is a pretty creative solution. I'm assuming this wont have any impact on performance either because the additional WHERE clause, so the query that isn't meant to run won't run at all?
I think it's a fairly safe assumption that any modern, pro-grade db engine will be able to optimize the query to not waste cycles on the "unused" half
Thinking about it, if the query has to be repeated anyway, isn't it better to just have an IF ELSE statement and repeat the query that way? 
the union solution would probably work. my solution makes some code redundant but it doesn't hurt anything - make them 2 separate queries, and wrap the entire thing in an IF choosing which path to go down based on the input parm. i don't think there is much a performance issue with this. IF parm = X then select elsif parm = Y then select the other one end if; 
Doing this could introduce an issue where the sproc saves the plan for the first query it executes. This could cause issues if for instance one requires more memory than the other etc. If you take this approach you should use the IF to call seperate sprocs which will each get independent plans. I.e. IF @param = 'a' THEN EXEC somesproc @param1,@param2 ELSE IF @param = 'b' EXEC someothersproc @param1,@param2 END IF;
You will want to research pivot. You will need to know the branch names to do this. If a new branch is added you'll have to rewrite the report. Conversely if you flip the report and do a column with each branch name or gets a lot easier. I had a similar ask and convinced the business to use the flipped report. Which was good as the were like 200 branches and the report was basically unreadable.
Merging the procedures seems to be a bad idea to me. While they return the same columns they are distinctly diffrent queries and you're setting and you're setting yourself confusion in future by rolling them together. Passing a procedure two sets of variables with a flag deciding which to use to is pretty classic "code smell" because it begs the question why you aren't simply selecting between two diffrent ones at the point you're running the logic for setting the flag. But if you do have to go this way for what ever reason I'd at least keep the procedures separated and wrap them in a new one that selects between them. One way to approach doing that that would help keep the code clean would be be to turn them from procedures into table-valued functions. Which, if you've not come across them before, are basically a way to make it easier to work with sprocs that return a table. You'd create one like this CREATE FUNCTION [dbo].[function1] ( @UIOID [type], @Level [type]) RETURNS TABLE as RETURN ( SELECT A B C FROM X WHERE ( @UIOID = CASE WHEN @Level = 'Single' THEN C.C_UIOID_PK WHEN @Level = 'Children' THEN CLC.UIOL_P WHEN @Level = 'Parent' THEN CLP.UIOL_C END OR ( @UIOID = '0' AND @Level = 'All' ) ) And select form it like this SELECT A ,B ,C FROM [dbo].[function1] ( @UIOID, @Level) Which would both be much nice to work with however you set up the new procedure. 
UNION requires the same columns across both SELECTS meaning you can be sure of getting the same the number and types of columns returned regardless of the flag. With IF ELSE you could potentially return something completely diffrent meaning you could accidentally add a column to one statement and not the other which can cause problems down the road. 
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/linux] [Today I discovered a nice tool](https://www.reddit.com/r/linux/comments/aognnu/today_i_discovered_a_nice_tool/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Nice advertisement.
Holy damn, thank you! But as I read it I feel like you made some mistakes in the code? Or at least the different tables used for columns don't seem to be correct, imo? 
Did this on my phone, no intellisense. Insert disclaimer here. Just trying to communicate what's happening in the CTE using an ordered list of integers. Recurions will work with multiple columns as long as you follow the pattern. If you find an issue in my psuedo and need me to debug it, let me know.
 SELECT * FROM Users u WHERE ManagerId IS NULL AND NOT EXISTS ( SELECT 1 FROM Users WHERE UserId = u.ManagerId) &amp;#x200B;
Have an upvote for Row Numbering *forward*
Ahh, yeah, as I said, "little googling." I can only wish we had windowing and I would die to have CTEs in the stuff I work with.
Postgres supports indexes on expressions or functional indexes. You may want to compare the execution plans before and after adding a functional index if you haven't already.
That's also a good point. My original thought process was to create a SQL server on the backend that would feed into an Access database that the majority of the team would use for input and downloading data. As you mention though, perhaps I need to take a breath and work this project out in baby steps vs. the big chunks I'm currently trying to make happen. The good news is that we do have team buyin already, but as you mentioned, proof of concept is key to get them on board as well as ironing out any potential issues.
You need to partition the rows by the fruit\_bought, then order by date desc, then use Lag() combined with date arrhythmic to calculate how many days between purchases. You can subquery that then select from it Pseudo-sql &gt;select \* from &gt; &gt;(Select id, fruit\_bought, quantity, date, LAG(date, 1) OVER (PARTITION BY fruit\_bought ORDER BY date desc) as PurchaseLag from fruits f) x where x.PurchaseLag &lt;= 10 &amp;#x200B; Doing this from memory, so probably missing something. &amp;#x200B;
The point about query plans is very important. A common issue with sprocs that use multiple parameters that can be used in various combinations is "parameter sniffing". You can read up about that at your leisure, but the take away is that the plan that gets created for one set of parameters may not work well for another. When you test your proc, run tests with each of your options back to back, alternating which option you run first and using the same parameters. So: clear the query plan (sp\_recompile procname), run option 1 followed by option 2. Then clear the query plan again and run option 2 first, followed by option 1. If either of these cases results in one of the options running slower if it's the second one in the test, then you have a plan that works well for one option and not the other. There are various solutions to the issue, but first you need to be aware of whether your proc is vulnerable.
This is purely hypothetical in nature. Lets say that there are API that only use current data (less than 6 months old) and we want to increase that performance by limiting the volume of data and users on that system. The older data is used for reporting and analysis and they would access via the secondary system. So that the CPU and I/O are not affected to the API read instance. That's the thought behind the question. 
This looks like it might be a good use of dynamic SQL. I'm not sure what flavor of SQL you use, but here's a simple example for mssql. DECLARE @sql NVARCHAR(MAX) DECLARE @select NVARCHAR(MAX) DECLARE @where NVARCHAR(MAX) DECLARE @on NVARCHAR(MAX) DECLARE @param INT = 2 &amp;#x200B; SET @select = 'SELECT \* FROM customers c INNER JOIN orders o ON ' &amp;#x200B; IF(@param = 1) BEGIN SET @on = 'c.customerId = o.customerId' SET @where = '' END ELSE IF(@param = 2) BEGIN SET @on = 'c.customerId = o.parentId' SET @where = ' WHERE c.sales &gt; 500' END &amp;#x200B; SET @sql = @select + @on + @where &amp;#x200B; PRINT (@sql) Then execute @sql
Hey, Tnx for taking the time to help me out! I think I get it now! Cheers, 
Are you sure? Because I had no databases setup when I ran it and I had to select the Server object (since there were no databases) when running the query to get it to work. After that I was able to import with no problem. Really made it seem like it was ran on the server itself as oppose to the database instance.
I like this one more than mine. SELECT id FROM Users WHERE ManagerId IS NULL EXCEPT SELECT managerId FROM Users
That's great advice thanks.
I think we're confusing terminology here. &amp;#x200B; SQL Server when installed on a computer is an Instance. There can be multiple Instances of SQL Server installed on a single server. Each Instance can have multiple Databases installed as well. SP\_CONFIGURE manages configuration items of an Instance of SQL Server. &amp;#x200B; Does that help? 
Thanks for the reply. I'm getting the following error: ERROR: operator does not exist: date &lt;= integer
My bad, you gotta do date arithmetic, and I don't remember the specifics for Postgres, but it's whatever function that gets you date - LAG(date, 1) OVER (PARTITION BY fruit\_bought ORDER BY date desc) as PurchaseLag 
I am brand new(1 week) into learning SQL, so any help will be appreciated. I know how to create a table, database and that's about all lol...
yes it does, it was terminology, what you just described is how I understood it. I thought databases were called database instances and there were also server instances. I'm more of a dev than dba so my db vocab is a little lacking.
Start 70-761 if you want MS certs, followed by 70-762. Book wise Database Design for Mere Mortals and The Datawarehouse Toolkit are commonly recommended books. [This](http://www.sqlservercentral.com/stairway/72494/) is a good coverage of SSIS.
Depends on where you want your career to go. Microsoft ecosystem is massive. There’s DBA level stuff (which would probably require some certs and some extensive sql experience), SQL Developer, SSIS developer (Microsoft ETL tool), Azure (Microsoft’s cloud platform - this is most likely the future of computing and the learning curve can be steep). There’s a lot more, but those are the basics for someone with minimal ETL/SQL experience. I’m my opinion the best way to pivot your career is within your current organization (assuming the pivot isn’t just between SQL dev and ETL dev sense they require very similar skills sets). Talking to your manager about where you want to growth path to go can be a huge starting point. If they suggest certs, then by all means, yes start focusing there. Certifications are usually never a bad thing. But sometimes they require a lot of prep for little gain.
Why dont you just put the SSN numbers into a new table and than join it to the data table you are using. You can then outer join it and get all the results plus the problem with 1000 records in the IN clause will disapear.
This can be done in CASE statement, just add one more layer of CASE: WHERE CASE WHEN @option=1 AND (copy all the first where clause here) THEN 1 WHEN @option=2 AND (copy all the second where clause here) THEN 1 END =1 To avoid parameter sniffing, use local variable instead of the input parameters. Google it. BTW, you use some function in WHERE clause, which is not a good idea for performance, as function tends to do row by row scanning.
If SSN is a number then You could try something like this: select b.ssn, a.* from ACCOUNT A, (select ssn from (SELECT ROWNUM ssn FROM DUAL CONNECT BY ROWNUM &lt; 100000) where ssn &gt;=1 and ssn &lt;=99999) b where a.SSN(+) = lp; &amp;#x200B;
Saw this today. Thought of you (No Fauxmo) https://www.brentozar.com/archive/2017/02/im-just-saying-valid-t-sql-syntax-thats/
Flip the report? Can you explain this. I did actually return horizontal data vertically so that it can be used branch wise but i dont know how to return a table field value as the column name of the said branch so that i dont have to change the branch name everytime another company uses it.
Does not work without the parens, either (both for running the query and asking for the execution plan). Parens were left in because I copy &amp; pasted the query and then changed the sign directions and OR to AND. What is a good resource for learning how to analyze execution plans for performance problems?
I'd personally suggest creating a table driven query builder that you can reuse for any query... but... Dynamic sql with proper indexes will help you target exactly what you need. It's a string so you can add your params as actual values, or use sp_executesql's parameter inputs. Declare @sqlscript_select nvarchar(max) Declare @sqlscript_where_01 nvarchar(max) Declare @sqlscript_where_02 nvarchar(max) If @where_param = 1 Begin Set @sqlscript_select =@sqlscript_select+@sqlscript_where_01 End Else if @where_param = 2 Begin Set @sqlscript_select =@sqlscript_select+@sqlscript_where_02 End Exec sp_executesql @@sqlscript_select 
/u/idi_idi , I'll save you the Google search because I wanted to learn more about parameter sniffing and this is pretty well-written: https://www.brentozar.com/archive/2013/06/the-elephant-and-the-mouse-or-parameter-sniffing-in-sql-server/
Some people mentioned dynamic SQL, well, if performance is not an issue, it is fine. But query plan of a dynamic SQL is not saved well, so for a heavy query, be careful using dynamic SQL.
That can never happen, there is no need to alias your sub query tables if you are referencing them from within the sub query. 
Try Google Data Studio
Ah you're right. I went back and referenced where I read that before. Its when the field doesn't exist in the subquery table it looks outside to the outer query. Not an issue in this case!
The ones that send you passwords arent build appropriately, they obviously have the ability to login with your data.
They are probably also storing the password as clear text! &amp;#x200B;
select geography, product, sum(sales) from table group by geography, product
Check out "WITH ROLLUP" or "GROUP BY ROLLUP". I'm not sure what the syntax would be with Netezza specifically, but I'd look into something like the following pseudocode: SELECT Geography, Product, SUM(Sales) AS 'Sales' FROM Table WITH ROLLUP I use Microsoft SQL Server, so I tried to translate the syntax into something a little more server agnostic, but that should get pointed in the right direction, anyway.
No necessarily, a lot of websites will have a random password generator which will send you the password just before it hashes it and stores into the database. Also, a lot of these sites that do this are doing it as a 1 time use only passwords just so you can log on.
The passwords are not hashed. Either they're stored in plaintext (BAD!) or they're encrypted with a simple method that can be easily decrypted (STILL BAD!). Either way, if at all possible, avoid those sites.
It is possible to retrieve a hashed password and undo the hash. All someone needs is the key for the encryption (which the owner/creator should posses). Computerphile probably has a video related to it.
I agree, this does seem possible.
At this point, it's like you've got a whole workshop of tools available to you but you're trying to build something using only a single screwdriver. Time for a new tool! Make a table to hold the SSNs: create table my_ssns ( ssn integer ); Now take the spreadsheet and get rid of all the columns except for the SSNs. Save this copy as a CSV. Use SQL Developer's Data Import Wizard to put the saved CSV into your new table ([example doc](https://docs.oracle.com/database/121/ADMQS/GUID-7068681A-DC4C-4E09-AC95-6A5590203818.htm#ADMQS0826).) Now do: select ms.ssn ,nv.(a.start_date,'unknown') as start_date from my_ssns ms left join account a on ms.ssn = a.ssn If you want, you can delete the SSNs to get the table ready for the next time you need to do this: truncate table my_ssns; Or even drop it, so that it doesn't show up while it's not needed: drop table my_ssns; Alternate solution: Keep doing the SQL that you're doing, but use this hack to get "IN" lists with more than 1000 items: ... where ssn in (/*1000 ssns*/) or ssn in (/*another 1000 ssns*/) or ssn in (/*another 1000 ssns*/) /*etc*/ Paste the results into a new sheet in the workbook you've been given. In the sheet you've been given, create a new column "start_date" and fill it with: =vlookup([cell in this row with ssn] , [range on new sheet with ssn and start date; be sure to use an absolute range like $A$2:$B$100000] , 2 , false) This vlookup with return an error for ssns that don't have any start_date returned by your query. You can make that prettier by wrapping the formula in "iferror": =iferror(vlookup([etc]),"unknown") or to just leave it blank: =iferror(vlookup([etc]),"") 
This is very useful. Thank you for the link. 
&gt; which I find kind of boring. What do you enjoy about what you do and what specifically don't you like doing? &gt;On the side I'm working with a buddy to get a new project off the ground, and that has me starting to delve into PostgresSQL which is just different enough that I'm spending a lot of time googling again. &gt; &gt;I'd like to primarily keep working in the Microsoft ecosystem, given experience does it make sense to start focusing on certs? It kind of sounds to me like you enjoy learning and being challenged. Your current duties at work don't seem to be challenging or all that difficult and you aren't pushing yourself to learn more. ETL / Data Warehousing is a very large beast and can be an entire career in itself. I would argue there are many amazing things in DW / ETL that you may not have touched yet. If you learn of features that can improve the business, you may be able to make a use case to implement and learn those technologies on the job to apply to the business. This makes their business better and you grow in the process. Some jobs can be quite restrictive, you may have zero say in what your day to day is. I'd say 75% of my work is assigned either through project managers or real time fires that take immediate precedence. The other 25% of the time is what I make of it which I feel like is already decided for me as it's usually just me implementing best practice techniques into the environment. 
Your syntax is fine, "With Rollup" is ANSI standard and Netezza supports it. 
I don't know OpenEdge. The solution below works with the SQL standard. Your syntax might be a little different but I think it'll work in OpenEdge (I only browsed the documentation.) If OpenEdge diverges too far from the standard for something like what's below to work, then god help you :) select a.quality_1 ,a.quality_2 ,sum(a.quantity)/b.group_quantity as quantity_percentage ,count(*)/b.group_count as group_percentage from my_table a join ( select quality_2 ,sum(quantity) as group_quantity ,count(*) as group_count from my_table group by quality_2 ) b on a.quality_2 = b.quality_2 group by a.quality_1 ,a.quality_2 ,b.group_quantity ,b.group_count 
Thanks, I've never heard of the ROLLUP function before. It seems very useful.
``` SELECT Geography,Product, SUM(Sales) AS Sales FROM T GROUP BY Geography,Product UNION SELECT 'Canada', 'Maple Syrup', SUM(Sales) AS Sales FROM T; ```
Yeah, it's pretty handy. I use it a lot in reports to make sure the majority of processing is done before it even gets to the "prettifier".
Encryption and hashing are different things.
Use a CTE: WITH t AS ( select charge, max(elapsed_time) as ElapsedTime from feedback where charge in (select charge from bases) group by charge ) SELECT MIN(elapsed_time) FROM t Same can be achieved with a subquery, that's a matter of preference I think.
 SELECT MIN(...) FROM ( SELECT ... /* your query here */ ) AS x 
Writing this off the top of my head without testing, and I don't do pgsql much, but something like this should be what it looks like you were trying to to: SELECT MIN(t.max_elapsed_time) FROM ( SELECT MAX(elapsed_time) AS max_elapsed_time FROM feedback WHERE charge IN (SELECT charge FROM bases) GROUP BY charge ) t
Is report builder different and not have cast or convert? Or isnull?
That's the most beautiful thing I've seen in my life. 
&gt; What do you enjoy about what you do and what specifically don't you like doing? So a couple months ago I switched jobs (out of necessity, company went under). That's where I wore a lot of different hats and had a lot of different responsibilities/ever shifting priorities. It was chaotic and rough but I enjoyed it mostly. So this new gig I'm doing is super corporate and super rigid, and I'm locked into doing ETL development on new data sources only. They already have everything established they just needed someone to do the grunt work - which is largely in a 3rd party tool that generates ssis packages for you. I have no doubt that I am not even scratching 1% of the potential working in ETL/DW, especially given I did some at my previous gig and that was far more interesting than what I'm doing now. I've received a lot of pushback when I try to learn more about overall DW, and it's a lot of stay in your lane type attitude. I can go entire days without writing any SQL there, and it's really boring. And the mentorship that I was hoping to get that I didn't at the first DB Dev job isn't happening. So I figured I lurk here enough and would ask and formulate next steps to try and take these upcoming months. 
This would fail because you didn't reference the correct fieldname from t.
Yeah 100% use the data import wizard, I get random Excel stuff every once in awhile that I need to weave in with Prod tables. Super easy to do. [http://www.66pacific.com/sql\_server\_import\_from\_excel.aspx](http://www.66pacific.com/sql_server_import_from_excel.aspx). In Crystal you can save the .CSV or .XLS locally or on the network and create a connection to it so you don't even have to import the data which is nice in enterprise environments since you're not adding in random tables but obviously the OP wasn't asking about Crystal.
Ehh. What's the real question?
Group by Grouping Sets would by another solution. Google tells me netteza supports it.
Venn Diagrams worked for me when I was learning 
Second the Venn Diagram analogy. The diagram in this [Stack Overflow answer](https://stackoverflow.com/questions/406294/left-join-vs-left-outer-join-in-sql-server) is great.
Pretty much this. Visually the Venn Diagram is the best way to show what you're querying for. [https://stevestedman.com/wp-content/uploads/joinTypeThumbnail1.png](https://stevestedman.com/wp-content/uploads/joinTypeThumbnail1.png)
https://blog.jooq.org/2016/07/05/say-no-to-venn-diagrams-when-explaining-joins/
Venn Diagram as others have linked, and then just straight examples. Make two tables with like 10 records each and then demonstrate all of the different joins between them. Whiteboard it one record at a time if some extra interactivity would help.
A left and right venn is good for left/ right outer and full, and inner. An up down venn is good for intersect, union (and union all), and except Not sure how to diagram cross join, cross apply and "from table1,table2" And I think that's all the joins. 
What I mean by flipping the report. Maybe rotate is a better verb. What you're proposing is | Boston | New York | Cincinnati | | Sales | 500 | 678 | 255 | | expences | 80 | 67 | 25 | What I'm suggesting: | | sales | expenses | | Boston | 500 | 678 | | Ny | 80 | 67 | You can do the top one using dynamic SQL. It'll be a HUGE pain to maintain and anyone who ever has to touch it will curse your name. It's also going to be absurd to read when you get more branches. My last employer did this and I had to convert it to the bottom one because they got too many branches and it exceed the mass columns for some application that need to consume it. 
Select elapsedtime From feedback f Where elapsedtime &lt;= some ( Select max(elapsedtime) From feedback )
Came here to say fruit rollup.
https://www.reddit.com/r/programming/comments/ad1bo5/say_no_to_venn_diagrams_when_explaining_joins/?st=JRWS85VA&amp;sh=6171c037 First comment, uses a church to demonstrate 
You took the time to downvote me but didn't even fix your query. QA is going to love you. 
That is hilarious 
I first explained the idea of a cross join, using a deck of cards as the cross between [A,2,3,4,5,6,7,8,9,10,J,Q,K] and [Spade,Heart,Diamond,Club]. That's generally a pretty easy one, so I then explain an inner join as a cross join, where we remove rows the don't meet the join condition. That is, the join condition is the filter that indicates whether an item from the cross product is an item in this join set. Next, I introduce Left/Right joins as keeping all the items from left/right that don't have a matching (read: meets the join condition) record in the right/left set, pairing them with NULL values (We don't know what those values are, so NULL). You can take it farther, but I've given some simple demos to QA people a few times, and this seems to click for most people.
What's the confusing part? The idea of joins themselves? Or the different kinds of joins? If it's merely the idea, then maybe a concrete example can help.... Students in your class have taken a standardized test. The results came back but instead of having the students' names, it merely has their ID number and score. Luckily you have another sheet that has their ID number and their name. Given this, you can do an "inner join" where you match up the IDs in both tables... now for those results where you have a match, you can tell the students how well they did on the test. But wait a minute! You have 25 students, but you were only able to give results to 20 of them because these were the only ones that matched. So, which students didn't get scores? Well, if you put the students table on the left and the scores table on the right, then a "left join" will match up the IDs like you did on the inner join, but it will also give you the student IDs that don't have a match. If you then filter those "where score is null", then you have a list of students who didn't have scores. Ahh... these 5 students are the ones that transferred in after the test was taken. But wait a minute, you have 25 sets of scores, but only "used" 20 of them... so what about those extra 5 scores? Well now you can do a right join to get a match where IDs are equal, plus the scores (+ID) that don't have a match in your student table. Filter those where "student name is null" and you have a list of just those IDs. You send that list to your school admin et voila, you find out those are the students that transferred out to another class. 
Really? No one's posted it yet? https://i.redd.it/yku64jkyz9121.jpg
Venn diagrams literally made it click in seconds for me. 
I have this on the partition wall of my cube at work. 
Fuck - so damn hilarious!
Nice - I wish there was a slightly less drab looking version in terms of color and res. Anyone know how to make magic happen or have a more poster print friendly version? Curious how big your version is? Did you just print it out and standard paper size?
Came to make sure. Thanks!
Order by the max you selected in descending order and limit 1.
As someone who's constantly asked this question in interviews and always stumped with a quick answer... I'm going to be using in all interviews going forward.
Honestly, figuring out how to do it is the most fun part for me. I've got two years on this, primarily self taught. Even though I enjoy it, I can't imagine ever not needing to search for the answer or method in some cases, less as I gain experience. It's a skill unto itself. I am happy to help but would love to hear where you got and where you're stuck from the hints I gave you.
There is something in another table that I want/need in my data. A join says, "hey, you and me have the exact same data in one or more areas. BUT, you have more/different types of data than I do. If we both have customer names that match...can we both match on customer names and let me borrow the matching customers address? I need the customers address, but don't have it anywhere in my table? Wanna share info (a.k.a. join)?"
Vlookup
Having been in the intern's position once upon a time - I think just examples on the computer screen are best. Not on a whiteboard, but actual here-it-is on the computer. Start with left joins. 
 Books =&lt;- Editions 1 Alice In Wonderland &lt;----- fk=1 - 1 1865 Printing &lt;----- fk=1 - 2 1870 Printing &lt;----- fk=1 - 3 1910 Printing 2 Gulag Archilpelago &lt;----- fk=2 - 4 1973 Printing &lt;----- fk=2 - 5 1975 Printing &lt;----- fk=2 - 6 1985 Printing 
The union is what you want. 
Is there a way to use in the select function? Bassically I have a huge query that gets these ids then I'm only picking the ids that meet certain criteria. Really don't want to run the same query 4 times and Union it :/
the apartment building, units, tenants, and pets. the office has fish, some people don't have pets, etc
Not sure where i downvoted you? If so, it was unintentional.
It would all be one query.... Select blah blah blah Union Select blah blah blah Union Select yadda yadda yadda You know? 
this is amazingly good... also, i agree with the title of that thread -- **Say NO to Venn Diagrams When Explaining JOINs**
Hmm I think so. Can you actually go. Select id Union Select id2 Union Select id3 From (big query) 
Can you not just use CONCAT() ? CONCAT(id1, id2, id3, id)
Doesn't that just put the results onto the same line? 1,2,3,4 Instead of 1 2 3 4
no, you can't do that can't you loop over the single big query in your application layer to get the same results?
Yeah concat would do that. I misunderstood what you were asking for. Like others have said, I think you just need to use UNION.
You could do a column to row swap in a dynamic table either with numbered unions or a cte. I can post a code example Monday when I'm back in town. I refuse to type code on my cellphone lol
Possibly! Try it out!
No because that'll just be the same table since you aren't unioning two tables. You need multiple from's
Thats very strange. Are you running the EXPLAIN via query analyser? What does SHOW FULL PROCESSLIST say, while the EXPLAIN is running?
No problem pal, If you're just starting out I'd recommend checking out some YouTube tutorials too (WiseOwl have some good SQL ones) that might provide a different perspective too .
what the question fails to clarify is whether you're looking for any part which had more than 300 ordered, or only orders where all parts added up to more than 300 however, since that second interpretation is more difficult, let's go with the first SELECT s.sname , p.pname , sp.qty FROM s INNER JOIN sp ON sp.snum = s.snum INNER JOIN p ON p.pnum = sp.pnum WHERE sp.qty &gt; 300 note a few things -- you should show the qty (even though you know it's more than 300, you still want to see how much more) you need two joins with ON clauses **please don't** (use (*unnecessary* (parentheses))) 
You could just filter using a where clause, item =orange and qty&gt;0 But that will skip those with 0 Of you want to display 0 then you'd have to create a variable table with distinct customers which will help you left join with the original table or using the cte from original query and using a case when qty is null then 0 else qty. 
google sum having select distinct C.CustomerID, Oranges = sum(case when C.Product = 'Orange' then C.Quantity end) from Customers C group by C.CustomerID having sum(case when C.Product = 'Apple' then C.Quantity end) &gt; 0
Hint: Left outer join that table to itself
Not sure we understand the question as posed... Do you mean what’s the advantages of storing data in a SQL database over storing in Excel? Or what can SQL do that Excel?
An Excel workbook is a cluster fuck of user error, where you are shown a final product, and that final product requires you to dig into a variety of tabs, pivots, and calculations to figure out what it's doing. What happened if the user who created it forgot to copy and paste a formula correctly? What if one cell gets moved, or you otherwise "break the file?" Congrats, you now get to rebuilt the whole thing, and now you might be struggling to find out how you produced the number you saw before. SQL lets you build this "final product" using code, which is easily peer reviewed, and which you can easily post to a forum like this for others to see/critique. You are never confused by what it's doing.
Really good explanation, plus SQL has much more functionality that Excel. Think of actually automating the entire process, versus only the calculations of the process. (I know that’s overly-simplified, but it’s a thought.)
You can put the query inside excel with vba and keep it simplistic to get core data and then use excel to perform extra functions for reporting that may cause the query to have long run times or need multiple queries to get the various reports. We do this and also use excel just for formatting the results to be printed as sheets
A join doesn't implicitly include all it's columns in the select statement (unless you've selected all columns with *). Your initial statement can be written more explicitly to be: SELECT iris.species, iris.sepal_width FROM iris JOIN (SELECT species AS spec, AVG(sepal_length) FROM iris GROUP BY species) AS sub ON iris.species=sub.spec If you want to include columns from your sub-query you'll need to either include the explicitly (`SELECT iris.species, iris.sepal_width, sub.* FROM iris ...`), or select everything with `SELECT *`
SELECT Customer, SUM(QTY) TotalFruit FROM example.table.1 GROUP BY Customer
I have the code worked out but can't seem to post at the mo?! &amp;#x200B;
The first video has a cursor.. 
Thank you very for your quick response! I think I see my error now. I thought JOIN was working on (SELECT species, sepal_width FROM iris) and the subquery, but in reality JOIN worked on iris and the subquery. I should've done the following instead: SELECT * FROM ( (SELECT species, sepal_width FROM iris) AS sub_one JOIN (SELECT species AS spec, AVG(sepal_length) FROM iris GROUP BY species) AS sub_two ON sub_one.species=sub_two.spec ) 
Building off of this an SQL database will often be set up on a computer much more powerful than your own. Couple that with an engine that is designed from the ground up to take advantage of this power will make complex calculations much faster. &amp;#x200B; As an example, if you wanted to do an array version of INDEX &amp; MATCH within a few hundred rows the workbooks calculation cycle would slow to a crawl. a SQL database can do that easily without suffering much performance impact across 1000's of rows.
 -- if (CustomerID, Item) is the primary (or unique) key, then you don't need to aggregate. SELECT CI.CustomerID , SUM(CASE WHEN CI.Item = 'Orange' THEN CI.Qty ELSE 0 END) AS Qty FROM CustomersItems CI WHERE CI.CustomerID IN (SELECT I.CustomerID FROM CustomersItems I WHERE I.Item = 'Apple') GROUP BY CI.CustomerID; &amp;#x200B;
You still don't understand, since you just wrote the same query as your second "working" one, but with additional clutter.
This is the same as your first query actually, there's no difference apart from the selected columns. There is really no point in `sub_one` being a subquery. 
&gt; You are never confused by what it's doing. exactly. 
You could use unnest( concat( id1, id2) ) if you want the results in separate rows.
The actual question you are asking is largely, "What is the difference between a spreadsheet and a database?" If you look into those things, you'll get better answers when you're doing your own research. In a nutshell, it's a matter of scope. Excel/spreadsheets is good at being a container for a report that might have some degree of displaying data involved and requires little start time to get going. Databases excel when there is a lot of data, there is relationship between data that is otherwise difficult to represent in a flat file format (such as many-to-many relationships), or there is a lot of concurrency (which is to say, many users operating on the same database; even something very lightweight such as a SQLite database backend can power hundreds of thousands of concurrent users for a website which is something that would be difficult to manage with an Excel/spreadsheet backend.
I agree there is no point in writing it that way. I was just trying to illustrate how I thought it worked.
Any enterprise grade DB will out-data-crunch Excel to a ridiculous extent. Tens of millions of rows spread across multiple tables? Not a problem with PostgreSQL, SQL Server, etc. Even Access can handle millions of rows with relative ease... assuming the db is on your C drive of course. In Excel? Probably impossible. And if you do get it to work, it won't finish for hours or days. They both have their uses though. It's hard to beat Excel for digging around reasonably sized data sets and trying to figure out what you've got, how to work with it, how to present it, etc. 
I'm not at all sure what they have in common other than rows and columns.
Downvoting for an extremely unclear question. 
https://www.humblebundle.com/books/ Don't click the link
I cant imagine trying to use a workstations as a prod machine. What kind of terrible practice is that? computer must always be on network and available. It also means your sharing its CPU and memory with others while also using those resources for all your other tasks. You really don't install a SQL server on a personal machine unless it's for dev/testing purposes. 
Touched a nerve?
from my understanding truncate only removes the existing data, so then i should do a truncate in the first line and then an insert statement following it? 
What monster is hosting DBs on their C drive ??????
Could it be a port problem? I'm no expert, but your php page is probably connecting via port 80 like all http traffic. The database requests could be getting forwarded to the proper DB port.
I don’t see any parent child relations in your queries. I only see peer tables. 
&gt;Even Access can handle millions of rows with relative ease... assuming the db is on your C drive of course. 
Not really. I just think it's bad advice that's going to cause OP more headaches than solutions. Especially when they can aquire a more durable solution for next to nothing. And the advice I offered was by no means dismissable just because you technically can make it work using a duct tape type of approach. We shouldn't be offering poor advice or dismissing good advice just to bolster our egos. I don't think theres any place for that here. 
I'll have to give this a try! 
That would be great thanks! 
Are these digital or paperback books?
Digital
I'm really bored... Are vacations supposed to be boring? SELECT (CASE dup.idtype WHEN 1 THEN mydata.id1 WHEN 2 THEN mydata.id2 END) AS 'id' FROM mydata LEFT JOIN ( SELECT 1 AS 'idtype' UNION SELECT 2 UNION SELECT 3 UNION SELECT 4 ) dup You basically have four rows of forced duplication and case on the duplicate ID to choose which value to display from the main query without repeating the main query itself. From experience this is faster than relying on query cache and it looks nicer. You can also do extra stuff with the data this way like duping an extra row for a totals row. 
Depends on where the vacation is! Haha Thanks for the right up. I think I get it... Once I'm at work next week I'll give it a shot! Still rather new to sql so when queries get this big I get rather lost in how to apply these techniques properly. 
You are correct. I do this with a working table nightly for a few tasks to run off of, instead the production tables. 
Yeah, it was me misinterpreting the structure of the query. I thought I had JOIN between `SELECT species, sepal_width FROM iris` and `SELECT species AS spec, AVG(sepal_length) FROM iris GROUP BY species` whereas in reality it was between `iris` and `SELECT species AS spec, AVG(sepal_length) FROM iris GROUP BY species`, and `SELECT species, sepal_width FROM` was applied to the result of the inner join. It will probably sound silly to those experienced in SQL, but I kinda wish there were { } like in C/js or colons with indentations like in Python to make it easier to interpret SQL code.
Thank you! This was a big help
http://hyperboleandahalf.blogspot.com/2010/04/alot-is-better-than-you-at-everything.html
Just make sure you handle the duplication in any other aggregate functions. A freezing cold beach. 
Something like this? ;WITH vals AS ( SELECT * FROM (VALUES ('1', '2', '3', '4')) AS v(id1, id2, id3, id4) ) SELECT x.* FROM vals v cross apply (VALUES (V.id1), (v.id2), (v.id3), (v.id4)) AS x
This worked! Thank you!!
This is interesting. Where would you actually place it though?
Did someone mention these were outdated last time? 
I have never seen 5TB of data stored in excel, have you?
That summing thing is throwing me off. Maybe it's just early for me, but that feels more like a logic puzzle than a SQL knowledge question.
I feel like the question meant to say SUM(x) and not COUNT(x).... But it can't be 20 since 3 more values got added, 2 of those 3 got removed, and the initial sum is 20. It's a really dumb question but I'd answer it as (B) and not look back
I agree. Because the COUNT isn’t counting distinct values, then hypothetically there could be an infinite number of values equal to zero. Each of these would be counted as one in the COUNT function. My answer would be any of the above. 
Assuming COUNT is correct, and that INT can be positive or negative, or 0, then D is the answer since any number of records can SUM to 20. Serializable just means that each transaction happens in the order they are committed, or at least applied in that order.
I'm pretty sure it's E. Could be any number of values because you can have negatives as ints. 
According to my solutions C is the correct answer. 
You are probably correct, here is a similar question on StackOverflow: https://stackoverflow.com/questions/42334081/understanding-transactions-in-sql
But the initial sum is 20. It adds a positive number. Based on the answer being C from another post, I'm guessing they just want to to recognize that the 20 can't decrease...
Right, but I could have a bunch of numbers that sum to -580 and then +600. 
But the question says that the sum of the numbers is 20 and that there are no values equal to 3, 6, or 9. So the initial sum of table R is 20. If there are negatives in there, it is irrelevant for the problem
Taking "thinking with portals" to the next level.
It's not irrelevant, you're tricking yourself by assuming it must be sum(x). It's asking for a count. There could be an infinite number of transactions in there that add up to 20.
We are just going to have to agree to disagree. I have tried your solution and had some issues that were deal breakers for me. 1. MS Access is terrible if your database is going to be anywhere near 2gb of data 2. MS Access does not allow pass-thru querying. So if anyone is using VPN to remotely connect to your company network it is going to take a year and a day to execute a simple query. I agree that Access has benefits for people new to SQL but in most business cases it is not a good long term solution. What was I basing my advice on: I have spent &gt;24 years at a fortune 500 company. While this company was large it was very hard to get IT projects done. You needed to convince finance there is enough value in what you are proposing to get the project funded. To do this, in many cases, you need to create a very low budget POC. **I contacted our Microsoft rep and asked for his advice because Access would not work for us. He listened to what we were trying to do and suggested exactly what I proposed above. Note, this was \~15 years ago. Have a dedicated, high power desktop that was always connected to the network to run your SQL Server Express. The workstation was not to be used for other things.** It was locked in a small room connected to the network. Build your application and prove the value to the organization so that you can get real hardware to support it and expand. Fast forward 15 years and I manage 6 VM servers running Sql Server and have 70+ applications that hit those servers. **I would say that Microsoft gave me some good advice.** The user who wrote the original question admits that he is only trying to give \~5 others access to the database. That certainly doesn't warrant a large hardware or IT investment. Yes, the user can make Access work. But if he ever wants to scale up or support people who aren't in the same location he had better think hard about going a different route. I do not think that your advice of using AWS was bad advice. I was just pointing out that, from my experience, it is not at all hard to install Sql Server on a dedicated workstation and allow others to access over the company network. You wouldn't want to give 100s of users access to it but you wouldn't want to give 100s of users access to an MS Access database either. I have used both Access and each version of Sql Server. From my &gt;24 years experience, I will always give people the advice to use Sql Server Express over Ms Access. I am not alone either, there are many Sql bloggers / experts who regularly give the same advice. With that, I wish you well. I certainly didn't mean to offend you. I apologize if I did.
Yes, it adds up to 20. So the sum is 20. It adds three more numbers in there, 3, 6, 9. It removes 3 and 6. I don't understand what you're saying
\-1 + -1 + 22 = 20. 0+0+0+0+20=20 You could have an infinite number of counts that sum to 20. None of them have to contain 3, 6 or 9.
my CTE only simulates your situation. You could have your whole complex query instead of the CTE. If your problem is converting columns to rows, that's a job for UNPIVOT and this usage of CROSS APPLY with VALUES can do the same. Try to apply it to your situation and see if it helps.
Yes. But how does what you first make sense? it doesn't matter what the other numbers in R is. It is irrelevant. &gt;I'm pretty sure it's E. Could be any number of values because you can have negatives as ints.
Got some bad news for you, mate. SELECT * FROM Bikers WHERE Biker.Club = 'Hells Angels'; &gt; **Query Error:** Error: ER_NO_SUCH_TABLE: Table 'test.Bikers' doesn't exist You either learn to alias tables and use aliases correctly, or you die to a gang of bikers.
lmao... that's where I woke up. Query failed.
Tutorials right before bedtime, huh?
lol yah I was watching vids.
First_value! Select distinct businessentityid, first_value(rate) over (partition by businessentityid order by ratechangedate desc) From humanresources.employeepayhistory
I feel like this would have helped your nightmare 😭🤣 Delete * from Bikers as Biker with(NoLock) Where Biker.Club = 'Hells Angels'
You can set a row number. Partitioning it by the persons id and ordering it by the date descending. Then select from that where the row number = 1. Look like this roughly. Select st.employeeid, st.payrate From (Select employeeid, payrate, row_number() over ((partition by employeeid, order by paydate desc) as rownum From paytable) st Where st.rownum = 1 Yeah that should do it. Inner query will give you employeeid, payrate, and rownum. Then you’re grabbing the top result for each employee in the outer query. Adjust this to your tables and field names. Hope this helps. 
It starts with an "S" instead of an "E", and "S" is clearly superior? Your question needs more detail to have any actually useful answers. It's the equivalent of asking "what is the advantage of a banana over a bicycle?" Considering you can embed a SQL query in an Excel workbook, now what?
Pics?
Sounds like you haven’t added your windows user as an authorized user to your database server. Login to your new server using sa and make sure your allowing windows authentication. Then make sure the windows user you are using in ssms is allowed to connect to the sql server you created. It was a step in the setup process that asks you what accounts to add from windows auth
Try to reinstall. This article has all the link to download SQL server and studio. http://www.straightforwardsql.com/database-playground/ 
It has a name. https://en.wikipedia.org/wiki/Tetris_effect
I feel like DBAs experience the real life equivalent of SQL nightmares. Like...a-hole devs nuking a production database and them having to race against the clock to fix it during primetime business hours.
At work now. Will post pics when i get home
Thank you for the response. I am not sure how to log in as sa - tried googling and following instructions but they all seem to predicate on being able to access the database in Object Explorer in the first place.
When you created the server it would have asked if you want windows authentication only or mixed mode authentication. If you picked mixed mode it would have asked you for a password for this sql authentication account for sa. If you didn’t it would have made you pick a login to setup in windows authentication. If you can’t remember either of those it’s likely best you re-setup the server
Fresh install in a newly created vm development environment. Do everything from scratch with all fresh installs. I did that for "clean work" and messed with installs in my own environment for self learning and optimization or trying different setups. 
Whats the best way to make sure you uninstall everything? I was reading about it and ppl were saying if you dont do it right it wont let you reinstall after
And depending on what you're going to use it for, look at the Express edition. It lacks some features but is free to use in a non-Production environment (that is, you can't make money from it), if I recall the terms correctly.
Don't uninstall. Create a new vm, install windows server, install SQL, install any production software you need, then open the ports and keep the vm running with network bridges and connect to it from your I deans application as is its another machine. 
I would have zero idea where to start with that tbh
Thanks - considered Express but I think that I will blow past 10GB very quickly.
Thank you! I'll have a read up on cte's then give it a shot! 
HAVING is a GROUP BY filter, WHERE is a row filter.
Someone else has locks on the rows you want and your statement succeeds, but deletes no rows.
Start a loop. Use the Modelo function against the loop count. print the loop number if input mod loop number equals zero. Increase your loop count by one. Exit the loop when loop count equals input
[https://imgur.com/8vJ7Oz1](https://imgur.com/8vJ7Oz1) &amp;#x200B; [https://imgur.com/a/eoVwnYH](https://imgur.com/a/eoVwnYH)
What is the Modelo function? &amp;#x200B; This is what I did so far, I think I did it right &amp;#x200B; DECLARE counter NUMBER(2) :=1; input NUMBER; BEGIN input :=\&amp;input; WHILE MOD (input, counter) = 0 LOOP DBMS\_OUTPUT.PUT\_LINE(counter); counter := counter+1; EXIT WHEN counter = input; END LOOP; END;
Yeah I think you need to try a fresh install. 
Season two of Westworld anyone? 
I would suggest Pluralsight, if you’re willing to pay for college classes just save the money and pay for Pluralsight. 
Also tried this but it just displays every number up until the input number &amp;#x200B; DECLARE counter NUMBER(2) :=1; input NUMBER; BEGIN input :=\&amp;input; IF MOD (input, counter) = 0 THEN LOOP DBMS\_OUTPUT.PUT\_LINE(counter); counter := counter +1; EXIT WHEN counter &gt; input; END LOOP; END IF; END;
So you simply look at it in different ways that it can execute. Also assuming the count is supposed to be a sum. I - Insert query S - Select query D - Delete query 1. S runs first. Sum remains 20 and B is not an option. 1. I -&gt; D -&gt; S. Sum is 29 since 9 was added. D is not an option 1. D -&gt; I -&gt; S. Sum is 38 since 18 was added. A is not an option. 1. E is not an option since any of the three above are not possible 1. C is the only answer left.
This would make for a good SQL game, bikers attacking your building and you have to write queries to stop them. 
Assuming that COUNT was meant to be SUM (which is a valid assumption, as the question doesn't make much sense with COUNT): C is indeed the correct answer. Each commit block can be ran in any order with regards to the other blocks. The only thing you know, is that they won't "overlap" - i.e. sum won't be selected after only one of the value was inserted. 11 will never be returned in SELECT, since you know that at the start the SUM is 20 and there are no records containing 3 or 6. So even if the DELETE statements were executed before SELECT and INSERT statements (which is possible scenario), they would not really delete anything and the SUM would remain unaffected. There is no way the SUM will ever get under 20 in this example. All the other results are possible by some combination of select,insert and deletes blocks and E is incorrect, since 11 is not a possible result.
A relation doesn't have duplicates. 
There's no such thing as a duplicate in a relation. ab(R) is 1,2 and 2,2, and a,c(T) is 3,2 and 2,2 and 1,2. The double ups are ignore, so it's 2*3 = 6.
https://imgur.com/a/iyPtusz are you able to explain these ones? i'm struggling to understand the joins
This is terrible advice. You are telling people to flush the plan cache instance wide without any warnings as to what that actually does and the impact it could have.
Yeah, don't do that. If tempdb is to large, either optimize your queries or add resources to the machine. 
Or, how to ruin the day for everyone depending upon your instance.
Natural join means join on matching column name. A,C and A,D would join on A.
Write your query and add a row_number() over (partition by thev fields that make a row unique order by id) as RowNum. Wrap the whole thing with WITH CTE AS(), then select from CTE where RowNum = 1.
The answer will have an extra step (and layer of difficulty) I you want to format the results in columns as shown. Otherwise you will need to use the SUM() function along with a GROUP BY clause. I'm on mobile so bear with me but it should look something like SELECT Code, SUM([Ext. Cost]) FROM MyTable GROUP BY Code
&gt;Thanks very much! I didn't know this syntax was possible.
That’s a good option and I’ve tried that in the past! The problem is that when I already have a CTE going I would rather not add another, which is why I’m asking about doing it within a WHERE clause. 
Thanks. I did look at the SUM / GROUP BY, but there are further considerations that I didn't include for simplicity. I really need the output to look as shown in the question. Further considerations if anyone is interested: There's another column that is a part number. Each part number will have multiple codes associated with it, and each with their own qty/cost combinations. So the final output will look like: |Part\_no |Total Ext. Cost for CodeA | Total Ext. Cost for CodeB | Total Ext. Cost for CodeC I don't believe the SUM / GROUP BY will solve this larger issue.
You want a cross tab query. SELECT SUM(CASE WHEN Code = 'A' THEN [Ext. Cost] ELSE 0 END [Total A], SUM(CASE WHEN Code = 'B' THEN [Ext. Cost] ELSE 0 END [Total B], SUM(CASE WHEN Code = 'C' THEN [Ext. Cost] ELSE 0 END [Total C] FROM MyTable;
Then you need to look into PIVOT.
woot woot! That looks like what I want. Thank you very much. 
mildly theoretical stuff below, you can skip to "answer here" IMO, granularity - to what level of some hierarchy or some dimensions your data is attached to - is one of the better "soft" concepts to grasp when you're just learning SQL. This is somewhat different from uniqueness, as normally you wouldn't consider measurements a part of your granularity. Granularity is also somewhat different from uniqueness because it goes by distinct values (i.e. granularity allows NULLs, technically). Granularity is different from a key also while it tells you that you cannot distinguish records beyond certain level, it does not explicitly guarantee that the records do not have duplicate sets of grain columns values (you'd need a constraint - a key or uniqueness to enforce that). "GROUP BY" (with a list of columns/expressions) is generally, a way to explicitly set granularity of the output to those columns/expressions. Since there could have been multiple records in your source for a single "value" (set of values) of your new granularity, you need to provide a way to aggregate ("collapse") those record values down to your granularity. This is done via aggregate functions - you probably can tie these two (aggregate functions and group by) and realize that aggregate functions will work across all records with the same output granularity. GROUP BY has a quirk where it is not specified when you are finding a grand total, i.e collapsing your data set down to a single record. SQL detects it by seeing only aggregate functions in the output list. A good thing to learn is also the "CASE" expression. A basic use is like this "CASE WHEN &lt;condition1&gt; then &lt;value1&gt; WHEN &lt;condition2&gt; then value2 ... END" and it will return the value from the first pair where the condition is true. &amp;nbsp; answer here E.g. "Total C" is just a sum (aggregation) of Ext. Cost for the records where code is C. You can write it so: select sum( case when code = 'C' then "Ext. Cost" end) as "Total C" from "atablelikeso" You can add other totals in the same manner.
No problem
Ya, I thought that was what I wanted to. Looks like /u/jgs84 was able to solve the issue in a very simple way. 
Thank you. That was informative.
Don't add another. Add the row number function to your existing select and add the AND RowNum = 1 to the existing select. 
Thank you! I'll look into that right away :) 
Ahhhh, got you! Good workaround. Will give it a try. 
Note: no comma between partition by and order by, so it would be: Select st.employeeid, st.payrate From ( Select employeeid, payrate, row_number() over (partition by employeeid order by paydate desc) as rownum From paytable ) st Where st.rownum = 1 If your were partitioning or ordering by multiple columns, there would need to be commas between them.
&gt; the info in the fields are all the same, at least the info I need if this were really true, then DISTINCT is the correct solution can't be true therefore, since you claim DISTINCT doesn't do what you want
This sub has some good links if I remember right. I like codeacademy (free trial) and then sqlzoo
&gt; what i want from this query is the name of the task that the search belongs to which column, in which table, contains the name of the task? 
The name of the task = *Tasks.name*
So maybe I’m using DISTINCT wrong then. I guess I’ll have to read up on it more. 
Thank you, but I can only use commands like SELECT, WHERE, HAVING, GROUP BY and the aggregates, MAX, COUNT, etc.. They are not expecting us to know partitions. Is there a way to do it only using the above commands? Basically I want to show the employeeID and their rate, based on the most recent RateChangeDate.
Thank you, but I can only use commands like SELECT, WHERE, HAVING, GROUP BY and the aggregates, MAX, COUNT, etc.. They are not expecting us to know partitions. Is there a way to do it only using the above commands? Basically I want to show the employeeID and their rate, based on the most recent RateChangeDate.
I'm not clear on what your desired output is. From this example why would joining on task_id not work? 
I solved it. 
Need some data/tables as an example. Please.
See above. Edited. In that example, the highest pay is not necessarily the most recent (example Employee #4).
Rename the table as [Database].[Schema].[Table]
Can you OrderBy RateChangeDate desc? Or assuming their Rate doesn't go down, GroupBy employeeId and take Max Rate?
you need a 3rd table the Team table should just be the team name, other team descriptive data, e.g. location membership of a player on a team goes into the 3rd table, often called a relationship or many-to-many or junction table the PK is the two FKs combined, and the table can also carry attribute data, in this case it would be position (and yes you can use GK1 and GK2 as separate position codes if you want to designate starting keeper and backup)
Here's one solution, not the best performance wise but it should do the trick for you. &amp;#x200B; SELECT EmployeeID, Rate, RateChangeDate FROM table_a AS a JOIN ( SELECT EmployeeID, MAX(RateChangeDate) AS MaxDate FROM table_a GROUP BY EmployeeID ) AS b ON a.EmployeeID = b.EmployeeID AND a.RateChangeDate = b.MaxDate &amp;#x200B;
We can't assume their rate doesn't go down, else could just do MAX(Rate). Using ORDER BY will show their whole pay history, I just want to show their most recent rate and the date it took effect.
if you're including data you don't need in your results, then probably those columns have differing data.
You're a god. That worked.
Ok cool, so you're saying have something like a Team-Player table, that uses the TeamId and PlayerId as a composite key? I'll try and lay out some schema and understand this. Thanks!
I'm actually also interested in doing essentially the same thing, but don't have any programming / SQL experience, so curious as to where to start. An alternative (and maybe similar) project I'm thinking that would be useful is a web-scraping tool for fundraising / non-profit donors in the particular field I'm in, so that my non-profit might be able to generate a comprehensive list of donors / contacts to reach out to.
So, you didn't mention your database, and /u/jgs84's answer is the way to go if you're using MySQL for example. If you're using SQL Server, you could use `PIVOT`, like so: SELECT A AS TotalA , B AS TotalB , C AS TotalC FROM (SELECT code, extcost FROM tbl) t PIVOT (SUM(extcost) FOR code IN (a, b, c)) pvt [Working example](https://rextester.com/CPLH38911)
hey, tek-tips... i remember them... too bad that article is 404... here it is on the Wayback Machine™ -- https://web.archive.org/web/20180311083205/http://r937.com/relational.html whatever you do, please **do not** name your tables with the "tbl" prefix pls post your schema when you're ready
db link
In general if you need to convert rows to known amount of columns use PIVOT clause.
Good luck mate and check out the free Microsoft courses and CBT Nuggets
Is the last field in the select missing a field name and as such error is the last field before FROM?
yeah my database is nuneedua and the table where i pulling it from is called datamining &amp;#x200B; so would it be from datamining.Zipcode\_info &amp;#x200B; ??
my database is nuneedua and the tables i need are from datamining &amp;#x200B; CREATE VIEW v1\_Q2 as SELECT State, SUM(EstimatedPopulation) as Population, COUNT(ZipCode) as NumZipCode FROM dataming.Zipcode\_info WHERE ZipcodeType='STANDARD' AND LocationType='PRIMARY' AND EstimatedPopulation IS NOT NULL GROUP BY State; ???
I'm not sure what flavor of SQL you are using, but (MS) SQL Server uses a [four-part naming convention](https://www.mssqltips.com/sqlservertip/1095/sql-server-four-part-naming/) to allow you to query objects that might not be on the same database or even on the same server. The structure is \[server\\instance\].\[database\].\[schema\].\[object\]. I believe the engine does this under the covers for you by implicitly using all four parts even if you only explicitly state the right-most one or two or three. To query a database table from the context of another database that's on the same server use \[database\].\[schema\].\[Table\]. To query a database on another server use \[server\].\[database\].\[schema\].\[database\] but you will need a Linked Server object to handle authentication. SQL Server will not pass your Kerberos credentials on to a remote server further than one hop. You wouldn't need a Linked Server if you were using SSMS directly from an RDP session to the initial SQL Server, but if you SSMS'd into it from another remote box, it will throw a failure on the remote server if you deny NT AUTHORITY\\Anonymous. &amp;#x200B;
Do you mean the " --as "average" or something else? If you mean the " --as "average", I wondered about that too but get the same error if I remove it altogether. 
Without looking at what you're actually talking about, its a little hard. But it should be possible. Could you hard code the select statement into the package as a cursor? 
Sounds like nuneedua.datamining.Zipcode_info, assuming that datamining is the schema.
I wouldn't focus on wintac specifically, but more the fundamentals of databases. If it's mostly queries and not design, focus on joins, windowing functions, and aggregate functions. Sounds like Excel is just acting as a front end, get the manipulation done with SQL before loading. Maybe research ETLs, but that might be over kill in this case. I'd be curious what the backend of wintac looks like, and what they allow access to.
You need a space between the -- and 'as' on line 35.
I have the same question for oracle. I'd like to use a package and get the 2nd output variable in the select result. I'm just posting here as a sort of subscription :D
Packages are mainly used as collections/encapsulation of procedures and functions, like a Class in .NET. You’d have to write a function or procedure inside each package that “validates it” like selecting a hard-coded value from dual or something. Then you can write an anonymous block to call the “validate” function or procedure within each package and confirm its validity. You can then deploy that anonymous code to another proc or package to be auto-running. But if you want to verify the validity, why not just use: SELECT * FROM DBA_OBJECTS Where status &lt;&gt; ‘VALID’
Lol whoops, hard to type this on a phone. Thanks for the correction 
So, this is stupid but here's what the problem ended up being: `+nvl(cf$_turn_times,0))` `/` `sum(` When I changed the code to this - really I was just dicking around - it worked: `+nvl(cf$_turn_times,0))/sum(` So, something about having the / and SUM separated from the rest of the query - which I only do to make the code more readable for me - was causing the issue. Thanks for your help everyone!
&gt;You’d have to write a function or procedure inside each package that “validates it” like selecting a hard-coded value from dual or something. &gt; &gt;Then you can write an anonymous block to call the “validate” function or procedure within each package and confirm its validity. &gt; &gt;You can then deploy that anonymous code to another proc or package to be auto-running. This sounds promising. I think part of the issue is that the package is designed to performing numerous checks on a single ID, and I'm interesting in the outcome of those checks on a selection of IDs (around 40,000) based on logic from some tables in the database. Would that still be possible with the method you're proposing? &gt;But if you want to verify the validity, why not just use: &gt; &gt;SELECT \* FROM DBA\_OBJECTS Where status &lt;&gt; ‘VALID’ I think we're using the word "validate" differently. What I mean is to validate the outcome of the checks in the package in relation to the functional business process that it's tied to, not the technical validity.
Fair enough. Validation of the logic itself and not the status. Yeah, maybe you can just write the output of what I proposed to a table and then throw it away once it’s verified? How often are you wanting to perform the validation?
Thank you so much! I really appreciate it. Nervous as hell. Haven't futzed in years, goodness gracious. If I can get a peep, I'd come back and chirp about it.
Assuming sub queries are OK, I would do this: &amp;#x200B; SELECT A.BusinessEntityID, A.Rate FROM HumanResources.EmployeePayHistory AS A INNER JOIN ( SELECT BusinessEntityID, Max(RateChangeDate) AS LatestDate FROM HumanResources.EmployeePayHistory GROUP BY BusinessEntityID ) AS B ON A.BusinessEntityID = B.BusinessEntityID AND A.RateChangeDate = B.LatestDate &amp;#x200B; Your sub query is giving you the latest date per ID and you're then joining that back onto the original table to get the actual rate for that date.
The progress stays at 0.000 and the State is says “Statistics”. This is true for examining the process list whether or not I have the EXPLAIN in my query. 
Awesome! Glad it worked for you!
Maybe you can change compatibility level before doing a backup ? But il you use something not compatible in 2014, you are screwed. Can you install express 2016 ? I suppose you need the data ?
Use a having clause, after your group by use: Having count(*) &gt; 1 This will filter your dataset to only those id's with a count higher than one 
Or if you dont want records filtered out change your count in the first query to a window function: Count(*) over(partition by id)
Ah this is great thank you. I never use having in my queries so this would give me good experience, thank you. 
so what I would do is a subquery and an IIF (immediate IF) would work better then a case. &amp;#x200B; `Select id, sum([Delived Naive]), count(*) from` `(select id, iif([case outcome] = 'Order Recieved' AND [Patient Type] = 'Naive', 1, 0) as [Delived Naive] from cases) a` `group by id`
What version? select @@VERSION; What is this value set to? show variables like 'optimizer_search_depth'; 
Try pluralsight
You can't restore a database backup to an older version of SQL Server. You might be able to script all of the objects out of a recent version of SQL Server, then build those objects in an older version and then copy all of the data using SSID or BCP. This would be a lot of work for anything but the simplest databases. 
Sign me up!
Do that case AS some_alias then do a sum of some_alias. 
Bleh, MySQL doesn't care about that, sucks that Oracle is more picky. teach yourself better formatting standards that you just type in the formatted manner and you'll never notice those annoyances. This is how I formatted the query before giving the first answer: SELECT cf$_vendor_no, cf$_party, cf$_environmental, cf$_inspections, cf$_invoice_process, cf$_ncr, cf$_on_time_delivery, cf$_qms, cf$_safety, cf$_schedule, cf$_scope_of_work, cf$_turn_times, ( SUM( nvl(cf$_environmental,0) + nvl(cf$_inspections,0) + nvl(cf$_invoice_process,0) + nvl(cf$_ncr,0) + nvl(cf$_on_time_delivery,0) + nvl(cf$_qms,0) + nvl(cf$_safety,0) + nvl(cf$_schedule,0) + nvl(cf$_scope_of_work,0) + nvl(cf$_turn_times,0) ) / SUM( IF(ISNULL(cf$_environmental),0,1) + IF(ISNULL(cf$_inspections),0,1) + IF(ISNULL(cf$_invoice_process),0,1) + IF(ISNULL(cf$_ncr),0,1) + IF(ISNULL(cf$_on_time_delivery),0,1) + IF(ISNULL(cf$_qms),0,1) + IF(ISNULL(cf$_safety),0,1) + IF(ISNULL(cf$_schedule),0,1) + IF(ISNULL(cf$_scope_of_work),0,1) + IF(ISNULL(cf$_turn_times),0,1) ) ) AS 'Average' FROM supplier_scorecard_clv GROUP BY cf$_vendor_no, cf$_party, cf$_environmental, cf$_inspections, cf$_invoice_process, cf$_ncr, cf$_on_time_delivery, cf$_qms, cf$_safety, cf$_schedule, cf$_scope_of_work, cf$_turn_times
Tempting... though there's a certain industry I can't do this for due to conflict of interest 😂
Interested!
Interested
interested
The company you worked for was the problem. Your solution might have gotten you through your particular situation but lets not assume that's the norm. There are standard approaches to things which should be the first recommendation you make. If your situation makes those unfeasible then you can explore options such as what you've done. But putting someone new to all this down those routes off the bat is going to set them up for failure.
Interested!
A backup taken on SQL Server 2016 cannot be restored to a lower version. You can create a DACPAC (data-tier application) which should work. You'd probably be better off just installing an instance of 2016 to restore your clients' backups into. But why are you shipping entire databases back and forth, instead of sending change scripts back to the client?
Interested, I want to know more, please
Interested 
Interested and sent you a message with my details
If I'm understanding correctly, you want to be able to call your package from within a select and get it's output in the select list. As long as what you have is a function that returns a standard type (character, number, or date) then you just need to add a "pragma restrict_references (function name, WNDS)" to the spec. Then you can just do something like "select my_package.my_id_function (column1) from table1 where..." You can't do this if your package does any kind of data manipulation (updates, inserts, alters, etc). This is assuming Oracle as the database. If I'm completely misunderstanding, my apologies.
select u.r867u5n309 from reddit where user_experience = 'has experience' and user_dedication = 'will meet deadlines'
 SET @MyNumber = 50; SELECT @MyNumber, n.nums, MOD(@MyNumber,n.nums), @MyNumber % n.nums FROM ( SELECT CONCAT(n0.num,n1.num,n2.num,n3.num)+1 AS nums -- +1 to make it a number and start at 1 instead of 0 (+0 to start at 0) -- 0-9 FROM (SELECT 0 AS num UNION SELECT 1 UNION SELECT 2 UNION SELECT 3 UNION SELECT 4 UNION SELECT 5 UNION SELECT 6 UNION SELECT 7 UNION SELECT 8 UNION SELECT 9) n0 -- 10-99 JOIN (SELECT 0 AS num UNION SELECT 1 UNION SELECT 2 UNION SELECT 3 UNION SELECT 4 UNION SELECT 5 UNION SELECT 6 UNION SELECT 7 UNION SELECT 8 UNION SELECT 9) n1 -- 100-999 JOIN (SELECT 0 AS num UNION SELECT 1 UNION SELECT 2 UNION SELECT 3 UNION SELECT 4 UNION SELECT 5 UNION SELECT 6 UNION SELECT 7 UNION SELECT 8 UNION SELECT 9) n2 -- 1000-9999 JOIN (SELECT 0 AS num UNION SELECT 1 UNION SELECT 2 UNION SELECT 3 UNION SELECT 4 UNION SELECT 5 UNION SELECT 6 UNION SELECT 7 UNION SELECT 8 UNION SELECT 9) n3 )n ORDER BY n.nums This will show you how it works, then you can add a WHERE clause to it like WHERE MOD(@MyNumber,n.nums) = 0
Wiseowl SQL youtube
Interested!
I'd like to know more, please.
W3schools.com is how I got started. Learned on the job through that website as a matter of fact and have been promoted twice. Served me good.
I’d have to say these were a great combo in my SQL class last year: [SQL Queries For Mere Mortals](https://www.amazon.com/gp/aw/d/0134858336/ref=dp_ob_neva_mobile) and [Murach’s MySQL](https://www.amazon.com/Murachs-MySQL-2nd-Joel-Murach/dp/1890774820/ref=mp_s_a_1_1?ie=UTF8&amp;qid=1549935577&amp;sr=8-1&amp;pi=AC_SX236_SY340_QL65&amp;keywords=murachs+mysql+2nd+edition&amp;dpPl=1&amp;dpID=51QEIPmobcL&amp;ref=plSrch) Jon Duckett also has a MySQL &amp; PHP book coming out next month that I’m looking forward to. Loved his books on HTML, CSS and Javascript/JQuery [PHP &amp; MySQL: Server Side Web Development](https://www.amazon.com/PHP-MySQL-Server-side-Web-Development/dp/1119149223/ref=mp_s_a_1_2?ie=UTF8&amp;qid=1549935706&amp;sr=8-2&amp;pi=AC_SX236_SY340_QL65&amp;keywords=jon+duckett&amp;dpPl=1&amp;dpID=31tYCd%2BGxIL&amp;ref=plSrch) 
PL/SQL version: DECLARE counter NUMBER(2) :=1; input NUMBER; BEGIN input := &amp;input; LOOP IF MOD(input, counter) = 0 THEN DBMS_OUTPUT.PUT_LINE(counter); END IF; counter := counter +1; EXIT WHEN counter &gt; input; END LOOP; END; Man, haven't touched that since college, lol, had to make a new Oracle LiveSQL account to test it.
Just wanted to say this worked perfectly! Thanks for the help. 
Surprised no one has given gold yet. That's almost as good as bending the edge of a prize entry form. Lol
Joes2pros is great for beginners
Dm me please
Interested! Let me know!
Interested here as well
PM'd.
Interested!
Interested 
Interested as well
Wut
TSQL Fundamentals (focused on SQL Server but is a solid introduction to SQL)
I would check our Learn SQL the Hard Way. https://learncodethehardway.org/sql/ Best of luck!
Version is 10.3.11-MariaDB-log Optimized search depth is 62 All data tables are InnoDB I may have had some issue with the ports tree acting funny when I upgraded FreeBSD (I suspect this because `mariabackup` is reluctant to work without segfaulting (and that’s a BSD issue where a port is not properly upgraded with the rest of the system). However, I have no idea why or how that specific issue would also manifest in a query being unplannable. 
On second thought, I'm pussing out. There's no way I can refresh all of this knowledge that I won't even know what I'll need, for an interview in twelve hours. It was such a seductive, wonderful idea for the perfect job for me. eughk.
Joes 2 pros volume 1-5. You’re welcome!😉
I deeply, truly hope for the good of the patients, the hospital, and HIPAA that this is a hypothetical class project and not a real one.
Interested! 
It's not exactly a database file. I'd bet you could open the file in Excel and use Text to Columns (using the hints in the layout file probably) to read the data.
Yeah it’s just a senior design project. Patient information will be made up. Any help? 
The platform isn't likey to make a lot of difference, nothing wrong with MySQL for this. The matching is likely to be heavily dependant on what data you've actually got to match, you haven't really given us much to go on.
I’ll definitely try that out, the guy at the tax office said specifically not to do that though since it’s a list of 2,000,000+ people but it’s worth a shot. Thanks for the response!
Restore a copy of a backup to a different database name, such as Restored, then compare the data of the tables in Restored to your actual database to find the records that are missing. Alternately, you could try some of the recommendations here: [https://dba.stackexchange.com/questions/18463/view-delete-statements-in-the-transaction-log](https://dba.stackexchange.com/questions/18463/view-delete-statements-in-the-transaction-log)
What if there are no backups? 😭
Then you don't care about the record that was deleted.
Interested. 
You could script out the DDL. Data within tables would also need to be scripted out as insert statements. This may prove to be too painful compared to restoring a backup though. 
Not interested, count me out.
USE \[TicketProduction\] GO /\*\*\*\*\*\* Object: StoredProcedure \[dbo\].\[AddTicketToOrderv32\] Script Date: 2/11/2019 6:30:53 PM \*\*\*\*\*\*/ SET ANSI\_NULLS ON GO SET QUOTED\_IDENTIFIER ON GO \-- select \--100 free ticket can be bought \--101 -- reserved \--102-- Bought Tickets \--103-- Locked Tickets ALTER procedure \[dbo\].\[AddTicketToOrderv32\] @ticketclassid int,@eventid int,@ticketquantity int,@orderid int,@expirytime datetime AS declare @orderAmount Table( amount decimal ) &amp;#x200B; BEGIN TRAN BEGIN TRY ;WITH CTEUpdateTickets(Orderid,TicketexpiryLock) as ( SELECT Top(@ticketquantity)Order\_Id,TicketExpiryTime FROM Ticket T WITH (ROWLOCK,XLOCK,READPAST) inner join TicketClass tc on tc.TicketId = T.Class\_Id WHERE Class\_Id = @ticketclassid AND Status = 100 and DATEDIFF(MINUTE,GETUTCDATE(),TicketExpiryTime) &lt; 0 AND T.Event\_Id = @eventid ) update CTEUpdateTickets Set Orderid = @orderid,TicketexpiryLock = @expirytime &amp;#x200B; \--with no of tickets booked ;With cteGetOrderReserved(TCID,TCPrice) AS ( Select TC.TicketId,sum(TC.Price) as TicketReserved FROM TicketClass TC INNER JOIN Ticket T on T.Class\_Id = TC.TicketId WHERE T.Order\_Id = @orderid and T.Event\_Id = @eventid GROUP BY TC.TicketId ) insert into @orderAmount select coalesce(sum(ct.TCPrice),0) from cteGetOrderReserved ct &amp;#x200B; update OrderTransaction set Amount = (select amount from @orderAmount) where OrdersId = @orderid
I'm not sure what a list cursor is, I've never heard that term before. There is a DMV in SQL Server that will list the attributes of all active cursors though: [sp\_cursor\_list](https://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/sp-cursor-list-transact-sql?view=sql-server-2017). Do you have any context to help describe what you're looking for?
I did read this document before, but was unable to understand it 
A [cursor](https://docs.microsoft.com/en-us/sql/t-sql/language-elements/declare-cursor-transact-sql?view=sql-server-2017) is SQL Server's equivalent of a FOR loop (kinda'). It's iterative and, written improperly, can be extremely resource intensive. They aren't bad things in and of themselves, but any time you can achieve the same result with a *set* then SQL will almost always be more performant. sp\_cursor\_list is a system stored procedure (not a DMV, my mistake) that will give information about active cursors in a session to a SQL developer so they can troubleshoot or debug their code. If you can describe your problem or what you're trying to achieve then folks may be able to assist you more thoroughly. :)
Then your data is gone
Also interested 
Like others have said, if there's no backup, the data is gone. However, depending on what kind of data it is and how your database is structured you might be able to pinpoint the missing row. For example, if the table in question has an incremental INT as ID, you might look for the "hole" left by the row, compare it to the preceding and following rows and try to figure out when it would have been inserted, maybe figure out what happened around the time it was deleted based on timestamps, etc. This is **a lot** of ifs and buts, but if it's important enough and the data is gone your best bet is to play Sherlock Holmes.
By reduce do you mean less code or go faster? It's not properly formated for me on mobile, so I'll let you know later but for start, get rid of the locking hints and rewrite the where condition so it doesn't use Datediff - google SARGability.
what if i don't want two people to choose same ticket?thanks
Hey what's up, so I've created a create base tables file and managed to successfully not prefix tables with tbl. -- Create tables -- CREATE TABLE dbo.Players( PlayerID int identity(1,1) primary key, Forename nvarchar(max) NOT NULL, Surname nvarchar(max) NOT NULL, WeekPoints int, TotalPoints int, ClubID int NOT NULL, PlayerValue decimal(18,2) NOT NULL ) CREATE TABLE dbo.Clubs( ClubID int identity(1,1) primary key, ClubName nvarchar(max) NOT NULL ) CREATE TABLE dbo.Team( TeamID int identity(1,1) primary key, TeamOwner nvarchar(max), TeamName nvarchar(255) ) CREATE TABLE dbo.Team_Players( TeamID int NOT NULL, PlayerID int NOT NULL, Position int, --position int enum CONSTRAINT PK_TP primary key (TeamID, PlayerID) ) -- Add constraints -- ALTER TABLE dbo.Players ADD CONSTRAINT club_id_fk FOREIGN KEY (ClubID) REFERENCES dbo.Clubs(ClubID); ALTER TABLE dbo.Team_Players ADD CONSTRAINT player_id_fk FOREIGN KEY (PlayerID) REFERENCES dbo.Players(PlayerID) ALTER TABLE dbo.Team_Players ADD CONSTRAINT team_id_fk FOREIGN KEY (TeamID) REFERENCES dbo.Team(TeamID); 
Is it possible to have Sql Server Express 2014 and 2016 on the same box ?
It is as far as I know.
^--- yup
A DBA is only as good as their last backup.
Order by date, id
Order by ID, date
That gives me the lowest numerical ID first and then the records under that ID ordered by date. I want my output to start with my first customer by the date of their first transaction and all of their records, followed by my second customer by the date of their first transaction and all of their records etc etc.
Left join to a sub query of customers and their first order, and then order by that order date and then by the regular order date.
This worked and is quick, thanks!
if you're on oracle and have archive log enabled, log miner. 
How come you didn't have backups?
that looks great... what's the difference between a club and a team?
So say it's the English Premier League, the club will be the clubs in the league, like Manchester United, Arsenal and Liverpool. The team will be the players selected by the users. I understand it's not the best notation but it's just a university project and won't go to any production. So as long as I can understand it I'm happy enough! Thanks for your help.
&gt; The team will be the players selected by the users aha, a fantasy app ;o)
you have Position as INT... why not VARCHAR(9) or similar? note you can still apply relational integrity via a FK from Team_Players to Positions... PKs and FKs do not need to be integers, you know!! 
I was thinking of just using an enum, the mid-tier is .Net and Enums are usually simple enough to use and pass through as ints. Gonna start work on stored procedures now. I predict problems.
&gt;written this blog to talk through our findings so far. I read through it but it seems more like an ad for Redgate software than anything informative. The TL;DR is that provisioning and maintaining SQL Server is hard, shocker.
"Teach Yourself SQL in Ten Minutes" by Ben Forta
In Oracle (unlike many other databases), an empty string is considered to be a NULL value. So your to\_date function is going to return NULL—which is fairly typical for builtin functions when passed NULL values—rather than throwing an exception. It's essentially the same as running: ``` SQL&gt; set null NULL SQL&gt; select to_date('','MM/DD/YYYY') test from dual; TEST ------------------- NULL ``` Since no exception was thrown, execution simplycontinues to the `RETURN 'Y';` statement.
You were very close. While you could use UNION ALL to tie them together, you can just add another parenthesis full of values. ;WITH table1 AS( SELECT * FROM ( VALUES ('win2012std', 'http://win2012std/', '0', '2018-08-23 23:05:13.327','0:99','queue reloaded at 8/22/2018 5:17:25 PM') , ('win2012std', 'http://win2012std/', '0', '2018-08-23 23:05:13.327','0:99','queue reloaded at 8/22/2018 5:17:25 PM') ) AS ti(Machine, Url, Status, LastRun, Turf, Comment) ) select * from table1
Since I don't see your flavor of DB specified. The WITH clause defines a CTE, it doesn't create a table, so there's nothing to 'add'. Depending on how much data you need to mock/fake, you could use union like so (done with mssql): ;with table1 as ( select 'win2012std' as [machine] ,convert(datetime2, '2018-08-23 23:05:13.327') as [lastrun] UNION select 'win2012 as [machine] ,convert(datetime2, '2018-09-23 23:05:13.327') as [lastrun] ) select * from table1 
So, something like this? DATEADD(MINUTE, 15, timestamp)
My go-to for missing indexes(with one caveat): \-- Find missing index warnings for cached plans in the current database (Query 70) (Missing Index Warnings) \-- Note: This query could take some time on a busy instance SELECT TOP(25) OBJECT\_NAME(objectid) AS \[ObjectName\], cp.objtype, cp.usecounts, cp.size\_in\_bytes, query\_plan FROM sys.dm\_exec\_cached\_plans AS cp WITH (NOLOCK) CROSS APPLY sys.dm\_exec\_query\_plan(cp.plan\_handle) AS qp WHERE CAST(query\_plan AS NVARCHAR(MAX)) LIKE N'%MissingIndex%' AND dbid = DB\_ID() ORDER BY cp.usecounts DESC OPTION (RECOMPILE); \------ \-- **Helps you connect missing indexes to specific stored procedures or queries** \-- This can help you decide whether to add them or not The bold is mine. This is great for linking an index to a specific stored proc. Sometimes those pesky change management folks want that kind of info. The caveat is, of course, this is mainly for stored procs. It will capture some ad-hoc queries, but maybe not all, or the ones that have the highest index advantage. I use both, yours and this stored proc dmv query. I run this one first, since the query plan it returns takes a good first stab at scripting out the index itself. Just add the index name and an IF NOT EXISTS statement in front of it. The query is from [here:](https://www.dropbox.com/s/wmsfaqks7lwzqe4/SQL%20Server%202017%20Diagnostic%20Information%20Queries.sql?dl=1) starting at line 1644. &amp;#x200B;
Thanks for your suggestions..
Aha, did not know that, thanks for breaking that down. What a damn pain...
Awesome....this works....Verified
Yeah unfortunately function don't exist in Teradata 
You could also shorten it to be: ;WITH table1 (Machine, Url, Status, LastRun, Turf, Comment) AS ( VALUES ('win2012std', 'http://win2012std/', '0', '2018-08-23 23:05:13.327','0:99','queue reloaded at 8/22/2018 5:17:25 PM') , ('win2012std', 'http://win2012std/', '0', '2018-08-23 23:05:13.327','0:99','queue reloaded at 8/22/2018 5:17:25 PM') ) select * from table1
I’m no expert in Teradata but reading these date functions, have you tried sth like select current_timestamp + INTERVAL '1' hour; I found it on http://dwgeek.com/commonly-used-teradata-date-functions-examples.html/
&gt;;WITH table1 (Machine, Url, Status, LastRun, Turf, Comment) AS ( VALUES ('win2012std', 'http://win2012std/', '0', '2018-08-23 23:05:13.327','0:99','queue reloaded at 8/22/2018 5:17:25 PM') , ('win2012std', 'http://win2012std/', '0', '2018-08-23 23:05:13.327','0:99','queue reloaded at 8/22/2018 5:17:25 PM') ) select \* from table1 This isn't working. I'd love to shorten it if possible
I still keep that book handy for when I forget something that I don't use often.
Ah, it's in postgresql. There must be some differences in CTE syntax between the dialects.
I'm using the scripts here: [https://rextester.com/l/sql\_server\_online\_compiler](https://rextester.com/l/sql_server_online_compiler) with this, I can play with it and get it finished and clean
select \* from table where "date" between '2019-01-01' and '2019-01-31' I think that's what you are asking for? I've double-quoted date because it might be a reserved word. As far as "not supposed to check every single row for date range" that sounds like an index?
This should be two tables. One for people and a 1-to-many table that is roles. You would then add a row in the second table for each role the person has. Otherwise you have to alter the table every time you add a role. You could also have a third many-to-1 table that has more information about the roles. &amp;#x200B;
I’m not a DBA. One of the developers accidentally deleted something they shouldn’t have and I was trying to help. 
Seeing as there are no other answers, I’ll give it a shot (I’m on mobile): 1) Do a cross join of the two tables 2) WHERE ABS(Table1.ColumnA - Table2.ColumnB) BETWEEN 0 and 49 I’m sure there are better answers. Good luck!
I'm not exactly sure what you're trying to do, but you should check out OPENQUERY().
Can you just put an identity on the tables? Then use IDENT_CURRENT or SCOPE_IDENTITY to get the ids of anything you are calculating within the procedure? You can't do a strict join without a rule to identify one row to another. Otherwise how is the engine going to know what to match to what? I do think what you are saying is possible if you want to define your rule to match if and only if cola &lt; colc &lt; colb, but this could end up with ambiguous results depending on what you're trying to accomplish.
That sucks man. I take it you guys don't have a dba or anyone working with you who has relevant dba skills? I would be hastening to make sure all of your databases have backups if I were you. I take it that this database was on a server? Do you guys have server backups that you can restore the database files from, atrach the database files as a database with a different name and compare the tables between the restored database and the live database?
Here is a link to what I'm testing with. You can see, I'm using CROSS JOIN to bring in the tbl\_queue table, but I would rather have the join identify that the column HASHKEY fits between the TURFSTART and TURFEND. [https://rextester.com/IIBEP32315](https://rextester.com/IIBEP32315) &amp;#x200B;
This query lets me run the query at the source server, EXEC ('SELECT TOP 100 \* FROM \[data\_base\_name\_2\].\[dbo\].\[source\_table\]') AT \[linked\_server\_name\] &amp;#x200B; I am trying to get the results back, and then inserted into the destination server. So, the issue isn't getting the query to run, but how to insert the results back into the destination server.
I think openquery actually might work! Testing it more now!
Oh, so just do something like this: INSERT INTO dbo.localtablename SELECT blah FROM OPENQUERY() Been awhile since I used the precise syntax so it might be slightly off, but what it will do is go to the server, run the query , return the data and put it into a table on the local server. Wrap that up into a stored procedure and then just `exec storedprocedure`.
That worked perfectly! Thank you so much for the help!
One time I wrote something, I'll try to dig it up out of my archives, but it was a nested openquery that went to a server to select data back from the server it came from, and then a bunch of other redundant shit. It was just for a lark, but you can do a lot with it.
Just move your WHERE criteria to the join. https://rextester.com/OJBJP47824
Sorry if I've missed something here, would the below work Select * from table2 t1 inner join table1 t2 where t2.ColC &gt;=t1.ColA and t2.ColC &lt; t1.ColB Alternatively Select * From t2 Cross Apply (select top 1 from t1 where t2.colC &gt;= T1.ColB) Apologies if I've got it wrong here - mobile an all! 
That would be cool to see. Yeah, you really can do quite a bit with this.
I would suggest the below video courses, pretty comprehensive and detailed course you can get just for few bucks. [https://www.udemy.com/the-complete-sql-bootcamp/](https://www.udemy.com/the-complete-sql-bootcamp/) [https://www.udemy.com/the-ultimate-mysql-bootcamp-go-from-sql-beginner-to-expert/](https://www.udemy.com/the-ultimate-mysql-bootcamp-go-from-sql-beginner-to-expert/) &amp;#x200B;
Here are my suggestions: [https://www.amazon.com/SQL-Dummies-Computer-Tech-dp-1119527074/dp/1119527074/ref=mt\_paperback?\_encoding=UTF8&amp;me=&amp;qid=](https://www.amazon.com/SQL-Dummies-Computer-Tech-dp-1119527074/dp/1119527074/ref=mt_paperback?_encoding=UTF8&amp;me=&amp;qid=) [https://www.amazon.com/Practical-SQL-Beginners-Guide-Storytelling/dp/1593278276/ref=sr\_1\_2?s=books&amp;ie=UTF8&amp;qid=1550002079&amp;sr=1-2&amp;keywords=practical+sql](https://www.amazon.com/Practical-SQL-Beginners-Guide-Storytelling/dp/1593278276/ref=sr_1_2?s=books&amp;ie=UTF8&amp;qid=1550002079&amp;sr=1-2&amp;keywords=practical+sql) &amp;#x200B;
Interested
Thanks I'll try testing it out
Ping
Your table has 2 columns: id, and answer. Your insert only inserts into a single column, answer. Since id is auto_increment, you aren't required to provide a value, but you do need to be explicit so the engine knows what you mean: INSERT INTO tNuneedua (answer) VALUES ('A table is a relational object to store data. A view is derived from a table or multiple tables as a product of a query.');
It's faster to find something that exists than to verify something doesn't exist. In this example, the difference will likely be negligible.
I would second the first version. That's how I would do it. 
It sounds like TABLE1 has ranges of values, 0-49, 50-99, etc. And maybe TABLE2 has values like 30, 60, etc... If TABLE1 has regular increments, (always by 50, starting at 0) you can use the floor and division (integer division) on the value in TABLE2 to figure out which record should be matched in TABLE1. This is in R, but it's an example of what kinds of values you can get this way: colc &lt;- seq(from=10, to=150, by=15) cola_values &lt;- floor(colc / 50)* 50 df &lt;- data.frame(colc, cola_values) colc cola_values 1 10 0 2 25 0 3 40 0 4 55 50 5 70 50 6 85 50 7 100 100 8 115 100 9 130 100 10 145 100 
how do you interpret an ID with a NULL? what does that actually mean? 
Thank you!
On a table with two columns, one for ID number and one for a job (String) the job could be null meaning this ID has no job. But also if the ID doesn't exist in the table it also has no job. I might not have explained that clearly.
Strange thing is, I get a count total of 19, whereas it should be 20. I've been hacking at it and found that the data is possibly not numerical or INT, so I've had to convert it. &amp;#x200B;
I've been able to successfully figure out my join. Here is my resulting query. [https://rextester.com/YTRS62807](https://rextester.com/YTRS62807) &amp;#x200B; Thanks for the helps!!!!
Perfect. Glad to see you got it figured out.
You're missing a comma in front of the second DATEPART statement. Runs with the added comma &amp;#x200B; I ran it as follows: select * from ( select datepart(month, getdate()-54) as iPart ,datepart(month, getdate()) as bPart ) q where iPart &lt;&gt; bPart &amp;#x200B;
so your ID has meaning by itself? it's not just some auto_incremented sequence number?
Yeah, was just trying to get a quick example of what I meant by nesting it, because I'm not so sure that's a standard. It runs just fine with random days, but passing it only around 22,000 dates from a table and it can't handle it. It's not anything else from the actual script, because selecting just the DATEPARTs gives me exactly what I expect, and comparing just the int response of DATEPART still causes it to stall. &amp;#x200B; It's probably impossible to answer the question without the specific data, which I can't share. Oh well, just thought I'd send it out there and see what happens. Thanks for the time.
If you're looking up the record by the ID value (which is the primary key), then it would be slightly faster to not have a record than to have a record with that ID value. If you have a record, the database will check the index, retrieve the record location, then retrieve the data block from disk. If there is no record, the database will check the index, see there is no record with that ID value, and tell you the record doesn't exist. It will be slightly faster because it doesn't have to retrieve a data record.
Do you have indexes? If using a good index (index seek in exec plan) then no. Furthermore a delete or update is going to take a far bigger performance hit than a read. Of course if you're doing table scans every time then as the record count decreases it will get faster, but overall the deletes will greatly outweigh any gain. &amp;#x200B; But what is the context, what does "later lookup" mean? I work with SSIS so i'm thinking looking from an ETL job. If by chance you're talking about that then just cache the lookup. &amp;#x200B; &amp;#x200B;
You are saying the second one is faster?
Yeah I already cache the result but I guess I just meant for initial loading of values.
If you're looking up by the pk then it doesn't matter. You seek to the key then you have the record, there's nothing else to check. PK = Clustered index, the data and the index are the same thing. If its a non-clustered index that isn't covered then yeah it needs to find the value retrieve the key then go to the clustered index (by key) to check the value. So in that case yeah its slightly faster. But again you need to be talking about a shit ton of rows before this makes any difference at all. 
Yeah it has meaning it's a unique name.
You can use Microsoft's AdventureWorks data: https://github.com/Microsoft/sql-server-samples/releases/tag/adventureworks
SQL Server ? If yes, probably statistics. 
Yeah, and thanks, I'll look into it. I was just so taken back by how silly it was that I didn't even really know where to start.
Bad stats make bad plan. So you can drop the plan in cache just to be sure. And yes, that kind of thing drives me crazy. I always see that problem when we use date field.
How useful is this? My current employer has a vast RDBMS that has an archive you have to sift through. What’s the adventureworks data involve? 
Sqlzoo.net
SQL zoo is a good public site to practice. Cheers! 
Be prepare to learn my friend. Joes 2 pros volume 1-5 is a good book to start off with. Happy learning!! 👍🏾
I assumed it was the date field, too. That's why I dropped it down to the date part and was still getting the same issue when it was an int (or maybe the optimizer was ahead of me and still seeing it as a date part, who knows). The Estimated Plan wasn't anything goofy, but couldn't get it to finish to see the actual plan. I'll check it out tomorrow and clear out stats and see how it does. Thanks for the help.
I found the exercises on Hacker Rank to have a nice spread of difficulty - the questions can be a little odd and ambiguous sometimes but still not a bad resource to get your brain working. 
you can share the estimated plan at pastetheplan.com
what do you believe will happen if the two rows in the table are exactly the same. as in if you would do select A, B from data if the table had the following rows: A B 1 1 1 1 ??
Very useful to practice on. You can sign up for an Azure account and have an Azure SQL database up and running in 5 minutes. I think you can get 1 year free when you sign up. 
Technically, it would be faster. But not perceivably so. Whether to delete the record or leave it in the table with a null Job value should be determined based on the business requirements (how you plan on using the data in the database) rather than performance considerations. The performance difference is moot.
I recently got the SQL Cook Book which has been good so far. As far as websites go... Can't really remember the site name but it's something like w3school? Has good breakdowns of the basics. 
thanks :-) &amp;#x200B;
My favorite is Hacker Rank. https://www.hackerrank.com/domains/sql Feels like a game where you're trying to outdo others' scores and such.
It got me making 55hr so don’t knock it. Lolol
When I did Microsoft Virtual Academy they have free videos to do that follow it. I also believe Adventure Works and Northwind are used by a lot of classes. 
Can you please explain it a little more? I don’t understand the question. 
Thank you for the links! I am currently exploring Khan Academy online. Hoping to get out of my rut soon
There so many resources out there, it can be a bit overwhelming. Thanks! 
OP needs to join a study group instead of spamming this subreddit every day.
Work with your university, contact your professors and ask them if they know of any contacts where you could learn additional SQL skills as well.
Awesome thanks I will 
I sent quite understand what your asking. Do you have a separate table that tracks the number of employees per department? If so, why? A view or sproc would presumably be more efficient depending on how often the table is used. If you must keep this tracked in a table, you would want your trigger to join the inserted department to the table, and update quantity equal to + 1 Again - I dont really understand what you're asking or why you need to do this. 
If you’re just running heaps of inserts you should already use a clustered index on a unique incrementing value. But you will always need to use an ORDER BY if you 100% of the time want the set to be ordered. The moment you join onto the table, or if it’s using a different index, your results will turn up in anyway the engine sees fit
A SELECT statement won't increase anything. 
Register with Microsoft Visual Studio and get a few months free Pluralsight through their Dev Essentials program. https://visualstudio.microsoft.com/dev-essentials Download the test SQL databases (northwind and adventureWorks from Microsoft. https://docs.microsoft.com/en-gb/sql/sql-server/tutorials-for-sql-server-2016?view=sql-server-2017 For bigger datasets, you can download them from kaggle.com and most Data Science blogs or websites. Have fun! 
Hi, so I studied your replies, and I studied about recursive CTE's more and more, and the end product I came with is this: `WITH cte` `AS (` `SELECT obj = smt.ParentObjectName` `,smt.ParentDatabaseName` `,smt.ParentSchemaName` `,path2 = CAST ( CONCAT ( smt.ParentDatabaseName, '.', smt.ParentSchemaName, '.', smt.ParentObjectName ) AS NVARCHAR(MAX) )` `,level = 1` `FROM #xxx AS smt` `WHERE NOT EXISTS ( SELECT 1` `FROM #xxx AS smth2` `WHERE smth2.ChildObjectName = smt.ParentObjectName` `AND smth2.ChildDatabaseName = smt.ParentObjectName` `AND smth2.ChildSchemaName = smt.ParentSchemaName )` `UNION ALL` `SELECT obj = smth.ChildObjectName` `,smth.ChildDatabaseName` `,smth.ChildSchemaName` `,CAST ( (COALESCE ( cte.path2 + ' &gt;- ', '' )) + REVERSE ( CONCAT ( smth.ParentDatabaseName, '.', smth.ParentSchemaName, '.', smth.ChildObjectName )) AS NVARCHAR(MAX))` `,cte.level + 1` `FROM #xxx AS smth` `JOIN cte` `ON cte.obj = smth.ParentObjectName` `)` `SELECT --DISTINCT` `cte.obj` `,Path2 = COALESCE ( cte.obj + ' -&gt; ' + REVERSE ( cte.path2 ), 'Leaf' )` `,cte.level` `FROM cte` `ORDER BY 1` `,3` `OPTION ( MAXRECURSION 0 );` &amp;#x200B; The problem is, it still loops and loops infinitely, I cannot understand how. You said that the first column should also be from the cte and not the temp, but it doesn't seem to fit to my case somehow. Sorry for troubling you with it, it's just that this is probably the hardest case of an sql "problem" I have encountered, and there's not many articles about it.
Thanks for reading the article, as you mentioned the configuration side of SQL IS tricky and difficult, which is exactly why we're looking into how we can help support DBA's when they're working on it. We'd love to talk to you to hear your perspective if possible?
This is a personal opinion, having been a freelance MySQL DBA. The actual need for a DBA/SQL architect/optimizer is real, and the demand is greater than the offer. However, unless you get hired by a larger business, it is unlikely to get you a full time job. On the other side, contracts are mostly monopolized by support companies, that can offer a lower price because scale, 24/7 support and, as you said, low cost competition. In my opinion, SQL is indeed valuable, but unless you aim to go big (larger company or more than 1 freelance), you should diversify to either get several stable clients or offer something beyond SQL ("hey, I can take care of your Cassandra and Hadoop too, as well as your backup policy!", "security hardening? Sure!", "Let me introduce configuration management into your best practices", "I can help you automate deployments", etc.). I went for the SRE/devops profile, but any other route will be good. &amp;#x200B; \&gt; I learn best with real problems &amp;#x200B; Consider donating some of your time to a non-profit, which will both be a good thing (karma-wise), will make you face the same problems than a Business, and it will look great in your resume. Disclaimer: I work for a non-profit so I am biased.
How long does the job run for?
The job is very small and is usually finished within a second.
Clearly the file is in use by something else during that time, not that you needed me to tell you that. Check on the other jobs that affect that file, or applications, it'll be your best bet. Could even be OS-level backups?
Dude you’re 40 not 70. 
If I recall correctly, you can alter the database to point to a new location of the files. This is only a meta data change so you can do it, take the database offline, move the files, then bring the database back online. Double check that since this is from memory but I think that's right.
For SQL Server it's pretty easy. Find the logical name of the file(s) you want to move. This can be done through the UI in the database properties or by looking at the sysfiles DMV in the context of the database you want to use: USE [DatabaseName] SELECT * FROM sys.sysfiles Using the 2017 version of AdventureWorks as an example, we can move both these files with two commands: ALTER DATABASE [AdventureWorks2017] MODIFY FILE (name = 'AdventureWorks2017', filename = 'H:\MSSQL\DATA\AdventureWorks2017.mdf') ALTER DATABASE [AdventureWorks2017] MODIFY FILE (name = 'AdventureWorks2017_log', filename = 'T:\MSSQL\DATA\Adventureworks2017_log.ldf') This will change the metadata for your database file locations, but leave the database running. Shut down the SQL Server service, *copy*, the files to their new location(s) and restart SQL Server. If the database starts up and *sys.sysfiles* reflects the new location correctly then it's okay to delete the .MDF/.NDF/.LDF files from their original location.
You didn't mention what platform. In Oracle you'd have to shut down, make parameter changes for one or more log file locations such as "log_archive_dest_##" (don't forget to write out your spfile to pfile and create spfile from pfile if necessary), and control file location if that is changing "control_files", then for moving database files you issue a command such as ALTER DATABASE MOVE DATAFILE '/u01/oracle/system01.dbf' TO '/u02/oracle/system01.dbf'; for each datafile, then start up. If you're using a somewhat current version of Oracle you can do an online move. 
* [Detach the database](https://docs.microsoft.com/en-us/sql/relational-databases/databases/detach-a-database?view=sql-server-2017) * Move the files * [Attach the database](https://docs.microsoft.com/en-us/sql/relational-databases/databases/attach-a-database?view=sql-server-2017)
Better answer than mine.
That won't work for system database, will it? I imagine you can't detach master.
System DB's are a little bit more difficult. You need to change the startup parameters / alter the database file locations, shut down the SQL Service, then move the files, then turn it back on. &amp;#x200B; [https://docs.microsoft.com/en-us/sql/relational-databases/databases/move-system-databases?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/relational-databases/databases/move-system-databases?view=sql-server-2017)
&gt;I'm hoping to develop some kind of SQL skillset with local value, so that means in practice, combing SQL with another more localised skill such as business development, management or sales . Sounds like a business analyst to me. Someone whose expertise is really business processes and knows enough about querying/manipulating data to get what he needs to show trends argue for improvements etc.
I’m 39, learned sql last year and now work in data analytics. I would also recommend Tableau or like for visual analytics as well as python 
The real value you have to offer is likely how to work with people to understand their data needs, aka being a business analyst. The soft skills of being able to tease out and clearly define their data needs is really valuable, and I don't think it will be likely to be outsourced. SQL is really not that difficult. The "meaty" parts of being a data analyst (I was one for 5 years) is finding the data in possibly several huge and poorly documented databases, and then validating your query/report.. You could maybe take a community college course on it if you want structured learning, and hopefully they would cover the "requirements gathering" and validation steps. 
I wrote my first SQL at 38. Now I'm 41 and pretty decent. You'll be fine
It will work for \[model\], \[msdb\] &amp; \[temp\], but not for \[master\]. You definitely have to use the startup parameters to adjust where \[master\] files will live, since SQL can start without any of the other system databases.
Nah, your answer was perfect, I just expanded some of the details. :)
For real... is 40 a barrier now?! haha
I was lucky that I was in the right place at the right time and was given an opportunity at 39, now I'm working as a BI manager. As someone else did you're 40 not 80. Go for it
where / how did you learn sql and tableau?
I turn 30 this year, fuck anyone who's gonna make me feel like I'm on the edge of irrelevant lol.
I leaned on udemy using$10 promo code
Feels like it to me. If only cos you can exploit/manipulate kids more easily
If it can be broken up into small tasks, I use a kanban board 
I'm 37 and started my first full time SQL gig about six months ago. &gt; MYSQL low level jobs are likely to suffer from low cost competition overseas Only if you're trying to get work from one of those "pay someone $5 to do a week's work" sites. If you're looking to be in an office building, that's not your competition. &gt; I'm hoping to develop some kind of SQL skillset with local value, so that means in practice, **combing SQL with another more localised skill such as business development, management or sales** . &gt; I learn best with real problems. I'm not good at being motivated to work on theoretical issues. **So what is a good application to build using SQL?** I was thinking of creating some kind of personal finance app to track household spending or similar. Those are two completely different tracks. I work as a business/data analyst, I spend zero time building apps. If you want to go into sql for development, you don't need any business development, management or sales skills (for the most part).
Lots of good advice here already. If you're looking for a personal project, a personal finance dashboard sounds good. You could also try something like a home inventory. Start tracking all of your purchases, where they came from, how much you paid, quantity, etc. Maybe even take pictures of your items and/or receipts. You could really get into the weeds of data management that way. And it'd be invaluable if (God forbid) your house ever burnt down or something.
But you’re 40.... that’s not very old. Maybe if you were 55+ it’d be a different story. 
I dunno, maybe I'm just negative, but I feel (just guessing as I've been self employed for ever) it levels off from 35-55. Hope I'm wrong!
It doesn’t no worries go learn SQL
What Python course did you take? Thanks!
Sql is not difficult but can get complex fast. 
Can you please share what resources you used to learn SQL to become proficient? Thanks!
I took Jose Portilla zero to python and his PostgreSQL course 
I was around 45 when I first started to learn SQL.
Actually 40 has always been a barrier. 40 and over is a "protected class" in employment discrimination. It shouldn't be, but it can be a barrier. Source over 40, was in IT.
I work for a large company with a huge IT department. We hire people over forty all the time. Age doesn’t even come up at all unless we think the applicant is close to retirement age. It comes down to experience. 
I work for a huge company, and here age isn't an issue. I've also worked for little companies where sometimes it was. It's a thing, it happens. Certainly not everywhere.
Do you have a synonym named "B", that happens to reference a table where "id" is a valid column?
is there an ID field in table a? What's the schema of both tables? 
The 'ID' field was an example, but there is a field with the appropriate name in table A
I don't think so. The table name is very unique, though I'm not sure how to check that 
Create a CURSOR and populate the CURSOR with the part numbers you wish to run against the second query. Place the second query inside the CURSOR loop and it will do what you want it to do.... mostly. You'll need to expand/clearify the second half of your post because the dataset you're looking to achieve looks odd.
&gt;the dataset you're looking to achieve looks odd. No arguments here. I'm likely doing something silly and overly complicated. The CURSOR solution looks promising. Thanks.
All of A's columns can be used in the subquery. Since you didn't qualify the column name to a specific table, the engine uses the only id column that is valid: the one in table A. Compare to a query like: SELECT * FROM tableA WHERE EXISTS (SELECT * FROM tableB WHERE tableB.a_id=tableA.id) If tableB has no "id" column AND tableA has no "a_id" column, this query is exactly the same as SELECT * FROM tableA WHERE EXISTS (SELECT * FROM tableB WHERE a_id=id)
&gt; I have two queries. They both work great. What I'd like to do is run the second query with input from the first query. That is: Let's say the first query returns a dataset where each row has a part number. I want to take each part number and run it against the second query (i.e. in the second query's 'where' clause). Repeat for each row from the first query. classic example SELECT b.foo , b.bar FROM sometable2 AS b WHERE b.partno IN ( SELECT a.partno FROM sometable1 AS a WHERE a.qux = 'bat' ) 
I'm not quite sure I get the particular issue, maybe if you post some real code? Generally speaking, you know that you can treat query results as derived table data source in your "from" clause, dont you? I.e. select Query_Alias.xxx, Query_Alias.yyy from (query) Query_Alias In the similar manner, if 2 queries are giving you results at the "part number" granularity, you can join those select coalesce( query1."part number", query2."part number) as "part number, query1.a, query1.b, query2.a, ... from (query_One) query1 &lt;whatever needed&gt; join (query_Two) query2 on query2."part number" = query1."part number"
I did not know that... I do now. Thank you very much.
your subqueries CAN be correlated, so since the sql engine cannot resolve "id" from B, it tries to resolve it from outer queries, i.e. it runs this code: select * from A where id in (select a.id from B)
select A.* from A where id in (select B.id from B)
You'll find way more middle-aged adults working in SQL than college kids. Don't forget, SQL is based on the 1970s Codd paper and was formally an ANSI standard in 1986. It's old just like we are. Until something better comes long to join sets of text, SQL is going to live on. Hadoop and "big data" is what the kids are learning, but that's not, and never will be SQL in terms of set operations on normalized databases. 
You want to get into analytics, predictive modeling (statistics), and ETL. Lots of overseas competition, but it's really hard to "replace" the people that are local. Generally people in this field will work with the overseas competition and "manage" them. I would say there is a point to it. I am 36 and got my first "official" SQL job about 5-6 years ago. Pay is great, I work from home, and I have been steadily promoted. I used to run my own online business too and that was actually what I used as a spring board to start applying for more "senior" level positions doing "marketing analytics." It was an entry level position in terms of SQL, and they were willing to teach me on the job. They didn't care that I lacked experience with SQL because I had a lot of past experience in marketing, web building, etc., and those skills were considered more critical to being successful in the role than "simply using SQL." At this point I could probably get a certification and try to go down the DBA career path, but honestly I enjoy analytics and modeling a lot more, and the pay potential when get up into senior management is extreme. &gt;So what is a good application to build using SQL? I was thinking of creating some kind of personal finance app to track household spending or similar. This is exactly how I taught myself. I went to work 9-5 and learned on the job, then went home and worked on side projects. Get involved here in this sub reddit, read the problems/questions people are having and try to solve them, or read the answers and try to understand them. I got a lot of value over the last few years just trying to help people here with their problems, even if I was wrong and gave them a shit answer. I might have done someones homework, but I got to learn the material and they didn't. Laughed all the way to the bank.
A CURSOR probably won't do what you want it to because as it loops though, it will produce and deliver multiple record sets. I'd populate a temp table with the part number along with any other relevant columns, and then run a PIVOT over that. In any case, here's an example CURSOR statement. DECLARE @PartNumber INT --If it isn't already declared, then declare it here. DECLARE PartNumberCursor CURSOR FOR SELECT PartNumber FROM SomeTable WHERE Something = Something FOR READ ONLY OPEN PartNumberCursor FETCH NEXT FROM PartNumberCursor INTO @PartNumber WHILE (@@FETCH_STATUS &gt;= 0) BEGIN ------------------------------------------------------------- SELECT * FROM SomeTable WHERE PartNumber = PartNumber --This is the second query you were talking about. ------------------------------------------------------------- FETCH NEXT FROM PartNumberCursor INTO @PartNumber END CLOSE PartNumberCursor DEALLOCATE PartNumberCursor So let's say the first query returns 4 part numbers. The CURSOR will execute the SELECT statement 4 times with each part number. But again, it will return 4 different record sets.
this is a classic gotcha. maybe the classic gotcha other than "null not in." the "id" in your subquery is A.id. 
Hey, [I just wrote an article on this](https://jonshaulis.com/index.php/2019/02/12/properly-reference-your-columns/)! I'll paste the relevant snippet so you don't need to read the whole thing though. &amp;#x200B; &gt;The subquery, in fact, cannot resolve and find the column ID. So what happens is that when the query cannot find the column ID in the inner scope, it moves to the outer scope of the query to find the column that it was referencing and it finds \[A\].\[ID\]. Effectively changing your query to: &gt; &gt;SELECT \* FROM A &gt; &gt;WHERE ID IN (SELECT ID FROM A) &amp;#x200B; In my example, it was basically the same thing. Except for your table B, the real column name was FakeID, but I put the value ID instead. 
This is not correct. The value of ID is still selected from table B - e.g. if table B is empty, you won't get any records back.
I gave it a test and you're right, I'll make some changes to my article later this week, thank you! &amp;#x200B; Code I tested: IF OBJECT_ID('dbo.FakeTableSource', 'U') IS NOT NULL BEGIN DROP TABLE [dbo].[FakeTableSource]; END; CREATE TABLE [dbo].[FakeTableSource] ( [FakeID] INT PRIMARY KEY, [FakeVarchar] VARCHAR(10) ); GO IF OBJECT_ID('tempdb.dbo.#T', 'U') IS NOT NULL BEGIN DROP TABLE [#T]; END; IF OBJECT_ID('dbo.FakeTableDestination', 'U') IS NOT NULL BEGIN DROP TABLE [dbo].[FakeTableDestination]; END; CREATE TABLE [dbo].[FakeTableDestination] ( [ID] INT PRIMARY KEY, [FakeVarchar] VARCHAR(10) ); GO INSERT INTO [dbo].[FakeTableSource]( [FakeID], [FakeVarchar] ) VALUES( 1, 'One' ), ( 2, 'Two' ), ( 3, 'Three' ), ( 4, 'Four' ); --INSERT INTO [dbo].[FakeTableDestination]( [ID], [FakeVarchar] ) --VALUES( 1, 'One' ), ( 2, 'Two' ), ( 3, 'Five' ), ( 5, 'Four' ); SELECT * FROM dbo.FakeTableSource WHERE FakeID IN (SELECT FakeID FROM [FakeTableDestination]) &amp;#x200B;
Might be a bit overboard, but I use DATEDIFF(HOUR, DOB, TIMESTAMP)/8766
Interested
In my last three jobs I've been the only person on my team under 40. I'm a data analyst which most people think of as something very new. 
Do you use Python at your workplace? What packages do you use? Also, how complex is your SQL code at the workplace? Like, average lines of code? Thanks!!
I don’t really use python at work as it’s not required but I’m having to use it this week because I have to create a web scraper using Selenium. Sql queries aren’t complex. Not using triggers, window functions, etc but I’m sure I can incorporate those in. I haven’t used those so I’m not too familiar with those although I know they are useful 
&gt;The "meaty" parts of being a data analyst (I was one for 5 years) is finding the data in possibly several huge and poorly documented databases, and then validating your query/report.. In my opinion the big jump from an analyst, to a developer, to an architect in analytics is being able to find these things and then propose a recommended solution whilst working with a DBA to help 'normalize' your proposal and have it 'conform' as much as possible to basic standards. A lot of analytics breaks the rules of normalization, for good reason, and the difference between an analyst and an architect is the ability to understand when and where these rules should be broken, and when and where you must conform. As an analyst you go in and do a "deep dive" using "rudimentary SQL" to find commonalities between shitty databases, or datasources, and then coming up with a "process" that will produce a "meaningful" result that the business can use. In the same breath you go to a lot of meetings relative to future technologies, deployments, redesigns, etc., and try to explain to them how to make it easier for you to produce something meaningful. I always say that you don't need permission from analytics to do something, but you do need to approach us first to tell us what you want to do, so that we can tell you how you **must** do something in order for us to continue to produce the deliverable that the business is used to. Most companies like to come to analytics last and tell them what they want, which is great until we respond by saying that it is physically impossible to deliver what the business wants because no one bothered to engage us early in the process to see what we might actually need.
1. Convert each date into an integer in the format YYYYMMDD 2. Subtract them 3. Divide by 10000 (with integer truncation) It works with leap years and Feb 29th. During subtraction days borrow from months and months borrow from years, arriving at the accurate age. TSQL: `(CONVERT(int,CONVERT(char(8),SomeTime,112)) - CONVERT(int,CONVERT(char(8),DOB,112))) / 10000` 
select name, date, sum(sales) from tablename group by name, date &amp;#x200B; You can add more detail for the date if needed, but be sure to add to the group by as well
Thanks! 
The first part is asking you for the unique identifier: in most cases this is an ID, but in some cases it’s actually a combination of two IDs (eg department faculty)
Np. Let me get my laptop. Brb
I can't believe that I never knew this
I don’t think so. The optimizer could be reworking your query behind the scenes anyways 
You already have your answer, but honestly, this is like the very first thing you should've learned in SQL...
No, doesn't matter what order you put them in. 
this is one of the most evil bugs in sql scripts you can encounter. DELETE A FROM tableA A WHERE a.ID in ( SELECT id from tableThatDoesNotContainID ) is pretty much, truncate tableA. Its quite nasty to actually find, after staring at it for hours.
 SQL is not difficult until you have a couple million rows you need to do a bunch of stuff to and you need your query to run in a reasonable amount of time.
Did some basic sql in college ~20 years ago, did a lynda.com course as a refresher, asked a lot of questions and googled a lot of stuff on the job. 
Can you give an example for a newbie like me of always accessing a column by alias? I know how to use aliases - just new to the idea of always using them to access the column. 
[https://pastebin.com/2NTJQRay](https://pastebin.com/2NTJQRay) &amp;#x200B; This should do it. &amp;#x200B; I cover single Parent child.. then i cover all parent child combinations while using another TREE variable.
SELECT * FROM tableone a WHERE a.id IN ( SELECT b.id FROM tabletwo b)
Crystal clear. Thanks, kind stranger!
&gt; Would it be faster if it is the other way around? You can try it and find out. No, the ordering of the criteria in the `WHERE` clause should not make a difference.
If you're concerned about keeping (or being able to restore) data, you don't want to drop or truncate the tables. While I think you can recover a dropped table's data from the transaction log, that's really not the way you want to have to do it. If you're concerned about accessing old data, I'd recommend putting that data in a separate table. Without going into way more detail than you probably need to know right now, it sounds like there's not too much activity that happens in this database. Backups are important, and how much data you're willing to lose if something goes wrong should go into your backup strategy. That said, if you don't feel the need to back up your database more than once day, I'd recommend putting the database in the Simple recovery model. The reason I recommend Simple recovery model is so the database transaction log does not continue to grow and grow and grow. You can read about the different recovery models and types of backups if you want more detail and explanation.
The order of the columns doesn't matter in the where clause, but order can matter in indexes (and whether the column is sorted by ascending or descending).
The order of the columns doesn't matter in the where clause, but order can matter in indexes (and whether the column is sorted by ascending or descending).
Is there a way to clear the DB transaction log? I’m a lot more concerned about keeping the DB running quickly and smoothly than about potential data losses.
In the same way a spare tire is a tire ish. You can use it to limp down the road until you can get proper equipment, but don't expect to go far or fast with it. After all, everything is a data source if you are brave enough.
Logs aren't likely to impact your performance. But if you do want to shrink them, https://support.managed.com/kb/a447/how-to-shrink-your-mssql-database-log-file-truncate-transaction-log.aspx
If you put the database on the Simple recovery model, the log won't grow (that's not entirely true, but it's true enough for this case), it will just reuse the space it has. https://www.mssqltips.com/sqlservertutorial/4/sql-server-simple-recovery-model/
Technically yes ... 
You have to think of the top query and the Nest numeric to get why you want the cte parent. Take this data set Original(a, b) Great Grandparent, grandparent Grandparent , parent Parent, child The first part in a recursive cte is the select parent child.. you get the dataset above Original(a,b) To abbreviate Ggp,gp,0 Gp,p,0 P,c,0 Then you get that data again with a join on the above child [Original(b)] to the new data coming in [Cte(a)] aka original child to cte parent Looks like Original. Join Cte. Ggp,gp,0. Gp,p,0+1 Gp,p,0. P,c,0+1 P,c,0 Since we used Original(b) and cte(a) in the join... I can say we just want the other unused columns in the select after the union (original(a),cte(b)). What that looks like.. I put a - around columns I want back. Original. Join Cte. -Ggp-,gp,0. Gp,-p-,0+1 -Gp-,p,0. P,-c-,0+1 -P-,c,0 So now the cte looks like (first recursion still in - ) Gpp,gp,0 -ggp-,-p-,1 Gp,p,0. -gp-,-c-,1 P,c,0 But the UNION is still going.. so we do the same again.. I put a * on the only match = on columns back .. remember columns we want original. Join. Cte =Gpp,gp*,0. -ggp-,-p-,+1 Gp,p,0. *-gp-,=-c-,+1 P,c,0 Now.. we cant join any more (trust me) and this is our final data set Gpp,gp,0 -ggp-,-p-,1 =-ggp-,=c, 2 Gp,p,0. -gp-,-c-,1 P,c,0 So hard to explain... it's something you have to see to get. I'll try and make an animation. I think reddit will like it. 
“If you are brave enough” what a man!
I need to admit that it's difficult enough for me to not even be able to write some queries. Probably because I don't use it on a regular enough basis to have my brain think in it.
Excel is a great way to explain databases to people, takes away the mystery. I've read here, "Some Excel books should be Access, and some Access should be databases."
The answer is yes. Look into simple joins, group by, where and having clauses. Most likely, you won't need arrays in SQL pretty much ever.
only if you link it to a half dozen other workbooks scattered across network drives with different permissions rules.
Awesome, thanks for the quick response! Does it matter that I will have to create a new column that doesn't exist in the original dataset and apply a condition? Example, sum x, y, z (x, y, z available in original dataset), sort by new column sum and do SQL equivalent of vlookup to return a date. 
Please don't give people ideas like that
I store all of my important data in plain text csv, on a shared network folder with full permissions for everyone. /s
God... My fiance's last work place did that for their freaking accounting and tracking labor hours! They called her a couple times asking how to 'fix' it when they broke links and various workbooks were "corrupted"
Here's a real life scenario for you... Program -&gt; database -&gt; SQL query results -&gt; Excel sheet -&gt; Excel functions -&gt; another Excel sheet -&gt; Access database -&gt; Access form I shit you not... people out there do this. 🤦‍♂️
These other answers are wrong... the right answer is Yes. Usually you try to put indexed items first, fastest data minimizers first, and slowest stuff after. WHERE a.id between 100 and 200 AND a.thing1 = 0 and a.thing2 REGEXP '^STUPIDVALUE$' will be faster than if you did the regexp first when id is indexed and it shrinks the data more. You have less items to REGEXP against which is slow to begin with. You would also do functions that remove indexing after checks that don't. You can reorder them, do a full flush, run the query and you'll see the times change depending on the order of your clauses and how they behave.
Why not just TIMESTAMPDIFF(YEAR,DOB,AgeAsOfDate) ? Spits out the year without extra math.
I've seen early tutorials use a *grocery list* as an example of a simple database. 
 SELECT c.id, c.Total FROM ( SELECT a.id, SUM(a.num)+SUM(b.num) AS 'Total' FROM a INNER JOIN b ON a.id = b.id ) c WHERE c.Total &gt; 50 &amp;nbsp; SELECT a.id, SUM(a.num)+SUM(b.num) AS 'Total' FROM a INNER JOIN b ON a.id = b.id HAVING Total &gt; 50
That would work in Oracle as well, if a the syntax for the column alias is fixed. Single quotes are for string literals, identifiers need double quotes in (standard) SQL - so it should be: `AS "Total"` &amp;#x200B;
I don't know who you are stranger, but thank you so much! This is awesome and I look forward to figuring out the rest of the puzzle! 
Thank you for the tip, I'm going to work on this tomorrow, I can't wait! 
SELECT c.id, MAX(o.order_dt) FROM dbo.ORDERS o LEFT JOIN dbo.CUSTOMERS C ON o.cust_id = c.id WHERE c.age &gt; 20 AND o.order_dt &lt; DATEADD(YEAR, o.order_dt,-3) GROUP BY c.id
Give a man a hammer and every problem is a nail. Some issues are best solved in excel. My predecessor created many reports that did the excel-work for my colleagues, so that they don't need to think. Every month or so I have to update them to incorporate some new business exception or new promotion. Yet teach a man to manipulate data at a basic level, and he will have the answers he needs for life.
And some databases should be excel, if we're being honest
Why are you trying to avoid ORDER BY? Perhaps the solution to your problem isn't to avoid ORDER BY but to amend your query so that it might be included?
I was thinking how to reproduce certain behaviour of a graph database. 
**Short answer**: no. **Medium length answer**: the speed at which SQL can return results isn't related to the total number of entries in a table when you are querying the primary key and index. If you had a table of 50 million rows, but only two of those rows have a primary key of *X* then query WHERE key = *X* returns a table of 2 only. SQL doesn't query every row when using the index. Rather it uses branching logic. As a simple example, consider the index of a book: you only need look under A to find Aardvark and you don't need to check the index of Z. **Longer answer**: you need to research indexes and primary keys.
What behaviour?
This depends on the database product being used. With Postgres you would just stop the service, move the directory, then re-create the Windows Service pointing to the new directory. Essentially something like: net stop postgresql-11 xcopy /e c:\old\path\datadirectory d:\new\path\ pg_ctl unregister postgresql-11 pg_ctl register -N postgresql-11 -D d:\new\path\ net start postgresql-11 Once you verified that everything is fine, you can remove the old directory on c: &amp;#x200B;
&gt;but only two of those rows have a primary key of X In that case it's not a primary key. By definition a primary key will not contain the same value twice. A select on the primary key column(s) always returns either no rows or exactly one row, never two rows. &amp;#x200B;
 &gt;Yet teach a man to manipulate data at a basic level, and he will have the answers he needs for life. On the flip side of this, a little bit of knowledge can be dangerous. Manipulating data without fully understanding it usually leads to inaccurate or wrongly defined reports. As a DBA, please leave reporting to people who can do it properly. 
Still better than FileMaker
It looks like you want to convert it to the Italian standard. Use [CONVERT](https://docs.microsoft.com/en-us/sql/t-sql/functions/cast-and-convert-transact-sql?view=sql-server-2017) with the appropriate style you want to display. In this case it looks like Style 105 which will output dd-mm-yyyy hh:mm:ss.
`TIMESTAMPDIFF`/`DATEDIFF` counts the [year, month, day, ...] boundaries crossed between two dates. So if you calculated the Age from DOB 2001-12-30 to 2002-01-02 you would get "1" because the year boundary 2002-01-01 was crossed, when the actual Age 0 (4 days old). Using days or hours and dividing by the number of boundaries you expect to be passes helps, but still has some precision issues depending on how many leap years may or may not have occurred. [See this comment](https://old.reddit.com/r/SQL/comments/aqbnma/calculating_age_at_time_stamp/egez49a/?st=js4n0gjl&amp;sh=bb66adf5) on a more precise way to calculate age.
Seriously? Mind elaborating?
I can [work with that](https://metacpan.org/pod/DBD::CSV).
&gt; Manipulating data without fully understanding it usually leads to inaccurate or wrongly defined reports. I once had a boss tell me "you don't need to know anything about the data to build the database." I got out of that place quickly. &amp;#x200B;
I believe you. I've seen worse. It was like a spider's nest of excel sheets, access databases and vba code no one could untangle.
Wow. Yeah good move, you wouldn't see me for dust. Just lining yourself up for a massive fall by working somewhere like that, you'd naturally be the one that that boss' fingers get pointed at when it all goes tits-up. 
is vlookup an inner join??
Assuming a small set of users and backups are in place, maybe. If it is critical to the business, then no way in hell is any IT department going to take responsibility for it. You lose your data, that's on you. 
SELECT TIMESTAMPDIFF(YEAR,'2001-12-30','2002-01-02') in MySQL 5.5-&gt;8... returns 0.
so foxpro?
TIL
Wow, that's a name I haven't heard in a long while! lol
&gt; DATEDIFF(HOUR, DOB, TIMESTAMP)/8766 Worked like a charm. I did try the TIMESTAMPDIFF suggested by jc4hokies, but i it doesn't work on my ancient version of ms sql. 
Thanks for the reply! Is there a course you would recommend for learning web scraping using Selenium? Thanks again!
I think you are just describing nested loops. Recursion is a somewhat different concept. e.g. for( i=0; i&lt;10; i++) { &lt;do something&gt; for (j=0; j&lt;100; j++) { &lt;1000 times yeah!&gt; } } I'm not quite sure what are you looking for in terms of issues though.
Stop
Well, your manager should tell you what specifically he's thinking of. But in the meant time, you can look into sequences, maybe? https://docs.microsoft.com/en-us/sql/relational-databases/sequence-numbers/sequence-numbers?view=sql-server-2017
[removed]
Wow, uh.. ok. You could do some sort of weird self join using a lead function (assuming 2014 or higher) to find a number that doesn't exist. You could create a temp table for numbers and outer join to that to get the lowest number that isn't an ID in the table. But like the other guy said, your manager should enlighten you and then you should tell us. But seriously.. it should be an identity column.
I'm a DW Dev and one source system I had to get data stored all fields, data and types in a single free text field. Was a fun exercise getting something useful out. 
Sure, nested loops. &gt;Also, is there any particular reason that you think that loops are horrible? I was being sarcastic. In terms of issues though I insert a row of data so my general loop today looks like this: while having begin set @sql = 'DYNAMIC QUERY' insert @sql into dbo.table set @loop = @loop + 1 end In the new world it might look like this: while having begin while having begin set @sql = 'DYNAMIC QUERY' insert @sql into dbo.table set @subloop = @subloop + 1 end set @loop = @loop + 1 end Nothing to worry about there in terms of functionality? This "should work" just fine?
Heh.. maybe you could use this to continually try inserts until one doesn't fail. No I'm kidding don't do that.
What is your specific concern or what is the particular use pattern that you consider to be a problem?
If you're dead set against making the column an IDENTITY with an auto-increment, use whatever method you want to generate the ID and then make sure the [transaction isolation level](https://docs.microsoft.com/en-us/sql/t-sql/statements/set-transaction-isolation-level-transact-sql?view=sql-server-2017) for all processes that use INSERT are set to SERIALIZABLE. This force a transaction to complete fully before another transaction can take a lock. If it's just an INT ID column though there's not a lot of reason to not make an IDENTITY, even if you don't make it a PK with a clustered index.
&gt; while having Other than ^this and kinda weird format for insert, nothing particularly striking. This should just work.
It was just a flippant comment, nothing disparaging your suggestion. I'd never even heard of sequences. I was mostly using it as a vehicle for a ridiculous solution to a ridiculous problem. I'm sorry if I came across like I was talking shit about your comment.
GIS data developer using GIS software primarily &amp; I was handed off data products where I produced flat files matching vendor codes to spatial boundaries in Access. I eventually figured I could do that in SQL Server &amp; also process the spatial data as well.
The industry standard for querying and administration for MS SQL is [SSMS](https://docs.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms?view=sql-server-2017) (SQL Server Management Studio). It's free. &amp;#x200B; Microsoft is also developing a modernized alternative to SSMS called [Azure Data Studio](https://docs.microsoft.com/en-us/sql/azure-data-studio/download?view=sql-server-2017). It doesn't have all the bells and whistles that SSMS has, but they are slowing rolling out features. I still prefer SSMS to Azure Data Studio.
Uhm, I may have done that at my old job... To be fair I was an intern who only had access to excel and the team leaders have all been working 20+ years (manufacturing) and could barely even understand how to open excel. But yeah I told them that it will break and as soon as someone new was added it would break, but it worked for now and that's all that mattered 
&gt; I do know the difference between datatypes but I'm not sure how to figure out what datatype to use for each column go by common sense -- would the invoice date column `I_Date` really be a string? or perhaps one of the **date** datatypes? would the invoiced product quantity really be a string? or perhaps one of the **numeric** datatypes?
Complicated queries aside: it is bad practice to have an application or report use a single stored procedure for multiple datasets. Split them up into multiple stored procedures ASAP. I would argue that this would be **easier** to maintain. &amp;#x200B; As far as simplifying the queries goes, I would suggest finding common nested queries and creating views to replace them (if the data is not heavily aggregated). If that's not possible or plausible, convert the larger sub-queries to CTEs to make it more readable. &amp;#x200B; If you're still struggling with performance, learn how to read an execution plan and troubleshoot from there. Knowing how to work with temp tables, think in sets, reducing logical reads and table scans, indexing, etc. will take you from pro to super-pro. If all else fails and the data doesn't need to be live, you could always run a daily or hourly job to store the results in a table and simply SELECT that table for faster results. But since you're working with parameters, this may not be the best solution.
use the DATE_FORMAT() function since you neglected to mention which database platform you're on, i assume this function will work fine for you 
What you want is a proper ETL. Maybe minus the E part. It's very likely that the data can be split into partitions and joined in parallel. So you need a parallel ETL framework. Normally people use SSIS for this, but nothing beats the performance of scripted frameworks with IO pipes, like Mara (github.com/mara). You can look at their example project to get an idea about how fast ETL processes look like. 
Can you elaborate? Going to paste my exact code below but you can probably see what I'm trying to do. Just adding an @SubLoop so that I can cycle through (1) sproc for multiple rows of data instead of having to have (1) sproc for each calc. In business terms this specific calc has a CalcID of 1, which is a metric for "onboarding compliance", which is a fairly simple datediff(). I have 4 other stored procedures (calcid 2, 3, 10, 20) which identical to "onboarding compliance" and you could say that all 5 of these calculations are part of a "family" of calculations called "cycle time" (i.e. a simple datediff()). Now the problem is that each client can use different fields, or different labels, naming conventions, etc., which are all contractually based. So for example Client ABC has a Cycle Time calculation named Onboarding Compliance which is a datediff() between Field A and B. Client XYZ also has a calculation named Onboarding Compliance, but it is a datediff() between Field A and Field C. Meanwhile Client HIJ has a calculation named Submittal Compliance, which is a datediff() between Field T and U. So right now to accomodate this I have (1) sproc "hardcoded" as "onboading compliance" and another sproc coded as "submittal compliance" and then all the relevant parameters are passed to the Dynamic SQL and inserted into a final holding table which connects to my application layer. I want to simplify this and just have (1) sproc named "Cycle Time" which I can @SubLoop through as many times per client as necessary to calculate everything, and then just drop a "label" over the hard coded name "Cycle Time" so that the business knows which calculation it is. In order to accomplish this I've had to modify my tables, modify my ETL job, etc., to include a SubCalcID (int) column, and now I'm just having to go into the dynamic calculations to write the nested loop. I'm fairly confident it can be done, but I wanted to ask/make sure before I get too far into the weeds. ALTER PROCEDURE [SLA].[090b_OnboardingCompliance_Calc_1](@CalcID int, @SessionID int, @IncrementID int) AS BEGIN TRY BEGIN TRANSACTION DECLARE @SLAType nvarchar(255) = (SELECT SLAType FROM [AnalyticsMapping].[SLA].[mpCalcs] WHERE CalculationID = @CalcID) DECLARE @ClientList TABLE ( ID int identity(1,1) , Client nvarchar(255)) IF ( SELECT SUM(CAST(isTrigger AS int)) FROM [AnalyticsMapping].[SLA].[mpClients] A INNER JOIN [AnalyticsMapping].[SLA].mpCalcs B ON B.CalculationID = A.CalculationID ) = 0 INSERT INTO @ClientList SELECT DISTINCT Client FROM [AnalyticsMapping].[SLA].[mpClients] WHERE CalculationID = @CalcID ELSE INSERT INTO @ClientList SELECT DISTINCT Client FROM [AnalyticsMapping].[SLA].[mpClients] WHERE isTrigger = 1 AND CalculationID = @CalcID DECLARE @Loop int = 1 WHILE @Loop !&gt; (SELECT MAX(ID) FROM @ClientList) BEGIN DECLARE @Client nvarchar(255) = (SELECT Client FROM @ClientList WHERE ID = @Loop) DECLARE @TempSource nvarchar(255) = (SELECT TempSource FROM SLA.mpClients WHERE Client = @Client AND CalculationID = @CalcID) DECLARE @SegmentID nvarchar(255) = (SELECT SegmentID FROM SLA.mpClients WHERE Client = @Client AND CalculationID = @CalcID) DECLARE @ClientProfile int = (SELECT ClientProfile FROM [AnalyticsMapping].[SLA].[mpClients] WHERE CalculationID = @CalcID AND Client = @Client) DECLARE @Clause1 nvarchar(MAX) = (SELECT Clause FROM [AnalyticsMapping].[SLA].[mpVariables] WHERE ClauseID = 1 AND CalculationID = @CalcID AND ClientProfile = @ClientProfile) DECLARE @Clause2 nvarchar(MAX) = (SELECT Clause FROM [AnalyticsMapping].[SLA].[mpVariables] WHERE ClauseID = 2 AND CalculationID = @CalcID AND ClientProfile = @ClientProfile) DECLARE @Clause3 nvarchar(MAX) = ISNULL((SELECT Clause FROM [AnalyticsMapping].[SLA].[mpVariables] WHERE ClauseID = 3 AND CalculationID = @CalcID AND ClientProfile = @ClientProfile), '') DECLARE @Clause4 nvarchar(MAX) = ISNULL((SELECT Clause FROM [AnalyticsMapping].[SLA].[mpVariables] WHERE ClauseID = 4 AND CalculationID = @CalcID AND ClientProfile = @ClientProfile), '') DECLARE @SQL nvarchar(MAX) = ' SELECT CASE WHEN B.SLALabel IS NOT NULL THEN B.SLALabel ELSE ' + '''' + @SLAType + '''' + ' END AS ''SLAType'' , A.DateRange , ''OrderID'' AS ''IDType'' , B.ORD_ID AS ''ID'' , A.Client , A.Country , A.Region , A.LaborTypeCase AS ''LaborType'' , B.Numerator , B.Denominator , B.MeasureQuantity , B.MeasureDescription , NULL AS ''Goal'' , NULL AS ''isFinancial'' , NULL AS ''Red'' , NULL AS ''Orange'' , NULL AS ''Yellow'' , NULL AS ''Chartreuse'' , NULL AS ''Green'' , NULL AS ''Threshold'' , NULL AS ''ComputedMinutes'' , 0 AS ''isContractual'' , ' + '''' + CAST(GETDATE() AS varchar) + '''' + ' AS ''Mod_Stamp'' FROM SLA.veDate A LEFT JOIN ( SELECT * , CASE WHEN ComputedMinutes &lt;= MeasureQuantity THEN 1 ELSE 0 END AS ''Numerator'' FROM ( SELECT DateRange , ORD_ID , A.Client , Country , Region , A.LaborTypeCase , ' + @Clause2 + ' , ' + @Clause3 + ' , dbo.NetworkMinutes(' + @Clause2 + ', ' + @Clause3 + ') AS ''ComputedMinutes'' , 1 AS ''Denominator'' , MeasureQuantity , MeasureDescription , B.SLALabel FROM ' + @TempSource + ' A LEFT JOIN [AnalyticsMapping].SLA.mpLaborMappings B ON B.Client = A.Client AND B.SLAType = ' + '''' + @SLAType + '''' + ' AND (B.[Labor Type] = A.[Labor Type] OR (B.[Labor Type] IS NULL AND A.[Labor Type] IS NULL)) AND (B.LABOR_TYPE2 = A.LABOR_TYPE2 OR (B.LABOR_TYPE2 IS NULL AND A.LABOR_TYPE2 IS NULL)) AND (B.LABOR_CATG = A.LABOR_CATG OR (B.LABOR_CATG IS NULL AND A.LABOR_CATG IS NULL)) LEFT JOIN [AnalyticsMapping].SLA.[mpGoals] C ON (C.SLALabel = B.SLALabel AND C.Client = A.Client) OR (C.Client = A.Client AND C.SLALabel = ' + '''' + @SLAType + '''' + ' AND B.SLALabel IS NULL) OR (C.Client = ''DEFAULT'' AND C.SLAType = ' + '''' + @SLAType + '''' + ' AND A.Client NOT IN ( SELECT DISTINCT Client FROM SLA.mpGoals WHERE SLAType = ' + '''' + @SLAType + '''' + ' AND Client &lt;&gt; ''DEFAULT'')) WHERE A.Client = ' + '''' + @Client + '''' + ' ' + @Clause4 + ' AND A.SegmentID = ' + @SegmentID + ' ) Z ) B ON B.DateRange = A.DateRange AND B.Client = A.Client AND B.Country = A.Country AND B.Region = A.Region AND B.LaborTypeCase = A.LaborTypeCase' BEGIN TRY INSERT INTO [AnalyticsMapping].[SLA].[tabDashboard] EXEC sp_executesql @SQL INSERT INTO [AnalyticsMapping].[SLA].[tabCalculations] SELECT @Client , @SLAType , @SQL SET @Loop = @Loop + 1 END TRY BEGIN CATCH INSERT INTO [AnalyticsMapping].[SLA].[logErrors] SELECT @CalcID , @SessionID , @Client , @SLAType , ERROR_NUMBER() , ERROR_MESSAGE() , @SQL SET @Loop = @Loop + 1 END CATCH END COMMIT TRANSACTION END TRY
Looks like TIMESTAMPDIFF isn't a function in my version of SQL Server
Thanks!! I about choked when I saw it was a single SP. The good thing is that the current SP is pretty modular already. The common subqueries are in logic branches that are shared among the reports that need them. So just splitting each step into its own separate SP will likely be the simplest approach. I will definitely see if I can move any of the simpler subqueries into Views. And getting rid of the dynamic SQL will allow each of the queries to be optimized. My limited knowledge of indexes suggests that it is OK. At least every column that is searched or aggregated is indexed. Key constraints are in place and unique indexes where appropriate. The upcoming changes are supposed to improve normalization so it should be easier. I do need to learn more about Execution Plans. Until now the app has had a small enough user base that we could throw hardware at any performance issues. But the DB has over 3B rows in it now and usage has tripled in the past 2 years. Thanks again! 
+1?
Where +1?
It sounds like you have a good idea of what issues need to be addressed. For learning execution plans, here's a great conference talk/presentation from Brent Ozar to get your feet wet: https://youtu.be/uwGCPtga06U Good luck!
I dont think my comment changes anything dramatically. I would suggest using a cursor and @@Fetch_status for getting your Client instead of using ID of the temp table and selecting on each iteration. on the second point - I do prefer to have column lists for inserts even if you controlled the table creation. Other than that and as much as I understood your plan sounds completely workable.
To be honest I've never written a cursor, don't really understand how they work, and have always been able to "hack it" with a loop. At the moment my entire process is very efficient and quick so I'm not going to be rewriting anything at the moment. I realize this is a technical deficiency on my part, but I generally ascribe to the school of though that something isn't stupid if it works, and why fix something that isn't broken.
Is there really a valid use for Access anymore? Honest question, I'm not being snarky.
This is the current issue I'm facing at work with a system that was put in as a spare tire in 2008 with excel, vba, csvs, and ms access. The original project vendor for an MES implementation did a poor job and wanted a lot more money to fix it. 
Thanks! Learning about ETL now.
How stored procs are used, named, and documented is going to be highly dependent on your organization. I'd start by reading through the names, then reading through a few with names that seem to be relevant to your task. Hopefully they have comments explaining their purpose. Once you've taken some time exploring things on your own, you might discover that you have enough info to move forward. Otherwise your best option will probably just be to ask a more senior dev if there are existing stored procs that would be helpful for the task.
In SSMS, you can right-click SPs in the object explorer and choose "View Dependencies." You can view objects that depend on the SP and objects that the SP depends on. Could be helpful to get a high-level picture. &amp;#x200B; But like the other guy said, you'll just need to spend a lot of time familiarizing yourself with the database. If the stored procedures don't insert/delete/update, pull the query from the SP definition and run the individual statements to see what they produce. It would be best if you have the same DB on a dev instance. From the sound of it, that's probably not available to you. &amp;#x200B; Also adding to what has already been said: don't be afraid to ask another dev that is more familiar with the code. It's not a sign of weakness. Don't waste time trying to figure something out when someone has already spent the time to do it. &amp;#x200B; I'd wager that you are probably in a position where there is no "senior dev" to help out. If that's the case, this is a great opportunity for you to document your findings. What's your comfort level is with T-SQL? Do you have a software development background? Just out of curiosity.
Write a recursive procedure that examines the table and generates a new value using SEQUENCE. If the new value exists, call the procedure again from itself and let it run until one isn't found. SEQUENCE isn't transactionally bound, so it'll still increment before you issue your COMMIT TRANSACTION.
&gt;datediff(day, '2018-05-01', order\_date) / 7 as weekorder datediff(day, '2018-05-01', order\_date) / 7 **+ 1** as weekorder
Sure, but it's likely that potential employers will glance over it since the queries have no practical application. I would suggest creating an interesting project that utilizes SQL and solves a problem -- even if it has been solved before. Kind of like a programming assignment. This and contributions to open source projects are the type of things interviewers look for on GitHub. &amp;#x200B; Tbh, there is soft divide between software development jobs and database development jobs. I can't say that I've ever interviewed a database developer that included a GitHub link on their application. Maybe 1 or 2 if they had software development experience.
We ([SeekWell](https://seekwell.io/)) support MS SQL Server, check it out and let me know if you need a hand getting started or would like a quick demo. The app is designed with analytics in mind, so it won't be a good fit if you're mostly doing DBA work. Some of the key features: - Shared code repository with you and your teams entire query history - Automation (e.g. automatically send query results to Google Sheets or Slack every hour) - Modern UI / UX and tons of shortcuts to fly around the app and get analytics done faster
&gt; Learning it once doesn’t really require re-learning Learned SQL over 10 years ago and it's still mainly the same stuff. Learned JavaScript a few years ago (mostly starting with AngularJS) and the libraries and tooling around it constantly change. If you're looking for staying power, learn SQL. As the author mentions, CTE's have been a solid addition. If you're more familiar with something like Python or R and learning SQL, CTE's will be a more familiar way of working thru a data problem.
Query Express is probably the most minimal and is just an executable.
If you also do development work, SSMS is apart of Visual studio so I would suggest using Visual studio 
I personally use Toad Data Point since my company has a license for it but it isn't free. I love it because of its historical results sets (it saves the results of your last 5 query runs), error messaging, and auto-completion. I've heard SSMS may have the ability to do historical results sets but I haven't found a way to do it. It is expensive though (like $600 for 1 license) so unless your company has a license in not sure they would pay. 
If you have a data dictionary, get familiar with it. It will make alot more sense if you can get a good idea behind what every column's data represents to the business. After that, take a look at the SPs and try to write a description of what it does step by step. After that, follow some other suggestions (like taking the SQL in your sp and running the query and see what happens). Also, there is a possibility that the SPs that exist are used for internal maintenance and are not applicable to reporting at all (if you see inserts, updates, or deletes . . these wouldn't be for you).
Thanks for the reply, I'm leaning towards not putting it up and will probably look into doing a project as you suggested.
SSMS is a separate product from Visual Studio. It's built with the Visual Studio Shell, but they're separate and distinct pieces of software. You may be thinking of SQL Server Data Tools, which *is* part of Visual Studio. And not terribly useful on the administrative side.
Don't fall into the scraping trap. It's a black hole, don't waste time on it. It's a gray area legally speaking, looked down upon by many, not any good ways to make money off of it, and it's frustrating to work with, and a very bad source of data.
Ha! A month later and I'm coming back to this because we've decided to create a new table. I'm using temp tables in the meantime. :)
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/powershell] [Help with dbatools\/Powershell](https://www.reddit.com/r/PowerShell/comments/aqo2p3/help_with_dbatoolspowershell/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
He was being sarcastic about that because of the hard time that I gave him before. Set-based queries 4 lyfe!
Really depends on what your other skills and experience are. You aren't going to make money on SQL alone. You have to leverage other experience and skills together. Building an application is a bad way to learn SQL since it costs a lot to develop other stuff around it. Instead, I suggest you play around with open datasets. Check out /r/datasets for ideas. You'll need a database server. If you're in the US, Ireland, Canada, India, corporate-ware holds weight, so MSSQL, Oracle are your choices. If you're in the rest of Europe, PostgreSql and open source stacks hold more weight. You can either waste time setting it up on your computer, or if you rather spend money or have a bad internet connection or weak computer, then you're better off getting a cloud server instead. AWS, Azure, Google, etc. So then download and make sense of the NOAA data, do some statistics. Download the NY cab rides data, play around, invent questions, arrange the data in a proper way, read about OLAP, normal forms (3rd and 4th), star schema, snowflake schema, flat structure. Use ETL tools if you have to. This little project will make you learn deep knowledge about data modeling and SQL, in practice. It will enable you to do whatever you want with SQL. You can then be an analyst, data engineer, go into data science with additional courses, DBA if you get more experience, etc. 
Is this Microsoft SQL Server? If so, chances are these are all scheduled via the SQL Server Agent
Yes, this is Microsoft SQL Server. Forgive me if some of what I say doesn't make sense or if it doesn't really fit with the SQL lingo, I'm pretty new to the whole SQL thing and I'm just trying to get the server to run more efficiently. How would I go about taking a look at these? Is there a SQL Server Agent application that shows the schedule?
You need to set the scale and precision of your decimals. &amp;#x200B;
It's like the variables are being turned into ints because a scale and precision wasn't specified. That leads me to believe that it's a decimal with a precision of 0, and the decimals beyond the precision of the variable are rounded off.
 DECLARE @decA DECIMAL = 1.0, @decB DECIMAL = 0.5, @decC DECIMAL = 0.4, @decD DECIMAL = 0.1; SELECT @decA * (@decB - @decC - @decD) AS DEC_RES_NR , @decA AS decA , @decB AS decB , @decC AS decC , @decD AS decD DEC_RES_NR|decA|decB|decC|decD -:|-:|-:|-:|-: 1|1|1|0|0
Besides the obvious solution which would be to address the potential refferential integrity issues by doing exactly what your manager told you not to do, You could address this by usings a subquery with your insert like (SELECT MAX(ID) + 1 FROM Table). If you incorporate T-SQL to carryout the transaction you won't have to worry about inserting over yourself. The downside to this approach is performance because you will be locking tables up to do this, but if you cannot do the former, I'd say the latter is the next best solution &amp;#x200B;
(2,1) worked for what I needed. Thank you
You are correct, SQL Server Data Tools is what I was referencing and not SSMS sorry!
Nah, I looked at it a month ago and it simply sucks.
I loved Azure Data Studio but it is buggy. I almost always use SSMS again. 
&gt; Program -&gt; database -&gt; SQL query results -&gt; Excel sheet -&gt; Excel functions -&gt; another Excel sheet -&gt; Access database -&gt; Access form &gt; &gt; I shit you not... people out there do this. They typically do this because IT says "no". Or when they ask for something to get this information process they get "3 years and 3 million dollars". So they figure out what they can using the tools they have available. Then you end up with an enterprise full of people hacking together their own solutions because IT can't/won't provide the solutions they need. Then IT folks complain about and mock their solutions.
&gt;everything is a data source if you are brave enough. I just printed this out and put it on my office wall. :)
I wish I could say the same. Unfortunately I work with a lot of legacy systems. This week I have been working on migrating a component out of VB6 and VFP. It feels more like archeology then programming. 
SQL Server Management Studio, you'll connect to the SQL Server with that, then at the left side, Object Explorer, will have a "SQL Agent" tree... expand that out and under the "Jobs" tree is where they should all be. https://docs.microsoft.com/en-us/sql/linux/sql-server-linux-run-sql-server-agent-job?view=sql-server-2017#create-a-job-with-ssms
Oh man! If I had a dollar for every time I said "excel is not a database" I would be an extremely rich dude! =\]
I'd agree to a certain extent. The basics don't really change but since moving into a more DBA / optimisation mindset I've had to relearn a few things and get into the nitty gritty of the engine itself. It's simple enough to write query X for problem Y but if query X tanks the app you need to know how and why and what to do to make it better. 
This article says almost nothing at all, it's just blogspam. 
I use the SSMS (SQL Server Management Studio) with the Red-Gate SQL Prompt Plugin. It a powerful tool that helps writing queries and I find it much better than IntelliSense. 
Thank you so much! This was super helpful! Thank you for taking the time to respond to an internet stranger! 
This also often happens because IT doesn’t want split databases. We are stuck using Excel even if we know SQL, because we aren’t allowed to put Access or SQL Server on our PCs.
It bothers me that some IT departments take such a totalitarian approach with the business. If IT already hosts a SQL server, and doesn't have the resources to devote development time to moving Excel monstrosities to SQL, they should allow savvy business users the opportunity to create a database on this server. It is not hard to lock permissions down to a single database and if they are that concerned, the business user could always develop on a local instance of SQL server and send scripts to the database administrator for deployment. I can already hear the argument that a lot of higher ups would make with this approach. They would say that these business users don't know enough to be using SQL and that they will just inadvertently create inaccurate reporting, etc.. However, that already happens in Excel all the time. Plus, unlike most developers, the business users (hopefully) have a good understanding of their data and how it relates to the business. I've been on the IT side for awhile, but came from an analyst position and this sort of thing is a huge pet peeves of mine. /* end rant */
Find the what highest?
Any number - find the 4th highest salary from a list for example
Could you do order by [whatever] descending offset 4, then sub-query that and do order by the same [whatever] ascending offset 1? I'm not extremely familiar with MySQL, but it's just a thought. 
CHAR is fixed length. Cast it as VARCHAR, which is variable length. 
&lt;3 Thank you!
To expand on this, SQL will pad a CHAR to its full length. VARCHAR will be stored as a number of characters equal to the length of the string plus one. The plus one marks the end of the string, but it's not stored as a space. Unless you **know** the values in a column will always be a certain length (like consistently-formatted social security numbers or something), don't use CHAR. If there's any doubt, you should use VARCHAR.
Sql is easy, the set based paradigm is the rough part. I've seen devs using cursors because they're used to imperative programming for problems that are solved much more efficiently with an upsert, a update from select, etc
One thing that I’ve been wanting is something to notify me when a certain value apparatus in the database. For example, I have an audit table, and a long running job. I want to be notified via email or text if and when an entry appears in the table with Name = “FooJob” and IsSuccessful = 0. Can seekwell do that?
... I'm talking about the ability to SUMIF
I'm talking about the ability to SUMIF
Fuck. You have no idea how badly I needed that laugh.
Bet you haven't even seen my queries yet
I feel this so hard
I do not so I apologize beforehand. But I am curious how you set this API up to push to a database. We are using typeform for similar purposes and I want to me able to make more sense of the submissions. Any info you have is greatly appreciated. I've never set up an API before for something like this.
It's pivoted because of the case statements being their own column. Trying something like this: select datename(dw, HireDate) as Weekday, count(*) as [Number of Hires] from HumanResources.Employee group by datename(dw, HireDate)
Except you can do it from the query straight or from a query to Excel... The Access part is wholly redundant and stupid lol
Thanks mate that sounds like the best idea.
Cheers Pal I'll look into that next time I'm in work, it's the managers fault if the performance suffers.
Yeah, not sure why he's being so cryptic, think he thinks I'll learn something if I find the command myself.
Cheers mate I'll have a look into that.
Thank you! I knew I was over complicating 
Can you use a CTE to delete old numbers? Select Cust_id, phone_type, phone_number, date Into #table ;with Loseolds as (Select cust_id , phone_type , Row_Number() OVER(partition_by cust_id, phone_type ORDER BY date DESC) as lastnumber From #table ) Delete from Loseolds where lastnumber &gt; 1 ; Select * from #table 
That error means not all needed components are on the machine you are doing this from. Similar discussion here: [https://social.technet.microsoft.com/wiki/contents/articles/35832.sql-server-troubleshooting-could-not-load-file-or-assembly-microsoft-sqlserver-batchparser.aspx](https://social.technet.microsoft.com/wiki/contents/articles/35832.sql-server-troubleshooting-could-not-load-file-or-assembly-microsoft-sqlserver-batchparser.aspx)
You definitely want to use varchar here, but as a side note about char and varchar in SQL Server: &amp;#x200B; In this scenario, char and varchar default to a length of 30 characters. In other scenarios, the default length is 1. As a best practice, always specify the length of varchar and char. 
Because an index (sorted, maybe a btree) tells you where you can find the data in the primary (unsorted) table. But a multi-column index might already have the data you are looking for, so no need to read a section of the primary table to get that data. If you know your queries are always going to be looking for A where B, then a B,A index can be an optimization. Just remember indexes are not free.
Grapholite is available for iPad, android, windows.
All you ever wanted to learn about indexes: [https://use-the-index-luke.com/](https://use-the-index-luke.com/)
I work with millions of addresses and we mostly query them by state and county. We almost never would filter on just county without including state, so the multi-column index is better. Another example is that we almost never pull a sales report without filtering by date and product type, so those two make an effective pair in our case. If you find you are heavily filtering with the same set of columns, you might have good reason to use them, but as another person already mentioned, indexes have a cost. 
Use subquery and get nth highest (It's for Mysql) &amp;#x200B; set @ab=0; select [b.](https://b.bb)emp1,b.salary1,b.rank from (select a.emp as emp1,a.salary as salary1,@ab:=@ab+1 as rank from (select emp,salary from empdetails order by salary desc) a)b where rank =4;
Switch to Mongo DB. It's super fast and completely webscale. SQL is for old dads that eat kale chips
No. &amp;#x200B; (Anything that has data in it) != (a database)
Wow this is great
I would suggest that you run a server side trace that will capture everything being run on your instance, you may need to let it run for several days to capture everything. After you have the data it should be easy enough to load it back into a table and have a look at it.
MySQL 8.0 finally supports window functions - including row\_number()
Thanks for your reply. By "names" you mean to look at the custom SPs that have been named by someone, right? They don't have comments but I am trying to understand what they're doing at the moment. Just overwhelming at the moment. Other issue is the only people who have access to this are 4 people (one of them being me) and the 2 senior SQL people work in a different country with a whole different time zone.
I've viewed dependencies and its not really given much information, unfortunately. Yeah I think you're right. I will have to just go through the code and try to understand it bit by bit. Maybe just look at it line by line so its easier to digest and not overwhelming. I think we do have the same access to the same databases as we don't have many databases. The issue with asking another dev is that they're all the way across the other side of the world and I think they would have to take some significant time out to walk me through the databases and just teaching me some of the stuff. My background is, honestly, expert at excel, pretty good at VBA, can query in SQL and have created one or two dashboards in Tableau. Also, we're using SQL Server Management Server which is SSMS, right?
I've never heard of a data dictionary but will ask the senior devs if they have any. I'm not unfamiliar with the actual data in the databases as I have worked with the data for a while. However, I'm not 100% sure which table has which data. I think this will be one of them things where I will have to manually look through the tables and try and get an understanding of where everything is.
[apt username](https://www.youtube.com/watch?v=YpufzJNrZ2Y)
Yep. This is indeed one of those things
Try the traces that come with sql server to see what's running, output to a table and then filter out what you need Also Brent ozar has some cool scripts for diagnosing About the ram, sql server will use all it can get unless specified not to
Thank you very much, this really was a difficult thing to explain, especially when the internet only contains easy examples with the typical ManagerId = 0 etc. So thank you, and what is your youtube channel? You don't seem to have one yet? I will definitely subscribe.
Index is basically the same table data, only sorted differently. This helps queries that use that same column order to read data faster, because it's already pre-sorted. Think of it as a telephone book that is sorted by last name only. If I ask you for all names that begin with "Smith", all you have to do is look up which page starts with letter "S", and then flip forward until you find the next letter, and so on, until you hit the first last name that is different. Very efficient. That's basically your clustered index. But what if I ask for all Smiths who live in a certain zip code only? Now you'll have a much harder time, because you'll need to read each Smith's zip code to decide weather or not to give it to me. But what if there was a second phone book that was sorted both by last names and zip codes? Now your search is fast again: find first Smith with that zip code, and keep reading until one of those things changes. The downside - you now have two phone books with the exact same information, only sorted differently. So yes, each additional non-clustered index that you create for a given table will make some queries faster, but it does take up disk space.
Ha-ha, been there! I can usually spot if a full stack person has created some sp/function as they think it's c# code style in SQL. Still trying to educate them on set theory .. 
Gotcha, from your article, I found my way here: https://stackoverflow.com/questions/47930742/processing-sql-xevents-in-c-missing-an-assembly This sounds like my issue. The OP said he "manually included it in my project." What does that mean? How do you do that through Powershell?
In short, you are right. There isn't going to be an elegant solution to this in SQL so you can either make a messy/complicated solution, or try to use other tools in combination to accomplish your task. Depending on how often your report the data in this format, you could look into getting DBAs to set up a data mart that will feed that data to you in the format you want. Are you limited to only SQL? This would be solved easier by feeding your SQL data into a console app iterate through your rows and build a new table (report) as it goes. You could do this with pretty much all the mainstream OOD languages (python, java, C#, ect.) and that will give you greater flexibility on how this report can be delivered (could become a dashboard or GUI hosted on a website).
Yes, we only support Slack alerts at the moment, but you can set up a SQL statement that will check for that condition and send a message when it's hit.
I literally had 'learn how indexes work' on my to-do list when I woke up this morning. This is beyond coincidence... I think I may have an exceptionally high concentration of midichlorians 
No problem. Yes that's what I mean by names. Sounds like a challenging task, but it shouldn't be impossible. Good luck!
https://youtu.be/b2F-DItXtZs
Yeah, SQL for analytics and SQL for DBA are different skillsets. There's a whole list of functionality that barely overlap (e.g. you might never even need to think about creating an index as an analyst as long as your DBA is doing their job).
For what it's worth, this is not my blog. I found this on Hacker News and thought people here might like it.
You can more or less rewrite a lot of your WHERE clause stuff into a join. You're telling it not to join on those rows, but to join on the keys where the rows do match. It isn't nonsense, it's what you told it. SQL, like any language, operates using the fundamental principle of **G**arbage **I**n, **G**arbage **O**ut. It doesn't know it's garbage, and it doesn't know it's nonsense, and it doesn't know what it's telling you... you should know those things and not try to trick it.
Hmm, so I have a second question then. This query results in a key1/key2 pair that has a non-null value in a.agg_1 and NULL in b.metric_1 . How is that possible if both sub queries have values for agg_1 and metric_1 , and both values *are* equivalent, for the same exact key1/key2 pair and same exact time1 period? This is why I threw in the garbage verbiage.
It makes sense when you consider that a JOIN condition is just a WHERE condition in a convenient location. For example, the following two queries are the same SELECT * FROM Foo INNER JOIN Bar ON Foo.BarId = Bar.Id SELECT * FROM Foo, Bar WHERE Foo.BarId = Bar.Id
Thanks for the reply. This is a one-time request, but it is an important and urgent request. I don't expect that I'll need to reproduce the report again (if I do, it wouldn't be until next year). For these reasons I elected to go with CASE statements just to have something deliverable. The basic solution consisted of figuring out, within the award subquery, which awards a person had received; and assigning a number to it (so someone with A only got 1, someone with B only got 2, C-only got 3, A+B got 4, A+C got 5, etc etc). Then in the outer query taking some action in each of the six columns based on what that number was for each person. It's pretty ugly but it works just fine. I am not limited to SQL only. If I start getting a regularly recurring report of this sort I will definitely explore those options.
You can join on 1 &lt;&gt;2 if you want. It's just going to give you a cartesian product. In your example, if a.agg_1 and b.metric_1 are truly never equal, then the first two join predicates are the only ones that have an actual effect on the outcome. Of course if a.agg_1 and b.metric_1 happen to be equal by coincidence, those rows will be excluded from your result set.
I'm not sure but are you asking why if one value is NULL that it won't join (i.e. why isn't NULL &lt;&gt; 1 = TRUE) or are you asking why if both values are NULL why they are joining? NULL's are a special case when it comes to joining is all I can tell you, and joining on NULL can be a lot of fun. Someone smarter than I am can probably explain to to you precisely why NULL &lt;&gt; NULL is different than 1 &lt;&gt; 2, etc.