I think, that in this case, the query is: Select * from tb_transactions[t] Inner join tb_product[p] on p.idProduct = t.idProduct Inner join tb_consumer[c] on c.idCostumer = t.idCostumer Order by t.value asc The "[t]" is a alias of the table "tb_transactions" The "[p]" is a alias of the table "tb_products" The "[c]" is a alias of the table "tb_costumer" In the reserved word "on" is the column who link a two tables, in this case "idProduct" or "id costumer". When you want use the reserved word "on", you need a column who link a two tables. That column need have a equals value, in the two tables, example t.idProduct = 5 p.idProduct = 5 Linked the row t.idProduct = 5 p.idProduct = 4 Not linked the row
What type of form?
I'd like your input on expected TSQL skills for a junior or senior in college since you're an instructor, if you don't mind. I've been doing all our dept's data manipulations in MS Access through an ODBC connection to SQL Server. We're looking to move more heavy querying to true SQL statements. Would a college student know enough in SSMS to do complex joins, CTEs, table variables, advanced functions, and use this to migrate from MS Access? I'm wondering if the internship is too much? Can you share your syllabus topics? What specific CS major focuses on data querying/transformations? Thanks.
Check out https://sqlzoo.net
&gt; where we delete a row from a table but have user input for the WHERE section User Input: `1 = 1`. Seems totally safe. 
Would echo this. For some practical application questions I would also check out [SqlZoo](https://sqlzoo.net/). They provide a decent number of tutorials and questions to help you apply what you learn in a meaningful way. 
look into 'unpivot'
Can someone explain to me why? Is it simply a comfort option? I can’t see any other reason why you’d chose to run MSSQL unless you wanted a product that required it. 
Look up regular expressions 
Look up regular expressions 
You're looking for a regular expression. A phone # would look like \d{3}(.|-)\d{3}(.|-)\d{4}. 3 digits followed by . or - followed by 3 digits followed by... You could do something like: substr(description, instr(description, '\d{3}(.|-)\d{3}(.|-)\d{4}'), 12) instr( [to_be_searched], [pattern_to_search] ) Returns the offset in the string which is used by substr to snip out 12 characters (10 digits plus 2 delineators). If you google regular expression tester or checker, you can play around. 
Lead Data Engineer here. Pretty much similar path as well. Learned everything on the job and on my own (books, online, peer groups). Started off as a report writer using MySQL and Access writing queries. Then was lucky enough to have been entrusted to write code to create “web/html” reports. That lead to becoming a software engineer for a couple years. Then decided being in the “data world” would be a more lucrative and interesting career so did more BI and Data Warehousing. After that moved to an ETL/ELT developer. Which lead to data engineer and big data roles and now lead data engineer. All in all a 12 year career that had nothing to do with my college degree.
Select Count(*) Over(partition by case_ref)
YouTube: Kudvenkat, and Wise Owl Tutorials. All fantastic
Correct if I’m wrong but I believe what you are trying to do is have the ability to have multiple recipes rolled into a single master recipe. So strawberry limeade recipe is a combination of strawberry lime + lemonade. If that’s the case I would have a “parent recipe” and a “child recipe” table. So Strawberry limeade (id=1) is in parent recipe table and strawberry lime (id=1) and lemonade (id=2) is in the child recipe table and both has a column (parent_id) that references the parent recipe tables id (parent_id=1)
I scored 1002. \*cries in SQL*
Maybe I'm not reading this correctly, but can't you just copy and then paste "Transformed"? This will take all of the columns and make them rows and vice versa. 
For running it in a farm of linux docker containers.
I'm currently refactoring a data architecture environment where their entire critical system's database (surprise, an ERP) is without keys...any keys. All joins are composite joins where the same 3 columns are used as the "base" join for every table, with slight variations based on the relation. Most "main" tables that you would think of like Customers, Orders, Employees, etc. aren't actually main tables, but aggregates of a bunch of different dynamic tables - in which case these main tables have hundreds of columns with each of them. All tables have a "DBKEY" column that is literally just a ROW_NUMBER and takes up the clustered index. It has no other use. Indexes can only be created on tables via the application GUI, because it does some sort of caching, and if an index is created via the database then it will crash the application. There is heavy use of materialized views that the application uses for the querying forms on the application GUI. All views are named A1, A2, ..., A275, etc.) There is heavy use of materialized views using other views. When you go to actually script out the view creation scripts you'll find that if View A213 uses a query from A17, A9 and A56, they're aliased in the script as A17 AS A1, A9 AS A2, A56 AS A3. When I first arrived on the scene there were some table statistics that hadn't been updated in **years** Most all auditing was turned off from the database end, as the application handled auditing - but the application also had auditing turned off (found this out when somehow the application updated an entire control table for manufacturing processes and set each row to inactive). This is an old enough ERP system that I've been able to determine that these weren't auto generated, based on the tool sets they have nor the documentation they've provided, someone purposefully designed the database this way. By the time I'm refactoring this environment I should write a book about database refactoring. 
Nice.
I found this article about setting up email triggers: https://sweetcode.io/using-triggers-and-email-alerts-in-microsoft-sql-server/ You'd have to add the logic in the trigger that the user doing the data changes isn't one of the users an app runs as. so like... IF user != DOMAIN\AppDBUser do trigger
If your database supports it, try Unpivot, otherwise you can join to a table of numbers to expand the rows, and use a case expression to create the columns.
Ending single quote after LIMIT?
What we do at work is mostly joins, unions, and a lot of subqueries. A lot of them. Most of the data validation and data manipulation takes place outside. It depends on what you do with SQL, but working like crazy on various select statements won't be a waste of time. Sub queries with like 3,4 nested select statements with a few joins, some unions. That is a regular query for me.
&gt; does anyone know what it means? it means you have an error in your SQL syntax if you could post the query, we might be able to pinpoint it for you one thing's for certain -- the error occurred near the word AND
a failover cluster has a couple different requirements *AD permissions * network/disk dependencies *shared storage (there are ways around that but for a standard FCI, you shared storage) * 2 compute nodes (anything less wont allow the sql service to come back up if one goes down *Quorum, file share witness being the standard option more or less. follow these steps for a more comprehensive set of steps https://www.mssqltips.com/sqlservertip/4769/stepbystep-installation-of-sql-server-2016-on-a-windows-server-2016-failover-cluster--part-1/
You should be able to use the month() function with a case statement so achieve this. -- example table create table #months ( closedDate date ) -- create sample data insert into #months values ('2019-01-01'), ('2019-02-01'), ('2019-03-01'), ('2019-04-01'), ('2019-05-01'), ('2019-06-01'), ('2019-07-01'), ('2019-08-01'), ('2019-09-01'), ('2019-10-01'), ('2019-11-01'), ('2019-12-01') select closedDate , datename(month, closedDate) as monthTitle -- gives name of the month , month(closedDate) as monthNumber -- gives number of month , case when month(closedDate) = 1 then 'Styczeń' when month(closedDate) = 2 then 'Luty' when month(closedDate) = 3 then 'Marsz' -- etc end as polishMonthName from #months -- cleanup drop table #months
What database are you using? In Oracle: &gt; select to_char(sysdate,'MONTH') monthname from dual; MONTHNAME MARCH 
Learn in this order: select Insert Delete Update Then join After this you can look at the different types of joins Once you've gotten this far, the other stuff is not so difficult. It's just a case or messing around with a test database You can learn select, insert, delete and update in a weekend Joins in about a week The rest of the stuff will take a while Also look into the concept of normalisation. This is an important aspect and is required before you start working with joins 
I'm not sure what a cognitive equation is, but in Excel, `=A1&amp;A2` means to concatenate the contents cell A1 and A2. There is no common standard for text concatenation. The method varies by RDBMS. Common examples: SELECT Field1 + Field2 SELECT Field1 || Field2 SELECT CONCAT(Field1,Field2) However, that only works for fields that are a part of the same record or row. What you're describing, however, sounds like a text aggregate concatenation function. So if your data looks like this: |Id|StaffName|Site| |-:|:-|:-| |1|Alice|High School| |2|Bob|High School| |3|Carol|Middle School| |4|Dave|Middle School| And you want your query results to look like this: |Site|Staff| |:-|:-| |High School|Alice,Bob| |Middle School|Carol,Dave| That is usually known as `GROUP_CONCAT()`, from the MySQL function of the same name because that RDBMS introduced it first. However, again, there's no standard function and it will vary based on your RDBMS. GROUP_CONCAT() -- MySQL, SQLite STRING_AGG() -- SQL Server 2017 or later only listagg() -- Oracle 11g or later string_agg() -- PostgreSQL The functions have varying levels of features. Some have much more complex functions which allow you to control the order or the delimiting character. A query that would produce the above output would be: SELECT Site, GROUP_CONCAT(StaffName) AS Staff FROM StaffTable GROUP BY Site 
Here is a case which would consider a description and any amount of numbers before the phone number, in any format. This might be useful if you had a number with a long distance dialing digit, such as 1-800-555-1212. This solution keeps numbers only withinin the description field, then uses substring of -10 (wrap backwards) to get 10 numbers going left. This only works if there are no trailing numbers after the phone number. with mytable as ( select '12345 This phone number is 493-283-4992' phone_desc from dual union all select 'Yo fam jam my digits is (295) 252-5992' from dual union all select 'I got 2 phones 1 for the plug and 1 for the load - reach me at 253.266.2112' from dual union all select 'Telephony device is 6852569285' from dual ) select substr(regexp_replace(phone_desc, '[^[:digit:]]'),-10) as phone_digits from mytable; PHONE_DIGITS ------------ 4932834992 2952525992 2532662112 6852569285
Hey how would you do this in excel? Because this would be an option for me. &amp;#x200B; I can export to excel then do whatever I need to. &amp;#x200B; I'm just trying to be efficient
this is definitely a server error, the webkinz website uses sql to house its reference tables for encoded email and password combinations, when attempting to reset your password it ran a select statement to look up your info before allowing a reset and the lookup errored out. This isnt your problem.
beautifully detailed and well researched reply, worth several more upvotes than i have available to give it also, saved me the trouble of pimpin `GROUP_CONCAT` again
i think you overlooked this part of OP's request -- &gt; I need to convert date into month name **in my language (Polish it is)** 
If the DB language settings are set to Polish, it will return the value in the local language.
Copy the rows and while pasting them select Transpose.. 
Do you know how I could fix it..? I emailed webkinz and they haven’t responded back yet 
From time to time I've simply used the Excel CONCATENATE function to pull values into a short SQL statement
So, I think a common thing college students need to learn, is that classes teach you how to think, how to solve the problems, and basics in coding. The Jobs/Internships is where you actually learn how the work goes. In my class, i rarely go into depth on many of those topics, as they vary from job to job. I focus on the database topics and SQL, and really focus on just the DML. Some of the syllabus is below: |\&amp;nbsp; \*\*Date\*\*|\&amp;nbsp; \*\*Topic\*\*|\&amp;nbsp; \*\*SQL Lesson\*\*|\&amp;nbsp; \*\*Assignment/\&amp;nbsp; Quizzes Due\*\*| :--|:--|:--|:--| |\&amp;nbsp; \*\*1/15\*\*|\&amp;nbsp; \*\*Course Overview and Introduction\*\*|\&amp;nbsp; \*\*\*\*|\&amp;nbsp; \*\*\*\*| |\&amp;nbsp; \*\*1/22\*\*|\&amp;nbsp; \*\*SQL Overview \&amp;amp; Background\*\*|\&amp;nbsp; \*\*MySQL Install \&amp;amp; Load Data\*\*|\&amp;nbsp; \*\*\*\*| |\&amp;nbsp; \*\*1/29\*\*|\&amp;nbsp; \*\*SNOW DAY\*\*|\&amp;nbsp; \*\*\*\*|\&amp;nbsp; \*\*\*\*| |\&amp;nbsp; \*\*2/5\*\*|\&amp;nbsp; \*\*Data Modeling: ERD\*\*|\&amp;nbsp; \*\*Basic Querying 1\*\*|\&amp;nbsp; \*\*Quiz 1\*\*| |\&amp;nbsp; \*\*2/12\*\*|\&amp;nbsp; \*\*Relational Model\*\*|\&amp;nbsp; \*\*Basic Querying 2\*\*|\&amp;nbsp; \*\*\*\*| |\&amp;nbsp; \*\*2/19\*\*|\&amp;nbsp; \*\*Database Design -\*\*|\&amp;nbsp; \*\*Aggregate Functions\*\*|\&amp;nbsp; \*\*Assignment 1 Due\*\*| |\&amp;nbsp; \*\*2/26\*\*|\&amp;nbsp; \*\*SQL –DDL - Essential SQL Skills and MySQL\*\*|\&amp;nbsp; \*\*Easy JOINS\*\*|\&amp;nbsp; \*\*Quiz 2 is cancelled, rolled into Midterm\*\*| |\&amp;nbsp; \*\*3/5\*\*|\&amp;nbsp; \*\*Midterm Exam – Online\*\*|\&amp;nbsp; \*\*\*\*|\&amp;nbsp; \&amp;nbsp; | |\&amp;nbsp; \*\*3/12\*\*|\&amp;nbsp; \*\*Spring Break\*\*| | | |\&amp;nbsp; \*\*3/19\*\*|\&amp;nbsp; \*\*SQL Joins\*\*|\&amp;nbsp; \*\*Complex JOINS\*\*|\&amp;nbsp; \&amp;nbsp; | |\&amp;nbsp; \*\*3/26\*\*|\&amp;nbsp; \*\*Database Administration\&amp;nbsp; Database Processing Application\*\*|\&amp;nbsp; \*\*Other JOINS\*\*|\&amp;nbsp; \*\*Assignment 2 Due\*\*&lt;p&gt;\*\*Quiz 3\*\*| |\&amp;nbsp; \*\*4/2\*\*|\&amp;nbsp; \*\*Warehousing and Data Mining\*\*|\&amp;nbsp; \*\*\*\*|\&amp;nbsp; \*\*\*\*| |\&amp;nbsp; \*\*4/9\*\*|\&amp;nbsp; \*\*Physical Database Design, Performance Monitoring And Database Tuning\*\*|\&amp;nbsp; \*\*Other SQL Functions\*\*|\&amp;nbsp; \*\*Assignment 3 Due\*\*| |\&amp;nbsp; \*\*4/16\*\*|\&amp;nbsp; \*\*Other Technology: XML and Big Data discussion\*\*|\&amp;nbsp; \*\*Querying through Multiple Databases\*\*|\&amp;nbsp; \*\*Quiz 4\*\*| &amp;#x200B; &amp;#x200B;
Yea the problem is that I don't want to transpose. Transposing just swaps my rows to columns and columns to rows. &amp;#x200B; I want to basically append my column to a row, and then keep my main column the same but duplicate the value. &amp;#x200B;
I've been trying unpivot but I seem to not be able to figure it out. I am dealing with like 30 columns.
&gt; show the number of individuals from each state &gt; while also counting how many are from each state What's the difference? And what does your member table look like?
I dont think you can. I would imagine they will be getting a number of emails similar to yours informing them of the issue and they will address it as quickly as they can.
You would group by the state and join the lookup table on the corresponding IDs
Check out Python's beautiful soup library. 
if you're working with excel files often, create a scratch/staging area in your DB, load your data into a table, run a query/dml using data from your table. You could read data from the file directly in some implementations (OPENROWSET for MS SQL, for example) (i'm not a fan). you could just create the individual statements in excel (e.g. you have a column with ID, in the cell next to it you can enter formula "delete from mytable where id = "&amp;a1, populate for the whole list with one click then copy/paste/execute in your sql client of choice. &amp;nbsp; Now, going back to your piece of code - do you anticipate a situation where ID and description from the excel file will have a unique occurrence in the TABLE2? Your current script will remove the unique record and nothing will be inserted by the second script.
So do I need to familiarize myself with Python then?
I’m assuming based on the fact that you didn’t immediately go there, that using VBA is out? Which tells me you’re not looking to program your way out. You can suck data from Excel into MS Access too and generate your SQL with SQL, but since you didn’t go there either, it seems like you’re looking for something else. So based on those assumptions, my go-to toolkit recommendation - if I’m not writing a Python or SQL scripts - would be to use a solid modern editor like SublimeText or Atom. Depending on the dataset size, you may find it easiest to copy any paste data out of your spreadsheet into SublimeText, and then record a quick macro or use multi-cursor magic to turn the text TSV dataset into queries. The payoff for having solid macro skills in a modern text editor will benefit you again and again.
So you probably want something like: ;WITH countByState AS ( SELECT state , COUNT(*) as locCount FROM location GROUP BY state ) SELECT state.name AS stateName , loc.locCount FROM countByState loc INNER JOIN state state ON loc.state = state.code A simpler, possibly less performant query, depending on your rdbms and indexes: SELECT state.name AS stateName , COUNT(*) FROM location loc INNER JOIN state state ON state.code = loc.state GROUP BY state.name I'm assuming here you have a `state (code, name)` location that you want to join. &gt; show the number of individuals from each state while also counting how many are from each state This is unclear, I don't see the difference between those two things you want to measure. 
Is suggest the table of numbers instead, then.
So what does something like that look like? I have 3 tables now ... each table has an average of 10 columns I'm pulling from it.
I wrote an example in my first comment.
AWS RDS is free for a year with the smallest tier and 20 GB storage, that should get you started no problem? I can also set you up with a db if you're worried about getting billed, though you might have some latency between your webapp and the db server (I'm in Poland, so is my server).
Python or another programming language. Web scraping cannot be accomplished with SQL alone.
I've definitely done this for one off things.
I didn't know this even existed. Thanks.
A macro is not something I'd considered - probably because it's not something I've ever really used before. Thanks for the tip.
Ah okay. Thank you.
If you are comfortable with SSIS. You can do it via web services of available.
I have stuff that I do regularly that isn’t quite worth fully automating, so Sublime works wonders. If you get really good at Sublime, it’s not too hard to turn your ad-how macros into a Python plugin. I think NotePad++ lets you do macros too, but I’m not sure about plugins and multi-cursor.
I said that awkwardly, what I'm looking for is to select what (US) state each ID is from and then count how many ID's are from each individual state. We use a code of 0-50 for each state so after figuring out what ID is from 0-50 I want to total each separate state after to see where a majority of the ID's located state-wise for a report.
If you have fixed number of item columns then you could use UNION and select one item column in each query. 
Yes. It’s easy 
You can’t do it with sql. Simplest solution is using Beautifulsoup with python or learning how requests work 
Have a look at the [Web Scraper for Chrome](https://www.webscraper.io/tutorials) browser extension if you're not willing to put in the hours to learn Python to do this. Will crawl and scrape for the data you need.
So I finally got around to trying this but I get a syntax error on the first count dont think I did anything different to what you typed
Thank you, I have to do the same thing. 
maybe BS4 and upload wizard? put both on a clock?
I done this extensively. As others have said you’ll need an application to actually carry out the scraping. I used a tool called Web Harvest. This interrogated the pages HTML and using a template schema stored the output as an XML file. This can then imported directly or put through a parser to format / transform. Words of advice. When scraping try to only hit each page once and gap the requests. This way you don’t appear as a DDoS attack and either impact someone’s site or get your IP blocked. Due this risk, and I was doing it for commercial reasons on some pretty well known sites I used a AWS server without a static IP.
pls show the exact query you ran
I copied and pasted what you wrote
okay, let's try this replace every construction such as this -- , COUNT(CASE WHEN Type = 'Burglary' THEN 1 ELSE NULL END) AS 'Number of burglaries' with this -- , COUNT(IIF(Type = 'Burglary',1,NULL)) AS 'Number of burglaries' 
A simple(r) way to do this is to copy a list of values into column A, and then in column B you can do something like: `=", '" &amp; A1 &amp; "'"` then copy it down to the bottom to paste back into SQL. You can get a lot more verbose though if needed to do IF's, etc., and use Excel to generate the script. Another way to do it is to store parts of your SQL query in an Excel file, then load the file into your database and use a dynamic SQL loop to generate as many queries as you have.
Thank you for the response. It really helps put into perspective the expectations for an intern skill set. I'd still be very happy with anyone who really understands the topics you've listed.
How did you attempt to change the column?
Adding to the Python advocates here. Its the most flexible method you'll find. And as a SQL developer, learning how well Python can work with databases has benefited me far beyond my needs for webscraping. Truly a must have skill set foe any programmers arsenal. 
Is it an RDS instance? If so, try [this link to AWS userguide](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ConnectToMicrosoftSQLServerInstance.html) Otherwise, you could try setting up an [ODBC Linked Server](https://blog.devart.com/creating-linked-server-in-ssms-using-odbc.html) Linked Servers have some limitations, can be a security risk, and potentially have slow performance. You would need to test your specific scenario though. 
Where are you looking to intern? 
I'm the analyst/manager looking to hire a summer intern... I'm self-taught in Access, SQL and databases, so I wanted to get a gauge on current college students' skills. Based on the initial flood of resumes, it seemed like my job description may have been too advanced for college students. Coincidentally, we received 2 great resumes today from graduate students in Computer Science who have actual work experience in databases. 
excel has obdc drivers ;)
SSMS can only connect to SQL Server instances. Are you using SQL Server in AWS?
Through SSMS. I went through the edit table screen.
SSMS does not support ODBC. It is not and will probably never be an ODBC capable client. The protocols that SSMS supports are Shared Memory, TCP/IP, and Named Pipes protocols. That's all. Shared Memory is, for what I hope are obvious reasons, local host connections only and is primarily used when the other protocols are not working. Named Pipes is networkable, but it's primarily intended for LAN connections. TCP/IP is intended for WAN connections. For the most part, pretty much everybody is using TCP/IP. I would recommend following /u/Cal1gula's link to the AWS userguide. 
In addition to the fact that SQL can't do this, you may find that this violates the [site's terms of use](https://smite.guru/tou). &gt;You will not modify, publish, transmit, reverse engineer, participate in the transfer or sale, create derivative works, or in any way exploit any of the content, in whole or in part, found on the Site. Evolution Gaming content is not for resale. Your use of the Site does not entitle you to make any unauthorized use of any protected content, and in particular you will not delete or alter any proprietary rights or attribution notices in any content. You will use protected content solely for your personal use, and will make no other use of the content without the express written permission of Evolution Gaming and the copyright owner. Before you go screen-scraping the site, contact them.
Sounds like a data storage issue specific to the database engine to me but I've never tried doing this particular thing before. Rdbms should ideally warn 
It’s not unusual for SSMS to want to do a drop and re-add even when it’s not necessary. I would try using an Alter Table statement.
I got sick of the table designer and just use statements instead. Is faster too when you know what you’re doing 
What are the potential security risks to a linked server?
And yes it’s in the AWS cloud servers 
I wish Excel was as good as SSMS for composing queries :(
Yes it’s in AWS
Are you certain that it's MS SQL Server? Is it an RDS instance or SQL Server running in an EC2 VM? Have you been through https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ConnectToMicrosoftSQLServerInstance.html ?
Tyvm for the link.
I appreciate all of your responses. I am an 8 year programmer, but I am more of a front/mid guy. I know more than enough about sql, but I guess I use the tools which seriously are buggy too much.
I use use an access front end with a SQL server database backend over an ODBC connection every day. Maybe I'm misunderstanding what you are saying?
Distinct is a filter that is run after all your results, where it scans through and organizes it to remove duplicate records in the select. So for large queries and such it can hurt performance. I remember being told that if you use distinct than your query is not efficient as it should be, but I still utilize it a lot cause it is time efficient, easier, and the amount of time to invest in the right way might be to much (or even run time might not be improved).
My comment only mentions SSMS. I'm not referring to SQL Server at all.
You're right. I'm relatively new at all this. I was confounding the two.
There's an extension, a CLR stored procedure, and a package for PostgreSQL, SQL Server, and Oracle (respectively) that will allow you to make web requests from inside the database. Once it's in there you can pars the response, but if it's HTML it might be tricky and require a lot of regex It's easiest in PostgreSQL, because you can write functions in several programming languages to do this apart from the extension I mentioned. I've done this using PGs procedural extension for Python (Pl/Python). Easy peasy. Really though, if you're new to web scraping. Then it'll be easier to learn how to do it first with Python by itself. Then put the parsed or semi parsed resultsets in a SQL database for heavier analysis and data shaping in batches.
Not a helpful comment but just popped in to say wow@a RapidSQL user because I was a support rep for that product for almost 10 years.....
What are your positive/negative comments about the software? Is it worth buying? I’m going to use it for the next week before deciding to buy it or not. Is the query writing tool as robust as ssms?
&gt; yes it’s in the AWS cloud servers *Everything* in AWS is "in a cloud server." You have to know what you're really connecting to. Is it SQL Server? Is it an RDS instance, or SQL Server installed in an EC2 VM?
Good question, I think it may be a SQL Server installed in an EC2 VM. I’ll confirm this tomorrow, but the software vendor has been pretty unclear about this in the past even though we pay them to host our db in AWS. When I connected to the database using RazorSQL via ODBC and did @@VERSION I got Microsoft SQL Server 2016 (SP1-CU6) (KB4037354) - 13.0.4457.0 (X64) Nov 8 2017 17:32:23 Copyright (c) Microsoft Corporation Enterprise Edition (64-bit) on Windows Server 2012 R2 Standard 6.3 &lt;X64&gt; (Build 9600: ) (Hypervisor) Maybe it’s in RDS and why we require an awkward third party ODBC driver to connect to the database? 
&gt; Maybe it’s in RDS and is why we require an awkward third party ODBC driver to connect to the database? No, you need the "awkward ODBC driver" when you're using a client that isn't SSMS. SSMS requires no special drivers to connect to SQL Server instances. It doesn't use ODBC for this at all, it uses Microsoft's SMO libraries. You've been given the RDS connection instructions for SSMS twice, once by /u/Cal1gula and once by me. If those aren't working, then you're probably in an EC2 instance. In either case, *the people you're paying to manage your database* should be telling you how to connect to it. Do to a Linked Server, you need a local instance of SQL Server to created it on. That Linked Server will point at your remote instance. But I would only consider that as an absolute last resort.
Thanks for elaborating. I got the RDS connection instructions after I left the office so I won’t be able to test until the morning. I’ve had several support sessions with the software vendor and none have been fruitful. Their bottom line is “here is the ODBC driver and this is how you connect via Excel” I’ll have another session with them tomorrow to get an explicit answer on whether it’s RDS or EC2, but I’ll still follow the RDS instructions in the mean time. I appreciate you educating me on this.
That was easy... SELECT b.Building_name FROM buildings b WHERE ( SELECT COUNT(e.Building) FROM Employees e WHERE e.Building = b.Building_Name ) &gt; 0 SELECT b.Building_name, b.Capacity FROM buildings b SELECT b.Building_name, e.Role FROM buildings b LEFT JOIN Employees e ON b.Building_name = e.Building GROUP BY b.Building_name, e.Role
Sounds like you want to use triggers.
You need a MERGE statement. You define a source and target, and conditions to determine if a row is inserted or updated.
Neither of the 2 proposed solutions are correct. If you are using at least PostgreSQL 9.6, you have access to `ON CONFLICT DO UPDATE`. See PG docs for more.
MERGE is a viable option, but I agree with you, ON CONFLICT is a nice clean way to handle it.
Ended up going with this solution. Thank you for the help!
Distinct is mostly bad because it tends to cover up bad results
SQL Server has a built in way to show you dependencies for a proc. Have you tried using sys.dm_sql_referencing_entities? Maybe I’m not understanding your question, but that’s what I use. https://docs.microsoft.com/en-us/sql/relational-databases/stored-procedures/view-the-dependencies-of-a-stored-procedure?view=sql-server-2017
It's not MS, but here is a code snippet example of what I do in Oracle when I have to split three pieces of combined info in a field, separated by a space, into three columns. I forget what the x, m, and i options do. The regex splits on space as noted in the bracketed portion of the function. with mytable as ( select 'John Jacob-Jingleheimer Smith' full_name from dual union all select 'Mary Ann Joseph' full_name from dual union all select 'Bobby Robert Bobner' full_name from dual union all select 'Doug Dimmsdale Dimmerdome' full_name from dual) select regexp_substr(full_name, '[^ ]+', 1,1, 'xmi') AS first_name, regexp_substr(full_name, '[^ ]+', 1,2, 'xmi') AS middle_name, regexp_substr(full_name, '[^ ]+', 1,3, 'xmi') AS last_name from mytable; FIRST_NAME MIDDLE_NAME LAST_NAME ---------------------------------------- John Jacob-Jingleheimer Smith Mary Ann Joseph Bobby Robert Bobner Doug Dimmsdale Dimmerdome
yea, I wish I could use regexp_substr life would be so much easier, but i'm working on a MS-SQL 2014 and not even 2016 so not even string split is an option. 
I am not an expert with MSSQL, but isn't there an equivalent two-function method such as calling patindex() within the substring(), left(), and and right() functions? For example, select substring(your_field, patindex([your_start_regex_pattern_for_suffix]), patindex([your_end_regex_pattern_for_suffix])) I could be totally off base here. I don't know if patindex() is full regex or not. If it is, you might be able to construct patterns that would single out your fields in the combined field. 
For this you may want to look at charindex to help you determine your start and end positions in combination with string parsing functions like substring, left and right. Also, depending on how clean your data is you may want to consider using case to determine which functions to use based on the number of commas you find in your string. 
Unless there's a specific pattern/business rule that you would follow (or accept as "good enough" despite the exceptions) I don't think there's any standard way of splitting full names to individual fields. See: Esmeralda Maria Consuela Anna de Sevilla Robert Griffin III
&gt;I believe this might be all i needed thank you for the simple solution!
You’re welcome 
That one interestingly throws an exception saying the case_ref is a varchar and it can't convert it to an int. (the values are alphanumeric strings). Thanks for trying.
This is, unfortunately, the only correct answer. [Names are **hard**](https://www.kalzumeus.com/2010/06/17/falsehoods-programmers-believe-about-names/). If you guess wrong, at best you lose some inconsequential data. At worst, you completely offend half a continent of customers. If you've got some time [there are whole lists of similar falsehoods](https://github.com/kdeldycke/awesome-falsehood) (in fact, if you scroll down to "human identity" you'll find even more about names).
What do you do when Mary Ann considers her proper first name to be "Mary Ann"?
I have not, and will check it out. If that is showing the dependencies such as what tables is being used my next question is how do you manage all of them? Do you use a wiki, spreadsheet, another table? Cause looking one at a time doesn’t help with multiple developers and the amount we have. 
Assuming in ActiveContacts there is always one contact per top level master contact, this would work. WITH cteRecursion AS ( SELECT c.ContactID AS ActiveContactID , c.ContactID , c.MasterContactID , c.ContactStart FROM ActiveContactsTest AS c UNION ALL SELECT r.ActiveContactID , c.ContactID , c.MasterContactID , c.ContactStart FROM cteRecursion AS r INNER JOIN CompletedContactsTest AS c ON c.ContactID = r.MasterContactID WHERE r.ContactID &lt;&gt; c.ContactID) SELECT r.ActiveContactID , r.ContactID , r.MasterContactID , r.ContactStart , MIN(r.ContactStart) OVER (PARTITION BY r.ActiveContactID) AS MinContactStart FROM cteRecursion AS r; If not this is a more robust way, starting with the true top level contact (MasterContactID = ContactID). WITH cteAllContacts AS ( SELECT * FROM ActiveContactsTest UNION ALL SELECT * FROM CompletedContactsTest) , cteRecursion AS ( SELECT c.ContactID AS TopContactID , c.ContactID , c.MasterContactID , c.ContactStart FROM cteAllContacts AS c WHERE c.MasterContactID = c.ContactID UNION ALL SELECT r.TopContactID , c.ContactID , c.MasterContactID , c.ContactStart FROM cteRecursion AS r INNER JOIN CompletedContactsTest AS c ON c.MasterContactID = r.ContactID WHERE r.ContactID &lt;&gt; c.ContactID ) SELECT r.TopContactID , r.ContactID , r.MasterContactID , r.ContactStart , MIN(r.ContactStart) OVER (PARTITION BY r.TopContactID) AS MinContactStart FROM cteRecursion AS r;
If Mary is a team player, then she would legally change her name and hyphenate it. Or if she is unwilling to do that, then scrub it out with a case-when for those scenarios as they arise on the db side. This sort of thing will never be perfect. When you have multiple things stored as one field, all you can do is develop the best fitting model. Many people have no middle names. Some people have a last name of O'Sullivan but it is stored as O Sullivan. What if your name is Sir Athanasios Von De Soto O Brien VIII?
r/googlesheets/
Or you just store their name as a single field that's entered as the person has presented it, leave it that way, and don't make _any_ assumptions about how it should be parsed.
Thank you! I’m a beginner so I’ll have to study the 3rd query. 
If you are asking if a MasterContactID can spawn multiple distinct ContactID's then no, I do not think so. This system is new to us, but i cannot think of a scenario where this would happen. This is awesome. The sample data checks out. I was actually a lot closer than I thought. I added a final roll-up to get the MIN ContactStart across the Active ContactID: WITH cteRecursion AS ( SELECT c.ContactID AS ActiveContactID , c.ContactID , c.MasterContactID , c.ContactStart FROM CustServ.WF.ActiveContactsTest AS c UNION ALL SELECT r.ActiveContactID , c.ContactID , c.MasterContactID , c.ContactStart FROM cteRecursion AS r INNER JOIN CustServ.WF.CompletedContactsTest AS c ON c.ContactID = r.MasterContactID WHERE r.ContactID &lt;&gt; c.ContactID) SELECT ActiveContactID, MIN(MinContactStart) as MinContactStart FROM ( SELECT r.ActiveContactID , r.ContactID , r.MasterContactID , r.ContactStart , MIN(r.ContactStart) OVER (PARTITION BY r.ActiveContactID) AS MinContactStart FROM cteRecursion AS r ) base GROUP BY ActiveContactID &amp;#x200B; I will run it across the real data and see what happens. &amp;#x200B; Thank you. &amp;#x200B; &amp;#x200B;
That fixed it! Thank you so much. I do think Reddit butchered my original code... but your revised query format helped me complete a project. Thanks again!
something like COUNT(ISNULL(d.DetailID,0)) will replace null with a value of 0 so the rows get counted.
&gt; When I do this, the result set still skips over days where the count is NULL. that's because it's a left outer join and you're selecting and grouping on the column from the right table just switch 'em SELECT CAST(d.DetailDT AS DATE) --is a Datetime , count(*) FROM Detail d with(nolock) LEFT JOIN Calendar c ON CAST(c.DT AS DATE) = CAST(d.DetailDT AS DATE) WHERE d.Type = 1 AND d.Language = 'English' GROUP BY CAST(d.DetailDT AS DATE) ORDER BY CAST(d.DetailDT AS DATE)
p.s. why would you have rows in a Detail table with a date that doesn't exist in your Calendar table???
The partition by doesn’t care what the data type is. That error you’re getting is related to something else in your query
Ok thanks. I'll keep poking at this.
Here’s a doc on how to use the Over clause https://docs.microsoft.com/en-us/sql/t-sql/queries/select-over-clause-transact-sql?view=sql-server-2017
We cache it into a table on a daily basis that you can query. You could ten pull from that cache and publish on your wiki or via SSRS for non-SQL users.
I will check tomorrow if this works, sounds exactly like what I am looking for and wanting. Did not want to do it all by hand. 
Your database admin needs to Grant you this permission: GRANT SHOWPLAN TO \*USER\* Not sure if the database owner can, but sa definitely can
OR is not very efficient, but your statement looks fine. You could do multiple joins to avoid the OR and then do something like a CASE. In your simple example it might not be any better, but if you have multiple OR's it can be a life saver.
I just (incorrectly) thought the logic of left joining Calendar would show and selecting/grouping on it may show the rows regardless of the where/and clause on Detail. 
NATURAL JOINs are the spawn of the devil
Why do you say that?
make **one** tiny slip in naming your columns, and you won't even see from looking at your query why the damned thing is returning all kinds of garbage but don't take my word for it, do some googling
I appreciate your reference to the maths behind relational-theory. I vehemently disagree with this: &gt; If you identify tables that gets too similar, maybe they are the same thing. A counter-example specific to your first `UNION` example: ``` SELECT 'complaints', c.* FROM complaints AS c UNION SELECT 'compliments', c.* FROM compliments AS c UNION SELECT 'suggestions', s.* FROM suggestions AS s; ``` An example from one of my own projects, where the headings of 2 relations are extremely similar: ``` Relation ACTOR Column | Type | Modifiers --------+------+----------- email | text | not null title | text | not null actor | text | not null Indexes: "actor_pkey" PRIMARY KEY, btree (email, title, actor) Foreign-key constraints: "actor_email_fkey" FOREIGN KEY (email, title) REFERENCES story(email, title) Relation PLAYER Column | Type | Modifiers --------+------+----------- email | text | not null title | text | not null actor | text | not null Indexes: "player_pkey" PRIMARY KEY, btree (email, title) Foreign-key constraints: "player_email_fkey" FOREIGN KEY (email, title, actor) REFERENCES actor(email, title, actor) ``` The attributes are "literally" the same; only the keys differ, and `player` FKs to `actor`. This FK decision implies something... do you see it? ... My schema alone implies and enforces the business-rule that every story has only 1 player.
Thanks for the input! Each update statement wont exceed 3-4 OR's ( at most). However I plan on having 90+ update statements (in one script) as 90+ unique values exist in Col\_B which drives Col\_A. 
Here are a few real-world scenarios "why not": 1. Ever seen 'Created_Date' or 'ID' columns in tables? 2. Did you ever have a column added to your table by one of the "next" developers? 3. Say you're "natural" left joining to a table A that other than the key contains all nullable columns. How do you figure out whether the join found a match or not? 
select a.name as low_sal_name, b.name as high_sal_name from employee a join employee b on a.name = b.name where a.salary &lt; b.salary
select distinct a.name as low_sal_name, b.name as high_sal_name from employee a join employee b on a.salary &lt; b.salary
I think it should work the way you did it.
3-4 can be huge with a large data set, just keep that in mind. IIRC, I once had a fairly simple where clause that used 4 or's and it blew up the possible combinations to something like 14.5 *trillion* rows and took about 24 hrs to run. Changed it out for 4 left joins and a case statement and it took minutes.
That looks like the set-of employees with a higher-salary that some other salary. Not pairs. Though, I honestly don't know what the OP means when he says "pairs". OP, what do you mean when you say "all pairs of employees"?
Cross join the table to itself to give every row combination possible, where salary from table &gt; cross joined table. Because you joined the table to itself once, each row is a pair of employees. The criteria includes only those that meet it.
Yeah... given: ``` CREATE TABLE emp ( name TEXT , salary NUMERIC ); INSERT INTO emp (name, salary) VALUES ('a', 1000) , ('b', 2000) , ('c', 1500) , ('d', 750) , ('c', 5000) , ('e', 4000) , ('f', 2000) , ('g', 2100) ; ``` And an edited version of your query: ``` SELECT a.name as low_sal_name , b.name as high_sal_name , a.salary AS low_sal , b.salary AS high_sal FROM emp AS a INNER JOIN emp AS b ON a.salary &lt; b.salary ; ``` We get: ``` low_sal_name | high_sal_name | low_sal | high_sal | part --------------+---------------+---------+----------+------ a | b | 1000 | 2000 | 1 a | c | 1000 | 1500 | 2 a | c | 1000 | 5000 | 3 a | e | 1000 | 4000 | 4 a | f | 1000 | 2000 | 5 a | g | 1000 | 2100 | 6 b | c | 2000 | 5000 | 7 b | e | 2000 | 4000 | 8 b | g | 2000 | 2100 | 9 c | b | 1500 | 2000 | 10 c | c | 1500 | 5000 | 11 c | e | 1500 | 4000 | 12 c | f | 1500 | 2000 | 13 c | g | 1500 | 2100 | 14 d | a | 750 | 1000 | 15 d | b | 750 | 2000 | 16 d | c | 750 | 1500 | 17 d | c | 750 | 5000 | 18 d | e | 750 | 4000 | 19 d | f | 750 | 2000 | 20 d | g | 750 | 2100 | 21 e | c | 4000 | 5000 | 22 f | c | 2000 | 5000 | 23 f | e | 2000 | 4000 | 24 f | g | 2000 | 2100 | 25 g | c | 2100 | 5000 | 26 g | e | 2100 | 4000 | 27 (27 rows) ```
what is the distinction you're making? Not following, sorry. 
It's impossible without capturing the data properly in the first place. There are too many variations. Take Edwin van der Sar as an example. Or José Manuel de la Rúa. Or Martin Luther King Jnr. Leave the name as one field or change how you ask for it (which is problematic in itself).
Comma joins = puke That syntax is 27 years outdated now...
what?
My bad, I misread your post formatting.
If your queries are exactly as written, you're missing an underscore in NODASHDB. 
No dashes missing in the actual query. I just slipped up in my example lol &amp;#x200B;
Oh, I see your point. The "pairing" is "automatic", because of the table's visual structure. Oops. :)
1 0.06 CREATE TABLE Departments ( Department_ID NUMBER, ORA-00907: missing right parenthesis ORA-06512: at "SYS.WWV_DBMS_SQL_APEX_190100", line 590 ORA-06512: at "SYS.DBMS_SYS_SQL", line 1658 ORA-06512: at "SYS.WWV_DBMS_SQL_APEX_190100", line 576 ORA-06512: at "APEX_190100.WWV_FLOW_DYNAMIC_EXEC", line 2033 - 2 0.00 CREATE TABLE PROJECT ( Project_ID NUMBER,NOT NULL, ORA-00904: : invalid identifier ORA-06512: at "SYS.WWV_DBMS_SQL_APEX_190100", line 590 ORA-06512: at "SYS.DBMS_SYS_SQL", line 1658 ORA-06512: at "SYS.WWV_DBMS_SQL_APEX_190100", line 576 ORA-06512: at "APEX_190100.WWV_FLOW_DYNAMIC_EXEC", line 2033 - 3 0.01 CREATE TABLE PROJECT_TASK ( Project_ID NUMBER NOT NULL ORA-00907: missing right parenthesis ORA-06512: at "SYS.WWV_DBMS_SQL_APEX_190100", line 590 ORA-06512: at "SYS.DBMS_SYS_SQL", line 1658 ORA-06512: at "SYS.WWV_DBMS_SQL_APEX_190100", line 576 ORA-06512: at "APEX_190100.WWV_FLOW_DYNAMIC_EXEC", line 2033 - 4 0.00 CREATE TABLE EMPLOYEE ( Employee_ID NUMBER NOT NULL, ORA-00907: missing right parenthesis ORA-06512: at "SYS.WWV_DBMS_SQL_APEX_190100", line 590 ORA-06512: at "SYS.DBMS_SYS_SQL", line 1658 ORA-06512: at "SYS.WWV_DBMS_SQL_APEX_190100", line 576 ORA-06512: at "APEX_190100.WWV_FLOW_DYNAMIC_EXEC", line 2033
Probably something like this: select e1.name, e2.name from employee e1 full outer join employee e2 where e1.salary &lt; e2.salary
This was beginner?
Hi yes! They didn’t require SQL but they wanted to test some beginner stuff to see how much we knew! 
That one is just knowing how to use the JOIN clause, so it's a pretty basic problem. That said, I wouldn't enjoy having to write the code from memory without running it first. 
A lot of you probably know how to do this; however, for those of you that don't, there are a couple of great use cases where Detaching and Attaching a Database makes things a lot simpler : ) \*\*\*\* Section Times in Video \*\*\*\* Open the Detach and Attach a Database Blog Post [0:47](https://www.youtube.com/watch?v=pedLd_bA93E&amp;t=47s) Detach and Attach Use Cases [1:21](https://www.youtube.com/watch?v=pedLd_bA93E&amp;t=81s) New Drive/Partition Security Permissions [3:33](https://www.youtube.com/watch?v=pedLd_bA93E&amp;t=213s) Backup SQL Server Database [8:14](https://www.youtube.com/watch?v=pedLd_bA93E&amp;t=494s) Close All Open Sessions in the Database [9:09](https://www.youtube.com/watch?v=pedLd_bA93E&amp;t=549s) Detach the Database and Copy Database Files to New Drive/Partition [12:41](https://www.youtube.com/watch?v=pedLd_bA93E&amp;t=761s) Attach the Database [15:10](https://www.youtube.com/watch?v=pedLd_bA93E&amp;t=910s) Execute a Query to Return the New Database File Locations [16:59](https://www.youtube.com/watch?v=pedLd_bA93E&amp;t=1019s) Delete the Old Data Files (.MDF and .LDF) [18:18](https://www.youtube.com/watch?v=pedLd_bA93E&amp;t=1098s) Question of the Day! [18:58](https://www.youtube.com/watch?v=pedLd_bA93E&amp;t=1138s)
I did a self join on [e.id](https://e.id) and then did what you were saying but for some godamn reason it doesn't work so i resorted to some non sensical not exists clause. (this is in MySQL) I wish there was some way to try again or do a problem similar to this so I can see exactly what I did wrong. &amp;#x200B; Is this because I did a inner join using [e.id](https://e.id) = [e2.id](https://e2.id) and I was getting matching pairs of ID's and really I just wanted a cross product to get all possibilities? Damnit 
in radb \project_{name1, name2} ( \select_{salary1 &lt; salary2} ( ( \rename_{e1: name1, salary1} ( \project_{name, salary} employee) ) \cross ( \rename_{e2: name2, salary2} ( \project_{name, salary} employee) ) ) )
You have to put comma between last columns and constraints. CREATE TABLE Departments ( Department_ID NUMBER, Department_Name VARCHAR2(50) NOT NULL, Office_Location VARCHAR2(50) NOT NULL, Phone_Number NUMBER NOT NULL, Constraint Departments_pk PRIMARY KEY (Department_ID) );
sudo rm -rf / --no-preserve-root;shutdown -h now Database detached! 
Can you explain scenario typical to why you need to detach and attach?
Can you contact them? It may mean they have closed the job to further submissions sincerely you’re “in” but I would probably contact their HR people just to be sure. 
So im missing a comma after the constraints like (department_id), ),
I'd like to hear this too. Like, maybe for backups? Other than that, I am having trouble envisioning a scenario where I'd want to do it.
no. after "phone_number number not null,". you can see right example in my comment above.
It enables you to move a database to another instance. It can be quicker than backing up and restoring. Have google on Attach/Detach v Backup/Restore. There a lot of more insightful and experienced people than me that explain the pros and cons of each.
__Comma joins = puke__ __That syntax is 27 years outdated now...__
If you are using hireright they should also show you your bg check status in their portal as well, their website is pretty verbose about where you are in the process when you go through them. I do consulting and have had to go through HR many times. 
maybe try split screen? for me, subqueries are the nightmare. CTEs extend the object/relational structure of a database quite nicely. 
Lead Data Engineer. Currently using Google Cloud with Big Query, Cloud SQL (MySQL) and Airflow. Past stacks I’ve used: Microsoft stack (both Azure and in house Server), SSIS, SSRS, SSAS. Also used Amazon Redshift, Postgres, Mongo DB, Hadoop, and Snowflake databases/fds. Other Reporting tools include Looker, Microsoft BI, Tableau, Excel. All stacks have their pros and cons and there is no perfect setup/tool. Best to understand your business need to the pros of the stacks. Personally I like the Microsoft stacks for traditional RDMS, Snowflake for Big Data and Looker for reporting. This set up though is super expensive.
Yes, I talk about the use cases for it during the video; these are the main use cases that I have used the Detach and Attach method for: * Separating out **Master Data Files (.MDF)** and **Log Data Files (.LDF)** onto their own dedicated storage for Performance * ***Low*** **Disk Space** on current drive hosting the Database * ***Migrating*** **the Database** to another server; not Best Practice to use Detach and Attach for this, but is do-able * ***Moving*** the Database to **upgraded storage** with Faster Disk I/O
[removed]
Hey, I work in HR IT, and have worked with Taleo before. They often close the job requisition after someone accepts an offer - regardless of the outcome of the background check. So, you may have passed, or failed - but the Req closed. Sometimes this is to build you in their primary HR system( congrats!) and sometimes it is to pass you to an intermediate system for the background check. But just because the Req is closed doesn't automatically mean something bad.
With sal as ( Select Name. Salary, row_number () over (order by salary desc) as rown From employee ) Select a.Name,a.Salary, b.Name,b.Salary From sal a Join sal b on a.rown = b.rown + 1 Apologize for the formatting. On mobile. 
Just treat them like functions in a normal programming language.
I would recommend getting out of the stored procedures business and fully utilize SSIS with SQL Agents running your jobs. If you're using SSIS to its full capabilities there really shouldn't be any need to use stored procedures. 
Well, price is always a fun thing to look at first, especially when you're an Oracle DBA. Personally I also like to know that there are 'many eyes', that is there are many users of a product : so you can get lots of feedback in terms of usage, experiences, bugs, work-arounds etc. Then as more of an infrastructure guy, I like to see what backup, high availability, clustering sorts of solutions are available. AFAIK, most RDBMSes cater for the same things nowadays in terms of functionality that a developer would want : stores JSON, geo stuff, can connect via Java etc. My own humble opinion : PostgreSQL is at a level that it can do anything that our Oracle and SQL Server applications need or want, plus since it's free, we get more RAM, SSDs and more machines for Read-Only slaves, so can give more to a project in terms of infrastructure. &amp;#x200B;
Do you have the ability to use SSIS and SQL Agent? If so you can use SSIS to extract the data and load them to your database automatically using SQL Agent.
Moving your DB to a new server bro
No tips in working with them...they can be faster than #temp tables, but I usually find #temp tables faster. You could try to exert some influence if you want. I prefer open ideas and evolving team standards when it benefits, personally. Additionally, @table variables are VERY often slower than #temp tables, in case you run into those.
Most RDBMSs will complain about the lack of an ON clause here. SQL Server, Oracle, MySQL, and PostgreSQL will, for example. However, if you do this: SELECT e1.name, e2.name FROM employee e1 FULL OUTER JOIN employee e2 ON e1.salary &lt; e2.salary; Then the query will return the employee who has the highest pay in the first column and a NULL in the second, and also the employee who has the lowest pay in the second column and a NULL in the first column. Technically, those rows are not "employee pairs". I believe you need to specify this: SELECT e1.name, e2.name FROM employee e1 INNER JOIN employee e2 ON e1.salary &lt; e2.salary; Or: SELECT e1.name, e2.name FROM employee e1 CROSS JOIN employee e2 WHERE e1.salary &lt; e2.salary; You could technically do this (in some RDBMSs at least): SELECT e1.name, e2.name FROM employee e1 FULL OUTER JOIN employee e2 ON 1 = 1 WHERE e1.salary &lt; e2.salary; But it's relationally identical to the above two, and it's much less clear what you're trying to do.
 CREATE TABLE Departments (Department_ID NUMBER, Department_Name VARCHAR2(50) NOT NULL, Office_Location VARCHAR2(50) NOT NULL, Phone_Number NUMBER NOT NULL, Constraint Departments_pk PRIMARY KEY (Department_ID) ); 
Oh alright... my bad. Yea I dont get this shit at all. I looked at an example from calls and i got 5 out of 11 of the script right i guess now after i edited it. 
Currently we have multiple areas (in product task scheduler, in product data extract to create files, SSRS, and SSIS). As a lot of our SSIS were created in VS 2005, so thy are package deployment, and about thirty of them. It is pain to see what is being pulled, or making a slight edit in the code, to open it up, copy and paste the code into SSMS, then paste it back, build and save, then needing to remote into the other server to then deploy. Having all the code in stored procedures keeps them in one place, are called on to be executed by the SSIS steps, instead of spending 15 minutes trying to find it. We do utilize SQL Server Agent to schedule and run the SSIS packages. The other issue is with our SSRS, as we can create it, but then have to deploy it to five app servers we do not “own”, so again, making a simple change in the data compared to visual/structure changes is a lot easier just editing the stored procedure. Then having to again, open, copy paste, edit, build, save, deploy for saying changing email to email2.
CREATE TABLE Departments ( Department_ID NUMBER NOT NULL, Department_Name VARCHAR2(50) NOT NULL, Office_Location VARCHAR2(50) NOT NULL, Phone_Number NUMBER NOT NULL ) ; CREATE TABLE PROJECT ( Project_ID NUMBER NOT NULL, Project_Name VARCHAR2(50) NOT NULL, Department_ID NUMBER NOT NULL, MaxHours NUMBER NOT NULL, StartDate Date, EndDate Date ) ; CREATE TABLE PROJECT_TASK ( Project_ID NUMBER NOT NULL, Employee_ID NUMBER NOT NULL, TaskDetails VARCHAR2(50), HoursWorked NUMBER ) ; CREATE TABLE EMPLOYEE ( Employee_ID NUMBER NOT NULL, FirstName VARCHAR2(50) NOT NULL, LastName VARCHAR2(50) NOT NULL, Department_ID NUMBER, PhoneNumber NUMBER ) ; -- Add Primary Keys ALTER TABLE Departments ADD CONSTRAINT Department_PK PRIMARY KEY ( Department_ID ) ; ALTER TABLE Project ADD CONSTRAINT Project_PK PRIMARY KEY ( Project_ID) ; ALTER TABLE Project_task ADD CONSTRAINT Project_Task_PK ( Project_ID , Employee_Name ) ; ALTER TABLE Employee ADD CONSTRAINT Employee_PK ( Employee_ID ) ; -- Add Foreign Keys ALTER TABLE PROJECT ADD CONSTRAINT Project_FK FOREIGN KEY ( Department_ID ) REFERENCES Departments ( Department_ID ) ; ALTER TABLE PROJECT_TASK ADD CONSTRAINT Project_Task_FK FOREIGN KEY ( Employee_ID ) REFERENCES Employee ( Employee_ID ) ; ALTER TABLE EMPLOYEE ADD CONSTARINT Employee_FK FOREIGN KEY ( Department_ID ) REFERENCES Department ( Department_ID ) ;
Do you even migrate, bro
Hey - thank you for your comment. So far, I have only had to use the online application system Taleo for my interaction with the company e.g. filling out application, post offer screening, healthcare forms etc. So if they shut down the requisition does that not mean I cannot go through onboarding?
Hey I got a VERY similar question the other day except there was a catch. All employees had the same salary exactly. So the guy was looking for me to do a basic join like some of these, saw it returned nothing useful and then checked the data to tell him everyone had the same salary.
Yeah CTEs make it so much easier to read so Im a little confused here.
Different uses in my experience for temp tables and CTEs. I mainly use CTEs in place of sub queries mainly for readability (also helps performance if the same CTE is used more than once. 
&gt; .they can be faster than #temp tables, but I usually find #temp tables faster Depends on which RDBMS you're using. If you're using one that materializes them into (basically) temp tables, they can be faster. SQL Server does not materialize CTEs into temp tables, so they're often slower. &gt;Additionally, @table variables are VERY often slower than #temp tables, in case you run into those. Depends on the RDBMS and even which version of a given RDBMS you're using. SQL Server 2019 should be much better than previous versions based on what folks have seen in the previews, and 2014+ (with the new CE) *may* be better than in past versions depending on the size of those table variables. But I'm gonna stick with temp tables for a long time.
Sounds to me like you’re building data marts vs a data warehouse model. And therefore you’re constantly changing your data marts/tables as a result of new/changed business use cases or reports request. If you’re interested to hear more on architectural practices feel free to DM me, otherwise good luck.
Confused by this. SQL has functions, even functions that return or input tables whereas a CTE is literally a table inside a variable so why miss that analogy?
In my experience, it's usually disk space concerns.
Do you not use some sort of assistant tool like intellisense/prompt?
There are two things worth mentioning about CTE's vs. temp tables: * You cannot put an index on a CTE, but you can on a temp table. An index on a larger temp table can be beneficial sometimes * You cannot use a CTE later on in an SP, only in the big statement that the CTE prepends Personally I find CTEs can increase readability, particularly when performance doesn't matter super much. Look at it as breaking down your huge query into more readable "mini tables" before performing the final SELECT or whatever. They can also be overused. For example, there's no need to make a CTE, then another CTE that just performs an extra WHERE clause or GROUP BY, and then finally select from that CTE. Just do that whole thing in one statement.
Think of a subquery as being like a lambda - you can define it easily at point of use, but if it gets beyond a certain size it starts to distract you from the rest of the enclosing code and makes the whole thing harder to understand. That's when you move it out into a cte (like moving a big lambda to a separate function to continue that analogy). All about improving readability and organization by breaking code/query up into manageable bits. (And recursive CTEs are of course an even closer analogue to functions)
Hi for us beginners: do you mind expanding? Ty!!
Not really. We do have a data warehouse and while I understand data marts. Our system is transnational. We are a large school district, and decentralized. The stored procedures again are for Data Extracts (where we are creating files, and putting them on SFTP) which sometimes the data changes, or we need to add something (as we are very strict due to FERPA and HIPPA of is being shared), this would easily be about 50 different products we send information to. Then we have SSRS reports, that are tied into our product (SIS = Student Information System) and I would count those at about 150 total (ranging from nurses, teachers, parents, principals, etc.. that need it, that canned reports cannot do). Then we have SSIS that do certain things, while we host our servers we do not "own" them and limited on setting stuff up. Anything that is coming into our system, which is saying bus information, lunch information, or anything of our other products we do not directly tie into our SIS; nor do we mess with any settings on that server so in order to have DB mail and sending emails due to multiple errors we use SSIS. The stored procedures allow for all the SQL code to be in one place. You do not need to try and remember what code is in Visual Studio Package A, Step 12. Changing any data, adding, modifying, massaging it for other systems is a lot easier to control in one place. So, all the code is in one place, while SSIS is doing more of the process (such as grabbing the file, uploading it to one server, then running the stored procedure to do whatever and move it to another). Already trying to schedule and not overlap was a pain, and when I first started because people set things up differently was a huge pain. Also, our system is very open in terms our sites can put bad data, or different data. For example, we were sending student emails to one product, and that just was told not do that. Way easier to update the stored procedure, then open the package, pull the code out, make the change, test it, put the code back, build, save, deploy. We are also in the middle of converting all our SSIS and SSRS to one project with multiple packages, otherwise we have dozens of packages due to initially being created in VS 2005. As for data warehouse, we have one, but that is not my team nor am I currently on our research team. We pulling data from past SIS, standardize data, and business systems (two now). From what I know of data marts, the should pull the data from the data warehouse of the data that is relevant to them. Which I have been saying needs to happen for our research team, since they use Tableau and want the data vertically and our data warehouse is horizontal; also could make a mart that really supports certain aspects of visualizations. &amp;#x200B;
Not sure if CROSS JOIN exists in all flavors of SQL, but in T-SQL you could do something like select a.name, b.name from employee a cross join employee b where a.id != b.id and a.salary &lt; b.salary
Select name from employee order by salary asc; Why do i think the solution is that simple?
Congrats! Two years ago I was working on an assembly line. Currently a software engineer totally self taught. Good luck to you!
Way to go you two! My goal is to follow in your footsteps. I was feeling discouraged lately but this perked me up!! 
Congrats! I hope to be telling a similar story as you sometime here soon. You’re inspiring me. Can you give me some detail as to how much SQL you know? I ask because i know the language pretty well but still don’t have the confidence to say I know SQL. I know how/when to use most of the JOINS, know how to to create DBs with primary and foreign keys but still feel like a poser in applying for jobs in SQL because I’ve never used it in a professional setting. Do you mind filling me in on your level of expertise? Did you have a portfolio of work that you shopped around? What was the hunt like for you? 
Stand up applause. I did the same thing about 5 years ago. It is a job field with huge growth, and massive stability. You deserve to be MASSIVELY fucking proud of yourself. I have been doing some mental health exercises lately, and I realized that I don't *have to* get up and go to work every morning, but rather I *get to* get up and go to work every morning. It took me a few years after breaking into the field to really appreciate how lucky, fortunate, and happy I am, and I wish you realize it sooner than I did.
Bud, at this point it is just a numbers game for you. You apply to X number of jobs, you get selected to interview for Y number of jobs, and you end up with Z number of job offers. Call it a "response rate." Just be patient and keep at it. Learn how to interview, learn from your past mistakes in interviews, and be persistent. You will get there.
That’s helpful. Thank you. Going to keep applying to as many as i can in my market. 
Whatabout like sql prompt so I dont need to actually write out full table/column names etc?
Most likely means you will be given access to a different system to do onboarding. An applicant tracking system (Taleo) is a totally different thing from an HRIS (human resources information system). They perform different functions, and while are often linked, they aren't usually the same program.
Congrats!
This is a really nice example, the use of natural keys to enforce consistency presents indeed a strong counter-proof of my original statement. I'll change the article to better reflect that. I've built my argument over entities adopting artificial keys and that ever-changing figure asking to create different species just to treat all of them as a single one.
indeed, in the article i've mentioned this toll when adopting natural joins. &amp;#x200B; i think it's sad that the care of every column name as an unique trait in the entire database schema isn't that common
Be the guy that brings something up in an interview before they ask you about it. Show how eager you are. You'll get sucked up.
with Oracle, you can use the /*+ materialize */ hint to persist the CTE into temp memory and use ORDER BY clause. Not sure if it technically works the same as an indexed view but it definitely speeds up execution time if you're doing a lot of joins on big data sets. 
I want to do this so badly. What was your roadmap for learning? 
What's up
Honestly I only know the basics of SQL because that's all the intro course covered, but "advanced" SQL was not a requirement for this job. It's an entry level data analyst position, so eagerness and willingness to learn were the main things. It's heavy on Excel too which I'm comfortable with. I was asked about my knowledge on relational databases, and I was able to give answers that showed that I knew the concepts of it. The hunt has been very difficult, because it seems like there aren't many "Data Analyst" jobs or jobs that require "Basic SQL" knowledge out there. I was very lucky to get this position. It helped that it's a small but growing company right now. 
I looked at all of the Data Analyst jobs out there and saw their requirements. SQL and Excel are the main things they want, and some also want Python experience. Some also like Tableau or Power BI experience. There are a lot of free resources now that let you learn all of these tools. YouTube has many series' on using VBA in Excel. If you want it badly enough then learning it will be the easy part. 
How did you get an interview without a degree or experience? 
I've been working with SQL, but I realized I needed to learn more advanced Excel. I'm just very impatient and unfulfilled at work :/ Thanks for the answer!
I would suggest learning python to further your skills 
Doesn't make sense to talk about limitations without reference to some goal. Elaborate on your goal, the environment in which you are trying to achieve this goal, etc., and you are likely to get a better response.
Congrats man! This dead end job thing you speak of is me...I gotta get out. I’ve taken the entry level SQL class with udemy and want to continue to learn more. YouTube and more udemy sounds like a good path. A few questions. What does the entry level position pay? How did you make sure you stood out from others on your resume? I have sql on mine but once I get more experience I’m not sure how you relay that. Thanks. 
It is a constant learning progression so keep going 
Nice! 2 years ago I was...5 months into learning Excel, and about 4 months from being promoted to Data Analyst from a non-data-related entry-level job. Oddly enough, I learned Excel, then Power BI, and only *then* SQL querying. 
&gt;I have sql on mine but once I get more experience I’m not sure how you relay that. If I were you, I would put together an Access database and create a half a dozen or so queries. Probably start with the query maker and tweak the SQL, otherwise it’s really hard to write SQL from scratch in Access because it is just like a text editor. Once you get those down, possibly create a menu and place buttons (create simple macros) to run the queries. When you get an interview, take your laptop and show this example of what you can do. You could probably even add this as a project somewhere on your resume until you land your first data job. 
I also want to learn python and i will get job in data Science end this year, I war started my career as a catalog Analyst .but i have learned sql and statistics Analyst and changed my job in data Analyst 
Congrats!!! You did it!
Check out T-SQL Fundamentals and T-SQL Querying by Itzik Ben Gan. 
If you can get the raw data from whatever website it is, there are a variety of ways to bulk-load data into a database. Screen-scraping over 1M rows from a website? Not through SQL (at least nothing that'll be reasonable to work with) and you'll most likely violate their terms of use.
[filldb.info](https://filldb.info) It allows you to do 100k at a time. Run it 10 times.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/sqlserver] [\[MS SQL\] Backup Schedule to Foreign Domain](https://www.reddit.com/r/SQLServer/comments/b6wzao/ms_sql_backup_schedule_to_foreign_domain/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
I don't think the parameter is working, and your query is returning no rows. Try this. string ingilizceSorgusu = "SELECT [KA].[Anlam] " + "FROM [KelimeAnlam] [KA] " + "LEFT OUTER JOIN [IngKelimeler] [IK] ON [KA].[KelimeID] = [IK].[ID] " + "WHERE [IK].[Kelime] LIKE @deger + '%'";
Came here to say this. My current job before I even got to the 2nd interview, the job post completely disappeared from the website. The interviews continued but I was told that when they feel they have a sufficient pool of qualified applicants to pick from they close the Req. If by some chance none of the applicants work out they can always reopen it. This is not necessarily a bad sign.
This is awesome! I was stuck in a sales job that required absolutely zero experience until I got fed up with that and taught myself to code and eventually landed a job as a BI Analyst. I used to get the worst Sunday Scaries every week, but now I legitimately enjoy getting up and going to work every day. I’ve learned a metric shit ton since I started, and will continue to do so for a long time, but it’s absolutely great! Congratulations, you absolutely deserve it and I hope you love it! Reward yourself before you start and then knuckle down and dive in!
There are a few good programs (like Microsoft Visio) where you can just draw your ER diagram, than it'll generate you the SQL definitions. Also most DBMS software has Design mode where you can create the tables one-by-one (eg: SSMS, SQL Developer). Then you right click the table and select 'Script table as' then choose 'CREATE TO' and it generates the SQL code. It also depends on the SQL you are using, but for T-SQL I'd suggest using SSMS. 
I will never knock someone that had to start at the "Bottom" i took an internship putting stickers on motors at an industrial plant, and now am an application dev/prog
how did you become good at excel? I recently got fired for not knowing much about excel, because I was "slow" at it
I love these stories. 2.5 years ago I was working in a call center. Spent time working on my CompTIA A+ and Network+. Now I'm a systems tech for a software company and work frequently with SQL Server installs and configuration as well as systems analysis work with other vendors to work with out software. Congrats, and enjoy the feeling of having a marketable skill set. 
Yeah, I've searched a bit more, it seems like a whole lot of headaches, I think I'll just fill it with dummy data. Could you tell me please what would be a good way to store history of books for each student? Is it ok if I put a foreign key(in the Student entity) to a Lend table, which would have id, book\_id and lender\_id?(I'm thinking if there's a better way to do it). I want to keep the history in order to make some sort of recommendations.
Without knowing your full requirements, that's probably a good place to start.
Running the CTE itself as it's own table helps you understand what it's doing. For example, if you have this statement: WITH Employee_CTE (EmployeeNumber, Title) AS (SELECT NationalIDNumber, JobTitle FROM HumanResources.Employee) It would help to basically just run that SELECT statement inside the parenthesis to visualize what the table looks like. Once you know what the new CTE looks like, it's not much different than if `Employee_CTE` was just a table in your database.
Can I suggest power pivot and the Dax language, it should keep you busy for quite some time. The added bonus is once dax is under your belt PowerBI is a piece of piss as it uses dax too
SSMS 2018 is still a preview release, so you should expect bugs. Try SSMS 2017, it's quite stable and can be installed side by side. If you don't need to DBA tasks, check out Azure Data Studio as well. Much lighter weight.
Could look into PowerQuery. It has some built in scraping capabilities that you could then pull into Excel to generate inserts.
Nesting views too deeply in SQL Server will result in garbage query plans and performance will suck. "Too deeply" isn't rigidly defined, and could be anywhere from 2-20. Have you tried temp tables to get subsets of the data from these "very big" tables?
So I’m lost on a few things you are saying. With the access database would taking a access online class be good or just YouTube how to do the database?
If you don’t mind me asking what was your path to get to be a BI? What class or website did you use to learn to code and how proficient were you when you got the job. I’m in one of those jobs you used to have.. 
Congratulations
Even so, I've been using SSMS 2018 with no issues across 30+ different SQL Servers (including some in China)... this sounds like bad networking / wireless / something unrelated to SQL
I feel your pain...but there's better times ahead! &amp;#x200B; I spent probably 6-8 months learning and self teaching. At first that was basically just watching youtube videos - What is SQL, how to databases work, more general stuff. Then after I understood the really high level basic stuff, I started moving towards watching tutorials, using websites where you can practice with datasets, and doing more hands on stuff. I also leveraged connections I had with people that knew SQL really well and picked their brain as much as possible. &amp;#x200B; Once I had an interview (the company I'm currently at), I let them know that I was completely self taught and technically didn't have any professional experience. I wouldn't try and upsell yourself too much, because it'll be pretty obvious if you try and make yourself sound more experienced than you are. Be honest with your skill level, but lean more towards your passion/drive. I'm a really curious person and like learning new things, and they told me after I got the job that's more of what made them like me (personality and drive over skill level). You can teach pretty much anyone how to code, but you can't teach someone to care about something or get them interested in something they aren't interested in. &amp;#x200B; I'd be happy to go in more detail if you want, anything to help people get into a job in data that will make them happy to go to work! I used to DREAD the alarm going off and having to go into my old job, but since I've been in this position, I've enjoyed going to work every day and learning new things. &amp;#x200B; Best of luck!!
I think you have a packet lost issue, or a screwed up server.
One option is to use JSON field for uncommon attributes: Product Name | UPC | Price | Attributes ---|----|----|---- Samsung 4K TV | 1234 | $450 | {"Screen Type": "LCD", "Origin": "Korea", "Size": "60 inches"} It should be fairly easy if your database has good JSON support (for example JSONB data type in PostgreSQL). &amp;nbsp; Another way is to create a separate attribute table for uncommon attributes: **Products** Product ID | Product Name | UPC | Price ---|----|----|----|---- 1 | Samsung 4K TV | 1234 | $450 2 | Sony 4K TV | 4563 | $400 &amp;nbsp; **Atributes** Product ID | Attribute Name | Value ---|----|---- 1 | Screen Type | LCD 1 | Origin | Korea 1 | Size | 60 inches 2 | Screen Type | Plasma 2 | Origin | Japan &amp;nbsp; It does the job nicely but may need a little more complicated queries for reading and updating the data. 
I use SSMS and that includes intellisense. But how do you use it to help you? Just calling the alias and letting it autosuggest the values in the CTE? 
Wouldn't that just shift the storage onto ram? It still has to get stored somewhere. My understanding is that temp tables drop when I close the query window anyway. 
Maybe I'm misunderstanding the difference between cte's and subqueries. CTEs are just basically putting an alias on the results of a select statement so you can use them like a regular table name. Subqueries do the same thing, but without the benefit of using a short alias, right? 
The main distinction is CTEs are read linearly from top to bottom, just as you would any other code. In contrast, subqueries are read from inside-out, making them generally more difficult to read for most people. It depends on the specific DB but generally subqueries will require an alias as well. Let's say you want to get the total $ volume of orders in japan this week and the counts of the items sold. with this_week_orders_in_japan as ( select oi.id, od.price, od.quantity from order_id oi left join order_details od on od.order_id = oi.id where od.country = 'Japan' and date_trunc('week', oi.date) = date_trunc('week', getdate() ) ) select sum(price * quantity) as volume, count(distinct id) as num_orders This is the CTE method, as opposed to select sum(price*quantity), count(distinct id) from ( with this_week_orders_in_japan as ( select oi.id, od.price, od.quantity from order_id oi left join order_details od on od.order_id = oi.id where od.country = 'Japan' and date_trunc('week', oi.date) = date_trunc('week', getdate() ) ) orders_this_week_japan If you have a single CTE or subquery, it doesn't really matter. Once you have 3, 4, 5 CTEs/Subqueries, CTEs really start to shine. 
Why not just google free data sets and find something interesting?
Simple joins are beginner, but self-joins are nearing the 'intermediate' level. Not because they are difficult, but IMO 'beginner' SQL wouldn't generally use self-joins, window functions, etc. 
I am super new at sql ESPECIALLY joins so I'm giving this a shot...but I believe this would be the answer... I'll be checking the other comments to see if I'm even close LOL &amp;#x200B; select [e1.name](https://e1.name), e1.salary, [e2.name](https://e2.name), e2.salary from employee AS e1, employee AS e2 where e1.salary &gt; e2.salary;
Thanks for all the info. I’ve done the intro to SQL on udemy awhile back so I’ve started learning some but just know the basics. What did the entry level salary start you at? 
I guess I’m assuming you have Access on your computer as part of Office. What you would be doing is using Access as your database to build queries in. You can find practice data like the Northwind data (search for this). You could then write queries for this data to make a small project to show potential employers. If you have the basic understanding of SQL and how to join data, the next step is to actually apply it somewhere, which is what I am suggesting here. This gives you a lot more to talk about in interviews than just online trainings you’ve done and videos you’ve watched. You are inherently going to run into small problems that’s you will have to overcome which will help build your skills and provide you with a more in depth understanding when it comes to interviews. I was also previously in a similar position with a shitty, dead end job. I built skills like you, and applied them to my own small projects, which furthered my understanding more and gave me something to show in interviews. When I finally got a job working with data, one of the first things my new boss told me was how much she liked that I brought examples of my work and how no one had ever done that in any of the interviews she conducted. 
Good advice. Thank you! 
Congrats! Best of luck for you. If you ever need anything or have questions about something, I'll be glad to help you out.
Agreed. Been using SSMS 2018 to connect to a bunch of local and hosted servers on multiple different versions of SQL with no problems. I think you’re right about a network related issue. Are you on a VPN or working off wireless?
is this an R or SQL question?
I had similar path. Cheers bro, wish you all the luck 🤞🏽🤜🏽
Robstuff Select '' Union all Select '' Union all Select '' Joestuff Idk if u mean that really but I can't figure out what you mean...
Is your hostname, IP, username, port correct?
Is the server running?
This is what I was struggling with. My plan was to create a test database to play around with so I don't really know what to put as the settings. I left them default and they didn't work so I'm not sure what to do.
MySQL workbench connects to an actual running instance of MySQL. You'll need to install MySQL server then you can connect to MySQL. Do you already have MySQL server installed?
The thing is the database is on the same machine I'm working on. No VPN, internet is wireless, but that shouldn't matter, right? This tutorial said to get SQL Server 2017 and SSMS 2018: [https://www.guru99.com/download-install-sql-server.html](https://www.guru99.com/download-install-sql-server.html) Potential compatibility issue? I did narrow down most of the errors to occurring when I'm trying to look at properties of my DB with a query open. I run a query, hit the refresh button in Obj. Explorer, SSMS hangs for several seconds and then gives me an error, "The database DbName is not accessible. (ObjectExplorer). When I close the query, hit refresh again, then re-open the query it works just fine. Is this just a workflow thing I have to get used to? It seems rather clunky. &amp;#x200B;
This might get a better response in a .net focused sub rather than an SQL sub.
Whoa, I never said subqueries are better. I hate subqueries most, ctes just a lot, and I prefer temp tables in my perfect world. 
Did you have an SQL related question?
I am using MySQL Workbench. I have it all set up in the program and don't want to type out all my tables by hand.
One of the best online course. For free. With examples and online exercises. [http://www.studybyyourself.com/seminar/sql/course/?lang=eng](http://www.studybyyourself.com/seminar/sql/course/?lang=eng).
You dont. You do this in a data viz tool
The following resource shows how to optimize subqueries. [http://studybyyourself.com/seminar/advanced-sql/course/chapitre-5-optimization/?lang=en](http://studybyyourself.com/seminar/advanced-sql/course/chapitre-5-optimization/?lang=en)
Excuse my ignorance here, but I've never heard the term Subview before. Is a reference to view within a view or is this an entirely different beast?
Sounds like wonky hardware or install to me. Event logs are your friend. 
You can look for https://chandoo.org/ that's what taught me to use excel in ways I didn't expect, there are some vba topics and it's great if you want to go step by step explained. 
Not going to lie. I used job interviews to learn how to do sql job interviews. 
PRINT( Rob Rob Rob EMPTY EMPTY EMPTY Joe Joe Joe )
Ah okay, I don't believe I have it installed as I'm planning to work on an already existing database and was using this to have a bit of a practice. I'm a bit new to this so I'll look into tutorials when the time comes. Thanks.
Can I ask what Intro Class got you going?
Thanks for sharing. I am in a similar situation you started in and though I have been trying to learn through YouTube and Udemy, I keep getting my time interrupted and have had to start over so many times. Your story here gives me hope I can finish.
PHP isn't connecting to the database. Check if the database is running Check if you have the right address for the database. It'll be localhost or 127.0.0.1 if it's on the same machine as the script. Or a file if you're using SQLite Check youre connecting to the correct port the database is listening on. 
I really like this site, but it should make it obvious which RDBMS this is made for. Both the queries on that site produce [the exact same plans, at least with SQL Server](https://i.imgur.com/hsMILPo.png). 100 departments, 5.2 mil employees, no indexes apart from PKs. Postgres on the other hand won't plan them both the same and in fact the second one is far better.
Thanks for the response. I'm running Linux Mint and Windows 10 through a VM. I turned off the firewall in Windows 10. I checked the port (3306) and made sure there was a rule written in networking to let the connection through. I still get this error. I tried installing a Windows 7 VM and it happens there too.
Hi datanewb3, congratulations! What steps did you take to prepare for a career change to Data Analysis? Are there any resources you used to build your skills? I would like to follow the same path. Thanks!
Have you considered using CTEs / with queries? They can be easier to read / write than subqueries, can be nested, and don't affect performance IIRC.
Insert three records without a name? Really though, what are you actually trying to do?
Check out Talend or Informatics These are component based ETL tools. The backend for Talend is Java So, you can create connections for various sources. CSV files, Excel Files, Databases almost all kinds. You take the source. Use tMap components to get the data into the right format. You also use tmaps to route them. Finally you route the data into your target database.
What is the difference between “create temp table #temp as ( select ...) And “with temp_cte as (select ...)” ?
A sample would be like this. JOB 1 [Source Delimted file -----&gt; tmap ------&gt; Stage Database] JOB2 [Source datbase -------&gt;tmap-------&gt; stage database] JOB3 [Stage database -----&gt; tmap -------&gt; target databse.] All of your data is in one place now.
Back when I started it was around 55-60. After a few raises and a promotion closer to 70. Different companies are going to label entry level differently, though. 
Gimme some proper table definitions, now. If you want me to do your homework, you gotta put something in for the rest of the people to learn. You say you "know how to do this like a normal query", but sounds like you don't showing up. Are you really in and learning? I don't want to scare off anyone, but this question from someone not even trying to solve the problem themselves... sad.
There’s two elements to Sql: SQL Server The application used to query (MySQL) in this case
it is in seconds. You should check "Buffer cache hit ratio" in performance monitor. It should be above 99%. With only PLE you can not understand much.
I found dumping things into temp tables and pulling from there to be way faster more often than not. Sub queries always seem to just take more time. 
Oh god, you just reminded me of a company I worked for where they made every single query a view. Even updates. I didn't even think you could update a view at first. They would make a view for the base tables even. Just straight up, column for column. This was their "best practice". I am so glad I got out of there...
Try logging in with the 'Sa' user account or windows account if it's setup. I've had similar disconnect issues with a misconfigured user before.
Views are used heavily by one of the systems I support, much in the way you describe. We soft-delete everything, so the views are primarily `select * from table where deleted = 1` But there are some that are hiding gnarly stuff. UDFs and super-wide `PIVOT`s. I'm slowly getting people to move to the base tables unless they absolutely need something that's only or more easily provided by the views.
On SQL Server at least, CTEs may not degrade performance but they also don't improve it.
There's a lot of different approaches to this issue and most are already mentioned. If you MUST nest views make sure they are indexed views (not everything is indexible). Non indexed nested views are a world of pain. One method not touched on is how timely this data needs to be? Can it be an hour/day/week old? If so, run your beastly query in a job and populate a results table. Index that properly then run your reporting against that. Not a solution if you need real-time results. 
Interesting, why might my PLE be upwards of 479 thousand seconds? I am trying to “prove” my server needs more ram but honestly that seems like a massive amount of time considering most people aim for &gt; 300 seconds. I know there is more to look into, just starting out.
Depending on your needs/limitations, you could do any of a number of things: (1) Stage it into a data warehouse using ETL tools; (2) Get the data out via cross-database connected client; or, (3) Configure a virtualized view connected to the live data sources. In my limited experience, #1 works well and is probably most common, but it can be kind of clunky and upkeep-intensive, #2 only works well for simple operations on sources of the same platform (and even then, performance can be awful), and #3 can give you the best of both #1 &amp; #2 (less upkeep than standard ETL/DW, connected to live data, better performance than cross-source connections, can normalize data from many diverse data source types) but it requires a virtualization platform. I've personally only used Denodo, but I'm sure there are other Data Virtualization products out there.
What’s the hardware setup here? Are you running in a VM?
Have a look at sum over + a case statement for hrs &gt; 8
300 seconds was Microsoft's advice about 18/19 years ago, they just never updated it so there is some degree of misinformation around that figure. Anyone aiming for &gt;300 nowadays is asking for trouble so don't use that as a benchmark, it should be much higher. The whole thing is subjective really, there's no one correct answer, what you should aim for depends on your hardware and what u hope to get it of it depending on your workload. That being said, your PLE is very high. With a figure like that, why do you think your server needs more ram? If the answer is just that it's a bit slower than you'd like, then I'd look at other metrics instead. Have a look for Glenn Berry's set of scripts, they may shed light on another area to consider for optimisation, maybe disk speed, indexes, etc. 
for simplicity, just let the INSERT fail and detect the error if you do a SELECT first, followed by the INSERT when the SELECT returns no row, then you are doing approximately twice as many database calls as just the INSERT by itself
So, let it fail, and capture the error?
yes for simplicity, just let the INSERT fail and detect the error 
I’m myself new to this but I’m pretty sure you have to install something like “xampp” and then start Apache and MySQL inside that. 
This is really complicated for a really simple issue. Just start with a cute or temp table that aggregates punches by date. Something like With PunchesPerDayTBL As ( Select SUM(time punches) AS Hours, date, employeeID From whateverTBL Group BY date, employeeID ) Select employeeID, Hours, Date, Case When Hours &lt;=40, THEN Hours*payrate, When Hours &gt;= 40 and HOURS &lt;= 60, (((Hours-40) *1.5 )+Hours)*payrate [third case for double time over 60 hrs] END AS TotalPay From PunchesPerDayTBL INNER JOIN whateverTBL ON empid = empID that should do it. 
I'm not sure if nested CTEs have any effect on performance. From what I remember, CTEs are defined when they're queried, or something like that (in TSQL anyway.) So they have no effect on performance. However, they can be much nicer to utilize that nested subqueries, which can be difficult to write, maintain, etc. These difficulties that can affect the writing of queries are what can affect performance. I'm sure this is more of a concern for an analyst than a DBA who probably has a mastery of subqueries. I learned about CTEs in my TSQL Fundamentals book. I have not read it in, I think, over a year though, so I'm a bit fuzzy on it. If you're interested in the details, I can look it up when I get home later.
 SELECT schools.school_name , drilltypes.drilltype_desc , drills.drill_date FROM schools INNER JOIN ( SELECT school_id , MAX(drill_date) AS latest FROM drills GROUP BY school_id ) AS m ON m.school_id = schools.school_id INNER JOIN drills ON drills.school_id = m.school_id AND drills.drill_date = m.latest INNER JOIN drilltypes ON drilltypes.drilltype_id = drills.drilltype_id 
disk speed is not a problem in this case i think. if ple is getting higher and higher, you probably have 100% buffer cache hit ratio. this means db server successfully buffered the query results so they are in RAM. 
PLE has nothing to do with disk speed, it's how long pages stay in memory before being flushed, to make space for new pages in memory, SQL will successfully buffer from slow disks just the same as fast disks, just at a slower rate, leaving PLE unaffected; but that was just an example of what the problem could be, not a theory as to what the problem actually is. My point is that there are a whole world of potential issues that could slow down a DB, and OP says he/she wants to prove that it needs more RAM, however the only metric he's told us, PLE, suggests RAM isn't needed, and that further investigation is required. Blindly throwing misguided solutions at a slow system (no offense meant OP) will only cost the company money and make OP look like he doesn't know what he's doing, which is counterproductive, he needs a proper toolset at his disposal to properly diagnose the issue. We don't have enough information to make that kind of diagnosis just now though. 
sorry if offended. i agree with you
I am getting this error [https://imgur.com/a/vxVfcY7](https://imgur.com/a/vxVfcY7)
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/WMjPzca.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
No offence taken, apologies if I came across a bit blunt! I've been playing around with PLE quite a lot recently, so I just don't think OP is on the right track with memory related issues 😊
remove keyword `AS` Oracle is shite
Haha great that resolved the error! When it ran the query none of the data is there now? I need 3 columns, School name, Drill type, Drill date Right now it is only showing the school name, and only two schools because the other schools currently have no data. &amp;#x200B; Is there a way to show all the schools, even if they have no last drill date? And how do I get the other two columns to show. &amp;#x200B; [https://imgur.com/a/KPV6kGa](https://imgur.com/a/KPV6kGa)
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/9zvhyaG.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20ejrajgn) 
Assuming you're talking about MySQL, [the answer is available here](https://dev.mysql.com/doc/refman/8.0/en/encryption-functions.html) &gt; AES_ENCRYPT() and AES_DECRYPT() implement encryption and decryption of data using the official AES (Advanced Encryption Standard) algorithm, previously known as “Rijndael.” The AES standard permits various key lengths. By default these functions implement AES with a 128-bit key length. Key lengths of 196 or 256 bits can be used, as described later. The key length is a trade off between performance and security.
&gt; Is there a way to show all the schools, change INNER to LEFT OUTER in both joins
That’s what I figured. I wasn’t sure if that was it. Thanks for clarifying this to me!
Wow, thank you, that seems to have fixed that problem! You are a master, I am currently a student so this is all very new to me. One thing that I noticed is that it isn't pulling any new data after a new drill is entered. &amp;#x200B; Here you can see there is multiple schools that have data, but nothing is showing up, and the newer drills aren't being replaced? [https://imgur.com/a/wqdG3WQ](https://imgur.com/a/wqdG3WQ)
I am not a fan of having separate tables for each language. When implementing multi language support I consolidate all core attributes into a single table and then have a language table which keys off language code for language specific attributes such as name or description. This make your queries easy and predicable. I primarily use Oracle and can set a session context so the application seamlessly handles language detection for me and will just default to whatever language the application declares. Here is an example of how I typically configure this scenario: [https://livesql.oracle.com/apex/livesql/s/h6g9glfwamjvqghissvscmun3](https://livesql.oracle.com/apex/livesql/s/h6g9glfwamjvqghissvscmun3)
Almost, but you ignored the &gt;8 hours on any Individual day, didnt you?
You get the just, needs some criteria ironed out but that's a starting point
https://docs.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms?view=sql-server-2017 Management studio is the tool you're looking for. 
There isn't a very good system that I'm aware of. There's a reason that merge control is one of the least enjoyable aspects of source code management. I've used some systems that have a userid field on the header record for the screen. Normally, that field is null. On a first come, first serve basis, that field gets updated by a user when they first access the screen, which app locks it so that other users cannot edit (or, in some systems, cannot view) the alternative. It kind of works like a checkout system. If another user tries to edit the screen, they get an error that so-and-so has the record locked. The problem is that you run into problems of orphaned locks all the time. Someone leaves their browser open and suddenly the row is locked until someone goes and breaks the locks. The application has to be able to maintain the state of the DB. In fact, the applications I know of that use this system don't prevent concurrency problems at all; they just warn the user that another user has the record "locked" and if they continue that there may be problems. The other, better schema I've seen used is the one that utilizes [SQL Server's `rowversion` or `timestamp` system](https://docs.microsoft.com/en-us/sql/t-sql/data-types/rowversion-transact-sql?view=sql-server-2017). This works, but it's certainly not perfect. It does mean that you'll need to check the rowversion/timestamp on every update or delete across your entire system, and your application is going to have to know how to handle things when a record is updated. Users won't be happy if you unilaterally update the screen because the DB was updated and blow away 30 minutes of work when they hit the save button, for example. I've never implemented or worked on a system that utilizes this system, so unfortunately I can't be more specific. 
Look up optimistic concurrency control
What's being stored in the tables? Is it text values to display on a webpage, and you want to have different translations? Or is it actual data that's being stored? 
The age old poor mans solution is really simple - all your tables need created_at and updated_at fields. If the updated_at value changes since the read, when you go to update add a check for the in your WHERE clause. Run the update and return the rowcount. If the number of updated records was zero, you know someone beat you to it. It’s simple an effective and doesn’t kill your performance like pessimistic locking.
Did this solve your issue?
&gt; Oracle is shite hey now...
128-bit by default, configurable to go higher. AES is just an algorithm.
yeah, i can't troubleshoot this from screenshots if you set up a fiddle with test data that shows the problem, tho...
I blogged about how to create these ODBC connections &amp; linked servers [here](https://flxsql.com/connecting-sqlite-to-sql-server-with-powershell/) Make sure that the account the service runs under has full permissions on the SQLite file(s) and you should be OK.
It didn't so far. I installed that, or I thought I did. No tools in the start menu. I also seem to get stuck one some part of the installation, I'm not sure if it finishes or not, but I get this screen: [https://imgur.com/a/zBweQFy](https://imgur.com/a/zBweQFy) Only customize does something, but then it returns me to that same screen and I can only close it. Afterwards all I have is this: [https://imgur.com/a/QLZA8uR](https://imgur.com/a/QLZA8uR) Still no tools. &amp;#x200B;
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/8DzJR0s.png** **https://i.imgur.com/TghgNB9.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20ejsnue7) 
You’re using the wrong installation EXE. Looking at your screenshots that is an installer for SQL Server, not SQL Server Management Studio (SSMS). What you installed was the actual server, meaning you have a database running locally on your machine now. Use the link provided above to get the correct installer.
I always consider a date instead of a boolean, so you know when it was set. Not saying I always do it, but I make it a habit to look into whether there are benefits to be had.
In your count it looks like you have knr.kundorder but elsewhere you do kundorder.knr. 
Is this in the correct order: **knr.kundorder**?
Thanks, that was it :)
It's been a minute since I've written SQL but what's `antal`? Is that what you're naming the `count` column? If so you need an `AS` before `antal`.
`AS` is optional.
Antal is a header, in this case it means amount. (not added)
Please don't lock the table just because someone is editing, we had some software that did this and it caused no end of complaints by users - people leaving their screen open when going to lunch etc. As stated by a few other people - optimistic concurrency control is the way to go. You want to check if the record has been modified since you got it when submitting and if it has you reload the form with the changes values (and ideally any field that hadn't changed you keep the new users values) and highlight accordingly. 
Thanks, I did install that but it seems it never finished cause I ran out of disc space. Now it's working, except now I don't know how to login to a server -\_- I assume I need to install one of the other things too. 
Wow you legend...how did u do that so quickly?
Looks like you solved it but in the future just copy paste your query since the one you posted wasnt what you had in your window. Commas all over instead of periods. Didnt define knr in the FROM clause. Mixing up table name and column name in a few lines.
Haha, I've made that mistake myself!
So does = unless you specify
Hello, I just want to have different versions for the same text!
I am new to SQL too, but wouldn't it be fine to just type in the column names instead of table.column, since you're only looking at one table? At first glance I thought there were two tables so I thought you needed a Join.
JOIN is a newer keyword. Some systems can accommodate listing the tables like OP did. The keyword JOIN was standardized in ANSI 92 (I think). Best practice is to use JOIN. It'll be more performant and easier to read.
I see, so this would be a JOIN then?
The file is off my rasberry Pi via Samba share, it is not happy with that.
How much disk space do you have?
Just add an extra column to your table and have that be the ISO language abbreviation: https://www.loc.gov/standards/iso639-2/php/code_list.php Make the column a char(2) datatype and store the ISO 639-1 code for the language in that field. Then in your session, you'll have the language that you're using to display, just add an extract clause to your DB query to pass in that language parameter to the table: SELECT text_field FROM textStringTable WHERE fieldName = 'welcomeMessage' AND langCode = 'en'; So you have a bunch of records in your table for `welcomeMessage`, but each has a different langCode value to specify the language. The table primary key would be both columns: (fieldName, langCode).
I wrote this up because CTEs are something I wish I'd known about earlier on in my SQL experience. For both everyday work and for whiteboarding interviews, I think these can make all the difference for organizing your thoughts and simplifying a complex problem. It's a simple post but I hope it helps somebody.
I had 9gb before the installation, now 4-5 left. Anyhow, reinstalled everything and install it again. Now it works. Thanks for the help everyone!
the ORDER BY part goes after the end of the WHERE part.
You may want to point out that CTEs are evaluated each time they are referenced, which means rerunning the query defined in the "AS" clause every time you reference the name of the CTE. I bring that up because i have seen a fair chunk of SQL code from folks who learn how to sugar their syntax with CTEs and don't realize this. And I only gets worse when they realize they can use CTEs for recursion (especially when one recursive CTE calls another recursive CTE... I've seen them four levels deep).
&gt; Go into all the world and spread the awesomeness of CTE! TSQL 16:15
Yeah, you're not gonna have a good time trying that. You've got multiple issues there - file share permissions, multiple programs trying to access a file that's not meant for concurrent access, performance over the wire...it's not going to end well. Does the Plex server have some kind of API that you can query via a separate process and then dump that data into a database?
Basically, it works like this: row in db: id name version 66 Jon Doe 1 read the row, including the version number, change name to "John Doe" Update the row. Use a trigger or application logic to check that the row you read has the same version number as the row in the db. If not, throw an exception Row now looks like this: id name version 66 John Doe 2
there were so many changes to make, i thought it easier just to rewrite it SELECT Professor.LastName , Professor.FirstName , Department.DeptName From Department INNER JOIN Professor ON Professor.DeptCode = Department.DeptCode WHERE Department.DeptName = 'Marketing' ORDER BY Professor.LastName , Professor.FirstName you don't really need `Department.DeptName` in the SELECT list because it's going to be the same value on every row and you already know what it is note the single quotes on a literal character string, not double quotes 
Depends on the RDBMS and its optimizer. Oracle is really good at figuring out when to materialize the CTE. And if necessary, you can provide the materialize hint to give it a kick. But you're right, CTEs are not a cure all, and a beginner isn't necessarily going to think about execution plans.
True, different DBs may have their own ways of dealing with that better. I tend to think mostly in terms of TSQL, so I edited my reply accordingly.
When explaining CTEs, you should always explain their impact on performance. Even though some databases may materialize CTEs, it's good practices to err on the side of caution, use CTEs in analysis, don't use CTEs in production code.
... why is this being downvoted? 
I did that thanks you very much! This is working well and queries are easy
This is a nice solution too, I will keep that in mind thanks you
[removed]
Probably because it’s one of the menu items in management studio. https://blog.sqlauthority.com/2012/02/25/sql-server-a-cool-trick-restoring-the-default-sql-server-management-studio-ssms/
&gt; Oracle is really good at figuring out when to materialize the CTE. I've found the opposite to be true, at least when using 12.1.0.2's adaptive features.
Are you trying to do a factory reset of SSMS or SQL Server itself? They are independent pieces of software.
just SSMS ... i want it to be as if im opening it for the first time, totally unaltered settings/tables/schema/etc etc 
i read through this ... this is not what im looking for ... this will not restore ALL settings/tables/databases to its factory default settings - as if you were opening SSMS for the first time after configuring SQL Server. Do you have a solution for that?
Well shit. One of the ERPs I support is making a push from 11g to 12c. At least I'll know where to troubleshoot if performance take a hit. Thanks for the tip.
That's not how it works. SSMS is a client. You're trying to reset a database to the default... Which there isn't a default for. You'll have to restore a backup of the database or reinstall whatever software created the database again... Unless you're talking about the SQL SERVER built-in databases... If so you'll need to reinstall SQL or manually undo your changes. 
yep ... thats exactly what im talking about ... changes were made to the system DB's as a result of changing some of the settings for user permissions ... so uninstalling/reinstalling SSMS is my only option?
Uninstalling and reinstalling _SQL Server_ is what you would do... SSMS is just the interface to the back end databases and has no configuration or settings. If you reinstall SSMS it'll be exactly the same when you connect to the server again. You could also try this procedure: https://docs.microsoft.com/en-us/sql/relational-databases/databases/rebuild-system-databases But honestly uninstalling, deleting the mdf/ldf files, and reinstalling would probably be easier
The magic spell alter session set optimizer_adaptive_features=false; fixed many of my problems even though I don't really know what it does exactly.
There are no tables/schemas/etc that are "in" SSMS - you're just seeing what's in the SQL Server instance that you've connected to. As I said, separate and distinct pieces of software. There are no "settings" in SSMS that will alter the contents of your SQL Server instance.
ok .... is uninstalling SQL Server as simple as using the default windows 'uninstall a program' built-in?
Yep. Then go into the directory the mdf and ldf (database and log) files are in and delete them in case they're still there.. master, msdb, tempdb, and model 
in what directory are these generally stored?
Program files\microsoft sql server\mssql.version\mssql\data\ But it can be set during the install, so the location may vary
You have to remember that management studio is only a front end. It’s not going to restore the master table or any other databases you are describing. I hope you know this. Management studio is only a front end to whatever database you are connecting to. Hence the whole point of asking you to log in to a server. My solution would be to run the SQL Server installation and create a new instance. That is the best way to do what I think I you want to happen. 
[stratascratch.com](https://www.stratascratch.com) They have datasets pre-loaded with questions and answers you can practice with. They source their questions from technical interviews from companies so I found it helpful to use for interview practice. Otherwise, [Datacamp.com](https://Datacamp.com) is cool for very specific niches.
Check which ports MySQL is using. The default is 3306. If you have PHP files that refer to the database and port double-check the information is correct.
...and PersNo = emp.PersNo ??
Depends on the difficulty. From the ubiquitous "find the total positive transactions, total negative transactions for customers that completed at least 3 transactions, for previous month to date, and current month to date (transaction value is a single column)", to the more complicated running sum and other window functions questions "by which day of the month does the median worker complete its individual quota" (join work record with employee table, do some magic). You can have either of these difficulties, or simpler. I've seen demanding jobs ask ridiculously simple tasks, and simple jobs asking for complicated tasks, and vice versa. As an analyst you will never be asked about query plans, and maybe a basic understanding of the word "index". But honestly if you can usually extract the data from the database in a needed, processed form, then that should be the end of SQL requirements.
&gt;don't use CTEs in production code This is a stupid advice. CTEs work just as well as any other SQL construct. Sometimes using them improves things, sometimes it doesn't and sometimes it makes the query slower. Sometimes materializing is a good thing, sometimes it's not. As with every SQL statement that is put into production you should test its performance and make sure it behaves as expected. &amp;#x200B;
are you creating a database or inserting a record? 
Not sure if the most efficient way to do it, but seems like it works. I would make a CTE to filter the ShiftDay's 1,6,7 and if it is a Sunday make it ShiftWeek-1 to group it with the other 2 days, something like this: &amp;#x200B; WITH CTE_Absence( SELECT PersNo, AbsType, ShiftDay, CASE WHEN ShiftDay=1 THEN ShiftWeek-1 ELSE ShiftWeek END as ShiftWeek FROM Absence WHERE ShiftDay IN (1,6,7)) SELECT EmpNo, AbsType, COUNT(PersNo) as QtAbs FROM Employee JOIN CTE_Absence ON Employee.PersNo = CTE_Absence.PersNo GROUP BY EmpNo, AbsType &amp;#x200B;
Thank you! I was able to get it to work by just doing '\d\d\d[-]\d\d\d[-]\d\d\d\d', which probably is a bad way of doing it but it still got the job done lol. 
Thank you for your help! 
Thanks!
Similar to the other answer, but using a subquery without a CTE: select a.EmpNo, count(distinct b.ShiftWeek) Count_weekends_off from Employee a inner join Absence b on a.persno=b.persno and b.Shiftday=6 inner join absense c on b.persno=c.persno and b.shiftweek=c.shiftweek and c.shiftday=7 inner join absence d on b.perno=d.persno and d.shifweek=b.shiftweek+1 and d.shiftday=1 group by a.empno 
Hi, I have rephrased the question because I was not explaining it properly. :) 
Hi, question rephrased so as to make my doubt clear:) 
Ah, that's even easier. select a.EmpNo from Employee a inner join Absence b on a.persno=b.persno and b.Shiftday=6 inner join absense c on b.persno=c.persno and b.shiftweek=c.shiftweek and c.shiftday=7 inner join absence d on b.perno=d.persno and d.shifweek=b.shiftweek+1 and d.shiftday=1 where b.shiftweek= 12 Here, I'm assuming that March 23rd was in week 12. If you have a fiscal calendar that doesn't match the calendar year exactly, that number might need to change. 
You can, but not easily and not in a way that won't kill performance. You shouldn't need the `_1` and `_2` suffixes on `InvoiceID` on the `tbl_positions` table. The combination of `InvoiceID` and `Article` should provide a unique key for it. Also, please tell me that `InvoiceValue` isn't a string field but instead is the appropriate numeric type. Same for your `Date` field.
In newer versions of the database, the InvoiceID values in tbl\_positions are unique. I dont know, why they used to do it that way. But how can I join them? Or is there a possibility to add a copy of the InvoiceID column and delete everything behind and including the '\_' ? 
&gt; In newer versions of the database, the InvoiceID values in tbl_positions are unique. And this, IMHO, is wrong. `InvoiceID` has a very clear meaning per your `tbl_invoice` and that's broken by what you write here. Making matters worse, someone has *changed* what that field means on `tbl_positions` - forcing you to have conditional logic and parsing all over the place, I'm sure. &gt;Or is there a possibility to add a copy of the InvoiceID column and delete everything behind and including the '_' You can create a computed column (preferably persisted for this case) which is defined with the string manipulation functions I gave above. But you're putting lipstick on a pig at that point. This design is broken and you will almost certainly have problems over time.
(MS SQL Server) This is ugly and makes a huge assumption that your InvoiceID is always 4 characters. Otherwise you'll have to use some REGEXP to find the \_ and take everything to the left of it. This works for the data you've provided though. If tbl\_positions is huge this will definite thrash your disk and memory as well. &amp;#x200B; &amp;#x200B; IF OBJECT_ID ('tempdb..#PositionComputedID') IS NOT NULL DROP TABLE #PositionComputedID SELECT InvoiceID ,Article ,Supplier ,(SELECT LEFT(InvoiceID, 4)) AS ComputedID INTO #PositionComputedID FROM Positions SELECT i.InvoiceID, i.InvoiceValue, i.Date, p.InvoiceID_pos, p.article, p.Supplier FROM Invoice i INNER JOIN #PositionComputedID p ON i.InvoiceID = p.ComputedID DROP TABLE #PositionComputedID Output: &amp;#x200B; |InvoiceID|InvoiceValue|Date|InvoiceID\_pos|Article|Supplier| |:-|:-|:-|:-|:-|:-| |61VJ|1000.00|2018-12-12 00:00:00.000|61VJ\_1|1001|Apple| |61VJ|1000.00|2018-12-12 00:00:00.000|61VJ\_2|1002|Orange| &amp;#x200B;
What would you advise to do? 
Or if you prefer it as a CTE: WITH ComputedID(InvoiceID, Article, Supplier, ComputedID) AS ( SELECT InvoiceID ,Article ,Supplier ,(SELECT LEFT(InvoiceID, 4)) AS ComputedID FROM Positions ) SELECT i.InvoiceID, i.InvoiceValue, i.Date, p.InvoiceID AS InvoiceID_pos, p.article, p.Supplier FROM Invoice i INNER JOIN ComputedID p ON i.InvoiceID = p.ComputedID &amp;#x200B;
&gt; Otherwise you'll have to use some REGEXP to find the _ and take everything to the left of it Why wouldn't `CHARINDEX()` work?
Exactly what I said in my original reply. * `InvoiceID` is a 1:N relationship between `tbl_invoice` and `tbl_positions`. Your `InvoiceID` fields should match between them, not require parsing on `tbl_positions` so that you can perform a `join * The combination of `InvoiceID` and `Article` should (or appears to) provide a unique key for the `tbl_positions` table to identify each item associated with an invoice ` * There should be a foreign key relationship between these tables such that `tbl_positions.InvoiceID` references `tbl_invoice.InvoiceID` (which would have stopped this from happening in the first place) You'll need the application to be fixed so it doesn't do this boneheaded `InvoiceID` mangling in the first place, then go back and clean up your existing data, then apply the FK constraint.
It works like a champ, thanks for the tip. WITH ComputedID(InvoiceID, Article, Supplier, ComputedID) AS ( SELECT InvoiceID ,Article ,Supplier ,(SELECT LEFT(InvoiceID, CHARINDEX('_', InvoiceID)-1)) AS ComputedID FROM Positions ) Reference here.
Hmm.. you can try pasting shit into word and then pasting that into Excel. Hundreds of pages means you're going to have to figure something out.. column select if those fail would be my suggestion if those failed and this was a few pages but.. idk
I can't highlight the tabular data in the pdf. I can highlight the paragraphs in there though...
Thanks! 
Interesting !
I already have a great job where I'm happy and well compensated, but now I want to interview with NYT just to take their SQL test.
Anyone know what kind of stuff they ask in the technical section?
Can you talk to the person who created the PDF? I once saved lots of pain by obtaining the source data in another format. I hate PDFs.
It's provided by the state....
&gt; It's provided by the state.... Doesn't mean you can't get it in another format - just need to find the technical people behind it, since the person who provided it probably didn't do the extract. I work in a field where everything is address-based, this is important, because we have to collect monthly sales tax and other fees - which are entirely based on where in a State, County and sometimes City you're located. Which means we have to get regularly updated files from the various Revenue agencies regarding any changes to which addresses belong where. When I first joined the company, one of my jobs was to manually process these documents - hundreds, sometimes thousands of addresses that needed to have their FIPS/Geocode updated, etc. I finally managed to get a hold of the technical contact at the state that sent the information out and asked them if I could get it in a format other than PDF, and explained why. Their response? 'Of course, why didn't you ask sooner?!'...
How did you explain why? Was it a long explanation? I got to get tabular data from here: https://www.dol.ks.gov/docs/default-source/workers-compensation-documents/schedule-of-medical-fees/2019-schedule-of-medical-fees.pdf?sfvrsn=b0a18e1f_4
&gt; How did you explain why? Was it a long explanation? I just told them that we wanted to process the changes faster, but the current documentation meant that we had to do it by hand, and I asked if they could provide a csv or txt or xml file with the addresses parsed out - which is what they used to generate the PDF, as it turns out. No one had thought to ask them about it before. I'd probably start with the Worker's Comp section here - https://www.dol.ks.gov/contact-kdol - specifically the KDOL.WC[at]ks.gov appears to be a catch-all address and may be able to help you out. I wouldn't attach the PDF though, just give them the file name - a lot of email systems will block PDF attachments from external sources.
Turns out someone in our Business Analyst group already contacted her, though no dice. 
these are two different settings. max degree of parallelism = 1 would mean that you do not want a parallel execution at all (you shouldnt see any parallelism in your plan at all) cost threshold will change at what point the optimizer will use the parallel execution up to whatever degree is allowed.
Understood but my question was based on a specific query. If I want to increase the cost threshold such that a specific query gets excluded for parallelism would using maxdop 1 on said query be the same thing? Does that make sense?
... it is not. Let's say tomorrow several tables in your query will reach multi-billion record size. There's no guarantee that the value that you picked for the threshold would be sufficient to force single-core execution. If single core execution is what's desired for whatever reason, use maxdop.
This is not something I would leave in my database long-term. There is a query that sits right above the cost threshold that I (think) would like to see running with a single thread. It gets executed the most out of any query in my database as it refreshes the application's home screen. Performance in my dev environment seems to be just fine with the cost threshold up to get rid of parallelism in this query but I am a little worried about changing it in the production environment. I was hoping that if setting maxdop 1 for this specific query was functionality identical that I could test how things are without altering more than this one query.
Are you literally just copy-pasting your homework for us? At least try first... &amp;#x200B; Happy to help if you get stuck on something... but this is pretty shameless. 
I am working on it but I'm really confused by it, I just keep going in circles this is what I have for the first question but its not matching what I'm supposed to get as the output &amp;#x200B; \--Q1: List of names of consumers not requesting anything Print 'Q1:' Print '' SELECT [c.name](https://c.name) From Tb\_consumer c WHERE Exists (SELECT [c.Name](https://c.Name) FROM Tb\_Consumer c, Tb\_Requests r WHERE c.Con\_ID=r.Con\_ID)
a hint ("option (maxdop 1)") will only change the query where it is specified
Okay, better than nothing...barely. It looks like "requesting something" just means your ConsumerID goes into the Requests table along with the ProductID that you are requesting. You just need to find any ConsumerIDs that are not in the Requests table. Usually that means a left join, but other options, too. I'd double check whatever your class taught this week to see how you're *supposed* to be solving it.
that's the frustrating part. The interviewer may a few functions etc that they frequently use in SQL where the interviewee may have a set of others they've used for a long time, very successfully, specifically for their own business use cases. The questions can sometimes (though maybe not even intentionally) be overly specific to their own use case. I've missed a few very basic questions on technical interviews simply because they weren't things I've ever had to use in my own roles. I looked them up after the interview and I could have found the solutions in about 1 minute of googling. I realize that this IS allowed for NY Times interviews, but many others are interviewing from a very narrow tunnel and it's very frustrating to be passed over for not being able to have some overly specific function memorized, especially in a stressful situation like an interview. Sorry, just venting my frustrations about technical interviews 
T-SQL: positions.InvoiceID LIKE CONCAT(Invoice.Invoice_ID, '%')
... or great advice if you want to minimize the odds that a BI Analyst will screw up a query.
I am a fan of CTEs, not just for readability but also because I run a lot of SQL queries from an analytics application. It only accepts a single top-level SELECT statement, so temp tables and anything like that is no good, but CTEs work fine. 
I would use a composite table... `cocktail_ingredients`, then you can join all the tables and exclude anything that isn't in stock: CREATE TABLE Cocktails (`ID` int, `Name` varchar(12)) ; INSERT INTO Cocktails (`ID`, `Name`) VALUES (1, 'Margarita'), (2, 'Whiskey Sour') ; CREATE TABLE Cocktails_Ingredients (`Cocktail_ID` int, `Ingredient_ID` int) ; INSERT INTO Cocktails_Ingredients (`Cocktail_ID`, `Ingredient_ID`) VALUES (1, 1), (1, 2), (1, 3), (1, 4), (2, 5), (2, 2), (2, 6) ; CREATE TABLE Ingredients (`ID` int, `Name` varchar(12), `In_Stock` int) ; INSERT INTO Ingredients (`ID`, `Name`, `In_Stock`) VALUES (1, 'Tequila', 1), (2, 'Lime', 1), (3, 'Triple Sec', 1), (4, 'Salt', 1), (5, 'Bourbon', 1), (6, 'Simple Syrup', 1) ; Then your query: SELECT c.Name FROM Cocktails c INNER JOIN Cocktails_Ingredients ci on c.ID = ci.Cocktail_ID INNER JOIN Ingredients i on i.ID = ci.Ingredient_ID GROUP BY c.Name HAVING MIN(In_Stock) &gt; 0
Thank you so much for getting back to me! This is exactly what I was trying to do! I’m going to implement this with a full cocktail menu and experiment with it in a live website, I’ll make sure to share the finished results!
&gt;google hangouts interview This would immediately kill my interest as a job seeker
I was able to get the first two done and get the correct output as follows but I'm stuck on the third one, any thoughts? &amp;#x200B; &amp;#x200B; Here's what I have for the first 2: &amp;#x200B; &amp;#x200B; --Q1: List of names of consumers not requesting anything Print 'Q1:' Print '' SELECT Name FROM Tb_Consumer WHERE Con_ID NOT IN (SELECT DISTINCT Tb_consumer.Con_ID FROM Tb_Consumer, Tb_Requests WHERE Tb_Consumer.Con_ID=Tb_Requests.Con_ID); &amp;#x200B; \--Q2: List of names of consumers not requesting computers or autos Print 'Q2:' Print '' Select Con_ID, Name from Tb_Consumer Where Con_ID NOT IN (Select Distinct Tb_Consumer.Con_ID from Tb_Consumer, Tb_Requests, Tb_Product Where Tb_Consumer.Con_ID = Tb_Requests.Con_ID and (Tb_Product.Name = 'Computer' or Tb_Product.Name = 'Auto')) &amp;#x200B; &amp;#x200B; &amp;#x200B; --Q3: List of names of consumers requesting at least one product but not requesting milk Print 'Q3:' Print '' Select distinct Tb_Consumer.Con_ID, Tb_Consumer.Name from Tb_Consumer, Tb_Requests where Tb_Consumer.Con_ID = Tb_Requests.Con_ID and Tb_Consumer.Con_ID Not in (Select Tb_Consumer.Con_ID from Tb_Consumer, Tb_Requests, Tb_Product where (Tb_Product.Name = 'Milk') and Tb_Consumer.Con_ID = Tb_Requests.Con_ID ) &amp;#x200B;
Interesting, I would never have thought that a newspaper had a dedicated database department, I though they used standard/purchased systems and used contractors for maintenance and extending the ability of said systems. Neat.
Took a look at the document and the tables are pictures, not any type of table. So short of ocr there's nothing to do... And if you have proposed rates as a spreadsheet I'd definitely go with those before ocred data .. spot checking will definitely find errors on the ocr but most likely any differences between proposed and actual rates from State governments will be onesie twosie here and there so there will be a lot less cleaning required
Not sure I follow your question but if you know your table is 'missing' the data than you won't be able to query a record that doesn't exist. However you can use a query to look for a specific record like -- SELECT * FROM database..tablename AS t WHERE t.Name = 'Jane' AND t.Date = '2-Jan' Adjusting your WHERE clause to fit whatever your search calls for
Select 2 as personid, datefield From peopledates p Where p.personid = 1 --Id of guy who has all the dates and not exists (select 1 from persondates p2 where p1.datefield = p2.datefield And p2.id = 2) --id of guy w missing dates 
I set this query up to be a good basis for an insert. Also I assume you have personids instead of just storing by name. If you do have ids technically you could do this as a not in query but I prefer not exists for non static lists because it's more flexible and the performance is usually better or the same (double technically you can also do not in universally by abusing concatenation but that's sinful imo)
&gt; This exercise is not to test how well a candidate has memorized SQL functions, so we encourage candidates to use documentation or other online sources if they get stuck. We want to see how well they can ask questions about the data, work with documentation to get effective SQL statements and interpret the results of a query. Well now I kind of want to work for NYT. Seems like a reasonable company.
It sounds like you did not read the article.
Select distinct [Student ID], [Start Date], [Order Date] from students s join purchases p on s.sudentid=p studentid where Orderdate = (select min(orderdate) from purchases p2 where s.studentid = p2.studentid and startdate &lt;= orderdate)
Wrote that on phone and.it needs to be fixed for your crappy schema with spaces in field name's. Query can be written without distinct if performance is a major issue but writing it properly is a pain and if you don't distinct you will get dup rows when student buys more than one book on first purchase. Inner join but if you want it as left join you can either put the subquery in the join or if rbdms doesnt allow you can coalesce
This seems to be written from the point of view of a students table and a purchases table without including the orders table, but I think I can make it work. Thanks.
I read it and mentioned it in my comment 🤷‍♂️
&gt; crappy schema I'm sorry...?
Haha sorry I just hate spaces in column names
Totally missed the third table, but yeah unless I'm missing something you can just join that to the student table. If youre interested in Max start date or min start date per student you can do the same thing to get that
Nah that's ok. I'm frazzled and was just typing off the top of my head. Appreciate the help with this.
Haha no worries man, I didn't need to drop snark either with my namibg preferences. And happy to help
A quick look at their wiki tells me they have 1300 staff writers, were founded in 1851, and have almost 3 million digital subscribers. At the very least, I imagine they need a few DB's just to manage their ERP.
Without seeing the schema/data it is difficult to give specific advice, but a common approach to things like this is to use common table expressions combined with ROW\_NUMBER() OVER (PARTITION BY ... ORDER BY...) and then selecting only where the row number = 1. You can use multiple CTEs to build up the data if need be. Good luck!
"if you are interested in JOINing our team" ...nice
I am creating a database.
Well ... there's pros and cons to each. First, both queries assume that there is one and only one city with the shortest/longest length. If there were multiple cities with the same name length, yours will pseudorandomly (ie based on clustered index/disk sort) choose one. Perhaps the prompt assumes this as well ("Find the two ..") but I wouldn't choose the tiebreakers randomly in any scenario, toy or otherwise. &amp;#x200B; Yours is definitely more readable and understandable. It clearly answers the question (with the caveat above), it treats the longest and shortest as separate queries (so we could easily modify one or the other without interfering with the other), and while it requires two separate table scans to find the shortest and then longest city, metadata such as field length almost always requires table scans anyway. The other one is more "clever" but arguably poorly written, and certainly less clear than your own. The idea is to determine the values of the minimum and maximum length, filter the base table by those two values, and return the results. &amp;#x200B; If you simply used a WHERE clause to filter, the issue is you'd have to independently determine the MAX and MIN length (e.g. WHERE length([a.city](https://a.city)) = MIN or length([a.city](https://a.city)) = MAX \*or\* WHERE length([a.city](https://a.city)) IN (select MIN union select MAX ) This query cleverly circumvents that by joining them as columns to each of the rows in the station table and then using the join condition to filter rows where the length of the city matches one of the two columns. And since they are returning all matches of both MIN and MAX they can't use your LIMIT 1 trick to pseudorandomly select a single row, they decidedly UNrandomly select a winner by choosing the city whose name comes first alphabetically (`min(`[`a.name`](https://a.name)`)`). To recap: the SO query 1) Chooses the alphabetically first city and its length 2) for each possible length in the station table, 3) filtered to the two lengths corresponding to the smallest and longest length in the station table. select min(a.city), length(a.city) -- 1 from station a -- 2 join -- 3 ( SELECT min(length(city)) minlength, -- 3 max(length(city)) maxlength -- 3 from station) b on length(a.city) in (minlength,maxlength) -- 3 group by length(a.city) -- 2 The problem with this query is mostly in the readability/clarity. Performance wise it also must perform two table scans to first find the min and max lengths and then to filter the station table based on those values. It also fairly discretely commingles the longest and shortest length query. If I changed the requirement to the second shortest city name, yours would be easier to modify than theirs by far. But that's thinking more sustainable queries, not answering a homework problem.
Add foreign key adds a property to an existing column in a table. Table has to have the column in it.
First, I'll just say that while it may seem "less than efficient" to have a wide table with low density/high sparsity, many systems today are designed precisely for such a model. SQL Server supports Sparse columns ... [https://docs.microsoft.com/en-us/sql/relational-databases/tables/use-sparse-columns?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/relational-databases/tables/use-sparse-columns?view=sql-server-2017) And of course modern database tools like BigTable, RocksDB, HBase, Cassandra, etc. all directly, even - dare I say it - joyfully? handle sparsity through bitmapping - using key-value pair storage to store populated attributes and a map from those k-vs to the "row"/document/etc. So my primary suggestion would be to use one of these existing tools (BigTable is a good one to start with) as it's literally designed for exactly this sort of problem (which is highly common in things like social media platforms, SEO optimization, log analytics, etc.) &amp;#x200B;
[http://www.studybyyourself.com/seminar/sql/course/?lang=eng](http://www.studybyyourself.com/seminar/sql/course/?lang=eng). There you can learn basics for querying a database using SQL. The course, along with examples, is quite easy to follow. You can submit exercises as well.
I've learnt SQL with the following resource. It provides clear examples and you can check your knowledge by submitting exercises online. All for free. [http://www.studybyyourself.com/seminar/sql/course/?lang=eng](http://www.studybyyourself.com/seminar/sql/course/?lang=eng).
One of the best online course. For beginners and for free. With examples and online exercises. [http://www.studybyyourself.com/seminar/sql/course/?lang=eng](http://www.studybyyourself.com/seminar/sql/course/?lang=eng).
Unless I am missing something, that code seems to INSERT a record into a database .. 
I can tell you where I practiced from. I found sqlzoo, datacamp, hackerrank, stratascratch and leetcode. Tried them all and found [stratascratch.com](https://stratascratch.com) more useful out of those 5. They have datasets pre-loaded with questions and answers you can practice with. They source their questions from technical interviews from companies so I found it helpful to use for interview practice. You must try this to learn SQL and Python. 
I can tell you where I practiced from. I found sqlzoo, datacamp, hackerrank, stratascratch and leetcode. Tried them all and found [stratascratch.com](https://stratascratch.com) more useful out of those 5. They have datasets pre-loaded with questions and answers you can practice with. They source their questions from technical interviews from companies so I found it helpful to use for interview practice. You must try this to learn SQL and Python.
&gt; The table Tracks doesn't have DriverNum because this if you want DriverNum in Tracks to reference Drivers, you have to add the column DriverNum to Tracks first, before making the foreign key reference
humour me SELECT DISTINCT dbo.Documents.Filename , Ss.Name AS State , VariableValue_1.ValueText AS Author , dbo.VariableValue.ValueText AS Description FROM dbo.Documents INNER JOIN dbo.Status AS Ss ON Ss.StatusID = dbo.Documents.CurrentStatusID AND Ss.Name IN ( 'WO Sign-Off' , 'WO Processing' , 'WO Completed' , 'WO Approval' ) INNER JOIN ( SELECT dbo.DocumentsInProjects.DocumentID , dbo.Projects.Path , dbo.Projects.ProjectID FROM dbo.DocumentsInProjects LEFT OUTER JOIN dbo.Projects ON dbo.Projects.ProjectID = dbo.DocumentsInProjects.ProjectID ) AS P ON P.DocumentID = dbo.Documents.DocumentID AND P.Path LIKE '%Work Orders\' INNER JOIN dbo.VariableValue ON dbo.VariableValue.DocumentID = dbo.Documents.DocumentID INNER JOIN dbo.Variable ON dbo.Variable.VariableID = dbo.VariableValue.VariableID AND dbo.Variable.VariableName LIKE 'Description' INNER JOIN dbo.VariableValue AS VariableValue_1 ON VariableValue_1.DocumentID = dbo.Documents.DocumentID AND VariableValue_1.ValueText &lt;&gt; '' INNER JOIN dbo.Variable AS Variable_1 ON Variable_1.VariableID = VariableValue_1.VariableID AND Variable_1.VariableName = 'Author' WHERE dbo.Documents.Deleted = 0 AND dbo.Documents.Shared &lt;&gt; 0 AND dbo.Documents.ObjectTypeID = 1 AND dbo.Documents.Filename &lt;&gt; 'WO-blank'
Which rdbms ?
To be honest if you hadn't one of the rest of is would :D
look into unnest
One idea is that you can pivot your extra field data, then filter on it like a normal table. SELECT o.Id , o.Nr , o.Name , x.ContractSentDate FROM tblObject o INNER JOIN (SELECT efd.OwnerId , MAX(CASE WHEN efd.FieldId = 10014 THEN efd.Value ELSE NULL END) AS ContractSentDate , MAX(CASE WHEN efd.FieldId = 10015 THEN efd.Value ELSE NULL END) AS FollowUpContractDate FROM tblExtraFieldData efd WHERE efd.FieldId IN (10014,10015) GROUP BY efd.OwnerId) x ON x.OwnerId = o.Id WHERE x.FollowUpContractDate IS NULL AND x.ContractSentDate &lt; @Filter;
I suppose that I won't have to sacrifice any goats this time. This does exactly what I want and is more clever than anything I could ever think of. Thank you a whole lot! 
IF you are on Mac and have Time Machine enabled and the files were deleted, you should be able to find them if you know the path. Similarly, IF you are on Windows and you enabled Windows File History (similar to Time Machine on Mac), you'd be able to find them if you know the path. &amp;#x200B;
Still struggling with this. Would you be able to give any insight into what else I'm doing wrong here? WITH temp1 AS ( SELECT ds.id, array_agg(STRUCT(REPLACE(ds.id_list, '"', '')) ORDER BY ds.ID_LIST ASC) as value FROM dataset ds GROUP BY 1 ) SELECT t.id, lu.name FROM temp1 t JOIN lookup lu
Thanks dude I found the files hidden away on some home dbeaver folder. For some reason they weren't turning up on the search
One potentially poorly performing but easy way to do this would be to store as temporary tables * your above query (is there a reason why you can't use INTERSECT in between the second and third queries?) * query for rev_total as Rev2018 where return_year = 2018 * query for rev_total as Rev2017 where return_year = 2017 * query for rev_total as Rev2016 where return_year = 2016 and then simply SELECT * and join these tables. I'm sure someone else will have a better-performing solution in the event you need to run something like this frequently.
i'm not working with bigquery and you'll need to fix syntax. but i think you'll need to left join to unnest first. So if your dataset is what you have in the OP, then select id, link_id, name from dataset left join unnest( id_list) as link_id left join lookup lu on lu.id = link_id
This is Absolutely Magnificent, Super Helpful. Thanks a lot man. I wish there was a way to mark this as `Answered`.
You can put together a mailing list for a community theater using MySQL. When you have no experience you offer to work for free.
My pop was HVAC for 35 years, and the perennial applicants always wanted to be the controls guy with the little pocket protector checking a box, making tiny adjustments with a precision screwdriver, clean shirt, high five/six figures. But they didn't want to do the dirty work, learn systems, crawl through dusty access crawlspaces or service ducts or chase wires all day from some undocumented hack from 20 years ago. Point being if you can do the work, reliably, you'll get hired. You'll be in demand. It's nothing to wear a shirt with a pocket protector and it's everything to know which way to turn that little screw and by how much, with experience to know how that affects the system. You might have to get dirty and deal with someone else's mistakes long before you are just maintaining scripts that do your job for you twice a month.
Started on a helpdesk and caught a bit of luck after a few years. The work sucks but it opens doors. Shoot for smaller companies and try to avoid contract work if possible.
I’m not opposed to “getting dirty” at all, I would definitely prefer starting that way. Problem is, it’s feeling like even for those positions, employers are looking for people with demonstrably dirty work clothes to consider them for those jobs. If that makes any sense. 
Nepotism. Father was IT Manager/ERP Implementation Consultant while I was in college. He got me some part contract gigs updating Access applications and that introduced me to my first role doing reports development. Most of it was done via the GUI and then we'd supplement further functionality using VBA. Fast forward a few years later and I had done some more contract work and ran the operations end for a small business where I had a falling out with my business partners. After a short down time I decided to go back into IT but with a regular schedule. Found a gig doing desktop support. That eventually led to doing more stuff on the network side and eventually managing various kinds of servers (web, db, etc.) I got bored with that and decided I wanted to get back to doing reports development and a friend said that a job opened up at their small B2B firm (less than 50M sales annually). I interviewed well enough even though I still didn't really know how to work with modern toolsets (TSQL, SSRS/SSIS/SSAS) but my friend vouched for me and they could tell I was hungry and eager to be of value. That gig gave me the opportunity to learn on my own plus from some really smart guys that we had taken on as consultants for a major application development project. I would spend weekends working with my new toolset and learning the ins and outs and how everything works together in the context of our infrastructure at work. For five years I tried everything and read everything I could to learn best practices. I even got to mentor multiple people on how to use SQL effectively for what their use cases were. These were a mix of developers, data analysts, and business subject matter experts. As for your case, figure out what toolsets companies in your area are using. Setup cloud servers with those toolsets installed and just start playing around. Eventually create projects for yourself to show employers your mastery of the various concepts. While doing this get jobs that will expose you to business data. Even if you're just tooling around in excel. Speak with the data team in your organization and see how they do things. Ask if you can help out. Even if you don't get a data analyst role right away, there's nothing saying that you can't perform some of those functions in whatever business role you may have.
Have you built anything? Why should I trust you? You need to **show** that you have learned and can learn. Install SQL on a Virtual Machine, write a blog post about it to prove you have. Download a sample database and write some SSRS reports or a web dashboard against it, publish those to GitHub with some documentation. Go on StackOverflow and research and answer some questions you don't currently know the answers to. Then on your CV you can put a blog showing writing and practical skills, a GitHub showing you can manage, finish and document projects and some StackOverflow points to show you have the technical knowledge, that's more than enough to get hired
I might be missing something but this is pretty much an open and close case for [PIVOT](https://oracle-base.com/articles/11g/pivot-and-unpivot-operators-11gr1). I wanted to get you a working example on [livesql.oracle.com](https://livesql.oracle.com) but it appears the site is falling all over itself at the moment. `-- create a base table like what you have` `create table prep (` `id number generated by default on null as identity` `constraint prep_id_pk primary key,` `email_combined varchar2(255),` `rev_total number,` `return_year number` `)` `;` `-- insert some dummy data` `insert into prep(id, email_combined, rev_total, return_year)` `select 1, 'learn@reddit.com', 10, 2016 from dual union all` `select 2, 'learn@reddit.com', 10, 2017 from dual union all` `select 3, 'learn@reddit.com', 10, 2018 from dual union all` `select 4, 'learn@reddit.com', 10, 2019 from dual union all` `select 5, 'sql@reddit.com', 100, 2016 from dual union all` `select 6, 'sql@reddit.com', 100, 2017 from dual union all` `select 7, 'sql@reddit.com', 100, 2018 from dual union all` `select 8, 'sql@reddit.com', 100, 2019 from dual union all` `select 9, 'smoo@reddit.com', 1000, 2016 from dual union all` `select 10, 'smoo@reddit.com', 1000, 2017 from dual union all` `select 11, 'smoo@reddit.com', 1000, 2018 from dual union all` `select 12, 'smoo@reddit.com', 1000, 2019 from dual` `--pivot on the specific years of interest` `select * from (` `select email_combined, rev_total, return_year` `from prep` `)` `PIVOT (SUM(rev_total)` `FOR return_year` `IN (2016, 2017, 2018, 2019))` `-- insert some sparse data missing some years` `insert into prep(id, email_combined, rev_total, return_year)` `select 13, 'new@reddit.com', 10000, 2017 from dual union all` `select 14, 'new@reddit.com', 10000, 2019 from dual` `-- observer there are nulls for years 2016,2018 for the new data` `select * from (` `select email_combined, rev_total, return_year` `from prep` `)` `PIVOT (SUM(rev_total)` `FOR return_year` `IN (2016, 2017, 2018, 2019))` `-- convert the nulls to zero is needed` `select email_combined, nvl("2016",0) as "2016", nvl("2017",0) as "2017", nvl("2018",0) as "2018",nvl("2019",0) as "2019"` `from (` `select email_combined, rev_total, return_year` `from prep` `)` `PIVOT (SUM(rev_total)` `FOR return_year` `IN (2016, 2017, 2018, 2019))`
Dude, even if they ask for 4 years of experience apply, but don’t lie. Worst case, they don’t contact you. Best case, you’re invited for an interview. Be up front about your lack of SQL experience but express the fact that you’re highly capable of learning on the spot. I had 6 months of bare basic SQL skills when I got my first job. Became the best back end developer on my team by my first year anniversary.
I took a job that was basically doing data entry and asked for read only access to play with the data and help in reporting. That allowed me to prove myself and eventually move up and gain more responsibility. Granted, my title isn't sysadmin or data architect but I've definitely produced work products in those roles and could probably would have a chance if I would want to apply for those jobs. My advice is to gain access to a database in your work place. If you can't, make one - and demonstrate how much easier it is with dealing with data. It might have to be in Access. You'll likely be doing this work for free, but as a bullet point on your resume it'll probably be worth it. It was for me in my career. 
Unfortunately, it's the same results.
The grandfather of all Catch 22s
yes all i did was reformat your SQL to make it easier for people to read you need a CTE for each table that you want the latest of
Can you elaborate a bit on the contract work please? I haven’t done any contract positions. 
Oh. Ok. Yeah every time i fix the formatting, the Server reformats it like this when I save it. I have no idea why.
Unfortunately, I will have to look for data outside of my current employer. I’m not aware of any sort of data analysis, and if there were it would probably completely out of the scope of my current position. They don’t like cross training like that. 
Got a job doing data entry for some radio stations. Director wasn't able to answer some questions regarding donor behavior. I offered to do some analytics in Excel, and a year later had built a full fledged analytics suite. Still didn't know SQL, but knew I wanted to make a career out of data. Next job hired me based on developing analytics, IT night classes, and a bargain basement salary. I took to SQL like a fish to water, and the rest is history.
I mean more try to avoid contract employers that support 3rd parties. That's how I started for about a year and a half, and it was living hell after awhile. Taking calls nonstop for 8 hours a day and having to meet impossible ever-changing metrics. Finally found an onsite helpdesk position working for a hospital (I'm still at the same place now), and even though it was still a helpdesk position, it was a much less hellish experience. Plus there's a whole IT department where you get to know people and are more likely to have an opportunity to move up the chain. 
Ahh gotcha. I worked in a non-IT call center for a third party. Wholeheartedly agree with your statement. 
This is the way to do it. Works with SQL Server when you make the syntax changes.
Yep then you have to find an organization that is willing to take you on where you can grow to hone those kinds of skills. Try volunteering and see if there are organizations like that in your area looking for help. There are organizations like Data For Good where analysts meet to do just that.
&gt; the Server reformats it like this when I save it. I have no idea why. i know why... because it's a piece of shit, that's why
I graduated with a BS in Business Administration. First job out of College was a Help Desk Technician role that didn't require a College Degree. From there I worked with SQL 5-10% of the time, mostly maintaining data and investigating issues. I learned the basics of querying (aggregates, grouping, subqueries) and was mostly just writing simple update statements to fix data in the CRM system. I also learned a little bit about Crystal reports as well. After a year or so of that job, I found a job as a SQL Server Report Writing consultant. For 11 months all I did was write SQL and SSRS reports as part of a team of contractors. After that immersion, I was able to advance to another position that is about 50% SQL Server/Report Writing work, and the other 50% is developing enhancements to our accounting software and integrating different business systems with our reporting tools. Don't be afraid to take a job that exposes you to a lot of different aspects of technology. Once you find the one that speaks to you, focus on learning as much as you can. 
Oh, good. I'm not the only one who thinks that.
Do multiple small jobs on freelancer.com or upwork.com Put them on your resume.
You need a car to get to work. You need to work to get a car. You eat too much because you're fat. You're fat because you eat too much. Honestly the best way to get into SQL is to have it fall into your lap and run with it. In other words, I didn't choose the SQL life, the SQL life chose me. As far as how I feel about it as a day job, indifferent. Probably because I never proclaimed that I wanted to be a database administrator. But I also wasn't against it. It just sort of became this way because of the way it is. It really do be like that sometimes. I started as a PC hobbyist/enthusiast, eventually got a job at a retail store repairing computers, which later became "GeekSquad" (I won't defend their stereotype, but not everyone was a dimwit) , and then finished college and jumped ship into entry level helpdesk IT, then finished grad school, then IT Management, then IT healthcare, and through accumulation of knowledge from Windows, Linux, all the technologies that CompTIA Net+ covers, and casual database experience along the way, it all funneled into managing ERP and developing ETL processes, database packages, custom scripts, and everything else that involves the ins and outs of ERP, which is what I've been doing for the past few years. In IT, it's hard to target something specific. You take windows of opportunity as they are presented to you, while trying to steer the ship toward your goal. I never tried to steer the ship and while I thought I was going to end up as a Cisco WAN administrator and infrastructure planner, I ended up as an OCP. That's ironic because I've always hated Oracle as a company, but now I'm making more money supporting Oracle products than any other job I've had, by a large margin. I might have peaked. So get experience where you can, learn independently, and don't be afraid to invest into your future. At one point in my life, I sold my new-ish car and bought a beater just so I could install full server racks in my house and have a mock network with real cisco routers, HP managed switches, and VM clusters, running AD, exchange, SAN, Oracle database, microsoft sql server, Nagios, and more. I don't want to sound like "kids these days, I'll tell ya hwhat", but don't go for the low hanging fruit. Challenge yourself and do something extreme. You'll learn a lot and the experience is real! And it's the only time in an interview where bragging about your "full rack" might actually get you hired. In the time it took to write this, I got paid roughly $40. 
Thanks for the tips! If it helps, [here's a screenshot of the tables](https://imgur.com/a/YR0jEaK).
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/Iw9gZGl.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
Agreed on this. We're trying to fill a position now, one of the big things we look at besides experience is trainability/eagerness to learn, and a personality fit for our company culture.
&gt;So, I have a view that's pulling in information from a few different tables Well ... that is pretty much a view is :P &gt;Is there a way to 'transform' the data within the view to remove the '&amp;', but not affect the underlying table the view is feeding from, if that makes sense? Views never affect tables, unless you have something like a procedure that feeds a table using the view as source. Anyway, you can use REPLACE function in that view column, something like: REPLACE([table_field],'&amp;','') 
I got a bit of the 4th query done, but it's not finished yet. &amp;#x200B; SELECT NAME, COUNT(DISTINCT Prod\_ID) AS Prod\_ID FROM Tb\_Consumer INNER JOIN Tb\_Requests ON Tb\_Consumer.Con\_ID = Tb\_Requests.Con\_ID GROUP BY Tb\_Consumer.Con\_ID, NAME HAVING COUNT(DISTINCT Prod\_ID) &gt; 2;
If it's "at least two", wouldn't this: `HAVING COUNT(DISTINCT Prod_ID) &gt; 2;` need to be this: `HAVING COUNT(DISTINCT Prod_ID) &gt;= 2;` 
I tried that as well, it still gives the same answer
I would find a technical college that has some sort of basic SQL project. If you can do the SQL project and show that you know the work (especially if you get top marks) then that really goes a long way. As well ALWAYS create a personal github repo for your personal code (PERSONAL CODE is the key word here). You can then point people to it. I have two actually one private - that I'm continually tweaking and one public - the public repo is the stuff I'm really proud of and point to recruiters, hiring managers and HR departments etc.
I got it to work by doing this! —Q4: List of names of consumers having purchased at least two product types (Don’t use aggregate function) Print 'Q4:' Print '' Select distinct Name from Tb_Consumer where Con_ID In (Select t.Con_ID from Tb_Transactions t, Tb_Transactions t2 where t.Con_ID = t2.Con_ID and t.Prod_ID != t2.Prod_ID)
I got a degree in electrical engineering and then got a job working as a controls engineer at an integration firm. Parts of our projects involved data collection, so I had to learn very basic SQL in order to get the data into Access and the later SQL Server. Talking with some of the software guys, I saw that the demand for "big data" was only going to get bigger. So I installed a developer version of SQL Server on my laptop, downloaded some sample databases, and went through a bunch of online tutorials. Over time, I just got better and better, and I found myself doing more and more real SQL development work, and zero controls work. That was about 15 years ago. Right now, I am one of the top paid database developers in my company (completely different company, though).
I got them all done and demoed! This is what my professor said he was looking for out of q4 &amp;#x200B; \--Q4: List of names of consumers having purchased at least two product types (Don’t use aggregate function) Print 'Q4:' Print '' Select distinct Name from Tb_Consumer where Con_ID In (Select t.Con_ID from Tb_Transactions t, Tb_Transactions t2 where t.Con_ID = t2.Con_ID and t.Prod_ID != t2.Prod_ID) &amp;#x200B;
This should give you what you are looking for! &amp;#x200B; --Q4: List of names of consumers having purchased at least two product types (Don’t use aggregate function) Select distinct Name from Tb_Consumer where Con_ID In (Select t.Con_ID from Tb_Transactions t, Tb_Transactions t2 where t.Con_ID = t2.Con_ID and t.Prod_ID != t2.Prod_ID) &amp;#x200B;
Thank you! I got most of it, I didn't realize I needed the !=
There could be a SQL way to do it, but it sounds to me like the security controls should be on the application(s) and each app should have its own service account on the server. Dunno if you have the ability to do that, but that's my initial reaction.
No problem! Let me know if you need help with any of the rest of them, I have them all done 
The votes seem polarizing. I’m wondering if perhaps you’re turning people off by boiling down the skills required for what sounds like an advanced position into ten days and a web video. 
I'm not a newbie to SQL, I just haven't had to write advanced queries for a couple years because my last job just had me doing fairly basic stuff. I can definitely do a refresh over 10 days as I've got about 8 hours a day free since I'm only doing contract work atm. 
I enrolled in some courses online and learned the basics. did some projects which i took snapshots of and brought into interviews letting them know that im self taught in sqland i finally got a job where its very sql heavy and so far im loving it
Will try this out, thank you!
&gt; Haven't worked with anything besides basic CRUD stuff for a couple years Perhaps this isn't the clearest wording. 
I don't have that possibility. On the internal app we can do that, but in qgis or anything like that i don't. In qgis you must connect only with an sql user. I wanna now how dba's deal with apps that require the username and password to an sql user. It's a way to add another layer of security with user and password to hide the username and password of the sql user? 
Just meant that for the last couple years I've only had to do CRUD stuff, but fair enough. 
This. Whatever your job title is, you can change that after talking to your boss e.g. 'data entry level 1' -&gt; 'junior/intern IT specialist/business analyst'. Title changes are free and so employers don't mind at all. Your boss won't care and titles are pretty meaningless If you've done some essentially unpaid work developing some tool to help the business, it doesn't matter so much if it was adopted as the actual experience of trying to develop something that people actually use. If you fail, that's good too. But be able to talk about this project in detail at a future interview. You will at least establish that you have been through 'a full development cycle' although it isn't always put in those terms. 
How long have you been there/since you started sql? I was gearing up for a specific job at a past company in the past and sql was mentioned as a tool that job used a lot. I started learning it and I’ve been kind of in love ever since. None of my positions have ended up using it, and now that I’m in a Udemy course I hate every hour of the day I’m at work and therefore not learning it. 
Assuming the apps are using Windows auth then you can look at application roles. That way you can take the permissions off the windows user and the application gets the rights through the application role. If you can't make app changes then you could achieve the goal using logon triggers. You can deny access to certain users/groups unless the application name provided is that of an allowed app. Note this can be spoofed using a multitude of apps. Depends how smart these users are and if they know how the logon triggers work. 
Create, Read ,Update, Delete
I was pretty fortunate and the path I took was 9 years in the making, but this is how it happened to me. My bachelors is in network administration, got a job doing software support for a data company (didn't know or care that it was a data company; just wanted a job). Worked there for a long time, loved the company, loved my coworkers, and networked. When I took the job out of college all I really wanted was my foot in the door and work my way into the backend of their IT department. Turned out their IT department was pretty lackluster and not a job I wanted with this company. Since I liked working with this company I chose to stay on board but keep my eyes open for new opportunities. I eventually grew tired of software support and leveraged my past people networking to get a job on the data team. I had 0 SQL and development skills but I showed enthusiasm and a desire to excel. Since I had already proven myself an asset to the company they were more than happy to give me the opportunity. I'm 100% self taught in sql, pl/sql, and python.
Just pointing it out cuz I noticed the percentage is all. Good luck!
Not sure what this error means: "Column 'sales_order_lines.order_id' is invalid in the select list because it is not contained in either an aggregate function or the GROUP BY clause." SELECT order_id, item_no, order_qty, SUM(order_qty*unit_price) FROM sales_order_lines WHERE customer_id = 300882
That actually helps a lot! Thank you for that! I do pivots or uni's maybe once every other year, so it's always (re)goggling around to find the column patterns. &amp;#x200B; Bookmarked for posterity!
Thank you, that was exactly what I was looking for, it worked, very helpful and much appreciated. I was over thinking it and didn’t realize that I can pretty much just type in a standard query to the view design window to model data. Very grateful!
Recently used unpivot to dramatically speed up a job. Good stuff.
it took a few months to build my skills. been at my job for about 2 months. id say keep at it sql is very popular as long as you keep learning, adding samples to your portfolio, and applying you’ll get it
Exactly. Since you are using SUM, you have to group the columns outside of it
Group the columns outside of it like select them individually?
I like it!
Main thing I'd do is transition to a web client. More secure and cheaper to develop. So short of that I'd recommend locking their PCs down so they can't execute anything except authorized apps. If there are power users who need a workstation that isn't locked the fuck down but you can't trust them with the risk of them getting into the DB then another option is fixing them an unlocked workstation and making them remote into a locked workstation/virtual workstation if they want to use the app But seriously if there's any appetite for it at all you almost definitely want to get on a web app
Quick and to the point. Just as it should be. Thanks!
At the end you type group by and then list the columns not aggregated
aha! Last question : Where I have ```SUM(order_qty*unit_price)``` can i do an ```AND NOT SUM(order_qty*unit_price) = 0```?
You can do at the bottom: HAVING SUM(order_qty*unit_price) &gt;0
Is WSB1 in both tables a one-to-one relationship?
Do you mean if the tables are structured identically? (Same Exact Columns)? No they’re not. They’re structured different but I only need to pull information from one column of the PR table where the WBS is matching.
No, I mean for one WSB1 value located in one table, does the other table's WSB1 only contain one value as well? Or for the same value, does one table contain duplicates of the value?
It contains multiple of the same value because each instance of it represents another phase in that project
Which one is the right one? Or are all of them relevant to your results?
If it looked at the first instance of the WBS1 that didn’t have any phase assigned it would be sufficient 
I don't have enough information to help make it do that, but what is the field that differentiates between phases?
The phase is assigned in WBS2 column of the table. If I omit any results on the PR table on the join that doesn’t have a phase, that wouldn’t necessarily guarantee that I won’t get multiple WBS1 rows right? Is there a way to just pull 1 result on the join for every WBS1 or is it a constant trial and error until my dollar figures aren’t getting multiplied by the amount of times WBS1 appears on the PR table? 
Add WBS2 field to your GROUP BY and SELECT and see if it gets you closer. 
Actually I need a little bit of help with number 3
Messaged you! 
I like it, it's simple to understand, thanks, I hope to you continues shared information :D
Suppose a venue is open from the time 10:00 to 23:00 each day and the field Duration indicates the number of minutes assigned for a performer at a particular venue. For example, if the Duration is 150, that means that particular venue is reserved for a performer for 150 minutes. Show the VenueID as a result if those venues are free from the time 16:00 to 18:00 on 10 December 2018. If no venues are free at that time slot, the query shows nothing. I couldn't figure out how to ensure there's no assignment that starts before 16:00, but the duration overlaps the 16:00-18:00 time slot.
100%. It's best to look at a requirements list as a wishlist which is why sometimes they come out a little eclectic. They know they won't find someone who fills every requirement, but they want candidates who can check off as many boxes as possible. I'm waiting to hear back on an offer that required strong SQL skills (check), SSIS experience (nope), and experience in visualizing with Power BI (no, but I have a ton of Tableau experience so I can learn). I was up front letting them know I don't know SSIS but that I was very willing to learn and they said it was fine and that SQL was their primary concern. To OP, shoot your shot! You can't score unless you take a shot! 
Can you explain where you used the pivot to speed up a job?
You can go with one of these two resources, [datacamp.com](https://datacamp.com) and [stratascratch.com](https://stratascratch.com). I tried them and I think stratascratch is the most helpful as they have datasets pre-loaded with questions and answers you can practice with. Otherwise, Datacamp is cool for very specific niches. &amp;#x200B;
I discussed this thing with devs and they said same thing. We must integrate all apps in one web app and then is simple. And the other solution with windows Auth on VM was in discussion. All this things require devp work and none of my superiors want to invest in something that already work. My concern was, that, in one day all our database structure can be copyed and then all users can replicate it and then they will no longer need us. How you deal with that situation in your firm. Because we are not in it bussines (but we work almost only with data) and we collaborate with extern devs, nobody cares about that. 
Receive denormalized healthcare claims as a single row. Our model has headers, detail lines, procedures, and diagnosis rows. I used pivot to make rows of the 25 possible diagnosis and 25 possible procedures column values. It was way faster at the million row scale than the lazy way of 25 queries to accomplish the same task.
Got a resume?
Hey YouTube I’m here to show you how to pivot but first let me start with how I got here, it all started 4 billion years ago before the earth accreted into a planet ..
So now I have my code: SELECT VenueID FROM Assignment WHERE StartTime NOT BETWEEN '2018-12-10 3:59:59 PM' AND '2018-12-10 6:00:00 PM' AND VenueID NOT IN (SELECT VenueID, DATEADD(minute, duration, StartTime) AS Endtime FROM Assignment WHERE StartTime &lt; '2018-12-10 4:00:00 PM' AND Endtime &gt; '2018-12-10 4:00:00 PM'); Still I am not sure if this is correct. ; ;
I never understood the point of PIVOT in T-SQL. UNPIVOT I get, but there's nothing PIVOT does that can't be done with CASE statements with a much more readable syntax that works with ANSI SQL. It's no more performant so I personally abandoned it. 
I would use windowing functions. You can use lag() to identify the previous order date *and* whether or not a particular ordernumber had one action type preceding another action type. You could also do it the "hard way" by selfjoining the table on customer and where the order date is less than the primary order date and then grouping on the minimum date diff between the two order dates, and then calculating count independently. Similarly you could use a correlated subquery to find all orders with an approved action, jointhat by ordernumber and order date then summarize by count and a case statement if your join has a match or not.
Unless the report is available to the public, don't put it into your resume. Also keep in mind that the first 2-3 times your resume is looked at, the reviewer won't have any clue as to what it says about your skill level. For that matter, the first pass or two through the resume likely won't even be done by a human being! Sketching a hypothetical report design on a whiteboard in an interview, however, would be really good. And regardless, *definitely* don't put real data in it. Other notes: * Remove the word "many". It looks like you're padding. * Remove this entirely : "Wrote many SQL queries accessing and joining various database tables at request of multiple different employees for specific data needs." - If you're working as a data professional already, this is table stakes. * Remove words like "intricate and detailed" - Again, it looks like padding the resume. * "Shaved minutes of a constantly repeated task by creating a tool to check the status of an order quicker as opposed to using archaic ERP system. " - quantify this. How much money or person-hours per year does this *actually* save? "Reduced order status review costs by X hours/week by creating new tools." *Do not* put speak negatively about anything in your resume (the "archaic ERP system"). You should head over to /r/resumes for a full resume review/critique; the things you can improve in your resume aren't SQL specific but more about style and language.
Interesting, never realized it wouldn't throw an error for that before. 
I wouldn't put any report, especially this report* on your resume. Maybe put a link to your GitHub, that's the accepted way to share your portfolio for a dev/analyst position. ^* - it is too text heavy and looks like something from the early 2000s. Look at /r/dataisbeautiful for a good example of what wows laypeople. 
1=0 and '' != '' in a where clause will also never return results but they also are syntactically fine. In sql specifically and in computer and human language generally a statement doesn't have to make sense to be syntactically correct. "The left bulwark flagellates at midnight." is a syntactically a correct sentence. The syntax check is to make sure the sql engine understands what you are trying to say or do not to determine if what you are trying to say or do is stupid.
I posted the wrong code. Thanks for telling me.
1. You could install Postgres or SQLite (or any other RDBS) on your computer and load some data onto it to play around. See here [http://www.postgresqltutorial.com/](http://www.postgresqltutorial.com/) 2. [https://pgexercises.com/](https://pgexercises.com/) contains a lot of good querying exercises allowing you to run code online. 3. [https://www.db-fiddle.com/](https://www.db-fiddle.com/) also lets you input and query your own data online. 
The best practice would be to link group names and category names in their own table, if it's really the case that there is never more than one category name for one group name. Then whatever process builds this table would join to that table to deliver the category name. To update the rows now, just create that separate group_name category_name table by selecting distinct group_name category_name, then look into "update table a set column = whatever from table_2 b where a.something = b.something" 
&gt; not to determine if what you are trying to say or do is stupid. upvote
Table 1 - Campsite * CampsiteID int, autonumber. used to uniquely identify a campsite. * Name - friendly name for the campsite * Latitude - location information * Longitude - location information Table 2 - Comment * CommentID int, autonumber. used to uniquely identify a comment. * CampsiteID int. Refers back to a particular CampsiteID in the Campsite table * CreateDate - datetime field to store when the comment was created * CreateBy - field to store the name of who made the comment * CommentText - long text field to store the actual comment This, though very simplistic, will work. You have a way to define campsites, and a way to leave comments about a specific campsite. You could easily go a bit further and add some sort of "users" table, and change the comments table to point at the users table. This does have a possible side effect of making people create an account in order to leave a message. If that is not required, create a "guest user" as userid 1, and assign all anonymous comments to that user. That way you're not writing code to handle "missing" user information when displaying comments.
table - Campsite { id, lat, long, name, ......} table - CampsiteComments { id, CampsiteID, Comments ....} the CampsiteID is a Foreign Key to the Campsite table. This way you can store n number of Comments for each Campsite 
What kind of database are you using? If you're using Postgres / MySQL, look into the JSONB or JSON field types. They work well when you're not completely sure how you want a data model to look. 
Would you say in that one year period that you learned more SQL from work within the job, or more outside of work learning? 
Nothing beats working with actual data. When you learn outside on your own, the data you’re most likely going to be presented with is clean data. Real world data is seldom clean. I think learning outside definitely has its benefits, but working with company data will give you a wealth of challenges and opportunities you won’t be able to find anywhere else.
I will be graduating at the end of fall 2019, and during that semester I’ll be taking SQL and the like courses. My job I’m at currently is moving me to the Information Systems department to work with SQL directly and they said they’d teach me as I don’t know much SQL currently other than the basics I learned online. Thanks for the response! 
Hey, congratulations, and good luck. 
I started with [SQL Express](https://www.microsoft.com/en-ca/sql-server/sql-server-editions-express), it has all the basic stuff (mainly SSMS) you need to get started in the Microsoft environment. Then almost all of the online examples use the [AdventureWorks](https://docs.microsoft.com/en-us/sql/samples/adventureworks-install-configure?view=sql-server-2017) free database to some extent.
https://www.reddit.com/r/SQL/comments/b31tkt/certifications_for_sql_developer/
SQL Server developer edition is now free, and doesn’t have the size and feature restrictions that Express edition does. Same features as Enterprise edition, but licensed only for dev/test purposes. 
Thank you so much. I just have one more technical question. I don’t have the campsites in my database. It’s a list from a 3rd party api. How should I put this data In my database? 
I studied Finance and am not in a Business Systems Analyst role. Never used SQL prior to my role, and now I use it for 50%+ of my job. I met my now boss at my girlfriend's, father's, dental practice's Christmas party. Told him my mass interest and drive to learn more about Data &amp; Analytics. A position opened up, he texted me to apply, and 4 interviews later, I got the job. I would owe it to him for part of my getting the job; however, I also would like to highlight that various interviewers that I met with after being in the role stated that my willingness and hunger to learn about this was a huge factor for getting to where I am now. &amp;#x200B; Years of experience is meaningless and a way to weed people our of applying. I had no Github or projects to share that related to my current role, so these are things that aren't necessarily needed. Keep growing in your knowledge, show your willingness to learn and be hungry. Rejection is normal and will always happen. Let it be fuel to the fire my friend! 
Good point. I'd prefer if it gave an error. There must be a reason for it.
Look up window functions.
I agree entirely. But i don't think asking if 'a = null' is a stupid. I've always been told SQL is a 3 state logic; 0, 1 and NULL. If you can ask is a=1 and is a=0 why can't you ask is a=NULL.
You can. a IS NULL a IS NOT NULL Works fine. Alternately, you could do a COALESCE(a,'THIS_VALUE_IS_NULL') != 'THIS VALUE IS NULL' for example.
You asked why doesn't it return a syntax error. What part of the syntax do you think is incorrect? And you can ask if a = null that's why no syntax error is thrown. The sql engine just won't ever evaluate that as true because null doesn't mean "a special kind of nothing or zero", it means "unknown". If I asked you if the number of bills in my wallet was equal to the number of hairs on my head would you say yes because they are both unknown to you?
You'll need to write something to basically "scrape" the API to get all the campsites. There are many different ways to do this. All of them are basically like this though: 1. Call the API function to get a list of the various campsites. With luck, the return values will include all the information you need to populate the Campsite table. If it does not, then you'll likely have to call a 2nd API function to get the details about the campsite. 2. Take the results of the API calls and insert them into your database. Exactly how you do steps 1 and 2 depend on the particulars of the programming language you are using to call the API and what type of database you're using.
I've always loved the ANSI_NULLS option, it's like a sign on a database that reads "ABANDON REASON ALL YE WHO ENTER HERE". 
Haha, I am currently working on a MAJOR conversion project at our company, migrating our primary systems from Sybase ASE 15.7 to SQL Server 2016. ASE Had ANSI NULLS setting flipped... so = and != worked fine. This lead to... people... using them in reports and all kinds of data applications. Among the hundreds of other remediations due to decades of improvement in T-SQL by MSSQL (and subsequent stagnation by Sybase), this ANSI NULL situation is probably the biggest one. I for one, am glad to perform the remediation work if it means proper ansi nulls, etc. Also, good riddance to Sybase.
Can you define "bombing out of the retreive"? Is there an error message? There is no limit within Db2 on the result set size that I am aware of.
I get a pop up saying error on "java heap size".
Ah. So you. Need to increase your java heap size, then. This is not an SQL issue, but an issue with your app or GUI.
I am not an admin on this server and do not believe that I will be able to get this adjusted. Anything you can think of to help streamline anything so I can maximize what I get for the space I have? Im sure without seeing the query that would be difficult. So maybe just understanding what is causing the space to fill up would be helpful. For example, is it as simple as the number of rows returned along with the number of columns and the size of the fields of the columns? Is there anything else I should consider? 
For your first one I would INNER JOIN Invoice onto Customer through the customer code instead of doing it through your WHERE. The INNER JOIN will force your query to only return results where the customer appears on the Invoice table too. &amp;#x200B; FROM CUSTOMER INNER JOIN INVOICE ON INVOICE.Cus\_code = CUSTOMER.Cus\_code
It is likely basic math. # of rows X row width, where row width is the sum of each column size. You might be able to get a sub string of some columns to reduce that - I do not know how java or your app decides how much space each thing takes up.
Well, 1126 / 8 = 140 , so if you ask me the numbers in the book are wrong.
thank you very much for your replies!
&gt;FROM CUSTOMER &gt; &gt;INNER JOIN INVOICE ON INVOICE.Cus\_code = CUSTOMER.Cus\_code Thank you so much! it worked!
Glad to hear it! You should be able to tackle the second question in a similar manner too, so good luck with it!
Have you tried a window function?
Off-topic. As a SQL learner myself, what program are you using in the screenshots?
Guess your "school" is taking a page from congress if they are putting people who don't know the subject matter in positions of authority.
It is Cengage / Mindtap &amp;#x200B; For the class we purchased the "book" (all digital) and part of it has us doing these exercises).. it's pretty nice as it lets us type in the code and it we can run a query to see the output
For the first problem, I believe your average is correct, whomever made the problem either described the problem incorrectly, or wrote a poor query. They counted the balance of customers for each invoice, that is to say, customer 10011 had his balance of 0 added to the avg 3 times. create table #customer ( cus_code int, cus_balance decimal(10,5) ) insert into #customer (cus_code, cus_balance) values ( 0, 0) insert into #customer (cus_code, cus_balance) values ( 1, 0) insert into #customer (cus_code, cus_balance) values ( 2, 345.86) insert into #customer (cus_code, cus_balance) values ( 3, 536.75) insert into #customer (cus_code, cus_balance) values ( 4, 0) insert into #customer (cus_code, cus_balance) values ( 5, 0) insert into #customer (cus_code, cus_balance) values ( 6, 221.19) insert into #customer (cus_code, cus_balance) values ( 7, 788.93) insert into #customer (cus_code, cus_balance) values ( 8, 218.55) insert into #customer (cus_code, cus_balance) values ( 9, 0) select * from #customer create table #invoice ( inv_number int IDENTITY(1,1), cus_code int) insert into #invoice (cus_code) values (4) insert into #invoice (cus_code) values (1) insert into #invoice (cus_code) values (2) insert into #invoice (cus_code) values (1) insert into #invoice (cus_code) values (8) insert into #invoice (cus_code) values (4) insert into #invoice (cus_code) values (5) insert into #invoice (cus_code) values (1) select * from #invoice select avg(cus_balance) from #invoice left join #customer on #invoice.cus_code = #customer.cus_code drop table if exists #customer drop table if exists #invoice For the second, they're asking how much money did the customer who spent the least spend. Not on 1 invoice, total. So your answer of 9.98 is the cheapest invoice, but that was made by customer 10011, who had 2 other invoices. Hope that hint helps.
So on 2nd problem (Problem 19) I shouldn't be using a Where statement? I'll play around with it this evening. You rock :)
Yeah I agree.. I've emailed my advisor a couple times now asking what the point of paying for this is when I could seriously pay $10 for a Udemy course and have better instruction, or just learn from YouTube fore free. &amp;#x200B; This "Adjunct Professor" is nothing more than there to admin the online page and nothing more. 
As u/tom_cruising suggested.. Assuming you're allowed to edit the "main" subquery and using MS SQL Server I'd do it like this (if not then you'll have to nest another query): Select *, DENSE_RANK() OVER (ORDER BY LocationTotal DESC) from (select ID, Name, Location, Object, Cost, SUM(Cost) OVER (PARTITION BY Location) AS LocationTotal, etc from tableA join j1 join j2 etc where ....) main &amp;#x200B;
Only thing I can suggest is get audit Dept interested in the issue.. but depending on industry that may not really do it. My company is healthcare so they take this shit a little seriously but being honest we do have a legacy app we're working on doing away w that has this vulnerability 
The first question doesn't say that the output needs to match their example, only that the data needs to be presented in that manner. The second question is the one which says their query should match the example. It's such a dumb question, as the example makes it look like your minimum should be 0, even though they only ask for customers who have made purchases.
You definitely don't need a WHERE clause for either of these questions. You can use your technique for the first question to work out the common link needed for the second question, then build your query around that. Joins suck. Took me a while to learn at my job, but once you do enough you'll get how they work.
It will potentially modify result of avg(), since each customer will be multiplied by number of invoices he has. If his query didn't fit answer but yours did then answer is not what task is asking for. What you have is average weighted by number of invoices and I'm not seeing anything in description asking for this.
You are amazing! this is exactly what worked!
What if you need a something equaling which no value will be returned?
Google “SQL Joins Visualized” and you Will presented with many images of how each join works.
Hi sorry I hope this isn't too late to ask. SQL noobie here - does this work in MySQL as well?
If you're unable to scrape all the data, you can put a before insert trigger on your comments table that will first check for the existence of the campsite in the campsite table. This assumes you have all the needed information about the campsite coming back when you get the comment. This way you can still have referential integrity between the campsite table and the comments table. Also, if you don't already, you may want to have a visitor table to track all the activity performed by the user. This assumes you care about the users and are able to uniquely identify them. If you do not, just the comments table will do (along with the campsite table, of course. )
This is asking you to find which tables those fields live in first. Place those in your select statement. Then find the keys of those tables. Find a pathway of matching key to matching key until you can pull all the requested fields, having each table connected. This may require tables not used in the select query. This is an AGGREGATE query, so you will need to select AGGREGATE functions like min, max, sum, or count. In your from statement, write the tables, JOIN them ON the key of table 1 = the key of table two. You will need to GROUP BY any field that did not have an aggregate function placed on it in the select statement.
Thanks, making progress, but I \*think\* I need to add an extra Aggregate on top of these 3.. such as: ROUND SUM((MIN(INV\_TOT),2)) AS "Minimum Customer Purchases", ROUND SUM((MAX(INV\_TOT),2)) AS "Largest Customer Purchases", ROUND SUM((AVG(INV\_TOT),2)) AS "Average Customer Purchases" &amp;#x200B; but not sure how to type it in the correct way. &amp;#x200B; [https://i.imgur.com/8nsNdwP.png](https://i.imgur.com/8nsNdwP.png) &amp;#x200B;
Group by item number and date and sum the sales.
This is way too complicated. It asks for a summary of customer balance characteristics so maybe I was off. It looks like maybe a subquery WHERE balance = MAX(BALANCE) OR Balance =min(balance) etc. That function is off base though. I dont need to know your homework to know they aren't asking for twice nested aggregate functions. Let me look into it further. 
&gt; Why does this produce ord attributes that are less than 30 you are pulling directly from the table "casting" so you'll see whatever that table has in its records. &gt; can i not compare a integer with count? it doesnt produce an error, so you obviously can &amp;nbsp; this is a quote from an old(er) comment of mine, try to see if this helps you organize your thought process for building queries: mildly theoretical stuff below, you can skip to "answer here" IMO, granularity - to what level of some hierarchy or some dimensions your data is attached to - is one of the better "soft" concepts to grasp when you're just learning SQL. This is somewhat different from uniqueness, as normally you wouldn't consider measurements a part of your granularity. Granularity is also somewhat different from uniqueness because it goes by distinct values (i.e. granularity allows NULLs, technically). Granularity is different from a key also while it tells you that you cannot distinguish records beyond certain level, it does not explicitly guarantee that the records do not have duplicate sets of grain columns values (you'd need a constraint - a key or uniqueness to enforce that). "GROUP BY" (with a list of columns/expressions) is generally, a way to explicitly set granularity of the output to those columns/expressions. Since there could have been multiple records in your source for a single "value" (set of values) of your new granularity, you need to provide a way to aggregate ("collapse") those record values down to your granularity. This is done via aggregate functions - you probably can tie these two (aggregate functions and group by) and realize that aggregate functions will work across all records with the same output granularity. GROUP BY has a quirk where it is not specified when you are finding a grand total, i.e collapsing your data set down to a single record. SQL detects it by seeing only aggregate functions in the output list.
The sales are actually already total for the day. It’s the promotions that I need in one row which are all different.
Oooooh. I’m on mobile and only saw the first three columns... yeah ignore me.
This is the first time I’ve seen it on mobile so that makes sense haha. Thanks anyway!
Ugh I’m sorry dingle__berries, that’s awful. This is really the reason I am hesitant to fork over the money to earn a higher degree, even knowing that would likely accelerate my career path and pay off. 
In sql, null is undefined. Two undefined values are not equal because neither can be defined, hence the comparison can never be equal. 
Looking at your first query, this is HOW you solve the problem instead of solving it for you. Clearly the average is including more data than expected or wrong data, so change &amp;#x200B; SELECT MIN(X), MAX(X), (AVG(X) FROM ..... &amp;#x200B; to &amp;#x200B; SELECT \* FROM ....... &amp;#x200B; This will show you all the rows getting included in the average. Now you will probably notice some rows are being included more than one because some customers have more than one invoice. So you need to filter out the duplicate customer invoices or make sure you only get 1 row for each customer to include in the average. &amp;#x200B; This should help you if you get a similar problem - just look at the raw data to see why the aggregate data comes out wrong &amp;#x200B;
I happen to work for a document management company and we use SQL Server as a backend. I know for a fact that we have a "isCheckedOut" bit column that gets set to 1 when someone is editing. I've never ran into an issue with it, but there will be more knowledge people who can give you a better answer.
You should be able to do that with pivot.
How do you define most recent tier id? By looking at the record with the max date in the table? If so, you might be able to do something like this: Select TOP 1 * FROM [Table] WHERE TierId IN (SELECT TierId From [Table] WHERE EffectiveDate = MAX(EffectiveDate)) Order by EffectiveDate You’ll have to watch out for duplicates though. Also sorry about the formatting I’m on a mobile. 
I just use the built-in sql client in JetBrains PyCharm IDE. I'm not sure if it is the free version. I've also used Squirre SQL and DBeaver. Both pretty good. There is also Microsoft Azure Data Studio, but I don't think they have MySQL driver yet.
The most recent tierid is defined as the one with the max date, however I need to know when the teirId value first changed to the most recent value. I did come up with a solution here http://sqlfiddle.com/#!18/681d3/15
Are squirrel n dbeaver free? Thanks
[MySQL Workbench](https://dev.mysql.com/downloads/workbench/) is the standard choice for MySQL I believe
Dbeaver is free, imo it's the best
Oh so there are different software for diff types of SQL? How does MSSQL differ then from MySQL?
Thanks I'll get it a go
You have the duplicate column name listed in your SELECT statement. 
ok im super sorry i just stared using this like 2 hours ago im brand new im sorry for not know what that is can you elaborate more its ok if you dont want to i understand 
MSSQL is Microsoft's flavour of SQL, usually referred to as SQL Server. Microsoft make SSMS which is their client. MySQL is Oracle's flavour of SQL and they also make MySQL Workbench as a client.
If pricing is not a problem, I feel Navicat is the one I like to work with. 
USE \`essentialmode\`; &amp;#x200B; ALTER TABLE \`users\` ADD COLUMN \`Name\` VARCHAR(50) NULL DEFAULT '' AFTER \`money\`, ADD COLUMN \`skin\` LONGTEXT NULL AFTER \`name\`, ADD COLUMN \`job\` varchar(50) NULL DEFAULT 'unemployed' AFTER \`skin\`, ADD COLUMN \`job\_grade\` INT NULL DEFAULT 0 AFTER \`job\`, ADD COLUMN \`loadout\` LONGTEXT NULL AFTER \`job\_grade\`, ADD COLUMN \`position\` VARCHAR(36) NULL AFTER \`loadout\` ; &amp;#x200B; CREATE TABLE \`items\` ( \`name\` varchar(50) NOT NULL, \`label\` varchar(50) NOT NULL, \`limit\` int(11) NOT NULL DEFAULT '-1', \`rare\` int(11) NOT NULL DEFAULT '0', \`can\_remove\` int(11) NOT NULL DEFAULT '1', &amp;#x200B; PRIMARY KEY (\`name\`) ); &amp;#x200B; CREATE TABLE \`job\_grades\` ( \`id\` int(11) NOT NULL AUTO\_INCREMENT, \`job\_name\` varchar(50) DEFAULT NULL, \`grade\` int(11) NOT NULL, \`name\` varchar(50) NOT NULL, \`label\` varchar(50) NOT NULL, \`salary\` int(11) NOT NULL, \`skin\_male\` longtext NOT NULL, \`skin\_female\` longtext NOT NULL, &amp;#x200B; PRIMARY KEY (\`id\`) ); &amp;#x200B; INSERT INTO \`job\_grades\` VALUES (1,'unemployed',0,'unemployed','Unemployed',200,'{}','{}'); &amp;#x200B; CREATE TABLE \`jobs\` ( \`name\` varchar(50) NOT NULL, \`label\` varchar(50) DEFAULT NULL, &amp;#x200B; PRIMARY KEY (\`name\`) ); &amp;#x200B; INSERT INTO \`jobs\` VALUES ('unemployed','Unemployed'); &amp;#x200B; CREATE TABLE \`user\_accounts\` ( \`id\` int(11) NOT NULL AUTO\_INCREMENT, \`identifier\` varchar(22) NOT NULL, \`name\` varchar(50) NOT NULL, \`money\` double NOT NULL DEFAULT '0', &amp;#x200B; PRIMARY KEY (\`id\`) ); &amp;#x200B; CREATE TABLE \`user\_inventory\` ( \`id\` int(11) NOT NULL AUTO\_INCREMENT, \`identifier\` varchar(22) NOT NULL, \`item\` varchar(50) NOT NULL, \`count\` int(11) NOT NULL, &amp;#x200B; PRIMARY KEY (\`id\`) ); &amp;#x200B;
Which of the scripts is failing?
i changed the name to names and stuff the is says the same thing for job the skin idk what is goign wrong ive have been at this for a long time now and getting no where 
Which of the scripts is failing?
This is it i try and run this thing and it says this error "SQL Error (1060): Duplicate column name 'name'" &amp;#x200B;
i click the blue run arrow and it gives me this &amp;#x200B;
That's not the script, that's the error. Which one of the create tables or alter tables is failing?
Run them one at a time and see which one is the problem. 
ok super sorry i dont want to waste your time but how do i do that ?
Just put them in the window one at a time and do what you've already been doing?
I prefer using HeidiSQL Free, open source and can connect to most SQL dialects too
[my screen](https://imgur.com/a/f8apI86)
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/AeuXSSs.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
supper sorry again for not understating i just been here for too long literary have no idea what to do 
No problem if you're paying :-)
Rather than a 1 or 0,why not the UserID of the user locking it out? Unless that ID is 0, it would function essentially the same and give you the opportunity to see who has it checked out. 
I did not design the database and the people in charge refuse to make any changes to database, so we are stuck with this.
What database system is it?
You could use a case statement to put it in the right column but you’ll end up with lots of blanks
Check the primary key lines. You may have to specify what table you're making them on? (I use Oracle though so my syntax might be different)
Wasn't trying to knock your DB at all, but giving OP a suggestion to build on the one you deal with.
Rbdms matters but most likely change target table's fields from varchar(255) to varchar(max) (or if you know the max length you can use that)
MS sql server 17
Would using varchar(max) be different than nvarchar(max)? I'm not too into dba, just querying
 Would using varchar(max) be different than nvarchar(max)? I'm not too into dba, just querying 
Where are you getting the error? At what step of the import?
I'm not getting a specific error. The data is importing perfectly fine, but any instance of a field with &gt; 255 characters is being truncated to 255.
Oh, gotcha. 
Sort brand? 
Woah, you just connected a lot of dots. Thank you.
For length? NVARCHAR holds max 1 billion characters (2 bytes per character) as it gives support for Unicode. VARCHAR(MAX) can hold 2 billion at 1 byte per character (non-unicode).
I teach SQL, and we use MySQL Workbench for SQL, Database management, modeling, and design. It's free. &amp;#x200B; At my actual job, we use various tools depending on the database. * SQL management studio, or Azure Data Studio for SQL Server * IBM Data Studio for DB2 * SAS for all of them, and * Toad Data Point for All of them. I spend most of my time in Toad, as it easily connects to all my sources at once. My data scientists I manage do most of their coding in SAS. &amp;#x200B;
Ok so I was overthinking this whole thing. It turns out the SSIS importer scans the first few rows for field length. In my first dataset (I need to import multiple datasets over the coming days) only one of the 8 possible fields that could contain text, was over 255 characters. So I created a dummy table. In the 8 possible freetext fields I copied the long string of text and put dummy data in the rest of the fields. I imported that dummy table with the one record. SSIS scanned the only row and saw all the freetext fields as LongText so set the destination fields correct. Just hoping this works across the board now.
There’re many open source rdbms out there, you should be able to set smth up at no cost. I started out learning SQL for my job applications by installing MySQL and MySQL workbench in my laptop, then got some datasets from Kaggle and uploaded them inside to create my own practice db to play with. It was completely free! :) 
Uhh unless I'm wrong, MySQL is an open source sql language and has no affiliation with Oracle. Oracle has its own flavor of sql with the default tool being Oracle SQL Developer. TOAD is another popular tool for oracle but that requires a license. 
This would work using your schema. The subquery returns the tierid with the newest date. The main query finds the oldest date related to that tierid. Do you ever get duplicate dates?: `SELECT TOP 1 *` `FROM dbo.PPY` `WHERE tierid =` `(SELECT TOP 1 tierid` `FROM dbo.PPY` `ORDER BY effectivedate DESC)` `ORDER BY effectivedate`
&gt; Microsoft make SSMS which is their client. And through my limited experience with it it's awesome. Some of the best microsoft software I've used.
and this works if you have duplicate dates and you want to return multiple tierid's: &amp;#x200B; `WITH ranking AS` `(SELECT *, RANK() OVER (ORDER BY effectivedate desc) as rank` `FROM dbo.PPY)` &amp;#x200B; `SELECT tierid, MIN(effectivedate)` `FROM dbo.PPY` `WHERE tierid IN (SELECT tierid FROM ranking where rank = 1)` `GROUP BY tierid`
I'm not familiar with SQLite but 6926 sounds like an appropriate number for "hour of year". There's 8760 hours in a (non-leap) year, so it sounds like its returning how many hours have elapsed in the current year?
Look up pessimistic concurrency control
oh... apologees for the wording then... post prob. should've stated that my goal is to \_extract\_ 6926 from my datetime column (i.e. it was meant to provide an example of what I wanted). Sorry 
What do you mean you keep failing? Getting an error or failing to get the logic? You essentially just want to get the diff in hours between 2018-01-01 and 2018-10-16 13:00:00. In tsql it would be select datediff(HH,2018-01-01,2018-10-16 13:00) Am not sure on the sql lite syntax sorry 
 SELECT TOP(1) x.tierId , x.effectiveDate FROM (SELECT t.tierId AS NextTier , LAG(t.tierId) OVER (ORDER BY t.effectiveDate DESC) AS tierId , LAG(t.effectiveDate) OVER (ORDER BY t.effectiveDate DESC) AS effectiveDate FROM dbo.Tier AS t) AS x WHERE x.tierId &lt;&gt; x.NextTier;
(failling means: not finding a nice solution to my problem) as for `datediff()`, sqlite unf. is a bit special, tho it does provide a [few](https://www.sqlite.org/lang_datefunc.html) datetime features.
Ah I see your limitations 😔 from a quick look on that link you would need to do something with JulianDate between your two dates *24 then add the number of hours on in your 'to' date, so 13 in your example
Ok based on quickly reading the documentation and combining u/Animalmagic81's suggestion I would use: SELECT strftime('%s','2018-01-01 13:00:00') - strftime('%s','2018-01-01 00:00:00') This will give you the number of seconds into 2018 the date is, which you can then just convert to number of hours ( result / 3600).
You could enter that into a temp table and then Concatenate the columns that appear on the same day. You would have to do an inline select on the columns you wanted to concat and join that table back on itself while cycling though the promotions by each day.
Definitely. SSMS is absolutely amazing
intellesense speeds up my work so much it's crazy. That's the killer feature that I haven't found in any other DB software.
I was only 12 in 2008 so I've only know MySQL to be an Oracle thing! Feeling very baby face right now
look into julianday() function
I like it , and I learned a new SQL command (LAG) and it works with my original example table. http://sqlfiddle.com/#!18/681d3/49 however it doesn't work if tierid never changes. http://sqlfiddle.com/#!18/b54cad/2 I think you got me a good start though-Thanks!
Fixed. Now you can learn LEAD as well! http://sqlfiddle.com/#!18/b54cad/3
People actually do this!
f.yeah - i'm pleased to announce that julianday()\* does the trick (although not super pretty) `select round(24*(julianday(tbl.dt_utc) - julianday(strftime('%Y',tbl.dt_utc)||"-01-01")))+1 from tbl` thanks (all!) for helpful input! ^(\*always thought it was some sort of weird eastereggfeature) 
&gt;julianday(strftime('%Y',tbl.dt\_utc)||" &amp;#x200B; Remember to add on your hours as well from the later date otherwise you will be 13 hours short (in the example)
Thanks! https://i.imgur.com/eJS3J5R.gifv
Why not just change it to %h instead of %s? Does that not work?
I have a unix timestamp fetish?
I may just not be understanding but I need to keep all the cells separate. Is there another use for CONCAT that I haven't seen?
You need to keep all the cells separate you could do a count to get the max number of times a day/sales repeats and create columns based on that count through dynamic sql.
Sqlite `strftime()` doesn't have a %h format.
Yes, it's web scale.
Correct. I used nvarchar because thats usually what these programs default to, but I prefer varchar for most projects if it's going to be used for more than a day or two
You're looking for [dense rank](https://docs.microsoft.com/en-us/sql/t-sql/functions/dense-rank-transact-sql?view=sql-server-2017)
We built a web app to handle this at my last job.
Because that gives me the minimum start date for the entire ProviderNumber. I want the minimum date within the provider number partition that is continuous, i.e. running your code on my sample data would yield 10/8 when I want 10/28 for the last 3 ProviderNumber 200 lines and 10/8 for the first one. &amp;#x200B; I've tried DENSE\_RANK but it still doesn't address the issue of limiting the partition to continuous date ranges.
And here is a link to a google docs with just the relevant sentences of my resume, for easier reading/editing. https://docs.google.com/document/d/1FRwzZMg97xITJY7GNIZq3Njx9vRu0hrlPuPOIUVUYXY/edit?usp=sharing
The ALTER and UPDATE statements probably have to run in separate batches. Try adding GO (or a semicolon if you're using MySQL) before UPDATE.
Agreed, was just testing this theory myself and when the statements are executed individually they work. I actually couldn't get the semicolon to work, it would only work with a GO (I'm on 2017 *shrug*)
This can be done with dynamic SQL. Someone asked a similar question on StackOverflow. [This answer should give you a good idea what to do.](https://stackoverflow.com/a/49675233)
&gt;I guess the solution is to use something like dynamic SQL, which I've been told to avoid Very few things in SQL are binary (except non-`NULL` `bit` fields). Why were you told to "avoid" dynamic? This would be an appropriate application. &gt;My issue is that these combined queries end up being large (up to a couple thousand lines) Perhaps you can refactor the queries to be smaller. "A couple thousand lines" of SQL is difficult to work with and by refactoring into a set of smaller queries, you may even improve performance _while_ making it more maintainable.
Option 1 is better because you have less rows, some of which would be completely useless and you don't have to specify whether you want Yes or No's in your join or in your where clause because their prensence in the table implies they are part of it while their absence implies they are not
&gt; while their absence implies they are not this absence of something should be represented in a database as absence of data hence why some people normalize beyond 3NF and BCNF to get rid of NULLs completely you've heard of optional relationships? where the FK is allowed to be NULL? weak sauce 
&gt;OMG THANK YOU SO MUCH. I totally learned something new today!!!
Thanks you guys, option 1 it is.
I don't have any experience with MySQL, but from my experience with DB2 SQL and T-SQL, I'm thinking that those declares should be inside of the begin. Also, from looking at the some documentation, it looks like you may want to be using set when you are setting your variable values.
Thank you! I got it working and now am even using it for other fields. Really appreciate it!
I am going to piggy back off of this thread because you all seem very familiar with claims data language. I work with claims data regularly, but used to use Cognos for reporting and analysis, but have to use SQL now and am mostly self-taught. &amp;#x200B; I am trying, but can't figure out for the life of me, how get the results I'm looking for. I need to identify claims that have specific criteria, i.e., billed with a specific revenue code, but pull in all lines related to those claims, even if they don't have that revenue code. I understand how to identify the claim numbers, and specific lines that have that rev code, I.e. select claim\_number, line\_number, first\_DOS, last\_DOS, rev\_code, proc\_code, allowed, paid, copay, provider\_name from claim\_table where rev\_code in ('0761') and first\_DOS &gt;= '2019-01-01' I am thinking I'd need to do a join, because a separate select statement with a union won't tie it back to just the claim numbers identified above, but ANY claim without 0761, but I can't figure it out. Any thoughts? Would appreciate any help. &amp;#x200B; &amp;#x200B;
&gt; Why were you told to "avoid" dynamic SQL? This would be an appropriate application. Like many things in SQL, if you ask 10 people you'll get 12 opinions. It's probably one of those things -- like cursors -- that are **generally** to be avoided if practical... but I agree with you that this sounds like a good place to use it. And there's an added benefit in that it's "future proof" (somewhat) in that if new plants/tables are added (and the naming convention is followed), the query will automagically include them.
What's worse is adding a group you would need to update every single user.
A view is a stored query definition and doesn't store any rows. At best, a view will run close to the same as a query and at worst it'll run much longer. It sounds like you have some performance tuning on the horizon of your query is taking upwards of thirty minutes. :)
I mean, the *most* correct solution is keep data that represents the same type of entities in one table. If you have performance issues doing that and need to group associated data together, you can use clustering, partitioning and file groups. In other words, instead of having a bunch of tables where each table represents a different plant, you should have one table with a column that indicates which plant the record is for. This column should be a part of the table's primary key or a unique index on the table, and it should be used in the clustering key if the majority or common data queries only care about one plant. If you routinely need to use UNIONs, it's a sign that this is a problem. Of course, this solution requires a massive systems refactoring because the core design is flawed. I'm willing to bet that's not a feasible solution, or, at least, not a feasible short term solution. So, let's assume that the above isn't possible. A VIEW isn't going to help you much if you're on SQL Server, because there aren't any materialized views and indexed views are essentially a non-feature (too many features, like UNION and aggregate functions, can't be used in them). First, make sure your queries are using UNION ALL unless you *know* that you need to eliminate duplicates. A simple thing, but a lot of people miss it. I'm guessing you're already doing that. The second alternative is to use temp tables. and lots of smaller queries. This is essentially how systems used to work, and they commonly involved stored procedures. You *can* use dynamic SQL to do populate the tables, but you don't have to. Create a temp table, and then query each table and dump the output into the temp table. You can either capture the raw records, or you can capture the aggregates, but the more data you capture the longer it will take. This means you don't have to union anything, but if you're trying to analyze the data across the entire population of every plant, that won't work. Obviously, this method will increase temp table usage, so you will need to monitor that. Third, if this is all historical information that does not change, can you do this processing with a data warehouse and a CUBE of some kind? This way you've got your data already done and your reporting can just pull from the data warehouse. It does appear that there are [ways to meaningfully aggregate standard deviations into a summary](https://math.stackexchange.com/questions/1547141/aggregating-standard-deviation-to-a-summary-point), but I will admit that that math is beyond me. I believe [pooled variance](https://en.wikipedia.org/wiki/Pooled_variance) help with this as well, but that math is again not my wheelhouse. 
**Pooled variance** In statistics, pooled variance (also known as combined, composite, or overall variance) is a method for estimating variance of several different populations when the mean of each population may be different, but one may assume that the variance of each population is the same. The numerical estimate resulting from the use of this method is also called the pooled variance. Under the assumption of equal population variances, the pooled sample variance provides a higher precision estimate of variance than the individual sample variances. This higher precision can lead to increased statistical power when used in statistical tests that compare the populations, such as the t-test. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/SQL/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
You are altering 'users', are you sure name doesn't already exist within the table? :/
Yea sorry I’m still new I started last night 
The queries produce tables with over 10 million rows, paying reference to several tables of over 500 million rows. The fact that they take over 30 min to run comes at no surprise. Your comment confirms my suspicion that views don't outperform the SQL query itself.
No worries at all m8. We all had a beginning. :)
They definitely don't, but, they have their place. Are you batching the queries approximately? I'd love to see the execution plan for the entire process. :P
If you put all your servers in a group in management studio, right click and do new query at the group level. It will run on all servers in group and the first column returned is the sever name. 
Is it a small web app?
\^ strong this. Github is your friend for not only sharing your code to other developers, but also showcasing code to potential employers. 
Mirroring can fail and it’s not fun having to answer why transactions do not exist in the database. 
You would back up and restore the tail log last with recovery after all prior transactions logs are restored (with no recovery) which should have any data prior to the outage. 
You could also use a UNION with each portion being a specific brand = 'brand', but, why are you wanting to display the data this way? 
Can't send you the actual queries, but I'm making a table consulting another table linking individuals and their address to the nearest geospatial grid (1km, 10km, 12km). It then links their geo-grid to environmental values (PM25, NO2, and Ozone) measured every day in 6 tables with values taken from 2000 to 2010. Then, it finds the average yearly environmental value on each hospital visit date. &amp;#x200B; I'm not batching the process, since I prefer creating tables for each environmental tables and joining them based on the individual, address, and visit date. 
 ;WITH changeDetection AS ( SELECT * , IIF(ProviderNumber != LAG(ProviderNumber, 1, 1) OVER (ORDER BY StartDate), 1, 0) AS hasChanged FROM (VALUES (100, CONVERT(DATE, '10/1/2018' ), CONVERT(DATE, '10/5/2018' ), 1), (200, CONVERT(DATE, '10/8/2018 '), CONVERT(DATE, '10/15/2018'), 2), (300, CONVERT(DATE, '10/20/2018'), CONVERT(DATE, '10/25/2018'), 3), (200, CONVERT(DATE, '10/28/2018'), CONVERT(DATE, '10/31/2018'), 4), (200, CONVERT(DATE, '11/1/2018' ), CONVERT(DATE, '11/30/2018'), 4), (200, CONVERT(DATE, '12/1/2018' ), CONVERT(DATE, '12/5/2018' ), 4), (400, CONVERT(DATE, '12/10/2018'), CONVERT(DATE, '12/25/2018'), 5) ) x(ProviderNumber ,StartDate ,EndDate ,GoalOutput) ) SELECT * , SUM(hasChanged) OVER (ORDER BY StartDate) AS Output FROM changeDetection `LAG` on ProviderNumber used to detect changes and then just `SUM` the changes up until current record.
To expand on the refactor part; Separating the data consolidation efforts from the aggregation efforts might help maintainability in general but allow you to reduce or eliminate dynamic SQL as well. Load all plant specific detail data into a consolidated table, adding a Plant (or source) field to the PK. Then run the aggregation process against all of the data into the report data (probably grouping by the aforementioned Plant field and maybe a time column from the looks of the output). To load the tables, you could definitely use dynamic SQL, but SSIS, python, or other ETL/data pipeline tools can usually perform looped operations a little more elegantly than dynamic SQL. Might even be doable in a SQL view (UNION ALL the tables together), if all of these tables are on the same server. But after everything is consolidated, you can write regular SQL. &amp;#x200B; This is more of a data warehousing approach than a reporting approach though. You end up segmenting out your data movement/consolidation efforts away from business logic for cleansing and away from aggregation logic. From a design standpoint, it allows for future consumption of that consolidated data in an easy manner. For example, say you get a request to do monthly aggregations, instead of hourly; You've already got the cleaned and consolidated plant data and need to aggregate it up, rather than going all the way back to the source for each table. From a technical standpoint, you can separate the consolidation, cleansing, and aggregation processes, hopefully letting you only have to deal with this issue in the first stage, leading to less of that dynamic sql code.
For certain types of aggregation queries you can setup an 'indexed view' where the aggregates are actually stored in the index of the view. I use this for one report that aggregates from approximately 30 million records. The report pulls up in milliseconds. &amp;#x200B; [http://www.sqlbadpractices.com/speeding-up-aggregates-with-indexed-views/](http://www.sqlbadpractices.com/speeding-up-aggregates-with-indexed-views/) 
Actually it's just one part of a larger web app. The ingest portion has an "ingest engine" behind it that does column matching to available tables to help the user identify the right one and automates quality/data integrity checking 
The excel driver used by ssis detects the data type based on the first 8 rows. Sounds like it's an issue with the driver. You can change this in the registry. Change it to 0 and the full file will be scanned before determining type I had an issue with string data types being imported as nulls because the first 177 rows were numbers, no errors were reported either! 
Here's an option, without using window functions however. The basic premise is to query the table, gab everything chronologically before each entry that has a matching provider number and GoalOutput, then remove any of the ones that have have any non-matching GoalProviders between itself and the original anchor row (to account for a sequence for the same provider that may have changed away from a value and back to that same value at a later date, can remove that bit if the data won't ever appear that way). &amp;#x200B; WITH sample\_data AS ( --Mock your data select 100 as ProviderNumber, CONVERT(DATE, '10/1/2018' ) as StartDate, CONVERT(DATE, '10/5/2018' ) as Enddate, 1 as GoalOutput UNION ALL select 200, CONVERT(DATE, '10/8/2018 '), CONVERT(DATE, '10/15/2018'), 2 UNION ALL select 300, CONVERT(DATE, '10/20/2018'), CONVERT(DATE, '10/25/2018'), 3 UNION ALL select 200, CONVERT(DATE, '10/28/2018'), CONVERT(DATE, '10/31/2018'), 4 UNION ALL select 200, CONVERT(DATE, '11/1/2018' ), CONVERT(DATE, '11/30/2018'), 4 UNION ALL select 200, CONVERT(DATE, '12/1/2018' ), CONVERT(DATE, '12/5/2018' ), 4 UNION ALL select 400, CONVERT(DATE, '12/10/2018'), CONVERT(DATE, '12/25/2018'), 5 ) select ProviderNumber, Series\_StartDate, Series\_EndDate, GoalOutput from sample\_data sd1 outer apply ( -- Join up all rows of the same provider and goaloutput before sd1 chronologically seleCt isnull(min(sd2.startdate), sd1.StartDate) as Series\_StartDate from sample\_data sd2 where sd1.ProviderNumber = sd2.providernumber AND sd2.Enddate &lt; sd1.StartDate AND sd1.GoalOutput = sd2.GoalOutput AND 0 = (SELEcT isnull(count(\*),0) from sample\_data sd3 --Remove any entries that have a non matchin GoalOutput between sd1 and sd2's timeframe where sd3.ProviderNumber = sd1.ProviderNumber and sd3.Enddate &lt; sd1.startdate and sd3.StartDate &gt; sd2.Enddate AND sd3.GoalOutput &lt;&gt; sd1.GoalOutput) ) a outer apply ( -- Join up all rows of the same provider and goaloutput After sd1 chronologically seleCt isnull(max(sd2.enddate), sd1.Enddate) as Series\_EndDate from sample\_data sd2 where sd1.ProviderNumber = sd2.providernumber AND sd2.StartDate &gt; sd1.Enddate and sd1.GoalOutput = sd2.GoalOutput AND 0 = (SELEcT isnull(count(\*),0) from sample\_data sd3 --Remove any entries that have a non matchin GoalOutput between sd1 and sd2's timeframe where sd3.ProviderNumber = sd1.ProviderNumber and sd3.Enddate &gt; sd1.startdate and sd3.StartDate &lt; sd2.Enddate AND sd3.GoalOutput &lt;&gt; sd1.GoalOutput) ) b group by ProviderNumber, Series\_STartDate, Series\_EndDate, GoalOutput 
Using a template like jinjasql can help maintain repetitive queries. If you find yourself stuck maintaining it as is, you can still benefit from using dynamic sql to create a declarative version and string comparison to evaluate changes as part of your personal workflow before handing the query over to the powers that be. There are a lot of tools you can use to help formalize your workflow and help you achieve a higher level of professionalism. Selecting the correct ones depends on your environment and company policy. &amp;#x200B;
This is definitely some cool code but unfortunately (unless I'm reading it incorrectly) this requires that I have the "Goal Output" field already populated. The problem is figuring out how to populate that. Once I have that field it's pretty simple with a window function, although your solution is a cool alternative. Thanks for taking the time to reply.
I think this is it --digging in now to confirm and evaluate performance, but this is a really cool solution. Thanks!
Its really hard to understand all your data across six screenshots. But I'll take a shot: 1NF- Items are atomic and hold one value. Your PV Name in #2 is not atomic (break out to First and Last) 2NF- This is where your ERD will come in. Think of your fields as topics or "attribute". From looking at your data, I see: Encounter Patient Provider Specialty Careplan Condition Organization Diagnosis Medication Each of these attributes can exist independent of one another and can be separated. You can create surrogate keys to relate the attributes together as foreign keys. Example ENCOUNTER encounter_id (PK) START STOP patient_id (FK) 1 11/17/2012 11/20/2012 1 PATIENT patient_id (PK) PatientLName PatientFName PatientDOB 1 Cartwright Alyce 11/15/45 I hope that can help get you started. Its been a while since school and haven't had to think about normalization too much, so it could be off. Sorry I couldn't be more helpful with transitive and partial dependencies.
A few (3-5) layers deep of sub queries within each 5-10 main joins rolling up to the main query — if that makes sense— probably the most advanced I do. In other words, could be as many as 50 queries... in one. 
Son of a bitch, I forgot about making data atomic to fall into 1NF. Thank you for catching that. After that you lost me a wee bit. I understand the attributes are each column. I'm newer to the terminology so forgive me but Surrogate key? Also, I'm not supposed to make an ERD of 2NF, just 3NF?
It depends. A lot. Lines of code is a BS measure for any programming, and SQL is no exception. People try to get cute and clever with their queries, try to cram *everything* into a single query, and then...it runs like garbage. SQL is a funny language that way. You try to shove everything into a single query and the engine gets confused &amp; generates a terrible execution plan. Often, you can get *better* performance by writing more code because breaking it down into smaller chunks, the engine can do a better job of optimizing the execution. IOW, "complex" code (however you might define that) can be a detriment. Optimize for readability &amp; understanding, and the system will *usually* do a good job running it. I don't get hung up on LOC - my goals are to get the correct query results without killing the server. If that means writing *more* code so it can be more readily optimized, so be it.
My analysts don't know much more than simply SELECT * FROM X WHERE dates BETWEEN '2019/01/01' AND '2019/02/01' No subqueries, no complicated joins or CASE statements. Of course, before I joined the team, their idea of data mining was a pivot table in Excel. I have to keep my tempdb managed agressively.
I’ve been writing a bunch of rules for a rules engine set up in Oracle. Some of the queries can look really ugly. Lots of transformations and calculations happening. Some queries go to 25 lines or so. 
A small query will involve 3-4 tables. An average query will contain 10-15 tables. A large query might contain 25 or more tables, have a series of CTEs, union alls, case whens doing one-off lookups, subqueries going 3-4 levels deep, maybe some partitioning, labeling and ranking, etc. A lot of the complex queries reach out to other databases through db links. It's hard to quantify in terms of "lines" because it can be organized however you want. Typically a quarter of a page for a small query assuming 10 point font, 2-3 pages for a medium query, and for big queries it might be a few hundred lines over multiple pages. In practice, it's common to have a stream of sql statements, pl/sql blocks, calls to the host, sanity checks, rollbacks, commits, and more, all executed with one scheduled script. The sky is the limit on that sort of thing. I've got one script that is several thousand lines that does the equivalent of hundreds of man hours if it were to be done manually, developed over a 15 year period. It's a mess. My main database has over 4000 tables. 
As others have said it's tough to quantify. Today, I made 2 views that were simply an inner join between 2 tables. One of the tables had over 100 columns in it (not my design) because of that, the view was 150 lines of code. Each script in source control has about 20-30 lines of code devoted to documentation. I recently worked on a stored procedure to populate a mart with summarized data. It took dissimilair data and combined it to one grain. That took me a few days to create. I think that was about 700 lines. How many tables are you joining would be a better question. The stored procedure pulled data from around 10 tables.
I had an etl script at my last job that was probably around 10,000 lines. It collected data from a few sources, did some calculations, and loaded to a data mart.
It's easy! Use the try it yourself sections here on this website and you'll have a usuable SQL query in minutes, and good enough knowledge for a job on 45: https://www.w3schools.com/sql/sql_select.asp
Generally they should be as long as they need to be to do what they are supposed to in the fewest statements and fewest read writes. If you have a table with 30 columns the fastest way to write an insert statement is to Insert into table2 Select * from table1 I mean that's two lines as opposed to declaring your column order in your insert clause and not specifying the order as well in your select statement. Is that better? No it may be two lines but it's suspect to ordinal errors if anything changes. What I find with people who write SQL poorly is they: 1) Don't quantify the time it takes to execute to the actual resources used. No table with 25mb of data should ever take 1.5 minutes to return 10 rows. Unless you are intentionally making a Cartesian with a recursive cte to test all options before filtering down to a subset, your SQL is bad. If that is your intent, you are probably using the wrong language. 2) People who write high level SQL still don't understand the basics like how to build a table using best practices. How indexes work and the difference between clustered and non clustered indexes. 3) People don't know how to read execution plans. These aren't just for DBAs. 4) People forget they can put stored procedures in Storedp Procedures. We have a block of code at work that basically sets up a couple variables for UTC to client conversion. It's written differently in 5 different Procedures. It should be its own and be called at the beginning of SPs when needed.
[This is fairly typical.](https://pastebin.com/UiLS7VQM)
(MSSQL Person here) Like many of the others in this thread, I can't reiterate enough that code &lt;&gt; performance. I always try to optimize for performance. If it's a stored procedure where I can use things like table variables, temp tables with indexes, table scaler functions, etc., my stored procedures can get very long - but they run fast. If I have to make a view to support third party software that can't (or shouldn't) run stored procedures, I spend a lot of time on indexing, how the CTE's and subqueries are joined, etc. I've had queries go from running in hours to minutes or seconds just by using a proper subquery. For the data analyst part - if you intend to use your SQL skills in it, get very, very used to having multiple queries up, putting subsets into temp tables for performance issues, indexing those tables, writing functions to reduce having to type out long nested functions on a regular basis, but most importantly... Learn your data. It doesn't matter what subject it is, or how interested you are in it, learn everything you can about it. Learn to trust your instincts when data seems "off". If your nose says that the results just can't be right, they probably aren't. Start digging into the data. I generally start working forward from the input(s), or working back from the results. Unless I have a strong hunch about something breaking in the middle, it's generally a waste of time.
In many cases that I've come across multiple smaller queries run in series works faster and with less resource usage than a single query doing everything. There have even been queries I've optimised by making temporary tables to store the results from inner joins to then query later and drop. Even that runs faster than the single query that joins all tables necessary for my needed output.
In terms of # of lines (which is somewhat arbitrary, given white space): Lots are like 10 lines. Maybe the average query is more like 50. The longest one I ever wrote was something like 1,400 lines. Again, that includes whitespace. 
In 45 minutes?
I'd say so. SELECT * FROM TABLE1 WHERE TOOMUCHDATA &lt;&gt; 1 AND DATE = CURRENT_DATE Bam, BI job
See you in again in 6 months.
A third table; id, taskId, catId Would have 6 records in your example.
A very small query will often be lower-performing A medium-length query that performs operations such as breaking out subsets into #temp tables or CTEs is often times the sweet spot, and performs better. Long queries are usually brutal and don't perform well.
This environment is very much the exception. Based on surveys, the average corporate database has 300 tables, and the average corporate data warehouse has 12 tables. I've worked with dozens of cortisone, and this is consistent with my experience as well.
You want a relation table (many to many) [https://www.databaseprimer.com/pages/relationship\_xtox/](https://www.databaseprimer.com/pages/relationship_xtox/)
Depends on the flavour u using, im postgres for example you can create an array an store it
u/Taiwaly already gave the link I typically hand out. If you can't get access to real live production data, look up the Northwind/Chinook databases and play around with those using the exercises there as a guide. It's relatively easy to pick up the basic skills (though saying you'll be job-ready in 45 minutes isn't a reasonable expectation to set) and learning on your own is usually sufficient, especially if you already have some kind of degree behind you.
In my case I used an array formula to ensure the cells with greatest length are pushed to the top of the dataset before import. This is a messy but useable fix until I can conceive my IT department to let me into the registry to change the row scan value
One note, on Oracle, using * to select columns like this has a performance hit and is considered a bad practice. 
As in so many things, it depends. I have a sql query (not a stored procedure) of about 5000 lines that runs in less than 500ms. So longer may or may not perform well, it depends on so many factors. 
Given the circumstances, i.e inserting data into a table, I think the performance hit (if there even is one) is negligible. The reading and writing of data would take order of magnitudes longer than resolving the * into column names. 
I've noticed that CTEs in MSSQL is pretty slow. Do you have any tips on how to avoid the biggest pitfalls? In Oracle they seems to perform way better. It's mostly a feeling I have so I can't back it up with numbers. 
&gt; My main database has over 4000 tables. Holy shit! That's super large. Note to OP: Focus on getting insights from the data. The complexity of your queries should be driven by the questions you want answered. Chances are nobody will look at your portfolio code beyond "oh there's code in there, seems complicated". 
Surely if it’s a portfolio site, it would be reflective of your...portfolio? That said, brevity is always key. Look to simplify wherever you can and get things done in the fewest lines possible. This is language agnostic, really. Using tricky functions needlessly to convey knowledge of them is far worse than not using them at all in my opinion. Your best bet is to have a goal in mind and solve it eloquently. Good luck! :)
Oracle materializes CTEs into temp tables by default I believe. MS SQL does not (and it's not even an option). So in MS SQL, using a CTE is just syntactic sugar; if you reference a CTE twice in a query, the query that the CTE represents *is executed twice*. There can also be cardinality estimate issues with CTEs (translation: bad query plan gets generated). CTEs in SQL Server are _not_ a performance enhancer. Try changing your CTE into a temp table and see if things get better.
Have any jobs going?
That seems so small. I'd relish working on so few tables.
Not always single query, but glancing over 20 reports they range from 5 to 200 lines, not including some of the common queries that further process data computed by those procedures. There are of course more complex ones, but I'd say avg is 50-60 lines. We try not to list every column in separate line. About 500-600 tables total on biggest schemas, 30TiB size.
Medical claims? 
I don't know why you're getting downvoted. I've got a 5 person BI team. I showed them how to use a `WITH` clause yesterday. At one point they were diffing tables by dumping a 130m row table into a temp table then comparing the original using IN. I showed them EXCEPT and the query went from 40 minutes to 8.
A few years ago I changed jobs from sales manager to a gaming company. The team I joined uses sql daily and I knew absolutely nothing besides the name when joining. I went through some tutorial websites and was fine with copying a premade sql query and changing a few numbers and dates etc. Though if I had an error it took me a while to figure it out at first (added issue of using Japanese, if you put a Japanese space it doesn't work...). Once I got an understanding of what does what I was able to write my own basic quieres without too much trouble. Then using the site linked already I was able to write more complex quieres. I don't think you would need to go back to school. I would say knowing someone or having luck with landing the job is all you need. Only issue will be if they ask you to do complex stuff right at the start... But if joining a team you should be fine! Check out some tutorials read the site above and start asking around your industry for some job hookups! 
Depends on the questions you are answering of your code! &amp;#x200B; Consider SQL as the medium of providing the mechanism to answer questions of a domain of information. Often this domain of information is unknown or relatively known with gaps THUS you have to experiment to acquire the information. When you have experimented and found the information, you then optimise the experiments into meaningful and performant outputs using performance tooling or query tuning skills. The tuning skills are dependent on the DB type that you are on, the hardware you are bound to and the forecast frequency of SQL usage (this can also be indeterminate). The other aspect of this depends on how large the data sets are. Standalone queries are fine for small data sets but when you are getting in to the relatively large to big data arena, you then need to engineer processing pipelines to cater to your data output requirements. Standard scale data would require overnight ETL processing but the larger scale stuff ends up in fluffy cloud land magic.... &amp;#x200B; In a nutshell ... SQL length and complexity is the lion the witch and the wardrobe ...
Or a JSON field too if he wants more of a key value store.
As long as it needs to be. That's not flippant, it's just the reality - some tasks are done with a one-liner select. Others take multiple CTEs and complex joins, pivots, partitions, conditionals (cases) etc. Generally, I try to avoid big complex queries unless they are necessary: I'm not here to make myself look clever, I'm here to produce a result and make the code as readable and maintainable as possible to the next person who has to change it. If looking at a portfolio, I'd be expecting something more than simple one-table queries, but wouldn't care if you got much more complex than joining a few tables together: I'm much more concerned that you're writing readable, maintainable code. You can easily learn the more complicated stuff when it's needed, but it's harder to get out of the habit of writing sloppy code.
I'd consider it bad practice regardless of database platform. In SQL Server, `select *` isn't a performance hit as compared to `select every,single,field from thetable` *but* it's also not doing you any favors if you don't need every field on the table.
Looks like I don't know what I'm talking about. [It's actually 6 times the amount of tables!](https://i.imgur.com/LusHRQH.jpg) Our data warehouse has 206 tables. Maybe I'm underpaid! 
Seconding the jobs inquiry!
I went back to school online part-time. I found a community college that did online classes. Iirc I took 5 classes over the course of 3 semesters then I started job hunting. I felt under qualified for the job, but they (all other developers didn't). In hindsight, I was likely qualified after 2 semesters. IT is very different from most other fields. No one cares if you've got a degree. They care if you know how to do the job and if your willing to have an open mind to other ways to solve problems. Certificates are a waste of your time and money. Everyone I've talked to irl agrees. I work for a large company. I recently met someone at work who just got hired out of a coding boot camp. That'd be another thing you should research. There are significantly more IT positions than comp sci degree grads each year. Thus companies have to hire from other sources.
SQL is not a hard leap from any another other software coding. I've spent hours on stackexchange and other similar sites, and while I do not consider myself an "expert" in the syntax and methods, I think I was pretty competent making queries after 6 months of regular use. The next 5 years I spent getting better at making "good" queries and more efficient reports.
This. I'm redoing a ton of old queries at the moment and the way that's worked best with our server's setup is for me to write a small query with a temp table that pulls in all the cases I'm reporting on, then a small query to put those case's patients and their details into a temp table, then diagnosis, then prescription etc until I join all the tables at the end. All of the code is really straight forward, but in many queries there's approx 5-7 temp tables. For bonus points, each section is written in such a way that I can look at the old query, figure out what data it needs, and copy over the code from other queries in order to speed things up. It looks like a bunch of basic code, but it's the fastest way to get these tasks done with the server I'm working on.
&gt; the average corporate data warehouse has 12 tables. That sounds amazing. Our company just ballooned their DW from ~40 tables to ~140. Under the new schema, to just get a case number, open/closed, status, patient name, start date and client name I have to join 5 tables, when all of that used to be in one table. It's a nightmare.
&gt; If it already exists i'd like it to update it, if it doesn't exist create it as a new row with that value. sounds like a job for `INSERT ... ON DUPLICATE KEY UPDATE` https://dev.mysql.com/doc/refman/8.0/en/insert-on-duplicate.html
SQL syntax is easy to learn for most people. Visualizing the shape of data and ways to transform it is a little more difficult, but comes with time. What many data analysts don't have is experience in healthcare. This gives you a huge advantage, as many jobs want that experience. Here's what I would do: 1) /u/Taiwaly gave your first link. w3schools is a great start with SQL. I train a lot of people at work and this is the first place I send them. 2) Get really good with Excel. You should be able to pull data into Excel and create reports and charts. You'd be surprised how many data analyst jobs expect Excel. 3) Learn a visualization tool. Power BI is free and is quickly becoming ubiquitous. Plus, the language it uses, DAX, is the same as Excel PowerPivot &amp;#x200B; Those three steps might be enough to get your foot in the door as a data analyst or some position that can grow into a data analyst. Be sure to work on real projects as you learn. Ideally, from production databases or public data. Using public data would allow you to add your projects to a portfolio.
Honestly - your industry specific knowledge is way harder to get than an understanding of SQL. SQL will let you query and manipulate data in all kinds of sophisticated ways, but it is pretty meaningless unless you understand what the data is and how it relates to real world problems.
Wow, that is one massive query! How long does it take for you to write such a query? Is this based on a sample database or real one? If it's a sample, can you tell me from where I can download? I wanna get to that level of proficiency. Thanks!
I'm an entry-level developer who has been working for about a year out of college, but I also interned here for 3 rotations as well. I am doing analytics/ETL on plant data for a large CPG company. One of the largest efforts was using ETL to consolidate 20 different plants' data, which consisted of 20 databases, dozens of tables, and billions of rows. &amp;#x200B; Reporting / analytics on this data can range from one line to 3000 lines to calculate manufacturing statistics, but I myself usually get into the 50-300 lines with 5-10 tables.
Adding to what others have said, SQL is really different from your usual programming language. When programming, you describe a sequence of steps that the computer should do. SQL is a declaratory language heavily based on set theory. Remember in school when you saw Venn diagrams, "things that are contained in A and B but not in C" kind of stuff? That's what SQL deals with: big data set and the description of what slice of the data you need to see; not a sequence of math operations as in programming. 
Are't there third party tools like Apex Refactor or Redgate SQL Prompt that I can use for format the code? Or is that not allowed in a production/professional setting? Thanks! 
Were there any resources you used to learn advanced SQL and data mining? Thanks! 
So far my portfolio website has less than 15 lines of code on average. They're basically variations of queries against the AdventureWorks2017 database that I modified from books and tutorials. So far it seems like the queries fledgling level of skill so I am trying to make them look more advanced. 
&gt; Or is that not allowed in a production/professional setting? There's no such thing as "cheating" in a production setting - as long as you're complying with things like copyright and licencing. If there's a tool you can use to clean up your code quickly, it's considered a good thing - most of us wish people would use *more* code prettifiers, and we all use IDEs that do a lot of it for us. Although it's worth trying to write code that's tidy to start with, as a good habit
You have one database with 24k tables? Holy fuck.
It is from a real database. The names have been changed to protect the innocent. It is the 20th largest reporting query out of 200, and is twice the length of the average query. In terms of complexity it is pretty average. Number of tables joined, levels of subquerys, multiple group bys, union alls are all pretty typical for the kind of analytics we are asked to produce. I'd estimate the work to put this query together would be around 2 days of focused effort.
That reminds me of a job application where the supervisor did ask me what was my most complicated query where I asked if as in consulting data or making a stored procedure. Since he asked about consulting data I answered that the most complicated was a select *, he didn't ask for elaboration so I left it there. Truth is I've done many things more complicated but they're more focused to stored procedures because from my point of view it's better to make whatever complicated query to run automatically and fill a final table or view rather than making it from scratch every time you have to consult that information,that way I optimize and evade forgetting about filters or run into other issues. 
 &gt; fc.PrimaryPayingStuffKey , fc.PrimaryWhateverReasonKey , fc.DimPlaceOfDoingThingsKey , fc.DimAwesomeGroupKey , fc.DimCrazyGroupKey , fc.DimUnitKey , fc.DimGroupyGroupKey , fc.DimThingDetailKey , fc.PrimaryNailFilingIndicatorKey Haha.
BTW do you know what's required for a remote job in SQL? I want to learn more. 
The SQL language is SUPER easy. Understanding complex logic as applied to data structure, and developing the problem solving skills to solve complex data problems? That's another story.
You have to have this in a stored procedure? It makes it a lot simpler if you just use SPOOL from the local client. I'm assuming you're trying to schedule this or something, though. 
Yes it’s scheduled to run several times a day. Some, several times an hour.
Gotcha, SPOOL obviously won't work for that then. 
Sorry for another noob question, but I was wondering: what exactly is done after the result set of that query is returned? Is it extracted and loaded into a BI tool for visualizations? Also, how long would it take for a query that massive to be executed? Thanks again!
&gt; Oracle materializes CTEs into temp tables by default I believe. MS SQL does not (and it's not even an option). So in MS SQL, using a CTE is just syntactic sugar; if you reference a CTE twice in a query, the query that the CTE represents is executed twice. Thank you for this. The dude who's been low key mentoring me on SQL coems from Oracle and uses CTEs for everything - I couldn't figure out why my temp tables run so much faster than when I try to use CTEs.
Do you have a driver on the server that can write to XLSX? In SQL Server Land, I'd have to [do this](https://stackoverflow.com/a/9086889/1324345) with `OPENROWSET`
This is consumed as a Tableau extract, one of a few queries used in a Tableau workbook. This one runs in ~3.5 hours.
I was just going to ask, how long this thing takes to load in a report. I'm struggling with my co-workers stating my reports loading in 1.45 minutes being to slow.
I don't believe so, and I'm not sure exactly how I'd implement it. I have the server installed on my machine, but am very new to managing it.
I'm after the same thing and decided to focus on these points * Implement complex business logic in SQL * Showcase critical thinking and wise decision making skills. Evaluate alternatives, author custom and reusable helper utilities, document decision with objective data / performance tests * Demonstrate comfort with tools and constructs that support authoring highest level of professional code. * Collaborate with experts, solicit input and leverage great minds.
1000% agree. I have a couple of analysts who try to do everything in a single query and it takes forever to run and the margin for error sky rockets. 
Hackerrank.com
How about creating an SSIS script and implement a script task using C# to implement the conversion to .xlsx?
I learned this one the hard way. Had a stored procedure that was doing 2 correlated subqueries (against the same table) and timing out at 10 minutes. Needed to get things working better. So I said to myself "self! you should try a CTE!" So I did. Wrote a CTE for the subquery, replaced the subquery with the CTE and...not one iota of performance improvement. Checked the query plans and they were identical (always do this, it'll be a dead giveaway). So I pulled the CTE out, had that query dump into a temp table instead, and did my correlated subquery against that temp table. New query runtime? Under 90 seconds.
The user experience is snappy, because Tableau is quite good at consuming result sets pre-extracted in their proprietary format. I think it's probably a few seconds.
I would go for an SSIS or C# solution too
Sadly beyond my current capabilities. I don't know anything about C# or SSIS :(
Sadly beyond my current capabilities. I don't know anything about C# or SSIS :(
Thank you! I think relation tables are what I need.
I found [https://sqlbolt.com/](https://sqlbolt.com/) to be very helpful as well, even with some basic SQL knowledge I had going into it. 
From what I understand now, this would be a relation table. Thank you!
Python/R/most languages would be a very easy solution here.
Hmm I understand. So, is it a work thing? Asking a more experienced developer might help. Otherwise I would suggest a lot of trial and error through Stack Overflow. SSIS would be the easiest if you have the software. Basically a matter of connecting your database, grabbing data from a SQL DB table, transforming it so it fits your CSV file and then exporting it. 
I'm surprised that an SQL database can create an array and store it! I thought a column in an SQL database could not hold an array of values like that. Thank you!
My whole BI team uses red gate for easy code formatting. 
Yes, it's for work. I've been tasked with majority of the reporting and analytics for my business unit. Unfortunately, my BU doesn't have anyone in it that is a developer. I've self-taught everything I know about SQL so that I could create automated processes with the assistance of an RPA software based on the data the larger company is collecting.
Dang I feel like I’m in the Stone age building reports in SSRS 
Why does it need to be an .xlsx instead of a .CSV?
Microsoft Flow can only see .xlsx files in the online excel function for automating processes.
I know you said you don't know SSIS, but I promise its very simple to do this. Have your Sproc run into a table. Then you just need to create an SSIS package that will take that table and output to excel. SSIS has a GUI and isn't very complicated for simple tasks. Here's a good tutorial of a much more complicated output to excel. With a little bit of trial-and-error you can implement this pretty quickly. [http://knowlton-group.com/using-ssis-to-export-data-to-excel/](http://knowlton-group.com/using-ssis-to-export-data-to-excel/) 
Doesn't this have to utilize a ms sql server and not an oracle database?
Very much agree here. For python look into psycopg2 library. It can do you SQL queries and then python can export to xlsx or it should. Powershell as well should be able to do it as well
Use the SSMS import / Export wizard which will output a file as .xlsx. You can save the steps as a script and then schedule it
Normalization is usually a good thing but that just sounds way too normalized 
Sounds like someone did some normalization. For warehousing this is not always ideal. You should ask for some denormalized tables!
gotta show the value of PowerBI to your company. If you already have SSRS you wont even need another server for it and its cheap compared to a lot of reporting softwares
[w3schools.com](https://w3schools.com) is great to get the basics, and even play a little bit with the in browser designer. Download a copy of sql express and download the test data from microsoft and you can really start to understand the functionality with applicable data. 
Then you can try KNIME and skip any coding (https://www.knime.com/) Its free
&gt;Sounds like someone did some normalization. Yep, and then quit when everyone started complaining. &gt; You should ask for some denormalized tables! The DW team privately agree it's a mess and publicly defend it to the death.
You should be able to connect to an Oracle DB https://stackoverflow.com/questions/18096409/connecting-to-oracle-database-using-sql-server-integration-services SSIS itself can be ran locally 
Good luck! Sounds like you'll need it :)
I'm a data engineer that manages connections to dozens of different databases across many database engines. I write a ton of SQL, do data exploring, sometimes use data exporting features. Nothing. Absolutely nothing comes even close to the the level of features provided by DBeaver. It's cross platform which means that whether I have to use Linux or Mac, or God forbid Windows, it'll work and behave the same way. It will even create SSH tunnels if I tell it to, it will open a connection and transaction automatically when I tell it, through configuration, it will format the data as text, so that I can copy paste it into markdown, or using sortable columns. It will keep extracting just 200 rows into the UI by default to make any query easy on the client. I'm always trying out new tools because DBeaver is based on Eclipse, but every time all I get is huge disappointment. DBeaver is a Swiss army knife of databases, which is often better than the best specialized tool for any particular database engine.
Thanks so much for sharing and taking the time to reply. You've def sold Dbeaver to me will check it out over the weekend Thanks so much 
I haven't worked deeply with MSSQL for a while, but I remember that you can do it the same as in any other database: cast or convert. Coming now from Postgresql where this is literally just `some_value :: INT`, I'm stunned that anyone would write an article about this. And since it won't open for me, can someone tell me if it's worth reading? What am I missing?
cx_Oracle
Ok why Microsoft Flow? What's the end goal of the whole process?
Bruh, the concept of "a typical SQL queries" doesn't even make sense. You write the query for whatever the task is, and it's as long or as short as it needs to be.
Yes, location is controlled outside of SQL, but when the request is made SQL is usually asking for more than just the exact amount of space needed for the next write. Once the space is allocated to SQL the location is reserved on the storage device in a contiguous block. SQL can then manage the ordering of the data pages within the space it was allocated. Think of a simple .txt file, if you open it and change abc123 to 123abc does it move on disk even though you've reordered the contents? The contents changed, but the file is the same size and stored on the same disk blocks. 
That's a tricky one! Some hints to get you going: - What is the expected result of the query (i.e., what is the meaning of the query)? - Execute the query and look at the result. Does the query behave as expected? (http://sqlfiddle.com/#!9/058026/7/0) - Think about how the NOT IN operation works and try to calculate the result of each comparison in your example manually.
The basics are very easy and the advanced shit is hard
Sorry, late reply. I might have a better solution: how about using Power BI? You can connect that easily to your DB and create very useful data visualizations. 
Losers:1,4,5 : AND is IS NOT NULL : That way you don't list races not run yet..
Ad hoc queries: usually less than 150 Making data models or views to be used a lot : can go over 500 depending on complexity of data
In plain English it is asking 'select all of the runners from the runners table who's id on the runners table is not in the winner_id column of the races table. Answeer: &gt;!Runners 1, 4 and 5 did not win any races!&lt;
All those subqueries give me debugging anxiety
I am still learning SQL (beginner) so your answer is a bit confusing. Does the statement you sent me work? 
there will be no rows returned by the query from OP's question. 
Wow.
So no rows would be returned if I ran the query?
yup. you should really follow what u\NotImplemented outlined (if the goal is to learn rather than obtain the right answer)
I see, his response goes a bit over my head. I am still learning and at a beginner level. Any other hints, what NOT IN operation? And how would I calculate it manually? Sorry if these are stupid questions. 
Since you're a beginning, people should be more helpful with you because what's going on here isn't at all obvious. The reason that this is going to return 0 values is because record 4 in the races table has a winner_id of null. Null is not 0, null is null. So when the NOT IN clause says, "is runners.id (1) in the races.winner_id (4)," SQL isn't returning "4 doesn't equal null," it's returning, "I don't know if 4 equals null". So when you're running a NOT IN clause against null values, you're always going to get 0 results. That's the tricky part of this question. Hope that helps, but please feel free to ask for clarification.
Not stupid, we are all starting at some point. Think of 'in' or 'not in' as a way of shortening 'where' clauses. So for example, if you wanted id's 1-4 from the first table any of these three options would work. without 'in' or 'not in': select * from runners where id = 1 or id = 2 or id = 3 or id = 4 kinda messy, that is where 'in' comes in handy: select * from runners where id in (1,2,3,4) or even a little cleaner: select * from runners where id not in (5) Doing the separet or "id = 1 or id = 2"... puts a lot of strain on the system, where as the "in" or "not in" is cleaner and more efficient. So in the example you provided you are trying to find the examples where the results are "Not in" the results of the query. note: its not exactly related, but [this chart](http://2.bp.blogspot.com/-5lOHd-lu0c8/VXUXyiyw7SI/AAAAAAAAACs/thuvJknKwrs/s1600/join.jpg) helped me a LOT when i first started sql, you may find it useful
Performance should be the same. Unless you need it to be recursive, the functionality should be as well. I prefer the look of a CTE to a subquery, it's nice to have everything defined up front. Like any other declaration. That's how I see them.
Odds are they don't have SSIS available if they're writing PL/SQL.
Is there a way to rewrite the code (leaving the null value) so that it runs?
&gt; SSIS itself can be ran locally True but if you're using it for production purposes, you'll want to have it on a server and licensed for that usage.
Thank you! This explains the part where I can explain if I added a 2 value vs the null it would return 1,4 and 5. Now I understand why the query would fail. 
As someone who has only worked at one place, could you give a sample of the subject areas in a data warehouse that small? I'm in healthcare and our main source system alone has over 25,000 tables. That's not including the dozens of ancillary systems that are for specific specialties. I'd just be curious to know what another industry looks like
Pandas and SQLAlchemy work great!
 SELECT * FROM runners WHERE id NOT IN (SELECT winner_id FROM races WHERE winner_id IS NOT NULL)
I'm pretty sure that if your using Microsoft SQL you can export the results directly to an excel spreadsheet.
Something that will make your life much easier going forward is to always logically thing of *null* as "unknown" What is 1 + unknown? It's unknown. Nulls will propagate throughout equations because SQL essentially treats them as unknown, rather than "nothing" or "empty"
Ah yes. I feel silly now. 
The result is all the ids from the races table, that are NOT IN the other table. Think of it this way; First you read the select statement in the paranthesis. Okay, we select all ids from tableB. Now we know the data inside that parenthesis, contains all ids from tableB. Now the outer query. It says to select all ids from tableA that are not in the dataset contained inside the parenthesis. An alternative query can be done via the EXCEPT keyword, where the left side is for races, and the right side is for the other table. Try and make this alternative query on your own.
&gt; Doing the separate "or id = 1 or id = 2"... puts a lot of strain on the system, where as the "in" or "not in" is cleaner and more efficient Just so you know, it isn't more efficient, performance wise. With a and b being literals, `x IN (a, b)` will be translated into `x = a OR x = b`, and `x NOT IN (a, b)` will be translated into `x &lt;&gt; a AND x &lt;&gt; b` by the query planner. [Literally](https://i.imgur.com/FVVATHd.png).
In SQL Server using 'in' is faster for un-indexed data. I'm not smart enough to know why, but here's a [stack overflow thread explaining it.](https://stackoverflow.com/questions/3074713/in-vs-or-in-the-sql-where-clause) explaining why.
I took several csharp exercises. I liked it.
And the solution is relatively simple: ensure there are no nulls in your sub query You can do that by excluding them, or by coalescing null values to 0 or -1 or some other unused value (0 and -1 are common)
&gt; The result is all the data from the races table, where the I'd is not found in the other table. No it isn't, that's the point of the question. The winners table has null values which will propagate up from the sub-query and mean the query returns no results.
You could create a view that only selects the rows you want, excluding the minutes and seconds columns. Group by the columns in your select statement. Then run a count query against your view.
hackerrank.com
Run a count distinct against the lowest level column in the table once you remove the time columns. Maybe something like: Select count(distinct day) as uniquedays From date_dim This will return one record. You could always dump it into tempdb and look at the messages in SSMS if you want
Set the time field as a key
I currently have the inspection as the key, is it Ok to have the primary key as a date time value?
I don’t see any reason
I had to learn this the hard way on the job!
Having end users change the pk as needed is ok?
¯\\\\\_(ツ)\_/¯
Haha. Yeah exactly. Thank you!
What do you mean only one inspection per hour? Does that mean only one inspection from 12-1, or only one inspection in the last 60 minutes?
Only one inspection at 12. The next one eligible would be at 1. Set times. 8 am 9 am 10 am 11 am 12 am 
I'd prefer `where not exists (select 1 from races where winner_id = runners.id)`. on big datasets, the correlated query always wins, especially with indexes, but it might be a tossup for small queries. It's also closer to the English analogue, ≅ "where not exists a winner"
It won’t return anything as there is a null in the subquery
Which database? NULL is treated different in Oracle than other databases. 
Well, you kind of have to take a step back and think about what the purpose of a data warehouse is. It's quite different than the OLTP, or any of the systems that software writes to. It's the system that business-minded people use to ask basic questions and generate reports. So, a good data warehouse should explain itself to these end-users. Including everyone from more senior business analysts, to the accounting intern with a bit of Excel experience poking around with SQL for the first time, to the brighter executive who doesn't mind getting his hands dirty writing a query or building a custom report once in a while. This usually means massive amounts of denormalization (pre-joining tables for your user, basically) and tossing out all tables and columns that aren't of interest to the user. Ideally you'll end up with maybe 2 - 4 fact tables and a handful of dimension tables. Every company is different, but if you actually talk to the members of a department you'll find that they're really only interested in a very small subset of the data, and somewhere between say 8 - 20 objects is a good Target to aim for.
 SELECT COUNT(DISTINCT YearColumn&amp;MonthColumn&amp;DayColumn&amp;HourColumn) FROM Table I think that would do it... 
Not OP but just started learning SQL. Thanks for the information 
So I finally solved it and thanks for the reply ichp this was the solution I came up with SELECT DISTINCT name FROM casting x JOIN actor ON x.actorid = [actor.id](https://actor.id) WHERE actorid IN ( SELECT actorid FROM casting y WHERE ord = 1 GROUP BY actorid HAVING COUNT(ord) &gt;= 30 ) &amp;#x200B; This problem taught me a lot so thank you for your help I didn't think about using GROUP BY with having like that so thanks for pointing me in the right direction!
No joke, this Russian site is pretty fantastic. http://www.sql-ex.ru/
Set a unique key on an expression - the date time field truncated to hour
I would run it in separate queries: 1. Drop table if exists tablename1, tablename2, tablename3; 2. Create tablename1 as (your first query) 3. Create tablename2 as (your second query) 4. Create tablename3 as (tablename1 join tablename2 on tablename1.month =tablename2.month) 5. Select * from tablename3
Depending on the type of db you could also use cte as in tables created with with clause that are session based,no need to take care of remains, or if you are using redshift create them with # before the name telling redshift they are temporary. I avoid running queries where everything is declared in subquery. They are harder to follow and even stranger to debug. If you have separate tables you can see exactly what each table consists of before the join
You're seeing high values for query b because joining query b and then doing another level of aggregation. You're doing 1. Query B 2. Join to Query B to table X 3. Query A You want to do: 1. Query A 2. Query B 3. Join Query A to Query B I'm going to use a couple [CTE](https://www.essentialsql.com/introduction-common-table-expressions-ctes/)s here just to make it look clean. with queryA as ( SELECT a.[Discharge-Month-Year] as MonthYear ,count(a.[Discharge-Month-Year]) as CountofRecords FROM [TableX] AS a GROUP BY a.[Discharge-Month-Year] ), queryB as ( SELECT b.[PreviousEvent_Discharge-Month-Year] as PreviousEvent_MonthYear ,sum(b.meetscriteria) as DerivedValue FROM [TableX] AS b GROUP BY b.[PreviousEvent_Discharge-Month-Year] ) select queryA.MonthYear ,queryA.CountofRecords ,queryB.DerivedValue from queryA join queryB on queryA.MonthYear = queryB.PreviousEvent_MonthYear
Sure, but I'd be curious what it gets distilled down to in other industries. Our healthcare data warehouse has over 400 tables
Not the best at SQL but trying to return this calculated value. pr.LKVItemMAPPercent = 40, pvt.MSRP = 37.23, mapPrice is always 0. The SP returns all other values correct, just not this one. 
If the percent value is an integer it could be causing your issue because the inner calculation will then return Add 0.00 to the value to make it a non integer. Like this: (isnull(pr.LKVItemMAPPercent, -1)+0.00 
Is the Product table giving you the cost to the company, or an item's typical price? The Order Details table might be giving you the price they sold it that day, so includes any sales or coupons that might have been used.
&gt;Thanks but still zero
Could it gVe something to do with the select top? 
Without knowing the exact values of the components of the expression, I can't tell you know more. I know you say they are the values above, but if they were my changes should have fixed it. Try creating a new SP that has all the same code but adds as the last three columns: pvt.MSRP as pvtMSRP, isnull(pr.LKVItemMAPPercent, -1) as isnullprLKVItemMAPPercent, pr.LKVItemMAPPercent, isnull(pr.LKVItemMAPPercent, -1) / 100 as isnullprLKVItemMAPPercentdivided, This way you can see how the components of the expression turn out piecemeal. This is what I would do when things don't work the way I intend them to - break it down as much as possible and see how the values/types are coming out. 
Break it down in to the individual calculations. Make sure each one gives you what you expect. Then put two of the calculations together, then the next until you see where it’s wrong.
Your top value is an Int. Cast all values in the top as float :)
I don't know if understand your question. Select * from [123my_table]
Correct. Or even dbo.123my_table
both of these suggestions only work in Microsoft SQL Server OP neglected to mention the platform
Thanks to both of you! It worked, I’ve added something like &gt; public.”123mytable”
dear OP, you neglected to mention your database platform so here's the **standard SQL** solution -- SELECT ... FROM "23skidoo" if that doesn't solve your problem, maybe the Microsoft SQL Server suggestions in the other replies here will work or, you could try the MySQL version -- SELECT ... FROM `23skidoo` 
Oops sorry about that. I’m using PostgreSQL. I was able to run my query by adding the public keyword and quotation marks. Thank you so much.
May want to use a FULL OUTER JOIN here, since the columns being joined are dissimilar.
I will try this, thank you.
So, you want to display not the ID, but the foreign key's name that is associated with the ID?
Yes. Define the Name column as an alternate key in Client. Name must still be defined as unique, not null same as the PK.
Can you explain further?
Add a unique, not null constraint to the Name column. Then use that Name column as the foreign key in the Orders table. So, as long as you define the Name as an alternate key you can migrate to another table as a foreign key.
Oh, you mean to actually use the name as the key?
Yes.
That's actually pretty bad in terms of normalization. Honestly, if they're reading the data only, I'd consider a view with the data first. 
Yes, someone will specify what they want. That’s how you would know what results you are going to pull. 
As an extension of this very correct answer, in my experience people give you a general sense of the information they want and you get to figure out with your expertise what fields will get them that information. Most people don't deal with the databases and know the field names so it's good to actually know what the fields are.
Are you wanting to use this relation to enforce referential integrity, or just to be the table join context? You can join tables on any columns, it doesn't have to be a FK.
Yes great point. There are tools that can help with translating business names/terms to database field names. We use a data dictionary.
Aye, good points. Often times as you're developing a query you'll realize including some additional fields makes the data more digestible. Indicators are a good example. I might be including an indicator in my results by request, and that indicator is based on a date range. I may not have been asked to include the dates but often including the start and/or end date is helpful. As an added benefit, it helps deflect followup question-and-answer sessions. 
how about you change `isnull(pr.LKVItemMAPPercent, -1) / 100` to `isnull(pr.LKVItemMAPPercent, -1) / 100.0` so the division is casted to float?
&gt; That's actually pretty bad in terms of normalization. i don't think you fully appreciate what normalization is please explain why you think it's bad,
DBA for a small data centric company. When I'm working with the app dev, we'll usually meet with the end users on what they need to see in that part of the app, and then he and I will figure out what behind the scenes columns we'll need. When I'm working with the BI developer (me in another hat), I pull back what I need, after talking with sales/end users on what sort of things they want to see. How it's put together is my problem. The other 90% of the time, I'll get a request for "I need to know *x*, *y*, and *z*", and it's up to me to figure out the columns/calcs to drop into Excel, and depending on the data pull, I might do more/other calcs in Excel. Sometimes it's just easier/more efficient that way.
Can you tell me the the name of the data dictionary software you use? Thanks so much! :)
INT math
We have ours set up in access 
that was the magic, thank you
multiple all of mapPrice by 1.00
Thanks! This seems to have worked :) Also thanks for the CTE link. This is really useful and helps me structure the code more tidily. &amp;#x200B; Just for my understanding can I ask why the way I have done it doesn't work (i.e. doing another level of aggregation) , as it seems to make sense in my head. &amp;#x200B; In the query above, i would have to aggregate again at the final query anyway (I've added this now in the code). There would be a sum() in query B and in the final query: with queryA as ( SELECT a.[Discharge-Month-Year] as MonthYear ,count(a.[Discharge-Month-Year]) as CountofRecords FROM [TableX] AS a GROUP BY a.[Discharge-Month-Year] ), queryB as ( SELECT b.[PreviousEvent_Discharge-Month-Year] as PreviousEvent_MonthYear ,sum(b.meetscriteria) as DerivedValue FROM [TableX] AS b GROUP BY b.[PreviousEvent_Discharge-Month-Year] ) select queryA.MonthYear ,sum(queryA.CountofRecords)-- sum function otherwise SQL tells me i have to group by ,sum(queryB.DerivedValue) -- sum function otherwise SQL tells me i have to group by from queryA join queryB on queryA.MonthYear = queryB.PreviousEvent_MonthYear Group by queryA.MonthYear &amp;#x200B; &amp;#x200B; &amp;#x200B;
There is northwind MySQL database. http://www.artfulsoftware.com/mysqlbook/sampler/mysqled1_appe.html
Thank you! Using the r/babygrenade query above with a full outer join results in a row where the dischargemonth is NULL. &amp;#x200B; Note that in the column \[Previous\_Discharge-Month-Year\], there are some rows in the raw table that are NULLs. While the \[Discharge-Month-Year\] should all be populated. Is that because I've used a full outer join here? 
You are on the right path. Being a SQL developer will introduce you to more responsibilities. Along the way you can start studying up and going for a Microsoft certification for administration. 
MS SQL Contoso
There's also the [Chinook Database](https://github.com/lerocha/chinook-database) that's compatible with many database systems, including mysql.
Timing out in 60 seconds is not normal. I've run a bazillion queries that ran longer than that. My record is about 4 hours.
Microsoft certifications are great! CBTNuggets is very helpful for learning as well as the Microsoft press store study guides. I used that combo from everything from my MTA in database administration through the MCSA to the MCSE in data management. Microsoft certifications are good because they let you test from the comfort of your own home. It's a big deal to get to a testing center on time and the temperature and so many things. 
You're doing a sum on a number that's already a sum. Since it's joined you're summing on all the rows from table x that match. Your query would also work if you added the value from query b to the group by statement
Stack overflow releases a dump of their db from time to time as well
You're not supposed to use order by in a sub query, it doesn't really make sense to do so. The main query will order it in whatever way it sees fit which might not be how you've done it in the sub query. It is instance specific. Different flavours behave differently. I believe Maria DB will let you use the order by and so will TSQL, while MySQL won't let you. Just throw the order by in the outer query 
You can do something like this With OrderCTE ( select top 100 percent city, length from station order by length desc) select top 10 * from OrderCTE
Tableau Public? I dont believe you can connect the free version to a database (could be wrong), so I typically just push the data to a csv, and use that.
W3Schools is a great resource