If another query needs the row you have locked, sure it'll slow it down... but that's probably nanoseconds--maybe smaller. Dirty reads are reading uncommitted data... so yeah, you could potentially see an uncommitted transaction from another session, depending on how the application is loading the data. For the most part, this isn't an issue as data is sanitized prior to attempting the database insertion whether it's on the application side (eg: making sure your birthday is an actual date) however if there are constraints / foreign keys on the database, those usually aren't checked for until the insert / update happens.
Oracle is one of the various DBMS we use. I'd go nuts if a single query took that long.
It may. If I can't help with it someone else might. Are you able to break up your query at all? I have had Oracle increase the time of a query significantly by trying to combine to queries into one. It was something like this: WHERE (T1.C1 = 'Value1') AND T1.C2 = X) OR (T1.C1 IN ('Value2', 'Value3') AND ( T1.C2 = X OR T2.C1 = 1) By splitting the query into two where the OR is performed, the original query went from taking minutes of running (canceled before completion) to mere seconds for both queries to finish.
You could also work with pre-made datasets that already represent an interest of yours. In my case I wanted to learn GIS for landscape design. Not long after I became aware of attribute tables and then databases. There are plenty of databases already out there for plants, so I query those to answer questions. It's something I'm already interested in and the database already exists. 
[removed]
Not really sure if lock or only slow down, I'm almost sure it only slow down.
Won't that just flip the service to the other node and the tempdb remain the same size?
Yup. There are more precautions you can take, but hardcoding the variables so it doesn't split the SQL into transactions and force something malicious is the easier / best practice. People can get around that with enough information and determination, but it's better than nothing.
Failing over is a restart, instance stops on one node and starts on the other. I definitely echo the other advice in the thread, find out what caused it to grow in case it happens again. You can use the default trace to track it down to a time &amp; login, see if it correlates with any maintenance or perhaps a silly query that someone issued. Never shrunk a tempdb, AFAIK the instance has to be in single user mode which !sans downtime
Could you elaborate on the default trace a bit more?
Some good points, but I'd strongly recommend against Access as a first database product. Sure it's easy to transition to and plays nice with other MS products, but it's a dead end. It scales only slightly better than Excel, and the stuff you have to learn to use it well doesn't translate to other "real" dbms's like Oracle/MySQL, etc.
The answer depends on what your current skill set is and what type of job you're looking for. If you're a business analyst type, then you'll want to focus on SQL for analytic queries and for reporting. So things like querying aggregated values to put into charts or tables; or querying for finer grain data to do additional analysis on. If you're an infrastructure/IT type, then you'll want to focus more on SQL for creation and management of tables and other objects. I'd recommend doing a local install of a free DBMS like MySQL. If you just want to practice analytic queries, MySQL has a sample db called "sakila" you can download. Google to find lots of exercises out there against that data set. If you are more on the infrastructure side, then pick a subject area you like (books, movies, sports, etc.) and create a schema that holds data about it. Next, go get some raw data from the web (list of books and authors, team/player stats per game, etc.) and figure out how to insert it into your structure. edit: adding on another big type of SQL user: software developer. As a software developer, you'll mostly be querying and inserting. A local MySQL installation + Sakila sample db would be a good starting point again. Except this time you include your query in some code. Optimization exercises become more important here (as analyst,you usually don't care if a query takes 5 seconds or 5 minutes.)
It should be handled in both places. If SQL rejects a record due to a constraint, that error condition has to be handled anyways. Put it in the app to make it user friendly, and then put it in the DB to ensure consistency.
&gt;I don't think I understand why I couldn't just use excel Because Excel does not support multiple users and ACID compliance. Want to learn? Take something you'd use Excel for and then go make it happen in SQL.
Have a look in the folder the errorlogs are in for a bunch of .TRC files, find the one from the time period of interest, open it in profiler and do a search on tempdev. You're looking for file growth events
Multiplied records (assuming your source tables don't have the multiples) is always going to be a join related issue. If you understand how this happens you'll have taken a big step in truly understanding what a join really is. I'm assuming that pluspgbtrans can have many records per customer. Suppose we have customer = X, who has three lines in pluspgbtrans. Now suppose that quantremove = 10 in the toremove subquery for customer X. Next we do the join, and here's the key point: *when you do a join the result contains rows for every possible set of matches on your join criteria*. So after the join we've got a set of records like: pluspgbtrans.customer|pluspbgtrans.eip_lineprice|toremove.customer|toremove.quantremove|toremove.line :--|:--|:--|:--|:-- X|30|X|10|3 X|20|X|10|3 X|15|X|10|3 Thus, sum(toremove.quantremove) = 30, which is not true for customer X. The easiest way to fix this is to get your two tables aggregated at the same level *before* joining. So something like: select toremove.cus, sum(a.eip_lineprice) - sum(toremove.quantremove)as net_change from ( select customer ,sum(eip_lineprice) as eip_lineprice from pluspgbtrans group by customer ) a ----------------------------------------------------------------------------------- ,(select customer as cus, sum(eip_lineprice)as quantremove, count(eip_line) as line from pluspgbtrans group by customer) as toremove ------------------------------------------------------------------------------------ where (a.customer = toremove.cus) group by toremove.cus having sum(a.eip_lineprice) -sum(toremove.quantremove)/count(toremove.line)) &lt;&gt; 0 order by 3 asc; Now as written, this won't return any rows because really both subqueries are returning the same sum per customer, and then we're filtering rows where the difference of the sums is 0, but I'm guessing that there is more going on in your "actual" query and that you can just use this as a pattern.
This isn't a "conversion", it's formatting for presentation. Which should be handled elsewhere (not in the database, not in the query). Do it in your presentation layer - web page, report, Excel file, etc.
Not particularly. If your DBMS supports arrays, you can store an array of parents as one property and children as another. The traditional way is a separate table with (parentId, childId) structure. Just reread your post. SQLite does not support arrays, so separate table it is.
If the data really isn't relational, why are you trying to use a relational database? Have you looked at [graph database](https://en.wikipedia.org/wiki/Graph_database)? I've played and like OrientDB, but there's lots of others.
Self referential table where the parent id of each row references the primary key of another row except for the base element that has no parent? Not sure this is what you mean by a tree but we've used tables like this at my work along with recursive queries.
People don't like doing other people's homework. Show us what you've got so far and we can help you along the way. I'm more than happy to spend some time helping but I won't just do it for you.
Sure. select sum(amount) from ( select client , sum(amount) as amount from table ) a inner join ( select client from table group by client having count(*) &gt; 10 ) b on b.client = a.client
It technically _is_ a table, there's no difference, you're just filtering / selecting specific records from that table instead of using the entire table. Just think of the SELECT being used in the FROM clause _as_ a standalone table with _only_ the rows it's selecting in it.
CASE / MIN / DECODE are your friend.
I don't really get what's going on here..
Hello. But why do you need to filter/select specific records from the table instead of using the entire table? And can you give me an example of a situation where this would be necessary?
You're just saying `from` a subquery, like you could do in a `join` or a `where` or a `select` clause. You're ultimately saying select the sum(amount) from this process.
[notasqlstar](https://www.reddit.com/r/SQL/comments/54c4ef/help_with_inline_views_having_select_statements/d80kqs0) already gave you a good example. In in the subquery aliased as **b** they used aggregation to find all the clients that had 11 or more rows and then used that to "filter" records in the subquery aliased as **a** via an INNER JOIN. Assuming that each record in **table** equated to one order (aka, the grain of the table), the question being asked was probably something like, "Show me the total dollar amount for clients that made more than 10 purchases."
Bare with me here, I'm very new to this so it's still confusing but I'm trying to grasp this concept. Why couldn't you have just done : &gt;select client &gt; from table &gt;group by client &gt; having count(*) &gt; 10 Why wouldn't that have gave you the answer your searching for? It gives the client who has more than 10 purchases.
I have to read and modify these at work sometimes. I'm not an expert, but perhaps that's what you need! [Here's a visual of the post](http://imgur.com/a/DsCJj) /u/notasqlstar posted. I hope it makes things just a little bit more clear.
1) It's explained pretty clearly in the documentation. I'm not saying that to be a jerk - I'm just saying I can't explain it much better. https://docs.oracle.com/cd/B28359_01/server.111/b28286/statements_6015.htm
The documentation is mostly clear, but to put it another way (for MS SQL Server). Cycle: if you sequence range is small (or not) it will loop through it when it hits the end ... so if it were say a sequence range of [1-3] it would go 1,2,3,1,2,3... Cache is simply how many numbers in the sequence are read into memory ahead of use. however the numbers read into memory can be lost if the server crashes or is shut down in a not normal way. Also, from experience using Sequence numbers in a high-ish volume MS SQL Cluster, don't put the default incrementor on the table, call it from a sproc or something. I have had repeated issues with the Sequence number resetting and causing key collision errors.
mhm. Let me flip this around for you and see if it helps: For modern sql implementations, queries work on the basis of result sets (n columns by m rows) and return result sets as their output. Some result sets are useful to keep around and we store them in tables. This way we don't need to rebuild the result set for every query and can reuse the one stored in a table. If the one stored is not useful directly, you can always use another query to change it to whatever you need. The join operations also work with result sets - tables are a convenience, rather than a requirement for joining. There are also a few useful special cases: A single record result set - singleton - cross-joining these to any set keeps the row count. A result set that has 1 column - a list, can be used in the "in' operator. A result set with exactly one row and one column - a scalar - can be used in expressions as a value of a certain type.
Not to be rude at all because I appreciate you tryig to help, but I don't see how this applies to inline views / the questions in the original paragarph.
This is incredibly helpful, my only concern is that I'm pretty sure both of my group bys are for the same level, customer, why is the outer showing three different rows for the same customer if I group by customer in the outer? Should I group by the inner column alias in the outer group by as well? If my eiplineprice is used as the result of an aggregate why would it return 3 separate rows as it does in your table
Lets try a real world example. We have a Table A which has raw data that is possible to cause duplication issues when joined to another table. Maybe it's the baseball stats for each player, for each year of their career, and the teams they played for, such as: | Player | Runs | Team | Season | | :--- | :--- | :--- | :--- | | Babe Ruth | 45 | Tigers | 1988 | | Hank Aaron | 62 | Yankees | 1988 | | Jackie Robinson | 37 | Cubs | 1988 | | Babe Ruth | 55 | Tigers | 1989 | | Hank Aaron | 36 | Yankees | 1987 | | Jackie Robinson | 49 | Cubs | 1988 | | Babe Ruth | 12 | Yankees | 1986 | | Hank Aaron | 57 | Yankees | 1990 | | Jackie Robinson | 62 | Cubs | 1989 | | Player | AvgCareerSalary | | :--- | :--- | | Babe Ruth | 250000.00 | | Hank Aaron | 225000.00 | | Jackie Robinson | 300000.00 | And I ask you to come up with the average career salary for each player between the years of 1985 and 1989 who did not play for more than one team between the years of 1985 and 1989. Sure you can talk about this not being an optimal way to store data or whatever, but that isn't your job and you're being asked a question by your boss so you have to figure out a way to make it work. So first order of business is coming up with a list of players who only played for (1) team during 1985 and 1989: select player, team from table1 a where season between '1985' and '1990' group by player, team having count(*) Now calculating the average salary is simple enough: select player, avgcareersalary from table2 b Now we need to put these two pieces together. select distinct player, avgcareersalary from ( select player, team from table1 a where season between '1985' and '1990' group by player, team having count(*) ) a inner join table2 b on b.player = a.player Now you could also do it like this: with cte as ( select player, team from table1 a where season between '1985' and '1990' group by player, team having count(*) ) select distinct player, avgcareersalary from cte a inner join table2 b on b.player = a.player But you could go further if you wanted and create #tables or @tables and calculate this in steps: begin select player, team into #table1 from table a where season between '1985' and '1990' group by player, team having count(*) end begin select distinct player, avgcareersalary from #table1 a inner join table b on b.player = a.player end Each one will have different execution plans (typically) and may be useful for different types of jobs. Don't get hung up on why an inline view is different than approach A, or B, but rather focus on what it allows you to do and how you can use the various combinations to achieve your goal. For example you could use an inline view within a CTE that is within a series of BEGIN and END statements that use #tables. 
Imagine if the salary table was not averaged and you had to average it yourself: | Player | Salary | Year | | :--- | :--- | :--- | | Babe Ruth | 250000.00 | 1988 | | Hank Aaron | 225000.00 | 1986 | | Jackie Robinson | 300000.00 | 1984 | | Babe Ruth | 260000.00 | 1987 | | Hank Aaron | 275000.00 | 1985 | | Jackie Robinson | 400000.00 | 1983 | | Babe Ruth | 350000.00 | 1985 | | Hank Aaron | 125000.00 | 1982 | | Jackie Robinson | 400000.00 | 1981 | Now you can't do a direct join. You have to first calculate the average. Specifically I think it is helpful to think of a `view` like this: It is a way to give a user access to summary table from a raw table / server which it may not be appropriate to give a user direct access to. But that isn't "how it works" -- and how it works is just like any other sub-query.
Cool visual.
&gt; mhm. Let me flip this around for you and see if it helps: For modern sql implementations, queries work on the basis of result sets (n columns by m rows) and return result sets as their output. Some result sets are useful to keep around and we store them in tables. This way we don't need to rebuild the result set for every query and can reuse the one stored in a table. If the one stored is not useful directly, you can always use another query to change it to whatever you need. The join operations also work with result sets - tables are a convenience, rather than a requirement for joining. This is the big point you're missing. Inline views are identical to sub-queries in every way shape or form (like CTE's, or when you use them in joins, or in the select, or in the where like we've discussed in our other posts). What this person is saying is this: *Look man. Some data should be stored in tables because we only have enough storage to accommodate our business. You don't need to store weekly averages as a separate table, I can create a "view" for you. All I have to do is modify the `FROM` statement to do a little subquery and then I can have it set as a view. You can access this "table" (its not a table, its a view) and I won't have to give you access to the main table which is always being updated and might cause the server to crash if you aren't careful.* That's what a "view" is... an "inline view" is what you call it when you modify a from statement. A cte is what you call it when you do the thing I showed you up above. You can use any of these to "make a view."
 Select name, avg(salary) from table b where season between 1985 and 1990 That is exactly correct. First you would average the salary and then you would join. 
So what will be joined with what? &gt;Select name, avg(salary) from table b where season between 1985 and 1990 will be joined with &gt;Select player, team, year FROM table a group by name, team HAVING count(*) Is that right? So where would the "JOIN" operator come in. 
It'll take me some time to fully understand and grasp this concept. I somewhat understand - not fully though. Thanks for your help sir.
I think whats holding you back is that you don't realize that you can create a "view" with a query, such as select name, sum(salary) as salary from table group by name That view is *not* a table, and you can restrict access to the table so that a user can only see the view. The user can then query the view directly such as: select * from view Taking both of these concepts in step the full query would look like this: select * from ( select name, sum(salary) as salary from table group by name ) 
I'm not talking about how to create a view, I'm explaining what the concept is and how the same concept can be used to in a JOIN(), or a WHERE() or even a SELECT(). Consider this: select ( select sum(salary) from players where year = 2015 ) / ( select count(distinct playername) from players where year = 2015 ) as 'avg_player_salary' You don't even have a FROM. But you can do this: select ( select max(salary) from players where year = 2015 ) - avg_player_salary as 'difference between max and avg salary' from ( select ( select sum(salary) from players where year = 2015 ) / ( select count(distinct playername) from players where year = 2015 ) as 'avg_player_salary' ) a
Is "added" the same as "purchased"? Also, what DB are you using? So... Lets start with "Customers who purchased a product this month." select distinct customer from revenue where datediff(month, cast(getdate() as date), date) &lt; 1 Now, the datediff and getdate functions might not exist in your version of SQL. They'll be proprietary, but I think this will work in MS SQL. I'm not in front of my work PC, so I can't test. Also, you might need to switch around the getdate part and the date field to get a positive number here. Customers who did **not** purchase in the past month select distinct customer from revenue where customer not in (select distinct customer where datediff(month, cast(getdate() as date), date) &lt; 1) AKA, Select customers not in that first batch. 
Am I missing something here? I don't see a need to do a count or sum to obtain your required output. Considering your table name is my_tab select distinct a.fname,a.lname, CASE WHEN a.status='Available' THEN a.count_of_status END available_count, CASE WHEN b.status='Sold' THEN b.count_of_status END sold_count from (select * from my_tab where status='Available')a, (select * from my_tab where status='Sold') b where a.fname=b.fname and a.lname=b.lname and a.status&lt;&gt;b.status; Here's the SQL Fiddle - http://sqlfiddle.com/#!4/11044/5
In short, "from" clause indicates a result set. A column is not one, so "from Column" is semantically wrong. What combinations are you looking for?
In short: select column(s) from table So * means all columns, but you can select one or more columns by name. The reasoning is that once you select more than one column, the FROM clause would get complicated so: select [column1],[column2] from [Database].[dbo].[Table] is much easier to type and on the eye than: select * from [Database].[dbo].[Table].[Column1],[Database].[dbo].[Table].[Column2]
Sounds reasonable, sure. 
 Select q1.values ,q2.values from (query 1) as q1 , (query 2) as q2 where q1.key = q2.key
 Select TO_CHAR(SYSDATE, 'dd/mm/yyyy') 'Report Generation Date' , rep, manager, etc from data
Exactly what I was looking for - thank you!
The default state is that a job (project?) runs on it's specified schedule (certain week days, and I'm assuming that schedule is in another table or tables), but on specific days of the year, like holidays, most jobs will not run. As there may be some specific projects that exceptions to that, and you want a way to account for that. I would do this with three tables rather than the two you've got displayed. Project (as you have already created it) Project_holidays (INT Id Column, and Holiday_date date column) Holiday_Exceptions (Int ID column, Project_Id (FK), Holiday_ID (FK) ) That should be the minimum number of rows over all. It's similar to you plan of null project ID for universal holidays, but with a lot more design clarity because no one has to go look up in a design document what it means when the canceled dates column has a null project id.
can't diagnose your problem without seeing your GROUP BY clause
SELECT will get columns, if you want all columns you can write * instead. FROM will get from a dataset (Tables, Views, Tabular Function ...) So your 2nd example will fail just like SELECT [Database].[dbo].[Table] will fail.
What are you attempting to do? What are you attempting to do that SELECT * FROM [Database].[dbo].[Table].[ColumnName] Would be a better option than SELECT [ColumnName] FROM [Database].[dbo].[Table] ? 
as a follow up i got it working, you were 100% right, instead of comparing a subquery to the main table i made two subqueries and compared those two, making sure they were grouped at the same level before joining
I don't think it would be very hard to write your own logic. Something like CASE WHEN TRY_CAST(column as int) IS NOT NULL THEN 'column int' WHEN TRY_CAST(column as datetime) IS NOT NULL THEN 'column datetime' ELSE CASE WHEN (SELECT MAX(LEN(column)) FROM Table) &gt; 1024 THEN 'column varchar(max)' ELSE 'column varchar(' + CAST((SELECT MAX(LEN(column)) FROM Table) as varchar(4)) + ')' END END
AlwaysOn Availability Groups is what you should use indeed. Though it's not purely active/active, more like active/read-only with failover options. For availability groups you don't setup shared storage, both servers would use an independent copy of the databases. There is an edition requirement, however, for such configuration: Enterprise in prod and Developer in test.
Not crazy in depth but the field and software is pretty niche, not a lot of 3rd parties out there will know the layout and industry like I will. Thanks for the reply!
https://msdn.microsoft.com/en-us/library/ms173454.aspx
Based on what you've written, I guess this select Name, sum(Balance) From A Join B on A.Product = B.Product Group by Name
I want it as a column in the data returned as part of the query, not as a separate query entirely. A separate query entirely means nothing to me if the only thing it's returning is one line with the date that I need repeated tens of thousands of times.
Niche expertise is valuable. If there isn't a lot of competition with your knowledge, I'd pitch at least $150/hr. A generalist won't understand the data model as well as you, and is likely to make bad assumptions and write generic SQL.
Real-life example: In my current job, I've built a few DB-backed things... one of them is an employee roster. It used to be managed (by me) as one big HTML file, until I decided that I didn't want to do all that work to hopefully keep things consistent. There's a set of data that is the same (or close enough) from one record to the next - for example, first name, last name, login ID, job title, manager (et al.). I extracted all of the data and put it into a spreadsheet, then built table around it (we'll save normalizing for later). Next step is looking at how to handle all of the different data, and how that data needs to be accessible to the application(s) using it. Finding a book is good (to add to what /u/Feurbach_sock said), you'll get one set of information to start working from. Also check out Wikipedia ([Database Design](https://en.wikipedia.org/wiki/Database_design) for example), sites like [w3schools SQL Tutorial](http://www.w3schools.com/sql/), and even some of the DB sites (like [MySQL](http://www.mysql.com/)).
So it's showing how many digits can be left to the decimal and how many can be in the right. So 12.1 , 1234.23, 12345.5 are all valid numbers but 1234567.494 would not be, correct? What will happen if the number is 1234.345 - will it round up to make it .35? If not specified, do you know what Oracle will make the default numbers in parentheses as?
I think you are a little off. 5,2 means 5 digits total, 2 of which are after the decimal. So xxx.xx is (5,2) 1000 does not abide by (5,2). Inserting into numeric (5,2) will result in overflow. 123.45 does abide.
&gt;If a precision is not specified, the column stores values as given. If no scale is specified, the scale is zero. http://docs.oracle.com/cd/B28359_01/server.111/b28318/datatype.htm#CNCPT1832 But keep in mind the first number is the _total_ number of digits before AND after the decimal, not just before. So 12345.5 is invalid since it's (6,1) precision/scale. 1234.5 is also not valid since it's (5,1) but you are trying to change it to 1234.50 which is (6,2) and it will error. 1234.345 will get rounded to 1234.35 assuming the precision was (6,2), it will error if it was (5,2).
I'm using Oracle developer. Do you know if it's the same as MSSQL? And my mistake, i should've specified my database. 
 I'm using Oracle developer. Do you know if it's the same as MSSQL? And my mistake, i should've specified my database. 
I don't know how I missed that it my research. I got all the way through creating the two servers, the WSFC, install SQL then trying to enable AlwaysOn errors out as it's not Enterprise. Well, I'll have to shelf that for now as I need to get this out the door for a pending project but I might speak with our vendor to get some pricing. We are local gov't so we get a very competitive price and if I can reduce the number of total databases the cost of Enterprise licensing might be attractive.
Maybe I should be clearer in my reply. Subjectively for me, the new syntax is very hard to decipher. I keep reading that it is easier to see where the joins are, but for me the query structure is much harder to interpret and understand.. For simple queries I can just about follow it, but for more complex queries I would not have a clue how to write it.. it is just not clear.
Procedures are reusable bits of code that can take an input and can also provide output based on the input. Simple example is mathematical procedures... calculating discounts, special pricing, etc... for examples let's pretend you need a function to square a number. So you write a procedure that takes an INput (x) and provides an OUTput (`x * x`). No idea what flavor of SQL you're talking about... so I'll just assume Oracle PL\SQL: PROCEDURE squareNum(x IN OUT number) IS BEGIN x := x * x; END; Now that you have your procedure, you can use it in your code. Declaring a variable `a` and calling `squareNum(a)` to run the procedure on the number `a` and return the output. DECLARE a number; BEGIN a:= 23; squareNum(a); dbms_output.put_line(' Square of (23): ' || a); END; / http://www.tutorialspoint.com/plsql/plsql_procedures.htm If you've ever done functions in Algebra in school or any programming class, it's the same concept.
Wait you mentioned functions and I haven't learned that yet either. I will review that after functions but first I want to focus solely on procedures.
No worries - I wasn't being condescending at all - this video series is a jump start course and goes right into detail very fast, so should be relatively interesting quite quickly without starting off too slowly with the bits you already know. What I mean by fundamental concepts is around some of the things outlined in your question, and then on to sub queries, common table expressions, variable uses, building your own triggers and much much more... definitely worth a half hour a day for a couple of weeks, and you'll be flying in no time
If you can redefine your tables you will be bettyer off. Add a column [main_tbl].[StateID] INT NOT NULL Then have a table called [States] That has an INT ID, a NCHAR(2) called [Abbreviation] , and a NCHAR(50) called [Name] Scrub your data on input only storing the ID for the correct state. Invalid data gets held, corrected by somebody, and reapplied. That INT column in your main table now takes 4-bytes (MS SQL) every time so you save space. Also your tables are better normal form. You don't have to run UPDATE statements after the fact to clean your data.
looks good. If you want to structure it though in a star schema for faster BI queries you could do: FACT: EpisodeCharacter (i.e. each character in each episode = 1 line of fact table) DIMENSIONS: - Episode details (sk_episode_id) - Character details (sk_charachter_id) - Animie details (sk_anime_id) With cosplays, as the fact can map to multiple records, you can make that a snowflake dimension of dim_character like you have. Practically the only design difference in this case would be that the Animie table would join to the EpisodeCharacter table instead.
Ah thank you very much for the response! I will definitely take another look to improve it according to your suggestions!
Check the list in the left column: http://www.w3schools.com/sql/ Also CASE statements, which I don't see in that list. Like mac-0 said, the functions at the bottom may be the most useful for an analyst. I like when analysts have a grasp of indexes and log files too.
Okay. Thank you. I am looking for an entry level position involving sql. Will knowing this trick set me apart in job interviews?
 SELECT DOCUMENT_ID, LINE_NO, MAX(BOOL_VAL) AS BOOL_VAL, MAX(STRING_VAL) AS STRING_VAL FROM USER_DEF_FIELDS WHERE DOCUMENT_ID = 'S4546' GROUP BY DOCUMENT_ID, LINE_NO 
You are right, I spaced. I answered on about 2 hours of sleep. 
You are right, I spaced. I answered on about 2 hours of sleep
You are right, I spaced. I answered on about 2 hours of sleep
Get a certification. The books are cheap on amazon and if you need to prove you have the basic skills that should do it. It's also amazing what little things you learn that make the rest of your code better.
"Move" in terms of a query result? Use /u/Tabooyah statement. If you need permanent change - use update/delete.
Please learn how to format (this is not your first post).
try cross apply
I agree with this - CROSS APPLY ( SELECT TOP 10 ldap_users t_ldcs_candidate_room_draft b WHERE b.candidate_room_draft_id = a.to_draft ORDER BY whatever )
&gt;the DB was already corrupted when the backup was taken right? Indeed. `restore verifyonly` only verifies integrity of the backup file, not the database integrity.
Force users to select only Mondays :D Dropdown or something. Or scale up to weeks instead of days. No seriously. How would you start your chart with Monday, if user selected Tuesday as a start date? The only option would be to automatically fix the date user entered, making it most recent Monday regardless of what he chose. But that wouldn't look pretty.
What is Anime.Genre (int)? Is there another Genre table not shown that it'd be a FK for?
well, there are product codes with no product in the mapping table. This is because there are roughly 10k plus product codes and someone has just manually mapped them to their product names and its always out of date.
This looks promising. I am reading through it now. Thanks!
Hi, I've dealt with this before while trying to find the maximum number of simultaneous phone calls at a time for a given day using only a CallID and the Start / End times of the call. Maybe searching on that might provide some insight on how to figure the problem out. I'll dig through my saved code to see if I can find how I handled it... I'm pretty sure I did this via MS SQL Server (SSRS) though. Edit: I found the document I remember referencing: http://sqlmag.com/t-sql/calculating-concurrent-sessions-part-3 and here's the code I ended up using: WITH C1 AS ( SELECT dateTimeOrigination AS ts, +1 AS TYPE, ROW_NUMBER() OVER(ORDER BY dateTimeOrigination) AS start_ordinal FROM cdr WHERE dateTimeOrigination &gt;= '9/26/2016 00:00:00' and dateTimeOrigination &lt;= '9/26/2016 23:59:59' UNION ALL SELECT dateTimeDisconnect, -1, NULL FROM cdr WHERE dateTimeOrigination &gt;= '9/26/2016 00:00:00' and dateTimeOrigination &lt;= '9/26/2016 23:59:59' ), C2 AS ( SELECT *, ROW_NUMBER() OVER(ORDER BY ts, TYPE) AS start_or_end_ordinal FROM C1 ) SELECT MAX(2 * start_ordinal - start_or_end_ordinal) AS mx FROM C2 WHERE TYPE = 1 Basically the way it works is each time there is a "Start" event, it'll add 1 to the start_or_end_ordinal column. Each time there is an "End" event, it'll subtract one to the start_or_end_ordinal column. So at each start/end event, the ordinal column will have the actual number of concurrent events at that time.
&lt; le sigh &gt;
 CREATE TABLE dbo.Call ( CallID INT NOT NULL, StartTime DATETIME NOT NULL, EndTime DATETIME NOT NULL, CONSTRAINT pk_Call PRIMARY KEY (CallID) ) INSERT INTO dbo.Call VALUES (1, '2016-09-26 08:00', '2016-09-26 08:30'), (2, '2016-09-26 08:02', '2016-09-26 08:21'), (3, '2016-09-26 08:03', '2016-09-26 08:45'), (4, '2016-09-26 08:05', '2016-09-26 08:06'), (5, '2016-09-26 08:10', '2016-09-26 08:11'), (6, '2016-09-26 08:14', '2016-09-26 09:45'), (7, '2016-09-26 08:14', '2016-09-26 08:20') DECLARE @StartDate DATETIME = '2016-09-26 08:00', @EndDate DATETIME = '2016-09-26 12:00' ;With DimTime(Time) as ( SELECT @StartDate as DATETIME UNION ALL SELECT dateadd(minute, 1, Time) FROM DimTime WHERE Time &lt; @EndDate ) SELECT * INTO dbo.DimTime FROM DimTime OPTION (MaxRecursion 10000) -- Calls Per Minute SELECT Time, COUNT(Call.CallId) AS CallsPerMinute FROM dbo.DimTime LEFT JOIN dbo.Call ON DimTime.TIME BETWEEN Call.StartTime AND Call.EndTime GROUP BY Time -- Calls Per Minute Histogram SELECT CallsPerMinute, COUNT(*) Minutes FROM ( SELECT Time, COUNT(Call.CallId) AS CallsPerMinute FROM dbo.DimTime LEFT JOIN dbo.Call ON DimTime.TIME BETWEEN Call.StartTime AND Call.EndTime GROUP BY Time ) CallsPerMinute GROUP BY CallsPerMinute DROP TABLE dbo.Call DROP TABLE dbo.DimTime
My 2 cents would be to look at query tuning (like someone else said, query plans and indexes). There are different ways to write queries and often 2 ways will give you the answer and both will work on a thousand row table, but only one will work on a billion row table. 
Can you elaborate on your IT degree? Is it a 2-year degree from a technical school or a 4-year BA/BS from an accredited university? If it is the latter, your company is getting one hell of a deal. Go poke around on Glassdoor to see the going rate for entry-level SQL, DBA, and analyst positions.
I would do something like this: DECLARE @new_domain varchar(32) SET @new_domain = N'domain.new' SELECT 'IF(SUSER_ID('+QUOTENAME(@new_domain + '\'+stuff(SP.name, 1, charindex('\', SP.name), ''),'''')+') IS NULL)BEGIN CREATE LOGIN '+QUOTENAME(SP.name)+ + 'END;' COLLATE SQL_Latin1_General_CP1_CI_AS as SERVER_LOGINS, 'ALTER LOGIN [' + SP.name + '] WITH NAME = ['+@new_domain+'\'+stuff(SP.name, 1, charindex('\', SP.name), '')+'];' as DB_LOGINS FROM sys.server_principals AS SP LEFT JOIN sys.sql_logins AS SL ON SP.principal_id = SL.principal_id WHERE SP.type_desc IN ('WINDOWS_GROUP','WINDOWS_LOGIN') AND SP.name NOT LIKE '##%##' AND SP.name NOT LIKE '%NT AUTHORITY%' AND SP.name NOT LIKE '%NT SERVICE%'; Then you'll have to run the "server_logins" column once against the new server and for each DB you'll have to run the "DB_Logins" column. Haven't tested this out... so not sure if it's actually functional, but I don't see why it wouldn't work....
Just to add to this, you can run a checksum on the backup too. I like to have a Full backup on a weekend, diffs nightly, and T-Logs every minute. Prior to the full, a DBCC runs to evaluate the instances. 
so my date field is called [Accounting Month] do i put that at the @string= ? The reason i ask, is it keeps returning [Accounting Month] is an invalid column name when i try to run it
so my date field is called [Accounting Month] do i put that at the @string= ? The reason i ask, is it keeps returning [Accounting Month] is an invalid column name when i try to run it 
Please post the query you're using and the table definition (right click -&gt; script -&gt; create as)
I have a BS in Information Technology. Emphasis on data management. I'm definitely a newbie and learning a lot on the job. I'm kinda getting thrown tasks that I just have to own and get done. 
Query: declare @string varchar(32) set @string = [Accounting Month] SELECT TOP 1000 [Dealer Group ID Level 1] ,[Dealer Group Name Level 1], DATEFROMPARTS(LEFT(@string,4),RIGHT(@string,2),1) ,[Primary Cust SVOC ID] ,[Primary Cust Name] ,[Business Unit] ,[Product] ,[Revenue] FROM [Sales Tool].[dbo].[Master Table]
This should work then: SELECT TOP 1000 [Dealer Group ID Level 1] ,[Dealer Group Name Level 1] ,DATEFROMPARTS(LEFT([Accounting Month],4),RIGHT([Accounting Month],2),1) ,[Primary Cust SVOC ID] ,[Primary Cust Name] ,[Business Unit] ,[Product] ,[Revenue] FROM [Sales Tool].[dbo].[Master Table]
Without getting too deep into your schema, what scenario would cause you to have multiple "area" attributes for a single row? Embedding a CASE inside of a LISTAGG should look like this: ,LISTAGG(CASE WHEN es.area = '0' THEN 'RIGHT' WHEN es.area = '1' THEN 'LEFT' WHEN es.area = '2' THEN 'FRONT' WHEN es.area = '3' THEN 'BACK' WHEN es.area = '4' THEN 'TOP' WHEN es.area = '5' THEN 'BOTTOM' ELSE es.area END, ',') WITHIN GROUP (ORDER BY es.area) area In short, if we're using the [official documentation](https://docs.oracle.com/cd/E11882_01/server.112/e41084/functions089.htm#SQLRF30030), your CASE statement should make up the entirety of the measured expression.
got it :p
Also take a look at Managed Service Accounts. I haven't been able to try it out yet but they appear to make a managing individual domain accounts a thing of the past. https://technet.microsoft.com/en-us/library/dd560633(v=ws.10).aspx Group Managed Service Accounts are new in Windows Server 2012 and take it a step further by allowing you to manage groups of servers together. https://technet.microsoft.com/en-us/library/hh831782(v=ws.11).aspx Kerberos can be tricky to set up but, once it's working, is well worth it. The main thing you get from Kerberos is that users have the same permissions in the database no matter what front-end they are using, whether it's an application, SSMS, or something else.
He probably has something like this: DECLARE @Debug BIT &lt;QUERY HERE&gt; IF @Debug = 1 THEN BEGIN PRINT ... END And set the @Debug manually when in developing to 1 and to 0 in production
OH, so it's not comments and information in-line with the query, it's a bunch of data about the query at the end? How would it help you in the case of your query starting to return a bunch of 0 rows in the middle and you have to find which table is the culprit? I have to manually count how many messages down in the list it is until the first "0 rows" and then manually count that many temp tables into my query to find which table to look at.
I code commands. DECLARE @StartTime AS DATETIME2, @Debug BIT = 1 SELECT @StartTime = SYSDATETIME() -- DO SOME STUFF WAITFOR DELAY '0:00:00.123' IF @Debug = 1 PRINT 'Step 1: ' + CAST(DATEDIFF(MILLISECOND, @StartTime, SYSDATETIME()) AS VARCHAR) + ' ms' SELECT @StartTime = SYSDATETIME() -- DO SOME MORE STUFF WAITFOR DELAY '0:00:01.059' IF @Debug = 1 PRINT 'Step 2: ' + CAST(DATEDIFF(MILLISECOND, @StartTime, SYSDATETIME()) AS VARCHAR) + ' ms' There is additional performance-related information that you can turn on using SET STATISTICS IO ON SET STATISTICS TIME ON but this information will not be available to you as part of your SSRS dataset (and neither will the PRINT statements from above).
I've just had to tell a developer that the reason the application is slow and waiting for IO is because of the inefficient nested sub query they wrote.
Look at [this](http://stackoverflow.com/questions/2965521/oracle-and-triggers-inserted-updated-deleted) page. You need to reference which buffer you're looking at, since this is an insert trigger and not an update trigger, the only buffer you care about is the NEW, since there's no existing row. It's been an age since I wrote a trigger, Oracle or otherwise so watch for syntax errors CREATE OR REPLACE TRIGGER triggerthing BEFORE INSERT ON xtab3 REFERENCING NEW AS new_buffer FOR EACH ROW WHEN (new_buffer.field1 = 'thisvalue') -- For every new row where you are adding a row with this value BEGIN IF INSERTING THEN -- new row with this value, so add 5 to all existing rows UPDATE xtab3 SET Price := Price + 5 WHERE tvalue = thisvalue AND xtab3.uniquekey != new_buffer.uniquekey; -- since this is happening before -- this should not be necessary -- but helps documentation clarity END IF; END;
Start with unpivoting the question columns into rows. Since you have a variable number of question columns, a dynamic SQL approach is probably best for re-usability. You will then have a denormalized dataset that you can normalize as needed. CREATE TABLE #staging_table ( ID INT NOT NULL, Respondent VARCHAR(100) NOT NULL, Date DATE NOT NULL, [Question1.1] VARCHAR(100), [Question2.1] VARCHAR(100), [Question3.1] VARCHAR(100), [Question4.1] VARCHAR(100), [Question5.1] VARCHAR(100) ) INSERT INTO #staging_table VALUES (1, 'John', '2016-09-27', 'Response1', 'Response3', 'Response4', 'Response7', 'Response9'), (2, 'Jane', '2016-09-27', 'Response2', NULL, 'Response5', 'Response7', NULL), (3, 'Tom', '2016-09-27', NULL, 'Response3', 'Response6', 'Response8', 'Response10') DECLARE @table_name SYSNAME SELECT @table_name = '#staging_table' DECLARE @SQL NVARCHAR(MAX) SELECT @SQL = ' SELECT ID, Respondent, Date, Question, Response FROM ' + @table_name + ' UNPIVOT ( Response FOR Question IN ( ' + STUFF(( SELECT ', [' + c.name + ']' FROM sys.columns c LEFT JOIN ( SELECT i.object_id, i.column_id FROM sys.index_columns i WHERE i.index_id = 1 ) i ON c.object_id = i.object_id AND c.column_id = i.column_id WHERE c.object_id = OBJECT_ID(@table_name) AND i.object_id IS NULL AND c.name LIKE 'Question%' FOR XML PATH(''), TYPE).value('.', 'NVARCHAR(MAX)'), 1, 2, '') + ' ) ) X' PRINT @SQL EXEC sys.sp_executesql @SQL DROP TABLE #staging_table
In general, you should *expect* that working with denormalized data is tedious and repetitive. That's just the nature of the beast. The operation you're looking for is UNPIVOT... which MySQL doesn't support. It's not too difficult to write a script to do it before you import it, however, especially if you've got a language that understands CSV like PowerShell or Python. Say this is the input file: ID,Respondent,Date,Question1.1,Question2.1,Question3.1 1,John,9/27/2016,Response1,Response4, 2,Jane,9/27/2016,Response2,Resonse5,Response6 3,Tom,9/27/2016,Response3,,Response8 Here's a PowerShell script: $InputFile = 'C:\Path\To\Your\Input\File.csv'; $OutputFile = 'C:\Path\To\Your\Output\File.csv'; # Import the file as a Csv file $Csv = Import-Csv -Path $InputFile; # Get the headers that aren't ID, Date, or Respondent. We don't care about the order, but I'm sorting because it might be easier to work with in the file. $Questions = $Csv | Get-Member -MemberType NoteProperty | Where-Object Name -NotIn ('ID','Date','Respondent') | Select-Object -ExpandProperty Name | Sort-Object; # Do the unpivot operation $Unpivot = foreach ($Record in $Csv) { foreach ($Question in $Questions) { if (![System.String]::IsNullOrWhiteSpace($Record.$Question)) { [pscustomobject]@{ ID = $Record.ID; Respondent = $Record.Respondent; Date = $Record.Date; Question = $Question; Response = $Record.$Question; } } } } # Write the output as UTF8 because that's what MySQL probably expects $Unpivot | Export-Csv -Path $OutputFile -NoTypeInformation -Encoding ASCII; Here's the output I get: "ID","Respondent","Date","Question","Response" "1","John","9/27/2016","Question1.1","Response1" "1","John","9/27/2016","Question2.1","Response4" "2","Jane","9/27/2016","Question1.1","Response2" "2","Jane","9/27/2016","Question2.1","Resonse5" "2","Jane","9/27/2016","Question3.1","Response6" "3","Tom","9/27/2016","Question1.1","Response3" "3","Tom","9/27/2016","Question3.1","Response8" There's a lot of caveats here with PowerShell (`Export-Csv` always exports with double quotes, line endings may be CRLF, UTF8 may encode with a BOM, not specifying an encoding may result in UTF-16, may choke on files that are larger than 10 or 20 megabytes) so Python is probably preferable to use, but I'm more immediately familiar with PowerShell. You should be able to use Python's `csv` library to do essentially the same sort of transformation. 
I suppose I'm just wary of my skills since I'm so new. I look at data analyst positions and sql jobs but I don't have near enough the experience that they want. Also, I feel like I am doing so many things it's spreading me so thin as far as being competent goes. I'm doing a sql reports, writing passable at best perl, tech support, spreadsheets in excel, server administration and monitoring, and instead of becoming really good at a few things I'm skimming the surface of most of these things. Hopefully my skimming will be enough to get a raise since I doubt it'll be enough to get a job anywhere else.
One of the things that I know from [MS SQL] and [My SQL] is to never rely on indexes to sort your result set. Use ORDER BY for that. You might be lucky and generally get results back in what seems like an order. Without warning you will suddenly get results back in what looks like random order. You will get frustrated by that.
No. Functions can be used in a SELECT, for example, to do things like performing a complex formatting on a column.. You are right in that if you have permissions for everything procedures might not be required.
I've seen this kind of pattern in ETL before, except there are more parameters (CRUD record counts, "batch" or "version" identifiers, execution duration) and the parameters are OUTPUT back to the parent. If it is in SSIS, the values can be pushed to a variable and then events are utilized to run a SQL statement/stored procedure to log those values. If it is in SSMS, then the parent query/script can have variables setup to do whatever you want with the information (print to console, SELECT statements, logged). The only downside is that if the pattern changes (more parameters are added, parameter names changed) then there are tens, or hundreds, or thousands of objects that *should* be updated. To get around that, a coworker suggested we get rid of all executables like stored procedures, move the SQL code to tables, and then execute our ETL as concatenated dynamic SQL on the fly so that we could in-line whatever logging code we wanted. I quickly grabbed a DBA who then utterly and mercilessly destroyed my coworker's idea.
It's very convenient and way easier to understand and read when you do longer or more complicated queries. The example you have is just for you to learn how to write procs. It's unlikely you'd have a proc in the real world that does a one line update with no logic. When you're learning multiplication you'd probably wonder why you need to learn it when you have an example of 3 x 2, 3+3 is just as simple! Once you have to do 3+3+3+3+3+3+3+3, knowing multiplication starts to make sense. Once you encapsulate queries in a proc then you can apply concepts to that proc that you wouldn't be able to bits of code (Like security). With regard to functions the above applies. With a single line it doesn't matter, but lets say that update takes 100 lines to do what it needs to do? And lets say you need to run it with 10 different names and ids. You could copy the 100 lines of queries 10 times and change the values for each set of lines... or you could encapsulate those 100 lines in a function and just call that function 10 times, turning what would be 1000 lines of queries into 110. On top of that, when someone reads your code, instead of reading and trying to understand 100 lines of code, they can just read the function name and that 100 lines becomes a black box that they don't care about, they just know that you input values and it spits back your result.
Can you help me understand this piece of code? I understand this first part, but I don't get what's going on / why it's necessary to put the second part (after commit/end) &gt;CREATE OR REPLACE PROCEDURE prodname_chg_sp &gt; (p_id IN bb_product.idproduct%TYPE, &gt; p_name IN bb_product.productname%TYPE) &gt; IS &gt;BEGIN &gt; UPDATE bb_product &gt;SET productname = p_name &gt;WHERE idproduct = p_id; &gt;COMMIT; &gt;END; &gt;EXECUTE prodname_chg_sp(2,'CapressoBar Model #388'); &gt;SELECT idproduct, productname &gt;FROM bb_product &gt; WHERE idproduct = 2; 
The idea is to keep it rather simple and not to overdo it, because it then becomes useless. At least that's my experience, based on working with a framework that a dozen people work on and barely any project uses 1/10th of its features, because they are too complex to actually use efficiently. Ain't nobody gonna read through 30 pages of documentation and provide 20 parameters just to log how much time a given piece of code takes. &gt; get rid of all executables like stored procedures, move the SQL code to tables, and then execute our ETL as concatenated dynamic SQL on the fly so that we could in-line whatever logging code we wanted. Kill me now. Making everything as flexible as technically possible isn't always the right choice.
So the part before the END is creating the procedure. In the real world, this would only ever be done once and that procedure would be saved in the database and could be called whenever needed. The EXECUTE is then calling that procedure. This is the part that would appear in your codebase all over the place with different parameters instead of repeating the first statement over and over again. The SELECT part is just displaying the data so you can see that it's changed, I guess that's just there to show you it worked - you wouldn't actually do this after every execute in the real world.
Is [Accounting Month] a field with a date datatype? If not you'd need to look at casting/converting it to one: use [Sales Tool] SELECT ,[CustID] ,[CustName] ,[Business Unit] ,[Customer Level] ,[Product] ,max(CONVERT([Accounting Month], date)) as [Max Date] , Min(CONVERT([Accounting Month], date)) as [Min Date] , (SELECT MAX(CONVERT([Accounting Month])) FROM [Complete Master Table] WHERE [Accounting Month] NOT IN (SELECT MAX([Accounting Month]) FROM [Complete Master Table])) as [2nd_Max] FROM [Sales Tool].[dbo].[Complete Master Table] group by ,[CustID] ,[CustName] ,[Business Unit] ,[Customer Level] ,[Product] Also I'm not sure if your subquery for 2nd_max is doing what you're after - what is that supposed to do?
MySQL doesn't have the UNPIVOT syntax (or PIVOT).
It shows example call to the just created procedure and then a select to show that the proc worked. 
In T-SQL (the Microsoft implementation) you don't actually need to use the keyword EXECUTE on a stored procedure but it's still good practice to. COMMIT is a way of telling the DB to actually apply whatever change has been made. In your example omitting it wouldn't actually do anything since that's the end of the statement anyway, so it's there just because it's a good habit to be mindful of COMMITs. If you had a bit of SQL that was several statements, and the first one failed, if they were all part of the same transaction (i.e., didn't have separate COMMITS for each one), the whole lot of them would not be applied to the db - this can be good or bad, depending on what you're doing. Functions are never executed. They sit within another SQL statement, so a more typical example would be "SELECT functionarea(height, width) from shapes", where height and width are fields in the table Shapes. Functions always return values (including entire tables), and they can't make changes to the database - meaning you can't call a procedure from a function but can call a function from a procedure. Procedures can make changes, and don't have to return values, which means you can't have them as items in a SELECT or WHERE clause in a query. In your example, you could create a function that would take the phone number string and format it how you wanted it. But that would exist only in the context of the query you're running, and couldn't actually permanently change the data. A procedure could permanently change the phone number. These are things you could try asking your teacher - you pay tuition in return for tutelage. 
I love using this too. Also good as you can see where a long running procedure has run up to.
So, The first query selects all categories and makes an array of them in my PHP script. The 'lft' and 'rgt' fields in the category table indicates which subcategories the parent category contains. e.g. Parent Category with id='100' has 'lft' field value = 101 and 'rgt' field value = 103. This means that categories with IDs 101 through 103 are subcategories of the category with id 100. Hard to explain. Gets messy quickly. Anyway, To select the total sales of each category and subcategory I would need to first select the categories. This would give me a result like: 'id','text','lft','rgt' with values for example: '100','Computers','101', '103'. Let's call this query q1 then. I then would need to reference this when counting the total sales price, call this query q2 - like so: WHERE lft BETWEEN q1.lft AND q1.rgt AND date.... etc. How can I reference q1.lft and q1.rgt in q2? When I tried the examples from above replies I get: ERROR: invalid reference to FROM-clause entry for table "q1" HINT: There is an entry for table "q1", but it cannot be referenced from this part of the query.
There are two ways to do a case. One is like an IF-ELSIF-ELSE: case when [condition] then [commands] when [condition] then [commands] else [commands] end The other one is like a switch: case [variable] when [value] then [commands] when [value] then [commands] else [commands] end You were using the second one where value is expected but you put "IS NULL" which is an incomplete boolean expression. If you ever need to map values again you can use decode in Oracle: Select SUM(decode(vehicles.mileage,NULL,1,0)) NO_MILEAGE, SUM(decode(vehicles.price,NULL,1,0)) NO_PRICE From [data] You trade explicit readability for implicit readability through shorter queries.
A bit tricky without seeing related data as there are levels of issues that I foresee that need to be answered for a full understanding of what you have. However, at face value... ..Your description below does not quite make sense.. *Parent Category with id='100' has 'lft' field value = 101 and 'rgt' field value = 103* Relationship should be *Parent &gt; lft &gt; rgt* where 100 is parent of 101 is parent of 103 .. am I right? But your description does not imply that unless I misunderstand, your description implies that 101 and 103 are children of 100. You have to square out in your head how the relationships mesh.. if not you will not fully understand the issue..or you do get it and I misunderstand your description Then you would need to match 1 or more of those IDs across the two independant queries to marry the data in the tuple. You may be able to marry it at the lowest level (rgt) or you may have to marry the data at all levels.. again, without seeing the data it is hard to know.
In all that I have read and understood, this does not make sense.. You are saying you have two fields to indicate the same level subcategory..lfg (101) and rgt (103). lets assume that is correct... you need to pull those values from both queries and marry them to get the required tuple.
Maybe I've got it wrong. I need to investigate this system further. But I don't know what the hell the lft and rgt fields are if they're not what I say they are. I mean. ID, Name, lft,rgt,parent .. what does the parent field need to do there if the hierarchy is like you interpreted it?
This is actually closer to what I wanted... I would just write it once, and then run after new imports, and add new discrepancies as they come in. Thank you!
The best ad-hoc way of doing that that I know is to configure an alert for tempdb database to trigger when data size is over certain threshold and make this alert start a SQL Agent job, that would capture active requests into a table.
I guess I'm a little confused why you hard coded those dates in the average calculation. My guess is it that your getting an error when subtracting '1899-12-30' from whatever data is in acceptance_dt. select distinct right( replace(stock_part_no, '-', ''), 9 ), count( 1 ), max(acceptance_dt), min(acceptance_dt), (max(acceptance_dt) - min(acceptance_dt)) / (count(acceptance_dt)-1) as avg_date from test.test_dd250_uii where stock_no_type_cd = 'PRODUCT STOCK NUMBER' and acceptance_dt is not null group by right( replace(stock_part_no, '-', ''), 9 );
Ok so it seems that the database stores string_val and bool_val as different 'ID's so they show up as duplicates in different rows: https://imgur.com/O980rpB Can you think of any way to combine all IDs?
You can either find MAX for each row using OVER clause `MAX(COUNTSUB.gb2countquant) OVER (PARTITION BY COUNTSUB.eip_location, COUNTSUB.eip_itmtype)` or use same OVER clause to sort your data set within defined windows and select only first rows: ;WITH COUNTSUB AS ( SELECT eip_location, eip_itmtype, eip_unitprice, SUM(eip_quantity) as gb2countquant FROM pluspgbtrans WHERE eip_linestatus = 'RELEASED' AND eip_revtype = 'MRC' GROUP BY eip_location, eip_itmtype, eip_unitprice ) , COUNTSUBRN AS ( SELECT eip_location, eip_itmtype, eip_unitprice, gb2countquant, ROW_NUMBER() OVER (PARTITION BY eip_location, eip_itmtype ORDER BY gb2countquant DESC) RN FROM COUNTSUB ) SELECT c.eip_location, c.eip_itmtype, c.eip_unitprice, c.gb2countquant AS ModePrice FROM COUNTSUBRN c WHERE c.RN = 1 
I have been in the same situation as OP, and I did exactly what u/nvarscar suggests. I got the stored procedure I run from this article: http://www.littlekendra.com/2011/02/01/whoisactive/ from Kendra Little; procedure written by Adam Machanic: [sp_whoisactive](http://sqlblog.com/blogs/adam_machanic/archive/2012/03/22/released-who-is-active-v11-11.aspx) It showed exactly what was hitting tempdb; in my case, a query with an infinite loop, run as a scheduled task. Edit: credit where it is due.
is that just the subquery? im sorry, im a noob. i dont know where to put that in terms of my query. is it a new unique query?
Thanks for the help. I'm using the hardcoded dates because when I used ... sum( acceptance_dt ) / count(1) as avg_date ... I would get the error "function sum(timestamp) does not exist." It won't even work on my smaller test set. But subtracting timestamps from the hardcoded timestamp would get me the average on the test set. It's when I use it on the full set is when I would get the error. I'm limiting on all non-null acceptance_dts and the min &amp; max for all the acceptance_dts are '2001-06-14' and '2016-07-7', so all the acceptance_dts should fall between these two dates. The recommended method above gives me a similar error ( "ERROR: operator does not exist: timestamp without time zone / bigint" ). Also, we didn't want the mid-date between the top and the bottom, but rather the mean on all the acceptance_dts for each part_no. 
Nine times out of ten, the culprit is that someone has written a lot of #temp tables to use as intermediate steps in a more complicated query in a stored procedure somewhere, and there are large inserts being put into those #temp tables. #temp tables exist in TempDB. 
really good question, i guess im leaving room for error, so if there is two modes i would just want to return a scalar, even if thats not technical correct
I included ID only in the example I just posted, but when I first ran the query I did not select ID. 
I've got, well... a lot of things to try from my googlefu this afternoon, any personal recommendations for identifying said queries/tables?
so, im an idiot who has never done CTE ....could you put that in my query for me lol. because i cant figure out how ot place it and this would be a godsend at work for me
yes, ive never used a CTE before, so i have 0 clues oon where to put it. after googling, i did the following 1) i put your CTE above my original query posted 2) did nothing else. everything else stayed the same it errors out on line 34. "From the minmax" as invalid syntax
Re-checked the statement, added one missing row: FROM [Sales Tool].[dbo].[Complete Master Table] It supposed to work instead of your original query, not together.
Citext is great.
Thanks for the help. What kind of syntax would I use for the 'zones match' 'dates match' 'time of day match'? I only started working with SQL very recently so any help is appreciated, thanks again.
worked BUT, its returning tons and tons of dups with varying dates http://imgur.com/a/IZGGQ thanks a ton btw, if you want reddit gold or something, i dont mind throwing it your wya
If you want to bring everything and just list the average of each Zone, Date and Hour then the following is enough: SELECT Zones, Dates, Hours, AVG(price) FROM pricedatabase GROUP BY Zones, Dates, Hours If you want to get specific information then it will be something like: SELECT Zones, Dates, Hours, AVG(price) FROM pricedatabase WHERE Zone = 'houston' AND Dates = '2016-09-28' AND Hours = '16:00' GROUP BY Zones, Dates, Hours 
You sure you didn't forget to add the last row? That row limits the CTE to only one row per group. AND r.RN = 2 
Thanks for the help! I guess it's one thing learning the syntax, but I'm still figuring out how to apply it.
I'm glad you got it working!
yup. have that row. its the 2nd max date that is throwing it off. the other two fields are the same for every duplicate. its creating a row for every potential 2nd max date. so if i have date for a year. it is creating 12 rows with a 2nd max date of jan, feb, etc
That's kinda strange, it works perfectly fine for my generated set of data. Ok, let's try this. Run this and verify that there's no duplicates that you see. this should show only second max entries: SELECT * FROM ( SELECT * , ROW_NUMBER() OVER (PARTITION BY [CustID],[CustName] ,[Business Unit],[Customer Level],[Product] ORDER BY [Accounting Month] DESC) as RN FROM [Sales Tool].[dbo].[Complete Master Table] ) a WHERE RN = 2 Try the whole statement too, I moved the filter to the CTE instead of it being in a JOIN (which should make no difference...) ; WITH R as ( SELECT * FROM ( SELECT * , ROW_NUMBER() OVER (PARTITION BY [CustID],[CustName] ,[Business Unit],[Customer Level],[Product] ORDER BY [Accounting Month] DESC) as RN FROM [Sales Tool].[dbo].[Complete Master Table] ) a WHERE RN = 2 ) , MinMax AS ( SELECT [CustID],[CustName] ,[Business Unit],[Customer Level],[Product] , MAX([Accounting Month]) as MaxAM , MIN([Accounting Month]) as MinAM FROM [Sales Tool].[dbo].[Complete Master Table] GROUP BY [CustID],[CustName] ,[Business Unit],[Customer Level],[Product] ) SELECT mm.[CustID] ,mm.[CustName] ,mm.[Business Unit] ,mm.[Customer Level] ,mm.[Product] ,mm.MaxAM FirstMaxAccountingMonth ,r.[Accounting Month] SecondMaxAccountingMonth ,mm.MinAM MinAccountingMonth FROM MinMax mm LEFT OUTER JOIN R r ON mm.[CustID] = r.[CustID] AND mm.[CustName] = r.[CustName] AND mm.[Business Unit] = r.[Business Unit] AND mm.[Customer Level] = r.[Customer Level] AND mm.[Product] = r.[Product]
And just in case your jobs have SQL in them - SELECT j.job_id, s.srvname, j.name, js.step_id, js.command, j.enabled FROM msdb.dbo.sysjobs j JOIN msdb.dbo.sysjobsteps js ON js.job_id = j.job_id JOIN master.dbo.sysservers s ON s.srvid = j.originating_server_id WHERE js.command LIKE N'%KEYWORD_SEARCH%'
Why create functions when you program? You could just copy and paste the same stuff over and over again if you're not using recursion. Same idea.
This is a fundamental part of MS SQL server. If you install SSMS on the client machines, they can connect to the instance on your server PC. Just remember to set up a SQL account that isn't sa for client access.
Thanks for the quick response mikey but sorry, i didn't phrase my question very clearly! I want to create that database on the server but the data from that database would be accessed by an app which i will create using c#. So what(if any) configuration should i change on SSMS to let me do that, and if anyone knows how using c# could i do that connection, namely what would be my Sqlconnection parameters in c#. Thanks (Sorry if this sounds really stupid but I'm fairly new to SQL/Databases/Networking but due to a University project i need to start learning by doing. I don't have much time to study them from top to bottom)
www.sqlsaturday.com There was one about a year ago. Usually they're once a year - at least in my area. Either way, seek out your local SQL PASS chapter.
In my experience, massive tempdb usage is caused by DB maintenance. Index rebuilds, statistics updates, DBCC CHECKDB, etc. The built in plans are stupid and do everything every time. If it's a problem, consider looking at Ola Hallengren's scripts for index maintenance and integrity checking. [Here](http://dba.stackexchange.com/questions/13911/how-to-find-the-sql-statements-that-caused-tempdb-growth) is what looks like a good query for looking at the stats. You can use the queries [here](https://technet.microsoft.com/en-us/library/ms176029\(v=sql.105\).aspx) to help monitor tempdb space in real time. You can also take a look at the tempdb size/growth history, which is a canned report in SSMS. Things to look for are queries with large CTE expressions, queries that use large table valued parameters or TVPs with varchar(max)- or varbinary(max)-like fields, queries that aggregate very large quantities of data, etc. And in the end, remember: If you're sure it's not DB maintenance and none of your application's queries can be rewritten and it's not doing something stupid like opening 1000 sessions and not using them and leaving the temp tables unable to be cleaned up, then *that's the amount of tempdb space your application requires*.
Look into [sp_who2](http://sqlserverplanet.com/dba/using-sp_who2). Direct the output into a #temp table (!) and then you can filter on the results to look for various things. Things like who is running processes right now, how long have each of those processes been running, and which database are those processes running from? If something is taking TB's then it's likely to be running for a long-arse time. For the long-run, please look to setup some sort of monitoring of your environment such that you know what was being executed at which time. Whether it's logging in SSIS packages or hitting the system DMVs every 30 seconds and logging the results, get something. Then you'll be able to track usage and see how effective your tuning efforts have been, something that equates to $$$ when it comes to performance reviews.
The backup works just fine with SQL Server Express for manual backups. What is missing is SQL Agent. For automated backups, you can set up a simple script file and call it using windows task scheduler. [Example.](http://stackoverflow.com/questions/493886/sql-server-automated-backups/493952#493952)
Oh that's perfect thank you so much.
Hi, So I just went through the shop structure with the creator of it. It was like I understood the thing... A category with id 100 has subcategories from value lft to value rgt. So again, let's say lft value is 101 and rgt value is 103. Then categories with ID 101,102,103 are children of the category with id 100. https://en.wikipedia.org/wiki/Nested_set_model However, it looks like it's hard getting this thing wrapped up as I don't know how to f***ing join them together.
How do you join 2 tables like that? 
from PM I would be playing around with this line `AND orderdetail_category IN (SELECT id FROM category.categories WHERE lft BETWEEN 44 AND 65)` either removing it or changing 44 to 1 - I can only guess without the data to play with.
I apologize for the omission, I mentioned her name because of the article.
It was quite a piece of work, two temp tables referencing each other. I've still not found the author of that gem.
First you need to create Endpoints in both servers: --Endpoint Main CREATE ENDPOINT [Ponto_mirror] STATE=STARTED AS TCP (LISTENER_PORT = 1101, LISTENER_IP = ALL) FOR DATABASE_MIRRORING (ROLE = PARTNER, AUTHENTICATION = WINDOWS NEGOTIATE, ENCRYPTION = REQUIRED ALGORITHM RC4) --Endpoint Mirror CREATE ENDPOINT [Ponto_mirror] STATE=STARTED AS TCP (LISTENER_PORT = 1101, LISTENER_IP = ALL) FOR DATABASE_MIRRORING (ROLE = PARTNER, AUTHENTICATION = WINDOWS NEGOTIATE, ENCRYPTION = REQUIRED ALGORITHM RC4) Then you need to restore the database and the log in the mirror server but as "WITH NORECOVERY" After that you need to point the mirror to the main server: ALTER DATABASE [DBNAME] SET PARTNER = 'TCP://&lt;Main server domain&gt;:1101' Then point the main to the mirror: ALTER DATABASE [DBNAME] SET PARTNER = 'TCP://&lt;Mirror server domain&gt;:1101' That should work
OK, cool thanks. I'm restoring again with norecovery, will try your suggestions once complete. Do the mirror endpoints need to be recreated once the DB is restored? They should be independent of the DB right? I just need to a) make sure that the ENCRYPTION matches, and then run the alter database bit on each end to get them to mirror properly. 
No need to recreate the endpoints, those are part of the server, not the databases. The order of the alter database us important, you need to point the mirror server (the one in recovery) to the main 1st and then point the main to the mirror. Once my mirror wasn't working properly and I ran the following in both servers: GRANT CONNECT ON ENDPOINT::Ponto_mirror TO PUBLIC I'm not 100% sure how secure that script is, but solved my issue.
 Select * From movie Join sessions on sessions.movieid = movie.movieid Join branch on branch.branchid = sessions.branchid Something like that should get you started
I totally agree, boss man is being lazy. It's gonna bite him in the ass.
Have you tried the "GRANT CONNECT" command on your mirror?
Could you give a little more information about what sort of data you generate and how you want to access it?
As close as I can come to not doing your homework for you.. To get Movie and classification details linked where a given schedule and classsifcation are true you need to link everything together. What you have here is a classic many to many situation where each movie can appear at one or more braches, and each branch can show one or more movies. tblSessions is the joining (middle) table that allows this many to many relationship. SELECT FirstTable.dataelement1 , FirstTable.dataelement2 , SecondTable.dataelement1 , SecondTable.dataelement2 FROM FirstTable INNER JOIN MiddleTable ON MiddleTable.FirstID = FirstTable.FirstID INNER JOIN SecondTable ON SecondTable.SecondID = MiddleTable.SecondID WHERE Firsttable.Criteria = 'Value' Secondtable.Criteria = 'Value' Your second question about changing the straight join to a subquery is pretty straight forward, I'd do it with an exists subquery. SELECT singetabledata.field1 , singletabledata.field2 FROM singletabledata , helpertable WHERE singletable.foriegnkey = helpertable.primarykey AND helpertable.criteria = 'criteria value' First, is more clearly written as SELECT singetabledata.field1 , singletabledata.field2 FROM singletabledata INNER JOIN helpertable ON singletable.foriegnkey = helpertable.primarykey WHERE helpertable.criteria = 'criteria value' Because that helps separate what puts these two tables together every single time you use them from the criteria you need for this particular instance. Both methods are valid syntax, the second just has better clarity. The reason you can easily make this a subselect is that you're not returning any data from the helper table, so you could just as easily write this as SELECT singetabledata.field1 , singletabledata.field2 FROM singletabledata WHERE EXISTS ( SELECT 1 FROM helpertable WHERE singletable.primarykey = helpertable.foriegnkey AND helpertable.criteria = 'criteria value' ) Another way to do it might be SELECT singetabledata.field1 , singletabledata.field2 FROM singletabledata INNER JOIN ( SELECT helpertable.foriegnkey FROM helpertable WHERE helpertable.criteria = 'criteria value' ) VTable ON vTable.foriegnKey = singletable.primarykey Now if you needed to actually show what the movie in question was, I think your original select without a subquery makes the most sense.
Give your tables unique aliases: select a.name, b.name from foo as a join bar as b on a.key = b.key
Double join: SELECT a.ProjectID, b.Name, C.Name FROM Project a JOIN Employee b on b.ID = a.ProjectManagerID JOIN Employee c on c.ID = a.SalesLeadID I'd link a SQL fiddle but it's crapping out every time I try to run it. ProjectID|ProjectManagerID|SalesLeadID :--|:--|:-- 1|10|11 2|10|12 3|13|11 Name|ID :--|:-- John Projman|10 Billy Sales|11 Carla Sales|12 Lucy Projwoman|13
 Declare @User Table ( id int identity(10000,1) , FullName varchar(50) ) Declare @Project Table ( id int identity(20000,1) , ProjectManeger_ID int , SalesLead_ID int ) Insert @User Values ('John Smith') , ('Mary Jonson') Insert @Project Values (10000,10001) Select * From @User Select * From @Project Select p.id , PrjectManager = u1.FullName , SalesLead = u2.FullName From @Project p Inner Join @User u1 on u1.id = p.ProjectManeger_ID Inner Join @User u2 on u2.id = p.SalesLead_ID 
&gt; how you want to **access** it ...
Thanks. I should have thought of that...
Try to avoid using temporary tables at all cost to use inside intermediary steps in complex SQL queries and using them for large INSERT statements.
The nested sub queries may be poorly written and slow the server to a crawl. Use them if you know what you're doing and not just for convenience.
Am I missing something here? Why not just use a cross join? SELECT T.Name.id,T.name,S.ProjectManager, p.Sales Lead FROM Name T CROSS JOIN Project S 
how slow would the hardware have to be for a table structure copy to be this slow? Even on an rpi-zero does'nt take me 5 minutes..
I hear ya. But the only other thing I can come up with is the base table (from the like) has a bunch of triggers or something and so has locked the query. But the way OP framed the question it seems like everything is very slow. Maybe corruption somewhere?
maybe it's active whilst being copied (so locking like you said). Sounds like there is too much we are not being told
If you have more than one row in a table with the same monthly penetration rate *and* product *and* dealer group ID *and* accounting month, you are going to get duplicates. Also you have the product join condition twice. Edit: this gets to the concept of a candidate key, meaning a column or group of columns that can uniquely identify a row. You almost always want to join two tables based on a candidate key.
Without even looking into that hot mess of a statement - left join your final table to the penetration table ;Group by every single column of your final table ; take Max of whatever columns you need from the penetration table; then run your final in summary report on the result of that
im really not sure what you mean, in my above example if there were two records that had 4 as the quantity i would want to return a record for each of those prices
It looks like with matlab you are pretty free as it support a wide range of DB conections ([page1](https://www.mathworks.com/products/database/?requestedDomain=www.mathworks.com), [page2](https://www.mathworks.com/help/database/ug/database.html)). While it sounds like you don't really need a relational database, I would stick with one anyway. The new-user/non-programmer documentation and tutorials will be better. This leaves you generally with mysql and postgresql. As databases go both will have the functionality necessary. You have to get into some fairly complex structures to see one fail and the other succeed. For gui tools you will likely be using 'pgadmin III' or 'mysql Workbench.' Again they are different and the same. Both will free you from the command line if you dont want to be there. There are of course other GUI tools. Here is a big list for [postgres](https://wiki.postgresql.org/wiki/Community_Guide_to_PostgreSQL_GUI_Tools). I don't have a similar one for mysql. As to which one? Again I don't think it is going to matter. Personally I chose postgres for everything I have an option. From the command line I find postgres to be more professional feeling but as I said for you, that is likely to be the only difference you ever notice. One other note, while mysql [documentation](http://dev.mysql.com/doc/refman/5.7/en/select.html) is good I consider postgreSQL [documentation](https://www.postgresql.org/docs/9.6/static/sql-select.html) to be better.
can you give an example, i genuinely don't know where recursion could be useful in sql 
Another question that I didn't want to make another thread for. What's the point of making a variable for "USER" and "SYSDATE" ? For example [in this video](https://www.youtube.com/watch?v=i5fw-67P3XY), you can see his code. Pause it at 4:08 to see the code. Look at line 12 of his code. He's inserting values with an IF condition. My question is, why did he even need to declare sysdate and user as variables? What benefit does that give over simply doing this ? Insert into sh_audit (new_name, old_name, user_name, entry_date, operation) Values(:new.sh_name, Null, USER, SYsdate, 'Insert' ) Declaring username and sysdate like he did seems pointless?
So we actually have an internal GUI took that uploads csvs/txts into postgres tables. It asks to create an index and I picked an arbitrary field (its a unique key field) because I don't plan to use the table I upload - I plan to transfer the table into a larger table that I will use. So its only one index ... I feel like the hardware should be fine but who knows. Just wasn't sure if it was something obvious but it doesn't seem to be.
Imagine you have a trigger that grabs some values from your insert, calculates a new value for one of the fields, and displays it. A BEFORE trigger would end up with the new value inserted into the table, and an AFTER trigger would end up with the original value put into the table, but displaying the new value. If you're not changing any of the inserted values, there's no real difference.
For this example it doesn't really matter, but imagine if there were several different inserts in each of the IF/ELSEIF blocks? If the whole script runs longer than a second, the time returned by sysdate could change, and you get different values in different tables. By temporarily storing the value, it will remain the same until the entire trigger completes (or you change it explicitly). Same for USER, though it would be harder to change that unintentional.
Couple options. One, stop asking for help from various IT teams and go above their head. Talk to your manager or even department head and state that certain business users should have read-only access to certain data. Provide explicit examples of how that data would be used and how it can affect the department's bottom line to help. Then have them go to IT teams/departments/architects and say, "We need this to make/save money/time, build an acceptable system." Two, actually join the IT teams that have access and attempt to change the system from within. While you're accomplishing that over the next decade, become the "subject matter expert" and cater to your former department and their business processes.
Which database engine are you using?
OK. If you have to do it that way, you were very close with your first try. `(MAX(wineID) + 1)` isn't quite the right syntax, because it isn't a whole subquery, it's just a clause. In order to make it a proper subquery, you have to select that expression from somewhere: INSERT INTO WINE (wineID, wineYear, wineName, winePrice, wineHarvest) VALUES ((SELECT MAX(wineID) + 1 FROM WINE), '2013', 'Vidal icewine', 33.50, '2013-01-29');
Oh, awesome! Thank you so much! This has been plaguing me for days. Glad to know it was only a small mistake on my part, too.
Ok for the sysdate, that makes sense. But for user, I don't see how that works. The user now will still be the same user 1 minute from now who performed the query , so why declare the user? &gt;Same for USER, though it would be harder to change that unintentional. I think you tried explaining that here but can you clarify a bit more please?
When working on a compromising solution be firm with what you want out of the solution. If someone else does the work but you get the results you want with the same flexibility then great. But if you are not happy with the results, push back. I would definetely share some of the queries with the gatekeeper and have them come up with a solution that meets your needs. If it's same 10-20 reports you run with adjusted parameters they will probably have someone develop a report UI for you with dynamic parameters based on specs provided by you. Like mentioned before, have the security investigated. Typically the dB are on the same network but being able to query database via excel seems odd since security check is at dB level for dB access. Good luck
Couple of things... 1. If you use MS Query out of Excel to query a Database Server, I wouldn't give you access either. One sufficiently badly written SQL Script (even if it returns the correct data) is able to severely affect overall performance of the database server while it runs. 2. If you are able to query the databases directly, someone screwed up big time. Might be the AD admin, might be the DBA, might be your boss, requesting security roles you should not have. 3. "Turn data into value... somehow". I venture the guess, that you are talking about production databases. Those already have value, without that data, you have no business at all. What you want / should want, is a read only replica, as your personal plaything, where you can not break anything. 4. It depends on the data if you should be able to access it directly or not. You are a security risk if you are able to. Should you be able to see marketing related data ? probably. Should you be able to read out social security numbers and credit card information? probably not. These are questions for the compliance department. Anyway, since I have the feeling you will just query the database directly anyway, be very careful with what you do. If you run something like this on a production server, you will get to meet your DBA in person. SET TRANSACTION ISOLATION LEVEL SERIALIZABLE BEGIN TRANSACTION SELECT COUNT(1) FROM dbo.sales --COMMIT Looks harmless doesn't it? This would read lock the entire sales table, and never release the lock, until your session / connection is closed. Anyone trying to write to this table would meet a query timeout and then call helpdesk "the application is broken". After 2 hours of ping pong with 1st, 2nd, and 3rd level support, someone would find your session on the database server, kill it, and then proceed with killing you. You can break a lot of things really easy on a production database. You should not want direct access, you should be afraid of even going near production. If I was you, I'd push for the creation of a reporting environment, and get the compliance department involved, so you don't get into hot water with data security.
Well it is being ridiculous by not sharing schema and data dictionary though. Clearly, things aren't normal at ops company. 
its not. Business users have no business on a production backend environment. Hell, I don't even like those frontend developers messing around with the schema on the development environment. If I caught on of them playing on prod, I'd go mental. If I was asked to hand over an ERD of our production DB to a business user, I'd make sure he'd not be able to access the database in the first place, instead of handing out the documentation. After that, I'd ask him what he actually wants to do, and propose a reporting environment, but that expense would have to be cleared with people higher up the ladder.
Hi, I ran the grant connect and I think I've moved on, but my suspicion is that the issue now is that the database is not in a state capable of receiving the mirror as it's in norecovery. error is "neither the partner nor the witness server instance for database". As I understand it this is either, the db is in norecovery and there fore isn't started and capable of receiving the mirror partner OR there is an issue of quorum, neither partner is in a state where SQL can adjudge who should be sending what. Any thoughts?
Maybe GROUP is a protected system word, as it also denotes a SQL word when in GROUP BY.. It could be tripping up the parser. If any other name works (within reason) then that is the reason. Try wrapping it in square brackets a la MSACCESS!!
The first and third str is the variable with the text value to split. It looks like the second str is the alias for the column for the returned split strings. LEVEL is a value supplied by the Oracle engine in combination with CONNECT BY to give an index number to the split items. This allows you to select an explicit split element within the list or limit the elements returned 
Can you begin to imagine, explaining datamodels and schemas to every single person in the marketing department, and every other person in the entire company that read some buzzwords on the internet, cause they want to generate value of the data? They don't care about schemas. They care about the information contained in those schemas, and that is a very different thing. That is what I would talk to them about, to learn from them, what I have to ETL into a datawarehouse, so they / me can build a cube on that so they can create and run their reports. But the production data schema? They would stop me 5 minutes in and ask me to explain it in less technical terms anyway.
It is up to your application to format the value. Your query environment picks a format by default that it thinks might be most appropriate for a given case. In other words, it doesn't analyze that you did TRUNC and expected only 2 digits after the decimal point. Due to the way floating point numbers are stored, they are not precise, unlike decimal(x,x) storage. I.e for example, a calculation that 'theoretically' brings you a specific value will be off by a very small fraction, i.e. 1.5/3.0 != 0.5 with 'float' expressions/columns might actually evaluate as 'true'.
You need to use dynamic sql to do this. Quick googling says that HANA uses EXEC for this: exec 'delete from '||:v_to_table; 
thats so strange, if i were to do a filter on the result of the query that returns 0E-17 and another column that has a float value of 0.0000 would they return true?
Good information, thank you. I'll take notes from this thread, and set up a few internal conversations next week. I just connected through SQL Management Studio to a few databases, I don't have the fear yet, and was clicking around trying to find a data warehouse with no luck. I hope you're right, and something already exists for my needs. Lots of good info today, more learning than usual for a Friday! Have a great weekend everyone!
this. Heard a rumour that SQL Server will start supporting sequences...but this will have to do for now. 
2014 does support sequences. Still, using a SEQUENCE or IDENTITY put a unique constraint on the column. Neither sequences or identity columns are guaranteed to be unique.
Here is an easy way to remember. Think of your tables in two groups. Those on the left and those on the right. In this case there is only one table on the left and that is named in the FROM. Good. Now you get all the rows from the left table and matching rows from the right tables. The WHERE clause are used to throw out the rows on the left that you don't want. Yes I know that WHERE is mostly to pick what you ***do*** want but I always think of it as rejecting all ***but*** what I specify.
Try truncating to 2.0 instead of 2. Vertical SQL behaves the same way with ROUND(); it maintains the same precision of the truncated value with 0s if you pass an integer as the length to which to truncate. 
thanks for this tip, i would have never guess exec command. I've gotten pretty far with the dynamic sql, but no luck actually doing what i wanted.
i figured out the problem, the ';' must be removed on line 11 for the exec command on line 17 to work. ||SECOND(CURRENT_TIME)|| ' as (select top 5 * from v_filtered_admissions);'; ||SECOND(CURRENT_TIME)|| ' as (select top 5 * from v_filtered_admissions)';
SELECT RIGHT('00'+CAST(DAYOFMONTH('2011-10-01') as varchar(2)),2) as 'DayOfMonth' from DUMMY
thanks, LPAD did the same thing.
Select * from [table] where [column b] like '%' + [column a] + '%' Columns need to converted to varchar if they are text type. 
~250 - 350 million rows 
Why are you using the group by clause in your statement?
You either remove the GROUP BY, or add the fields from the SELECT in the GROUP BY PS: Are you also Brazilian?
Depending on what you're trying to accomplish, you need to either remove the GROUP BY clause, or add the other column from your SELECT statement to it. Think about it logically. You're grouping by palete.id, meaning you're only going to return a single row for every possible value in that column. If you have multiple values in produto.name that will match with those ids when you join the tables, you're going to straight up lose data and get a fuzzy result set, which kind of defeats the point of writing explicit SQL statements in the first place. So without knowing exactly what you're trying to do, I can't really tell you how to fix the query, but the mismatch between SELECT and GROUP BY column sets is what's throwing the error. Would a DISTINCT clause in your SELECT statement get you where you need to be? Otherwise, if you actually need to group the data by patele.id, you'd need to change it to &gt; GROUP BY patele.id, produto.nome
concat converts all input types to strings and nulls to empty strings. also, to OP, I don't believe that either solution proposed by u/krad0n or u/technical_guy will work for you. That is, both solutions will return both 'Smith' &amp; 'Smithe', and you said that you didn't want the 2nd result. The '%' symbol is a 'one or more character' wildcard - so it would also return results like 'HammerSmith' (altho' I don't think that is usually spelled with a capital 'S' in the middle like that). If you want to return only whole words (that is, bounded by a space character before &amp; after) based on your columnA value you might try the following: where columnB like '%' + ' ' + LTRIM(RTRIM(columA)) + ' ' + '%'. The LTRIM &amp; RTRIM will remove any accidental spaces from the columnA value, and the "+ ' ' +" inserts a single space before &amp; after your target word to ensure that a word like 'Smithe' isn't returned when you want 'Smith' or 'Williamson' isn't returned when you want 'Williams'. It's a bit hacky but that's what you get at 7am.
Ah. I try not to have nulls in VARCHAR and NVARCHAR columns and always set them to NOT NULL.
Where I work, it is frequently the case that data gets input/updated piece-meal &amp; fields must be left NULLable because that data is not known. Also, at least in T-SQL, a null-valued field takes no space on your mass storage, where an empty string does. Finally, when pulling data from multiple tables based on left join, a null or nulls would indicate not record found. In short, gotta be able to handle nulls, 'cuz they're everywhere!! :-)
At the end of your query say: Order By orderDate You can add 'asc' or 'desc' to specify ascending or descending order.
I think what he's saying is that he can't think of a scenario where the user would change, but that doesn't mean it can't happen. Probably better to design it to be extra robust so that the possibility is gone. Business users will change their requirements at the drop of a hat, better to design queries using flexible design patterns so that when that unanticipated requirement comes dowb the pipes you're ready for it.
how do you define "not completed" -- shippedDate NULL or status cancelled? to extract the year and month from a date column, you'll need YEAR() and MONTH() functions, or else the standard sql EXTRACT function, depending on which platform you're using, which you neglected to mention (please see sidebar) these functions will go into both your SELECT and GROUP BY clauses
Thanks, this helped.
"days they get delayed in shipping" i am guessing would involve only those rows where shippedDate is later than requiredDate? you certainly could easily tell us what columns and/or calculations are involved in "delayed" also, what did you learn from your previous thread about date functions to extract year and month?
select x.dayofweek, x.dayofweek_count, sum(y.Transaction amount) from (select a.date, a.dayofweek, b.dayofweek_count from tbl1 a join (select dayofweek, count(*) as dayofweek_count from tbl1 group by dayofweek) b on a.dayofweek=b.dayofweek) x join tbl2 y on x.date=y.date group by x.dayofweek 
I've just been promoted to a Data Analyst after having worked a secondment in that area for a large company. The particular company phrased the role (with no experience required) as a 'junior production assistant' as part of a larger MI team, but not sure how similar that may be for other companies. Best way would be to get your foot in the door at any level you can to show them what you're capable of!
Worked, thank you!!!!
Yep, i'm brazilian :D
It is aliasing for both result set and columns, so the 'select distinct....' returned result set can be referenced as sub, it's first column as a and second - b.
 INSERT INTO dbo.Data_Industry (Entry_Description) SELECT 'My Foot' I'm sorry.
use backticks (when needed*) around column names -- `Time EST` instead of single quotes -- 'Time EST' which makes it a string \* try to name your columns so that backticks aren't needed
I need it to be from each table in the database. 
I'm on my phone but lookup 'EXEC sp_msforeachtable'
This is great. I'm gonna try it out. Thanks!
Worked, thanks! ill keep that in mind though!
1 Look up GROUP BY 2 You're close, but you're focusing on the wrong part of the problem. You need to join HR.Employee on HumanResources.Employee.BusinessEntityID = Person.Business Entity.BusinessEntityID. For now, don't worry about selecting HumanResources.Employee.*, Person.Person.jobTitle. There are two huge hints there. Good luck!
What is your goal for doing that? Prevent them from excessively locking tables due to long queries? Your approach won't work there, the user can still run an order by query on an unindexed column. 
I'll use your hints and give it a try, thanks!
&gt; SQL report writer I'll have a look into this, thanks. how about getting your step into the IT industry if all else fails? IT is still a somewhat related field so what job titles should I search for in that regard? I was told help desk / support but job searches always ask for experience with those job titles. It's annoying :/
You're not joining CUSTOMERS and STORES. Is that intentional?
I was just able to complete both. I've been enjoying this class a lot and today almost made me wanna quit. Thank you again for the hints.
For getting into the help desk, you can approach this in a few ways. Really it's just about what you're willing to go through to get in the door. If you want to do Windows desktop support, you can try applying for entry level call center gigs in your area. If that doesn't seem doable for you, try Craigslist and search for entry level tech support opportunities. One of my first gigs ever was at a little voip company in my area. The job sucked but I learned voip and got 6 months of experience out of it. I used that experience to build my resume. I did IT help desk a couple years ago for Fossil's corporate headquarters and the PC guys all came from call centers like that. But the Mac OS 10 guys were mostly from Apple Stores around the city. You could take a relevant path and apply for a gig at geek squad if you are really wanting to make your way into IT as a whole. It's a far cry from "big data" but it's a fucking start man. If you're not into retail via Apple or Best Buy as your path to get in, try a call center. Once you get your experience in the smaller gigs, you will be able to prove your viability with more niche gigs like SQL reporting and data driven analytics. It just takes time and effort. 
Not sure if you've already tackled this, but yes. I just installed a SQL server express instance on my mac (running Parallels) and created a C# app on VS to access it. Please make sure that you're not just running random select /insert/update queries from the app. You should be asking yourself what you want the app to do, and write stored procedures and views which allow the database to control EXACTLY what is done. This allows for full control and risk mitigation should someone take control of the application. You should also ensure that you create a user/role specifically for your application, that has limited access to only the components you create for it. Applications should never use sa or the like. Here to help if needed.
This is more a question about the frontend client and the database you're using than anything SQL related. A inelegant way, given you use excel and have a list of the table names would be to use excel to help write a query for each table: "Select * from " &amp; A1 &amp; " order by random() limit 1;" where a1 is the first tablename then copy paste it into your client and export a bunch of csvs somehow. Hope this is useful. This sort of stuff can be hard to Google.
&gt; For example, if column A is 'Smith' and column B is 'Michael Smith', I want it to appear. *However, if column A is Smithe, then no results.* I took the part in italics to mean that he **does not** want results like 'Smithe'. I took column A to be a last name field &amp; column B to be some sort of Full Name, and so assumed he wanted a match on the exact name only. However, it wouldn't be the first time I've been mistaken on things like this. edit: om reading closer, I have the columns reversed - if the search term (column A) is 'Smithe', he doesn't want a match on 'Smith'. Alrighty, then.
So, we do behavioral electrophysiology experiments, meaning we are recording electrical activity from inside a rat's brain while the rat does stuff. We do several recording sessions per day. I would definitely want one table for recordings, with columns such as subject #, date, and various experimental parameters. One thing that will probably become apparent soon: I am unclear on what kinds of data storage are possible. I am accustomed to being able to store data in MatLab cell arrays, where C{1,1} could be empty, C{2,1} could be a 1x105 array, C{1,3} could be a 20x11 array, etc. So when I want to store that kind of data in a database, I don't know if that means that the contents of each such cell would need to be a saved MatLab data file stored as a blob, or if there's some more efficient way to store that data. Each recording contains several types of data: "Event" data is stored as several independent sets of timestamps, with a name for each set. It is not necessary to further subdivide this data. In MatLab, this would be stored as a struct. I'm not sure how best to store this, as noted above, since different recordings will have different lists of different lengths, with different event names. There is only one collection of events per session (events is 1:1 with sessions). Different sessions will contain different types of events. "Spike" data for a recording contains up to several million 'spikes'. Each spike has a timestamp, and some other attributes that may later be used to separate one recording's spikes into groups ("units"). Typically, after the spikes from a recording are sorted into units, only some of the units are of interest, and if more than one unit is present, data from only one of them at a time would be accessed. For this reason, I think "units" should have its own table, in which each row represents one unit and has an attribute indicating which row in "session" it belongs to. Now, I'm unsure about whether I would rather have each record in "units" have its own cell to contain the data (timestamps, etc.) of all spikes belonging to that unit, or have a table of spikes which contains all spikes and has a column containing the unique key of the unit each spike belongs to. Each unit may have anywhere from 5,000 up to 5,000,000 spikes, and there could be as many as a thousand recordings (ballpark). "Continuous" data for a recording may contain zero, 8, or 16 vectors of single precision values. Typically these vectors are of length ~ 2 million. Obviously, the same issue as with the "events" data arises here, since the data from different sessions will have different sizes. Typically, I would want to retrieve all event data for one recording, plus all continuous data for that recording and/or all spike data for one of the units on that recording. When retrieving spike data for a unit, I would often (but not always) want only one column of that spike data. I would seldom want to retrieve data from multiple recordings concurrently. I would want to avoid retrieving spike data for all units when I only want one unit. This data would need to be sent to MatLab. Another important part is the ability to browse the metadata stored in the "recordings" or "units" tables, with various filters etc., ideally without too much work to create the interface to do so.
your google document link is to edit, and you haven't given public permission to edit. Maybe put up a view link?
The problem is that it's a log of activity... so it will be: Rep|TicketID :--|:-- John|123 Jane|123 Jane|123 John|456 Jane|789 and because Jane has two entries for TicketID 123 and John only has 1, Jane is the "owner" of ticket 123 and it should be counted for her and not John.
Ahhh.. so, from your original example though Shira was the first rep listed for 9876, as Nir took ownership of it on 05/04/2009? I'm still not seeing this as a having. I'd see that as an exists clause looking for a more recent date for this ticket id. SELECT Rep , COUNT ( DISTINCT TicketID) -- ANSI SQL the DISTINCT goes inside the parenthesis FROM TicketsLog WHERE NOT EXISTS ( SELECT 1 -- Only include this row if there is no more recent row for this ticket. FROM TicketsLog iLog WHERE iLog.TicketID = TicketsLog.TicketID AND iLog.TicketDate &gt; TicketsLog.TicketDate ) GROUP BY Rep Or with a subselect.. SELECT Rep , COUNT ( DISTINCT TicketID) -- ANSI SQL the DISTINCT goes inside the parenthesis FROM TicketsLog INNER JOIN ( SELECT TicketID , MAX( TicketDate) MostRecent FROM TicketLog GROUP BY TicketID ) dLog ON WHERE dLog.TicketID = TicketsLog.TicketID WHERE TicketsLog.TicketDate = dLog.MostRecent GROUP BY Rep still not seeing a HAVING path that makes sense.
 SELECT continent , name , area FROM world x WHERE area &gt;= ALL ( SELECT area FROM world y WHERE y.continent = x.continent AND area &gt; 0 ) list all continents, name and area where the area of that continent is greater than or equal to all the non-zero areas of the world's continents. Which.. if I'm understanding what we're doing here is going to list the Name with the largest area on each continent? That's just weird and badly named all around. SELECT continent , name as CountryName , area FROM world WHERE area &gt;= ALL ( SELECT area -- The area of this country is greater than or equal to the area of all other countries FROM world iworld -- on this continent WHERE world.continent = iworld.continent = AND area &gt; 0 ) Fellow SQL geeks.. when/why is this superior to SELECT continent , name as CountryName , area FROM world WHERE area &gt;= ( SELECT MAX( area) -- The area of this country is greater than or equal to the area of all other countries FROM world iworld -- on this continent WHERE world.continent = iworld.continent = ) 
No, not at all. The main difference between the two, apart from the query engine, is the sql dialect, and while it is related to the query engine. It is mostly a question of mysql being a datastore and postgres being, well more, in my opinion. Mysql does not have analytical functions, or sometimes called window function. And depending on your need, I'd really much rather like the option of those, over not having them and needing them. Price wise they are on equal terms. So I'd go for the one with more features. 
An sp for counting rows is perhaps overkill. But you can easily insert data from an sp into a temp or permanent table. Give the sp a return value, and insert that I to your table, or print it or whatever else you want to do with it. 
Does MySQL finally support CTEs? I haven't used it much in 5+ years, but I recall that being a big pain for me.
I think you want to look at "common table expressions" (A/K/A "CTEs"). You should wind up with something like this: WITH T as (SELECT X,ID FROM Y WHERE Z= 'q') SELECT N,X FROM Q INNER JOIN T ON T.ID = Q.ID You can refer to T as many times in your query as you'd like. This is not always the case with derived tables, which is a an older technique which isn't as popular anymore. For completeness, a derived table would look like this: SELECT N,X FROM Q INNER JOIN (SELECT X,ID FROM Y WHERE Z= 'q') T ON T.ID = Q.ID 
You're in the ballpark. I would summarize the inner select by year so you're not getting one row for every vehicle in the Vehicles table. More like: (This is off the cuff and not run through any syntax checkers.. ) SELECT YEAR_GROUP, SUM(CountByYear) Count_Group FROM ( select vehicles.year , CASE WHEN vehicles.year &gt; (SYSDATE('YYYY') - 4) THEN 'Newer' WHEN vehicles.year &lt; (SYSDATE('YYYY') - 5) AND vehicles.year &gt; (SYSDATE('YYYY') - 9) THEN 'Middle' WHEN vehicles.year &lt; (SYSDATE('YYYY') - 9) THEN 'Old' ELSE 'ERROR' -- There shouldn't be anything that falls through here so don't hide it with null END AS YEAR_GROUP , COUNT( VehicleID) as CountByYear FROM vehicles GROUP BY vehicles.year ) GROUP BY YEAR_GROUP, ORDER BY YEAR_GROUP;
Excellent. So "ghost page" recovery ***does not*** happen if you don't have a clustered index? Wow! Yet another reason to try to put a clustered index on every table. I was asked, a long time ago, to come up with the true cost of having empty tables in a database. I found that if there is no clustered index the cost can be zero. That is because there might be enough spare space in system tables that the creation of yet just one more table might not cause any new allocation at all. What you show for an empty table ***with*** a clustered index is that the minimum impact is 3 pages (IAM, root, first leaf). Yet mostly the folks concerned as those on Express. We tend to sweat every page.
Don't you need a group by clause after the where?
Wouldn't grouping them by year in the inner select cause issues with the CASE statement? 
Ah. In trying this, I'm getting a "missing right parenthesis" error at the (SYSDATE('YYYY') - 5) portion of the first WHEN statement... Any ideas? I've opened &amp; closed all of my parentheticals, but am still getting the error.. Specifically, this is the line I'm having issues with: select med.MDL_YR -- , (CASE WHEN med.MDL_YR &gt; (SYSDATE('YYYY') - 5)
i know what an SP with no output does. I dont know what an SP with output does.
This is garbage
Yes, it unrolls the CTE - it's basically syntactic sugar in SQL Server. Which means that if you have a large, complex CTE in your query and reference it twice, it's going to be evaluated twice. You do not use CTEs for performance gains, you use them for readability/maintenance gains. With a sufficiently large/complex CTE and/or data set, you may be better off with a temp table. Do it both ways, evaluate all aspects of performance, take the one that gives the best value proposition.
&gt; You can refer to T as many times in your query as you'd like. Just be advised that the query defined in `T` will be processed *every* time you refer to it. If `T` is a really expensive query, you're gonna be hurting.
Follow up question. Thanks for your long response, i appreciate it. The way this came across in the code was as follows : Insert into bb_audit (userid, logdate) values sys_context 'userenv', 'os_user'), sysdate); So what exactly is a "namespace" - in this case 'userenv' and why is it necessary? Also, If os_user is giving us the operating system username, why do we even need the "namespace" and why do you need all of these functions instead of simply saying "USER" . Also Can you explain what exactly a parameter is (I've seen this in procedures/functions). I know that's a lot to ask for but believe me, I would appreciate it more than you could imagine.
&gt; The way this came across in the code was as follows : &gt; Insert into bb_audit (userid, logdate) values (sys_context('userenv', 'os_user'), sysdate); ---------------------------- &gt; (1) why do you need all of these functions instead of simply saying "USER" . The above example SQL statement creates an audit record of someone doing something. Suppose in an office, there is an Oracle database D on a Linux computer "accserver", and the people directly log in to that computer (which is not the best security practice). For example, Bob logs in to the computer with his username 'bobsmith'. The Oracle database has just a few schemas, including one named ACCOUNTING that has dozens of tables that store accounting data. Bob, once he logs in to that computer, uses some application program directly on the computer to view and update the accounting data. That program starts a session and connects to the ACCOUNTING schema. When that program wants to record when and by whom some data was changed, or wants to record the time at which it connected to the database, the USER function would simply return the value ACCOUNTING - the *Oracle* username to which that program is connected. But what is desired is to record that 'bobsmith' changed some data, or to record when 'bobsmith' started his session. For that, the program needs to call a function that can return the operating system username, not the Oracle username. So the program calls SYS_CONTEXT('USERENV','OS_USER'), which will then return 'bobsmith' or 'maryjones' or whatever. Similarly, as in some TV shows, the program could record more detailed information, such as "which terminal in the building was bobsmith logged into when he did X" or "what SQL statement did he run to change the data". So the application program can call SYS_CONTEXT() and specify some other value for its second parameter to get various kinds of information, so that the program can then record more detail into its audit table. SYS_CONTEXT() has dozens of possible values for its second argument, to meet the needs of various situations. ---------------------- &gt; (2) So what exactly is a "namespace" - in this case 'userenv' and why is it necessary? Basically, Oracle Corp wanted to give some of its customers more features. In many companies, you just want to restrict access based on someone's operating system username, or which PC they connected from, etc. Or similarly, when someone makes changes, you want to record that sort of session information in an audit table. The old USERENV() function [documented here](http://docs.oracle.com/database/121/SQLRF/functions244.htm#SQLRF06157) could return the needed information so that some companies can set up that kind of security. But companies also have security which is specific to an application. For example, they want only those people who work in the HR department to be able to view salary information. In that case, the developers can set up a view that gives the rows about salaries *only if* a person using that view works in the HR department. The SYS_CONTEXT() function is more general than the old USERENV() function. An application developer can define a 'context set', which has a 'namespace' and associate that with a procedure that checks whether a person works in the HR department, or has a high-enough security clearance, or whatever custom security criteria. The application developer can then use SYS_CONTEXT() with their own custom namespace, instead of using the 'USERENV' namespace that uses session information to control security access or auditing. Look at the example section at the bottom of [this page about the CREATE CONTEXT command](http://docs.oracle.com/database/121/SQLRF/statements_5003.htm#SQLRF01202) So, the first parameter of SYS_CONTEXT() is usually 'USERENV', but some developers will define their own namespace and give that for the first parameter instead. 
Without the tables and data it's difficult to verify the answer with 100% certainty. That said, my *guess* is you're doing the wrong join. Since it says "including the ones in the outcome table" you likely need an outer join (possibly with a distinct) instead of an inner join.
This is pretty poor design. Why not properly name the columns in your new table? 
It's not my design and the feature to name these generic text fields was an afterthought when the company realized no one could keep consistent data in the fields. 99% of the data is in a custom field table we can now create, but 24 text fields still remain housing data from years past. I'm slowly upgrading, but the people I work with are not technical so major changes or outages while a large export/import occurs scare them.
why are you joining this way: from classes join outcomes on classes.class = outcomes.ship
Fail
People need to stop representing joins as venn diagrams. This is wrong. A union is not a join. Also, calling these anti-joins is intellectually misleading. There is still a join. The logic here is in what you do with the new dataset caused by the join, not what you do with the join.
100% agree. I was thinking I was about to learn something new. "What is this anti-join wizardry?". But yeah, that is an outer join with a WHERE clause that determines which rows get returned. Anti-join? 
I'll give it a shot. Note that not every regular expression parser behaves exactly the same, so check the docs for whatever language this is to confirm. The caret (\^) and dollar sign ($) just match the start and end of the field, so the expression is matching the entire field. The first group is (?:[^/]*/) Taking each element in turn: ?: means don't capture this group for later reference, [^/] is a character class made of everything that isn't slash, * means the preceding element zero or more times, and / is the literal slash *So, the first group is zero or more characters that aren't slash, followed by one slash.* {7} means the preceding element (the whole group) exactly seven times. The second group is ([^/]+) Again, [^/] is a character class made of everything that isn't slash. + means the preceding element one or more times. *So, the second group is just one or more characters that aren't slash. The parentheses are creating a group so that whatever matches can be referenced later.* Finally, .* just means zero or more instances of any character. So, putting it all together, the regex will match (zero or more characters that aren't slash, followed by one slash) repeated seven times, **followed by some characters which aren't slash**, followed by whatever. The bold part will be saved for reference (typically as \\1). 
Probably to get you to click through. I know it got me -_-
1. Skip the first 7 frontslash characters, which may or may not have data in between them ("/a/b/c/d/e/f/g/h/i" is treated the same as "/blahblahblah//////g//") 2. Grab every character until you hit another frontslash. There must be at least 1 character or the Regex will fail to find a match. There **does not** have to be a trailing frontslash. (This is the location of "g" in every example) 3. Ignore the rest of the string. Edit: The single best one-page reference for RegEx: https://courses.cs.washington.edu/courses/cse190m/12sp/cheat-sheets/php-regex-cheat-sheet.pdf
Very informative sir, thanks much again. Some more q's... I get why you would now use sys_context (userenv, os_username) - it gives you the exact user of the person who logged into the operating system. what then does sys_context (userenv, current_user) give you? You mention the word 'schema' - what is the difference between schema/database? For example, in my lecture slides, there's a code that says this : Create or replace trigger bb_logon_trg After logon on schema''begin insert into bb_audit_logon(userid, logdate) values (user, sysdate); end; I understand that this trigger is inserting values into an audit table everytime someone logs in - based off the code "After logon on schema" - is the schema the database itself, or what? I don't think it is the database itself because in the slides as well, there is another code for a different example that mentions this code "After servererror on DATABASE" To me, based off these 2 examples and what they're supposed to be doing, I'm not sure I know the difference between database / schema. 
 ^ "Starting with the first character in the string" (?:[^/]*/) "Match this non-capturing subpattern: ([any number of characters* that are **not** a frontslash], until you hit a frontslash") {7} "Repeat the last thing I asked for 7 times in a row. (7 frontslashes that might have text between them)" ([^/]+) "Capture the following subpattern: ([one or more+ characters that are **not** a frontslash])" .*$ "Ignore everything else, if anything at all, until you hit the end of the string (this is redundant. You could delete all 3 characters and get the same behavior for single lines of text)"
You can look at online courses like https://www.experfy.com/training/courses/big-data-analyst or https://www.experfy.com/training/courses/mastering-data-visualization-using-tableau-from-basic-to-advanced
Oh wow, the formatting got jacked somehow :\ sorry about that. The last 2 Rows should have ProjNum 12, not 11. Example output would be: ProjNum | TypeOfWork | DateOfWork | Ranking 11 | Install | 2016-09-21 19:99:55.000 | 2 12 | | Install | 2015-04-18 12:04:55.000 | 2 --In other words, the Lowest Ranked from DENSE_RANK (Partition by ProjNum, ordered by DATETIME) that contains 'Intall' or 'Upgrade', and ignore everything else. Sorry again, not sure what happened but I've never used the formatting editor on Reddit and FUBAR'd it evidently.
This is one case where I would recommend labeling the columns on the front end application through a second query, and not via the SQL select statement. 
It's not clear to me regarding the Ranking and why/if you need it. If you do not - you could just use a partition statement: SELECT DISTINCT ProjNum, FIRST_VALUE(DateOfWork) OVER (PARTITION BY PROJNUM ORDER BY DateOfWork) FROM TABLENAME WHERE TypeOfWork in ('Install','Upgrade') That will give you the first DateOfWork per ProjNum, when it falls within the WHERE clause's Type constraint
&gt; FIRST_VALUE That sounds like it might b pefect, but it returns: &gt;"'FIRST_VALUE' is not a recognized built-in function name." I am on MS SQL 2012 so it should be available...
Outer join where table1 not null and table2 not null
First, for auditing, you should add three fields, none of which should ever be named current_datetime because that's a terribly non-descriptive name * Create_datetime -- Populates on insert * LastUpdate_datetime -- populates on insert/update * LastUpdate_user -- Populates on insert/update (Not needed for your listed requirements, but really handy to have) Then.. if they want to know prior content of positions, you have two options. * Audit Table, contains things like 1. table name 1. field name 1. old value 1. new value 1. change_datetime 1. change_user More flexible, but recreating entire old records if they changed more than one thing becomes more complex. Or, just create an Old_Positions, give it all the fields of the current positions table, and it's own unique ID field so that in an update trigger on positions, the after validations so you're going to process that update, you copy the current state of the positions row to old_positions, then proceed with your update of the positions table. That makes it a snap to get back to old contents of positions, but could grow huge very quickly depending on how often the positions table changes. edit: formatting
What's the issue you're trying to solve - i.e. what "work" do you intend to avoid?
I'd say SSRS is less about dashboards and more about table style presentation of data, which sounds like what you need (yes, ssrs 2016 has better visuals now). It's going to fall on its face if you try to use it for massive data exports though.
Unless the efficiency is tremendously better for the "anti-join" I would consider the clarity of the EXISTS to be a far more compelling argument than gaining efficiency.
I'll be sure to mention this in our talks. Quickly I found this [link] (http://smallbusiness.chron.com/difference-between-microsoft-ssrs-ssis-ssas-34689.html) and I see value in both SSIS and SSRS.
Not really sure what you mean by "large field count". 100k rows should be OK as long as your query runs in good time. 
Go [here](https://my.visualstudio.com/Downloads?PId=2057) and sign up for the free "Essentials" service and then download SQL Server 2016 Developer Edition. It's a full featured SQL Server, but you can't use it for "Production" use. I'm not sure exactly what that means.
Fiverr.com Cheap foreign labor. Paid a guy $20 for a lot of baseball scraping data.
It's not very difficult to learn or build. If you collect all the data in excel, you can use a wizard to import it to your db. MSSql is free to download, you can find videos on YouTube on how to do the import and also codeacademy.com has 15 or so hours of lessons for free. If you do the basic 4 hour course, you can understand most of what you need. Basically one afternoon will help you learn a new skill and save you money 
Well it's all application based, so when you try to add a new record it throws out: "Cannot insert duplicate key row in object 'CUSTOMER' with unique index 'pk_cus_no'. The statement has been terminated." If I point it to the 2015 backup db it works perfectly. After I tried to use the import utility, it gives that error. 
Running a similar query in MS SQL runs a clustered index seek (I joined on the primary key) and not a full table scan... is there an index for `col1` on table2? Looks like you can use the `EXPLAIN` keyword in a MySQL query or `EXPLAIN QUERY PLAN` in SQLLite to see what the query is doing...
Or if you're doing group processing.. make the first query all records in your local table that are in remote table 1 (maybe returning the results to a local temp table?) and then make your second query all those that weren't found in the first. That should be equally conservative of your resources without having to take things one record at a time.
use 'union all' to avoid running a 'distinct' on your result set 
My guess is it's using connection pooling... so it's not re-authenticating the user. Is this on-premises SQL or Azure? &gt;Continuously active connections to SQL Database require reauthorization (performed by the Database Engine) at least every 10 hours. The Database Engine attempts reauthorization using the originally submitted password and no user input is required. For performance reasons, when a password is reset in SQL Database, the connection will not be re-authenticated, even if the connection is reset due to connection pooling. This is different from the behavior of on-premises SQL Server. If the password has been changed since the connection was initially authorized, the connection must be terminated and a new connection made using the new password. A user with the KILL DATABASE CONNECTION permission can explicitly terminate a connection to SQL Database by using the KILL command. For more information, see KILL (Transact-SQL). https://msdn.microsoft.com/en-us/library/ms189828.aspx?f=255&amp;MSPPError=-2147217396
Check if all tables in the database are having some sort of the unique key, which would uniquely identify the row. Then you can run statements like INSERT INTO db2015.dbo.table1 SELECT * FROM db2016.dbo.table1 t1 WHERE t1.ID NOT IN (SELECT ID FROM db2015.dbo.table1) With all the foreign keys you may have hard times identifying the order of inserts, because some tables would require certain data in other tables. Regarding the error you get, try to identify which statement raises this error (using Profiler) and then check data in tables to understand why it is happening.
i'm not sure what specific issue you're looking to solve, but your rewrite will not work - "range" can only come from table1 and you cannot bring it into the subquery.
both MS SQL and DB2 support values table constructor: https://bytes.com/topic/db2/answers/614236-values-statement
Can you post the SQL? You could also setup a data connection to run the query directly in Excel, cutting out Access. Then you would just load Excel, hit "Refresh Data" then run your macro from there.
I don't know if this will make any sense, it doesn't to me. I can start the application using the 2015 mdf and it starts and I'm able to add customers, everything works perfectly. I used a db sync tool to compare the two database files (2015 and the 2016 "repaired" file). It showed that there are 1770 records not in the 2015 database for that table. If I go ahead and copy those records over, I go right back to not being able to add customers. Same error message about duplicate keys. This is making me drink. More. Anyone want to make $100 and merge these two fucking stupid pieces of shit into one working pile of shit? I'm lost and way over my head.
That's a great answer. Thanks for the insight. How much versatility would VB front end have as far as design? I dislike Access forms because of the graphics limitations. It's sorta like designing a website with CSS and no Javascript. Is that still the case with a VB front end? I most likely won't go that route, but I'm curious.
OK, this is the procedure that adds the customer (sorry just copy pasted the CREATE statement for it): set xact_abort on; go begin transaction; go set ANSI_NULLS off; go create PROCEDURE s_ADD_CUSTOMER @cus_name varchar(50), @defcity varchar (30), @defstate varchar (2), @defarea varchar(3) with recompile AS BEGIN DECLARE @new_no int, @count smallint SELECT @new_no = 0 SELECT @count = 0 WHILE ( @count &lt; 20 ) BEGIN SELECT @new_no = unique_no FROM unique_no WHERE unique_descr = 'customer' IF ( @@ROWCOUNT = 1 ) BEGIN SELECT @new_no = @new_no + 1 UPDATE unique_no set unique_no = @new_no WHERE unique_descr = 'customer' IF ( @@ROWCOUNT = 1 ) BEGIN INSERT INTO customer ( cus_no, cus_name, cus_city, cus_state, cus_phone ) VALUES (@new_no, @cus_name, @defcity, @defstate, @defarea) IF ( @@ROWCOUNT = 1 ) BREAK END END SELECT @new_no = 0 SELECt @count = @count + 1 END SELECT @new_no END go grant execute on s_ADD_CUSTOMER to prog; go commit; go
It sounds like you may have a sequence of some other generated ID duplication. If you examine the DDL or the table structure for customers table it may jump out at you (likely the first column?). So essentially the database thinks you have n number of rows, you are inserting 1770 records so now the database will have to fail 1770 times until it can insert a record. Take a look here: https://msdn.microsoft.com/en-us/library/ff878058.aspx If it is a sequence you can alter it. Something like: SELECT MAX(IDLabel) FROM Samples; Result 123456 Then: ALTER SEQUENCE Samples.IDLabel RESTART WITH 123456 ; 
Would that cause a problem for the time after that? Say the number is 47911 (because I think it is), if I threw that in so that it said: UPDATE unique_no set unique_no = 47912 wouldn't the next that function ran it would just start at that number again? Or am I missing something?
The procedure gets the last number from unique_no table and then adds 1: SELECT @new_no = @new_no + 1 and then updates the date: UPDATE unique_no set unique_no = @new_no So you should be good :) 
Could you speak to C# WinForms? To me it seems like a good solution after I migrate to SQL Server
The Outcomes relation may contain ships not present in the Ships relation.
Directly no, what you can do is a SUM grouping by the customer levels and divide by the total SUM from a subquery. Well, assuming you are using MSSQL, I'm not sure in other RDBMS
OK, solved For the record, I had to put DOUBLE quotes around "order".
Tested on db2: WITH temp1 (Col1, Col2) AS (VALUES ( 'Apples', 2), ( 'orange', 3), ( 'pears',4) ) SELECT * FROM temp1 
It is using indexes. Just scanning using indexes. As I understand it, the ENTIRE subquery is run first, without any recognition of what's going on outside it. This means that the entire table is scanned, does it not? Joining conditions do not constrain the scan because the subquery doesn't know about the joins until the query is complete. This seems like a prohibitively expensive operation on any reasonably sized operation. 
Are you sure someone didn't add data and then delete?
Are you using SQL 2012? They changed the way IDENTITY works in 2012 and there's a trace flag to set it back to the 2008 R2 behavior. Basically, it sounds like it pre-allocates 1000 values at a time, so anywhere between 1 and 1,000 rows being skipped is part of the issue. https://connect.microsoft.com/SQLServer/feedback/details/739013/alwayson-failover-results-in-reseed-of-identity%5d#tabs
Basically, the method below finds the current record number and uses a running total to assign 'groups' to your sequences: with tt as( select * from( values ('A0103', 'C',2015, 2016), ('A0103', 'C' ,2016 ,2017 ), ('A0103', 'I' ,2017 ,2018 ), ('A0103', 'C' ,2018 ,2019 ), ('A0103', 'C' ,2019 ,2020 ), ('A0103', 'C' ,2020 ,2021 ) )t (val, stat, start, en) ), g as( select val, stat, start, en, rn = row_number() over (partition by val order by start), dist = count( case when stat ='C' then 1 end) over (partition by val order by start range between unbounded preceding and current row), grp = row_number() over (partition by val order by start) - count( case when stat ='C' then 1 end) over (partition by val order by start range between unbounded preceding and current row) from tt ) select val, min(start), max( en) from g where stat != 'I' group by val, grp 
lol ... that's encouraging 
D'oh! I knew that already but didn't think of it.
Thank you. This work and is scales decently. I greatly appreciate it.
As you said, double quotes. But, seriously, don't use reserved words for identifiers.
Momentary lapse because I need to get the site up by Monday. I usually use an ORM but I just needed to update a few rows and it was easier to just use psql. I should probably change the name...
Well, here's some help: the part percentage of whole is calculated as part (numerator) divided by whole (denominator) multiplied by 100.
just an update, had an issue with some conditions in the where clause which was causing it to pull the incorrect data. thanks though!
[removed]
Ah, perfect - with that, it works! Thanks!
I'd do something like this for the function: CREATE FUNCTION UPDATE_PARTBUFFERSUMMARY (@PARTNUMBER VARCHAR(50)) RETURNS INT AS BEGIN IF EXISTS (SELECT * FROM RPLUS43_DATA..PARTBUFFERSUMMARY WHERE PARTNUMBER = @PARTNUMBER) BEGIN UPDATE RPLUS43_DATA..PARTBUFFERSUMMARY SET &lt;COLUMNS YOU WANT TO UPDATE&gt;, MODIFIEDDATE = GETDATE() WHERE PARTNUMBER = @PARTNUMBER END IF NOT EXISTS (SELECT * FROM RPLUS43_DATA..PARTBUFFERSUMMARY WHERE PARTNUMBER = @PARTNUMBER) BEGIN INSERT(&lt;YOUR COLUMNS&gt;) SELECT &lt;YOUR DATA&gt; END RETURN (@@ROWCOUNT) END; GO
i guess that is my issue! thanks
I guess is due to the reseed thingy, someone pulled down the main switch... zzzzz
Yep, just double checked, and you can't do any DML activity in a function (it's read only). Should be an SP. Same basic code, just put it in the CREATE PROCEDURE wrapper.
Yep that did it, Thanks!
Alternatively, you could look up MERGE. It does the same thing in this case and would be a little cleaner.
Can you do *.xml 
I don't know - I can try it. I assumed that I would have to change some other elements for the code to cycle through all of the XML files. 
no you can't
&gt; I am on MS SQL 2012 so it should be available... [Check your database compatibility level](https://msdn.microsoft.com/en-us/library/bb933794.aspx). If it's less than 110, it's not operating as a 2012 database and not all 2012 features will be available.
When you cast really big numbers as varchars, they seem to get put into scientific notation. Try using STR instead of the CAST. You might need to pass a parameter for string size if you have really big numbers (I think the default is 10), and LTRIM/RTRIM it.
Thanks, I ended up writing a macro that added a character to each value in the cell, then remove it on the DB side, it seems to be working ok.
The temp table works well. I don't know why, but I get error "Msg 156, Level 15, State 1, Line 13 Incorrect syntax near the keyword 'ORDER'." I'm also not quite sure how to incorporate this without ruining the script. Sorry, but I don't completely understand my own script.
Well I stumbled upon this author when I was watching a masterclass(Write accurate sql queries) on safaribooks https://www.amazon.com/Database-Design-Relational-Theory-Practice/dp/1449328016 By CJ Date He is an authority figure on RDBMS and explains it very well. 
Forgot to add &gt; FROM #DirectoryTree Fixed now.
Case is an expression, the totality of it: &gt;CASE &gt; WHEN type_of_object LIKE "%Satellite%" THEN "Satellite" &gt; WHEN type_of_object LIKE "%Planet (terrestrial)%" THEN "Rocky Planet" &gt; WHEN type_of_object LIKE "%giant%" THEN "Gas Planet" &gt; ELSE &gt; "Misc" &gt;END 
Answer "SQL will do it for me"
At a guess, identify the relationship between the tables - one to one, one to many, or many to many - and then identify which column they would join on. Classic Example: if you have two tables, one a list of possible departments at a work place and their ID codes, and a second a list of all employees including which department code each one has - then: The relationship is One to Many (One Department to Many Employees - each department has many employees, but it's not a Many to Many relationship because no employee is in many different departments.) The primary key id the department id code in the Department table, and the candidate (or foreign) key would be the department id column in the Employee table. &amp;nbsp; That's assuming I understand what they're trying to ask correctly, but I think that's where they're going. &amp;nbsp; Some programs viewing database models or working with an existing database will identify and handle this stuff for you, but not all - and if you ever need to add a table to a system, or design a database from scratch, these relationships are *very* important to understand. Don't let your teacher blow them off if you can. 
The `primary key` is the main key by which you expect to identify a row in a table. A `candidate key` is any key that would be good enough. The big difference is that most SQL engines will put in effort to make working with the `primary key` efficient. For example, MySQL's InnoDB stores things on disk physically in order of `primary key`. So, think about a table you might make to store CDs for a hypothetical online store. You'll want to make your `primary key` an unsigned large-ish integer, because comparing integers is cheap (rather than, say, comparing strings.) So, like every other store on Earth, you're just going to assign your products a number, counting upwards from maybe ten thousand (to give yourself space for test codes and whatever.) However, there's going to be a bunch of other stuff in there which is uniquely identifying, too: the ISBN, the SKU, the disc's ID, maybe the ASIN if you track Amazon listings, maybe a Shazam thumbprint, et cetera. A `candidate key` is a key that could be a candidate to fully identify a row. The reason is that maybe you have an inbound target from outside that's using the ISBN, so even though you don't generally use that internally because it's silly, you have the ability to support it if some query needs it for some horrid reason. So the question is probably about two things: 1. Making sure you can find a bunch of indices which would be good enough, and 2. Making sure you pick the smart one for the one that generally does the work
SQL is great for data storage but is lousy for procedural programming. On the other hand C# is great for programming but has very little in the way of great shared data storage. By combining the two you can get the best of both. All versions of Windows (from XP through Windows 10) have the drivers for SQL Server already installed. The Dot Net framework comes with all the client libraries for SQL Server already installed as well. This means that about 30% of your project is done when you start. You already know about forms in Access so most of the thinking is the same. You can use Visual Studio Community version for program development or something like Sharp Develop. In Access forms the "code behind" (the stuff that make buttons work) is in Visual Basic. You ***could*** set up your forms and do the code in VB. However C# is a more supported programming language. As a long time VB developer I had to teach myself C# and don't even look back these days.
&gt;This paper exam approach is rather odd. The computers have monitoring software. However, I am unsure why all students must take it on paper. It makes it easier for your professor to grade on a sofa. here's a tip: get an pencil with a large eraser! good luck. 
&gt; It makes it easier for your professor to grade on a sofa. 11" or 13" notebook will also make it easy... 
We had the same thing in MsSQL. I put square brackets around all of my identifiers (tables, columns, etc.). I don't know what does this for you in PostgreSQL. The following drives my coworkers nuts. SELECT [*] FROM [FROM] WHERE [ORDER] &gt; [LESS THAN] GROUP BY [GROUP] ORDER BY [BY] What really ***gets*** them is that this returns a single column!
If you are using Oracle, then PLSQL. Scripting is a valuable skill to have and be able use. 
Expanding on this a bit - the IN statement acts like an OR. Just list all values you are interested in seeing inside of your parentheses using a , to separate them. 
Learn how to read and apply explain plans. This will help you understand what is really going on in the database. Also, it will prevent performance issues before they start and will be a life saver when you are troubleshooting a slow process or SQL statement. 
There's no practical limit on the number of tables in a MS SQL database (assuming you are using a fairly recent version of MS SQL Server) if that's what you are worried about. As to if this is a good idea or not, it would depend on what your boss is doing with the data. Can you give us a better overview as to 'what happens next' once the data is in the SQL server?
Does the structure of the file change from month to month? If not than create only one table and add some extra column to the table, like fileName, Periode, loadDate and even lineNumber if you can so you know where all the data is coming from. 
They are very useful but it takes a little while to get your head round them. From a reporting perspective they are much faster than querying the database directly as everything is pre-computed (you would normally generate your cubes over night). So you can drill down and slice the data in the cube up much faster. As for querying via ASP.NET, I'm not sure as I am not really much of a coder. However a quick Google yields lots of 3rd party options. 
Good catch!
Went to PM :)
What are "entries"? What's your database schema? What have you tried so far?
I'm not all that familiar with Medoo, so please let me know if I'm wrong on this. I'm also assuming that each record is stored in a single column. Couldn't you use the query functionality to use UNION to combine queries for each argument ("grape" and "orange" in this scenario), and then order it based off of count? Here is an example in T-SQL. DECLARE @Fruit AS TABLE ( fData VARCHAR(250) ) INSERT @Fruit ( fData ) SELECT 'Apple, grape, banana' UNION SELECT 'Apple, banana' UNION SELECT 'Orange, apple, grape' SELECT fData, COUNT(*) FROM ( SELECT fData FROM @Fruit WHERE fData LIKE '%grape%' GROUP BY fData UNION ALL SELECT fData FROM @Fruit WHERE fData LIKE '%orange%' GROUP BY fData ) f GROUP BY fData ORDER BY COUNT(*) DESC If something like that doesn't work could you put multiple queries into an array and order by number of instances?
Correct, the logical layout is: \--&gt;Server \--&gt; Database \--&gt; Table \--&gt; Column \--&gt; Data So you would create the database, switch to the database context, then create the table(s), then populate the table(s), then query any data you would need to: CREATE DATABASE example; USE example; CREATE TABLE test (id INT, test VARCHAR(8)); INSERT INTO test(id,test) VALUES(1,"test"); SELECT * FROM test;
so basically, do i create the a new table first then use create view command on the query to view the table? i am still kind of confused, sorry and thanks for helping
Yes (if I follow your statement). Think of a view as a subquery inside of a query. or maybe a view is simply a select statement that's been saved inside of the database and is now queryable like a table for simplicity.
i see, thank you so much for helping. I will try the following steps. Hope it's gonna work :D
It looks to start out beginnerish and go to intermediate, however I did a skim of it, and it seems a very odd order to teach sql in. The reviews are really bad too. I'd pick a different book or just watch some videos. If you are completely new to it, I might suggest ww3 as a starting point. http://www.w3schools.com/sql/ Direct and to the point crash course, then go from there. 
That might be solved by a subquery in the SELECT clause, but the performance of this... wouldn't be best. This example will build such an average for every existing row in the table. SELECT Timestamp , (SELECT AVG (Value) FROM Table t2 WHERE t2.Timestamp &gt;= t1.Timestamp AND t2.Timestamp &lt; dateadd(minute,21,t1.Timestamp) ) AS Average FROM Table t1
You can try something like this DECLARE @Starttime datetime = '2016-10-11 14:10'; WITH periodes as ( select @Starttime as start_time, DATEADD(mi, 21, @Starttime) as end_time union all SELECT DATEADD(mi, 1, p.start_time), DATEADD(mi, 21, p.start_time) as end_time FROM periodes p WHERE p.start_time &lt; DATEADD(mi, 9, @Starttime) ) select start_time, avg(t.float_clm) from your_table t join periodes p ON t.daretime_clm between start_time and end_time group by start_time; 
Nice thanks alot. ((&amp; normally, logical "AND"s are executed before "ORs" )) thanks for this aswell i wasn't aware of this. Good to know.
Here's a working example and some comments to get you started: EDIT: Oops, I misread, the windows were meant to be 21 minutes long but 1 minute apart. It's easy enough to fix on your own I think. create table test ( t datetime2, val float ); -- seed some random data over the past hour insert into test select top 15000 dateadd(second, 3600 * rand(convert(varbinary, newid())), dateadd(hour, -1, getdate())) , rand(convert(varbinary, newid())) from sys.all_objects so1 cross join sys.all_objects so2 -- declare the start of your window -- 90 minutes in the past; we only have data for one hour, -- but this way you get to see some nulls when there was no data declare @windowStart datetime2 = dateadd(minute, -90, getdate()) declare @windowDurationInSec int = 21 * 60 declare @numberOfWindows int = 6 -- recursively create windows; you maybe have them defined in a table or wherever ;with slider as ( select @windowStart AS WindowStart , dateadd(second, @windowDurationInSec, @windowStart) AS WindowEnd , 1 as i union all select WindowEnd , dateadd(second, @windowDurationInSec, WindowEnd) AS WindowEnd , i+1 from slider where i &lt; @numberOfWindows ) select slider.WindowStart , slider.WindowEnd , AvgValue = avg(data.val) from slider left outer -- change to inner if you want to omit the windows that had no data join test data on slider.WindowStart &lt;= data.t -- find rows with timestamp inbetween the window and slider.WindowEnd &gt;= data.t group by WindowStart, WindowEnd 
without the parentheses, query would return 1) books having title like '%head%' and pub_year = 2009 + 2) ALL books having pub_year of 2012
There are tools that can reverse-engineer a database into a diagram (ERWin, Toad Data Modeler, etc.). If you have an option to do so, I'd trace the app on the DB side for a few days and to get a sample and distribution of queries that are being run on the DB.
So I was afraid of that. Was hoping that Agent would be smart enough to pool. Any source for this? I can't seem to find any. Thanks 
Yes, it sounds like you would write the script file then FTP (FileZilla) the file up to the server to execute. If they have Internet access you could also post it on www.pastebin.com or a similar service and copy/paste.
u/chapmjw, this is a good solution, but has an error. If you select * from the recursive CTE you'll see the periods switch from 21 to 20 minutes after the first row. Also, I would avoid using BETWEEN in the join. It is inclusive on both ends (which may or may not be desired), and is less clear (in my opinion) than just using inequalities. Here is u/lbilali 's code with those things "fixed". DECLARE @start_time datetime = '2016-10-11 14:10'; WITH p as ( SELECT @start_time AS start_time, DATEADD(mi, 21, @start_time) AS end_time UNION ALL SELECT DATEADD(mi, 1, start_time), DATEADD(mi, 1, end_time) FROM p WHERE start_time &lt; DATEADD(mi, 9, @start_time) ) SELECT start_time, AVG(s.[value]) AS average_value FROM simpled s JOIN p ON s.[timestamp] &gt;= start_time AND s.[timestamp] &lt; end_time GROUP BY start_time
Firstly, this might answer you: you can join more than 2 tables: from x join y on... join z on... You can use columns from X in your join to Z as well. Secondly, I do not understand what you're trying to do or what issue you're having: your second syntax is wrong, your first one is an equivalent of a cross join between Tables A and B if something from X matches to something from Y or an empty result set. P.S. You might also be trying to do a dynamic query - in the sense you don't know what columns you need to join on. You can use either dynamic SQL or case statements with that (if there's a parameter that drives the selection).
edit: the below is wrong and illustrates why I don't use `translate`: if you don't pay attention, you'll completely screw it up. See the regex answer instead. (DON'T) Use [TRANSLATE](https://docs.oracle.com/cd/B19306_01/server.102/b14200/functions196.htm). It "translates" the first parameter by mapping each character in the 2nd parameter to its corresponding value in the third parameter. If 2nd is longer than 3rd, the leftovers are removed. select translate('abcd1234', 'abc1234', 'ABC!@#') from dual; --returns: ABCd!@# Translate this... | ... to this ------------------|----------- a | A b | B c | C 1 | ! 2 | @ 3 | # 4 | The "d" is not in the second parameter, so it remains untouched. For your situation, to check for numbers when we're expecting only alphas, we just remove translate all numbers to null and check if the result `is not null`. Vice versa for alphas when we're expecting numbers. --Contains NUMBERs where translate(DATABASE.FIELD, 'A1234567890','A') is not null --Contains ALPHAs where translate(upper(DATABASE.FIELD), '1ABCDEFGHIJKLMNOPQRSTUVWXYZ','1') is not null
That's probably the fastest way at least. Realistically, I can never remember how `translate` works without looking it up, so I just use regex. --Contains NUMBERs where regexp_like(DATABASE.FIELD, '[[:digit:]]') --Contains ALPHAs where regexp_like(DATABASE.FIELD, '[[:alpha:]]') Probably a bit slower, but not enough to matter.
The Access version of SQL doesn't support views. The closest thing to views are query objects which work pretty much the same as views, but afaik can't be created with an SQL statement. So basically what you're asking isn't possible within a access unless you're missing some details. I assume this isn't an access front end linked to a different backend?
Ah thanks, that's starting to make a bit more sense! Sorry if this is a dumb question, but what will paste bin help me do?
like rbardy said, break it down into minutes and then encapsulate it in an if statement that checks for both sides of daylight savings.
Translate is somewhat confusing - I did the 2nd part and added that to my query, but it returned all numeric instances in my count. The first one returned all alphabetical instances for some reason. Using the regexp_like worked like a charm as expected - but either way, i got the info I needed. Appreciate it!
Yeah, I completely screwed up the TRANSLATE examples. Ignore them and use Regex. ¯\\_(ツ)_/¯
&gt; Obviously this doesn't work but is there a way to do something similar? No clue. Your query logic is inscrutable to me. I have no idea what outcome you're expecting based on the code you've presented. &gt; The reason i need to do this is because it is a one to many not a one to one but the linking table doesn't currently exist in the database. A one-to-many relationship doesn't typically require a linking or junction table. Only a many-to-many relationship requires that. I recommend posting a) relevant sample data from all tables, b) expected output. &gt; I was thinking my subquery would be a two column linking table with the first column being the first part of the ON and the second column being what i'm joining it to. You could do this: SELECT * FROM TableA a JOIN (SELECT X, Y FROM TableC) c ON a.X = c.X JOIN TableB b ON b.Y = c.Y But that's typically indistinguishable from just doing this: SELECT * FROM TableA a JOIN TableC c ON a.X = c.X JOIN TableB b ON b.Y = c.Y 
I recommend an ERD, use case models, and a data dictionary. 
MSSQL is not my primary database engine and I had to look quite a few things up, and I have not tested this but I think something like this would work: select empid, dateWorked, sum(timeWorked) from (select empid, cast(startTime as Date) dateWorked, case when (cast(startTime as Date) &lt;&gt; cast(endTime as Date)) Then datediff(hh, startTime, cast(endTime as Date)) else datediff(hh,startTime,endTime) end timeWorked from Punch UNION ALL select empid, cast(endTime as Date) dateWorked, datediff(hh,cast(endTime as Date), endTime) timeWorked from Punch where cast(startTime as Date) &lt;&gt; cast(endTime as Date) ) tbl1 group by empid, dateWorked
I use `JOIN`s in my `FROM` clause to link tables. I've never seen the method you're using with `WHERE` clauses. Your `ORDER BY` clause is probably not needed. The strangest issue is that you are using `HAVING` without a `GROUP BY`.
It's an older syntax called an implicit join. The community/industry has gone away from using them and any DBA worth their salt will tell you to avoid using them, however, they do still work.
You are right about the group by, that is needed too in this case, though not necessarily required for a having clause.
I'm not disagreeing that the best method is storing UTC natively but often that isn't the case and you really shouldn't need more than one DST calendar unless you're working with multinational data, and if that's the case and it isn't in DST... then you're going to need multiple DST calendars. &gt;DATETIMEOFFSET Not familiar with this function.
I see, this makes sense since my teacher is at least a 1000 years old. 
Thanks, I'll try your suggestions 
&gt; DATETIMEOFFSET I'm assuming this is based on the server. Not exactly useful if you are importing datasets in their own native local times (i.e. CRM data in CST, web data in EST, etc.)
DATETIMEOFFSET isn't a function, it's a [data *type*](https://msdn.microsoft.com/en-us/library/bb630289.aspx) - one that stores the date, time and most importantly offset from UTC. &gt;you really shouldn't need more than one DST calendar unless you're working with multinational data Or you're dealing with places in a single country that [do or don't observe DST for various reasons](https://en.wikipedia.org/wiki/Daylight_saving_time_in_the_United_States#Local_DST_observance).
&gt; Or you're dealing with places in a single country that do or don't observe DST for various reasons. Easy to overcome with an additional zip code based params table. It isn't ideal, but often times (every time, in my experience) there is no datatype which does this for you. Would be nice to see, but my point is that you can get around it fairly simply.
Left and right are essentially the same. What labels the table as the left table or the right table is not the side of the equals sign. It is the order they are listed in the FROM clause itself. "Left" is listed first then you are *joining* the "right" table to it. When you put the keyword LEFT or RIGHT in the syntax all that you are doing is telling the engine "I REQUIRE all the rows from THIS table for sure. Anything from the other table would be great if you can find it, but not required for the row to meet search." That being said the result set after the join can of course be filtered down more afterwards. Where I work we rarely use right join, but favor left joins just to keep things uniform. In fact this is why [SQLite does not have right joins](https://www.sqlite.org/omitted.html), because it is seen as redundant.
Ok, so it seems like the way I thought about in my head is pretty much in line with that. Thanks for the clarification! Can you by any chance think of a scenario what a `RIGHT JOIN` actually makes more sense, be it aesthetically or functionally?
1. Show the ***id*** of all ***students*** who have ever took a ***course*** in a ***building*** whose ***namesstarts with the letter 'A'***. 1. Show the ***building*** and ***room numbe***r of each ***classroom***, with the ***number of sections*** that were ever taught in that ***classroom***. 1. Show each ***building*** with the ***number of different classrooms*** in it. There you go, I highlighted the key words that would equivocate to something in a query. Types of objects usually equate to tables, Student, Course, Building, Classroom. Properties of those are usally columns. Id, room number, name. Phrases like number of, largest, smallest, will equate to aggregates.
If you are up for the subscription, pluralsight has a lot.
For free. For beginners. http://www.studybyyourself.com/seminar/sql/course/?lang=eng.
For free. With online exercises. http://www.studybyyourself.com/seminar/sql/course/?lang=eng.
Experts, please feel free to correct me if I'm wrong. I'm not a wizard, but I once successfully joined nine tables. For this db to work correctly, you're going to need a linking OrderItem table. Otherwise, you'll have to replicate a bunch of data to complete an Order with multiple items. [Reference this post.](http://stackoverflow.com/questions/27693885/add-multiple-items-for-one-orderid-in-mysql-database) If you create an `OrderItem` table with pk(`OrderItem`.`OrderItemID`) int auto increment , fk(`Order.``OrderID`) referencing your `Order` table, fk(`Item`.`ItemID`) referencing your `Item` table, and `OrderItem`.`Quantity` int not null, you will only have to repeat the OrderId. You will need to associate your Items from your `Inventory` table to one or many of your prescribed pet types. This should be in another linking table, `Item_PetType`. pk(`Item_PetTypeID`) int not null auto increment, fk(`Item`.`ItemID`), fk(`PetType`) from a PetType reference table. Don't do this if you're not supposed to use linking tables or whatever you're calling associative entities. It looks like you've been instructed to use enum instead of reference tables, which I was not. This will return today's pet birthdays with the type of pet, which is necessary to determine which freebies to send and necessitates associating a PetType to each Item. It could be a scheduled job each morning. more bad pseudocode follows: INSERT INTO `Order` SELECT `CustomerID`, (blah,blah,blah...the rest of the order info) FROM `Customer` INNER JOIN `Pet` WHERE DATE(`Pet`.`Birthday`) = curdate() You should post the actual requirements so we can really help. I guess I'm bored right now.
something like this? select Paper_ID, count(Paper_ID) from table group by Paper_ID
Insert into newtable Select insertedtimestamp from inserted join deleted on id where inserted timestamp is not equal to deleted timestamp. Inserted os the new values deleted is the old values. Since we know nobody updates ids (for the love of everything hopefully this is true) we can use it to join the two tables. Then we just add a simple filter to bring it down to only the rows that changed the timestamp column. That is then taken and copied to a new table. Use this as an after update trigger not an "instead of" trigger unless there is already an "instead of" in place
This can be done easily with SQL Server and the full-text index feature. 
would you please answer [this question](https://www.reddit.com/r/SQL/comments/56s21e/searching_tags_with_multiple_words_and_ordering/d8m5fc1)
use a correlated subquery SELECT t1.Author_ID , t1.Paper_ID , ( SELECT COUNT(*) FROM daTable AS t2 WHERE t2.Paper_ID = t1.Paper_ID ) AS Total_Author FROM daTable AS t1
Thank you. I a few things to try out. Sorry about the improper tagging (no tagging rather).
I don't know what you want the Percentile to be calculated off of, but I'll assume it's revenue. Replace that with whatever value you actually want to use to calculate the percentile. But anyway, if you're using 2012 or above, you should be able to just do this: SELECT CustId, PERCENT_RANK() OVER (PARTITION BY [Customer Level], [Segment] ORDER BY SUM(Revenue) ASC) as CustomerRank, [Customer Level], [Segment] FROM [Current Month Master] GROUP BY [CustId], [Customer Level], [Segment]
The first thing that looks off is that the alias that belonged to the count has ended up in the FROM clause. Move the AS part back up after the COUNT and try again.
/u/nvarscar had a more elegant solution if your platform supports it,but a simpler way is to just add WHERE day(start_date) = 1 AND month(start_date) = 1
Wouldn't this just order my query result YEAR(Start Date)? That's not really what I want. Here's an example of my price table: | ID | Item No | Price | Start Date | End Date | |----|---------|-------|------------|------------| | 1 | 100 | 50 | 2000-01-01 | 2010-06-30 | | 2 | 100 | 55 | 2010-06-31 | 9999-12-31 | And the query result would need to be something like this: | Item No | Price | Year | |---------|-------|------| | 100 | 50 | 2000 | | 100 | 50 | 2001 | | 100 | 50 | 2002 | | ... | ... | ... | | 100 | 55 | 2011 | | 100 | 55 | 2012 | | ... | ... | ... | Where I have a price for every year. If I just order it by YEAR(Start Date), then my query result would only return a price for 2001 and 2010.
Which fields specifically are you being prompted for?
Check my response to him, as I think there may be some confusion as to what my result should look like (and what type of data I am working with). That solution would just filter my results to where the start date = January 1, but that is not what I am interested in. I am interested in every January 1 between the Start Date and End Date.
Group by might work for you. Something like group by datepart(year,start_date),product_name
Ah gotcha. Thanks, I'll need to read up on PARTITION BY
They never go into the database. If they did it would be sepirated.
Thanks, literally exactly what I was looking for!
In Oracle (before 12c, at least) a database is a set of files that hold data contained within a database. An instance is the set of binaries and memory structures used to access the database. An instance could only ever host 1 database, and a database could only ever be accessed by 1 instance (RAC an exception). A schema is a user within a database, and that user's associated data. In 12c, there's the concept of pluggable databases, which changes things slightly in that 1 instance can host several databases. A schema is still the same. Other database systems use "database" to mean roughly what "schema" means in oracle terms (e.g. a MySQL instance can hold many databases), but as you're asking specifically about Oracle, it means a user and their associated data. 
1. Off the top off my head while pooping... :) select max(s.order), s.customerName from shit s group by customername
lol no
From shit s? and would this all be one line of code?
use double quotes instead of apostrophies
It's cool that they offer MySQL, Postgres *AND* SQL Server. But this nonsense about writing programs to read from STDIN and write to STDOUT... Well that's not really SQL at all.
Thanks for the concern - will make sure we roll out an update that can help a more streamlined towards giving a better experience for SQL :)
&gt; "understanding of the mechanics of SQL Server" you want /r/SQLServer, not /r/SQL 
think of a schema as a "namespace". it let's you have things with the same names, but in a different context
SOrry for my ignorance, could you elaborate on how other users would utilize this? and how this would function?
Beautiful! Any suggestions for getting this to work in 2008? I'm trying to make an ssrs report with this and we have quite a few 2008 servers still :(
You can use ROW_NUMBER() to workaround lag, the technique would be a mix of your initial query and query using LAG(): ;WITH Data AS ( SELECT DateCreated, DriveLetter, PercentageUsed, ROW_NUMBER() OVER (PARTITION BY DriveLetter ORDER BY DateCreated) RN FROM #Temp_AllDrives ) SELECT DateCreated, a.DriveLetter, a.PercentageUsed, lg.PercentageUsed as PercentageUsedPriorDay, a.PercentageUsed - lg.PercentageUsed as PercentageUsedDiff FROM Data a LEFT OUTER JOIN Data lg ON a.DriveLetter = lg.DriveLetter AND a.RN = lg.RN + 1
You guys are both my heroes of the day. Very much appreciated! 
Nah, this interesting problem is most definitely not platform specific, despite the fact his title mentions one.
Thanks for the backup.
&gt; this interesting problem is most definitely not platform specific oh, goodie i trust, then, that all solutions offered in this thread will use only cross-platform sql
Why #Edges? Imagine #Points as a tabular representation of points of interest on a map and the distances between them. You need recursion to do this, so you will have some kind of looping structure regardless of your answer.
FYI, you don't need to cast your second "Path" definition after the UNION ALL, because the data type has already been set by the "root" query where you did the first cast. Also, our solutions are basically exactly the same. But I'll argue mine runs faster ;) (LIKE is faster than CHARINDEX, and I've rooted the starting position to begin with, so SQL isn't going to evaluate any paths that don't begin with @startPoint)
Yours would run faster, and in practical terms is a better answer by at least 10x performance wise. Why is LIKE with a wildcard on the left faster than CHARINDEX starting at 0? Thanks for the tip on the cast. I wrote this a few years ago, and IIRC I was casting everything because I was getting type cast errors while trying to work through the problem.
Thanks for the insight, Phil! I don't have formal training, so I always appreciate lessons like your edge and path definition. CHAR(1) was an arbitrary decision made from the bathroom while I wrote this in response to the other "6 hour test" that I saw earlier today. In reality, it should be NVARCHAR(1).
And really, my solution should not be to try to exploit a data type length ;-) But I had no idea these CTE things existed. The lesson from challenges is, the language reference is always your best friend!
Honestly, your answer was both amusing and humbling. Hats off to you old heads who laid this beautiful foundation for us young punks to work on.
I see your explanation in your above post now, thanks. If I'm understanding that correctly, then when there is a wildcard on the left of a rooted expression, the wildcard is effectively ignored, is that correct? Is the query analyzer smart enough to not waste resources there?
I'm not sure I follow. But I rescind my comment re: LIKE being faster than CHARINDEX, because as you pointed out the LIKE has a wildcard prefix, rather than just in the suffix.
Assumptions: * Points are given in lower-higher order (A,B) never (B,A). It is possible to do without this assumption but it makes the answer unnecessarily tedious. * Paths shouldn't pass through the same node twice. (Obvious, but unstated by OP.) Here's my solution. I know I won't "win" because I don't believe in one-statement solutions if that only serves to be oh-so clever but unclear. So, here's a multistep solution that makes sense to me: It can be run at RexTester [SQL Path Finder](http://rextester.com/VEKWF50059). CREATE TABLE Points ( PointA CHAR(1), PointB CHAR(1), Distance INT ); CREATE TABLE Paths ( StartPoint CHAR(1), EndPoint CHAR(1), Steps varchar(100), Distance INT ); INSERT INTO Points (PointA, PointB, Distance) VALUES ( 'A', 'B', 2 ); INSERT INTO Points (PointA, PointB, Distance) VALUES ( 'A', 'C', 3 ); INSERT INTO Points (PointA, PointB, Distance) VALUES ( 'B', 'C', 4 ); INSERT INTO Points (PointA, PointB, Distance) VALUES ( 'C', 'D', 3 ); INSERT INTO Points (PointA, PointB, Distance) VALUES ( 'B', 'D', 2 ); INSERT INTO Points (PointA, PointB, Distance) VALUES ( 'B', 'E', 6 ); INSERT INTO Points (PointA, PointB, Distance) VALUES ( 'A', 'E', 7 ); INSERT INTO Points (PointA, PointB, Distance) VALUES ( 'D', 'E', 1 ); DECLARE @Start CHAR(1) = 'A'; DECLARE @End CHAR(1) = 'E'; SELECT * FROM Points; DECLARE @newrows INT = 0; DECLARE @loops INT = 0; -- Limit loops (i.e. path length) to prevent a runaway code bug -- Add initial two-point paths that begin at @Start into Paths INSERT INTO Paths ( StartPoint, EndPoint, Steps, Distance ) SELECT PointA, PointB, PointA+PointB, Distance FROM Points WHERE PointA = @Start; SELECT @newRows = @@ROWCOUNT; -- Extend paths to adjacent nodes unless the path already ends at @End or already contains the To node WHILE ( @newRows &gt; 0 AND @loops &lt; 10 ) BEGIN INSERT INTO Paths ( StartPoint, EndPoint, Steps, Distance ) SELECT pth.StartPoint, pnt.PointB, pth.Steps+pnt.PointB, pth.Distance+pnt.Distance FROM Points pnt JOIN Paths pth ON pth.EndPoint &lt;&gt; @End AND pnt.PointA = pth.EndPoint AND CHARINDEX( pnt.PointB, pth.Steps ) = 0 WHERE pth.Steps+pnt.PointB NOT IN ( SELECT Steps FROM Paths ) SELECT @newRows = @@ROWCOUNT; SELECT @loops = @loops + 1; END -- SELECT * FROM Paths; -- All paths -- List the solution SELECT TOP 1 Distance, Steps FROM Paths WHERE EndPoint = @End ORDER BY Distance, Steps; -- Paths arriving at @End DROP TABLE Points; DROP TABLE Paths; 
Why do you feel this step is necessary? On a non-indexed column SQL server doesn't really care what order you do your inserts in at all.
Because the record: A, B, 3 is an identical "edge" as: B, A, 3 And now that I've tried it out I see that it doesn't matter. My solution doesn't work for all points (set @startPoint = 'E' and @endPoint = 'A' and it all falls apart)
This is because in your data there is no valid start point of E
Yes there is. The following record clearly shows I can travel between D &amp; E. D, E, 5
But in table @Points, for that record, the value 'E' is in the column labeled 'B' and not the column labeled 'A', so in this statement: SELECT CAST(A + B AS VARCHAR(MAX)) as [Path], Distance FROM @points WHERE A = @startPoint SQL finds nothing for A, because there is no record where the value A in table @points is 'E' unless you insert the reverse of all possible connections before executing your CTE.
Right. See my updated solution.
I think this might be one of those questions where the only real answer with the information we have is, "it depends".
This was fun, I'm sure my boss was happy with my productivity this past hour. I made a random table loader because i couldn't be bothered to write out that many insert statements. Also, I expect no efficacy awards for this. SET NOCOUNT ON IF Object_id('tempdb..#Points') IS NOT NULL DROP TABLE #Points CREATE TABLE #Points( PointA CHAR(1) NOT NULL, PointB CHAR(1) NOT NULL, Distance INT NOT NULL ) ALTER TABLE #Points ADD CONSTRAINT PKtPoints_AB PRIMARY KEY (PointA,PointB) DECLARE @loop INT, @cnt INT, @A CHAR(1), @B CHAR(1), @C INT SELECT @loop = 300, @cnt = 0 WHILE(@loop &lt;&gt; @cnt) BEGIN SELECT @A = char(cast((90 - 65 )*rand() + 65 as integer)),--LEFT(NewID(),1), @B = char(cast((90 - 65 )*rand() + 65 as integer)),--LEFT(NewID(),1), @C = ABS(CHECKSUM(NewID()) % 50) INSERT INTO #Points SELECT @A, @B, @C WHERE NOT EXISTS( SELECT 1 FROM #Points WHERE PointA = @A and PointB = @B) DELETE --Horribly innefficient, but will work FROM #Points WHERE ISNUMERIC(PointA) = 1 OR ISNUMERIC(PointB) = 1 OR PointA = PointB SELECT @cnt = Count(1) FROM #Points END DECLARE @Start CHAR(1), @Destination CHAR(1) SELECT @Start = 'A', @Destination = 'F' ;WITH CTE AS ( SELECT a.PointA, a.PointB, A.Distance, CAST(a.PointA+a.PointB as VARCHAR(100)) as [Path], 1 AS L FROM #Points a UNION ALL SELECT a.PointA, b.PointB, A.Distance+b.Distance as Distance, CAST(a.PointA+b.PointA+b.PointB as VARCHAR(100)) as [Path], L+1 AS L FROM #Points a INNER JOIN CTE b ON a.PointB = b.PointA WHERE L+1 &lt;=4 ), B AS( SELECT MIN(Distance) as Distance, [Path] FROM CTE WHERE PointA = @Start AND PointB = @Destination GROUP BY [Path] ), c AS( SELECT MIN(Distance) as Distance FROM B ) SELECT B.[Path], c.Distance FROM B INNER JOIN c ON b.Distance = c.Distance 
Use lag() if your system supports it
Thanks :)
The isnumeric deletion is pretty damn clever and amusing. It definitely made me laugh.
It doesn't help unless I have access to the Indeed database! I have been thinking about scraping their data and making my own database which is public to everyone to query lol! How cool would that be!
Well, if you were to provide a bit of detail about what your current skills and experience look like and where in the world you lived I'm sure some less sarcastic redditor would be able to give a more useful answer. 
Are you calling the SUM() function in SSRS or in the TSQL query?
What engine? How are you importing it?
Move into a data analyst role. It is a common entry into IT for the more data-savvy business (non-IT) folk. It's more challenging on a few fronts. One, you have to learn a language and two, you use that language to encapsulate business logic or rules.
Can you help me relocate? I'll take a lower salary.
MS SQL 2008/R2. I'm using a bulk insert to load it. I normally do this by SSIS but this time requires me to do it via SQL
When I saw this posted at work, I thought there would be something truly clever that would come out, but we're seeing an imperfect Dijkstra's algorithm with a CTE. This isn't so much a SQL problem as it is just a straight algorithm problem. Both your solution and the above solution do not check that once a path (or multiple paths) have been found that there no longer exists any edges that are still less than the existing shortest path. This means your run time always has the worst case. That is to say, in a graph with 1 million points and every point connected to every other point and every edge having the same distance, if you try and find the path from a -&gt; b you will still try every single possible path despite the fact that you immediately find a path, and every other path you try and build already has a higher distance. What's more, if we are trying to find the path from a to e, and if we discover a -&gt; b -&gt; d is 100, and that a -&gt; c &gt; d is 10, you will still try a -&gt; b -&gt; d -&gt; e even though we know without a doubt already that it will never be a shorter distance than a -&gt; c &gt; d -&gt; e. I didn't post anything before because the only thing I could think of implementing is a really uninspired procedural version of Dijkstra using a while loop and manually keeping track of the nodes that still need exploring. Anything with a CTE that I was coming up with to add these checks of disallowing non viable paths during path creation in a CTE would start to really clog the self joins because it would recheck all existing paths each time for being a viable candidate for path building. Indeed, now that I've typed this up, I decided to cheat around a bit, every where I'm seeing other people implementing Dijkstra in SQL they're using loops, [even this guy who uses a CTE at the end just for formatting the output produced by the loop](https://github.com/bjornharrtell/pggraph/blob/master/dijkstra.sql)
i change the 7.54 to 5.347 and when i added your code in, it kept giving me the error message below. "Conversion failed when converting the varchar value '5.347' to data type int" This is weird for me because i have other numbers such as 2.34 and 4.27 but they converted correctly. EDIT: All the numbers came in as varchar.
In most of my work, I choose to use temp tables over table variables. Few reasons: 1. You can create nonclustered indexes on a temp table(cannot on table variable, although you can create an unique constraint which would act as an index, but you're constrained to a unique column) 2. Inserts, updates, and deletes for a table variable are going to be executed serially (will not benefit from parallelism) 3. Temporary tables have much better statistics created on them. This means your execution plan will better choose operators (merge or hash join over a nested loop operation). Table variables up until 2014(iirc) have an estimate of 1 row (2014 bumps this to 100 rows). Temp tables will actually use statistics and give you a semi accurate estimated row count. Sorry that this is brief and might contain spelling errors as I am typing on a small phone screen. 
https://technet.microsoft.com/en-us/library/ms159847(v=sql.100).aspx
I think you're looking for the serial data type, if you define id as serial instead of integer, the sequence is created automatically. 
You most likely don't want a BI Developer position as you will have a harder time becoming a BI Developer than a SQL Developer. BI developers are traditionally required to know the Microsoft BI stack (SSIS/SSRS/SSAS) , basic knowledge of SSAS is fine or non at all is not uncommon. Just search for "Junior Database/SQL Developer on indeed or Dice. 
Business Intelligence is set of technologies used for analyze and gather data for helping business to make better decisions. Exago and ETL is the tool used to accumulate the previous data, stored and often used data from the data warehouse to help for the business growth.
Are you using a text box or a tablix. The usual method I have seen is to use a tablix thatis grouped with a header row. All the child rows are then hidden via the properties. This is known as drilldown. Drillthrough and sub-reports are the other methods of solving this problem. You can find out more here: https://msdn.microsoft.com/en-us/library/dd207141.aspx
Unpivot order records to per-month records, then total up by month.
What do you get if you just do Sum(Fields!Usage_2016.Value, "SYS4RPT_7")&gt;0 This returns a boolean, so putting it in an IIF statement is redundant. Since the error was on the FalsePart of the IIF statement, it may help.
you want us to guess what the error message was? okay... you can't ORDER BY something that isn't in the SELECT clause 
Thanks for the reply. I rewrote the code like this since DISTINCT is not working. It does seem to eliminate duplicates but is the style correct? SELECT String1 + ' ' + String2 AS String3 FROM Table GROUP BY String1, String2 ORDER BY String2; 
SELECT DISTINCT should work fine for you, it was your ORDER BY clause that was the issue. I'm happy to help with specifics, what is it that you're trying to do?
I'm trying to select unique rows of the new column String3 i.e. no duplicates and sort the set by String2.
Well it's good to start thinking about future problems like that. :) But sql is so mature that most things have already been figured out in optimal ways . 
If you need unique combinations of String1 and String2, you can get them by: SELECT DISTINCT STRING1, STRING2 FROM &lt;TABLE&gt; ORDER BY STRING2 This will eliminate duplicate combinations of String1 and String2, without concatenating them into the String3 alias, which created your ORDER BY issues. Note: SELECT with DISTINCT will consider any NULL values to be duplicates to each other, so if String1 or String2 are nullable columns, bear that in mind.
Is it possible to use numeric(6,4) (or something similar) for the destination, rather than a float? Do your values have significantly more significant digits than your example? Also, I realize this makes it a manual process, but you can always import to the table from SSMS and manually map a VARCHAR to a FLOAT. Or at least, I've been able to do so in SQL Server 2014 without issue.
People will tell you that WITH(NOLOCK) is the best query hint ever made. It's a good way *in certain circumstances* to improve your read performance. However, you WILL eventually get a dirty read if you use this hint on a table that is updated with regularity. If you know for a fact that the table you're querying is updated once a day, or even once an hour (obviously, not during the time when you'll be reading from it), feel free to improve your query speed using WITH(NOLOCK).
your first example, with the date or datetime ranges, is not the best solution this is the best solution -- WHERE AdmissionDate &gt;= '2015-04-01' AND AdmissionDate &lt; '2016-04-01' why is this the best? because 1. it works for **both** DATE and DATETIME columns 2. it doesn't require a time portion to be specified 3. it uses standard sql date string values, so no CONVERT is required 4. it doesn't require specifying the high end value, which would be problematic depending on millisecond precision 5. it's easy to automate and avoids that crazy February 29th issue
Thanks for your response, I think that is good information to take into consideration. As an analyst, what SQL do you use? I'm sure your basics like SELECT, FROM, WHERE, JOINs, UNIONs, aggregate functions, but what else? Also, how in-depth does your SQL use get, i.e.: how many lines of code would you use to display your typical information? I'm curious how your use of it in your role would compare to something like a reporting role. 
I recommend avoid using BETWEEN because people other than yourself may interpret it incorrectly.
That means the hidden functionality is working, but your expression 'Sum(Fields!Usage_2016.Value, "SYS4RPT_7")' may be evaluating to a different Value than you suspect. My next step would be to put 'Sum(Fields!Usage_2016.Value, "SYS4RPT_7")' into it's own textbox and see what value is being returned there. 
Thanks. I never thought to format the comma's on the left side like that. 
I am not sure if I understand your question, but if you are trying to control visibility based on the value found in a cell within that same scope, you can refer to cell by using =ReportItems!*CellNameHere*.Value So your visibility expression would be =ReportItems!*CellNameHere*.Value &lt; 0
I'm probably not a great example, since a lot of what I do is really development work, but I can't have 'developer' or 'engineer' in my title because the IT department "owns" those titles. But, that being said, I use a lot of SELECTs and JOINs (obivously) in addition to creating stored procedures, user-defined functions, create tables, indexes, sequences, &amp;c. (DDL statements) on my own local DB, and a lot of other stuff besides. PIVOT recently came up in a situation where I had data in rows that needed to be in columns (getting our reporting systems to play nice isn't always easy). CTEs (common table expressions) are also pretty big because I'm lazy and don't want to write out a subquery over and over, and sometimes I actually recurse them. Length of queries can vary a lot; usually my ad hoc queries are 10-30 lines (depending on how complex the underlying logic/data is) but I have one stored procedure that I was working on today that just cracked 15,000 lines. (This particular case is my own damn fault for not writing modular, portable code, but whatever it's my ugly baby and it does what I want.)
Came back to mention this, and saw that it was already covered in the top comment now. Date formatting sucks, and anything you can do to make it less of a headache is good to know. I also skip dashes in my code, but if at all possible I try to throw dates into variables since 90% of the time I'm not just using a date once.
LMFAO I know the type
That was exactly it! I'm super green at sql in general and i know even less in ssrs. Thanks alot
Scrubbing content from your data is generally a bad idea. Doing it in a stored procedure, rather than a constraint or a trigger, is an even worse idea. If the data is truly invalid, it shouldn't be in the database at all. You should focus your efforts on building systems that work with *all* user input, handling special characters correctly rather than settling for a design that does not work 100% of the time.
Butting into the conversation to complain about ISNUMERIC(). What a junky function. It frequently returns 1 for data that still will not CAST to FLOAT.
I'm not OP however I started in customer service moved into reporting and then became an analyst. Now I'm in charge of data quality and process improvement. What I did was dive in and look for ways to make things more efficient. I looked for business problems that I could solve with vba at first, then databases and SQL. I just learned by doing. I read a lot of forums and the Querying SQL Server exam book. If you're in reporting now then read a lot, find things you can do to prove your analytical mind. Keep some examples of good things you've done and when you apply for the jobs be confident and make sure it's something that pushes you to learn more. I applied for jobs that had requirements that I didn't have. I've seen so many job profiles asking for so much. They are mostly wish lists rather than minimum requirements. Anyway that's been my experience. Hope that my ramble helps. If you have any questions feel free to ask.
I love the alt shift arrow in some of the editors it let's you insert the same text all down a column even in front of existing text.
Not within SQL Server itself. It pisses me off every time I have to download a result set to another tool that's capable of correctly identifying and parsing numeric strings. #bahumbug
&gt; It also avoids issues when dealing with dates in different region settings. I personally prefer skipping the dashes Use ISO 8601 format and it should be interpreted properly every time.
Between is a tricky beast. Forget human interpretation - you have to be careful about SQL Server's interpretation of it. http://sqlblog.com/blogs/aaron_bertrand/archive/2011/10/19/what-do-between-and-the-devil-have-in-common.aspx
Looking at the example I can see right off how you would have gone astray. It would seem that the column that you are working with (AdmissionDate) is a DateTime column rather than a Date type column. This can get you in real trouble even though everybody thinks that all the data are just dates. This is why you have to be very careful when writing queries. Understanding the data types is critical. I do the comma thing sort of the same way that you do.
Please note this post has details and examples that apply to SQL Server. Sorry for the wall of text. I'm not /u/Erudition303 but he's spot on; I would also second a good statistics education. You won't (or actually you will, since you're coming from marketing ;) believe the arithmetical questions you have to field in the middle of presenting your findings. Some make you want to question how that person got to where they are without rudimentary skills but others are tougher to convincingly explain, like how taking an average of an average can sometimes give inappropriate results. After learning the proper syntax and learning how to consistently format your queries, there are a few next steps to evolve your skills. None of these are actually a "step" where suddenly you have them; they are gradual slopes with sometime steep faces that we scale when the big *AH-HA!* moments happen. Even after ten years on this platform, there is still plenty to learn. This is the reason for specialization as it can be difficult for one person to master the administration side, the performance tuning side, the ETL side, the architecture side, etc. One would be query patterns and best practices. Just focus on writing solid queries that perform reasonably well. Stuff like, when should I use a CROSS APPLY with a correlated sub-query using TOP 1 versus a LEFT JOIN and some table filters in the ON clause? Preferring equivalence operators like = and IN over &lt;&gt; and NOT IN. Design, which is really an exercise in various levels of abstract thinking, will start becoming a concern. Whether you knew it or not, you're already a query designer when you decide to select data from a set of tables in a particular fashion. Next, you'll look to build scripts that "do stuff" via not just one but multiple queries. Questions like should I use a CTE or a temporary table for this? What are the various steps I need to do in order to account for all the intricacies of the data to get the right numbers? After progressing from query-level to script-level, next comes process-level where you start thinking about creating -- and properly utilizing -- multiple SQL objects in order to transform the data into the results you need. Should I have really written 15,000 LOC in one stored procedure? Or should I have broken up one of the queries I use five times into it's own table-valued function and just reference that function everywhere? At this point, you're at the doorstep of being an **ETL developer**: just learn how to use an ETL tool to extract, clean, transform, and load your data. At all levels of abstraction, there are principles that guide what we do. Traits like maintainability and readability (how quickly can I or someone else change this in the future?), performance (what types of resources are used when I run it?) and business constraints (we need the numbers to our question in less than 3 business days) all shade what choices you make in design.
Sorry I think I was kinda vague with my question, I'll try to be clearer with a simplified example of my problem: Lets say my dataset has 3 columns and 3 rows. Column A values are (1,2,3), Column B (4,5,6), Column C just holds a unique key for each row. Now, in the first report the first field is the sum of column A (6), so when I click on that value, I want to jump to another report that would show 3 rows of the relevant unique ids.
I'm clueless. Could you help me understand what you mean by "I was kinda vague"?
This worked for me. I added semi colon after your first table, and removed "references" from "not null" on your second table creation create table Artists( Name varchar(50) primary key, Genre varchar(20), StartingYear int, CHECK (StartingYear &gt; 1900 AND StartingYear &lt; 2017) ); create table Album ( Title varchar(50) primary key, Artist varchar(50) not null, ReleaseYear int, CHECK (ReleaseYear &gt; 1900 AND ReleaseYear &lt; 2017) ) 
Ill be honest I got my degree in MIS, but have a few certifications as well. It all depends on the job, and none of them care so much about the papers, it all depends on your skill level. 
Try putting a semicolon after the first query. SQL wants something between queries so it knows where one stops and the next begins, and semicolons are the easiest way to do that. 
add WHERE userID = $userID Be wary of sql injection when using php. 
Shouldn't it be more like `WHERE userID = 12345`? And how can I protect myself from SQL injections?
Alt selection: hold alt click and drag down, you can now edit in multiple lines at once Save every script you write, you will thank yourself later. Some helpful ways to do this is sql prompt or apex complete. Work smarter not harder, automate everything you can. Ask "Why?". Don't just implement a solution that is given to you. Find out what problem they are trying to solve in the first place. \^\(.*\)$ing Regex is awesome. 
I don't think it is possible to do what you suggest in SQL without an overly cumbersome and resource intensive procedural approach unless you root the path in the anchor statement. As this is set based, SQL is evaluating all of the valid paths at once and returning one. Your suggestion is a procedural, row by row, cursor-esque (while loops are cursors) approach to avoid an extreme edge case (no pun intended). If this were run against a set of data which represented paths from the center of a sphere to a point on the outside, thus all having the same distance, SQL would return you an arbitrary path provided that this data is not sorted. That path, and in fact any path it returned, would be a valid answer to that query for that dataset. This, however, would be a pointless exercise for anyone who understands the geography of a sphere. This is clearly not the language of choice for this problem. That's what makes it fun.
Yeah I've had intentions to attempt what you are talking about, but we are now moving to tableau. This is obviously the basics only. My thought would be to maybe incorporate a look up calc on your table into the parameter maybe? Let me know if you find a solution.
Well, that'll always return the FirstName of the user with userID 12345... you will need to pass the query your userID variable. In this case, you can just ensure the userID is a number to prevent SQL injection (since it'll fail if any escape characters or bad query ("drop table1;") data exists: if (is_numeric($userID)) { $query="SELECT * FROM `Table1` where userID = $userID"; if($is_query_run=mysql_query($query)) { // echo "&lt;br&gt; Query executed &lt;br&gt; &lt;br&gt;"; while($query_execute=mysql_fetch_assoc($is_query_run)) { echo $query_execute['FirstName'].'&lt;br&gt;'; } } else { echo "Query not executed"; } } &gt;Also, do i have to add require 'database/connect.php;` every time I want to query the db? I'm pretty new to this. This is more of a PHP question... but yeah, you'll have to have it once on each PHP page you intend to access the database access from. Might make more sense to look into a PHP MVC framework, which handles database access, page loading, etc... in a more consistent way. It might be a little more difficult to pickup than throwing a simple webpage together, but it will be much easier to build a site once you get the concept down.
I know how to use VBA and SQL to do ETL. What do you mean by "how proficient" am I in the skills I listed above? I have about 2 years of experience with those skills.
Yeah I tend to do that a lot XD i ask a lot of questions as i go because i want to understand everything cmppletely, but sometimes there's a good reason for not teaching everything all at once :P
Which are the fields you need to consider when choosing an entry?
What kind of PHP framework would you recommend for a social site?
I would also store close and vacancy intervals in that table and add an interval_type column that can be populated with event, close or vacancy. Then select ... where interval_type = vacancy and startdatetime &gt; current_datetime order by startdatetime limit 1. 
Haven't done much with PHP in a dozen+ years... so I'm way out of date.. Zend was always pretty popular. Quick search turned up: http://www.yiiframework.com/ https://elgg.org/ http://kohanaframework.org/ 
All the ones mentioned in the link. they are ordered by importance.
A full stack dev with 2-3 years experience should be around 75,000. My one recommendation would be to learn how to develop for the cloud, that's a bug differentiator these days on resumes.
If you have two years experience than why are you asking how can one get an entry level SQL position? With your two years, you should already know how to get a SQL Developer position since you have worked in SQL development for 2 years. 
Could you explain what you mean by date variables? My first solution to this was to use a set dateformat line before starting which I thought was easy enough, before the convert tip was given. I prefer the example given by r3pr0b8 anyway, so will probably use that going forward (although not what my boss recommended).
This is awesome, cheers!
Just stating again, as I have to an above comment, that the alt selection tip is great! Thanks. Could you explain your last point, ^(.*)$ing Regex, is it like a 'search and replace'? Google offers nothing but a wiki page for Regex (regular expression).
I worked in a related(ish) position in the same company where my analyst position is, which definitely helped. I ran reports on numerous systems, however had never touched SQL. Entry positions don't really exist around here so I made sure I read up on SQL, looked at how the company used it and met with some of the people who I knew used it to get a better understanding. I sold myself on my Excel proficiency, understanding of the data used and how it was used to determine targets/outcomes, and general desire to progress from where I was at in to the field. I wish I could offer you better advice but really, this is a one-off position that doesn't exists around these parts. There are either analysts who seem to just move between companies, or people like myself who want to get in to the career but can't due to no lesser position where you're able to develop your skills first. I count myself extremely lucky landing the position, I also used to work in recruitment so if you need help with applications/interviews let me know.
A quick and dirty way is to create a temp table to store the new group ids. There probably are be better ways to do this, but this works as a one time data fix. Below is some pseudo-sql showing how to do it. Table #tmp Id int autonumber (1,1) OldGroupId int Insert into #tmp (OldGroupId) select distinct groupId from data order by groupId Update data Set groupid = tmp.id From data inner join #tmp as tmp on data.groupid = tmp.OldGroupId Drop table #tmp
The Oracle SQL Fundamentals cert is a good entry level cert.
Top answer in thread did not complete after 11 hours. Your code modified as http://pastebin.com/NB6S6YDb did not complete after 5 minutes searching for '.aa' -&gt; '.av', I'm not running this one for a long time like the other one. There's nothing in your algorithm that's really different from the other one that ran for 11 hours, it still builds every single possible path, a massive number, before selecting the shortest one. Just for a proof of concept, modifying your code with a recursion limiter [as such](http://pastebin.com/GVQCLJVP) is able to successfully find 16 different paths from '.aa' -&gt; '.ct' in 7 seconds. Increasing the recursion limit from 5 to 6 increases the running time to 21 seconds, and finds 21 paths. What result were you expecting?
`Update data Set groupid = tmp.id From data inner join #tmp as tmp on data.groupid = tmp.OldGroupId` fails `#1064 - You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near 'From data inner join tmp as tmp on Shabad_copy.ShabadID = tmp.OldGroupId' at line 1`
I said pseudo-sql for a reason. I know tsql, but not MySQL. Look up the syntax for how to to an update statement in MySQL using multiple tables.
Ah, ok. *EDIT*: So I got this query made, but it times out: ```UPDATE Shabad_copy, tmp SET ShabadID = tmp.ID WHERE Shabad_copy.ID &lt; 1001 AND Shabad_copy.ShabadID = tmp.OldGroupId```
the problem here is you doing `groupid + 1` what you want to do is something like... SELECT MIN(groupid) AS next_groupid FROM table WHERE groupid &gt; $this_groupid trivially inexpensive if there's an index on groupid 
I'm doing `groupid+1` in code, not with SQL Queries
&gt; Save every script you write, you will thank yourself later. Some helpful ways to do this is sql prompt or apex complete. If using MSSQL, look into [script solutions](https://technet.microsoft.com/en-us/library/ms167154(v=sql.110\).aspx). Then, create projects for each kind of task you're working on (ie, deployment or client). Those scripts can even be placed under source control with products like Redgate.
&gt; ^(.*)$ing Regex is awesome. He's using symbols instead of saying "Fucking". Not sure what platform you are on but MSSQL has garbage-tier support for regular expressions.
I would guess that most are badly designed by application developers that have very rudimentary understanding of SQL. Or over normalised by junior DBAs because that is how it is oftern taught.
so the answer is, why don't you try it in sql?
It's far, *far* easier to deal with gaps in an index than it is to constantly re-numerate your database. It's also dangerous to re-numerate your database. Just get the next ID instead. You can either select it dynamically, or you can build an index to the IDs. Stop building pieces of your SQL in the host language. Use SQL directly instead.
she's trying to do live lookups for next and previous, like a traversal, so rank - which sorts everything at once every request - isn't what she wants
It's not uncommon for an application engineer with little or no formal database training to implement anti-patterns in the course of designing a database. However, some of the issues that you have highlighted such as separate sales tables shows that the developer has little or no formal programming training either, at least in OO design. If the plan is to maintain this application for more than a year, I would seriously consider re-architecting it to align with accepted design patterns and best practices.
Having a lot of joins isn't always indicative of bad DB design, especially in a well normalized database. The more a database is normalized, the more tables there are, the more joins are needed to retrieve all necessary data, especially for reporting purposes
It's definitely common at my small shop, even among the "database guys". I had to argue with 3 different people that having one table (about 50 rows) with 6 varchar fields was better than having 6 lookup tables with between 2 and 5 rows. The data storage difference is trivial and it makes it 6x easier to write your stored procedures.
&gt; spreadsheets and MS Access Yeah, I've seen places with that legacy. My experience there was there were only a few tables that need to be mastered to figure out the system.
What's wrong with that? If you need to pull from 31 tables then you need to pull from 31 tables. They could just be normalized. Better than few joins and flat tables which are a pain in the ass to update or do anything with.
Well to be fair that's a discussion about the pros and cons of the degree of normalization a new set of data will need. That's not indicative of a bad shop at all. **Also,** you could just set up a view so that you get the added benefit of the normalization and the increased efficiency and simplicity in writing your procedures.
I really like the alt selection trick, I hadn't known about that, but it would be more useful if my text were perfectly formatted so the things I'm trying to change would be lined up. I currently use [SQL Pretty Printer](http://www.dpriver.com/products/sqlpp/ssms_index.php) and I love it, but it doesn't accomplish exactly what I'm talking about. For instance, if I wanted to change the "sp" alias on the join conditions here, alt selection doesn't help: SELECT * FROM SECTIONS s JOIN SECTIONPER sp ON s.ACADEMIC_YEAR = sp.ACADEMIC_YEAR AND s.ACADEMIC_TERM = sp.ACADEMIC_TERM AND s.ACADEMIC_SESSION = sp.ACADEMIC_SESSION AND s.EVENT_ID = sp.EVENT_ID AND s.EVENT_SUB_TYPE = sp.EVENT_SUB_TYPE AND s.SECTION = sp.SECTION I'm interested in some examples where you might use this.
It was over six tables, and after I was done with it, it was down to eight joins. What's wrong with that is that that's almost never actually necessary in the real world, and when it is, you should be working with things like materialized views. Please don't argue based on guesswork, sir. . &gt; Better than few joins On what basis?
I guess my point is that a lot of people I work with have the immediate instinct to normalize everything as much as possible. The idea of "overnormalization" seems ludicrous to them. I agree that it was a productive conversation to have. The view idea is interesting. We mostly use views to manage security, but it probably would be a good way to simplify some of our relational data.
Okay. Sorry, thought your name was Sara B Veer. Hope it wasn't offensive for some reason.
&gt; Having a lot of joins isn't always indicative of bad DB design, Having done professional software work for more than 20 years, sometimes at billion dollar companies that managed terabytes of data, I can count on my hands the number of times I've seen joins in the double digits for a good reason. &gt; especially in a well normalized database Yeah, that's nice. In the real world, a well normalized database with complex queries is generally backed by materialized views and triggers. SQL gets used in two ways: for ad-hoc queries and for permanent queries. Very complex queries are almost always permanent queries. There are two ways you get to very complex queries: enormous datasets that you need to ask extremely complex questions of, and junior engineers writing bad SQL. I'll give you two guesses which one's more common. Even in the case where it's warranted, in those circumstances, with dozens of tables, you're also nearly always looking at enormous volumes of data. And, as a result, yes, every single join is expensive, and you'll be backing it up with precomputation left and right. Extraordinarily complex queries get replaced, in the real world. &gt; Having a lot of joins isn't always indicative of bad DB design Yeah. It really is. Reports that need 30 tables need to run in a reasonable amount of time. Please read chapter 3 of your SQL textbook. SQL offers tools to manage this ***for a reason***. The 31 join query I was referring to, that you know nothing about and started arguing with, was not over normalized data. Normalizing the tables was part of fixing it. It was a shoddy slapped-together organically grown set of tables at a startup where things were growing so fast that nobody had time to learn how to do it right. By and on the whole, in the real world, that's how this happens. Most software isn't complex. High join count is one of the strongest code smells in practical SQL. Yes, rocket surgery might warrant it. Yes, reporting warrants it. Reporting also needs to run in a reasonable amount of time. If you can fix it with a one-liner, which you can in SQL with views, you ***should***. Sophism that ignores practical solutions is not as smart as it sounds when spoken uncriticized.
Well you did throw out a completely reasonable scenario given a large company and large report with normalized tables. It could have been over-normalized, yes, and if that's your situation, then it makes sense that it'd be ridiculous. I said "better than just a few joins" which could mean you have a pretty flat architecture...this *can* be beneficial in certain situations but it's usually safer to have it more normalized. That's just my take, do with it what you will.
Well, best of luck to you, then.
Filemaker apps that grow to be mission critical, that get ported over as is to sql backend.
If you have 31 joins in one query out of 1000, then okay, I can maybe let that slide. Even then, I bet I can come up with a better way to get that result set or actually push back and question if every data point is really truly needed in one object. But if you have 31 joins in a sizeable number of queries, then yes, bad *architecture* lurks about. Might be in the table structure (hello, EAV) or in the reporting structure (use ETL to put data in a more reporting-friendly format) but it shouldn't be common.
&gt; What's wrong with that? If you need to pull from 31 tables then you need to pull from 31 tables. There's a nuance you're missing: grand-parent stated JOINS not tables. This means there were a lot of sub-queries or correlated queries across a small number of tables.
&gt; ...you have a pretty flat architecture...this can be beneficial in certain situations but it's usually safer to have it more normalized. One of those situations is reporting: read up on data warehousing concepts surrounding star schemas. These are reporting-friendly structures which rely on denormalization to minimize JOINs. They are able to do so because a BI system's main purpose is to query and present the data. This is different from application-driven systems where the goal is to write, update, or delete data.
Incredibly common since a lot of programmers have a very poor understanding of DB design; I find even if they understand OOP they tend to still get things very badly wrong on the DB side. I work with Rails though and 'the rails way' basically educates people to ignore many db concerns resulting in applications that need extensive re-architecturing for long term maintainability (and often performance). They even have patterns (STI) to do so! 
&gt; Fast forward 10 years and nobody knows what half of them do anymore, but nobody wants to be the one to remove them for fear of breaking some wacky functionality. Ah yes, technical debt. I know thee well.
Yeah that's why I included that remark there.
Use concat to create a key column like [CUSTOMER|PRODUCTID] and then see if that key column exists both in the SalesTable and QuoteTable select CUSTOMER ,PRODUCTID ,concat(CUSTOMER,'|',PRODUCTID) from SalesTable where salesdate &gt; '01-01-2016' and concat(CUSTOMER,'|',PRODUCTID) in ( select concat(CUSTOMER,'|',PRODUCTID) from QuotesTable where quotedate &lt; '01-01-2016' ) This will find all instances where a customer made a purchase on an item where that same customer was quoted on the same item beforehand. 
&gt; Can you show why they aren't? I can't show why any two things that aren't correlated aren't correlated. The person being asked for evidence should be the person who made the claim, not the person refuting a non-existent correlation. It's like asking the guy saying Bigfoot isn't real to prove it. The three causes I already gave for my belief should be enough. &gt; Because I can point out how normalization does equate to more tables for the same data set Cool. I already addressed this repeatedly &gt; tables in data warehouses. Almost nobody works in a data warehouse. &gt; If you are building a reporting objects on top of highly normalized tables that are in use for an OLTP system, then yes, it will equate to more joins. Unless the three things I said that you're replying to and ignoring hold. Also, almost nobody works in OLTP. I don't understand why people try to get into discussions of code by addressing them in circumstances that do not model common use. You might as well debate the safety systems in a car based on how bank robbers work.
Dear lord you're insufferable. I truly feel for the coworkers that are forced to interact with you every day of their lives. I ask for some supposedly simple proof and you come at me like a child and outright dismissing my claims with superlatives which are out-right lies (nobody works in data warehouses? Damn, I better tell me and about 100 other people our environment doesn't exist). The kicker is that you then proceed to tell us that almost nobody works in OLTP! Well Mr. Ivory Tower, if no one works in OLAP and almost no one works in OLTP, then where the fuck does everyone work then? Just learn to admit when you're not able to sufficiently answer or explain a question, it's a lot easier than bald-faced lying and the things you learn in your research should help you in your career. I still don't see any proof as to why normalization equates to less tables than de-normalization. I mean, I'm questioning your logic and you point to your logic as proof; it doesn't work that way.
&gt; Dear lord you're insufferable. I truly feel for the coworkers that are forced to interact with you every day of their lives. Thank you for your contribution. &gt; I ask for some supposedly simple proof and you come at me like a child I simply explained, politely, why the proof you're asking for isn't possible. &gt; Well Mr. Ivory Tower, if no one works in OLAP Not what I said, but, okay. (By the by, "ivory tower" means academics who don't work in the field.) &gt; Just learn to admit when you're not able to sufficiently answer or explain a question I did. And why. And you seem to be insulting me and swearing in response. &gt; I still don't see any proof Yep. It's not much different than if you asked me to prove that god or unicorns aren't real. First I'd explain why that kind of proof can't exist, then you'll tell me that I'm coming at you like a child, and that you still don't see proof. I'm actually being polite to you. Have a nice day
The databases I'm currently working with were designed by application developers and it shows. Almost every table is wide (e.g. several tables with 100+ fields), absolutely no foreign keys or constraints, and no naming standards in sight. I've done what I can with new development, but the application developers are highly resistant to change and will pitch a fit until I denormalize everything down to a single table with 250 varchar(50) fields.
Thanks, I feel better about what I have now. The FK thing really bothered me for a while.
I worked for a multi billion company where the primary key in a table was used as the source for the label on the front end. Millions of rows of data used it as the foreign key. Somebody decided to change the label and all of a sudden the financial report broke. 
I am able to write Select, Update, Alter, Delete, Insert, multiple table Joins, Union, Distinct, Unique, Auto Increment, Aliases, Between, Rank, Group By, Order By, Having, Foreign Keys, Primary Keys, Views, Aggregate Functions, Regex, Fuzzy lookups. 
Some examples: 1. In a multiple table relationship, move repeated data from feature tables to central tables (eg taking the mail label name off of the invoice table and shipping table, and moving them to the customer table) 1. Converting copied strings to FK references (copied strings on insert are depressingly common) 1. Converting 5NF down to ETNF, or 4NF up to ETNF 1. Enforcing DKNF through unique composite indices 1. Moving columns from one table to another to support a normal form that's otherwise almost done (most commonly to re-support BCNF in a system whose needs have changed: consider Wikipedia's pizza example, but the pizza chain has grown; it didn't used to have regional toppings, but now that it's international, it does, and so moving to two tables used to support BCNF and is now ass-backwards) 1. Destructuring a composite column into feature columns (name -&gt; first,last; address -&gt; street1,street2,city,state,etc) - sometimes this is 1NF, other times it's just needing a datum represented differently, eg suddenly needing to histograph the domains in a list of emails 1. Adding or removing columns to/from keys to maintain 3NF as business needs change (adding: a hosting provider that now does several operating systems, or removing: no longer does anything but this one location) 1. Moving non-key columns to existing feature tables to restore 3NF (eg we started with profile pictures, then we added alt-profiles, now we need to move the picture ref to the profile table) 1. Most changes around enforcing superkeys as indices 1. Usually, converting something in BCNF to EKNF Some people might add, but others might argue: 1. Adding FKs to tables that didn't already enforce them 1. Adding temporal 6NF to a system that didn't already have it 1. Using materialized views to represent the top line of temporal 6nf as a distinct table Generally speaking, most of this stuff is the result of needing refactoring as business needs change, or repairing existing bad design
My eye involuntarily started twitching after reading this. You can still do things right. Provide a few layers of abstraction around crud operations (stored procedures that hide how the sausage is made) and views that properly join your tables together (with correct key constraints and indexes). What they're suggesting is non-scalable mess in a very short time. I pitty the DBA that had to put up with that bullshit. 
Maybe share what you've found out with your boss, trust me everyone worth there salt is looking for better ways of doing things. As /u/alinroc mentioned using ISO 8601 means you will never need to deal with misinterpreted dates. Using it increases quality.
 No Foreign Keys is not unusual, in fact I'd say it's actually the norm in the databases I look after (note : These are third party databases). I manage one system (supplied by a billion dollar company) where some tables don't have *primary* keys, let alone foreign keys.
&gt; It was designed by a guy who made a front end application and needed a place to put the data. That explains the issue right there, and in my experience it's not uncommon. There are developers who know some SQL and then there are developers who have business doing things in the database. The first are common, the second are rare.
Also tried this, but this will not give me the continent or countries SELECT avg(LifeExpectancy) FROM country GROUP By Continent ORDER By LifeExpectancy DESC ;
The correct question to ask is "How common are good DB designs?"
When I changed the SET @num statement to use my own table I get 53, 46, 51, 52
Brew install MySQL; MySQL.server start
I'm able to just use a select statement and have it convert from 5.347 to 5 but when I try in insert it, I get the error &gt; Conversion failed when converting the varchar value '5.347' to data type int. My source table where the value 5.347 is data type varchar. My destination table has the column as int. It seems to be giving me trouble when it is trying to convert so it can be inserted into the destination table but not an issue when just using a select statement &gt; SELECT CAST(CAST(COLUMN AS FLOAT) AS INT) FROM TABLE 
"The easiest way to get started with PostgreSQL on the Mac" http://postgresapp.com/
[removed]
Thanks I will look into these, thank you for the hints!
after reading several comments like this it as become clear that I need to brush up more on database language and structure basics... Any good sources/books/toilet paper writings?
Forgive my ignorance. I finally went to the wiki on the sidebar!
How do you plan to use the attribute?
What do you mean?
THANK YOU SO MUCH!
Give this Wikipedia page a quick read: https://en.m.wikipedia.org/wiki/Entity–attribute–value_model Apologies in advance but I am on mobile and note sure how to format a table... Your table could be something like this: (_ID, EntityIDType, EntityID, AttributeType, AttributeValue) Your records could look like this: (n, ReadingID, 1001, Priority, 0) (n+1, KanjiID, 1001, Priority, 1) ... It would be best to model the Attribute as a separate Entity (Dimension) and use the AttributeTypeID rather than the AttributeType directly within the table as I've shown above, but I don't want to add any unnecessary complexity for you. 
contrary to the other suggestion, you do not want EAV, you want supertype-subtype the supertype has the relationship with priorities, while the two subtypes have their own separate attributes
couple of really useful MySQL functions -- * GROUP_CONCAT * SUBSTRING_INDEX 
`order by` with the ordinal of the column is [no longer valid in the SQL spec (as of SQL-99)](http://www.justskins.com/forums/sql-is-it-possible-46695.html#post141509). Microsoft advises against doing so in [their best practices](https://msdn.microsoft.com/en-us/library/ms188385.aspx): &gt;Avoid specifying integers in the ORDER BY clause as positional representations of the columns in the select list. For example, although a statement such as SELECT ProductID, Name FROM Production.Production ORDER BY 2 is valid, the statement is not as easily understood by others compared with specifying the actual column name. In addition, changes to the select list, such as changing the column order or adding new columns, will require modifying the ORDER BY clause in order to avoid unexpected results.
As per [the MySQL documentation](http://dev.mysql.com/doc/refman/5.7/en/create-database.html): &gt; CREATE SCHEMA is a synonym for CREATE DATABASE. See also [this comment on SO](http://stackoverflow.com/a/1911215) regarding how different implementations deal with schemas. 
&gt; GROUP_CONCAT I suspect that the STUFF operator in MSSQL is used more often as part of a work around for a lack of GROUP_CONCAT than it is used for the intended purpose. Why oh why won't MS just implement this.
You mean FOR XML PATH('')? STUFF is just string replacement.
 ^ is the start of a line ( starts the capture of an expression . matches all characters * means match the preceding character 0 to many times, the preceding character in this case is all the . wildcard meaning all characters ) ends capturing an expression $ is the end of a line This is the basic regular expression I use to wrap a line in quotes or what not. Ultimately you should read up on regex it will save you time in the future. /u/VaporDotWAV is right about MSSQL regex is crap. However 2016 is better as it uses what VS is build on. You can also use Notepad++ or your editor of choice.
To give you some rough ideas on where to start: 1. Create a subquery inner joining academics to papers which returns just an acunum. Then in your outer query, select every academic who isn't in the subquery. Look into using NOT IN. Bit of an odd question as there are ways of tackling this that don't use subqueries. 2. Similar to the last question, but your subquery needs to be grouped and use a HAVING clause with a COUNT to get those with more interests. Be careful to make sure you're counting distinct interests and not records. 3. To meet the requirements of the question you'll need to select research fields that aren't in a subquery using EXCEPT. 4. First, you'll want a subquery that gives you distinct academic ID + field name. Use that subquery in the FROM clause for a grouped query that will group on the field name and give you a count. Easiest but not most elegant way forward from there is to then use all of that as a subquery for another query that would return top 5 * ordering by the count of academics. SELECT * from (SELECT fieldname, count(*) NumAcademics FROM (SELECT DISTINCT fieldname, academic ID FROM [your joins here]) x GROUP BY fieldname) y WHERE rownum &lt;=5 ORDER BY y.NumAcademics 5. The trick to this one is to make sure you include people with NO papers, which would normally be excluded by inner joining to the papers. So do a subquery that uses HAVING to get people with MORE than 20, and select academics NOT IN that subquery. Hope this helps. Also if you find yourself turning to random redditors for help on an assignment it's a good clue that you need to reach out to your teachers and tutors for support - it's what they're there for.
You can't use TOP 5 in an Oracle query. You'll have to use a rank() in the inner query then apply a WHERE clause to it.
Oh yes you're right. I'm not an oracle expert (obviously) but wouldn't rownum &lt;=5 be easier than using rank(), as per my edit? Or would you still need that from a subquery to get the rownum to work properly?
Sure thing. One thing that always helps me when writing complex expressions; break it into it's individual pieces. The computer always does exactly what it's told. If it isn't behaving like I expect: that means, most likely, one of the functions is returning a value different than I intended. 
&gt; They don't want to pay for access to any real development tools so... There are several good open source options. Those are always free.
Man, Oracle sucks... In order to connect to an Oracle database, you need a lot more information than just a hostname and a port number. So they invented these awful configuration files called "tnsnames.ora" that have to be setup with connection information on each &amp; every computer that wants to connect. Usually when you install the database server, it will create a "tnsnames.ora" file somewhere under the *server* directory. That needs to be copied into your Oracle Client folder before you can connect. In my case, my Oracle client is using the path C:\Oracle\product\11.2.0\client_32\network\admin And my Oracle database engine (which, in my case, comes embedded in *another* Oracle product) is located at: C:\oracle\E1Local\NETWORK\ADMIN So figure out where your engine is installed and go *make a copy* of its tnsnames.ora file. Place one copy into your client folder and you should be able to connect.
&gt;I can count on my hands the number of times I've seen joins in the double digits for a good reason. You must not get out very often. I'm working on a label printing application that uses 11 JOINs just to load product info. A high join count is simply what happens when you have large and well structured data.
&gt; You must not get out very often. I'm working on a label printing application that uses 11 JOINs just to load product info. Uh huh. &gt; A high join count is simply what happens when you have large and well structured data. Yeah, I see a whole lot of people saying this, while ignoring the specifics I already gave about my own opinion about why this isn't true. Have a nice day, sir
I may need more info on this one!
THANK YOU!!!! YOUR LINKS ARE EXTREMELY HELPFUl! :)
&gt; supertype-subtype The attributes are exactly the same, only differing in the source that they are related to, which is either a Kanji or Reading Element. I feel like for this reason a supertype-subtype relationship would be unneeded, because all information needed would already be conveyed in the supertype entity with the inclusion of the Type attribute. The only attribute necessary in the subtype would be the foreign key relating it to either a Kanji or Reading Element entity. Is this really the right way to do it?
Follow-up question.... So we're getting our initial database input from [this google sheet](https://docs.google.com/spreadsheets/d/1_zX-UzxtyF794erQMmQEDrStEB7IAChU7gcJeCu144E/edit?usp=sharing). Is there an easy way to parse the information from that sheet into our database using SQL or node??? (I'm aware the sheet is currently empty but the top row at least shows what info we're going to be storing).
Put your translations in a table and add it in via a join. That way, if you need to add another location you just add another row to the translation table.
Use [Google Sheets API](https://developers.google.com/sheets/quickstart/nodejs).
Thanks, that does help!
MySQL is weird in that there is one database server, with one database, with many schemas, but it calls the schemas "databases" Others, like Postgres, Oracle, MS, have one database server, with many databases, with many schemas
Look up the 'Alter table' command and how to add a foreign key.
To be clear, the foreign key in the SECTION table is pointing to the CourseID column in the COURSE table? If so, you're already good. You won't be able to insert a row into SECTION with a CourseID that doesn't already exist in the COURSE table.
I understand that but to think like a data analyst or data scientist takes time and so I feel using it is just a part of the equation. I find online classes help you to think in that way for future use. But thanks for the suggestion. 
http://www.sqlcourse2.com/ has been alive for probably at least a decade.
Thank you!
Of you're writing the SQL out for that, you can use an ALTER TABLE ADD CONSTRAINT for that referential integrity. That will ensure that the record for the course ID exists in the table before anything can be committed to the Section table 
I would recommend split these categories into individual flags (over 21, under 24, 1YR1, etc.) and then have a translation table from a collection of flags to a category.
Use knex for less obscure query building.
yep it work. thanks for the answer. just new to sql so learned a new trick max(case when ...) and dts is just a temp data table right?
I'm sort of a beginner. But I would imagine you would try to use your wildcard while parsing only the first word of the address to check for any similarities. Forgot the actual terms so sorry if that doesn't help.
It's going to take forever. Each 'fuzzy' address you want to search for will need to be scanned against the entire larger table. That's the big problem with your situation. If that is manageable for you, then you need to come up with some kind of algorithm that evaluates each possible match, ranks it, and then takes the top match for the join (or joins on all of the matches over a given % --&gt; which means duplicates) A simple way of looking at this would be to take `100 MAIN RD.`, remove the spaces so it becomes `100MAINRD.` and then evaluate each positional character to see how well it matches to your other records... so `100MAINST` would match on 7 of the 10 positions (70%), or you could remove the `.` or come up with rules that take into account the record is 1 character shorter... come up to about a 78% match. That would be a lot of work. A simpler and clever way to handle it might be to parse each column and take the largest "cluster" of characters, something like this: | FirstName | LastName | Address1 | Address2| City | State | ZipCode | Phone | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | | Samantha | Von Miller | 16719 Main St. | NULL | New York | NY | 12345-1234 | (555) 555-5555 | Becomes: | FirstName | LastName | Address1 | Address2| City | State | ZipCode | Phone | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | | Samantha | Miller | 16719 | NULL | York | NY | 12345 | 5555 | And then proceed from there to match, and do it for every column instead of just the address, or do a check to see if you can join on each column and only take the ones that match on multiple columns (last name, zip, city?)
Lots more similarity algorithms too: jaro winkler, LCS are two that have worked well for me, phonetic ones like sounded and double megaphone work for smaller typos. Thoroughly standardizing and parsing addresses first is crucial though. Similarity scores don't help if the strings aren't actually similar. Another potential option is to use Google maps api or a third party service to get Lat/long and map.
&gt; Another potential option is to use Google maps api or a third party service to get Lat/long and map. That's a really clever idea. 
Ok that makes sense I'll try that now. Can I ask you another question? When I post any question on this sub I always get downvoted to 0 without fail. Am I structuring the titles wrong or posting to the wrong sub?
CAST(ParamValue AS varchar(32)) will not change your decimal places Check that you do not have any cast or column with data type with scale 2 
Okay, so I'm assuming SQL Server on this. Pretty sure you've got a misunderstanding about data types. When you declare a variable or create a column in a table, you also decide its datatype (Numeric, Varchar, Datetime2, etc...) so whatever you declare it as is what it will store the data as. When you're querying that data, it will return in whatever format its stored in. You can change that format by using CAST() but only if the originally stored value can be explicitly converted to that format. Example; SQL Server couldn't cast the string "123ABC" to a number because it has alpha characters in it. Based on your description; it sounds like you have a column (ParamValue?) which stores varchar datatype data in it (both characters and numbers). It sounds like you want to CAST the numbers as NUMERIC(##,5), replacing # with valid number length, and leave the character strings alone. To do this, look into the CASE and LIKE statements. Basically when the string contains only numbers, CAST as NUMERIC, else just the original value. Hope that helps!
Hm, sounds like you want us to do your work... Post what you already have and what parts are giving you problems. 
I haven't run into that error, but here's how you can write it without a CTE. SELECT TaskID ,TaskName ,Duration ,timestamp1 ,CONVERT(char(9),DATEADD(ss,Duration,timestamp1),108) AS endtime FROM (SELECT TaskID ,MAX(CASE WHEN ParamName LIKE '%Duration%' THEN TaskName END) AS TaskName ,MAX(CASE WHEN ParamName LIKE '%Duration%' THEN CAST([ParamValue] AS float) END) AS Duration ,MAX(CASE WHEN ParamName LIKE '%timestamp%' THEN ParamValue END) AS timestamp1 FROM View_table WHERE Id = {?ID} GROUP BY TaskID) dts
select month(RequestDate) month, year(RequestDate), craft, count(case WHEN actualcompletiondate is null or month(ActualCompletionDate) &gt;= month(requestdate) THEN 1 ELSE 0 END) from mdworkorder where ActualCompletionDate &gt; '8/31/15' AND Purpose = 'General Maintenance' OR ActualCompletionDate is null AND Purpose = 'General Maintenance' group by month(RequestDate), year(RequestDate),craft I figured I'd start by getting number of work orders that are starting for each month. As you can guess, I'm very new to this - and trying to teach myself so I come here for help. A few issues I already know with this coding - 1.) If there were no tickets of a certain craft made in a particular month, it will not show up as null, even if there are some open work orders remaining. 2.) Figuring out the proper way to count a past month if a work order has been completed in a more recent month. Thanks. 
The thing is that the rows are not uniquely identified by one name or id. In the case of our import they are uniquely identified (a call record) by this combination [channel,Date,Timestamp,Sender,Destination] it is meta data from calls and the 5 columns above represent a unique call. But the channel, sender and destination is junk information as we never use them in our business logic. More specifically, we don't want certain of the original data to be present in the database and in this case some of it is in the PK .
Haha.. no idea, I won't down vote anything unless it's spam or blatant product advertising. The title was perfectly clear to me :)
Haha man I was thinking of throwing in a lot of stuff but didn't want to over do it either :P
Well for the first one, you're close. I'm not sure if `BEFORE` and `AFTER` are actual keywords in MySQL, but I don't think that's what you want anyway... since you care if they were in the band DURING 2005... so you would want to use `&lt;=` and `&gt;=`. You also won't get any results unless they left the band after 2005... since if they're still in the band the dateOUT is `null`, so you have to account for that: SELECT DISTINCT artist.artistID, artist.fName, artist.lName FROM band, artist, inBand WHERE band.bandID = inBand.bandID AND artist.artistID = inBand.artistID AND band.name = 'Animal Collective' AND inBand.dateIN &lt;= 2005 AND (inBand.dateOUT &gt;= 2005 or inBand.dateOUT is null); Note the use of parenthesis to make sure that one of the two dateOUT checks is true
Mine are similar but I have additional programming / networking / system qualifications. ~80K or 40/hr.
 SELECT SUBSTRING(StringA, FilenamePos, PathLen - FilenamePos + 1) FROM ( SELECT T.StringA, LEN(T.StringA) - CHARINDEX('/', REVERSE(T.StringA)) + 2 AS FilenamePos, LEN(T.StringA) AS PathLen FROM #Test AS T ) X
Thank you! 
While I'm absolutely in love with SQL Server, I find the rest of Microsoft's development stack to be toxic. SSIS included. If you can't do it in pure T-SQL, look to outside tools (preferably open source) for manipulating data and import/export kind of stuff. PHP was the one I used back in my day, and is still my go-to for a lot of string manipulation. Node is a much more modern and sane way to go about the same work. Even Excel can be a useful tool for writing one-time data imports, you can write a formula using string concatenation to build a bunch of INSERT statements into a table variable, and then use T-SQL the rest of the way from there.
Is it okay to provide a table structure for the involved tables? Dummy records would be awesome too but I know that can be a real pain in the butt (maybe PM)? You've intrigued me with the ridiculous mess of a script you have and there's GOT to be a way better way to do this... edit: by "table structure" I meant the SQL for the create statements on the tables, you clearly posted the screenshot of the relationship. 
 declare @t varchar(50) set @t = 'Test Data/Report/Field.xlsx' select REVERSE(SUBSTRING(REVERSE(@t),0,CHARINDEX('/',REVERSE(@t)))) 
I've done this on several occasions. Less likely to screw it up once halfway down when you let a computer do it for you :P
I would leave this stuff to creative web developers. In DBA world it counts what's in your head and not how fancy your business card is. You have to be reliable, not necessary creative. If you are a genius though you can probably get away with being eccentric. 
I dig it. Was thinking exactly this. Thanks for the input. 
Hah... I'm not a genius... but my marks are high (again... not a genius :P). I appreciate the input. Thanks for the reply. 
P.S. I'm a CTO that rewrote a TCL procedure a week ago because i'm down a programmer. A junior programmer on my team had the balls to rewrite my procedure. His was better than mine. We went with his.
I should be able to get something together for you. It won't be hard to remove any personally identifiable information. I'll try and do that when I get some spare time at work (though I'm very busy right now). Thanks for your interest!
Someone probably said they should use a SSIS package instead. You'll probably find out weeks later that John talked to Bob in the hallway, or you'll find out that Bob didn't like it because it wasn't their idea. You didn't kiss the narcissist ass and make it their idea first. Force them to be your teacher. Read 48 Laws of Power.
I'm not saying it's disrespectful or anything. I just want to know what I can add to make it look a senior level person done it. I'm not a senior so I don' mind but I want to improve my TSQL skills and hopefully prevent this issue from happening again. Can I send you my code and have you critic it? Lesson learned to always add comments. 
Did you have cursors and/or while loops in your code? I would fail everyone who writes cursors/loops and it is possible to do the same thing without
I could even use the hash as the PK... How much slower does the system then become if I have to rehash the composition to compare with the original?
I actually think it looks good as it will show that you are willing to learn and improve. Provided that you will ask in this context. 
Yes! I created a while loop so the SP can loop through each record. Each row has its own unique id so I just used a while loop. No cursors as I know cursors are not optimal. So is it best to use a CTE instead for a loop through each record or what is a better way of doing it?
What happens if your import job fails? Will it roll back the changes or will it leave the database in an inconsistent state? NOCOUNT I use for running SQLCMD from command line in a batch file.
Yes.
I think here is your issue. Loops are as bad as cursors. Not sure the use case but i think the part where you used if you can use a case in the query itself select col1, col2, case when col3 = 'A' then 'valid' else 'whatever' end as calc_col from table where condition = true
What would I use to loop through each record? I have a bunch of case statements in my insert query which is nested in a while loop.
I think they wanted you to work in Sets rather than looping through Rows. Yes, looping through each row works, but how's that going to work if you have millions of rows? Without knowing the exact question they asked and seeing your code, it's hard to say exactly how to change your code. 
This is the answer. Almost guarantee a cursor was not the correct solution. Write set based logic using a case statements. Use a sub query if you need to set the data up differently. Bottom line, while cursors and loops have their place I'm sql. They will never be as performative as a set based logic query
What do you mean by work in sets? That would be so many transaction. What is another way of looping through records besides cursors and while loops? CTE?
Like in your example "If value is A then input the word VALID", you probably made an IF statement inside your WHILE, instead you could do something like "UPDATE Table SET Field = 'VALID' WHERE value = A" without any loop. Cursor, while loop and recursive CTE all have bad performance because they verify row by row, if you do the UPDATES using WHERE/GROUP BY HAVING/OVER and so on, the engine will look into the whole set instead of only the row.
This is either homework or a job interview. Neither of which you are going to have someone give you the answers for.
Not a job interview and not a homework.I got them from a friend and solved them all.The thing is, I don't know which are correct.
why don't you try to create the database and run the select statements using some sort of rdbms software?
Yeah no problem! Take your time. I think it can be solved by generating each semester's data by a simple cross join with a Tally/Numbers table, limiting to just 8 numbers (for the 4 years * 2 semesters)... Probably in a CTE too to avoid the subselect duplication you've got going.
Based on my experience (I've been an analyst for years and now hire quite a few analysts), very few organizations are looking to pay relo for data analyst work. They either accept that they will hire someone to work remote, or they focus on hiring local. I've worked for an uber-large bank and they would only hire local among three major sites. I now work for a smaller software company and am remote (even as a manager) and have hired remote analysts. You just have to find the right company! I just checked and we don't currently have any openings, otherwise I'd let you know. Good luck.
NYPA
Yup, 1 UPDATE statement for each needed business requirement, but if several columns needs updates and fit the same requirement then only 1 UPDATE is needed, like: UPDATE STAGE T SET T.column1 = B.Column1, T.Column2 = 'something', T.column3 = T.column3+1 INNER JOIN PRESTAGE B WHERE T.ID = B.ID AND WHERE B.VALUE = A
perfect time to fiddle with the http://www.sqlfiddle.com/ 
just did ! 
For wildcard comparison you should use `LIKE` operator, and for regexp in Hive it would be `RLIKE`: where cmdy_code RLIKE '^[a-z][a-z][0-9][0-9]$' or cmdy_code RLIKE '^[a-z][0-9][0-9][a-z]$' If you want that to be a case-insensitive comparison, the string should be something like `'/^[a-z][0-9][0-9][a-z]$/i'`
sorry for the confusion... I want one flag per record assigning each record a 1-7 depending on which rule it fits into
Depending on the encoding that was used for the HTML documents, you may be able to CAST or CONVERT that data (perhaps using multiple steps) to NVARCHAR(MAX).
Check out any postings in Colorado.
You wouldn't necessarily need more than one statement even with multiple business rules. So say one business rule was if value1 was 'A' input the word VALID into field1, and another business rule was if value2 was 'B' put invalid into field2. You could do them both in the same statement: UPDATE table SET value1 = CASE WHEN field1 = 'A' THEN 'valid' ELSE value1 END, value2 = CASE WHEN field2 = 'B' THEN 'invalid' ELSE value2 END etc. 
here is what i wrote. any issue? case when [Total Spend]&lt; 1 then 1 when [Opportunity Spend]=0 then -1 when [revenue]&gt;[Spend 50th] and [revenue]&lt;[Opportunity Spend] then 2 when [revenue]&lt;[Spend 50th] and [revenue]&lt;[Opportunity Spend] and [revenue]&gt;0 then 3 when revenue &gt;0 then 4 when revenue &gt; [Opportunity Spend] then 5 else -1 end as [Opportunity Classifier] 
We're running our ETL data warehousing in a VM... 16 vCPU, 1TB+ disk, 64GB RAM last I looked. I don't know if I'll ever create a physical SQL server again..... I don't see a need to and it prevents having to mess around with migrating to new hardware... just moving the 60GB VHD to a new Host, configuring any special networking it had, and booting it back up is just too nice.
I would only "scale out" if I had some need for extreme disk speeds and could live without AlwaysOn availability. Then I would strap terrabytes of SSD directly to the PCI-E bus of a dedicated server. Outside of needing extreme disk performance, I wouldn't ever build outside of VMware. The ability to move &amp; redeploy an existing server onto new hardware is just way too valuable to ignore.
1. That's a particularly tricky one to solve. To make this happen, you need to generate a data set that contains all the months &amp; years you want to display, then take that data set and LEFT JOIN it to the one you're building now. 2. Take a look at this. I've changed some of the logic a bit. SELECT MONTH(RequestDate) as [month], YEAR(RequestDate) as [year], craft, COUNT(\*) as TicketsOpened -- How many records exist for this month + year + craft SUM( CASE WHEN MONTH(ActualCompletionDate) = MONTH(RequestDate) AND YEAR(ActualCompletionDate) = YEAR(RequestDate) THEN 1 ELSE 0 END ) as TicketsClosed -- How many records were closed this month FROM mdworkorder WHERE RequestDate &gt; '8/31/15' -- All records have a request date, so filter on it instead of close date. AND Purpose = 'General Maintenance' GROUP BY MONTH(RequestDate), YEAR(RequestDate), craft ORDER BY -- Apply consistent sorting MONTH(RequestDate), YEAR(RequestDate), craft EDIT: Who knows why that SQL statement won't format correctly. There are 4 preceding spaces for each line in the query. **FUCK** Markdown.
Well,one problem I'm seeing is that you're naming things that aren't in your column list, so they have to be computed. Or you're calling things by different names in your column list (and screen shot) than you are in your case statement. I'm guessing you're just calling things by different names SELECT CASE -- WHEN [Total Spend] &lt; 1 -- Customer has 0 spend in any product. -- Can't work because you're not grouping by customer, so instead WHEN [Revenue] = 0 -- Customer has 0 spend in THIS product. THEN 1 -- WHEN [Opportunity Spend] = 0 WHEN [Total Opportunity] = 0 -- No expectation of any revenue for thus customer/product THEN -1 -- WHEN [revenue]&gt;[Spend 50th] and [revenue]&lt;[Opportunity Spend] WHEN [Revenue]&gt;[50th percentile] and [Revenue]&lt;[Total Opportunity] THEN 2 -- WHEN [revenue]&lt;[Spend 50th] and [revenue]&lt;[Opportunity Spend] and [revenue]&gt;0 WHEN [Revenue] &lt; [50th percentile] AND [Revenue] &lt; [Total Opportunity] -- Why is this here, is 50th percentile ever greater than total opportunity? AND [Revenue] &gt; 0 THEN 3 WHEN [Revenue] &gt;0 THEN 4 -- WHEN revenue &gt; [Opportunity Spend] WHEN [Revenue] &gt; [Total Opportunity] THEN 5 ELSE -1 END AS [Opportunity Classifier] FROM loct99Table 
If they have to have it as an email, my goto way is c# code sending emails with reports rendered in SSRS. Usually orchestrated by SSIS when different recipients need different reports/parameters. Though I guess you could just ask if on demand reporting in a browser would fit their needs and use something like powerbi.
Reporting Services are your best choice here. Create a report, set up email subscriptions - and that's it!
If you wanted absolutely bulletproof code, handle 4 in its own WHEN statement, but this code should work for all of the cases so long as Revenue is not null.
Problem is, I am trying to send out multiple views within the same email body. I definitely do not want these people getting 6+ emails every morning. 
Can't a report have as many Tablix objects as you need?
Power bi on phone is dashboard tiles only so you have to be a bit clever about your modelling, but that runs trye for any reporting that can deliver value by quick glances from a mail client during meetings where the end user cant be arsed to slap it together in the online report editor in 15 seconds anyway. System.net.mail in v# helps you send emails. HttpClientConnection lets you interact with SSRS web service and stream the report renderings into email attachments. I got more info but it would be faster to build it than to explain it. Let us know about your solution, sounds interesting.
Hmm - just did a bit of testing on this. Tickets opened seems to work well, but Tickets closed only counts correctly if the ticket was both created and closed in the same Month/Year- anyway around that? Or am I going to have to create 2-4 invidiual queries as follows: 1. Sql Query that has the years / months I want to report on. 1. SQL query that shows total number of work orders at beginning of month by craft 1. SQL Query that shows total # of work orders opened by craft / month / year 1. SQL query that shows total # of closed work orders by craft /month / year 1. SQL Query that shows # of work orders left open at EOM Then join them all together ? Thanks for your help! 
Recursive CTEs are useful for a couple of common reasons. Hierarchies (Org Charts, Bill of Materials) and also for generating sequenced data sets (say, the dates for every weekday from now until the end of the year). Table variables are nice because they only exist for the scope of your query. You don't have to fudge with much DDL or use any temp tables... Combined with UPDATE FROM, they offer a quick way to stage data for mass updates. Two of my favorite tools for custom web development include PHP (old school) or Node.js / NWJS.
Well this isn't actually MS SQL if you're using SQLite... you don't have to actually run the movies.sql file... the line commented out at the top of the file tells you the command you need to run: sqlite3 movies.sqlite3 &lt; movies.sql
Yeah, you're gonna want to look into Common Table Expressions to help. Basically you will define a bunch of queries in a row and then join them together to get one final result set. Here's a way to get your years &amp; months: DECLARE @startDate DATETIME2 = '1/1/2015'; DECLARE @endDate DATETIME2 = '12/31/2016'; WITH dates as ( SELECT @startDate as [date], MONTH(@startDate) as [month], YEAR(@startDate) as [year] UNION ALL SELECT DATEADD(m,1,[date]), MONTH(DATEADD(m,1,[date])), YEAR(DATEADD(m,1,[date])) FROM dates WHERE DATEADD(m,1,[date]) &lt; @endDate ) SELECT [month],[year] FROM dates ORDER BY [year],[month]
You could try and see if running SQLCMD post-install would work, since `NT AUTHORITY\SYSTEM` should have access: sqlcmd -q "CREATE ROLE new_role" Not sure how the SCCM install process runs if you can setup a "post install script" or something similar...
Table variables also cannot have indexes, while temp tables can. They cannot be used by the query planner either, so using them can mean that you don't get optimal performance. Table variables have their uses, but they are not a drop in temp table replacement.
Not a drama to run an SQL command as the whole install is in a Powershell wrapper. I didn't realize that the SYSTEM account has access! That should make things easier. (I'm not a DBA unfortunately, I'm in operations!) Thanks for the response!
Look into EXISTS condition
Also, I'm not sure why you're using `substring` here. Any reason why you want to use only first 70 symbols?
 select customerId, count(*) from orders group by customerId order by count(*) desc limit 1
What would you suggest I learn, I would like a SQL (platform??) that has a very reliable instruction set? I've coded for half a semester in c++ and that's my largest experience. So I'm not totally oblivious to code, but I'm not seasoned by any means. That's why I'd like to have a guide to teach me all of this, but not sure which is a good platform to learn.
Using a password is security through obscurity. It works because it's obscure enough.
 REPLACE(REPLACE(REPLACE(CONVERT(VARCHAR(MAX),TEXTFIELD),CHAR(9),' '),CHAR(10),' '),CHAR(13),' ') If your text fields are less than 8000 chars then using a smaller VARCHAR might yield better performance.
I hate this, our ETL has about 45fields where we have to perform multiple replaces because the system didn't implement any validation other than types. (I.e. dates from the future, special hidden characters from emails pasted in to varchar fields etc.)
Yeh but those 5 to 10 seconds of inconvenience are a still a layer of the majestic onion of security
I'm on mobile but I'll take a crack at it. Table: family Id | parentId | name 1 | null | Jonathan 2 | 1 | Chris 3 | 2 | Mark SELECT * FROM Family f1 INNER JOIN Family f2 ON f1.id = f2.parentId; --WHERE f2.parentId IS NOT NULL; This links Chris to Jonathan with Jonathan being the parent of Chris and Chris to mark with Chris being the parent. Printing the results is too much to type with this tiny keyboard. If anyone has alterations, please hop in. Edit: removed the where. Thanks, guys. 
We use ssrs and render as pdf, send email and everyone can open them from email. If it must be in the body, as images, we use scripting to render the report which just consists of several charts with a page break, script then saves each page as an image and send emails using embedded images.
Yes, just use a tablix for each view and the are good to go. Depending on if they want the content in the body, or as an attachment. 
it is good but I don't think you need this &gt; WHERE f2.parentId IS NOT NULL as the join condition will take care of it
The substring was originally set to 8000 characters (the max value) but I decreased it down to 70 in an attempt to see if that was the issue limiting the text. It does indeed look like the issue was with the default cap being applied to the char that was causing my text limitations. I was able to pull out a query with most of the line breaks resolved in the resultant .csv, but I still have some present. Any ideas as to the cause?
This could be an option for getting the data out. I primarily need to pull out an excel sheet of the data from the MS SQL database where one of the fields is a rather large text field, occasionally containing multiple line breaks. Any thoughts, or links that I can reference on linking the DB to Excel?
Smells like homework.
That is a pretty cool concept.
SQL Server Browser Service works for non-standard ports anyways...
You can create a SQL Agent job with three steps here: 1. Drop/disable indexes 2. Run the SSIS package 3. Create/rebuild indexes Agent job can be triggered by the builtin SP msdb.dbo.sp_start_job.
We use temp tables for lots of end-of-month or end-of-fiscal year processes (EHR Vendor, our software does data collection for reporting to state governments on services provided by our customers). These process can operate on millions of records, and taking the time to create and index temp tables can be a big deal for us. I've lost count at this point of the number of stored procedures and reports I've "optimized" by removing table variables and replacing them temp tables, even without indexes. YMMV, obviously, and it all depends on your organization, code base, and the details about your data, but I wish table variables could stop being a thing at all.
You can avoid excessive IF logic if you get return valuefrom the query: SET @returnValue = ISNULL((SELECT UserTypeID FROM TicketUser WHERE Username = @theUserName AND UserPassword = @theUserPassword),0)
I would suggest same as /u/ichp , you can use a simple SELECT...FROM sys.columns... to get what you want. If you still want to use SSMS, you can always open Object Explorer Details (F7) and sort columns by name there.
In the SSMS, check if the query you are running is in the right Database, like in this [image](http://sqlserverlearner.com/wp-content/uploads/2012/09/Available-Databases-Dropdown.png) Your query works when you click in the table because the SSMS sets the current database as the one from the object you are selected. EDIT: Is a good practice to write: USE [DatabaseName] in the top of your query to make sure it will ... well ... use the right database.
SQL 2008 ... it won't be a pretty solution, if you were running SQL 2012 or above the LAG() function would be available. Check [this link](http://blog.sqlauthority.com/2011/11/24/sql-server-solution-to-puzzle-simulate-lead-and-lag-without-using-sql-server-2012-analytic-function/) look for the code that like the best
&gt; I need to get the substring of this column starting from the first space after the colon until the first space after the first set of numbers. just to confirm, you want both those spaces included?
Don't think I understood what he wanted :) 
That's one of the benefits of hard-coding the server name into the query. If you're accidentally in the production database in SSMS you won't be able to execute it (even if the tables/databases are the same) because the server name will be wrong.
Use Regex. That'd be your best bet!
Not 100% sure on SQLite syntax, so you might have to play around with it, but basically it doesn't know _which_ `name` column you're referring to. You can solve this in a couple of ways, but basically you'll have to explicitly list which table to search for the column and if you want to check both tables, you'll have to use an OR clause to do so: SELECT foo, bar, ray, do FROM tab1, tab2 WHERE tab1.name="Kevin" OR tab2.name="Kevin" You don't need to explicitly list which table the initial columns are coming from, but it might make it easier to read and would make it "future proof" if you ever add a column with an existing name: SELECT tab1.foo, tab1.bar, tab2.ray, tab2.do FROM tab1, tab2 WHERE tab1.name="Kevin" OR tab2.name="Kevin" 
Probably want something like this in case you have multiple nulls. Although it needs a correction for the remainder. WITH Split AS ( SELECT * FROM ( VALUES (1,'A',NULL) ,(1,'B',50 ) ,(1,'C',10 ) ,(2,'D',NULL) ,(2,'E',NULL) ,(2,'F',10 ) ,(3,'G',NULL) ,(3,'H',NULL) ,(3,'I',NULL) )tbl(id,name,percnt) ) ,SplitSummary AS ( SELECT id, AssignedPercnt=SUM(percnt), nulls=COUNT(*)-COUNT(Percnt) FROM Split GROUP BY id ) SELECT s.id,s.name, percnt = COALESCE(s.percnt,(100.0-ISNULL(AssignedPercnt,0))/NULLIF(nulls,0)) FROM Split s JOIN SplitSummary y on s.id = y.id ORDER BY s.ID, s.name
okay, try this -- SELECT SUBSTRING(chunk,1,CHARINDEX(' ',chunk+' '))-1) AS ResultingCol FROM ( SELECT SUBSTRING(Col1 ,CHARINDEX(': ',Col1)+2 ,LEN(Col1) ) as chunk FROM strgs ) AS s1 
Excellent tips! Will try this when back at work. I should probably get to know SSMS itself as an application a bit better. 
Interesting things happened when I tried these. The first one pulled up 0 results but didn't give me a syntax error like I was expecting. The second one is pulling results, but instead of it giving me 6 digit numerical strings starting with 7, its giving me any numerical string that has a 7 anywhere in it. 
Best comment I ever saw in code was "whoever wrote this should be shot. It was probably me."
Aces, 100% what I needed. This has helped me a ton and learned a lot about glob and wildcards. Thank you!
 select a,sum(b) from table group by a
Is item guaranteed to be on both tables? You could do something like: SELECT t1.item , sum(t1.amount_in) , sum(t2.amount_out) FROM t1 INNER JOIN t2 ON t1.item = t2.item WHERE t1.item = '1234' GROUP BY t1.item; Change the INNER to LEFT IF item can be in t1 but not t2. 
The item isn't necessarily in both tables, but I only want the items that are, so INNER is what I tried at first, pretty much exactly as in your code. The problem is that where there are multiple records for the item in either table, the sum is multiplied by the number of records in the other table. So if t1.amount_in is 100 and t2.amount_out is 70+30, the sum of t2.amount_out would be 100, but t1.amount_in would show as 200. It worked fine when I used the subquery in the select after adding the missing field to the group.
Oh yeah. I didn't think about the multiple records issue. SELECT a.item , a.tot_amount_in , b.tot_amount_out FROM ( SELECT item , SUM(amount_in) AS tot_amount_in FROM t1 WHERE item = '1234' GROUP BY item) a INNER JOIN ( SELECT item , SUM(amount_out) AS tot_amount_out FROM t2 WHERE item = '1234' GROUP BY item) b ON a.item = b.item;
no kidding O_O , if half the people even just searched stack overflow or googled it they'd likely find what they're looking for
What? I'm doing the assignment for school you weirdo. Why would I be posting code if I was asking someone else to do this.
You piss off and learn some more about what the hell you are supposed to be teaching.
Yeah maybe if I was a teacher.
Why are you creating homework then? Are you doing the teachers job? Its delegated turtles all the way down?
I'm not creating the homework I'm doing it lol... The teacher asked us to do it so I'm doing it. I'm not creating the assignment. Fuck.
So...back to asking someone else to do your homework. Google, experiment, FIGURE IT OUT. Stop asking people to spoon feed you.
Nonsense. There's no reason to reinvent the wheel. If I have to write something I don't have a snippet already prepared for, I'll google it, see if it's efficient / good, and maybe tweek it. It's about 60/40. 60% is off the internet or revamped from it, 40% is original.
I'm partially convinced that there is no new program...ever. It's all been written before. We're just moving the pieces around to make them do things in different order. 
Idk why I'm even doing this with you.. I've messed around with it and I'm still running into problems so I posted here to see if anyone could help. Is it really that big of a deal to you? You know what Googling does? Brings me to forums where other people ASK QUESTIONS about what they're trying to do. I'm not asking someone to do the whole fucking assignment for me you prick. Why are you even in this sub your profile shows nothing from it.
I think there's new stuff all the time. Example, I had to write a CTE for a customer who had deadlocks in their XML but the server was not reporting the correct timezone in the node. So it ended up being a local and server call for deadlocks then converting the timezone and comparing timezone stamps for comparison so it could alert when deadlocks were occurring. I don't think anyone has had to do a mess SQL for that exact circumstance because people would probably resolve the problem of the timezone differences, not put a bandaid on it. I get your point though. I do end up saving 70ish % of my code that's not specific to the client. (Performance, security, backup, etc) Just generic stuff that can be taken with no context from site to site. It's just a pain to re-google or re-develop it each time.
http://i.imgur.com/EbEp1so.jpg Seriously, though, I think you're right on the 70% notion, although I think mine might be a bit higher. I tend to do the same things, or maybe, I just volunteer for projects that I know I can complete because I've already done something similar :) 
Heh, it's just recently I started to think that this is the right approach to teaching SQL - all the ones I know of, went through and, frankly, what I followed teaching/mentoring others, reverse the order - you have tables, tables comprise a database, let's extract data from tables. What I wrote is more of a concept though - you still can continue learning with whatever course you selected (Khan's pretty good, I hear).
https://www.simple-talk.com/sql/t-sql-programming/sql-server-cte-basics/ https://www.brentozar.com/archive/2014/06/capturing-deadlock-information/ CTE's and deadlocks, two huge things I recommend to any DBA to know about in detail!
Same thing in /r/C_Programming. Though it turns out that questions that are obvious to answer and easy to search for are neither for the novice, so cut them some slack.
I was just messing around, but good info on CTE's :) 
Then unsubscribe. If you don't like the content, submit something valuable, instead of shitposting (like this.)
I don't think they're being supported any differently than the SQL Server version it's on... but the way they update SQL is (this is just example and not accurate for the product versions): SQL 2005 got a database engine update SQL 2008 got an SSRS update SQL 2008 R2 got an SSIS, SSAS update SQL 2012 got a database engine update SQL 2014 got an SSRS update etc... so between most versions very little if anything changes for other products if it's a database engine update, whereas the database engine won't change much if it's an SSRS update... if that makes sense?
My first job as a developer, I discovered stackoverflow. This was one of the best moments of my life.
Agreed. I'm pretty new to SQL and I get value out of this sub. Even though most of the questions are obvious even to me, it helps me to form the answer in my mind then read other replies. That's been invaluable in helping me advance in SQL. But if you think this is all still too elementary, then submit things you'd like to discuss. It'll only help me get better. 
Ah, interesting... I guess the best evidence I can think of is it's all licensed together as one product, so why treat it any differently than the main product :)
I sometimes discover that my developers have been inspired by StackOverflow solutions. Often, they are having a hard time then. 
If you need pure sas, something like this should help: http://support.sas.com/documentation/cdl/en/basess/58133/HTML/default/viewer.htm#a001354483.htm For sql, I can either use a correlated subquery in a select list - corellate by all other pieces in your group by; another option is an analytical function ("over" syntax). 
Through using this sub and becoming active here I taught myself a ton about SQL for work. People are really great and helpful, but I've never understood some of the vitriol or downvotes. I understand homework seekers are annoying, but a lot of times I see people with work related problems who simply don't know enough about how to solve their problem being downvoted because their problem isn't hard enough.
You have three recourse options: 1. Stop answering people's homework questions... 2. Downvote 3. Unsubscribe
So, um, does your username still stand?
one man's shitposter is another man's philosopher king
To be fair I learned COBOL, RPG, and C 15 years ago and had a fair amount of database exposure / web site building experience before being dumped into SQL so honestly I found it pretty simple once I got over the hurdle of proper syntax, when to use a `;` for example, or a `,` -- those are the types of headaches that are the most frustrating but SQL itself I found to be very intuitive and easy to learn.
You could GROUP BY on dates and categories which will give you a distinct count of customers within those groups.
Honestly, way too often people would find their answer in the first 3 post on google. It's like people simply don't bother to search for 5 seconds. I simply ignore those post.
Thank you! I'll give this a try.
 x.c.value('(@com:sequenceNumber)[1]', 'int') as SequenceNumber, Should there be an `@` symbol there? This is the only line that does anything with an `int` so that would be my guess... but I spent all of 5 seconds looking at the code.
To me RPG was the norm so coming into SQL was like, "oh, that's simple."
Camel case please, CamelCase
I don't have the adventureworks db loaded at the moment but I just wanted to contribute to your problem. Off the bat I notice you're using a join between customer and store but you're joining on store.businessentityID in store on person.businessEntityID. Is there a businessEntityID in the store table you can join on? That second join you have just looks odd to me. also try using a left or inner join. You'll see the table you're joining will have some nulls (left) or wont' show anything at all (inner) if David Campbell does not have any records in the store table. 
Bcp Edit: and/or drop the indexes prior to inserting and re-create them after 
Would you be able to help over teamviewer? I will pay $50AUD
Perhaps the source data uses the varchar value 'null' instead of an actual null. Try looking through the data for anything like that and handling it?
Extranet considerations: https://msdn.microsoft.com/en-us/library/ms159272(v=sql.100).aspx Basic authentication: https://msdn.microsoft.com/en-us/library/cc281309.aspx 
What's returned when you run 'describe property'?
"Failed to run the after query" I should note that I'm not directly using MySQL for this. This class is making us do these exercises in what they call a "Query App" but based on error messages I've gotten it's definitely a MySQL server. 
It would seem as though there is a problem with the app. You could try rewriting like this: FROM OWNER A JOIN PROPERTY B ON A.OwnerID = B.OwnerID; If you still get an error I'd reach out to the professor and clarify whether that column really exists, or if it has a spelling error.
If Transact SQL (MS SQL), try using the window function OVER in combination with SUM. https://msdn.microsoft.com/en-us/library/ms189461.aspx. See "B. Using the OVER clause with aggregate functions".
Store name won't be null without a RIGHT JOIN or a FULL JOIN. You're querying the Store table.
You could create a view with something like this: select birthday,sex,id,date,grade as Grade_lesson1, NULL as grade_lesson2, ... NULL as grade_lesson60 from lesson1 union all select birthday,sex,id,date, NULL as Grade_lesson1, grade as grade_lesson2, ... NULL as grade_lesson60 from lesson1 union all ... union all select birthday,sex,id,date, NULL as Grade_lesson1, grade as grade_lesson2, ..., ..., grade as grade_lesson60 from lesson1 
I would create a table like this: CREATE TABLE lesson ( record_id INT NOT NULL AUTO_INCREMENT, birthdate DATE NOT NULL, sex CHAR(1) NOT NULL, id NOT NULL, date DATE NOT NULL, grade NUMERIC(5,2) NOT NULL, lesson_number INT NULL, PRIMARY KEY(record_id) ) `record_id` is just an artificial primary key because I don't know your data well enough to create a different one. `lesson_number` is the number 1-60 for each file. Next, I'd insert `lesson1.csv`into the table, letting `record_id` auto increment and with `lesson_number` inserting as a null, then I'd run `update lesson set lesson_number = 1 where lesson_number is null`. Then insert `lesson2.csv` into the table, and then run `update lesson set lesson_number = 2 where lesson_number is null`, and so on until all 60 files have been inserted into one table. Now, create an index: CREATE INDEX IX1_lesson ON lesson (id, date, lesson_number) Next, we need to PIVOT using OUTER JOINs: SELECT DISTINCT b.birthdate, b.sex, b.id, b.date, l1.grade AS grade_lesson1, l2.grade AS grade_lesson2, . . . l60.grade AS grade_lesson60 FROM lesson b LEFT OUTER JOIN lesson l1 ON l1.id = b.id AND l1.date = b.date AND l1.lesson_number = 1 LEFT OUTER JOIN lesson l2 ON l2.id = b.id AND l2.date = b.date AND l2.lesson_number = 2 . . . LEFT OUTER JOIN lesson l60 ON l60.id = b.id AND l60.date = b.date AND l60.lesson_number = 60; 
All good other than using 60 joins to pivot - I think a single group by and 60 case expressions will perform much better.
Ok, issue here is that you cant join the tables if they are on different platforms. I.e. oracle, TD, ms sql. You will have to have a dataflow that sends the data to a table somewhere else. The easiest way of course is to send the smaller table to where the bigger table is. In my company, sending ms sql data from ms sql server is very very slow. So we will typicall bring data to us. Sounds counterintuitive, but that is the faster transfer. Now. If you are talking sub 1k records in order to do a join on, then why not put it in an in statement? No it isnt efficient, but it is more efficient to do it on the datawarehouse side. To do this, you need to construct your query. Start with an execute sql task, store the results to a variable. Use the results of that variable to help construct another variable which would build the query with that variable in it. Teradata doesnt allow you to build a statement in SSIS with parameters, so you end up having to call a string of variables in your dataflow source that builds the query that will be used.
Join the manager table...something like this. SELECT * FROM Users AS u JOIN Managers AS m ON u.ManagerInitials = m.ManagerInitials WHERE u.ManagerInitials = "AB"
Try logging in as letshostdm from the command line. Do you have access to the database? Do you authenticate with a password? Where do you supply that password?
When I type in mysql into the terminal it says command not found. I am a local xammp server on macOS? I am looking into why but any suggestions?
Are you sure the server is running?
Yes, I have full access to it locally on phpMyAdmin which is really confusing me
Do you know what the "Paideach" field is supposed to signify in the ORDERITEMS table? In order to "get the actual value of books sold" you would probably only have to do a SUM() on the Quantity column of the ORDERITEMS table and join to it all the way to the AUTHOR table as the diagram indicates. Might have to make an additional filter based on "Paideach" whatever that is. If you don't know then just sum up the quantity, you'll be grand.
This won't help you much but just for my curiosity can you try to download and install MySQL workbench? Can you connect to MySQL from MySQL workbench?
&gt; MySQL workbench works fine and I can see everything...
Okay I will give that a try. I haven't used a variable to feed a query yet so hopefully I can figure out how to pass it in the correct format. Do you have to format that data and put apostrophes / commas like you traditionally would do when building a IN statement think WHERE Column IN ('X','Y','Z')?
Thanks Noah.
**This is where I ended up with this one. It appears to be working fine, but if anyone has any additional suggestions, please let me know.** SELECT AUTHOR.LNAME AS "Last Name", AUTHOR.FNAME AS "First Name", COUNT(ORDERITEMS.QUANTITY) AS "Books Sold" FROM AUTHOR JOIN BOOKAUTHOR ON BOOKAUTHOR.AUTHORID = AUTHOR.AUTHORID JOIN BOOKS ON BOOKS.ISBN = BOOKAUTHOR.ISBN JOIN ORDERITEMS ON ORDERITEMS.ISBN = BOOKS.ISBN GROUP BY AUTHOR.LNAME, AUTHOR.FNAME ORDER BY AUTHOR.LNAME
So to extend upon what u/LittleRedDot is getting at. Whenever you approach a SQL problem, try to focus on what is actually being asked of you vs what is just extra information needed in the report. In other words your first attempt at this shouldn't even consider the author or book tables. The core of this question is to get a number from order items. How would you write a query to get total books sold by only using the orderitem table(dont worry about book title or author name or anything else)? Once you have that answer, how can you link it back to the other information needed in the report? Also, I suspect *SUM(quantity)* and *GROUP BY ISBN*(if you are rolling up to book) or *GROUP BY authorid*(if you are rolling up to author) are going to appear in your query somewhere. If you are using count then you very likely have the wrong answer.
they are both run with the same admin account. But as I edited, I got the answer with the HOST_NAME() Thanks anyways :)
You know. The guy who built a boat and said let it rain.
Sorry for my sloppy typing. You can log in as letsjoindbm from MySQL workbench but not from the application? What application are you using?
Your design should include a "transactionDetail" table that stores 1 or more payment methods &amp; payment amounts, with a composite primary key of transaction_id and some sort of next number.
ok so one way is to construct your result set as a single string and include all of the quotes and commas you need. but that would be messy. i'm thinking a for each loop. i did this in the past. I query teradata for all of the unique values i need like call center within an execute SQL statement, then store then as a result set variable. from there i insert them into a for each loop in order to get them to all process. inside the for each loop i use each value in the variable within the SQL statement, that way you can do a query once for each value. in this case you can have a query as many values as you need, instead of limiting your in statement. then pass the single record into another table. here are some of the pics of the setup. your exec SQL task saves the list of items to a variable, your for each loop uses those values in the variable then cycles through a dataflow that will use each variable individually until it is done. http://imgur.com/a/TJ9Vm im sure there may be better ways to do this, but this is what i use for something similar. 
To do it with a `loop` is fairly simple. Just make the @MaxNumberFiles parameter a table with an auto-increment ID and use `WHILE @Counter &lt;= SELECT MAX(ID) FROM @MaxNumberFilesID` edit: Join to the @table on the @counter = @id and add a set @counter = @counter + 1 at the end of the loop. [Here](https://www.reddit.com/r/SQL/comments/51mn49/need_help_improving_an_openquery/d7d7ss5/) is an example.
Very good explanation of the thought process that has to go into building up a proper query/report. Start with the core task - in this case summing up quantities of sold books - and work your way from there adding all the bits and pieces as they become necessary (eg. first and last names). Of course in a real-world scenario (say one where they had thousands of books and authors) they would definitely want to group by authorid as you suggested, on the chance that several authors might share the same names.
Does your table have a key? 
What makes your table columns unique? Can you look into row number () partition by ( order by date ) then a where clause that selects from this selection where row number = 1. Im on my phone and your question is not giving an example of tables and columns but something like this should work for you
Assuming MySQL ? SELECT * FROM messages WHERE message like '%a%'; I think `CONTAINS` in MySQL is only for spatial data... or something I can't find any documentation on it.
Correlated subqueries magically solve this: SELECT * FROM tbl WHERE tweetdate=(SELECT MIN(innertbl.tweetdate) FROM tbl innertbl WHERE SUBSTR(tbl.tweetdate, 1, 10)=SUBSTR(innertbl.tweetdate, 1, 10)); Unlike the other solutions suggested here, this works in sqlite [at time of writing, sqlite has correlated subqueries, but not PARTITION]. You also have a uniqueness problem, as others have stated. Since the order is arbitrary past the date you already have, I'd suggest just picking one: SELECT tweetdate, MIN(body) FROM tbl WHERE tweetdate=(SELECT MIN(innertbl.tweetdate) FROM tbl innertbl WHERE SUBSTR(tbl.tweetdate, 1, 10)=SUBSTR(innertbl.tweetdate, 1, 10)); GROUP BY tweetdate
Thanks, I'm trying to learn MSSQL but this particular DB does not open in Management Studio. I'm having to use SQLite
cursors and loops will work but you do not need them. this will do what you are looking for without loops or cursors --result variable declare @sPaths varchar(max) = ''; --your temp table with paths as ( select 'path1' path union select 'path2' union select 'path3' ) --concatenate rows select @sPaths = CONCAT(@sPaths, ', @filename = ', path) from paths --remove the first coma ', ' select @sPaths = SUBSTRING(@sPaths, 3, LEN(@sPaths)) --here you can do whatever you want with the result print @sPaths 
Others have told you the "how". I feel it's important to point out the "why not". If you can avoid using this type of pattern too terribly much it's better to do so since it makes it so that your query can't take advantage of the database's indexes. Look up the term "non-sargable" if you want to read up more on it. 
please format your code, and show the error message you got
This was interesting, this returned only the years, but if its distinct by year I should be able to finagle it to be distinct by day. 
Does it have datepart function? You could use that. You could also make a table of all dates and do a left outer join where tweetdate is null that should give you days that did not have a tweet (or not null if you want days that did have a tweet). I'm surprised it only returned the year though...
I listed all books instead of the books that were borrowed right? :-)
Glad I could help :) Without a defined JOIN condition you made a [Cartesian Join](https://www.tutorialspoint.com/sql/sql-cartesian-joins.htm).
The number of backup files can be different though so wouldn't I need some kind of loop or cursor?
doh!!! leave it as UNION UNION ALL does not remove duplicates
I mean If I use some function of "NOT IN" and call to the original table.. it should essentially not allow dups.. right?
Misread the question, thought he wanted to keep duplicates
Is email an indexed column?
No, all just normal colums. 
OK so that's your problem. Try this approach. You're going to have to play with syntax as I don't have my client on this computer so I can't test the specifics and have to go from memory: create #table1 table ( [column1][datatype](length) , [column2][datatype](length) , [column3][datatype](length) , [columnN][datatype](length)) create index idx_email on #table1([email]) create #table2 table ( [column1][datatype](length) , [column2][datatype](length) , [column3][datatype](length) , [columnN][datatype](length)) create index idx_email on #table2([email]) insert into #table1 select * from table1 insert into #table2 select * from table2 Select * from #table1 where job_title like '%president%' and state like 'ca%' union Select * from #table2 where job_title like '%president%' and state like 'ca%' and email not in (select email from #table1) Or just add indexes to the tables if that's an option. 
If you right click the table in SQL Server you can usually find an option to SCRIPT AS QUERY which will open a new query window with the table schema already defined. Then just cut the create table part out and rename it from dbo.table1 to #table1 (or whatever) -- saves time if there are a lot of columns. edit: You may need to truncate the length of the email field in order to index it. Just make sure do a `select max(len(email))` from both tables to make sure you aren't missing anything.
there should not be any difference in how optimizer treats conditions within the 'on' clause of the inner join and the same condition in the 'where' clause. Depending on the query, data access pattern/execution pla can be very very different - there is not a single 'right' sequence, really. I would suggest reading about performance considerations for your specific platform if you are looking for in-depth discussion.
https://www.amazon.com/gp/aw/d/1430219025/ref=mp_s_a_1_sc_1?ie=UTF8&amp;qid=1477431198&amp;sr=8-1-spell&amp;pi=AC_SX236_SY340_FMwebp_QL65&amp;keywords=sql+server+2008+perfrormance
"Priority" is just a column/expression alias - the actual expression is "case".
Thank you, think I will order this book.
Ah I gotcha. Thank you, that makes sense. Stupid access with its stupid nested iif instead of case... 
Does the folder the database file resides in also have Write permission.. ? it should.
This is a bit messy as it has multiple subqueries but I think it should work. What it does is joins all the records and then assigns a row number to each row resetting on every email address. If two rows hve the same email address it only returns the first record. select tbl2.* from (Select row_number() over (partition by email order by tablenum) rown, tbl1.* from (Select 1 as Tablenum, TABLE1.* from [dbo].[TABLE1] where job_title like '%president%' and state like 'ca%' union Select 2 as Tablenum, TABLE2.* from [dbo].[TABLE2] where job_title like '%president%' and state like 'ca%' ) tbl1 )tbl2 where rown = 1
[coming from a mostly MySQL perspective] I'm wondering how you got in this situation. Having to search the whole title, a search that probably can't use an index, having two very similar tables where email may be the only way to deduplicate, searching state by a prefix instead of a sweet list of possible values... I assume this isn't an intentional design and you're trying to fix it or just do a one time thing. I'd redesign if you plan to keep using these. This query I'd do a join. Index the email fields and change your second query to Select * from table2 Outer join table1 using email Where table2.email is null And .... If a one time thing and email is unique within each table i'd also be tempted to create a temp table with a unique index on email and insert the results of the two queries separately into the temp table. (Do you care what the title and state are on the row in table1 with the matching email?)
 where job_title like '%president%' If the tables are large this will lead to poor performance. Do you have a list of all the job titles that you want? For example, if there are only two, you would write where job_title IN ('President','Vice President') Much better performance even on an unindexed column.
If you think the join order matters and the query optimizer is picking the wrong join order, try using the FORCE ORDER query hint. It helps, but rarely.
 SELECT * FROM ifsapp.time_pers_diary_result tp WHERE(tp.account_ DATE)&gt;= '2012-april-13' ; You have a space between tp.account_ and DATE.
Don't do that. LIKE 'President%' is sargable. LEFT(job_title,9) isn't. If you have an index you get an index seek with LIKE. With LEFT it's a table scan. I can't think of a good reason to use LEFT or RIGHT over LIKE wtr query performance.
Bear in mind some rdbmses are clever enough to cache some aspects of the query so running two logically identical queries (which join vs. where can be) consecutively can mean the second one runs faster.
I guess the input for the attributes comes from some sort of frontend? Than you should implement the logic there. It might be possible with a procedure as well, but I haven't worked with those yet.
I knew it would have something to do with CASE statement! Thanks, it worked like a charm!
How come no one asked "why?" OP, why do you have 2 tables with only 1 column different instead of a single table with a nullable attribute?
OK, I think my reply last night pretty much laid out what I was trying to get at. I am kinda self taught so there is no such thing as 'best practice' as far as I am currently implementing. I am just trying to implement a solution to have all our data in one place, and add/query from there. 
&gt; I thought that there should be no difference but actual testing shows that the improvement can be dramatic. I can't seem to replicate any gains by swapping the criteria, but you mention gains and I have read a couple blogs mentioning gains as well. Your perfect example makes sense, before I always thought the WHERE clause did the filtering first. Of course maybe the optimizer makes that call. &gt;Set up an example and look at the graphical query plan. Reading execution plans is a skill I need to work on for sure. &gt;I hardly ever have a WHERE anymore. For readability I definitely prefer my filtering done in the WHERE section. Since we pass around a lot of queries to replicate and change ad hoc reports its easier at a glance to see what the rules in place are.
https://www.mockaroo.com/ is probably your best bet. Assuming you want sample / test data....?
THANKS! Exactly what I was looking for, you're a hero!
It'd be a lot easier on us all if you could translate the table and field names into English in your statement. Are you asking for a list of people who have borrowed different books in English and Suomi, or someone who has borrowed a single book that is in both English and Suomi? The former makes more sense to me so pseudocode would look something like: SELECT borrowername FROM borrowers WHERE borrowerid IN (SELECT borrowerid FROM booksborrowed INNER JOIN books on booksborrowed.bookid = books.bookid WHERE language = 'English') AND borrowerid IN (SELECT borrowerid FROM booksborrowed INNER JOIN books on booksborrowed.bookid = books.bookid WHERE language = 'Suomi')
FoMo has your statement.
Short answer: use an association table (a table that holds pairs relevant fruit_ids and user_ids). Somewhat longer answer: there are ways to capture a collection in a single record, they are all different on different systems (oracle's, I believe, is Sys.AnySet) and doing this breaks first NF. I can think of several appropriate adjectives and 'nifty' is not one of them.
This worked, Thx man!
Hmm, I must be remembering wrong. It's been about 10 years since I took my ANOVA course in college.
I *think*, and I might be wrong here, is that since an EXISTS subquery refers to the a field in the outer query, it gets reevaluated for each row in the outer query, wheras using IN means each subquery gets evaluated only once, so it performs better. Not 100% sure on that though. I've never had the 1000 row issue so I guess it's specific either to certain platforms or certain configurations. 
Thank you. That is what I used in conjunction with "||" and "substr."
Try OUTER APPLY. SELECT a, b, c, FK_B FROM A OUTER APPLY ( SELECT expression FROM B WHERE ... ) e(FK_B)
Is the ID always at the end of the string or is there any other logic to the ID that makes it so you can easily extract it? &gt;Basically im trying to detect if an ID is present in one of strings from another table based on my list of ID's from another table. I think you probably want to know if it is more than "present" in the string. For instance if your ID was 1 then that ID would be present in any row that has 1 in it. So a column with the ID 1 or 12 or 311 would also have 1 present.
long story short, im trying to integrate two separate systems. One system is a ticketing system and the other is a timelogging system. I am going to make the staff input the ticket number they are working on in the description so i can check the total work time taken per ticket across all users. I might even try getting all tickets per department and show the total work time per ticket
What DB are you using? MS SQL Server could use something like this: select * from tableA where charindex(column1,(select column2 from tableb where condition),1) != 0 I think Oracle would use Find instead of charindex. 
Procedures can change structure and/or data functions can't Not sure how complex your logic is but you can always do: SELECT A.a, A.b, A.c, B.pk AS FK_B FROM A LEFT JOIN B ON &lt;Your logic&gt; 
Be weary of my code below. I didn't fill in everything for you. You will have to define the schema (i.e. dbo or otherwise), datatypes, and I highly suggest renaming the variables as I denoted in my comments. If you have any questions, feel free to ask! Stored Procedure: CREATE PROCEDURE [schema].[ProcedureName] @variable1 &lt;datatype&gt;, -- Rename variable to something more descriptive like ReservationNumber or whatever @variable2 &lt;datatype&gt; -- Rename variable to something more descriptive like UpdatedPersonCount or whatever AS BEGIN UPDATE &lt;table&gt; SET &lt;column&gt; = @variable1 WHERE &lt;column&gt; = @variable2 END Trigger: ALTER TABLE Customer ADD totalTimes INT CREATE TRIGGER [schema].[TriggerName] ON [schema].[TableName] AFTER INSERT AS BEGIN DECLARE @variable1 &lt;datatype&gt; -- Use for customer ID? DECLARE cursor CURSOR LOCAL FOR SELECT CustomerID FROM inserted OPEN cursor FETCH NEXT FROM cursor INTO @variable1 WHILE @@FETCH_STATUS = 0 BEGIN (Update logic for specific customer ID goes here using @variable1 in your WHERE clause) FETCH NEXT FROM cursor INTO @variable1 END END The cursor is meant to handle any potential batch inserts that may happen. Triggers are a tricky beast in the sense that you have to handle the possibility of not having only 1 row inserted. What happens in MS SQL Server is that you could have an INSERT statement that could have 0 to n rows that could be inserted. After that is done inserting, **then** the trigger fires off. Depending on how many rows were inserted, you must use a cursor to store all the rows that were inserted into a "temporary table" (i.e. the cursor), then loop through 0 to n rows. This is the safest and easiest way to handle this.
I think it is possible to do without the function and will perform much better. But it is hard to give you a proper advice without knowing the select statements in the function. If all of them have the same where condition than you can move it out of the function and use a case in the select part.
Do you have any control/influence over the time logging system? You could make your life much easier if you could get an additional field added to the form for the ticket number and make it mandatory. Trying to get users to voluntarily add that information to the description will result in many headaches. They may not enter it at all, they may enter it in a different place each time. Any report you create based on the ID being in the description is going to have high risks of inaccuracy and if you decide to proceed you should ensure stake holders know the risks. 
FYI, be careful with MERGE on SQL Server. It has several [undocumented features](https://www.mssqltips.com/sqlservertip/3074/use-caution-with-sql-servers-merge-statement/).
Was this machine generated? I can't see someone writing nested left joins. AND SORLCUR.SORLCUR_TERM_CODE = '(Select Last(SORLCUR.SORLCUR_TERM_CODE) From SORLCUR Group By SORLCUR.PIDM)' The single quotes around the query are likely the culprit.
Thanks for posting the full answer to your own question instead of the infuriating "nevermind guys, I figured it out."
for the benefit of any non-computers, i.e. human beings who might want to read your wall of code -- SELECT DISTINCT SPRIDEN.SPRIDEN_ID , SORLCUR.SORLCUR_PROGRAM , SORLFOS.SORLFOS_MAJR_CODE FROM ( SFRSTCR LEFT JOIN SPRIDEN ON SFRSTCR.SFRSTCR_PIDM = SPRIDEN.SPRIDEN_PIDM ) LEFT JOIN ( SORLCUR LEFT JOIN SORLFOS ON SORLCUR.SORLCUR_SEQNO = SORLFOS.SORLFOS_LCUR_SEQNO AND SORLCUR.SORLCUR_PIDM = SORLFOS.SORLFOS_PIDM ) ON SFRSTCR.SFRSTCR_PIDM = SORLCUR.SORLCUR_PIDM GROUP BY SPRIDEN.SPRIDEN_ID , SORLCUR.SORLCUR_PROGRAM , SORLFOS.SORLFOS_MAJR_CODE , SPRIDEN.SPRIDEN_CHANGE_IND , SORLCUR.SORLCUR_TERM_CODE , SFRSTCR.SFRSTCR_TERM_CODE , SFRSTCR.SFRSTCR_RSTS_CODE , SORLFOS.SORLFOS_LFST_CODE , SORLCUR.SORLCUR_CACT_CODE , SORLCUR.SORLCUR_CURRENT_CDE , SORLCUR.SORLCUR_KEY_SEQNO , SORLCUR.SORLCUR_PRIORITY_NO HAVING SPRIDEN.SPRIDEN_CHANGE_IND IS NULL AND SORLCUR.SORLCUR_TERM_CODE = '(Select Last(SORLCUR.SORLCUR_TERM_CODE) From SORLCUR Group By SORLCUR.PIDM)' AND SFRSTCR.SFRSTCR_TERM_CODE = '201650' AND SFRSTCR.SFRSTCR_RSTS_CODE = 'RW' AND (SORLFOS.SORLFOS_LFST_CODE) = 'MAJOR' AND (SORLCUR.SORLCUR_CACT_CODE) = 'ACTIVE' AND (SORLCUR.SORLCUR_CURRENT_CDE) = 'Y' AND (SORLCUR.SORLCUR_KEY_SEQNO) = 99 AND (SORLCUR.SORLCUR_PRIORITY_NO) = 1
It's unreadable.
Oracle SQL Developer
OK - Try this: SELECT DISTINCT SPRIDEN.SPRIDEN_ID , SORLCUR.SORLCUR_PROGRAM , SORLFOS.SORLFOS_MAJR_CODE FROM ( SFRSTCR LEFT JOIN SPRIDEN ON SFRSTCR.SFRSTCR_PIDM = SPRIDEN.SPRIDEN_PIDM) LEFT JOIN ( SORLCUR LEFT JOIN SORLFOS ON SORLCUR.SORLCUR_SEQNO = SORLFOS.SORLFOS_LCUR_SEQNO AND SORLCUR.SORLCUR_PIDM = SORLFOS.SORLFOS_PIDM) ON SFRSTCR.SFRSTCR_PIDM = SORLCUR.SORLCUR_PIDM LEFT JOIN (SELECT SORLCUR.PIDM, MAX(SORLCUR.SORLCUR_TERM_CODE) AS max_term_code FROM SORLCUR GROUP BY SORLCUR.PIDM) AS term_codes ON SORLCUR.PIDM = term_codes.PIDM WHERE SPRIDEN.SPRIDEN_CHANGE_IND IS NULL AND SORLCUR.SORLCUR_TERM_CODE = term_codes.max_term_code AND SFRSTCR.SFRSTCR_TERM_CODE = '201650' AND SFRSTCR.SFRSTCR_RSTS_CODE = 'RW' AND SORLFOS.SORLFOS_LFST_CODE = 'MAJOR' AND SORLCUR.SORLCUR_CACT_CODE = 'ACTIVE' AND SORLCUR.SORLCUR_CURRENT_CDE = 'Y' AND SORLCUR.SORLCUR_KEY_SEQNO = 99 AND SORLCUR.SORLCUR_PRIORITY_NO = 1;
Thanks, but I'm getting "missing keyword" at "AS term_codes"
&gt;What you can do is create a variable above this query to the effect of: DECLARE @last_term_code INT and then populate it like this: SELECT @last_term_code = LAST(SORLCUR.SORLCUR_TERM_CODE) FROM SORLCUR And then in your WHERE clause for the original query make it "AND SORLCUR.SORLCUR.SORLCUR_TERM_CODE = @last_term_code" Thanks very much, I never knew this! 
Did you manually clean that up or did you use software?
Maybe the "SORLCUR.SORLCUR_TERM_CODE" contains strings of queries..
manually... but i'm really good at it, been formatting sql like that for decades, with a very powerful text editor (Ultraedit)
If you know that you don't have overlap, then do a union all. Know that a union ONLY dedupes if all records are exactly the same and if one column is different you will get dupes.
I could. Maybe in the future since I'm not familiar enough with the time logging system to add some fields and meddle with the code. For now im trying to make this work. If no other option is available, i might go through with that.
Yeah, the statement with join does what I expect. I happen to be lucky that I still got the correct output even when I had cartesians. This function only uses parameters from Table A, Table A is not being queryed inside the function. This is the Insert to Table A'. It works now aready. Also I am very appreciative to the feedback. I usually don't work so much with SQL, so this has been quite a learning process. INSERT INTO [dbo].CDR ( ID_Verkeersklasse, Datum, Tijdstip, Duur, Totaaltarief) SELECT [dbo].Getverkeersklasse( raw_cdr_10_10_2012.platform, raw_cdr_10_10_2012.Verkeersklasse, raw_cdr_10_10_2012.Type, raw_cdr_10_10_2012,Roaming), ), convert(date,Datum,103), Tijdstip, Duur, cast(replace(Totaaltarief,',','.') as decimal(18,6)), FROM raw_cdr_10_10_2012 WHERE Totaaltarief is not null AND CAST(REPLACE(Totaaltarief,',','.') as decimal(18,6)) &lt;&gt; 0
I made a sqlfiddle (using Oracle) to show your answer in action. http://sqlfiddle.com/#!4/76f1d/9 
here is my explanation of the logic behind this sql statement -- 1. SELECT DISTINCT salary 2. FROM salaries AS s1 3. WHERE 2 = 4. ( SELECT COUNT(DISTINCT salary) 5. FROM salaries AS s2 6. WHERE s1.salary &lt; s2.salary ) start with line 2 -- each row of the salaries table will be examined in turn, and we "tag" this row with the table alias `s1` now jump ahead to line 5 -- each row in the salaries table will be examined again, but this time tagged with the table alias `s2` lines 4 through 6 are called a **correlated subquery** because of the correlation between the rows examined by the main query, tagged `s1`, and the rows examined by the subquery, tagged `s2` this correlation is specifically defined in line 6, which stipulates that the `s2` salary must be greater than the `s1` salary in line 4 we see what that the purpose of the subquery is to count the number of distinct salary values specifically, the subquery counts how many distinct salary values in the entire table (`s2`) are greater than the value on that single row of the table being examined in the outer query (`s1`) in concept it's like doing a loop over the entire table (outer query), and for *each* row, doing another loop (subquery) -- except that the database doesn't necessarily do loops now the punch line, line 3... if this count is exactly 2, then the `s1` salary must be the third highest salary value the distinct in line 1 ensures that this salary is reported only once
Great explanation. To add on slightly, the hard part to understand is that the top salary has a count of 0, because nothing in s2 can be greater [*than it] in s1, i.e.: | Salary (s1) | Count (s2) | | :--- | :--- | | 100,000 | 0 | | 90,000 | 1 | | 80,000 | 2 | | 70,000 | 3 | | 60,000 | 4 | So there is (1) salary higher than 90K, (2) higher than 80K, etc. Therefore `WHERE 2 =` is the third highest.
&gt; Erm I read a post a while back that said Union or Union All reduced the performance loss, not completely but to a certain extent. UNION ALL does that as it doesn't have to work to compare every record against every other record to ensure uniqueness. [Read StackOverflow question here](https://stackoverflow.com/questions/3627946/performance-of-union-versus-union-all).
prefix each line of code with 4 spaces and it will format correctly. 
subqueries for IN can only return one column and that likely won't return what you want. You probably want to use a correlated subquery for your hiredate condition so that it will return results specific to each department. You don't need the disti ct(whatever)/either as your condition should be sufficient for what you want. Also, you say it wants department name, but then say that name is department_I'd but then you get department_name right after that... Pretty sure it is just the department_name you need to worry about. You should likely focus on completing each of these tasks one by one, get a list of all employees for each department. Then figure out how to filter by the min(hire date). Pseudocode would be something like Select department name, employee name, hire date From department join employee as A on deptid=deptid Where hire date = (select min(hire date) from employees as Z where A.deptid =Z.deptid) Should get you there. Edit: on mobile so excuse BS formatting and typos Edit2: if 2 employees share a min(hire date) you may want to account for that as well (add a row number to your initial query then make it a subquery where rownum=1 to return a single row per dept)
I would write this like this: select a.department_name , b.last_name , a.hire_date from ( select department_name , department_id , hire_date , row_number() over(partition by department_id order by hire_date ASC) as 'RN' from department ) a inner join employees b on b.department_id = a.department_id where a.RN = 1
SQL is different from traditional programming in that it is set based, but it is still a very structured language and once you figure out how to manipulate sets of data, it gets easier with the main burden being understanding what data you have and what data you want. I've always enjoyed working with databases, so personally SQL wasn't difficult to pick up but it is something you need to be hands on with to really get a hold of. If SQL (and databases in general) aren't your cup of tea, there may be other object oriented languages that you might want to pursue as they will be conceptually different from SQL and you might find that more to your liking. Some simple steps to work on for your specific problem would likely be to 1) In a single query return a list of departments and their employees 2)In another query, find the employee who has been there the longest 3)Using #2 as a starting point, add in department to find an employee for each department (since employees has departmentid, you can likely stay in the employee table). 4)Using #1 and #4, return your final data set. Hope that helps, don't let a single persons answer get you down though... I'm sure someone will (if theybhavent already) swoop in with an easy answer :) 
I think hire date is in the employees table so likely just need to swap the department data for the employee info. Get employee last name. DepartmentID and hire date and keep the row_number as is then join to the department table. But I agree, this would be an easier way to do it than what I suggested.
I think SQL is easy and that you're struggling with certain techniques, in this case how to dedupe. I think your problem is thinking its a matter of learning the code, when its a matter of learning the process. In this case you have two tables with a possibility for duplication. Your initial thought was to take `min(date)` which could work, but it could also fail if there are duplicate `min(dates)` -- and that's where your mind needs to go. You can write a very simple statement like this: select min(date) , count(*) from table group by min(date) order by count(*) desc And then you'll know what you're working with, and whether your approach will work. And if there are duplicates you need to find out how you should tackle that. Get comfortable asking the data simple questions to look at it while you're constructing your approach. A really good example that I use often is zip codes. Lets say I give you a table(A) like this: | ColumnName | DataType | Length | | :---- | :---- | :---- | | Name | varchar | 100 | | Address | varchar | 100 | | Address2 | varchar | 100 | | City | varchar | 100 | | State | varchar | 2 | | Zip | varchar | 10 | | Donations | float | -- | And I say to you, "hey /u/help_me_will, I want you to join to this other table and give me some demographic results. I want you to tell me by county what the total donations are." Simple question, right? The other table(B) looks like this: | ColumnName | DataType | Length | | :---- | :---- | :---- | | Zip | varchar | 10 | | County | varchar | 100 | | State | varchar | 100 | | Region | varchar | 100 | So, since you're someone who studied this in school and you know all the SQL code in the world you come back and give me some graphs based on the following code: select b.county , sum(a.donations) as 'total_donations' from a inner join b on a.zip = b.zip group by county Psh, no problem, right? You know that (1) zip code can only belong to (1) county, maybe not in reality, but you were smart enough to ask: select zip , count(*) from b group by zip having count(*) &gt; 1 Nothing came back so you know there aren't any duplicates and you're confident with your demographics. But I look at you and say, "/u/help_me_will this doesn't make any sense. The number one county on your list is in a state with the lowest sales. Go back and figure this out." So you come on Reddit and you ask whats wrong with your code, and people tell you nothing, or they suggest a better way to do it that will execute faster, but none of this really helps you. Finally someone tells you to look at your join on zip, so you go back and ask another question: select len(zip) , count(*) from b group by len(zip) All of a sudden you see that half of table(a) only has a 10 digit zip, and table(b) has all sorts of distributions from 4 to 10. What the fuck? OMG idiots, this is the worst database in the world, someone should have fixed this a long time ago... these thoughts all run through your head, but that doesn't do shit because I'm your boss and want the *real* answer. So you decide to ask what the distribution of lengths are across the two tables. You don't know how to do this in SQL but you can [Ask Google](https://www.google.com/search?q=percent+of+total+in+sql&amp;oq=percent+of+total+in+sql&amp;aqs=chrome..69i57j69i60l4j0.1991j0j7&amp;sourceid=chrome&amp;ie=UTF-8) or just copy the above data into Excel and look at some quick percentages. [edit: Learning to do these routine things in SQL and *understanding how they work* are what separate the pros. If you want a fun exercise on this type of mentality go look at how to come up with a median using SQL, and how different methods execute differently in terms of performance.... compared to Excel's simple `=median()` function. It's a pain in the ass but you'll learn, and that's what you want.] You quickly discover that 97.5% of the data in table(a) has a zip code greater than 5 digits, and that 99.9% of the zip codes in table(b) are greater than 5 digits in length. A little skeptical still of how shitty the developers and designers are you decide to ask another question: select * from a where len(zip) &lt; 5 Out of a table of a million rows you see 25,000 rows that look like this: | Name | Address | Addres2 | City | State | Zip | Donations | | :--- | :--- | :--- | :--- | :--- | :--- | :--- | | TEST | TEST | TEST | TEST | XX | 1234 | 0 | | TEST | TEST | TEST | TEST | XX | 1234 | 0 | | TEST | TEST | TEST | TEST | XX | 1234 | 0 | | TEST | TEST | TEST | TEST | XX | 1234 | 0 | | TEST | TEST | TEST | TEST | XX | 1234 | 0 | | TEST | TEST | TEST | TEST | XX | 1234 | 0 | | *Wee Taded | 39267 Real Address | NULL | New York | NY | 5555 | 26125 | So you start feeling a little bit better and it sudden occurs to you that if you just take the first 5 digits of [Zip] from the left side you can make this work without duplicates... but you don't know how so you do a quick [Ask Google](https://www.google.com/search?q=left+5+digits+sql&amp;oq=left+5+digits+sql&amp;aqs=chrome..69i57.3383j0j4&amp;sourceid=chrome&amp;ie=UTF-8#q=5+digits+from+the+left+in+sql) (or come here and ask) and suddenly: select b.county , sum(a.donations) as 'total_donations' from a inner join ( select distinct --right? because there will be duplicates otherwise. same first 5, different last four. left(zip, 5) as 'zip' --wait, do all first 5's only belong to (1) county? need to validate. , county from b ) b on left(a.zip, 5) = b.zip group by county Now your demographics are completely different. So you weren't wrong originally in terms of code, but you didn't explore the data itself and came to a wrong conclusion. Here your problem is with joining based on the `min(date)` and what you're really after is a proper way to dedupe sets... so what you need to do is kind of change your way of thinking. You will find better answers and find them more quickly. So lets [Ask Google](https://www.google.com/search?q=left+5+digits+sql&amp;oq=left+5+digits+sql&amp;aqs=chrome..69i57.3383j0j4&amp;sourceid=chrome&amp;ie=UTF-8#q=sql+join+on+min+value+without+duplicate) a more specific question and you'll find a link to [this](http://stackoverflow.com/questions/19916298/left-join-without-duplicate-values-using-min) article on StackOverFlow which has the first response proposing to use a solution such as: ROW_NUMBER() OVER (PARTITION BY custno ORDER BY qty) RowNum ... WHERE RowNum = 1 Since you've never seen this before you might [Ask Google](https://www.google.com/search?q=left+5+digits+sql&amp;oq=left+5+digits+sql&amp;aqs=chrome..69i57.3383j0j4&amp;sourceid=chrome&amp;ie=UTF-8#q=sql+row_number+partition+by) what the function does, and play with it in your environment: select row_number() over(partition by left(zip, 5) order by county asc) as 'rn' , * from b And you see that it just kind of auto-increments a number based on the parition by and order by dependencies, e.g.: | RN | Zip | County | | :--- | :--- | :--- | | 1 | 12345 | Aberdeen | | 2 | 12345 | Brightmoor | | 3 | 12345 | Caledonia | | 1 | 12346 | Xavier | | 1 | 12347 | Oakland | | 1 | 12348 | Wyoming | | 1 | 12349 | Aberdeen | | 2 | 12349 | Brightmoor | So I'm kind of rambling now at this point, but perhaps briefly you can see why an approach like `row_number() over(partition by N order by Y)` is superior to `min(date)` when there might be more than (1) minimum value. I'm not trying to yell at you for not using Google, but rather try to refocus you away from focusing on learning syntax and instead focus on looking at the problem itself and investigating the tables so that you can catch issues with the data before you decide on your approach, and in fact you can ask questions around those issues and learn new techniques. You don't learn the code and then say, "oh I can use this to do that." -- Instead you learn the technique and that is how you learn the code. edit: I added that last row of data (*Wee Taded) to show you that often times you will find some random person who just couldn't or wouldn't get into the table the right way. User error, or an ETL problem, or whatever. Valid data, but for some reason their zip code is nonsensical. Depending on the total amount of donations these represent you might need to go back to your boss and have a conversation of how to represent his question. Maybe instead of county you need to do it by region. Or maybe you need to pick a county for these donations to fall into based on other types of reasoning, which will require you asking more questions of the data set and potentially looking at the distribution of donations across all counties in New York to see whether its arbitrary to just lump it in with one or not, or distribute it evenly across all of them. Again, you might not know how to do this but you now have a specific question to either ask us, or Google, which will allow you to keep going further and further until you solve the problem. That's how you learn SQL. 
Separate it out. What's the earliest hire date for each department? Get that query, then just join it to the employee table. 
As in instead of department_id call it department instead?
like a heading for a table in excel 
One of these three methods might be what you're looking for: https://explainextended.com/2009/09/15/not-in-vs-not-exists-vs-left-join-is-null-sql-server/ Edit: I should also perhaps mention that although the article is written for SQL Server, I think that any of those methods should probably work with SQL Lite and/or MySQL (although I don't know for certain).
Well, the basic idea would be Select memoid where memoid not in (select memoid from memostatuses)
Use the `AS` keyword to alias your column names. SELECT Field1 as First_Field, Field2 as Second_Field, Field3 as Third_Field FROM employees WHERE department_id = 100; This will also get you in the habit of specifying your fields instead of being overly dependent upon `SELECT *`
So what you need to do is to select all memos where an entry in memo_statuses doesn't exist: SELECT memo.* FROM Memos memo WHERE NOT EXISTS (SELECT NULL FROM Memo_Statuses read_memo WHERE read_memo.memo_id = memo.memo_id AND read_memo.user_id = @user_id) @user_id is parametrized ID of the user who is logged in. Don't mind the `SELECT NULL` construct in the subquery, the engine checks if a record exists here but it doesn't need any columns from the underlying table, it only needs to know whether or not a record exists. Alternatively, you could `LEFT JOIN` the memo statuses table: SELECT memo.* FROM Memos memo LEFT OUTER JOIN Memo_Statuses read_memo ON memo.memo_id = read_memo.memo_id AND read_memo.user_id = @user_id WHERE read_memo.memo_id IS NULL In SQL Server, this could be less performant than the `NOT EXISTS` query, depending on some factors, but I'm not sure about MySQL. Keep in mind that you have to put the `user_id = @user_id`in the `ON` clause of the `JOIN` statement and not the `WHERE` clause, unless you wrap it in `IFNULL` (e.g. `IFNULL(read_memo.user_id, -1) = @user_id`). 
Thank you! This worked perfectly and I learned something new! I appreciate yours and everyone else's help.
You mean in MySQL? In SQL Server the `LEFT JOIN` will never, under any circumstances (bar the hints of course), be faster, although if the constraints are not right, they can both be equal. `NOT EXISTS` or `NOT IN` will prefer an anti semi join, a faster logical operation than an outer join.
That is not completely true. It does not need to sort them. It just needs to filter out duplicates. That can be achieved by doing sorts or doing hash aggregates.
Some people have a subquery phobia.
wow, i can't belive you wrote all this
Not sure what you want but maybe this will help you. Select 'column1' || ',' || 'column2' From dual Union Select column1 || ',' || column2 From table1 
Both ways are valid and there are pros and cons to using both.
It's as simple as duedate &lt; returndate
The first table is obvious. [Service] This describes each individual service. In there will be a [ServiceID] column. The second table will be [Widget]. It will have the description and some other stuff. There will be a [WidgetID] column. The third table we will call [WidgetRule]. It will have [WidgetID], [ServiceID], and a column for a minimum and another for maximum. For each [Service] that a [Widget] has there will be an entry in [WidgetRule]. As a widget may have multiple services there may be multiple entries in that table per widget.
I thought so... but it returns an error for me. I am having to use SQLfiddle so I guess I will chalk it up to that.
How would I handle the situation where the rule is one of a group of services and two of another different groups of services?
We had this very thing crop up in a product that I worked on. You would have a [ServiceGroup] table and in your [Service] table you would have a [ServiceGroupID] column. This implements "A Service may belong to a group but only one group." If a service belongs to no particular group the [ServiceGroupID] would be NULL. Now in your [WidgetRule] table add a column [ServiceGroupID]. Then a constraint so that [ServiceID] might be NULL but [ServiceGroupID] then can't be. Also one that [ServiceGroupID] might be NULL but [ServiceID] can't be. Also a constraint that they can't both be NULL. And lastly a constraint that they both can't be ***not*** NULL. Working with "it might be a thing or a group of things" can get your head spinning. It's a load of fun figuring things like this out.
Thanks for the assist! This is perfect except It returns a null instead of 0. Is that possible? 
either change `sum(Opened) as 'processed'` to `isnull(sum(opened),0) as 'processed'`, or change `processed` to `isnull(processed, 0) as 'processed'`
Have you developed a formula to convert miles into latitude and longitude values? 
http://stackoverflow.com/questions/13026675/calculating-distance-between-two-points-latitude-longitude
I've been to that link more times than I can count, maybe I don't understand since I'm not really good at programming. All the places I've looked are for calculating point to point from 1 zip to another, not finding zips in a radius
What you will likely need to do is calculate the distance from all zips to all other zips. That will take some time but the resulting table will be able to handle your requests in a timely fashion. My initial thought would be to do a LOOP, so for each zip in the table you calculate the distance to all the other zips (probably can cut that down to all the zips in the same state.) Too tired tonight to dig into your problem but I'll kick it around and get back to you if I have any better ideas.
If I were you, this what I would do: http://www.nber.org/data/zip-code-distance-database.html Get that dataset, import it, and use it to find zip codes within whatever distance you want. 
W3schools is a good website to start with
Hold on: https://www.codecademy.com/learn https://lagunita.stanford.edu/dashboard http://www.w3schools.com/SQl/default.asp https://schemaverse.com/ http://sol.gfxile.net/galaxql.html https://www.coursera.org/course/db http://www.tutorialspoint.com/sql/sql-syntax.htm http://www.halfgaar.net/sql-joins-are-easy I used a combination of all those websites to learn. i'd start with code academy and then play with the others to find what works for you.
You can take a look at the site i've posted just now - https://www.reddit.com/r/SQL/comments/5abac9/a_really_good_site_to_learn_basic_sql_sql_oracle/?ref=share&amp;ref_source=link I've learned from it a few days ago, it's really good.
Using a like-statement really slows down searches. My ideas to tackle this would be hashing instead of searching for your values. Let's assume any search term is below 30 characters. Therefore there are "only" ~ 128^30 possible words someone would look for. Now you create a hash-function for those words. When searching you look for results containing the hash value, therefore resulting in O(1 * n) with n being the number of values matching the hash.
FULLTEXT indexing is also available in MySQL. 
Wrong. Indexing doesn't work for wildcard PREFIX searching. In these cases, the query drops down to a table scan.
Might be worth a learn.. you can set some procedures to automate what your trying to do.. Uploading updating data as csv is pretty simple. codecademy and w3schools pretty much tought me all I needed to learn (and a few ?? on this forum) to do what you are trying to get done. 
I would say yes. You can use SSIS to automatically import those CSV files by FTP and the do all of the processes you familiar with. &gt;I need to be able to manipulate the data in dozens of ways for reporting, dashboards, etc. (I can currently do this with Google Sheets). You will likely need another solution for reporting long term. This isn't a trivial task and it sounds like you need someone who works in analytics to join your company. There may be a way to export the data from SQL into Google to keep up with your existing reporting but over time I think that is going to be a hindrance. You could either hire someone full time, or hire a consultant to build the environment and make it ready for power-users such as yourself to use with minimal training. You could do it yourself but as a side project I would anticipate it taking quite awhile.
As the others have said it is the LIKE that makes it slow. If you parsed each word out of the string and put it in a word table with a key back to the original row then you could do some fast word searches. The other thing that makes some of these search engines so fast is massive parallelism. 
You need to be a little more specific. When you say "fill in the hours with the total hours worked" -- do you want the total hours to be in every row, e.g.: | repo | state | pipeline | hours | | :--- | :--- | :--- | :--- | | corey-feldman | open | Review | 146.62 | | corey-feldman | open | In Progress | 146.62 | | corey-feldman | closed | Closed | 146.62 | | corey-feldman | closed | Deployments | 146.62 |
The others have shown you correctly how to compare dates. Yet you have another problem If [detail_returndate] is NULL then the item has not yet been returned. Is that right? Lets assume so. Comparing [detail_duedate] to [detail_returndate] will answer the question "***Was*** the item returned late?" Your question was "Is an item ***not yet*** returned late?" That means that you have to compare [detail_duedate] to the date right now. By the way comparing a date to NULL returns error. Spend some time looking into NULL. If you are using MS SQL then look into COALESCE. Other SQL will have a similar function
That would be great. Ideally I would want the total hours to be in place of the 0 currently in the closed out state. I'm not sure if thats possible.
are you sure this is MySQL? maybe i missed it, but i don't think MySQL supports window functions like LEAD OVER yet no database that i know of supports a double equals sign anybody that would write this needs a firm talking to -- `IF(MAX(IF(IF(`
| repo | state | pipeline | hours | | :--- | :--- | :--- | :--- | | corey-feldman | open | Review | 69.29 | | corey-feldman | open | In Progress | 77.33 | | corey-feldman | closed | Closed | 146.62 | | corey-feldman | closed | Deployments | 146.62 | Seems confusing? Or do you want this: | repo | state | pipeline | hours | | :--- | :--- | :--- | :--- | | corey-feldman | open | Review | 69.29 | | corey-feldman | open | In Progress | 77.33 | | corey-feldman | closed | Closed | 146.62 | | corey-feldman | closed | Deployments | 0 | 
The first table would be great if thats possible
It is. Not happy about that embedded if statement either. Working/cleaning up code from a project a co-worker was doing before he left. 
This should work, I think: SELECT repo, ticket_number, title, assignee, state, pipeline, created_at, closed_at, points, quarter_closed, year_closed, CASE WHEN state = 'Closed' THEN sum(hours) ELSE hours END AS hours FROM ( SELECT repo, ticket_number, title, assignee, state, pipeline, MAX(IF(closed_at IS NOT NULL, 0, MAX(ROUND((end_epoch-start_epoch)/3600,2)))) AS hours, MIN(created_at) AS created_at, MAX(closed_at) AS closed_at, MAX(points) AS points, QUARTER(closed_at) AS quarter_closed, YEAR(closed_at) AS year_closed FROM [icxmedia-servers:icx_metrics.issues_and_zenhub] AS historical, ( SELECT repository.name AS repo, IF(issue.number IS NOT NULL, issue.number, pull_request.number) AS ticket_number, FIRST(IF(issue.number IS NOT NULL, issue.title, pull_request.title)) AS title, IF(issue.number IS NOT NULL, issue.assignee.login, pull_request.assignee.login) AS assignee, pipeline.name AS pipeline, IF(MAX(IF(IF(issue.number IS NOT NULL, issue.state, pull_request.state) == "open",0,1)) == 1, "closed","open") AS state, IF(issue.number IS NOT NULL, issue.created_at, pull_request.created_at) AS created_at, MAX(IF(issue.number IS NOT NULL, issue.closed_at, pull_request.closed_at)) AS closed_at, NULL AS assign_times, TIMESTAMP_TO_SEC(IF(issue.updated_at IS NOT NULL,issue.updated_at, pull_request.updated_at)) AS start_epoch, LEAD(start_epoch, 1) OVER (ORDER BY ticket_number, start_epoch ASC) AS end_epoch, MAX(estimate.value) AS points, QUARTER(MAX(IF(issue.number IS NOT NULL, issue.closed_at, pull_request.closed_at))) AS quarter_closed, YEAR(MAX(IF(issue.number IS NOT NULL, issue.closed_at, pull_request.closed_at))) AS year_closed FROM [icxmedia-servers:icx_metrics.gh_zh_data_production] WHERE issue.number IS NOT NULL OR pull_request.number IS NOT NULL GROUP BY repo, start_epoch, pipeline, ticket_number, created_at, assignee) AS prod WHERE title=="LinkedIn" GROUP BY repo, ticket_number, title, assignee, pipeline, state, quarter_closed, year_closed ) A GROUP BY repo, ticket_number, title, assignee, state, pipeline, created_at, closed_at, points, quarter_closed, year_closed, 
This is so close I think. I got an error that says that the field `hours` can't be found. I think its because its stored in the second table but calculated from the production table
There is an extra comma at the bottom of the outer group by.
Got that cleaned up. Still getting the error about hours. I thought about adding hours to the select statement but that didn't seem to work
Thanks. I did some poking around. I know MySQL doesn't support CTE's but you should be OK using #tables -- I don't know your data well enough so there might be some duplication issues going on which is why I threw that DISTINCT in. Let me know if I can be of any further help.
Here's a stab in the dark to start with: SELECT c.CourseID , i.Instructor , c.StopDate , c.Enrolled , COUNT(r.Submitted) AS Responses , CASE WHEN c.Enrolled IS NOT NULL THEN COUNT(r.Submitted) ELSE NULL END AS 'ResponseRate' , CASE WHEN c.Graded IS NOT NULL THEN 'Y\' ELSE '\' END AS 'Graded' FROM hb2504.Courses c LEFT JOIN hb2504.Instructors i ON i.[field] = c.[field] LEFT JOIN hb2504.Results r ON r.[field] = c.[field] WHERE ABS(DATEDIFF(c.StopDate, CURDATE()))&lt;'30' &amp;&amp; SUBSTRING(c.CourseID, 9, 1)!=\'L\' &amp;&amp; c.Enrolled&gt;=5 --No idea what this does GROUP BY c.CourseID , i.Instructor , c.StopDate , c.Enrolled , CASE WHEN c.Graded IS NOT NULL THEN 'Y\' ELSE '\' END ORDER BY c.StopDate DESC , i.Instructor , c.CourseID
Right so try this: select * from ( select gl_account , amount , row_number() over(partition by gl_account order by date_field DESC) AS 'RN' from gl_account_table where amount &lt;&gt; 0 ) a where RN = 1
Oh yeah, change it to: DATEDIFF(d,c.StopDate, GETDATE())
You need to give it the argument which tells it what interval of time to calculate the difference from: | datepart | Abbreviations | |:--- | :--- | | year | yy, yyyy| | quarter | qq, q| | month | mm, m| | dayofyear | dy, y| | day | dd, d| | week | wk, ww| | hour | hh| | minute | mi, n| | second | ss, s| | millisecond | ms| | microsecond | mcs| | nanosecond | ns| 
&gt; Based on the fact this is a side-project, I am already VERY handy with Excel/Google Sheets, and the organization is very familiar with Google Drive/Docs, would SQL be the best route for me? Learning SQL and getting access to a database, loading that data into a database instead of google sheets/excel It's not something you're going to do as a weekend project, but it sounds like your data needs have reached a point where it's time for the business to invest. Running it and maintaining it once it's set up should not be a full time job, setting it up and testing it (load and all the various outputs) will be a full time job for someone.
This is a good candidate for a `CASE WHEN` statement: SELECT Part ,FutureFree ,SafetyStockQty ,CASE WHEN (SafetyStockQty = 0 AND FutureFree &lt; 0) THEN (ABS(FutureFree)) WHEN (FutureFree &lt; SafetyStockQty) THEN (safetyStockQty - FutureFree) ELSE 0 END as ReorderQty FROM Stock Table: Part|FutureFree|SafetyStockQty|ReorderQty :--|:--|:--|:-- Widget|-3|0|3 Badge|5|10|5 Cleaver|-3|5|8 Box|3|0|0
Any search with an actual defined first character(s). The reason like '%foo%' doesn't use indexes is because that initial % matches every record in the index.
A lot of really good answers in here, but I suggest that you post the database you are using behind the medoo framework. You might look into sphinx if you are specifically doing search. http://sphinxsearch.com/
Everyone hates it, but it's a great DB starter...Microsoft Access. SQL is great for databases of 1M+ lines, Excel is great for tables of 10000 and less lines, and Access is a great mid-size tool to fill in the applications in the middle. This is something that can be set up in a weekend. 
What are you populating your cache with then? How would one avoid database calls altogether besides not using the data?
SQL is very, very, *very* commonly used to update/modify databases. As an analyst though you'll be more interested in using SQL to get data rather than change it. I'd suggest focusing initially on SELECT statements, particularly with GROUPing. You should be comfortable with joining multiple tables, grouping and using aggregate functions (like SUM and MAX), and CASE statements. After those, get comfortable with CTEs/subqueries. About 90% of everything you'd need to do as a pure analyst (as opposed to someone who creates their own data solutions like your data warehouse provider did for you previously) is covered by those, and they're fairly simple to master. Maybe look at pivoting after that too.
The company I work for has a pretty large business intelligence and analytics department (100+ analysts) and we use a lot of SQL. We have a lot of processes that massage and clean up data as well as complex calculations against the data. I would suggest learning to build stored procedures and functions and how you can automate various processes. Perhaps consider learning how to take advantage of dynamic SQL as well.
&gt; I would say a couple hours a day for a week would get you familiar with &gt;90% of common SQL operations and a good chunk would just be memorized naturally. Agree 1000%: learning SQL is nothing at all compared to learning a full programming language, and 90% of what you'd do daily is with the simplest 10% of the language. 
I use SQL to extract data from data warehouses at my large bank and then do most of the heavy lifting/formatting in SAS.
sqlzoo.com
SQL has two main parts of it's language: Data Definition Language (DDL) and Data Modification Language (DML). DDL are basically statements to create tables and modify database metadata. Typically Data Analysts don't have to worry about this part, as a Database Administrator will handle that. Depending on the size of your shop, you might have to learn about making indexes, but i'd leave that for DBA's. I'd just focus on the DML part of Databases, most specifically, the Queries. Things i'd focus on learning are: JOINs, Functions (these changed depending on your Database system(oracle, sql server, db2....)), and the basic querying of tables. Really what makes and breaks analysts is knowing your data. Knowing what columns are indexed and how tables are related to each other will change your queries from taking 30 minutes to run to 5 seconds. The more you learn about your data and mastering how to use their indexes, the better you will be. Source: Me, I was a DBA, then switched to a Data Analyst in a Law Enforcement Agency, then i've now switched into a Data Scientist Position. Also, i teach Databases at Marymount University as an Adjunct Professor, and am actually teaching SQL right now. 
&gt;. Perhaps consider learning how to take advantage of dynamic SQL as well. Your DBA hates you, just thought you should know. [And here's why.](https://www.brentozar.com/archive/2015/10/the-five-stages-of-dynamic-sql/) 
I would really start to explore using window functions, they were a little hard for me to grasp at first but once you do they're a great asset
Thank you so much... I've been working my way through exercises and the database modifying bit mostly annoyed me and had me saying 'bah, I wont need this!' when the correct answer is 'dude, you dont know enough to make that call yet'. Thank you so much!
Honestly that is a really good intro and having worked on a client that was like 140k lives (with like 16 records per life) I did learn that once your data set gets large enough you need to have really good habbits to speed things up or avoid it just locking up. It makes sense that applies to sequel so I'll be very mindful of best practices. 
Learn to think in *sets*, not records. Your SQL will be more efficient.