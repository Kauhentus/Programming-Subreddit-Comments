This is the answer I was looking for. Thank you so much. My thought right now is that I’ll likely be focusing on data retrieval, but eventually I’d like to be able to be useful and augment the efforts of the devs at work for smaller database-related projects for our product. Thanks again.
I tried Toad Data Modeler. It kinda of works. It was not designed to work with Azure SQL DB. It works well with an on-premise instance of SQL Server. Of course so does ER/Studio.
We are not speaking about the same thing. Forward engineering in a data model means that the application will update the DB after changes were made to the ERD model.
Learned it from an older colleague with decades of sql experience. It has always stuck with me.
Learn an etl tool so you can move data between databases.
When I interview resources and ask them about their SQL knowledge, I look at it this way: 1. If you can do basic Select statements, joins, inserts and updates, you're entry level. 2. If you understand things like views, table creation, data types, functions, you're advanced. 3. If you can write stored procedures, triggers, etc, know how to properly index a table, custom tables, you're being advanced.
Search stackoverflow.
Why MySQL. You could load a CSV into MSSQL in second.
Because MySQL Workbench is the tool I have available to me...
You’re “diving into it for resume building experience” that sounds like it means you can use any tools available
I would like to know how also
Honnestly, if this is for resume experience.... It'd be a good time to learn how to read the documentation or answer a simple question like this with existing materials.
Where is the file? Where is the database? Sounds like a DB config or architecture issue.
You can also load CSV into MySQL super fast by putting the file into a place where mysqld can read from it directly.
Why use either? You could do it with HyperSQLDB or PostgreSQL too.
He was already using MySQL, plus MySQL is probably the most commonly used open source relational database, so if you want to learn something that is a very good place to start.
And MSSQL is probably the most commonly used closed source system he’ll encounter in the workforce. Probably more than MySQL, especially if you work for a SaaS company Except Intuit.
Ok, you went over head with ERD. I will stick to the shallow end and keep those little orange floaty things on my arms. Sorry to intrude.
Sure, but he was already using MySQL, asking a question about how to do something in MySQL so answering it by saying "Use something else" is not helpful especially when MySQL is perfectly capable of doing what he wants. What's your point here?
fyi I just did it with https://modeanalytics.com free online tool. but i would still like to do it from MYSQL with null values
I am going through Master SQL for Data Science right now, also. I have taken Database courses in grad school before, and used it in work, but needed to really brush up and sharpen my skills. So far, I like it a lot. Keep in mind that the Udemy course focuses almost exclusively on running queries on the data, as opposed to database design, or even insertions, deletions, and updates. Still, it is a very good course.
I didn’t answer it by saying to use something else. I asked a question, why this platform? Being overly attached to one isn’t a great thing. Being flexible and understanding the syntax differences and pros and cons of different solutions is a very good skill to have.
Both resources are beginner level. You can consider yourself truly proficient in SQL if you can understand and write SQL code like this: https://pastebin.com/UiLS7VQM
&gt;Would there have been a more efficient way for me to handle this situation? Is there a certain way I can get mySQL to accept NULL values in int fields without screwing my data with fake "0" values? Absolutely - avoid fake dummy values whenever possible. I don't know how MySQL Workbench handles nulls offhand, but I just looked up the LOAD DATA command in the MySQL documentation, and it looks like \\N is how you'd represent a null using that command. There's a good chance MySQL Workbench runs that very command under the hood. &gt;Is 10 minutes an abnormally long time to load a file via csv? Why is this type of load SO much less efficient than loading via a script? If your row count is only 14k that sounds absurdly long and something strange must be going on. I haven't done much loading of csv files into table in MySQL, but very roughly speaking (because it could depend on many factors like machine power) I would expect to be able to load millions of rows in 10 minutes, according to my experience in other databases. I would expect MySQL to be about the same. Maybe try out LOAD DATA yourself: [https://dev.mysql.com/doc/refman/8.0/en/load-data.html](https://dev.mysql.com/doc/refman/8.0/en/load-data.html) (you can change the version number in the URL to your version number) &gt;How the heck do people create INSERT scripts for thousands of rows?? I imagine they somehow automate the process using Python? Is that the normal process? It's not the best way to load data from a csv but it would be pretty easy to generate a big insert in a programming language like Python. Or lots of little inserts (this is usually slower, though).
I remember when the guy posted that a few weeks ago. It's really not that difficult at all, just a really long query with a bunch of subqueries and a lot of joins. More a chore to write than really being anything advanced.
Did you get an answer regarding this yet?
Did you get an answer on this?
All I see is derived table joins down to a bunch of null handling and case statements. It looks like a lot of inefficient aggregation.
Typically, put enough experience on there to show you're qualified plus a year or two. I had someone comment on the number of jobs I've had when interviewing for my current job. I simply told him that when you're dealing with contract work they're fixed term and that was basically enough to move past that nonsense.
SQL server is also free. Just a heads up that there are options out there
I agree, looks pretty straightforward to me. The formatting is much more clean and consistent than anything I typically see on the job. For real-world SQL, be prepared for statements that have been added to by many people over the years and touch tables with schemas that nobody completely understands anymore because of cryptic naming systems. Successful SQL is as much about the business logic as SQL coding.
But it's not. If you can't understand why the business is asking you to do what you're doing, and recommend ways to improve it, you're extremely replaceable. It's easy to find people who can churn through assigned work, it's a lot harder to find people who actually add value on their own.
I figured it out haha thank you though!
Today I got motivated and moved two of my projects to their own repositories: DataTier.Net [https://github.com/DataJuggler/DataTier.Net](https://github.com/DataJuggler/DataTier.Net) &amp;#x200B; DB Compare [https://github.com/DataJuggler/DBCompare](https://github.com/DataJuggler/DBCompare) &amp;#x200B; I now have a bunch of NuGet packages so moving the projects was easier than I first thought. &amp;#x200B; NuGet and Git are like Chocolate and Peanut Butter. &amp;#x200B; I removed these two projects from the Data Juggler Shared Repo. Eventually I will archive the shared repo or leave miscellaneous projects in it that are not worthy of their own repository. &amp;#x200B; Thanks for putting up with me, and now DataTier.Net and DB Compare should be easier to work with. &amp;#x200B; Happy June
A lot of inference there &gt; If you can't understand why the business is asking you to do what you're doing There may be no sane reason the business asks you to do certain things &gt; ways to improve it Assuming that it would be desirable or practical to have devs improve many things. It's the minority case when there is enough time &gt; you're extremely replaceable 50/50. You should never try to make yourself irreplaceable, as it makes it harder for others to pick up your work, but it might be that you really are the best at what you do and someone would be an idiot to replace you &gt; It's easy to find people who can churn through assigned work Only if your work is low value boring work. &gt; it's a lot harder to find people who actually add value on their own. Perhaps you work in a specific niche, or have a higher bar for "adding value"
I'm familiar with things like aggregation, subqueries and data modification. Any good resources to learn the other things?
Data science has little to do with SQL, aside from just being able to do a simple `SELECT` query. Unless you count statistical analysis as data science. But even statistical analysis is quite simple SQL. Using the word "Master" in that context is a bit weird. Master SQL would be for stuff like data manipulation, transformation, optimization of queries. Data science however is more about using adjacent tools like R, Python, libraries for them, and understanding the underlying math and making sense of results. Which has very little to do with SQL. I don't trust titles that seem misleading.
Learn practical SQL first. Databases are everywhere, and SQL is the standard to pull from and manipulate data into them. Learn it well and you'll be able to take many paths, only one of which is Data Science.
SQL is pretty standardized, but the emphasis and specific tools aren't. I use SQL at work, but the SQL tools and tricks I find in my data science books is very different. One difference I've noticed between the "Data Science" SQL community and "practical" SQL community (let's call this "businesses") is the choice of databases/platforms. I work in Finance and find that Microsoft's SQL Server reigns supreme. I've also used DB2, SAP's Hanna and lots of small daily tasks done in MS Access at work. The Data Science community seems to favor MySQL, PostgreSQL, and SQLite. A second difference is *how* they use databases. Businesses ("practical") tend to be more interested in transactions rather than analysis (OLTP rather than OLAP). Simple querying, Extract Transfer Load and Reporting are huge parts of "practical" SQL skills. The Data Science community seems to use tabular data as a jumping off point for applying statistical methods and algorithms. I don't think of regression, clustering, neural networks, deep learning or whatever else as SQL tasks per se although some of that work could be done in SQL.
I feel like the perspective behind this comment really outlines the current Data Science skill gap. Many Data Scientist believe this and it handicaps them, unless they are in large organizations where they are able to be ignorant of how they get their data. I'm a data engineering consultant, this mindset is what keeps me in business. Organizations hire us to bail them out after they onboard a bunch of Data Scientists that don't have SQL skills and are completely at a loss. I mean..Kaggle and DS courses make you think that data always comes in a neat and tidy package - which it never does.
It's amazing out many DS folks miss how fundamental and important SQL skills are.
Really agree with this too. There is nothing complex about that at all outside of being excessively long. Personally, I'd make some of those sub queries separate views so it'd be easier to isolate issues during development and be able to troubleshoot/update them more efficiently.
Cool, sounds like I know most of the basic stuff and still need to learn a lot more. Need to learn GROUP BY, HAVING, SUM and find PERCENTS (I usually do this on google sheets after running the query). I am good with logical operators and wildcards/LIKE Temp table, CTE, derived table - need to learn that Need to learn ranks Need learn NOT EXISTS, I already know IN and NOT IN &amp;#x200B; Thank you for your comment will try to learn the things I haven't here this next month.
Dope, joins have become pretty easy for me, subqueries are good too. I definitely need to work on the window/aggregate functions. Thank you all!
Cool, will definitely continue learning after I leave my current job, hopefully I am able to land a SQL job so that I can keep on learning. Joins I feel comfortable and confident in. I have no idea what a cartesian product is. Need to learn aggregates but I know subqueries now and need to do more so I can have more confidence in them. Question subqueries mostly used in SELECT, FROM, and WHERE clauses? That's where I have practiced my subqueries mostly.
Cool, I definitely feel like I'm on the entry level side still but not too far from intermediate.
&gt; I have no idea what a cartesian product is. Have you ever done a join and either forgotten the ON condition, creating a result set that's every possible combination of rows? That's a cartesian product. &gt; Question subqueries mostly used in SELECT, FROM, and WHERE clauses? They get used in all of the above. Understanding cross apply and outer apply is a bit more advanced, but also very powerful
I like Itzik Ben-Gan's T-SQL Fundamentals. He emphasizes theory. He isn't always easy to read, but these are the best explanations I've run across. You'll get windows functions, set operators, and common table expressions from this book plus a great foundational knowledge.
100% this.
You're ahead of a lot of developers already. (Maybe not SQL Developers, but others). Good Luck!
Try use Reverse.
I don't understand the down-votes. My only point was that the sample code posted is more complicated and reflective of the real world than the basic (mostly) single table queries presented in Practical SQL and Master SQL for Data Science. The code examples presented in both sources are less than 15 lines of SQL code. SQL is a deceptively tough language and those two resources barely scratch the surface. Neither even discuss writing queries for performance.
Yes, I'm a data engineer and I do believe that a company that needs a data scientist most first have a good team of data engineers. Otherwise there's nothing to do science on. Companies start off with Excel spreadsheets and whatever online systems they might use to ease logistics or accounting. But the data in there is either very little (Excel) or inaccessible without doing API stuff. Basically you can't manage a large data volume without a developer to optimize it. And without a large data volume you can't do data science properly.
&gt;Edit -- or perhaps you're right that DS folks shouldn't have to be proficient in SQL and that companies should simply hire Data Engineers as well. In any case, if a company is going to ONLY hire DS folks then those data scientists need to be proficient with SQL and data curation. Essentially, small companies need full stack Data Scientists... I think this is a good way to see it. If you're at a small company nobody's going to hand you a dataset. You'll have to dig it up from whatever awful system it resides in yourself. That usually means combining data from several SQL databases and perhaps some excel reports too. If you can't do that the cost of having you around suddenly doubled since they will have to hire an extra person to do all those steps. If you're in a big company you could probably get away with only doing the analysis/models and have the data handed to you by a data engineer. But you'd have to be really good at what you do to motivate that you need support personell around to be useful. That said, I don't want to downplay the energy and effort it takes to stay on top of all the new cool techniques and research in ML. It's hard work and you can't be an expert on everything. Some companies need the cutting edge stuff and can afford to have that expert. But most doesn't actually need very advanced stuff to be successful.
Hey, I am very interested in becoming a Data Engineer. I was wondering if you had any reading materials or information that you can give to help out someone who wants to go down that career path?
Yeap. Companies don't realize this until their data scientists have been unproductive for months trying to do data engineering work that they have no preexisting skill with. Big companies absolutely have to hire more data engineers. Still though, I predict growing demand for "full stack" data scientists in smaller orgs that need both skillsets with fewer people. Also, I should have mentioned that I agree with the part about the title "Master SQL for Data Science" being misleading. I've never seen any courses like this that even get close to describing what SQL mastery actually is. It's just marketing and fluffery though. Kind of the same as those...."Learn Data Science in a Week!" advertisements. Lol..... As if anything in this field is that easy. The reality, of course, is that real SQL expertise is like anything else - it takes years of hands-on expertise.
Yar. Indeed. For smaller orgs, you don't need to be a PHD Data Scientist breaking new ground. A Data Engineer that understands the data can easily implement and productionize many of the simpler and more commonly used ML algorithms. That's where the full stack data guys come in. If you're a fortune 500 company then definitely stick with experts and specialists though. I suppose all this goes back to the "generalist vs specialist" argument.
Well. SQL and Python are the main languages. Scala is less common. ETL development, Data modelling skills, and Data Warehousing are crucial. Distributed systems and frameworks (like Spark), cloud infrastructure and cloud vendors, Performance tuning with SQL, Python, etc.. API development. Security and encryption. Maybe a bit of domain knowledge helps.
&gt; I predict growing demand for "full stack" data scientists in smaller orgs that need both skillsets with fewer people. Did you ever see a small organization having enough data to play with, other than with statistical analysis and cohort analysis? Just curious. I've seen people in situations that you describe (full-stack BI rockstar), but I have yet to see the use case for data science there.
Very wise words. I need to learn how to interview with that frame of thought!
do you happen to how hard the sql questions are based off hackerrank? do they usually ask medium hackerrank questions or hard hackerrank questions in internship interviews?
Well sure. But when I say "smaller", I don't mean "small". I mean "not a fortune 500 company". There are lots of medium-sized companies that can get good use out of a few well placed multiple regression or clustering algorithms here and there. These are not things that require you to be a top-notch Data Scientist to get real business value from. Plus, there are plenty of algorithms out there that are optimal for smaller data sets.
Excel spreadsheet - sized smaller data sets? I've worked at start-ups, and we already had RDS instances for our data since it was too big for any spreadsheets. And every time some data scientist wanted to do something with it, they came in with confidence like "oh, we can work on small datasets, I have done it before using random forests", and after a couple of months the predictions are off by some 20%, which literally means that by doing a simple cohort analysis would've been much more accurate, but nothing actionable nevertheless. It still required data engineering to pull the data together in a clean manner. The only smaller company that I've seen use data science with some actionable results was a company that requires their own IaaS or baremetal even, for self-hosted self-configured and self-administered database combination solutions, because just running it on an RDS instance wouldn't even be possible. Not Big Data, but big enough to require at least 2 good data engineers working full-time on taming that hydra with 60 data sources.
Lol, no. Not Excel spreadsheet sized data sets. Traditional data analytics works just fine for that (in fact, even in DS-ready orgs, like 90% is still just regular old analytics/BI). I'm talking about orgs that have datasets edging into the terabyte range. Not big data, but not small data. Companies new to AWS or Azure that have growing data, but still don't really know how to make the best use of it. A little bit of ML alongside traditional analytics does the trick.
&gt; I'm talking about orgs that have datasets edging into the terabyte range Yep, and those do need data engineers first :) I've seen data scientists f**k up when it came to making sense of the raw data, and simply ignored most issues, that occurred, not to mention more specific business cases.
Precisely. Companies really should hire data engineers first and then onboard DS folks once the data ecosystem if mature enough. Otherwise they end up using the Data Scientists as Data Engineers and throwing money down the drain because they aren't well suited to the job. Many companies with data the size of the example above may never even have enough ML work to truly merit hiring a full time Data Scientist though. In such cases, it's better (IMO) to hire a full stack data engineer (or full stack data scientist) with heavy back-end skills plus some cross-training in ML and general analytics vs. a Data Scientist specializing in ML with next to zero backend skills.
How do you plan on making the two interface? What you're asking is possible, it just depends on how you invision it working
I have no idea what you're actually asking. The answer, whatever your question is, is most likely 'yes you can do that' but first put some more time into clearly defining your problem.
Exactly. And let's not forget that unless the DS or someone else can make the target audience understand what the data is saying, this whole thing is an exercise in futility.
Yeap yeap. A model may be a work of art, but gaining trust is pretty paramount. The "credibility crisis" is real. Data provenance and all that is getting more attention and future regulations might make black box algorithms a harder sell. And as the DS field is demystified, business folk will be able to look past the hype and start to ask better questions instead of throwing a DS at the raw data and expecting magic. As a data consultant I'm seeing a lot of companies recently having that 'aha' moment where they now understand that they can't run before they can walk. Gotta staff appropriately, and gotta do your homework.
You need to learn how to act like you belong there, and to present yourself in such a way that you are going to help them solve a problem, and that you understand the problem they are trying to describe, and that you have experience solving problems like that in the past.
Easy to Medium level questions from my experiences
I'd list temp work as consultancies. I didn't read through all the comments, but LinkedIn seems to be a great platform for job seekers. Set up a strong profile with a professional picture search for jobs in your location; there are tons. Working on or improving your interviewing skills may help too. Finally, don't let this rough time get the best of you. Persevere and you will succeed.
I can't figure out which is your target destination and which is your source. When you say, "I can only read data from sql server" do you mean that you only have permission to read data from SQL Server? If that's what you mean, you should still be able to create a temporary table in SQL Server in tempdb. Preface your table name with a hash (#): IF OBJECT_id('#MyTable','U') IS NOT NULL DROP TABLE #MyTable CREATE TABLE #MyTable ( MyID INT, MyText VARCHAR(25) ); INSERT INTO #MyTable VALUES (1, 'Hello jihana'); SELECT * FROM #MyTable;
Please describe what do you think of the standard SQL
Well thank you! I got some reading to work on.
Don't we all? Never stop learning 🤙🤙
Looking forward to the comments!
I used Udemy for my SQL practice and I found it cool but if you want something, you might have to wait a week or two to get it for $15 as opposed to $400 or something insane. Besides, you can easily get courses that are either poorly taught or are way over year head. After using Udemy, I moved to Stratascratch and it sounded better. I found datasets pre-loaded there with questions and answers we can practice with. They source their questions from technical interviews from companies so I found it helpful to use for daily work and interview practice.
Leaving a comment as I'm very curious about this. When I get time later I'll go into greater detail Multi master replication will use both dbs as primary nodes and allow changes to be made on either and reflected on the other Streaming/logical/synchronous/asynchronous replication will create a read only replica which accepts connections but doesn't allow direct writes. Very curious If someone has a way to do what you are after though
I’m assuming that you’re referring to MS SQL Server since you mention management studio? Mirroring was deprecated back in 2012. Try Availability Groups: https://docs.microsoft.com/en-us/sql/database-engine/sql-server-business-continuity-dr?view=sql-server-2017 https://docs.microsoft.com/en-us/sql/database-engine/availability-groups/windows/always-on-availability-groups-sql-server?view=sql-server-2017
thank you
Depends very much on the company. I have recently brought in two new juniors and asked zero technical questions at interview. The basic technical understanding will come across from how they talk about previous experiences in roles/eduction. Technical aspects are very easy to teach however the attitude and personality are not, and that’s what you need from an intern/junior. I see my role as a mentor to help my juniors develop, not as a manager who expects them to hit the ground running. The questions I asked were to see how they approach situations rather than a right or wrong answer. Sorry if this doesn’t really answer your question, but you might want to prepare for more unexpected questions!
Those dreaded backticks are invalid standard SQL: CREATE TABLE table_name ( `id` varchar(255), `name` varchar(255), `age` varchar(255), `gender` varchar(255) ); Standard SQL used double quotes to quote identifiers: CREATE TABLE table_name ( "id" varchar(255), "name" varchar(255), "age" varchar(255), "gender" varchar(255) ); But that makes the column names case-sensitive. It would be better to not quote them at all. Btw: there is no magic optimization in any modern database for varchars with the length 255. A value of 300 will be just as efficient.
SELECT year, SUM(idps) AS idps, SUM(idps_returned) AS idps_returned FROM persons WHERE UPPER(origin) = ('IRAQ') AND UPPER(destination) = ('IRAQ') GROUP BY year;
Edit: formatting
Thanks all, I've managed to solve it with your help! Especially u/ichp \- :D So I used your Join (select...) etc which gave me what I expected would run in my Table2, then I added in: ROW_NUMBER() OVER(ORDER BY MX.TableID) Which I found somewhere on the net (can't remember where) which essentially gives you the row number of your Results, I then split that by TableID with "PARTITION BY" to end up with: ROW_NUMBER() OVER(PARTITION BY MX.TableID ORDER BY MX.TableID) Put them all together to end up with: Et Voila! M + ROW_NUMBER() OVER(PARTITION BY MX.TableID ORDER BY MX.TableID) I'm sure there's much better ways of getting around my issue, but the important thing is it works! (Now to testing!)
Show what you've already tried and explain what exactly is getting you stuck. Your teacher would not appreciate you getting the entire answer from the internet.
I think what you are describing is a group by SELECT Name, colA, colB, max(colC),max(cold), etc FROM table GROUP BY name, colA , colB Under the assumption the data is always exactly like you described Could you try it and give some feedback?
Seems to be working right! Can’t believe i forgot about group by, thanks
It depends on your DBMS. In some (like Oracle) you can convert this to XML, apply XML functions and then use any filters you want.
&gt;To upgrade / move a database from one instance of SQL to another I understand the proper way to do that (per the support persons from 3rd party software company) is to run the installation file for the version of SQL you want to move the database TO. Select "upgrade" not "install" at the start, then choose the database you want to upgrade. What? This would be valid if you were looking to upgrade the SQL Server *instance*. But not for a single database. To upgrade a single *database*, the most common method is to install the new version of SQL Server as a new instance (preferably on a new server), then backup the database and restore it to the new instance. In-place upgrades of SQL Server itself are _better_ than they used to be, but by no means perfect and still not recommended. Can you post the instructions this 3rd party gave you? Either you're reading them strangely, they're written strangely, or they're wrong.
I would look into using regex for SQL and extract UId into its own column.
I can cast this into an XML. Thanks. :)
This procedure information is coming from multiple phone calls from more than one tech support person with the company providing this very expensive practice management software. &amp;#x200B; Discussing with them the need to put the database onto 2014 they said "you don't have to manually do anything, it's simple and safe. All you gotta do is run the installer for 2014 SQL, choose upgrade not install, select the database, and it does it all itself." &amp;#x200B; I think part of the issue, but am not certain, is that doing the upgrade will preserve some information such as database users and so on, that might be lost by just doing a backup / restore. &amp;#x200B; If this "advice" is sorely incorrect, what do you suppose their real motivation is by telling me the "upgrade" method is correct?
Can you be my boss please?
&gt;This procedure information is coming from multiple phone calls from more than one tech support person with the company providing this very expensive practice management software. And they don't put any of it in writing or have written upgrade guide/instructions? Red flag! "Very expensive niche vertical" software is often...less than great in my experience. Especially for things like this. They make it work and that's about it. You end up having to jump through hoops if you want to deviate from their little utopia. &gt; Discussing with them the need to put the database onto 2014 they said "you don't have to manually do anything, it's simple and safe. All you gotta do is run the installer for 2014 SQL, choose upgrade not install, select the database, and it does it all itself." They've grossly overstated the safety and simplicity. Yes, it's _possible_ to do this and it's _probably_ going to be OK, but the fact that they're making it sound so simple and not advising you that if the upgrade goes sideways you'll be rebuilding from bare metal using your last good backup makes me nervous as hell. Did they say anything to you about the new cardinality estimator and whether you should enable it or leave it off? Upgrade the compatibility level of your database(s) (which would affect that)? &gt;I think part of the issue, but am not certain, is that doing the upgrade will preserve some information such as database users and so on, that might be lost by just doing a backup / restore. Except those things *can be copied to a new instance!* The `dbatools` Powershell module has a huge group of functions to do exactly that. You can even copy **everything** in one fell swoop with `Start-DbaMigration` - run that, then point your clients at the new instance. Done. &gt;what do you suppose their real motivation is by telling me the "upgrade" method is correct? They're selling you on simplicity, targeting folks who don't have a DBA on staff or even one in their back pocket that they can call for advice (if you need the latter, LMK and I can throw a couple names your way). "Sure, just toss the disc in, click NextNextNext, and you're all set!" But when it fails, they'll say "welp, I guess you screwed up somewhere, sucks to be you!" BTW, I recommend upgrading to SQL 2016 if not 2017 to give yourself the longest possible runway with the new instance. Unless, of course, this vendor doesn't support releases that have been out for 2-3 years already.
why cant Microsoft create a free tier like firebase for their SQL Server then?
Because they choose not to? You can get a basic, 5GB Azure SQL DB for $5/month (price will go up from there based on your final requirements, but for a basic "hey I wanna play with this thing" it'll be fine). That's really not much, but it's enough to put a barrier between them and total freeloaders. SQL Server Express Edition (requires a VM or physical machine to run on) is free as in beer. If your requirements fit within its constraints, have at it. But Microsoft isn't going to let other cloud hosts like Google &amp; Amazon offer SQL Server of any flavor for free.
5 dollars is insane when converted to my currency. Lop
And he is nihilistic. This dude has mad chill
I’m sure you are fun at parties.
They didn’t ask for it but prob add order by year for extra credit.
I think you win the prize, essentially your logic can be used to weed out the problem children... Since "not exists" doesn't work in Vertica - i simply did a select from results - then a left join so a select from results with your logic.. followed by a where is null - if there is no results from r2 - then it is a true value, if there are results, then it is a problem child &amp;#x200B; I am testing this now, but you take the cake
thank you very much
Sweet! Yeah, I'm not familiar with Vertica- but yeah, in SQL Server it could also be accomplished (albeit possibly slightly less efficiently, I think) with SELECT r1.PatientID, r1.ResultDate FROM Results r1 LEFT JOIN Results r2 ON r2.PatientID = r1.PatientID AND r2.ResultDate &gt; r1.ResultDate AND r2.ResultDate &lt;= DATEADD(DAY, 14, r1.ResultDate) AND r2 IsPositive = 1 WHERE r1.IsPositive = 1 AND r2.PatientID IS NULL Happy to help! :)
Yep.. Pretty much exactly what I did... Thank you soo much... And yeah sql server is a lot more robust.. I hate vertica. But its what my job uses... Sadly Again thank you.. Once you said "not exists" it completely clicked on how I could write it out... I dunno why I struggled... But you saved me
Here is the query I'm using to filter in the individual report card, I basically need this to run for each student at the school and append the result to a table or csv file: SELECT duedate, asg_name, course_name, term_name, std_name, standardgrade, commentvalue FROM (SELECT DISTINCT asec.duedate, asec.name asg_name, c.course_name, tc.name term_name, sta.name std_name, gsec.standardgrade, com.commentvalue, RANK() OVER (PARTITION BY sta.name ORDER BY gsec.lastupdated DESC, gsec.whenmodified DESC, rownum) as rnk FROM students s JOIN assignmentscore ags ON ags.studentsdcid = s.dcid JOIN assignmentsection asec ON asec.assignmentsectionid = ags.assignmentsectionid JOIN sections sec ON sec.dcid = asec.sectionsdcid --AND sec.id = ~(sectionid) JOIN terms t ON t.id = sec.termid AND t.schoolid = sec.schoolid AND t.yearid = ~(curyearid) JOIN ps.courses c ON c.course_number = sec.course_number JOIN assignmentcategoryassoc aca ON aca.assignmentsectionid = ags.assignmentsectionid JOIN PS.teachercategory tc ON tc.teachercategoryid = aca.teachercategoryid JOIN assignment a ON asec.assignmentid = a.assignmentid JOIN assignmentstandardassoc asta ON a.assignmentid = asta.assignmentid JOIN standard sta ON asta.standardid = sta.standardid JOIN standardgradesection gsec ON sta.standardid = gsec.standardid LEFT JOIN assignmentscorecomment com ON ags.assignmentscoreid = com.assignmentscoreid WHERE gsec.standardgrade IS NOT NULL AND s.id = ~(curstudid) AND t.yearid = ~(curyearid) AND (tc.name = 'Term Three' OR tc.name = 'Term 3') AND c.course_name NOT LIKE 'Elem!_Growth!_%' ESCAPE '!' ) WHERE rnk = 1 ORDER BY course_name, duedate DESC
Change all excel cells to Text before pasting in your data. That should do it.
I should have read then entire post. Sorry im an impatient idiot.
In your previous posts to /r/SQL, you had: select * from ( select asec.duedate, asec.name, c.course_name, tc.name, sta.name, gsec.standardgrade, com.commentvalue, RANK() OVER (PARTITION BY sta.name ORDER BY gsec.lastupdated DESC, gsec.whenmodified DESC, rownum) as rnk from &lt;a dozen irrelevant tables&gt; JOIN standard sta ON asta.standardid = sta.standardid JOIN standardgradesection gsec ON sta.standardid = gsec.standardid ) where RNK = 1 The duplicate records only came in to play with the standardgradesection table. If you just left that table out, your query would return the correct results (apart from the selected field gsec.standardgrade not being accessible). So why not handle the deduplication _before_ you join to standardgradesection, by deduping in a subquery? select * from ( select asec.duedate, asec.name, c.course_name, tc.name, sta.name, gsec.standardgrade, com.commentvalue from &lt;a dozen irrelevant tables&gt; JOIN standard sta ON asta.standardid = sta.standardid JOIN ( select standardid, standardgrade from ( select standardid, standardgrade , row_number() over (partition by standardid order by lastupdated desc, whenmodified desc) as rnk --row_number() accomplishes the same thing as tie-breaking rank() on rownum from standardgradesection ) where RNK = 1 ) gsec ON sta.standardid = gsec.standardid ) The above would be my approach, but I feel I should also address this: &gt; The problem here is that - using a rank function - it will get the most recent grade for that curriculum item for the whole school so if multiple students have been graded on the same subject, only the first student will have that grade show up and it filters out subsequent students. the `partition by` clause of the rank syntax indicates the "window" or set of rows, the query will operate against. In your original rank, it's every record with the same sta.name, but there's no reason it can't be windowed on *two* (or more) fields: RANK() OVER (PARTITION BY sta.name, student.id
You could concatenate the date output with an apostrophe before it. This will convert the cell to a text for you when you paste it in.
https://stackoverflow.com/questions/12568408/how-to-convert-an-integer-time-to-hhmmss00-in-sql-server-2008
This is a database with a lot of legacy data. The only way to get all the data I need is to join those tables in that way. This code block came from the creators of the database. I might be able to optimise it one day, but for now, I have a couple weeks to get this working. I was trying to partition by multiple columns before, but maybe I was doing something wrong. I'll try again.
Glad to know it worked for you. Any time :)
Create table databasename.schemaname.tablename
I use PostgreSQL on my Mac and it works great. [https://postgresapp.com/](https://postgresapp.com/)
No worries! Always appreciate the responses
While that works, it returns JAN11' where the apostrophe is not needed in excel
It depends on the company and what they are expecting out of their applicants. I was involved in hiring several of junior level people at my last company and (unfortunatly) with the starting pay range for the position we knew that getting people who already had excellent skills would be problematic and were willing to accept doing on the job technical training. Thus, like StylishNihilist, we were interviewing more for general aptitude, attitude, and personality than for actual skill, though SQL experience was preferred if possible. I would only ask one or two easy questions if they claimed SQL experience to quickly weed out the disappointingly large amount of candidates who were lying about it (if you are claiming 1+ years of experience but don't have the slightest clue about how to select everything from a table something is wrong). At my current company we were recently looking to bring in some more advanced interns who were possibly nearing the end of their time at college with the hope of finding someone with enough knowledge to be immediately useful (and perhaps not too many ingrained bad habits). Those interviews included several more difficult questions, probably similar to the medium and hard ones.
On our trial page we have a brief description of how to setup a local database, and also a fictitious dataset for download (4 database tables in total). https://www.sfdataschool.com/trial.html Click on the **Tool Setup: Database &amp; SQL** tab to see our recommendations for a SQL client (i.e. where you'll write SQL), and the software for running a local database. Long story short, we recommend TablePlus as a SQL client and DBngin for running a local database Click on the **Dataset Introduction** tab to access the dataset we've built for practicing SQL. If you have any specific questions about our approach, feel free to reach out!
If adding an apostrophe in front of the text doesn't work for you, you could try adding a blank space to the front of it. Unfortunately Excel tries really hard to make dates easy.
[SQL Server Running in Docker on Mac](https://medium.com/@reverentgeek/sql-server-running-on-a-mac-3efafda48861)
Here's how I'd dig out the value... just put it in your WHERE clause and use the column name instead of the variable. SQL Server 2008 R2, YMMV. &amp;#x200B; `declare @d varchar(max) = '&lt; Id="-1234" UId="-20000" PV="3" DV="4" Name="" MVS="5" DVD="6" PDX="87654F" /&gt;';` `select` `cast(` `substring` `(` `@d,` `charindex('Uid="',@d) + 5,` `charindex('"',right(@d,len(@d)-(charindex('Uid="',@d) + 5)))` `)` `as int)`
That is an interesting way to store time. I would probably do something like this: SELECT r1_received_date, CAST(CAST(((r1_received_time / 100) % 100) AS VARCHAR) + ':' + CAST((r1_received_time % 100) AS VARCHAR) AS TIME) FROM public.test_result a WHERE "r1_received_date" &gt; '2019-5-1' Also, don't forget &gt;= instead of just &gt; in your where clause if you also want to include the first of the month.
I know most of these from the program I am taking online. The difficulty for me is visualizing the business questions I create/would answer. Theory is easy. I understand the code. Do you have any recommendations for more “practical” uses? Would you think (if I had those concepts mastered) I would be considered for a junior development role? Thanks for your help!
&gt; If this "advice" is sorely incorrect, what do you suppose their real motivation is by telling me the "upgrade" method is correct? To me it sounds like whoever is "helping" you is probably a sales rep, doesn't have a clue what they are talking about, and is trying to read steps from a poorly written checklist. I would start the conversation over and make sure that all that needs to be done is moving the database from the old 2008R2 instance to the newer 2014 instance and that you do not need to upgrade the entire instance. If they cannot understand the difference between the two I'd see if you could push to find a replacement software provider.
So you are wanting to have one machine running two instances of SQL Server, one instance is the main production version and the second will be a read only copy of the first instance to be used for reporting and other stuff without impacting the production environment. This is further complicated because data from two different companies is being stored within the same database, and potentially the same tables, so giving access to one company lets them see all of the data? As far as mirroring the two would go, either Availability Groups or Transactional Replication could probably work for you. You would essentially be forwarding on changes made on the primary instance to the secondary one, but not vice-versa. As far as permissions go they would not be replicated between the instances/databases, but if you only want to give a user access to part of the data that could be complicated and it would require knowing how the data is split between the two companies in the database as there is a big difference between having different schemas and/or tables for each company and having the data for both companies being stored in the same table.
thank you, at your new company is the interview process the same for new grads and the interns?
I'm not really involved in the hiring process and my team hasn't really hired anyone other than for the one position since I started. I do know that my manager likes to ask practical questions so I assume there would still be at least a few, though tailored to the level of ability that he's looking for.
got you, thank you for providing insight into the hiring process
Yes that's correct, seeing the data is fine on the read only version. I will look into Availability Groups and Transactional Replication, that is exactly what we want to do so if any data changes it doesn't go back into the production database. Permissions were always going to be the tricky part, as long as they are replicating between the instances that should be fine. Thanks so much for the feedback, I will report back with my findings.
Thanks! The client is using MS SQL server 2012. I will look into this.
Here was my quick and dirty solution in T-SQL: --MSSQL with example (r1_received_date, r1_received_time) as ( select '2019-05-03','1231' union all select '2019-05-03','1231' union all select '2019-05-03','1231' union all select '2019-05-02','822' union all select '2019-05-02','822' ) select * , case when len(r1_received_time) = 3 then cast(r1_received_date as datetime) + cast(left(r1_received_time, 1) + ':' + right(r1_received_time, 2) as datetime) when len(r1_received_time) = 4 then cast(r1_received_date as datetime) + cast(left(r1_received_time, 2) + ':' + right(r1_received_time, 2) as datetime) else cast(r1_received_date as datetime) end as r1_received_datetime from example I performed some basic string parsing to create the string version of a time (e.g. 8:21 or 12:31), then cast that to a valid time. As a shortcut, if you cast your Date and Time types to Datetime, you can add them together. I used a case statement to account for both 3-length and 4-length times.
SQLite is the easiest, as it's file based, and not a server
Yep. It is a vendor and we just have read access to run reports/querys. Definitely a lot of little quirks with their DBs. Most I have been able to get around without issue. Appreciate the response. I will give it a shot.
Haha, I’ll add that to my cv/resume.
You’re a couple of weeks too late, closest I can do is [cutesy motivational stuffs.](https://imgur.com/gallery/IA7VTF0)
Just added this "hotstring" to my AutoHotKey script and it's magically auto-corrects it for me now: :\*:select 8::select \*
I don't really understand why you need to do this, but as it's not for HA/DR reasons - and must operate in one direction only - I think you want log shipping (transactional replication).
Thanks for the reply, the main reason is that the client doesn't want the person creating the website and viewing the data to have any access to the live data. I am looking into Transactional Replication.
The downvotes are presumably because you don't have to be proficient, or even very good, at SQL to understand that code. It's conceptually simple, just long, arguably confusingly structured/formatted, and seemingly uncommented. I've not seen any of the course content, but we use concise queries and sample data when learning concepts because it's much simpler than including messy data and weird business logic for no real reason. Some rules of thumb about performance would be nice, but the engine is complex enough that I'd consider it generally out of scope for a beginner course.
Isn't it going to be the same data? I expect I'm still misunderstanding, but why not grant them db_datareader only?
Hi still misunderstanding, but why not grant them db_datareader only?, I'm dad.
Yes the data is going to be the same, however this is a rerquest from the client to keep the live data protected, there are also some tasks that the website admin needs to add to the instance to display the information he needs, we cannot risk a change to the live data.
The tasks being added make it make a bit more sense. Still seems like an odd request, but it's ultimately up to the client! Best of luck with it.
You can apparently use LOAD DATA [LOCAL] INFILE to load a CSV. The MSSQL equivalent would be BULK INSERT or using bcp. This sort of function should be pretty well optimised as people must run huge imports with them. There are various options for null handling in the documentation.
Before it. I think a preceding apostrophe just tells Excel that this is a text string, please don't try and do anything fancy with it. The character shouldn't be visible in the cell unless its value is being edited.
Thanks, i will try a preceding one when I get to work.
With regard to their motivation, by any chance does this vendor offer implementation consultancy or hosted instances? They may also just be not very good. Vendors tend to do that.
If you have a continuous run of dates, join the table to itself using the prior day (DATEADD) as the join key, and subtract one figure from the other. I don't know what Snowflake is.
Check out the LAG function
Is there any real need to store widgets and raw materials in the same table? They're distinct entities, and you can use a view to present them in one table if you need it for analytical purposes or something. My initial thought would be to use one table for each of the three types. A table with product and widget keys, and a quantity. Then probably the same thing twice more for raw materials as widget components, and raw materials as product components. Or you put all objects, including products in one table (they probably have distinct attributes so I doubt this would be a good idea). Then a table with a key for the product or widget, a key for the raw material, and a quantity.
Here's How I would do it. Materials Table; Material ID Identifier Key (a Letter code used to reference which table it belongs to) (optional, could be merged with the Material ID, I just like it because it's an easily defined thing which allows for "ID number" reuse, IE MAT1234 and WID1234) , Any other fields related to what the thing is (Description, Cost, Etc) Widget Table, Same as above Finished Product Table Same as Above. Product Construction Table ProductID MaterialID+IdentifierKey Quantity You can then make a view and join all 4 Tables to get the full list of parts
Honestly I see two issues here. Using Joins as filters when you don’t need data from the table you’re joining to. Using IN clauses as filters when the Filters aren’t constants. EXISTS and NOT sexists are what I would use for this situation.
This is called a Bill of Materials. Use an Adjacency list
Joining is heavy which is why you usually avoid it unless you’re actually pulling columns from the joined tables. If not then there are lighter ways to filter based on tables. As the other reply said, WHERE EXISTS would probably be a better solution compared to either presented.
Ah, sorry, wrong dbms. I believe Postgres uses double pipes for concatenation, so replace the + with || or re-write it to use the CONCAT function.
So this sounds silly but the reason for this sort of "master inventory" table is that Raw Materials, Widgets, and Finished Products can all be sold to customers. Can I just make this "master inventory" table a view in my DBMS and properly normalize it like you're suggesting and accomplish what I'm after?
So this sounds silly but the reason for this sort of "master inventory" table is that Raw Materials, Widgets, and Finished Products can all be sold to customers. Can I just make this "master inventory" table a view in my DBMS and properly normalize it like you're suggesting and accomplish what I'm after?
I learned with the Head First SQL book. I found it super helpful to get me from *nothing* to a solid foundation. There's still a lot to learn after that, but nothing that couldn't be googled
It took me a second to figure out what you're doing. That's a strange query. I'd use variables. Both of your examples are called correlated subqueries. They're terribly inefficient. For every row in emp, the server executes both of the subqueries to see if that row qualifies to be returned. In a small table, it's no big deal. In a big table with millions of rows, that'll take a long time to run.
Product master table Product structure table – parent/child relations w qty transactions tables: Inventory, labor, GL Order tables: customer, purchase, work. That is basic. MRP and ERP systems have many more.
Not correlated, but a derived table join.
It really depends, what is your teacher trying to teach? I'm thinking that they're trying to teach subqueries to the class. In that case, the teacher's query makes sense. But your query doesn't really, at least to me. I'd try to write it as a self join first like this: SELECT e1.* FROM EMP e1 join EMP e2 ON e1.JOB = e2.JOB WHERE e2.EMPNO = 7788 and e1.SAL &gt; e2.SAL But this may not be what your teacher is trying to teach.
Raw materials table with a raw materials ID and widgets table with widget ID. Put raw material ID column in the widget table.
This doesn't work as written: it's not empno 7788's salary being filtered on, it's the salary where ENAME='JONES'
Oh good eye, I missed that. I'd replace e2.sal with (Select sal from Emp where empname = 'Jones')
www.w3schools.com/sql/default.asp good starting resource to understand basic queries and also has some basic tests to check your knowledge
I started with stratascratch and then got to write some queries at work. Also there are cool exercise for practicing. For that you'll just have to Google it and you can find their website and advantages.
Yeah, absolutely! If you have good indexing, it should be pretty fast to query raw material amounts for a given product/widget too, as it's just key joins and an integer (which you could also index).
I've done a tandem of SQL Queries for Mere Mortals, Udemy courses and Kris Wenzel's articles on essentialsql.com. Anything that's in simple English will work!
ok thank you
Sqlbolt.com taught me enough to do my current jr data engineer job (plus a comp science degree)
I don't understand. Why don't you just leave the where clause out?
He wants to be lazy and create queries on the fly using string appends rather than using ORM or just array joining e.g.: where = predicates.join("AND");
Is it an interactive book? Like does it come with videos and what not to practice what the book shows?
I will definitely bookmark that so I can use it! Thank you 😊
Definitely love the exercises!
Awesome! Thank you!
Awesome! I’ll look those up!! Thank you 😁
I'd definitely suggest the subquery. That said, I would put the bucket calculation into the same subquery, and the multiplication factor too if you can. That turns the final query into a group by bucket, sum of outstanding amount times multiplication factor, both of which are essentially precomputed.
I agree with what pythor said, but cursory tests on a bigish table we got didn't show the aggregations as a bottleneck. Can you show a execution plan? That might be helpful in finding the actual bottleneck. I also found a sleeker way to limit all those ugly case when statements into a single one: &gt; &gt; SELECT CASE WHEN Bucket = 210 THEN 99999 ELSE Bucket END, count(*) FROM &gt; (SELECT &gt; (SELECT MIN(Buckets) &gt; FROM (VALUES (DATEDIFF(DAY, cast(TRANS_CREATION_DT as DATE), GETDATE()) / 30),(7)) AS AllBuckets(Bucket)) * 30 as [AGE BUCKETS] &gt; FROM TRANSACTION_HISTORY) &gt; as b &gt; GROUP BY Bucket (No guarantee for correct column or table name)
No videos, but there's lots of practical work to do in the book. Check out their website: https://www.oreilly.com/library/view/head-first-sql/9780596526849/
You can just use 'WHERE TRUE' so your SQL engine doesn't have to evaluate it for every row.
Just create the table first and then do individual insert statements instead. If you don't include a particular column on an insert it will get a null value.
,NULL as SubQ6
This was close. I figured out the issue. My Q6 columns were Strings, but the null values were INT64. I had to write `safe_cast(NULL as string) as SubQ6`
Thank you! I’m going to try this out today. I appreciate it!
Just concat a -01 at the end and concert the whole thing to date
I'm on mobile so sorry for the formatting SELECT [CustomeraiD] , [CustomerName] , [Amount] , [Etc.] FROM [sever_name].[schema_name].[table_name] WHERE [purchaseDate] = '4/20/2069'
With no description of the schema and no effort shown on your part, there's not much that can be done for you here. 80% of the answer is in the question.
Nah mate, we need to know how many tables you have, column names etc. this is just lazy.
The actual EASIEST way to install MySQL on a mac is through a web development stack like MAMP (built originally for macs) or Laragon (my current favorite) &amp;#x200B; Here is the link to MAMP [https://www.mamp.info/en/](https://www.mamp.info/en/) From there, you can do all of your coding through PHPMyAdmin &amp;#x200B; I am using Laragon now and love it [https://laragon.org/](https://laragon.org/) What I like about Laragon is that I can add other RDBMS and run them at the same time. Right now, I am using PostgreSQL &amp;#x200B; There are a few others like XAMPP ( [https://www.apachefriends.org/index.html](https://www.apachefriends.org/index.html) ) and AMPPS ( [https://www.ampps.com/](https://www.ampps.com/) ) but they all do pretty much the same thing. &amp;#x200B; Another option is to forget installing SQL all together and use a cloud version. I have been learning Oracle on Oracle's cloud for free, through APEX ( [https://apex.oracle.com/en/](https://apex.oracle.com/en/) ) The great thing about APEX is that because it is web based, you can run it on almost ANYTHING. I have used tablets, my phone, a kindle fire, etc. As long as I remember my credentials, all of my data is saved.
Im sorry guys that was pretty lazy on my part: I have the following columns (first,last,address,city,st,zip,selldate,email) in One table called sales I’ve come up with this: Select first,last,address,city,st,zip,selldate,email From sales Where selldate between ‘12/1/2018’ and ‘3/4/2019’ Would this be correct?
Thanks for your help!
Early morning sorry man, thanks for the help.
Do you have another table with customers?
I do not. I just have that one table 🙄
I do not ... just that one table😕
How are your dates stored? If they're stored as DATE (they should be) Then i'm not sure that'll work, as they get stored in "YYYY-MM-DD" format.
They are stored as Date/ time they are in the format MM-DD_YYYY
If they're date-time wouldn't they have HH:MM:SS in them too?
That’s what I’m confused on about that .... because there is no time ? But the data type for the format is in Date/Time 🙃
If i'm reading your question correctly, the current type is VARCHAR. If every value you want to convert is in the year '2019##' than you can do something like split the string and inject '-' and then convert it to DATETIME type. [https://docs.microsoft.com/en-us/sql/t-sql/functions/string-split-transact-sql?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/t-sql/functions/string-split-transact-sql?view=sql-server-2017)
Are you using a cms/plugin builder or some other tool to build the database? Or is it directly in mySQL you're reading this?
Im using access 🙄
Hmm, Sounds like a custom format then: https://support.office.com/en-ie/article/format-a-date-and-time-field-47fbbdc1-52fa-416a-b8d5-ba24d881b698 Try the code I linked above and see if it returns anything? Really not sure on how access interacts with it all.
`CAST (CONCAT(LEFT(YEARMONTH,4) , '-' , RIGHT(YEARMONTH,2) , '-01') AS DATE)`
Awesome. I’ll check that out! Have you heard of GoSkills? They have a course for Intro to SQL and idk if it is worth it or not.
Thanks for your help mate ! I was actually able to figure it out ... I did. It was certainly the time format..giving me troubled WHERE ((([Sales].SELLDATE)&gt;#12/01/18#) AND ([Sales].SELLDATE&lt;#5/04/19#)); This worked perfectly.
Also, inventory isn't a table, it's a view on top of a table. What you want to do is track movements of parts, ie from purchased to WIP to finished, or lost or damaged
I use this trick in sql all the time when I want to force a join or something similar.
[MSSQL] To get a baseline for query performance without the overhead of returning millions or records to my client, I wrap the query with CHECKSUM_AGG(CHECKSUM(*)). It's better than other aggregates because the query still has to compile all the columns. You can also compare the checksum values to confirm that the result set is the same after making optimizations. SELECT COUNT(*), CHECKSUM_AGG(CHECKSUM(*)) FROM (&lt;Query&gt;) x;
My tests show that MSSQL doesn't evaluate 1=1 or 1=0 at all, and certainly not for every row.
Honestly it's probably easier to just add a HAVING clause to your SQL only looking for dates older than 30 days. USE msdb SELECT CONVERT(CHAR(100), SERVERPROPERTY('Servername')) AS Server, msdb.dbo.backupset.database_name, MAX(msdb.dbo.backupset.backup_finish_date) AS last_db_backup_date FROM msdb.dbo.backupmediafamily INNER JOIN msdb.dbo.backupset ON msdb.dbo.backupmediafamily.media_set_id = msdb.dbo.backupset.media_set_id WHERE msdb..backupset.type = 'D' GROUP BY msdb.dbo.backupset.database_name HAVING MAX(msdb.dbo.backupset.backup_finish_date) &lt; DATEADD(d,-30,GETDATE()) ORDER BY msdb.dbo.backupset.database_name
Ahh, I get it now. Would never have thought of that. Probably because i've never needed to think of it ;) &amp;#x200B; This example, helped me understand. var sqlQuery = "SELECT * FROM FOOS WHERE 1 = 1" if (shouldFilterForBars) { sqlQuery = sqlQuery + " AND Bars &gt; 3"; } if (shouldFilterForBaz) { sqlQuery = sqlQuery + " AND Baz &lt; 12"; }
A slight re-write that should pick up any databases that are completely missing backups as well: SELECT CONVERT(CHAR(100), SERVERPROPERTY('Servername')) AS Server, a.name AS [database_name], MAX(b.backup_finish_date) AS last_db_backup_date FROM sys.databases a LEFT JOIN msdb.dbo.backupset b ON a.name = b.database_name LEFT JOIN msdb.dbo.backupmediafamily c ON c.media_set_id = b.media_set_id WHERE ISNULL(b.type,'D') = 'D' AND a.name NOT IN ('tempdb') --Exclude databases here GROUP BY a.name HAVING ISNULL(MAX(b.backup_finish_date),'1900-01-01') &lt; DATEADD(d,-30,GETDATE()) ORDER BY database_name
lol, nothing lazy about it. it's smart.
Op is using MySQL not MS Sql server so the syntax / functions will differ. I don’t use MySQL but this looks like what op may want. https://www.w3resource.com/mysql/string-functions/mysql-substring_index-function.php
Sometimes lazy is smart, I know lol
SQLzoo (https://sqlzoo.net/) is very good for getting a basic understanding. Lots of tutorials, exercises, etc. It's not exactly real world application, and it won't help you use Python for SQL, but it's definitely a really good introduction.
TIL that mySQL is not MSSQL
I'm using PostgreSQL with Golang and in Go the sql and postgres packages are somewhat primitive, so sometimes I need to do a couple of string manipulations.
:) I’m also sad as I know that page you linked. It gave me so much joy one day to find it when I was sorting out an etl job way back in time. “Wtf I’m getting an error” *check that page* applies to Sql server 2016 onwards. We were on 2008. Sad panda.
I’ve been using the SQL bootcamp on Udemy. It teaches PostgreSQL. My friend used the bootcamp and now has a job where he uses MS SQL daily. It was $11.
Not sure I got a slight performance increase in postgres (technically redshift) but it's my teams prod server so maybe it was just a slight difference in queue time. (shrug)
Island of Mistfit ~~Mascots~~ SQL Server languages
Personally, I'd do it like this: $Query = @' SELECT @@SERVERNAME AS Server, bs.database_name, MAX(bs.backup_finish_date) AS last_db_backup_date FROM msdb.dbo.backupmediafamily bmf INNER JOIN msdb.dbo.backupset bs ON bmf.media_set_id = bs.media_set_id WHERE bs.type = 'D' GROUP BY bs.database_name HAVING MAX(bs.backup_finish_date) &lt; DATEADD(dd, DATEDIFF(dd, 0, getdate()), -30) '@ Get-Content "C:\sql_servers.txt" | ForEach-Object { Invoke-Sqlcmd -ServerInstance $_ -Database msdb -Query $Query } | Export-Csv -Path C:\LastBackup.csv -Append -NoTypeInformation `DATEADD(dd, DATEDIFF(dd, 0, getdate()), -30)` means "midnight 30 days ago". Note that I changed the output to a CSV file. That's enormously more useful than dumping the formatted output of a command. I'd also recommend something like this: SELECT @@SERVERNAME AS Server, bs.database_name, MAX(bs.backup_finish_date) AS last_db_backup_date, getdate() AS check_datetime FROM ... So that you capture when your script actually checks things. Finally, I'd say that you should be aware that msdb.dbo.backupset can contain entries to databases which are no longer attached to the server. It's rare, but not unheard of for them to get orphaned. You can clean them up with `sp_delete_database_backuphistory`.
Select col1, count(1) as count from tableA Group by col1 Having count(1)&gt;1 ;
Thanks for suggestion, I'll scroll through some courses and see which gives most content for least price while having positive reviews
Looks like a great place to start learning SQL, thanks ! Well its not exactly Python for SQL but its rather a module for PostgreSQL/MySQL that I will use to "shape" my database and import data that I want. I am trying to use Python as much as I can since its known for being good DevOps language.
I second this one as a great starting point. I taught myself enough SQL from this site within a week to land a job that required it.
i'm going to reformat this for you &gt; I have tableA with a single column Col1 122 222 112 334 556 112 222 &gt; How do I create tableB like: Col2 Count 112 3 222 2 assuming 112 has a count of 3 instead of 2 is **because of a typo**, you want this -- SELECT Col1 , COUNT(*) AS "Count" FROM tableA GROUP BY Col1 HAVING COUNT(*) &gt; 1
SQL Queries for Mere Mortals is a great source to get lots of practice!
I think dbatools.io has something for this, check it out
https://www.reddit.com/r/SQL/comments/buk8u8/want_to_learn_sql_on_your_own_try_oregon/
&amp;#x200B; TRY\_CONVERT(TIME(0),STUFF(RIGHT('000'+CONVERT(varchar(4),time\_int),4),3,0,':'))
The easiest thing would be to get a cheap windows laptop and install MS SQL.
What if both can be sold directly to customers? Keep the same technique and just have a view for the client/user that shows all?
 create table az_qw (something_id varchar(20), else_id varchar(20), CONSTRAINT PRIMARY KEY (something_id, else_id), CONSTRAINT FOREIGN KEY something_id REFERENCES azerty(something), CONSTRAINT FOREIGN KEY else_id REFERENCES qwerty(else) );
Yes, they should be using an API if one exists. In what way are they "using a browser" to do this? Are they scripting the browser itself via COM or similar automation? This is going to consume CPU and memory resources and steal them from SQL Server, which may present performance issues for your other users. It's not unlike a user logging into the server interactively, which is heavily discouraged. How are they going to authenticate to Sharepoint with this scheme? Will they be storing credentials in their package? Who's responsible for this? My recommendation? Have this developer set up a process _on another server_ to fetch this data (at this point, as a DBA I don't particularly care how anymore because it's not on my SQL box) and load it into one or more tables in SQL Server. Then point the SSIS package at those tables.
Look at the SQL command in strUpdate in the debugger. You’re trying to set intGenderId to a value other than 1 or 2, resulting in the constraint violation
Debug.Print your update statement before you execute it. My guess is you are inserting “0” into your gender column because comboboxes start indexing at 0 and when nothing is selected, it can be -1. I would use something like `if cboGender.SelectedIndex &lt;&gt; -1 Then intGenderID = cboGender.SelectedItem.value Else intGender = 1 End If` SelectedItem.Value should be populated with the genderIDs associated with your descriptions. You can use a tuple to populate the box. If you can’t recode the box, you can cheat by using `intGenderID = cboGender.SelectedIndex + 1` ..but that is a bad practice.
I believe the general scheme is to use a proxy server to make the https connection, other than that the person seems to be expecting to navigate to this page as if it were just a typical url and then login to it using their SSO credentials. My suspicion is either a lazy dev or one that doesn't know what they're doing (although I don't know any better myself). Thank you for the recommendations, I needed to know the right questions to be asking. :)
1. Why do you want to represent the data in this way in your database? 2. Have you considered that not every building will have the same (or same number of) columns? 3. What will you do every time a new feature is added to your "inventory"? Add another column? At a high level, what you're describing is a `PIVOT` operation. But if you have irregularly-shaped data, you may be looking at a dynamic SQL query and those can get painful in a hurry.
&gt; then login to it using their SSO credentials. And what happens when this person changes their password? Or they leave the company? Or their role changes and their access to that portion of Sharepoint is revoked? Spoiler alert: The whole process breaks. Who gets to mop that up? &gt;My suspicion is either a lazy dev or one that doesn't know what they're doing Or both.
I could see this being handy for the way they need/want to store their data. Perhaps they are tracking supplies for expense on the building. Not all buildings would require the same attributes and some buildings may require attributes that no other building contains. They wouldn't require denormalized data with x amount of columns, one for each expense item.
Dryer? You could input all of the variables into an Excel file, then load it to a database, then write some dynamic SQL to run a loop across all the variables you're after. Then give you the final result so you can paste it back into Excel. Or you can just write all those queries one by one, and save them one by one. Not seeing another option.
Does MySQL allow IN? where modelID in (1958,1972,1988)
The problem with this though (using your example), the query would return all cars that were around in 1958 and 1972 and 1988, and all Wranglers. I only want Wranglers from 1958, 1972, and 1988. &amp;#x200B; Thank you for the response though!
Dryer comes from the acronym 'dry'- don't repeat yourself (not sure if you knew that and were just saying there isn't any dryer code). &amp;#x200B; Thanks for the response. Sadly, you may be right. Out of curiosity, when you say "dynamic SQL to run a loop across all the variables you're after" do you mean 'dynamic' like how the word is used in common parlance? Or is that a SQL term that refers to something I'm unaware of. I ask because I'm pretty sure I've come across that word before in conjunction with SQL.
Your current query doesn’t do that though? You just have a list of OR conditions. If you need them to be Wranglers then make it an AND condition: where modelid in (1958,1972,1988) and model in (‘wrangler’)
Instead of what you did I'd go with: SELECT *, (Jobber * 0.7) * 0.92 AS 'final_price' FROM lundmay WHERE MAP &gt; 0 AND ( ModelID in (286,491,493,548,666,667,688,696,1033,1037,2766,2780) OR Model in ('WRANGLER(JL)','WRANGLER(JK)','WRANGLER(TJ)') ); If you wanted to filter based on the correct combinations of model/id and year something like this: WHERE MAP &gt; 0 AND ( (ModelId = 123 AND Year = 1999) OR (ModelId = 456 AND Year &gt;= 1979) )
This is an excellent answer. (I already upvoted, but it was worth highlighting the goodness of the answer.) One alternative - certainly not as elegant, but I have occasionally found it helpful - is to create a third column in az\_qw for a PK instead of using a composite key (say az\_qw.id). This can sometimes make UPDATE or DELETE statements marginally easier because there's a single INT to hang from. (In fact, just today I added just such a column because I grew tired of having to type out two values - one a long-ish VARCHAR - in my UPDATE statements. Super lazy, I know.)
HAVING is a filter condition not a column. You don’t need to name it.
I remember Excel being a bit wonky with that sometimes either requiring the column alias to use square brackets instead of apostrophes and/or requiring you to alias every single column regardless of if you were changing the name.
It does, we even [walked OP through using it yesterday](https://www.reddit.com/r/PowerShell/comments/bwc59x/script_to_query_sql_servers_to_see_backup_schedule/).
How are the answers from [your question yesterday](https://www.reddit.com/r/PowerShell/comments/bwc59x/script_to_query_sql_servers_to_see_backup_schedule/) not helping you solve this?
It looks like you have "smart" quotes around `‘example’` which will likely give you trouble. Use square brackets for your identifiers instead. `SELECT sheet1.col1, Sum(sheet1.col2) AS [example]`
This
Yep, this is what you what to do, with each year/type separated too. Not sure what cars the modelids relate to, so the below is kind of pseudocode, but the general idea is there, with a few different combinations to hopefully make it clear. &amp;#x200B; SELECT lundmay.*, (jobber*0.7)*0.92 AS final_price WHERE ( (modelyear IN (1958,1972,1988) AND model = 'wrangler') OR (modelyear &gt;= 2008 AND model = 'dodge challenger') OR (modelyear &gt;= 1979 AND modelid = 498) ) AND MAP &gt; 0
As a SQL term. Like you might have a template query such as: select blah from tables where &lt;parameter_field&gt; between &lt;parameter_1&gt; and &lt;parameter_2&gt; and &lt;parameter_field2&gt; = &lt;parameter_3&gt; and client = &lt;client_parameter&gt; So from here you can see how you could import mapping variables on a client basis to feed each loop of the query, so long as there is a template. You're probably gonna just want to run everything by hand.
We actually do "1=1" as the first where criteria in straight TSQL where I work. It's not for the same reason as OP, but it helps us. The effort to add it into our queries is negligible (especially with an SSMS snippet), and the time it has saved when you're trying to figure where the disconnect between your understanding of the data and your query results by commenting out individual bits of the clause to determine how you're losing rows is beneficial. Some of the ETL processes we've been working on lately have some really gnarly queries that are trying to wring sanity from several years of poor business processes that left the data in a mess, and just getting things consistent enough to do trend or fiscal year analysis had yielded single queries that are hundreds of lines long. It helps that we use SSMS Boost to format all our code, so there are never compound parts to a where clause on a single line. It's super quick to highlight a chunk and comment them out in one go, then uncomment, or vice versa. We lose some ability to see the queries "as a whole", but as a group we're all bigger fans of lots of whitespace to make individual criteria easier to distinguish than saving vertical real estate.
Initial answer looks good indeed. But, as you mentioned, isn’t it a good practice to create an int placeholder for keys, instead of using varchar, for indexing purposes? What you call lazy sound just good to me. Unless you’re very space constrained I guess.
I mean this could be a but lengthy but have you considered using UNION? Select * From table Where year = 2000 and make = Jeep UNION Select * From table Where year = 1979 and make = dodge UNION 3rd select statement for different make and model combo UNION 4th select statement for a 4th make and model combo UNION On and on until you are have all your combos you need.
just set up a temp table or a cte (with union all) of your combinations, then join to your table i.e your temp table Brand Model yearFrom Ford Mustang 1999 Dodge Challenger 2008 .... then join to your table from lundmay l join #filter f on f.brand = l.brand and f.model = l.model and f.yearFrom &lt;= l.modelYear
And they still want to do it the hard way... Lol can't help that
Look up "gaps and islands".
You can do something like ( select \* from table where date = current\_date and Profit BETWEEN low AND high). or Connect your PostGres database to [Scai](https://scaidata.com?utm_source=red&amp;utm_campaign=ps_r_sql_promo_q119) Platform. Find your table, add a filter to the profit column and look behind the generated SQL.
ROW_NUMBER() partitioned by user and project ordered by timestamp. Earliest two would be row 1 &amp; 2. Datediff from there.
I actually did what the AND to apply to everything (everything had to have MAP &gt; 0 for it to be included). Sorry if I wasn't clear about that in the open. &amp;#x200B; Thank you so much for the detailed response!
I did try a UNION, but it significantly slowed down MySQL to the point that it was almost not worth it. Granted, the horrendous code I went with slowed MySQL down a lot too, but it did run a bit faster IIRC.
Windowing Functions are awesome, and I wish I got into them sooner.
1. This is more of a data wrangling attempt before exporting from the database, in a reproducible way for even less tech-inclined colleagues. 2. Yes, my attempt at it was building new variables through case when (which of course get tied to the specific row that the feature is stored) and I can live with missing values in those new variables as long as I’ve defined the useful ones to me. 3. My thought process has so far been pre-defining variables so I guess I would have a static amount of columns until I decide I need more (or I could just prepare for all possibilities as arduous as that might be). Also - I have logic that will deal with duplicate building features for the same ID in case that concerns anybody. I’ll look into pivot today. Thanks
window / analytical aggregate functions (name depends on which DB)
Can you select one at a time? So you have 13 results (or however many combos you need) instead of just 1
Also maybe your tables aren't indexed correctly.
Perfect use case for LAG/LEAD.
Well that worked perfectly, thanks
They're not indexed at all. I should have done that.
Here is the text query, in case you'll need it: select MAX(LastEnforcementMessageTime) as Ltime, LastEnforcementMessageID, ResourceID from v_Update_ComplianceStatus where LastEnforcementMessageID is not null group by LastEnforcementMessageID, ResourceID order by ResourceID asc
Here's the query in case you'll need it in text format: select MAX(LastEnforcementMessageTime) as Ltime, LastEnforcementMessageID, ResourceID from v_Update_ComplianceStatus where LastEnforcementMessageID is not null group by LastEnforcementMessageID, ResourceID order by ResourceID asc
Hey, this is untested but I'm sure should give you what you need. &amp;#x200B; WITH CTE_Data AS ( select LastEnforcementMessageTime, LastEnforcementMessageID, ResourceID, ROW_NUMBER() over(partition by ResourceID ORDER BY LastEnforcementMessageTime DESC) rn from v_Update_ComplianceStatus where LastEnforcementMessageID is not null ) SELECT * FROM CTE_Data WHERE rn = 1
Yes, it works! Thanks very much!
What other people said..windowing functions. I use RANK() mostly then order by results by the rank number, it's down hill from there.
Thanks for the help everyone :)
Not answering your question, and maybe your code is just academic or a test, but please use parameterized sql (aka bind variables)! Concatenating user inputs is a really bad practice in databases.
Came here to say this, just did this the other day at work for a similar problem, works like a charm.
Thank you!
Which sql vendor?
How would you solve adding hierarchy levels? And how would you query for all subclients (not just direct ones)?
Not sure if you still need it, I had this same problem, what I did is I opened mySQL workbench, clicked the local instance. In the left side, you look for the startup/shutdown in the instance menu and then clicked on restart, it may ask you for your password, but should work
Can you please elaborate on those ? Where do I find resources to learn them
Same issue here. I authenticate and it immediately goes to "could not be fetched". Not a timeout issue. I can run queries against it from PowerShell, just not in Workbench. Let me know if you beat this.
I found this site helpful for me to understand the concepts. As others have mentioned though these will depend on the DB and also version of DB. [https://mode.com/sql-tutorial/sql-window-functions/](https://mode.com/sql-tutorial/sql-window-functions/)
No prob. Be sure to look at [nsomneeak’s reply](https://www.reddit.com/r/SQL/comments/bwwk3h/how_to_specify_multiple_conditions_that_are/eq131gk/) that has some examples of how to use multiple conditions.
Just a thing to keep in mind is Rank() allows for duplicate rankings, while Row_number() does not (I guess when there’s a tie it arbitrarily orders them). So if you only want a single result for each rank then row_number() might be a better option.
Look into Advanced SAS. The industry is pivoting to python, but the old guard still employs people with certs to fill contracts. Until that changes, Advanced SAS ports SQL and will give you somewhere in a $65-90k starting range.
You want to break this down and move away from @tables and towards #tables. In your final example, the one taking so long... it's taking so long because you are using a combination of subqueries and temp tables (without indexes), but it would be very easy to clean it up like this: SELECT DISTINCT C.CustomerEmail ,C.TotalOrdersCustomerBase INTO #CC FROM @CustomerBase C SELECT CC.CustomerEmail ,CC.TotalOrdersCustomerBase ,RC.TotalOrdersRecurringBase ,LC.TotalOrdersLifetimeBase From #CC You can even throw indexes on your #tables. Break the entire thing down into chunks and see how that helps your performance.
I think that as a recent grad with a marketing degree, it will be difficult to find an advanced analytics role in general industry. I think you could find a role as a marketing analyst pretty easily. The actual cert itself shouldn't matter. It would help if it's a known company, such as EdX for example. I would also look at certs in R or Python as well.
Thank you very much. I work on SQL server 2012 and learning it as I go along.
Yes, I know. Just had an issue with that but I have to rank by multiple columns so I just tack a column onto the end of my rank with an identity column or timestamp such that I don't get duplicate ranks.
What is the difference between an @table and a #table?
Look for entry level jobs that would use SQL in general. You may be able to land an analytics job, but it's unlikely out the door since HR uses software to filter resumes for specific skills and years or experience. I personally graduated with a marketing degree and worked as a Quality Engineer on an application for a while before transferring to Analytics. I know a lot of other people have done market research or were marketing analysts.
It's possible to have parts that have not been ordered.
You need `where colA is null`. It generally doesn't work to do equality comparisons involving `null` because `null` means unknown and you can't tell whether two unknown values are equal to each other.
An @table only exists until the end of your query, and then it goes away. An #table will persist after it is finished and allow you to put an index on it. An ##table is a global temp table, which will not only exist after you create it/allow you to index it, but it will let others query it, or allow you to query it from other windows.
There are a couple. First off table variables (@table) only exist for the duration of the batch, while temp tables (#table) exist for the duration of the session. They are also handled differently by the server and generally speaking table variables are okay for handling relatively small amounts of data but temp tables are much better suited for larger amounts like you are dealing with. Alternatively if you aren't reusing the subsets of data much you might see some performance gains using CTEs instead. Lastly, the SELECT DISTINCT statement can potentially be quite resource intensive so it's best to avoid using it whenever possible.
That is what was told to me as well by my professor although it confuses me, I thought that order for ORDERPROD to even “exist” it must contain and order with a part. when you say that it hasn’t been ordered, is it the idea that in the system parts always exist as a tangible thing (ie. in a warehouse) whereas salesorder must be generated and doesn’t necessarily always exist?
I think you are trying to overthink things. The ERD is supposed to measure and reflect a real life system, so let's walk through how that system would work based on what we see. Imagine that you are starting up a store and order 100 different kinds of parts to stock the shelves. At the moment, you have 100 different rows in PARTS, but as you have no customers you have no entries yet in CUSTOMER, and therefore also no entries in SALESORDER nor ORDERPROD. Once someone comes in and makes an order for 5 different parts, you'll end up with one CUSTOMER, one SALESORDER, and 5 ORDERPROD. You still have 95 PARTS that have not been ordered. Several months down the line you discover that after a few hundred customers and over a thousand orders, part number 'XY79' still hasn't been ordered at all. Each CUSTOMER has at least one SALESORDER, each SALESORDER has at least one ORDERPROD, and each ORDERPROD has exactly one PART, but there still exists PART.partno = 'XY79' that has no corresponding entry in ORDERPROD.
That seems to have answer it! Thank you, I appreciate it.
SQL Server Express would be the best / easiest from a setup and security standpoint for a Windows AD Environment. The 10GB limit is per database. You can easily move larger tables into another database, it'll just require you to prefix your table names with the database name as well. &amp;#x200B; Postgres is also a good option, it's a little more involved to implement and setup than SQL Express, though.
Can you elaborate on what you said about the 10gb limit? Is that a sort of workaround?
Also worth mentioning that @tables (table variables) don't have statistics. For the benefit of OP, statistics record approximations of the cardinality and spread of data in a table, and are used by the server to figure out a reasonably efficient way to run a query. What this means is that @tables should only be used for low numbers of rows.
Me thinks you are reading/wording relationships slightly incorrectly. in a relationship A-&gt;B, if you are trying to word the relationship, the entity A should be the singular and the subject of the statement. so, part to order: "A part can be in 0 or many orders" order to part: "An order can contain 1 or many parts"
Yeah, so you can have: Database1: Table1 =&gt; 10GB Database2: Table2 =&gt; 10GB Query: SELECT * FROM Database1..Table1 t1 INNER JOIN Database2..Table2 t2 ON t1.ID = t2.ID So effectively you have 20GB of data, but each database is only 10GB. This won't work as easily if you have a single table that needs to be &gt; 10GB, but if you can partition it so that it's split to &lt;10GB chunks, you can make it work by UNIONing the tables together in a query or just create a view to do this for you, then select from the view. It isn't the most ideal thing for scaling, though, so if you do envision having very large data at some point and upgrading from Express to Standard (~$1k I think) will always be out of the question, might make sense to go a different route now.
#tables don't have statistics either, correct?
Sure you can. Use the CAST function to transform the field. cast(fieldname as varchar(255))
I tried earlier but didn't work. I'm on 2008. I'll try again.
Ok thanks, that makes sense.
* Do not use table variables (@table) for anything more than a few rows * I'd personally use [Common Table Expressions](https://docs.microsoft.com/en-us/sql/t-sql/queries/with-common-table-expression-transact-sql?view=sql-server-2017) instead (assuming you're on MSSQL as that's what I know) if you're only running one batch, i.e. returning one set of results * Temporary tables should work just as well AFAIK, or much better if your queries would benefit from indexes * Review your index coverage, e.g. I think BETWEEN predicates can be slow and can benefit from indexed date columns * Obviously there are also costs associated with indexing, else we'd index everything possible! * Review your query plans, consider pre-calculating anything which is transformed in a join predicate (though I didn't see anything expensive looking in your code) - this can be a calcuated column with the PERSISTED property * Generally just review your query plans. If you know how to read them then they'll point you towards where time spent optimising is best spent
No go Conversion failed when converting the varchar value 'Pallet' to data type int.
Awesome to hear! I may get punished here for saying this haha, but I generally think SQL is SQL - just pick a syntax and run with it (e.g. MySQL, PostgreSQL, etc.). Once you truly understand the building blocks of writing SQL, you'll be able to easily transfer that to any SQL style/syntax. It's not so much the attainment of a certification that matters, but more-so, can you confidently articulate and display your working knowledge of SQL to others. To which, this only comes with practice (both writing SQL and then communicating/displaying your ability to others). One thing I've found to be incredibly valuable is understanding how SQL, and databases in general, fit in to the broader ecosystem of data analytics. For instance, do you understand how a database and BI Software (e.g. Tableau) work together? If the data in a database is incorrect, would you know how to pass requirements to a Software Engineer to have them help you fix it? Once you have data in your possession, do you have a framework for analyzing it? There's many more questions, but the context surrounding data analytics is just as important as being able to do it. With respect to finding a decent job, it sounds like you already have a well-adjusted perspective. That is, someone just needs to give you a chance (like we all did). From there, you'll be able to progress really fast. So, how do you get a chance? Personally, I think it comes from really honing your "personal sales skills" + having enough technical know how to prove you can learn fast once on the job. To be more specific on the "personal sales skills"...it's about telling your story in a clear fashion, asking questions you truly care to hear answers for, and putting yourself in a position to get as many chances as possible (i.e. interview as much as you can...no decision has to get made until an offer is put in front of you). Somewhat of a selfish plug, but we're starting an online education program where you can learn SQL, access BI Software tools we've partnered with (Chartio and Looker), access data analytics training curriculum, and more. We're currently in "building" mode, so we don't have 100's of hours of content yet haha...but if you are interested in learning more check out https:///www.sfdataschool.com and/or shoot me a DM here on Reddit!
No go Conversion failed when converting the varchar value 'Pallet' to data type int.
You can't convert a string into an integer, unless it's made up of numbers. Appologies I thought you needed the opposite.
What is the actual query or case statement you are trying to execute, and what are the various columns involved defined as?
It's in OP intfield is int I want my then to be like 'Value'
How can I be a competitive applicant for a Marketing Analyst position?
For a recent grad looking to get into Data Analytics, with a Marketing degree, getting a SQL certification shouldn't be a high priority, IMO. Having such a certification would not impress me as an employer. As an employer, what I would be looking for is a solid background in statistics, maybe some light machine learning and some understanding of either R, Python, SAS etc.
Oh, so if table.intfield is an int then you want to replace it with 'Value' otherwise you are using the string stored in [poop].[pee]. Got it. I just wanted to make sure there wasn't anything else weird going on. The problem is occuring at the point of the &gt; 0 comparison you are using. It's trying to convert all of the values to int and failing because there's not a way to implicitly convert non-numeric characters. In this case you'll want to use: WHEN ISNUMERIC([table].[intfield]) = 1 THEN 'Value'
I *think* AUTO\_CREATE\_STATISTICS is on for them by default. [https://www.brentozar.com/archive/2014/02/statistics-matter-on-temp-tables-too/](https://www.brentozar.com/archive/2014/02/statistics-matter-on-temp-tables-too/)
&gt; Oh, so table.intfield is a string field No it's an int field.
Right, but if you create them then there are no statistics by default, no? It isn't as if they persist over time, each time you run the job it creates a new instance?
Oh. What's probably happening at that point is that when it first evaluates something it decides to cast "Bathroom" as an int, and it can't stuff 'Value' into it when that branch of the case statement comes along and fails. CASE WHEN [table].[intfield] &gt; 0 THEN 'Value' ELSE CAST([poop].[pee] AS VARCHAR(10)) END AS "Bathroom"
I’m in the same boat. I just got my degree in December, but I’m working at a low paying small company.
Yes, but it looks like they get calculated and used even if the table is only queried once. I assume they're used to help in selecting a physical join operator, which could help a fair bit with big tables.
Looks like you need to cast the return value to the original datatype. I didn't get a NULL when I didn't, but to the engine whatever you're returning is presumably a binary representation of a value in a particular datatype, which is unknown to it. DECLARE @encVal varbinary(8000) DECLARE @decVal varchar(4000) SET @encVal = ENCRYPTBYPASSPHRASE( 'stackoverflow' ,'C:\Users\brogeli\Documents\Personal\select-network-option.pdf' ,1 ,CAST('10' AS varbinary(4000)) ) SET @decVal = DECRYPTBYPASSPHRASE ( 'stackoverflow' ,@encVal ,1 ,CAST('10' AS varbinary(4000)) ) -- Not NULL, but not our value either SELECT DECRYPTBYPASSPHRASE ( 'stackoverflow' ,@encVal ,1 ,CAST('10' AS varbinary(4000)) ) -- Ayy SELECT CAST ( DECRYPTBYPASSPHRASE ( 'stackoverflow' ,@encVal ,1 ,CAST('10' AS varbinary(4000)) ) AS varchar(4000))
Can you elaborate here? I don't always index my #tables, but I do sometimes because it increases performance. I assume the process of creating indexes would increase the execution time... but their presence might help joining when I am not purposefully creating an index?
Works! I learned something new! I never would have thought to order it like that. Thanks.
You can have stats without having an index, but indexes need stats. Is that what you're wondering about? [https://docs.microsoft.com/en-us/sql/t-sql/statements/create-statistics-transact-sql?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/t-sql/statements/create-statistics-transact-sql?view=sql-server-2017) I'm by no means an expert on optimisation though, a wizard may well turn up shortly and severely correct me!
&gt; **Recent Grad looking to get into Data analytics.** You kinda just posted an Oxymoron, no offense. It's rare you get an analytic job right out of College. Companies look for a great amount of experience in that field. And your diploma doesn't cut it. Regardless of type: Undergrad/Masters etc. *You've* never touched software in the Real World that has an affect on anything. They can't just trust you'll "get it" and trust you with their Data because, of your Degree. It's not a bad thing to want to do but, that's a building block Job Type. You'll need to grind Entry Level positions for a bit. eDX courses are awesome and so is uDemy (when they have their 10.99-14.99 Courses). you can also, take the Certification from Microsoft when you're comfortable. Especially the Querying with Transact SQL Cert. It was the first one I did. Got a Study Guide from Amazon and passed it first try. &amp;#x200B; I went from knowing nothing with no College to a Support Job, where I learned C# and SQL on the Job (Pharmacy Software &lt;--- I actually got this Job from only Experience Building Computers and as a Pharmacy Tech, I got Lucky), had to grind my ass off for 5 years, finally was able to touch Live code at that job. Then, after getting dicked around with my Salary, got my Job as a Data Analyst at a new Software Company with ONLY experience. Everyone here has a college Degree. I don't. That goes to show how Experience is more important. I still try and learn in my Free time because maybe this isn't my Last Stop in Life &amp;#x200B; Try those Courses. Get a Job that has a SQL structure you like and show em what you got. Entry Level is the way to go here. Entry Level Software Support (not call center). Level 1 IT Support at a Software Company or Property Management Company (big SQL users). Stuff like that
Changing it from a table variable to a temp table was the only thing I needed to do, query went from 10 minutes to 10 seconds, thanks!
Does anyone know how to make the numbers for this chart look good? I am okay with SSRS and know how to mess around with the properties, but I can't figure this out. I set the y label on the chart to only go as high as my highest column, which helps a little, but does not help much when there is a column that dwarfs all others.
I don't think there's any way that many data labels, so close together, are going to look good. I'd remove them and create a data table to accompany the chart instead.
You probably assigned the wrong server role/s to the login.
Can't you do something in SSRS where you have a pivot table of the raw data beneath the grap? Looks nice for line graphs so you don't have to label the points.
My idea is to set the True/False value of using an expression, since if there are less than 6 columns, the numbers look okay. So I only want to remove the numbers when I have more than 6 columns. Does anyone know how to do this? I tried doing: =MAX(RowNumber(Fields!Date.Value))&lt;7 But I can't have an aggregate field within an aggregate field.
only public is checked
I'm not sure this has been thought out. Or maybe you're asking the wrong question. My first question is - is that how your data is stored or how your are retrieving your data (table structure vs. a select)? I think if this is how your data structure is, it's mostly good, and what you really want is a different way to *present* your data. Data structures and norms (namely normalization) exist for a reason and provide a ton of value for usually little to no cost. I see a couple things I'd probably adjust ("Building Feature" not normalized and storing hard coded percentages - this seems ripe for over- or under-representation), but for the most part if you stay on top of data integrity you should be solid. &amp;#x200B; Now - the next question is why/how are you trying to present this data *in this way*? This is in line with /u/alinroc 's first question. Think about what you're asking for - you'd essentially have: |Building ID|Wall Brick%|Wall Concrete %|Wall Drywall %|Wall Masonry%|Floor Carpet %|Floor Tile %|Floor Hardwood %| |:-|:-|:-|:-|:-|:-|:-|:-| |12345|75|25|0|0|25|75|0| I hope this image conveys how out of hand this data could get - every building feature/building material combo would need to be represented as a column. Only you know your data - maybe this isn't that crazy. But - every time you wanted to offer a new material type you'd have to add a column. &amp;#x200B; TL;DR - we can help you get what you're visualizing - just our experiences say we should double check that you want what you think you want, because it's out of the normal conventions. And also - it sounds like you just want a better way to present your data, not a new way to store it.
One way, using window functions to rank each row with the same `fruit_name` by date and only picking the first ones: SELECT id, quantity, fruit_name, date FROM (SELECT *, row_number() OVER (PARTITION BY fruit_name ORDER BY date DESC) AS rn FROM fruits) WHERE rn = 1;
 declare @var int = (select datepart(dd, getdate()) if @var &gt;= n select * from table where datefield &gt;= dateadd(datepart, something) and datefield &lt;= dateadd(datepart, something) else select * from table where datefield &gt;= dateadd(datepart, otherthing) and datefield &lt;= dateadd(datepart, otherthing) Not in front of a computer, but you mean something like that?
Make sure server role is set to public . Then go to user mapping and select the two databases and check the data reader role membership.
Yes, you can use GETDATE() in the WHERE. Depending on the amount of rows returned your query performance will suffer (albeit could be only slightly).
Thank you, this worked perfectly. I was then able to wrap a `sum` query around this to get the total quantity.
Depends on what your position at the company would be . SQL itself its a monster on its own. If they mentioned joins and how to detect duplicates . I would suggest to study basic TSQL queries . Be familiar with the select statement, the “where” clause and joins. Also normalization, store procedures , views and indexes . Also , if you don’t honestly don’t know the answer to the question, don’t lie . Just say that you would research online for a solution or ask for help. Be honest about your abilities .
Hmm, entry level you say? &amp;#x200B; \-Explain the different backup types and recovery models \-Explain blocking and deadlocks \-Explain statistics \-Explain the different types of indexes (+ what is a composite index; can indexes be filtered) \-Explain 1NF, 2NF, and 3NF (+ what's the difference between a natural key and a surrogate key) \-Explain the different types of joins \-Name some of the aggregate functions and how would you use them \-Explain an execution plan \-Explain how you can detect duplicates with and without using a window function \-Explain what a CTE is and its potential advantages versus disadvantages \-Explain what dynamic SQL is and its potential advantages versus disadvantages
you were so close!! SELECT columns FROM table1 UNION ALL SELECT columns FROM table2
I recommend this to people in you situation. https://academy.microsoft.com/en-us/professional-program/tracks/data-science/ It's not easy and fast, but it's cheap and you'll learn alot.
An entry level what? i.e., what is the job?
For an entry level database developer maybe. For an entry level data analyst, of that list I'd only expect &gt;-Explain the different types of joins &gt;-Name some of the aggregate functions and how would you use them with these two as a stretch goal: &gt;-Explain how you can detect duplicates with and without using a window function &gt;-Explain what a CTE is and its potential advantages versus disadvantages
PostgreSQL on a Linux VM? Can't connect to the DB if not in pg\_hba.conf - good security All our datawarehouses are either on Postgres or being moved to Postgres, talking terabytes.
You really ought to paste the query you've got so far and describe the data you're working with, and the DB you're working with. But at a random guess, something like SUM (case when orderdate &lt;= sysdate + 10 then orderquantity else null end) Might steer you in the right direction.
I use to work with a guy who graduated in Marketing, in his senior year he taught himself how to code in Python and landed a job at my company a month or two after he graduated.
Know the difference between an inner and left join and why you'd want to use one of the other. Aggregating make sure you know how to use group by and also converting integers to decimal hint multiply by 1.0. maybe how to use a subselect in your main select. For duplicates there's a few ways you can do that. one is with a group by and having count star greater than 1 or using row number with a CTE. Though they're probably only looking for the group by with having.
Very little of that has to do with the actual querying of SQL.
I start with tableA, and I am trying to sum colA, colB, ColC by year and id. I came up with the following code, but it won't run: Create table tableB as select ID, year, sum(colA), sum(colB), sum(colC) from tableA Group by ID, year; Can anyone give me some suggestions? Thanks.
Declare a variable, set it to the date you want to compare to and use the variable in your where clause. Way more efficient than using functions in the where clause.
 where case when day(getdate()) &gt;= 5 then [something] else [other thing] end
Will it insert into sql from Google sheets as well?
I don't see any code here....it looks like you want to group by id, year.
Hold on ... I just put my code here
Whoa that looks useful. Thanks.
Create tableB as select id, year, sum(colA), sum(colB), sum(colC) from tableA Group by id, year;
The software is cool, but could it would be appreciative to title them as 'Connecting SQL to Google Sheets using WayScript'.
MDX and DAX
Where GetDate() &gt; DATEFROMPARTS(Year(GetDate()), Month(GetDate()), 5) This means where (today) is larger than [current year],[current month],5 aka if today is beyond the 5th of the current month.
Yes you can use a [google sheets trigger](https://www.wayscript.com/documentation/trigger/google_sheets_trigger) if you want to have it run every time a row is added to the sheet (or other events) or [read in a google sheet with the module](https://www.wayscript.com/documentation/module/google_sheets), and then pass the data into [SQL](https://www.wayscript.com/documentation/module/sql).
Have you tried a temporary table or table variable?
Make sure you understand how DISTINCT works, how GROUP BY works and if you have time try looking at windows functions.
This sounds like it could be similar to a shopping cart. You have an Items table with each item id and its description (couches and chairs). Then a Cart table for each cart (collection) with the cart ID and the ID of each item in that was in that cart transaction. And finally a table that has the customer ID and their cart id (and usually a transaction timestamp). So then to get the items in a customer’s cart you use the cart ID to join to the Cart table then join again to the Items table to get each item name.
Please tell me where OP stated what exactly the "title" was?
I'm glad you contributed to an " entry level position where I'll be working with SQL". Very little of your comment has any value.
Ooooooo weee. I model highly abstracted systems for scalability and speed. Let's start without the taxonomy tables I use a group table. The main pieces of this table that apply to your idea are the primary key (identity) and the SpecificDataSetNumber (a system unique Id). This SpecificDataSetNumber is generated in my DataSet table using a SQL Sequence (views, tables, external files get one). This allows me to create my own object_id() and keep it if I drop the object. There is a column in every table for that tables SpecificDataSetNumber. If I have 3 rows in a group table (pk 1,2,3) the SpecificDataSetNumber would be the same for every row because it is that table's SpecificDataSetNumber. So my group(Id, SpecificDataSetNumber) table in key value pair (1,22)(2,22)(3,22). My people table (1,76)(2,76)(3,76) My parts table (1,887)(2,887)(3,887) Now for the relationship table... Rowid_parent, SpecificDataSetNumber_parent, rowid_child, SpecificDataSetNumber_child. (1,22,2,76),(1,22,3,887)... get it? Relate any row in any table to any row in any table. The taxonomy is what makes this super powerful. Include additional tables to the relationship subject. The relationship table now has a relationshipTypeid, the relationshipType table has a relationshipFamilyId, family has class, class has realm. My realm would identify the relationship People 2 Group it would have a class of Tree, a family of Direct, and a Type of Member. That's how you split your relationships to map a meaning to them... which allows you to create a more families under the same realm / class like descendant and ancestor. You can use a recursive cte to fill this data and never have to use it in code, only when things change. One relationship table witch dynamic where in clauses. A group is an abstract concept. Could be a company, could be a department, could be a race... doesn't matter. The below would be some faux SQL that would find all people in the current group that exist in any sub group where the person is related to an ancestor. Parent from relationship where type is people to group / direct and child in ( select child from relationship where type is group to group / descendant and parent in (select child from relationship where type is people to people ancestor and parent in ( select peopleid from people where name is 'great great great grandfather')))) The design exists and is powerful. But you'll have to understand the physical modeling also.
An overly critical gatekeeping response with a complete lack of grace and an almost audible sneer? You must be a senior DBA.
What is your question?
Typical comment I would expect out of someone who gave zero valuable input to the OP.
You should probably name each of your aggregate columns: sum(colA) as sumA, etc
can you try this? [https://docs.microsoft.com/en-us/sql/reporting-services/report-design/add-scale-breaks-to-a-chart-report-builder-and-ssrs?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/reporting-services/report-design/add-scale-breaks-to-a-chart-report-builder-and-ssrs?view=sql-server-2017)
The answers are gonna sway heavily dependent on your goal(s). I'd learn GO, PowerShell, and Python if you're wanting to stick with data ( + MDX and DAX if your path is geared toward it).
Marketing Analyst roles have a reputation for being sort of "soft" compared to other analytics roles. In my experience, those roles place more emphasis on more user-friendly marketing software (Google Analytics, Looker, Qlik etc.) than on raw SQL and statistics knowledge. For this reason, you would probably be better off improving your knowledge of Excel, data visualization software, and marketing analytics concepts/jargon than you would be studying advanced SQL. &amp;#x200B; &amp;#x200B; I disagree with the above comment about beginners pursuing Python and R certifications. The various SQL dialects may be worth pursuing a **low level** certification in because those cert tests are backed up by Fortune 500 companies. However, I haven't met a manager at my current role that would think a cert for open source software like R or Python was more valuable than the paper it was printed on, especially from a non-CS new grad. Python and R are core analyst skills, but they are best developed in college or after landing the entry level job. &amp;#x200B; &amp;#x200B; If you are a new grad, you will not be a "competitive" applicant. Most HR resume processing systems will filter your resume out automatically, and your lack of work experience will make you a risky hire compared to an applicant with experience. **However**, your chances improve dramatically once you get past HR and can express your energy to the manager filling the position. If you have connections that can make this happen, use them. Otherwise, be ready to submit hundreds of applications for each handful that make it past HR. Don't take the rejection personally. Keep trying and eventually a manager will like you or their first choice will refuse the offer.
I love how a legitimate comment is downvoted. It is EXTREMELY rare to land an analytical job straight out of college (unless the college is highly prestigious and you have connections). Certifications will NOT help you unless you have experience which tag along with those certifications. While I don't agree with everything said; most is applicable to your situation. You don't have to follow the same path as /u/jethrow41487 as you have a college background, but, I would focus on an entry level position which has a great mentor and allows for growth. No exam (or cert) will teach you as much as a great team. I think a wise decision would be to try and understand the foundations of data analytics and were your radar is aiming. Someone who understands a WYSIWYG isn't really an 'analyst' IMO, but, to each their own.
This is what I thought. I’ve filled out hundreds of apps and have gotten only a few callbacks as someone with two unrelated, brand name internships + notable ECs.
Are you using MSSQL Server? Are there existing logins for AD Security Groups that this user could be a member of? If you are using MSSQL, execute this bit in each of the database contexts and it will tell you what permission path the user is using in order to gain access: EXECUTE xp_logininfo @acctname = 'domain\username'
Remind me in 13 hours
Lag function can return the previous record or later depending on the function.
Why is the last price of the 4/2/2018 catfood record that of a price in the future?
my mistake, I was creating a sample data, messed up that date, I corrected it, it is date in past.
Remind me in 12 hours.
I am not an expert in SQL, doesn't lag return immediately previous price. If I have the same price for a few days, can I still use it to return price before it started to be constant?
There are parameters you can enter to increase the Lag.
1. What's the difference between WHERE and HAVING? Provide an example of each. 2. Write a query that returns the second most expensive foo for each bar. -- think about windowing and ranking and how you would do it , this would be different in postgreSQL vs SQL Server. 3. Write a query that returns month over month change of MAU for product x, y, and z. I was asked these recently at MS for SQL/data gigs.
Not sure what kind of sql OP is using but some flavours let you use QUALIFY which is a filter on a window function and would let you bypass making the cte.
Its sql server 2012
Use something like notepad++ and do a replace for the end of line. If I recall, it would be extended mode, replace \n with ;\n
What I would do right now in your shoes is do a personal project to show recruiters and managers that you’re capable in doing data analytics or proven to have such data analytic skills. - choose a topic that you’re interested in - find some data sets that pertains to it - clean and manipulate the data set in any way you want - apply data analytics to it, such as use Tableau to tell the story behind your data sets - extra points, do exploratory analysis on it This will 100% strengthen your resume as well as your skill set in data analytics, trust me I’ve done this a couple times. As for SQL, I think you should just teach yourself the basics and then practice sql interview questions. And if you think you want to be certified in sql just to show more proficiency, then I’d recommend MCSE Sql certs. In most jobs working in data analytics, half the time you will be writing queries and doing the technical part of the job. The other half is really understanding your business model and why your company is data driven in the first place.
Python or R
Or just use ssms/vs and use alt/shift to add a character to all the lines.
Thx
[https://www.sololearn.com/Course/SQL/](https://www.sololearn.com/Course/SQL/) &amp;#x200B; Enjoy - Had everything i needed to start a new job, knew nothing before this course and taught me the required fundamentals
I would be happy if there would be native SQL Server connector in GDS but I do not think it will ever happen.
Show us what the output should be
Only if each statement is exactly one line and the lines are the same length (or you find the longest one and start there)
This is true :) Good points.
This is a great answer. Thank you so much!
Almost every language interacts with SQL in some way. The question is what do you want to do as part of the grand strategy. Like if you're going to be manipulating website databases, PHP probably is the best way to go. Big Data? Python would be your choice. There's a lot to consider.
Easy mode: if it is sequential insert statements, put the colon before each statement by a find and replace. Find INSERT Replace with ; INSERT and just clean up the last/first manually. And since it is Oracle, don't forget an explicit COMMIT.
Ok it is sequential so I can put the semicolon before the INSERT instead of at the end of the VALUES()
Yeah as long as the statements are separated by semicolons. SQL is blind to linebreaks.
https://automatetheboringstuff.com/ This is a great (and free!) resource to begin doing just what you're asking. Check it out and if you like it, I hope you support the author and buy their book.
If you want to augment SQL there is no shortage of choices. As others have pointed out, nearly every language accommodates working with SQL. Since you said you were interested in learning programming languages to *improve* your SQL and work with data, R is nice for working with data sets/data frames/vectors. If you really like the programming style of SQL (declarative rather than imperative) you might look at functional programming languages (e.g. F#, Haskell, Scala, OCaml, Clojure).
If you do much work in SSIS then C# is very useful.
Networking. As in knowing people. That’s literally the best way to get a job in the industry. While a hiring manager may not give a shit if you have a degree, in my experience they’re gonna have a helluva time convincing HR to hire you over people with a degree. Any degree. If you really want to get an intro job working with databases without having a degree, id learn enough to get certified by Oracle.
Hiring managers don't convince HR of anything when it comes to making a hire. HR isn't involved at all in most orgs beyond setting up the interviews, doing the paperwork, background checks, etc.
Agree with other comments on this thread. I never finished my degree and have not had trouble finding work in a decade. My boss doesn't have a degree and is in the same boat. He has been offered CIO positions but turns them away because of the politics. Once you solidify skills, network and put your availability and resume in front of everybody. You can have a comfortable life without that degree.
That depends on the workplace. At my previous company HR would screen resumes and do initial phone interviews before sending resumes to the Hiring Manager. Even when I applied internally I had to get past an HR interview and they told me they wouldn’t send my application to the hiring manager because they didn’t think I was the right fit.
HR definitely will do pre-screenings.
Maybe I'm more of a newb than I thought... I'm not sure where I'd put that in Access. Thank you for your reply :)
At my old work, a State government, the managers interviewed you, but had to fill out a form with questions created by HR and they would turn that in. After that, HR would make the hire. We ended up with some unqualified employees because of this.
write two separate queries, making sure that they return the exact same number of columns and that the columns are compatible by datatype for example, SELECT memberID, claim_amt, claim_type FROM dental_claims and SELECT memberID, claim_amount, claim_code FROM medical_claims open each of them in query **SQL view** copy and paste one of them behind the other, and type `UNION ALL` between them save it as a new query
Volunteer somewhere. See if your fav charity needs some DB cleanup or some queries/reports built. Put boundaries around what you will be willing to help with, and deliver it. Put that on your resume as work experience.
Red Gatetools sql prompt will add semicolons after each statement whe;you use it to format code, they’ve got a 28 day free trial
This is an old tutorial I made to pull data from SQL Server to Excel using VBA. Basically a macro pass TSQL query and pull the dataset to Excel.
Degrees are largley irrelevant. They will help slightly with some base knowledge. But there is no substitute for real life problems You just need to get in front of a database in anger. I took a massive pay cut for my first true SQL role. Then built up. Look for internships, charity work, even data entry if there is an element of SQL. Good luck my dude
To get around the degree requirement I go in as a contractor through an IT headhunter. There are a lot of contracting opportunities. HR hasn't typically gotten involved in my experience in this approach. Most of my contracts turn into F/T offers, at which point HR has little say because you are already proven and vital to keep around. The degree has been a moot point.
To get around the degree requirement I go in as a contractor through an IT headhunter. There are a lot of contracting opportunities. HR hasn't typically gotten involved in my experience in this approach. Most of my contracts turn into F/T offers, at which point HR has little say because you are already proven and vital to keep around. The degree has been a moot point.
I migrate data from old SQL data bases from software created in 90s to a new application.(also SQL db) As a example, let say legacy db has only 2 address fields so I have to reformat data to 6 fields. So far I have wrote many scripts, use them depends on data set, clients preferences etc. Do not deal with any form of data visualization just pure data sets. At this point considering to put everything together in one function or looking for better/easier way to do it. Happy to spend some time and learn something new. Before I do it I just need to find best tool for this job.
Dev team is working in C# or C++
As easy as sql is, and how is changed so little in decades, it's surprising that now people can't put it on their resume. I'm a big fan of the applying for jobs you are confident you could handle... Not those you Beverly think you're on paper qualified for. So any entry level analyst job just start applying and see what the feedback is
&gt;I dont want to get a degree if I dont need to, looks like all the information and resources are out there for free. While I don't think a degree is absolutely necessary to do the work, you're going to have a hard time getting a job without one. Heck one of the entry level hires at my last job was fresh out of school with a cs degree and didn't know any SQL.
It looks like you are grouping by a lot more things than you are selecting, so you could try removing columns from the group by clause that you aren't using in your select clause. This will fix it if your problem is being caused because you are grouping things more precisely than the report cares about at the moment. You aren't really duplicating data but instead are splitting it into smaller categories that aren't important. If you want to find out which columns are causing the "duplicate" rows to appear, I would take the contents of the group by clause and add them to your select statement and run it. Find some duplicates and compare the results and see which columns are being split due to different values.
&gt; You just need to get in front of a database in anger. Gonna need some details here. :)
I will definitely check it, thx.
Sorry by 'in anger' I meant 'for real' although i have been very angry at lots of databases in my time :)
So two things: Firstly, in most variants of SQL you can't refer to an alias created in the SELECT clause in other clauses. So you can fix your HAVING clause by substituting the alias for the whole calculation, i.e., HAVING SUM( ROUND( a.UNIT_PRICE * ORDERED_QTY ,0)) &gt; 0 Secondly, you can't group on that because it's an aggregate: the GROUP BY decides what values are grouped together to put into the SUM. I'm not sure you want that in your GROUP BY at all - if you're aiming to have one record returned per combination of SALES_REP_ID_1 , EGW_FN and ORDER_NUMBER that displays that group's total sale amount, you should remove Amount from the GROUP BY altogether. You're also going to need to do something with a.SALES_REP_ID_2, either removing it from the SELECT list or adding it to the GROUP BY.
I added a.SALES\_REP\_ID\_2 to GROUP BY and I also put that statement in HAVING. It didn't change anything of how its being sorted out. 2nd photo posted was the output on the edit
I thought the problem was you were getting an error - what is the problem now?
Well I was getting an error when I attempted to group by Amount. I removed Amount from the group by. I still can’t seem to sort Amount in Desc order
You don't have an order by clause. Again, you need to put the whole calculation in. Add this to the end: ORDER BY SUM( ROUND( a.UNIT_PRICE * ORDERED_QTY ,0)) desc
yeah, with views, that is the way to do it Oracle Security is very robust if you do it right and its not complicated. you can also do it the "right" way which came at Oracle 12c which is row level security: https://docs.oracle.com/database/121/TDPSG/GUID-72D524FF-5A86-495A-9D12-14CB13819D42.htm#TDPSG90066 please do note that if you create the view in the user's schema and granted select for that view on user's schema that user can select the whole table effectively going over the view. create the view on the same schema as the tables. you will have to create many views if you have many users with different privileges. you can create scripts and triggers that will make sure to automatically create views and grant stuff when new rows / users are created. your choice.
Usually they are the non-normalized legacy databases I have to work with. Sometimes those really tick me off.
Excel has a data source option for SQL. I have pushed some repeating reports this way. My current favorite way is to make an API on an internal web server that sends the data as a JSON object. Excel formats and refreshes every time the Refresh button is pressed in the Excel sheet.
you'd write a query against the INFORMATION_SCHEMA [key_column_usage](https://www.postgresql.org/docs/9.1/infoschema-key-column-usage.html) view
My recommendation is to just get the degree unless you financially are unable. While they are largely irrelevant, some organizations will demand it. I agree with other commenters that going in to a charity is a great idea, and working on certifications. If you want to be a developer, I'd focus on that. Learn a language, contribute on GitHub to larger projects and put that on your resume. Setup a homelab to perform a task for you (Plex server is a good option) and work on automation. Automation, specifically recovering in the event of failure, is a sought after commodity. Problem solving skills, and working on coding just snippets inside of larger projects also look great. I'd focus on the career you want rather than going from DBA to development, as you may get pigeonholed into a specific path.
People all over internet talk about how you don't need a degree, and I took that seriously and didn't bother majoring in compsci. I just self studied and became a good programmer and got a degree in something I really love instead. While I was getting a degree, I worked for my school as a programmer and figured that finding a job after would be easy. Took me 8 months and over 80 applications to get an offer. Out of all of those, I had three in person interviews, and two phone only interviews. I was on the verge of giving up. Can't say for a fact that a degree world have made it easier but I can say that every job to which I applied preferred candidates with a degree
I think I noticed it because I'm not familiar with Netezza.
What do you mean by people can't put it on their resume? Is it not enough of a qualification these days?
I just want to get a job in the field asap, I'm tired of the quality of jobs I've been stuck with. I was under the impression that database work would be easier to get into with no degree. I'm not completely against getting a degree, I'd just rather not if I dont have to. Anyways, thanks for the advice. Everyone thats commented has helped out alot, I'll keep coming back to read them whenever I feel like I need more direction. Appreciate it, everybody
Tell us the exact result you’re after.
No I mean anybody who had even a basic eye for useful skills in the future would have learned it, but they didn't
SELECT a.*, b.* FROM...
Can you tell us what information you're trying to collect with a SQL query?
Oh I see what youre saying. Yeah, it's engrained in so many areas of technology now that it's not going to go away any time soon. I'm early 20s and only semi recently decided to pursue a career involving computer software, otherwise I'm sure I'd have learned it by now
I only see 5-6 in the last few weeks, most of them by the same couple of accounts. Wouldn't call it an influx.
The downvotes are because you were critical of the person's learning material and then shared code that wasn't actually advanced, which heavily suggests you're also at an early stage with SQL but pretending to be advanced. Multiple tables aren't complicated. Joins are easy, subqueries are easy - in the greater scheme of things.
&gt;bind variables We haven't been taught what these are yet.
Might be a longer term solution, but have you considered using SSRS or PowerBI report server? You can create paginated reports that can have variables to filter down to things like department. Also when you open the reports, they always pull the latest available data. Setting it up allows it to be one place to store reports and you can even setup subscriptions for the reports to email who needs then (even with preset filters). If you are stuck with just excel, VBA is an option.
I didn't pretend to be anything. Multiple tables may not be complicated to you, but they're surely more complicated than single table queries covered by both resources OP is using to learn. What is considered "complicated query" to you?
You got a degree in something though which is still a leg up over no degree.
Definitely don't disagree, and I have no regrets except that if I had known it was actually important I would have at least minored in compsci
In your main stored proc uspAddCustomerAndJob the last execute statement runs uspAddCustomerJob, but only passes in 2 parameters. uspAddCustomerJob is the 2nd to last proc defined in this whole script ad requiring 3 parameters. You were only calling it with 2. I *think* what you’re going for: exec uspAddCustomerJob @inCustomerJobId OUTPUT, @intCustomerId, @intJobId I’m on mobile not in front of a computer so I may have a typo.
Out of curiosity, how does Netezza compare to other platforms? My company is transitioning from SQL Server to Netezza for our EDW. From what I’ve heard, the processing power is far greater for Netezza, but my team hasn’t had the opportunity to use it much.
Dealing with junk data, cleaning up data, turning complicated business processes into single queries that give consistent output, coming up with creative ways to remove duplicates, writing complex queries that run fast, indexing, pivoting, cursors, triggers, error handling etc etc etc. Joins aren't complicated. You just line them up. In real business situations having multiple joins is just a thing that happens, it's like complaining about having to go through multiple doors to get to your desk - the joins aren't the hard part.
I ran: DECLARE @intCustomerJobID AS INTEGER; DECLARE @intCustomerID AS INTEGER; DECLARE @intJobID AS INTEGER; EXECUTE uspAddCustomerAndJob @intCustomerJobID OUTPUT, @intCustomerId, @intJobId, 'Ethan Malloy', '513-555-9644', 'edmalloy@yahoo.com', 'Fix cracked sewer pipe', '6/1/2019', '6/5/2019' And it yielded: Msg 8144, Level 16, State 2, Procedure uspAddCustomerAndJob, Line 0 [Batch Start Line 157] Procedure or function uspAddCustomerAndJob has too many arguments specified.
Well....I wrote a big long answer here that was thorough and I was pretty proud of myself, but he nuked his account it seems! Argh!
is the bidder info in its own table? if so you can LEFT join the auction items with the bidder info. when there is no bidder info a left join will return null.
I think the original reply just had a typo. You need to add @intCustomerId as a parameter to where the uspAddCustomerJob proc is being called WITHIN the uspAddCustomerANDJob proc.
Yes - these are on separate tables but this view has some extra calculations and case statements based on some combinations of different fields. I would like to bring these columns over without copy and pasting the calculations.
Are you saying there's a view that has calculations done on one of the tables? If so, Can you just join the other table to the view that has those calculations rather than the underlying table? If not, maybe it makes sense to make a view that does the calculations in one place, then you can join to it (in other places too) to get the other information. It's hard to know exactly what to suggest without a couple simplified examples.
MySQL is not my forte, but have you tried dumping to any other location... perhaps a folder local to the MYsQL instance instead of direct to your machine? ( to establish if the issue stems from your machine denying access or if it is a permissions issue with MySql)
I ended up going to the DB administrator asking him to do the dump directly from the server. We first tried with my credentials, same issue, no clue at all and no more descriptive errors were given hence we simply did the dump as root. Dumping as root user obviously worked flawlessly xD
Probably need to grant the account `LOCK TABLES` permission in addition to select GRANT SELECT, LOCK TABLES ON *.* TO 'myuser'@'%';
I reckon your problem is this: &gt; --all-databases You say you can normally connect to 'a DB'. Have you tried exporting that one DB on it's own?
I tried also specifying that particular DB yes.
Thank you!!
Tried also to use the single transaction option without success,
 Select trim(both from colA) from tableA; http://dwgeek.com/different-types-netezza-trim-functions-examples.html/
How would you add more conditions to this? E.g. Select trim(both from colA), colC, colD from tableB;
On the contrary what everybody thinks, the bottleneck is on your log files, not your data files. Some like to put the log files on a separate disk which is fine but it should be on your fastest storage. SQL Server doesn't consider data committed until it his hardened in the log file. Second fastest storage should be your tempdb. Again, it's up to you whether you want to put it on a separate disk. Some clients keep their data and log files on their SAN and then put tempdb on some local storage for pure speed. Since tempdb is temporary data anyway, you can get away with reduced resilience. (A simple mirror for example) What I would not recommend is sharing tempdb, log or data files with your OS disk. If the disk fills up for some reason, you are gonna have a helluva time trying to fix it with a full OS disk.
I don't see what's wrong with `Select trim(both from colA), colC, colD from tableB;`, so I don't know how to answer that.
I hope they teach you. I've been in this business for over 16 years and so many developers don't use bind variables. I've seen piles and piles of tickets related to poor performance, strings needing to be escaped or else the SQL breaks and the subsequently hacked-together (and sometimes buggy) escaping functions, and of security (SQL injection) issues...all of this stemming from lack of bind variables.
You'll want a couple of tables to do this right. Table - Mushroom 1. MushroomID int autonumber primary key - identifies a record in the table 2. FileName string 3. Family string 4. LatinName string 5. EnglishName string 6. Notes string 7. Other columns On any of those values which you only allow a mushroom to have up to one value, add those columns to the Mushroom table (like CAP). If a single mushroom can have more than one of a value (like habitat), you'll need to define both a Habitat table (to list the defined habitats) and a MushroomHabitat table (that links a MushroomID to one or more HabitatIDs). Habitat and MushroomHabitat tables would look like this: Table - Habitat 1. HabitatID int autonumber primary key - identifies a record in the table 2. HabitatName string Table - MushroomHabitat 1. MushroomHabitatID int autonumber primary key - identifies a record in the table 2. MushroomID int - points to the Mushroom table 3. HabitatID int - points to the Habitat table This design lets you have all the entries that a mushroom can have only one of in a place where it's easy to query against, along with defined separate attribute tables for entries a mushroom can have more than one of.
If your workload was read heavy would you use fast drives for data and slower drives for log?
I think this is good advice. I would add to this, create a data dictionary to define your columns/fields, the data type, and a description of what each column is supposed to represent and where the data comes from (you might need a sources table). Having a data dictionary/metadata will be invaluable later as you query, add new items and especially if you share this database with others.
Still the faster drives for log. If you have properly maintained indexes, most reads should be sequential IO, even "slow" disks are pretty fast in sequential IO. But you should really be looking at the waits stats for your server and decide for yourself. Every rule has exceptions.
I would assume that it is working correctly and that the columns with {null} aren't pulling from a.PURCH_DATE or a.ORDER_DATE but I don't have enough information based on just the screen shot and WHERE clause.
 SELECT DISTINCT a.ORDER_NUMBER ,a.SALES_CATEGORY ,a.ORDER_DATE ,a.PURCH_DATE ,DATEDIFF(a.ORDER_DATE,a.PURCH_ORDER_DATE) FROM SOBOOK a WHERE DATE(a.ORDER_DATE) BETWEEN DATE(:startDate) AND DATE(:endDate) AND a.PURCH_DATE IS NOT NULL; There is the rest of the code. I'll update the post for you for more of the output
I don't see a PURCH_DATE column in the image...
Yeah I forgot to write order when I typed it into Reddit
maybe your dates aren't dates but actually strings (else why use `DATE()` on a.ORDER_DATE) and maybe what's in PURCH_ORDER_DATE is actually a zero-length string, and this is simply shown on your output as {null} instead of nothing i'm just guessin, tho try AND a.PURCH_ORDER_DATE IS NOT NULL AND a.PURCH_ORDER_DATE &lt;&gt; ''
I attempted that and I get the same results. I am quite stumped
What RDBMS?
It will only show records from t1 and add records where t1.id = t2.id. If multiple matches it will add multiple lines in your output, not sure how size of table 2 is relevant? It's table 1 that defines the base
It should not matter, but are all of the ID's in Table1 unique? Have you checked to see if all the id's in T2 are in fact unique? e.g., select id from t2 group by id having count(*) &gt; 1?
MySQL
Do you mean to trim all those columns? In that case: &gt; Select &gt; &gt; trim(both from colA), &gt; &gt; trim(both from colC), &gt; &gt; trim(both from colD) &gt; &gt;from tableB;
Hmm, strange... Two things you could try -- I wonder if something is strange with how the date field is being stored -- Just to make sure, try running a filter for blank cells (vs. NULL) a.PURCH_ORDER_DATE &lt;&gt; '' I wonder if you wrapped this in a subquery, if you'd get the same result: WITH setup_table AS ( SELECT DISTINCT a.ORDER_NUMBER , a.SALES_CATEGORY , a.ORDER_DATE , a.PURCH_ORDER_DATE , DATEDIFF(a.ORDER_DATE,a.PURCH_ORDER_DATE) FROM SOBOOK a WHERE DATE(a.ORDER_DATE) BETWEEN DATE(:startDate) AND DATE(:endDate) ) SELECT * FROM setup_table WHERE a.PURCH_ORDER_DATE IS NOT NULL;
So just another guess - some genius didn’t make the default value a ‘{{null}}’ string for that column did they? You could just validate whether it’s a date at all using one of the date functions like so: WHERE DAY(a.PURCH_ORDER_DATE) IS NOT NULL To see if that properly filters the set. I don’t know much of anything about MySQL but you may remove the DATEDIFF in your SELECT statement and see if that works, maybe it’s implicitly doing a GROUP BY because you don’t have one?
I had already attempted the first suggestion you put. The 2nd one I quite literally copy and pasted in and got You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'WITH setup_table AS ( SELECT DISTINCT a.ORDER_NUMBER , a.SALES_CATEGORY ,' at line 15
Just updated the query, I still had the alias ("a.") in front of PURCH\_ORDER\_DATE which wasn't necessary
 WHERE DATE(a.ORDER_DATE) BETWEEN DATE(:startDate) AND DATE(:endDate) AND a.PURCH_ORDER_DATE IS NOT NULL AND a.PURCH_ORDER_DATE &lt;&gt; '' AND DAY(a.PURCH_ORDER_DATE) IS NOT NULL So this is now my WHERE statement and I still get null values
If you remove the DATEDIFF from your SELECT do you still get them?
First you say: "In real business situations having multiple joins is just a thing that happens on pretty much every query, it's like complaining about having to go through multiple doors to get to your desk - the joins aren't the hard part." Then you follow that up with: "You don't need multiple tables to explain CTEs, temp tables, window functions or most other advanced SQL concepts" So if joins are an everyday thing in a professional setting, then it only makes sense that one is should be comfortable applying SQL concepts with multiple table queries. You're contradicting yourself here. Also, you can't assess someone's sql skills based on a his/her comments, so please get off your high horse and mature. You're no different than folks resorting to insults when they're lacking any substance. This is something you're not getting. Thanks!
Download SSMS. Much nicer syntax and will stand you in good stead going forward.
&gt; So if joins are an everyday thing in a professional setting, then it only makes sense that one is should be comfortable applying SQL concepts with multiple table queries. You're contradicting yourself here. A lead or lag function is the same if there's a join or not. You do not need a join to explain to someone how and when to use most window functions. You can teach joins separately but including joins when teaching a window function... There's just no reason to do it. It's needless. To go back to my analogy, you need to go through several doors every day to get to your desk to do your job. I'm not going to make you walk back through those doors every time I want to show you how to do something at your desk just because it'll be part of your every day job. I don't care enough about this to continue trying to explain it to you. Good luck learning SQL.
Sadly, I can't download anything on my work computers. Would this work ordinarily? Is there a different way to do it?
I haven't done Access in ages, but try something like this: UPDATE table1 INNER JOIN table2 ON table1.column2 = table2.column2 SET table1.column1 = table2.column1;
Can you try where column is not null and column != '';
Ok.
Thank you so much! Worked like a charm!
Not MS, but in Oracle turning on flashback has saved my ass several times, while also providing same-day "who dun it" proof when users disagree with what they claim they did or did not do. Retention uses resources and disk space so I limit mine to one day. But ever do a delete and commit and then you realized your where clause was a little loosey goosey and you deleted 70,000 rows instead of 15? Simply re-insert those records from the flashback (or temporal) data from 10 minutes ago and move on with your life like you weren't almost about to get fired.
That's annoying been there. I'd suggest trying to get SSMS, much easier to deal with and Access SQL is awful. Similar but not quite the same. Can't remember access but try converting this from SQL. update t1 set t1.c1 = t2.c1 from t1 a inner.join t2 b on a.c2=b.c2
They won't even let us change our desktop background, lol. u/mwdb's suggestion worked, though. Thanks.
Looks like you're setting the value of the same cell repeatedly.
Thats whats i was thinking but not sure what i would add to correct the issue. I originally thought the range but that only repeated the value. Also, it is pulling the last record. Not sure if that is a cause for concern, as well.
You need to use a variable for your row index. Probably call it i. Increment i per loop iteration. I would also just set one column per row (maybe just the quantity column) for now until you can get this figured out.
So something along the lines of: Dim i as integer Sht1.cells(i, 1).value = rs.fields(“quantity”) Or do i need to set “i” as something? If easier, i can post the whole code for reference. Thank you again for all the help!
In pseudo-code because I haven't touched VB in ages: &amp;#x200B; declare i and set i=1 begin loop set cell value using i as an index i=i+1 end loop &amp;#x200B; Of course the loop needs an exit condition. Looks like you were on the right track with your original post with the "Do while not rs.eof"... Also this isn't really a sql problem anymore, fyi. :)
Haha good point. I absolutely appreciate the feedback! Forgive me for my dumb question as i am fairly new to vba and more of a “find a code and replace values to fit my spreadsheet” person. Where exactly would i add your suggested code? Before my code i mentioned? Or get rid of my code all together?
I was rewriting the whole loop in pseudo-code, so replace your loop in the original post with that - of course you should translate it to VBA.
I got it to work! Thank you for the input!
Solution verified
You won't be able to "find and replace" on the code OP has provided, as they've given you a VB-like pseudo-code. Pseudo-code is basically a fancy way of saying programmers can write "something that looks like code, but isn't" to give other programmers an idea of what to do. The code above won't be useable, it just looks a bit like VB As an aside, this is a VB question, not a SQL question and you might get better responses on a VB/.NET subreddit
okay, just spitballing here... AND a.PURCH_ORDER_DATE IS NOT NULL AND a.PURCH_ORDER_DATE &lt;&gt; '' AND a.PURCH_ORDER_DATE &lt;&gt; '{null}'
You keep referencing cell 1,1. So you just keep updating that specific cell right?
It won't have more rows than T1. But because it will be T1's columns plus T2's columns, it will be wider. So if by size you meant disk space, sure.
You may need a null safe equality operator. Postgres has the standard "is [not] distinct from" whereas MySQL has "&lt;=&gt;": https://dev.mysql.com/doc/refman/8.0/en/comparison-operators.html#operator_equal-to
Can you post your query?
Select cola, colb, colc, count(cola) over(partition by cola) as cnt from tablea;
Which database?
SQLite
nice
You mean something like SELECT * FROM importance AS imp JOIN table2 AS t ON imp.id = t.id WHERE imp.level = 5; You'd probably want an index on `importance(level)`, and on `table2(id)` (Unless it's already the PK column). [EXPLAIN QUERY PLAN](https://www.sqlite.org/eqp.html) will tell you how indexes are being used (If they are), and the sqlite3 shell's [.expert](https://www.sqlite.org/cli.html#index_recommendations_sqlite_expert_) command can suggest others.
This guy is correct. Also op, you need an inner join not a left join. Also an index in sql is a stored physical address location listing of column data in tables. It allows your server to process queries quicker. It does not seem like this would be helpful here.
How do I index on importance if it's not in the table though? I agree that's what I want to do, I just thought I shouldn't add it as a columnn when it's already in the db
Not a data duplication issue. I just want to be indexed by the importance which is stored in another table. I agree I messed up with left vs inner join
Generally speaking index both tables on what they are going columns you join on.
Don’t worry too much about it or beat yourself up. The world simply f*cked up and that’s just what it is. Why do you need a job? Are your basic needs met to live (food, water, shelter, clothing, sunlight, and air)? Are you living above your means? When working on contracts —save your money. You have to learn how to survive a different way. The good days are over—go to college, get a job. Everybody simply doesn’t get a job. With the advancement of technology, there are less jobs. Try starting your own business. Instead of waiting for jobs, create jobs. You want someone else to tell you what to do. Unfortunately many companies save more money by not having permanent employees. And again, it’s not you—the world is f*cked up.
You said it was in a table...
I'll give you the OLAP answer: put the column in the table and skip the join. Denormalization ftw.
Smells like homework. What have you tried on your own so far?
Using SSMS won’t change anything about the syntax used to query and the errors returned by an Access database. If anything, it’ll make it harder to work with.
FILL a table or temp table using {preferred programming language} select and join on matching primary key (auto-increment integer would work). Doing it all in SQL is possible, but likely proprietary to the database system you use (vendor extensions). This feels like it shouldn't be solved by SQL. It looks like pm2/pm1 sets are generated. Why list 1 goes up and list 2 goes down I've no idea, but it's simply reading forwards or backwards based on n (possibly n%2 == 0). If pm1 were &gt; 2 lets say 3 or 7 or 9 how would it work? It's an odd design. Interested to see if there is a sql only solution. Even more curious if it's standards SQL or vendor SQL.
You need more detail in your question to get a full answer. SQL works on sets so you have to force it to work at row level and it does not like it =) You need to break it down, first step is to take your parameters and generate a list of possible combinations. Store this in a tqble For speed you want to write a set based (ie it does it all in one step) query to generate all the sums at once. Rather than iterate row over agonising row
This is what I feel like would be the fastest, but I have this problem with almost every table, and that just seems like bad practice.
&gt;Interested to see if there is a sql only solution. Should be possible with a recursive WITH statement. I'm not posting a solution though since this seems like homework.
You _just_ missed the awesome free SQL Saturday Dallas last weekend :(
SQLSaturday.com
The answer is that you can use any programming language. But, you can also misuse any programming language so it makes the same (roughly) mistakes as Excel. I.e if you read in 1TB of data in RAM you... won't be able to do that, because you don't have that much RAM. So yeah, you need to learn how to develop professional software. :) Feel free to ask more questions as this is a very general answer.
You could analyze the data with SQL queries- Insert the results into a table, and then query that
I’m not sure where in Dallas you are but my mentor suggested the SQL class at Collin College and Dallas County Community College. I’m currently in Plano and work in Irving.
Yeah but I need to do statistical work. Like run correlations and create regression models. Of course that’s after I clean the data. When I export something into excel, there’s too much data and the thing either really runs super slow, laggy, or becomes unresponsive.
R and Python have good packages to write queries in and perform manipulations. [RODBC for R.](https://www.rdocumentation.org/packages/RODBC/versions/1.3-15) I haven't used this much, but I've heard good recommendations about [SQLite for Python.](http://www.sqlitetutorial.net/sqlite-python/) Hope this helps.
whats the query plan look like? Is it desirable or like putting breadsticks up one's nose?
Export to a CSV and analyse it with Python, R, Julia etc. Don't use Excel for anything other than playing with small exports. There are many reasons for this approach other than Excel slowing down and crashing, the main benefit is that you can keep your scripts/programs under source control.
Why does it "smell like homework"? This sounds like the kind of mad crap I'd ask people years back. I'm uncertain if I'd be happy if someone on my team wrote it, as we have many more application instances to do things like this in. But it could very well be a dev learning. I forgot all about recursive CTE's until /u/reallyserious mentioned them
If you're pulling in query results that are crashing Excel, you might try using MS Access as an intermediate solution until you find your programming legs. Access can hold much more data than Excel without crashing, plus they play well together so you can dump subsets, filtered data or aggregated data for manipulation into Excel without any trouble. If you have lots of text data, be careful to change Access' default text data type otherwise you'll use up a lot of memory. Older versions used to default to 255 characters. You can net a lot better performance if you trim these fields down to something you actually need.
Do you know about power query? It would take the first 200 rows from your dataset and then you can aggregate the same to your desired output and simply have that built into the spreadsheet. I have done this to txt files ranging in size form 8-15 gbs into excel xlsx files no larger than 40 MB.. Give it a try..
R or Python (pandas) will handle this no problem.
i'm sorta in your shoes as well and can second (and third) all the folks recommending Python and pandas. I've taught myself a fair amount over the past 6 weeks and it's pretty awesome how much stuff you can do with only a bit of python/panadas knowledge.
I suggest downloading Anaconda and getting started with python in Jupyter Notebooks. Due to its popularity there is a surplus of beginner tutorials, ESPECIALLY for stuff like data manipulation, visualization, correlation, and regression analysis. As for actually importing the data, the pandas library will help you store the data in a tabular format. If you are reading from a csv (or any other delimited file), pandas has an import function that will allow you to choose how many rows of the data you want to read in at a time. This can help if your data is too large to fit into memory (I.e. data file &gt; RAM available for use on your machine) or even if your file is large and you want to be able to process smaller batches more quickly. Additionally there is an ODBC library available in python that will allow you to connect to your database instance and query that data directly into pandas. I believe the library is pyodbc, but it's been a while since I used it for anything myself. Hopefully this helps give some direction in terms of where to start looking for new tools to use. There is definitely a learning curve going from excel, but don't worry about it! Expect to struggle and don't forget to give daily thanks to the Stack Overflow Saints who actually respond to the errors you run into and light the way to successful programs. Good luck!
You can use the PLSQL analytic functions to do correlations and regression. If you expect to be using Oracle a lot it could be a good investment in your personal knowledge development to learn how to do this. It is not especially difficult but intimidates many people. You will not have to worry about scale if you analyze in the database.
R is an excellent option. www.r-project.org RStudio is a more friendly user-interface for R, so I'd download that too. Just learn how to install packages, then try running some code using examples from help forums.
Sounds like a good use for Power Query. Connect to your dataset instead of trying to make Excel own the whole thing.
Should I be concerned about any security risk? This is sensitive HCM data and if I make any connection into the server it might post a risk. I tried connecting Oracle server to Tableau and IT was giving me a hard time with regards to security. In fact, I still need help with that but that’s another discussion on its own.
Awesome! Thanks for this!
Some ideas: 1) Aggregate and filter as much as you can in your SQL. 2) Split your data into smaller logically related chunks (like product groups or something) and do your analysis separately on each chung. 3) PowerPivot for Excel can help you manage much larger datasets in Excel if you really want to. 4) I don't know what analysis you have to run, but here's a pretty cool series of posts about doing regressions, Pearson's coeff., etc. directly in SQL: https://www.red-gate.com/simple-talk/blogs/statistics-sql-simple-linear-regressions/ 5) Sampling. You can get good statistics from proper samples. Oracle has a built in sample clause. I prefer sampling with ORA_HASH() because the samples are reproducible and can actually cover your entire dataset if you want (i.e., you split the data into 5 random chunks and run analysis on each chunk.) Example: https://www.deep-data-mining.com/2016/10/oracle-orahash-function-part-1-random.html 6) Learn Python. There are libraries perfectly suited for what you're trying to do. 7) Learn to get away from local compute/storage. Topics to explore: Hadoop, AWS's EMR and S3.
To be honest, connecting an external SaaS to your company database _is_ a big risk and should be done with care and the full knowledge of your IT team. The tools I mentioned above will all run on your own machine, and as a result pose less of a risk. If you're exporting Excel files and analysing them locally then the risk is not really any different than doing the same with CSVs and one of the above languages. Similarly, if you're connecting Excel to the DB via ODBC or whatever, the risk is similar - you're just switching Excel for a tool better-suited to the job.
Depends on how much data it is.
&gt; Should I be concerned about any security risk? /u/petepete has answered this well. I just want to add that in case you have CREATE/DELETE/UPDATE/DROP privileges in the database you might want to be a little careful about the code you write. You don't want to accidentally delete data from the live database. If you stick to SELECT you should be fine as it doesn't destroy any data.
Oh yeah the developer made that very clear. I have full admin access but I was advised to do most of my practice in my own folder in the developer part not in production. Nothing is live and working with archival data. I mainly use the built in features of Oracle to create a report using drag and drop feature, and then study the generated SQL code to understand what’s going on. I’m still learning about different tables and stuff. I was also shown a very simply report with a query NK longer than 25 lines. That was a bit easier to read since some of the data models are very long with a lot of “AND” in them.
Yeah, getting all the requirements with @pm1 and @pm2 from your OP is a little tricky. I'm guessing you use postgresql and I'm not familiar with that. But I'd guess you could use dynamic sql to: * Do a recursive WITH clause with one column with numbers 1 to (@pm2-@pm1) * Do a self join with the set in previous step @pm1 number of times. * Select all the rows that add up to @pm2. * Perhaps remove some duplicates using UNPIVOT. Something like that. I'm sure there are better ways of doing it but that's as much thinking my head does on a Saturday night. :)
You could also try using tableau if you don't have the time to devote to learn software development. It will allow you to do alot more on the data visualization side of things but not so much on manipulation. It can connect directly to your oracle databases and acts like a SQL GUI that can create dynamic powerpoints
It \*is\* tricky to read large queries. I have 20 years experience of SQL and I too have to sometimes take a deep breath and brace myself going in when tackling large queries that join lots of tables, each of which has lots of where, group by etc. My advice is to get familiar with the WITH clause. With that you can create tricky queries and give them friendly names that are easier to work with. You can stack several such "aliased" queries after each other. Most examples online doesn't show that but it's very useful especially when joining several tables. Oh, and in case you haven't seen it. Oracle actually have a few statistical functions built in: &gt; Included in every Oracle Database is a collection of basic statistical functions accessible via SQL. These include descriptive statistics, hypothesis testing, correlations analysis, test for distribution fits, cross tabs with Chi-square statistics, and analysis of variance (ANOVA). Source: [https://www.oracle.com/technetwork/middleware/index-092760.html](https://www.oracle.com/technetwork/middleware/index-092760.html)
Or try Power bi desktop. It can easily handle millions of records. Plus it will feel like a natural upgrade coming from excel.
Power bi uses power query.. It's the same thing for both excel and power bi data pulls..
You could load the data into Google BigQuery, then use Google Sheets to manipulate the data. This approach gives you the ability to do complex transformations on your data in a fancy excel-like environment.
A lot of people are giving you some good answers, but if you are a newer analyst you may want to look into some more user friendly statistical programs, instead of just using queries and programming. R is great and free but a lot of companies have concerns about using it for sensitive data. Depending on budget the big 3 programs are SAS, STATA, or SPSS. They have the ability to connect to databases and use queries and then analyze the data. However, I would suggest you learn about the data and make an analysis plan before you start running regressions, running a lot of spurious regressions can lead to type II error.
What’s your database? I consult for a retail analytics company and produce these types of reports all the time. Our DBMS is SQL Server 2016 and we do adhoc querying and PowerBI. If this is adhoc and SQL Server, put your base set in a cte or temp table. Then build your query leveraging ctes to produce your output. That’s probably the most simplest and quickest way. There are other options that will sustain longterm customer metric insights.
Alteryx is kind of a neat visual tool for connecting to data sources, data cleaning, and even doing statistical analysis. It's good at making connections to live databases as well as data file extracts. If you don't have the budget for a tool like that, I'd probably use R/RStudio to do the statistical work, while writing good SQL statements to get just the data you need out of the database. Or if you already have the data in a file, just R/RStudio. Or maybe load the data into something like SQLite so you can run basic SQL on it to get smaller sets of data into R.
Yep, you were totally right.
This is some good stuff. My company uses Oracle cloud, is this the same? And since I am still all new to this, I’m still trying to understand the system and “subject areas” which include a bunch of data or Tables as you call them with many different data fields within those. Still trying to figure out which subject areas are applicable to my case. The developer finally showed me a list of the tables included in the subject area we use so I can better understand the query he developed and can try to get a sense of where the data is being pulled from in the server.
Right yes I am familiar with SPSS. And wouldn’t it be a Type I error? I thought that the larger your n the more power you have, and therefore almost anything would be significant at that point. I mainly focus on the effect sizes, F test, and the actual model and coefficient values rather than solely rely on the p value. And I also 100% agree with you that any statistical analysis should be guided by a research question (plan) and (if possible) supported by theory that further builds the validity of results. Good advice and reminders for any analyst. Also, the problem with those programs is as you mentioned budget. Those programs aren’t cheap and since there are alternative options a company would certainly lean more towards that direction. I believe this is why we see an explosion of those programs since anyone can get their hands on them.
Just forget about Excel at this point. Check out one of many open source or just free BI front-end software like: * Saiku + Mondrian (takes some time and knowledge to configure, and it's quite dated) * Metabase (can be quickly connected to any database, but its features are quite limited) * Knowage * Qlik Sense Cloud Basic * Google Data Studio * Apache Superset * PowerBI (up to 1GB I think) Most of these handle quite a lot of data, by exporting the actual load of the operation to the database it connects to. But if you want to, you can learn Python, and design your own Jupyter Notebook data navigation and visualization. It's mostly for exploring data statistics, not for exploring actual data. It takes time to prepare a visualization, and it's just not on the same level as some other software mentioned here. It's more suitable for scientific applications, less for business, unless you buy some proprietary data visualization libraries.
Look at community colleges you can attend online! SQL or database related.
Case statement? Case when product = a and product = b then c else product end
Any recommendations for certs?
Case statement evaluates one line at a time though. I tried making a temp table where the shipping table was joined twice and I did something like: CASE WHEN (a.quant = 100 and b.quant = 200) or (a.quant = 200 and b.quant = 100) THEN 1 ELSE 0 as Count This would still give me 0s for these shipments where a was 100 and b was 100, but i joined that table to my main query twice, once for 1s and once for 0s, and then made a case statement to say if there's a 1 for that shipment put in the 300 otherwise use the regular quantity, but I couldn't get it to work right. And even that felt like over engineering.
SPSS or Stata. Everybody in business uses SPSS. Academia uses STATA. STATA is more similar to SQL, SPSS is a robust version of Excel essentially. Just buy a copy of SPSS and go to town. I've run 2M+ data points on an i5 processor with it.
If it's purely unit size, why not use a sum? select CustomerId, Sum(Quant) as Quant from *whatever join* group by CustomerId
Because there's about eight different unit sizes, but it's only this specific combination that I need to combine.
That's an important detail. :) If you can provide a bit more detail on what you're dealing with (sample, scrubbed data would help) we can you better assistance.
please don't take this the wrong way but I would advise reaching out to your IT before you either put your firm at risk by pulling out tons of data pointlessly or waste a significant amount of time messing while you clearly do not seem equipped to do whatever you are trying to do. Perhaps there are resources internally that could help you. Other answers here are mostly all viable in the right context and in the right hands but I am not sure of either in your situation. Again, this is not meant as a diss but rather as an advice so you can get actual help in your specific context. If you want a more contextualized answer it would be beneficial to know what size were talking about here (number of rows and columns), type of data, type of analysis, etc.
What if you set up a temp table or permanent table like #productgroup(groupname, product) and insert ('Product AB', 'Product A'), ('Product AB', 'Product B'). Then you left join #productgroup as pg on pg. Product=mytable.Product, group by Coalesce(pg.Groupname,mytable.Product). In the select you have the coalesce and sum(quantity). This allows you flexibility to group other combinations later. This is rough, you will likely want to use the productid and have the date in your group by, etc... But this should work
You could do a left join, where a and b are added, and then union join that to a right join where a is null.
Or, just do a union join and group by customer ID.
Yes, I see what you mean that any solution to this problem feels like it will be a little "over-engineered". But well, you need to selectively transform rows depending on other rows in the rowset, so that's gonna require another "stack layer" whether you like it or not, right? How I think I would do it - have a CTE where you do a sum grouping by customer, date, and CASE WHEN A or B then AB else NULL end (and if you ever get more of these double products, you can just add them as arguments to this case statement). Then do a union all where one half is just the non-null product rows from the CTE, and the other half is rows from your normal query, but with a WHERE clause something like WHERE ( ( PRODUCT &lt;&gt; 'A' AND PRODUCT &lt;&gt; 'B' ) OR NOT EXISTS ( SELECT 1 FROM CTE CTE_INNER WHERE CTE_INNER.CUSTOMER = SHIPMENT.CUSTOMER AND CTE_INNER.DATE = SHIPMENT.DATE AND CTE_INNER.Product = 'AB' AND CTE_INNER.Sum &lt;= 300 ) )
What’s a book? Look at plural sight
Cool- but I specifically am wondering about books. I know there are lots of online courses and tutorials.
I liked Murach’s SQL Server for Developers series.
I don’t know any specifically off the top of my head. My suggestion would be to look up prolific names in the SQL industry and see if any have written books
[https://use-the-index-luke.com](https://use-the-index-luke.com)
T-SQL Fundamentals or T-SQL Querying. Both by Itzik Ben-Gan. Depends if you need starting info or you are advanced.
If you explain your needs to the developer he/she can probably easily whip up a query for you that combines the different tables that are relevant. You can mentally treat the result of such a query as a table. On that (query result) table you can then play around with the different built in statistical functions. If you need to do the query yourself you need to be familiar with joins and make sure you join the tables on the primary/foreign key. Since you're admin on the db you can find out the promary/foreign keys yourself by either looking in e.g Oracle's SQL Developer (I assume you use something like that) or script a CREATE statement of the tables you're interested in. The CREATE statements will contain the primary/foreign keys.
So I'm guessing your table is at line level with a shipment id. Create a shipment level temp table and use case statement to mark shipments with that combination. Left Join back to line table on shipment id. Use a case statement to change one of the two products to the combined one where the affected shipment flag is true. Then filter out the other product using the affected shipment flag again.
Go to cloud.oracle.com sign up for a free trial and try the autonomous database. U can load up your data in a data warehouse and use desktop visualizer, a free Oracle app to analyze the data much like you would with tableau. Headsup, you’ll need sql developer to load your data into the warehouse.
+1 To this suggestion. His books are fantastic.
Select * from newmemes Where site=‘reddit’
Create a temp table with unique customer list. Select distinct cust_id From order_table Where product =B Then to your main query, left join this new temp table. Select *, case when Main.product = A and Temp.cust_id is not null Then C else main.product end as Product From main Left join Temp On main.product = temp.Product
I actually have a Meme Downloader I made with python. I'm only a first year Computer Science student and its written in python but it gives you an idea. [https://github.com/easeypeaseyweasey/MemeDownloader/](https://github.com/easeypeaseyweasey/MemeDownloader/blob/master/reddit2.py)
Any book from Itzic Ben Gan! Seriously that man lives to query and clearly explain them. The 70-461 book he did was perfect IMO but the red TSQL Querying is also good.
The title of this post and the text don't ask the same question IMO. The title of the post says that you want to concatenate the strings together, and the text says that you should add numbers together. Let's start with the easy one, which is adding the numbers together. You would simply SUM(CONVERT(FLOAT, UnitSize)) GROUP BY Customer, DateColumn If there was only one item shipped, the sum still works. &amp;#x200B; For the more difficult question, if you wish to concatenate, you can simply use the STRING\_AGG function if you're lucky enough to be on SQL 2017 or greater: [https://docs.microsoft.com/en-us/sql/t-sql/functions/string-agg-transact-sql?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/t-sql/functions/string-agg-transact-sql?view=sql-server-2017) &amp;#x200B; The more old-school way would be to use FOR XML PATH [https://sqlperformance.com/2016/12/sql-performance/sql-server-v-next-string\_agg-performance](https://sqlperformance.com/2016/12/sql-performance/sql-server-v-next-string_agg-performance)
Thanks for replying but it's neither, one product is called "Standard product 100ct" and the other is "standard product 300ct". Neither sum nor concatenation will work, but thankfully there are several responses who understood my garbled question.
I am going to flip the script on your comment. If I was a supervisor or manager, I would assign a special project to my subordinate that I know he or she doesn’t have the skills to do. Of course that employee is not going to be equipped to do the job at that moment in time, but if you provide them with the right environment for them to learn and apply those skills they will be equipped to do the job. This is how you develop strong retention among your employees, challenging them to grow and pushing their comfort zones. Believe in people that have little experience in doing something challenging, teach them or show them how to do it and they’ll do it. I know that I don’t have the current skills to do exactly what others have mentioned here and I also understand the IT risks that come along with it. But I also know to work in collaboration with the IT team to understand the risks, with the developer to make sure that we are doing something worthwhile, with my supervisor to make sure it’s aligned with the business goal, and searching through google for additional insight while learning R or python online and eventually applying it. The only way people learn is by experience but if you don’t allow them to have that experience then they’ll never learn.
This is a great idea, for some reason I was forgetting about the shipment ID. Thank you.
CodeAcademy has some good courses for learning from scratch. Your employer may be willing to help subsidize the cost, mine did. Best of luck!
Udemy $10 using promo codes
https://sqlbolt.com/
Thank you!
Another good way is to use Microsoft Access and develop queries in design mode and then convert to SQL. The language is not 100% the same but it helps understand the concept
Usually by SELECT * FROM Prod Followed by a call from the dba and/or CIO. More seriously, it's like credit, op, you need it to get it, I.e. data appetite fed by reporting which is driven by reports created according to data appetite... SQL still more or less backbone of data in the industry so resources plentiful but it is less a sandbox like a programming language and more a sand distributor... Start work in basic connections to existing programs fed by data &amp; language syntax and soon you'll have more resources than you'll know what to do with.
[www.postgresqltutorial.com](https://www.postgresqltutorial.com)
Its easy nowadays you have the whole of the worlds knowledge in your pocket. I remember having a dusty 800 page Access book on my desk *oldmanshakeshandatcloud.gif* Some good sites mentioned in other coments. Its a case of putting in the hours my dude. Good luck! Oh something I wish someone told me when I started. Anytime you alter ANY data (update, delete etc) start your query with BEGIN TRANSACTION and end with --COMMIT DOUBLE CHECK YOUR CHANGES. Then uncomment and run the commit comand Please, its saved my job probably 3-4 times.
U wot m8?
Also, XACT_ABORT ON
https://sqlzoo.net/ https://lagunita.stanford.edu/courses/DB/2014/SelfPaced/about
For the absolute beginning I would recommend W3Schools (https://www.w3schools.com/sql/), it's free and It got me threw computer science middle school in Slovenia.
I'm in a similar position, namely, my job has to do with PIM systems, which are sort of an interface on top of an oracle sql database. The interface is shitty and has limitations, so I find myself having to / wanting to access it directly over sql developer. My employer does not discourage this, so I end up using it more and more. Besides using every chance I have at work to use it, trying to solve real world problems (but with read only mode, I only commit over the interface) with the help of Google, stackoverflow.com, w3 etc., I've installed a LAMP stack at home (free) where I can really learn, try out things, follow tutorials (udemy, YouTube [programming with Mosh]). I find it very interesting and useful.
If you have a parameter \[date\] your chosen SQL dialect should have some mechanism of determining what day of the week \[date\] is. (e.g. DATEPART() in T-SQL,, WEEKDAY() in Access or MySQL, DATE\_PART() in Postgres, etc.) If you know that, you can arithmetically deduce how many days you'd need to subtract or add from that date to get the Monday of that calendar week. If you don't want to use integer arithmetic you can then just create a new date object via the method appropriate for the RDBMS you are using.
This is awesome!! Thank you! Q: Do you know of any websites as such to learn scripts, interfaces and servers?
That's all very context dependant. Though I haven't found any of that youtube didn't explain.
&gt;Anytime you alter ANY data (update, delete etc) start your query with &gt;BEGIN TRANSACTION &gt;and end with &gt;--COMMIT &gt;DOUBLE CHECK YOUR CHANGES. Then uncomment and run the commit comand I dont have access to update anything on sql, but in case in the future i do have the access, may i ask why?
I'm very well aware of that and I do it with the employees I supervise but there must be some form of accountability / guidelines.
fook ur reporting server I only query live data fight me bro
SELECT * FROM DUAL
Pass in a table-valued parameter of your strings to search for, instead of a CSV string. Or, split your CSV string into a temp table, then compare against that. If you're using SQL Server 2016 or later, there's a [string_split() function](https://docs.microsoft.com/en-us/sql/t-sql/functions/string-split-transact-sql?view=sql-server-2017) that can make your life easy.
Udemy has a bad reputation for putting their head in the sand with stolen content and not being responsive to copyright owners when they complain. There are plenty of other options that are more responsible and ethical. https://www.troyhunt.com/the-piracy-paradox-at-udemy/
Because accidentally missing an important where clause in an UPDATE statement means you just updated every single row. Next thing you know, there's no back-up of said table and all of your data is ruined.
I did something similar to this. Thanks!
I don't know what platform you're on, but it's usually not bad practice to do integer math on a date. Most dbms handle that gracefully and it makes things a lot more readable/typeable. Now, if you're doing something like "myDate - 365" to get previous year date, you're going to have a bad time due to leap years, so maybe that's what they're talking about. Suggestions for solving your problem: 1) Read through the documentation on your platform's date operators. The whole thing. Just do it. It'll be worth it over and over. 2) Most platforms will have a function that takes a date as argument and spits out the first day of the week that date falls in. Your database will have some environment settings that will make the first day of the week fall on either monday or sunday. If monday, youre done. If sunday, just do: funtionThatGivesMeFirstDayOfWeek(myDate) + 1. 3) For previous year, it'll depend on what you're really after. For example are you trying to match 2019-06-09 up to 2018-06-09? Are you trying to find the The monday of the same "week number"? The former can be done by extracting the year from the date, subtracting 1 and making a new date out of that. The latter is probably a pain in the ass, and is a good reason for having a calendar table in your database with the dates, what week numbers, month number, etc they fall in.
A foreign key of one table points to a primary key of another table. In your example the Stores table is directly linked to Geography, i.e. Stores.GeographyKey is the foreign key that references the primary key Geography.GeographyKey. Sales is only indirectly linked to Geography. Sales is linked to Stores which is in turn linked to Geography. In the screenshot of the diagram we can't see all the columns in Sales, but presumably it has a StoreKey column. This Sales.StoreKey column would reference Stores.StoreKey.
I used Strata Scratch. It's an awesome website for learning SQL. Just being competent will be enough to qualify for SQL roles. A lot of the skills you can only really learn by doing professionally.
Best advice I can give is to use one of the many sites linked by others on this thread AND try to find a practical application of what you’ve learn—even if it’s composing random CSVs or dashboard for yourself! This may sound really obvious, but for me, if it wasn’t for actually needing to pull specific cuts of data via SQL in a work setting, I probably wouldn’t be fluent in the language
Great tip, thank you.
In version 1.1.3, I Added quotation selector to sql output, you can choose the correct quotes according to the sql engine.
The following site has many useful articles on relationships. https://radacad.com/what-is-the-direction-of-relationship-in-power-bi
It looks to me like you have tried to alias the join? the first "pc" needs to go after "product" not INNER JOIN. And I think your trying to join and a value? You can "filter" a join but you have to make the join first.
Syntactically OP seems fine. Not sure what you’re getting at here.
Not sure what you mean by it passes the first database. Just from a glance, your code should work.
The website checks your code on a database which you can explicitly query and play around with, and if it passes on that it checks it on a second, hidden database -- to make sure you didn't get lucky or something. It tells me I fail on the second db, but because I can't look at it it's difficult to see why.
Hmm. It shouldn’t matter, but maybe add a where product.type = ‘pc’/‘laptop’ to their respective queries?
Sorry for a late response. Actually it is a job application "task". I've tried doing it by creating dynamically variables in dynamic SQL (exec(query)) but it's not optimal.
Thanks for the suggestion. That gets the same result: "Your query returned the correct dataset on the first (available) database, but it returned incorrect dataset on the second checking database. * Wrong number of records (less by 2)"
Should it not be FROM PRODUCT pc INNER JOIN Could be different in t-sql
OP must’ve fixed it because that’s not what I see. Thanks!
My bad, I misread as indexing on importance in table2
It's nothing to do with your syntax. Given the error said you had two few two rows, my best guess is that there are model codes shared by two different types of product. Either that, or there simply isn't a record in the laptop table for each laptop in the products table. If the former is the case, I would expect this to work: SELECT DISTINCT maker FROM product INNER JOIN pc ON pc.model = product.model AND product. Type ='pc' EXCEPT SELECT DISTINCT maker FROM product INNER JOIN laptop ON laptop.model = product.model AND product. Type ='laptop'
Thanks for that, but it also returns the same error. As long as I'm not misunderstanding something fundamental, I'm happy to move on, but it is bugging me slightly!
Try finding products which have a value in maker column but no existing match in either pc or laptop table. That should be the difference. Also you don't need DISTINCT. EXCEPT operator is distinct by nature.
If I'm understanding the question correctly then with Table Trigger that fills system user name on insert or update.
I think? So we have a start time button that fills the table data all except the end time which goes in as Null. We need an end time button that'll replace that null with the users end time. We have about 15 users so it will need to be specifically to that user and only the field that is null, no other field with that users name.
 SELECT Table1.*, Table2.* FROM Table1 FULL OUTER JOIN Table2 ON Table1.ID = Table2.ID
When you INNER JOIN you only get results that are in both tables. So maybe there are models categorized as ‘pc’ that are in the Model table but not in the PC table. So when you join you miss some results. Maybe try: SELECT DISTINCT maker FROM product LEFT JOIN pc ON pc.model = product.model AND product.Type ='pc' EXCEPT SELECT DISTINCT maker FROM product INNER JOIN laptop ON laptop.model = product.model AND product.Type ='laptop' ;
The vast majority of employers are going to view a certification with no relevant experience as meaningless.
I completely agree with you, but for the junior position or at least to start an internship I guess it is a good start. ( In case if you don’t have any work experience...)
✅ How do you Restore SQL Server Databases and take Native SQL Server Database Backups in AWS RDS for SQL Server? This Vdeo covers a step-by-step process that you can implement in your AWS RDS for SQL Server Environment.
Pc is the name of a table that he is joining to Product, not an alias.
Oh yea. Oops. That makese sense.
The result of SELECT Table1.ID as ID1, Table2.ID as ID2 from Table1, Table2 is every possible combination of rows from Table1 and Table2. Are you *sure* you're not wanting match something up between the two tables at least when it's possible?
Ignition as I'm the energy supplier software?
Basically I have a bunch of values on different servers. I was told to write a SQL report to see if there were duplicates (all the values should be unique), but in all honestly the easiest thing to do is a quick compare in Excel. When I did use the above query to combine the tables, I ended up with 100s of thousands of rows. I am ready to give up and just go with excel.
Was just about to suggest this :) You are totally right, if you do: select Table1.id from Table1 join Table2 on Table1.id = Table2.id You will get back a list of id that appear in both tables (notice that it doesn't matter whether you're use Table1.id or Table2.id in the select because you're forcing them to be equal with the join criteria.) Nothing wrong with being a newbie :)
Another tip: when you're asking for help with SQL, or really anything technical for that matter, try to include what you're *really* trying to do. To use an analogy: A guy goes on a carpentry forum and asks "How do I make glue from dog hair?" He gets lots of complicated answers about breed selection, safely shaving a dog, boiling temperatures, additional ingredients, etc. Finally someone asks "Why dog hair glue?" and the guy responds "I need to keep two 2x4s attached to eachother and I'm out of glue." It turns out that the real answer to help this guy is "Use two nails and a hammer." :)
I realized a join will not work. I have 5 tables, and I need to see if any of the tables have duplicates, so I need to compare Table1 with Table2, Table1 with Table3, Table3 with Table4, and so on..... :/ Using a join (the way I know how to do it) can only compare multiple tables with just one other tables.
Also look into Jeff Modan's inline table-valued function: DelimitedSplit8K. That'll parse your string quickly using tally tables (assuming 8000 characters or less). But as redneckrockuhtree said, passing in a table-valued parameter means the parsing has already been done (less work for SQL Server to do!).
If they all have read access to everything then potentially they could export/steal all of your company's confidential information and depending on your auditing setup you might not have any idea that it happened.
That makes, I'll try this with dynamic SQL, it doesn't seem very optimal but I'm out of ideas...
It sounds like you are setting up the foreign key relationship backwards. So you would say Foreign Key PersonPhone.BusinessEntityID References Person.BusinessEntityID rather than trying to set up PersonPhone.BusinessEntityID as a primary key.
Here's an idea: select ID ,max(in_table_1) as in_table_1 ,max(in_table_2) as in_table_2 ,max(in_table_3) as in_table_3 ,max(in_table_4) as in_table_4 ,max(in_table_5) as in_table_5 from ( select ID ,1 as in_table_1 ,0 as in_table_2 ,0 as in_table_3 ,0 as in_table_4 ,0 as in_table_5 from table1 UNION ALL select ID ,0 as in_table_1 ,1 as in_table_2 ,0 as in_table_3 ,0 as in_table_4 ,0 as in_table_5 from table2 UNION ALL /*similar query for table 3*/ UNION ALL /*similar query for table 4*/ UNION ALL /*similar query for table 5*/ ) group by ID having (max(in_table_1)+max(in_table_2)+max(in_table_3)+max(in_table_4)+max(in_table_5)) &gt;1 This will give you a list of all ids that show up in more than one table as well as which tables they show up in.
Thank you I will give it a try! I am really going to have to digest this!
Install SQL Server and connect to your localhost database.
&gt; PersonPhone has both fields as well as its own unique ID field it should not have its own ID CREATE TABLE PersonPhone ( BusinessEntityID INTEGER NOT NULL , PhoneNumberID INTEGER NOT NULL , PRIMARY KEY ( BusinessEntityID, PhoneNumberID ) , CONSTRAINT Phone_Person FOREIGN KEY ( BusinessEntityID ) REFERENCES Person ( BusinessEntityID ) , CONSTRAINT Person_Phone FOREIGN KEY ( PhoneNumberID ) REFERENCES Phone ( PhoneNumberID )
Postgresql supports a few different procedural languages. Perhaps that is easier than shoehorning something in with SQL. From the [documentation](https://www.postgresql.org/docs/11/index.html): [PL/pgSQL - SQL Procedural Language](https://www.postgresql.org/docs/11/plpgsql.html) [PL/Tcl - Tcl Procedural Language](https://www.postgresql.org/docs/11/pltcl.html) [PL/Perl - Perl Procedural Language](https://www.postgresql.org/docs/11/plperl.html) [PL/Python - Python Procedural Language](https://www.postgresql.org/docs/11/plpython.html)
https://www.microsoft.com/en-us/sql-server/sql-server-editions-express
So by using 3 tables you are creating a many to many relationship, which may not be what you want, unless one Phone will be potentially attached to multiple people AND one person be potentially attached to multiple phones. This also allows you to have Phones not attached to anyone. You probably want a one to many relationship, which can be done with only 2 tables. In this case, just have a column in the phone table reference the person table. This way multiple rows in the phone table can reference a single row in the person table.
I will add this to my list, thank you. I have been tasked with making a list of potential hazards of this setup.
lol typical. Do it and then let’s talk about the risk.
SQL Server DEV edition is free to use for non-production purposes. EXPRESS is free and can be used in production, but has some resource limitations.
Not just locking tables, but taking up precious I/O and processors, too.
In that case you can throw in the possibility of a third party compromising anyone's account and doing the same. While there's always a chance that even if you limit access the accounts that can see it might get compromised, with this setup it doesn't matter who gets compromised.
Is this not the same as just using SQL Server Management Studio? Sorry for my newb question haha
SSMS is a client - you need a server too.
Largely data theft, also bigger attack surface but I don't know how much of a concern that realistically should be. Ideally, you could set up a secondary server with some sort of replication and have them connect to that instead.
There are instances where a single person will have more than one phone number (Work, Mobile, Home, ...) and instances where one phone number is assigned to multiple people (Main business line assigned to multiple people).
But PersonPhone needs a unique identifier, correct? BusinessEntityID won't be as a person will have more than one phone associated with them. Same with PhoneNumberID.
`PersonPhone` ~does~ have a unique key -- PRIMARY KEY ( BusinessEntityID, PhoneNumberID )
 [https://docs.python.org/3/library/sqlite3.html](https://docs.python.org/3/library/sqlite3.html)
Sql-ex is fun
Install docker, create a container fron a mssql image, it'll give you a self contained environment without the complexity
`WHERE FLAG = 'Y' OR (Variable = '')` Or too simple?
Does this accomplish what I want though? So if flag = '' then will this return all records with Y value and all with N value? &amp;#x200B; EDIT: this did NOT work
Will the latest entry be based on an auto incrementing identity? If so just use MAX to get the max Id for that user. Or will the latest entry be the one that isn’t ended? So just use endtime Is null
How many fields are you looking at here? Just one column, you want all the Y's and all the blanks?
say we have four fields A|B|C|D &amp;#x200B; flag\_value is for D column D can be either 'Y' or 'N' and has many records of each. The other columns are irrelevant for filtering. A sproc I have will set flag\_value to = '' and if it's blank ('') then I want all records where D is 'Y' or 'N' else if 'Y' I just want 'Y' records for D, vice versa for 'N' for no
You need to share a sample of your data because I'm not following you. You have columns A|B|C|D, where D is flag_value, and it can be Y or N, or blank, and you want to see all the records where it is Y or blank?
I don't know what kind of SQL this is but I think what you want is a case statement https://dev.mysql.com/doc/refman/5.7/en/case.html
 Where 1 = Case when x=y then 1 when x=z then 1 else 0 end
You can do this with LAG() function.
what do you mean by “the variable is blank”? You mean another column has a blank value? or is null?
&amp;#x200B; if (@flag_value = '') begin select * from table_name where flag_value in ('Y', 'N') end else begin select * from table_name where flag_value = @flag_value end
Try this: Where flag = variable or variable =''
I’m not sure i fully understand the question or data, but maybe something like: WHERE (flag_value = ‘’ and column in (‘Y’,’N’) or (flag_value = ‘Y’ and column = ‘Y’)
Yes, as distraughtthoughts said you can use LAG() to get the previous row's ID, then compare the current row's ID with that one, probably subtracting the two. Filter by this difference being &gt;1 and you'll see where the gaps are. I also want to note that gaps are expected behavior with sequences, and you should never rely on gap-free sequence-based IDs for any reason. (Maybe you already know this, but I find many people don't and they rely on this bad assumption in their business logic, in their application coding, in their database administration, etc.)
&gt; you should never rely on gap-free sequence-based IDs for any reason this ☝
^^^
Ok, cool, you hadn't mentioned that in the OP. Sounds like you are on the right path then :)
Imho pgadmin 3 os much better than 4. If you want to use console, use psql. Example - psql -U user -h host
Docker makes development so nice, absolutely recommend! Here's the [base Linux image](https://hub.docker.com/_/microsoft-mssql-server). Definitely read through the linked [github](https://github.com/twright-msft/mssql-node-docker-demo-app) page if you want to seed your container with data. Do yourself a favor and grab the Linux image for sure. The Windows one is massive.
I am strictly learning how to query - I understand that sequences are always expected to have gaps and there's no reason to worry about it being purely sequential as long as it's unique.
Especially common with people using sqlite who confuse rowid with row number, sigh.
I use MS sql server express to test. I know that there are limitations, but for populating tables and reading data for testing I haven’t had any issues. It’s free. I should probably learn to setup dev environments with dicker but I haven’t spent the time yet.
I would recommend SQLite. It's small, simple, local, open source, API compatible with the ACID-compliant Postgress, supports advanced encryption if needed, and is used in thousands of products from web browsers to games to mobile phones.
Sqlite, Mysql, Postresql are common free database systems you can use. You can often find pre-made databases (e.g. Northwind, Adventureworks, etc.) ready to load in any of those systems. (such as: https://github.com/jpwhite3/northwind-SQLite3) Sqlite is probably the simplest, but also least capable - probably probably plenty to get you started.
Not a direct answer but I've had decent experience with https://dbeaver.io/ which supports pretty much every db out there.
Which database product are you using? Postgres? Oracle?
How about WHERE CASE @flag WHEN '' THEN 1 = 1 ELSE column = @flag END
This completely depends on your DBMS. In Postgres for example, `to_date('2019-24', 'iyyy-iw')` will return the Monday of week 24 in 2019 (based on [ISO week numbering](https://en.wikipedia.org/wiki/Week#Week_numbering)). And `date_trunc('week', to_date('2019-24', 'iyyy-iw') - interval '1 year')` will return the Monday of last year's week. If you want the Monday of "today's week", it's as simple as `date_trunc('week', current_date)`.
I can't do it with two selects. Has to be in the where only.
Basically this below: [https://www.reddit.com/r/SQL/comments/bz47si/dynamic\_where\_clause\_question/eqq263k?utm\_source=share&amp;utm\_medium=web2x](https://www.reddit.com/r/SQL/comments/bz47si/dynamic_where_clause_question/eqq263k?utm_source=share&amp;utm_medium=web2x) But I need it to be one statement and make the WHERE conditional
Your question is hard to answer because I don't know how you are passing in `flag_value`. Assuming you can just drop it in the middle of a query via `@flag_value`, this should work: select * from my_table where @flag_value = '' or flag_column = @flag_value If `flag_value` is blank (''), then that first condition will be `true`. If it isn't blank, then we will compare the column in the table `flag_column` containing the 'Y' and 'N' values with `flag_value`.
Sql server express. Or just use anything with pyodbc, and you work with similar function calls.
flagvalue can only be '', 'y' or 'n' if it's y or n I just do WHERE column_d ='y' or column_d='n' if blank or '' I want to return all rows with y or n
Check my OP edit. I clarified.
Check my OP edit. I clarified.
Check my OP edit. I clarified.
I clarified in my OP. I mean passed in as a parameter. So sorry. Should have clarified to begin with :(
When 'y' then 'y' When 'n' then 'n' Else '%' end
Did you try my solution?
What you're saying doesn't make sense. It can only be Y or N, but if it's blank you want it to do something?
Try this: Where 1 = Case When @flag_value = ‘Y’ and columnD = ‘Y’ Then 1 When @flag_value = ‘N’ and columnD = ‘N’ Then 1 When @flag_value = ‘’ Then 1 End
Thank you!
Thank you!
Thank you!
Thank you!
Thank you!
Thank you!
parameter is blank and when it's blank it means return all fields regardless if Y or N
Damn, we just shifted away from netezza.
Sorry forgot to say... Sql server
Can you try this? &amp;#x200B; `declare @table_ as table (` `a int identity(1,1),` `d varchar(3)` `)` &amp;#x200B; `declare @flag_value varchar(3)` &amp;#x200B; `set @flag_value=''` &amp;#x200B; `insert into @table_ (d) values ('Y'),('Y'),('N'),('N'),('Y')` &amp;#x200B; `select * from @table_ where (@flag_value='Y' and d=@flag_value) or (@flag_value='N' and d=@flag_value) or @flag_value=''`
I'm basically at a point where I have all the numbers that add up to sth, I have to do sth like distinct between f.e 1 2 17 1 17 2 Do you have any idea how to do it?
Personally I am not a big fan of flags. In your scenario what if a certain location name is both a neighborhood and a city? Will you then have two rows, one with a 0 and another with a 1? A neighborhood and a city should be more like a parent/child relationship, but a flag could be confusing between the two?
does a neighbourhood always belong to a city? if so you may want a simply hierarchy in a single table, with a foreign key another option: separate tables for neighbourhoods and cities
Neighbourhoods always belong to cities, however they are treated as separate entities in our system. For example, both cities and neighbourhoods have population densities, and have their own vibes, and their own culture. So what I'm thinking of doing is having a Locations table which can be used for both cities and neighbourhood data, a cities table which defines which locations are cities, and a neighbourhoods table which defines which cities own which neighbourhoods. But maybe I'm making trouble for myself...
Docker will do it for you, docker run --name mysqlcontainer -e MYSQL_ROOT_HOST=% -e MYSQL_ROOT_PASSWORD=Testing123 -d -p 3306:3306 mysql:latest Connect on localhost:3306 username root password Testing123
Use psql. Stop giving this man alternative GUIs when he clearly defines his question. psql -h db1 -d myDB -U user1 Since we are giving useless advice... Subarus and Arch Linux are best!!!
Are you using this data in an application environment? Is it possible your definition of a location might grow to include other types of locations other than city or neighborhood? Generally, when I encounter something like this, we create an enum in the application, and slap the integer value on the table, Locations in this case. 0 = None, 1 = Neighborhood, 2 = City, 3 = Unincorporated As another user pointed out, what if your location is a neighborhood within a city? How do you draw that relationship? That might be many:many relationship between a Locations and LocationTypes, so you'd have 3 table in So, how you do this is really dependent on what you're trying to do with your data.
Any time!
Curious what platform you moved to. Aginity Pro currently supports Snowflake, Redshift, Hive/Spark, Postgres, and Greenplum.
&gt; I have all the numbers that add up to sth, I have to do sth like distinct between f.e I don't understand what "sth" and "f.e" are. What language is this? &gt;Do you have any idea how to do it? I'm sorry, but I don't know how to fix code that you aren't showing. My crystal ball is in the shop this week.
So it does :)
OK quick and dirty solution but I'm not sure whether this can be done easier: Do 2 CTEs like this With A as ( SELECT COUNT(*) AS countA, 'abc' AS foo FROM WHERE COL='X' ), B as ( SELECT COUNT(*) AS countB, 'abc' AS foo FROM WHERE COL='Y' ) SELECT countA/countB From A Join B on A. Foo=B.foo Written on smartphone could cause problems with formation
I tried this and it doesn't like the group by statement. Any idea why? It says expecting AS, ID, or Quoted_ID.
What database are you using? Can you post your current query or sent it in a PM?
&gt; not sure whether this can be done easier yes ;o) this requires no dodgy fake cross join -- SELECT count_X , 100.0 * count_X / all_o_dem AS percent_X , count_Y , 100.0 * count_Y / all_o_dem AS percent_Y FROM ( SELECT COUNT(CASE WHEN col='X' THEN 'humpty' ELSE NULL END) AS count_X , COUNT(CASE WHEN col='Y' THEN 'dumpty' ELSE NULL END) AS count_Y , COUNT(*) AS all_o_dem FROM daTable ) AS foo
you're probably going to have to illustrate this by showing sample data rows in all four tables, then show the sample rows in the new table you want to create you mention processes and searches, but with SQL, you don't tell it how to get your results, you just say what results you want and it figures out how to get them
Just brilliant I'd never come to that solution by myself yet but now I know better thanks for reviewing :)
Updated
Ok, apologies for stating the obvious then. :) This is one of the topics I like to evangelize because I see a lot of people getting it wrong.
No, it's always good to point out as many people read this and likely don't know.
thanks
If you need any further explanation I just got the OK from my boss to pull a SMALL sample from the actual tables.
&gt;SELECT count\_X , 100.0 \* count\_X / all\_o\_dem AS percent\_X , count\_Y , 100.0 \* count\_Y / all\_o\_dem AS percent\_Y FROM ( SELECT COUNT(CASE WHEN col='X' THEN 'humpty' ELSE NULL END) AS count\_X , COUNT(CASE WHEN col='Y' THEN 'dumpty' ELSE NULL END) AS count\_Y , COUNT(\*) AS all\_o\_dem FROM daTable ) AS foo Damn! What a hole in one. This works. Thanks a lot for the help!
What error messages does the other user get when they run it? Do they need SA on the server or DBO on the database? What happens if you create a SQL Login with SA and run it using those creds that aren't your own?
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/sqlserver] [Question about table creation from other tables](https://www.reddit.com/r/SQLServer/comments/bzeaqz/question_about_table_creation_from_other_tables/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
You could create a loop using dynamic SQL to do this.
This is one of those times where it’s ok to use a loop or cursor
I can try those steps and get back to you, but as far as the error, there is none. It simply fails to apply the specified roles. If SA privileges are indeed needed, however, I can imagine that would explain a lot. But would lack of SA status not just throw an error on top of failing to apply it? Mind you, I'm using an earlier version of SQL Studio and the user is using 2014.
Ooh, you're not going to be happy, but Ford is best. If you kill a Ford, you were probably killed in a Ford. But, yeah, that bugged me. "I know you asked for help with X, but let me tell you about Q."
The SSMS version shouldn't matter as long as it's able to connect and execute commands, the SQL Engine determines the order of operations internally. I tried this myself and it definitely throws errors if permissions are the issue (created a User using your TSQL, dropped db\_owner permissions, ran again as User trying to create User2: Creating User2... Msg 15247, Level 16, State 1, Line 9 User does not have permission to perform this action. Msg 15247, Level 16, State 1, Line 14 User does not have permission to perform this action. Msg 15410, Level 11, State 1, Procedure sp_addrolemember, Line 35 [Batch Start Line 3] User or role 'User2' does not exist in this database. Msg 15410, Level 11, State 1, Procedure sp_addrolemember, Line 35 [Batch Start Line 3] User or role 'User2' does not exist in this database. Msg 15410, Level 11, State 1, Procedure sp_addrolemember, Line 35 [Batch Start Line 3] User or role 'User2' does not exist in this database. User2 was created The only other thing I can think of (outside of something weird), does "User" already exist in sys.server\_principals? If so, it will just bypass your entire IF code block.
With a `UNION ALL` of two queries?
Upon closer inspection, it looks as though there is an issue removing the database entirely (I'm working with a supplementary removal query) and while it works on my end, there is an issue where it is not removing User from the other persons database and thus User still exists. So it appears to be bypassing the IF. I've placed an ELSE clause to attempt to account for this by checking if db_owner returns false and if so, it will apply the role changes. I will see if this helps and report back.
Makes sense. Removing a database doesn't remove the associated server login. In SQL Server there are server logins and object permissions and you need both to be able to access specific objects. The login is on the SQL Server instance level and will usually grant CONNECT permissions and bunch of other SQL-y stuff that's not relevant here. The SQL Server login will have a name and a unique SID (you can see it in sys.server\_principals). Making the SQL login gets you in the door, but doesn't automatically grant permission to any objects. There are a bunch of pre-defined roles for servers (sysadmin, public, diskadmin, etc.) that can make life easier, or you can make up your own to fit your org's needs. The user for a database object is essentially mapping the SID to have permissions on the database and is typically referred to as a user. The user can also be assigned pre-baked permissions which you are already using (db\_owner, db\_datareader, etc). This can be a big deal when you migrate a database from one server to another because if you just create a login with the same name the actual SID won't match on the database permissions and the login will be denied access.
Cool, I didn't know that would work. My code is now: &amp;#x200B; select sum(extprice) as 'Sales' ,count(*) as 'Row Count' from SalesTable where ProductName is null union all select sum(extprice) as 'Non-Null Sales' ,count(*) as 'Row Count' from SalesTable where ProductName is not null and it gave me: &amp;#x200B; |Sales|Row Count| |:-|:-| |100|20| |75|10| This is pretty close to what I'm looking for! How would I add labels to the rows so I could show which rows are "Non-Null" and which are "Null"?
Break it down into #tables.
 SELECT 'Null count', ... UNION ALL SELECT 'Non-null count', ...
Just hard-code them in your query, such as: select 'Not-Null' AS 'Label' , SUM() , SUM()
Wow, simple as that. I'm amazed. Thanks for your help!
&gt; This is not generally a best practice, but I've done it and will probably do it many times in the future. Ha, this made me laugh! Out of curiosity, what is a better practice?
I didn't have my coffee and was pretty grumpy but it does get frustrating when people circumvent your specific question. That being said - I love dBeaver as well I guess :P
Can you modify the sproc to run from a table of inputs?
Thanks, I'll give it a shot! Do you mean #tables to replace each event table since they're referenced twice or even for the Union and Aggregation steps? Is it correct to say there wouldn't be performance benefits the first time running the query since the temp table would need to be created but if I were to do further filtering on this data it should be faster?
You should experiment a bit, but generally speaking get rid of the CTE and use #tables. You can create them first and insert data into them vs. creating them with an into. You can also experiment with putting indexes on them. For example, you could also do a hybrid where you create it with an INTO for the top of your union, then INSERT INTO for the bottom half of your union before moving down to another step.
Did you read the Amzazon reviews for the book you bought? A lot of the official guides for SQL Server are either Great or Horrible. The exam ref for 70-765 ain't great. https://www.amazon.com/product-reviews/B077XKS1TZ/ref=acr_search_hist_2??ie=UTF8&amp;filterByStar=two_star&amp;showViewpoints=0
I did and one of the reviews said you'll pass, so I assumed if I put in the studying it'll work enough with the other resources
i thought for sure somebody would ask my about humpty and dumpty
Create a table containing a row number and the params for each run, then use a WHILE to loop the table and execute param set @i, where @i is your iteration and the corresponding row number in your parameter table.
You can probably do this without dynamic SQL, and IMHO you shouldn't use dynamic SQL unless absolutely necessary because it's horrid.
I'm still slightly confused as to how you are qualifying your flag\_value variable. With that said I'll give it another shot. select * from table_name where colD in ( case flag_value when '' then '''Y'', ''N''' else flag_value end )
How was 70-764 . I’m currently studying for that one .
Paid training unfortunately would probably be your best scenario if its new to you. I've got an exam in the same scenario but it has worse reviews. I'm going to bite the bullet and pay out of pocket because my work won't cover it.
Ur face is horrid, and please show me how to do this without a dynamic SQL loop.
Can I ask how much it costs to take these exams? I'll need to take them as well relatively soon
Comparatively it was much easier considering you can easily answer a lot of the questions if you understand SQL in general
$165 for just the test I paid $265 for test, practice test and two retakes (promo sale)
Unfortunately I don't have access to paid training so I'm going to study what I think I lacked and retake the exam in a few days while I still have everything fresh in my head
this is a new twist - you figured the pivot and now you are asking how to group data simply group by case ProductName is null then 'NULL' else 'Non-NULL' end
See my other comment ITT. ```sql CREATE DATABASE paramLoop; GO USE paramLoop; GO CREATE PROCEDURE dbo.ExampleProc @p1 INT AS PRINT @p1; CREATE TABLE dbo.Params ( ParamVal INT ); INSERT INTO dbo.Params VALUES (1) ,(2) ,(3); DECLARE @i INT = 1 DECLARE @max INT = (SELECT COUNT(*) FROM dbo.Params) WHILE @i &lt;= @max BEGIN EXEC dbo.ExampleProc @i SET @i += 1 END; ```
How are you going to inject the client based parameters into the where clause?
This is way easier than what I did. As you can tell, I am not a good SQL person.
 WHERE D = CASE WHEN COALESCE(@flag_value, '') = '' THEN D ELSE @flag_value END I think that's the clause you're really looking for. Essentially, WHEN your variable is either BLANK ('') or NULL (handled by the COALESCE) we're saying match D on itself, otherwise, if it's 'Y' or 'N' (since those are the only values allowed, as you said) it will only match on the one you pass in as @flag\_value. &amp;#x200B; If @flag\_value got passed in as 'E' or 'Z' or something else that would never match, then none of the rows would match either of the CASE conditions. If you're database has case sensitivity turned on, then you may want to wrap all of the references to @flag\_value and D, in the WHERE clause with LOWER() just so it treats 'N' the same as 'n', and 'Y' the same as 'y'. &amp;#x200B; If it's impossible for @flag\_value to be unset (or set to NULL as opposed to being SET to blank (''), then you don't need the COALESCE(), but, if you want it to elegantly handle NULL the same way it handles blank, then the COALESCE handles that for you. if it's possible to set @flag\_value to a space, then you need to decide if you want almost-blank (' ', with the space in between) as something-that-isn't-Y-or-N-but-also-isn't-blank, or if you want it to act like blank. IF you do want it to act more like it was blank, then you can LTRIM(RTRIM(@flag\_value)) to get rid of the left and right blank spaces, which will trim it down, and allow it to match the COALESCE().
Oops, I assumed "proc ID" was a parameter (´∀｀•)
Issue appears to be resolved. Turns out there was a stray check accounting for the existence of an Agent Job from another database that was not supposed to be there but had a very similar name to one that was supposed to be there. Minor oversight in the removal query that was precipitating into a larger issue. &amp;#x200B; Thank you for your help.
It can be tough. I usually throw in a few comments for different sections of the query, like for a subquery that serves a specific purpose. Just like with "regular" coding, if anything is unintuitive or "odd" it's a good idea to leave a comment, but commenting on stuff that IS obvious is just counterproductive - wastes your time and makes the code harder to read. Maybe you have a null check for column x in one subquery but not in another - it could be worth explaining why. Or if there's something funky in the data model that needs to be handled specially. There could be a performance tweak whose purpose is not immediately obvious: "/* this makes the query planner use the function-based index */". I guess the common thread is document the not non-obvious stuff. :)
I would start out with a database diagram. As someone who just started at a new company themselves. I was brought in with very little documentation on what their systems are, how to access them, any relationships and whatnot. They sent me several “basic” queries and said they use these to build off of. This doesn’t help me AT ALL. I started building a data model on how things flow, and will be digging into each table to figure out common relationships.
I'd set it up dynamically within a TRY, and then save the @SQL query that is built per client to be stored in a table for debugging. Your approach might work if there aren't any customization needed, but if its all the same query then I'm curious why a loop is necessary in the first place.
I'm not an SQL professional as anyone reading this post will attest to shortly, but... I'd make a cities table, a neighborhoods table, a venue table and an amenities table. Cities has the city names and interesting city info. Neighborhood has neighborhood names and exciting local info. The venue table is joined to the city and neighborhood tables and has the venue info and amenities. This let's you have 2 or more of the same named venues in different neighborhoods with independent amenities.
Will this work? https://www.itechtics.com/microsoft-sql-server-versions-direct-download-links-2008-2012-2014-2016/
Thank you for the link, but I tried all the downloads on this page and it seems they are all the "express" variant. They are in need of the Standard edition :( There is a good license key we can use for it, but just no installer.
this is called denormalization or chacheing
can you share the queries you have behind the view so that we could suggest any optimizations thanks
You’ve gotten stuck with simultaneously the worst and (possibly) the best assignment for a systems analyst. I recommend ERDs (Entity-Relationship Diagrams) which are basically just how the tables connect to each other via PKs and FKs. Good luck!
Sounds like a sweet deal. Where did you get it?
I'd talk through my logic as I'm working and writing it out.
Create an entity-relationship diagram. It would be so helpful in the long run! It could be also helpful to convert your queries into views. There might be software out there that can identify the relationship between your view queries to your tables. If you have Tableau Server, maybe consider creating Tableau data sources off of those views (name them to correspond your view names) and replace the data sources in those workbooks with those views instead of just using Custom SQL for each workbook. You can also navigate the data sources to see which workbooks access each one and vice versa.
all of your comments should reference action movies...... all. of. them.
I found it on the official Microsoft certification page to buy the test, check if the deal is still there. Definitely worth the money imo!
I’ve been using [LucidChart](lucidchart.com) lately to document both ERD and flow diagrams of stored procedures and ETL, and it’s been great.
Yep, they want to know your logic more than anything else. Anyone can Google syntax.
If you or the client have a user with a Visual Studio (formerly MSDN) license, they can download the installer. I just checked, and it shows in the list of available downloads, as do the service packs.
&gt;I feel that these coding interviews are not all about getting a perfect code but also approach. As someone who does interviews, this is exactly correct, at least for me. I know you are getting put on the spot. I don't care that your syntax might not be 100%, that's why we have syntax checkers. I want to know why you approach a problem the way that you do. Either start writing or talk your self through the problem outloud. Show that you have a plan for approaching the problem. Maybe it doesn't work out, but at least you are attempting to solve the problem. I've had a couple of interviews now where the person simply stares into space for a couple minutes and then guesses at the answer. Don't do that.
If someone did this while I was interviewing them, hired on the spot.
all of my error codes in prod are terminator 2 quotes... audit doesn't quite get it
When they ask you a question, about solving a problem, don't just straight up give them syntax and move to the next question. That could make it sound like you memorized something and are just repeating it. &amp;#x200B; Propose a couple of different ideas, and tell them that you would test each and determine which was most performant.
There is an open source project called [dbt](https://www.getdbt.org) that might be worth looking into. It supports some documentation formats and makes it very easy to serve that documentation in a web browser. It will let you write descriptions of tables and columns and visualize the relationships between the different tables in your project.
I gives SQL quizzes on interviews and I let the interviewee choose how to answer the questions: whiteboard, code on the computer or my preference (and usually theirs) just explain the idea. At the basic level I test simple joins and yikes, about 50% of them don't know what outer joins do. But definitely talk about what's going on in your mind, otherwise all I can assume is that nothing is going on. One guy was just like (for the outer join question) "oh I know this one but I can't remember what the kind of join is called!" - at least that was something.
Don't feel pressure to have the answer immediately. Feel free to think out loud, and talk your way though the problem. I want to hear you thinking in sets, logic, relations, etc. You can mention that your solution is a sketch and that in reality you'd be bouncing between google and test runs of the query to get the syntax straight. If you can propose multiple solutions and a valid reason for favoring one, that looks really good. Alternatively, look for optimizations on your solution (performance/readability, etc.) Don't mention any concepts or jargon that you're not ready to discuss in depth. I've had so many candidates drop "optimization" but then not be able to talk about real-life query optimization beyond vague mumbling about indexes. If a question or discussion covers something that you're not really familiar with, readily admit your ignorance rather than trying to fake your way through. If you have to do this, try to find a good "why", e.g., your previous work involved an alternate method/technology.
Unless it's something straight forward, I give them a business case/reasoning for my solution and how I got to it/why I got it. I make sure that they know I'm not just technical and think about things from a technical, business, and logical perspective.
Sharing a "framework" for how you work through a technical problem is helpful. For instance, a potential framework for solving a SQL problem could look something like: 1. **Understanding Context** – is there any terminology in the problem you'd like to verify? What's the purpose of solving this problem (i.e. is this a real problem the business is trying to solve, or is it just to literally test your SQL prowess)? Any assumptions you think are important to verify (e.g. are dates stored in a special format, is the data typically trustworthy, etc.)? 2. **SQL + Unit Tests** \- as others have mentioned, communicating your thoughts out loud with regards to how you'd approach the SQL **and check your work** (i.e. Unit Tests) is a fantastic approach. Reason being, nearly every large query is built in phases (you figure out one part of a problem, start the second but notice your numbers don't match-up so you tweak it, then when that's right you move on to the third piece, etc.). At the end it looks like a one-shot masterpiece, but it's typically a build-up of numerous thoughts and error checks...so show that thinking out loud 3. **Next Steps** \- writing sophisticated SQL is awesome, but is quickly shadowed if you can't "layer-up" your thoughts and communicate what you'd do next to iterate on your approach and/or share your findings. I know this might seem "out of scope" for a SQL interview question, but I think you can stand-out by extending an interaction like this to more than just "Yes, I can write SQL". For instance, how would you document your SQL? Are there any other datasets you think you could join to this query for additional dimensionality? Above is the internal thinking you'd do, but I also see the framework alone as fair game to communicate (i.e. you could kick off your response with something like..."For the sake of being transparent, here's how I'm going to approach the problem as I've prepared for this exact thing. First, I want to understand more context about the data. From there I'll walk through my SQL approach, and then finish with a wrap-up..."). A little long-winded haha, but hopefully this helps! &amp;#x200B; – Authored by [The San Francisco Data School](https://www.sfdataschool.com)
This is a super legit response.
Have you listened about strata scratch? I prepared myself using this platform. They have datasets with questions and answers you can practice with. All the questions are from technical interviews from the companies so I found it helpful to use for interview practice.
Web Edition is only available under the Services Provider License; you can't deploy it yourself. My (admittedly very limited) understanding is that you can only use it if it's being used for a public web site/application. So whether or not you can use it will depend upon how you sell/distribute your application. If you sell an application that the vet clinics install, self-host and only can access from their offices, you can't use Web Edition. If you're managing the application hosting (on, say, AWS), then I think you can use it.
Get your resume ready. Looks like he wants to map your GUID and Session ID to a look up table and reference potentially an identity column that works as a primary key.
Yes, but I need to create the identity column. The trouble is, he has given me no information on how to go about that except to say my idea didn't work.
Also, thank you for booking it down to digestible sentences for me.
what is the logic behind `XX B 2` what is the logic behind `XY B 0` also, why do you want to store the count? why have duplicate rows? is there even a primary key?
Which database? Where do the parameters come from?
 CREATE TABLE example ( Id int IDENTITY(1,1) PRIMARY KEY, GuidValue VARCHAR(MAX), SessionID VARCHAR(MAX) );
If i understand what's in your mind, the last row `XY B 0` should be 1 because XY has one A on column B. &amp;#x200B; Here is the code ; declare @table_ as table( a varchar(4), b varchar(1) ) insert into @table_ values ('XX','A'),('XX','A'),('XX','B'),('YY','A'),('XY','A'),('XY','B') select * from @table_ SELECT t1.a,t1.b,t2.flag FROM @table_ t1 left join (select sum(case when b='A' then 1 else 0 end) as flag,a from @table_ group by a) t2 on t2.a=t1.a
The dataset is made up as I can't share the real data. So I tried to make it as simple as possible as I just need to figure out how aggregate a count based on same values in the first column. I guess there would not be a primary key in this instance.
SQL server. The parameters come from another table that lists all of the IDs and Customers that need to be changed. The sproc moves the information from the current table to another table.
My SQL Server is a little rusty but can't you just do this? `SELECT my_func(a,b,c) from my_table? --add where clause etc. as necessary`
how do you know (what logic) that 3 goes with 1 and not 2? is this some kind of odd/even grouping? divide `b` by 2 and round down, so match with 1?
at the first blush, it appears that all you need to do is (if your platform supports it) count(*) over (partition by a,b) I'm with /u/r3prob8 in not understanding how (for example) "XY B 0" came up.
They maintain their order in the column So had the col been 4 then 3 , 1 would be with 4 and 2 with 3
in sql storage there's no explicit row position for a particular record (at any given time there is, but SQL engine can usually re-arrange storage on-the-fly), so while in your example record 'A--5' comes fifth, in the table it could be stored at the position 1. So, unless you give that ordering explicitly (let's say you add a column "sequence order #" or craft a relevant ORDER BY clause) the order of the records that YOU see on the screen cannot be perceived/captured by the SQL engine.
&gt; They maintain their order in the column unfortunately, rows in a relational database *have no order* any other ideas?
What if the order just didn’t matter? So it doesn’t matter if 1 is with 3 or 4? Edit: and it was all about getting the nulls gone?
Is there a solution out there? Probably. Should you find it? No. The "meaning" of data should never depend on the order in which it appears in a table. Rather than find a solution to the current question, you should direct your/your teams effort towards fixing the upstream process. There is something badly broken upstream of this. Whatever solution you implement is going to be as hack-y as the upstream process and the whole thing will be the business intelligence equivalent of an [OSHA violation.](https://i.redd.it/42bc3zn9nj511.jpg)
I’d probably go with a cross apply if I understood the problem correctly. SELECT t.A, t.B, c.C FROM table t CROSS APPLY( SELECT COUNT(*) AS C FROM table t2 WHERE t.A = t2.A AND t.B = t2.B ) c Edit: formatting
I'm kind of curious about the use case for such a result set since I don't see how it would be any more useful than the table as it exists now. However, assuming that you don't really care how things get squished together and just want to eliminate as many extra nulls as possible you could use something like this: WITH workingData AS (SELECT ID, a, b, c, ROW_NUMBER() OVER (PARTITION BY ID ORDER BY NEWID()) AS [Order], 'a' AS [columnType] FROM #mydata WHERE a IS NOT NULL UNION SELECT ID, a, b, c, ROW_NUMBER() OVER (PARTITION BY ID ORDER BY NEWID()) AS [Order], 'b' AS [columnType] FROM #mydata WHERE b IS NOT NULL UNION SELECT ID, a, b, c, ROW_NUMBER() OVER (PARTITION BY ID ORDER BY NEWID()) AS [Order], 'c' AS [columnType] FROM #mydata WHERE c IS NOT NULL ) SELECT COALESCE(a.id,b.id,c.id) AS [ID],a.a,b.b,c.c FROM workingData a FULL OUTER JOIN workingData b ON a.[Order] = b.[Order] AND b.columnType = 'b' AND a.columnType = 'a' AND a.ID = b.ID FULL OUTER JOIN workingData c ON a.[Order] = c.[Order] AND c.columnType = 'c' AND a.columnType = 'a' AND a.ID = c.ID WHERE COALESCE(a.a,b.b,c.c) IS NOT NULL
What if I added a col so it’s 1 | A | 1 | - | - 2 | A | 1 | - | - And so on
Sounds like you need a new job to be honest. This guy doesn’t seem to care about you getting the knowledge you need. I know this isn’t the answer you wanted because you need to solve this specific problem but there is a much deeper issue going on here.
There's some truth here. I'm roadblocked and flapping in the wind more often than I'd like. Other areas of my work are wildly supported, however. I'm given ample opportunity for self-teaching in a lot of environments I couldn't get my hands on or wouldn't know how to emulate otherwise. We're also in the first stages of a SQL Server-to-AWS transition and I'm expected to be here for at least 18 more months to get it in working order. This lack of mentorship and semi-regular frustration is well-worth the line items on my resume from being part of a in-house-to-cloud project. For now, though... yep. Fun times. I made enough noise about it today that it's been deferred to next week when they have time to walk me through it. So... we'll see how that goes.
"i've been writing SQL since 1987, i've watched it grow up, and i know its strengths and weaknesses... more importantly, i know when to use it and when not to"
Do you have experience with simple/complex analysis? What about stored procedures, macros &amp; ETL?
&gt; Edit: and it was all about getting the nulls gone? easy-peasy lemon squeezy SELECT ID, MAX(a), MAX(b), MAX(c) From tbl GROUP BY ID vwalah, no more nulls
Try it and see. You didn't specify how you wanted to handle duplicate values or uneven distributions so as written it wouldn't do anything special for the duplicates and it would collapse things as far as it could.
Well that’s good they at least pushed it back to give you some breathing room. I personally have first hand experience with toxic coworkers, it was a job I had before switching careers to software instead of hardware, and I’m so glad I haven’t had to deal with that again. I would definitely try to find someone you can tell who would listen that this person (probably a senior dev or dba form the sound of it) that he’s not being helpful at all, and you don’t have a problem with being self guided but you also need some help and direction every now and then. That’s not too much to ask in my opinion, even for a mid level or senior person to ask for help. If you’re on a large team that has deep silos of knowledge then you could very well have a shallow knowledge of sql but still be the expert on another topic and need help.
Hey, I think the webpage lies again :(
But will certifcates get me a job?
^(For DML skills) * You've written select queries * subqueries (including correlated subqueries and derived tables) * Window functions * Common table expressions * You've written, INSERT, UPDATE and DELETE statements (including truncate where appropriate) * You've used temp tables * You've used control flow statements (IF/ELSE) * You've used CASE statements * You've used various join types (INNER, OUTER, LEFT, RIGHT, FULL, CROSS JOIN (and explain the rare circumstances where CROSS joins are needed)) * You know the difference between UNION AND UNION ALL &amp;#x200B; For DDL skills * You've created 3NF schemas or denormalized schemas * You've written stored procedures, triggers and/or functions * You've created views &amp;#x200B; For environment skills * Performance tuning * Index creation and maintenance * Index hints
So if I have: &amp;#x200B; R| ID | a | b | c :--: | :--: | :--: | :--: | :--: 1 | A | 1 | - | - 2 | A | 2 | - | - 1 | A | - | 3 | - 2 | A | - | 4 | - 1 | A | - | - | 5 2 | A | - | - | 6 &amp;#x200B; And wanted : &amp;#x200B; R| ID | a | b | c :--: | :--: | :--: | :--: | :--: 1 | A | 1 | 3 | 5 2 | A | 2 | 4 | 6 &amp;#x200B; Would there be an easy way of combining them then?
Anyone with any pointers at all?
Ah, a sort order column? Yeah, just replace ROW_NUMBER() OVER (PARTITION BY ID ORDER BY NEWID()) With the column named R in each part of the CTE.
Sometimes...sometimes to get the job done, you have to do things you're not proud of. I mean, we've all been there, right? You hack something together and it works but afterwards, you need a shower and a [Scotch-Brite pad](https://images.homedepot-static.com/productImages/82f2a365-bfb9-4331-8aac-5f0bfa7e4703/svn/scotch-brite-sponges-scouring-pads-226-cc-64_1000.jpg) to scrub that feeling off you.
yes, now it's just a group by by R and ID (with max or min as the aggregate function)
No but it will make you not sound like a dipshit when interviewing with a manager that isn't clueless.
so, what stops you from doing max( effective_date) or whatever column?
You wouldn't necessarily need a subquery and would instead use the MAX aggregate function, something like MAX(t.EFF_DATE) instead of just t.EFF_DATE. Since you would then be aggregating t.EFF_DATE you would also need to remove it from the GROUP BY clause. Now, if that would actually give you the result set you are looking for depends a bit on how the data is structured and how you are wanting the data broken down. As it currently stands with just the suggested change you would end up with the max effective date for each possible combination of c.client_code, c.client_name, c.STATUS_CODE, p.employee_code, p.employee_name, and t.percentage that exists. This might work fine for some purposes but if you are needing to change the way it's grouped to only show you which C.STATUS_CODE corresponds to that particular date (as in the current value) then you might have to get more creative with some rewrites and subqueries.
Then what would you recommend for somebody who has a useless undergrad and is wanting to get into data?
If he's a SQL Server DBA and the company is going to AWS and he doesn't have AWS experience, he could be threatened about his job and he's trying to slow things down.
functions should NEVER be used for primary key generation - they have a HUGE impact on performance. and for clarification - don't care whether you're talking about a UDF or sproc... it's sucky. what exactly are you trying to do that the guid or whatever is insufficient? (granted guids generally have negative impacts on indexes) edit: TLDR: data architect sounds like an idiot.
Simplest way would probably be to use a sum instead of a count, and use a CASE WHEN. If I'm understanding the question and obfuscated data correctly, something like: SUM(CASE WHEN [B] = 'A' THEN C ELSE 0 END) If you have XX A 2 XY A 0 YY B 1 then the query should only calculate a total of 2 A's. In the first row, column B is A, so it sums the value of C, which is 2. In the second row, column B is A, so it sums C, which is 0. In the third row, column B is B, so it ELSEs to 0. 2+0+0 = 2.
I'm not 100% sure what we're trying to do besides relate several tables of data to one another l, but I've been told that as a special-character containing alphanumerical key, the GUID is a poor primary key candidate. I imagine we're willing to forgo upload performance to improve extraction and analysis performance using a fixed-length integer primary key.
I tried that and it appears to have no effect on the data. I'm bringing back the same number of rows.
I did try: convert(nvarchar, max(t.EFF_DATE),101) [Effective Date] And it's bringing back the same number of rows...with or without the "Max" in front of t.eff\_date - which is why I'm thinking it may need a sub query?
fixed-length can be dealt with easily. CREATE TABLE PKTable AS ( [IntID] int NOT NULL IDENTITY(1,1) , [IntTXT] AS RIGHT('0000000000' + CAST([IntID] AS varchar(10)),10) PERSISTED , [other columns as necessary] , CONSTRAINT [PKTable_PK] PRIMARY KEY CLUSTERED ( [IntID] ) , CONSTRAINT [PKTable_AK] UNIQUE NONCLUSTERED ( [IntTXT] ) ) add into the PKTable (full speed) then use the IntTXT for your foreign keys look... like magic, you have a simple fixed length integer, fully parallelizable, easy for FK.
Yes! Thanks all for help
It sounds like each unique combination of c.client_code, c.client_name, c.STATUS_CODE, p.employee_code, p.employee_name, and t.percentage have exactly one t.EFF_DATE value and you need to re-evaluate how you want things grouped. Without understanding the goal of the query I can't really give more advice on how to go about changing it.
this seems obvious, but did you also remove t.eff_Date from the group by?
I'm going to have to stare at this briefly, but I greatly appreciate it as a starting point. Thank you very much.
so basically... int32 (assuming non-negative) is 10 digits... intID is a standard identity... TXT just pads the left side with 0's then picks the right 10 chars - this is a hack since FORMAT() isn't considered deterministic, which is true in some cases but would've been much cleaner here... PERSISTED means that it calculates it upon insert... PK is 50/50 - it could've been on the TXT column... the AK also guarantees that the other is unique to support FKs.
also, since the txt column is a calculation instead of UDF/sproc, it can be performed in parallel (same as using the calculation in a SELECT statement or a view)... you get about a 5% performance hit with it, but not terrible. had some college dipshit demand that I use a UDF with a while loop to generate the PK... 300x performance hit (300 TIMES slower)... did basically the same thing, and it worked pretty well.
Lol to the last part. Thank you so much, btw. I really appreciate the hand-holding.
if you don't know how to list your experience, you don't have enough.
I'm such an idiot. I think that's it. You know....sometimes you can't see the forest for the trees.... It's like looking all over the damn house for your keys when they've been in your hand the whole time. &amp;#x200B; Thanks!!
First, I'd start with ensuring that the SQL is correct for the problem. A quick confirmation there then allows for the questions of performance, flexibility or opportunities for modularity. &amp;#x200B; You can explore shortcomings faced or other issues with the code you wrote.
Yep, looks a tiny bit broken. I’m likely just missing some of my old monitoring/restart services. Thanks for the heads up! I’ll let you know when it’s back.
Where does that dark coding interface come from?
To start, probably some of these courses. Hell, even W3 has some decent tutorials I hear. It's just to help you talk a bit more fluent, to help get your foot in the door.
I'm sorry to hear that you didn't pass and feel crappy about it. When you say, "after reading the entire book", I assume you mean the *Exam Ref 70-765: Provisioning SQL Databases* book. My impression of these MS Exam Reference books is that they really don't pretend to be all-inclusive teaching books, but this-is-a-list-of-the-kind-of-stuff-you-should-know books. I spent a lot of time on the 70-761 Exam reference book. As I progressed through it, I realized the topics weren't very in-depth. The examples were meant to review, i.e. they assumed I already knew the material -- I often didn't. If I really wanted to know the material, I needed to supplement with other sources. I think it is a rare occasion when one of these books is truly comprehensive.
Tru
&gt; and the whole thing will be the business intelligence equivalent of an OSHA violation. Going to be stealing this for future use.
You can google it, but essentially a manual option under Tools -&gt; Options -&gt; Environment -&gt; Fonts Colors There might be an easier way, but everything I’ve seen...it’s pretty manual to get it to look that way.
Totally depends on your job. If you are a DBA concerned with servers and storage space, then it matters a lot. If you're completely unconcerned with storage, then it doesn't matter that much at all. In fact, you can get a lot of performance increases by breaking normal form depending on what you're trying to do, and what technologies you are trying to connect to the data. I personally think it is very important to understand the basic concepts, and to, in general terms, build towards normalized data unless you have a specific reason (cough, which you should put in your documentation) for doing otherwise. It isn't even so much "trying" to stick with normalization that's a benefit, but rather learning the concepts are important to understanding databases on a deeper level, and how they *should* be designed... or rather, what the reasoning is behind those concepts. That I think is very important. Actually implementing it is more like a 50/50.
Depends on the role, but it isn't hard at all if you are trying to work in analytics. Probably a bit harder to be a full bird DBA, or senior DBA.
Thanks for the response. I bet your fro is gonna be awesome.
Probably true lol but I'm gonna try
Thank you this was really helpful!!
3rd normal form is basically the lowest level of NF on which DB schema should rest. You basically use normal forms to avoid reduciy in the DB. The redundancy is the main cause of inconsistency in DB meaning if you have some data stored twice in DB there is a big chance that the data won't be updated in both places and then you don't know which data is correct. That's the main use of normal forms. If you need more info and brake down of normal forms pm me or write here...
And you note it in the changelog as a temporary fix. Perhaps you still live in genuine denial and you mean it when you say it, but somewhere deep in your psyche you know that it's going to stay that way for years to come.
Hard. Even a junior DBA, they're going to likely want a little bit of experience.
I actually just this morning accepted a new job offer for a senior role. To put this in context I've spent the last 3 years working as a senior SQL developer for a F500 company, and recently have begun working in the capacity of an architect. The final interview was over 3 hours long and had a total of 6 rounds, and only one of those rounds where I was meeting my peers did SQL competency come up. I just answered that my current project was a total of 43 stored procedures, spanning about 5000 lines of code, which uses everything from cross joins, to cross applys, to custom functions, to dynamic sql, to loops, etc. The two guys just kind of looked at each other, nodded, and then we moved on to another topic. Certifications weren't a topic of conversation, mainly because I don't have any (in SQL), and mainly because they didn't give a fuck. When I got the job I currently have there was no real SQL quiz, and I was coming from an 'entry level SQL' job with a 100M company, and I got that first job without knowing much SQL at all, in fact for the role I was one of three applications who had any previous SQL experience at all and they had fully expected to train SQL on the job. Analytics is a good field to look into, but it requires some marketing, statistics, and other soft skills. Can really go far without knowing much SQL at all, and in some cases no SQL... although the SQL side of it increases your value.
I would suggest that you need to get involved in your current job (i assume you work for a software company or you have been in touch with databases). Learn from the senior DBAs, read blogs, books, videos, all you can. I read once that it's easier to get promoted to junior DBA than getting a job with no experience. &amp;#x200B; Good luck!
I had 7 years of SQL experience with one as an systems admin position before I got a DBA job. It's a pretty critical position if the company needs one so generally you've got to prove that you know your shit and also that other people have already trusted you with their data.
Quite difficult, unless you're getting into an organization that either: * Never had a DBA and doesn't fully understand what the job entails * Is looking for people with zero experience to train from the ground up
It's going to be near impossible unless you know someone at the company. I doubt there are many positions with administrator in the title that would hire someone with no experience. I can't see a finance department hiring a payroll administrator with no prior experience in processing payroll. If being a DBA is your goal, set your sights on a junior developer, analyst, BI specialist etc. Learn the skills you need, learn from more experienced people and build your network/connections. Let your company know your goal is to be a DBA and put together a career track to achieve your goal. If they won't help with that, you may want to look at other companies. At a reputable company, they'd likely be looking for at least 5 years experience before considering anyone for a DBA role.
Without ANY experience? Very Difficult. With some peripheral experience and a demonstrated ability to learn under stress? Better. I worked on document templates with a sql backend. Volunteered to back up our sole DBA so he could take time off and learned how to keep things running. From there, used some creative (but truthful) writing on a resume and interviewed really well for an associate DBA role, with a great team of DBAs. I’ve learned how much a company trusts a DBA with their livelihood, and grown to fit that role. It would be easy for an inexperienced solo or DBA duo to accidentally bring a whole company down. Data is everything, and putting trust into someone without experience is betting the company on your ability to protect it.
Hope this doesn't come across as spam, etc. but I've just released a week ago a new course to learn SQL using PostgreSQL called Mastery with SQL: https://www.masterywithsql.com/ I created the course because in my experience the problem with most other courses is they don't have anywhere near enough exercises that provide you with the opportunity to practice what you've learned. I spent about half of the time creating my course just focused on building great exercises that re-enforce the material and are based on the types of queries you need to write to solve real-world problems. Lots of other courses also use some pretty antiquated parts of SQL and also don't really showcase the new good stuff in modern SQL (and modern stuff in PostgreSQL in particular). Happy to answer any questions if anyone has any!
A good guide to get you up and running with psql is available at: http://postgresguide.com/utilities/psql.html Would definitely recommend you check that out - it's the most common command line tool for interacting with Postgres.
And, we're back! There was a problem with the reset script. Should be resolved but I'll keep an eye on it.
The lack of a cert or having a cert has never gained or prevented me from obtaining a job that I know of and I'd be surprised if it did. A lack of a degree has affected me, but it's never stopped me. Focus on your education, experience, and portfolio and you'll be fine. Most folks I know skip the portfolio, but personally I think it's important in the long run.
Man 6 rounds! That's gotta be exhausting! Thanks for your response. A few others have said basically the same thing. Actually really puts my mind at ease and
I'd almost say junior positions are more difficult because they have more competition and less frequency in postings. It's usually easier to slide in from another lower level IT position.
Dude it was one round after the other, and most of them were asking me questions about "how I thought," and/or, "how I approached problem solving." A lot of theory was discussed but almost no practical application. To be fair, you will encounter tests, etc., in your quest for lower roles, but if you can pass their tests without a certification then getting hired is another discussion completely. If you have two perfect candidates, one with a certification, and one without, then you might take the guy with one. In reality you often have a better candidate without a certification than a weaker candidate with one, and it just doesn't make any difference at all. No one cares. You have so many other areas to focus on, including how you interview. If you want to get a cert, then get a cert. I'm thinking about getting one or two, but why not? My company will pay me to at this point. I can spend hours on the clock working to get the cert. Why wouldn't I want to get one? But at this point all it shows is that I have a vested interest, and have "taken the next step." Like, "oh yeah, I got this because I was bored, of course I could get this, look at my work history."
I've actually got an interview for a D.A. job soon so I wanted to see if this would help me get the edge (due to my lack of my experience)
Lack of degree thing is really opening up, and I see most jobs now say, "or equivalent experience." There are definitely still some companies that **require** a degree, but whatever, the job market in these fields is strong.
Thanks for the advice kind sir. Much appreciated!
It's affected me, but it's never stopped me. There are three jobs where I distinctly remember that stopping my application process. That's honestly a drop in the bucket but the location on this anecdotal experience also matters. So while it's not always necessary, I would still typically recommend college if you're at a stage in your life where you have this opportunity at a reasonable cost. (Money, time, etc.) That said, it's not for everyone and it's not a requirement to be successful. It can help however and I'll never suggest to stop educating yourself. (Regardless of college or not.)
I would also recommend college. I have 124? credits out of the 126 I needed for a BA. Things happened, but whatever, it never held me back, just made it a little more difficult. Honestly now at this point (I'm 36) I'm trying to ride the whole "college drop out" wave to "increase diversity" because that seems like a thing. In a weird way it makes me stand out, but honestly I'm not sure that benefit outweighs the benefit I would have had if I received my degree.
I wanted to become a DBA, but it's hard. So I got a job as a database analyst, which is nothing like a DBA. I wrote queries and built reports. Which open me to another world of programming and automating things, now I work as a developer, sorta. Pay is good. Never know where your first job takes you...if you really want to be a DBA, then keep trying...but there are a lot of other things out there that just pay just as good and also fun.
I would agree that I don't think there would have been any career or financial benefit to myself if I had finished my degree. There could be in the future, but who knows. There was a great quote I heard recently. "The pace of how quickly technology changes is the great equalizer."
Finishing my bachelors would be worth nothing to be either now, or in the future. Having an MA might be a different story.
Harder than being a developer. You dont want a new hire accidently dropping tables or messing up update statements, etc
My first job was as a SQL DBA, the opportunities are out there, but don't expect to get market value or anywhere near it. Big consultant agencies are usually hiring entry level positions around this time due to school just letting out, and will likely get you the training for it too. Either that, or a small shop with a nice senior who is willing to train you. I got my first job offer out of college from TCS, but turned them down for a better salary at a local hospital instead where I got trained by two really knowledgeable seniors. I will say getting that job offer from the hospital was a lot of luck. Study up on fundamentals, read blogs, watch videos, and if you can try and get a fundamentals cert somewhere.
Practice as much as you can! I can tell you where I practiced from. I used sqlzoo, datacamp, strata scratch and leetcode. Tried them all and found strata scratch more useful. They have datasets for practicing with questions and answers from technical interviews from companies so I found it helpful to use for interview practice.
Nope
But in comparison with software or web development do you think that the following market is more accessible for newcomers?
I loved the way you handled this, and it inspired me to try to find something a little more concise to get the same result. I figured if we UNPIVOTed the null filled columns and then PIVOT the intermediate output back out, we could get there. SELECT ID, a, b, c FROM ( SELECT ID, k, v, ROW_NUMBER() OVER (PARTITION BY ID, k ORDER BY ID, k) AS R FROM ( SELECT ID, a, b, c FROM #mydata) AS orig UNPIVOT ( v FOR k IN (a, b, c) ) AS unpvt ) AS piv PIVOT ( MIN(v) FOR k IN (a, b, c) ) AS pup With this approach, I was able to handle the OP's scenario where b=3 should be paired with a=2 and b=4 should get paired up with a=1
This also works if you don't want to do a `UNION ALL`. SELECT CASE WHEN ProductName IS NULL THEN 'Null' ELSE 'Non-Null' END AS SaleType ,SUM(COALESCE(extprice, 0)) AS Sales ,COUNT(*) AS "Row Count" FROM SalesTable GROUP BY CASE WHEN ProductName IS NULL THEN 'Null' ELSE 'Non-Null' END
Cool, thanks. Didn't know it was an option.
facebook has, what, 5 trillion images. tell me an algorithm that I can use to quickly check (on upload time) if a new image is a duplicate? also, no one stores images in a db
This is speculation. The image would be renamed with a uuid and placed on a server as any image would. Attributes would be stored in a DB of the files current name, original name, owner, path to file, ECT....
According to [this](https://www.usenix.org/legacy/event/osdi10/tech/full_papers/Beaver.pdf) Facebook use Haystack, an object storage system for storing images. I can't see any reference to deduplication in there, and [this Quora entry](https://www.quora.com/Why-doesnt-Facebook-use-data-deduplication-for-images) asserts there is no deduplication, speculating that this would consume more resources than would be worthwhile.
There's no fucking way FB does some hash match against trillions of pictures every time someone uploads. They throw that shit it a file store and that's that.
Database administration is not entry level and it's not welcoming for the most part to new folks in the sense of work availability. The community and resources are the opposite, but to gain experience is difficult. I started as an application administrator which involved just a little of everything, then system administration with a sprinkling of database, and this allowed me to move to a pseudo DBA position. At that point I was then able to easily find new database jobs. Breaking the barrier is difficult, but I think there's tremendous value NOT starting in only database but landing there after.
This seems a lot easier than the union all. Why do you use a coalesce? Is that so you return a 0 instead of a null?
stop thinking that you need FB's solutions... you aren't big like FB... FB has had plenty of growing pains - deal with them as they come. solve today's problem based on the last 3 years of trend, not by comparing to &lt;insert 'web scale' company X&gt;
Keep a hash of each image uploaded and compare the hash of the inbound image. Is this not common practice?
&gt; also, no one stores images in a db County Jail Inmate Roster
Rename the file to the hash of its contents. Store the file name and everything else in the uploads table.
Most people don't upload the same image many many times anyway. They would just share it from an existing post.
that's not going to work at that scale
Exactly. He said "quickly check". I know I don't have FB resources at my disposal, but hash matching even a few thousand rows of binary data takes longer than it does to upload a picture to FB. Now multiply that by a factor of like 100,000,000? And again by multiply by thousands of uploads per minute. Not even feasible in the slightest.
Cool! PIVOT and UNPIVOT aren't really things that I've worked with very much so they are not usually something that I think of.
I also dropped out. Also 36. Also just got my first senior SQL role. Certs, degrees, pretty much all of that is meaningless right now. The industry moves too fast for it to be of hardly any use. Not only that, I *constantly* see shit on StackOverflow from students learning stuff like comma style joins from their 60 year old CS professors. Like, you're out of date by 3 decades dudes. You're going to have to relearn half that shit. So far, pretty much zero schooling that I've done in my "field" has progressed my career in any way. It's been 100% on the job learning (maybe 2% of that actual training). I don't want to encourage people to say "fuck school". But I definitely said "fuck school", and it turned out great.
Yep, that's exactly why.
I pretty much never think about which normal form I'm using (as a developer with a focus on databases/sometimes DBA since 2003) honestly. I try to normalize as much as possible, so I THINK that's Third Normal Form or close to it, but I denormalize once in a while for performance reasons. When to denormalize comes with experience mostly. Sometimes it's reactive tuning - we have a bunch of queries all making a join from table1 to table2 just to get one column, and they're all a little slow, so maybe if we just stick that column in table1 we can speed things up. Not ideal but the real world isn't always ideal. :) That said I usually try to find other performance solutions before denormalizing because the worst is when data becomes out of sync. (I am talking about transactional (OLTP) databases, by the way.)
This can’t be the whole query. It wouldn’t run.
TLD, PD and OCC are all table aliases which are defined after the FROM. You would need to join to the tables these aliases are supposed to be referring to in the FROM component. At the moment the query won't work as these tables and the aliases aren't defined. For example: SELECT ORD.OrderID, CUS.CustomerName, ORD.OrderDate FROM Orders ORD INNER JOIN Customers CUS ON ORD.CustomerID=CUS.CustomerID;
TSQL doesn't have arrays. You might want to rework your syntax to use table variables or comma-separated lists, for example. I would also question using "union" in queries (vs re-factoring reports to eliminate that, for example) for any relatively large tables.
So help me codd? Lol we just learned that quote as well
CDN dedupes
Me neither! That's why I wanted to give it a shot.
As I say, it’s pseudocode. That array in real life is a comma separated list, turned in to a table with a table value function. I can try to refactor without a Union, that should be doable. Will test when my current build is deployed to our performance testing environment.
Yeah I got nervous sharing the whole query since it's work related but you're right there are join statements after the query I cited. So aliases are defined in the join clause when you're working with different tables?
If you choose to use an alias for your tables, always use the alias in all parts of the query. Even if the query will run without the alias reference.
Haha, yes that's the one!
Why does the match have to be instant? You can always batch-purge matching hashes at a later time. You couldnt use a traditional RDBMS at massive scale, but this kind of matching is exactly the type of thing that column store databases were built to solve.
Correct. You can give aliases to the column names which will be used rename them on output or when directly creating a table from the query but don't worry about that.
Awesome. Thanks for your help!
I would do something like this Select count(*), Sum(case when parameter1 = some condition then 1 else 0 end) as param1count, Sum(case when .... From Beautifullymodelledtable
the details of implementation can and most likely will change the performance evaluation. i don't quite understand your full scope, but right now all you seem to need is to calculate counts from your main query and un-pivot the results (and you shouldn't need sql for that since ssrs): select count(*) "cohort", count( case when &lt;condition1&gt; then 1 end) "cohort size c1", count( case when &lt;condition1&gt; and &lt;condition2&gt; then 1 end) "cohort size c2", etc... from &lt;original data table&gt;
For real. Why would you do all that just to save 100kb?
I shall give this a go. Whilst there are some other items not scoped in here, they’re all trivial to get to, regardless of how I implement this. Thanks for your suggestions!
because at facebook's scale, you're accumulating data faster than you can batch-purge it
FB probably don’t, because why bother? But hashing an image isn’t going to take much time, maybe a couple of microseconds? Then doing an index lookup of that hash value? Again that’s going to be very quick, even if you have 3 billion rows because the whole point of indexes is that you don’t read all of the data! You traverse a structure that tells you where the next bit of data you need to read is - for 3 billion rows that were mostly unique you’d end up doing about 5 IOs, that is not going to take very long at all.
Dynamic SQL?
I've never heard that term.
If you listen to the people on this sub Reddit they'll tell you it's the devil, and they're mostly right. You can do something like this: DECLARE @SQL varchar(MAX) = ' SELECT * INTO ' + @TableName + ' ' Then execute @SQL as a query. You can define @TableName, and any other parameters you need for your alter statement and then run a loop so it will dynamically build your query and execute for all of your variables. While much more complicated than your purposes, [here](https://www.reddit.com/r/SQL/comments/bnj60j/ms_sql_built_a_cool_little_tool_to_compare_tables/) is something I did in dynamic SQL recently that can give you an idea of what it looks like.
why does this need "a variable"? just create a script for all 8 of your tables -- section for Dan truncate table daily_dan insert into daily_dan ... -- section for Sam truncate table daily_dan insert into daily_dan ... -- and so forth
Dynamic SQL is exactly what I was looking for. I didn't even know the term but I know it's what I'm looking at. Thank you!
&gt; And you note it in the changelog HAHAHAHAHAHAHAHA, say what now?
I want to be one of those people who writes a script to do anything for me that takes more than 15 minutes and I have a lot more to do than I described, but the solution is scaleable. It seemed a good time to ask.
Go with God, my child.
Maybe things are different where you are, but I have one really basic cert and have had no trouble getting jobs. In most (maybe infra/networking are exceptions, I dunno) areas of IT, experience counts for much more.
Less accessible, because there's much more scope for things to go wrong. As a junior dev, it's okay if you submit some shitty pull requests - they get reviewed, you learn from the feedback, make commits to fix the problems, and they get accepted. No harm was done. As a DBA, you're often making decisions such as 'can I kill this blocking SPID which is stopping paying users from logging into our platform?' with people breathing down your neck.
TinEye does more than compare file hashes though. \&gt; These unique digital fingerprints describe the patterns found in the pixels of the images. MatchEngine is able to identify even partial matches of these fingerprints. That said, not suggesting comparing hashes for new images would be practical.