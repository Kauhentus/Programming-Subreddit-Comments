This statement is not entirely true - there are differences in how the SQL engine handles filtering in the ON clause vs in the WHERE clause with outer joins.
Should I have assemblies called "Microsoft.SqlServer.XEvent.Linq" and "Microsoft.SqlServer.XE.Core" in C:\Windows\assembly?
I was mostly talking about from a logical perspective. But yes, there will be some differences, but for most (95%+) simple joins they will produce the same execution plan. Not that I'd ever recommend doing joins in the WHERE clause...
Going to give you silver instead of gold. Frankly I'm a little disappointed with you. I didn't need a subloop, I should thought in set theory. I just needed to add the new subcalc to the @ClientList table, then add it as a parameter in my join. Each subcalc row will get it's own ID in the loop set, so one loop will work instead of two. :P Have a nice weekend.
It's a queryable system consisting of rows and columns of data. That's why I say it's technically a yes. And the formal definition of database is "a structured set of data held in a computer".
SQL neither knows nor cares what your data is doing. It doesn't know, and has no way to know, that your "offending nonsense join condition" is offensive or nonsense. It does exactly what you tell it to. No more, no less. If you write bad code, that's on you.
Don't really agree with that actually. Whether you're an analyst, engineer, dev or a DBA you should know how to write performant queries and how to leverage indexes. Writing a query and then leaving it to the DBA(s) to fix / optimise / send to hell is pretty lazy in my eyes. 
Sure, you're totally right from that side - just wanted to add in a bit of clarification in case someone took the statement too literally :) I'm not speaking of joining through WHERE, but rather filtering through ON. As an example, these two queries will produce different results and each have their place. select * from foo a left outer join bar b on a.id = b.id where b.id = 123 --- select * from foo a left outer join bar b on a.id = b.id and b.id = 123 
The question is, if you imagine I have a key1/key2 record with the data below.... Sub query A results in: key1 | key2 | time1 | agg_1 | agg_2 | a | b | something | 5 | 6 | Sub query B results in: key1 | key2 | time1 | metric 1 | a | b | something | 5 | So the outer-most query is a left-join between the two right? "Give me all rows from query A that may or may not match with Table B on the following conditions": - a.key = b.key1 - a.key2 = b.key2 - a.agg_1 &lt;&gt; b.metric_1 Using the original query in my post, the result set is: a.key1 | a.key2 | a.time1 | a.agg_1 | b.metric_1| a | b | something | 5 | NULL | That is clearly non-sense, as we just proved for the specified key1/key2 pair of (a,b) the values are equivalent *AND* are NOT NULL. So what gives?
Try this and tell me what it does: select a.key1 ,a.key2 ,a.time1 ,a.agg_1 ,b.metric_1 from ( select key1# ,key2 ,time1 ,agg_1 ,agg_2 from ( select key1 ,key2 ,time1 ,sum(metric_1) agg_1 ,sum(metric_2) agg_2 from source where time1 = 'something' group by key1 ,key2 ,time1 ) where agg_1 &lt;&gt; 0 ) a left join ( select key1 ,key2 ,time1 ,metric_1 from target t where time1 = 'something' ) b on a.key1= b.key1 and a.key1 = b.key2 and cast(a.agg_1 as int) &lt;&gt; cast(b.metric_1 as in); &lt;-- Offending nonsense join condition
Same result, eliminating it being a datatype issue.
Sent a message. Hopefully the script worked for you. 
This vaguely sparks a memory of something I learned a long time ago. I can't really remember exactly what is going on here, but someone like /u/ichp can probably help you.
Ah, yes, that's a good example of a join where you can't get the same result by simply moving the join condition to the where clause. You used to be able to do it using the *= operator, but that's been deprecated in MS SQL. 
Sounds like you may want to explore expanding your query to something like: SELECT amount FROM table WHERE type IN(1,2) And then PIVOTing the results 
I have no idea why you would want this if there's no unique column. However you could look at doing a self-join. I mean this isn't the ideal way to do it and I wouldn't suggest it, but I also wouldn't suggest doing what you're trying to do. ``` with fake_table as ( select 10 as [amount] ,'1' as [type] union select 20 as [amount] ,'2' as [type] ) select a.amount as amount ,b.amount as [amount_2] from fake_table as a inner join fake_table as b on convert(int, a.type)+1 = convert(int, b.type) ``` 
I'm going to rephrase your join criteria using the dataset you provided: a.key1 = b.key1 (a=a; TRUE;) a.key2 = b.key2 (b-b; TRUE;) a.agg_1 &lt;&gt; b.metric_1 (5&lt;&gt;5; ,**FALSE**) The NULL in b.metric_1 in your final dataset means the join failed for that row. The join failed because you're asking it join when agg_1 does not equal metric_1. The 2 values are the same, so it's not joining.
Holy Cow, how did I not realize that? Thank you! I skipped ahead to the finish line without realizing what was in front of me this entire time!
Instead of casting it, wrap both sides in ISNULL() or the Oracle equivalent. Like ISNULL(a.agg_1, -1).
Well, "A left outer join B on condition", if you want to "spell it out", says "give me all rows from table A, and if condition is true for any rows of B, also give me those joined". &amp;nbsp; So that's why you are getting the record from A in your result regardless of whether the condition evaluated true or false and since the condition evaluated to 'false' for all rows, you are not getting anything from B.
Yup, I was expecting what I wanted to expect. Not really reading what I wrote.
You can use a case statement SUM(CASE WHEN Type = 1 THEN AMOUNT ELSE 0 END) SUM(CASE WHEN Type = 2 THEN AMOUNT ELSE 0 END)
maybe see if you can plow through every one of the questions on sqlzoo.net. That's a great overview of the "basics". Another classic interview question that isn't very hard and not deceptive in any way but tends to stump people: How do I find out of the value in column A is a duplicate? 
Depends on the company. At some places I've worked and analyst wouldn't have right to create an index. I agree they should be notifying the DBA when they think there should be one, but many times they wouldn't have the ability to do it themselves.
So in a nut shell: By adding his condition to the LEFT JOIN it will execute as a "true" LEFT JOIN, but by putting it in the where condition it will act as an INNER JOIN (sort of) and remove rows from A which are no longer evaluated as true?
This worked perfect thank you! Can you please explain how the sum is working in this. I usually just use the case when but I've never used aggregate functions with case
I think you're talking about moving the inequality (a.agg_1 &lt;&gt; b.metric_1) and in this yes, the result will be the same as if he'd use an 'inner join'. If you want a deeper dive, it's not going to be true for other kinds of conditions. For example, for the same data, i _can_ add "a.key1 = 'a'" to the left join condition and it will mean that in the left join only the records with key1 = a will be attempted to be joined, the rest (if any) would have been without any joined records. Moving this condition to where part could eliminate records where key1 is not a, but it wont make it an inner (since the output record will still conform to the condition).
The SUM() is there so you don't get multiple rows when the Type = 1. Theoretically you could have &amp;#x200B;
Because Left Join will return all records of a and null for b fields if it cannot find anything that matches b’s criteria for an a record. You want to use Inner Join instead if you’re just looking for where all criteria are met.
Got it. That's exactly what it was doing. Giving me multiple rows when I wanted the results on one row. Makes sense thank you again I learned something new.
What I mean to say relative to the inequality is that: LEFT JOIN B ON A.Key = B.Key AND A.Inequailty = B.Inequality That will give me everything in A, including A.Inequality and B.NULL Inequality so long as A.Key matches. However moving that do the where clause such as `WHERE A.Inequality &lt;&gt; B.Equality` will then remove sets from A which the left join will not remove? Is that what is going on with the OP?
If it is a shared database, which it sounds like because both you and the developer have permissions to it; talk to the person who first created or restored the database to the server. I am not a DBA but I work with MS SQL DBs daily and the ones where the data are important I set up a quick nightly backup routine with their wizard. Takes 5 minutes or less, so just because you and dev didn't run a quick on demand backup there could be one from the night before in a designated backup location. 
You're welcome.
styfl3yr, You could probably move through Learn SQL the Hardway if you focus. It’s a good full spectrum beginner book/program. If you don’t feel like you have the skill set at this time, just make sure to drive home the fact that you are passionate and eager to learn. Best of luck! -ninjarob
Note that the order of the columns matter. Consider a phonebook, indexed on last name then first name. If I look for lastname='Smith' I might still get thousands of names, so a multi column index could help narrow it down. If I try to find firstname='James' with no last name specified, I'd have to scan the whole phonebook looking at every last name. Also, if I have an index on lastname+firstname I don't need a separate index on just lastname, but might still need one on firstname.
And don't believe the hype around the all-column index. It's not that great.
&gt; non-null value in a.agg_1 and NULL in b.metric_1 (meaning it exists on dataset a and not dataset b). NULL is special... it's the "lack of value"... it's not equal to anything, thus it can't be unequal to anything either. It also has a special feature than any expression containing a NULL kind of contaminates the whole result as NULL unless you take special care to deal with it. That comes from functions like ISNULL, NULLIF, COALESCE, etc. https://www.w3schools.com/sql/sql_isnull.asp So, where you have: and a.agg_1 &lt;&gt; b.metric_1; &lt;-- Offending nonsense join condition b.metric_1 being NULL is NOT "&lt;&gt;" to a.agg that's not NULL. 
&gt;Microsoft.SqlServer.XEvent.Linq The location for assemblies defaults to GAC. gci c:\ -Filter 'Microsoft.SqlServer.XEvent.Linq' -Recurse -ErrorAction SilentlyContinue | Format-List Directory: C:\Windows\Microsoft.NET\assembly\GAC_32 Name : Microsoft.SqlServer.XEvent.Linq ... Directory: C:\Windows\Microsoft.NET\assembly\GAC_64 Name : Microsoft.SqlServer.XEvent.Linq ... &amp;#x200B;
I haven't used it with BigQuery, but I quite like DBeaver and it does support BigQuery from what I see: &amp;#x200B; [https://dbeaver.io/](https://dbeaver.io/)
DBeaver supports BigQuery, but honestly it's just on paper. Best way to work with it is still through the web UI unless you have actual tabular data and not hierarchies
Just took a look at sqlzoo. Thats a great resource for any beginner! (I passed it onto my little brother and I would definitely use these questions when interviewing someone.)
Lol that dude's life's work is indexes. He's pretty good though.
Right, the infamous, ‘How would you get rid of duplicate..’ question. I also have been asked about database normalization, given a certain table with columns, which would be a good candidate for a key. 
Also, best of luck!! I am sure you will do well. 
The simplest example would be like SELECT T.A, T.B, T.C, T.D FROM TABLE T WHERE T.A = X AND T.C = Y If column A is indexed but column C is not, that query is still going to have to go through every row in table T and look at the value for C (full table scan). If C is also indexed then it will be faster. If both A+C are indexed together it will be even faster.
We had a guy that suggested we index every column combination to fix performance issues once and for all. Like they wanted every possible combination...I told them we can't have that because it is bad luck. 
What is your current table looking like in terms of data field attributes for colors?
Name your patterns...create column "pattern" Give each different element of the garment it's own column easy peasy
Yeah sure, that makes sense for sure. It's correct that not everyone should all have SA access or whatever. Sticking with the index theme a bad one can make things worse before it makes them better so you shouldn't have anyone just throwing them in without knowing what they're doing but that just means I would still argue that devs/analysts/engineers should know this stuff. It's an interesting debate whichever side of it you fall on.
NULL isn't even equal to itself
[https://pgexercises.com/](https://pgexercises.com/) Check that website, its postgresql but has a very nice exercices section from basic to recursive queries and has all answers explaining them... Good luck!
You might have to have a garment and parent garment relationship in the table, if for instance you have a vest that's part of a three piece suit, or you have the same cut used on multiple garments. 
An all-column index is just a sorted copy of the table. That would probably be dumb in most cases. I'm talking about a two column index of a presumably many column table.
So I am a novice with SQL but wouldn't this answer depend on more context? For one if you are joining tables, the duplicates could be a result of your join. More specifically either a bad join(Cartesian) or you need to add some criteria to your WHERE. Or you could select distinct. Am I totally wrong here?? I appreciate any response! This is a good test for myself haha!
Well, they are going to ask, ‘How would you get rid of duplicates if every field is the same?’ Identical records were inserted by accident, now you want to get rid of one of those.
You can select `count(*)` and group by all the columns you want to look for duplicates in and then filter with `HAVING count(*) &gt; 1`. If you want to delete it I'd find it with a window query and `row_number` using a partition probably, (note you don't want rank or dense_rank here) but there are a few ways. You can also just try to make a unique index and see if it fails, but that won't get you very many points in an interview :).
This is a meme subreddit now?
Oooo good stuff. Thank you!
I hope so.
Well, Oracle and SQL Server are similar in that neither have anything to do with decibel.
Just do big data in the mangos!
I got you.
You can execute a runaway query if you accidentally included a cartesian join. Some commands can set locks that are undesirable as well. So, the short answer is yes it’s possible. This is one of the reasons why production systems and reporting systems are normally kept isolated with a ETL process of some kind. Out of curiosity, what problem are you trying to solve or address here?
As long as it's not MySQL 4.
&amp;#x200B; ;WITH Data\_Unpivot AS ( SELECT PersonId, ColumnName, ColumnValue, Row\_Number() OVER (Order by ColumnName Partition by PersonId) RowNum &amp;#x200B; SELECT PersonId, A\_amt, B\_amt, C\_amt FROM Person )x UNPIVOT ( ColumnValue FOR ColumnName in (‘A\_amt’, ‘b\_amt’, ‘c\_amt’) ) PVT &amp;#x200B; ) , Data\_Unpivot\_Modified AS ( SELECT PersonId ,’Award’ + CONVERT(NVARCHAR(100),RowNum) NewColumnName , CONVERT(NVARCHAR(100),LEFT(ColumnName,1)) NewValue FROM Data\_Unpivot UNION ALL SELECT PersonId ,’Amt’ + CONVERT(NVARCHAR(100),RowNum) , CONVERT(NVARCHAR(100), ColumnValue) FROM Data\_Unpivot ) &amp;#x200B; &amp;#x200B; SELECT PersonId, Award1, Amt1, Award2, Amt2, Award3, Amt3 FROM ( SELECT PersonId, NewColumnName, NewValue FROM Data\_Unpivot\_Modified ) X PIVOT ( MAX(NewValue) FOR NewColumnName IN (‘Award1’, ‘Amt1’,’Award2’,’Amt2’,’Award3’,’Amt3’) ) Pvt
When you want the scoop on them. Hit me up. Especially if/when one of your @table loops slow down (disk IO), I'll convert it for you so you can WITNESS ME. No disrespect, you know me. Legit, I will show you all the things if you are ever interested. 
ANSI! FIGHT ME! Lol
It's entirely possible for a horrible query to crash a DB. I've seen it done. A customer facing DB was down for a few hours because Dan P. is a dumb ass. And also because the db was designed by a monkey throwing shit at a keyboard. There aren't any additional monetary costs to providing additional people access to a DB, but there are additional processing costs. More queries = more processing time. That can degrade performance to an unacceptable level. Your company can build a reporting DB that is basically a mirror of the prod DB. This can cost serious $$$$. They can give access to a limited number of trusted people. They can give limited access to a larger number of people. How that access is limited depends on the DB. They can setup a BI type environment. There are open source or paid solutions. &gt;all the reports are static where the user has to export the data from the system and run multiple lookups and formart them. As long as you can export the data, the rest can be automated... one way or another. Give me Excel and the data and I'll automate damn near any final report distributed to anybody you want. 
I am trying to convince our IT department to provide ODBC access as I have seen the benifits it provides with reporting in my previous firm. I will read up on ETL and will suggest such setup in order to avoid any issues which may arise with a faulty query. Is setting up an ETL complicated process or does require additional investment? Thank you!
I once had a problem where I had added the same set of 100,000 records to a table with over a million records. Whoopsie. Normally if I do something like that I can just use a flashback query to select what the table was before I fucked it up, but in this case it was way after my retention period so it was fully commited. I don't recall exactly what I did, but I wanted to preserve the original rowid, so it involved partitioning into sets of rows and sorting by rowid and assigning a row number to each partition, then deleting where row was greater than 2, while leaving row 1 alone. If someone asked me an interview to write it on a whiteboard, I'd fail. Life is too short to memorize complex one-off fixes for stuff.
They already have a BI tool (not the popular ones like Power BI or tableau) and I am not sure what are its capabilities as I have not used it yet, will try tinkering with it to see what it offers. I am fine with if they provide limited access to certain persons only. However, I was not aware that setting up ETL process would expensive. I currently use VBA to automate but still I feel things would be much simpler when we can process live data on the fly using the power of server. Thank you!
In my DB Administration class in college, they said to learn Oracle because if you know how to administer Oracle, SQL Server is easy to learn. But going the other direction is not so easy.
can you or someone elaborate? is there something with oracle? I am currently using oracle.
The costs of your ETL workflow can be minimized just like any other cost. It needen't be prohibitively expensive.
ODBC bringing down whole system?! This is all or nothing type thinking and the truth is between 0 cost ( nobody uses it) and whatever production downtime for n seconds costs your entity (deadlock, crash, etc..) Establishing clear boundaries, define examples, best uses, and problem areas. &amp;#x200B;
Could you please let me know what are key drivers for cost are. Like, is it size of DB or the number of users or is it based on the functional requirements.
When there are reporting needs, usually (at first, any way) that reporting doesn't need to be real-time, and it doesn't need to be high performance. It could be refreshed nightly by an ETL process, which just means you run queries against the production data to create the minimum necessary data sets needed for the reports and you load them into your read-only reporting instance. &amp;#x200B; Everything you have costs something, whether it be in terms of equipment, licensing, maintenance, or even lesser-considered aspects such as documentation and asset tracking efforts. How costly and complicated are questions that can only be answered by your IT Department. &amp;#x200B; Have discussions with them, but start with explaining the business problem you're trying to solve or process you're trying to improve. If you start with "I need you to install a new instance of SQL Server on a server with 1TB of storage, and I need an ETL process set up to load data into these reporting tables I sketched out..." you're going likely to get resistance. Explain where you want to end up in terms of functionality and let the rest be an open discussion.
Thanks to you and OP for the question and answer. I wanted to say VARCHAR but only because that is what I am used to seeing and not because I knew why.
I dont think so - it might be that OP has not expected the join to work like it does. Look @ this response https://www.reddit.com/r/SQL/comments/aqx7s8/technical_database_question_why_do_databases_give/egjmlxg/ 
I believe they already have an ETL process or something similar as there is a BI tool which generates reports nightly. Will check with them. Thank you.
From my personal experience, it just seems like SQL Server has a lot of nice bells and whistles/ nice to haves. I don’t use Oracle often but things like SELECT TOP 100 vs WHERE ROWNUM &lt;= 100 come to mind. To me, the syntax just makes more sense. In recent versions of SQL Server you can use something like: SELECT FORMAT(GetDate(), 'yyyy-MM-dd'); I forget what the Oracle equivalent is, but I’m sure it would probably irritate me.
I jump between Oracle and SQL Server throughout the day everyday and the tiny differences are infuriating. 
[removed]
Interesting tutorial for sql. 
To_char(sysdate,'yyyy-Mm-dd')?
I think it was more in terms of administration tasks rather than than the SQL syntax. SQL Server can be run right out of the box, while Oracle requires more knowledge about the under-the-hood goings on to get it running smoothly.
is the day number relative to some starting date? SELECT calendar_date , DATEDIFF(DAY,'2019-02-14',calendar_date) AS day_no , SUM(ShowedUp) AS total , SUM(ShowedUp) / DATEDIFF(DAY,'2019-02-14',calendar_date) AS avg FROM Appointments WHERE status = 'Confirmed' GROUP BY calendar_date
The differences aren't tiny. 
I am starting right now on sqlzoo, i am leting you know how things go :D Thank you
It actually did. It's very slow with &gt;5000 rows, but I will deal with that somehow myself. I think recursive CTE's overall are not really performance friendly so it's just something to bite through
When the process is fully done I was going to do a post up on it and then possibly think about going to a cursor and then start breaking it all down for SSIS.
Ugh, I'm so dumb.
A lot of the syntactical differences are. 
The solution that I follow, and that I would suggest: Make a table that holds all of your tables, with a FK to a table that holds all of your databases. &amp;#x200B; make a relationship table with a relationshipType table (I go Type, Family, Class, Realm.. but that's another story) Fill them up, in the relationship table under type TABLE DIRECT, put the relationship data in there. Cluster on Type, Parent, Child. Run the CTE and put the results in there under TABLE DESCENDANT Relationship Type. Now.. you dont have to run the CTE until your model changes. Its a straight lookup. normal Parent Child lookup becomes: Select Child from Relationship where Type = Table Direct and Parent = @Parent &amp;#x200B; and the CTE becomes: Select Child from Relationship where Type = Table Descendant and Parent = @Parent
Some of them, sure. But it's the semantic differences that'll get you.
I am not sure this course is for you https://www.udemy.com/ms-sql-server-70-764/ Hope that helps.
Absolutely. Most of my SQL work is SELECT based. I imagine the differences increase exponentially from a DBA perspective.
I'm not sure of the exact situation but if you just need the day number in the month you could do something like this: &amp;#x200B; SELECT calendar_date, SUM(ShowedUp)/DAY(calendar_date) AS [avg] FROM Appointments WHERE [status] = 'Confirmed' GROUP BY calendar_date &amp;#x200B; If you need just a running count of the days in the month where the status is confirmed then you could do something like this: &amp;#x200B; ;WITH cte_row AS ( SELECT calendar_date, ShowedUp, ROW_NUMBER() OVER(PARTITION BY calendar_date ORDER BY calendar_date) AS row_num FROM Appointments WHERE [status] = 'Confirmed' ) SELECT calendar_date, SUM(ShowedUp)/MAX(row_num) AS [avg] FROM cte_row GROUP BY calendar_date &amp;#x200B; I didn't test out this out as I'm on mobile but it should be a starting point. If you still run into problems let me know and I can load some dummy data when I get home and test it out.
You could also use SUM instead of the Row_Number. Just specify the correct frame.
Even at the DML level, the semantic differences are pronounced.
No brackets `select to_char(sysdate, 'YYYY-MM-DD HH24:MI:SS') as the_date` `from dual; -- can't ignore the from dual`
&gt;while Oracle requires more knowledge about the under-the-hood goings on to get it running smoothly. Oh yes Tablespaces...
Right. I use Oracle every day, yet it's so unnatural to type on Reddit mobile :D
Might he want you to break it into multiple tables. An actor table and a movie table for example? That way each actor has his/her own line.
Because right now it has a lot of redundant data. Each star is listed multiple times along with their data like birthday and nationality which is redundant. You should break it into a movie and actor table where you only have each actor and all their details one time with a primary key and same for movies and then link them to some kind of main table
It's not. It needs to be chunked up into at least three separate tables. Read through the Wikipedia article on normalization. It's got some examples that would probably be helpful for you.
&gt; I thought the whole point of normalizing was so every attribute was fully atomized (can't be broken down further). that's only First Normal Form there are five other levels
There are multiple names and nationalities listed. You want at least an actor table and a nationality table and have each row refer to the each by a foreign key instead of having it listed out multiple times in the table. That way say, an actor gets married and changes their name (for example) and instead of having to update each row where their name appears, you only have to update the one row where the name is changed then every reference to that name (via foreign key) will be updated without having to find every instance of the name in the database.
Oh I see, we only covered 1nf; maybe I missed something in the text though. Thank you 
No, this is not really close to fully normalized yet. I could see this being a lot of tables depending on how picky and how flexible you want it to be. For instance, one for film name and length, one for the actor name and birthday, one for countries, one for roles, and then the linking tables based on their primary keys to link actor to film, actor to country, actor to role, film to country, film to director, etc... Because you can have multiple directors for a film, multiple countries involved in production, actors who play multiple roles, and so on. In a real world database, you would even leave length out of the film name table and have a film to release type linking table, for films that had different length cuts depending on release (DVD, VHS, BD, theatrical, TV, one country vs another, etc). But that's not data that is supplied in the example, I'm just giving you an idea of just how many possible different element relationships could be at play in a truly normalized database.
Here's how you could do it by way of the Standard. Depending on your version of MSSQL you might be able to do something like this. WITH Summary AS ( SELECT calendar_date, SUM(ShowedUp) AS amt FROM Appointments WHERE status = 'Confirmed' GROUP BY calendar_date ) SELECT ROW_NUMBER() OVER( ORDER BY calendar_date ) AS day, amt, AVG(amt) OVER ( ORDER BY calendar_date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW ) FROM Summary
I see, so I am very wrong haha. Thanks man, the examples helped 
Hey, don't sweat it. Normalization is more involved than many people realize early on, but is definitely worth the time to learn. It's also sorely lacking in some very large, commercial applications because many software devs who did not specialize in database design simply do not realize how important it is, so you're headed in the right direction.
Database Normalization is about breaking tables into relevant sections. What I envision he is asking you to do is to take this single table and break it into different tables, &amp;#x200B; i.e there should be one country table, with a country id. Said table could be used to populate both ethe nationality and country fields. &amp;#x200B; then I would have something like a years table with year id, then you could populate both your year and birth year fields from a single table. &amp;#x200B; then I would have another table that stored the film title, with all the roles in it with a role id. &amp;#x200B; Finally then your "master" table that presented this info would just be a bunch of correlated integer ID's. &amp;#x200B; You do this because pulling a bunch of integer ID's when stuff gets big is a lot less memory intensive then pulling a bunch of plain text strings. And to a computer the id is as good as the plain text string. Then if you ever wanted to output info for a human you have ID's that can be used to generate the plain text strings, but if you just need the computer to catalog stuff in the background you are using way less memory just passing integer ID's around instead of a bunch of strings. &amp;#x200B; Lastly - the obvious problem I see with that table is that there is not any unique ID on the table. Generally, you want/need some means of uniquely identifying records in a table, whereas this table, at scale is just a big jumbled mess that doesnt play nice with the general best practices of database use and administration &amp;#x200B; With a table this size its pretty meaningless - but its the idea that as sets get really big, strings are an inefficient way to id something in a table over an integer ID
Without knowing what degree of normalisation your professor wants, this question cannot be answered. As you noted, the table is already in first normal form (1NF). However, it is not in second normal form (2NF).
That master table would be more akin to a view than a view.
There is another benefit to normalization that's often overlooked. By avoiding duplication you're also creating validation or "lookup" tables that can be used to validate incoming transactions through foreign key and other value constraints. I've seen way to many poorly designed dbs where a lack of referential integrity has led to really poor data quality.
1NF form is fully atomized, but there are still partial and transitive relationships between this data. Depending on what degree your professor wants you to normalize to, you can redesign the data into tables that eliminate partial dependencies (2NF) or to eliminate the transitive dependencies as well (3NF). These designs have different advantages depending on your goal. If you are going to have high I/0, you can see you have alot of redundant values that could be stored more efficiently behind more primary keys and having less values to insert. The trade off is that your data is more segmented and you will have more joins to perform to retrieve all of it in a query which will affect query performance. Because both of these aspects are important, you will need to design around the business requirements you are given to properly infer what logical database design is best
Those factors are dependent on your entity's current situation and growth plans. Overall, costs should be less important than the benefits like increased capacity, additional report output, reduced development costs on new BI software or user licenses for current software. &amp;#x200B; If your entity's cost is unknown and the benefit is unknown, it helps to have a sandbox to explore possibilities and solidify the resources required and timeline to achieve your objective.
I thought Nicole Kidman was Australian?
You could possibly run a sub query if you wanted to update all of the traction entries by accountcode Or if you always wanted the last one for each transaction entry you could look to use a LAG window function and just slap it as a view over the table. https://docs.microsoft.com/en-us/sql/t-sql/functions/lag-transact-sql?view=sql-server-2017 LAG(trans_date,1) OVER (PARTITION BY accountcode ORDER BY trans_date ASC)
&gt;Life is too short to memorize complex one-off fixes for stuff. I'll drink to that! I've long since stopped trying to know how to do everything, now I'm content with being confident that I can figure it out. 
You need brackets around the parameters CREATE PROC GoDoSomeStuff (@SomPar BIT) AS
Just random software dev here, do you have any recommended database design resources? I'd like to optimize my tables but as you said, that's not my specialty lol
I changed my query to include brackets but still getting error : Error: PLS-00103: Encountered the symbol "@" when expecting one of the following: &lt;an identifier&gt; &lt;a double-quoted delimited-identifier&gt; current delete exists prior Line: 2 Text: (@distance float, &amp;#x200B; Here is my updated query: CREATE PROCEDURE uspCalculateTime (@distance float, @velocity float) AS BEGIN IF (@velocity &lt;&gt; 0.00) SELECT @distance / @velocity; ELSE SELECT 0.00; END GO
I wouldn’t use the @ symbol in your parameter names in pl/sql. I could be wrong but I also think you may have to add “from dual” to your select statement. Been a while since I’ve done any pl/sql lol.
[removed]
Nice of you to do their homework.
I always like to explain normalization to new people as "only store any given piece of data once, unless you have a good reason not to". In this case, there's lots of dupes in film_star. There's a few columns that won't ever change that are related to film star, so those should probably be grouped with the film star. So table 1: film_star birth_year gender Film star is just their name, makes sense to store it once here. Birth year is literally just an integer, keep it here as well. Databases store integers really efficiently. Gender - if this is the only place you're going to use gender in the database, a single char is fine in this table. Nationality is a bit more tricky - in this case, film_star to nationality is a 1:1, so it won't hurt to have nationality in this table. Now for table 2: The film title is unique, so it goes in here. It shouldn't go with table 1, because each star is in multiple films. The role in this data set is unique, so let's include it. The country of the film shouldn't have more than one value, same with year and length, so include those in table 2 as well. Now I'm sure you're thinking director should go with the film, right? Each film only has one director! If you look carefully, Singer has directed two films in this data set, so let's go back to "only store any given piece of data once". As a director can direct more than one film, and does in this example, you should make table 3 now. So table 3 only gets the director. Now this data set is fairly normalized. As others have pointed out in this thread, *how* the database is used in the real world makes loads of difference in how you model it, but from a pure normalization standpoint, with real world knowledge, I'd break it down into: Gender id gender Nationality id nationality Country id country Actor id film_star birth_year gender_id nationality_id Director id director nationality_id (not in this data set, but let's be pros here, and anticipate future needs!) Film id film_title year length country_id director_id Film_Actor film_id actor_id role (this is a bridge table - each film has multiple actors, and each actor has multiple films. This is the best way to maintain normalization in a many:many relationship) 
I assume you want to do this for your entire table... So. DECLARE @Cursor\_account BIGINT DECLARE @Cursor\_TransactionType NVARCHAR(100) DECLARE @Cursor\_TransactionDate DATETIME2(7) DECLARE @Cursor\_TransactionId BIGINT DECLARE @TransactionDate\_LastReload NVARCHAR(100) DECLARE @Account\_Current NVARCHAR(100) \-- Using STATIC because you are updating the same table. DECLARE Transaction\_Cursor CURSOR LOCAL STATIC FORWARD\_ONLY READ\_ONLY FOR SELECT TransactionId, Account, TransactionType, TransactionDate FROM Transaction ORDER BY Account, TransactionDate ASC \-- try and get the accounts next to each other with a date OPEN Transaction\_Cursor FETCH NEXT FROM Transaction\_Cursor INTO @Cursor\_TransactionId, @Cursor\_ACcount, @Cursor\_TransactionType, @Cursor\_TransactionDate WHILE @@FETCH\_STATUS = 0 BEGIN IF @Account\_Current IS NULL BEGIN SET @Account\_Current = @Cursor\_Account END IF @Cursor\_Account != @Account\_Current BEGIN \-- If we switch accounts during the cursor, update TransactionDate with a default date and set @Account\_Current to the appropriate value SET @Account\_Current = @Cursor\_Account SET @TransactionDate\_LastReload = '1/1/1900' END &amp;#x200B; IF @Cursor\_TransactionType= 'Reload' BEGIN \-- if we are a reload... update the TransactionDate SET @TransactionDate\_LastReload = @Cursor\_TransactionDate \-- If you want to update this new column for this type.. add update statement below (you have the @Cursor\_TransactionId, so its a PK update) END ELSE --if its not a reload type BEGIN \-- update that new column with the last reload date. UPDATE Transaction SET NewColumn = @TransactionDate\_LastReload WHERE TransactionId = @Cursor\_TransactionId END &amp;#x200B; FETCH NEXT FROM Transaction\_Cursor INTO @Cursor\_TransactionId, @Cursor\_ACcount, @Cursor\_TransactionType, @Cursor\_TransactionDate END CLOSE Transaction\_Cursor DEALLOCATE Transaction\_Cursor &amp;#x200B; Perfect use of a cursor here.
They still have to know what level of normalization is required for the assignment and adjust accordingly. If they just copy what I said verbatim, they'll either have too few tables or too many.
Is this all in sql?
The biggest challenge for me in implementing normalization in a database is making sure inserted records are inserted in the correct order, and foreign key references are set up at exactly the right time when records are added to the database, to avoid conflicting with any restraints in the DB. I've found having to develop logic around this to be the biggest challenge so far. Note that I'm working in a lower level analyst role currently (out of college for only 18 months or so, finally working my first real development job....more of a DBA role than a developer role though I'm finding out). Ensuring referential integrity is maintained throughout the insertsion and update processes has been a bit of a mindfuck. I mean, I get the basic ideas and understand how I want to get there, but making sure every atomic transaction is valid within all the setup constraints is fairly challenging. I'm learning a lot though.
This gets into transactional design vs reporting design. Most Data Warehouses are going to be for reporting, and follow a Star or Snowflake schematic. Those are not even bad in terms of starting out, the key with normalization that I was taught is eliminating duplicate data. You cannot always remove it, but for example the column that is USA could just be an integer. That integer corresponds to a country table, so it reduces that data. I would look up some data warehousing ideas and designs of fact vs dimension tables, that can help.
Are these parameters being set somewhere else and passed through? Also, what are you using (MS Sql, My Sql, etc) as the reason I ask is syntax is slight different. The bulk is the same, like English, but accents change a little of each. Do you even need parameters? Are these values in a table, if so I would just call them and do a case statement. If you are wanting to calculate normally, I would change it to a function instead. Otherwise for me, I would declare and set the parameters. I also don’t think you need the if it does not equal, because I would ISNULL the values instead. 
It's valuable that you have a database with strict constraints to work through as having them in place is one of the most efficient ways in learning and maintaining referential integrity. In my experience, it's been an unfortunate reality that constraints often find themselves disabled due to a lazy or sloppy developer who "just wants to test data in a single table." The downwind effects of this can be a nightmare to cleanup. A few things that may help: merge statements, cascade updates/deletes, if not exists then insert. Triggers can be helpful as well depending on the volume of data your consuming. 
Who downvotes this? This is a good comment. I mean, other than maybe kinda giving the answer to OP's HW question. But that's not on you, that's on OP.
I'm actually developing a total green-field database project on my own and making sure I set up and stick to my constraints right from the start =). I also need to implement an Access DB front end interface to provide to others in my org to allow them to modify/update data. Like I said, this is a completely new project and I'm the main developer here. Access is definitely not my most preferred front end solution, but it's actually really powerful in the right contexts in what I can do for internal company solutions. If you're not an app company but still want to be able to streamline data update procedures, Access can really help you deploy your project without any complicated configurations or web servers or sharepoint apps to deal with. I do understand that Access is probably not the best solution, but for a single developer it really provides you with a lot of tools to quickly deploy a project. However developing a DB from scratch, all the ETL processes, and a UI form interface is a lot to handle for one person. I probably should've asked for more money (but it's a non-profit company, I'm making 48k in upstate NY Buff/Roch/Syr area) 
Are you wanting people just missing Germany, or missing anything in the Favorite Country table? I would not do a not in, I would do a left join where it is null. This is a great visual and code template: [http://i.imgur.com/1m55Wqo.jpg](http://i.imgur.com/1m55Wqo.jpg) &amp;#x200B; If you are wanting to gather anyone in the Person table that is not in the other table. It would be something like this: SELECT \* FROM Person per LEFT JOIN favoriteCountry fc on [fc.ID](https://fc.ID) = per.ID WHERE 1 = 1 AND fc.ID IS NULL &amp;#x200B; If you are able to do this, I would edit those tables to make a little more sense. My examples are below, but basically the Person table should have personID, firstName, lastName. Where favoriteCountry would have favoriteID, personID, and countryName. This way the personID is the foreign key from the Person table. 
The query seems fine in terms of syntax and I believe you're not getting error when running it. So I'm guessing that the issue is that the [person.id](https://person.id) identifies the person, the [country.id](https://country.id) identifies the country (at least that what logically makes sense). This means that the [person.id](https://person.id) should never match the country's id. You should add the [country.id](https://country.id) to the person table as a foreign key (eg: person.fav\_coutry\_id) and compare those.
In my experience it is not disabled due to just wanting to test data in a single table, but because of "efficiency". As you create constraints, and to go into the issues with correct order and foreign key references, that is where you can have triggers created.
I am supposed to use a database that I can't do any changes on. I am well aware that NOT IN is not a very efficient way of doing this task, but that's the only option I've got. (NOT EXISTS could work?) My bad. I should have Alaborated a bit better. There are 100 people and every person is supposed to have selected 8 favourite countries. What I'm trying to find is the people who have not selected Germany among their favourites.
I deal a lot with spreadsheets and testing files, that can change. We used to pick certain columns to ETL; I have since changed that process to create a "data lake". I have a database that is as generic columns, and another table has all my column layouts. So they get uploaded to this table, and then the specific data gets ETL. The nice part is when they come back and say, we now need column XYZ, the data is there and saved. I do my own normalization with a dictionary and definitions table, that way duplicated data is only integers.
You are absolutely right that those two ID's should not match. I will try your suggestion as soon as i got my SQL workbench up and running
This. &amp;#x200B; There is also space, indexing, and joining for efficiency. Having an integer identity/key is a lot better than repeating 100 characters a couple thousand times. Also, I dislike having to match on text fields. Having drop downs in front end now for data that you have in those look up tables instead of free form text boxes is another big key. 
This is what I initially thought, but in looking at the way it is setup it is not the Country table. It is the Favorite Country table, which the ID should then be the foreign key, if they are not the same ID and as you said in my response of not making edits. I am not positive on how you are going to fix that. Your NOT IN should work fine, with our response to my comment of having 100 people and each having 8 options in the table. NOT IN, NOT EXISTS, and LEFT JOIN can do all the same results, just different in efficiency: [https://explainextended.com/2009/09/15/not-in-vs-not-exists-vs-left-join-is-null-sql-server/](https://explainextended.com/2009/09/15/not-in-vs-not-exists-vs-left-join-is-null-sql-server/) &amp;#x200B; Again, the reason I said to update the table and column names; was because of the very issue that was mentioned. I originally thought that they were different and should be different. Honestly if it was a DB I was building, I would have the following tables: Person (personID int, firstName varchar(50), lastName varchar(150)) Country (countryID int, countryName varchar(200)) favoriteCountry (favoriteID int, personID int, countryID int)
I noticed this as well. You need to declare the variables you're trying to use first.
Since you are only wanting Germany, it might not be working because the Person ID (100 of them) are in the other tables 8 times (800 total rows). With your query above (assuming ID is the same in each table). You are not qualifying not being Germany. So my logic is you want to select everyone that IS IN a query where it does not equal Germany. I also added Ditinct, because it will bring back all 8 of their rows. SELECT Distinct name, lastname from person WHERE [person.id](https://person.id) IN (SELECT [favoutiecountries.id](https://favoutiecountries.id) from favouritecountries where name &lt;&gt; 'Germany');
First step would be this! There is actually a great post on Stack Overflow for generating a data dictionary: [https://stackoverflow.com/questions/6487885/generating-data-dictionary-for-sql-server-database](https://stackoverflow.com/questions/6487885/generating-data-dictionary-for-sql-server-database) I run this once a month, save the results to an Excel document as our database gets updates of new tables and columns. It allows me to filter based on a column name, and see all tables that have that similar name. The second step, as someone else pointed out was stored procedures. Personally what I would do is go through and just look at stored procedures, views, functions, and any queries that are saved. Take note of the tables and joins, you start to see a pattern. I would "rebuild" queries, so I am writing it to get it to memory of the joins and adding comments. Where ID = 957, I would go and find what that ID means. We do a lot where endDate is null or endDate &gt;= getdate(). I have a comment to show this is active. &amp;#x200B; Even to this day another thing I do, (being a MS SQL shop) is: select top 10 \* from table \-- write all the columns, or columns I need and use I do this as I am building stuff. Say I am in Table X, go to the data dictionary and see 5 other tables with that column name. I will go and pull those above, to see the column names in the table, and how the data "looks".
She is. That's got to be worth extra points. 
You don’t need separate tables for stars and directors. Just a “person” table and then another to indicate their job type. Because the same person can be both actor and director. 
Good morning, I’m a newbie about 1 month in- the following is what I have done... can anyone tell me if I’m way off or semi on track. I added 4 tables to Access for my needs and wanted to share with a co-worker, I discovered Access was no longer supported by Sharepoint so I thought I would move the data to a SQL Server. I successfully added the tables to SQL Server Studio but was unable to get Sharepoint speaking with SS of course so I added Sharepoint Designer 2013. At this time I have SD acting as the mediator between Sharepoint and SQL Server but unable to get them to Sync for updating one another. In designer I can see my SQL DB having beed added as well as see my Sharepoint SQL Lists being attached… Any thoughts on Syncing issues? Thank you, any direction is welcome.
The advice you've been receiving here is MSSQL-Centric. 1) Oracle does not accept "@" to indicate Parameters or Variables 2) Oracle does not allow SELECT commands without FROM. There is a table called DUAL which serves the purpose, however it is not needed in this case. 2a) Just like MSSQL uses SET to set the values of variables/parameters without the need for a table, ORACLE has this functionality as well. 3) "usp" is not a stored procedure indicator in ORACLE, but you're more than welcome to use whatever naming syntax you'd like. 4) Brackets are not acceptable means of indicating a column/object. 5) The datatype you're looking for is NUMBER (which houses intigers and decimals) CREATE OR REPLACE PROCEDURE uspCalculateTime (pDistance IN NUMBER, pVelocity IN NUMBER, pResult OUT NUMBER) AS BEGIN IF (pVelocity &lt;&gt; 0.00 AND pVelocity IS NOT NULL) --Added a NULL check here THEN pResult := pDistance/pVelocity; --This is like the SET command in T-SQL. ELSE pResult := 0.00; END IF; END uspCalculateTime;
This looks good, I will give it a shot, yeah "gift card" is "account" in the table so i was hoping to do that: sort by account,date and have a script that just recognizes that the number has changed then reset last used date.(although may have issues with cards that were sold before the data set that I pulled, I could only get 7 years (about 120k transactions)). It's funny how I look at this statement and say "this guy must be some sort of a sql wizzard" I am a sys admin that dabbles in sql statements a few times a month, just to get what you have here would have taken me all day, thank you!
Seconded. I would love to find something like SSMS for BQ. 
When you select from two tables without your query establishing the relationship between them, it results in a cross join (every combination of rows from the first table with rows from the second table). You're then filtering the cross join to show employee rows matching your query, but those employ rows are still crossed with every region row. What you want is an inner join: SELECT e.employee_id, e.region_id, r.country FROM employees e JOIN regions r ON e.region_id=r.region_id WHERE r.country IN ('Asia','Canada');
you don't need favoriteID make the PK a composite key consisting of `( personID, countryID )`
Worked perfect thank you. The online course I decided on is done in T-SQL and I use PL/SQL at home and work. I didn't think the differences in the two would be a big deal but I'm seeing now that the syntax is different. I will have to look for a course on stored procedures solely for Oracle. Thank you for you response much appreciated.
If you want to keep the same syntax structure: &amp;#x200B; `SELECT e.employee_id, e.region_id,` [`r.count`](https://r.country)`ry` `FROM employees e, regions r` `WHERE e.region_id = r.region_id` `AND` [`r.country`](https://r.country) `IN ('Asia','Canada')` &amp;#x200B; Since you are using country to determine region\_id for your outer where clause, you can speed up query performance by simply filtering by country instead. You will still get the same result set assuming countries are unique to region\_id
But you really shouldn’t write joins like this. Please use JOIN when joining. 
Unless his prof hasn’t allowed the use of join yet :)
Is there an error returned when you run it?
Yeah, it says I have a syntax error around this line(12 down): &amp;#x200B; ") start\_count on impression\_count.date = start\_count.date"
I can give you the answer but I'm going to need a $75k starting base salary with benefits first.
I definitely follow that, but I've heard of a variety of other concepts like pivot tables for example. Generally I break data into smaller tables when applicable as of now. I'll look into the schematic concepts you mentioned as well, thank you!
Correct. 
I find it interesting you learned IN clause and sub queries before JOIN. Where do you go to school?
Profs don’t always make you do what you should at first
it's a whole lot easier to see where the error is if the query is formatted a little bit SELECT start_count.date , start_count.start_count , end_count.end_count , total_sharings.total_sharings FROM ( SELECT DATE(CONVERT_TZ((viewings.created_at),'GMT','US/Eastern')) AS date , COUNT( IF( offers.interface_class = 'Boomeo' , IF( duration &gt; 0, 1, NULL ) , IF( duration is not null, 1, NULL ) ) ) AS start_count FROM viewings JOIN offers ON offers.id = viewings.offer_id WHERE offer_id in (128639) AND viewings.id &gt;= 2827665250 AND viewings.id &lt; 2827665260 GROUP BY DATE(CONVERT_TZ((viewings.created_at),'GMT','US/Eastern')) ) start_count ON impression_count.date = start_count.date LEFT JOIN ( SELECT DATE(CONVERT_TZ((viewings.created_at),'GMT','US/Eastern')) AS date , COUNT(*) AS end_count FROM viewings JOIN offers vid ON viewings.offer_id = vid.id WHERE offer_id in (128639) AND viewings.id &gt;= 2827665250 AND viewings.id &lt; 2827665260 AND duration &gt; (length - 2) GROUP BY DATE(CONVERT_TZ((viewings.created_at),'GMT','US/Eastern')) ) end_count ON impression_count.date = end_count.date LEFT JOIN ( SELECT DATE(CONVERT_TZ((s.created_at),'GMT','US/Eastern')) AS date , COUNT(*) AS total_sharings FROM sharings s JOIN viewings v ON s.viewing_id = v.id JOIN sharing_types ON sharing_types.id = s.sharing_type_id WHERE v.offer_id in (128639) AND v.id &gt;= 2827665250 AND v.id &lt; 2827665260 AND sharing_types.category NOT LIKE "%ad_choices_icon%" AND sharing_types.category NOT LIKE "%Video Click%" GROUP BY date(convert_tz((s.created_at),'GMT','US/Eastern')) ) total_sharings ON impression_count.date = total_sharings.date look very carefully at exactly **where it says the error is near**
You join to "impression\_count" a few times but it's never "from"med. Like, if it were "from impression count" on line two, it'd have a chance.
I took a class through Udemy called Master SQL for Data Science taught by this guy: https://www.jobreadyprogrammer.com/p/master-sql-for-data-science Which academic programs would you learn SQL in? CS? 
From employees AS e INNER JOIN regions as r ON e.region_id = r.region_id Add that and it will work
I can spot lots of them.
You want WHERE person.id NOT IN( SELECT person.id FROM favoritecountries WHERE country.id NOT *(Germanys ID goes here)*
Using comma instead of JOIN is a throwback to the olden days. I'm sure someone can explain why people used to do that, but it predates my 15 year career. Now that you've got JOIN down can I recommend CTEs? [https://youtu.be/ZXB5b-7HJHk](https://youtu.be/ZXB5b-7HJHk)
Probably it got downvoted due to some of the inherent design flaws. For instance, the film table is inherently limited by putting title, length, year, country, and director all on a single row. Many films have more than one director, so you can't put that in the same row as the film definition (title) without repeating the rest of the definition. Now, you could make a linking table for additional directors and use the one in the film table as the primary, but why make your joins more complicated (and conditional) than they need to be instead of just using a linking table for everything in the first place? Basically, my problem with this "answer" is that it works swell for the supplied dataset and *only* the supplied dataset. Designing your storage structure to meet the exact requirements of only your current data is often a very poor idea. Sufficient for a classroom, maybe, but not really useful.
 select start_count.date,start_count.start_count, end_count.end_count, total_sharings.total_sharings from (select date(convert_tz((viewings.created_at),'GMT','US/Eastern')) as date, COUNT( IF( offers.interface_class = 'Boomeo', IF( duration &gt; 0, 1, NULL ), IF( duration is not null, 1, NULL ) ) ) as start_count from viewings join offers on offers.id = viewings.offer_id where offer_id in (128639) and viewings.id &gt;= 2827665250 and viewings.id &lt; 2827665260 group by date(convert_tz((viewings.created_at),'GMT','US/Eastern')) ) start_count on impression_count.date = start_count.date You have an ON clause but you're not joining any tables at that point.
Don't use cursors where window functions can solve your problem much faster and with far less code
You can do that. I personally like to have an identity columns on all my tables, which is my primary keys. 
You’re very welcome. I learned in T-SQL and now administer and develop on both. It takes a bit for it to “click” but you’ll get there. The best analogy for me personally is Spanish and Portuguese (maybe American English vs UK English or even Patois/Pidgin) but everyone sees it differently. It’s mainly dialects of the same language. You got this Chief.
all in MSSQL
I did not know you can use if statements! Still relatively new
This would also need to filter Asia and Canada 
Oof at $50k maybe I’m underpaid
I am a wizard... =) Don't listen to the anti-cursor people. Let them fight to try to get their SQL optimized queries... to actually optimize. A join and a window function are just a preprogrammed cursor. For each row increment/check condition. However, these functions and joins are optimized by the SQL engine. you can get a Left join to create an inner join execution plan. You are left to optimize the cursor... so if you are not practicing code reuse (if your procedures are more than 500 lines of generously formatted code)... you are going to have a bad time (and then hate cursors). &amp;#x200B;
Also, if you want to remain “in sync” at home I’d suggest downloading and installing sql server developer edition or SQL server express at home. Both are free. Oracle is a great tool to have in your toolbox, but they are quite different engines so it may be a bit confusing.
Please provide more information regarding your experience, then we may be able to help with the wording. Overall, "SQL" under your list of technical skills should suffice. Under your job description you will have details regarding what you've done with that skill, it sounds like you have experience populating tables as well as extracting data to other environments (R) so I would talk about that and if you can any quantification of results is always a great idea.
Look up IIF() too. Fun stuff.
I had a csv file. I wanted to extrapolate only certain bits of data. There were two groups of insurance: personal or commercial. I just wanted commercial. So I went into sql and got only those inputs. I then used that date in R to run a few linear models. 
I always try to replace ‘IN’ statements with JOINS. You may want to try that to see if it helps.
Not to downplay what you've done, but I once wrote a simple API in C# to pull data from our 3rd party finance system. I would not use that as a reason to add C# to my resume. 
PostGres &amp; SQLite are free across the board, Oracle &amp; SQL Server have versions that are free for non-production use.
IMO using buzzwords (like R, SPSS, SQL, etc.) are great, but I'm more interested in what you did/accomplished using those tools. Don't tell me you use R to perform better statistical analysis, tell me you saved someone 10,000, or increased a marketing budget 60% based on your work, or that you did some project where you did &lt;insert something cool&gt;. Tell me what you did with it and I'll listen, but if you just tell me what you can do then I'm going to zone out. Make sense?
Comma join is the original syntax. The JOIN keyword syntax was introduced with ANSI SQL-92, the third version of the standard (SQL-89 and SQL-86 came before). SQL the language however dates back to the 1970s. A lot of people learned the comma join syntax and were reluctant to change because of the extra verbiage.
I'm not using C only sql and R. 
Do you know their names?
So if I say "analyzed data extracted from sql to run multiple linear regressions in R to prove that covering autonomous vehicles for commercial use was not a sound business decision"
That's not what I meant. It was an analogy. What I meant is that what you did is not enough to put SQL on your resume. Not trying to be condescending, btw. 
I want them to not only see "beginner level in SQL" I want something. Lol Any advice ?
No offense, not to downplay you, but you're dumb not to. You'd be dumb if you were going to apply for a C# position, but if you were interviewing with us for a job that is primarily focused in SQL, the fact you've worked in C# (and so have I) will set you apart from other candidates who either fail to bring, or don't feel that their experience is worth putting on their resume/talking about. I really don't care about C# and it has very little to do with my current role, but what it does it tell me you know multiple languages, are a techie, and that you can help out with multiple things as it relates to the job. For example, do we work with C# at all in my job? Nope. Do we have some web jobs in C# that tangentially relate to our environment? Yep. And, if they fuck up my boss asks me to look at them. Not to be a developer, but to put fresh eyes on it because he knows I've worked in C# before as well as a bunch of other languages. I actually solved a problem last week that was taking down our entire client site because in the middle of all their C# stuff their was a database job failing. The C# had determined this was not the root cause of the site outage, but I thought that it was and presented them with a hypothetical situation where it failed and other jobs were trying to execute at the same time... and begin stacking on top of each other... until things basically just froze. They took my advice, took care of the failing job by wrapping it in proper error handling, and the outage stopped. Just because you're a C# person doesn't mean you're a database person, and just because you're a database person doesn't mean you're a C# person, but I can guarantee you that C# jobs want to hear all about your experience in databases, and I sure as hell want to hear about all of your C# experience if you're applying for a database position. A lot of jobs in analytics require you to be a specialist in one or two disciplines, but as or more importantly to be a generalist in many other disciplines. One of my strengths is having a diverse background in networking, network security, firewalls, web building, telephone systems, procedural languages, social engineering, etc. -- I might not rely on those general skills on a daily basis, but they allow me to really apply my specialist skills in statistics to leverage SQL to do some cool things. 
Google for SQL Server Developer Edition, that's a free version for development/testing. 
I mainly use SSMS and I'm not familiar with asp .net Core # but I can explain the concept to you. The foreign key is not unique itself but is some unique key in another table. Based on your UML you need to add a "school subject" and "classroom" attribute to your "Class Session" table where "school subject" is a foreign key to the primary key in the "School Subject" table and "classroom" is a foreign key to the primary key "class name" in the "Classroom" table. Hope this helps!
I hate Oracle, but they have a website that you can create non persistent databases. You can save your scripts and rerun them to recreate your database. People can share their scripts too.
Do most of the commands from regular sql work?
That sounds like too much word salad, "Prevented business from making bad decision which would have resulted in $100,000 loss through analysis." That's a "dumb" bullet point for your resume, but then you list SQL and R as a tool that you leveraged at that job. So it looks like this: Company Name - Dates Title Skills: Blah Blah, SQL, R Accomplishments: Blah blah provided insights to the business for autonomous car design.
&gt; When should I put foreign key ? whenever the row that contains the FK column is **bad data** if the referenced PK row doesn't exist 
What do you mean by "regular SQL"? It has the same feature set as the Enterprise Edition of SQL Server, so all th commands from any other edition work. If you mean ANSI standard SQL, then it's about as compliant as any other database platform (which is to say largely, but not fully - each major database implements its own extensions to the standard, and may be lacking some elements of full ANSI SQL). 
What would you suggest to those who have not extensively used SQL (or at all) professionally, but has extensive knowledge of the language and also related certs? Would a blog portfolio website discussing various SQL topics like subqueries, window functions, stored procedures etc. somewhat compensate for lack of real world experience? Thanks!
off-topic: What tool did you use to create the diagrams?
The diagram was gived by my teacher, since I'm french, i jsut replaced the name into english name. But I used to use starUml once for another class.
ok, Thanks !
thanks ;)
Thanks, da_chicken. 
All your joins are cross joins as you're not specifying any join criteria. That join syntax also isn't recommended, look into ANSI Joins, e.g. Table1 inner join Table2 on &lt;criteria&gt;. You're probably then going to want to do something like SUM(calories per gram * grams), using Group by for your food. So important areas to learn for this: Joins, grouping and aggregation. Distinct isn't your friend here.
Get creative. I used to be in your shoes. I got creative. Now that I am in a position where I look at resumes or participate in interviews, I have to tell you that I really want to talk to people who have *done* something. I don't care *what* it is, per se, but tell me what you did, don't try to tell me what you *can do.* &gt;Would a blog portfolio website discussing various SQL topics like subqueries, window functions, stored procedures etc. somewhat compensate for lack of real world experience? We often interview people with no experience with the understanding that we can train them on the job. I would think you would rise to the top of the stack with a portfolio, blog, etc., that demonstrated your technical ability even if you lacked any professional skill. I put together a similar portfolio without experience and was hired fairly quickly by a group that were more than happy to "teach me" on the job. 
I wouldn't say he did it for me; but he did make this assignment a lot easier. There actually is not any constraint on the amount of relations to use/level of normalization (we haven't really covered it yet so that is probably why). This assignment seems to be more about knowing the syntax and how to create a schema that semantically makes sense. His comment was very helpful and I a working out how to implement it on my own. 
yeah good points, my professor is doing a good job in respect to this. He has stressed future proofing a database for the inclusion of more data and changing technologies
so I would use foreign keys to link tables? &amp;#x200B; So for example if I wanted to link a table for roles to a table of films, I would use the roles as a foreign key in the creation of the films table?
&gt;vView Does the v also stand for view? [RAS syndrome aside](https://en.wikipedia.org/wiki/RAS_syndrome), the SQL looks fine. What's the error?
Yes but i found the solution. Thanks though 
*Wooo* It's your **6th Cakeday** da_chicken! ^(hug)
You would need foreign keys, yes, but not quite sure it would be in the way you described. I would say you would end up with at least four tables for this sort of thing, depending how you want to do this. One for the film. Has the film name and an ID. One for persons. Has the person's name and an ID. One for film member types. Has a set of generic members like "lead actor," "extra," "director," "producer," etc, and their respective IDs. In all tables the ID is the primary key. The fourth table would be comprised pretty much entirely of just foreign keys linking to the three main tables on their primary keys (film ID, person ID, and member ID), and would be the film-person-member table. The linking table, when joined back to the main tables, now tells you who did what in what film.
Hi, thanks for your reply! Do you still have a link to your portfolio website? I know this would be some great inspiration for ideas to me. Thanks again!
I have a "blank" template set up that I have shared with people in the past. I will try to remember to link it to you in the morning, but if I don't please feel free to PM me.
You can sign up for a free trial of Azure that has $200 in credit. Download Microsoft SQL Management Server and then you're good to go.
so like this? CREATE TABLE lab3.role( rid INTEGER UNIQUE, role VARCHAR(50) NOT NULL UNIQUE, primary key(rid) ); CREATE TABLE lab3.film ( fid INTEGER UNIQUE, title VARCHAR(100) NOT NULL UNIQUE, year INTEGER NOT NULL, length INTEGER NOT NULL, primary key(fid) ); CREATE TABLE lab3.film_role( frid INTEGER UNIQUE, rid INTEGER UNIQUE, fid INTEGER UNIQUE, primary key(frid), foreign key(rid) REFERENCES lab3.role(rid), FOREIGN KEY(fid) REFERENCES lab3.film(fid) ); 
Yeah, that looks ok.
Say now I wanted to show a table that listed every film and the associated role, would the query be: &amp;#x200B; SELECT \* FROM [lab3.film](https://lab3.film), lab3.role; &amp;#x200B; No need to for any joins or anything? 
Without a join condition, that will just give you every possible combination of film and role. You would need to select from film, join to film-role, join to role.
Triggered...
ah okay so I am missing something here, why do I need to link the tables? What happens if I don't?
If I am reading it right you are using the brackets after the type as a size. You normally do not need to do this with things like integers and dates as they are fixed sized. With VarChars however this is extremely important to size correctly and something you need to think about what is the max number of characters that will be going into the field, too big and the database gets too big, too small and you end up truncating data. If we look at the Faculty Name and email as an example you have it sized to 5 &amp; 17. What happens if you used my name which is 6 characters or my email which is a good 20+ characters. You've already done this on one of the fields where they said the sample data is No but you gave it only 1 character in size.
Like I say, with no join condition, you end up with a Cartesian join. Basically, every value in the "film" table gets matched to every possible value in the "role" table. This mean whether or not the role belongs to the film, you'll get a result set where they're paired up. Film_Table entries: Peter Pan The Dark Knight Role entries: Captain Hook Batman Joining those with no condition will get you results like: Peter Pan - Captain Hook Peter Pan - Batman The Dark Knight - Captain Hook The Dark Knight - Batman
yes I understand that; I am confused on what the linking tables and foreign keys actually do 
&gt; When should I put foreign key ? Januray 12th, 2067 sounds about right.
Your FROM statement is causing a cartesian. For every record in employees, you get a record in Region. if you want to see the same results with a different query.... do &amp;#x200B; &gt;SELECT e.employee\_id, e.region\_id, r.country &gt; &gt;FROM employees as e &gt; &gt;CROSS APPLY regions as r &gt; &gt;WHERE e.region\_id IN (SELECT region\_id FROM regions WHERE country IN ('Asia', 'Canada')) &amp;#x200B; You could even use CROSS JOIN. Looking up these two items will give you more information on what is going on. &amp;#x200B; You will have to use a JOIN or APPLY to get both table E and table R in the Select statement.
"non persistent database"... that's a new high in useless concepts. 
I am trying to help him try to learn how to write a resume so that he can make more money. 
It's for homework project. It's dump for everything else.
it asked for the data type and size should i stillnot put it?
No offense but t sounds like I’d like his resume tactics more than yours. Good luck though
He doesn't seem to have resume tactics except to not put things on it that matter... so cool story, bro.
I guess you don’t understand his opinion. Unless you don’t have enough content to fill your resume you should (in my opinion) include languages you have a working knowledge of, not just ones you’ve worked in. Do you though, it’s all good
I understand his opinion, and now in a position where I have influence over hiring decisions I can tell you that his opinion is not a good one as it relates to getting hired and paid. But that's just my experience and you do you, fam.
Straight out of school or first job it may not be a bad idea. Anything past that it seems like noise to me
Hard to say as will depend on your course and instructor. If they want a size for the int's and other types then it will be kinda like the varchar's where the size is unknown unless you know the data, for a course however the sample data might be the size they are after but it can be argued that size may not always be the max size.
It's not that there is some "full" SQL and not all products support it. All products use different SQL, so Postgres will know different commands than Oracle which will be different than SQL Server, etc. First you should find out what SQL dialect you want to learn. Or if you have no preference in this regard, just use some free SQL software and learn that, because it's 90% the same. Don't get too hanged up on these few commands which are different across implementations or don't exists everywhere.
You can install Oracle XE + Oracle SQL Developer. It's not very difficult. 
Just use Postgresql or SQLite. They're good enough and compliant for any homework. The only difference among SQL implementations are how identifiers are quoted. Mssql uses square brackets by default, Postgresql and SQLite use ANSI quotes ("). And they have different names for some standard types, and slightly different date functions. But you can read about that in the basic guides for any of these.
[PostgreSQL](http://postgresql.org/) (or "Postgres") is a good choice to start learning. It's pretty close to the SQL standard and for each command it also documents how it deviates from the SQL standard (which no other DBMS I know does in that detail). Although MySQL is quite popular, I wouldn't recommend it for learning, as it ignores the SQL standard in many ways (even for some of the most basic things). &amp;#x200B;
Why do you need the UID to behave like this? A UID shouldn't be used for anything other than uniquely identifying a row. Any analysis that you want to perform on that row should then be done with consequent columns 
Sounds like a SQL class question. There is no logical reason to create a user id with that naming convention, let alone how low the count is. When storing that ID in the database, its going to be char and that is going to take up more space and indexing isn't going to be great either. Tell your prof to come back with a better real world task.
That's a pretty weird stipulation for a UID. Is this required by other systems? Why do you need to know how many JDs there are? How would you know what order they are in anyway? Seems quite an arbitrary stipulation to be honest, I've seen these sort of things before and its generally stems from a product owner that has a distant history of some sort of data role attempting to dictate technical design. I'm sure that's not happening here though, right?
Here is a site you can use to practice with out having to install anything. &amp;#x200B; [http://www.sqlfiddle.com/](http://www.sqlfiddle.com/)
If this is a real business (which I doubt) - Any decent developer should be able to solve this problem. If yours can't, you should be terrified of the quality of their work. You need to find somebody new ASAP. Also, if you're paying them, pay me. Why should I work for free? If this is homework (more likely) - We can get you pointed in the right direction, but we're not going to write your code for you. Post a bit of what you have and maybe we can help. 
&gt; This is obviously not going to work bullshit it will work just as well as your scheme of incrementing each pair of initials
The storage space requirements of a CHAR(5) are trivial. Indexing and joining on INT's is a bit faster, but if the string in question is short the performance will still be perfectly acceptable. A problem doesn't need to be a realistic business task to be a good problem. I rather like this one. It's not easily googleable and it forces the students to combine functions and think through a series of steps to solve it.
I haven't used this myself, and it's still web based, but you might want to look into https://web.superquery.io/
SQL Server Express is a free, resource-restricted edition (10GB max), which can be used in production. See https://www.microsoft.com/en-us/sql-server/sql-server-2017-editions. SQL Server Management Studio is an excellent tool for developing against SQL server: https://docs.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms?view=sql-server-2017
Your opinion is duly noted, as well as your use of the word, "triggered."
It's done so we can track our sports players. We're a sports analytics co and this is the UID system we use for our analytics and hence need to db to use the same. It also doubles up and a sort of 'membership num' and hence has to be short and catchy, for recall value. I really wish it was a college project, I wouldn't be nearly as bothered. Thanks though. 
Haha, might be? Honestly, this is done so that our analytics model and dB can be in sync. This is the UID system we used for our analytics model and hence need to continue with the website itself. These also double as 'membership nums' and hence having them in this format is great for recall value of a seemingly odd number. 
Have requested for the code. I am and hence even if you czn point me in the right direction it'd be appreciated. I fully plan on giving them a piece of my mind, provided I, the non techie, can come up with the documentation that has the solution. 
I'm looking for help/assistance.. Where I can find documentation that would guide someone who knows what to do.. Not asking you to write the code yourself. Again, this isn't homework. Lol. 
I'm looking for help/assistance.. Where I can find documentation that would guide someone who knows what to do.. Not asking you to write the code yourself. Again, this isn't homework. Lol. 
Yes, and that's quite an important factor, no? Why waste sooo many UIDs! Making the letters as important increases the number of possible uids... As of now will only have a 100. Will have 10000 more possibilities if the letters can be factored in. 
It's done so we can track our sports players. We're a sports analytics co and this is the UID system we use for our analytics and hence need to db to use the same. It also doubles up and a sort of 'membership num' and hence has to be short and catchy, for recall value.
Thanks for all the comments guys! I see there are a lot of poeple that try to get their college assignments done using your help! Lol I assure you, this is a biz requirement. Post history will validite, if you're that keen on finding out. Have requested for the code that's being used currently. Will share soon. TIA. 
How are the Analytics IDs generated? How do you know that JD0001 is actually John Doe, is it inferring a start date earlier than John Denver? If they know how to sort the JDs then creating a unique ascending ID in the JDs would just be an algorithm or function in the SQL database, used to locate the record, but shouldn't be a unique key. If the database is where the records are originally created then it should call the shots and add a tag to the analytics for the user. If the Analytics package creates the user first, then it passes in the "Unique" Identifier as a foreign key to initialise the user in the database.. 
Is that Maria Butina? O\_o
Here's my advice to you as a _business owner_: make sure whoever developed and/or maintains your code is willing to do the update and agree to payment terms offer $50 for an hour of consulting services here and I'm sure you'll have plenty of folks offering their service. Look @ their post history, pick one. Schedule an hour with whoever developed your process and maintains the code for your website to walk them through the code needed Enjoy doing what you are supposed to do without wasting your time on stuff that's not relevant
Alias your tables so these joins and selects aren’t so verbose
This doesn’t answer the question - it shouldn’t be used as a UID. It should be something which is generated from other columns for each UID
CS or Data Analytics. Other IT fields will probably touch on sql as well
Nice try, Russia.
I heard you like honey pots. So we used a honey pot to advertise a honey pot so you can look at honey pots while we steal your money... pot.
I just want to add, that in the descriptions you will find a Corrections section, and also an Additional Comments section. If I have forgotten anything of the isolated subject matter, feel free to comment so I can add that information to the description.
OK, so it's for your business. Fine. This is a lousy identifier. If you're in business long enough, you are going to run out of ID's for at least one set of initials... then what do you do? Start over at 000, better hope you don't need that old data anymore... and you better be darn careful to delete it all correctly. Or you could switch to 4 digits, have fun updating your code and restating all the UIDs in your database. At the very least, you need to either add the middle initial or another digit. But as I said earlier, you need to get a new web developer. I wouldn't trust anybody who can't solve this problem on their own. It's not that difficult. 
It looks like your trying to pull back the first instance of Aa/Bb on the table? If you don’t have a time stamp or key to go off of there’s really no way to do this. You could pull back one instance of each data point but what numbers it returns in column 2 and 3 will be effectively random. 
Sure, can we try this? Random is ok - I would then use some sort of anti join to get the remaining values.
 with mytable as ( select 'Aa' col1, 11 col2, 1 col3 from dual union all -- select 'Aa', 12, 2 from dual union all select 'Aa', 11, 3 from dual union all select 'Bb', 22, 4 from dual union all -- select 'Bb', 21, 5 from dual union all select 'Bb', 22, 6 from dual ) select * from mytable where (col1 = 'Aa' and col2=11 and col3=1) or (col1='Bb' and col2=22 and col3=4);
Looks like your last column is an identity or sequence. The below code finds the values for the lowest 'identity' per col1. (Mssql) Select * From dataset as datasetTop Where col3 !&gt; all ( Select Col3 From dataset as datasetBottom Where datasettop.col1 =datasetbottom.col1 )
Thank you so much for this!
Not a problem.
 ;WITH yourTable AS ( SELECT t.Col1 , t.Col2 , t.Col3 , ROW_NUMBER() OVER (PARTITION BY t.Col1 ORDER BY t.col3) as seqNo FROM (VALUES ('Aa', 11, 1) , ('Aa', 12, 2) , ('Aa', 11, 3) , ('Bb', 22, 4) , ('Bb', 21, 5) , ('Bb', 22, 6) ) as t (Col1, Col2, Col3) ) SELECT yt.Col1 , yt.Col2 , yt.Col3 FROM yourTable yt WHERE yt.seqNo = 1; 
Hello, this is SelectAsterisk bot. I have detected that you have written a query using "SELECT * FROM...". An escort of armed guards is now en-route to your house to execute you. Have a nice life.
Another vote for sqlite or postgresql. SQLite is a single file database, so you can take your DB and copy it other places and use sqlite there. [https://www.sqlite.org/about.html](https://www.sqlite.org/about.html)
A foreign key is, basically, a primary (or other unique) key from one table appearing in another table. Sonic I had a customer table with a customerID and an Order table, one of the columns in the order table would be a customerID. Each of my customers can have many orders, each relates to a customer via this foreign key. You can use foreign key constraints to enforce this relationship, so that the database engine won't let you populate a value in the customerID in Order that doesn't appear in Customer, though this isn't actually required (You should, though...) The order of inserting into each table shouldn't be relevant. 
I'm not in Melbourne, but have been keeping an eye on their market on seek.com.au: it looks like there's plenty of opportunities there.
The term UID is causing confusion - it has a specific meaning in databases. To save confusion maybe edit your post to call it a 'membership number'.
Big thanks, planning to take it next month. Any idea on how many videos you think you'll need to cover everything?
That's a good question. I've started planning my next few videos. AGGREGATES: avg min max sum count, the GROUPING and GROUPING_ID syntax. Offset and fetch next for pagination. ROLLUP and CUBE RANK functions: ntile, rownumber, rank vs dense rank. (I think I can skip analysis functions, might be in the BI exam) TEMPORAL TABLES: short video. Not much to cover. New feature but limited in querying (remember this is a QUERYING exam) DML - insert update delete TRANSACTION: isolation level, try catch throw raiserror (it angers me that it isnt RaiseError, just 1 e). Using DML to show ROLLBACK COMMIT JSON querying - decent length video. You can do a lot with very little when it comes to json. XML querying - my kryptonite, but it has to be done. And to be honest, that might be it. Chime in Reddit, if you think I'm missing something. 
&gt; The order of inserting into each table shouldn't be relevant. How does the dbms know what customer a new order is associated with if they are entered in out of order? &amp;#x200B; &amp;#x200B;
I’m sure you can do better - but have you considered converting them from 6pm-6am and assigning them an integer 0-24? For example, just do the number - 18. So 6pm = 0, 24 = 6, and 4am = 10. You would accomplish this somehow like so: ABS(Number - 18)
Ah that is a good way to look at it. I kept trying to play with DATEDIFF from 6PM. But obviously 1AM wouldn't show up as 7 like you would hope.
What kind of database? For example, you can do this on Oracle: Number - 18 HOURS On others: DATEADD(HOUR, -18, Number)
Just to add, I'm covering JSON and XML, however... these were not on my test. I do not know if they are part of the random pull for the 2016 version... but I have heard stories of people getting these questions.
Yeah MS. I'm starting to play with that. 
I’m on Mobile, but still trying for you: CONVERT(TIME,DATEADD(HOUR, -18, DateTimeColumn))
I'm not sure I fully understand, is there a 3rd song table that has an Artist ID Column for the artist that sang it? If so you can JOIN that on Artist ID right?
hm if I understand you correctly, something like this would do the trick: `select * from Submission s` `where s.artistID in (select ua.artistID from UserArtist ua where ua.userID = 1 /* Jordan*/ )`
\^ I think this might be what I need! Going to check now and report back :)
please do :)
[https://en.wikipedia.org/wiki/Hierarchical\_and\_recursive\_queries\_in\_SQL](https://en.wikipedia.org/wiki/Hierarchical_and_recursive_queries_in_SQL)
You the man or woman! Hierarchical looks like the search term I needed. Thanks
Works! Thank you very much :)
you need a **junction table** between actors and roles google should be helpful now that you know the term (also called relationship table)
Pivot. Select item, [1],[2],[3] From ( Select date, item, sum(value) From table Group by date, item ) base Pivot ( Max(value) For date in ([1],[2],[3]) )pvt
Thank you for your efforts!! 
First, I would suggest you look into building a proper calendar table so you do not have to keep calculating the dates. This over at [Stack Overflow](https://stackoverflow.com/questions/34352887/how-can-i-list-all-dates-between-two-date-parameters-in-sql) may provide other options.
 LOAD DATA LOCAL INFILE '/Users/eduardonunez/Desktop/US\_states\_data.csv' INTO TABLE tHW1\_Q6 FIELDS TERMINATED BY ',' ENCLOSED BY '"' LINES TERMINATED BY '\\n' IGNORE 1 ROWS; &amp;#x200B; also tried direct path and nothing
A proper calendar table doesn't suit my needs. An on the fly data driven solution based on which of multiple fiscal calendars I'm using fits my needs perfectly, especially since they don't end on year boundaries.
Thanks! Had to go pick up the kid, but I'll test some stuff out in the morning and report back on solution.
Without being able to see any of the source data it's hardto know... can you tell us more about the tables?
Short answer: no Long answer: It looks like you are trying the percentage of orders not percentage of customers. The question honestly needs some clarification, but to me if there have been 100 orders from 80 customers and of those orders product X was purchased 20 times the answer should be 20/80 = 25%. It looks like your query would give 20%. Also since you are doing an inner join you won’t get products without customers/orders. Hope this helps
how much was the cost and how much of everythnig were you using?
can i just use date between '1' and '3'? Also, is there another way instead of typing the date values 1 by 1 in the main query?
Umm. How far back do you need explaining to go? Let's try sql basics. First of all, a table in 1st normal form needs to have a key or even multiple keys. And a single key can be one or more columns (aptly named multi-column keys). So, while most often you will have a single key consisting of a single column per table, you can have a table with several multi-column keys. Quite often you will have so-called synthetic keys. These usually are just some values generated via some method usually independent of you business data. Examples could be a sequential number, globally unique identifier, etc. Key should be comparable (identifiable) so it cannot be NULL (in the same way any part of the multi-column key cannot be NULL). Secondly, most likely you will have other columns besides the key in a table. Let's say you have a Person table and you have 2 keys to it - Person_ID (integer) and SSN (char(9)). You also have first and last name. Now, say you want to record annual salary history for a bunch of people. You can certainly record ssn, first/last name, date effective from, date effective to and salary amount as a bunch of records that will fully describe the salary history data for you. But you have the Person table, so instead of recording and repeating same combination of ssn, first/last name you can record only the reference to your Person table as "Person_ID". Thus your salary history data becomes Person_ID, date effective from, date effective to and salary amount. Person_ID is a key to another table (not to your Salary_History) so it is a _foreign key_. So, since you mentioned joins already, to "restore" the original salary history with ssn and first/last name you will need to utilize join to bring this data together with the data from your normalized salary_history table. Another (besides not repeating data) good thing about this arrangement is that if John Doe got his first name changed to Priscilla, you need to update only the Person table and the "restored" salary history (the join result) will show the new name for each of his (her?) records. You could have used the other key - SSN - to reference a Person table record. It could lead to complications though as a Person could change their SSN, potentially. There are ways to deal with it but it's a hassle for some scenarios. There's no real difference how this would work with 1000 columns in the Person table, for example. The order of attributes in a table, usually, has no relevance to your querying as well.
Understood and have edited my post. This isn't a 'uid' as used in the db, this is more for a player tracking system and membership number. Eitherways, would you be able to point me in the right direction?
Yes, I've realised. Have edited. Thanks!
I haven't loaded files like this in years, but remove the word local from your command. Also, most file locations are 'c:/users/....'
Also, stack overflow is your friend. https://stackoverflow.com/questions/7623984/load-data-local-infile-error-2-file-not-found
Currently using mac terminal and tried removing local but gives me error 1045 access denied for user nunedua
You're on the right track with your first query. Just wrap your totalpages in a window function to get cumulative: `sum(sum(t1.total_pages)) over (order by usage_day ROWS UNBOUNDED PRECEDING) as total_cumulative,`
Every user should have a ‘date created’ field and then, depending on which version of sql you have, you would just use a string function to get the first letter from each name and then use the date created date to order them and rank each field
How about both? Both have strengths and weaknesses. Also the example is the worst way to do ETL in Python. Pandas is not for ETL.
Content looks great, but what’s up with the annoying background music?
What is?
Pandas is bad for ETL
I use both
Thank you, that's good to know. 
Nice! Will check this out. 
That's been my dream
Learn it? In a month should be possible. What does your job use mysql, ms server, nosql, mongodb, so on. Find tutorial for that specific sql and learn it. Its easy enougb that 10 hr a day can get you there 
Ok Im watching YouTube videos now. Let’s see lol 
Any sources or methods that can be quicker ? 
Imo options are: Try and find a different job. Try to learn it in a month. Show up and see if they'll teach you. Any technical interview should ask you technical questions to ensure you have the skill level you claim. If they didn't they likely have low expectations. I'm betting this is an entry level position. Try to learn it in the next month.
Maybe other people will disagree but it seems bizarre that it's a big part of the job and they didn't test you. It seems like they can't have too high of standards if they don't test. Anyway, start cramming. You can use things like: Sqlzoo W3schools Sqlbolt Edx has some good intros to SQL Good luck. 
A lot of my roles have been super technical and in my cv I made it sound like I was the tech brains behind it. My last job was with eBay and I had a buddy who helped me do all the tech stuff. So in my resume under tools used for every role I put “python, sql, Java, html. Which technically isn’t a lie. But also is not the whole truth 
They sent me a data set on excel to Manipulate some data . I paid some guy on Fiverr to do it. Reason I’m panicking is they have sent me a project to start working on before I Move to Geneva. 
What exactly did you expect was going to happen?
I guess Fake it till you make it will really shine here. &amp;#x200B;
God damn dude. You are on another level of lying on a resume. &amp;#x200B;
To be honest im betting that its some analyst job, and you have to execute store procedures all day. Its not likely that you get full db acess to do anything major. 
Lol... study it. Kudvenkat sql tutorial on youtube helps. Its 130ish videos at about 15 minutes each. That will be a good start.
It’s a lie. I hope you get fired.
No need bro. Chill 
Is “store procedures” a topic in banking ? 
If it’s not, what is.
Hahahaha. Good luck. Time to sink or swim.
Start looking for a new job that you can start at the end of March.
Just keep bringing your work home at paying people in fiver to do it. Im sure nothing will go wrong. 
I’m Allowed home office 35%. So I have thought about this for sure 
I want an update on this in a month. 
I will be an sql savant in a month 😂
I would resign before you even start. Any company that would hire someone without first verifying the person has the required skills to do the job isn't any sort of place you want to work. And, then, stop lying. You apparently are very good at it and it's going to get you in deeper trouble at some point down the road.
Honestly I'm at a loss here, because you are basically arguing with everyone here and I see no reason why your "UID" should function the way you want it to, versus the way it is functioning. You can do whatever you want to do either way, so it's not an issue in any meaningful sense, however, this is my advice about what is going on: 1. You haven't been listening to your developers, and since you outsourced this project I'm going to assume there is a bit of a language barrier and you might not have been hearing them correctly, or they have not been hearing you correctly. As everyone else here has stated, this is a bad idea for a UID, and you very well may have confused your devs. 2. However, the bottom line is you're paying money and your request is pretty simple to implement. If not for a UID, and just a "client alias" or "user name" or whatever term you want to call it. It's a very simple thing that can be done with a relatively basic query. &gt;Why waste sooo many UIDs! Making the letters as important increases the number of possible uids This is where you are driving me nuts with comments like this. Who cares? I can just add another 0. In another post you mention you don't have a tech background, but then you argue with people who are making good points for a demand that at the end of the day appears to have no other value than "its something I want," and "I think it looks cool." -- Which is fine, because it is your money, and it can be done... however I'm trying to get you to look at why you're in the situation you're in. You're confusing terms, arguing with the experts, etc. You don't even really need to fix the current "UID" and you can just add a new column with a fairly simple snippet such as: SELECT UID , CAST(LEFT(UID, 2) AS varchar) + CAST(ROW_NUMBER() OVER(PARTITION BY LEFT(UID, 2) ORDER BY CAST(RIGHT(UID, 3) AS int)) as varchar) AS UID_Alias FROM TABLE Just have them write it in a view and use that Alias for whatever you want to do.
Sending proprietary financial information to a rando on the internet. Sure, what could go wrong? You're probably better off if you just come clean, tell them you don't know anything about SQL, and let the chips fall where they may.
&gt; Any company that would hire someone without first verifying the person has the required skills to do the job isn't any sort of place you want to work Maybe OP does want to work at a place like that. *I* wouldn't want to, but I also don't lie on my resume or in interviews.
SQL for transformations, streams for extracting and loading. Pandas loads everything to memory and does a lot of copying. And it's by far not as advanced and optimized as SQL.
Except... you took the job from other applicants who were qualified. And wasted everyone's time.
RIP
stored procedures are a topic in business database systems. What you will probably doing won't be very technically demanding, but doing it wrong will create alot of cleanup for higher paid employees, so they are hiring you to execute said procedures. The only skill in that is knowing where your F5 button is and how to open SSMS and navigate to your database.
IT field has plenty of positions for qualified applicants, in fact, more jobs than qualified applicants. Hence OP's current situation.
This article is comparing Python to PLSQL (not SQL). PLSQL is an abomination that should only be used as a last resort. Python + plsql is the type of thing you expect cheap contractors to do.
Ok used this in the group by statement. Tested it on numerous dbs and it's for sure giving me what I want. Thank you for your help! DATEADD(hh, 12, MAX(DATEADD(hh, -12, cast(table.val as time)))) &amp;#x200B; &amp;#x200B;
All the more reason to not waste an employers time lying in an interview...you could find a job that you're qualified for and the over-worked hiring manager could get the position that s/he needs filled the first time with somebody who's qualified. If what you say is true (which I'm not so quick to assert) then all the more reason to ***not lie*** in an interview.
Right . . . but from an economics standpoint, and individual does what is best for them, not the employer. Also, I would not assume that just because OP is qualified in another area or expertise doesn't mean he can get a job in that field. There are many areas where expertise is now low value work or simply invalid do to automation, ect. So from the perspective of a busines . . yes, any way you slice it is better for candidates not to lie, but not really in OP's perspective
Again...I'll disagree. It's not in OP's best interest to get a job he's not qualified for and get fired in a month. Would be hard to explain to any future employer that *will* be more scrutinizing of skills and employment history. I do want to make clear, I do hope OP learns what he needs and excels at his new job, and I'm glad he got it (assuming he's able to to the job). Just hope that in the future OP - and any other job-seekers reading this thread - don't see OP's luck in not being questioned and see it as an invitation to lie on their resume and waste an employer's/hirer's time.
Can you please do a video on Stored Procedures? It's such an important feature of SQL but there's no real good as source that's thorough on when and why to use these, or how to write Stored Procedures that have good performance. 
100% sure this will be in the 762 exam. I hear your needs and concerns, but there is a bigger need to isolate these exams based on content. The exams are almost like stepping stones. Expect functions, triggers, procedures (natively compiled), columnar indexes, views (schemes bound), memory tables, database and file creation and more... to be in the 70762 exam. 
No it's not? It's comparing it to Postgres? 
Ugh. Procedural language. Before Postgres there was oracle. That’s where PLSQL comes from. Forgot my audience was millennials...
I would say use Khan Academy! That is what I did, maybe about 30-40 minutes a day, when I had free time are work and now I know enough to do my new role well. I learnt that I would never really be creating my own datasets so, I really looked in to learning the joins and functions that will help me analyze. 
It's from one of my unreleased albums: "grita al la nube, senior simpson". 
So, I had one project where I used a recursive CTE to generate a list of dates. I inserted them into a temp table table, and then I merged from the temp table into the reporting table to effectively create dummy values for SSRS reporting reasons. Looked like this &amp;#x200B; `IF OBJECT_ID('tempdb.dbo.#dummyinsert', 'U') IS NOT NULL` `DROP TABLE #dummyinsert;` `WITH theDates AS` `(SELECT @StartDate as theDate` `UNION ALL` `SELECT DATEADD(day, 1, theDate)` `FROM theDates` `WHERE DATEADD(day, 1, theDate) &lt;= @EndDate` `)` `SELECT theDate, 0 as theValue` `into #dummyinsert` `FROM theDates` `OPTION (MAXRECURSION 0);` `MERGE Foo_Target AS TARGET` `USING #dummyinsert as source` `on target.DateOfService = source.theDate` `when not matched by target then insert ( DateOfService)` `VALUES (source.theDate);` &amp;#x200B;
Says the guy in the SQL subreddit. 
It's totally possible to learn it in a week. If you immerse yourself in it. Download datasets, import them into database, ask questions and try to answer them using SQL. After you learn the basics
I don't know if this is what you're looking for, but you run this daily so may it will work. You can set up a variable (@value) in the SSIS package and set it to your field in the table, then at the end of the package set up a constraint where @value is greater than your threshold. If the constraint is met, use a STMP connection to send an email or nothing if it fails to meet.
Duuuuuuuuuuuude....
Dude, it's objectively faster and easier to read in most cases. I love Python and use it every day, but it's nowhere close in bulk data processing when compared to SQL. Especially when you also insert Pandas as an additional layer. It becomes a huge layer cake. To get speed you need to remove layers, not add them.
Totally. I’m motivated to at least learn the basics. It will be well. 
What’s the job role? That will affect requirements greatly. 
Head of product and business intelligence 
Oh boy. Do you have a team below you who will be doing the work and you manage them or are you expected to take an active technical role?
!RemindMe 30 days
I will be messaging you on [**2019-03-21 16:58:49 UTC**](http://www.wolframalpha.com/input/?i=2019-03-21 16:58:49 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/SQL/comments/asai78/help_i_lied_about_knowing_sql/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/SQL/comments/asai78/help_i_lied_about_knowing_sql/]%0A%0ARemindMe! 30 days) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
There’s a team of analysts and associates below and I am responsible for delivery of product and projects. I’m sure I can delegate. And look after big picture stuff. I’m Coming from a big 4 firm so I can waffle if need be. 
It is very difficult, for someone like me, to even attempt to create a lesson plan. This is my journey in doing just that. I'm happy it benefits you.
That’s a blessing for you, you might be able to get away with an understanding rather than actual knowledge. Here’s something’s you can do to become more comfortable (from a Microsoft T-SQL perspective) Look into downloading sql express or the dev version from Microsoft. Get adventure works. This will give you a very clean but good example database and you can familiarise yourself with SSMS. Write a few SELECT statements, learn joins (inner join, left outer join are the two important ones) Learn aggregate stuff (count, sum etc) Basically - Anything you can do in excel, learn to do in sql. Try and watch a few guides on SSRS and SSIS, this isn’t really a hands on thing from your position but try to get a feel for it. Add stackoverflow to your bookmarks. Find Jesus.
Dude I use PL everyday and it's not the same as Postgres😂
Check again! Only the second part of the article uses PLSQL, the solution to the first part is pure SQL.
When criteria is met or when there is data in the table? 2 different questions 
Yes, we all must start somewhere. Use it as a building block to greater things. 
It would be when a value (Count) is below a threshold, it would deploy and email. 
would this be something I can set up in VS? 
&gt;come clean Judging by his responses on this thread, I dont think that's going to to happen.
Depends on you're data. It'd be a nightmare to use straight SQL for ETL projects I have to deal with. Imagine a bunch of people creating their own personal access databases and spreadsheets with no idea about normalization or best practices for storing data. Id use SQL for moving from one established database to another but it doesn't have nearly the muscle as R or pandas has for data wrangling. 
What error are you getting?
xy problem? why would one time change in 5 stored procs give you a pause?
can you narrow down 'doesn't work' to the specific error message that you get?
It isn't a one time change. My IT group seems to have no problem at all during business hours to just "nuke" core database objects that I need to use for my own processes. Yesterday this did this right in the middle of some testing I was doing, and the core object is currently filled with 0 rows of data. They are going to trigger a process that is going to take about 12 hours to finish before it has data in it, and that data is not going to be the same as it was before, which sort of invalidates my test. Luckily I had the mental foresight to take a backup of the source table first (because this happens a lot). I actually have about 9 sprocs that I need to point to the new table to finish my testing, but this happens all the time because apparently basic database etiquette is not something our company puts much value on. I can change them by hand, but this is something that happens pretty frequently and I was curious if there was a better way to update multiple sprocs with a simple query I can save.
&gt; It'd be a nightmare to use straight SQL for ETL projects I've been in numerous jobs that have used SSIS for the only form of ETL. Or are you speaking of ETL processes made up of only SQL queries? SSIS can pretty much do anything you need to do. If I could easily execute an SSIS package in Python I would have everything I ever wanted. 
That would be the case for "extract", maybe some basic "transform" jobs. When it comes to matching order ID from an ERP with conversion ID from Google analytics sessions, with time windows for previous sessions to also be attributed to the same order if they can be identified in any way to the same person, , , then doing it with Pandas for millions of rows is just shooting yourself in the foot with a 120mm HE tank round.
Haha, it's spelled correct in the query. I fat finger on my phone a lot. It doesn't give me any errors. I have a WHILE loop that looks at the information it pulls but it doesnt seem to count it as valid information. Here is the specific section isolated in it's own query. The first query works when I just hard code the definition of @str and it works as intended https://imgur.com/a/bMCAI8O The second one (a but of a mess, been trying different things) seems to drop data which I think may be causing it to not work. https://imgur.com/a/biR0063
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/t9WuCTN.jpg** **https://i.imgur.com/r7tnMwP.jpg** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20egtdwsz) 
Sorry, gave a much more in-depth explanation here: https://www.reddit.com/r/SQL/comments/asciis/what_is_the_default_variable_type_when_inputting/egtdvzo
I am a SQL developer who is trying to explore the world of Python ETL. I've been getting into Cython, Spark and general big data lately. Hopefully one day Python could rival SQL Server/SSIS but for now Python is nowhere close. I have heard good things about Rust.
You should be able to eliminate your lower bounds and your AND statement. If the value didn't qualify in the previously nested IF statement, then that condition isn't necessary. For example, you can do something like: If([dimension] &lt; 50) Then "Tier 1" ElseIf([dimension] &lt; 75) Then "Tier 2" ...etc Forgive my lack of syntax, trying to type on mobile. I might have the () in the wrong spots... In BO, I use the semicolons instead. IF([dim]&lt;50; "Tier 1"; IF([dim]&lt;75; "Tier 2")) 
use varchar in your casts/converts with length, like so varchar(4000)
&gt; SQL for ETL projects I have to deal with. Imagine a bunch of people creating their own personal access databases and spreadsheets with no idea about normalization or best practices for storing data. Imagine someone with no idea about proper ETL architectures. Why the hell would “a bunch of people be creating their own access databases” when you mentioned ETL. Also, ELT is infinitely more manageable I’ve found then ETL. 
You made your bed, now lie in it. I don't understand the users who want to help you get away with the lie - doesn't matter if the company had bad screening, you are basically stealing. Now downvote me to hell, but I don't want to help people like that succeed.
I was giving them lengths of (1000) earlier. Still didn't work. But I will try again.
Sounds like you're going to have to schedule a SQL job or something similar to run every xx minutes/hours checking that count, then reacting based on the value
Yes, I have set up the job to refresh once a day.... but now using VS to send an email is the hurdle.
Are you using SSIS? Yes. Its a script task + email
First, I believe the generic definition you were looking for was SQL/PSM, not PL/SQL. When speaking, in general, regarding the procedural additions to SQL - it’s much more common to reference just SQL/PSM. As PL/SQL is a specific implementation, as is SQL PL. Additionally, PL/SQL and PL/pgsql is not the same thing, while they resemble each other on the surface, like most other SQL/PSM implementations that’s basically where it stops both in the intermediate-to-Advanced implementation aspects as well as how the engine actually handles the execution. Furthermore, I love the continuation of digging the hole deeper and crying “millennials” when anyone who has been around longer than that would still know that based on processing power it’s really, really hard to beat the raw performance of an imperative, procedural, statically typed language when interacting on transactions. Which is exactly what it’s used for in an RDBMS and the reason why you still see financial and market institutions still clinging to languages like FORTRAN.
It's pretty straightforward. You just need the details for the exchange server. Or even better have an email profile with dB mail (if using mssql) 
I am unable to have the my email whitelisted to send emails this way, but what I'm thinking might work is to write the table to an excel destination and then use FLOW to trigger an email? 
First of all, Spark is good if you want a standard catch-all solution for big data. But its performance is abyssmal. That's probably because of the multi-layer architecture that's usually done in Java and Python. Java is slow, compared to C. That's why ScyllaDB is faster than Cassandra, that's why no major database engine uses Java. I actually hate that Spark is so popular, because of this. ClickHouse is a very simple contender that, with a few additions, could probably serve as the fastest big data engine that also does ETL. But popularity factor is really deciding here. Nobody will hire you for ClickHouse. Everyone wants Spark, Hadoop, and other slow mumbo jumbo. I have some ETL in Python and Postgresql that sweeps away in performance anything done in Spark, but try to convince other people that is good stuff. &gt; Hopefully one day Python could rival SQL Server/SSIS but for now Python is nowhere close. Oh it does, and exceeds. Python with a database engine and an ETL framework like [Mara](https://github.com/mara) is so far the fastest ETL you could get. It leverages the fastest ways to move and process bulk data on any platform, with virtually no overhead, while every bit of resources of the server are consumed, so you use everything. &gt; have heard good things about Rust. Maybe. I found Rust to be very unfriendly and hard to manage data because it's difficult to cast or convert data in it. It's. . difficult. Many people regard it as a language with a very steep learning curve. D however (Dlang) is very simple. Compiles fast, is easy to use, versatile, simpler than go, as fast as C. There are many great experiences in using D for data. (D stands for Data, people joke) eBay even has a repository of CLI programs written in D that do query processing on flat TSV files. But Python is good enough in most cases for data processing. Just not data joining, data navigation and big data in general.
Request a new email address instead of relaying your own through the server. 
I'm a fish biologist, and deal with other fish biologist at a state agency. I'm not working with anywhere near an able IT department, don't don't want to be IT. I have no say on how people deal with their data, I just take it and consolidate it into proper data structures.
I know I can create some kind of "single object" where I could go in and pick the table name, and then have the sprocs look at the object to know where to pull data from. Are you basically saying there is no way to use a query to update a stored procedure/view in the same way I could use one to update a table? Not really looking for PowerShell or other types of solution. As it stands I could just export the sprocs I want, do a CTRL+H and replace the table name with the new table and then run. This question was more a curiosity.
Well I can say you probably sold me on Mara. I'm downloading some example templates/ETL tools now. I'm very excited to try it out! I also see it's integrated with BigQuery, which is very nice. Loving what I'm seeing so far. And I agree with everything you said on Rust. I tried learning it but realized I would need to drop my other tasks and learn Rust full time. It was an extremely complex language.
Well, I think synonyms might help you - basically, you define a "pointer" of a sorts pointing to your original tables, compile your sproc using synonyms, and at a later time you can change the definition of what the synonym resolves to. It will lead to recompilation of your sprocs tho. https://docs.microsoft.com/en-us/sql/relational-databases/synonyms/synonyms-database-engine?view=sql-server-2017 
There's nothing basic about transforming this data. There's people using a different access database for every year of data with different structures back to the 90's. I'm not dealing with millions of rows, I'm dealing with a shit ton of transforming and manipulation of the data to consolidate it into a modern well designed database. As I told the other dude, if you have any advice I'll take it, but all I've seen so far is people harping on pandas. 
A lot != Complex You have a lot of "extract" with very basic operations that spill into "transform". Pandas is easy to use in such cases, but I'd try to design a unifying pipeline with a descriptor for the source data. Make it declarative. Also I'd remove those many access databases. That's not how an organization should work. That's just appalling.
Are synonyms basically views? From an abstract perspective, I mean. If I'm reading this correctly I would create a synonym for dbo.table, and then I would point all my sprocs to that synonym, but then anytime I need to make a change I just update the synonym and I'm cool? What do you mean recompile my sprocs? Do you mean like if I were to use a view in this example I might have to update the view metadata to reflect a change from one table to another?
Let me know if you need help with it. PM.
Welcome to state government, where IT asks biologists to solve their computer problems. 
Synonyms are sort of like aliases (or 'foreign keys') on the schema level (well, not quite but i guess this works for comparison). They will point to a single object. You can define a synonym for a sproc, for example. Views imply an action and can pull data from multiple tables, for example. Your sprocs are compiled to an intermediate code by sql server when you create them or if any of the dependencies change. Synonym is one such dependency.
&gt; My IT group seems to have no problem at all during business hours to just "nuke" core database objects that I need to use for my own processes. There's the crux of your XY Problem. Why is this group doing this in the first place? Address the root cause, don't work around the symptoms.
Why not just inform your IT group to white-list your db/tables and leave them alone. Follow up and shake the tree if they refuse. If this is something you need, then they should not have an issue with it....unless you're placing your stuff in a place it should not be. Simple as that.
I could use a lot of colorful words to explain why, but in the end I am going to just ask whether my question is possible.
I have, many trees have been shook, but at the end of the day it continues to happen, and I have no reason to think it isn't going to simply continue into the future. They apologize and take "ownership" of it when it happens, but at the end of the day nothing is ever going to change or be done about it, so why shake more trees? 
So when you say "recompile" my sprocs, what you're saying is analogous to me talking about "refreshing metadata" when a new column is added to a table, but the view doesn't reflect it yet (because someone coded it as select *).
Well that sucks. I'm not a SQL Programmer, I only dabble in writing SQL. Couldn't you script out all of the Stored Procs and have them go all at once?
As far as I remember 1. Yes. The queries still run as there are no issues with the DB. The only issue would be if there is a limit set for how long a query could run. 2. If the database has the memory and processing power to handle all the requests at once then they will all run, else there will be a queue. 3. If a user drops the DB should cancel their query. 4. The results are stored in memory. 
PySpark, but you've got to have the option to move away from Oracle, MS SQL or postgres. The industry is increasingly moving away from stored procedures.
You are correct. PLSQL, like Fortran has a place...a place I am happy to be very far away from. Same with procedural languages.
But why is "the industry" moving away from stored procedures? What exactly is bad about stored procedures in your opinion? And what exactly does PySpark do better? So far, I'm only seeing very broad statements but no arguments to support them.
&gt; If the database has the memory and processing power to handle all the requests at once then they will all run, else there will be a queue. This isn't related to the networking aspects of the question, however I wanted to add a tidbit to this. Since all the queries referenced in the scenario are the same - not only will the queries still run, and simultaneously given the number of threads based on resources. But, SQL Server is smart enough to recognize that portions of the result sets may all be the same and use these indexed result sets as portions of the result sets for the other queries. Thus optimizing how many reads to the pages it needs to do. 
Well I guess that explains why there wasn't a technical screening.... Seriously, though, don't lie about your skills. It's disrespect to all of your future colleagues who'll have to pick up your slack, and everyone who has worked long and hard on a actually having those skills. 
I agree - I'm always willing to help others learn, but I draw the line at someone whose lied their way into a job and don't want to be found out. Other skilled people didn't get that job because they did.
make one lower case and another one upper case, maybe?
In the real world the queries are not all the same, but I was being general in my example, however all of the users requesting these queries to be ran might all be experiencing the same network latency which prevents them from "receiving" the data as quickly as it as able to be sent.
Sql server (as far as i know) uses network buffers to stage data to be retrieved by the client. As the output records 'bubble' up from the execution pipleline, they are added to the network buffer. If the client is not capable of receiving all the data, the execution pipeline would be paused. so. &gt; What happens to all the other users who request? Their queries are processed concurrently. There's a server setting that limits the number of concurrent connections. Since these are all 'select's, only shared locks will be placed on the table records/pages/etc. &gt; What if user1 drops? The networking should detect this (takes some time though) and send the signal. The relevant execution pipeline (spid) should be killed automatically. &gt; Where does all this "data" reside? Is it taking up the tempdb? Does the query execute and produce a result which then begins to transfer to the end user, and if so where does it reside while it is "waiting" for the end user to download it? There are some resulting records that will sit in the network buffer. The execution pipeline state resides in the server memory. Tempdb is only used if it is used by the query execution (sorting, for example). Tempdb space will not be recovered until execution pipeline completes or is killed. 
It's been 2 months and I'm still waiting for him to return to this post safely :(
&gt; But why is "the industry" moving away from stored procedures? &gt; &gt; What exactly is bad about stored procedures in your opinion? And what exactly does PySpark do better? &gt; &gt; So far, I'm only seeing very broad statements but no arguments to support them. What's bad about stored procedures? Generally speaking, there are better alternatives. "PLSQL" was popular and sometimes unavoidable when the industry was dominated by Oracle &amp; MS SQL. We're far from that today. RDBMS DBs have more competition and people have more choices. Using PySpark I have all the features that come with python. Even pig can be substituted for PLSQL. Point is having options means I can choose to not use PLSQL. Plus distributed computing is cannibalizing the RDBSs industry. More people with CS degrees are working in the industry, they don't bring PLSQL with them out of college, so I see PLSQL adoption diminishing over time. Banks and their huge Oracle/MS SQL instances are an example of a niche industry keeping PLSQL alive but even that is changing due to projects like Google Spanner. 
This has nothing to do with SQL in any form. 
&gt;As the output records 'bubble' up from the execution pipleline, they are added to the network buffer. If the client is not capable of receiving all the data, the execution pipeline would be paused. Can you pretend I'm a 13 year old kid trying to hack your network and rephrase this in a way that I can intellectually digest as a non-expert? What do you mean paused as it relates to multiple "stacks" of request that begin to "bubble up?" -- In terms of old school server design, where does this "data live" -- is this in the RAM, or tempdb? &gt;Their queries are processed concurrently. What if their queries lock tables? And each query depends on a lock? &gt;There's a server setting that limits the number of concurrent connections. Since these are all 'select's, only shared locks will be placed on the table records/pages/etc. I'm not smart enough to follow you here. &gt;The networking should detect this (takes some time though) and send the signal. The relevant execution pipeline (spid) should be killed automatically. Relative to the points above about multiple users stacking up, and/or the possibility that this user has tried to "refresh" the request, which from a web perspective --&gt; i imagine this would initiate a new request without necessarily terminating the first request? &gt;There are some resulting records that will sit in the network buffer. The execution pipeline state resides in the server memory. Tempdb is only used if it is used by the query execution (sorting, for example). Tempdb space will not be recovered until execution pipeline completes or is killed. You're losing me bud. Let's say we have 100 concurrent requests going on. Where is all this shit at? When you say "server memory" what do you mean? 
You could rename the old table and rename the new table to the old ones name. Otherwise no.
If you have the Exchange creds you should be able to send email with whichever address you'd like as the from field. Also, by the time you get to hacks like writing to a file, you should consider just writing a few lines of .Net or something to run the query and trigger an email on completion. 
Zapier.com makes you happier
Happy Cake Day!
I feel like I'm reading the output of some kind of AI chatbot fed the marketing output of the last five years of memetech. PL/SQL is Oracle: has nothing to do with MS SQL &gt; when the industry was dominated by Oracle &amp; MS SQL. We're far from that today. No we really aren't. Unless your measure is the number of blog post headlines, RDBMSes are still very firmly dominant. &gt; Plus distributed computing is cannibalizing the RDBSs industry. I hesitate to ask what you think distributed computing means. Distributed transactions have been a thing in RDBMSes for a long time. There are other forms of databases that allow for better distribution by sacrificing ACIDity, but it's hardly an either/or of one beating the other.
Worked like a charm, thanks!
Join
Yes but its not working with the function I put
Ned to be more specific with your question 
Hmm. So, very roughly SQL server is a program that knows how to 1) read and write data to disk 2) interpret and convert pieces of code written in SQL into a number of internal "procedures" and processes that it can actually execute to produce result. So queries are converted to execution plans - and you can think of each query as a pipeline of a kind where you feed initial data to "one end" and get results back from "another" end. &gt; When you say "server memory" what do you mean? This is memory that is allocated (given) to SQL server by OS. &gt; Let's say we have 100 concurrent requests going on. Where is all this shit at? The information about those is stored in the server memory. There's a piece of server process that will determine (depending on the resources needed/available/quotas/etc.) which request and how many steps of it will execute next. &gt; Relative to the points above about multiple users stacking up, and/or the possibility that this user has tried to "refresh" the request, which from a web perspective --&gt; i imagine this would initiate a new request without necessarily terminating the first request? This depends on the actual architecture of the application. Let's say a Tableau page refresh will detect the same data request and will wait for the original request to finish to allow for the result to be reused. In a different system the application behind the page might finish all the data work before detecting that the client is dead and the page refresh might initiate a different process using very same queries second time. &gt; What if their queries lock tables? And each query depends on a lock? queries place shared locks on the table storage (records/pages/etc) and stability (?? i think they are called that) locks on schema. These dont prevent readers from reading.
My life expectancy decreases by a few days every time I read something like this. Elaborate!
Put `[ sql "select 1" = 1 ] &amp;&amp; mail foo@bar "hello"` in cron or wtvr
If you were to look up the Strata conference from 10+ years ago, then you would see dozens of sessions on Oracle and MS SQL. This year there are zero. This is true for most conferences. The industry has been for some time. When someone says RDBMS DBs or PLSQL are still useful, my response is that's cute.
you send an email with T-SQL if you have the mail profile setup. I don't understand your question very well, but you could have a proc with an if statement run the send mail proc when a condition is met. in theory this is the simplest method to send mail with DB, assuming you are familiar with T-SQL. https://docs.microsoft.com/en-us/sql/relational-databases/database-mail/configure-database-mail?view=sql-server-2017 https://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/sp-send-dbmail-transact-sql?view=sql-server-2017 
30, which is shown in your screenshots. https://docs.microsoft.com/en-us/sql/t-sql/functions/cast-and-convert-transact-sql &gt;length &gt; An optional integer that specifies the length of the target data type. The default value is 30.
select * from both, copy data to excel sheets, perform VLOOKUP.
Ohhhh shitttttt..... you may be on to something there. I've reviewed this query a hundred times to make sure I understand it but I think I overlooked LEN this whole damn time.
I am sensing that your answers are too technical and you don’t like it. Really you are asking DBA questions that can be configured in different ways if you properly know how to admin a db. In the most basic sense when sql gets a query request its results are in RAM. If multiple remote users in your example query the same thing in the same table, if they select without the “no lock” option then, yes, it will lock that table until it completes querying thus not completing the other requests for that same table. In your example though the SQL server has adequate resources as well as the network around it save for the multitude of 56k remote users right? Then the query data is committed to memory (ram) and is fed down the network to its requestors. That data is transported in packets and is stored in local memory (typically) on the remote user node. 
You're taking a Scorched Earth approach to demonizing SQL and SQL/PSM while making the failed recognition that OLAP architectures and OLTP architectures are two different areas of study. The Data Science/Analytics community has taken hold of a toolset that works best for them - which are easy tools to prototype and perform rapid studies of data on projects. This is all well-and-good, but it doesn't correlate to the Data Management side of things within Data Architecture and isn't going to be a replacement for RDBMSs for 90% of the companies out there. The NoSQL movement already tried and failed in that arena. I think the current state of affairs have finally leveled out and these solutions have fit snugly into the places where they're most useful - which are those mentioned above - rapid prototyping and getting access and analytics on data for the business to make initial well-informed decisions. However, if my own anecdotal experience is any consolation - once the Data Science/Analytics teams have hammered out the data sets they like and have determined concrete means of measuring a given set of core data for the business - it's then passed back to people like me to handle the staging, transformation and loading of that data into a tested, ACID-compliant solution that the rest of the business can then interact upon. Two final points: * Strata was originally a Hadoop conference, so linking to a conference that has bias in NoSQL solutions is a bit laughable. * The game development community has already solved this issue when it comes to multiplayer gaming and their solution was much like my own. The need to access data extremely fast and perform calculations/operations on that data exceeds what the capabilities of RDBMS are able to do within the time they need to do it - so they developed in-memory key stores to hold that data, with an ever-so-often write to an RDBMS that ensures data integrity and acts as a persistent store. 
&gt;OLAP architectures and OLTP architectures are two different areas of study. The distinction is fading due to projects that combine OLTP &amp; OLAP, like Apache Ignite, Rockset &amp; TiDB. DW departments that used to run on RDBMS systems now run on S3+Hadoop. The only conferences that care about MS SQL or Oracle are put on my MS or Oracle. I look at the tea leaves and think RDBMS databases are increasingly being marginalized. YMMV.
He moved on, started a family! 
Heres a pretty simple (and poorly written) block of TSQL code that will use gmail to send emails. &amp;#x200B; Hopefully this helps you and gets you started. &amp;#x200B; public void SendEmail(string body) { &amp;#x200B; MailMessage message = new MailMessage(); SmtpClient smtpClient = new SmtpClient(); &amp;#x200B; string HOST = "[smtp.gmail.com](https://smtp.gmail.com)"; int PORT = 587; string USER = "your [email@gmail.com](mailto:email@gmail.com)"; string PASS = "pass"; string FROM = USER; MailAddress fromAddress = new MailAddress(FROM); message.From = fromAddress; &amp;#x200B; message.To.Add(["Recipient@gmail.com](mailto:"Recipient@gmail.com)"); &amp;#x200B; &amp;#x200B; message.Subject = "Recording Progam Error"; message.IsBodyHtml = true; message.Body = body; &amp;#x200B; // We use gmail as our smtp client [smtpClient.Host](https://smtpClient.Host) = HOST; smtpClient.Port = PORT; smtpClient.EnableSsl = true; smtpClient.UseDefaultCredentials = true; smtpClient.Credentials = new System.Net.NetworkCredential(USER, PASS); smtpClient.Send(message); &amp;#x200B; }
yes, its great! :) 
&gt;Rockset &amp; TiDB. With a total of 5 jobs across the U.S. for both technologies between Monster and Indeed.
I use [my favourite text editor](https://www.vim.org/) with SQL highlighting and completion support, combined with the [`bq` command line tool](https://cloud.google.com/bigquery/docs/bq-command-line-tool).
 I found a free online tool, it can convert Unix timestamp to human readable date format: [http://www.unixresources.net/](http://www.unixresources.net/) 
Didn't see this comment, thank you for the help. That's great I am going to actually generate one right now. I have actually been going through the stored procedures looking through each function to understand why we're using the queries we are. I have not seen any joins (surprisingly). Instead, I've found that our Developer has made a temporary table where she places all of the data and then the final report uses the data from this temporary table. I think the temporary table is a holding place for the data to be formatted according to the end result of the report. I've also begun doing the select top 10 query to see what columns are ion what tables. This has been such a simple way to get a gauge of the database. 
Does the duration really matter if you can run it in an automated fashion, say at 2am?
Sqlite only supports one write transaction at a time. Unless it's using [WAL mode](https://www.sqlite.org/wal.html) no other connections can read from the database when a writer is active. So if you have hundreds of creates (I *really* hope you meant inserts, because if you're creating hundreds of tables you have other issues (that also include the potential for significant slowdowns)) a second, it's going to likely be slow and you should look into WAL mode or a different database engine.
yes I did mean insert. Would postgres or mysql work better? I read that sqlite can do thousands of writes per seconds but for some reason it's obviously not working. It seems to break at around adding 150 per second, and it probably needs to handle 10000+ per second. What about caching 100 inserts and doing a single transaction?
Yes, I'm working on it "interactively".
Hard to say without more details. In SQL server I use SSIS packages for big jobs but simpler ones are just a script or stored procedure that executes each step in the process. It might look like If you can script everything as a series of sql statements you can run it as one script or a stored procedure. Or check out Drake which works like Make but is designed for data pipelines. 
I'm just wondering why you'd need to import a DB everyday. Couldn't you migrate rows higher than the last import maximum?
Oh man, you shouldn't have. This is super and all, but my in-house dev figured it out after I gave him a piece of my mind when everyone here told me that this isn't really much of a task. Some Googling + Stackoverflow and he managed it. As of now, it's: John Doe - JD1 Jack Doe - JD2 etc (same as desired, without the extra 0s). Additionally, every time the numbers are exhausted, it automatically adds another one. This eliminates the possibility of running out. I am really sorry for not commenting here earlier and mentioning that it's mostly done, was swamped with work and since the issue was more or elss rectified, wasn't too concerned about reddit. That, and I never thought I'd have someone come in and write the actual code. You, sir, are the hero that we do **not** deserve. Now, I apologise if you think this was me arguing me people that know simply because it's the way I want or cooler, like you seemed to think. The thing is this UID is **NOT a substitute** for the function that a UID serves in a db. The 'UID' in the db is just as everyone has mentioned, this is being called a UID only because my dumb non-techie ass couldn't think of anything when I first coined the term for a unique membership/customer num. Again, This is something else altogether that I'm just calling a UID. It is a membership/customer number. *Who cares? I can just add another 0 -* I'd pointed that out cuz @LetsGoHawks had pointed out that once I run out of IDs, it'd be a task to update the code to add another digit and ensure that I can continue with the old IDs being read, or so i interpreted. Again, a big big thank you for your comment man, I truly feel bad for not coming here sooner and telling the people that they were a big help, even if it was just to tell me to tell my devs to buck the fuck up, and that it's solved. I'm going to reply to you on PM about the other stuff. 
Are you trying to say "an event happened" or "this is the event"? I have a dozen email alerts for sales information. On a MERGE UPDATE to the \[I already sent this data don't send it again table\] I take the output, if there is any, and put it into an HTML email and send it. I could probably just use an "insert where null" sort of thing, but this gives me the flexibility to say "hey you deleted something" or "hey you changed something" should the need arise. First enable database mail, like biggrigg and mirz99 said, then you can call something like this: &amp;#x200B; This email looks for inconsistent pricing between a customer price list and what the sales person sold it for or requested it be sold for. If the condition is met then the rows are inputted to a table that tracks what rows have already been emailed, and the $output gets sent to a declared table, then the email is sent to the proper salesperson and their manager, CC'ing myself and other people. I had to cut off the top part of the query that generates the temporary tables since it was too many characters for reddit. The query is an element of an SSIS package that SQL\_Agent calls every 30minutes. `mergeupdate stuff goes here` `)` `OUTPUT` `]` `INTO @summaryOfChanges;` `IF OBJECT_ID('tempdb.dbo.#summaryOfChangesOnlyInsert') IS NOT NULL --gets rid of the temporary table.` `DROP TABLE #summaryOfChangesOnlyInsert;` `SELECT` `CHANGE` `and other columns` `into #summaryOfChangesOnlyInsert FROM @summaryofChanges WHERE change = 'INSERT'` `IF ((select count(*) from #summaryOfChangesOnlyInsert) &gt; 0)` `BEGIN` `DECLARE @xml NVARCHAR(MAX)` `DECLARE @body NVARCHAR(MAX)` `SET @xml = CAST(( SELECT` `SalesOrderNo AS 'td'` `,'', OrderDate AS 'td'` `,'', ItemCode AS 'td'` `,'', [ItemCodeDesc] AS 'td'` `,'', QuantityOrdered AS 'td'` `,'', [UnitPrice] AS 'td'` `,'', [AcctLevelPrice] AS 'td'` `,'', [StandardUnitPrice] AS 'td'` `,'', Discounted AS 'td'` `,'', SalesPersonNo AS 'td'` `,'', SalespersonName AS 'td'` `,'', AcctNo AS 'td'` `,'', CustomerName AS 'td'` `FROM #summaryOfChangesOnlyInsert` `FOR XML PATH('tr'), ELEMENTS ) AS NVARCHAR(MAX))` `SET @body ='&lt;html&gt;&lt;body&gt;&lt;H4&gt;` `Multi Account Price Level alert for wrong priced items` `&lt;br&gt;The following sales orders contain items ordered at the wrong price as prescribed by the Customer Price Agreement or are discounted from list.` `&lt;br&gt;` `&lt;br&gt; The discount% is the discount from list price. Note that there are additional columns if you scroll to the right.` `&lt;br&gt; Please review correct prices and procedures for the accounts listed in the table so that the problem is corrected going forward.` `&lt;br&gt;&lt;br&gt;&lt;br&gt; This email is automatically dispatched by the server every 30 minutes if there are any invoices &lt;font color="red"&gt; that have any items that meet the following criteria: &lt;br&gt;An item` `that has a price agreement was not sold at the agreed upon price |or| the item was not sold at list price &lt;/font color&gt; &lt;br&gt;&lt;/H4&gt;` `&lt;table cellpadding="1" border = "1"&gt;` `&lt;tr&gt;` `&lt;th bgcolor="#00ccff"&gt; SalesOrderNo &lt;/th&gt;` `&lt;th bgcolor="#00ccff"&gt; OrderDate &lt;/th&gt;` `&lt;th bgcolor="#00ccff"&gt; ItemCode &lt;/th&gt;` `&lt;th bgcolor="#00ccff"&gt; [ItemCodeDesc] &lt;/th&gt;` `&lt;th bgcolor="#00ccff"&gt; QuantityOrdered &lt;/th&gt;` `&lt;th bgcolor="#00ccff"&gt; [UnitPrice] &lt;/th&gt;` `&lt;th bgcolor="#00ccff"&gt; [AcctLevelPrice] &lt;/th&gt;` `&lt;th bgcolor="#00ccff"&gt; [StandardUnitPrice] &lt;/th&gt;` `&lt;th bgcolor="#00ccff"&gt; Discounted &lt;/th&gt;` `&lt;th bgcolor="#00ccff"&gt; SalesPerson &lt;/th&gt;` `&lt;th bgcolor="#00ccff"&gt; SalespersonName &lt;/th&gt;` `&lt;th bgcolor="#00ccff"&gt; AcctNo &lt;/th&gt;` `&lt;th bgcolor="#00ccff"&gt; CustomerName &lt;/th&gt;` `&lt;/tr&gt;'` `SET @PRCemailAddress = REPLACE(REPLACE(concat('',(select EmailAddress from #summaryOfChangesOnlyInsert GROUP BY EmailAddress),';'),CHAR(10),''), CHAR(13),'') --8/31/2018 added ' to beginning of concated string, otherwise it wasn't sending the email. I don't know why it worked for the sales manager email address without that but what the fuck ever. \` `-- 10/5/2018 added the replace function from: https://www.reddit.com/r/SQLServer/comments/9lomsf/carriage_return_in_concat_do_not_want_seems_to_be/e78he10/?context=8&amp;depth=9` `print @prcemailaddress` `SET @ManagerEmailAddress = concat((select SalesManagerEmailAddress from #summaryOfChangesOnlyInsert GROUP BY SalesManagerEmailAddress),'; readacted@redacted.com')` `SET @EmailSubject = (select concat('MALP Sales Order Alert for:',' ', SalespersonName) from #summaryOfChangesOnlyInsert GROUP BY SalespersonName)` `SET @body = @body + @xml +'&lt;/table&gt;&lt;/body&gt;&lt;/html&gt;'` `exec msdb.dbo.sp_send_dbmail @profile_name = 'ardexreports'` `--, @recipients = @PRCemailAddress` `, @copy_recipients = @ManagerEmailAddress` `-- , @blind_copy_recipients = 'readacted@redacted.com'` `, @reply_to = 'readacted@redacted.com'` `, @subject = @EmailSubject` `, @body = @body` `, @body_format = 'HTML'` `END;` `SET @Count_of_rows = @Count_of_rows+1` `delete from @summaryOfChanges` `END`
Thank you all! With your abuses and encouragement, I was able to pass on the abuses to my in-house dev, telling him he should be able to figure it out himself and, with stackoverflow and some Google, he's managed to. We also incorporated some feedback from u/LetsGoHawks to ensure that the rule set for the IDs doesn't end (run out of IDs). Additionally, I apologise if I came across as someone that wasn't willing to listen to any of your feedback even though you'll are the ones with the tech knowledge. The thing is, this *UID* is **not** a substitute for an actual UID in a DB. This is just the term that I coined when coming up with a rule to automate the generation of a unique customer id. We'd use this in our analytics model and this is how the db would identify which player the data belongs to. Now that the website is soon going to go live, we wanted this to be automated by the db itself, on account creation, and not have to manually create them like we'd been doing up until now. My bad, for not being clearer at the very beginning. An important factor was also recall value of this ID so that we can try and get them to remember their IDs, like a habit. &amp;#x200B; A big shout out to [**u/notasqlstar**](https://www.reddit.com/user/notasqlstar/) for his post wherein he took the time to actually write the code down that could be used for this, without me even sharing the existing code that was being used! People like you are the reason I trust and use reddit as much as I do. &amp;#x200B;
fitty bucks to do homework assignments, my man
Who said anything about every day? I'm importing it whenever new source data is published, but I'm frequently modifying and improving the query, adding new data sources, adding aggregated fields, and so on.
Thanks! Is that [this Drake](https://ropensci.github.io/drake/)? It sounds exactly like what I want, except it's for R and not for SQL. I suppoooooose I could trigger SQL queries from R, but then I'd have to learn R first...
Well, my point still stands. You'll need some way to track what you already imported in order to migrate the Delta's. I've never heard about an efficient tool that would do this for you, and if it exists I'm curious :p
So in a nutshell, the more 56K users we have, the more data has to stay in the RAM, and the higher the probability becomes of the server crashing, or slowing down for other users who aren't on 56K modems.
Don't feel bad. I come here to learn and teach myself by helping others.
It is not recommended to shrink your database / database files because there can be a chance of corruption, but doing this SOMETIMES is alright. &amp;#x200B; If your T-Log is growing daily and you have to keep shrinking it, this means you have not given the drive enough space or your T-Log backups are too infrequent. Doing regular shrinks is not a good thing and your drive should be properly sized so that it's an exception to the rule to shrink the T-Log, not a daily activity. 
&gt;Well, i actually mean 'sql server memory', just a bit lazy to type. This is memory that is allocated (given) to SQL server by OS. So you're talking about RAM. &gt;The information about those is stored in the server memory. There's a piece of server process that will determine (depending on the resources needed/available/quotas/etc.) which request and how many steps of it will execute next. So in this example, if we have 100 concurrent users all asking for 1GB of data, the server is trying to transfer 100GB across a network that is not able to accept it efficiently, so we have 100GB of data filling up the RAM. &gt;This depends on the actual architecture of the application. Let's say a Tableau page refresh will detect the same data request and will wait for the original request to finish to allow for the result to be reused. In a different system the application behind the page might finish all the data work before detecting that the client is dead and the page refresh might initiate a different process using very same queries second time. I understand my questions are highly specific to however our server is configured, but I have no way of seeing how it is configured, and have reason to believe the people who have configured it don't know, either. Regardless of what, I know Tableau sends quite a lot of data when a user is requesting a result from a workbook connected to a large data set. For example, I can open 5 workbooks at the same time and watch how 4-5MB are sent across the VPN in seconds. This multiplied across 100 requests would seem to be a non-trivial amont of data being sent and stored in RAM over long periods of time (hours.) &gt;sql server has different kinds of locks; queries place shared locks on the table storage (records/pages/etc) and stability (?? i think they are called that) locks on schema. These dont prevent readers from reading. I don't know exactly what is going on behind the scenes but I know there is a database on the server that deals with user accounts, requests, etc. So compounding whatever data is being requested and sent, there are jobs going on which may lock transactional tables, and in the event they fail or start to hang... wondering how that would interact with the example of 56K users. 
I created a reddit account to tell you that i sincerely appreciate your efforts and I look forward to viewing your current and future content. Cheers!
Look to VCs for trends. Look at Indeend and you’ll walk away thinking there’s hope in learning FORTRAN. These HTAP DBs previously mentioned have about $50M in funding each, they’re not well know yet, but they will be. Snowflake was at $50M in funding not too long ago and they’re now at $1B total. Google Spanner is growing in adoption. These are exciting times. We can put a fork in MS SQL, Oracle &amp; move on to better tools.
Who said anything about deltas? :P But you're right, some kind of tracking would be needed. It could simply be based on the timestamps of the input file and last-modified timestamp of the table (insofar as there is such a thing), or it could use an auxiliary table that contains hashes of source data.
You can write a while or if into a stored procedure (or several) to create a similar outcome, t-sql would have an easier learning curve and integration. Depending on your checks, server rights, and server capacity I would suggest a table that stores meta data or what step was the last to finish correctly then read from the table, I have process automation that run as such.
Hey man. Viewing the image, I can’t make out any of the conditional logic on mobile. Could you screen cap and upload that? Feel free to redact schema and propriety pieces from it.
Overly simplified example (please ignore the obvious lack of normalization on the tables and lousy field names): select p.firstname,p.lastname,p.DOB, c.CrimeName,c.EventDate from person p join crime c on p.personid = c.perpetratorid From there, do the math on their age using `c.EventDate - p.DOB`. That's where things start to get nuanced - do you want whole years, partial years, round to the nearest year, etc. The simplest would be`datediff(year,p.dob,c.eventdate)` but that won't round up to the nearest year.
&gt;I am sensing that your answers are too technical and you don’t like it. I think you are 100% right on this one, but that's not a reason to give him wrong answers that he can "agree" to. In sql server, the result set is not necessarily fully instantiated before returning the result to the client. Readers place shared locks and these are compatible, so you can place another shared lock on the table, so other queries WILL COMPLETE on the same table (deletes won't but this wasnt his question). 
So you want to see which IDs had duplicates in Table A that have been removed in Table B?
Yes
dude, I agree with /u/DoctorRin and I think you are fishing for an answer that you like. &gt; So in this example, if we have 100 concurrent users all asking for 1GB of data, the server is trying to transfer 100GB across a network that is not able to accept it efficiently, so we have 100GB of data filling up the RAM. No, that's not right, generally. The fact that you have 100Gb of data to process does not mean that SQL server HAS TO process them all before giving SOME results to the client. For example, in your "select * from tableA" example, the server just need (roughly speaking) to have a structure in a way similar to a cursor to point to the place where it is reading from and only read the data when the client is ready to receive that. &gt; Regardless of what, I know Tableau sends quite a lot of data when a user is requesting a result from a workbook connected to a large data set. For example, I can open 5 workbooks at the same time and watch how 4-5MB are sent across the VPN in seconds. This multiplied across 100 requests would seem to be a non-trivial amont of data being sent and stored in RAM over long periods of time (hours.) Tableau is a tangent but I think you also need to look into the concept of caching if you want to start thinking of the internals of the processes. Meanwhile, you can probably watch or read more about locking in sql server to understand the implication of concurrency. Here's one such video: https://www.youtube.com/watch?v=f57F-ov30gs 
So the problem is both tables have the same IDs so when you try to join just the tables they return the same data. What I would do would be to set up a temp table with a count of the IDs with duplicates in Table A. &amp;#x200B; SELECT "ID" , ROW\_NUMBER() OVER(ORDER BY "ID) AS "COUNT" INTO #temp FROM Table A &amp;#x200B; Then you select the IDs from the temp table where the count is &gt; 1. SELECT DISTINCT "ID" FROM #temp WHERE "COUNT" &gt;1 &amp;#x200B; This should give you a list of the IDs that have duplicate values. &amp;#x200B; You could then take that list and query Table A or B to get a the rows that you need. &amp;#x200B; SELECT \* &amp;#x200B; FROM Table A &amp;#x200B; WHERE "ID" IN (SELECT DISTINCT "ID" FROM #temp)
This will show all rows from A that are not in B SELECT * FROM TableA EXCEPT SELECT * FROM TableB You must have the same columns otherwise, swap the specific columns for the star. Also, it depends on the ordering, A must be bigger than B. 
Can I modify this... Create table want as SELECT * FROM TableA EXCEPT SELECT * FROM TableB 
I am fishing. I have a hypothesis and I am asking you and others to try and tell me it is wrong. I'm not asking you for the answer. &gt;No, that's not right, generally. The fact that you have 100Gb of data to process does not mean that SQL server HAS TO process them all before giving SOME results to the client. For example, in your "select * from tableA" example, the server just need (roughly speaking) to have a structure in a way similar to a cursor to point to the place where it is reading from and only read the data when the client is ready to receive that. So, the process will read 128K send to client and wait until the client is ready to receive more before reading more rows. I understand this. What if any criticality is there when requests come in ~100 at a time? Again, I realize this is probably best answered by "it depends on your configuration." &gt;Tableau is a tangent but I think you also need to look into the concept of caching if you want to start thinking of the internals of the processes. My team is not involved in this process at all, we are the clients, and our site used to be going down every week, although I noticed this was because a database job was failing. I had our IT team rewrite that job with error handling so that it "can't" fail and the crashes have stopped. However the site is still slow. Management wants to know *why* it is slow, but based on an analysis it just seems that Friday is one of our peak usage times, and that when you combine peak usage + failing jobs you get a server crash. So now the question is changing into "how can we make it all faster," and my hypothesis is that our internal VPN is very slow on Friday's after lunch because on Friday after lunch people are bored, go on YouTube, whatever. This would be happening "in the background" and have nothing to do with our site, per se, except that if the VPN is suddenly throttled and no longer able to push data as quickly as normal... my questions might start to make more sense. &gt;Meanwhile, you can probably watch or read more about locking in sql server to understand the implications of concurrency. I think I understand the issue fairly well, and in our internal analytics environment we have this happen pretty regularly where me and another user are "competing" for an objects attention, and then someone drops connection, or their process starts rolling back... the server begins to hang... we have to ping the DBA to give it a kick in the ass... and then everything works. The problem here is that we have no DBA for this particular cloud environment, and I can't see the configurations going on. I can just see the behavior of the environment and try to make educated guesses, then bring those guesses up when meeting with the team that is responsible for managing this environment in order to force them to analyze/configure around specifically mentioned requirements. For example, I cannot ask them, "is this because our tables are locked," because I will just get an answer that is either, "Nope," or, "We don't know." But what I can do is force them to write their processes using WITH (NO LOCK) and then see whether or not that had any noticeable impact or not. Presently I am of the mindset that there is nothing "wrong" with the cloud environment, and it is just slow because it is peak usage. The question then becomes how to we optimize our Tableau objects, and whatever database jobs are going on in order to accommodate peak usage times, and not a discussion about, "what is going wrong?" I'm not here fishing so much as hoping someone can tell me, "No, that is wrong, that can't be happening, and here is why."
What kind of use are you envisioning this data as having in the future? E.g, is it going to have a very occasional request that you could run a bespoke query for, or are your users going to expect direct access to it? How complicated are their requests going to be: is it always going to be a case of 'here's a client number, show me history' or are they going to need to look at different entities in different ways? What type of sql database is it? My first thought would be to do a recce of the requirements from the business users, use the DB dictionary to build views or similar to support those reporting requirements, and use a reporting tool to serve that data up to users. The exact tools would depend a bit on the platform the data is on. 
Personally, I don't. Like you I've often looked for something, but I always end up spending waaaayyyy too much time learning and managing the application. 
How about creating a table in your PostgreSQL system to track the job, job duration, and job completion time. That way you can just query that table for all the relevant info.
AgeAtTimeOfCrime, CurrentAge, if you are running analysis you need to preprocess those necessary fields... and I hope the state knows that. So.. you could write a case... sum(case age between...) Or... you can create a table with a RangeType of CommonAgeGroup.. Where the table would have a min and max column. Then you just Inner join tblRanges on age between min and max and typeid ='common age' group by rangeid
Your requirement isn't totally clear to me. Perhaps I'm missing something. Usually an ETL tool like Pentaho (open source) would tick all the boxes in terms of handling loads, load dependencies, recoverability and version control. If that's a sledgehammer to crack a nut then you could just create your own in whatever programming language your comfortable with e.g.build framework in python or bash to execute sql and version control code in git, then automate via Cron. There are open source tools such as airflow which would help with job dependencies but that might be over engineering your solution somewhat. 
Sounds to me like what a query cache does. At least for the "base" queries that run on the input data. If you invalidate the caches by rewriting the data even when it is not necessary, you could store a hash value for every tables content and only run a query if one of the base table hashes changed.... kind of like a self made query cache.
what makes you think it won't grow back to 60GB again?
Hello dear friend, you sound exactly like me 5 years ago. Got an Econ undergrad and a MBA, graduated in 2009 when there were no jobs. Needless to say, I was in construction sales for a while. It’s going to be tough to drop back to absolute entry level, but it’s most likely what you’ll need to do. It’s what I did and I don’t regret it at all. By cutting my salary in half for a year, I gained all the experience and skills I needed to make up for that lost salary. Find a company that will take you in, and while you’re interviewing them, make sure they have some tenured analyst/DBA/etc staff and hopefully someone that can mentor you. Become the student, be kind, thankful, helpful, and willing to bust your ass to learn and get better. Do some quick learn tutorials on excel, power pivot, power query, powerBI, SQL, and the like. In my experience, these trainings don’t do half as much as hands on experience, but it’s nice to have some familiarity with the functions. That’s been my experience and I’m glad I did it. Love where my career is now and where it’s headed. Best of luck!
A typical career path is doing support role (help desk, app support, etc) and then moving onto a more specific role. You should look into it, just to get your hands dirty, also these roles will get you exposure to a lot of other roles that you may not know exist just yet and might interest you. Let the company know what your ambitions are, and all the best. Smaller companies, or smaller teams will typically give you the greatest exposure to various tasks while larger companies have more structured roles. Reach out to employment agency as they usually have a bunch of entry level tech positions available. 
Sorry, in your particular scenario you might need a) a decent DBA who can understand your service configuration (beyond just the database itself) and can start monitoring the DB and the instance performance b) a decent network engineer who can monitor your network to see what kind of congestion/quality issues you might be having c) a decent systems engineer to look at what is happening with your instances themselves d) a decent Tableau engineer to re-engineer your workbooks/data sources, because, let's face it, you should not be running live queries in Tableau that bring back 100Gb (not even 1Gb, for that matter) (unless you're running behind memsql or something). All this takes money and time, so, oftentimes, a quick 'win' is just to throw money at the problem and upgrade everything. And, more often than not, your specific situation will be a result of combination of factors. You can find out stuff that you haven't thought of before as well. For example, you seem to be dead set thinking that sql server chokes when clients retrieving data too slow. Well, SQL server (and networking) can choke when your clients are too fast also - in fact our servers used to choke at certain level of output. We had to upgrade the server instance and networking to sustain peak request volume and magically, the problem disappears.
I think so. That looks like an R package for [Drake](https://github.com/Factual/drake) but I don’t know for sure. 
I've been considering that, but I was hoping there was some existing tool out there so I wouldn't have to reinvent the wheel... :)
Pentaho and Airflow are words I had not heard before, I'll look into them! Homegrown Python scripts are the fallback plan, but I'm not looking forward to parsing SQL to figure out the dependencies.
To some extent it's like a cache, yes. But your average query cache is not persisted on disk or accessible for debugging purposes, which is why I use a bunch of intermediate tables. (And also to keep my queries at least somewhat small.)
You need to scope this, and a single 30 minute call will not be sufficient; you're miscalculating this by multiple orders of magnitude, assuming Solution A is even slightly business critical. **The client needs to specify what historical data they need and how they need it, in very precise language.** Screenshots of the sort of info they have access to in Solution A that they will need to replicate (piecemeal or otherwise) in your interim solution are a good idea, annotated or highlighted if appropriate. You've said you're a server admin here, not a software architect, so that interim solution will probably require someone else, unless the 1700-column dump is simple enough for the client to reference themselves as a spreadsheet, Access database, denormalized list of records... that sort of thing. The more complex and difficult way forward is to actually migrate A's data to B, with as much of the data and relationships intact as possible, and using B wherever possible for historical lookups. Obviously, that's completely dependent on how versatile B is and how ridiculous A's schema is; mapping highly organized data into very disorganized "notes" or generic user-defined fields, etc. is very common as a stopgap. It's typical that a read-only copy of A remains available (if not online) indefinitely as a legacy system just in case some conversion bug is discovered and you need to see the original data or interface years down the road... and to do that properly you'd have to keep paying for A, or replicate the view-only parts of A without an intimate understanding of it, which is rarely feasible.
If your comfortable with python have a look at airflow,pinball or Luigi. It will handle dependencies for you, they are frameworks for chaining code together not ETL tools.
Thanks! That link looks better. I'll have to see if I can find plugins for DBMSs and hopefully SQL dependency inference, but the "make for data" concept sounds spot on.
Python and I are best friends. And I have part of my workflow in Python already. I'll check those out, thanks!
So, if your goal is to be a BA and you want to get there as fast as possible, I would first figure out what industry suits you best. The responsibility of a BA will be different depending on that industry, and the technology involved may be different. For example, in the health insurance you will need a strong foundation in operations and policy, and most of the questions you will answer or report will only involve basic sql query syntax. Other BA roles may focus more on the visualization aspect which would probably involve using tableau, powerbi, ect. and the querying of the data is someone non consequential (like if you are just hooking up one of these tools to a view / table). BA work is half way between data science and half operations usually. Begin looking at job postings at lots of industries and get a feel of what kinda mix of responsibilities might work for you. &amp;#x200B; TLDR: SQL is a staple, but you may want to look into other software as well depending on where you want to be a BA. If you want to stay in a sales / marketing performace BA then I would look into visualization software as well like Tableau and PowerBI
&gt;a) a decent DBA If we had one of these working for us, I wouldn't be asking half the questions here that I find myself asking. &gt;b) a decent network engineer These we may have, but none of them are currently engaged because at the moment the thought is that everything is related to some kind of "error." &gt;c) a decent systems engineer to look at what is happening with your instances themselves See above. &gt;d) a decent Tableau engineer to re-engineer your workbooks/data sources, because, let's face it, you should not be running live queries in Tableau that bring back 100Gb (not even 1Gb, for that matter) (unless you're running behind memsql or something). That's me, kind of. And I was being facetious with my numbers to look at extreme possibilities as it relates to points A-C. The numbers are much smaller, but can add up quickly when you're talking about 100 users at a time. &gt;All this takes money and time, so, oftentimes, a quick 'win' is just to throw money at the problem and upgrade everything. And this is what I'm trying to avoid. I can see the cluster for what a fuck it is, but need to ask questions that get outside of my area of expertise. I know you can't tell me what's happening, just as I can't "divine" what is happening, but if I can plausibly come up with a hypothesis that *may* be happening then I can force our IT groups to do &lt;insert plan here&gt; and then see if it mitigates performance. Sounds stupid, but it is what it is. &gt;For example, you seem to be dead set thinking that sql server chokes when clients retrieving data too slow. Well, SQL server (and networking) can choke when your clients are too fast also - in fact our servers used to choke at certain level of output. We had to upgrade the server instance and networking to sustain peak request volume and magically, the problem disappears. I'm not dead set on it, but this exact statement here is what I was looking for here. In summation: 1. It matters on how the server is configured. 2. I can assume our server is not configured for either too fast, or too slow. Because no one ever thought about it, and we don't have a DBA asking these kinds of question. Make more sense why I'm here now? 
Just to give you a little additional background into my situation. Same group I'm dealing with recommended we move our entire analytics database to the cloud (separate environment than the one being discussed here) because our weekly load times are taking too long, and they figure by moving to the cloud and "upping the cores" they can get it down to a reasonable amount of time. So, going through some of their code last week I found a view that leveraged 15 nested subqueries which was taking 120 minutes to run. Took me about 15 minutes to take all the subqueries, put them into #tables, throw an index on the #tables, and then just rewrite the 'view' as a sproc that joins to the #tables and now it takes 4 minutes. So I know what I'm being told by them is shit, and I know they don't have any quality DBA working on the environment, and I'm just trying to think outside of the box relative to what might cause the symptoms that I can see. -- If they can't fix a simple thing like that, what are the chances they have the environment properly configured for 100 concurrent users, making sure tables don't lock, etc.? I realize that isn't a best case, and it is largely hypothetical, but it's an interesting conversation where I can learn something.
Please see how I stated some of this behavior can actually vary based on config. Problem is the question isn’t quite direct.
This conclusion is generally true but you should factor in indexing and sql intelligence that takes place when multiple queries of similar requests happen. I recommend looking at profiler at a time when activity is low and cracking open a DBA book. I am self taught and have no certs but I been heavy doing SQL server maintenance, reports and queries for close to three years now and have experienced what you are essentially getting at. That is slow remote requests of large data holding up the lunch line.
I don't think you understand, buddy. If I had access to any of those things, or if this were my job, I wouldn't be here asking you questions. I can't see these things, and I'm not interested in being a DBA. I'm the client and I have a feeling my "DBA's" are lying to me, and in fact I know they aren't DBA's. So when I hear that things are "generally true", or "possible unless you configure your database otherwise," then my assumption immediately becomes that none of those things have been done, and I'm on the right path.
I’d like to learn from you in this moment. What have I said that is wrong? I don’t want un-earned gold.
I wish you luck...I’m in the midst of a career change myself, after 10 years of tech support it’s been a bitch trying to break into a database admin role or data analyst even after obtaining a DBA certification. 
Tableau has HOURS of free training videos. It helps to know SQL so you have an idea as to how the engine works, but those videos will give you some great exposure.
The problem with PIVOT (and, I think, where you're going with your reply to u/abstractsqlengineer's question) is what happens when your data gets new weeks added? Do you change the query every week to include the new week? Do you write all 52 weeks into the PIVOT right off the bat? You can; the PIVOT syntax doesn't mind if the weeks you define don't exist yet. You just end up with a bunch of Null values in your columns: Select item, [1],[2],[3],[4],[5],[6],[7],[8],[9],[10],[11],[12],[13],[14],[15],[16],[17],[18],[19],[20],[21],[22],[23],[24],[25],[26],[27],[28],[29],[30],[31],[32],[33],[34],[35],[36],[37],[38],[39],[40],[41],[42],[43],[44],[45],[46],[47],[48],[49],[50],[51],[52] From ( Select [Week], item, sum(value) as [Value] From #table Group by [Week], item ) base Pivot (Max(value) For [Week] in ([1],[2],[3],[4],[5],[6],[7],[8],[9],[10],[11],[12],[13],[14],[15],[16],[17],[18],[19],[20],[21],[22],[23],[24],[25],[26],[27],[28],[29],[30],[31],[32],[33],[34],[35],[36],[37],[38],[39],[40],[41],[42],[43],[44],[45],[46],[47],[48],[49],[50],[51],[52] ) )pvt Weeks has a limited set of values, but what if you don't know what your column values will be? Maybe you want the "Items" to be the PIVOT columns instead. It gets pretty messy then. So the (an?) other way is to build up the pivot query in a string and execute it, so that the column list can adapt to new data in the table: declare @Weeks varchar(500) ;with weeks as ( select distinct [Week] from #table ) select @Weeks = STUFF(( SELECT ', [' + cast([Week] as varchar(2)) + ']' from weeks FOR XML PATH('') ), 1, 1, '') exec (' Select item, ' + @Weeks + ' From ( Select [Week], item, sum(value) as [Value] From #table Group by [Week], item ) base Pivot (Max(value) For [Week] in (' + @Weeks + ') )pvt ') It's a little hacky, but it's the only way I've found to make a SQL PIVOT useful. Most of the time when I've wanted pivoted data, the data in the pivot column isn't finite, or it changes based on parameters. I worry a little about SQL injection with this method. I've tried to intentionally compromise the query and I've never been able to manage it, but that could very likely be because I don't know enough about how to do it in general. Here the weeks are defined as INTs, so I doubt it would be an issue. Please correct me if you know better.
This. Btw. It's not hacky. I do an enormous amounts of data driven actions. You could get your list from a date table prior to dynamic execution. I would print out several different dynamic exec querys and make sure your exec plans and indexes are good... but no. This is not hacky and its very efficient if your plan is efficient.
Also, QuoteName(week) returns [1].
So, you have a database backup (*.bacpac?) and some Excel-based documentation. Are you saying you need to keep the SQLServer instance live for 7-10 years? Or are you saying you need to pull 7-10 years worth of data from it? If you just need to get the data out, you best bet is to either (stand it up and then): 1) Write a simple SSIS package to extract the different data points you need into a CSV or whatever format it is you prefer. 2) Write some C#/Powershell/[INSERT LANGUAGE HERE] to connect and do the same. If you need to stand it up and keep it running, I mean, you'll need a dev/DBA to work with it. Finally, if you're saying you need to pull the data out in some denormalized manner you'll likely need someone. Having docs is one thing, but attempting to read+understand a DB structure can take time. This is lessened, if you already have a grasp on the data, but any person who tries to pull from it will need that knowledge before they can do it *right*. 
Ugh, is this what slows down - your Tableau workbooks? Are you running Tableau server or is this all from desktop users? 100 concurrent users is not a trivial load on Tableau server. If it is desktop load - do you have the server? If you have the server - how is it configured? I hope it doesn't share the box with your DB instance.
This is Tableau Server in an Azure cloud integrated with a SQL database of some kind for things like users, etc. &gt;If you have the server - how is it configured? Once the people who configured it find out, I'll let you know, but they sure find it helpful when I push them in the right direction. Do you feel me? &gt; I hope it doesn't share the box with your DB instance. HAHAHAHAHA, are you saying it shouldn't?
Nah, I dont agree. Your hunch(es) could be good. Could be not so good. There could be other stuff happening. If you don't have the expertise in-house - hire folks. Use standard best practices. Measure. Adjust. Measure again.
Everyone's experience differs, but this is the path I went down. Started out at a big, horrible, mind-numbing helpdesk as a contractor. Finally caught a break at a small helpdesk at a hospital where our whole IT department is ~35 people. Learned SQL as a hobby and made some friends down the hall over the course of 2 years. Data Analyst position opened up and got the job. Still here another 1.5 years later.
The dba at my office compares an auto int column. Insert into ...Select * where [auto int column] greater than @maxid. I believe he assigns the max id from the new table to a variable.
I'm dealing with the consultants they brought in. They are unable to answer these questions. When I said they find it helpful, what I mean to say is that they get annoyed that I make them answer very specific questions which you and others are providing me with.
I am assuming the database is in full recovery model and you're taking transaction log backups. Restore the transactions logs after the full backup onto the destination server? Sorry if this makes no sense. 
It’s definitely dependent upon the company you end up at. I started working for a small company (~130 ppl) last year after college and I’ve been able to get my hands in every part of the company. Background: I majored in engineering and got my MBA specializing in BA (mostly SAS experience with some SQL). After being there for a couple months, I convinced management to get a BI software called DOMO because I didn’t want to have to redo the same report over and over again in Excel. Since then, I’ve been building out dashboards for every department (customer service, supply chain, operations, marketing, product development, etc...) and learning more and more SQL everyday. I’m probably going to start learning R so I can further my career and BA knowledge. I would suggest looking at smaller companies because most people can barely use Excel, let alone write SQL queries. In no time you’ll be the one everyone comes to for Excel help and report requests. Good luck!
I agree with this. Is the transaction log 60GB because it was never backed (or seldomly) up or does the transaction log truly balloon to 60GB (due to index maintenance, heavy transactions, etc). Monitor your transaction log growth events to help isolate why it was 60GB. Also, check your log growth to make sure it's nothing silly like XX% and for BULK inserts without telling SQL to treat it was such.
In Oracle you'd apply the archive logs. Depends on the dbms
The question is definitely a thought experiment. I like the above answer and for what it's worth (with a massive grant of salt as I've been drinking): inb4 it's not SARGable bro! ;D select [p].[age], count(*) from person p inner join crime c on [c].[id]=[p].[crime_id] where datediff(yy, [p].[dob], getdate()) &gt;= 18 and datediff(yy, [p].[dob], getdate()) &lt;= 25 and datepart([p].[booking_date]) = '2019' and [c].[crime_type] = 'DUI' group by [p].[age] &amp;#x200B;
Agreed. He said PostgreSQL and I still spoke of MSSQL. My fault. :/
I apologize if this has been answered, but, in short, YES. Create your alter statement on server 1, ensure it works. Once you have confirmed your desired outcome works, setup the servers required under the same folder in your local registered servers through SSMS (ctrl+alg+g or view &gt; registered servers). Right click &gt; New Query, run the alter statement. Or, you can use SQLCMD through SSMS to paste the server name into the :connect variable and run the alter five times.
/u/GrandmasOkra are you in San Francisco? The company I work for will be hosting a free after-work SQL and analytics course, sometime next month. 
Replace SQL Server with Postgres or Redshift and I agree wholeheartedly
I agree there have not been any major changes compared to JS or other languages... but that made me think of the ways in which SQL has changed over the last 3-5 years, IMO. With the transition from row-based DBs (MySQL, SQL Server, Postgres) to usage of columnar DBs like Redshift or BigQuery over massive datasets, a different "variant" of SQL is more valuable. Columnar db's are extremely fast at grouping, window functions, aggregations over large, denormalized data, but "prefer" denormalized tables over joined normalized tables. Depending on the dataset and DB in question, I might be more careful about requiring as few CTEs/subqueries/joins as possible, but for other datasets and the access to a lot of compute power, I might write a 7-CTE query when only 4 were truly required.
That is what we do before a monthly ssis import. Get max ID before we start and if there is issues use that to remove the bad data. 
How will they provide the backup. To get the data will they use SQL server and just write queries or do they want a web front end of dragging and dropping. Sorry on mobile.
Crap I have copies of this query on laptop but am on mobile 
You might be able to pull it off but while delegating learn because the team will see through you in a second, I am shocked you pulled it off. I need to be less nervous in interviews.
If it’s still in your plan cache you can use sys.dm_exec_requests and sys.dm_exec_sql_text to find the query. Send the sql_handle from requests into sql text and filter sql text based on some keywords from the query you are trying to find
SSRS is extremely easy
It didn't look too bad I got a udemey course I plan on completing this weekend.
Im a healthcare data analyst...get ready for no analyzing and a-lot of report building 
The following (free) resource for learning SQL may help you [http://www.studybyyourself.com/seminar/sql/course/?lang=eng](http://www.studybyyourself.com/seminar/sql/course/?lang=eng).
You can learn all computer science related things online on your own without attending any school The following (free) resource for learning SQL may help you [http://www.studybyyourself.com/seminar/sql/course/?lang=eng](http://www.studybyyourself.com/seminar/sql/course/?lang=eng).
If you can get the database up and accessible, and all the data is there, making it accessible shouldn’t be terribly difficult...basically a matter of effort vs how nice they need it. If they just need to be able to search tables, some php pages can do that pretty dynamically...dropdown to pick the table and a reasonably straight select query can dump everything to the screen or file. If the tables are more ‘technical’ than the users can handle, the dictionary and someone with knowledge of the business rules and content should be able to provide enough insights for anyone with some reasonable sql knowledge to build the joins necessary to get the data easier to display. These are all relative, of course. 1700 objects could be a lot to drudge through if the database was set up haphazardly...or they could just be lots of data points that map easily to actual data they want. Similarly, if they just need to be able to pull data in chunks occasionally, a pretty simple interface should work...if they want fine grained search and sorting and grouping, it’s more work on that side.
If you just need some help looking the data over and some initial direction, I wouldn’t mind giving you a hand with that just out of curiousity. If you needed more in depth help with doing some of the more technical stuff and don’t find any better options, I could offer some consulting level help.
&gt; in some kind of readable, intelligent format. This I'd say is your problem. You need to have a great idea of the wants and needs and so on. What if they used BIDS or something to create the reports? You might not have the skillset to replicate, or give the exact reports in the same layout (and if I know clients they 'expect' you t replicate it exactly - otherwise it is wrong) - though perhaps your clients are different. With regard to output, something that can connect to the SQL and output reports will be fine - Excel probably might be a good bet. They have some awesome features for creating connections (that can be live-ish too!) 
Reminder
Ok, i managed to figure my first question out: &amp;128 is a BITWISE AND. SO it checks wether bit 8 of a given number is set. Some for &amp;64, it's checking the 7th bit of a given number. Still no clue on number 2 though.
That is so true. My time is constantly spent report building rather than data analysis
You're not really clear what you're asking, do you want a left join on a range like: table1.date &gt;= table2.Startdate and table1.date &lt; table2.Enddate Then use an isnull on the price modifier: table1.Price \* ISNULL(table2.PriceMultiplier, 1) AS Price &amp;#x200B;
Do you not have DPA? It reads the system tables and OS counters and stores and trends all sorts of really useful metrics, including query performance over time. Reboots and plan changes lose history, so while it’s possible to do something yourself, you’re likely just implementing a bad homegrown version of what DPA does.
SSRS is made much easier if you know how to pull the data out properly for reporting and use appropriate row and column groups on tables / matrices. I've fixed so many SSRS reports because the SQL "expert" relied on the built in query builder and then had an error laden mess of lookups. &amp;#x200B;
I would look at using a date table, with records down to an hour rather than a day. If/case logic is likely to get very complicated over a few years, making the function difficult to extend or debug. 
You could say it's a left join (if there are no matching dates in #dates2, then no extra periods should appear), but the result should output one row per "date range". The dates in #dates1 are the "master date ranges", and whichever date ranges are present in #dates2 - which lie within the "master date ranges" - should result in the output having extra date ranges. So many ranges. Try if you can make sense of the "desired output" SQL I wrote. :) I just updated it with some more sample data.
If you have access to create functions, you could create a function to do this that you input the two dates and it gives the difference. Then you can use that function on any select. Ready made? That could be a fairly monstrous bit of code using many functions. It also depends on your dbms. 
I have ipad in office today I will switch to laptop for login shortly 
If you want to optimize it, get rid of the subquery. Build a temp table with those values then join to your temp table. Subquerys perform poorly.
My calculations left me with this option only. this look so small from this point. 
Nice, I'll have to check this set up. Always hear that Vim has a steep learning curve but it's worth it. 
We have a table for working days that does this. Trying to do it without one is so complicated. Have to account for not only weekends, but holidays and half days. 
This and other replies are all excellent advice. I love this community, thank you. I will clarify with client the scope to make sure expectations are met and have them gather screenshots of the front end interface to help both pull together the data formatting they are used to. We also considered merging the data A-to-B and maybe we also need to compare cost of B doing that. Cut-over is scheduled for June 2019, so there is some time to plan. I'll follow up here and again, thanks!
I think extracting the data to another format like Excel for example would preclude the need to keep a full SQL server up and running, but yes the data itself needs to be available for audits and historical lookup. I will find out more and get back. I'll study up in SSIS.
Make a date dimension. This will contain a row for every date (01/01/1900, 01/02/1900... - 12/31/2100). Once you have this frame work doing any kind of date stuff is easy. Left Join the price table on dimdate.date &gt;= price.startdate and dimdate.date &lt;= price.enddate Same with multiplier table. Then create the price on any given day. Using case statements or whatever. 
Outstanding questions. I asked and this was reply: Audits – They will be having an audit in 2020 where they will need to access a specific set of data. One-off calls - Requests from time to time to confirm dates for legal, educational or benefit reasons. Would need ability to look up a client by name to confirm dates they were in care or other such info. Returning Clients – If we have someone previously in care with them, wanting to retrieve documents that have already been provided instead of requesting them again. **We see this being the case most often.** Yes, this is MS SQL. How complicated is it to build views in SSRS? Are there other tools that you would suggest to facilitate that? Thanks for your input and taking time to reply.
Thanks - I'm getting a few PM's offering consulting and coding help. I'm going to gather more details next week and follow up here and to the PM's in the event that we will need to contract out a portion of this. Thanks again for your willingness to help out!
If you’re trying to hard code the codes, the use the IN clause/condition https://docs.oracle.com/cd/B19306_01/server.102/b14200/conditions013.htm If you’re trying to get the codes from another query, then use the EXISTS clause/condition https://docs.oracle.com/cd/B19306_01/server.102/b14200/conditions012.htm
To clarify you want to enter multiple patient_ser values and multiple case_date values and the result set should contain only the doctor code?
It's a good idea, if I needed this data in denormalized form (the calculated price per day), but alas in this case, I need the date ranges as output.
I'd go with something like this: [Schema](https://imgur.com/a/C1Emqna) The idea is that when you have a journal, you create a Journal_Metadata row, and then link it to your Publication (not your PubType). I don't know Django specifically, but in DBIx (perl's ORM), from my Publication model object, I'd be able to access the "x_metadata" relation, where x is the model's PubType description
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/HBmKfVv.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20egygi8d) 
google "supertype and subtype tables"
Make a function similar to Excel's NETWORKDAYS() and it should do the trick. I have one that calculates "network minutes" for more precision, which might be what you're after in a ticketing system. https://stackoverflow.com/questions/4767627/simulating-excel-networkdays-in-sql
Omg. Do I have to do all the work. There are several ways to go from demoralized to a date range. One way are group bys and some kind of table calculation that changes every time the price changes.
Can you provide at least simplified tables? It's hard to write queries without knowing structure.
It doesn't need to be Vim; any editor that you're comfortable with will do. The main point is, you don't necessarily need an IDE.
OK, I made a horrible solution which works to generate the date ranges. Now the next trouble; to get the price and price multiplier onwards to actually use them.... ugh! ;WITH comb AS ( SELECT d.PropertyId, d.StartDate, d.EndDate, 1 AS [IsMasterDate] FROM #dates1 d UNION ALL SELECT d.PropertyId, d.StartDate, NULL, 0 AS [IsMasterDate] FROM #dates2 d UNION ALL SELECT d.PropertyId, NULL, d.EndDate, 0 AS [IsMasterDate] FROM #dates2 d ) , xx AS ( SELECT comb.PropertyId, ld.date, CASE WHEN ld.date = (SELECT MIN(d.StartDate) FROM #dates1 d) THEN 1 ELSE 0 END AS [IsFirstDate], CASE WHEN ld.date = (SELECT MAX(d.EndDate) FROM #dates1 d) THEN 1 ELSE 0 END AS [IsLastDate], ROW_NUMBER() OVER (ORDER BY ld.date) AS [rn], ROW_NUMBER() OVER (ORDER BY ld.date) % 2 AS [type], comb.IsMasterDate FROM comb JOIN dbo.lib_dates ld ON ld.date = comb.StartDate OR ld.date = comb.EndDate ) , yy AS ( SELECT xx.PropertyId, xx.date FROM xx UNION ALL SELECT xx.PropertyId, DATEADD(DAY, -1, date) FROM xx WHERE xx.type = 0 AND [xx].[IsLastDate] = 0 AND xx.IsMasterDate = 0 UNION ALL SELECT xx.PropertyId, DATEADD(DAY, 1, date) FROM xx WHERE xx.type = 1 AND [xx].[IsFirstDate] = 0 AND xx.IsMasterDate = 0 ) , zz AS ( SELECT yy.PropertyId, yy.date, (ROW_NUMBER() OVER (ORDER BY yy.date) + 1) / 2 AS [type] FROM yy WHERE yy.date BETWEEN (SELECT MIN(StartDate) FROM #dates1) AND (SELECT MAX(EndDate) FROM #dates1) ) SELECT zz.PropertyId, MIN(date) AS [StarDate], MAX(date) AS [EndDate] FROM zz GROUP BY zz.PropertyId, zz.type
I've used these two queries in the past. They don't show you a specific instance of a query being executed, but they look at "expensive" queries. Might not be exactly what you're looking for, but perhaps you can modify them to suit your needs. Top queries by logical reads, logical writes, or worker time. &gt;SELECT TOP 10 &gt; &gt; SUBSTRING(qt.\[text\], (qs.statement\_start\_offset / 2) + 1, ((CASE qs.statement\_end\_offset &gt; &gt; WHEN -1 &gt; &gt; THEN DATALENGTH(qt.\[text\]) &gt; &gt; ELSE qs.statement\_end\_offset &gt; &gt; END - qs.statement\_start\_offset) / 2) + 1) AS sql\_text &gt; &gt; ,qs.execution\_count &gt; &gt; ,qs.total\_logical\_reads &gt; &gt; ,qs.last\_logical\_reads &gt; &gt; ,qs.total\_logical\_writes &gt; &gt; ,qs.last\_logical\_writes &gt; &gt; ,qs.total\_worker\_time &gt; &gt; ,qs.last\_worker\_time &gt; &gt; ,qs.total\_elapsed\_time / 1000000 AS total\_elapsed\_time\_in\_sec &gt; &gt; ,qs.last\_elapsed\_time / 1000000 AS last\_elapsed\_time\_in\_sec &gt; &gt; ,qs.last\_execution\_time &gt; &gt; ,qp.query\_plan &gt; &gt; ,\* &gt; &gt;FROM &gt; &gt; sys.dm\_exec\_query\_stats AS qs &gt; &gt;CROSS APPLY &gt; &gt; sys.dm\_exec\_sql\_text(qs.\[sql\_handle\]) AS qt &gt; &gt;CROSS APPLY &gt; &gt; sys.dm\_exec\_query\_plan(qs.plan\_handle) AS qp &gt; &gt;ORDER BY &gt; &gt; qs.total\_logical\_reads DESC --logical reads &gt; &gt; \--qs.total\_logical\_writes DESC --logical writes &gt; &gt; \--qs.total\_worker\_time DESC --CPU time &amp;#x200B; Top queries by gross cost or subtree cost. &gt;SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED &gt; &gt; &gt; &gt;;WITH &gt; &gt; XMLNAMESPACES (DEFAULT '[http://schemas.microsoft.com/sqlserver/2004/07/showplan](http://schemas.microsoft.com/sqlserver/2004/07/showplan)') &gt; &gt; ,core AS &gt; &gt; (SELECT &gt; &gt; eqp.query\_plan AS QueryPlan &gt; &gt; ,ecp.plan\_handle AS PlanHandle &gt; &gt; ,q.\[text\] AS SQLText &gt; &gt; ,n.\[value\]('(@StatementOptmLevel)\[1\]', 'VARCHAR(25)') AS OptimizationLevel &gt; &gt; ,ISNULL(CAST(n.\[value\]('(@StatementSubTreeCost)\[1\]', 'VARCHAR(128)') AS FLOAT), 0) AS SubTreeCost &gt; &gt; ,ecp.usecounts AS UseCounts &gt; &gt; ,ecp.size\_in\_bytes AS SizeInBytes &gt; &gt; FROM &gt; &gt; sys.dm\_exec\_cached\_plans AS ecp &gt; &gt; CROSS APPLY sys.dm\_exec\_query\_plan(ecp.plan\_handle) AS eqp &gt; &gt; CROSS APPLY sys.dm\_exec\_sql\_text(ecp.plan\_handle) AS q &gt; &gt; CROSS APPLY query\_plan.nodes ('/ShowPlanXML/BatchSequence/Batch/Statements/StmtSimple') AS qn ( n )) &gt; &gt; &gt; &gt;SELECT TOP 10 &gt; &gt; QueryPlan &gt; &gt; ,PlanHandle &gt; &gt; ,SQLText &gt; &gt; ,OptimizationLevel &gt; &gt; ,SubTreeCost &gt; &gt; ,UseCounts &gt; &gt; ,SubTreeCost \* UseCounts AS GrossCost &gt; &gt; ,SizeInBytes &gt; &gt;FROM &gt; &gt; core &gt; &gt;ORDER BY &gt; &gt; GrossCost DESC &gt; &gt; \--SubTreeCost DESC &amp;#x200B;
Hit the nail right on the head. Could have not said it any better. @beaner921, in search for raking in BIG bucks, you acted unscrupulously, shame. 
Super helpful! Thanks.
Sorry but SQL brain is worn out after a full day of working on this. How about this, can you help me translate it into date ranges with the prices multiplied? SELECT * FROM dbo.lib_dates ld LEFT JOIN #dates1 d1 ON ld.date &gt;= d1.StartDate AND ld.date &lt;= d1.EndDate LEFT JOIN #dates2 d2 ON ld.date &gt;= d2.StartDate AND ld.date &lt;= d2.EndDate WHERE ld.date BETWEEN (SELECT MIN(StartDate) FROM #dates1) AND (SELECT MAX(EndDate) FROM #dates1) Thanks in advance
What engine? MSSQL? For SQL Server, it's just "KILL \[PID\]".
It's only "duplication" if you keep the standalone scripts around. If you stop using them and only use the stored procedures, you only have one "thing". Executing your stored procedures in parallel vs. serially, that's a different topic that's outside the database engine itself. The database *can* execute multiple stored procedures simultaneously (assuming there are no dependencies between them), it's just a question of how you invoke them.
Sybase and how can you kill a process if you can't access the database? I understand killing a process before running shutdown, but the problem is after initiating a shutdown command and not being able to use iSQL to login and kill the stubborn process
Perhaps if he were to have 10 jobs scheduled to run the stored procedures at the same time? I guess you would be limited on the number of connections that user would be able to have open at the same time.
That’s what I’m thinking. I guess the process is putting logic into stored procedures and call them from Java.
One thing I am wondering: I am not including "journal" as a Pub\_Type option. The option for that is "article". Is it enough for me to have an Article\_Metadata table that connects to the Publication table, or do you think I would also need to create a Journal table (and/or a Journal\_Metadata table), which would have a one-to-many relationship with Article\_Metadata?
Also, why do you need to loop? You may be able to loop trhough the table using cursors, but that will destroy performance. If you really need to utilize looping, I would look into using SQL to dump the data into a program and let the program iterate through the data and spit out your results.
Great. Glad to see people helping out.
We need a lot more information... What flavor of SQL are you using? What is your table schema? Are these new records or going to be linked to existing records? (Eg: do you have userID and want to match a specific usedID with a specific Name?) &amp;#x200B; If it's SQL Server you can open SQL Server Management Studio, right click on the Table, select "Edit Top 200 Rows", then copy/paste data into the grid that loads up from something like Excel or Notepad (if it's a single column). &amp;#x200B; This also assumes you just need the names as new entries, not linked to existing records...
Thank you! The records are new and will need a new UserID number. Other than that the table is a ClientID#, FirstName, LastName The Client ID number will link this table to other services as they're initiated. I think that I'll just use SQL Management Studio and paste the names into the table as you've suggested. I've never had to enter more than a few pieces of new data at a time so I wasn't aware that you could simply paste the information into the table using Management Studio. 
&gt; I don’t think sql can’t do those well because it’s not a programming language, Most dialects of SQL are Turing-complete. It is in fact a programming language. &gt;I would like to have one SP and put in parameters for Source location, tablename, how many top lines to skip, etc, You can do this with dynamic SQL, but it can make debugging a little tougher, and depending on how you do it, you may take a performance hit. &gt;I guess the process is putting logic into stored procedures and call them from Java. Do you already have a Java program driving this, or are you spinning up something new? Even in Java, unless you do something to multi-thread the execution of each stored procedure, they'll be sequential.
&gt; I wasn't aware that you could simply paste the information into the table using Management Studio. You technically *can*, but it's not the best way to do it. SQL Server can execute queries against files (especially CSV and Excel) and insert the data that way. There are also ways to do it as a bulk import from command-line tools, the import/export wizard (which creates a small SSIS package if you save it), and myriad programming/scripting languages.
Is that enjoyable at least. I am currently in a job with way too much tedium and I am trying to escape that.
So my case table is basically [Case ID] [Patient ID] [Received Date] [Close Date] And then some info around statuses, HCPs etc. And then our consent table is basically [Patient ID] [Form Name] [Received Date] [Consent Start Date] [Consent Expires Date] My output needs to be, on the first of each month, what cases were open (start date &lt; the 1st, close date NULL or &gt;= the 1st), and what consent they had on the 1st (based on the 1st being between the start and expire dates). I have written that query where I set a @datestart value and run the dataset into my rollup table - set it to 1/1/2018, run it, set it to 2/1/2018, run it etc. I did that for all months for 2018 and that gave me a table with [Run Date] [Case ID] [Patient ID] [Received Date] [Close Date] [Form Name] [Received Date] [Consent Start Date] [Consent Expires Date] But I'm being told not to run it into a rollup table - it has to be a standalone proc that will output the monthly rollup back to 1/1/18. So my struggle is how to get the proc to iterate through the dates and pull out those cases and status on that date, and then the next date, and then the next - and if a case is open four months, it's in there four times, potentially with different values for the form data if they initially have no form, then they submit a form, then the form expires etc. Hope this makes sense.
Take a look at [this reply](https://www.reddit.com/r/SQL/comments/at38hw/ms_sql_need_to_write_a_report_that_gives_a/egytsev/), basically I need it to loop as I'm capturing multiple snapshots from the same data based on a certain date - so one case might be in there several times if it spans several months.
Nice. This is very useful. Thanks!
Building reports can be enjoyable. Check out a person by the name of Stephen Few. He has books/articles, etc. He's got a lot of best practices on data visualization and conveying information. If you fold in some of those techniques to your report building, it's going to be a lot more fun than just running through SSRS wizards.
Every new company I walk into I end up spending the next 3-5 months cleaning up their entire reporting environment. It's usually the MOST requested support issues from the database standpoint. Once this is cleaned up - it frees up a lot of administration, development and design time for anyone near the database. For whatever reason, no one seems to be able to build reporting architecture correctly. 
Hey, sorry for bringing up an old topic, but I was wondering if you could explain the purpose behidn the ORDER BY \[scan\_date\] ASC at the end. &amp;#x200B; I realize it gives different results, but I don't quite understand why
Honestly, using Report Builder these days for new SSRS Reports is a bad idea. You should be using Report Designer via Visual Studio. The benefits are much better: * Integrated Version Control * Namespacing Groups of Reports (such as By Department) with one VS Solution * Sharing of like-Data Sources within a Solution instead of the old cumbersome way. * Easy one-click deployment to Dev/Test or Prod depending on what environment your build is targeting.
If the data is in a flat file format like Excel, or a comma delimited text file (.csv, .txt, .dat, etc.), I just r-click the database in SSMS and choose Tasks--&gt;Import data which opens up a data flow type of wizard which you can point to the Excel and will map that data to either an existing SQL table, or you can create a new one! Love, Craig
Since this is the SQL sub, you should post this on the SQL server sub. This isn't the same across databases that use SQL. 
Yes 
Thanks, this is good to know as well. Unfortunately, now that I've had a chance to see the data there's a lot more than what was initially described to me. It looks like I will need to clean up the excel sheet and query against it once it's in a manageable format. I inherited a database that is a mess and doesn't follow database normalization standards. 
Thank you!
Where is your list? If it’s in excel, you can easily make a formula to add the proper escapes and paste it into SQL.
check out MyWebSQL [http://mywebsql.net/](http://mywebsql.net/)
I’m working on a system where the end dates got corrupted. What’s the easiest way to fix this?
Yeah, if it's a simple ETL operation, SSIS is the easiest and lowest overhead way to do it. It's also 'programmed' using a GUI, so the barrier to entry is tiny. Best of luck.
Best bet is probably an update statement using the LEAD window function. You can partition by your PK and order by the Created_Date and then set the Deactivated_Date equal to the next record's Created_Date.
Use a subquery to join TableB where it's already aggregated. JOIN (SELECT AcctNum, PayeeNum,SUM(Amt) Amt,MAX(Date) Date FROM TableB GROUP BY AcctNum, PayeeNum) B ON YourConditions Sorry for formatting, on mobile. 
Use row_number to get just the latest Inner join (Select row_number() over (partition by acctnum, payee order by paymentdate desc) as RowNo, paymentdate,paymentamt, acctnum, payeeid from ..) x on x.payeeid = a.payeeid and x.acctnum = a.acctnum and x.rowno = 1 Sorry if the fields aren’t quite right, doing on mobile. Hope you can make sense of this 
THANK YOU!!!! This. I don’t understand why it’s so difficult to build a sound environment for report building. Thankfully, we JUST put a data governance and data dictionary in place. Yikes
Is there a Postges version of this?
why are you not using join on the same table? is it returning different result?
In MySQL, you're looking for `LOAD DATA INFILE`
In UDT, 2:29 PM (or 14:29) is 2029 because you are in a time zone with a -06:00 offset. 
 select SYSUTCDATETIME() , CONVERT(varchar(20), SYSUTCDATETIME(), 112) , REPLACE(CONVERT(varchar(20), SYSUTCDATETIME(), 108), ':', '') ; 
&gt; What’s the easiest way to fix this? Without any further information? Restore from backup. After that I'd have to understand what exactly you mean by "end dates got corrupted." I have no idea what your data is *supposed* to look like.
What should the values be? If not equal to create date, then what is your logic? 
I'm not sure about mysql... but you have LIMIT i where i is not always 1?
I ran into this exact problem and fixed it in this exact way, do this.
Nvm.. I looked it up. Limit row,howmany.
that aint corruption, thats bad data. either from poorly written t-sql or bad application code. fixing the data is part of the overall fix, but don't forget you got a rogue entity in control of your data and it will strike again. but as others have said, this will be fixed by using LEAD/LAG window function (depending on which way your traverse it). 
What about select into N, that looks like the only other place for multiple rows. Can you run that code by itself? 
If you were familiar with SCDs you’d see right away that the end dates are messed. None of the date ranges for this UTAN should overlap. The LEAD window function did the trick! 
The glitch got fixed already. I’m doing cleanup. I used LEAD. Thanks!
Thanks! This did the trick. WITH CTE AS ( SELECT UTAN, Created_Date, Deactivate_Date , LEAD(Created_Date,1) OVER (PARTITION BY UTAN ORDER BY Created_Date) Lead_DT FROM Application )UPDATE CTE SET Deactivate_Date = Lead_DT WHERE Deactivate_Date &lt;&gt; Lead_DT
SCDs use multiple rows with created dates and deactivate dates to track history. These date ranges shouldn’t overlap. I posted the solution in a reply above. The first four rows have bad deactivate dates. The rest of the rows are correct because they are the same as the next row’s created date. 
Select top 1 blah blah blah
&gt; If you were familiar with SCDs you’d see right away that the end dates are messed I know what *I* think SCDs should look like. I need to know what *you* think they should look like. I'm sure you know your stuff, but this is the Internet. I want to know what you want because people asking for help often aren't using a well-designed system.
Check out the sidebar.
Is this a question or a statement?
The problem has already been fixed by a suggestion from a user that didn't require any further information from me. This is because this user understands how SCDs work. The functionality of how SCDs work isn't something that is up for debate. They work how they work. If you can't tell what's wrong with this SCD after a millisecond of looking at the picture then you don't understand SCDs....which is totally cool. You probably know tons of stuff about your job that I don't know. I just find it weird that you're questioning an established concept that's been around for many years. If you want to see the answer I posted it in response to the top comment. If you want to understand how a SCD works then you can read about them here: [https://en.wikipedia.org/wiki/Slowly\_changing\_dimension](https://en.wikipedia.org/wiki/Slowly_changing_dimension)
It was a question. It got solved in the top comment.
Not sure whether it’s only me or not, I actually don’t understand what the question is. 
You could use a data comparison tool (there are a number out there, it depends on the what db system you are using - PosGres, MySQL etc.) though that would be pulling the data locally and comparing them row by row... Perhaps not the most efficient but would do the trick if you're willing to deal with network latency and your machine has enough disk space. 
How did the get corrupted? 
If you have a knowledge of Python, this should help https://github.com/aws-samples/aws-etl-orchestrator/blob/master/README.md#the-challenge-of-orchestrating-an-etl-workflow
SCDs aren't supposed to have overlapping date ranges. The Deactivate Date is messed up on the first four records. The other ones are fine because they start where the previous ones end. This is how you track history on records like this. Every time there's a change instead of directly updating a record you insert a copy of it and make the change to the new record. The record with a NULL Deactivate Date is the current record.
the sidebar is blank
They didn't explain. I'm not sure. I've been informed that it shouldn't happen again tho. LOL!
I hope you sign all your work emails with “love, Craig” too.
Google date dimension.
This was amazing to read. I’m really inspired at this moment to make this work. I appreciate that very much. 
I’m in Atlanta but thank you very much 
I’m in Atlanta but thank you very much 
Thank you everyone for your words. I really appreciate all the support. 
No it's not?
Use max for the time column
And how would that look in the query? I'm new to this.
Hey guys, i know this is very late. But can anyone point me to a good FREE source for the exam reference guide. All the paid links Im finding look very sketchy, I dont want to giv my credit card info
Yeah, I'd probably do a 'completed test' and 'current test' type thing - easiest way out.. 
Almost the same with your current query but you'll replace this: TO\_CHAR(TO\_TIMESTAMP(\`itime\`)::timestamp, 'YYYY-MM-DD HH24:MI') AS time to this MAX(TO\_CHAR(TO\_TIMESTAMP(\`itime\`)::timestamp, 'YYYY-MM-DD HH24:MI')) AS time and this group clause GROUP BY user\_src, remote\_ip, vpn\_group, time, dur to this GROUP BY user\_src, remote\_ip, vpn\_group, dur Can't test this right now though but if you need a reference, this might help [https://www.sqlservertutorial.net/sql-server-aggregate-functions/sql-server-max](https://www.sqlservertutorial.net/sql-server-aggregate-functions/sql-server-max) 
https://www.reddit.com/r/SQL/wiki/index
Works! Thanks!
Add a where class; Where itime = ( Select max(itime) From $log Where a.user = b.user )B Join the inner query to outer. Name your outer referenced from table as 'A'
As a sql guy I would eagerly like to know what caused it. Rather prevent than fix again right? &amp;#x200B; Whas this problem there already since august-'18?
Sam’s teach yourself sql in 10 minutes gives you the basics where I learned most of it. W3 is a good site though 
Gotta love those! Like the people who want me to automate stuff.... me:"Where do you get the spreadsheet Megan? Megan: 'I don't know'"me:. "So here we are...."
 --Second most recent SELECT * FROM [acas_master_test] WHERE [scan_date] IN ( SELECT TOP 1 [scan_date] FROM ( SELECT DISTINCT TOP 2 [scan_date] FROM [acas_master_test] ORDER BY [scan_date] DESC ) as d ORDER BY [scan_date] ASC ) This one? Inner most subquery says, give me the two most recent scan_date. The subquery just outside it says give me oldest one scan_date of the two. The final select says give me all info from acas_master_test where the scan date is the second most recent. Run all these separately to break it out. It might help make sense SELECT DISTINCT TOP 2 [scan_date] FROM [acas_master_test] ORDER BY [scan_date] DESC SELECT TOP 1 [scan_date] FROM ( SELECT DISTINCT TOP 2 [scan_date] FROM [acas_master_test] ORDER BY [scan_date] DESC ) as d ORDER BY [scan_date] ASC SELECT * FROM [acas_master_test] WHERE [scan_date] IN ( SELECT TOP 1 [scan_date] FROM ( SELECT DISTINCT TOP 2 [scan_date] FROM [acas_master_test] ORDER BY [scan_date] DESC ) as d ORDER BY [scan_date] ASC )
I really like [Master SQL For Data Science](https://www.udemy.com/master-sql-for-data-science/) he's a great teacher. Focused on writing queries.
Ive used 'exists' as an easy fix for myself but was looking for an easier way with value()
Instead of an EXCEPT, which is doing something differently, why not simply eliminate the EXCEPT and just say `WHERE CITY &lt;&gt; 'City Here'? An EXCEPT is going to take one dataset (the top) and compare it to the bottom dataset (the bottom) and give you the difference, it's going to function the opposite of an INTERSECT. So you're saying: SELECT A FROM Table EXCEPT SELECT B FROM Table In your example A = Supplier Name, and B = CityName, so it is going to give you everything from the top that doesn't exist in the bottom... which in this case is going to be everything, because the two sets you are comparing do not have any common values. If you were to switch your EXCEPT to an INTERSECT then I imagine it would give you 0 rows, because there are no common values that "intersect" between the top and bottom data sets. But you don't want to do that, you want to just isolate the suppliers `WHERE City &lt;&gt; 'Value'`. See the difference?
What you said made a lot of sense to me and gave me that feeling of "Why didn't I think of this before"? Thank you! The only thing that I don't really understand is how the EXCEPT statement gives me everything. You said that it gives me everything because the two sets have no values in common. I was under the impression that if I joined the 3 tables during the A part and then joined the 3 tables under the B part I would have two identical tables? I hate to think that I'm misunderstanding something as basic as an INNER JOIN, but that appears to be the case here. 
I would follow exactly what has been recommended already by the previous comments, unfortunately investing the book will help greatly than any free resources IMO. I'm taking the exam next week, wish you luck friend!
Let's say you have a table of ID's that you produce using a complex query. Now let's say you want to rewrite that complex query to make it more efficient, but you are making large structural changes to it and using a completely different methodology. How do you know if your new process and your old process will produce an identical set of data? So let's look at it: Query 1: SELECT ID, Blah INTO #ORIGINAL FROM Table WHERE Things Query 2: SELECT ID, Blah INTO #NEW FROM Table WHERE OtherThings This is a very simple example, but imagine that these queries have lots of joins, lots of subqueries, lots of case logic, lots of where conditions, etc. So now we can do something simple like this: SELECT ID FROM #ORIGINAL SELECT ID FROM #NEW You could then look and see if the row count matches, but this isn't really telling you if they are the same or not because you could have some ID's in the #ORIGINAL table, and some other ID's in the #NEW table and the total row counts just might coincidentally be the same. * - See note below. So let's try this: SELECT ID FROM #ORIGINAL EXCEPT SELECT ID FROM #NEW What this will do is give you everything from the top EXCEPT the id's that are not in the bottom. If they are equal, you would except this to return 0 rows. Now we can turn around and flip it: SELECT ID FROM #NEW EXCEPT SELECT ID FROM #ORIGINAL Same thing happens except now it's comparing the #NEW set to the old set, and again if they are equal you would expect 0 results. Simplistically speaking if you run both versions of EXCEPT, and both times you get 0 rows, then you are confident that both sets are equal. The reverse of this is INTERSECT: SELECT ID FROM #ORIGINAL INTERSECT SELECT ID FROM #NEW And here what's happening is that you are asking for all the records in the top that INTERSECT with the bottom set, so if they are equal you should get the same row count as you would get if you juts `select count(*) from #new`. Note: When you use EXCEPT or INTERSECT with multiple columns, it will not just compare the list of ID's to make sure they are the same, but it will compare the values in the row. So if both ID's match, but one has a *null* value for a date column, but the other dataset has a date... it will alert you to the fact that these two things are not the same. &gt;You said that it gives me everything because the two sets have no values in common. In your example you have a list of suppliers, so ABC Supplier, XYZ Supplier, etc. In your bottom set you have a list of cities, so Detroit, Cleveland, Chicago, etc. You are saying "give me everything from the top EXCEPT the records that do not exist in the bottom."
If you wanted to continue using EXCEPT with your base logic it would look like this: SELECT DISTINCT P.Name FROM Tb_Product P INNER JOIN Tb_Offers O ON P.Prod_ID=O.Prod_ID INNER JOIN Tb_Supplier S ON O.Supp_ID=S.Supp_ID EXCEPT SELECT P.Name FROM Tb_Supplier S INNER JOIN Tb_Offers O ON S.Supp_ID=O.Supp_ID INNER JOIN Tb_Product P ON P.Prod_ID=O.Prod_ID WHERE City = 'Example City Name' And it would be basically the same as saying `WHERE City &lt;&gt; 'Name'` or even `WHERE City NOT IN ()`.
This is a good starting point https://sqlbolt.com/ 
Sorry for the delay in response but I prepared an example for you illustrating the IN clause and some options of how to combine multiple predicates in your where clause. I understand that the sample data I generated might not make complete sense but it should be good enough to illustrate the points I was trying to get across. https://livesql.oracle.com/apex/livesql/s/h0emvi3u3nvzh9mo8iqn58kle
Thank you! Your explanation was incredibly thorough. I suppose I'll have to practice a bit more looking before I leap or maybe more rubber ducky debugging so that I don't create queries that I don't understand. I think I'll attempt to implement the &lt;&gt; solution, but thank you for explaining both methods to me. It's better to come to terms with my lack of understanding now than when I get back a poor exam grade. You might want to think about changing your name, because as far as I'm concerned you are a SQL star, or maybe it's an ironic name. I *think* I have an idea of how to solve the other queries presented in the assignment, but in the event that I'm wrong and I get super lost would you be comfortable with sending you a message looking for more guidance? I'll be sure to exhaustively try many different solutions so that I'm not pestering you with every question in the assignment. Thanks once again! &amp;#x200B;
One other thing to note. If Query 1 has 1,000,000 rows, and Query 2 has 1,000,001 rows then select * from query1 except select * from query2 will give you 0 rows. It won't be until you run `select * from query2 except select * from query1` where you will get (1) row back. Since all one million rows from query1 do exist in query2, then except will give you nothing back. Conversely speaking if you did `select * from query1 intersect select * from query2` you would get 1,000,000 rows and it would make you think they are totally equally. Flipping that around would also give you 1,000,000 rows, but you'd know there was (1) row not being included if you did a `select count(*) from query2`. &gt;but in the event that I'm wrong and I get super lost would you be comfortable with sending you a message looking for more guidance? Sure or just add on to this thread. I'll be around for a few hours.
thanks for your reply. I have a list of more than 2000 patient serial and case date I want to search the doctors code at the same time from this 2000 values. Example patient serial 123 and case date 24th February 2018. I want to know which doctor he came to. I can easily search for it one by one but I need to search more than 2000 values this is what I am not understanding how can I input 2000 values at the same time and get the result . I tried following query : select * from patient_visits where pat_ser in ( 'value', 'value', '..(1000 values)') and case_date in ('20180101', '20181231', '..... corresponding case_date values of pat_ser'); and I executed this but the result is not accurate. Sorry I am on mobile and far from home that's why I cannot format it nicely. Can you please help me regarding this. Thanks 
I like to think of the except clause as the opposite of union all. Union all says Query 1 + Query 2. Except says Query 1 - Query 2. In other words, except says "Remove all rows from query 1 where there is an **identical** row in Query 2. You don't have any identical rows in your 2 queries, so nothing is being "excepted". 
Build an index on user id?
This is difficult to achieve in pure SQL but is pretty easy to do with PL/SQL. Out of curiosity how do you receive the input? Are you manually typing it in or is it generated from a report or maybe in csv format?
This is the eli5 answer. I find intersect, except, union, and union all to be very similar. 
I think its a good article in it shows a programmer realizing the strength of SQL when it comes to databases. A programmer friend of mine (who knows C, C++, Python etc) said that it was impossible to have a career knowing just SQL. I thought it was a very narrow minded and down the nose perspective of SQL from someone whose grasp of the language was a simple SELECT FROM WHERE. SQL is the old workhorse that keeps going when it comes to handling data and databases. It may be looked down upon since its not at its core a programming language, but its essential to business. I myself plan on expanding my skill set outside of SQL eventually, but its possible to go deep on report/analytics/warehousing with just SQL.
I am receiving the input from my database 
Is it the result of a query? What format is it in?
Where you're going wrong is that when you use EXCEPT, INTERSECT, UNION, or UNION ALL you need to use compatible sets. You are attempting to compare a set of Product Names with City Names. "I want all the widget names except the widgets named Burbank, Fresno, and Schenectady."
You could compare all 500,000 objects, and this is way beyond my level of experience... but at that point wouldn't it just be easier to run a backup and make sure they're all the same? Otherwise it sounds like you're going to have to run ~500,000-1M queries to see what is and isn't different, and then update accordingly. 
If you want to check every row, I don't see how you're going to get around looping through every row...? &amp;#x200B; It's really going to depend on the capabilities of your end point... if you can export all the data in a usable format, you can just re-load it all on your SQL end... or if there's a "LastModified" timestamp and you can query all records since X date, you can do a "full sync" (scan every row) now, then incremental syncs based on the last sync date... 
If SQL isn’t involved in your “data pipeline,” your data and your need for it must be extremely simple.
&gt; That or it’s truly “big,” I'm curious. Is that because truly big data systems tend to use things like distributed file systems instead of traditional databases, for example?
Thanks...forgot about that one.
Lol, yeah, very narrow minded. The business world runs on databases. Case in point - DBAs - highly sought after and compensated. Do they know 'only' SQL? No, there's networking, hardware, understanding of whatever people/processes are using to access your db, etc... But SQL is their main language. &amp;#x200B; Then there's app development, t-sql, data warehousing, data modeling, ETL, analytics, etc.... All built on top of databases &amp;#x200B;
&gt; A programmer friend of mine (who knows C, C++, Python etc) said that it was impossible to have a career knowing just SQL. Have worked and known these types before. Until they find how how much you’re raking in - then their attitude changes. Applications Development and Databases are equally important, but there are far more Applications Development/SE people out there passing out their resumes. It’s much, much harder to find an equally skilled database professional. Which is why DBAs, Database Developers and Data Architects are usually some of the highest paid non-management positions in your run-if-the-mill tech department in the corporate industry. With maybe Senior Network or VM Engineers comparing. 
Post the entire query otherwise no one will be able to help.
That I cant do, proprietary. Really the first thing I need to know is why does activity monitor say "running" and sp_who2 says "runnable". 
Sorry, you'll need to paste the code. Anonymize any column names, reduce it down to a sample, otherwise you're not going to get much help here.
Like, based on what you've said, your query sounds like it sucks. No offense. It is poorly written, and somehow causing things to hang. No one is going to have any idea why until you show us what it does, but my initial guess is that this is some really long query with lots of nested sub-queries. Yes? You need to rewrite that then. If you gave a sample framework I could give you an idea how to do that, but without any idea what you're trying to run I don't think many people here are going to be able to help you based on the information you have provided.
I won't take offence. I didn't write it. As far as it's structure it's Insert into client.transform.table Select from client.staging.table a Left outer join ClientMemberMap b on a.row_guid = b.row_guid Where a.Flag = 0 There's a bunch of string manipulation in it on the fields, but that's largely whatever, and the amount of data it's processing is less than 500k rows at any given point. Execution plan is showing Index scan costing 61% on client.staging.table and 26% on the ClientMemberMap. The 12 hours is abnormal for this, and I think there's something wrong with the system, but again when I check sp_who2 and SELECT status,* FROM sys.sysprocesses, the query in question is showing as runnable. I'm getting told "no everything is fine it's running" because the other guy is looking in activity monitor and it's showing status of running. So my first question again - why the discrepency?
&gt; There's a bunch of string manipulation in it on the fields, but that's largely whatever Ahhh, wanna bet? Something tells me your query is just poorly written, and while it used to work well, it is no longer working well because the amount of data has increased, or the amount of manipulation has increased, etc. But I really have no idea because you can't share the query. If all it is a simple insert into select from left join on guid's then it shouldn't take that long, and you might have larger issues than I am competent to speak of.
I mean no reddit bet is worth getting in trouble over nda. Is what it is. I primarily am trying to figure out that discrepancy first as a step into the next part of my troubleshooting. Ideal situation - I'd use what I see on the execution plan, implement the non-clustered indexes that it's suggesting on the two tables, and then see if that resolves the problem. But that discrepancy is being used as evidence that nothing is wrong - and I'm the "junior" here. 
He could just do a simple EXCEPT or whatever to avoid a loop, but he's going to have to loop that simple query across all 500,000 tables. Something like: IF = ( count(*) from ( select * from @localtable except select * from @remotetable ) x ) &lt;&gt; 0 EXEC SyncTables I know the syntax is off but its late Friday and I don't care. Just supply it with a list of object names, run it through the loop, pass the object name as a parameter to the stored procedure SyncTables and do a simple DROP TABLE / SELECT * INTO, or if you know the columns are the same INSERT INTO to preserve data types. Or create a list so you can then drop those tables and then import the "script tables as" for the other tables before you do an insert into, etc.
"Big Data" usually refers to the Hadoop architecture, which doesn't have a relational database. Google (who gave the world the basic Hadoop architecture with HDFS and Map Reduce) is now betting that the relational model (which 99% of the time uses SQL) is the way to go on their distributed database called Spanner. &amp;#x200B;
Don't violate your NDA, but you can take that big query and go through and anonymize it, take out relative information to see specific operations, etc. I would doubt that would violate your NDA, but my bet is that your issue is with the query itself.
use VALUES only once -- INSERT INTO Movie ( movieID, title, yearReleased, totalRevenue ) VALUES ( 101, 'Avatar', '2009', 2787965087 ) , ( 102, 'Avengers: Infinity War', '2018', 2048359754 ) , ( 103, 'Jurassic World', '2015', 1671713208 ) , ( 104, 'Black Panther', '2018', 1346913161 ) , ( 105, 'The Lord of the Rings: The Return of the King', '2009', 1119929521 )
I don't understand what you're saying. Sorry i am just new to this field
Rubber Ducking is my favourite device when debugging or I just can’t see the solution. I’ve completely lost the self-consciousness speaking out loud to my cup of coffee!
&gt; That or it’s truly “big,” but that’s another topic. You do realize that there is over a dozen SQL based big data platforms? BigQuey, Spark SQL, Presto, Hive, Impala, Kafka(KSQL), Drill, Phoenix, IBM Big SQL. Those are the ones I can name off the top of my head.
It seemed to me like they were saying "if you're not using SQL you're doing one of these two things" and not "SQL can't be used for these two things"
thanks!!!!!!!!!!!!!!!!
You'll need to join actor to actormovie then have a second join to movie. Really the order if tables is up to you. JOIN is the keyword you'll want to research.
&gt; I am receiving the input from my database I was seeking clarification on what this means.
You need to join the 2 tables together on a column. That way, you can get a single table that connects actors to which movies that appeared in. When you use aggregation, you also need to include the group by clause. You're grouping the actors because you want to retrieve the avg of the total revenue of their movie appearances. I hope this helps 
I am manually entering the the query in toad and then running it to get result from my database. 
Firstly try running the code in SSMS to see if you get desired results. If so, in Crystal try to refresh data source. 
Fantastic. Can you give me a sample dataset that is returned by your query? I am assuming it returns dates and patient info in some way. &amp;#x200B; e.g &amp;#x200B; |PATIENT|CASE\_DATE|PATIENT\_NAME| |:-|:-|:-| |67890|yesterday|Digg| |123456|today|Smoo| &amp;#x200B;
So I tried: &amp;#x200B; `SELECT DISTINCT Tb_Product.Name` `FROM Tb_Product, Tb_Offers, Tb_Supplier` `WHERE Tb_Product.Prod_ID = Tb_Offers.Prod_ID` `AND Tb_Offers.Supp_ID = Tb_Supplier.Supp_ID` `AND Tb_Product.Name NOT IN` `(SELECT DISTINCT Name` `FROM Tb_Offers, Tb_Product` `WHERE Tb_Offers.Prod_ID=Tb_Product.Prod_ID` `AND City = 'Example City')` &amp;#x200B; and &amp;#x200B; `SELECT DISTINCT` [`P.Name`](https://P.Name) `FROM Tb_Product P` `INNER JOIN Tb_Offers O ON P.Prod_ID=O.Prod_ID` `INNER JOIN Tb_Supplier S ON O.Supp_ID=S.Supp_ID` `WHERE City &lt;&gt; 'Example City'` &amp;#x200B; Neither of them are working and I'm still puzzled as to why. I decided to look into what type of data I should be seeing so I can know the right answer when I see it and the answer should be Auto Gas Milk Orange Tv &amp;#x200B; But I keep getting back all 8 products instead of just these 5. 
Never mind this last post. I just found out that the question is asking sold as in transacted and not sold as in offered. So I'm looking at the completely wrong table for one of these 3 tables. 
The first query gives you all products excluding those sold in Example City. So if A City sold beets and corn, and B City sold corn and wheat, and Example City sold corn, the result would be beets and wheat. The second query gives you all products sold in any city that's not Example City. Using the example above, the results would be beets, wheat, AND corn because corn is sold in a city that's not Example City. I think neither of these match the assignment, which is asking for all products sold in both City A and City B but not Example City. So if I was approaching this problem, I would first find products that are sold in all cities. Then remove the ones sold in Example City. Let me know if you want more hints...
Can you group concat distinct of 1 through 15 in a subquery either as a table join or column and then use that with a find_in_set (my bad, SQL doesn't have that... [see this answer for it](https://stackoverflow.com/questions/41619245/find-in-set-equivalent-in-sql-server)) to join on?
So I've done this: `SELECT DISTINCT` [`P.Name`](https://P.Name) `FROM Tb_Product P` `INNER JOIN Tb_Transactions T ON P.Prod_ID=T.Prod_ID` `INNER JOIN Tb_Supplier S ON T.Supp_ID=S.Supp_ID` &amp;#x200B; that seems to give me all 8 products after merging the 3 tables, but I'm not sure what to do next as all of my other negation or except queries have failed. 
0 mb of memory usage sounds really strange. If no memory than is it writing everything to tempdb. That will make it real slow. Are you getting a bad execution plan due to bad index stats. Are you sorting the data, if so stop it. You mention string manipulation, are there some implicit conversions happening? Are you using some table functions because those dont scale very well. 
So I'll respond assuming my interpretation of the assignment is correct... The language is kinda unclear... What your query does is show products sold in ANY city. Not products sold in ALL cities. So start with a query that returns a distinct list of cities and their associated products. Something like City A: apples City A: beets City A: corn City B: apples City B: corn City B: wheat Example City: corn Example City: beets Let's call this "tbl". If you wanted to get products sold in all cities, an easy way would be to: SELECT product, count (*) as city_count FROM tbl GROUP BY product HAVING count(*) = 3 Because if a product has three rows in this distinct list of cities and products, then it's sold in all three cities. But you want a product that is sold in all cities except one. So only two cities. Let's try the above query to "count(*) = 2." We would get apples (sold in A and B) and beets (sold in A and Example). But we don't want beets. So you can add a WHERE clause to exclude products sold in Example City (which I think you know how to do.) Hopefully this makes sense!
Thank you! This is helping me greatly learning the concepts and commands I've been using on the Khan Academy's practice software. 
Crystal is finicky at best. I noticed that your second query has more tables in the select than your first of I'm reading correctly. Have you joined all these tables in your Crystal report? Are you embedding the SQL in the main report or using a subreport to handle your results? Are your report criteria in your report handler or are you expecting results straight from the SQL? I specialize in Crystal dev at my job. If nobody comes along with a better answer, feel free to PM me. Maybe we can chat and I can help you troubleshoot. 
I'm sorry, but I don't fully understand. I'll try to clear up some of the language issues. There have been a few problems because the professor is not a native English speaker and I'll have to go back and redo some of my queries because of it. The question states: "Name of products sold in all cities except Stevens Point." **Products:** Airplane Auto Boat Computer Gas Milk Orange TV &amp;#x200B; I've done a search and it appears that there are 3 products that are sold in Stevens Point: Computer Boat Airplane &amp;#x200B; So the final query result should look something like this: Auto Gas Milk Orange TV &amp;#x200B; I'm going to get a good nights sleep and come back to this tomorrow, because I haven't had a good nights sleep in a while. But to tell you the truth, I might be close to just writing the problem off as unsolvable. The class is a data warehousing class and we've yet to talk about SQL at all. Then this assignment comes by and he says we should be able to solve it in under 2 hours even if we're bad a SQL. So after about 5 hours I have 4 questions that I'm sure I have the right answer, 3 that I need to revisit, 2 that I have yet to get to, and then this one. The assignment comes with an incredibly brief and inadequate explanation of Intersection, Negation, Division, and "Winner Loser" queries. I've asked the professor for some advice and he answers with unhelpful short 2-4 word sentences and the class forum is somewhat of an unresponsive ghost town. The astonishing thing is that this question is explicitly worded "Except" so I was under the impression that it would be easily solved with an except query, but so far nothing I've tried has worked. I've never encountered SQL queries that have been this needlessly challenging before, especially from a class that is about dry data warehousing methodologies. 
Oh okay. Never mind then, l was overcomplicating things with my interpretation of the assignment. Your first query in your previous post should work. This one: SELECT DISTINCT Tb_Product.Name FROM Tb_Product, Tb_Offers, Tb_Supplier WHERE Tb_Product.Prod_ID = Tb_Offers.Prod_ID AND Tb_Offers.Supp_ID = Tb_Supplier.Supp_ID AND Tb_Product.Name NOT IN `(SELECT DISTINCT Name` `FROM Tb_Offers, Tb_Product` `WHERE Tb_Offers.Prod_ID=Tb_Product.Prod_ID` `AND City = 'Example City')` Maybe change the offers table to transactions? 
I really liked the Wise Owl series for SSRS. Easy to follow and understand. https://www.wiseowl.co.uk/reporting-services/videos/
I just tried `SELECT DISTINCT Tb_Product.Name` `FROM Tb_Product, Tb_Transactions, Tb_Supplier` `WHERE Tb_Product.Prod_ID = Tb_Transactions.Prod_ID` `AND Tb_Transactions.Supp_ID = Tb_Supplier.Supp_ID` `AND Tb_Product.Name NOT IN` `(SELECT DISTINCT Name` `FROM Tb_Transactions, Tb_Product` `WHERE Tb_Transactions.Prod_ID=Tb_Product.Prod_ID` `AND City &lt;&gt; 'Stevens Point')` and I got back Boat, Airplane, and Computer. So if I implement the rest of the negation query that should fix it. Let me try it out real quick. 
How do you keep information about relation between tables without primary and foreign keys? You simply can't have relational database without them - you just have collection of discrete tables. Primary key is unique identifier of each row in a table. It can be natural (if there is already column in your data that can serve as unique identifier per entry) or artificial (usually just increasing integer sequence or perhaps GUID), and it can be simple (only one column) or combined (multiple columns). There are two important benefits of PKs: first, you can identify exact row you want, second, you can reference this row. Foreign key is just PK of table A used in table B, so you can say "this row of table A is related to that row of table B". If you don't use FKs, what happens when you try to add entry in table A, which references row in table B, but that row in table B doesn't exist? Will your database system let you do it?
Such keys will guarantee data consistency. You can not delete row from table then rows in other tables are referenced to it. Also primary key creates index on its column
that's actually a good point. And right now all I do is just record the IDs of rows into the relations table. No linking done as of yet. &amp;#x200B; So if I were to remove the row of an Accountant who's ID was recorded in a contract row. Then... well... when I call up that contract, it just wouldn't display the name of the Accountant since that entry is now gone. Which is expected. So far there wouldn't be any critical failures, just lack of returned data. But you would know of more severe cases where there I might encounter worse results. I'm not sure at this point.
So, before I can properly explain why primary and foreign keys are important I want to explain what they are first. A primary key is a unique value for every row in a table and it is. This is important for data integrity (ensuring the data is accurate and not misrepresented in the database). Similar to how an excel spreadsheet has Column A, B, C, ect. and row 1,2,3, ect., A primary key is a singular value that describes a unique row in the database that can have Columns A,B,C,ect. values of data behind it. &amp;#x200B; A foreign key is simply a reference to a primary key in another table. Kind of like a reference to another cell in Excel (except imagine a reference to just another row). The benefit to this is mainly the data integrity provided by primary keys, and this is why all relational databases require a primary key. This speeds up performance, and reduces the times you have to store information redundantly. Now I can reference a primary key of another table where additional of my column data is stored. This design forms a relationship between records of data across tables and is why SQL RDBMS is classified as a relational database system, because all table keys represent a relationship that data has within the database. &amp;#x200B; So really its not that primary and foreign keys allow for pulling data from the tables *quicker*, but instead that they are what allow you to pull data from a table *at all.*
So this is purely an accidental deletion prevention? Like with the above **Accountant, Client** and **Contract** examples if I wanted to delete an accountant profile row, I'd also have to delete the Contract row that his that accountants row id is mentioned in?
Thank you for the thorough explanation. I've been using relative databases in the intended manner thus far just without linking tables together. I would just simply.... record the ids of rows into othe tables like with an example abolve. Basically table with **Contracts** collumn accountant\_id would contain the row id of that accountant in the **Accountants** table. But nothing more... I haven't specifically defined primary and secondary keys. If anything I feel like this makes it much more difficult to remove entries from a table. Like what if I want to remove an Accountant from the **Accountants** table? Do I have to now delete All the contract rows from the **Conrtacts** tables where that accountant id was recorded? I mean that makes no sens but you can also say would are we deleting the accountant row in in the first place instead of setting it to something like "Dormant / Inactive" status. So that ties into what sort of an application you're developing. I'm just curious. So far the use of linking Foreign and Primary keys of different tables just makes it accident proof is all...
It's basically feature of database system that helps protect you against your own mistakes (or someone else's when you collaborate with other people). If there *needs* to be accountant for each contract (it seems so), then it's bug if there isn't one. Sure, one could just use their own head and their own eyes and compare these tables with every change, but why, when you can tell machine "hey, this relation here needs to be there" and it will do your work for you - it won't allow you to add relation with entry which doesn't exist, and it won't allow you to delete entry which is referenced from somewhere else. Usually there is some program which uses data from this database and if there aren't data it expects (table A references row 356 in table B, but there's no row 356 in table B, *excuse me what the fuck*), it can crash if it isn't written well, and even if it's well written software it won't operate correctly without data it needs. And software critical for business not operating correctly can easily cost thousands or millions per hour of not working. So I'd say PKs and FKs are very useful tools. As a general principle, whenever I can put part of my own workload on the machine and let it being done automatically, I do it. Not only because I am lazy and such offloading allows me to work on more important and exciting tasks, but also because these kind of repetitive, boring tasks are what machines do much better than humans. But it often requires you provide machine with some more information - such as relation between two tables, that you kept in your mind, and now you have to spell it explicitly in terms it can understand, primary and foreign keys.
This depends on what you are using your data for. There are such a thing as non-relational databases (called "NoSQL" DBs) and they are quite useful for solving some problems. But traditionally, relationships exist in a database to save on storage space and try to speed things up while providing high data integrity (accurate information, not misrepresented in the database). &amp;#x200B; The specific relationships you mention describe the logical design choices made to solve the problem the database was built to solve. Also, keep in mind each row of data in a database is related to a singular event or "instance" of an object or "entity" the table represents. &amp;#x200B; For example, lets say we have three tables. a CLASSES table, a STUDENT table, and a TEACHER table. now each class can have many students, but only 1 teacher. That is how a class in real life functions normally, and each teacher can teach many classes, and typically a student can have many classes and a class can have many students. &amp;#x200B; So to answer your last two questions, defining these relationships help our database correctly add data and report data. Without these relationships, its harder to get an accurate representation of reality.
We use databases to model real world. Therefore the answer is simple - we have 1:1, 1:N and N:N relationships in databases because we have them in real life. Think about your socks. You are single object (single entry in table), but you have many socks. That's typical 1:N relationship. But what about your mom? You probably have exactly one mom, that's nice 1:1 relationship. And city where you live? There are many people living there, but also, lots of other people live in different cities, that's N:N relationship. Perhaps you could create database which would hold all these data without explicitly using (or thinking about) these concepts. It's just one of tools of database system which allows you to better model your data. 
Why there's no many to one relationship in relational database?
So relational databases are flexible in that they can be configured with constraints, triggers, procedures, ect. that automate a lot of those hassles and ensure you don't accidentally loose data or accidentally leave data lying around. database systems manage data primarily through these means. The power of relational databases is that you have much more freedom to design rules and constraints to automate data management while enforcing your rules (data integrity).
No but my experience is if you can't use management studio don't bother
Many to one ... one to many, same thing really, don’t you think ? You just look at it the other way 
I thought about it but i was not sure about it. So wanted to confirm
Yeah exactly this way it returns the data but it contains all details from starting date i specified till end date i specified. And it also contains patient files i didn't requested at all. It applies date as a filter not pat_ser. 
I think at the desk is a more comfortable place but then again I've never been to a gold mine
SELECT AVG(a.totalRevenue), b.name FROM Movie a, Actor b, MovieActor c WHERE a.movieid = c.movieid AND b.actorid = c.actorid ORDER BY a.totalRevenue DESC; May or may not be correct, but it’s 3am and it’s been a long night. 
What RDBMS product requires primary keys? I know for certain that PostgreSQL does not and if I remember correct, MySQL is the same in that regard. You can have relations between tables without foreign key **constraints** and from my understanding, OP is already using foreign keys and asks for purpose of constraints. Constraints are optional. You can use JOIN and WHERE on any column or conditions you want. Primary keys don't have to be singular value; you can use composite primary/foreign keys. Primary keys don't necessarily speed up anything, in fact they can slow down DML queries and use up space for nothing. You don't always use those keys, especially if you insist on creating additional column just for the purpose of having singular unique value. Foreign keys don't have to reference primary key. Only requirement is that what they reference is unique (and something enforces this uniqueness).
I'm not sure what sort of answer you're looking for, but I just watched a brief youtube video on the feature. It seems like a useful feature for the user, as it's just a fairly basic SSMS style window, but it's built into an application and may authenticate through a privileged service account.. It seems like there would be a lot of risk of your users running a delete or update unless the app accounts for it.
I suggest leaving the query alone if you get at least the data you are looking for. Use crystal reports select expert to filter on visit_type not equal to HEARTWORM TREATMENT
Primary keys and foreign keys are integrity constraints. They tell the database engine certain information that is always true about your data. This information is used for predicting what a query will result in. When you join 2 random tables on some conditions, the planner assumes that you do a cross product with a filter and will estimate a wrong amount of rows, which may influence further choice of execution algorithms and result in much slower queries. A foreign key tells it that every value corresponds to a value in the foreign table. Certain truths result from this which makes the planner estimate the results very accurately and choose the correct plan. Faster queries.
I used to work for a company with $100m annual revenue that calculated revenue by a mess of around 40 different linked excel workbooks. Entering anything into the workbook required logging onto a Remote Desktop (so you got slightly higher performance and so when excel inevitably crashed it didn’t crash your own computer). Recently started using oracle which cut our revenue close in half (along with transitioning all of our topside revenue entries from excel into sql). 
That's the first step to figuring your problem out, lol. As hard as this is going to sound, there are (2) secrets to success when it comes to SQL: 1. It's never wrong. You are. 2. Don't be afraid to completely start over from scratch if it isn't working out. Even if you're 99% done. Thing of SQL more like a command prompt than a programming language. You can check yourself whenever you want by `select * from (your query here) x`. Is that what you expect it to be? How many rows should it be? If you started with 2000 and only did left joins, are you sure there should be 12,000 now? Doing this will let you go back to find the offending line(s) and start to fix things.
&gt; City &lt;&gt; 'Stevens Point') Change `&lt;&gt;` to a `=` and then spend a few minutes thinking about it. If you're still struggling respond here and I'll add comments, or go back to my original comments explaining what's going on and look them over.
You can only run selects fortunately. But yes, its completely open access to the database tables as long as you know what tables you want. SSMS is the better option as you can see the tables, columns, and the data because damn does goldmine do a terrible job at storing some of their data.
As the other poster said, wiseowl tutorial is great. Also the official tutorial by Microsoft. https://docs.microsoft.com/en-us/sql/reporting-services/reporting-services-tutorials-ssrs Before my Jr. SSRS Developer job, I watched the wiseowl videos and went through the official tutorial. It was really easy to be productive the first week on the job. Also pluralsight has good videos. I think you can get a 3 month trial with VS developer edition
Thank you I will definitely watch through that :)
Thank you, Yeah from the little bit I watched it doesn't seem that complicated. Seems like any difficulty comes from knowing how to actually write the SQL query for the data you want.
No, it's not only deletion prevention. There's multiple things you may want to do, on update/delete of a row, that is linked to another table. For example when you have an *accountant* that is responsible for one *contract*. Now when you fire (delete) that accountant, you most likely still want to keep those contracts. You might want to set the accountans to NULL to indicate that someone has to take them over. In a different scenario if you had a table called *meetings* (let's assume they are no group meetings), then when you delete the accountant, you probably want to delete all the meetings with this person as well. You can also read this: https://en.wikibooks.org/wiki/Structured_Query_Language/Foreign_Key#ON_DELETE_/_ON_UPDATE
I guess, I want to know the difference between the sql query in goldmine as opposed to sql in general? Appreciate for watching the video. 
PostgresSQL is typically used for semi-structured data and is a "NoSQL" RDBMS. These types of databases sacrifice some data integrity for faster performance in some cases. Also, by definition a FK is the PK from another table. Also, when creating a table with a foreign key this is added to the table as a KF constraint. For example: &amp;#x200B; `CREATE TABLE COMMERCIAL (` `CommNum int Auto_Increment,` `ClientNum int Not null,` `ContactName varchar(30) Not null,` `ContactNumber bigint Not null,` `Website varchar(50) Not null,` `Primary Key (CommNum),` `CONSTRAINT \`client_fk\` FOREIGN KEY(ClientNum) REFERENCES CLIENTS (ClientNum) ON DELETE CASCADE ON UPDATE CASCADE);` &amp;#x200B; Also, when a primary key is more than one value it is called a composite key. I get your point, there is more than 1 way to represent a relationship within a database, but even if you do not explicity create a primary key for a table, an auto\_increment field will be made by MySQL on the back end to function as a primary key for that table, you just won't have access to utilize those PKs specifically in your query.
Chamelon and crystal reports, two things I would only wish on an enemy.
by "SQL" presumably you mean Microsoft SQL Server if the same query runs in both, it should produce the same results or was your question something else?
It's just a regular sql query running on MSSQL
/r/nonono
Here's a sneak peek of /r/nonono using the [top posts](https://np.reddit.com/r/nonono/top/?sort=top&amp;t=year) of the year! \#1: [Honey, I haven’t left home... yet...](https://v.redd.it/3124ljf2kx021) | [532 comments](https://np.reddit.com/r/nonono/comments/a0yh04/honey_i_havent_left_home_yet/) \#2: [Guy gets hit in the balls](https://i.imgur.com/MMUQHww.gifv) | [6 comments](https://np.reddit.com/r/nonono/comments/8w2ag4/guy_gets_hit_in_the_balls/) \#3: [You really shouldn't laugh at people's misfortunes, but ..... go ahead.](https://v.redd.it/noekowp6uf711) | [12 comments](https://np.reddit.com/r/nonono/comments/8vet8y/you_really_shouldnt_laugh_at_peoples_misfortunes/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/afd0dd/blacklist/)
So any regular sql task i can do i can do the same for the sql query in goldmine? Sorry, I'm new and not good with terminology? I want to learn sql by using the sql query in goldmine and curious to know if it is limited in any way?
You posted same question today wtf https://www.reddit.com/r/SQL/comments/atppzo/has_anyone_ever_used_sql_query_in_goldmine/?utm_source=reddit-android
Oh i want to know now if there are any differences
SQL will be useful for 99% of any type of development role (front end or back end) since pretty much every company uses some sort of database. It's an underrated invaluable skill if you ever want to work in a professional environment. Source: experience from being a backend dev
Another benefit is database documentation. Imagine a database with 100+ tables without relationships or Primary Keys. Good luck trying to understand the model unless you have a veteran on your team for you to ask your 1000 questions. &amp;#x200B; When learning a new application (corporate development), the first they i ask for is a data model, and if one does not exist, i try to reverse engineer the model using a tool. &amp;#x200B;
Best decision you'll ever make. Go for it.
You can do any regular SELECT statements without issue. The reason we mentioned SSMS earlier was due to goldmines table structure and how tables are linked..etc. goldmine won't show you any of that while developing the select statement so you have to know these things ahead of time.
Learning SQL is a fantastic decision, it's the basis for pretty much all data analysis work. I'd suggest using sqlzoo.net - better than code academy, gets you hot to trot quick. Reference pages on w3schools are also useful. Good luck! I owe everything I love about my current job to learning sql in two weeks before getting dumped into a project. 
My entire job is nothing but writing SQL. It is literally a career all by itself (and a highly paid one, might I add). Definitely learn it. If you fall in love with SQL, you're set. If you end up not liking it, you can definitely move onto other areas of programming, and all the SQL you learned will be 100% useful. After you've done a few of the online tutorials and get some basics down, my recommendation is to go download MySQL server and install it on your computer, along with MySQL workbench (the client where you write the SQL), and then go find a big dataset you can import into it, and start writing queries, exploring the data that is in there. Digging into interesting data will help make learning fun. You can go to /r/datasets and search for "MySQL" and find a bunch of dumps ready to easily import :)
im learning sql for a college course and that shit is kind of hard and confussing...its with oracle i think. foreign keys, primary keys, composite,one to one, one to many...idk its alot Would [sqlzoo.net](https://sqlzoo.net) be able to help someone like myself ? does it have problems that i can work on and examples? becuase right now i turn in my homework then i have to wait to see if i did it right thenget feedback once its too late and were already working on the next assignment
Well, here's where I am now: `SELECT DISTINCT Tb_Product.Name` `FROM Tb_Product, Tb_Transactions, Tb_Supplier` `WHERE Tb_Product.Prod_ID = Tb_Transactions.Prod_ID` `AND Tb_Transactions.Supp_ID = Tb_Supplier.Supp_ID` `AND Tb_Supplier.City = 'Stevens Point'` `AND Tb_Product.Name NOT IN` `(SELECT DISTINCT Name` `FROM Tb_Transactions, Tb_Product` `WHERE Tb_Transactions.Prod_ID=Tb_Product.Prod_ID` `AND City &lt;&gt; 'Stevens Point')` &amp;#x200B; and depending on which logical operators I use (&lt;&gt; or =) I'll get back a list of the 8 products that are sold in all places or a list of the 3 products that are sold in Steven's point, but I can't get a list of the 5 products that are all of the products that are sold except for those that are sold in Steven's Point. I feel like I'm close, but I'm not sure why this isn't working, it seems like the perfect situation to use a negation query, no? I'm also being told by one of my classmates that they solved this with a division query and that he got back a list of more than 5 items, so I'm not really sure what to believe anymore. I'm having a hard time seeing how this could be solved by a division query, but I'm going to go back and review some of them so that I can see if there is a way. Thanks for the help of everyone on this forum, no one really needed to help, but you still did and I appreciate that. 
Or you could hate yourself a lot less and just start with MSSQL. https://www.data.gov/ Lots of fun data there btw 
Let's start over completely. Give me your data in tables. Use the following formatting to do it, and you don't have to give me everything but give me some specific examples: This: | ColumnName1 | ColumnName2 | | :--- | :--- | | Val1 | ValA | | Val2 | ValB | | Val3 | ValC | Becomes this: | ColumnName1 | ColumnName2 | | :--- | :--- | | Val1 | ValA | | Val2 | ValB | | Val3 | ValC | 
AAH THAT is also a good reason actually. Making those link will tell others which table the foreign key is refferencing
sqlzoo is more for writing queries than database design, which it vaguely sounds like you're doing? Correct me if I'm wrong. It's worth having a strong grounding in the language. For me, using and knowing the language helps me "feel" how databases work, and you can use that inherent understanding to build on when you're looking at more complex db topics. 
yea it might be that actually ,DBMS..I think next week and the weeks after are when we do the sql stuff with oracle application express or a similar program. Is that sql? I think i know what you mean though and it sounds like i just have to mess around with it and read more so i can understand how it operates..like programming kind of 
If someone is a C# architect in .net for example, I think your percentage is way off 
Can you explain why? Is it because they would be taking on more of an 'architecture' than 'development' role? 
I mean simply because they would be writing code in C#, not just SQL alone.
SQL is different from databases so SQL Zoo won't be the best. This[Database Fundamentals](https://www.youtube.com/watch?v=nhqOAuARzOM&amp;list=PL1MJdy9N8XJKW4HYg_cOsz1-xRzD2zfsT&amp;index=1) series on YouTube I found to be very good at explaining the issues your having. Its really short, about an hour long, and it was an excellent supplement to my database courses. There are tons of free stuff on YouTube. Hopefully the one I linked you find useful as a starting point!
https://sqlbolt.com/
**Supplier Table** |Supp\_ID|Name |City |State| |:-|:-|:-|:-| |1|Joel|Madison|Wisconsin| |2|Herman|Madison|Wisconsin| |3|Bernstien|Madison|Wisconsin| |4|Smith|Madison|Wisconsin| |5|Redfield|Wausau|Wisconsin| |6|Godman|Wausau|Wisconsin| &amp;#x200B; **Transaction Table** |Tran\_ID|Supp\_ID|Con\_ID|Prod\_ID|Price|Quantity|Datakey|Timekey| |:-|:-|:-|:-|:-|:-|:-|:-| |1|1|1|1|798.9900|100|0|1| |2|1|1|2|789.9900|10|1|4| |3|1|1|3|196.9900|1000|1|9| |4|1|1|4|1.9000|10000|2|16| |5|1|1|5|1.0000|100000|2|25| |6|1|1|6|.9000|100|3|36| &amp;#x200B; **Product** |Prod\_ID|Name|Product\_catalogue|Product\_line|Product\_packaging| |:-|:-|:-|:-|:-| |1|Computer|Electronics|Computing|Standard| |2|Auto|Transportation|Ground|Standard| |3|TV|Electronics|Entertainment|Standard| |4|Milk|Food|Animal|Perishable| |5|Gas|Fuel|Refined|Flamable| |6|Orange|Food|Fruit|Perishable| |7|Boat|Transportation|Water|Standard| |8|Airplane|Transportation|Air|Standard| &amp;#x200B; &amp;#x200B; Sorry if this isn't the right format, I don't post on reddit too often, so I'm not the best at formatting everything properly. 
Alright, perfect, I like this, we're starting all over again. So, to make sure we are on the same page you want to: Find name of products sold in all cities except 'example city name'? And in this example the correct answer should be this: | Product.Name| |:--- | | Auto | | Gas | | Milk | | Orange | | TV | Because we know three products (Computer, Boat, Airplane) are sold in Stevens Point? Are we on the same page so far?
Give this a look see if you agree with my last message about starting over. query 1: select * from product A query 2: select * from product A left join transaction B on A.prod_id = B.prod_id query 3: select * from product A left join transaction B on A.prod_id = B.prod_id left join supplier C on C.supp_id = B.supp_id query 4: select distinct A.Name from product A left join transaction B on A.prod_id = B.prod_id left join supplier C on C.supp_id = B.supp_id --where C.city = 'Madison' --where C.city = 'Stevens Point' --where C.city &lt;&gt; 'Stevens Point' query 5: select distinct A.Name from product A left join transaction B on A.prod_id = B.prod_id left join supplier C on C.supp_id = B.supp_id except select A.Name from product A left join transaction B on A.prod_id = B.prod_id left join supplier C on C.supp_id = B.supp_id where C.city = 'Stevens Point' query 6: select distinct A.Name from product A left join transaction B on A.prod_id = B.prod_id left join supplier C on C.supp_id = B.supp_id where A.Name NOT IN ( select A.Name from product A left join transaction B on A.prod_id = B.prod_id left join supplier C on C.supp_id = B.supp_id where C.city = 'Stevens Point' ) 
I don’t think you understand the initial post. He was saying it would benefit 99% not be their entire job...
Learning from scratch is not hard. I think most people do. After the code academy stuff, think of something you want data on and start building a database. 
Thanks for recommending sqlzoo. I blew through the (free) codeacademy course earlier this week and was like “there has to be more”... I also bought the SQL book off of No Starch Press which is how I learned Python... so far it seems really good and goes into a lot more detail than Codeacademy 
The Codeacademy stuff is good for getting a handle on the syntax's. The best purchase I've made so far for learning the coding portion has been a book called "SQL Queries For Mere Mortals". The book really breaks down the statements and has been a huge benefit to my learning so far. The Microsoft MTA certification on database fundamentals is a great place to start as well. You'll gain a good understanding of SSMS as a RDMS and how RDB's work in general.
It's always a good idea to explicitly list the columns you want in the select instead of using `*`.
Probably for speed and efficiency. Currently I haven't worked on a project where I had tables with that many of rows to facilitate the need to get individual columns. Mostly I've used almost all of the data returned. But the question is not quite in what is better to use in my case. Does the function just return individual columns as arrays? Or is it single values every time you fetch and its u p to you to recreate the array with them via a custom function?
Keep an eye out here or in r/learnsql for examples. A lot of people here won't be able to show real world examples. What type of SQL do you primarily work with?
Mostly basic dml, ddl, and dcl in oracle 12, but would like to find practical uses for the other types
The fact that you know its Chameleon amuses me.
Do you have an example? I'm not sure how to "not equal" in select expert. Sorry, like I said, its been only (2, now) 3 days to teach myself all this.
Oracle Application Express is a wonderful tool. I’m happy to hear you’re learning this in school. Very jealous I didn’t have that opportunity Learn SQL, learn proper database design and I’m 100% positive it will help you with any programming job you get in the future. 
You can use odbc connections through proc sql in sas to link directly to the database...point and click works but I like to code because I have more freedom when I’m writing something 
The drive to learn SQL comes from a desire to learn the underlying data. If you have access to a presentation layer where you don't really need to learn the underlying data, and you have no desire to, nobody on Reddit is going to convince you to learn it. I've dealt with a BI tool for quite a few years now. My desire to learn SQL came from wanting to learn how to make the data model more efficient. And to make sure what I was presenting was correct. No one had to convince me of the value - I just saw it. And I would say that until you buy into the desire to learn it of your own accord, there will be no reason to learn it. 
I had a much longer post that addressed much of that, but decided not to overwhelm someone who was already shaky on normalization to begin with, not to mention once you start doing lot's of many-to-many joins with bridge tables, it's much easier for someone new to databases to mess up the query and get duplicates/wrong data. In the real world, I'd've put "persons" in their own table, with gender being a time state dependent lookup (start date/end date, 'cause changing genders in Hollywood is real, and the persons gender can only be referenced in the scope of a point in time), job/role/film as a table (Tyler Perry is a perfect example of why a single person needs to be able to have multiple roles - actor position A, B, director positions A, B, producer A, B, C, I'm sure he's credited himself with food service at some point). For what I know about the film industry (which is granted not very much), there needs to be a genre table, with a bridge table to the films, probably a locations table with a bridge (probably with start and end dates), companies, company type, all with bridge tables, etc. etc. From a normalization standpoint, films may be one of the better known "datasets" that would require a *lot* of normalization to come anywhere close to 3rd normal form (which as I said in the beginning of my last post, in the real world, only store data once *unless you have a good reason not to*. I maintain the 3rd normal should be a guiding standard, not an iron clad rule.)
What do you do? What do you love about your current job?
I don't understand what `where 1=1` means or does, but when you say `AND bit.id IS NULL` that LEFT JOIN becomes an INNER JOIN, I think.
&gt; I think they should be within the WHERE clause? If you put those AND statements in the WHERE clause you'd only get the records where there was a match between source.id and bit.id, negating the LEFT JOIN in the first place. The point of the LEFT JOIN here would be to retrieve all records from source plus the matches in bit, if any, but regardless you'd still have all records from source. &gt; I have no idea why they allow PK to contain NULL but I guess it does The primary key 'bit.id' isn't actually null in the bit table. It's null in the result set that comes back from your LEFT JOIN because there was no match between source and bit. Why is this here? &gt; WHERE 1=1
The multiple ANDs are assisting in what exactly is going to be deleted. In this case it looks as though the multiple ANDs are ensuring that valid logins and countries are being used. The key component to understand here is that we want to delete everything where we dont get a match on the multiple criteria we are joining on. This is shown with the "and bit.id is null" With a left join, if we aren't able to match on everything specified in the joins, we won't get a match, making all fields from the table we are joining to be NULL. So if we match on Id. AND the login is found in the master login list.AND if the country is found in the master country list...then we have a successful join between the tables. With a successful join, we will find that bit.id most likely won't be null..therefore these records with a match won't be deleted. Hope this makes sense ! 
Thanks, `WHERE 1=1` is probably just for text alignment, but What about the two ANDs after the LEFT JOIN? BTW from my understanding `AND bit.id IS NULL` removes every id that does not exist in table bit (right table), but this is not exactly an INNER JOIN? An INNER JOIN will only accept rows in which id is the same for both tables right?
This was the solution. I used a WITH statement with my original query, but added the Row number partitioned by payee. Then I did a select on the result where row number was 1 to only get the latest date.
You are joining a table on itself using pk and the other two join conditions, which are specifying which records are needed for the source table. If the lookup for LoginID in TTLoginID and country in TTCountries both succeed, that record will be in the left table. If you don't find a match, the row will have no value on the right table, this is where the AND [bit.id](https://bit.id) IS NULL comes in and picks those records to delete them. I'm just starting my career using SQL, and sometimes I find going through sps easier if you just create these tables or mock up some data and run it through.
Some SQL developers put WHERE 1=1 on the first row of where condition so they can copy paste all the AND clause after them. It's used quite frequently in my company.
I use WHERE 1=1 quite frequently when I have a multi-purpose query where I'm commenting out bits of the WHERE clause. It let's you comment out any line and without worrying about adding AND/OR before it. 
Yeah you are right about the `AND bit.id IS NULL`, I immediately realized that, how stupid I was. However I'm scratching my head about the first point. First, this is a self left join, assuming that we have the tempTable as: |id|LoginID|Country| |:-|:-|:-| |1 | a | US | |2 | b | CA | |3 | f | GB | |4 | g | US | |5 | a | US | A self-left join will give me something like this: |id|LoginID|Country|id|LoginID|Country| |:-|:-|:-|:-|:-|:-| |1 | a | US |1 | a | US | |2 | b | CA |2 | b | CA | |3 | f | GB |3 | f | GB | |4 | g | US |4 | g | US | |5 | a | US |5 | a | US | Now let's assume @TTLoginID only has "2" and @TTCountries only has "CA". Which rows should I expect to get deleted? Actually let me try this dummy example out...
I started using sql at work. First just copy+paste change some numbers and hit run. When I first started making my own queries w3schools was and still is an awesome resource. 
I personally really enjoy writing quieres. Still new at it (one year into writing my own, 3 months into writing more complex ones). What would you recommend as a good career path for sql jobs? I'm still new to the whole engineer side of things. Previously worked sales for years... 
Thanks u/maxinxin after reading through all comments and using some mock data for practice I'm getting the meaning of the query. You are absolutely right as the mock practice makes things visible and I'll definitely use this tip every time I'm inclined to post a question.
Thanks u/SekretZivilist, after reading all comments and using some mock data for practice, I'm finally getting the meaning of the query. Thanks again for helping!
Just one more question. Since the query is to DELETE all records that do not satisfy all conditions (LoginID and Country), couldn't I just write something like: SELECT * FROM #tempTable AS source WHERE source.LoginID IN @TTLogInID AND source.Country IN @TTCountries I think the above query does exactly the same thing without the awkward syntax. Is it because of speed? I heard from somewhere that SQL does not like a lot of conditions in \`WHERE\` but I cannot find the reference at the moment...
It's called an exclusion join, what it is basically saying is give me everything in the right table that does not meet the join condition to the left table, the best way to think of it is a better performing not in statement. However as others have said this query is odd, the self join seems spurious, the 1=1 is completely useless, and the two subqueries get rid of a lot of the performance advantage you would get from an exclusion join. &amp;#x200B; In any case the author is deleting all records from the temp table where the login ID and country ID don't exist in the two table variables. To put it into an easier to read format (that probably performs worse): delete from #tempTable where loginid not in (SELECT VALUE FROM @TTLoginID) and country not in (SELECT VALUE FROM @TTCountries) &amp;#x200B; To over analyze it a little bit, this part of the proc probably sucked performance wise, because there is no reason to go through those kind of acrobatics for an otherwise normal delete. I assume it's because those table variables tend to be big (forcing them onto disk), and since they are probably not indexed, it's basically doing a table scan. To give some unasked for advice since I assume you are refactoring this, the first thing I'd suggest is to figure out why those table variables are necessary, or if they are necessary at all. 
Close, what it actually does is exclude records that have a match in the left table. If we had values A, B, C in the right table and A, B in the left table, and joined them on value = value, where left table is null, the result would only be C, since for value A and B there is a match in the left table. &amp;#x200B; This technique is called an exclusion join, and it's a faster not in statement, because it uses the join engine, which is one of the most optimized pieces of the sql engine. 
This probably wasn’t built with performance in mind as the extra steps seem overly complicated. As someone else pointed out below. As for the query, your query will delete only when the record has neither login or included in country table. Whereas the original would delete if one was missing or both were missing.
1=1 is usually just a throwaway condition, it means that the real conditions are all in the form AND &lt;condition&gt; So can be easily manipulated. 
Complex is in the eye of the beholder, alot of people think verbose queries are complex, however most large queries are asking for simple stuff with a high level of specificity. Personally, I think really complex queries in sql come from relations and the tricky things you can do with them. For instance: Easy: select * from tablea a join tableb b on a.id = b.id Easy as falling off of a log, we are just grabbing everything in table a and B where the relationship is evaluated as true. &amp;#x200B; Intermediate: select a.* from tablea a left outer join tableb b on b.id = a.id where b.id is null This is an exclusion join, give us everything in A that does not have a match on the join conditions in B. I've used this alot when looking for errors where things were partially completed. It's also a fast way to do a not in query. &amp;#x200B; Intermediate: select a.*, b.* from tablea a full outer join tableb b on b.id = a.id where (a.id is null or b.id is null) Not the exact query used, but I was basically tasked with finding records in table A that didn't exist in table B, and records that existed in table B but not table A. Doing both in a single query just seemed most efficient. &amp;#x200B; Advanced: select a.value, isnull(b.value, c.value) as violations from tablea a left outer join tableb b on b.id = a.id left outer join tableb c on c.id = a2.id and b.id is null where (a.value2 is null or (b.id is not null or c.id is not null)) A conditional inner/outer join with a hierarchy of tables to choose from. For this one I was working with the FMCSA Cview DB, where the site officers kept screwing up which field they entered the violation code into. So there were three scenarios, there was no violation which it needed to function as an outer join, or there was a violation and we had to switch to an inner join, but the violation code could be in A or A2. To prevent join expansion in an aggregate field (not included here since that would make the example much larger), I had make the second join to table b only happen if we didn't have any luck with the first join to tableb. 
I find that with most of the "front end" type of tools, if you really know SQL you can cut out a bunch of bullshit between your data and the insight/analysis/reporting you want. In your description of your current work it sounds like you're already making multiple reports to aggregate and filter to your desired end. These steps could probably be handled in a single SQL query. 
alright i appreciate it, ill take a look at those 
i guess we wont use that im not sure, my teacher said we didint get the license for it but we will use something similar. I do know it is a vital skill though,its just seems like alot
I'm now an Architect / Innovation consultant. I get thrown at hard problems and get asked to fix it, using tech. 
Here’s a habit you should train yourself to get into. Always start out with a select statement. Get the select statement working the way you want it. Make a copy of the select statement and change only the top of it to whatever you’re doing (insert, update, delete, etc.). Then for extra credit always write another select to verify what you changed looks right. Prepare, change, check! If you stick to this practice you will be a better programmer. I know really amazingly smart programmers that don’t do this and it causes them problems. Good luck!
What it does is removes records from `#tempTable` for which there are no matching records in @TTLoginID and @TTCountries, basically some sort of validation it looks like. I would have written it as follows for readability: DELETE rec FROM #tempTable rec WHERE NOT EXISTS (SELECT 1 FROM @@TTLoginID WHERE value = rec.LoginID) OR NOT EXISTS (SELECT 1 FROM @@TTCountries WHERE value = rec.Country) [Here's a fiddle with both](https://www.db-fiddle.com/f/6DvJCRJcYREtBTegXyMa4B/1)
Top job. It’s good to learn new sql tricks. 
Good point, u/[levelworm](https://www.reddit.com/user/levelworm) this is a great opportunity to make the code a little better as u/pooerh explained. Furthermore, it's using table variables for @@TTLoginID and @@TTCountries. I'm not sure how large these are, but keep in mind that table variables often perform MUCH more slowly than#temp tables when there are a lot of rows. So there could be free performance to be gained there. You could also look into doing a join or exists further up and only loading #tempTable with appropriate TTLoginID and TTCountries criteria instead of loading everything and then deleting.
Apex comes with every oracle database. You only need a license for the database. If you have one of those you can use Apex for “free” Either way you can go to apex.oracle.com and sign up for a workspace that you can use for free to prototype. That gives you access to your own oracle database as well. I’d recommend doing that so you can keep your projects outside of your schools license. Apex isn’t a vital skill but you will learn a lot about web and back end development. Also there are lots of high paying jobs for apex developers out there... Let me know what tool you use if not Apex, I’d be interested to hear that. 
Thanks, agree with you, thing is this is not my code though, just trying to figure out what he tried to do.
&gt; exclusion join Thanks for pointing out this. Finally I managed to find some articles on this specific topic on Google.
I'm not sure but I think they are actually temp tables. The original variable is a string, and then gets converted to a table by another SP or function. So here is what he is doing: First define a variable for country list: @countrylist varchar(2048)=null Then you declare a table, here is our @TTcountries: declare @TTcountries table( value varchar(5) ) This string will actually take a list of countries from the front end (a web application), say you get: "US,CA,GB,CB" Then in the middle of the SP there is one line that calls another function: if @countrylist is not null insert into @TTcountries( value ) select Value from a.[dbo].[StrToTbl](@countrycodelist, ',') The thing is, I don't have access to this a.dbo.StrToTbl, but I can figure out this is probably a function that turns a string with comma delimiter into a table. The SP does the same thing for other filters (I only listed two in the original snippet but actually there are about 50-60 so half of the code actually consists of reading and transforming the filters into tables)
What he tried to do is write SQL, but it doesn’t look like he’s very familiar. The biggest take away here is he did a left join where one of the columns in the key on the right is null. This is a common join that does a very specific thing. A left join returns everything from the left table and rows from the right table if they exist. If a row on the right doesn’t exist it will be null. Because he’s saying where right column is null he’s taking records from the left table that don’t join the right table. Make sense? 
Yeah the problem with table variables is there are no statistics, row estimate from a @table is always 1 (I think they changed it to 100 in some version and 2019 got even better), which means terrible query plans for bigger loads. I'd definitely dump that data into a #tmp and slap an index on them bad boys for good measure. The amount of performance bottlenecks I solved with this one simple technique is exorbitant.
Yeah I get that exclusion join, thanks u/TopWizard! Took a while to figure it out.
In my opinion, complex is applying inter-row data transformations from a dataset to another. A good example for me is converting daily data to date ranges: ``` CREATE TABLE daily ( day_id INTEGER, measure_id INTEGER, value INTEGER ); -- Whatever, doesn't have to be integers, but this is to illustrate the structure -- So each measure has a value for each day. But values don't change every day. An example is sensor collection on a millisecond interval (as opposed to days). -- What I want to get is this CREATE TABLE measure_intervals ( day_from_id INTEGER, day_to_id INTEGER, measure_id INTEGER, value INTEGER ); INSERT INTO measure_intervals WITH flags AS ( SELECT day_id, measure_id, row_number() OVER (PARTITION BY measure_id, ORDER BY day_id) rn, row_number() OVER (PARTITION BY measure_id, value ORDER BY day_id) rn_val, value FROM daily ) SELECT min(day_id), max(day_id), measure_id, value FROM flags GROUP BY rn - rn_val, measure_id, value; ``` Generally, such window function tricks are neat, and for harder problems they can make the query look really complex.
I think this is first time I see someone call PostgreSQL a NoSQL RDBMS (I think those two words don't mix together, iksde). It does have JSON and XML type that do work similar to NoSQL but this is just small part and you don't have to use it. What definition says that FK points to PK of another table? MySQL documentation does not and syntax of constraint permits more than one column. Only requirements listed are index (not even unique) and type. KF constraint? Is that a typo? Never heard about that. Primary key with more than 1 column is indeed a composite key, but it is still primary key. Is that hidden auto_increment column feature described somewhere in documentation or elsewhere? I always thought that unless you specify it, it won't be created.
Yes this was a great example. I havent had much practice with the more difficult functions such as lag and partitions
Glad to help. In my other post I go a little into why they would have had to use this, and offered a suggestion to get rid of it if you are refactoring.
I love window functions, being able to perform multiple aggregate functions with different grouping conditions in a single select statement is a real life saver. It's odd, as powerful as they are, I rarely see them in problems on stack exchange, or in the code of my co-workers. &amp;#x200B; Though in all fairness, if we wanted to list the most difficult to use functions, I'd have to say pivot and unpivot would top my personal list. There is a reason I always do my pivots in excel.
Window functions don't replace aggregate functions. Aside from basic row number and cumulative sum, window functions are only used in complex analysis and even more complex ETL. So yeah, you won't see them in a lot of places. &gt; pivot and unpivot It is my opinion that SQL is for data, not for report formatting. For the latter I use Tableau or Mondrian with Saiku
You are probably right about the last part. Let me describe the user case: Basically we have a web application front-end that links back to this database. In the front-end you get to choose a maximum number of 50-60 filters (thinking OS, Device, Traffic Source, Campaigns, those kinds of stuffs). Each filter corresponds to one column of the transaction table (because it's DWS the table is pretty much de-normalized). Now some filters may contain multiple values, e.g. I want to see countries of US + GB + CA, so the web application will generate a string "US,GB,CA" and stores it into @countrylist. There is another SP or function transforms each string variable into a table, and hence we get @TTCountries (a table containing three rows in the case). Now all filters have been taken care of, and all filter tables (like @TTCountries) have been prepared, the author uses the LEFT JOIN to self I described to filter out all rows of the transaction table that do not meet all filters. After that it's just simple aggregation to calculate fees/revenue/etc. because they are also in the table. So as you said, those table variables could be big (say I want to investigate 500 campaigns), but not necessarily on disk (they are created at the beginning of the SP). They are definitely not indexed as you cannot index every column of the transaction table, so it's a full scan everytime. I'm only reading the code for fun though, as I do not belong to BI. I wish in the future I could move to BI, but for now it's just for fun and learn. Thanks again!
I like tableau as well, but being a microsoft shop, and working with microsoft partners we get a lot of push towards power BI. Which I think is kind of unfortunate because calling power BI half baked might be overestimating it. I've performed all sorts of aggregates using window functions, they might not be a full replacement for groups and having clauses, but man are they handy. My most used is first_value, It's just the nature of the hardware we deal with that near duplicates (duplicated in all but one or two aspects) are a common occurrence, so being able to identify the original record and ignore the rest on the fly is pretty handy. Beyond that I also use them in ETL and customer facing reports, sales in particular want several slices of data for each row, and it's just easier to use window functions for them. 
If you're using Postgresql, then the ``` SELECT DISTINCT ON (field1, field2) field1, field2, field3 FROM mytable ORDER BY field1, field2, field3 ``` syntax is far more concise and easy to understand and fast to execute in this case.
Split is probably the most common custom table valued function, you'll see it just about everywhere. As for optimization, it's as much an art as a science, because there are very few hard rules to follow. So without knowing all of the specifics and having a query plan it's hard to offer general advice. However getting in and looking at the code is a great way to get started in BI, because it can give you domain expertise in your current systems, and pick up a few sql tips and tricks along the way.
(((Datepart(year,getdate()) *100) + datepart(month, getdate()) * 100 ) + datepart(day,getdate())) On phone so probably have unbalanced brackets but you get the idea
Postgres is a better sql implementation.
 select t, convert(time, stuff(right('0'+left(t, len(t)-4), 4), 3, 0, ':')) as convertedTime from (values ('9420000'), ('9590000'), ('10060000'), ('10070000')) x(t) Gives: t| convertedTime ---|--- 9420000 | 09:42:00.0000000 9590000 | 09:59:00.0000000 10060000 | 10:06:00.0000000 10070000 | 10:07:00.0000000 Inner to outermost: * `left(t, len(t)-4)` - take whatever characters except for the last 4, ie. 9420000 =&gt; 942, 10070000 =&gt; 1007 * `right('0'+ ... , 4)` - left-pad the value with a zero up to 4 chars, ie. 942 =&gt; 0942, 1007 =&gt; 1007 * `stuff(..., 3, 0, ':')` - insert : at position 3, ie. 0942 =&gt; 09:42, 1007 =&gt; 10:07 If it doesn't work for some of your data, use `TRY_CONVERT`, it will give you a `NULL` instead of throwing an exception and you can tweak the query for that edge case.
You Orders table needs to have a CustID field in addition to the foreign key constraint. Your error is saying that you don’t have the field the constraint is referring to.
You forgot a line creating a column in the Order table.
thank you!! 
thanks! 
Hint: you're trying to create a table called 'Customers'. The error is saying 'There is already a table called Customers'.
I created a table called customers. There's only one table called that, and that's what the error message is referring to.
The hint is: you can't create something that already exists.
You've already created it, and now you're running a statement that says 'create this again'. That won't work - you'll need to either get rid of the first one you made and create it again correctly, or alter the one you've already created.
Thanks u/pooerh. I tested two code snippets with four filters: First one is similar to my example but instead I use [`b.id`](https://b.id) `IS NOT NULL` to retain the filtered rows SELECT a.* FROM #tmpAllTransactions AS a LEFT JOIN #tmpAllTransactions AS b ON a.id=b.id AND b.TransTypeId = 6 AND b.RevenueTypeId = 1 AND b.MembershipTypeId = 3 AND b.Currency = 'GBP' WHERE 1=1 AND b.id IS NOT NULL Second one is: SELECT * FROM #tmpAllTransactions AS a WHERE 1=1 AND a.TransTypeId = 6 AND a.RevenueTypeId = 1 AND a.MembershipTypeId = 3 AND a.Currency = 'GBP' On a 3 million row sample the second one runs significantly faster than the first one. However I'm not sure if it's the same when I need to `DELETE` instead of `SELECT`. I'll do more tests and report back.
I learned fist on [Udemy.com](https://Udemy.com) using this course: [https://www.udemy.com/learn-sql/](https://www.udemy.com/learn-sql/). It was only $12 
 Oh boy. I’ve been in your shoes before! Friends overexagerating on your own skills. Happened twice to me! I suggest you check out /r/learnSQL and /r/database and also take a look at [Stratascratch](https://www.stratascratch.com/#product) they help you prepare for specifically SQL related questions asked by top employers and you can try it out for free. Either way if the job requires extensive SQL you will have to get better sooner or later, so might as well do it before the interview. Good luck! 
Business / data analyst is probably a good next step, and then once you are more experienced at writing SQL, business intelligence developer is where I'd go. ETL developer is also a good option after that, if you want to delve into loading data into databases, which can also be SQL heavy depending on the company you work for (my company does all of our transformation in SQL). If you haven't already done so, work on Excel skills as well (get comfortable with pivot tables / vlookups / creating graphs). Everyone hates Excel, but the reality is that it gets used way too much, by everyone, and is great for quickly slapping data together in a visual, useful way. 
Thanks for the feedback! One good thing about working in Japan is that Excel is my best friend that I love to hate haha. Gotten some good practice with lookups, index, pivots, and some basic VBA stuff.
Lots of free resources available on the web out there, but if you're looking for any formal published material, check out SQL for Dummies 9th edition. It was just published this past December, and is on Amazon Kindle for about 16 bucks. Allen Taylor is a bit of a graybeard but he knows his stuff. About 470 pages long broke down into 7 parts. If you can handle covering 1 part per day, you'll finish it in a week. This will at least help you up to speed with all the SQL terminology and concepts they will likely be expecting of you.
your journey to SQL Authority begins and ends with [Pinal Dave](https://blog.sqlauthority.com/)
two things first, to get a decimal result for integer arithmetic, you have to introduce a decimal into the calculation SELECT 1.0 * COUNT(foo) / COUNT(bar) second, because of the inner join, your two counts are always going to be equal and the answer will always come out as 1.0
No. A join us faster in most cases. If the planner is intelligent enough it will optimize it to a join, but in Postgresql it won't, so this forces a nested loop, which can be very slow
Which dbms are you using?
I have access and was also thinking about working with mysql as well as sql server. Unless you have any other suggestions? 
Are you a student?
I am not. Trying to add more to my resume. 
https://javarevisited.blogspot.com/2018/07/top-5-advanced-sql-books-for.html?m=1 I used 1,4,5 of these books. But sql is just like other languages the more you do it the better you become. It's all practice, practice, practice. When I was learning I got my own data that way, I didn't just repeat stuff in the videos and books.
I found oriley sql cookbooks very useful once I’d got past the beginner stage
The second query, the sub query has to be run for each record so will be much slower
It isn’t technically an auto_increment column, it is a 48 bit monotonically increasing value. The InnoDB storage engine is index organized around the PK. If you don’t have a PK an internal PK called GEN_CLUSTER_IDX is created, but this index is not visible from the SQL layer. https://dev.mysql.com/doc/refman/5.7/en/innodb-index-types.html
One can also try Kernel for SQL Database Recovery software to repair and recover damaged SQL database MDF file. To know about the software, visit: [https://www.nucleustechnologies.com/sql-recovery.html](https://www.nucleustechnologies.com/sql-recovery.html)
Verbatelo is a good resource to start 
&gt; i don't really know how sql compiles nested select and join so please enlighten me. Run `explain plan` on both queries and see the execution plans chosen for each.
You have miss two columns ("CustID","ItemID") from your "Orders" table which is foreign key's to "customer" and "stock" tables.
I'm not an expert on loading, but a quick/dirty way to do it might be to break it into two, import the floats, and inport the nvarchar's, then use CAST() and UNION ALL to import them. I know my colleagues who are good at this kind of thing never (rarely) use the SQL import wizard, and handle issues like this in SSIS where they have more customizations and options for how to handle issues like this. I've only done a bit of that type of work, but it might be worth looking into if you plan on doing this a lot. If it's a one time deal though, see the quick and dirty method above. Or if it's a small enough file just open it up in Excel, create a new column B, and write something like ="xxTEMP" &amp; A1 in B1, copy the formula all the way down, the copy the column and paste it as values to remove the formula, then delete column A, or better still copy columsn B:n to a new file, save, import, then use SQL to get rid of the xxTemp string. 
Hi tell the point of left join temptable as bit on [source.id](https://source.id) = [bit.id](https://bit.id) there is no problem this normal left outer join. for and this is another condition to pick or to get only rows from (temptable bit) with loginID that does exists in this query (select value from ttloginID) and same are with country the country in (temptable bit) that exists in the table countries &amp;#x200B; note: convert delete statement to query to be able at the end to see exactly what you are about to delete ex: select \* from temptable as source left join tempttable as bit on [source.id](https://source.id) = [bit.id](https://bit.id) where 1=1 thanks.
 update table1 set field1=x.zfield1 from ( select field1 as zfield1, id as zid from table1 where id is not null) x where id = x.zid and field1 is null I use the z prefix just to avoid weird issues with my autoformat stuff so it's just habit, but it's probably not necessary
This worked perfectly. Thank you sir!
I'm new to sql myself and so far I've taken "The Complete SQL Bootcamp" on udemy which I thought was awesome and I learned a lot. For practice and just other avenues of learning sql I would recommend checking out: Sqlzoo Sqlbolt W3 schools YouTube Hope this helps!
thanks for the gold, kind sir!
I read this one in a similar post, its a little more complex position i believe, and comical, but it might give a little insight on what to study up on I am in a similar position as you :)
Why would it be 1.0? Wouldn’t the counts be different? 
&gt; Wouldn’t the counts be different? i don't think so please run this query -- SELECT account.accountid , subscription.packageid FROM account INNER JOIN subscription ON subscription.accountid = account.accountid ORDER BY account.accountid , subscription.packageid visually inspect the results to make sure you are actually getting the data results that you want to count okay, now count the **number of values** in each of the two columns unless one or the other of the columns contains NULLs (which, being `accountid` and `packageid`, they shouldn't) **the numbers will be the same** 
No, join is significantly faster. 
Use a CTE. You can do a WITH statement for both, then just join them.
Can you post the case?
Essentially, it's: SELECT CASE Cart WHEN 'Empty' THEN 0 ELSE 1 END as CartValue, CASE Cart WHEN 'Empty' THEN 'A' ELSE 'B' END as CartType, CASE Cart WHEN 'Empty' THEN 'Vide' ELSE 'Plein' END as CartTypeF From Mycart.
Good link :)
As a general rule, learning to code gives you more power and control than using an interface, much like driving stick vs automatic. If you don't need it however, that extra control isn't necessary. The other thing is that I've worked with people who only know how to use interfaces. They tend to not understand why they do things. For this reason, I've often seen errors slip into their code and over time make it unusable. A good example of this is an intern I had a few years ago who didn't understand what a relationship. He thought that he was good in Tableau, but because his relationships were wrong, then over time as the data grew, his charts stopped making sense.
And you're doing this for a stored procedure? There isn't really any way to optimize that, per se, but here's what you might think about trying: 1. Only include [Cart] in the stored procedure, and produce those fields in the view. 2. Look at other places in your sproc to optimize and do this work. I'm assuming your sproc is some big cluster fuck of a query, with subqueries, etc.? Or is it a clean SELECT Blah From Table LEFT JOIN WHERE?
You are looking for GROUP BY. Select cola, count(*) as cent From source Group by cola
Is Cart indexed?
Would that matter? It sounds like he has a proc that joins a bunch of tables and then does a CASE on all rows. 
&gt;I am hoping the same logic can be applied with two conditions .... e.g. source table: colA, colB Output distinct values and counts for BOTH colA and colB) and if you mean: &amp;#x200B; |ColA|ColB| |:-|:-| |Apple|Strawberry| |Apple|Kiwi| |Apple|Strawberry| &amp;#x200B; Should return: &amp;#x200B; &amp;#x200B; |ColA|ColB|Count| |:-|:-|:-| |Apple|Strawberry|2| |Apple|Kiwi|1| &amp;#x200B; Then just add an additional group by: &amp;#x200B; select cola, colb, count(*) as cnt from source group by cola, colb &amp;#x200B;
It seems you're doing a mapping of "Cart" to other columns. So, create a mapping table and join to your original data source, pull relevant mapped column values from the mapping table. Boom, one cart column comparison.
 i need help with a sql query to get gross net and balance,txoperation 100 is credit money in, 101 debit money out 
Rough and very likely problematic security move: make the domain of the new instance (where sql server standard is hosted) trust the domain of the instance where sql server express used to be. A correct and more involved process starts with: find out why exactly the connection is refused is the server reachable? is the port reachable? Is user authorized to connect to the server? Is user authorized to access the DB they are trying to access? Is user authorized to perform the action they are trying to do? Is DB in the mode that would allow such an action to happen?
SUM will be your aggregate function of choice for totals, use CASE expression to figure out the sign for the amount
So stupid I didn't think of that.
Look up joins and inner joins
Ah no it wouldn't. Sorry for any confusion OP.
If you want to combine them side-by-side (horizontall), then use a `JOIN` If you want to combine them up-and-down (vertically) then use `UNION`
All the column names are the same and I want to see for example all the rows for Stew first and then for Dan. I'm confused by what you mean combining them horizontally and vertically...
I'm also confused by what you mean exactly :) But I think you want a UNION
This isn't really a way to "optimize" anything so your original insight ("There isn't really any way to optimize that, per se") stands strong, no need for self-deprecation, imo. 
for example: &amp;#x200B; SELECT \* from Closed\_sales('Stew', '01/01/2019', '01/31/2019') &amp;#x200B; will give me this: &amp;#x200B; User x y Stew 1 2 Stew 2 3 &amp;#x200B; &amp;#x200B; SELECT \* from Closed\_sales('Dan', '01/01/2019', '01/31/2019') will give me this: &amp;#x200B; User x y Dan 0 0 Dan 0 0 &amp;#x200B; I want the end result to look like this: &amp;#x200B; User x y Stew 1 2 Stew 2 3 Dan 0 0 Dan 0 0
I did UNION: SELECT \* from Closed\_sales('Stew', '01/01/2019', '01/31/2019') UNION (SELECT \* from Closed\_sales('Dan', '01/01/2019', '01/31/2019')) &amp;#x200B; The following error I received is The data type image cannot be used as an operand to the UNION, INTERSECT or EXCEPT operators because it is not comparable.
Take online courses from DataCamp, Udemy, EDX, Coursera, etc... those help me a lot, hope them useful for you too.
 SELECT * FROM Closed_sales WHERE theName IN ('Dan', 'Stew') ORDER BY theName DESC
Why do you have an IMAGE data type in a Closed_Sales table? 
In terms of what the database engine is actually doing, it will only be reading the Cart field once. It's not the case statement that's killing your peformance.
This is not the same query as what’s trying to be done. 
Find and replace?
I also receive the following error: &amp;#x200B; An insufficient number of arguments were supplied for the procedure or function Closed\_sales.
Theres no where clause on this query? If there is, post the full query. The CASE requires almost a negligible amount of cpu, so your performance issues are not caused by this 
tried find + replace and got an error. i'm thinking campaign ID comes from a table called "Campaigns" but not sure if there would be another table :/
Ahhh, I see. Closed_sales is a function. I thought that was a sample of the output you were looking for. Use a UNION.
You have one too many pairs of parenthesis. SELECT * FROM Closed_sales('Stew', '01/01/2019', '01/31/2019') UNION SELECT * FROM Closed_sales('Dan', '01/01/2019', '01/31/2019') As far as the error for the IMAGE data type. You're probably going to need to move this to a separate function after you've obtained your Closed_sales result set. Create something like Closed_sales_image (whatever the image is). 
If you change just the two instances of: &gt; where offer_id in (128639) to &gt;where campaign_id in (123456) (change campaign id to a real one) does it give you an error and if so what does the error say? Also I'm guessing the viewing ID is specific to the offer/campaign so you probably need to change those too. tbh without being able to describe the data, you're probably stuck.
More my stupidity in failing to adequately explain what I meant about doing the work somewhere else. I was thinking a join and breaking the total sproc down into parts, but I didn't say it properly. One of my "personal development" goals is to improve my communication, and this sadly is a great example of how I still have a ways to go.
You could try: CASE WHEN [Column] IS NULL OR [Column] = 0 THEN 0 ELSE [Column] END AS [Column]
&gt;I tried a replace on '' and NULL but neither seems to catch an empty cell. Have you thought that the cell might be filled with spaces? Have you tried right clicking into the cell, clicking copy, and then pasting whatever is there into SSMS and seeing what length it is? Or have you tried doing something like: select len(fieldname) from table where fieldname not like '%[^0-9]%' Another way to do it would be something like: , case when fieldname not like '%[^0-9]%' then 0 else fieldname end as newfieldname
That's the problem though, the column isn't NULL or 0. 
That's a good idea, let me try thay. 
What is the data type for the columns? Is it a proper decimal column or something like varchar(255)?
Scratch this, sorry, sorry nevermind. I got bad information from someone. They're null. That makes so much more sense, it was driving me nuts!
omg it works thank you! 
I clarified twice too &gt;It's empty? It's not NULL? ... &gt;Nope, empty. Finally got in there myself, pulled up the view.. &gt;NULL &gt;NULL &gt;NULL
It's okay...I thought Closed\_Sales was a table....
If you are working with more than 2 of these will adding Union multiple times work? 
I wasn't aware it could be filed with '', it should be either NULL or decimals, no?
I was misinformed. It is actually NULL, that's why it was driving me nuts! I asked several times to clarify but was told "nope not NULL, it's just empty!" I think they were using a client that displayed NULL as an empty cell. Either way I just used ISNULL(Column, 0.00) to resolve the problem. 
Try UNION ALL?
We had someone at my old job who simply could not understand the concept of *null*, despite on three separate occasions having tried to explain the concept using examples. It became a running joke with the developers.
pgexercises.com Also check out sqlzoo and sqlbolt
I love you man #nohomo, thank you!
I love you both
It's fun.
So image data type is actually a bunch of notes... &amp;#x200B; what do you mean by create a function...?
No problem, glad I could help. If you want something more curated where you can practice advanced SQL, check out Vertabelo Academy.
Multicorn does this on Postgresql for years now, and supports pretty much any source, not just the few listed here. More people need to know about this :) Not that it's the right way to do stuff. It's the lazy way to do stuff. But the fastest is still getting your data locally, if you want to process it. So ETL is still king.
In most cases it's `UNION ALL`. It's the same with `LEFT JOIN`. I think that in a query that uses anything else than these, a comment is required to explain that it's not a mistake, and why it's necessary to do it so.
Use `UNION ALL`. You should use `UNION ALL` in all cases where you want to combine result rows from 2 queries, unless you know that you don't want `UNION ALL` but something else, concrete. Until then, just use `UNION ALL`. It literally means "Stick the following rows to the bottom of the above rows".
Depends on the database engine. In PostgreSql this can be done with a `CROSS JOIN LATERAL` with a single `CASE` that returns an array or JSON, that you can then use to populate the columns with. But that's not where performance is usually lost. 5 string comparisons are nothing.
I don't think those interview questions give value. They're technical for MSSQL and show nothing about understanding data. One could google that question (ex. `SELECT INTO`) and work with it. Bad people were hired, good people dismissed, just on the basis of such encyclopedic questions that show nothing about whether the person knows how to think. In my opinion it's always good to start with a question like "how do indexes work?". This will tell whether the person has any memory of CS classes or DB books, or any fresh experience with indexes. Deeper knowledge would mean a great deal of experience in any of the DB engines, and that even if he doesn't know MSSQL well, the candidate will be able to take it on easily. Or how about the steps of the execution of a query. Ask about the query plan. Or ask about the benefits of normalized vs flat tables. Ask questions that show experience with thinking about problems related to databases. Not stuff that one could read from a "Learn MSSQL in 3 days" website. Or show him a query that has something obviously wrong with it in terms of performance or data consistency, and ask the candidate to analyze it (use the internet if needed) and say what's good and bad about it.
Good to know! I really want to carve out time to work with PostgreSQL more frequently. Hard to take time away from SQL Server. Agreed, ETL is still king. But interesting that MS is trying to change how ETL works natively in SQL Server 2019. 
I *always*use the prefix even in cases where it’s completely unnecessary lol just so it’s always there 
Can you add indexes ?
Can you post the entire query?
There’s probably a much better way, but just in case nonone shares it, if the ‘ABW’ is always the first 3 characters, I’d try creating a temp table ... Select * , left(accountnumber, 3) as prefix Into #base From yourtable Select * from #base where prefix &lt;&gt; ‘abw’ &lt;&gt; shouls be a lot quicker than not like 
Try this. When you connect, go to options&gt;additional connection parameters, and enter "always encrypted = disabled" and see if you can connect. 
I'll give that a try.
Instead of using the CASE, could you break the case out into separate UPDATE queries with their respective 'CASE' clause? I am also curious to know what "killing" your DB means exactly. 
&gt;I have found that some UPCs represent different products at different stores. This is not normal, I would find out what is happening in detail. &gt; The product descriptors for a UPC sometimes change over time. This simply means that your 'product' is identified upc at a certain time. Change your pk accordingly. &gt; The data also has information about stores which have closed and presumably some of the transactions in the data took place in those stores. that is currently accommodated in your diagram, if I'm not mistaken. 
Definitely TRY\_CONVERT. Good answer m8. 
IMO, RegEx is your best friend. =\] 
Build yourself temp tables (whether truly # or not), use a command via Crystal, and work your magic. 
Check your disk to make sure there's enough space, other than that, I have nothing. ;D
Definitely from left field, but, what if you created a computed column for the left(col,3) and indexed it?
The force is strong with you. 
Do you know how I can check that?
I don't see how indexes and execution plans are considered entry level SQL.
Assuming MSSQL &gt; The easiest way is to right click on your database &gt; properties &gt; files (check the path). Compare the drive letter to the disk on the box. Also, while in your the database properties, check your auto growth (maxsize) to ensure it's not set to some absurdly low amount .
You could take this further by removing the \* and using a translation table which only contains the left(col,3). 
You're most likely going to have to rewrite the entire query in terms off campaign\_id vs offer\_id as without seeing the schema, the JOINs are going to be borked. 
short answer: yes, SQL is easy to learn without a degree. if the hiring manager is willing to give you a shot and you have some basic querying fundamentals down, you should be good to go. there are many free resources out there to help with learning, I personally recommend w3schools to friends. honestly you could read every book and take every class, but nothing is as good as experience when it comes to solving problems with SQL. Syntax will take some time to get proficient with. In general the SQL noobies seem to pick up querying/reporting first which means some learning reporting applications like tableau/crystal/etc as well. SQL is a great language to know, but theres a lot of other areas around it to learn as well. good luck!
Is it possible the "force encryption" setting got turned on for SQL server? [https://i.imgur.com/JnSekQC.png](https://i.imgur.com/JnSekQC.png)
I know this doesn't fit into "exercises" per say, but, with a free account, you gain access to the majority of "last than this year's" pass videos. [https://www.pass.org/Learn/Recordings/Listing.aspx](https://www.pass.org/Learn/Recordings/Listing.aspx)
Just as a pro-tip moving forward: Never trust anyone. Just look for yourself and write a simple query such as: select fieldname, count(*) from table group by fieldname order by count(*) desc Someone told you it isn't *null* but, "just blank?" Interesting. How come 40% of the data is *null*? First rule of data analytics is: Never. Trust. Anyone.
I will look into those learning resources. Is it something you think I should take some formal classes for as well? I will talk to the hiring manager/department and see what they would need from me before I apply. I will probably dive in to learning for at least a year or so. And experience is everything. After my trade schools I was a shitty mechanic that knew some stuff. Now I’m a pretty good mechanic even though I hate my quality of life with the scheduling. Thanks for your time. I appreciate it very much. 
Entry level what? Production? Development? Analyst? 
I would learn toward indexes are entry level in the sense of they should understand what they are, but, not be expected to lay down a single covering index over multiple queries. Agreed, Execution plans are definitely not entry level. 
Depending on your version of crystal reports developer, go to the report tab -&gt; select expert. Create a new selection. Open the formula editor within that dialogue box. Use the command NOT({command.visit_type} = “HEARTWORM TREATMENT”) The field name can be selected from the field list. I estimated what yours would be. 
&gt; * Since I've seen it on Reddit so much today, this seems like a good opportunity to throw out a "whoa, there, Satan!" ;) Or just the key column(s) and the left(col,3) column. 
Yes, but, the left(col,3) will not be SARGable as a computed column would. :/ But'll, I'll slow my devilishness down :) 
I was agreeing with you about removing the * but somehow now when I look at my comments it's a bullet point dot. I blame mobile. 
Hi, I was wondering, how complex are the SQL code you write at work? Are they usually over 50 lines? How large are the databases you work with - are they over a million rows? What is the most complex code you've written at work? I ask because I am a noob that wants to learn SQL to use professionally. Thanks so much!