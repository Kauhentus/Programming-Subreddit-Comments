Probably not the same instance.
You almost have it. Try this: Update s Set `Column A` = replace(`Column A`, 'E.A.' , 'EA') From Sample s Where 'Column B' = 'Every'
does it need to be HTML or just something you can open up in a web browser? SSRS would be the simplest out of the box solution to display the results of a query and give the user a front end GUI that will let them pick parameters, etc.
Apologies, I've managed to get back into work and it's not a network drive. Just a drive on the server where the SQL data is stored so unfortunately this doesn't help, appreciate your help so far. 
Where is your "s" in Sample s coming from? Meaning, what is it meant to represent. I am trying to find the correlation to my actual table but I am having trouble making the connection.
ignore it... that `FROM` nonsense is not valid MySQL syntax (note to /u/Cal1gula -- MSSQL is not ubiquitous) try this -- UPDATE Sample SET `Column A` = replace(`Column A`, 'E.A.' , 'EA') WHERE `Column B` = 'Every' 
It's coming there... because I typed it there to make it easier to read. :) It's called an "alias".
I got another option give to me as well. If you have time, i would love to hear when to use one versus the other. Update 'sample' SET 'Column A' = 'EA' WHERE 'Column A' = 'E.A' AND 'Column B' = 'Every';
Whoops wasn't focused so much on the SQL version. That syntax garbage with the backticks should have given it away.
Just looked it up and it makes sense. Still have to learn how to use it properly but appreciate the insight.
ditto. My biggest struggle is not with the workload, but in convincing DM to give access so I can automate this nonsense instead of manually pulling data weekly via 3rd party vendor apps' 'user friendly' reporting tools...
/u/r3pr0b8 gave you a better query. Part of the syntax I used will only work with the MS SQL flavor. But still, get used to aliasing it's a pretty key concept.
Got it. So this is the spreadsheet but you said you changed the format in the table? Or is it the same? You basically need to normalize the data and get some solid columns. This format will be wonky for sure. ``` [item_name][date_accessed] [date_accessed] ``` You should change it to ``` [item_name][date_accessed] [number_of_times_accessed] ``` So your data would look like so: ``` item 1 | 1/1/17 | 0 ``` ``` item 2 | 1/1/17 | 2 ``` ``` item 2 | 1/1/17 | 4 ``` If I were doing this, I would probably spend more time massaging the data in the spreadsheet before importing into the database. Writing a query against this would be very time consuming, how many columns would you have to check, ie dates? 
What is the exact query you are running for the restore?
the REPLACE solution will actually replace 'E.A.' with 'EA' even if the 'E.A.' occurs inside some larger string this solution works only on those columns where the value is exactly 'E.A.'
Ongoing ETL automation is mostly handled by the development team here, and I have almost full access to the resulting data warehouse. Whenever I have permissions issues I just let my boss know and he goes to fight that battle for me. :D The majority of my work lately has been RDL design, so I haven't been looking at automating Excel exports, other than finished products (like a subscription on the reporting server that drops a workbook in a share folder). We do have some recurring tasks that could potentially see some improvements with that though, I haven't had a reason to pick those apart again recently. There is some pressure to handle some of the processing manually just to ensure there is always a set of eyes reviewing the data before the final product leaves the department and enters the deadly wilds of the user base.
I transposed the table so Workbench would allow me to import. The way it was originally formatted was items across the top as columns so there were 1200 columns which exceeded the max limit. Yeah, that's what I'm thinking now too but I'm hoping that there is some kind of workaround where I won't have to reformat this mountain of data. The dates span for 3 months, so roughly 90 dates. But there are 1000+ items. If I was to reformat the data, would you happen to know an easier time going about that?
&gt; There is some pressure to handle some of the processing manually just to ensure there is always a set of eyes reviewing the data before the final product leaves the department and enters the deadly wilds of the user base. sigh.
says the guy whose database uses square brackets instead of backticks ;o)
I'm curious what your raw data looks like. Importing and formatting is probably a whole other question. I know I had problems importing large datasets but I was able to break it up using a few batch tools. I think you should be able to transpose the data [Check out this link](https://support.office.com/en-us/article/Transpose-rotate-data-from-rows-to-columns-or-vice-versa-3419f2e3-beab-4318-aae5-d0f862209744) 
Yeah, I transposed it already. Thanks My raw data looks exactly like that table x1000 columns lols. I think my best bet is to do some Excel googling on how to change the layout of my data then?
Yeah, well transpose means to convert your columns (all those dates across the top) to rows. Then you can have a column for all the dates and write a query against that column. ``` select [item], [date], [accessed] from datatable ``` You could probably add a group by 
But for that, I'd still have to move[accessed] to it's own column, right? And if I was to massage the data to the format you suggested above, for each item, I would have to repeat the [dateAccessed]. So for each item, I'll have to repeat 91 days each time? Also, I'm so sorry for my noobiness lol, I'm just really trash at db/queries
does your homework specifically ask for a loop? because, you know, this is SQL, and we don't do loops have you ever heard of a **numbers table**? a single query can produce your output easily
Unfortunately, we have to use a while loop. The numbers table does sound interesting though I will look into it, just for the sake of curiosity. 
Nesting a replace, while messy, is almost always going to be light years faster than a loop. That's just how SQL works compared to iterative languages. A CTE is very much like a temp table, except you don't have the benefit of things like indexes and there are a few other limitations. Don't be afraid of temp tables. Either a CTE or temp tables should work for you just fine though, especially since you are doing a single migration and probably don't have to worry too much about performance. Heck you could simply dump the data into a table and run your replace statement a bunch of times until there's no data left to parse. This seems dirty, and it is dirty, but also very quick and relatively painless. I would be more concerned about the consistency of the text you are parsing. Hopefully it is simple so that a find/replace is robust enough.
I'm learning Python on the fly to take over the companies ETL script writing. Currently a database developer so this will be something new and exciting to learn as it seems like the possibilities are endless. 
Not exactly. Your issue seems to be with the data no being normalized. Once that is done, then the query statement would be relatively simple. At my day job, this would fall under [ETL (Extract, Transform and Load)](http://datawarehouse4u.info/ETL-process.html) I'm not sure what you mean by 91 days each time. You might need to get back to your db class. Have you put this data into the database itself? DM me if you want some more help. don't think this back and forth is making much progress unfortunately. And don't worry, we were all noobs at some point. 
I think i answered my own question. You should Stage the BULK data to an empty staging table before merging it into the main reporting table. This reaps the benefits of quick insertions to the Stage, while preserving the clustered indexes and constraints of the main reporting table.
Totally useless comment here but here's how you could do the same idea with python. Maybe it'll help, maybe not. I think though the underlying idea has to be the same. inside, outside = 12, 12 x, y = 0, 0 while x &lt;= outside: while y &lt;= inside: print("{} * {} = {}".format(x, y, x*y)) y += 1 y = x x += 1
Thank you for that detailed reply. After trying a bunch of approaches, the one I think that will work decent for this is CROSS APPLY, each with its own REPLACE() so that it is more readable. Thank you again.
When you increment Len you need to set height back to 1. Step through the code in your head. After iteration 7 height is set to 8. Then Len is incremented. Then it checks if height 8 is less than iterations 7 which it isn’t. 
How is your MSCA studying going and which one are you taking? My 70-761 is on 12/8.
I'm doing the dba route, studying for the 70-764 administration test. I've been using the Microsoft virtual training, it's been kinda meh, but the labs are nice. I've heard the msca tests are pretty hard, so I'm trying to utilize my non existent study habits haha.
Can you just add a shipped (bit) column to your second table? Is the invoicing happening before shipping? You could have a third table for shipment IDs that reference both the invoice and the order.
The table that has **order_id** and **invoiced** it is the main one. The order id generated here will be carried by the orders in a particular order. Its not so relevant when the invoicing happens (to to my question here), that will basically just be an enum with 0 or 1 whether the invoice is generated or not. The problem I have is how to store the data is the least complex way possible. Lets say I have 3 warehouses. I know how the data will be assign to the warehouse like so: | order_id | warehouse_id | product_id | quantity | changed | changed_quantity | note | So does this mean that I need a parent table? Something lie this maybe: | order_id | warehouse_id | confirmed | shipped | Which is a child of: | order_id | invoiced | So to fetch the items for order XX for warehouse 3 lets say I would need to get the **order_id**, match that with **order_id AND warehouse_id** and finally match that with the products assigned to the order/warehouse. Is this an efficient way of doing it? 
Heres a visual aid: http://tinypic.com/view.php?pic=23k2e7p&amp;s=9#.WhOUP7aB0nU
Need a whole lot more info. How is the row being written to the table?
you would need to use dynamic SQL for a dynamic select clause like that
Are you referring to existing data or do you want the default value of "ticket price" to be set based on your criteria when the row is inserted? A trigger will only work on newly inserted or modified data. If the data is already in a table, then two simple update statements will suffice.
Yea I want the default value of ticket price to be set based on conditions when the row is inserted.. Like when the user enters his information, based on the condition, I want his ticket price to be automatically generated.. But I don't understand how
I'm a bit rusty on Oracle PL\SQL syntax for triggers, but there's a lot of resources out there per Google. https://docs.oracle.com/cd/B19306_01/server.102/b14200/statements_7004.htm https://docs.oracle.com/cd/B28359_01/appdev.111/b28370/create_trigger.htm#LNPLS01374 You'll probably want an "instead of" trigger for "insert" only. Something like: CREATE OR REPLACE TRIGGER yourSchema.triggerSetTicketPrice INSTEAD OF INSERT ON yourSchema.yourTable BEGIN IF :NEW.Age &gt; 25 THEN INSERT INTO yourSchema.yourTable( LastName, FirstName, Age, TicketPrice) VALUES( :NEW.LastName, :NEW.FirstName, :New.Age, 100); ELSE INSERT INTO yourSchema.yourTable( LastName, FirstName, Age, TicketPrice) VALUES( :NEW.LastName, :NEW.FirstName, :New.Age, 50); END IF; END; Note that I'm making A LOT of assumptions here and this is very arbitrary per the information you've provided. You may want to set this up in an sqlfiddle.com to test it.
 USE [master] RESTORE DATABASE [database] FROM DISK = N'S:\databasescheduletest.bak' WITH FILE = 1, MOVE N'database2000_Data' TO N'S:\database.mdf', MOVE N'database2000_Log' TO N'L:\database1_log.ldf', MOVE N'sysft_CVText' TO N'S:\CVText', MOVE N'sysft_ApplicantAbout' TO N'S:\ApplicantAbout', MOVE N'sysft_JobDescriptions' TO N'S:\JobDescriptions0000', MOVE N'sysft_Comments' TO N'S:\Comments0000', MOVE N'sysft_UnRegApplicants' TO N'S:\UnRegApplicants', MOVE N'sysft_ApplicantFileText' TO N'S:\ApplicantFileText', NOUNLOAD, REPLACE, STATS = 5 GO 
Use your code above to generate the entire SQL statement - then execute that in a separate step. Not sure if Redshift/Postgres offers dynamic SQL via SProcs - if so the that might be a route to 'build code and run it' in one sproc call.
I’d say first: you need the UN/PW for the application to use in the DB. This means creating a user/pw on the DB specifically for the page, with limited permissions. Please don’t give your application sysadmin or the like, it’s bad practice. Next create a DSN (assuming your app is on a windows machine. Go to start and type (data sources) and it will come up in the search results. Add a 32 and a 64 but version if you’re not sure which one you have. Make sure it’s a system DSN. Make sure you use the UN/PW you made specifically for the application. Next, hop over to connectionStrings.com and find the appropriate connection string for your DB. Next, use the connection string for DSNs and you don’t even have to enter the UN/PW in the web config. It just lives in the DSN. Now, you should be able to open a SQL connection from the app, using the DB connection string you specified in the config.
UPDATE Stuk SET stuknrOrigineel = NULL WHERE stuknrOrigineel in (SELECT stuknr FROM Stuk WHERE genrenaam = 'klassiek')
Finally relieved from this headache! It works like a charm, thanks a lot!
I think I have a (more) correct connectionString: &gt;connectionString="Data Source=(LocalDB)\MSSQLLocalDB;AttachDbFilename=|DataDirector‌​y|\kroger-webpage.mdf‌​;Integrated Security=True" but for some reason I keep getting the error &gt;System.ArgumentException: 'Invalid value for key 'attachdbfilename'.' Do you know why this is happening?
I'm a DBA, not a dev, but I've never seen a connection string reference the data file of a database. Are you using SQL Server? Is it Local to your machine? Is the service running? If all 3 of those are yes, then I'd start with the connection string. Usually has a server name, authentication type, DB name, and a few other options as needed in your environment.
I have heard they are difficult as well. :(
Can you post the error? Are you sure that both servers are using the same sa password?
&gt; The column can not exceed 255 characters and right now I have some that are upwards of 700 So we know your limit is 255. I would do a try catch, for the set that can fit into the column of 255 they would be in the try portion. For the set that is over 255, they would end in the varchar max column. (Or temp tables or table variables, etc.) &gt; From what I can tell 255 characters is usually 4 to 5 " : " delimiters So you essentially need SQL to count the number of " : " that occur in a row, then return the position of the value. Example: "I:am:a:test:value" There are four " : " values. One is at position 2, position 5, position 7, and position 12. Once you have found the fourth " : ", you would want SQL to return the position. Then you could tell SQL, let's return that many characters from the left side. So if I returned 12 characters, my result would be: "I:am:a:test:" That logic should help you in the general direction. You will probably need to google around, read a few MSDN articles on functions, and piece it together.
Hey! Thanks for your reply. I am having trouble figuring out the position piece. I have looked around for queries that will do this. Most seem to show a long CHARINDEX type syntax. Could you just confirm that it would be something to this nature: LEFT(@text, CHARINDEX(' : ', @text, CHARINDEX(' : ', @text)+1)+1 -- etc etc 
Does MS SQL have regular expression support?
Yup, you're on the right track. I've had to do things similar before, I'd recommend to create a function. Here's a function I've used before that should help with your task at hand. http://www.sqlservercentral.com/scripts/Miscellaneous/30497/
If I'm wrong, someone please correct me. I'm pretty sure T-SQL does not incorporate REGEX support. I believe you can incorporate it with Python or Powershell, but the T-SQL language doesn't have that.
Actually for TSQL jobs you can run as a different user it is just a little different/ in a weird spot. For a TSQL step it is in the ADVANCED page and believe the "run as" list is kind of confusing because it is the database specific SQL users that are options so it might not match your sql login list and might have to add the sysadmin login as a database user as well.
 &gt; TITLE: Microsoft SQL Server Management Studio ------------------------------ The linked server has been updated but failed a connection test. Do you want to edit the linked server properties? ------------------------------ ADDITIONAL INFORMATION: An exception occurred while executing a Transact-SQL statement or batch. (Microsoft.SqlServer.ConnectionInfo) ------------------------------ Login failed for user '######\\#######'. (Microsoft SQL Server, Error: 18456) For help, click: http://go.microsoft.com/fwlink?ProdName=Microsoft%20SQL%20Server&amp;ProdVer=12.00.5000&amp;EvtSrc=MSSQLServer&amp;EvtID=18456&amp;LinkId=20476 ------------------------------ BUTTONS: YES/NO To be clear, access works just fine when I select "Be made using the login's current security context". The username/password are *not* incorrect because they are the same ones I use to login to the RDP session (it is a cut/paste of the password). 
All I hear about, as a sql noob, are the limitations of MS SQL, never the strengths . . .
DELETE a FROM TableA a JOIN TableB b ON... WHERE... That is the SQL Server Syntax. Not sure if it works in MySQL.
MS SQL and T-SQL are very different things. T-SQL is the language MS SQL utilizes. MS SQL is also compatible and built in with R and Python. Oracle is the only RDBMS with the primary limitation being $$$. I would say MS SQL is the next runner up at this point in the game, but it depends on what you want to do. It's always about the right tool for the job though. 
is there a constraint on the column?
without the aliases, sure thing with aliases? i've never needed to try it...
https://dev.mysql.com/doc/refman/5.7/en/delete.html
How do I actually add someone to that list? The users there are very odd (in fact, a user I deleted a year ago is showing up in there)
Kind-of: from 2016 MS SQL supports R natively and you can use regex within R. 
yes
In MYSQL I would just do DELETE FROM *table* WHERE EXISTS ( *select statement in OPs post*) https://dev.mysql.com/doc/refman/5.5/en/exists-and-not-exists-subqueries.html 
I should add that if your select statement is pulling lots of columns from different tables you may have to nest it in another select so it only returns a unique column that is in your source table. Eg: DELETE FROM *table* WHERE *id* (SELECT *x.id* FROM (SELECT *subquery etc*) x ) Hope that makes sense :)
I understand where you are coming from -- I've only installed SQL once or twice before and both times, the configurations that you are asked to set seem very foreign. I would suggest doing the research to understand the installation of SQL given the goals you have outlined, as SQLite's scope is a single local user, which contrasts with your desired distributed and inherently multi-user approach. Perhaps you can leverage a cloud service like Amazon Web Services in hosting your SQL back end?
In testing, on my systems it seems to be linked to the master database users, so Databases -&gt; System Databases -&gt; Master -&gt; Security -&gt; Users, and the users on there are what will show up as options to run and you can add/ map users to Master to have them as an option. 
https://www.codeproject.com/KB/database/Visual_SQL_Joins/Visual_SQL_JOINS_orig.jpg
No. It has PatIndex and CharIndex. 
Any chance you’re running SQL Server 2016+? If so, STRING_SPLIT() would be useful. Otherwise, CharIndex and PatIndex is what I’d do in this situation.
I'm hoping the MySQL syntax isn't too weird, but I'd do this (In MS SQL) DELETE FROM TableA WHERE ID IN ( Select TableA.ID from TableA Join TableB On ... Join TableC On ... Where ...; )
I thought so too, but does it require a restart? The users aren't updating. Regardless, thanks - I really appreciate this!
there's only one difference -- in the second snippet, you're assigning the PK constraint name, whereas in the first, the system assigns its own PK constraint name
Hey, Thanks for responding! If I do the statement as I did in the 2nd snippet I can remove the constraint thus: alter table klant drop constraint ak_telefoon; But how do I drop the unique if I write it as I did in the 1st snippet? Cheers, 
1. https://dev.mysql.com/doc/refman/5.7/en/drop-index.html -- To drop a primary key, the index name is always `PRIMARY`, which must be specified as a quoted identifier because `PRIMARY` is a reserved word: DROP INDEX `PRIMARY` ON t; note this is non-standard SQL 2. https://dev.mysql.com/doc/refman/5.7/en/alter-table.html -- ALTER TABLE t DROP PRIMARY KEY
Hey, Thanks for responding. I did not mean dropping the primary key. I want to drop the alternate key, the unique in the first snippet. If I do, alter table klant drop unique (telefoon); That does not work. While this in the 2nd snippet: alter table klant drop constraint ak_telefoon; Does work. Why doesnt the first work? Cheers, 
Not natively. However, you can easily create a CLR that supports it.
It doesn't work because you can't "drop" a "unique". What are you trying to do? Alter the key or remove it completely? It's easy enough to drop and recreate the key but "drop unique" simply isn't valid.
You probably are in college and spend too much time around hipsters. No /s Everyone likes to hate on Microsoft just because they are Microsoft.
Nah I'm just talking about this subreddit. Everyone uses MS and they have to do crazy workarounds like some xml thing because there's no string agh function.
The newest version of SQL Server does have a STRING_AGG function.
&gt; I did not mean dropping the primary key. I want to drop the alternate key, the unique in the first snippet. by the way, there is no unique key in your first snippet
Sry my mistake. If I have: create table customer (customernr integer not null, customername varchar(35) not null, customerphonenumber char(25) not null, primary key (customernr), unique (customerphonenumber) ); How do I delete the unique now that I have given an id to the unique? If had: constraint ak_customer unique (customerphonenumber); I could just do: drop constraint ak_customer; Cheers, 
The constraint is going to have a name assigned by the database engine. You'll need to figure out what name it was given to reference it in the `DROP` statement. It's generally considered good practice to explicitly name all of your constraints.
Ok. Thank you!
Just bumping cause I could really use some help on this.
 SELECT itemname, price FROM item INNER JOIN price on price.itemid = item.itemid WHERE itemid in (SELECT itemid FROM price GROUP BY itemid HAVING count (*) &gt; 1) 
thanks for your help but that doesn't seem to work.
I'm all for doing people's homework for them but surely you can manage replacing a few field names.
 SELECT name, price FROM item INNER JOIN price on price.itemnumber = item.itemnumber GROUP BY name, price HAVING count (*) &gt; 1 You could do this also, instead of a nested query.
I did but it doesn't work Here is the error Ambiguous field name between table ITEM and table PRICE . ITEMNUMBER I appreciate your help
I missed an alias: SELECT name, price FROM item INNER JOIN price on price.itemnumber = item.itemnumber WHERE item.itemnumber in (SELECT itemnumber FROM price GROUP BY itemnumber HAVING count (*) &gt; 1)
Thank you very much. That worked perfect. I am new to sub queries and I greatly appreciate your time and help. Have a Happy Thanksgiving. 
This is super confusing to put into English. Null is nothing but you haven't passed a nothing to the function. You've just called it with no parameter which is different from a parameter of nothing. Try DECLARE @shippeddate date = null exec shipping_date_sp @shippeddate
looks like you highlighted the proc but not the date variable next to it. unless you assign a default value all parameters need a value. 
Ok, I understand now that just because the parameter is not supplied, doesn't mean it is a null. With assigning the default as null, I still am not able to get the print statement though. CREATE PROCEDURE shipping_date_sp @shippedDate DATE NULL AS BEGIN IF @shippedDate IS NULL BEGIN PRINT 'Please enter a valid ship date' RETURN END SELECT o.OrderID, c.CompanyName, c.Phone, CONVERT(VARCHAR(20), o.OrderDate, 107) as OrderDate, CONVERT(VARCHAR(20), o.RequiredDate, 107) as RequiredDate, CONVERT(VARCHAR(20), o.ShippedDate, 107) as ShippedDate FROM Orders as o JOIN Customers as c on c.CustomerID = o.CustomerID WHERE o.ShippedDate = @shippedDate ORDER BY o.OrderDate END
It works fine for me even without having the right tables in place. Note you still need to hand the SP a null parameter, even though you've allowed it to be null. What does DECLARE @shippeddate date = null EXEC shipping_date_sp @shippeddate return for you?
&gt; DECLARE @shippeddate date = null &gt; EXEC shipping_date_sp @shippeddate Ah thanks-It does indeed return the error message. I altered my code to hand the default date to the procedure each time so I don't have to declare the variable each time - see below. Thanks for the help CREATE PROCEDURE shipping_date_sp @shippedDate DATE = '2017-09-09' AS BEGIN SELECT o.OrderID, c.CompanyName, c.Phone, CONVERT(VARCHAR(20), o.OrderDate, 107) as OrderDate, CONVERT(VARCHAR(20), o.RequiredDate, 107) as RequiredDate, CONVERT(VARCHAR(20), o.ShippedDate, 107) as ShippedDate FROM Orders as o JOIN Customers as c on c.CustomerID = o.CustomerID WHERE o.ShippedDate = @shippedDate ORDER BY o.OrderDate END IF @shippedDate = '2017-09-09' BEGIN PRINT 'Please enter a valid ship date' RETURN END
You would still need to hand it a parameter.
Just my 2 cents, I would let the application to handle the validation, instead of a procedure. 
Here you go: http://sqlfiddle.com/#!9/deca84/4/0
Select distinct regionname, count(c.id) From x Where y Having count(c.id) &gt;= 3 I’m on mobile but does this work?
No, The ID numbers are 9 digits so I think that individually they're all &gt;3. 
But I’m saying if the id appears 3 or more times.
Select regionname, count(distinct id) From x Where y Group by regionname Having count(distinct id) &gt; 2 --(you said 3 or more)
Oh I missed the "having"! So this correctly narrows it down and gives me 125 rows, any idea what I would keep in the select part to just return a count of those regions? 
Remove Distinct, add 'GROUP BY regionname' and then have only COUNT(id) in the SELECT statement. 
Now it just gives an unnamed column with the number of members grouped by region. Still gives me the correct 125 rows but I want it to just return 125 not 125 rows... 
I appreciate the help! This does remove the regions with less than 3 members but it still lists each region. I basically want to just get a count of rows that this query will return... Any ideas?
If you just want the number of rows returned from your query, could you just nest it in a subquery and do a count(*) ? Select count(*) from ( Your Query )RegionCount
Then just remove the regionname from your select. 
Oh, I see. Nest it as a sub query. SELECT COUNT(*) FROM ( SELECT regionname, COUNT (*) FROM tbl GROUP BY regionname Having count (*) &gt;2 ) query
Yes. This was exactly what worked. Thanks a bunch for the help! 
Yup! Thanks for the help on this!
Oh, just the quantity of rows? Quick and dirty way would be to turn that into a cte, and then just select count(*) from that (on mobile but if you get stuck I'll try and remember to pop back tomorrow from my work machine)
Not quite. It's still grouped. See my reply and the other. They both suggest something similar. :)
Someone else pointed me towards a sub query which got the job done! Thanks for the help!
[removed]
No worries. Saw that after my post. Similar to doing it as a cte so should work in the same way. :)
What exactly do you mean by CTe?
Just reading through this quickly, have you misidentified your variable in your SELECT INTO? Maybe missing the underscore. 
With x_cte as ( Select regionname, count(*) as rec_count From x Group by regionname ) Select * From x_cte Where rec_count &gt;= 3 Sorry on mobile.... and a couple of drinks in. 
https://www.codeproject.com/Articles/265371/Common-Table-Expressions-CTE-in-SQL-SERVER
Thank you!! As I thought subqery with grouped count was required, just wasn't sure exactly in what order. 
Add AND productid = ? You want to rank just one product?
Hey, couple of thing I want to mention: 1) Your trigger for some reason fires on DELETE statement. If trigger execution is caused by DELETE statement, it won't have access to :NEW variable resulting in error. Looks like you didn't mean to have DELETE trigger, so remove DELETE from declaration. 2) Your trigger is on payment table. In the trigger you SELECT from payment table. If you ever execute multi-row UPDATE or INSERT, you'll get "Table mutating error". Use compound trigger to avoid this. 3) As for the actual question, are you sure the condition PAYMENT.Reservation_ID=:NEW.Reservation_ID is met on your test data? Because the error message is very straight forward: SELECT returned an empty result set. And please format your code next time.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/brasilonreddit] [\[r\/SQL\] This Brazilian company that awards developers for their "1st Update without a Where" certificate](https://www.reddit.com/r/BrasilOnReddit/comments/7f73ax/rsql_this_brazilian_company_that_awards/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
The only time I've ever done that was when I was undoing an incorrectly set new field (e.g. a new column defaulting to 0 not 1)
No, I only want to rank orders that contain a specific productid. The product IDs appear in multiple rows.
I mean are they just talking about doing a replace?
Nice. Replacing all the data in a column is definitely a right of passage as a DB dev.
Any developers, or only their own? If it's the former, where do I submit an application for two of those rewards?
Not quite sure what you are requesting but maybe this will help? SELECT productid, COUNT(order_id) AS Count_products FROM schema.order_items_regionalized WHERE discounted_item_price_in_gbp &gt; 1 GROUP BY productid Its not a window function but you will get a count for every order which contains the productid if you just want one product you can just add AND productid = your_product Is this what you wanted?
If only we could all be so perfect! 
I wasn't sure if this was an award for "congratulations, you've changed ALLLLL the data in this table and that's a big deal" or "congratulations, you've used a table join instead of where which is more efficient" update t1 from myTable1 t1 inner join myListOfUpdateKeysTable2 t2 on T1.key = t2.key set MyColumn = t2.NewMyColumnVal (your ansi may vary) I'm assuming it's the former... 
When you have leveled up your stackoverflow-fu. 
I have a bad habit of using “1=1” to replace real where conditions while I’m debugging code....yeah that’s blown up before. 
And for the love of God, use a transaction where you can. 
I should probably elaborate. I was saying that I'm not super experienced with SQL, so the only time I've done this is when I'm pulling a hack job on something. I'm sure if the award was for the same thing, or if it's for people who accidentally forgot a where clause and broke some data. I initially presumed it was for some complex thing where they're able to get the desired results without where's
[sql-ex.ru](sql-ex.ru) is a set of 141 excesses. The interface certainly is dated, but I fondly remember banging my head against some of the problems there. There's a [textbook](http://www.sql-tutorial.ru/en/content.html) associated with it. Remember, no amount of reading will make you good at the language. Practice makes it perfect.
I thought at first they were awarding the plaque to someone who managed to eliminate a costly where clause in an update. I think I get it now.
Brent Ozar is selling many of his classes for a discount right now. The Database Tuning class is only $99 all day Friday https://www.brentozar.com/product/enterprise-everything/
Well then.. there's your answer
You can't have a deep understanding cl from school alone unfortunately
for starters, you aren't joining your tables correctly you've joined Employee to OrderT and you've joined Inventory to OrderItem from that point on, you're going to get cross join effects do you understand this? 
 terrible once you take into consideration a million shitty sites and apps are using mysql or something like it. Just running simple basic queries, not actual database development. 
I took a sql test for a job at facebook, it was mysql and was a joke. Then again i am a database dev
Select * from database.schema.table
Thanks for the response, i'm sorry i've only a half semester of SQL under my belt. Could you got into more detail when you say I joined tables correctly? I felt just combining and linking them together for the query is what I'm suppose to do. So I was approaching the request of the question incorrectly? 
When you immediately can look at a query and know how to optimize it
first of all, my condolonces if you are required to use Acess for this course your approach is correct, it's just incomplete could you take a screenshot please, of the table relationships? if not, please explain what the OrderT table is for 
Does your teacher understand the purpose of sql?
Parameter sniffing 
Hahah Access isnt the best but I think its a decent place to start. [Here are the tables minus Customers Which I think is not needed] (https://imgur.com/a/yABDt)
not really
Hey, well for your 3rd point, the condition is met but its weird.. Like when I insert some data inside the payment table before starting the trigger, and then afterwards when I insert another row, it works but it only works for one ID.. Not the rest.. even tho the table has more than just one ID
okay step 1, run this query by itself to find out the average list price of all items -- SELECT Avg(ListPrice) FROM Inventory then run this, and verify that it's pulling the right items -- SELECT Employee.EID , Employee.Name , Employee.Zip , OrderT.OrderID , OrderItem.ItemID , OrderItem.SalePrice FROM ( Employee INNER JOIN OrderT ON OrderT.EmployeeID = Employee.EID ) INNER JOIN OrderItem ON OrderItem.OrderID = OrderT.OrderID WHERE OrderItem.SalePrice &gt; ( SELECT Avg(ListPrice) FROM Inventory ) ORDER BY Employee.EID , Employee.Name , Employee.Zip , OrderT.OrderID , OrderItem.SalePrice DESC when that checks out, modify the query slightly to produce the required results -- SELECT DISTINCT Employee.EID , Employee.Name , Employee.Zip FROM ( Employee INNER JOIN OrderT ON OrderT.EmployeeID = Employee.EID ) INNER JOIN OrderItem ON OrderItem.OrderID = OrderT.OrderID WHERE OrderItem.SalePrice &gt; ( SELECT Avg(ListPrice) FROM Inventory ) GROUP BY Employee.EID , Employee.Name , Employee.Zip; 
&gt;it works but it only works for one ID.. Not the rest.. even tho the table has more than just one ID Sorry, I don't understand what you're trying to convey. So, apparently, the way you see it is: I insert some data in the table, the trigger fires, the trigger *should* see inserted data in the table, but it doesn't. I hope I correctly understand your perspective. Now, the trigger can't see data you're inserting since the data is not yet committed. Reading uncommitted data is prohibited (please google "dirty reads" for details). That's why SELECT doesn't return anything. It doesn't matter whether a trigger fires BEFORE or AFTER, it's still a dirty read. If we still don't understand each other, I threw together a [testing environment](https://pastebin.com/LHY5h23S). Please see if I'm missing something (the trigger only has SELECT in it because that's what we're trying to fix). Maybe the INSERT example doesn't reflect what you're trying to do. To run the code I suggest you use LiveSQL in order to not pollute your server. 
Ok, so if i'm understanding correctly, the first query is just a a pull of the average list price. the second query shows me who is above the average list price (only one employee) And the final query Brings it all together. So i was missing the INNER JOIN and i didnt properly use the Subquery but more I had the concept
&gt; Ok, so if i'm understanding correctly, the first query is just a a pull of the average list price. yes, but this is just so that you could see what it was, and use that knowledge to verify that the second query is working properly &gt; the second query shows me who is above the average list price (only one employee) no, all employees who had an order which contained any item over the average price all employees, all items &gt; And the final query Brings it all together. just aggreagets all orders over each employee &gt; So i was missing the INNER JOIN and i didnt properly use the Subquery but more I had the concept well, sort of you were missing the join between OrderT and OrderItem!! &gt; So I needed to use the Inner join instead of the natural join no, you were using implicit joins, not natural joins natural joins are a completely different animal implicit joins have their join conditions in the WHERE clause explicit joins use JOIN syntax 
Trust me I am just as confused as you are. To be fair this is the first kind of problem she's given us like this, everything else has been the normal inserts and selects and cursors etc. 
Well, I need to brush up on just about everything lol! 
The certificate that everyone who works with databases either has earned, or will earn.
I don't know - maybe read something on entities/attributes/relationships? https://info.teradata.com/htmlpubs/DB_TTU_16_00/index.html#page/Database_Management/B035-1094-160K/xuq1472240596754.html 
I did that on a financial company's transaction table once. Added a trigger that would return a salty message if the whole table was ever being updated. Got that message once about 2 years later...
I am all for efficiency but really if the query takes only a few seconds to run then I don't give a shit. 
Got it begin tran delete table1 where 1=1 commit Am I doing this right?
Its a rite of passage
Don’t forget: Drop table &lt;important fact table&gt; And while you’re at it go ahead and drop some important users from important database roles. 
This sounds more like a general tech support issue than one related to SQL. If I understand you correctly, you're working from home using software that needs to connect to a SQL server on your work network? Most likely the issue is something to do with your network setup or how you're connecting to your work network, or just maybe someone has taken the server offline? Really need more details to have an opinion but it doesn't really sound like something /r/sql is particularly well suited to help you with.
What is VSTATUS? Your variable is V_STATUS
If you are a complete beginner you can go on Udemy.com and pick up some pretty good courses for $10 each. I think the sale is until Monday.
Nvm. Was dumb. 13,50 should be 13.50 Will leave up for other dumbs. Cheers, 
Nowadays, in an era of containerisation and cloud deployment, that kind of practice is going to become rare. People will be shipping their software together with their preferred database installation, all nicely packaged up in a Docker image and ready to deploy. It just reduces headaches for everyone involved.
Agree, my company is moving towards that approach. 3-4 years out is the timeline so for now we definitely need to support multiple systems, but not in the future
well, here's a hint/suggestion: omitting big parts of the theory behind it, a 'key' is something that's enough to figure out a single row in a table, could be one or more columns but all of them _together_ are the key. Out of the columns in your kindergarten_employee table, what would be just enough to identify a single employee? A foreign key from another table would be a 'reference' to a key in the 'master' table and should match the number of columns and their data types in the corresponding key. This should be enough to figure out (at the very least) why are you getting that error.
This is from my homework. I am not looking for exact answers, just possibly information on how to generate 5,000,000 rows of unique data. I believe my professor suggested creating lists of names, addresses, etc. and looping through them, but I am unsure how to do this or if this is possible/what she was saying.
Look into Oracle's collections and DBMS_RANDOM module. You're asked to generate 5kk rows. Than means you will need something like 100 first names, 100 second names, 100 cities, 50 states. That will yield you 50kk rows. Put them all into respective varrays, then loop through them generating random index for each array. Insert resulting data into table (use /*+ APPEND */ hint for faster inserts). Cheers.
Not sure what language you're using to script, but if you have lists of fifty of each of those parameters and then nest four for loops you should be able to make 5,000,000 rows. Just put the insert statement in the inside loop. I'm sure someone else will update this with more specific information, but I'd start by making my lists of unique names, cities, and states and then see if you can set up the loops. 
Firstly, thank you for the response. If you generate a random index for each array wouldn’t that potentially produce rows that are not unique? Also, wouldn’t 100 first names, 100 second names, 100 cities, and 50 states generate 50,000,000 rows? Last, what do you mean by +*APPEND* hunt for faster inserts?
I’m instructed to only use SQL and PL/SQL. I’m thinking of using 50 records in varrays, respectively to each required row (first name, last name...) looping through in 4 nested for loops.
Yes, that would be 50,000,000 rows, as I said. I suggested using more data then necessary to reduce risk of repetitions occurring. That said, if you're not required to return unique data each time your script is run, just set up four nested loops, no need for DBMS_RANDOM module. See [here](https://oracle-base.com/articles/misc/append-hint) on what /*+ APPEND */ hint does. 
It seems like you're asking "how do I use SQL?" 
I can do that on a normal sized database, I’m worried about the size of this one 
[removed]
The main issue will be the output and what you're using to "further process" the csv file, because excel can't handle more than ~1mil rows. If you're using Python or something it won't be much of an issue. 
Yes I’m using python. So I can just proceed like I would normally do except that is gonna take a shitload of time? Never operated on something this big and i’m still restoring it from the dump so I thought I would ask informations in the meanwhile 
Why loop when you could just cross join? Create four temp tables with 50 unique rows each for first name, last name, city, state and then SELECT * FROM FirstName, LastName, City, State
* start import of the db * get a cup of coffee * watch some youtube * read some blogs * import is finally done * write query * perform query * more coffee and surfing * export csv * get lunch * damn its still not done * work on side projects * finally done! * copy to portable HD * ....more coffee
I don't really have a problem with self promotion, but give people a little more in the main post than just "here's a link" - otherwise just make a link post. 
Now I'm kinda curious what approach is faster.
Tell your professor that doing this with 625 rows (5 unique example values for each of the 4 types) would have been sufficient to complete the assignment. Making you come up with 50 of each is tedious and adds nothing to the lesson or your education. You could've spent that time learning algorithms or best practices instead.
Then again, maybe she is trying to get you to understand how boring and "corporate" the Oracle ecosystem is, hah. Just like the data entry you'll have to do for this homework!
That’s Purdue for ya. You should see her lecture slides, they include more examples of code that won’t work than code that will. Literally just a slide with code on it. No explanations/pictures/highlighting nothing, just code because that’s all you need to see. Why learn how to do something when you can learn how not to do it? Aren’t you learning?
She actually did say this once about an assignment prior to this, lol.
What are you using the CSV for?
The exercise has its merits. Download a list of first and last names as well as city/state pairs, and then cross join them. Getting lists of items into SQL is something I do often.
The professor probably said "loop" when explaining the assignment, and will dock points of anyone writing a loop instead of cross join.
&gt; how boring and "corporate" the Oracle ecosystem can confirm. 
I think you are looking for the [CAST/CONVERT functionality](https://docs.microsoft.com/en-us/sql/t-sql/functions/cast-and-convert-transact-sql)
# [Link to Gist] (https://gist.github.com/ashish2199/8ad29d80f3195ce3166bee55b2624653) # Reddit has messed up the formatting, you can visit the above gist to see them in proper format.
This is pretty good for a quick reference guide. Thanks!! 
If this is a SQL class, then the goal of the lesson is most likely to use SQL to generate the data. I agree with your prof. 
The top 10 percent is incorrect, TOP (10) will give you top 10 rows, you need TOP (10) PERCENT to get the actual TOP percent https://technet.microsoft.com/en-us/library/ms187043(v=sql.105).aspx 
I tried to undo your terrible formatting using some regex. Part 1 of 2. ---- I would like to share my list of sql querries [Reddit has messed up the formatting, you can visit the above gist to see them in proper format. Link to Gist] (https://gist.github.com/ashish2199/8ad29d80f3195ce3166bee55b2624653) ***To create a Table*** CREATE TABLE TheNameOfYourTable ( ID INT NOT NULL IDENTITY(1,1), DateAdded DATETIME DEFAULT(getdate()) NOT NULL, Description VARCHAR(100) NULL, IsGood BIT DEFAULT(0) NOT NULL, TotalPrice MONEY NOT NULL, CategoryID int NOT NULL REFERENCES Categories(ID), PRIMARY KEY (ID) ); ***To create a copy of table( doesnt create constraints like primary key, not null , indexes ect)*** SELECT * INTO NewTable FROM OldTable Eg. SELECT * INTO clos_ext_bkup FROM clos_ext; ***To create a copy of table with its data (create and insert)*** SELECT expressions INTO new_table FROM tables [WHERE conditions]; SELECT employee_id AS contact_id, last_name, first_name INTO contacts FROM employees WHERE employee_id &lt; 1000; The format of new_table is determined by evaluating the expressions in the select list. The columns in new_table are created in the order specified by the select list. Each column in new_table has the same name, data type, nullability, and value as the corresponding expression in the select list. ***Inserting Data from another table ( only insert)*** INSERT INTO Table (col1, col2, col3) SELECT col1, col2, col3 FROM other_table WHERE sql = 'cool' INSERT INTO contacts (contact_id, last_name, first_name) SELECT employee_id, last_name, first_name FROM employees WHERE employee_id &lt;= 100; ***Inserting Multiple values*** INSERT INTO table1 (First, Last) VALUES ('Fred', 'Smith'), ('John', 'Smith'), ('Michael', 'Smith'), ('Robert', 'Smith'); ***To add a column*** ALTER TABLE table_name ADD column_1 column-definition,column_2 column-definition,column_n column_definition; alter table risk_user_approval_tree add lineusr nvarchar(100); ALTER TABLE table ADD columnname BIT CONSTRAINT Constraint_name DEFAULT 0 WITH VALUES ***To add a auto increment*** ALTER TABLE 'tableName' ADD 'NewColumn' INT IDENTITY(1,1); ***To add a column with computed value*** ALTER TABLE dbo.Products ADD RetailValue AS (QtyAvailable * UnitPrice * 1.5); ***To delete/drop a column*** ALTER TABLE table_name DROP COLUMN column_name; ***To drop a table*** DROP TABLE tablename; ***To modify a column*** ALTER TABLE table_name ALTER COLUMN column_name column_type; ***To update a row*** UPDATE clos_customer_master SET Prev = 'Reactivation' WHERE Prev = 'Reactivate'; ***To update a row from select clause*** UPDATE table SET Col1 = i.Col1, Col2 = i.Col2 FROM ( SELECT ID, Col1, Col2 FROM other_table) i WHERE i.ID = table.ID; The subquery results are substituted into the outer query. As we need table object in outer query, we need to make an alias of inner query. ***To add a primary key*** ALTER TABLE table_name ADD CONSTRAINT constraint_name PRIMARY KEY (column1, column2, ... column_n); ***To find the name of constraints*** SELECT * FROM INFORMATION_SCHEMA.TABLE_CONSTRAINTS WHERE TABLE_NAME = 'tablename' ***To find name of Primary key constraint*** SELECT name FROM sys.key_constraints WHERE type = 'PK' AND OBJECT_NAME(parent_object_id) = N'CLOS_ext'; ***Drop primary key*** ALTER TABLE table_name DROP CONSTRAINT constraint_name; ***To rename a column (alter command doesnt work here)*** sp_rename 'table_name.old_column_name', 'new_column_name', 'COLUMN'; sp_rename 'cl_ff_docm.WINAME', 'WI_NAME', 'COLUMN'; ***To rename a table*** sp_rename 'old_table_name', 'new_table_name'; ***To top 10% of records*** SELECT TOP(10) * FROM CLOS_EXT ***To find when a table was altered*** SELECT [name] , create_date, modify_date FROM sys.tables; ***To find which table contains a given column*** SELECT * FROM INFORMATION_SCHEMA.COLUMNS; SELECT OBJECT_SCHEMA_NAME (c.object_id) SchemaName, o.Name AS Table_Name, c.Name AS Field_Name, t.Name AS Data_Type, t.max_length AS Length_Size, t.precision AS Precision FROM sys.columns c INNER JOIN sys.objects o ON o.object_id = c.object_id LEFT JOIN sys.types t on t.user_type_id = c.user_type_id WHERE o.type = 'U' -- and o.Name = 'YourTableName' ORDER BY o.Name, c.Name ***To find which table has which constraint and on which column.*** Select * from INFORMATION_SCHEMA.KEY_COLUMN_USAGE; ***Selcting based on case*** SELECT CASE WHEN &lt;test&gt; THEN &lt;returnvalue&gt; WHEN &lt;othertest&gt; THEN &lt;returnthis&gt; ELSE &lt;returndefaultcase&gt; END AS &lt;newcolumnname&gt; FROM &lt;table&gt; Eg. SELECT ProductNumber, Name, "Price Range" = CASE WHEN ListPrice = 0 THEN 'Mfg item - not for resale' WHEN ListPrice &lt; 50 THEN 'Under $50' WHEN ListPrice &gt;= 50 and ListPrice &lt; 250 THEN 'Under $250' WHEN ListPrice &gt;= 250 and ListPrice &lt; 1000 THEN 'Under $1000' ELSE 'Over $1000' END FROM Production.Product ORDER BY ProductNumber ; ***Adding row numbers to the result*** //here we are creating SELECT ROW_NUMBER() OVER(ORDER BY name ASC) AS Row#, name, recovery_model_desc FROM sys.databases WHERE database_id &lt; 5; ***While Loop*** DECLARE @MaxCount INTEGER DECLARE @Count INTEGER DECLARE @Txt VARCHAR(MAX) SET @Count = 1 SET @Txt = '' SET @MaxCount = (SELECT MAX(RowID) FROM ConcatenationDemo) WHILE @Count&lt;=@MaxCount BEGIN IF @Txt!='' SET @Txt=@Txt+',' + (SELECT Txt FROM ConcatenationDemo WHERE RowID=@Count) ELSE SET @Txt=(SELECT Txt FROM ConcatenationDemo WHERE RowID=@Count) SET @Count += 1 END SELECT @Txt AS Txt DECLARE @i int SET @i = 0 WHILE (@i &lt; 10) BEGIN SET @i = @i + 1 PRINT @i IF (@i &gt;= 10) BREAK ELSE CONTINUE END 
***Try / Catch Statements*** BEGIN TRY -- try / catch requires SQLServer 2005 -- run your code here END TRY BEGIN CATCH PRINT 'Error Number: ' + str(error_number()) PRINT 'Line Number: ' + str(error_line()) PRINT error_message() -- handle error condition END CATCH ***To get date in DD/MM/YYYY format*** SELECT CONVERT(varchar, GETDATE(), 103); ***To get all foreign keys refrencing a given table*** EXEC sp_fkeys 'TableName' ***To get datatype, size of columns of a table*** EXEC sp_columns CLOS_EXT; ***To get empty string after concatenation of a string with NULL*** When ***SET CONCAT_NULL_YIELDS_NULL*** is ON, concatenating a null value with a string yields a NULL result. For example, SELECT 'abc' + NULL yields NULL. When SET CONCAT_NULL_YIELDS_NULL is OFF, concatenating a null value with a string yields the string itself (the null value is treated as an empty string). For example, SELECT 'abc' + NULL yields abc. ***To compile without executing*** SET NOEXEC ON; When SET NOEXEC is ON, SQL Server compiles each batch of Transact-SQL statements but does not execute them. ***Updating data from another table*** UPDATE table SET Col1 = i.Col1, Col2 = i.Col2 FROM ( SELECT ID, Col1, Col2 FROM other_table) i WHERE i.ID = table.ID ***Check if column exists in table*** IF EXISTS(SELECT 1 FROM sys.columns WHERE Name = N'columnName' AND Object_ID = Object_ID(N'schemaName.tableName')) BEGIN -- Column Exists END ***Converting Multi row data into a comma separated string*** DECLARE @Names VARCHAR(8000) SELECT @Names = COALESCE(@Names + ', ', '') + ISNULL(Name, 'N/A') FROM People ***Nvarchar*** allows storing of unicode data ***To remove duplicate rows*** select distinct * into t2 from t1; delete from t1; insert into t1 select * from t2; drop table t2; ***Check if the table exists*** IF (EXISTS ( SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'TheSchema' AND TABLE_NAME = 'TheTable') ) BEGIN --Do Stuff END ***Find tables with given column name*** select * from INFORMATION_SCHEMA.COLUMNS where COLUMN_NAME like '%clientid%' order by TABLE_NAME ***Find all user tables*** SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE='BASE TABLE' ***Allows explicit values to be inserted into the identity column of a table.*** SET IDENTITY_INSERT dbo.Tool ON The ***DBCC CHECKIDENT*** management command is used to reset identity counter. Example: DBCC CHECKIDENT ('[TestTable]', RESEED, 0); GO ***DECLARE and SET Varibales*** DECLARE @Mojo int SET @Mojo = 1 SELECT @Mojo = Column FROM Table WHERE id=1; ***Add a Foreign Key*** ALTER TABLE Products WITH CHECK ADD CONSTRAINT [FK_Prod_Man] FOREIGN KEY(ManufacturerID) REFERENCES Manufacturers (ID); ***Add a NULL Constraint*** ALTER TABLE TableName ALTER COLUMN ColumnName int NOT NULL; ***Set Default Value for Column*** ALTER TABLE TableName ADD CONSTRAINT DF_TableName_ColumnName DEFAULT 0 FOR ColumnName; ***Create an Index*** CREATE INDEX IX_Index_Name ON Table(Columns) ***Check Constraint*** ALTER TABLE TableName ADD CONSTRAINT CK_CheckName CHECK (ColumnValue &gt; 1) ***Single Line Comments*** SET @mojo = 1 --THIS IS A COMMENT ***Multi-Line Comments*** /* This is a comment that can span multiple lines */ ***User Defined Function*** CREATE FUNCTION dbo.DoStuff(@ID int) RETURNS int AS BEGIN DECLARE @result int IF @ID = 0 BEGIN RETURN 0 END SELECT @result = COUNT(*) FROM table WHERE ID = @ID RETURN @result END GO SELECT dbo.DoStuff(0); ***Pivot - To convert rows into columns*** SELECT Wi_name, Often, Sometimes, Never, NA FROM ( SELECT Wi_name, Past_due, 'Selected' T, '' F FROM NG_CA_MISCELLANEOUS_DETAILS ) P1 PIVOT ( MAX(T) for Past_due IN ([Often], [Sometimes], [Never],[NA]) ) P2 ORDER BY WI_NAME; ***WITH (NOLOCK)*** is the equivalent of using READ UNCOMMITED as a transaction isolation level. While it can prevent reads being deadlocked by other. ### ***Finding the last identity inserted into a table*** - ***@@IDENTITY*** returns the last identity value generated for any table in the current session, across all scopes. You need to be careful here, since it's across scopes. You could get a value from a trigger, instead of your current statement. - ***SCOPE_IDENTITY()*** returns the last identity value generated for any table in the current session and the current scope. Generally what you want to use. - ***IDENT_CURRENT('tableName')*** returns the last identity value generated for a specific table in any session and any scope. This lets you specify which table you want the value from, in case the two above aren't quite what you need (very rare). You could use this if you want to get the current IDENTITY value for a table that you have not inserted a record into. @@IDENTITY and SCOPE_IDENTITY will return the last identity value generated in any table in the current session. However, SCOPE_IDENTITY returns the value only within the current scope; @@IDENTITY is not limited to a specific scope. That is, if there was a second IDENTITY inserted based on a trigger after your insert, it would not be reflected in SCOPE_IDENTITY, only the insert you performed. IDENT_CURRENT is not limited by scope and session; it is limited to a specified table. IDENT_CURRENT returns the identity value generated for a specific table in any session and any scope. For more information, see IDENT_CURRENT. Identity doesn’t guarantee uniqueness. If you want that, make a PK or add a unique index. 
For ddmmyyyy strings you have to parse out the parts yourself. It's pretty straightforward since the date is fixed length. For formats like dd/mm/yyyy or dd-mm-yyyy you can use CONVERT: SELECT CONVERT(DATE,'15-11-2016',103)
Not only that, using `TOP` without `ORDER BY` can result in a different result set each time the query is executed.
Came here to say this. TOP is literally LIMIT unless you order by something
so you're saying that the column `created_date` is a string? otherwise why use the LEFT function on it? or maybe "created date" is actually "created datetime"? also, please note, you do not need the hotspot table in your query... ... and probably not in a LEFT JOIN, unless you're storing stuff in your history table for hotspots which don't exist
thank you for responding! created_date is a datetime field. i truncated it for readability in the dev stage only. good point re hotspot table. that is only there for later work so i've taken it out in the name of simplifying the example. i've started learning about OVER (PARTITION BY) but i don't think that's going to help..
It's probably that drop database line, that database does not exist so it cannot be dropped
Window functions is what you need.
hence the 'if exists' part. Or am I not using that correctly? Strangely this works: USE master; DROP DATABASE IF EXISTS database_name; CREATE DATABASE database_name; GO USE database_name; But why it works is beyond me. 
thanks for the tip, i'll look into that
Ok, so breaking this down, at a minimum you are going to need a field that increments each time you run this query and a new day has happened. So lets just play with things a bit and see where we go: SELECT , hi.id , created_date , hotspotID , hi.score FROM history hi WHERE hi.hotspotID IN(384,385) That is the framework we're looking at, and I'm replacing that weird LEFT() thing you were doing because I don't understand it, but I understand that it's the created date of the ID. You're going to need a table that looks like this at a minimum: | ID | Created_Date | HotspotID | Score | DayTot | DivBy | Avg | ModStamp | | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | Most of the calculations you're looking for are simple enough to do, but you need to make a decision here whether you're going to be storing each past row of data, or going to be updating the calculations each time you run the query. The key to all of those calculations will be something like this: , datediff(dd, Created_Date, ModStamp) AS 'DayTot' , GETDATE() AS 'ModStamp' So first you have to use the old ModStamp to come up with the total, and then you need to use the GETDATE() to update the ModStamp to reflect the last time you ran this query. So for example if you run it twice in a row, the ModStamp would update, but DayTot and the other metrics would not (unless your data is structured in a more complex way than you have indicated.) So you're going to need something like this: update x set x.daytot = y.daytot from table x inner join ( select , datediff(dd, Created_Date, ModStamp) AS 'DayTot' from #table ) y on y.id = x.id where y.DayTot &gt; x,DayTot --or just only put ID's in the #table that you want to update. The rest of the calculations you're after should be straight forward enough to derive using #tables or a CTE but I think the framework above will help that become more intuitive. edit: This feels like something you want to do as a stored procedure that uses #tables or something like it. I don't think you need a LOOP but that's not a bad way of thinking about it because basically you want to create an SPROC (or series of queries) that you store in #tables, or do as a part of a CTE, etc, and then at the end update your table with the differences. You could just create multiple queries and run them manually, or create one sproc, or hell you could create multiple sprocs and then run them all manually, or one sproc that kicks the other ones off... or you could even probably just solve this as a view that updates everyday when getdate() changes. Comes down to preference... I would just dump it into one sproc that does everything in chunks nestled like this: begin stuff --for example: figure out the ID's that need to be updated and put them in a #table end begin stuff --calculate the new average here and put the results in a #table for each id. end begin stuff --score things here by id and put the results in a #table end begin stuff --join the score and averages here in a subquery and update your main table with getdate() as modstamp end begin stuff end Shouldn't be too dfifficult. You'd want to make sure you have some error handling so you can't update the ModStamp until you've updated the other averages, etc. Make it re-executable so that your #tables drop if they exist, etc. -- Sometimes for other more advanced types of calculations it might be a good idea to have a column such as `isActive` which is bit, and instead of updating the averages and other records, you update `isActive = 0` and then insert all new records with a `1 AS 'Active'` which is cool because then you can track things like variances, etc. at the expense of generating more data. If you have 10million ID's you wouldn't want to do this because you'd get a new 10million rows every day, but you could see in this framework how you might track the variances as calculations in new columns, etc. Some other random things: select daytot + 1 as 'DayTot' , cast(DayTot + 1 as varchar) + '/' + cast(DivBy as varchar) AS 'note' It feels as if you're possibly looking to structure this a little bit beyond your initial thoughts and be able to do more advanced analytics with your data so feel free to ping me to discuss. All of the SQL is pretty simple.
Replace your count with sum when condition, then 1 else 0. Simplw
I'm afraid to say you shouldn't get your hopes up, for a couple of reasons. Firstly there's little work around for pure SQL anywhere close to entry level: most uses of SQL will have it as just one skill among many, whether it's data analysis, DBAing, ETL, visualisations etc. Roles that are purely SQL are fairly rare and almost always very high level roles in database programmeability. Can't say I've ever seen a short term, pure SQL role that would fit in with study. Secondly, in this age of the international gig economy you could well be competing against a much more experienced overseas dev who works for cheap.
thanks for replying so comprehensively, and i haven't digested all that you've written yet, but because i don't understand your reference to running the query daily, i see i should have said what i'm trying to achieve... this is an app to track improvement in things (hotspots). on monday, i might record my score for hotspot 284, then on another day I might record my score for hotspot 285. maybe the same day i will record my score for hotspot 284 as well. on many days i won't record any score at all. the aim is to create a graph of the scores over time. if i just wanted a timeline of scores for a single hotspot, that would be easy. but i will want to create a timeline of average scores for a given number of hotspots, which means returning a dataset of two columns, one being any date on which i recorded a score and the second being the average of scores for those hotspots on those days. i think you understood that, because in the planning stages I did consider whether to run a daily query to calculate averages, but rejected that. for one thing, i want the ability to choose on the fly which hotspotIDs to figure in the graph over time. could I ask, why did you say &gt;update the ModStamp to reflect the last time you ran this query since that is not a relevant piece of information ... 
A Business Intelligence (BI) role would be the closest but as Faux is saying you would also need business acumen, requirement gathering skills, front end report development skills etc. 
&gt;this is an app to track improvement in things (hotspots). on monday, i might record my score for hotspot 284, then on another day I might record my score for hotspot 285. maybe the same day i will record my score for hotspot 284 as well. on many days i won't record any score at all. My background is in programming, not SQL, but you are describing a LOOP. In reality it doesn't matter if the LOOP is run manually every few days, every day, every 10 days, or every 12 days, you are trying to a design a process where even if it is run twice in a row, your end results are as you expect them to be, yes? So in SQL there are LOOPS but here for this purpose I see no reason for you to leverage one, but ultimately you are talking about writing a process you can run multiple times, and when each time you do run it you have calculations (historic ones in this case) which will update and give you the information you're looking for, yes? At this point the conversation need only be architectural. You need to think about exactly what you want, and possibly what else you might be able to get, or lose, if you choose something like Architecture A over Architecture B (which might be more work.) Once you are firm with that then it really is just a matter of breaking things down very simply into sub-processes to draw your calculations based on the date difference between when a record was created and when you've last run the query. Now, for example, if you wish the average to be on a day, then you would need to first calculate the average per day, and then take the average of all days you've done this for. Sounds like a no brainer, but that may or may not have implications on the architectural design of the table and specifically what you're trying to track, etc. A simple way here to think of this argument is having (1) row of data per ID which has an average that updates versus having (*n*) rows of data for each time you run the query, and then being able to select the most recent one, or the history. A lot of this depends on the structure of the parent data and whether or not its useful to add complexity. &gt;the aim is to create a graph of the scores over time. if i just wanted a timeline of scores for a single hotspot, that would be easy. but i will want to create a timeline of average scores for a given number of hotspots I'm not sure I follow this. You're saying if you want to select 9 hot spots you want the averages (by day?) to reflect the fact you're only selecting 9, versus if you select 12, etc.? &gt;i want the ability to choose on the fly which hotspotIDs to figure in the graph over time. You could do this, it would be simpler I think than what I proposed. You would need to provide a little more detail I think in terms of how you're defining the "average" in this case because it might be less simple than just saying `AVG(Dimension)`, you might need to use a CTE to calculate the average per day for example and then average that. Averages may not be the best way to express the data if that is the way you're going to look at things and there are infinitely other ways you could look at the data (by hour of day, or day of week, etc.) -- and calculate all of those on the fly as well. This will largely depend on how large your data set is, and how long it will take for you to run the query each time you want the information -- which then re-introduces the question whether you run things daily and then create a permanent table to ping to more quickly get the answers you're after. &gt;since that is not a relevant piece of information ... How not? Or maybe I didn't fully understand you. If you have an ID and it was created on 2010-01-01, then if i were running a procedure at some kind of interval then each time the process ran I would want to look at the difference between modstamp (getdate) and created date in order to come up with the numerator for the average calculation. I don't think I understand enough about what you're trying to score or measure for your averages, and am more generally commenting on the structure of how I would go about accomplishing what I think you're after. It may be unnecessary, I don't really understand your data or what you're after just yet. 
Re-reading your post I think there is fundamentally missing from your description. How do you calculate DayTot? What does score represent in your output? If you have (1) ID that has two entries for two different days, and has two different scores, is DayTot = 2? Is score the most recent score divided by 2?
DayTot (day total) is to be formed by adding the most recent scores for each hotspotID. if i update the same ID twice on a particular day, the first of the two should be ignored. it's the scores just before midnight that count. the idea is to produce, for each day on which a score is generated, an average of all the scores (on that day) for the hotspotIDs named in the IN clause. if the ave is something divided by two it's because there are two hotspotIDs to add up and take an average of. in the first row of the desired output table there's only one hotspotID at the end of that day to take an average of. i guess more hotspotIDs would have made that clearer...
&gt;DayTot (day total) is to be formed by adding the most recent scores for each hotspotID. I don't understand this. Show me a sample dataset of how this is calculated. &gt;the idea is to produce, for each day on which a score is generated, an average of all the scores (on that day) for the hotspotIDs named in the IN clause. Ok, so you want to produce this based on the criteria of the IN(), which I understand. Does the parent data source structure its data in such a way where each day has a score, and no day is duplicated per hotspot? &gt;if the ave is something divided by two it's because there are two hotspotIDs to add up and take an average of. in the first row of the desired output table there's only one hotspotID at the end of that day to take an average of. i guess more hotspotIDs would have made that clearer... This is where I'm unclear what you're after. Are you saying that each ID has a score per day, and what you want is per day to come up with an average based on an IN(), where the numerator is the number of ID's?
(Search values anywhere in selected SQL Server database)(https://www.youtube.com/channel/UCKNeQ96WteazmrcsbZFW7Tg)
There's probably a more efficient way to do it but the first thing that comes to mind would be to insert both of the columns into a new table (or temp table) then combine them by doing: SELECT Column1 + Column2 FROM NewTable
That's actually really easy and I should have seen that before asking. Thanks. The table isn't too huge, so efficiency isn't too much of a problem.
Get MySQLWorkbench for the gui experience or just directly import it and wait for it to finish. If you have a spare computer, use that for the import since it may take quite a while.
This is going to come off harsh. But its for your own good. A restaurant POS is not the place to be learning software development. You're clearly missing some pretty vital skills for this to even be possible at the moment. &gt; Ideally I want to track everything, and will need to come up with a database for ordering information Do you even know what everything is? Cause there is a lot. * Inventory * Menu/Orders * Comps, Voids * Printing tickets for front of house and back of house to numerous printers * Splitting Tickets/Multi-payments for a single ticket * Roles/Permissions scheme * User management (which might include a time clock and payrates, tips, service charges, etc) * The back of house management tools for reports * Cash in/out and till audits * Sales projections * Tracking POs for vendors and Invoices from vendors * Integrating with a payment gateway (are you running batches at the end of the night or each time card is swiped) * Selling/Redeeming Giftcards * Loyalty Program This is a short list I just did from memory, I know I'm missing more. A POS is useless without at least half that right off the bat so its not just a slowly add more things to the system as we need it. Thats just the feature set. The more important part is that you cannot afford for your system to be down for even a couple hours. Missing a dinner service on a Saturday because things weren't working and they had to close would be really bad. What if that happens twice in a month. Thats a lot of lost revenue. Most small new restaurants take a long time to start recouping costs. It takes even longer when things are not running efficiently/correctly. Finally, 
Listen, I have about 20 years restaurant experience, and 20 years experience in software design... bout 4 years in analytics &amp; database development. I know what everything is. I'm not at all worried about the database, but ideally I'd like to find a POS system for a few grand that has native export functionality and not have to customize some kind of application that interfaces with a merchant account. &gt;Thats just the feature set. The more important part is that you cannot afford for your system to be down for even a couple hours. That's why I'm asking if someone knows about an out of the box solution I can just plug in to the rest of what I want to build, thereby not having to duplicate things such as keeping a manual register and still inputting things into a form for database consumption.
Have you checked out Toast or square? They really do so much already that its hard to justify writing anything extra on top of it. 
No, we are officially pulling the trigger on the business Tuesday so this was literally my first thought of needing to probably spend a couple grand on a POS system as opposed to trying to fuck around and build one, or build some kind of data entry system that is separate from the register. Seems like a worthwhile investment if its customizable and lets me track the metrics I want to track / takes payments / exports to my database where I can track other things like orders / etc through other consumption methods.
Make sure whatever you choose will allow you to have custom API integrations in the future. Square, Toast, TouchBistro have that. Things like aloha and digital dining don't. Read your agreement really closely. The wording on some agreements makes it risky to be developing competitive software solutions if you eventually plan on selling it.
Selling something would be fun, so thanks for that point. I would imagine building a master back end database (probably MS SQL?) that lives in the office, and it would just be nice to buy some kind of terminal that could track everything from throwing something out, to giving a comp, to discounts, etc. -- I can organize it on my own so long as it has a native export that's compatible with something like SSIS, or hell I could just manually import the files whenever I want to. I'd like to avoid trying to solution for that technology though. I'm pretty sure I could do it, but there are a lot of other things to work on.
The go acts like a commit and signals to send the batch. Can't use the database until it's been actually created.
There are 2 easy ways. For the sake of clarity, let’s assume your tables are named a &amp; b -The comma join, which has the unfortunate side effect of burning retinas: Select * from a,b -ansi-compliant cross join Select * from a cross join b 
I think you're exactly describing a "cross join" or "Cartesian product". Google mysql plus either of those terms and you'll find the built-in syntax and functionality for writing that kind of query. Hope this helps!
Ahhhhh my fuckin eyes what have you done. 
Yep. Upserve is a great choice upserve.com It's a POS platform specifically for restaurants with integrated payment processing and they provide data insights from you restaurant data. 
But the use database line is after the create database line anyway. So why is the Go needed? (total sql noob here)
What fields are in the prerequisite table? I'm guessing course_ID and prerequsite_ID? If so the trick is going to be to join the course table to itself via the prerequisite table: SELECT p.title AS PrerequisiteTitle , c.title AS CourseTitle FROM course c LEFT JOIN prerequisite pc on c.courseid = pc.courseid LEFT JOIN course p on pc.prequisiteID = p.courseid
I was going to say, isn't that a Cartesian join? Outer apply is what I was thinking. 
[removed]
You're getting close. Your calculation should not be in the WHERE clause, but rather the SELECT clause. You'll have to put the whole CASE statement in place of grade: Like, SELECT ID ,name ,sum( CASE grade WHEN 'A' then 4.0 when 'A-' then 3.67 when 'B+' then 3.33 when 'B' then 3 when 'B-' then 2.67 when 'C+' then 2.33 when 'C' then 2 when 'C-' then 1.67 when 'D+' then 1.33 when 'D' then 1 when 'D-' then 0.67 else 0 END * credits) / sum(credits) AS GPA etc etc (I'm not doing the FROM clause because only the laziest of wastrels use natural joins)
No. This is just plain wrong. Please don't do this no matter how big the source tables are. Also, please have a look at some very elementary sql course, cross join is the very first thing they usually teach you, when selecting from more than one table.
Not sure if you are trolling or genuinely think that this is a correct solution. 
&gt; Outer apply oh please did you not see "MySQL" 
I can’t remember thanks to my patented and mandatory self-performed ice-cream scoop lobotomy
&gt; No, we are officially pulling the trigger on the business Tuesday so this was literally my first thought of needing to probably spend a couple grand on a POS system This seems like the sort of thing you should be researching *before* starting the business, not an afterthought right when you're putting pen to paper. Right alongside labor regulations, health code, location, decor, all the other costs of running a business (rent, insurance, etc.). At this point, you don't know what metrics and data points you want to track, so fretting over these sorts of details and integrations is just going to delay your decisions and add cost or send you down a road to the wrong solution for you. *Any* decent POS package should have canned reports that will be sufficient for getting the restaurant off the ground. Once you have experience running the place and know what it is that you *really* need, start looking for something that meets those requirements. But right now you're looking for a solution without understanding the requirements. Honestly, I wouldn't recommend building *anything* from scratch unless you can make that software a product/service in and of itself and live off it.
May be you can try ETL and database testing
Was pretty sure I cam here for SQL advice. If you must know we would not be opening the doors for six months, and this opportunity just came about in the last two weeks. So I have had other things to be researching than an automated POS systems and a database --&gt; neither of which are even necessary to open or run a restaurant in the first place. &gt;At this point, you don't know what metrics and data points you want to track I know exactly what they are, but I know that I would prefer a front end POS that allows me to customize these, etc. -- hence doing some research and figuring out whats out there. &gt;Any decent POS package should have canned reports Not interested in canned reports. I would rather spend extra to get exactly what I want. &gt;Honestly, I wouldn't recommend building anything from scratch unless you can make that software a product/service in and of itself and live off it. I'm considering that but not sure timing is right. Right now I just want to know whats out there and how much it costs. If I need to I can literally write things down by hand and input the data myself.
&gt; upserve.com This looks cool. Going to do dig on a bit to see what kind of exporting it can. They kind of lost me on the whole "looking at complicated reports" bit... I'd just as soon see myself setting up the reports I'm seeing in their analytics tabs using something like Tableau or SSRS. It possible I wouldn't need to and that they have everything, but so long as the data is exportable then it doesn't matter.
The name won't affect the function of a constraint. It is however advisable to pick a suitably descriptive name, so you can remind yourself of what the constrain does without having to look in the definition should you forget at some point. So, you can have a constraint called **emp_employee_id** applied to a table called **Students**. The constraint is treated as an object in the database much like the table, and I believe the name needs to be unique across the database (I'm guessing Orcale behaves this way like MSSQL). [This](https://docs.oracle.com/cd/B19306_01/server.102/b14200/clauses002.htm) looks like a good starting point to delve into how constraints work and what the limitations/rules there may be.
It depends on your database. In Oracle, primary keys are usually implemented using a btree index. A btree index is an additional and separate object than the the table it is created on. The name can be any legal identifier. The database can name it I believe, if you omit an identifier, or you can assign one. 
Table "employees" has a field called "employee_id" that is a number field and is designated as the Primary key with a key label of "emp_employee_id" The table name, field name and label for the primary key are all designated manually and can be anything*, the primary key label could be "XXXXX_Hi_Mom_XXXX" instead of "emp_employee_id" and it should technically function. However, the main thing is primary key's are system wide and the label needs to be unique so it is important to have a completely unique label and be descriptive. We use the following naming convention, Key-Type_Table_Column-Name so we would name the primary key "PK_employees_employee_id" so that it is both unique system wide and fully descriptive by the label. Looks like here it is a abbreviation for the table name, then the column name which there isn't a problem with IMO, but just showing that it can be anything but there really should be a set naming system. For the student question, I guess "stu_student_id" would work assuming it is on the "student_id" column, not a fan of using table abbreviations for this reason though.
Thanks for the help I know what to do now
Each batch is parsed/validated against the state of the system at the time the batch runs (or would run). Since `database_name` doesn't exist yet, you can't `USE` it as you did originally. By breaking it into two batches (by using the batch separator `go`), when `USE database_name;` comes along, it *will* exist because the first batch created it.
 SELECT id , SUM(CASE WHEN type = 'BUDGET' THEN value ELSE NULL END) AS budget , SUM(CASE WHEN type = 'ACTUAL' THEN value ELSE NULL END) AS actual FROM input WHERE flag_date &gt; DATE(CONCAT_WS{'-',year,month,01)) GROUP BY id
Woops I did this with MS SQL before I realized that this was for mySql. Still may be relevant but need to replace how I did the DATEADD function with however they do that in mySql. create table #tmp ( id varchar(20) , Year varchar(4) , Month varchar(20) , Flag_Date datetime , value int) insert into #tmp values('A12','2017','8','10/27/2017',25) ,('A12','2017','9','10/27/2017',10) ,('A12','2017','10','10/27/2017',5) ,('A12','2017','11','10/27/2017',20) ,('F14','2017','9','9/21/2017',5) ,('F14','2017','9','9/21/2017',5) ,('F14','2017','10','9/21/2017',5) ,('F14','2017','11','9/21/2017',1) select id , SUM(value) from #tmp where dateadd(mm, (cast(year as int) - 1900) * 12 + cast(month as int) - 1 , 1 - 1) &lt;= Flag_Date group by id
Thank you very much for your reply. I think though that I'm not getting a very basic thing. Are all of the comments in a script executed at the same time (in principal) or first line, 2nd line, 3rd line etc? Cheers, 
If you hear of any, let me know! SQL does not tend to lend itself towards part time or side jobs like development can. There's a lot of reasons for this and a lot of aspects, but sometimes a few small jobs MAY exist.
They are evaluated before run by batch. They are executed statement by statement in the order written.
[removed]
Peoplesoft? 
The substition capability you describe sounds best suited to an additional, many-to-many table with this basic structure: recipeID | origitemID | subsitemID ---------------------------------- Each item (origitemID) in each recipe (recipeID) can have multiple rows in this table, allowing multiple substitutions. So the example above would have three rows in this table. Let's assume vegetable soup is recipeID 211, and that carrots are ingredID 12, tomatoes are 13, celery is 14, and corn is 15. The rows would look like: recipeID | origitemID | subsitemID ---------------------------------- 211 12 13 211 12 14 211 12 15 
&gt; Is there a way to make a recipe table without having a set # of ingredients for a recipe sure have the recipe ingredients in a separate table, one row per ingredient that way you can have as many ingredients as each recipe requires, rather than a "set number"
Have you thought about doing an array type of column? In BigQuery you can do this, so you could have three columns: recipe_id, orig_items, sub_items, where each recipe will only have one row instead of fanned out. Then orig_items and sub_items will have arrays of ids of all possible ingredients you could want in it. You could also try some string aggregates with wildcards ops like checking if [orig_item LIKE %item_id%].
Are you sure that your JOIN clause is correct logically? You didn't attach the structure of the two tables you are using so I'm just making a guess but this condition (e.EmployeeID = d.ManagerID) doesn't seem quite right.
Here is how the two tables are set up: https://imgur.com/egVmYfn I don't think that's the issue though because I was able to successfully execute that bit of code when I took it out of the procedure, thinking that maybe I set up the join wrong. 
Thank you for your help! I've heard that about natural join from several people. So, I'm guessing I should work away from that. lol I don't know why my instructor would teach us such a frowned upon method. 
Thank you for your help! I really appreciate it!
&gt; forced to use multiple rows for one recipe oh god!! the horror!! and arrays of ids is a better solution? please, do sit down
It's actually not uncommon to see on this subreddit examples of teachers teaching bad or outdated practices. I guess it happens because they're big on theory but generally don't have much real-world experience.
What error message are you getting?
That is very true. Thanks again for your help! I'll be back for more lol
[removed]
It just tells me procedure created with compilation errors. It’s a pretty unhelpful error, lol. I will try your suggestion after I’m off work. Thanks!
where is the enrollment information stored? p.s. you do not want to be using NATURAL joins 
I feel maybe a [picture may tell a thousand words here](https://imgur.com/7Pm3pSB). &gt;I don't understand DayTot. Show me a sample dataset of how this is calculated. This is what i need to calculate in order to generate an average of the scores on that day. this is in the second table in my post Every time a hotspot (say 384) has its score updated, a row is created in the history table. &gt;Does the parent data source structure its data in such a way where each day has a score, and no day is duplicated per hotspot? each day doesn't necessarily have a score. a record is produced in the history table only when a user updates one of his scores. if a day is duplicated per hotspot in this way, only the latest hotspot that day counts towards the average. &gt;This is where I'm unclear what you're after. Are you saying that each ID has a score per day no. a record is produced in the history table only when a user updates one of his scores &gt;and what you want is per day to come up with an average based on an IN(), where the numerator is the number of ID's? no, the numerator is the total of the current scores for all hotspotIDs in each day for which there is an entry in the history table. the denominator is the total hotspotIDs that have appeared in the history table by that day. hopefully the image is the most helpful explanation here. to take a bird's eye view, a singing teacher gives a series of things for his choir students to improve (hotspots). he asks them to score themselves (1 to 5) when they practice. at any time, he'd like a timeline of progress, measured as an average score over time. does this help?
I emailed my teacher about it because I couldn’t find it anywhere in the database. I’ve been told that natural join isn’t the way to do things, but I haven’t studied other joins enough to use them yet. 
well, take 5 minutes and google INNER JOIN and LEFT OUTER JOIN those are the only two you need for now
So basically using inner returns similar rows and outer return dissimilar rows from two tables. Natural join is just kind of like a sloppy inner join then
This is what I was given for enrollment. Enrollment meas the number of students which we should use "takes" table to find out. The takes table is takes(ID, course_id, grade, sec_id, semester, year)
similar and dissimilar are the wrong adjectives but yeah, a natural join is an inner join, using join condition(s) where all columns with the same name are matched this can backfire in spectacular ways, which is why most people advise against it
Thank you for the helpful insight!
I just noticed that you have a ";" after the procedure declaration: CREATE OR REPLACE PROCEDURE GetManager(....); IS The ";" at the end has to be removed.
The general principle is that your database tables should contain indivisible and unique rows (tuples). So you should have a table for each collection of like objects. In this case, that's departments, instructors, and students. I'm assuming that you're dealing with departments that have multiple instructors, each of which have the set of students in the department divided among them. This model wouldn't support an instructor teaching multiple classes (you'd have to add a Classes table) or a student having multiple instructors. You can inner join because, in my understanding, you want to limit your instructors to those who have a link to the given department and to limit your students to those who have a link to the subset of instructors. Something like this pseudocode should probably do. Table Departments DepartmentID INT DepartmentName VARCHAR(64) Table Instructors InstructorID INT InstructorName VARCHAR(128) DepartmentID INT FK to Departments.DepartmentID Table Students StudentID INT StudentName VARCHAR(128) InstructorID INT FK to Instructors.InstructorID Procedure to get numbers CREATE PROCEDURE GetNumbers @DepartmentName VARCHAR(64), @Instructors INT OUTPUT, @Students INT OUTPUT AS SELECT @Instructors = COUNT(DISTINCT Instructors.InstructorID) as [Instructors], @Students = COUNT(DISTINCT Students.StudentID) AS [Students] FROM Departments INNER JOIN Instructors ON Instructors.DepartmentID = Departments.DepartmentID INNER JOIN Students ON Students.InstructorID = Instructors.InstructorID WHERE Departments.DepartmentName = @DepartmentName
but its web scale
I'm a random dude from London so timezone might be an issue but if nobody helps ya shoot me a PM and I'm sure I can.
So the red line is the hotspot 384? What is 739? I'm not really understanding this. Set up an Excel file and fill it witih some sample data, use a second tab for your second table and write a VLOOKUP between them... I can probably help you with the SQL.
Does this do it for you? SELECT Company_Agreement__c.RecordId, CompanyAgreement__c.URL__c, RIGHT(RTRIM(Company_Agreement__c.URL__c), 4) FROM `Company`.Company_Agreement__c WHERE (URL__c != NULL) LIMIT 10;
cross join? there's only one table! 
this
I like this solution, I would take it one step further though for a more robust solution. I would create a separate "SubstitutionGroups" table. You can store common substitution groups that might be used (bonus feature) ID int PrimaryIngredientID int Name varchar Then an "IngredientXrefSubstitutionGroup" table. SubstitutionGroupID int ,SubIngredientID int Finally, RecipeXrefSubstitutionGroup RecipeID ,SubstitutionGroupID Now you can include substitution groups on your recipes, groups attach to their corresponding primary ingredient, and include all substitutes. Additionally, you can add more meta data to group itself, name them, etc. To be used in other recipes if they're common substitute groups.
Your flair is so appropriate here. I may steal a variation of it.
Yes, the red line is hotspot 384. as for 739 &gt;only the latest hotspot that day counts towards the average 739 doesn't figure in the calculation of the average because it wasn't the score of the day for that hotspot at midnight. only one hotspot score can count towards the average in any day if there are more than one, and i figure it should be the latest of them. i haven't done vlookups, and, just looking at them, i'm not sure how how that would help, since it searches for values. perhaps better if i explain in [pictures](https://imgur.com/a/lBnZu) that show what values the DayTots are made from?
A VLOOKUP is the same thing as a JOIN, just put your data in two tabs in Excel and I should be good. I can try to look at your other picture tomorrow at lunch but it's going to be a busy day.
Actually these photos might work. I'll let you know tomorrow when I have time to play.
thanks sir!
Since you're in Dallas, I'm going to suggest you check in with the North Texas SQL Server User Group, http://northtexas.pass.org and https://twitter.com/ntssug . They've got somewhere around 100 members and amongst them you'll find quite a few people who will fit every niche you're looking for. What's your deadline?
Thanks for the info! I'll check it out. My deadline is Friday, my scheduled interview that I'll still go through with got pushed back to Sunday evening. 
Thanks for that! I sent you a PM a bit earlier.
If you're looking to get into a career as a data professional (or app developer that interacts heavily with a database), think about joining PASS (it's free!). Go to a few of the NTSSUG meetings (they've got a terrific group there), talk to people, find out what they're doing and the types of companies they're working for/with. This kind of conversation is *far* better than filling out a survey or having an interview with fixed questions. They will probably be having a SQL Saturday in the Spring (they have a date in May reserved), check that out too.
That was the problem. I'm glad it was something small, and everything else was set up correctly! Thank you very much for your help!
He's cross from doing joins, so he won't do them anymore.
Select ID, Sum(value) From Table Group by ID
I don't think there's an "easy button" for migrating to SSIS but don't have a ton of DTS experience myself and would love to learn otherwise. &gt; Quick second question - does SQLEXPRESS 2014 allow the use of SSIS? I'd like to test making some packages but unsure if they will be executable on Express. No, but the Developer edition is free and perfect for this.
ugh. your PRODUCTID is integer, what are you inserting there?
Perhaps you misspelled "hug." Would you like one? 🤗 --- I'm a bot, and I like to give hugs. [source](https://github.com/as-com/reddit-hug-bot) | [contact](https://www.reddit.com/message/compose/?to=as-com)
Thanks Dev Edition sounds perfect ill spin something up tonight. I believe for the DTS Packages I can migrate them into 2008 R2 (theres a wizard, requires 2005 Backwards Compatability install optional install) from there I am unsure if they can go further. I should also mention when I write "Am I forced to rewrite them" I actually mean Am I going to end up spending 2 weeks learning about SSIS Packages and then attempt to rewrite them as I've never touched this side of SQL Before. Cheers,
By "express" I'm going to take it to mean you want to see it as a query. You can use the Count aggregation with a group by clause. Search for stuff related to that!
Probably the best way would be to have another table between those two, BookSales, with just the BID and Sales_ID. https://en.wikipedia.org/wiki/Associative_entity
**Associative entity** An associative entity is a term used in relational and entity–relationship theory. A relational database requires the implementation of a base relation (or base table) to resolve many-to-many relationships. A base relation representing this kind of entity is called, informally, an "associative table" (though it is a table like any other). As mentioned above, associative entities are implemented in a database structure using associative tables, which are tables that can contain references to columns from the same or different database tables within the same database. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/SQL/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
thank you. 
I will bring you some bad news, you have to rewrite your packages to be able to use them in SSIS 2014. There have been a lot of changes in the previous versions of SSIS and at some point the entire system had changed and developers had to recreate all their packages to make sure they'd work in the new version. Creating packages in SSIS is pretty straightforward and if it's not too complex and you have the time you can create the new ones pretty fast. It also depends on the amount of packages. If the packages are complex, or too many of them, I would advise you to hire somebody that is able to create BIML scripts to generate the packages and recreate them. The BIML scripts make it possible to recreate all your SSIS packages really fast. The thing is that at some point Microsoft had to change the underlying system of SSIS to make it work for future releases. Because that's done now you will probably not get this issue somewhere in the near future. 
Looks like you're on the right track. So when you add a new row, eff_start_date is the current time, and eff_end_date should be long in the future, like 31/12/2999 or something. If that row expires and is replaced, then in the old record you set the eff_end_date (now), and make the new record as eff_start_date (now) and eff_end_date (31/12/2999). Does that make sense? That way you can get the state of the table at a certain time using eff_start_date &lt;= (certain time), or the change from a certain date using eff_end_date &gt;= (certain time). The latest version of the table is always records with eff_end_date = 31/12/2999.
Thanks for the info - Bummer about having to rewrite them but deep down I think I knew this would be the case :/ Only 4 Packages are critical to coming across the most complex of which had about 5 steps so its not like I am jumping into the deep end but I need to wrap my head around SSIS first. From a quick glance I can see most of the steps in SQL2000 executed batch files so I can potentially bring them across if that functionality still exists. Thanks again for your response.
Try using a 'case when' statement. Case when the credit is null or F then 0. The question is too general to give a more detailed answer.
Google search 'sql convert string to number'
I was lined up for a similar migration project some tine ago. I've been in the business long enough to know dts... Alas :) They ended up rewriting everything; technically upgrading is not possible. Like treshr is saying, BIML may help you, but if it's mainly used to tie together batch file executions biml won't help you much. 
select ID, name, dept_name, tot_cred, case credits when 'F' or null then 0 end from course natural join studentcopy; __ I tried this, but it gave me a weird 5th column and it still has the original credits. I'm not sure if that is enough information for you to help anymore. Please let me know if I can provide any more data. Thank you
As an intro to updates, you should likely just split up things to make it easier for you to understand. While some experience will let you handle what is required via a single update statement, you should probably opt for two different statements and use conditions to filter your result set to the appropriate values. You will likely want to **select** your data **from** your new table **where** credits **is null** or credits equals **'F'** Once you understand that, you can craft an **update to **your new table** and **set** the credits to **0** **where ** credits **is null** or credits equals **'F'**. As for us being able to provide you with a better answer, typically we need to know which flavor of SQL you're working with as your [SQL] tag doesn't provide that information. Additionally, it is helpful to provide some form of create statement for the tables you're working with along with some sample data: create table SomeTableName ( SomeID int identity(1,1) primary key, WeNeedASampleField varchar(50) ) insert into SomeTableName (WeNeedASampleField) values ('1'), ('2'), ('X'), ('abcdef'), (nuill), ('F') then provide any queries you've attempted select * from SomeTableName where WeNeedASampleField = 'F' update SomeTableName set WeNeedASampleField = 0 where WeNeedASampleField is null
Mist https://www.varigence.com/Mist has a trial period and maybe helpful for this. 
maybe, but maybe not suppose each book is unique -- then it can only be sold once, right?
I'm assuming you don't know you data well, in this case, it is complicated. Are you trying to import .csv files or something? I would take a look at the first row of each file, and then create tables based on that. 
For PostgreSQL: select CAST([qrVacationSum."acCalcVacationTogether"] as integer) - CAST([qrVacationSum."acCalcVacationUsed"] as integer) as subtraction from &lt;table&gt;; For mysql: select CAST([qrVacationSum."acCalcVacationTogether"] as SIGNED) - CAST([qrVacationSum."acCalcVacationUsed"] as SIGNED) as subtraction from &lt;table&gt;; 
That “wizard” probably won’t get your DTS packages migrated properly. It might get you partway there but you’ll have to do enough cleanup that you’d be better off rebuilding them from scratch. 
Yeah, I did that already. But I dont know which columns are char and which are integer etc. Therefore, the import is failing. Thanks for your replay tho.
If DTS was just being used as a glorified job scheduler, you may want to skip SSIS and just implement everything as Agent Jobs, if appropriate. Just because something was DTS previously doesn’t mean it should be reimplemented in SSIS.
All columns can be char if you think about it. Even those which should obviously be integers. The opposite is not true. Why don't you create all tables with char columns and then later alter the tables casting the char field to a new integer field? That's a shitty way to solve your problem if you ask me, but I can't think of anything different than that.
I indeed did that too. It's not possible to alter the char columns into integer. You can only alter it into "character" and "text" unfortunately.
ALTER TABLE your_table ALTER COLUMN your_char_column TYPE INT USING your_char_column::integer;
 -- DROP TABLE FROM PRIOR TESTING. ;DROP TABLE IF EXISTS #Products -- CREATE TABLE FOR TEST DATA. ;CREATE TABLE #Products ( [Date] DATETIME ,[Product] VARCHAR(12) ,[Location] VARCHAR(2) ,[Flag] TINYINT ) -- FILL TEST DATA FOR REVIEW. ;INSERT INTO #Products ( [Date] ,[Product] ,[Location] ,[Flag] ) VALUES ('1/1/2017','123456','AX',1) ,('1/2/2017','123456','AX',0) ,('1/3/2017','123456','AX',1) ,('1/4/2017','123456','AX',0) ,('1/5/2017','123456','AX',1) ,('1/6/2017','123456','AX',1) ,('1/7/2017','123456','AX',1) ,('1/8/2017','123456','AX',0) ,('1/9/2017','123456','AX',1) ,('1/1/2017','654321','AX',1) ,('1/2/2017','654321','AX',1) ,('1/3/2017','654321','AX',0) ,('1/4/2017','654321','AX',0) ,('1/5/2017','654321','AX',1) ,('1/6/2017','654321','AX',1) ,('1/7/2017','654321','AX',1) ,('1/8/2017','654321','AX',0) ,('1/9/2017','654321','AX',1) -- USE A CTE TO FIND THE RESET RANGES WHERE FLAG IS ZERO BY PRODUCT. ;WITH cteRangeValues AS ( SELECT [Product] = P.[Product] ,[BaseDt] = P.[Date] ,[NextDt] = COALESCE( LEAD(P.[Date]) OVER ( PARTITION BY P.[Product] ORDER BY P.[Date] ASC ) ,DATEADD(DAY, DATEDIFF(DAY, 0, GETDATE()), 1) ) ,[RangeNum] = ROW_NUMBER() OVER ( PARTITION BY P.[Product] ORDER BY P.[Date] ASC ) FROM #Products P WHERE P.[Flag] = 0 ) -- JOIN OUR BASE PRODUCTS TABLE BACK TO THE RANGES TO GET A ROW NUMBER. SELECT [Date] = P.[Date] ,[Product] = P.[Product] ,[Location] = P.[Location] ,[Flag] = P.[Flag] ,[RangeRow] = COALESCE(R.[RangeNum], 0) ,[RunningTotal] = -- USE THE SUM FUNCTION OVER THE MATCHING RANGE FOR TOTALS BETWEEN. SUM(P.[Flag]) OVER ( PARTITION BY P.[Product] ,COALESCE(R.[RangeNum], 0) ORDER BY P.[Date] ASC ROWS UNBOUNDED PRECEDING ) FROM #Products P LEFT JOIN cteRangeValues R ON P.[Product] = R.[Product] AND P.[Date] &gt;= R.[BaseDt] AND P.[Date] &lt; R.[NextDt] ORDER BY [Product] ,[Date]
This isn't 100% true. There is an upgrade utility you can run that works... sometimes. I am actually doing a project right now to convert DTS packages to DTSX. https://msdn.microsoft.com/en-us/library/ms143496(v=sql.105).aspx The problem is when you have certain sources (like Excel) that were previously treated as things like access databases but that type of connection no longer exists so you have to recreate the entire thing. In those cases, I've been literally opening the vb file directly in notepad++ and reading the old connection and mapping information and manually translating it to DTSX. 
Hi KH, different KH here...I'm in DFW and have time to help you. I'm also on the board (term starting Jan1, 2018) of the NTSSUG user group mentioned below. DM me at your convenience. My morning is packed, afternoon open today.
You're right that you can convert them but as you mentioned it works sometimes. If you get some complex packages than you could end up with some conversions that just don't work. I've found that I'm better off recreating the packages, either manually of through BIML, and be done with it instead of having to troubleshoot my packages.
Sure will! Thank you, Kevin :) P.S. I'm digging your handle.
Were you tasked with building the data model, or is it something you were given to work with? Based on needing to email the teacher about enrollment information, I'm guessing it's the latter. Some of the tables you've presented seem to be lacking logical relationships to/from the others, in addition to lacking useful information needed to build the right query. Here are a few issues I see: 1. You don't have a "students" table, which you probably should have, otherwise your school is full of instructors talking to themselves in empty rooms for months at a time. 2. I don't see anything in the "takes" table to hold the number of enrollments. Presumably more than 1 student will be taking a course in a given semester. Who or what does the "grade" column belong to? You may also run into some ambiguous data with the "semester" and "year" columns here and those columns of the same name in the "section" table, presumably joined on sec_id. 3. I don't see a "good" way to get from the "instructor" table, save for on the "dept_name" column, which isn't ideal because there may be more than 1 instructor per dept. 4. You may want a "department" table, as both and an instructor may span multiple departments. Consider an instructor who gives a course on relational algebra to CS/applied mathematics students and also gives a course on geometry to engineering students. What you have here is what we in the industry call a "turd," and is something we would use in exercises going from a sub-optimal, poorly normalized data model to a properly normalized one. 
Thanks so much- I ended up doing it like this- it seems to be working, creating a unique value in ResetSum each time the flag flips, which I can then use to partition the groups. Select M1.[Product],M1.[Date],M1.ResetSum, M1.[Flag], SUM(M1.[Flag]) OVER (PARTITION BY M1.[Product], M1.[ResetSum] ORDER BY M1.[Refresh Date] ASC) AS RUNNINGSUM FROM( select M.*, sum(Case [M].[Flag] When 0 then 1 Else 0 END) OVER (PARTITION BY M.[Product], ORDER BY M.[Refresh Date] rows unbounded preceding) AS 'ResetSum' FROM [dbo_tbl] AS M ) AS M1
Also consider the RDBMS as well because the functions may be different.
I like the idea... one question, though. In the IngredientXrefSubstitutionGroup table, is there an OrigIngredientID column? When I substitute molasses for sugar in my cookie recipe, how do I tell the application that I'm subbing it for the sugar and not the flour?
you realize it couldn't be either of those platforms, right? (despite the fact that OP forgot to mention it, as requested by the sideboard) it's those [ugly] [square] [brackets] that gives it away -- either MS SQL Server or MS Access i'm a bet MS Access because qrVacationSum suggests a query, and stored queries are a routine fact of developing Access reports 
Someone needs help finding a substitute for salt...
Not super familiar with Jira or Salesforce, but the table identifiers in your SELECT statement seem to refer to two different tables (although only one seems to be called out in the FROM clause). With no other clues, it looks like you want to get rid of the underscore between 'Company' and 'Agreement'... the table alias for the table containing the URL appears to be 'CompanyAgreement__c', not 'Company_Agreement__c'. Not sure how this is working without a reference in the FROM clause; just parsing what you've given us.
I didn't even notice the square brackets. Typical copy paste. I should have, since I've worked with SQL Server 2008.
You replicate databases, not backups. Maybe I'm not understanding the question? 
Probably the easiest way for you would be to: * Right click on one of the databases you want to restore * Go to Tasks&gt;Restore&gt;Database * Go through each of the 3 windows (General, Files, Options) and properly fill out each one. You may want to run one of the restores to verify that it's successful. * Click the "Script" Button This will generate you a restore script with all the options you chose that you can run manually. You can now use that script again, or copy and modify it for the other databases. Or you could go through the wizard again to generate a script for each one and you won't have to modify each one.
Now have a look at this and migrate back https://www.microsoft.com/en-us/sql-server/sql-license-migration?wt.mc_id=AID627568_QSG_SCL_203118&amp;utm_source=m.facebook.com&amp;utm_medium=referral
Pro: you can find specific data without having to go through every piece of data for it. Con: it takes up additional space on the server as it has to store the data in different ways. Example: having a separate bin for each color lego block. if you're looking for a red block, you can skip the green, blue, and yellow bins, looking only in the red bin. this will cut down the time to find a red block by a lot, assuming you have about the same number of each color block.
Yes, i have also the original.mdf and .log files If i try to create a new database (Db1) and replace de db1.mdf and .log files with the original.mdf and .log files and rename them in db1.mdf and db1.log will work?
With regards to your analogy example, why does the number of each color block have to be the same? Thanks.
I tried this and it works for first database. Then, If i want to create another database and restore the same .bak file dont work
You can use the Attach Database command or TSQL to create a database from a given MDF/LDF file pair. Make the required number of copies of the MDF/LDF files... such as DB1.mdf, DB2.mdf, etc... CREATE DATABASE DB1 ON (FILENAME = 'C:\SQL\DB1_Data.mdf'), (FILENAME = 'C:\SQL\DB1_Log.ldf') FOR ATTACH; CREATE DATABASE DB1 ON (FILENAME = 'C:\SQL\DB2_Data.mdf'), (FILENAME = 'C:\SQL\DB2_Log.ldf') FOR ATTACH; 
Thanks for the try, sadly it hasn't worked yet. SELECT Column1, Column2, Column3 FROM Table WHERE Column3 != '' Limit 10; This gives me a full data set (well 10 records) but when I try to add RIGHT &amp; RTRIM, the results are empty. The column headers are there so I am assuming that the query ran correctly but the data is simply blank. Kinda trying to hit a home run for my boss here. Regardless, thanks for the help. If you have any other ideas please let me know. If it helps, here is the expected results |RecordID|URL__c|LastFour| |---|---|---| |1|http://www.testlink-1234|1234| |2|http://www.testlink-0525|0525| |3|http://www.testlink-3515|3515| |4|http://www.testlink-0658|0658| |5|http://www.testlink-6812|6812| What I'm seeing when trying to get the last four is |RecordID|URL__c|LastFour| |---|---|---| |||| |||| |||| |||| |||| 
You will have to change the physical file names for the data and log files.... look at the MOVE declaration for Restore Database. What's happening is your second restore is trying to create the data and log files for the database and they already exist.
Sorry for the malformed query. I have to make some changes to protect sensitive data and while I thought I cleaned it up nicely, obviously I didn't clean it well enough. 
_About_ the same, it by no means needs to be the same, but to make a good example, it helps if they're distributed equally. The assumption that the performance will improve relies on the block colors being similar in number... if 99% of your blocks are red, skipping the green/yellow/blue blocks doesn't help that much. 
You're right, I called it a cross join and it really isn't. I'm using Toad Data to pull the dataset and when i entered the query, it prompted me to create a cross join. Sorry for the confusion.
If you have a table of people and their attributes, you might place your primary index on last name (not really, but go with this example.) This will allow the engine to locate the record you want by seeking through an index, which is in last name order, and is narrower than the original table. Now, if suddenly birthday becomes important to your app, you can add another index on birthday, which will allow the engine to find all the rows with that birthday, without having to scan every row in the original table. Once the engine has identified the needed rows, it takes that list back to the original table, and seeks those particular rows. This is what makes it faster. Indexes are also used to speed up sorting operations. Cons include, extra disk space, and if you have a ridiculous number of indexes on a table, you can negatively impact the performance of insert, update, and delete operations, because all the indexes have to be updated, too. In general, create an index for your where clauses. This is a guide, and not a rule. 
Thanks! 
Thanks !
Two reasons; The first has to do with the size of the statistics "bucket" used to describe roughly where each block is in your room. The second deals with the time it takes the disk to skip over all the non-red blocks to get to the reds you want. Theoretically, if you have a disproportionately high number of yellow blocks relative to your number of red blocks, it may take ever so slightly longer (on the order of microseconds to milliseconds) to find the reds due to the time it takes to skip past all the yellows. In this same scenario, the DB engine may keep the yellows in memory longer than the reds, and reading from memory is orders of magnitude faster than reading from even the fastest SSDs. Both of these are usually non-issues when dealing with modern multi-core servers with shitloads of RAM and SSD storage. To the OP; the classic/best/easiest way to think of indexes on a table is to consider a phone book. In order to make it easy to find a particular persons phone number, the phone book is organised in alphabetical order by last name. This is a clustered index. It is the "main" or "default" means by which data is stored in a table, or our phone book. Using this clustered index, you can easily flip to the Smiths in the phone book to find the number for Joe Smith. If for whatever reason you wanted to find all the people in the phone book with the first name Joe, because the phone book is only sorted by last name, you have to look at every single page and check every single name to see if their first name is Joe. To support this means of accessing the data, this is where an additional index comes into play. You create a new index, organised by first name, and include the persons last name and phone number. Now you can easily find all the people with the first name Joe. As u/ihaxr mentioned, there is a cost to this, as you are basically creating a second copy of your phone book, just sorted in a different order. One must always consider the cost/benefits when creating new indexes. You may find the cost to maintain the additional index outweighs the benefits it provides, if not many people are searching your phone book by first name. 
Imagine a phone book with no index - names randomly entered Imagine a phone book indexed on last name, first name - same size as the random data and incredibly useful Imagine a phone book indexed by number - same size as the random data, but not terribly useful, unless your a cop So having a phone book clustered on the last, first AND also having a reverse lookup - it would be twice as large, but quite useful. Unordered data, such as a phone book with no useful order, is expensive to retrieve because, on average you have to scan half of the data to find what you want. Almost all data you encounter is ordered, other than my kids’ bookshelves. 
Ok I'm looking at these examples. Let's kind of reset here and go back to your first image and talk about what you're exactly trying to do. You want a graph that has multiple lines with days at the bottom axis, and "score" on the left axis. You want one line for each ID you feed to the where clause, and you want another line that is the average based on how many lines you give it. So if you fed it 10 hotspot id's you would see 11 lines? What is the ID on the hotspot table? Not HotspotID, but ID? Is that the score? 
Hey! Kevin3NF has told me a great deal more about NTSSUG. Aside from the May UTD event, what general part of DFW do you all host events?
It looks like the most recent meeting was at the Microsoft offices in Irving. http://northtexas.pass.org/?EventID=9248 That *might* be where they're regularly held, but I don't know; I'm about 1500 miles away. /u/kevin3nf can say for sure, or get signed up with PASS and join that chapter's mailing list (or follow on MeetUp.com) to stay in the loop.
I might be seeing this... maybe. So you want to take a trivial query such as: select * from table where hotspotid in () This will return a dataset (you have pictured on the left) where you get a bunch of scores and dates for those hotspotid's. DayTot (in another table) represents the sum of the id's you have in the `IN()`? If a certain hotspotId is null for a date, then you take the score it had from the day before and sum it with the score of the hotspotID which isn't null. So now you want to add a column to the above select for DayTot. DivBy is the # of ID's you have in your IN() AVG is DayTot / DivBy. Note is just a simple representation. My question is what if I were to feed in 15 id's but only 2 existed on a day? The sum of previous days when combined with the current day would give a high DayTot that might not be in your table. So in that case do you want the DivBy to be 15, or only the total number of hotspots that were active on that day?
Other people have given good advice, I just wanted to give my condolence on having to work with DTS and SQL Server 2000. DTS was one of the more frustrating tools I've had to use.
what do u mean? I am inserting an integer.... a number... for an ID.
yes I understand what to do, but my issue is I am lost on the how to do it part T_T
&gt; DivBy is the # of ID's you have in your IN() not true, and you then identified the answer...the last paragraph is correct. to be meaningful, the average has to be over the total number of hotspots that were active on that day. hence, in image one of nine, the average is over one, not two. thanks for sticking with this.
one more thought... try casting the column 'url_c' to a string/text data type, maybe varchar(254) or varchar(max)? It may be some other data type that string functions don't work with directly.
This could be used as a template for each database. Note that you need information from the results of the select query and "restore filelistonly" statement in order to complete the "with move" clauses of the restore database statements. create database NewDatabase; -- Replace name of DB to create go use NewDatabase; -- Replace with name of DB that is created go select file_id, type, type_desc, name, physical_name from sys.database_files; -- Populate the RESTORE statement with the destination files from this query restore filelistonly from disk='c:\DatabaseBackup1.bak'; -- Enter path to BAK file -- use the LogicalName column to populate the logical file names to be replaced in the MOVE clauses of the restore statement use master; go restore database NewDatabase -- Replace with name of DB to be restored over from disk='c:\DatabaseBackup1.bak' -- Enter path to BAK file (Same as from the restore filelistonly command) with move 'DatabaseBackup' to 'C:\Program Files\Microsoft SQL Server\MSSQL11.MSSQL\MSSQL\DATA\NewDatabase.mdf', move 'DatabaseBackup_log' to 'C:\Program Files\Microsoft SQL Server\MSSQL11.MSSQL\MSSQL\DATA\NewDatabase_log.ldf', REPLACE, stats=5; go
Dude I wish. I wish so much. Black Friday was pure hell. I continue to run into a weird situation where queries run great via SQL Developer or Toad. 10-15 second executions, explain plan shows I'm using indexes. No problem. Same query, using same credentials, hitting the same database, via SSRS takes forever and explain plan shows full table scans. I've discovered I can hard code index hints as a workaround but I'm becoming far too reliant on this bad habit. I'm tired of looking inept. I'm tired of late night phone calls. I had family visiting from Stanford, an old friend visiting from England, and shopping for the kids that I needed to take care of. Instead I spent half my holiday weekend on the phone and on the VPN playing trial and error while angry ops managers kept asking "is it fixed yet is it fixed yet?" The VP who demanded we move off of MS just doesn't like the company in general. I think the licensing issue gave him the perfect excuse he was looking for.
This is where I'm still confused. So lets say you have 100 ID's in your IN, and some of the ID's haven't shown up since July, or whatever... you want all of them to sum together to come up with a DayTot? The DivBy would only be the total number active on the day... but the DayTot is confusing me.
Get rid of those last columns "Cast" and "Shows" and give each table a numbered ID column, so that Matt LeBlanc is actor 1 and Friends is show 1 (and so on...) Then in a third table, match the actor to the show. As many times as necessary. So if Man With A Plan is show 2, this table would have rows like |Actor|Show| 1 1 1 2 ...
You are on the right track. One method would be a shows table, an actor table, and an intermediary table. Shows: show, startYear, endYear, id Actor: nameFirst, nameLast, dateBirth, id Involvements: showId, actorId, id Involvments is a bad name, but I am just thinking off the top of my head. So the Shows and Actors are self contained and those tables should generally only include intrinsic data about the subject - actors are people and people have names, dates of birth, numbers of toes, usually parents (which are other people and could be signified with another id) Shows, in general, have a name, a start year, and end year, a synopsis, etc Ahh! Roles would be a good name for the linking table. So your basic intermediary table linking the two would be actorID, showID, charName, charAge, etc. Then if you were going to display a detail page for an actor you would be able to select * from actors where &lt;id&gt;=actorId and you could get a list of their roles and what shows they have appeared in by joining the roles table and shows table. And do the same for displaying a detail page of a show and showing which actors were in it and what roles they played. Hope this makes sense and helps a little 
This is called a many-to-many relationship (many actors are in a particular show, and many shows contain a particular actor).
Clever! I like that approach too, using the running sum per product to create groupings works very well in this case. Nice job!
they are 3rd Thursday of every month (except maybe December...can't remember), at the Microsoft building in Las Colinas (114 and 161)
To just fuck everyone over? I mean, SSRS pointing at Oracle is such a bizarre thing to do that answers are going to be very tough to find. He sounds like a major douche. Could be time to look for a new job. 
It's working. I wouldn't say it's working well but it's working. I definitely agree with the strategy. And aggressive migration time frame he ordered. We spent 10 weeks over late summer / early fall moving off of SQL and onto Oracle. 10 sites, 10 weeks. Each site contained both transactional and replicated reporting environment for QA and prod, with approx 100 reports per site. Couple weeks after the migration was over, problems started occurring. Weird query plans and so on. Black Friday was a mess. *** I'm being promoted and moving tons completely different business unit. Full MS stack. Looking forward to it.
I think I found a workaround, There were two calls to the same column. Removing the original full text column seems to work. While not optimal, it will suffice for now.
Postgres doesn't have a median function, unfortunately. You can try creating one though: https://wiki.postgresql.org/wiki/Aggregate_Median
I also have an oracleDB I could switch to. Do you reckon it'd be easier there?
For me, the actual SQL code is the fun part. I hate stupid ER diagrams with a passion. The part in the red box is simply asking you to make the SQL scripts that create the tables you have in your diagram. create table seller ([Seller ID] int primary key identity(1,1), [First Name] varchar(100), [Last Name] varchar(100), [Property ID] int , [Stated Price] money, [Agent ID] int, constraint fk_agentid references agents([agent id]), constraint fk_propertyid references properties([property id])) ... etc for all the tables, note you have to create a table before you can reference it as a foreign key which means the order you create the tables is important. Then populate the tables insert into seller ([first name], [last name]) values ('John', 'Doe') , ('Jane', 'Doe') , ('Jack', 'Doe') , ('Jill', 'Doe') , ('James', 'Doe') Note that your teacher wants 5 records in each table and the foreign keys will need to a) match their respective values in the foreign table and b) they will be required on your insert. So again, the order you insert is important. If you specify identity columns (as I did above) SQL will auto generate the ID values. The constraints will require you use a valid ID for each of the foreign keys. This matters because... The final part you will have to do on your own. Your teacher wants you to show you have an understanding of the database you built by providing 5 queries to get data. Obviously they don't want you just doing a `SELECT * FROM SELLERS` so they are requiring you to do JOINS. So you will need to get a bit creative like "How many properties has each seller in the deals table" and "Agents that have deals over $100,000" etc. etc. I completely made up these examples to give you ideas. Hope this helps. 
 I see agent in both the deal and the seller fields, is this not redundant? Also your field should be dependant on the PK. Why would the stated price belong with the seller rather than the deal? What would happen if the seller had multiple properties to sell? 
Pay attention to the last paragraph of this reply! First comment that mentioned the performance costs of INSERT, UPDATE, DELETE operations. I feel the other commenters did a good job explaining indexes with phonebook examples, though.
Firstly, well done on finding the relationships between the tables. You are off to a good start. You might want to re-visit the cardinality in your ER diagram as well, especially the one-to-many relationships as there are probably more than just between Seller and Agent. You may also want to indicate which columns are your primary keys and which are your foreign keys, and maybe even data types, depending on what your course has taught you. I also like annotating the ER diagrams with relationship descriptions, but once again, it depends what your course has covered :) Red Box: The first paragraph is asking you to create the tables via Create statements that you have drawn in the ER diagram, and will need to contain your primary and foreign keys as well. You might find this helpful https://www.w3schools.com/sql/sql_primarykey.asp The second requirement is to insert rows into your database, with at least 5 rows. There are a few ways you might be able to do this. You can directly insert values (W3schools above has examples on how to INSERT INTO), or you can INSERT INTO in combination with a SELECT statement if you have a database to copy from. The third paragraph is just after some sample SELECT queries, one that queries over one table and four that join between two or more tables. If you look at the Sql Joins section on W3schools, this should give you the right syntax to begin. Most importantly, you need to explain what your SQL Query is doing. Not sure if you have covered aggregations in your course yet (things like SUM, COUNT, MAX), or but these would be useful to have. E.g. What is the most expensive property ever sold by an agent? Hope this helps. 
It is being overly cautious, since to the parser you have exceeded asking for more columns than the current table has it so it is going to demand a table prefix to resolve that potential issue. 
Here is a graphic of the relationships as well: https://imgur.com/aMWgrqj
Saying "create an ER diagram" can mean a lot of things and it's best to read the wikipedia article. One hurdle that you need to get over is that an ER diagram is simply how the data relates to each other. You can make it visually simple or include ALL the details. Both are ER models. Here is how I envision a very very basic real estate company: https://i.imgur.com/jRZvCFH.jpg I've separated buyers/sellers/agents because each each can have a different role. A property has (typically) 2 agents. One is internal to the company but the other agent is not. Same with buyer/seller. An internal agent can represent a buyer or a seller. So it's best to just call it a "client". But what about the other party? That can be a buyer or seller as well. So I've used the names "internal" and "external" to differentiate. The entity they have in common is the *property*, and each property has a *contract*. I've used generic ID nomenclature to ensure uniqueness in each table, and I've included the ID for as a foreign key so you can do your joins. This homework assignment is actually very time consuming, but you need a good foundation. I think we need to discuss that first. Conceptually, how to you envision a real-estate office running?
I'm triggered by taking a phone picture of a screen.
Get out of here with this juvenile bullshit.
Change p_Out to record type.
Awesome, thank you so much for doing this, it helped a lot!
Additional con: updating/inserting/deleting data also requires that any indexes that touch that data also have to be updated, increasing the number of writes required. 
Phew - Firstly thanks to everyone that provided advice and recommended software to work with. Took the DTS packages and ran them through the migration wizard in 2008 R2, from that I received DTSX packages (had to faff about with installing DTS Designer Components and so on) Setup a SQL2014 Dev Edition and SQL Tools this morning and opened the DTSX package in SQL Data Tools and to my surprise 90% came through (the DataPumps are an issue but it seems that's quite common) - Has likely saved me many hours of work getting this far. I looked into using the DTSxChange trial to attempt the conversion but I haven't got the time to submit the demo request, if I ever need to do this in the future (and I have the time to actually scope what I am doing) I'll give them a go. Cheers all!
To add to this, since it's a little implicit, is the importance of adding indexes based on the way that you use the data. If you are looking up names in a phone book based on their numbers (you are weird) you might want to index on the numbers. Most people would want to index on the names, but the most important part is how you are using the data.
Yep. Totally agree. Just giving an ELI5 example. 
You could create a database in SQL Server Express which is a free licence. But, you'll either need to update or add data using SQL or create an application using Access.
Thanks! How do we go about users updating the data, though, if they don't have SQL knowledge? Would they have to update via Excel and the database would fetch that spreadsheet?
That could be handled by a stand alone application that someone would need to develop or maybe access. Excel may be able to do it. But, I'm not sure.
I would use the case statement, and use a sub select to match employees
This is something that I would do in C#, I would create a win form, so you'd really only need a data layer and would be quite simple
we move forward through time from july as we calculate the query. once a hotspot's score exists, it counts in the query until it is modified. so yes, the DivBy is all IDs that have shown up so far and DayTot is the sum of the latest score for each. the sort of thing you'd do trivially with a loop in the middle tier, but i'm thinking that this is an opportunity to get into window functions, albeit just a bit out of my league at the moment.
I’m totally fine with that. The problem is I don’t really understand the CASE Statement as of yet. I made a query earlier that pulled employee A, employee B, and A third employee but the employee was duplicated for each skill they had... then I got it down by just using LIMIT 3, but this isn’t really a decent solution and my query couldn’t handle if A &amp; B has the same skill. Another time I was cheating using MAX() but that doesn’t work well since one individual has all the skills, so MAX would just pull him every time. I’m perfectly fine with doing the bulk by myself, because this is the entire point self-learning SQL, but maybe a bit more of a hint on how to implement CASE here would be great. Thanks for your help!! 
A couple years ago I would point you to light switch but.... You can have an access form frontend connected to an SQL back end. Bottom line though, if you don't have experience in at least VBA and basic SQL, you won't be able to build the input forms.
Solved!
&gt; My question is what if I were to feed in 15 id's but only 2 existed on a day? day 1 - one ID has a score - DayTot is score of that ID. Ave is that over one day 2 - a second ID receives a score. DayTot is the total of those two. Ave is that total over two day 3 - a third ID receives a score. DayTot is the total of those three. Ave is that total over three day 4 - the second ID's score changes. DayTot is the total of ID1's score, ID3's score and today's score for ID2. Ave is that total over three.
I agree but have to add that usually all indices consume storage space. Way less space than the complete original data because you only index certain columns or leave nulls out but it still consumes. You are describing a index organized table.
Thanks everyone.
I do not really understand what you mean by "more columns". Some random remarks: Instead of (v_Sales - (v_Sales * .1)) you could do (v_Sales * (1 - 0.1)) which i personally find easier to grasp. Second you could do this all in a select or view: SELECT o.ord_nbr ,case when (oi.ord_qty * i.ord_itm_prc) &lt; 500 then oi.ord_qty * i.ord_itm_prc * 0.95 when (oi.ord_qty * i.ord_itm_prc) &lt; 999 then oi.ord_qty * i.ord_itm_prc * 0.93 else (oi.ord_qty * i.ord_itm_prc) * 0.9 end discounted_amount FROM ord o ,itm i ,ord_itm oi WHERE oi.itm_nbr = i.itm_nbr AND oi.ord_nbr = o.ord_nbr PS: Depending on the preferences of whoever is grading this you probably should use ansi-joins instead of oldschool oracle style
I figured you handle the actual recipe client-side. But if you wanted to store that final ingredient list for the recipe to access later, you could either go JSON (NoSQL), but if you're staying true to SQL, a RecipeXrefIngredients table that updates with the final list of ingredients as you swap out items for this instance of a recipe. You'd then also have a Recipes table.
very thorough and easy to read
Well that's damned interesting. Thank you.
Excel can easily pull data (Data&gt;From Other Sources&gt;SQL Server&gt;Follow the Wizard), but updating data is different. A VSTO is another option besides VBA. Or any other language really. There are a few products (SQL Spreads?) that I haven't tried which probably serve as VSTOs that can do this task. Access forms are OK but I don't like recommending Access when it seems like MS is going to moonlight it soon. Might be /u/IAmMaximus first foray into VB here.
I'm curious, what spurred the question?
Just trying to transform some data and my queries are quite slow so I did a bit a research and stumbled upon indexing. I'm still a beginner in T-SQL
Yeah, the updating part was my only reservation about saying Excel.
This is pretty much what I was trying to suggest. But you worded it better, succinctly. :)
Define "until it is modified" So you're saying if HotspotID ABC exists on July 1 with a score of 7, that you want to see it scored as 7 for every day after July 1 even though it never appears again? Also, wouldn't that 7 be contributing to the DayTot? What happens if the sum of scores is greater than any value in the DayTot column? Do you want the max value?
&gt;SQL does not tend to lend itself towards part time or side jobs like development can. Curious why you say this. I wouldn't disagree with you at all, but curious why you think its the case.
Ah ok, I was curious if it was a homework question or otherwise. I have a one hour video, so it's a little bit of a time commitment. I guarantee you will walk away with so much more knowledge and understanding of indexes than you would believe afterwards. It may take several watches to take it all in however. It's a million bits of information a second. https://www.youtube.com/watch?v=MpAKdy54Eqg
This looks awesome. Thanks! Any similar videos on this topic/SQL in general?
There are definitely part time jobs out there for SQL, but the reason there are few part time jobs is a major compilation of issues. I'll start with why it's rare for a junior to start. 1. Security clearances. You are working with data. This data probably has PII, PCI, HIPPA, or some other regulation. Maybe even not sensitive but semi sensitive information. So the person has to be morally trusted. This can include running expensive background checks. When I say expensive, I mean $5,000 to $50,000 per person. 2. If you care enough to hire people to maintain your data, even if you pay someone $2500 to be checked out, $50,000 in base pay, and $10,000 in their benefits, you are saying your data is worth at least that much yearly. It's probably worth even more than that though. What if the person breaks your data and cannot recover it? Does this crush your business? How much technical debt are you put in? So trust worthy experience and a dedicated specialized skill set is necessary. 3. Experience, not just with keeping things from breaking or fixing broken things, but creating things that can scale and follow best practices. The person needs experience to say, hey guys, let's not implement a trigger in our primary OLTP DB in production without testing it or caring that it creates an exponential join to the 8th power on a table with 200 million rows that also loops and triggers off itself. (Side rant... sorry.) You are paying to have that scalability and best practice implementation with a senior person. (Even if you don't always get it.) So that's why I believe you don't see junior positions. On a similar note, understanding the database end can be pretty time consuming. I have seen a few basic databases. Odds are though, if they have basic problems, they have one person or a regular IT guy who can google and resolve it. If it's more complex, it's probably not a job that can be solved in 20-34 hours a week. Or if it is, it will take years. So either the timeline is not a priority or the job is not that big and they probably have the resources for it already. If the job is hard enough to where they need to bring in a consultant at all, it's probably a major project that will be a time consuming task. You are more likely to see 6 month to 1 year full time contracts than part time. And this is probably for good reason. My last place required 6-10 TB tables for fast loading and searching. Creative indexing, partition maintenance, etc was crucial. There was another database that ran business logic in the DB. It had over 10,000 objects (functions, procedures, triggers, views, tables, etc) that were involved. It was not a minor task to learn that system and it did take at least 6 months to get a good understanding. I can see BIML template making, report monkey work, or basic architecture planning being a part time gig. This would be in large cities though with major tech hubs and odds are there are people doing 6 month to 1 year contracting work there who are also on the hunt for low effort 5-20 hour weekly job contracts similar to what I stated. I'm sure there are the exceptions, but this is the kind of environment and trend I've been seeing locally in my State. 
I realized now I don't know what RDBMS you are working with, but I'll assume SQL Server because that's the only one I can give good advice on. For SQL Server, I'd check out Kendra Little, she has a podcast and a few good episodes on indexes too. If you find anything from Brent Ozar, also a good resource. Pinal Dave and MSSQLTips / SQL Central also have great articles.
I see, so you were more speaking from a DBA perspective. I work in 'database development' but on the analytics side of thing and finding part time work isn't terribly difficult to do. Totally agree with you with respects to someone in a junior role finding part time gigs, I thought you were more broadly commenting on the industry as a whole. &gt;It was not a minor task to learn that system and it did take at least 6 months to get a good understanding. Agreed here. I've been in my current role for about a year and I am just now sort of feeling like I have a grasp on our database, but it is no where near as complex as the one you describe... it's more the underlying data and how its being used, etc &gt;I'm sure there are the exceptions, but this is the kind of environment and trend I've been seeing locally in my State. I would tend to agree, although with analytics I see roles pop up now and again, however they tend to be extended to people who have experience. e.g., you worked somewhere for two years and learned their systems, then moved on to a new job and they offer you part time work. 
As /u/alinroc said we ended up re-writing pretty much everything when we moved from DTS to SSIS. I didn't do most of that work though so I'm not sure if it was purely because we had to, or if they also wanted to change things as well.
Until modified - that is until a new record appears for hotspotid in the table Yes, once a hotspot has a score, that is its score, from the querys point of view, on every subsequent day Yes, 7 is contributing to daytot. That is the point Your last questions make me think maybe I'm not explaining the point of the exercise well. It's to calculate how average scores change over time. I realise I've abstracted this to a method for working it out, which I dont know how to implement yet but the idea is simple. Order the history data in datetime order. Every time a hotspot score changes or becomes available for the first time, recalculate a new average. In the case of multiple scores for the same hotspot on the same day, ignore all but the last, to avoid a blip on the graph. Daytot is the sum, as at a particular day, of the latest scores of all hotspots for which there is a record in the table by that day. 
One more piece is that the unordered phone book is a lot quicker to enter data into
[removed]
What I'm saying is... how do you know that the Maximum DayTot in your one table is always going to be larger or equal to the sum of the ID's you're working with in the other table?
&gt; I would tend to agree, although with analytics I see roles pop up now and again, however they tend to be extended to people who have experience. e.g., you worked somewhere for two years and learned their systems, then moved on to a new job and they offer you part time work. That's more likely of what you can find. Even analytics runs into the issues of security. You are using data, data is sensitive and expensive. Do you want to pay minimal cash for a hired hand or would you prefer a dedicated resource that can stick around after paying for checks and such? It depends, as always. I feel that's why there's a substantially minimal part time market for SQL. Maybe as a web dev with a background in SQL. There's less sensitive and easier or quick things you could find with dev. 
Sorry for slow reply. Just based on googling it looks like Oracle does have a median function, so that might be easier.
In your insert, you're `TO_CHAR`ing everything, even fields that are defined as an INTEGER. See **** comments below. INSERT INTO PRODUCT_DIM --CREATE TABLE PRODUCT_DIM ( --( PRODUCTKEY -- PRODUCTKEY integer NOT NULL, , PRODUCTID -- PRODUCTID integer, , PRODUCTDESCRIPTION -- PRODUCTDESCRIPTION VARCHAR2(50 BYTE), , PRODUCTLINEID -- PRODUCTLINEID integer, , PRODUCTLINENAME -- PRODUCTLINENAME VARCHAR2(50 BYTE), ) -- CONSTRAINT PRODUCT_DIM_PK PRIMARY KEY (PRODUCTKEY) --); SELECT PRODUCT_KEY_SEQ.NEXTVAL -- NUMBER , nvl(to_char(p.PRODUCTID), 'Undefined') -- CHAR ******** , nvl(to_char(p.PRODUCTDESCRIPTION), 'Undefined') -- CHAR , nvl(to_char(p.PRODUCTLINEID), 'Undefined') -- CHAR ******** , nvl(to_char(pl.PRODUCTLINENAME), 'Undefined') -- CHAR FROM PRODUCTLINE_T pl, PRODUCT_T p WHERE p.PRODUCTLINEID = pl.PRODUCTLINEID; This would fail if you tried to insert a null PRODUCTID, because it would be `NVL`ed to 'Undefined', which is a string and can't be stored in an INTEGER field. Even if you succeed in inserting a record with a populated PRODUCTID, you're explictly converting to a VARCHAR2 with the TO_CHAR, then Oracle is implicitly converting back to an INTEGER to do the insert. Which is a negligible but silly waste of processing power.
What I have found is that analytic developers can cost quite a premium which is often beyond the budget of a lot of smaller companies, so there is a niche there for small contract orientated side jobs. &gt;It depends, as always. I feel that's why there's a substantially minimal part time market for SQL. Would tend to agree, just thought it was a good conversation and curious about your reasoning. I'm not sure the data sensitivity is quite the reason for it though, because quite honestly most companies I've worked for/with are rather lackadaisical with their data. I think cost is one of the prohibitive factors. 
What did you use to generate the relationships graph?
That came from some included plugin from [JetBrains Data Grip](https://www.jetbrains.com/datagrip/). It isn't very user friendly for doing anything custom with the graphs but it is quick and works for in a hurry stuff - as long as your tables have all the keys and stuff in place it seems to get it right most the time.
Are we clear that the other table is the table we are trying to produce?
We are now, lol. I won't have time to look at this until tomorrow at the earliest, but probably Friday/Monday. Kind of swamped now.
Ok, so to be very clear... you have (1) table such as | ID | Created_Date | HotSpotID | Score | | :--- | :--- | :--- | :--- | | null | null | null | null | This output is produced using this query: SELECT hi.id, (SELECT LEFT(hi.created_date, 11) AS DATETIME) as created_date, hotspotID, hi.score FROM history hi WHERE hi.hotspotID IN(384,385) ORDER BY hi.created_date Then using the logic we discussed you want to produce the DayTot, etc. And to be extra clear about the DivBy.... on day 1 the DivBy is 1 because only 1 hotspot existed on that day, even though you have (2) in your IN(). On Day2 it becomes 2 because the second one pops up, and from there on out it will always be 2 and never go back to 1, nor could it ever become 3 because in your example you are only searching for (2) in your IN() -- but if you were searching for (n) number of ID's then it could get much highier. DayTot is a sum of the scores, either that days score, or the most recent score?
I assumed a recipe table, since OP asked about one in the second-to-last sentence. Maybe he can clarify... OP, did you intend to have a database table for recipes, or were the recipes going to be stored elsewhere?
Sounds like ETL and not replication... use SSIS, it'll copy in bulk/batches instead of record by record (much faster than a simple TSQL select/insert).
If you don't need to track things like change history then my go to is an SSIS job with 2 steps. SQL Task (with truncate script). Data flow (set to bulk insert). For best performance, don't make any loops/containers, just let SSIS handle the parallel execution.
yay! now, i'm sure i could produce the data from my graph using subqueries, but, and i imagine you're like me in this regard, i hanker after the elegant approach. do you think it's possible?
Just want to point out - for SQL Developer - that Jeff Smith (https://www.thatjeffsmith.com/sql-developer/) is the program manager for SQL Developer at Oracle and is fairly responsive on Twitter. 
what is the error message? your sql looks okay to me by the way, you do realize the query can be greatly simplified and doesn't really require a subquery?
Oh for sure. I'll play with it.
Thank you! That makes more sense. Yes there is a simpler way but I just wanted to follow the text. I did notice one thing. When I run your query, the 9 row results are are put into a nice table. When I run my query, I still get 9 row results but not in a table. This is what it looks [like](https://imgur.com/a/ee8GS). Does SQL server not handle well subqueries? 
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/gwgL8A5.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
https://i.imgur.com/Gv6xa6C.jpg I'm at the bar and am intrigued in your problem. Is that how the tables are set up?
What’s the practical difference between coalesce and NVL? 
That looks exactly like how the tables are structured. The intersect statement looks cool - I haven’t even seen that before. I’ll have to play around with it and hopefully not too complicated. 
When I get home I will create the tables and try it. But the thing is you need to have static empid to check
I've been in the Oracle/sql/etc/dev industry for years, and after all this time I pretty much resort to 5 software programs: Putty - because your oracle shit better be running on some sort of *nix platform, right?? sql*plus - not for writing select queries but for many other one-liner dba tasks when I'm constantly switching between sql and shell tasks. SQL developer - I write the majority of SQL here then either FTP into *nix environment or copy/paste through putty/vi Notepad++: Best text editor for Windows, evarrrrr. I wish the SQL Developer code editor had even a quarter of the features NP++ does. At minimum it should at least support "alternate" highlight, delete, and insert for fixed-width text, but it doesn't. NP++ also highlights SQL syntax for some pre-validation before sticking it SQL Developer. Excel - You don't really need it but sometimes it comes in handy for mass editing due to quick and easy string manipulation functions. Also good for dumping SELECT data from SQL developer directly into Excel for some sanity checks as you can quickly sum/average/count data or quickly pivot data without doing a bunch of work. Also, honorable mention for Access. With ODBC you can created linked tables to Oracle and quickly join tables for quick queries rather than type all that shit out in SQL. Fairly limited of course. You're not going to do any partition over() stuff or any advanced code here. 
He is active on reddit too.
+1 for NP++ Do you know of a way feature/plugin to switch between documents on NP++. I know you can enable a setting to list all open documents vertically on the left. I was hoping for a search box like feature where I type in the file name partially and it does a search.
No rush. And yeah, the employee IDs are all static. 
yup, happy to answer questions here if you have them 
Great list. I love Notepad++ and also use Excel a lot for what you've mentioned.
Code academy has some pretty good ones. I think the intro course is free then you have to pay a subscription for others. I thought a month subscription was worth it. 
Yeah I'm not really sure how to efficiently check if an empID has skillIDs that two or more others have... I'm almost sure we could use predicates to intersect if an empID has skillIDs that different distinct empIDs know, but I don't know the optimal way to do that. I'm going to try more at work tomorrow because I really want to know this also.
Are you able to create a transactional publication/subscription? If so, you can select specific tables (articles) and filter out data to a different database. Almost instant replication. If you want it updated daily you can set it up as a snapshot. 
I recently started using DBeaver, free and it will connect to damned near anything. I currently use it for ms sql, teradata, aster, Oracle. It has an object explorer like ssms and sql developer with all of your server connections etc. And it has a connection filter, so if you have a lot of schemas you can black list or white list specific stuff. I recently started using sublime text as well and like the interface more than np++, but I do like np++ a lot.
 SELECT * FROM Associative AS A2 JOIN Employees AS E2 ON A2.empid = E2.empid JOIN Skills AS S2 ON S2.skillid = A2.skillid WHERE A2.skillid IN ( SELECT A.skillid FROM Employees AS E JOIN Associative AS A ON E.empID = A.empID JOIN Skills AS S ON A.SkillID = S.SkillID WHERE E.empID = 2 OR E.empID = 3 ) AND ( A2.empid &lt;&gt; 2 AND A2.empid &lt;&gt; 3 ) So I'm sure this solution is extremely inefficient for the query execution plan but I think it works. Change 2 and 3 for empid to 5 and 11 for your example and see if it works. Obviously it would be better to create a function/procedure with parameters and only have to execute it with the two numbers
Sololearn had a good SQL tutorial or go to tutorialpoint and find SQL tutorial
https://sqlzoo.net/
Does your workplace partner with Lynda.com? Lots of good stuff there.
How about the Google sheets Analytics add on to dump out the analytics data to a google sheet and feed that to MS SQL? Total shot in the dark too. 
NVL is not standard SQL and doesn't work in all databases. NVL needs exactly two arguments, not an arbritrary number. COALESCE has more keystrokes ;)
Didn't know SQL server supported ORDER BY .. NULLS FIRST/LAST. Will use. IS [NOT] DISTINCT FROM will also come in handy, once I figure out what it does.
&gt; IS [NOT] DISTINCT FROM will also come in handy, once I figure out what it does. Did you follow the link to the other article: http://modern-sql.com/feature/is-distinct-from It's explaining `is not distinct from` more precisely. Also: SQL Server doesn't support it and has AFAIK no convenient alternative.
Ah, I must've misread it. Thought SQL Server supported it.
In Oracle, I think they work pretty much identically, but if you find yourself working in SQL Server, there's an important difference between ISNULL() and COALESCE(): ISNULL() always returns the type of the first argument even if it's a bad fit for the second argument. If your first argument is varchar(5) and your second is varchar(20), that varchar(20) is going to get truncated to 5 characters. COALESCE(), on the other hand, uses the normal highest precedent type resolution. It's an easy quirk to miss, and can cause you all sorts of grief if you weren't aware of it. My preference is to only use ISNULL() for arguments who's types don't have length or precision components, or when the second argument is something universal like '' or 0. Otherwise I'll use COALESCE() just so I don't need to worry about the underlying types getting adjusted upstream.
it's got nothing to do with subqueries every query produces a table -- whether this is immediately apparent depends a lot on the user interface you're using
I had to switch from Toad to SQL Developer which was fine. The only thing i really miss is the "edit &lt;table&gt; where ..." command. It would also be really great to have a timestamp in front of the messages in the "Messages - Log"
[Hackerrank](https://Hackerrank.com) is decent. My suggestion is to take notes of solutions that you don't understand. Eventually you'll end up with the tools to solve most problems if you understand each solution.
Hi, I recently graduated and started working in the field since september I'm also from Belgium If you wish you can HMU in the DM
I’d ditch the Home and Away columns and replace it with a single status column. I’d also like to have another table with that status and some key constraints so only valid statuses can be used.
In the event of Employees 1/2 each having multiple skills, should Employee #3 have all the skills of Employees #1 and 2? Let's say Employee #1 has skills (1,2) and Employee #2 has skills (1,3), Employee #3 must have skills (1,2,3), correct?
Many libraries also subscribe to it. If they do, use your library card to get free access. They have a nice app for iPads and you can download the video courses.
Interesting. I always enjoy these projects. The cool thing about SQL is, at least in my opinion, there is no definitive right way to do it, just solutions that are more efficient than others. So with what you have outlined I'm going to try to turn it into a bit of a word problem to make sure I understand what you're going about. I spent a semester in Sydney so at least I can say I know what AFL is... I even went to a couple of games. Alright so let's start at the top. There are multiple leagues. In those leagues you have teams. Those teams have names. Teams play games against other teams who may or may not be in the same league. Teams have players who play in games who produce traceable stats that occur during the game. My first correction to your diagram would be to connect the game table to the team table, not the team list table. Team should be the root table of the whole thing IMO. You're tryin to track stats by year/season. I would add this information to the game table. You will reduce the complexity this way and eliminate the need league by year field in your AFL table. When you want to compare stats by year you will put the constraints on the game date in game table, or even group it by that field summarize averages by team year/month over year/month. If you're concerned about linking a game to a season, you can join tables using the between function. So you could join the game table to a season table where the season table as a begin date and an end date. The table Joi would be something like join table season on game where game.date between season.begin_date and season.end_date. I would mark this up more, but I'm on mobile sir forgive me. Player stats looks good, but you could go further if you want. For example in American basketball, they track where players shoot from, the time of the shot and the success of the shot. Those are time stamped and not a summary of the game. If you were insane enough or found a decent data source you could track the same. You could create another table thank links to the playerstats table with a time stamp that would allow you to track when and where a player scored from or any other major noteworthy events. Just join this table to the player stats table with the player's Id. Player table also looks good. If you want you could add calculated fields to it so the table is always updating summaries of the player stats. For example you could add a table that summarizes points scored to date this year vs points scored in the same period last year. These calculated fields can be taxing on resources but are still a great thing to learn. Also your data set isn't particularly massive. Let me know if you want a further explanation of any of this. I'm sure others will have different ideas. 
Can you clarify what constitutes a "duplicate account?" For the purposes of this discussion, should we assume that Person = Account? As written, your join condition between Person p and Person p2 may not return what you want, as it isn't too far of a stretch to consider two people with the same first and last names may be completely different accounts. Also, you don't need GROUP BY if you're not using an aggregate function in your SELECT. As far as tacking on the duplicate accounts, that sounds like a job for my bff CROSS APPLY. 
Basically this will be a query that will output a list view for system users. I agree that the other person record may not be a "real" duplicate. However, if I'm able to also show the duplicate person record, along with the main record, users will be able to "easily" open the person records and if it's essentially the same, go ahead and merge the multiples. That will then drop the main person record off the list view. I'll have a gander at cross apply. Not too familiar with that. Thank you.
I wonder if it will be an advantage for you if they are asking you to show your SQL ability on a piece of paper? Personally, I would prefer to do it on a computer but do you think that because they are asking you to do it on paper it will be more simple? 
That's a good question. Does anyone here have any input on this? 
Start writing out basic SQL statements on paper. We get used to thinking and doing things in a certain context, when we get outside of that context, stuff that would normally be easy, is no longer quite so easy. Throw in the stress of being in an interview and it gets even tougher. So get some practice in.
Okay so it's an interface problem due to large amount of text in the Preparation column. I was just wondering because with your query, the column is automatically shrunk. I'll look into the settings. Thanks again. 
what's a teamlist? and a team can be on only one teamlist?
Schemaverse.com - space battle played with SQL
It's not a programming language per se. It's a Domain Specific language. 
https://sqlbolt.com is nice
something like this? /* create table temp_emp( empID integer); create table temp_associative (empID integer, skillID integer); insert into temp_emp values (1),(2),(3); insert into temp_associative values (1,1), (1,2), (2,1), (2,3), (3,1), (3,2), (3,3), (3,4) ; -- update temp_associative set skillID = 2 where empID = 2 and skillID = 3 */ select e1_empID, e2_empID, e3_empID, min( Emp1vs2), case when min( Emp1vs2) = 1 then 'Un-matched skills' else 'Matching skills' end as Emp1vs2_agg, case when max( Emp12vs3) &gt; 0 then 'Not covering' else 'Covering skills' end as Emp3vs12_agg from ( select emp1.empID e1_empID, emp2.empID e2_empID, emp3.empID e3_empID, a.skillID, sum( case a.empID when emp1.empID then 1 when emp2.empID then 1 end ) as Emp1vs2, sum( case a.empID when emp1.empID then 1 when emp2.empID then 1 when emp3.empID then -5 end ) as Emp12vs3 from temp_emp emp1 cross join temp_emp emp2 cross join temp_emp emp3 join temp_associative a on a.empID in (emp1.empID, emp2.empID, emp3.empID) where emp1.empID &lt; emp2.empID and -- dont care to compare to himself and dont need AB, BA emp3.empID not in (emp1.empID, emp2.empID) -- dont care to compare either of the original ones group by emp1.empID, emp2.empID, emp3.empID, a.skillID )t group by e1_empID, e2_empID, e3_empID 
1. I'm not familiar with how to handle variables in MySQL. But, don't you need to declare a JoinDate variable and populate it with a user's JoinDate value? 2. Wouldn't youre proc need to accept a parameter so that it knows what user\viewer's record to process? 3. Why are you performing DDL (ALTER TABLE) in a procedure? Are you trying to update existing data or insert new data?
I think this is more of a Sharepoint issue than a MS SQL Server issue, much less specific to MSSQL 2014 or any other version. Have you tried any Sharepoint related subreddits or asking the question on StackExchange or similar?
I posted on /r/sharepointdev and cross-posted to /r/sharepoint but not response quite yet. Thank you for the quick response, I didn't believe it to be a SQL problem, but someone may know both sides, so just trying to get visibility on it.
&gt; We get used to thinking and doing things in a certain context, when we get outside of that context, stuff that would normally be easy, is no longer quite so easy. Throw in the stress of being in an interview and it gets even tougher. ^ this I think they're trying to REALLY test your SQL prowess if they want you to do it on paper. I've been on the other side of the interviewing table where I've asked people to write out queries because I want to see how they're going about doing it - out of your element - with no intellisense.
If AFL is anything like NFL, then a player can with any number of teams over the years and even within a single year - I dont see how TeamList/Player entities will support that. There are also various 'sub-team' - injured reserve, practice squad, etc. I'm not sure what the overall structure of the competition is, but you might need to designate regular season games vs pre-season/friendly games and playoff parts. An important part of the game setup is also who are the starters/who's on the injured/scratch list. 
Have you determined how you're going to obtain the data to make it available for loading for ETL?
I don't think the tabular results format in SSMS is going to show multi-line text. So, what you're seeing is a truncation of that field. You may be able to see it if you send results to text in SSMS, instead. Its a presentation issue, really.
You don't need to make each habit a table. Just have a HabitTypes tabled (HabitID, HabitName, HabitObjectID). Change your Habit column to reference the HabitID instead of text. Take objects out of that table too. Now make a HabitObjects table (HabitObjectID, HabitID, HabitObjectName). HabitObjectID + HabitID is the pkey. Now your HabitTypes 1:N to HabitObjects. Now each Habit is unique, and each Habit + Object combo is unique.
I've done this. Final approach was to have a separate console application pull the data from the analytics API and insert into SQL tables. This had the benefit of transposing the data as necessary and limiting to only necessary information.
This command: SELECT * FROM Docs WHERE (VirusStatus &gt; 0) AND (VirusStatus IS NOT NULL) This returns all (*) of the columns for all of the records in the "Docs" table where the value in the VirusStatus column is &gt; 0 (and not empty/NULL). It does nothing else except return data. You are correct that the newer versions have an "Alldocs" table (but still a Docs view that refers to the Alldocs table). https://msdn.microsoft.com/en-us/library/hh656481(v=office.12).aspx What do you get when you run this query? SELECT * FROM AllDocs WHERE (VirusStatus &gt; 0) AND (VirusStatus IS NOT NULL)
This seems to be the only way to automate it.
In my experience, if they are having you do it on paper, it is more to show you understand the concepts of SQL and how you go through translating the business request into a query. They are likely not as concerned with the exact syntax, so don't fret that your code should to be able to be compiled and executed on the fly.
In afl the game are very rigid, for example all player movement for next season was handled within 1 month, there is no trading or drafting within a season
I was just confused because both queries give you the same result set but one is like you said, truncated. Okay I’ll try doing that. Thanks! 
Pretty sure that's the problem that needs solving. OP will probably also run into API limits if they are using the free analytics...
Is it your print statement output getting truncated? Try pulling the RIGHT 100 chars... 
That returns the characters I would expect at the end of the string. So I assume it's definitely the PRINT. just strange it's capping at an odd number. 
It's a limitation in SSMS, you're is probably set to 12500 or there abouts... For SSMS 2016, go to Tools -&gt; Options -&gt; Query Results -&gt; Results to Grid and check the Non XML data value. The hard limit is 65535. If you need more than that, I have a trick for you :) In your code, change PRINT @SQL to SELECT @SQL AS [XML_F52E2B61-18A1-11d1-B105-00805F49916B] That will let you spit out as many characters as you want. The output to the Grid will be a single cell with a hyper link, and will open a new query window with the label of XML... and will have all the output you need.
Thank you very much. Worked a treat!
cross join
Very ambiguous answer, but it worked, I had to do some crazy sql to get my table to cross join itself but it duplicated the correct number of times and with some row_numbers inside and outside the sub-select I was able to have names show where inner-row-number = outer-row-number. Thanks!
CONCAT(vendor.name ,' [' , vendor.number , ']')
yeah but it says 'Column or global variable [ not found' It's like it won't allow me to add anything other than a valid field name. Are there any other SQL commands for joining field names and strings other than CONCAT?
The &amp; or + symbols could work. It sounds like Excel is mangling the SQL before it passes it on. Since you're doing it in Excel anyway can you just do the concatenation with Excel formulas instead?
Tried &amp; and + already. I'll try some more variations. We could just make a column but the data ends up in a pivot table. The pivot table has been a bitch to setup so they were hoping not to have to remove the VN column and add another one in and format it correctly. 
The idea was that the team holds a bunch of basic data relating to the organisation itself. Where as the Teamlist is where the players are assigned to a list for the given season (quirk of the AFL, no in-season trading at any point) as defined by the AFL table.
Sorry to belabor fauxmosexual's question, but did you try EXACTLY what he suggested, with single quotes around the strings?
Yup
 select * from ( with sample_data(TestIDNumber, TestType, TestMode, TestResult) as ( select 1, 6,'C','I' from dual union all select 1, 6,'C','I' from dual union all select 2, 6,'C','I' from dual union all select 2, 6,'R','I' from dual union all select 3, 6,'R','I' from dual union all select 4, 6,'R','I' from dual union all select 5, 6,'A','I' from dual union all select 6, 6,'Z','I' from dual ) select TestIDNumber, TestType, TestMode, TestResult, count (distinct TestMode) over (partition by TestIDNumber, TestType, TestResult) row_count from sample_data WHERE TestType = '6' AND TestResult = 'I' AND TestMode in ('C' ,'R') ) where row_count = 2 ;
You could try using a string function. SELECT string1 || ‘ ‘ || string2;
Thanks! I'll give that a go.
Sweet let me know if it works, may need to look at a few examples before testing her. I do believe thats the sqlite version so may be different in others. 
Maybe cast vendor.number as a string?
Not sure if you’re suggesting to use pipes to Concat, but that doesn’t work in MSSQL like it does in other RDBMS. MSSQL uses “+”
[QUOTENAME](https://docs.microsoft.com/en-us/sql/t-sql/functions/quotename-transact-sql) Or [FORMATMESSAGE](https://docs.microsoft.com/en-us/sql/t-sql/functions/formatmessage-transact-sql) 
 DELETE FROM Vendors should that be VendorsTable? It's not that simple, surely. 
[Updated Schema](https://imgur.com/h4FS8Ps) [DataSource](https://afltables.com/afl/afl_index.html) Hello! Thank you very much for the enthusiasm and insights, I have added 'more' stat fields and a method for tracking score progression, I think it works in my head, however I feel as though it will be complex trying to query the player stats and the 'scoring progression' table as you suggested. Could you please have another look to ensure there are no new glaring issues. Cheers mate. On a side note, what games did you go to while in sydney, did you enjoy them and do you still watch afl? 
What is this sorcery?
Interesting, in that sense, what do you believe I should know for a Senior Analyst role that utilizes SQL to find fraud within a company? I doubt they will ask me to modify the tables or insert and data since I'm assuming everything will be pulled from a database. 1) Obviously understand the SELECT / FROM / WHERE 2) ORDER BY 3) GROUP BY 4) Aggregate functions such as SUM(), COUNT(), AVG(), MAX() 5) AND / OR 6) IN(), NOT IN() 7) Querying in subqueries 8) LIKE / NOT LIKE 9) HAVING 10) AS function to rename column header 11) CASE - I'll get a better understanding of CASE function but I don't see them asking me to write it on paper but I'll be prepared. 12) JOIN / ON - along with inner join, left outer join, multiple joins etc 13) ANY / ALL Anything else you think I would be missing? 
I doubt they will ask me to modify the tables or insert and data since I'm assuming everything will be pulled from a database. * Obviously understand the SELECT / FROM / WHERE * ORDER BY * GROUP BY * Aggregate functions such as SUM(), COUNT(), AVG(), MAX() * AND / OR * IN(), NOT IN() * Querying in subqueries * LIKE / NOT LIKE * HAVING * AS function to rename column header * CASE - I'll get a better understanding of CASE function but I don't see them asking me to write it on paper but I'll be prepared. * JOIN / ON - along with inner join, left outer join, multiple joins etc * ANY / ALL Anything else you think I would be missing that I should learn more about? 
I know, right?! I didn't discover the secret. I was hitting the same character limitation on an SP I wrote to generate restore scripts from the backup history tables. I don't recall the article I came across that showed me this magic... but I'm guessing that SP_WhoIsActive from Adam Mechanic uses this as well.
You might want to add EXISTS/ NOT EXISTS to that list. But, being able to understand the problem at hand and demonstrating it is going to be more important. 
What you want is a table value function. On mobile, but if you Google that phrase, you should get what you need.
Thanks for your reply. It seems it would work but requires all views being collapsed into a big nested select statement? I'm just worried about maintaing a very long function like that, but if that's the only way I'll have to go with it.
Pretty much this
You set &gt;sql_error= True And never ever handle that afterwards. Of course, since it's always true until the end of time, you will always rollback and never commit! Hope that helped.
Uh, you can use this in a pinch to test your code: http://sqlfiddle.com/#!4 But just making a post that NOTHING WORKS in all caps isn't going to get you anywhere. The only response I can give to that is OK.
MySQL statements always end with a semicolon, though. Whitespace isn't enough. That means his statement is: declare continue handler for sqlexception set sql_error = true; 
What output, if any, do you get?
This sounds interesting.. but is it actually usable right now? 
I see, looking in Mobile threw me off. Regarding the documentation of the handler, there is usually a begin after declaring it for sqlexception. Perhaps the scope is getting thrown off to not having that? Or is it assumed to be there for set SQL error= true?
"AS function to rename column header" Aliases aren't really just to rename column headers. They're to make fields more readable, used when you're using the same table more than once, used when combining fields, used to make writing your query a bit easier and quicker by reducing long table names to something shorter and quicker to type. "CASE - I'll get a better understanding of CASE function but I don't see them asking me to write it on paper but I'll be prepared. " Case statements are just IF THEN ELSE statements. The syntax is like this. CASE WHEN Thing=otherthing THEN 'True' ELSE 'False' END You can have multiple conditions like this CASE WHEN Thing=otherthing THEN 'True' WHEN Thing=schrodinger THEN 'True and false' ELSE 'False' END There's not much more too it other than if you don't provide an ELSE it will just return NULL if no conditions apply. 
`SELECT ProductName, Lot, DATEDIFF(day, MAX(TimeStamp), MIN(TimeStamp)) as TimeDifference FROM Table GROUP BY ProductName, Lot` The GROUP BY clause is the important part for you. Sorry for the poor formatting, I'm on mobile.
sry, i typed that in wrong. updated the code above to reflect. 
I think you're on the right track with your new design. It's very difficult to get things right the first time, especially if you're new or still learning. My advice would be to just dive in. You can always transform data into new schemas with views later. I think you should start where you feel comfortable. As you learn you will feel compelled to go deeper. If you start at too intricate of a data level you might get discouraged or not get the results you want, either of which would defeat the purpose of your project. I don't want that to happen. On the surface I don't see any glaring issues, but that doesn't mean there won't be any. Also, tracking stats at a deeper level isn't fully necessary. You could do it on a per game basis, which from your diagram seems to be the direction you're leaning. The tedium of entering a time stamp for everything would difficult and time consuming to say the least. There are teams of people who work in sports stats that maintain these types of things for companies like ESPN. For personal use you can get away with lesser levels of detail. I would ask yourself what information you want back from your data base and how deep you would like to go. I was merely pointing out there there is a whole other level to these things. Don't feel the need to go there! I really like the changes you made regarding the team list, players, and season. I think that will work really well for you. You will easily be able to track player trades between seasons while maintaining their stat history for the teams they played on previously. I would argue, however, that perhaps the Quarter table is not necessary. You could simply add a Quarter_Int to either the ScoringProgression table and or the PlayerGameStats table. As for the other statistics in the Quarter Table, could those be calculated based off of any of the other stats you're collecting elsewhere. If not, keep the table, for now anyway. As an example of what I mean, it looks like you want to be able to calculate Goals at Home by Quarter. You could write a query/view to accomplish this simply by querying the scoringProgression table and grouping by the Quarter_Int I mentioned earlier and filtering on points scored at games when the team is at home. The general idea behind a database is to store as little information as possible, but at the same time be able to answer any question. Always ask yourself, could this field be calculated with information from another linked table. If it can, calculate that information in a view or query later. From the Data Source you linked, I see that that you have some nice data to work from, but it has already been summarized in an 'excel like' way. There's nothing wrong with this, especially for what you're doing. However, there is a more 'proper' way of structuring and storing this data. Don't feel the need to do this, I'm only going to go over it for the same of learning. Basically you would create a table with your keys, so idPlayerStats_Int, Player_pidINT, and game_GameidINT. You would then add two more columns, stat_id, and value. Stat_id links to another table that contains a list of stats you would like to track. Lets say for example tackles. When a tackle happens, a record is created for that player/game/time combination with a stat_id that corresponds to tackles. Tackles, unlike other stats, typically only occur in increments of one, but if you were tracking in on a per quarter basis perhaps the player might have more. When you want to return this data you would essentially summarize the table grouped by stat_id and the query would return the sum of each stat limited by whatever constraints you might apply (season, game, player) ect. This table structure is a bit difficult to grasp at first, but in the end it is much, much easier to work with. It's also the 'correct' way to store this type of data. As for why, think about it like this. Anytime a player plays in a game but doesn't actually produce a stat for a given stat type, you're wasting space. Why, because you need to write a record that shows the player as having received one tackle in that quarter even though he might not having any other statistics in that same time period. So now you have over a dozen empty stat fields taking up space in your database that are otherwise useless. The method I described above only produces a record when a stat actually happens, so it would save lots of space over time. It's also much easier to query against in the end. Now, given the data set you're working with, I'm not sure you can transform it into the database design I just described. So my post is more of a if you had a perfect source of data lesson. I do see that your data source does provide goals by time, which is cool: https://afltables.com/afl/stats/games/2017/012120170326.html That seems to be the only stat the do that, and looking at your diagram you've mirrored some of your structure off of that to be able to ingest that data. So ignoring everything I said above, how would you query the current design as you have it spec'd out? Start with your criteria and overall granularity. Are we talking about a player or team. A game or a season. For the sake of simplicity, lets go with one player in one game. Lets say you want to see Matt Crouch's stats from his last game. You select the stats your interested in from the playerGameStats table, join the other tables, and in the where clause limit it to his player_id and the game_id you're interested in. This would return his stats for that game. But now you want to see when he scored. It can be difficult to combine the rather static stats from the playerGameStats to a table with multiple entries that you need to summarize. If you join the playerGameStats table to the ScoringProgression table as is, you will get a cartesianed result. This means the playerGameStats will be repeated for each record in the ScoringProgression table that matches the join. So if Matt Crouch scores in 3 of the 4 quarters, then a query joining these two tables summarize by quarters would return three lines. His points scored in each quarter would be represented accurately, but because the information joining each of those lines to the other table is the same, it would pull in his other stats in three times, which would be bad for summary purposes. There are ways around this, but they're less than ideal. For that reason I would build queries that target those stats separately (much like the website you linked does). If you go down to the quarter level, don't include stats from PlayerGameStats. However, what you can do is create virtual fields that summarize data up to the PlayerGameStats level. For example you could sum all points scored in each of the quarters. SELECT *, SUM(SELECT SUM(SCORE) FROM SoringProgression WHERE PlayerGameStats_Id = PGS.PlayerGameStats_ID and PlayerStats_Id = PGS.PlayerStats_Id and Quarter_QuaterId = 1) FROM PlayerGameStats PGS The above relies on a sub-query, which is fine, but isn't as efficient. You could write sub queries that summarizes stats for each quarter this way. The tables would join based on the same criteria you've already set forth. Note the table alias. That's how you join items in sub-queries to the tables not in the sub-query. If you've made it this far in this wall of text, I'm impressed. Databases are a hobby of mine. I really enjoy building databases, and I think your project is rather cool. This post is essentially the equivalent of drinking from a fire hose if you're at all new to databases. I'm happy to answer any questions you have. Like I said I think your project is neat. As for my time in Australia, I went to games in Melbourne, Sydney, and Wollongong (where I was at school). I can't for the life of me remember the teams I saw, but I would imagine they would have been the local ones at the time. I found the sport vastly more entertaining than our version of football. American Football has way to much stoppage in play to keep my interest. AFL is like American Football and Soccer had a baby, sprinkle in a bit of Rugby while you're at it. I enjoyed the non-stop action of the sport even though I didn't always understand what was going on. I also enjoyed the variety of scoring mechanics. 
If you run Windows, try [Database.NET](http://fishcodelib.com/database.htm).
That will work if the MAX and MIN timestamps are always the Max and Min. I personally would join in the table again as Table2 and in the WHERE clause limit table to Status Init and Table2 to Status Complete just to be on the safe side. 
This worked! Thanks a lot. I used regular apostrophes, though and not the backwards looking ones in your example.
The BEGIN / END block should only be necessary if you need multiple lines. Otherwise the first semicolon you get to will end the `declare handler` statement. For example: declare continue handler for sqlexception begin set sql_error = true; set value = 4; end; However, this: declare continue handler for sqlexception set sql_error = true; Is equivalent to this: declare continue handler for sqlexception begin set sql_error = true; end; 
Group by DATEPART(month, date_id) I think should work. 
This should work for you I think: ;WITH CTE AS (SELECT date_id = CONVERT(date, CONVERT(VARCHAR(10),date_id,101)) ,loc_id ,count(visit_type) AS RepeatVisit FROM visit_fact GROUP BY loc_id, date_id) SELECT * FROM cte WHERE date_id &gt;= '20140101' AND date_id &lt; '20150101'
If that doesn't, you an always use nested trim functions to chop off the left and right values to just get the middle digits.
It's definitely usable, admittedly the public server has its good days and bad days. Somebody built a docker container for the server though, which I'm happy to point you towards if you want to run a private instance. 
Hmm, I might not be understanding correctly but I am assuming they have dates stored as integers and that's the underlying problem here. So using DATEPART will throw an overflow error without some kind of conversion first.
 SET vendor_id = 122 WHERE vendor_id IN (122,133) Are there any constraints on this table?
Googling the GUID shows SSMS uses that name for displaying XML and makes it into a hyperlink when it does. Someone must've discovered the trick by trying it themselves.
 GROUP BY DATEPART(MONTH,CONVERT(VARCHAR(8),date_id)) There you go, bud.
Yes that's what I suggested before, thanks for confirming :)
--Example using an ID i declared as the same format as your dateid declare @id int = 20170105 select @id as DateId ,convert(int,substring(convert(varchar,@id),5,2)) as MonthOfId ,DateName(month,convert(int,substring(convert(varchar,@id),5,2))) as [MonthName]
HI, thanks for the answer it really helped me! Sorry for taking so long to reply, I've been a little busy and couldn't work on the answer until now. You pretty much solved my problem with SQL. Now that I understand the concept I'm trying to apply it to Django but I having some difficulty but anyways, this is not the place for Django questions. So again, thank you very much!
How about GROUP BY SUBSTRING(date_id, 5, 2) ?
Glad I could help! :) No idea about Django though... good luck with that.
It seems to me your case is what I found in http://www.dba-oracle.com/t_ora_00979_not_a_group_by_expression.htm TL;DR: Add subject to the group by, or remove it, as you did. Here's the text that explains it: The ORA-00979 error contains two components: You attempted to execute a SELECT statement which contained a GROUP BY function such as MIN, MAX, SUM or COUNT.You attempted to execute an expression within the SELECT list which is not in the GROUP BY clause. To correct ORA-00979, you can include all SELECT expressions in the GROUP BY clause which are not group function arguments. Here are three ways to resolve ORA-00979: Make the expression or column listed in the SELECT list also in the GROUP BY clause by completely rewriting the SELECT statement. Completely remove the GROUP BY function from the SELECT statement, including MIN, MAX, SUM, and/or COUNT. If there is an expression which is not in the GROUP BY clause, remove it completely from the SELECT list. In the vast majority of vases, the ORA-00979 error is caused because a non-aggregated column is not included in the GROUP BY clause. 
The only software I'd add to this is vim, in which case I use it to do quick transformations via REGEX on data. Especially useful for formatting large amounts of string data for an IN or NOT IN. I believe you can also do this in NP++, though I don't have much experience with it - I've mainly only ever used vi/vim as my text editors. 
Isn't your a truck column serving the same purpose? Sounds like you just need to make Truck it's own table with an ID column and a foreign key.
Where tag in (‘Hathaway’, ‘Jackson’, ‘etc’)
Not sure why you want to do this... so there may be a better approach, but you can just use `DENSE_RANK()` to achieve this: CREATE TABLE Delivery ([Order] int, [Truck] varchar(6), [PickupDelivery] varchar(1)) ; INSERT INTO Delivery ([Order], [Truck], [PickupDelivery]) VALUES (112233, 'TruckA', 'P'), (112233, 'TruckA', 'D'), (112233, 'TruckB', 'P'), (112233, 'TruckB', 'D'), (112233, 'TruckC', 'P'), (112233, 'TruckC', 'D') ; SELECT * ,DENSE_RANK() OVER (ORDER BY [Order],[Truck]) as PairID FROM Delivery http://sqlfiddle.com/#!6/a1451/4
Definitely this. Also you don't need a new join for each where clause. And you don't have to do the first select at all outside of those parenthesis.
If you understand that, I’d think you’ll be fine. CASE is basically an IF..THEN statement 
Thank you for that clear explanation. Are IF THEN ELSE statements ever used outside of a CASE statement?
this just implements ORs, and it's not that simple the logic required is more along the lines of ANDs -- which assets have ~all~ these tags, not just one of them 
 SELECT assets.* FROM ( SELECT assets_tag.assets_id FROM tag INNER JOIN assets_tag ON assets_tag.tag_id = tag.id WHERE tag.title IN ( 'horror' , 'monsters' , 'cast-anne-hathaway' ) /* note 3 tags in list */ GROUP BY assets_tag.assets_id HAVING COUNT(*) = 3 /* note count = 3, must have all tags */ ) AS these INNER JOIN assets ON assets.id = these assets_id 
all your movies have only one actor? doesn't sound right what you're missing is that you should have a many-to-many relationship of junction table
Well no but my goal is to implement a small sample of what would be a larger project for the end of the quarter. In this particular example I have a table listing one actor who then is an actor in multiple movies contained from another table. Also thanks for the advice, will look into that. 
What if you create a normal Identity column. You'll end up with 1,2,3,4,5,6.... Then, create a computed column called PairID with the expression of CONVERT(INT, ROUND(IdentityColumn / 2.0, 0)) This way, your first value is 1 / 2, which is .5. We are rounding that up to 1. Your second value is 2 / 2, which is 1. Your third value is 3 / 2, which is 1.5 and rounds to 2. Your fourth value is 4 / 2 which is 2....etc.
Not to not pick syntax and naming but your group by values don't match your select values. Also, you are doing a group by with columns from both left and right table and are looking for actors... So what's the need for the left outer join? You want to return movies without an actor? Also, since you are not aliasing the tables, you may consider fully qualifying your columns?
Oh yeah the syntax is just a small mistake probably from how much I’ve been changing everything. I’ve read through a lot of the explanations of the different kind of JOINS and I’m not quite sure if I understand what each one is doing. What I want to do is return the movie id, movie title, and the name of the actor from the second table without the name of the actor repeating for each movie record. I’m not sure if it’s possible to do or how. Thanks for the advice, I don’t understand what exactly the last sentence means but I’ll try to look it up. Apologies I’m fairly new at SQL. 
Well that's what's soo cool with SQL is that there are so many ways to do things and there isn't always a single answer for how to pull data... Things can vary based on a number of conditions, such as data size, indexing, data types etc! Have you tried doing a select distinct movie_id, movie_name, actor_name? With that you probably don't need the group by statement...
Hi there, Thanks for quite the example. I built a quick schema and tried out your code here: http://sqlfiddle.com/#!9/ee0eb/2 The only thing is that the last line, it should be "these.assets_id" Thank you so much!
I haven’t tried that at all yet. I’ll look into it and check out the syntax and explanations for a select distinct statement. Thank you so much for your help!
So you know structured data? Turns out, it's sometimes useful to transmit that.
You need to further normalize this. There is a many to many relationship between movies and actors so you need a third mapping table that stores a mapping of m_id and actor_id. You will also remove actor_id from your movies table to remove duplication of data. If you need all actors in a movie, or all movies for an actor, you will go through this mappung table.
Yeah, you can use IF thing = something {SQL statement} ELSE {different SQL statement} I mainly use it when creating stored procedures and functions. 
Is there a row for each actor, in the movie table? e.g. 3 actors appeared in a movie, so there's a row for each within the movie table? Definitely need to look into restructuring like other folks have said. But anyway, given this setup, do you even need to have a join? Seems like maybe you can return what you need using subqueries
i always throw in a typo to see if you're paying attention j/k ;o)
[Updated Schema](https://imgur.com/a/2abiy) First off, I really want to emphasize the impact your support is having. I am feeling a hell of a lot more confident about my skills and understanding of what I am doing, (while it may not be there yet) I truly feel like I am on my way, all thanks to your replies, which I truly appreciate. Okay just na heads up it is well past midnight where I am so I thoroughly apologise if this became too ramby and I will try to clean it in the morning. **Level of detail in DB** I will cover this point first, to help give context to my answers further along. Basically I am tired of being unable to understand some of the mechanics of the game. I cannot get to live games and the TV broadcasts are as such that they do not allow for viewing of things like off-the-ball strategy and other aspects of the game that cannot be appreciated without watching the game itself. While statistics are not the be all and end all of understanding such a dynamic game as is AFL, I see it as a good method of being able to understand how teams play and the impact that individuals have in such a large team sport (18 on field at a given time, broadly split into 6 back, 6 forward, 3 in the centre, 3 roving and 4 on the bench) where one player such as Patrick Dangerfield ([highlights](https://www.youtube.com/watch?v=XJMwHbqido8)) has such an impact on the teams performance despite him being only 1 of 22. I went through previously the current climate of AFL statistics and how there lacks ‘outsider insights’ because of I want to create something that I can use (and possibly others) to understand the game better, I can't emphasize enough how frustrated I am with the organisation itself of the AFL, from the stat hoarding to TV rights deal, etc. **What Do you want from the database?** My knowledge and understanding of proper query syntax is abysmal, so I wont try at the moment, instead I’ll try and describe the types of insight I may want to look at. * Individual players against given teams * Performance as the players get older * The teams that perform well in a given statistic and those who do not * Players who may be playing out of position (position is something I have not added yet, because it is a little difficult to discern, similar to the NBA where positionless basketball, the afl is getting an influx of players who can play multiple positions. Making the old classification schemes difficult to use with certainty) * Broadly speaking however, most of the queries I envision will be about players vs players / Teams vs teams in basic stats (disposals, inside 50s) Performance of players/ teams in stats This is all off the top of my head. **Quarter table stats** Definitely agree. **Stat table:** I kinda get what you are saying, I'm going to use your analogy for separating Object oriented MySQL and 'excel-lite' tables in the excel lite version, (which I am currently doing) I am tracking all stats for everyone, no matter what and there will always is a space for them in the table. However in the method you are proposing I only add the stats as they come relevant to the player, so in theory I have a stat_Id that identifies a particular stats where stat_id = 1 relates to tackles and stat_id = 2 relates to kicks. The transaction table it will have a identifiers (IDs) for ( the table itself player game stats) the player and the game. So the question becomes, where do I store the stat types themselves? My thought is to have a table with a hard coded list of stat fields, that basically only store the names to give context to the data (this will be reflected in the updated schema.) Finally, I wanted to draw your attention to the value in the playerstattable, my initial reaction was to put it in the stat table, but In my mind the stat table only exists to act as a reference for the names of stats. **Going to the Matt Crouch example.** I think I follow, so the main point is that have an informal rule as a kinda note to self not to try and join the scoring progression on the stats table. **Finally** Thanks again for the help and support so far, I truly appreciate it. 
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/NEqQOex.jpg** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
I think you want a foreign key to order_item_id in your w_order_parts table. The foreign key to order_id is unnecessary. Also, in your sqlfiddle link, I think you wanted to insert 9898 into the order_id, not the customer_id. You don't need single quotes for ints. 
Yeah, fixed the fiddle thing. If I add a foreign key to w_order_parts, will I be able to select it to achieve a structure like in the image? And is is what I have designed efficiently?
I didn't want to add `order_item_id ` as child of `w_order_parts ` because calculating the price of the invoice would be more complicated.
It wouldn't be more complicated at all. You'd group by order_id anyway. Right now, you're getting incorrect results because of the design of the w_order_parts table. How are the invoices issued? One order_id to one invoice_id? 
Hey man, glad i could help!
Other people have given good advice on architecture and layout you should do that. To join to a table on duplicate id's, I'd do a select distinct on the values you need from the table you are trying to join. That's now your sub query, join onto the sub query with distinct rows instead of the table with duplicates. If that yields invalid results, then it's probably your data architecture that needs to be changed. 
`GROUP_CONCAT` is da bomb ;o) also, be very careful writing GROUP BY clauses -- this shouldn't be hard for you, because Microsoft SQL requires standard behaviour just be aware that MySQL allows some, ahem, exceptions to ANSI grouping also, function `SUBSTRING_INDEX` can often be useful
I think what you want to do is link order to order_parts via order_id and then link order_parts to order_items via order_id and warehouse_id. That should solve your multiples problem. My issue with the design is it doesn't cater for partial order fulfilment. Say they order 5 widgets and the warehouse only has 3. With this design the warehouse can't ship anything until they have all 5. Or if they order 2 widgets and 2 shmidgets if the warehouse doesn't have the shmidgets they can't ship the widgets. If that's really ok and they will really never ever do partial shipments ( don't believe them when they tell you they won't cause they will) then the design still feels a little clunky but you might find it workable. I think you should have orders, order_items and then a many to many link to order_shipments via order_item_shipments or something like that. I think the order_parts table is really just a proxy for warehouse_id in which case you don't need it or should be replaced by the many to many to shipments. As is it probably doesn't really help your situation. I'm just guessing though. 
Their official documentation is actually quite good. I've encountered a number of times where there's a built in function for one language, that doesn't exist in others. So don't assume that just because you couldn't do something in postgres, you can't do it in mysql. Especially true compared to ms sql. Sooo much more built in. Also subtle syntax differences. Like deleting from a table using an inner join is waaaay different in postgres. IMO mysql's syntax is more intuitive. Be aware that there's some goofy stuff because of mysql's history BUT don't be a dweeb about it. Most of that is in the past. Mysql 8.0 with InnoDB is an all around great RDBMS. Lastly, get used to the general controls. Like "show tables" "show create table" "\#" instead of the \d \dt kinda stuff in postgres. Also in mysql there isn't auto formating output. You choose between terminating a command with ; and \G for a table output vs a row detail form. 
&gt; BUT don't be a dweeb about it great advice, thanks
=D I just think language elitism is mostly a waste of time. 
Remove the ticks from around 33333
Show us your query. I bet you are using OR statements without bracets
try BETWEEN #2017-11-06# and #2017-11-10# 
Same results. I have the format as 11/06/2017 for example; would the slashes be the reason? I am getting back results but they do not look right. Very weird
I have only one order in that time frame and I am getting back several different products that are not present in that order.
i edited my reply -- it's not the date format, it's the bad join 
i edited my reply -- it's not the date format, it's the bad join 
&gt; I assume this is something to do with the data type? nope... you're messing up the query somehow post it, so we can see
I can't seem to find your original query. Interested to take a look.
You are right, really no need for Payment_T, and ill run SUM and see if it makes sense; thank you for your assistance! 
I am a very strong advocate for SQL and relational databases, however i do not think you understand the shortcomings of NoSQL. Many of the issues you point to in NoSQL are false or trivial to work around. For example. "NoSQL databases such as JSON are files/documents, making it easier to be hacked into" is a completely bogus argument. 
Is 8.0 on GA?
I haven't checked in a while. I don't believe so. 
I think you were right. (edited to add in the query) I've updated to: select* from table_name where uid = '123456' AND (url = 'url_name' or url = 'url_name') limit 99 Thank you! 
Have you tried selecting MAX(points) grouped by province?
Yeah. *select province, max(points) from wine group by province;* only gives me each provinces' highest score. I tried using Select *, but I get an error saying this is incompatible with SQL_mode = only_full_group_by
This is a scenario where a subquery might be appropriate. So build a query that returns one row for each province. Then build a query that returns the best wine for a given province &lt;hint, top 1 order by&gt;. Then combine the two into a query + subquery.
What else do you want to see in the output? If all the columns you listed, I guess I would just add them to the select if * doesn’t work. 
 With WineScore AS ( SELECT *, ROW_NUMBER ( ) OVER ( PARTITION BY Province, ORDER BY Points desc ) Ranking FROM {WineTable} ) SELECT * FROM WineScore WHERE Ranking = 1 Don't select * in release code, I just did it to demonstrate (and there's no point selecting the Ranking in the main query, since it will be 1 for every row). This is for MSSQL, use whatever the correct cte/row number format is if you're using something else.
Does ship registry = ( need to be ship registry in ?
Try this: CONSTRAINT chk_Ship_Registry CHECK (Ship_Registry IN ( 'NORWAY','Liberia','The Netherlands','Jamaica','Bahamas'))
That worked! Thanks so much. I still got an error when I fixed it with that, so I changed the code to this and it created it without errors! CREATE TABLE Ship ( Ship_Name VARCHAR(100) PRIMARY KEY, Ship_Size INT CHECK (Ship_Size &gt; 0), Ship_Registry VARCHAR(50), Ship_ServEntryDate INT, Ship_PassCapacity INT, Ship_CrewCapacity INT, Ship_Lifestyle VARCHAR(40), CONSTRAINT chk_Ship_Registry CHECK (Ship_Registry IN('Norway','Liberia','The Netherlands','Jamaica','Bahamas')) );
Use a query with MAX(Points) as Points and GROUP BY Province. This will give you the top Points for each Province. Then just join this query back to the wine table on Points and Province and select whatever data you want out of the wine table since the join will match only the max Points for each Province. Depending on the DBMS you are using, when you do stuff like this you usually can't really do it in just one simple SELECT statement, you almost always have to aggregate the results with a function like MAX and squash it down with GROUP BY. That's just a limitation on representing data in this manner. 
Try putting a comma after each constraint statement.I think that will do the trick.
So isn’t that answering your question? You said how do I get the best wine (points) by each province.
I think OP wants a way to the actual full record from the wine table associated with the one with the top Points in each Province, not just what the max Points in each Province are. As I and other said, you need a join for that. 
Thanks!
OP has ONLY_FULL_GROUP_BY turned on, though you understand the difference this makes, yes? 
&gt; This is for MSSQL, use whatever the correct cte/row number format is if you're using something else. yeah, good luck with that you're not familiar with MySQL, are you 
[removed]
What RDBMS? What have you tried and where it is that you're getting stuck? Have you tried writing a super-simple trigger based on a sample for your RDBMS (just google "YOURDATABSE trigger example")? The syntax is fairly straightforward if you've already gotten a handle on other DDL statements (`CREATE TABLE`, `CREATE PROCEDURE`, `CREATE FUNCTION`).
Point 1 is false on both sides. There are *plenty* of insecure SQL databases and lots of people use NoSQL DBs in a secure way. Security **is not a feature**, it's a **process**. &gt;since social media companies deal with relationships such as relationships between a user and their followers/friends or a relationship between a user and their posts. Social media companies deal with a lot of non-relational data too. Or, if you look hard enough, you can force a relationship between anything. &gt; SQL is better apt at storing data in different tables to avoid errors caused by duplicate data Only if you design your database properly. Social relationships/data do not always map to relational data. I feel like you've written this from only a cursory glance at high-level information about both RDBMSs &amp; document/unstructured databases, maybe a few trivial Google searches, and then some atmospheric extraction. What's the point of this list? Part of a course assignment? You might be interested to know that Facebook uses **both** relational DBs and non-relational/document DBs. As do many other large systems. It's not an either/or scenario - you choose the tool that's right for the task at hand, and sometimes that means breaking task down into smaller ones that have different requirements.
What you are using there is something that is often called derived tables. You are using a select query in the FROM part as a "normal" table. In that sample, you don't need to use a derived table
Have you used the query execution plan yet? Take both queries in one window and execute with show execution plans to see what the underlying difference could be. There is not normally one way over an other that is always better... It all varies with indexing, data sizing, predicates. Etc...
Honestly, Google is your best friend for examples of triggers in any DBMS out there. I highly recommend you give that a shot as you'll probably be using Google for the rest of your career. 
Is this post trying to get the tag of controversial??? 
in this example, the optimizer is smart enough to promote the subquery
I was trying to let our newbie see that ;)
just a tip... you can greatly simplify these types of mixed conditions as follows -- SELECT something , anything , just_not_select_star FROM table_name WHERE uid = '123456' AND url IN ( 'url_name_1' , 'url_name_2' ) LIMIT 99 see? no messing with ANDs and ORs also, why would you have LIMIT without ORDER BY?
The results of a query cannot be guaranteed to be in any order unless an ORDER BY clause is added to the statement. That said, an index on one of your tables may be altering the sort order of your data.
wow, thorough detail in your post, good job try this (not tested) -- SELECT CountKeptAppts / CountAllAppts AS pctKept , YEAR(ScheduleDateTime) AS yearStart , MONTH(ScheduleDateTime) AS monthStart , DAY(ScheduleDateTime) AS dayStart FROM ( SELECT ScheduleDateTime , SUM(CASE WHEN ApptStatus = 1 THEN 1 ELSE 0 END) AS CountKeptAppts , COUNT(*) AS CountAllAppts FROM Appointments WHERE ScheduleDateTime BETWEEN FROM_UNIXTIME(1484417551000 / 1000) AND FROM_UNIXTIME(1517437800000 / 1000) GROUP BY ScheduleDateTime ) AS data GROUP BY yearStart , monthStart , dayStart ORDER BY yearStart , monthStart , dayStart 
"at the end of the table" makes **no sense** in a relational database table (no, i don't want to discuss clustering indexes) just let them go where they go -- you can't really tell where they go anyway, because the only way to see ~any~ rows is to SELECT them you want a particular, use ORDER BY on your SELECTs
&gt; What is the advantage of having a subquery in the first example? During data analysis, for instance. Suppose I want to compare two crazy queries which consist of 200+ lines each. I may do a quick select * from (&lt;copy-paste first crazy sql query&gt;) as t full outer join (&lt;copy-paste second crazy sql query&gt;) as u on u.key_id = t.key_id where t.key_id is null or u.key_id is null 
oops, i forgot the 100... SELECT 100.0 * CountKeptAppts / CountAllAppts AS pctKept 
 SELECT Sales.Division , Sales.Store , Sales.Department , Sales.Item , Sales.RetailAmount , Sales.SoldAmount FROM Sales LEFT OUTER JOIN Exclusions ON Exclusions.Division = Sales.Division AND Exclusions.Department = Sales.Department WHERE Exclusions.Division IS NULL 
Thanks for the encouragement ^^ Now it's returning a binary value of either 1 or 0. (or 100 or 0 with your second modification) pctKept, yearStart, monthStart, dayStart '1.0000', '2017', '11', '28' '1.0000', '2017', '11', '29' '0.0000', '2017', '11', '30' I left customers in there in case I missed something that the basic logic was dependent on. 
&gt; Now it's returning a binary value of either 1 or 0. (or 100 or 0 with your second modification) that`s down to your data, good sir do the percentages yourself -- SELECT ScheduleDateTime , SUM(CASE WHEN ApptStatus = 1 THEN 1 ELSE 0 END) AS CountKeptAppts , COUNT(*) AS CountAllAppts FROM Appointments WHERE ScheduleDateTime BETWEEN FROM_UNIXTIME(1484417551000 / 1000) AND FROM_UNIXTIME(1517437800000 / 1000) GROUP BY ScheduleDateTime ScheduleDateTime CountKeptAppts CountAllAppts 2017-11-28 08:00:00 1 1 2017-11-28 09:00:00 0 1 2017-11-28 10:00:00 1 1 2017-11-28 11:00:00 1 1 2017-11-28 12:00:00 1 1 2017-11-29 08:30:00 1 1 2017-11-29 09:00:00 1 1 2017-11-29 09:30:00 1 1 2017-11-29 10:00:00 1 1 2017-11-29 10:30:00 1 1 2017-11-30 08:00:00 0 1 2017-11-30 08:30:00 0 1 2017-11-30 09:00:00 1 1 2017-11-30 10:00:00 0 1 all you have is either 0 kept appt or 1 kept appt, out of 1 total that works out to 0% or 100%
That was it, it returned the expected values !!^^
If you want to define a specific order, create a key on the temp table and then ORDER BY that key in a SELECT.
As an alternative to the answer already posted, my preference is to use a NOT EXISTS correlated subquery: SELECT * FROM Sales AS S WHERE NOT EXISTS ( SELECT * FROM Exclusions AS E WHERE E.Division = S.Division AND E.Department = S.Department ) Performance should be identical but I find my version easier to read -- at a glance we know that no duplicate rows will creep into the table even if the Exclusions table has duplicates. 
&gt; Performance should be identical "should be" LOL
Performance might be better than in your version because the system can use a SEMIJOIN. 
"might be" LOL my impression, and i could be wrong, is that correlated subqueries do poorly
Perhaps you should read a bit about them instead of continuing with your idiotic replies that add nothing to the discussion?
Where do I see the execution plan? I don't have MS SQL on my computer since I'm just preparing for a 30 min tech interview on SQL. 
He's right, they perform poorly and should not be used when other superior options exist. If it is a small table a where not exists is more intuitive and if the process executes quickly and has no fear of growing into a monster then use it... but it isn't as good as eliminating it.
Can you confirm SELECT * FROM Ship WHERE Ship_Name = 'Sunshine of the Seas' returns data?
It did return data, but I was able to figure out my problem from doing that! I had 'sunshine of the seas' lowercase in the ship table and in my cruise table I was trying to insert uppercase 'Sunshine of the Seas', I had no idea that would have caused an error like that. Thank you for the help!
When it comes to performance what's important isn't what a query looks like (correlated subquery or a join) but how that query is represented and executed by SQL Server. SQL Server has no keyword for a LEFT OUTER ANTI SEMIJOIN. To use it you write a NOT EXISTS query the way I did above. SQL Server usually reduces this to a LEFT OUTER ANTI SEMIJOIN. You can read about the join in more detail at [A Join A Day – The Left Anti Semi Join](http://sqlity.net/en/1360/a-join-a-day-the-left-anti-semi-join/)
It would be in SQL server management studio. There is a button you can click to"include actual execution plan" with a query or set of queries and it will tell you how the optimizer actually pulled the info. If you highlight two or more queries, it will tell you how much resources once used in comparison to another...
To go a little more into the "why" of the behavior, SQL has an order of operations (FROM -&gt; WHERE -&gt; GROUP BY -&gt; HAVING -&gt; SELECT -&gt; ORDER) but generally only FROM and SELECT are required operations. If ORDER isn't specified then SQL will output select results based on how the base data is stored and how it determines the most efficient way to compile the data, reasons can include the joins, where clauses, indexes and statistics. You could probably tell why they are different by looking at the execution plans but it also could be caused by the underlying tables, but really what it comes down to is if you need data to be in a specific order to give it an ORDER operation, otherwise just let SQL do it's thing.
If I'm not mistaken. If it was a larger dataset in the exclusions, then the"not exist" may perform better as once a match is made to exclude, the engine doesn't have to continue scanning the exclusions table to find other matches... Maybe we could get some execution plans to look at?
Great comment. Just to add a clarifying comment. On the same DBMS engine (same server even, and even moments after the first run of a query) you could have different order of results in subsequent executions unless you add that order by clause. 
&gt;SQL Server usually First thing I learned of worth is that the optimizer is a piece of shit and whenever possible to force it to do *what* you want, not giving it the option to usually do this, or that.
Can you show what you've got so far?
This is literally all I was given. I don't even know what to do with this data or how to manipulate it - given that Macs have nothing for free for this kind of stuff. 
Also, I think it may be better to change the emphasis onto the teamlists, so that the game has the foreign keys from teamlist so that conceptually it is a battle between two lists of players rather than the basic information of the organisations themselves. 
Think about it like a word problem in math. Everything you need to complete the problem is on that page or probably in your notes. You don’t need to manipulate anything. Just start with pretty basic SQL and work your way from there SELECT (whatever you need to select) FROM (what table or tables are you working with) WHERE (do you need to add a filter to anything?) ;
That helps me a bit. This is my first query, so its been causing way too much stress. Out of assumption alone, I'd assume I'd be doing the following: SELECT Average wrc plus FROM the batting table WHERE &gt; or = 400 PA in 2017 Like i have I believe the basic idea, but writing it in SQL is just what's killing me. A step-by-step walkthrough for writing this is what I need and then I believe I'll be set. With that said, do you have any advice on how to learn this/how long gaining a basic understanding takes? It's the one technical skill I severely lack
For free : http://www.studybyyourself.com/seminar/sql/course/?lang=eng. The course is broken down into a basic course and an advanced one. You can submit exercises online too.
Which flavor of SQL do you use at work? 
If your goal is simply learning you might want to try sqlzoo.net or codewars.com If you want to try MySQL just create a virtual machine with Virtualbox (its free/open source), install a linux distro like centos or ubuntu and then install MySQL on that. If you want to explore MS SQL server you can download a Windows Server 2016 image and install SQL Server Developer Edition 2017 free from Microsoft's website. 
Yes, it runs the aggregates then adds the final results together.
 select a.name, sum(a.shdw_clm_count)+sum(a.match_clm_count) as total_shadow_claims, sum(a.match_clm_count) as matching_claims, sum(a.shdw_clm_count) as shadow_outage (sum(a.shdw_clm_count)+sum(a.match_clm_count) - sum(a.match_clm_count)) / ( sum(a.shdw_clm_count)+sum(a.match_clm_count) ) FROM medtables Should do it. Alternatively you could do it by making your current query an inline view, something a little like SELECT x.name , (x.Total_Shadow_Claims - x.MATCHING_CLAIMS) / x.TOTAL_Shadow_Claim as YourField FROM ( select a.name, sum(a.shdw_clm_count)+sum(a.match_clm_count) as total_shadow_claims, sum(a.match_clm_count) as matching_claims, sum(a.shdw_clm_count) as shadow_outage FROM medtables ) x
Thanks this is exactly what I am looking for. The only issue im having now is I only need the numbers to the right of the decimal point and cut off the whole numbers (round to 4 decimal points). What kind of function should I look into to accomplish this?
select 
SELECT AVG(wrc_plus) AS 'Average WRC PLUS' FROM player p INNER JOIN batting b ON b.player_id = p.player_id WHERE pa &gt; 400
I'm too lazy to test this but does the following work? SELECT x.name ,Left( ((x.Total_Shadow_Claims - x.MATCHING_CLAIMS) / x.TOTAL_Shadow_Claim) % 1, 6) as YourField FROM ( select a.name, sum(a.shdw_clm_count)+sum(a.match_clm_count) as total_shadow_claims, sum(a.match_clm_count) as matching_claims, sum(a.shdw_clm_count) as shadow_outage FROM medtables ) x Watch out though, if that works at all I think that will return it as varchar instead of a numeric type so you might want to convert back again depending on how you're using the output.
You can set up an AWS database that’s free for the first year. It’s easy enough especially if it’s just you connecting to it. You could still connect Workbench or whatever SQL IDE to it and use it to learn on. This assumes you’re not using massive amounts of data that would push you up out of Amazon’s free tier. 
I believe MacOS comes with at least Postgres installed somewhere, and even if it doesn’t it’s not hard to install.
average by position was asked for, not overall average also, pa over 400 for 2017
1/ it is **A** FK -- a table can have more than one FK 2/ referenced key -- it can be either table1's PK or a UNIQUE key
My bad! SELECT AVG(wrc_plus) AS 'Average WRC PLUS' FROM player p INNER JOIN batting b ON b.player_id = p.player_id WHERE pa &gt; 400 AND year = 2017 GROUP BY position Didn't edit original comment to avoid confusion.
Look at the ROUND function. Example: Round(sum(column_name, 4) 
MySQL installs easily on a Mac
Why not just use Select fnVerschilInDagen('20171203') as Leeftijd
As far as I know, you can't return a column name in a scalar function. You could turn this into a table function, where you return a one-column, one-row table you define in the function, but it would likely be much easier to convert this to a stored procedure. Then you can do something like this: SELECT DATEDIFF(YEAR, @BeginDatum, GETDATE()) AS [Leeftijd] ; That will still give you a one-column, one-row table, but you don't have to define the structure explicitly.
Ahhhhh I got it, thanks a lot guys will use that solution!
&gt;MySQL just create a virtual machine with Virtualbox (its free/open source), install a linux distro like centos or ubuntu and then install MySQL on that. Why the complexity? MySQL, Postgres, and SQLite all run on macOS natively. &gt; MS SQL server you can download a Windows Server 2016 image and install SQL Server Developer Edition 2017 free from Microsoft's website. No need, anymore. SQL Server 2017 runs on macOS thanks to Docker (and Dev Edition is still free!) and SQL Operations Studio is cross-platform too.
Save rounding your numbers for the presentation layer. Don't do it in the query itself.
You can use sql developer or sql workbench depending on your sql flavor
JetBrains DataGrip hands-down. It’s free while you’re a student and comes with support for almost any SQL dialect. The UI looks good on macOS as well.
You’re getting there. Have you completed the Code Academy SQL course? It’s free and should give you enough context that you will be able to relay the context of that course into the assignment you’ve been given. I definitely understand that it can be annoying to try to solve that problem conceptually without the ability to troubleshoot. One way you can test out what works and what doesn’t is by using Oracle’s Live SQL application. You can query from your browser and try out different ways to solve your problem. They have information on the site about the tables. So instead of figuring out the average wrc for players with more than 400 PA, you can find the average salary for employees employed for more than 2 years. Go to code academy to learn the syntax for how to do this problem, and if you’re dead set on troubleshooting before you turn in your answer, or just want to screw around with more SQL, google Oracle Live SQL, make a free Oracle account, and go crazy.
What are you expecting "MAX(@Tweedenummer)" to do when you pass it a single value?
Show us how you're calling the function (the queries) and some sample data.
I think that makes a lot of sense. I'm sure that's the first of many little things you will find and adapt.
http://postgresapp.com Use https://www.pgadmin.org to load it up with data and practice queries. 
I didn't know MySQL had a native macOS build, good to know. Virtualbox is a little easier to setup than Docker and if the goal is to run it in a non-production environment (which seems to be the case by his post) it's one less thing to learn to get up and running. That said I am pretty hyped for SQL Operations Studio as I don't run windows on any of my non-server machines, there is just one giant feature that hasn't been added yet, actual execution plans: https://github.com/Microsoft/sqlopsstudio/issues/8 And it looks like its planned for a December release! Hype!
&gt; Virtualbox is a little easier to setup than Docker and if the goal is to run it in a non-production environment (which seems to be the case by his post) it's one less thing to learn to get up and running. True but it also requires acquiring a Windows license.
use LIKE '%xxxx%'
Try harder imo. This post has no table structure, no sample, no expected results, and you didn't even spell banana properly...
okay i got it going Like '*' &amp; [Checker Table].[ClientDeposit] &amp; '*'
This is a huge help - thank you! Can you tell me where I could sort of learn/practice SQL on my own? I do need it for work and have just so little clue how to use it. 
&gt; If it would find multiple rows with ID=1 in TBL_CUSTOMER ... i know you're explaining how joins work, but ~usually~ a column called ID in a table called TBL_CUSTOMER is going to be the table's primary key, so why would there be multiple rows with ID-1??? this could confuse people also, where did you learn to prefix your tables with "TBL_"?? this is silly, and just noise "Try to read 'Paris In The Spring' as 'nounParis prepIn artThe nounSpring' and **see if the prefixes make it easier to understand** -- from "[Ten Things I Hate About You](http://web.archive.org/web/20020611032650/http://intelligententerprise.com/001205/celko1_1.shtml)" by Joe Celko, December 2000 
also, not sure why felt it necessary to mention Oracle nothing in either of those two tutorials is Oracle-dependent, as far as i could see
Pretty sure I saw the OP with the same post on StackOverflow earlier. It's a non-issue. They are getting multiple results because they are passing more than 1 value to the function. There are definitely problems with this code but the function is working as expected.
[removed]
https://support.apple.com/en-ca/HT201468
You can put it in a virtual machine as well. Oracle has virtualbox and the Linux environment for it with the DBMS installed already. VM [here](http://www.oracle.com/technetwork/server-storage/virtualbox/overview/index.html/). Can find the OS with DBMS installed [here.](http://www.oracle.com/technetwork/server-storage/linux/downloads/vm-for-hol-1896500.html)
I am sorry about my formatting I am not sure how to make this look like a table and not like a blob. Basically just adding some text into a string where the condition is true.
UPDATE Table1 SET Field2Year = LEFT(Field1String,8) + 'THE YEAR ' + RIGHT(Field1String,4) WHERE Field1String = your condition 
I downloaded virtualbox this weekend but couldn't figure out how to get Oracle downloaded onto it. Is there a step by step guide you know of?
Thank you, I will see if I can give this a try. 
IIRC correctly I once asked about writing a process which evaluates each character in a string to come up with a match % and then joins if it is &gt; some arbitrary threshold. There was a specific name for that type of process. I think that is what the OP is looking for and he just provided a casual example.
Is this homework? What's with the zero effort posts from you? Lol
[removed]
Nope, this guy is just lazy. He didn't even try to figure it out. He comes here and makes these homework style posts then deletes them once someone figures it out for him. I've been following along.
I wish I could remember how I did it but I think you just attached it as a hard drive.
which database are you using? if it is Oracle it is possible to do something like this: https://livesql.oracle.com/apex/livesql/file/content_FXV9B3BNRT0H8UQV1VU5DRO98.html tl;dr regexp_replace using capture groups and rebuilding the string. Just demonstrating this because it gives you more flexibility in how you define the left side from the right side. 
Getting the same vibe, feels like: "I haven't attended class for the last month and now I'm trying to cram it into a week before my exam."
haha no this is an office job i simplified it down as much as possible using fruits and stuff like that then apply it back into what i need
thanks this worked great.
busted
yeah thanks, its corporate data i deal with so i just try and get a basic example and someone to help with logic then scale it back up using that. /u/Cal1gula likes to comment on people twice an hour every day so i guess he needs content to get on his high horse for.
 You got it!
My most frequent subquery is probably using a windowed function, like ROW_NUMBER(). Assume you have a dataset and you need the details for the highest order amount for a given day. SELECT * FROM ( SELECT o.* ,ROW_NUMBER() OVER (PARTITION BY o.ORDER_DATE ORDER BY o.ORDER_AMOUNT DESC) AS RN FROM dbo.Orders o ) sub WHERE sub.RN = 1; 
Yeah well you should have given him what you wanted because A) I want to read about it again, and B) It wouldn't have helped him at all. &gt;and you didn't even spell banana properly This made me lol.
Are you asking us to check your homework assignment? (which in turn looks like a homework assignment testing your ability to Google)
Just FYI google is an amazing tool for this kind of stuff. If you wanna find more w3schools and stack overflow are excellent tools for the basics and slightly more advanced queries. 
I just got one of [these](https://www.walmart.com/ip/Lenovo-ideapad-320-15-6-Laptop-Windows-10-Intel-Pentium-N4200-Quad-Core-Processor-4GB-RAM-1TB-Hard-Drive-Platinum-Grey/876002923) at Walmart last week. It has worked pretty well for me so far
Hey thanks! I’m pretty shit at this stuff and I find it really difficult to word what I’m trying to do as well. But I’ll keep practicing and try to get it before I come here.
You got your question answered right? It's just more rewarding if you come across it yourself and you learn more on the way. 
Where '%' + table1.column + '%' like table2.column;
In Access, the percent symbol only represents one character. You meant '\*xxxx\*'
Simple Excel add-in from Analytics Edge, then Data Connection to your database.
That's good for manual entry but I need to automate it.
CTE doesn't work if there are more than one wine that has same top score SELECT _W.WinesK, _W.Name, _W.Points, _W.Country, _W.Province, _W.Region FROM Wines _W JOIN ( SELECT MAX( Points ) Points, Country, Province FROM Wines GROUP BY Country, Province ) AS _W_MAX ON _W.Points = _W_MAX.Points AND _W.Country = _W_MAX.Country AND _W.Province = _W_MAX.Province; A short video about the problem and how to solve it with derived tables [Select top rows for a GROUP BY statement using a derived table](https://youtu.be/DLt3zA8ZmrI)
ill be completely honest, most of that went right over my head.... honestly 90% of the entire course has gone over my head. so to be specific: &gt; I see agent in both the deal and the seller fields, is this not redundant? what exactly is redundant &gt; Also your field should be dependant on the PK i didnt understand that at all &gt; Why would the stated price belong with the seller rather than the deal? deal is supposed to be the final agreement. like a receipt. stated price is what the seller is currently asking for which isn't the final decided price &gt; What would happen if the seller had multiple properties to sell? fixed &gt; You are at 1NF now... whats 1NF?
If you're taking class at a university or college, you can probably get a free/very cheap licensed copy of Windows .
Oracle provides a bunch of pre-built VMs here: http://www.oracle.com/technetwork/community/developer-vm/index.html
You're on the right track as far as the first part is concerned. I'm not sure what you mean by also show a list of earth only drivers. It's hard to know where to start if you don't know how you want the data presented.
Here is one example where a derived table is used to to get the final result. The problem is explained and solved in this video [Select top rows for a GROUP BY statement using a derived table](https://youtu.be/DLt3zA8ZmrI) If you have one result and want to group that result based on a column but only want to display some rows depending on whats in each group. That could be done with derived tables (subqueries). In this sample, there is a table with wines and you want the wines with most points in each based on each province SELECT _W.WinesK, _W.FName, _W.FPoints, _W.FCountry, _W.FProvince, _W.FRegion FROM TWines _W JOIN ( SELECT MAX( FPoints ) FPoints, FCountry, FProvince FROM TWines GROUP BY FCountry, FProvince ) AS _W_MAX ON _W.FPoints = _W_MAX.FPoints AND _W.FCountry = _W_MAX.FCountry AND _W.FProvince = _W_MAX.FProvince 
Search for 'oracle-base virtual box' you'll find Tim's site. He has loads of installation procedures. 
If you have computers running Windows and can connect your Oracle database using ODBC, then I might be of some help. Gorep Selection is a tool for tasks that is hard to perform in standard systems that the users have when they do the daily tasks. It has mainly been used for SQL Server and PostgreSQL (one Oracle database but that was a long time ago). In Sweden where I am located Oracle is a rare database. There is of interest to get it to be used and tested against Oracle by someone with high demands so I could assist. Have done some videos about the tool, it can do a lot. Much more than is presented here and it is easy to customize if someone needs some special functionality. I know this is a long shot so will not rant more here about it, if it sounds interesting then I can fill in with more information Here is the channel: [Gorep Selection](https://www.youtube.com/channel/UCKNeQ96WteazmrcsbZFW7Tg) It has mainly been used by our partners that are selling the tool
Are you asking how to move further in your expertise in SQL or are asking were to go next in your career? 
The former, I want to be better, more efficient, more forward thinking and i've hit the upper limit of what im learning organically.
&gt;In the past year I have created close to a thousand queries for various reasons and I would have thought that I would have a clear image of what i can improve/automate etc. For example, so many promotions are basically the same thing, I would have surely thought i could have built a package by now where we just dump in the values and it does my job for me, but alas know. So, try to create a stored procedure or package to do this. Automate it, use Dynamic SQL if you can/have to. &gt;what am I missing that can get me to being advanced? Writing queries is one thing, writing quality code is another. Ask your DBA for the worst performing code you've written. Look at the execution plan and try to optimize it the best you can, the onto the 2nd worst performing query ect.
[removed]
Thanks. What I mean is I want a list of all the Driver names that appear in earth report rows, that don't appear in any rows from mars reports.
Right. So think of it from a logical/mathematical perspective WHERE BLA BLA BLA AND ( IN earth list AND NOT IN Mars list )
What is an "earth report"? 
A few questions of mine to gauge where you're at: Do you know/use analytical or windowed functions? (e.g. Find the 5 highest performing campaigns in the past year. Give me a list of the campaigns run since January and give me a running total of how many emails were sent up-to &amp; including that campaign.) Do you know how to write/debug Dynamic SQL? Do you know what an execution plan is and how to read it and infer how to better improve your queries? Do you know about indexing, differences between clustered v. non-clustered? Do you know the system tables well enough? If I wanted you to give me a list of all tables with an EmailID column in it, could you? How about all procedures that reference the Customer table? 
Download more ram of course 
That doesn’t work. I’ve tried with 1Gb 2gb and 4gb servers!!!!
IMO, which isn't worth much, you need to make a choice and start to specialize. For example, why not become a DBA? Not into being a DBA? Why not learn Java and be able to write websites to databases? You probably need to become like a DBA lite for this. Not into that? Look into analytics. We more or less do what you do, but with threeish added roles A) taking data and prepare it for consumption by technologies such as Tableau, SSRS, etc. and often this requires doing things that DBAs hate, B) Using statistics to come up with mathematical models and then leveraging aforementioned technologies to make the work accessible to end users. This is like advanced report writing. C) Sometimes rely on tools like Python to ETL new data into our database which allows A &amp; B to happen. The word 'data scientist' is probably a step or two above what I do for a living now, but I've personally ran into 'data scientists' who know a lot less than I do, and who can't do as much. Start brushing up on your stats and learning something like Python or R. Beyond those specialties the only other next step is to go into management. 
IMHO, you are too focused on "doing the right thing". The hacker mentality is not about achieving perfection or some ideal state. It is about achieving simple goals. The primary goal is - if you have to do a task more than once, you should be automating it. AKA the [DRY principle](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself). Forget about frameworks and what-not, just do it. Automate something. Then automate another group of tasks. Then if you have a newer or better idea, or another way to restructure things so a larger group of tasks can be generalized and automated and made config driven, go back to your original automated code, rip it out, and rewrite it (aka refactoring). It is important to note that further levels of automation are usually achieved through abstraction. If you are able to generalize and abstract specific queries and what business function they perform, you start thinking of generalized solutions that can be reused for multiple business needs. Finding immediate solutions without overthinking, and repeatedly refactoring your solutions is the path to become more advanced. AKA emergent architecture. Not a moonshot strategy.
&gt;i've hit the upper limit of what im learning organically. This means its time to find a new job, even if its a lateral move for the same salary if it means you will be exposed to new things and learn more. In a perfect world you get a title bump and a raise along with that, but when you stop learning then its time to move on no matter how comfy you are.
Well that was a joke first of all. So is it CPU or Memory? Did you try Google? You obviously know it's by design so I would guess it wouldn't be hard to find a thread with a resolution. 
Well, in that case, I encounter a lot of people tend to think about how to use fancy tricks special window functions and learning neat ways to pull up specific data sets. In my opinion, a really good SQL developer should understand how the server inserts/updates/retrieves data from a system level. Understand indexes and the different types. Understand how to look at execution plans and statistics like scans and logical reads. It's one thing to be able to retrieve good data. It's another thing to write good SQL code. 
Generally your database should be pretty much the only thing running on the server and you try to configure it to use all the resources. Running applications on the DB server is a recipe for a disasterous lack of resources.
Stagnation in education is one key element in why I move jobs, but sometimes companies realize this and they are willing to supplement training, conferences, books, time off for education, etc. Other companies don't and then they lose team members. 
I agree, but the data is pretty clear. The more you change jobs, the more you make. I'm actually in a pretty great situation right now (just passed 1 year) and could easily see myself staying for 5-10+ because of how well they pay, etc. BUT... I know I would be sacrificing promotions, etc. It's a hard balance and the older you get, or the more experience you have, it starts to become a double edged sword. But stay somewhere too long, and get too old, and you're done.
I'm at that point now where I'm clueless where to go with my career. I have a good 2-3 year plan, but after that, I have no idea. Pay and benefit wise, it seems incredibly unlikely I'll find anything better in my city and I will have to move or take a remote job. Remote work is probably just as much as a pipe dream as finding something locally though. And if I move, cost of living increases drastically. The only saving grace at the moment is we typically receive 10-15% raises annually here and they do pay for training and certs. The downside is that I expect them to cap my salary next year and if we lose this particular contract, I'm out of a job. Figuring out what to do in 3-5 and 5-10 years right now is what I am scrambling to figure out. I actually have a SQL meetup mixer tonight that hopefully will give me more local insight to what I should begin considering or doing to navigate / create 3+ year plans for my career. 
First thing you need to do is determine if you want to go into management, and if you do then the sky is the limit... CEO, CTO, CMO, VP, etc. -- These all leverage a supplemental skill set that has nothing to do with SQL or databases at all. If you don't want to go into management then you're looking at picking which specialized niche makes you happiest, pays the best, and has the best work environment. Then find a company you like with the right title + salary and prepare to retire. &gt;The only saving grace at the moment is we typically receive 10-15% raises annually That's good, but I received a 30% raise with my last job switch after 2 years, and am hoping to receive somewhere between 30-50% in a year if I switch again. 
Commenting because I'm am in the same boat. Looking for best practices to do so. I just finished going through a query with 35 unions in it and basically just put in what data each union was grabbing. And this is query 1 out of many that I need to do.
The answer to all of those is no sadly. We have a BI team that does runs the numbers on how good a promotion runs. I’m not interested in that. But the rest of your points are things I will look into. 
Thanks for the reply. Will look into that stuff
Changing roles is on the cards for me. However my role as an Ops Analyst is very broad. I’m being pushed into the senior space to manage our staff and I’m also the subject matter expert on our entire platform. So I’m learning heaps of new stuff still and I have a future here but the SQL stuff has gone stagnant because I’m at the upper limit of what the just b requires. Still it’s good to hear someone say find a new job
I'm in the process of creating documentation for tier one help support desk in regards to the ETL process for each application. I'm putting links in there to the excel Dimensional analysis. A basic viso Diagram showing TP to stg to rpt. I thought about listing each table however nobody is gonna read that. The dimensional analysis Has the table structures along with original table name And star table name. 
[removed]
 Cheers :)
&gt; First thing you need to do is determine if you want to go into management, and if you do then the sky is the limit... CEO, CTO, CMO, VP, etc. -- These all leverage a supplemental skill set that has nothing to do with SQL or databases at all. Yup, and I have no desire for that at all. &gt; If you don't want to go into management then you're looking at picking which specialized niche makes you happiest, pays the best, and has the best work environment. Then find a company you like with the right title + salary and prepare to retire. That's where the problem is, there's not a large job market. So I'd be looking at moving. Plus for preparing to retire, I have probably another 40 years of work still at least. &gt; That's good, but I received a 30% raise with my last job switch after 2 years, and am hoping to receive somewhere between 30-50% in a year if I switch again. Which is also another issue for me. I make 60% more myself than the average family does in this town, and the town pays 15% less than surrounding areas. So to figure out what I want to do and make more money locally is a very tricky problem. 
What I can tell you is that it's all about the title. You undoubtedly know more about SQL than I do, but I have a feeling when it comes to business that might not be the case. A title change without a raise is worth its weight in gold. In... lets say 5 years time this was the progression of my "title." Now bear in mind that about 5 years ago I jumped into analytics with almost zero experience, and up until about 3 years ago had almost zero experience with SQL. Admittedly I do come from a tech background, have programming certifications, experience with networking, marketing, etc. * VP, Enterprise Architect - This was a company I founded and ran for two years. * Consultant, Analytics &amp; System Design - This was a job I did for free for various clients (normally small agencies,) and in some cases I worked out private contracts to build various things for them. Total money made was minimal, but some of the time I was making &gt;$100/hr for a $1000 check. * Healthcare Project Manager - Short contract that paid very well. * Marketing Analytics Data Analyst - Very broad title, lots of experience working with senior management and working in very direct ways to use data to make predictions that would maximize our profits. Working with &amp; helping manage a multi-million dollar spend. Privately held life insurance company, 100M a year in revenue. Healthcare contract was a plus. * Analytic Developer, Senior Manager - Fortune 500 company, publicly traded, employees around the globe. SQL subject matter expert (no joke, scary) as it relates to our analytic technologies, ETL, and modeling capabilities. Fully remote job. So when I go looking in another year, or two, or maybe four depending on how well I continue to like the current gig some titles I will be looking for that will be natural progressions are: Data Scientist, Database Engineer, Director, Vice President. Titles are subject to change based on the size of the next company. If it's a small shop then VP or CTO aren't totally out of the question. If it's a much bigger company than where I'm at then staying a Senior Analyst might be the way to go. If I stay with my current company than by next year I will need a title change to something like Engineer. In all reality there is no difference and these are made up words but they look good on a resume and let me negotiate for a higher salary. Contrast those 5 entries over the course of about 6 years and compare it to one single entry that spans 6 years and just says: * Ops Analyst. If you stay with a company long term then you need a new title ever 2-3 years at first, and then a minimum of every 3-4 years thereafter, if only to give your resume a progression and flow... and to future employers that is a progression and flow of salary so when you ask for their maximum range, or above their maximum range they can feel good about saying yes because it looks like you deserve it. &gt;I have a future here but the SQL stuff has gone stagnant because I’m at the upper limit of what the just b requires. Is SQL where you want to stay? If the answer is yes then leave. If the answer is no then stay but consider what I'm saying above and plot where you want to be in a few years accordingly. People don't go from analyst to VP unless they start their own shop. Do you want to start your own shop? Do you even want to be a VP? Do you just want to maximize your earning potential and stay in a similar job than the one you currently have? -- if the answer to that is yes, then probably staying where you are is the right answer if you get good raises and aren't worried about them replacing you with someone younger, prettier, and less knowledgeable, like me :)
Thanks for the time to write that. I really appreciate it :) 
Can you give an example of a row right now and what you want that row to look like afterwards?
The company I work for is starting to use an outside company for delivery routes. That was one of their requirements for the data pull. This worked perfectly for what I needed. Thanks for your help
Col4 = Col1+Col2+Col3
thank you for this! Have been racking my brain trying to find an efficient way to compare record counts across various time periods. The LAG function is EXACTLY what I needed but wasn't aware of it's existence. This discovery made my day!
I like it! Thanks for sharing. When starting with nothing, sometimes it's hard to see where to begin. This definitely gives me some insight.
I'm glad I could help it was a good starting point for me. I added the runtimes in test Dev and prod. I also have the native Database listed along with the DBA. The use of flat files, Calendar conversions, And scripting. 
EmpId 2 Empname Dick EmpCity Detroit for example 
so you want your 4th column populated with 2DickDetroit I'm guessing there's a probably a better way to solve the issue that this solution is solving, but I've used [STUFF](https://www.w3schools.com/sql/func_sqlserver_stuff.asp) before when reporting multiple results into one column. I think you've already got the *best* answer as the top comment, but if there's more that you aren't giving us, look at STUFF
What I’ve found online is that I should change some files, but if I follow those instructions Moodle crashes!”
So should I move the Moodle installation to one server and leave the dB on another ?
Does this help? https://docs.moodle.org/34/en/Installing_Moodle &gt;The basic requirements for Moodle are as follows: &gt;Hardware &gt;Disk space: 200MB for the Moodle code, plus as much as you need to store content. 5GB is probably a realistic minimum. &gt;Processor: 1GHz (min), 2GHz dual core or more recommended. &gt;Memory: 512MB (min), 1GB or more is recommended. 8GB plus is likely on a large production server &gt;**Consider separate servers for the web "front ends" and the database. It is much easier to "tune"** &gt;All the above requirements will vary depending on specific hardware and software combinations as well as the type of use and load; busy sites may well require additional resources. Further guidance can be found under performance recommendations. Moodle scales easily by increasing hardware. https://docs.moodle.org/34/en/Performance_recommendations
https://www.edx.org/course/querying-data-transact-sql-microsoft-dat201x-0 Blew through this course and was very informative for me. 
Thanks for the quick response! Did that prepare you for this exam though? How was the exam?
It helped solidify what I already new as well as provided me with practice rather than just with reading. I'd say it did help. Exam went smooth. Practice, practice, practice and take your time if you can.
Awesome thanks!!!! I will check it out!
Delete From Vendors Where Vendor_Id = 123
Do a practice exam. I did mine at MeasureUp. 
The exam cost less than the practice with the student discount. I am figuring that it is a practice for me if necessary :) Only drawback is that I wont know what I got wrong.
Ok, if you don't have them already, I would start with putting together development standards on Database objects/SQL and SSIS/ETL packages. Pull together people currently writing code for each discipline and just cobble together the concepts the majority can agree on. These can include inline commenting standards. It doesn't need to be perfect, just a foundation to build on and drive consistency in your delivery process. You will be able to iterate it as you discover things that do and do not work. Make sure everyone who starts in a position writing code reads the standards and has an opportunity to suggest updates. Next, you need a knowledge repository and modeling toolset. A suggestion of some of the most common artifacts are conceptual models, logical models, physical models, data dictionaries, data flow diagrams, system context diagrams and component integration diagrams. Look up examples and put together templates of the ones that you want to keep. I also suggest you locate an Enterprise Architecture Overview Diagram or try to get one bootstraped because they are very helpful to get a high level concept of the technology side of the business. I also suggest that you build out a development template library with common coding patterns and templates. You can crowd source your best practices and boost productivity immensely. Other important documentation covers your infrastructure topology, source control methodology, development process and deployment methodology. I know this is more than you likely wanted, but hope it sparks some ideas. 
Yeah I took that exam last month. I had purchased a bundle that included the exam, retake , and practice test. I feel like the practice test helped more than the book. 
Ahh I didn't see that bundle when I ordered this first test. I will def look out for that if I don't do well this go around and will look for it for the rest of the exams. Were you able to do the practice exam from home as well?
Yes as many times as you want in a 1 month period. So I studied the book then attacked the practice exam. It’s good because you can take it as a simulation of the exam OR allow it to show you the right answers with explanation as you go. 
Wow... sounds awesome...I have been reading a few pages of the book every morning and evening on my commute... It sure has been helping me sleep sometimes :) Other times it has been truly interesting and a wow moment.
Agree. I’ve been doing this a while so I already knew much of the book. There were some gotchas and lesser used things. I’m working on 762 now. 
Yeah I haven't really used error handling before, which is new for me. I'm used to if there is a data problem that causes an error that it's my fault and that I need to adjust the query to account for it without making an error.. lol... Some of the stuff will be handy for that. 
Good luck!
&gt; I know the query works but I am having trouble understanding it. you and me both, brother NOT IN subquery containing another NOT IN *in the join condition*??? GTFO with that logic, lecturers!!
It makes no sense to me. It's using the same table 3 times for no good reason. It doesn't even use the loan and customer tables. I could understand it if it did 
Sounds like corruption. Did you have the server go down recently? Do you have backups?
Looks like maybe they were trying to do a double negation query (kind of like this one we discussed a few weeks ago https://www.reddit.com/r/SQL/comments/7bde0a/oracle_understanding_double_negation_in_sql/) but I think they would need to use EXISTS instead of IN and also correlate the outermost query to the inner one. I don't really know what OP's query is supposed to do other than be overly complex. I think this could easily be done with a single subquery or join.
Thank you for your input! This definitely sparks some ideas but is going to be a rough road to tackle some of this stuff with the developers because I am essentially in a different department but with the same job description. At least one of them would be all for helping but the others...meh. Such is life in business. Thanks again!
Break it down further. find specific rows that SHOULD join and try to join on them. then dump their actual values and look for trailing or leading space, or embedded high values. and Excel will lie to you and hid stuff. look at the data raw: export to a text file and look at in notepad++ 
I realize many developers want to stay technical, but working with your peers to develop standards shows great initiative and helps you develop valuable leadership skills that will help you in your career regardless of whether you want to manage or not. Consider discussing it with your manager as it could look good on your performance appraisal / resume and help the company. 
Thing is both tables are from imported Excel files. I have pulled rows with the join issue and I can see a difference between them whether it's in Excel or notepad++. The text does contain a number preface in the form of "2 - text" so I'm currently trying to make a new column in the data table that isolates the substring to try and join on that.
I grabbed that in e-book format with the intention to take the test also. Good luck to you.
I have my 70-761 on Friday, I've been studying hard for about two months. My best resources: https://www.mssqltips.com/sqlservertip/4644/sql-server-exam-70761-study-material-for-querying-data-with-transactsql/ https://www.cathrinewilhelmsen.net/2015/01/28/preparing-for-and-taking-exam-70-461-querying-microsoft-sql-server-2012/ I have done some additional googling and reading. http://www.sqlservercentral.com/stairway/92778/ https://www.amazon.com/T-SQL-Fundamentals-3rd-Itzik-Ben-Gan/dp/150930200X https://docs.microsoft.com/en-us/sql/relational-databases/tables/temporal-tables (Yes, I read the entire tech thing on temporal tables. Took a little while.) I also read through the 70-461 study guide and 70-761 guide.
So the data doesn't match is what you're saying? :)
The middle inner select returns nothing. So when the outer select checks to see if the cname is not in the middle select all of the names are returned. It's ultimately pointless.
Only according to ssms apparently lol. Everything else says it's all exactly the same. 
First, if you're in MSSQL, CTEs are your friend. Next, datepart is your friend. You'll want to add columns and group by the mm and yyyy. Also, if you have gaps in your data but want those months represented with 0's it's best to have a calendar table.
Pull one row from each where those two rows should match and aren't. Copy and paste the join column values into the ssms query window and look at them. There's probably an rtrim(ltrim( needed.
dbcc checkdb ('yourdbname') with no_infomsgs. You might be able to fix it, but it is probably easier to restore from backup.
I passed this test last month. Just a little test taking tip: As someone who reads slow, and thinks even slower, I had to employ some creative test-taking techniques. I would recommend using your deductive reasoning and process of elimination on the multiple choice questions to get through those questions as fast as possible. Mark the questions where you have to write SQL statements for review, and come back to them when you're done, because you can really burn a ton of time on those, and puts you at risk at not even finishing the test in time. I ended up not even starting a few of those questions, but passed the test with flying colors. Good luck!
Notepad++ has a [compare plugin](http://www.technicaloverload.com/compare-two-files-using-notepad/). It doesn't handle formatting well - but it's a great quick and dirty way to compare outputs. Tells you if they match, and highlights the rows that don't, with a bolder highlight on the exact mismatched column. 
This is the correct answer. You'll only have to write it once. You can also use the row_number function to determine a member's first visit and subsequent visits.
Thanks I'll check that
embedded High values characters, leading spaces, the fancy quotes vs the " and ' all those things can contribute. unicode vs non-unicode. case sensitive?
It shouldn't be an issue. The map is an Excel uploaded to SQL just like the data. What's more the map was made by taking the column from the data files and removing duplicates and merging together. The map is literally made of the data, so there should be no way that they're different.
Thanks for the advice. For the ones where you write queries, do you have to type from scratch or did you select the phrases from a word bank?
They will have both. I took my exam last Thursday and ran out of time but still passed. I read T-SQL Fundamentals and the 70-761 exam ref. Don't waste your time reading every little detail for the querying questions. Time management is hard for the exam. I'm reading T-SQL Querying and 70-762 exam ref now.
Why are you passing such a huge parameter list? Shouldn't your parameter list just be the date range?
It includes a list of the actual invoice numbers so the end user can pick individual invoices to print. I didn't design the report, just troubleshooting it. I did determine the actual length of the parameter is 4500 characters. I thought it was much larger. 
how is [dateid] stored in your DB? string, int, or date?
I am hesitant to agree that this is a problem with parameter length. Almost smells like a BETWEEN issue. What is the query from SSRS? 
Yeah, I'd make that parameter nullable and add date range parameters, so you can either print off a single invoice, or all in a range. 4500 characters for a parameter seems like a lot. That might not be your problem, but it's worth checking out.
I am beginning to think the same. Data is actually returned from a stored procedure. Which I run inside of SSMS, and get the correct number of records. Here is the query (I hope it's readable) ;with agency as ( SELECT o.VendorId, o.MailingCode, CASE WHEN o.BusinessName LIKE ('BLM %') THEN 'BLM' WHEN o.BusinessName LIKE ('% NF %') THEN 'NF' ELSE 'Private' END as 'Agency' FROM dbo.Owners o ) SELECT i.InvoiceNumber , i.RegistrationNumber , r.SaleName , CASE i.FeeType WHEN 'R' THEN 'REG' WHEN 'B' THEN 'BURN' END AS 'FeeType' , CASE WHEN r.BurnReason = 'M' AND r.BurnType IN ('B','F','U','N') THEN 'MNT' WHEN i.BurnFeeType = 'L' THEN 'LDG' WHEN i.BurnFeeType = 'P' THEN 'PILE' WHEN i.BurnFeeType = 'B' THEN 'BRD' END AS 'BurnType' , i.InvoiceDate , CASE WHEN i.FeeType = 'R' THEN r.UnitAcres WHEN i.FeeType = 'B' AND r.BurnType IN ('L', 'R') THEN r.LandingAcres WHEN i.FeeType = 'B' AND r.BurnType IN ('G', 'H', 'T') THEN r.PileAcres WHEN i.FeeType = 'B' AND r.BurnType IN ('B', 'F', 'N', 'S', 'U') THEN r.BroadcastAcres END AS 'Acres' , i.Fee , i.VendorID , i.MailingCode , CASE WHEN o.BusinessName IS NULL THEN o.FirstName + ' ' + o.LastName WHEN o.BusinessName = '' THEN o.FirstName + ' ' + o.LastName ELSE o.BusinessName END as 'Customer' , o.Address1 , o.Address2 , o.City , o.[State] , o.ZipCode , r.[Ownership] , CASE WHEN o.BusinessName LIKE ('BLM %') THEN 'BLM' WHEN o.BusinessName LIKE ('% NF %') THEN 'NF' ELSE 'Private' END as 'Agency' , an.Code as 'AgreementNumber' FROM dbo.Invoices i JOIN dbo.Owners o ON i.VendorID = o.VendorID AND i.MailingCode = o.MailingCode JOIN agency ag ON o.VendorID = ag.VendorID AND ag.MailingCode = o.MailingCode JOIN dbo.AgreementNumber an ON ag.Agency = an.BusinessEntity JOIN dbo.Registrations r ON i.RegistrationNumber = r.RegistrationNumber WHERE (r.[Ownership] IN (SELECT * FROM dbo.StringSplitter(@OwnerType)) OR r.[Ownership] = '') AND (i.InvoiceNumber IN (SELECT * FROM dbo.StringSplitter(@InvoiceNumber))) AND ( i.InvoiceDate &gt;= @StartDate AND i.InvoiceDate &lt; DATEADD(day, 1, @EndDate) ) ORDER BY CONVERT(INT, SUBSTRING(i.InvoiceNumber, PATINDEX('%[0-9]%', i.InvoiceNumber), LEN(i.InvoiceNumber))) 
Formatted: ;with agency as ( SELECT o.VendorId, o.MailingCode, CASE WHEN o.BusinessName LIKE ('BLM %') THEN 'BLM' WHEN o.BusinessName LIKE ('% NF %') THEN 'NF' ELSE 'Private' END as 'Agency' FROM dbo.Owners o ) SELECT i.InvoiceNumber , i.RegistrationNumber , r.SaleName , CASE i.FeeType WHEN 'R' THEN 'REG' WHEN 'B' THEN 'BURN' END AS 'FeeType' , CASE WHEN r.BurnReason = 'M' AND r.BurnType IN ('B','F','U','N') THEN 'MNT' WHEN i.BurnFeeType = 'L' THEN 'LDG' WHEN i.BurnFeeType = 'P' THEN 'PILE' WHEN i.BurnFeeType = 'B' THEN 'BRD' END AS 'BurnType' , i.InvoiceDate , CASE WHEN i.FeeType = 'R' THEN r.UnitAcres WHEN i.FeeType = 'B' AND r.BurnType IN ('L', 'R') THEN r.LandingAcres WHEN i.FeeType = 'B' AND r.BurnType IN ('G', 'H', 'T') THEN r.PileAcres WHEN i.FeeType = 'B' AND r.BurnType IN ('B', 'F', 'N', 'S', 'U') THEN r.BroadcastAcres END AS 'Acres' , i.Fee , i.VendorID , i.MailingCode , CASE WHEN o.BusinessName IS NULL THEN o.FirstName + ' ' + o.LastName WHEN o.BusinessName = '' THEN o.FirstName + ' ' + o.LastName ELSE o.BusinessName END as 'Customer' , o.Address1 , o.Address2 , o.City , o.[State] , o.ZipCode , r.[Ownership] , CASE WHEN o.BusinessName LIKE ('BLM %') THEN 'BLM' WHEN o.BusinessName LIKE ('% NF %') THEN 'NF' ELSE 'Private' END as 'Agency' , an.Code as 'AgreementNumber' FROM dbo.Invoices i JOIN dbo.Owners o ON i.VendorID = o.VendorID AND i.MailingCode = o.MailingCode JOIN agency ag ON o.VendorID = ag.VendorID AND ag.MailingCode = o.MailingCode JOIN dbo.AgreementNumber an ON ag.Agency = an.BusinessEntity JOIN dbo.Registrations r ON i.RegistrationNumber = r.RegistrationNumber WHERE (r.[Ownership] IN (SELECT * FROM dbo.StringSplitter(@OwnerType)) OR r.[Ownership] = '') AND (i.InvoiceNumber IN (SELECT * FROM dbo.StringSplitter(@InvoiceNumber))) AND ( i.InvoiceDate &gt;= @StartDate AND i.InvoiceDate &lt; DATEADD(day, 1, @EndDate) ) ORDER BY CONVERT(INT, SUBSTRING(i.InvoiceNumber, PATINDEX('%[0-9]%', i.InvoiceNumber), LEN(i.InvoiceNumber))) 
Thank you!
Sqlite3 with sqllite browser probably the smallest available. Since I'm used to it I'd personally go for Postgres and pgadmin3, they're maybe a couple 100mb altogether.
I modified the stored procedure that returns the data and commented out the code below and the report returns the correct number of records (345). So it looks like it has something to do with the query itself. AND (i.InvoiceNumber IN (SELECT * FROM dbo.StringSplitter(@InvoiceNumber)))
Known behavior, look here for explanation, and basically it's a Banking standard to round any. 5 to the nearest even integer https://social.msdn.microsoft.com/Forums/sqlserver/en-US/c1a4971a-7df1-4d88-81f4-43518a6b37d2/strange-round-function-ssrs?forum=sqlreportingservices
Thank you so much. Tried it with the work around and worked perfectly!
Go get an azure account. You can get a sql database up and running in about 5 mins that you can connect to with ssms. Performance tiers start at $5/month which should be plenty for test and experimental workloads. 
I use DBeaver as gui, it's free and you can connect to multiple databases. As for installing a database I recommend using postgresql or mysql.
I second this. Go pgadmin with Postgres and never look back. 
Recommending Postgres and pgadmin, multi platform, easy to start up and connect to. Even though our production environments are SQL Server, I always use Postgres for prototyping and I’ve even started using it for data analysis to overcome the many flaws of excel 
Since dateid looks to me like a string intead of a timestamp format, I would just pull the last two characters from dateid into a column labeled month and group by that column, then you can pull the data from the entire year and having it grouped by your monh column 
I've always been and always will be a SQL Server guy, but I agree with these guys.
[removed]
Unfortunately i have 5hrs old backup(it's a huge loss considering i'm working for a bank), hence some existing transactions are missing, When i try to migrate transaction tables from the existing db everything else gets migrated excepted for one, i think some data got corrupted in that table.. however those data are visible for around 2 ms before that error comes.
This is definitely the path of least resistance. Nothing to install or configure. Sqlite is literally a self contained file.
Ha, that’s what I work in all day. Still I am really enjoying psql these days. 
[removed]
WITH numbers AS (SELECT 1 AS n UNION ALL SELECT n + 1 FROM numbers WHERE n &lt; 125 ) SELECT strng FROM ( SELECT 'Chair' strng,--You need to do some kind of string manipulation to get this out from Chair 125 'Chair 125' strngnmbr, LEFT(SUBSTRING('Chair 123', PATINDEX('%[0-9.-]%', 'Chair 125'), 8000), PATINDEX('%[^0-9.-]%', SUBSTRING('Chair 125', PATINDEX('%[0-9.-]%', 'Chair 125'), 8000) + 'X') -1) nmbr) a INNER JOIN numbers ON a.nmbr &gt;= numbers.n 
I'd start with a numbers table (a table containg only sequentia wholel numbers - Google this for platform specific advice) The it would be something like SELECT item FROM yourtable y INNER JOIN numbers n ON y.quantity &gt;= n.number The result set will be the item name repeated as many times as the quantity value. Kind of a weird and highly specific thing to ask which makes me wonder why you'd want to do this though. 
Why use s deprecated feature?
Have you tried looking at the string in Notepad++ with viewing all symbols on? I'm also thinking a hidden character (like CHAR(9)) could be to blame.
Which feature?
export the excel to a TEXT file. then look at the data in the TEXT file. 
Does https://stackoverflow.com/questions/2594829/finding-duplicate-values-in-a-sql-table help?
Thanks for the reply. Having tried that, I get is invalid in the select list because it is not contained in either an aggregate function or the GROUP BY clause. 🤦🏻‍♂️ It’s probably me, I’m very new to SQL and self taught (only started a week ago) 
Thanks everyone! Installing now, I'm pumped to try this out. 
What I was going to say. Is this data from Excel? It's notorious for putting non-standard characters into data.
You would do this in the presentation layer, not in SQL. What is the front end?
Are you sure it's not a *truncation* issue? What is the field size you configured for the source column in SSIS? What is the field size for the actual database field?
the data is an export from our CRM it comes in .CSV format and yes I am modifying it in excel.
There is no front end really. I've been asked to generate a weekly report. We use TaskCentre which automates Querying, exporting to HTML and emailing
So your front end is a web page then.
Currently I have a fresh excel document as the source I'm saving it as a .CSV or text file in preparation for the import. the dataset is currently just a column title and the text **No Longer At This Address Do Not Send Correspondence Waldeck** on row 1. I have been importing this into a table listed as varchar(max) just to ensure its not a size issue. try it yourself, copy the text into a 1 row table and import it has me completely baffled.. 
I checked still no luck....
I would do what /u/shrillpetrol9424075 said. Bring it into notepad ++ and see if there are any weird characters. Just be sure to go to View-&gt; Show Symbol -&gt; Show All Characters and make sure it is checked so you can see if there are any of those non ASCII characters.
Well this was fun... SELECT m.FeeEarnerRef AS Initials, COALESCE(dbo.ContractEntityCode(M.Entityref), CAST(COUNT(M.Description) OVER (PARTITION BY M.FeeEarnerRef) AS varchar(10)) ) AS Entity, M.Number AS Matter, M.Created AS [Date Opened] FROM Matters AS M JOIN Users AS U ON M.FeeEarnerRef = U.Code WHERE Created &gt;= DATEADD(dd, -7, GETDATE()) AND U.Department = 'DM' GROUP BY GROUPING SETS ( (M.FeeEarnerRef, M.EntityRef, M.Number, M.Created), (M.FeeEarnerRef) ) ORDER BY M.FeeEarnerRef
Sounds like you are on the same path I am, we'll know tomorrow if I have to do 761 again or if I can move onto 762!
the date columns in your WHERE clause are located in the [Guest_Visit_Fact] table, correct? so what table holds the data that a patient is a new patient? and how, i.e. what coumn(s)?
Yes, they're located in the [Guest_Visit_Fact] table. It's in the same table, under the Visit_Type column.
Go into the flat file data source on this package. Go to Columns Take a screenshot of this column definition and post to this thread. 
I checked that still nothing...
clearly, intended for oracle, as shown by the non-standard sql in the example, despite no comment near the top of the article to warn us of this also, problematic proprietary date arithmetic how much effort would it be to write your tutorials to be platform-agnostic, using only standard sql? 
so what table holds the data that a patient is a new patient? 
Try changing your output column width in the import wizard&gt;advanced section to 100 (or more than 60). Got the same error as you at first, but I was able to successfully import.
&gt; 0xc02020a1 They all seem to be caused by truncation. Do you have the fields in Excel set to a strict size? If not some of [these](https://www.google.com/search?q=0xc02020a1&amp;ie=utf-8&amp;oe=utf-8&amp;client=firefox-b-1) might give you a clue as to where to start.
The same table, Guest_Visit_Fact
Tune in next week when this query is posted and the new question is: *How do I sum the summed lines to get a total and then put the subtotal on each line then the total at the bottom?*
Yeah that's fixed it, Thanks all for your input! 
Great! Happy to help. :)
"I think we should add a PIVOT, do you think we should add a PIVOT?" "I think we should UNPIVOT, **then** PIVOT!"
please describe ~how~ the Guest_Visit_Fact column identifies whether it's a new visitor that month 
 WITH CTE1 as ( SELECT M.FeeEarnerRef AS 'Initials', dbo.ContractEntityCode(M.Entityref) AS 'Entity', M.Number AS 'Matter', M.Created AS 'Date Opened' FROM Matters AS M JOIN Users AS U on M.FeeEarnerRef = U.Code WHERE Created &gt;= DATEADD(dd, -7, GETDATE()) AND U.Department = 'DM' ), CTE2 as ( SELECT Initials, Count(*) as Entity, NULL, NULL, NULL FROM FullTable GROUP BY Initials ) SELECT Initials, Entity, Matter, [Date Opened] FROM CTE1 UNION ALL SELECT Initials, CAST(Count(*) as varchar(10)) as Entity, NULL, NULL, NULL FROM CTE2 ORDER BY Initials, Matter nulls last Would this work? One CTE for his query, one for the count for each Initials, then UNION ALL and ORDER BY.
&gt; n identifies whether i If the user doesn't have an ID, it assigns an ID (member_guid). And, the visit_type would be 'WALK-IN NEW'
Have left the office now but will definitely give this a go in the morning! 
All SELECT columns must be in the GROUP BY or an aggregate function. Explicitly list all your columns in the GROUP BY clause and SELECT clause. Then add the COUNT(*) &gt; 1 in the HAVING clause.
Not a big deal - PostgreSQL just needs to add listagg as an alias for array_agg if it does the same thing. I for one am happy Markus is pushing the standard. The ONLY reason I've been able to move between SQL Server, PostgreSQL, and Redshift in my last 3 jobs, with minimal impact, is (mostly)-good support for standard SQL. Can you imagine how limiting it would be to only be able to apply to jobs that use the databases you have been trained on (for other old-timers, remember DBASE and it's capable but completely proprietary language? thank GOD we didn't keep going in that direction...)
It’s returning some results!! Thank you! (Both of you!) 🙌🏻 I’ll need to check the results but it’s looking positive!
&gt; And, the visit_type would be 'WALK-IN NEW' i'm still lost... how does that help with your original problem -- **The repeat walkins are incorrect, because it is returning ALL repeat walkins.**
yup... write a query that combines data from only a few tables save the query then write another query that uses the saved query in the FROM clause and combines it with the remaining tables
Try this out: (NOTE: the line that starts with LEFT needs to have the 7 be replaced with however many characters it takes to cover the date and month of your date string. So a string format of '2017-01-21 11:22:33' would be 7. If your date string doesn't go &lt;year&gt; then &lt;month&gt; first, comment out the LEFT line and uncomment the following part) SELECT [gvf].[playsite_id] AS [Loc] ,DATEPART(YEAR, [gvf].[date_id]) AS [Year] ,DATENAME(MONTH, [gvf].[date_id]) AS [Mon] ,[gvf2].[visitNum] AS [RepeatVisits] ,COUNT([gvf].[visit_type]) AS [NewVists] FROM [dbo].[Guest_Visit_Fact] AS [gvf] LEFT JOIN (SELECT [playsite_id] ,[date_id] ,COUNT([visit_type]) AS [visitNum] FROM [dbo].[Guest_Visit_Fact] WHERE [visit_type] = 'WALK-IN REPEAT' GROUP BY [playsite_id] ,DATEFROMPARTS(DATEPART(YEAR,[date_id]) ,DATEPART(MONTH,[date_id]) ,1)) AS [gvf2] ON [gvf].[playsite_id] = [gvf2].[playsite_id] AND LEFT([gvf].[date_id],7) = LEFT([gvf2].[date_id],7) AND /*I'm matching up rows by just comparing the left part of your date string, which only works if your date string is something like '2017-01-01 &lt;and maybe time values too&gt;'. If you want to use the actual year and month values from the dates, you can comment out the above line of code, and then uncomment the following 5 lines of code*/ --DATEFROMPARTS(DATEPART(YEAR,[gvf].[date_id]) -- ,DATEPART(MONTH,[gvf].[date_id]) -- ,1) = DATEFROMPARTS(DATEPART(YEAR,[gvf2].[date_id]) -- ,DATEPART(MONTH,[gvf2].[date_id]) -- ,1) AND [gvf].[visit_type] = 'WALK-IN NEW' WHERE [gvf].[visit_type] = 'WALK-IN NEW' GROUP BY [gvf].[playsite_id] ,[gvf].[date_id] ,[gvf2].[visitNum] ORDER BY [gvf].[playsite_id] ,[gvf].[date_id]
Did that, broke the query again. Any other ideas?
run the first query and save the data as a new table then do that second part again, except this time you're using the saved data instead of the query
Wait, queries aren't saved..? Shit the more you know. Thank you so much!
Why?
Like, what's the endgame? What are you *actually* trying to achieve?
RowNum would get ya there if the rows aren't exactly the same (like one has Phone1 filled in and another has Phone1 and Phone2).
Then you can prioritize based on a CASE statement in the ORDER BY clause of the RowNum, like a weighted ranking
Haha, been there more times than I can count! The SQL data import wizard only analyzes like the first 50 rows to derive the datatype size.
when you save a query, it just stores the text of the sql statement so when you use it like a table in the FROM clause, the engine actually runs the combined sql not surprisingly this does not mean reduced complexity, but it can
Can you post the StringSplitter function? What does the invoice number data look like? I assume a string of CSVs? Can you post a sample?
I’m trying to apply a machine learning exercise and the package that I am using is only able to work with disaggregated data.
Trying to do market basket analysis. The package that I am doing and all the methods that I know how to do this with require the data to be disaggregated 
I don't think they actually clicked that button. Just left it at varchar(50) as the default. But yeah the "Flat File Import Wizard" in the new version of SSMS seems to handle them a bit better. Probably checks more rows?
Does people.active have any null values?
Can you do a ftp transfer of the file?
Yes, the invoice number data is a string of CSVs. Here is the string splitter function. With SQL 2016 there is a split_string() function that does the same thing. USE [SmokeDataSystem] GO /****** Object: UserDefinedFunction [dbo].[StringSplitter] Script Date: 12/7/2017 3:29:42 PM ******/ SET ANSI_NULLS ON GO SET QUOTED_IDENTIFIER ON GO CREATE FUNCTION [dbo].[StringSplitter] ( @stringToSplit VARCHAR(MAX) ) RETURNS @returnList TABLE ([Name] [nvarchar] (500)) AS BEGIN DECLARE @name NVARCHAR(255) DECLARE @pos INT WHILE CHARINDEX(',', @stringToSplit) &gt; 0 BEGIN SELECT @pos = CHARINDEX(',', @stringToSplit) SELECT @name = SUBSTRING(@stringToSplit, 1, @pos-1) INSERT INTO @returnList SELECT @name SELECT @stringToSplit = SUBSTRING(@stringToSplit, @pos+1, LEN(@stringToSplit)-@pos) END INSERT INTO @returnList SELECT @stringToSplit RETURN END GO 
rip to your bank's dba's career
Can’t you do this using views? 
that's exactly what a saved query is in MS Access
Oh right, I’m not very familiar with access 
If by access you mean will a dbcreator be able to modify another dbcreator then no.
You say you are using ssh. Just scp the file to the server, ssh into the server, and run it there. 
Well, now you mention it, a grand total wouldn't go a miss!!!!
These are the fields that you are selecting: &gt;"select T2.""FJBUYN"" ""c8"", T1.""FIBRDN"" ""c2"" , T1.""FIIDE1"" ""c3"" , T1.""FIPCKI"" ""c4"" , T1.""FISZEI"" ""c5"" , T2.""FJARCD"" ""c6"" , T2.""FJRSTI"" ""c7"" Then you have 2 tables that you are selecting from: &gt;from ""HSADATA"".""FJITMBL0"" T2, ""HSADATA"".""FIITMAL0"" T1 FJITMBL0 is being Aliased as T2, FIITMAL0 is being aliased as T1 These are the conditions of what gets displayed. The first 2 are important because they are what creates an intersection (join) of T1 and T2. &gt;where T1.""FICMPN"" = T2.""FJCMPN"" and T1.""FIITMN"" = T2.""FJITMN"" and T2.""FJDPTN"" = 'B' and LTRIM(T1.""FIITMN"") = 'C' and T1.""FICMPN"" = ' 11' and T2.""FJCMPN"" = ' 11'" 
Or cast(col1 as varchar) + '+' + cast(col2 as varchar), etc. Only commenting because if the are ints then Col4 will be a product not a composite.
My guess is something to do with the HAVING clause and the [forms] references. Try replacing the StartDateQuery and EndDateQuery with literal dates to test and see if the query will run. Then you would know that is the problem.
Sorry I'm new to access and SQL, is this the proper way to write that? HAVING ((([Monthly Reports Table].[Date of Reference Month/Year]) Between (01/01/2015) And (12/01/2016))); If so, the query ran through this time. However it brought back no results at all, just empty fields. Nvm, I got another error message, *" Syntax Error Missing Operator in Query Expression 'Date of Referenced Month/Year'"*
Dates need \# identifier in Access. Try this: HAVING ((([Monthly Reports Table].[Date of Reference Month/Year]) Between (\#01/01/2015\#) And (\#12/01/2016\#)));
Okay did that new error: "Data type mismatch in criteria expression" 
yes, if you are eventually going to board sensitive data in.
Be careful doing the whole port forwarding thing to open up your home network. Don't feed the script kiddies. Anyway, wouldn't it be cool to practice some security stuff? It's kinda important!
You can learn SQL easily at livesql.oracle.com
Absolutely, check out this [link](https://www.microsoftpressstore.com/articles/article.aspx?p=2201633&amp;seqNum=3) (A little way down, under the Logical Query Processing Phases heading). The key point being: *FROM *WHERE *GROUP BY *HAVING *SELECT *ORDER BY This is why you can use an alias from the SELECT in your ORDER BY but not in the WHERE clause. 
For finance, think about this different types of transactions that a company could have (accounts receivable, accounts payable, etc), then you could tie that to purchase orders of specific inventory items. Those inventory items could have categories, price (and don't forget the price change over time and how you would track that), discounts. Customer service is a huge area that could involve incidents, merchandise return, frequently asked questions (on the customer service side as like an internal "quick look up" of common tasks). You could expand the FAQ to be public facing too. All while hashing this out, think about how these new entities could impact, or relate, your existing entities. That should definitely get you closer to 50+ tables if that's required. I personally try to minimize as many tables as possible because the more complex your data model gets, the harder it is to maintain.
I looked at your query and didn't see any logic to filter out 'M' type records from table_c. Also wasn't sure why you have bc.B_ID_FK IS NULL in the where clause. /* Original Query*/ SELECT * FROM Table_BC_Line bc INNER JOIN Table_C c ON bc.C_ID_FK = c.C_ID AND c.C_Type = 'M' RIGHT JOIN Table_B b ON bc.B_ID_FK = b.B_ID WHERE bc.B_ID_FK IS NULL; See below. I wrote 2 queries that should return the same results . Wanted to show you examples for the "exists" and "in" clauses. Also I'd try to avoid using ANSI syntax and stick to the traditional join syntax since most online resources aren't shown in ANSI. /* Exists */ SELECT * FROM table_b b WHERE EXISTS ( SELECT 1 FROM table_c c,table_bc_line bc WHERE c.c_id = bc.c_id_fk AND b.b_id = bc.b_id_fk AND upper(c.c_type) = 'V' AND upper(c.c_type) &lt;&gt; 'M'); /* IN Clause */ SELECT * FROM table_b b WHERE b.b_id IN ( SELECT bc.b_id_fk FROM table_c c,table_bc_line bc WHERE c.c_id = bc.c_id_fk AND upper(c.c_type) = 'V' AND upper(c.c_type) &lt;&gt; 'M');
First, thanks for the info! My understanding of what my query did is: * Select all records in Table_BC_Line that had a FK for Table_C records with C_Type='M' * Right join that with Table_B on B_ID, so I get all the Table_B records related to the Table_BC_Line records, plus all remaining Table_B records (these will have null B_ID_FK related to them). * I then use WHERE to select only the B records where B_ID_FK is null, essentially removing any B that was related to a Table_C record with C_Type='M'. That's my understanding of how it works, albeit I might not be explaining it very well. Seems to work when I test it, it's just not the information that I want. I'm afraid I don't know what you mean by ANSI syntax? This was just the way my prof told us to do it (that's not saying much, he's not very good). I recreated your queries and tested them. I knew you could layer selects but I wasn't sure about how to do it. Unfortunately, they aren't giving me the output I want. I have B_ID=2 and B_ID=4 returned. #4 is the problem; it is related to Table_C records with types O, V, and M. Because it has M, it should be disqualified. I'm not sure I conceptually understand how WHERE EXISTS works. The inner SELECT is looking at both Table_C and Table_BC, and tries to find records in C that match BC, and records in BC that match B, where Type is 'V' and Type is NOT 'M'. If something is found, it prints out just the related record in B, correct? Am I understanding that correctly?
One downside to minimizing tables , say because they have all the same structured data, is that you then have many many tables with foreign Keys pointing to that singular table. So if you ever need to do maintenance on the table like deleting values, all of those foreign Keys have to be checked and it becomes very cumbersome to delete any values under normal circumstances.
Not sure I understand what you mean. The more table you have, the more joins you're going to need in order to perform any sort of delete function. The more joins you have in a query will result in slower performance. There's no need in having more tables than necessary just for the sake of having many tables. 
You can try the options in this https://stackoverflow.com/questions/38808516/how-to-save-sql-query-result-to-xml-file-on-disk 
Hey - I made a few updates to the sql which should fix the issue with my original query returning b_id = 4. Essentially I just added an additional anti-join ("NOT IN" Clause)to the query to filter out any rows from the final result set that had a c_type = 'M'. Both SQL statements below return the same result set. The first query shows an example of using an inline subquery in the from clause. The second query uses both an "IN" and "NOT IN" clause. Also the "EXISTS" and "IN" clauses can be used interchangeably, in older versions of Oracle the "IN" and "NOT IN" were a lot slower performance wise compared to the "EXISTS" and "NOT EXISTS" but in recent versions they both perform about the same. SELECT a.b_id_fk, a.c_id_fk, a.b_id, a.b_name, a.c_id, a.c_name, a.c_type FROM ( SELECT bc.b_id_fk,bc.c_id_fk,b.b_id,b.b_name,c.c_id,c.c_name, c.c_type FROM table_bc_line bc, table_b b,table_c c WHERE bc.b_id_fk = b.b_id AND bc.c_id_fk = c.c_id AND c.c_type = 'V' ) a WHERE (a.b_id_fk) NOT IN ( SELECT bc.b_id_fk FROM table_bc_line bc,table_c c WHERE bc.c_id_fk = c.c_id AND bc.b_id_fk = a.b_id AND c.c_type ='M'); SELECT b.b_id,b.b_name,c.c_id,c.c_name,c.c_type,bc.b_id_fk, bc.c_id_fk FROM table_b b,table_c c,table_bc_line bc WHERE b.b_id = bc.b_id_fk AND c.c_id = bc.c_id_fk AND (bc.c_id_fk, bc.b_id_fk, c.c_type) IN( SELECT bc.c_id_fk,bc.b_id_fk, c.c_type FROM table_bc_line bc, table_c c WHERE bc.c_id_fk = c.c_id AND bc.b_id_fk = b.b_id AND c.c_type = 'V') AND (bc.b_id_fk) NOT IN( SELECT bc.b_id_fk FROM table_bc_line bc, table_c c WHERE bc.c_id_fk = c.c_id AND bc.b_id_fk = b.b_id AND c.c_type = 'M'); 
You can also include like Employees and Payroll that should get you 10 or so more tables to tie into your finances.
I am in the same boat as OP so thank you for this response! I started learning T-SQL just recently but am still in the dark about the platforms to use it in everyday/work situations. The postgres website doesn't explain much to someone unfamiliar with it, so could you please explain how it works in layman's terms? Looking at pgadmin site I get it's the GUI into which I'd be inputting my queries. Thanks in advance
Table aliases are just so you can read the query and to remove any ambiguity as to which tables' columns you are referring to. SQL really isn't line-by-line. The DBMS reads the entire query and puts it through a sort of compiler and optimizer to get the most efficient way to get the data. This includes taking advantage of indexes, different methods to join tables together, etc. I'm not sure where you got the line by line thing but it's wrong. If has to look at the FROM clause first, obviously, or else it would have no idea where to get the data. Then it looks at the WHERE and applies it to the tables and columns specified in the FROM, JOIN, and SELECT. Then comes aggregate stuff like GROUP BY, and finally any ORDER BY.
[removed]
Are you planning on accessing your PC remotely? If not, there's not much you need to do.
I assume Im too late to help you with this, but on Pluralsight there's a course for the 70-461 exam. Part 2. That has a section on using XML that is explains it well and is concise, with basic examples. The section is only 30 mins or so. Think the site lets you watch a few trial hours for free without having to pay. Good luck.
Postgres is a database server, while it's running it can accept connections and respond to SQL commands. Pgadmin is a tool that you can use to connect to Postgres and send SQL commands. The Postgres website provides links to some pre-built packages for Windows, or tips on using a Linux package manager to install.
Ah I get it now. Thank you!
I am assuming they work through the courses in alphanumerical order for each course number (i.e. the first 3 letters) - I'd look to join the student table to the course table with some of rank/partition on the course table and adding a plus one (i.e. join on left(coursenumber, 3) AND rank = rank + 1. [This may be of use](https://blog.sqlauthority.com/2014/03/09/mysql-reset-row-number-for-each-group-partition-by-row-number/). You'll need to think about error handling; how you handle a student who has done the last of a course, does multiple courses of the same course number or if they do 2 and 3 without doing the first. 
Assuming the first three characters of the course number are unique (Realised now they might not be?): DECLARE @StudentId INT = 1911952219 SELECT SCI.StudentId ,SCI.CourseNumber ,(SELECT COALESCE(MIN(C.CourseNumber),'No Further Courses') FROM Courses C WHERE LEFT(C.CourseNumber, 3) = LEFT(SCI.CourseNumber, 3) AND C.CourseNumber &gt; SCI.CourseNumber) AS 'NextCourse' FROM StudentCourseInfo SCI WHERE StudentId = @StudentId
That's a tough one, mostly because of the table structure itself. If you have control over changing the table structure, I would recommend having a three table setup: * Student table: contains student ID, name, and other related data. * Course table: contains course ID, course name, and course order (integer), and course group ID. * Course Group table: contains course groups such as MATH, ENGL, SCI, etc. * Student_Course table: this will be a many-to-many table that maps students to their courses. Once you have that in place, you can get a list of course with the next highest course order based on the student's current courses. SELECT s.student_id, s.student_name, c2.course_id, c2.course_name FROM student s INNER JOIN student_course sc ON s.id = sc.student_id INNER JOIN course c ON sc.course_id = c.course_id INNER JOIN course c2 ON c.course_group_id = c2.course_group_id AND c2.order = c.order + 1 WHERE s.student_id = :student_id;
Definitely agree. If it's not an assignment with a fixed schema, change it. 
Wait I think I figured it out. SELECT Street, City, State, ZipCode, CustomerName FROM Address, Customer WHERE CustomerName is NULL Is this correct?
there are other ways to do this i just fixed your query, is all but there are other ways to do this SELECT T.tname , ' ' AS column_name , T.tname AS sort_table , 1 AS sort_order FROM tab T UNION ALL SELECT ' ' , column_name , tname , 2 FROM tab T INNER JOIN user_tab_columns U ON U.table_name = T.tname ORDER BY sort_table , sort_order 
Select OutputName From ( Select TableName ,TableName as OutputName ,0 as Sort From Tables Union Select TableName ,ColumnName ,OrdinalPosition From Columns ) o Order By TableName, Sort In mobile, so assuming poor formatting! 
Thanks, I only want tname and column name though. 
Says table or view does not exist.
fine... then you'll have to accept them in any order
That is not a requirement.
No, it won’t do. I’m not using real schema. You need to adjust the query to meet your schema. Think of this as a blue print.
fine, this is fine remove those extra two columns from the SELECT clauses, and drop the ORDER BY clause please do report back on how you like the results
I have nothing offhand, however, many programming books have samples that are provided for free. Go find a freebie book on download the practice material. 
The answer on SO appears correct. You use a subquery to exclude the IDs that have been disabled.
I don't quite get it, why would you need a separate primary key than - say either one of the significant columns to parse your records?
Stick with the basics - use standard numeric (INT+) primary keys and join records to INT columns between tables. There's no point or advantage to using compounds as a primary key and it will only make things unnecessarily difficult. You can always index fields to only contain unique values (like your station callsigns). If you want to be able to see multiple, combined info at a glance - like your example - you should make a query to compound the record information together instead. Honestly I would read up and understand the basics of table/database/SQL structure before you attempt it. Do some simple joins and queries. It'll make your project that much easier. Good luck!
can you please provide context, what are you trying to achieve?
Sure. I want list those students that enlisted first. Cheers, 
Have you considered using order by and top?
No, I haven't. The 2nd example I gave works for me. I am interested in understanding why the first one won't do the same as the second one. Cheers, 
Running the first query should return a pretty descriptive error: An aggregate may not appear in the WHERE clause unless it is in a subquery contained in a HAVING clause or a select list, and the column being aggregated is an outer reference. The second query does exactly this. This [link](https://stackoverflow.com/questions/42470849/why-are-aggregate-functions-not-allowed-in-where-clause) should give you an understanding of why. 
Thanks, Mark, for doing the benchmark. We are really excited that we came out to be the fastest GPU database. 
What if someone tries to access the records from the transaction before the timeout? Ask your teammate to give you a good reason why letting the transaction timeout is beneficial.
The dude that responded is extremely knowledgeable, and his solution is good. Go with the response from SO for sure.
Sounds like a "I do it because it works for me" sort of thing.
Bigger question is why do you think you need a primary key in the second table at all? Not every table needs a primary key. That is "Access" mentality
tl;dr - you can't say "= min(something)" because by itself there's not enough info to determine that. When you say "WHERE date = min(something), you are implying one thing and not more than one thing. But your value is min(date), but what does that mean? Taken by itself, there's not enough information to say what min(date) is. Min(date) from where? What date? Min(date) of what condition(s)? That's why you need a subquery. The subquery provides all the information needed to reduce the data down to one single value that is required by the equal sign. Essentially in the top example you've got the SELECT (you want min(date)), but it doesn't have a FROM (where are you getting it?). That's why you need a valid query that provides exactness and eliminates ambiguity. 
&gt; or increments by one each time. This is exactly what you want it to do... right? 
So, 'min()' is what's called an aggregate function. It takes a bunch of rows, and condenses it down to a single value. 'min()' produces the smallest of **a result set**. The where clause works on a row by row evaluation. It looks at each row, in sequence (half truth, but conceptually for now, true enough), and determines if it qualifies to be returned in the result set. By the time it's evaluating the where clause, it still doesn't know what the minimum value is of the results, and furthermore, it never even will look at the result set. The where clause will only look at a single row at a time. Saying "Does the minimum date of this single row equal the date of the same row" doesn't even make sense, it will always be true. 
No, I need it to tell me what order that row was in events that meet that criteria. Say i pull all events from projects with that user, it would be 5000 | RAdams | 00:14 5000 | JSmith | 01:13 5000 | JBryant | 02:48 5001 | JJones | 03:52 5001 | MTyson | 06:13 5001 | JSmith | 11:09 I want to know what order JSmith's event was out of all the events for that projectid without manually calculating it out of an export, so the result I want is 5000 | JSmith | 01:13 | 2 5001 | JSmith | 11:09 | 3 Since it's the second event in the list. 
Add `WHERE User = 'JSmith'` to the query to filter on that specific user.
You lost me a little bit with your date example. Did you simplify the date into a single number? If the column has been defined as a date or a datetime then simply order by [date] which should return the row by the ascending date. Sometimes if the date field is a defined as a string (and filled in by some external application) then you ought to recast it as a datetime like such Select * from dbo.myTable order by cast(mydate as datetime) 
I've got that. I just can't get it to tell me what place in the list of items "JSmith"s item was. 
What would you join on if not a primary key?
The date column is a datetime. This is the actual data: https://i.imgur.com/H78ffV6.png I need the SubmissionOrder column to tell me out of that SubmissionTotal column, what number, chronologically was that row. 
Ok it's still not very clear what you want but I think you might be looking for DENSE_RANK https://docs.microsoft.com/en-us/sql/t-sql/functions/dense-rank-transact-sql
you are allowed to join any column to any column whether it makes sense or not is really up to you you can join weather records to stations based on callsign without callsign in the weather table being declared a foreign key
&gt; Select *Lots of nonsense*, &gt; ROW_NUMBER () OVER (PARTITION BY abp.projectid order by abp.Submissiondate ASC) as EventOrder I don't think you need to partition this if you are looking at every row. ROW_NUMBER () OVER (order by submissiondate) as EventOrder
She tells me that as long as a commit is not fired the records are not affected, so letting the session timeout while getting a coffee etc. seems ok to her. She is pretty new to the team, and this was what she said. I couldn't give a concrete reason that what she said was wrog as I was not sure whether before the commit, the temporarily altered value only exists for that particular session or the entire DB.
not sure about the collations, but you have an error with WHEN NULL that particular syntax of the CASE expression uses equality, and as you know, nothing is equal to null try this SELECT T1.id , COALESCE(aliasSub.alias,T1.name2) AS NewName FROM T1 LEFT JOIN ( SELECT id , alias FROM T2 WHERE a bunch of stuff ) AS aliasSub ON aliasSub.id = T1.id 
This makes no sense, if your code worked as intended, the data should be committed immediately and available to be queried. If you haven't yet, at some point some blocking issues will likely occur. https://en.wikipedia.org/wiki/ACID#Locking_vs_multiversioning
Thanks! Tried it, but got the same error.
This is a typical case where you _want_ to have a synthetic key while also thinking about what would user do to identify a single record - this might become your alternative key/unique constraint if you want to enforce data integrity on the DB side. Simple integers of sufficient size tend to perform best as a key. Secondly, sometimes it does help to have a user-friendly synthetic ID - PO/invoice numbers, property MLS, etc. The common theme for these use cases is that the "object"/"record" will need to be quickly and uniquely referenced outside of your application/DB context and regardless of their version. I would think an individual weather reading does not fit this use case, so you do not need to create that smashed-together key, as long as your end-user has a way to identify a single record by a variety of attributes.
Don't do both in a single step. Do the row_number logic first, then filter down the result (as a subquery or a CTE). 
To clarify: you have 2 tables. One is the weather stations (pk station callsign) and the other is measurements. Why can’t the pk on measurements be (callsign, time stamp) ?
FYI, you're getting the same collation as aliasSub.alias in this case and shoe-horning a value in apparently different collation (t2.name2) into the same column. This possibly will mess the value up if the collations are sufficiently different and strings use relevant characters. 
Too late for you now, but you're looking FOR XML AUTO and probably with ROOT and ELEMENT portions added to it. It's a sort of niche utility SQL Server has, but it goes pretty in depth. This tutorial does a fantastic job, but it is quite dry. http://www.sqlservercentral.com/stairway/92778/
Yeah, this is what I was looking for haha. Oh well, I think I still get a B in the course. Thanks for the info!
SQL*Plus has an optional setting to commit automaticaly after every successful transaction: SET AUTOCOMMIT ON. You should check that setting.
Would an outer apply be more efficient here?
That's what I'm doing I think. As soon as I get one more criteria back, I'm going to just write the whole thing into a temp table with the ordering in place and then query that to get it down to the user level.
your ISNULL and my COALESCE are exactly the same