This looks like some old version of SQL, like Ingres or something like that. Do you know what this is running on? One thing that jumped out real quick (on mobile, haven't looked in depth) is the trunc and all of the conversations in the where for the dates. That could get expensive and time consuming. Another thing to look at, there are really 3 queries there brought together by the UNION ALLs. I would look and see which (if any) is the long running one of the 3. 
&gt; WHERE trunc(fh_request.create_date) If you have an index on the create_date, wrapping it in a function (trunc) will prevent use of the index. &gt; AND user_id.email_address LIKE '%@cybercases%' Using a wildcard at the start of the pattern also will prevent use of an index (if one exists) 
If you could post the information about the tables (names/columns) it would be easier to help. Also what database engine it is (SQL Server, Oracle, etc.). Since the syntax is different from them for the queries you are trying to do. TBH, I'd probably do this in Python and have it export an Excel file with the data. It sounds like this might be a little beyond what you'd want to do in VBA with the dynamic stuff. It sounds like you're pretty smart and have been around this type of thing before so a day of learning Python and you'd honestly probably be halfway there.
I'm not convinced this is correct. I've worked with normalized data that looks like this where you need to do something like `select pk, sum(total) from table group by pk` to get totals. Based on the PK/FK relationship I'm going to assume his data is normalized and he just wants to create a column for each item to make it simpler to read.
what sql server are you on? 2017 has STRING_AGG SELECT STRING_AGG(Comment, ',') AS Comment FROM Transaction if you dont have that version: https://stackoverflow.com/questions/31211506/how-stuff-and-for-xml-path-work-in-sql-server
The DB is a mess top to bottom. Didn't really want to get into that as I have no control over that and I just have to live with. I try your suggestion.
I'm on SQL Server 2014. Thanks for the link. I do want to concatenate but having the comment for each row on a single row as seperate columns would work as well.
Hey, CesQ89, just a quick heads-up: **seperate** is actually spelled **separate**. You can remember it by **-par- in the middle**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
I had someone tell me this morning that “comma joins” are still the convention in Oracle. 
Good to know, thanks for this
Add a team tag to the results table. It could be just a letter. So for a particular fight you have fighters on teams "A" and "B". If the team is blank then it is only an individual fighter. To find the team mates link to the results table on the fight and the team where the fighter does not equal the fighter you looked up.
First, what platform are you using? Second, you may need to account for the possibility of one ID showing up in multiple columns Say: ColumnA|ColumnB|ColumnC -|-|- 1|2|3 2|3|4 3|3|5 If you counted the distinct Ids in each column, and then added them all together, you'd get 8 (1, 2, 3 in ColA, 2, 3 in ColB, 3, 4, 5 in ColC), when the actual number of distinct values is 5 (1 in ColA, 2, 3 in ColB, 4, 5 in ColC). In MS SQL, you can use this query: SELECT COUNT(*) FROM ( SELECT ColumnA FROM &lt;Table&gt; UNION SELECT ColumnB FROM &lt;Table&gt; UNION SELECT ColumnC FROM &lt;Table&gt; ) Ids(Id) There's probably a faster way to do it, but if all the referenced columns are indexed, it should be fine 
Most DBs has the AVERAGE or AVG function, so try: &amp;#x200B; SELECT AVG( totalfreight) FROM Orders
Well there is no total freight, There is multiple ship cities, but each of these ship cities have varying freight costs depending. EX: Ship city a can have freight 10 and 51.2 later in the database
So let’s say Mario won his 3 stock match with Luigi on his team against Bowser and Bowser Jr (2-1-0-0) I’d enter into the results table fighter ID: Mario Fight ID: 0001 Result: W Stock: 2 Tag Team: A Then another entry Fighter ID: Luigi Fight ID: 0001 Result: W Stock: 1 Tag Team: A Another Fighter ID: Bowser Fight ID: 0001 Result: L Stock: 0 Tag Team: B Another Fighter ID: Bowser Jr Fight ID: 0001 Result: L Stock: 0 Tag Team: B Wouldn’t that still require me to add in a separate table that has the Tag Team ID’s (A,B)? Currently, If Bowser fought on like Mario’s Team next time that would cause issues, wouldn’t it? 
Not very far from my query then: &amp;#x200B; SELECT ShipCity, AVG(freight) FROM Order GROUP BY ShipCity &amp;#x200B; That will automatically calculate the average value of freights for each Ship City.
Great solution! Windows functions are awesome!
Sounds good. So this is my first SQL project and I’m trying to set up my schema still. It’s very confusing to me because what I’m trying to do is sort of complex for a beginner. When I was younger my brother and I basically created a WWE style season of Super Smash Bros Brawl on the Wii. I don’t know if you’re familiar but it’s basically a fighting game for Nintendo characters. We would pit characters like Mario and Bowser against each other and record the results (stock remaining, wins, losses). We had championships for the best characters. I believe I have over 1000 matches in a notebook. We would also have tag team matches so Mario and Luigi could fight Bowser and Peach at the same time. Currently I’m trying to work with these three tables: Fighter, Fight, Results. Fighter and Fight are tables for sure but I’m on the fence about trying to remove results. **Fighter** - Fighter ID - Name - Game Series - Color - Size **Fight** - Fight ID - Fight Type - Fight Location - [several more that aren’t applicable] Since a fighter can have many fights in his career and a fight always has at least two fighter I needed to create a table for the relationship and I called it results **Results** - Result ID (PK) - Fight ID (FK) - Fighter ID (FK) - decision (W or L) - Stock remaining This way it would be easy to count wins because I could just join and select count the decision column where = ‘W’ However, I want to be able to structure my database so I can write a query showing me Mario’s record with Luigi on his team as well as showing Mario’s record against Luigi in 1v1. I don’t know how to structure that. The reason I asked my original question was because I was thinking of making this as the fight table: - Fight ID - Winning Fighter 1 - Winning fighter 2 - losing fighter 1 - Losing fighter 2 But that brings up a possibility where Mario’s ID could be in the winning fighter 1 column and winning fighter 2 column (we have many different teams we’ve created over the 1000 matches we made). Which is why I was trying to see if it was possible to count how many times Mario’s name appeared in multiple columns and display it as one number so I could see his number of career wins. 
Alright, good detail. I’ll look over this some more tomorrow morning and see how I can help. I’m about to pass out as it is.
Thank you! I look forward to your response
This just sounds like the usual DEPT/EMPLOYEE examples, where the team is a dept, and a fighter is an employee. 
Try datacamp, mode analytics, and strata scratch. &amp;#x200B; Mode analytics has a nice tutorial with several levels from beginner to advanced. Datacamp is decent for some very specific applications of SQL and other languages. Strata Scratch has a bunch of practice problems on a real IDE that people use in Industry. I know it's the same IDE as what Airbnb uses.
You would only use the team letter to define who was on each team for a particular match. It would make no sense to select everyone who has been on team B. In a lot of video games with 2 teams you have red vs. blue. So the players on the red side are only defined for that game. On the next game some of the players could switch sides. But you still only have red vs. blue. So to identify the team mates of someone you would need to know which team the person is on and which game. 
When you do the join for the team mates you would join on both the Fight ID and Tag Team.
I think you're best off with three case statement inside a SUM aggregation: SELECT SUM(CASE WHEN ColumnA = 3 THEN 1 ELSE 0 END + CASE WHEN ColumnB = 3 THEN 1 ELSE 0 END + CASE WHEN ColumnC = 3 THEN 1 ELSE 0 END) as THREE_CNTR FROM table1
[Here](https://www.reddit.com/r/SQL/comments/9ip511/is_data_engineer_a_stepping_stone_to_becoming_a/e6m1eat/) is a comment that I made to someone else which is relevant here. From a high level Informatica, which my company uses, is an "ETL" tool. Tableau, which we also use, is an "analytics" (I hate saying that), or *reporting*, "front-end" tool. Totally separate branches of the tree. I use quote because lots of people have their own opinions. Anyway, an analogous question to the one you're asking is, "I eat ice cream every day, but have never tried chocolate or vanilla. Which one do you think I'd like more? Strawberry is my favorite." It isn't bad to learn things on all the different ends of the process, and it will make you stronger and a better asset for future positions. But which one do you really like the most? Go in that direction.
You poor soul. 2 minutes and 10 seconds. 
I was making a joke (tree fiddy).
Me too :-)
completely two different things. 15 years of experience in both here. informatica is for ETL development. Tableu is a reporting tool, it has some ETL, but its just for mini ODS purposes (Opereationl data sources). Informatica is a beast in its domain, just like SSIS, Datastage, ODI. you can start from Informatica. its not hard to learn. I teach Informatica, and in one week my students (that never had experience in ETL) start developing real stuff. 
For simple stuff like this I intuitively go to rollup as well. 
I use sql data tools (SSDT) in visual studio for large scale comparisons. It spits out a really handy table telling you what is identical in both and what is only available in the source or target database. 
Oracle SQL Developer.
 select isnull(paymentsagg.datepay,debtsagg.datedebts) as Dt ,isnull(paymentagg.amount,0.00) as totalPay ,isnull(debtsagg.amount,0.00) as totalDebts from ( select datepay ,sum(amount) as amount from payments group by datepay ) as paymentagg full outer join ( select datedebts ,sum(amount) as amount from debts group by datedebts ) as debtsagg on paymentsagg.datepay = debtsagg.datedebts
Ideally, both. You're in a similar position as I was before as Data Analyst trying to transition to a SQL Developer/ETL/etc job. There's a lot of question that are going to be asked about your knowledge. Having knowledge of a database (SQL and a Procedural Extension), a data visualization tool (Tableau), and an ETL tool (Informatica) covers the basics. I have never worked with OBIEE, but it seems to cover the same ground as Tableau. So, I would eliminate that unless it gives you a greater advantage at advancement internally. Hadoop would be useful to learn eventually, and in my opinion, after Tableau and Informatica.
Thank you all :)
Someone may know a better way, but I'd assume you just pick another column that's for sure going to be populated on real data... like Name? Then write a script that deletes from your table where name = ''. 
You can use the predicate "WHERE col1 IS NULL and col2 IS NULL..." But if the PK column is populated, just use that.
WHERE comes after the FROM statement, right before GROUP BY.
If you don't mind me asking, through what platform do you teach Informatica? I've sort of been tossed into ETL in my new job and Informatica is a big component for this work. Can only watch so many tutorials and read so many books but hands on learning works multitudes better for me.
Other folks have given you great information, but I see something else I wanted to touch on. WHERE c.name = (SELECT name FROM category WHERE name = ‘animation’) What happens if multiple results were returned by this subquery? I know what you're going to say, this will never happen and it can't happen. Well, it may happen someday, maybe not this table or query. So I recommend to write your SQL defensively, so it could be ported and maintain a standard that is consistent across your projects. When you are using a subquery in this instance or similar, you will want to make sure the query is limited to return only 1 result back. Otherwise your where clause should say IN or EXISTS instead of =. This prevents errors from bubbling up in this instance. Ideally you also have constraints on the table that help enforce this, but I recommend to code defensively too. Otherwise you can keep it the way it is, but add in error handling to your SQL.
Use Notepad++ to write your Inserts. It's a pretty easy/useful tool when you have to do things with code.
What would be the difference between writing the inserts there and writing them in MySQL?
Like, paper paper? Is it printed or handwritten? How much data are we talking about? If it's printed and more than a couple of pages, I suggest looking into scanning + OCR. Handwritten can get iffy. From a purely "My boss says get this done by lunch" perspective, manual entry for sure. If you're more flexible, I suggest something like Python + OCR, even for a small number of lines, because the learning is great, and once you have it in place, you *will* use it again.
It’s hand written and it’s a personal project so there is no time limit. 
Apparently it's either multiple self-joins or [correlated subqueries](https://stackoverflow.com/q/15529107) as DB2 lacks the very handy `PIVOT` operator. Or maybe [`DECODE`](https://www.ibm.com/developerworks/community/blogs/SQLTips4DB2LUW/entry/pivoting_tables56?lang=en) will work for you?
Scan, OCR, then read the resulting text to generate `INSERT` statements or format it so you can do a bulk copy/insert operation
[The tech for handwritten is still kind of a crapshoot](https://dsp.stackexchange.com/questions/1692/are-there-any-good-open-source-well-free-handwriting-ocr-programs). If it's super neat and consistent, you can maybe spend a fair chunk of time training something to recognize it with good accuracy, but you will be far exceeding the amount of time needed to just manually transcribe, with no guarantee of results.
My brother and I wrote this when we were young so the handwriting is poor. Thanks for your help, manual entry seems the way to go.
Thanks for the input. Question: is it thus protocol to use WHERE for 1 specific result, and IN/EXISTS for more than 1? Also, I'm having another issue, partly related to the above. I was able to clean up any errors by moving WHERE appropriately, but now I'm not getting any results whatsoever (that is, it runs, but I get a blank result) if I use a window function: ... WHERE name = (SELECT name FROM category WHERE name = 'animation') GROUP BY name, f.title ORDER BY genre; If I don't use a window function, but write: WHERE name = 'animation' I am told that the column "animation" does not exist, despite the fact that it does exist.
maybe? select AccountNumber, CASE WHEN RAmount = AAmount THEN AAmount ELSE 0 END as "R_Amount", CASE WHEN RAmount = PAmount THEN PAmount ELSE 0 END as "P_Amount" from ( select AccountNumber, CASE WHEN Attribute = 'R' THEN DollarAmount ELSE 0 END as "RAmount", CASE WHEN Attribute = 'A' THEN DollarAmount ELSE 0 END as "AAmount", CASE WHEN Attribute = 'P' THEN DollarAmount ELSE 0 END as "PAmount" from myTable ) tbl; 
Redundancy - having the same table 3 times Severe negative effect - Steps prior to this one finish in about 20 minutes but this step never finishes.
You can accomplish that by joining the Results table to itself. To view the fights where Mario is on Luigi's team, you can run this query: SELECT Mario.FightId, Mario.Decision AS MarioDecision, OtherFighter.FighterId AS OtherFighter, OtherFighter.Decision AS OtherFighterDecision FROM Results AS Mario LEFT JOIN Results AS OtherFighter ON Mario.FightId = OtherFighter.FightId AND Mario.Decision = OtherFighter.Decision WHERE Mario.FighterId = 1 AND OtherFighter.FighterId = 2 To view the fights where the "OtherFighter" was on the opposing team, change the second comparison in the join to a !=, like this SELECT Mario.FightId, Mario.Decision AS MarioDecision, OtherFighter.FighterId AS OtherFighter, OtherFighter.Decision AS OtherFighterDecision FROM Results AS Mario LEFT JOIN Results AS OtherFighter ON Mario.FightId = OtherFighter.FightId AND Mario.Decision != OtherFighter.Decision WHERE Mario.FighterId = 1 AND OtherFighter.FighterId = 3 If you want to do a basic "wins/losses" comparison with the above query, you can run this: SELECT SUM(IF(Mario.Decision = 'W', 1, 0)) AS Wins, SUM(IF(OtherFighter.Decision = 'W', 1, 0)) AS OtherFighterWins FROM Results AS Mario LEFT JOIN Results AS OtherFighter ON Mario.FightId = OtherFighter.FightId AND Mario.Decision != OtherFighter.Decision WHERE Mario.FighterId = 1 AND OtherFighter.FighterId = 3 I'm pretty sure all of this will work on MySQL, but can't be sure as I don't have it installed and SQLFiddle seems to be having issues
If you can use a case statement you can use group by with min/max functions... select acct# , min(case when att = "A" then $amt end) as Aamt , min(case when att = "R" then $amt end) as Ramt from table group by acct#
&gt; The attributes can be either R, A, or P. For any given account number, you can have either one record with R and one record with A, or one record with R and one record with P. &gt; You cannot have three records, one with R, one with A, and one with P, nor can you have just A and P. OK, so reading exactly you provided, the only legal combination of rows are "R","A" or "R","P" for a single account number. If that's the case then you can simplify the query quite a bit. Because if that's the case, why even filter for "R" if you can't have a row with just "A" and "P"? I'm thinking there may be some other cases that you omitted since you are joining by DollarAmount as well, but if AccountNumber and Attribute represent a primary key, then the below might work and require no joins. If will NOT work if those two fields aren't unique. SELECT AccountNumber, SUM(CASE WHEN Attribute = 'R' THEN DollarAmount ELSE NULL END) as RAmount, SUM(CASE WHEN Attribute = 'A' THEN DollarAmount ELSE NULL END) as AAmount, SUM(CASE WHEN Attribute = 'P' THEN DollarAmount ELSE NULL END) as PAmount FROM myTable GROUP BY AccountNumber HAVING RAmount &gt; 0 AND (RAmount = PAmount OR RAmount = AAmount)
&gt; but hands on learning works multitudes better for me. &gt; cannot agree more! you will only learn by doing!
Not sure your example fits here. &gt;You have redundant data that takes unnecessary space and are hard to maintain (e.g you have to update multiple records if you want to update one information in the header). There are no dates in his example. &gt;Querying the table is more complex, e.g. when you want to display a list of order headers, you would have to use DISTINCT. Normalization (feel free to correct me) is not relevant to complexity of queries, it is relevant to redundancy of data. I don't see how the data in the example given is redundant at all. &gt;The structure is not standard (not normalised) and difficult to read for other people who expect a standard approach to database design. This is where I'm not following you or the example given, and the example given doesn't seem to be similar to the one we're discussing here.
you actually have only one table ;o) however, i will concede your point about "never finishes" because i've had that happen to me the way i usually fix it is to ensure indexes exist to support my queries but in your case here's an alternative query... no idea if it will run any faster without indexes SELECT AccountNumber , MAX(a_amount) AS 'A Dollar Amount' , MAX(p_amount) AS 'P Dollar Amount' FROM ( SELECT r.AccountNumber , a.DollarAmount AS a_amount , NULL AS p_amount FROM myTable AS r INNER JOIN myTable AS a ON a.AccountNumber = r.AccountNumber AND a.DollarAmount = r.DollarAmount AND a.Attribute = 'A' WHERE r.Attribute = 'R' UNION ALL SELECT r.AccountNumber , NULL AS a_amount , p.DollarAmount AS p_amount FROM myTable AS r INNER JOIN myTable AS p ON p.AccountNumber = r.AccountNumber AND p.DollarAmount = r.DollarAmount AND p.Attribute = 'P' WHERE r.Attribute = 'R' ) AS u GROUP BY AccountNumber 
This sounds like something I encountered in MySQL. The subquery is stating to pull all rows from the category table where the column name has the value 'animation' in that row. Based on how you wrote the query, this would indicate you expect there to be only one row like this. When you receive that error, it's because the DBMS isn't sure who is what. (If it's MySQL, that's what's going on.) So in MySQL, I would do this: Where `name` = 'animation' The backticks indicates it's a system object and the single quotes indicates it's a value. If the error you are getting is actually just the query not returning any rows of data when you believe there should be, then there is a logical discrepancy between the query someone gave you and the query you are running. It may be easier to tell you what's going on if you can provide the database, version, and the exact error message. Otherwise I'm guessing based on context and clues. 
[Here's a link](https://drive.google.com/open?id=1eeaEPP1a5OljCWc4AbNjJh-TT5-biIp0) to the database, or at least what's in it. The film genres are actually under "category," in the "name" column, where action, classics, etc., are organized. I tried to do the backticks, but I'm getting a 'varying' error, where SQL can't read it, apparently. I'm guessing it's a logical discrepancy, but I can't figure out what. I tried to do this: SELECT name as genre, f.title, f.rental_duration FROM category c JOIN film_category fc ON c.category_id = fc.category_id JOIN film f ON f.film_id = fc.film_id WHERE name IN ('animation') ORDER BY genre; to eliminate some variables, but it's the same issue as before. Just blank, although it seems no different from any other similar SQL I've come across.
How much data are you storing? What's your access pattern? What type of data? How are you accessing it? Will you be indexing the data and tuning the queries? How is data loaded into the database when that does happen? How often/heavily will this be used? What else will be hosted on this instance? What kind of storage is this? Direct attached? SAN? Virtual server or physical? &gt;it will see more data read access than write access. Because of this I want to propose using SSD drives instead of 15k rpm drives Why? Especially if you're doing sequential reads, 15k drives should be fast enough for most cases here. SSDs pay off more on writes than reads. &gt;What would you think of using SSD in a RAID5 configuration for SQL application and SSD in a RAID1 configuration for OS and Program Files? I think you're probably over-specing the storage for files that aren't going to be read as often as you think. If your server takes 5 minutes to boot every 3 months instead of 3, is that really a big deal? &gt; I think if I could use SSD it would greatly exceed the write and read speeds of a 15k rpm drive. Have you tested any of this to prove your ideas, or is this a gut feel based on spec sheets? You may be better-served loading up on RAM; all data has to be read into RAM anyway so the more you can cache there, the less you'll have to go to disk in general.
I'm not using the dateadd() in the where, was just asking the question from memory. &gt;What's your reason for partitioning? To speed up loading the data into the final tables, or for query performance on the other end? This would be the goal. The staging tables are used for a series of loops where individual clients can be picking specific segments based on contractual language that is out of my control. I think the use case is a good example, but some of these staging tables are only ~500k records and might expand to ~1M, some are ~1M and might grow to be around ~2M. Not exactly large, and I'm thinking a partition might be overkill compared to just using an index. 
Performance enhancement. These staging tables are the source for a series of loops (several thousand of them) which all have something such as `WHERE SegmentID = n`.
Index first, if you start having issues explore the partitioning option.
Sadly, no one on my team has write access, so everything has to be done in common table expressions. Some of the initial steps make use of the existing indices, but the intermediate and latter steps are a free-for-all.
Man... as much as I appreciate my job I really wish I could have been brought on in this same fashion. I'm lucky in that my manager is giving all the time needed to learn everything, however, it is learning everything on my own. Sometimes it's nice to have direction and a little push in the form of guidance and lessons. I can't complain because I am learning... but I feel I could be much further advanced by now if these kind of initiatives were taken.
trust me, learning on your own is the best learning. it might take more time but this is for sure the way I prefer to learn.
Sublime text or atom are both on mac and are BRILLIANT. Sublime text is my preference but its nagware if you don't pay.
This is one of the things SSIS was designed for. If it's archival/historical data that isn't changing, do the big pull once, then copy only the new records in subsequent runs.
If you need to list all the columns you could query the table metadata to list out all the clauses.
Thank you for your help my friend!
Ahhh! W just covered R Trees in my databases class.
I usually just use left join out of habit, inner join would probably be the best join type to use. For being a beginner with SQL, you've done well, with your initial schema and picking out that I used the wrong join lol
Please come work for us. Our group throws a fit if we ask them to sync ~10M rows on a *weekly* basis, and acts like we're asking for something boutique. They insist we don't need to have that data and can work with only samples in our environment where we have the ability to create tables, procs, etc. Also, despite us having Microsoft licenses... they somehow have decided to all their ETL with other tools besides SSIS, mainly because none of them know how to use SSIS, but we have an offshore talent group who handles some intermediate steps... and then exclusively use SSIS... but our SQL versions aren't up to date and their version of SSIS is incompatible with using the scheduling agent on the server... so everything has to be run manually. Their solution is to find 3rd party software to fix this.
I'm learning SSIS, hire me! lol If all you're doing is getting the data, it'd really not that hard
I've been using SSIS since SQL 2000 when it was simply DTS. It's one of the best ETL tools on the market and comes free (I've tried Pentaho, CloverETL, Talend, etc.) with your license. TBH, if I worked for a company that said NO to SSIS and told me to go invest in 3rd party software, I'd assume they were too stupid to run a company and look for work elsewhere. And I have built ETL solutions for my current company, in SSIS, that push and pull hundreds of millions of rows a day. Sorry, just kills me when people who don't understand software make decisions for software to use. Carry on.
Good idea. I worked once for a company like that, and could only take it for 13 months. You shouldn't feel like you have to wait 3 years though, 2 years is a good stint on a resume. I've also been IT Director and hired folks like us - so 2 years is solid, IMO. Good luck to you in your future, kind person.
Let me rephrase my question: In terms of "normalization" each "SegmentID" would represent its own table. I am just putting them all in (1) table because I don't want to have 15 or 20 tables that all have identical column structures, but different data in them, and I find it easier to have (5) tables with about 20 SegmentID's. So in reality they are independent tables, and when I say: `WHERE SegmentID = 1` I am more trying to reference a unique table, than I am an "index" -- however, I don't know how much this might improve performance. These staging tables hold data in multiple segments and then a chain of several thousand loops runs queries against them based on specific client customizations (don't ask). I would assume that partitioning them would result in the highest degree of performance, but I'm curious what the difference would be between partitioning, using a clustered index, or just a regular index. As I said these tables are not that big.
I smell what you’re stepping in. My favorite was when a dev I was working with told me they had a million dollar system in the basement that the ceo bought and it was incompatible with their systems. Whenever I go to tech trade conferences with ppl that could potentially recommend things I would try to steer them away from the latest shiny stuff 
#1 - I was speaking more in generic terms about the normalization. In real life, OP has more columns. #2 - I agree that the point of normalization is minimizing redundancy. #3 - I originally didn’t read the question closely, and didn’t know exactly what OP was asking, but normalization does not really solve his problem. #4 - After reading the question more carefully, I agree that OP needs to pivot or concatenation using forXML or something. #5 - Carry on. I will read carefully next time.
Oh, we're on the same page... but I'm thinking about spending all of fiscal 2019 down in Mexico/CAmerica and working remotely. Get a title promotion and a nice raise by not having to pay income tax in the US. I've turned a corner and I'm Mr. Positive now.
If it's a linked server why does he even need SSIS? He should be able to query that server directly to pull it into his staging database.
Dood, sounds amazing! I'm on west coast and the thought of working in Mexico (Puerto Vallarta to be exact) sounds amaz-za-zing.
FWIW, if you’re an American citizen, you’re still liable for tax on income you earn outside of the US. There’s an exemption for the first hundred grand or so, but you still have to file. 
I use CREATE and DROP table often, especially while working out how to do something new. CREATE TABLE #tablename (id int identity(1,1), value varchar(10); --Fill table/use table; DROP TABLE #tablename; The biggest value you will get when starting out is by understanding and thinking of data in terms of sets instead of individual pieces. Once you understand the power, you'll view most tasks WAY differently. From one of the greats: Itzik Ben-Gan https://www.itprotoday.com/software-development/t-sql-foundations-thinking-sets
Can you elaborate on what you mean by the following? &gt;and never had the need to create my own table
What kind of analysis?
Interesting. I find it much easier to read SQL over python, even. Try Code Academy? Outside of that, all I can think is using the same directions for getting to Carnegie Hall.
I find SQL the easiest to read also. Are you writing SQL or reading it? Your comment about "reading long run on sentences" makes me think the latter so it makes me wonder what you are reading. Not sure if you can provide an example......?
Would echo alinroc's points. Realistically it's hard to spec disk without additional details, but would likely spend money on ram first.
Ok, it was hard for me to figure out what you got here. 1. Can you access the old database on the XP machine by using Lionwise?
Sorry for not replying. I am trying to figure out how to answer your questions. A lot of it I don’t know. I probably should have mentioned that the server is a physical server and will have 128 gb of memory. The server is going to be running a historian that will have maybe 16 clients connected to it reading data. It will be interfacing with 4 other devices to collect data for logging. How the SQL server is performing a lot of the questions you asked is black box to me and I can’t find anything in the manual to help answer your questions. I was hoping to get a better idea on how the SQL Server overhead function likes logs would perform on a setup that uses SSD. And if money wasn’t an object, would it make sense to go with the same size drives as SSD vs 15k rpm The spec for the software is not too helpful as it says 10gb of memory should be sufficient, but then says we should have 4 different raid5 arrays for OS, SQL logs, SQL Data, and Older Archives. This seems like overkill. However the way it scales up configuration based on the size of the system seems like the goal of the different raid arrays is for maximizing drive throughout, which I had always assumed would be better on a SSD. On our existing server, it does take a significant amount of time to start and load all of the various archives. So, I think SSD and memory will help with reducing the time it takes to start up. What do you all think? 
Yeah, I am clearly not familiar enough with how the software uses SQL to give good answers. We are loading up on memory 128gb. The old server we are placing had 24 or 32, so that by itself should help performance. 
You create a temporary table for your data. You run a command twice. If you don’t drop the table at the end of the first command you’ll get an error saying your table name already exists. There are a lot of reasons and whys behind this that I think go beyond ELI5.
Silly question, but have you called the company who acquired lionwise to see if they can help? I'd guess that you can export the data, but there might be limitations based on how it was built, they'd be qualified to give an answer without guessing. 
Google it you lazy boy
W3Schools SQL
[removed]
Write code. I can understand well written, well commented code fine, unfortunately it’s rare to find. So write your own code and remember SQL works best with sets. SQL is very different to any other programming language I’ve used, it took me six months to learn to think in the SQL manner. So stick in there it’ll come and if you can get someone who follows good practices to give you guidance.
The last sentence is key to understand SQL.
&gt;The thing you want to know is, it's executed in this order: &gt; &gt;FROM, WHERE, GROUP BY, HAVING, WINDOW, SELECT, ORDER BY Are you sure that is true for all RDBMSes? All optimizer modes? When partitioning is used etc etc.
This is the *logical* ordering of things as stipulated by the SQL standard. Any RDBMS (and in fact each single execution of any statement) is free to *physically* arrange for things happening in any ordering (or in parallel as the case may be) as long as that does not affect the expected results. A good / bad example would be `select * from t1, t2 where t1.c = t2.c`; according to the standard, this statement must act like first considering the cross-product `t1 x t2` (i.e. all tuples where each row in the left relation is combined with each row in the right), and then weed out all those tuples where `t1.c` doesn't equal `t2.c`. Actual RDBMSs are smart enough to forego the intermediate cross-product and still arrive at the right answer in a more efficient fashion. 
&gt;What is SQL? You want to learn something you have no idea what it is. This cannot be a serious question. 
If your values are ordered by `LocationID` in an ascending manner as in your example, a very fast way to compute the average is to use the [`LAG`](https://www.postgresql.org/docs/8.4/static/functions-window.html) function, assuming your database supports it. Here is the query developed for postgres: SELECT location, 'avg(' || location || ',' || (location + 1) || ')' AS name, value, COALESCE(LAG(value, 1) OVER (ORDER BY location ASC), 0) AS prev, 0.5 * (value + COALESCE(LAG(value, 1) OVER (ORDER BY location ASC), 0)) AS average FROM (VALUES (1, 0.15), (2, 0.14), (3, 0.16), (4, 0.123), (5, 0.134), (6, 0.312)) AS tmp (location, value) Replace the `VALUES` in the `FROM` section with your table and change the column names accordingly. Note that the first average is 0.15 + 0 because there is nothing which comes before `LocationID=1` (The \`COALESCE\` statement)
You've got your partition and order by backwards. NTILE(4) OVER (PARTITION BY genre ORDER BY f.rental_duration) as quartiles ... ORDER BY genre, f.rental_duration The query you wrote will provide a single row for each title. I suspect once you have this running, you'll want to take the resultset and do a GROUP BY genre, quartile and show the average rental duration instead of a row for each film? As a side note from a statistical perspective, when you do that, you would probably want to consider how many times each film is rented (a weighted average). The way this appears to be written a film rented once will have the same impact to that average rental duration a film title rented 100 times, which is probably not what you are looking for. 
i don't know about "proper" way to check first before adding a column i just go ahead and add it if it works, great, nothing else to do if it fails, the column already existed, great, nothing else to do
I recommend an app on your smart phone called 'SoloLearn'. That helped me learn the basics in a fun way. There are also other languages there for you to master. Why do you want to learn SQL? What's your interest? (just curious).
Thanks for the input, I'll try a few things out and see what happens. Not quite sure what I'll do with this data yet, but that's certainly an idea. By the way, I got an error that "the column 'genre' does not exist," which I assume is because PARTITION BY is scanned by SQL before the AS in the SELECT clause? I simply changed it to ORDER BY name, and that fixed it.
One might try looking for the column first... Do a quick select top 1000, copy the column names over to Excel, remove the commas and brackets using the replace function, then order alphabetically.
Can you make a copy of the .bak file? If yes, can you then take a copy of that .bak file and see if SQL Server Express can restore it? I couldn't find any documentation on Lionwise, but I have seen companies install and use SQL Express in the background to run the DB as long as it's a relatively small install. 
Oh I like your approach better. Thanks!
My version of tables if you were 5. &amp;#x200B; It's a new day at school, each semester you get a nice new three ring notebook. This notebook contains tabs to create sections for each class and in between those tabs you have pages. &amp;#x200B; If you are taking good notes, those pages should be structured and set up by topic, not just random writings. &amp;#x200B; The notebook is your database instance, it holds all of your databases. The tabs are your databases. The pages in the notebook are your tables in the databases. &amp;#x200B; Database instances are structured by topic or idea in a funnel shape, the more specific you get with data, the narrower the funnel gets. As an example, you would open your notebook (open the database instance), turn to Biology (open up the Biology Database) and turn to the pages about eating (the table regarding food and what the person ate). &amp;#x200B; Sometimes you decide you no longer need those notes or you made a mistake, sometimes you can erase a few lines and it's not a big deal. Sometimes you gotta rip the page out and start over again. That's sort of like creating and dropping tables. It's a structure that resides in files by your database system where it records and logs information. &amp;#x200B; &gt; I began using SQL a few months ago and have never had the need to create my own table or drop a table- possibly because I don’t fully understand the purpose and when it would be beneficial? &amp;#x200B; There is actually A LOT to this question. I recommend for now while you are still learning, try to understand the hierarchy of the instances / databases / tables, figure out how data becomes more specific as you drill down into tables. &amp;#x200B; If you want a project to practice, I'd recommend picking something very simple and create a database for it. Play a video game and record your win / loss streak in a database. Then create a report from that sample set. It sounds easy at first, but as you want more metrics or ask other questions, you'll realize you need to change the structure of the data, add more tables, add more information, etc. It would be a good learning opportunity in my opinion. &amp;#x200B; Keep it simple at the start, work on understanding the GUI and feeling comfortable moving around the database. Learn the syntax and as you encounter new things, try to learn as much as you can about them and see if you can teach someone what you learned. &amp;#x200B;
I did not perceive you as rude or anything, and merely wanted to point out the obvious which in this case is indeed something that may puzzle the n00b and trip up the expert. I guess it might be advisable to toy around with some tiny datasets and get used to the general ideas behind SQL; then at some point move on to medium-sized material (data volume and structure-wise) that is (ideally) somewhat close to your heart so you can spot unexpected outcomes and also get a feel how the thing runs and what the performance characteristics are. I always put `\echo ...` markers in my pSQL (Postgres command line tool) scripts and turn the timing on, so for each statement I get a feedback on where I am and how long that step took. I also occasionally use code like [this](https://github.com/loveencounterflow/intershop/blob/master/views/user-functions.sql) to get an overview over user-defined function performance. As a cabinet maker you'll have to handle chisels and saws, as a DB guy you need tools like these. Altho in retrospect I have to agree that learning a proper trade first is not a bad idea if it's relational databases that you want to do.
Well for starters, you need to learn to reddit. don't ask an innocent question, phrase an assertion that is categorically wrong, and the same moral poptarts that chide you for your audacity will instead write the great american codebook explaining it in exhaustive detail and nuance for you. Or you could, you know, put on the gi and start learning google fu, because those skills will save you.
I don't use the INFORMATION_SCHEMA columns.... and I'm assuming SQL Server here, by the use of the "GO"s :) I do it exactly as you do. Good on you to make sure your scripts are re-runnable in the event of an error.
That's great for one off work. But if you want to create a script containing lots of schema or data updates, then learning how to write one that is able to be run multiple times in the event of an error is needed.
Correct, it does not know your alias yet. 
So you have three options here. 1. Use (3) joins to get all the numbers, and then do a CASE WHEN NULL in your select to pick which three of the columns to take the number from. 2. Use an INNER JOIN and 2 UNIONs. 3. Use (1) join with 3 OR conditions... this is not ideal. Does that make sense?
Wat
I should have clarified that this is a pre-deployment script. I modified the database project with the NOT NULL constraint. Thanks for noticing! 
Can I ask what type of problem you are working on that you dont know what columns you have to work with. I ask because this has scalability issues and errors written all over it. I see a future where something breaks down the road and you have a hella hard time figuring out where the break is because you have queries that are conditionally adding columns. &amp;#x200B; Just my two cents but this seems like a very bad idea, definitely not a standard or best practice in anything I have done. As you get into building your environment structure is probably your most important control point. Adding conditional structuring is 100% going to mess something up 
Example 1: select case when b.accountnumber is not null then b.accountnumber when b.accountnumber is null and d.accountnumber is null then c.accountnumber when b.accountnumber is null and c.accountnumber is null then d.accountnumber when b.accountnumber is null and c.accountnumber is null and d.accountnumber is null then ?? else ?? end as accountnumber , * from table a left join accounttable1 b left join accounttable2 c left join accounttable3 d Example 2: select b.accountnumber, a.* from table a inner join accounttable1 b union all --do you need ALL here? select c.accountnumber, a.* from table a inner join accounttable2 c union all select d.accountnumber, a.* from table a inner join accounttable3 d Example 3: select b.accountnumber, a.* from table a inner join ( select joinkey, accountnumber from accounttable1 union all select joinkey, accountnumber from accounttable2 union all select joinkey, accountnumber from accounttable3 ) b 
OP what you did is the right way. If you want an example of the wrong way see the comment I replied to.
I see. Good to know, always good to learn something new. Thanks!
Ignoring the recycled joke, what the hell is this post supposed to be? 
Looks like an advertisement for a conference or something? With an old joke to get it under the radar?
Ohh I missed that line. My apologies! Thanks a bunch!
The Operations Manager made the current backup via the LionWise program, so it's the .bak file and of course the .xml. (I'd like to get into the machine to see if it can export to a different format, but I do not have an admin login since the OM is very threatened and hyper nervous of me making her look bad - she's already talking about torpedoing this project because 'of cost', even though she's not buying the equipment or involved in the setup of it in any way, shape or form - she wants to be in control of all facets of the project, even if she is in over her head - so it's nigh on impossible to sit after hours and work out the system.) I opened a copy of the .xml file last night in TextWrangler and it's got the data all in the file, bracketed with the &lt;&gt;list items&lt;/&gt; like so, so what I need is apparently in that file. 
&gt;You can use almost any text editor You often need to be able to replace on newlines and such. You need support for escape characters. Regular notepad does not support this. E.g. in notepad++ replacing \r\n with \r\nINSERT INTO....
Sure a coalesce would work. I rarely think of it before a CASE. &gt;You also commented on the other post that the best way to check a column exists in a table is to do a select top 1000, copy the column list to excel, replace the [], characters and order it. I didn't say "the best" -- although I'm not sure what the question was in the first place. &gt;I am not convinced you should be giving out t-SQL advice. Maybe you should learn the basics before trying to teach. I'm not teaching, I'm answering a question on the Internet and trying to improve my SQL by doing so and learning from people who comment/criticize my approach. If your only criticism here is that coalesce is superior to case, then point taken. If that even is a point... they will function identically and I prefer CASE to COALESCE for readability.
They want $$ for the 'support' and I do not have the authority to take the charge, so am trying to work my way through w/o. The thing I have concern about is the fact that the merchandise was entered into inventory with no rhyme or reason - I found the fancy soda we sell listed under both FOOD an IMPORTED GOODS categories in the .xml file last night. There is no flow. If it turns out that the inventory is too much a mess, will likely just rebuild from scratch over the new year - we have to do post-holiday inventory then anyhow. If it looks to be too much a mess, will hit that nuclear option and go directly to the owner and have her contact Tend (LionWise) directly. 
Yep. Found the data in the .xml file last night. Got to figure out how to extract it out of the file and into Excell.
See now I can wrap my head around the question. I never have that issue in my field or work. Ever. It's just not something I will ever touch or have any ambition to touch. I find it interesting to see the use case here and now I understand the desire to automate the process as opposed to physically looking at something. &gt;Now add multiple servers into the picture, you have many dev servers where the developers have been doing who knows what, multiple live servers and you need to just make that column be there everywhere. This is actually where I struggled with the question. We have multiple servers, and multiple devs, and people tend to add columns and name them slightly differently. Something like `CLOSE_DT` from an `ORDER` object might become `ORDER_CLOSE_DT` or something like `ORIG_ORD_CLOSE_DT` if there can be more than one order close date. Another dev might add something and name it `ORDCLOSEDT`, etc. Given the complexities of working with multiple servers and devs, I can't see how it would be practical to ever bulk add as you're describing. Even in past positions with larger transactional databases I can't see bulk adding columns like that to multiple tables without first knowing whether the data does or doesn't exist. It just seems sloppy and lazy to not know, or not look to see. Just my .02. &gt;Let me know next year when you are done. Probably I'll end up sending you an email and CCin'g your boss when I find examples of columns that already exist that failed your test, and then complain to my boss that I'm doing your job. That's probably how it's gonna go. &gt;Or you could just ensure you check for existence in the script and only create if its not there so your scripts can be run without error regardless of the column being or not being there. Again, with multiple devs, multiple servers, etc., I'm not sure how this is actually going to do anything other than determine if the "name" you've selected is on the table or not, not whether the data itself is there. If you're only working with ETL and raw extracts I could see some greater use here because you wouldn't have to worry about multiple servers, or multiple devs... but then I'd expect you already know whats in your data and what isn't. If you received a new file with new columns and you weren't sure which columns were already there versus which ones were... I suppose you could select the column names and do an `except`, or just paste the two lists into Excel and do a simple VLOOKUP. 
The only time I've personally used it in the past had to do with a very large table of raw transactional web data where we put old data into a partition so that we could more easily query the newer data. It improved performance a fair amount but the table was huge ~100M records across multiple years. Mentally I have come to thinking about partitioning as "splitting a table into multiple distinct tables." -- Is this a poor way of thinking about it?
&gt;Sure a coalesce would work. I rarely think of it before a CASE. Sure a boat would work. I rarely think of it before swimming across the entire ocean. But seriously, that entire case statement could be done like this: COALESCE( b.accountnumber, c.accountnumber, d.accountnumber ,??) as accountnumber But you go ahead and keep typing way more code than you need, I am sure its more 'readable' cause there is more to read. &gt;I'm answering a question on the Internet Badly. Let more experienced people answer and read those, don't offer bad advice as someone even less experienced may mistake it for good advice.
There are tons of use cases. RE-runnable scripting is one. If i have a long script that touches 5 or six objects, and it fails on the third one, I can now NOT run the same script again, because it will fail since the column already exists, or the object already exists, or it no longer exists, etc. This is especially important in properly managed environments where production deployments are done through automated build systems such as Jenkins.
&gt;We have multiple servers, and multiple devs, and people tend to add columns and name them slightly differently. Something like CLOSE_DT from an ORDER object might become ORDER_CLOSE_DT or something like ORIG_ORD_CLOSE_DT if there can be more than one order close date. Another dev might add something and name it ORDCLOSEDT, etc. For the same application/system? Y'all got some issues to get worked out here. &gt; Given the complexities of working with multiple servers and devs, I can't see how it would be practical to ever bulk add as you're describing. This is DBAs bang the drum of coding and naming conventions so hard. Not to mention change management and tooling to manage database objects in source control. You *can* do this if you have consistency.
That makes a heck of a lot more sense. Shouldn't break anything the way you have it
&gt;But seriously, that entire case statement could be done like this: I'm not disagreeing with you, however it is going to function and execute with the same performance as a case, and personally I prefer using cases and personally I think they are easier to understand. Also... say you only want to take C when B and D are null, or you only want to take C if B and D are null and some other condition is met. How will your COALESCE handle that? &gt;But you go ahead and keep typing way more code than you need, I am sure its more 'readable' cause there is more to read. I don't understand how this is a valid criticism and anything more than a snarky comment. &gt;Badly. Let more experienced people answer and read those, don't offer bad advice as someone even less experienced may mistake it for good advice. You're more experienced and you gave an example that will perform identically to the example I provided... and I still want to know how your COALESCE can handle multiple cases and conditions... which I don't think it can. I think someone should probably learn to use CASE first and then COALESCE, just my .02. 
&gt; In a lot of enterprise environments, scripts are created by developers and run by the DBA Team as part of Change Management. The DBAs aren't running those scripts blind though, they're being reviewed first.
&gt;Something like CLOSE_DT from an ORDER object might become ORDER_CLOSE_DT or something like ORIG_ORD_CLOSE_DT if there can be more than one order close date. Another dev might add something and name it ORDCLOSEDT, etc. Naming conventions. Communication. &gt;Given the complexities of working with multiple servers and devs, I can't see how it would be practical to ever bulk add as you're describing. Have you ever managed more than like 5 servers? &gt;I suppose you could select the column names and do an except, or just paste the two lists into Excel and do a simple VLOOKUP. Yeah why not add a pointless manual excel step into the middle of a database task. That's not a complete waste of time.
With your experience could you help me understand how to put this into a format where a COALESCE can replace the CASE? Thanks. case when b.id is null and d.id is null and a.city = 'Atlanta' then c.id when b.id is null and d.id is null and a.city = 'Detroit' then c.id + '01' when b.id is null and c.id null then d.id when a.id is not null and c.city = 'Philadelphia' then a.id when a.id is not null and c.city &lt;&gt; 'Philadelphia' then a.id + '02' when a.id is null and b.id is null and c.city = 'Tampa' then d.id + '04' else d.id + '06' end 
I would also check the PC that hosts Lionwise, you can probably see the list of services running. One of those services should be a database engine. There is the chance they made their own proprietary DB engine, in which case anything short of an export and import into a new DB system would probably be futile. 
&gt;Naming conventions. Communication. Yeah, that happens all the time in the real world. &gt;Have you ever managed more than like 5 servers? I have worked with people who poorly manage more than 5 servers, and am constantly shocked at how poorly they do their job. &gt;Yeah why not add a pointless manual excel step into the middle of a database task. That's not a complete waste of time. If something is quicker than doing it in SQL, it isn't a waste of time. Try calculating a median in SQL and tell me how it's a waste of time to pop into Excel. 
&gt; Also... say you only want to take C when B and D are null, or you only want to take C if B and D are null and some other condition is met. How will your COALESCE handle that? Have you ever googled coalesce?????? You do it like this COALESCE (B,D,C). If you need another condition, fine break it out into a case or even chuck an isnull check inside the coalesce. This is basic basic stuff. Get back to W3schools and stay there a few weeks before offering advice on SQL again.
I'm not trying to be difficult, but why would you want a rerunnable script to add columns. If you run it, and add columns... why would you want to run it again? I can wrap my head around the concept of using systems that 'automatically' bring new tables in, and then running a script to determine what fields it does or doesn't have, and then adding the ones it doesn't have... things like ModStamp, etc. but if it does exist, and there is data in the field then wouldn't you need to either rename the existent field to something new so you can add your field and keep things consistent?
I genuinely don't care enough to do so but it can be done. For getting the first null value coalesce is perfect and a case statement is just more characters for the same result.
Got you, so basically COALESE is not the same as CASE, but it will perform the same as CASE, and you're on me for trying to teach someone what a CASE is. Got it. Basically you will need to learn to master CASE, and COALESCE cannot replace it except for in some simple examples. 
&gt; am constantly shocked at how poorly they do their job. Yeah suggesting excel as part of checking whether a column exists in SQL comes under this. &gt;I have worked with people who poorly manage more than 5 servers You have worked 'with' people who manage servers? I have worked 'with' people who train government hackers, does that mean I know a thing about hacking at a state level? No. As I said, get back to W3schools and start on the COALESCE page. 
&gt;Thanks for schooling me. I didn't, you are too immune to new information. You learned nothing. I am sorry. Please leave me alone now.
I would hope so, but a DBA in that environment is likely working his or her ass off. Lol
Did you not read the part about the script failing midstream? I mean, at this point it's pretty clear you have never worked in an environment where change control and automated deployments are a thing, and thats fine, so let me try and explain simply as I can. At most places where data integrity and production uptime are of paramount import, database development work happens in multiple tiered environments. First there is usually a DEV environment. Then some kind of Test or UAT environment, and finally a production environment. When I am developing a script. I have to be sure the entire script will run without fail - this script will be ran by an automated system to make deployments to UAT and Production. So, while writing my script in DEV, if I mess something up and it has to be ran again, I need the entire script to be "re-runnable". In addition, if I am working on development and adding new things as I go, I need to be able to again, re-run the script. Again, the ENTIRE script needs to be able to be ran as one script, and in one go. There is no manual work in UAT and PROD. Now lets say, I get my script where I want it, everythings fine - I deploy to UAT. Now the testers come back and ask for a minor change to that deployment. OK, now I have to modify my script, and run it again in DEV, and then redeploy to PROD. Here is the catch. I have to run the same script in PROD that was ran in UAT. They cannot be different code. Therefore, the script has to be re-runnable because of that potential for re-deployment to UAT - even though the script will only run once in PROD. I understand you are missing the context of this type of working environment, but this is pretty much the standard when it comes to professional, high-availability, vital database systems (eg think enterprise level - the stuff that runs the worlds infrastructure). Most people don't do everything manually, and don't have only a single environment to modify.
Test it both ways. Linked servers tend to be a performance problem, and depending on your environment may also open up security concerns.
&gt; I would assume that partitioning them would result in the highest degree of performance, but I'm curious what the difference would be between partitioning, using a clustered index, or just a regular index. Assume nothing. Test each one and get real metrics on it.
&gt; Please come work for us. With all those issues you're describing? Only if I'd have the authority to fix it all. Just to be clear: I barely know the first thing about building things in SSIS. Mostly because I just haven't needed it. But I know what it's meant to be used for.
TIL that CASE and COALESCE execute the same way ,but that a COALESCE cannot do the same thing a CASE can and it's important to learn both. I also learned that you're an asshole.
&gt; I mean, at this point it's pretty clear you have never worked in an environment where change control and automated deployments are a thing, and thats fine, so let me try and explain simply as I can. I have never purported to. I have no ambition of becoming a DBA or working on that side of SQL. I am curious about it and want to learn, hence asking questions. Sorry if that offends you. &gt;So, while writing my script in DEV, if I mess something up and it has to be ran again, I need the entire script to be "re-runnable". In addition, if I am working on development and adding new things as I go, I need to be able to again, re-run the script. I do the same thing on my DEV server, and write code that is 're-reunnable' before deploying it to production. I don't deploy things to production until I know it will work, or until I know what's there. So here for this question we are talking exclusively about a DEV environment? &gt;Therefore, the script has to be re-runnable because of that potential for re-deployment to UAT - even though the script will only run once in PROD. You're saying you might add (5) columns to prod, then need to add (5) more, and you want to keep it in a single deployment script where if the first 5 exist you skip those and only add the next 5? I can wrap my head around that, but in our CR system that would need to be broken out into two distinct scripts, and packaged for a history. If you were planning to rerun that one day on another server, etc., I could see the value of keeping it all together as opposed to breaking it into two scripts (and then having to run both on a new server that has 0/10 of the columns you want to add.) &gt;I understand you are missing the context of this type of working environment, but this is pretty much the standard when it comes to professional, high-availability, vital database systems (eg think enterprise level - the stuff that runs the worlds infrastructure). Most people don't do everything manually, and don't have only a single environment to modify. I work in an enterprise environment like the one you are discussing, and we have 4 servers in sync that are maintained. I understand in the scheme of things this is a small fish in a big pond... but I am starting to see the value of doing this from a DBA's perspective. From an analytics perspective it still feels alien. It is very difficult for me to understand the perspective of someone who "administers" data but doesn't really have the foggiest clue what the data is, or how it functions, or what dimensions are present on the object. I'm not criticizing or commenting on that world, it is just alien to me. 
&gt; We are loading up on memory 128gb. The old server we are placing had 24 or 32, so that by itself should help performance. Make sure you're running the appropriate versions/editions of both SQL Server and Windows Server to make use of it. Prior to 2014, SQL Server Standard Edition was limited to 64GB (2014+ is limited to 128).
&gt; Only if I'd have the authority to fix it all. Hahahahahahahaha. You're a joker. &gt;But I know what it's meant to be used for. So our entire DBA group is Oracle, and we are the only SQL server in the company. Most of them don't know what SSIS is, let alone what it is for.
Spot on comment. I can guarantee you that person has never been a DBA. I can also guarantee that this person wastes a lot of time in excel that could be done in pure SQL much faster.
&gt; That's great for one off work. adding a column to a table is by definition "one off" -- it means you missed something in the early design stages pray that this isn't needed often enough to warrant a procedure
When to use will depend on how you want your data back... 
Parkinson's law If you have a week to learn sql and access and you are determine to learn sql and access in a week, you will learn sql and access in a week. At least enough to talk about it during the interview. 
&gt; if it fails, the column already existed, great, nothing else to do Are you suggesting this will be the only scenario in which a failure could occur?
i am not suggesting that, i merely stated what i do *of course* i double-check what error message i got
&gt; I tested this functionality on about a dozen systems prior to pushing it everywhere. With the checks, I didn't have to worry about excluding those test systems in the push. does that really compare with OP's situation -- "My SQL knowledge is rather limited" 
Since the OP's question was about "Best Practices"... yes. 
Try watching tutorials through [Lynda.com](https://Lynda.com). Your library could have free access through your library card, and you can watch the videos in double speed if you need to go faster. IMO, I don't the whole situation, but do you need to thoroughly learn those two programs in a week, or can you learn the minimal amount that is needed for the job and try to master those in a week?
I don't feel like you've really read and understood what I've said above - specifically as it relates to automated deployment systems.
Sorry, I'm just using SQL Server Management Studio.
where @yourdatefield &lt; dateadd(month,datediff(month,0,getdate()),0)
This should explain things. https://www.reddit.com/r/programming/comments/1xlqeu/sql_joins_explained_xpost_rsql/
[https://docs.microsoft.com/en-us/sql/integration-services/lesson-1-1-creating-a-new-integration-services-project?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/integration-services/lesson-1-1-creating-a-new-integration-services-project?view=sql-server-2017) &amp;#x200B; If you don't have Visual Studio and your company meets the requirements to use Visual Studio Community, then it's free. &amp;#x200B; [https://visualstudio.microsoft.com/downloads/](https://visualstudio.microsoft.com/downloads/)
A table with a row per item representing a one-to-many relation between users and items is the way to go.
One row per item per person is the right way to go about this. Properly keyed and indexed, multi-million + record tables are not likely to cause a performance problem in a relational database.
current_date () I don't use hive at all I just put your question into google.
just try the below and see if you get a date result. If you do, then it's some other syntax error in your code. select dateadd(month,datediff(month,0,getdate()),0)
It worked when I ran it by itself, but it won't work when I insert it into the where statement. I don't know why though. 
I mean create table is creating a new table and drop table is well dropping a table - the opposite of creating it. deleting it, make it go kapoot, the table will no more be.
the job description has "strong analytical and technical skills with experience with SQL and MS Access"
To me, it sounds like the candidate needs strong analytical and technical skills, but the experience is not related to those two things. If you have dabbled in both items for a day, technically you do have experience in it; however, I would learn what you can about both to give you a base needed for the interview, and if you get the job just learn to do something continually. I got hired by only dabbling a little in MS Access in school and no SQL experience despite my job making those skills highly preferred. I also had to learn a lot of Excel skills as well, but I've become proficient in Excel and am learning the other two on the job. Luckily, the department I work in hires sometimes based on drive and learning potential. You can do it! I'm sure you'll do great.
Sadly,I removed the spaces but still getting the same error. I also the removed the relation between the two tables and the code ran correctly , only after running the relation does the program bug out.
 Just reposting what you posted in original post, with some formatting for easier reading. No suggestions here. I am not familiar with MariaDB, so I'm not sure what advice to give you here. Sorry. I put extra spaces around the line the error is complaining about. ERROR: Error 1064: You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near ' INDEX fk_table1_has_user_table1_idx (table1_id ASC) VISIBLE, CONSTRAINT' at line 8 SQL Code: --------------------------------------------------------- Table mydb.table1_has_user ------------------------------------------------------- CREATE TABLE IF NOT EXISTS mydb.table1_has_user ( table1_id INT NOT NULL , user_user id INT NOT NULL , PRIMARY KEY (table1_id, user_user id) , INDEX fk_table1_has_user_user1_idx (user_user id ASC) VISIBLE , INDEX fk_table1_has_user_table1_idx (table1_id ASC) VISIBLE , CONSTRAINT fk_table1_has_user_table1 FOREIGN KEY (table1_id) REFERENCES mydb.table1 (id) ON DELETE NO ACTION ON UPDATE NO ACTION , CONSTRAINT fk_table1_has_user_user1 FOREIGN KEY (user_user id) REFERENCES mydb.user (user id) ON DELETE NO ACTION ON UPDATE NO ACTION ) ENGINE = InnoDB SQL script execution finished: statements: 7 succeeded, 1 failed Fetching back view definitions in final form. Nothing to fetch
I don't know why do you get syntax error, it's okay. Maybe you get error while converting because of diffrent locale used by DB by default? Instead of cast you can use function convert with proper style parameter: [https://docs.microsoft.com/en-us/sql/t-sql/functions/cast-and-convert-transact-sql?view=sql-server-2017#date-and-time-styles](https://docs.microsoft.com/en-us/sql/t-sql/functions/cast-and-convert-transact-sql?view=sql-server-2017#date-and-time-styles) If there is no right style in table, I think the only way will be extracting parts manually with help of substring and then put them into [https://docs.microsoft.com/en-us/sql/t-sql/functions/datetimefromparts-transact-sql?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/t-sql/functions/datetimefromparts-transact-sql?view=sql-server-2017)
I'm having a hard time understanding the question, or what the problem is? Its usually better to just post your exact query, then the results, and explain why the results are not what you want 
I'm a MSSQL guy, and not sure about MariaDB's quirks. I have two ideas. First, looking at the line with the error. Are you able to create constraints and indexes in the same statement? I'd think you're able to. But maybe create the table, and then alter the table to create the constraints on it. My second idea is looking at the larger statement. In MSSQL, the CREATE TABLE would have to be after the IF NOT EXISTS. Double check the syntax around that.
What problem are you having exactly? You can't just paste a huge query with no explanation, and expect people to spend time working on it for you
I want to take that query logic and create a function such as dbo.function(@param1, @param2) and have it spit out the results as the query would.
What version of MariaDB/MySQL are you using? Everything works fine for me if I remove the keyword `VISIBLE` from the indices. As far as I can tell, that option did not exist prior to version 8. Check the documentation, lines 13-15 on the syntax guide at the top. - [Version 8](https://dev.mysql.com/doc/refman/8.0/en/create-index.html) - [Version 5.7](https://dev.mysql.com/doc/refman/5.7/en/create-index.html) [Documentation for invisible indices](https://dev.mysql.com/doc/refman/8.0/en/invisible-indexes.html)
Does this work? select strftime('%Y', activity_date) as Year, strftime('%m', activity_date) as Month, i.facility_zip as Postcode, count(*) as Violations FROM violations v INNER JOIN inspections i ON v.serial_number = i.serial_number GROUP BY Year, Month, Postcode ORDER BY Year, Month, Postcode;
&gt; Unfortunately I still need to find the average for every month for every postcode. What does /u/wolf2600's query give you?
USER is a protected word. If you have a column using a protected word, you're going to have massive headaches getting things working. One of two options for you here. One, change the column name. Two, use back ticks in your script. https://dev.mysql.com/doc/refman/8.0/en/keywords.html#keywords-8-0-detailed-U
It gives me four columns - Year, Month, Postcode and Violations (a count of all of the violations per postcode per month). However, I think it needs to be the averaged across all of the days in the month. The question is very ambiguous.
I think that's what is causing the issue. It can be interpreted a bunch of different ways. I'll reach out to the course convenor and find out.
* You have the number of violations per month * You can [determine the number of days in each month](https://stackoverflow.com/a/29530094/1324345) * The average is simple division
Might be failing because of space in column names? Need to use backquote/backtick for those. Try this: `SELECT AVG(\`Total Price\`), MONTH(\`Order Date\`) FROM reddit_please_help_me GROUP BY MONTH(\`Order Date\`);`
I believe you need to take the semi-colon out of the query. If I'm understanding it, it's doing essentially: select * from (select * from ...;) 
Meh. Didn't notice that it was pulled AS... Anyways..
Github, and working as a database admin for a bit in a development shop.
If I understand correctly, you want query a table based on the tablename returned by your subquery. That means you are writing "dynamic SQL". In that case you need to make use of the EXECUTE command. Assign the result of your subquery to a variable, the use the the variable with EXECUTE to run the dynamically generated query.
Check if SSIS is installed and configured on one if your SQL servers. You can develop SSIS packages and deploy them for free using SQL Data Tools which comes with the latest version of visual studio community (also free).
 SELECT * FROM t1 JOIN zips ON (t1.zipcode = zips.zipcode); Rename tables and columns as appropriate.
If I understand correctly, when you insert into Table A, you want the rowcount of Table B as part of the record inserted into Table A. Is that right? If so, I would strongly recommend AGAINST triggers for this use case. You should be able to use a transaction to count the rows in table B and insert it into Table A. This will satisfy the ACID properties that you want. 
Interesting question. I have never considered that but I would assume your best bet for performance is to create a new column and index that. Did you check the execution plan when using datepart?
Lispql.
Isn't ansi standard with parentheses 
Select all from the table and add an hour column in a cte/temp table, then put an index on that
Functions are fairly platform dependent. Perhaps you would have better luck over on /r/SQLServer?
If the question is ambiguous, ask the professor to clarify. 
I would start again from scratch and do it in a view or procedure as that is a mess. The doubled up quotes and adding the @clauses as strings suggests it comes from dynamic SQL. Dynamic SQL is a sin. There are a few DBA type tasks where it is necessary but the general rule of the thumb I follow is that if I think it needs to be dynamic it probably does not need to be dynamic so I go spend a couple of hours looking at other ways to do it first, test the performance of those methods and then decide from there. If you think it must be dynamic and have not spent several hours trying other methods then it should not be dynamic. Select * is bad practice. If the code is not a one off quick query and needs to be re-used or deployed then you should fully define every column you need. The entire thing is about as sargable as a brick. If one our developers sent me that to deploy I would send it back and tell them to do it properly. If you want it in a function so you can join on it then that will be terrible performance. Ideally if you have to use lots of functions, do it in the select portion of the query so its only running the function against as few records as possible. Also you should really define what your inputs and expected outputs are when asking for help because we can only guess at what this is trying to do.
Don't use triggers if at all if you can help it. They just cause problems and 99% of the time are completely unnecessary.
Yes. There are a few caveats, the row count in table b would have several where clauses and the same with finding the correct row on table a. I would need it to be instantaneous, thats why the trigger has my attention. I'll be honest, I don't understand what you mean by ACID properties, but thank you for your help. 
Oh trust me, I'd love to avoid it, but I unfortunately don't have any other routes. I was told to make one, and here I am. 
Just create a view that gets the data you need or create a procedure that does the population for you and call that when you need the data refreshing. This is not an acceptable use case for a trigger.
This would work, and if you ever have the same question but it's spanning multiple years you can do: year(datefield) * 100 + month(datefield) as YearMonth
I'd use a view but unfortunately the table is read by a UI that only reads this table. So I need to update the table with this count. And I mean instantaneous as in asap. Otherwise I'd use a stored procedure or something. 
I have made views for this but there is a UI that only interacts with this table. And the count has to be available as soon as the row for the first table is created. Unfortunately I think that rules out the stored procedure. In fact if I could have a trigger that read data from a view, and inserted that into the first tables column, that be easier on me, but so far my advice has been to not do that. 
You have no idea how much failure I’ve encountered when trying to treat SQL like excel and innocently adding “helper columns”.
You can't have dynamic SQL in a function, but I think you can do it like this. CREATE FUNCTION ComputedMinutes( @Date1 datetime, @Date2 datetime ) AS BEGIN; DECLARE @NumberofDays int , @NumOfWeekDays int , @Ord1stDistDateisWeekend int , @OrdApprovedisWeekend int , @TotalMinutes int , @MinutesBefore9am int , @OrdDistAfter6pm int , @Minutes6pmTo9am int , @WeekendMinutes int; SET @NumberofDays = ...; SET @NumOfWeekDays = ...; SET @Ord1stDistDateisWeekend = ...; SET @OrdApprovedisWeekend = ...; SET @TotalMinutes = ...; SET @MinutesBefore9am = ...; SET @OrdDistAfter6pm = ...; SET @Minutes6pmTo9am = ...; SET @WeekendMinutes = ...; RETURN @TotalMinutes - @MinutesBefore9am - @OrdDistAfter6pm - @Minutes6pmTo9am - @WeekendMinutes; END; GO SELECT ComputedMinutes([Clause2], [Clause3]);
I only work with SQL Server but the idea is very similar: I advice you too look up this documentation on row number in orcale https://docs.oracle.com/cd/B19306_01/server.102/b14200/functions137.htm By partitioning the row_numbers by department and sorting by salary descending those employees with a row number of 1 will be those with the highest salary in their department.
Revo uninstaller worked for me [https://www.revouninstaller.com/revo\_uninstaller\_free\_download.html](https://www.revouninstaller.com/revo_uninstaller_free_download.html)
Why remove the parentheses? I don't see how they're unnecessary? (Novice dev here)
I would refute that to show trends over months with a dataset that only has about 64 days of data isn't very useful. Especially since May is your only complete month. It will obviously have a higher average since it has more days to build on (unless the ends and beginnings of April and June were especially strong, which may be what they are going for.) I would look for other trends that are a bit more meanigful with a larger sample size. * Is there a particular day of the week that generates more sales than others? * Are weekends better at sales than the week? * And since you have times, I would break it out into is there a particular time of day that generates more sales? Stuff like that. So hopefully you can bring up things like that in class to show that you are thinking about the bigger picture. Believe it or not, I work for a company that analyzed sales by the minute and we discovered that orders tend to spike between 1 PM and 1:15 PM during the week. Turns out people thinkg of things they need to order while at lunch talking with their co-workers. "Oh that reminds me, we need to order more legal pads." Interesting stuff. Who knew.
I will try that, thanks.
Thanks for the link, however, the example being used is only a single table. This question uses an employee and an department table that is linked via a department\_id primary/foreign key. Thanks for trying though! 
&gt; I don't see how they're unnecessary? removes dem, and you get 1. no error message 2. exact same results therefore, unnecessary parentheses are unnecessary
my approach would be to convert each datetime value to a unix epoch number DATEDIFF(second,'1970-01-01',datetime_column) if you do this to both start and end datetimes, and then subtract them, you get the number of seconds between them then you can see if that result is greater than 86400, which is the number of seconds in 24 hours
Depends how big the proc is. If it is short you could have a parameter @schema then in the proc an IF statement, with duplicates of the code in each branch, just one using one schema and the other using a different schema
As I understand it running the SP against different Databases is one of the main uses of dynamic sql. I hated troubleshooting dynamic sql when I was supporting apps so I don't really know much about it. there's some command that lets you execute a text value as a query so if that text field was 'select \* from names' it would run that query. combining this functionality with text manipulation functions you can write a stored procedure to assemble a query text and then execute it. included in that query text can be instructions as to what database to execute the query against. I think there is a system database that lists all of the other databases and thier tables so you can select the value of that database name from that table into the query text. 
take a look at sp_MSforeachtable... EXEC sp_helptext 'sp_MSforeachTable' to see the code etc. I haven't used this one, so I can't guarantee it's what you need, though I have used the related sp_MSforeachDB and found to be a handy shortcut. Undocumented, your mileage may vay, warning/danger etc...
ACID are properties of a relational database. Basically means data integrity. This is why I suggested to use a transaction. This makes it so multiple steps can trigger together as one thus making it "instantaneous". Either they all execute or none of them do. You can wrap your procedure in a transaction and run it.
You would just have the UI read from the view instead. I get that sounds easier than it sometimes is, but that would be much less impacting than having a trigger firing.
Assuming good indexes getting a count of records in a table on the fly should be very fast. 
Views generate their datasets on demand. If you create a view that does, say select count(*) from TableA then execute a block of code such as select count(*) from theView insert into TableA (yourcolumns) values (yourvalues) select count(*) from theView you will see the count is immediately different.
Thank you for the breakdown, I will have to review this and compare notes when I get a chance but I do appreciate you're help. 
Is your issue that you're averaging all values in one column? I'm also a little confused as to what you're asking. &amp;#x200B; I would do an explicit join so that the avg is filtered down to just that one character's record (If that's what you're trying to say) &amp;#x200B; ex: `SELECT IGN,` `AVG(attack1_stars + attack2_stars),` `AVG(attack1_newstars + attack2_newstars)` `FROM clan` `JOIN war ON war.*your character id* = clan.*your character id*`
Haha I think you nailed it. Easier said than done. 
Ah so views should be ruled out entirely at this point. I would need this to fire as soon as changes or inserts were made to the table, both tables really. 
If you think about it, querying results "generates" a table of information you queried. Your code right now is telling it to select everything from a subquery, not the table itself, which is why you're only getting subquery results. I'm not aware of any immediate solution but I will keep looking
Okay, so I went through it and added and formated it more to how I write things. I also realized that it didn't have the department name, so I added a join. `select e.FIRST_NAME || ' ' || e.last_name Name, d.DEPARTMENT_NAME, e.DEPARTMENT_ID, e.SALARY` `from EMPLOYEES e` `join (` `select DEPARTMENT_ID, max(SALARY) max_sal` `from EMPLOYEES` `group by DEPARTMENT_id` `) x` `on e.DEPARTMENT_ID=x.DEPARTMENT_ID and e.SALARY=x.max_sal` `join DEPARTMENTS d` `on e.DEPARTMENT_ID=d.DEPARTMENT_ID` `order by 3;` &amp;#x200B; &amp;#x200B; I'm going to have to go learn about using a subquery in a join. I feel like I learned about it, but obouis not enough. Thanks to those that help and good luck to those looking. &amp;#x200B; Joe
generate all numbers from start to end, exclude the ones you have in your records
OK i had a chance to edit my code into the post. I don't know if it will work and haven't had much time to look at it today but maybe you can look over it? No worries if not. I appreciate it regardless. 
Dynamice SQL would also handle this as ugly as it sounds.
Because the conditions I want to set are around the hours differences and minutes and seconds are not relevant. This is part if way bigger code.
Granted I don't know how/if this applies to Oracle, but personally I would use a row_number function to solve all but one of those problems.
Actually I found a way to make it work, but you have to do it dynamically.... by wrapping the query in the stored proc in an EXEC(''). Not a good idea. Your best bet is to create the SP needed in each schema.
The thought of my beautiful stored procedure, built to process elegantly across the worktable implementing my will upon recalcitrant tuples, caged in a text variable and stripped of formatting clad in red text coloring awaiting the harsh touch of the execute() statement... 
left join the table to itself on each row joined to its successor, and return when the column from the right table is null
What if The sequence is discreet. 
This might be workable. I could use dynamic SQL to select the worktable into a temp table and process over that. I don't remember how the temp table scope works when I do this. I might have to define the table in the procedure, then have the (select \* into from) live in the dynamic sql. That table has dozens of fields, and I'd have to do it for each of the procedures. I'll think about this one. 
This is what I wound up doing. I hate having two copies of the code floating around though. 
I already have a view setup to display what I want, but I don't know how to (if I even can) put that back into the UI that only reads from a specific table. If I could take that views information and update a specific column on the table referenced in UI, then I'd be all for it. The information needs to be returned for the end user to see. Also you're right, I don't have much sql experience but I was tasked with specifically making a trigger to do this, so I'm trying my best. 
so, the trigger you have written will only be fired when an UPDATE occurs but then you're checking if a row was inserted or not. Is there a reason you aren't using an INSERT trigger? Also, this looks like you want to use an INSTEAD OF trigger as you're just looking to modify the actual insert to append a count to the values being inserted. Here are a couple of scenarios and how your trigger will fire: insert into table1 -- trigger won't fire insert into table2 -- trigger won't fire update table1 set somefield = someval -- trigger will fire update table2 set somefield = someval -- trigger won't fire Additionally, for this kind of operation, you really want to utilize the inserted and deleted tables (these are temporary tables associated with the data being handled by the trigger) to limit your update statements unless you don't care which rows are being updated. Without some additional context to the actual problem you're trying to solve, it kind of just seems like you're hoping something sticks.
private, confidential, one-on-one, and discreet https://www.youtube.com/watch?v=oqhc101LxNs
Initially I had insert and update included but someone told me to remove insert, so I did. I have thought about using the inserted and deleted tables, but honestly I'm just getting caught up in the specifics. For example, I absolutely have to target the correct row and column, Im just not sure how to do that. And throwing in 2 more tables that exist but dont exist has discouraged me from thinking about them. I am not opposed to utilizing them, I'm just not sure how to make these connections. I'd be happy to answer questions if you think that will help. Surely there is a way to simply count the rows where this condition applies and put that total over here in this specific row in this column.
Do you not have access to the insert statement for Table A?
I should. Why? I don't think I follow. 
Thanks for the gold!
Post some of your schema... and example output. if I can see what you are working with I can better help.
 &gt;1. no error message 2. exact same results 3. **No reduction in readability** The last one is important too.
The "official" way to do it is to set the default schema on the USER's default mapping so that when you run a procedure, it uses the my schema, and when other people run it, it uses the otherpeople schema. When the system searches for and object, it will search the default schema for that user. I know this works for VIEWs because I have an application that does it. I'm not entirely sure that it works with stored procedures. You'll note that this feels really permanent and not something you want to switch back and forth. That's intentional. You can't use this method if your application uses a single account to access the database, and your procedure must *not* include the qualified schema name in any object that you want to call using the user's default schema. 
Just out of my experience, I you are practicing lookups on a production HR database... Stop. HR databases are on an extreme need to know basis. Make sure your boss is aware you are doing. Just because you have access to it, doesn't mean you have the right to read it. Specifically a query such as the one you proposed... 
Try creating a unioned view with a constant in each select of the Union to identify the table, and then use a parameter on the proc to filter to the correct base table in the view for each operation. I've used this technique a bunch, and it works just fine.
In your other comment you gave someone gold for posting the syntax for creating a function with some variables declared, a return and an example select using it. A new support guy at my work whos experience with SQL is just a few weeks could do that. So I do not think you are in any position to comment on someone elses knowledge. Especially considering your case statement you posted. You have since editted your comment to remove the offending code ( https://www.reddit.com/r/SQL/comments/9l209r/need_help_with_joins_im_lost/e73eyke/ ) But unfortunately for you I had that comment cached on a browser tab of my phone from before when I was showing a co-worker. Lets get that code that you have since conveniently removed out. The code that *you* wrote and have since removed. select case when b.accountnumber is not null then b.accountnumber when b.accountnumber is null and d.accountnumber is null then c.accountnumber when b.accountnumber is null and c.accountnumber is null then d.accountnumber when b.accountnumber is null and c.accountnumber is null and d.accountnumber is null then ?? else ?? end as accountnumber ,* from table a left join accounttable1 b left join accounttable2 c left join accounttable3 d Now considering: &gt;The CASE expression evaluates its conditions sequentially and stops with the first condition whose condition is satisfied Source: https://docs.microsoft.com/en-us/sql/t-sql/language-elements/case-transact-sql?view=sql-server-2017 Now lets step through your code. I will give your aliases human names just to make it simpler. a= andy,b = bob, c= charlie, d=david. If Bob is there we get bob. If Bob is not there and David is not there we get Charlie (even if he is not there) If Bob is not there and Charlie is not there we get David (even if he is not there). (Il write that off as a sloppy mistake). Now thats the first three when clauses. Could you explain to me in real simple terms and dumbed down because I as you noticed to not understand case statements how exactly you could ever reach the remaining when statement? &gt;when b.accountnumber is null and c.accountnumber is null and d.accountnumber is null then ?? Because call me dumb and I probably am but my limited understanding of a case statement suggests that these lines of code could never and would never get run and therefore similar in practice to supergluing a brick to a ferari - it adds *something* but not *value*. I feel at this stage this may apply to a lot of your 'advice' you post here. Feel free to deny you posted that SQL but I took a screenshot :) 
I don't believe I have deleted anything from my post history. This is not the same thing, or even relevant to that. Are you referring to [this](https://www.reddit.com/r/SQL/comments/9l209r/need_help_with_joins_im_lost/e73fibg/) comment? Please note it isn't edited. Here I was asking for a framework for building a function. Nothing else. You did not contribute to it, so you aren't going to get gold. Thank you for your input on dynamic SQL. I suggest you go back to learning how COALESCE is different than CASE, but how CASE will execute the same as a COALESCE. Thank you.
I'm familiar with the use of this for security purposes. The problem here is the stored procedure lives in my schema and when the procedure runs it looks for the table in my schema, even though it was called from the other schema.
This is an interesting approach to try. Thanks.
Hey, therealcreamCHEESUS, just a quick heads-up: **definately** is actually spelled **definitely**. You can remember it by **-ite- not –ate-**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Hey, notasqlstar, just a quick heads-up: **definately** is actually spelled **definitely**. You can remember it by **-ite- not –ate-**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
sys.databases is a table in master that will give you details about each database on the instance.
Well that is good to hear. Was not trying to be a dick - but I have seen many-a-terminations come from things like this. If you end up in a production setting, avoid HR databases like the plague unless instructed to do so!
Uh, I don't like this approach. What happens to your SP if a user drops their table, or changes their table definition. Tightly coupling a view to a bunch of distinct schema objects sounds like a nightmare.
If your environment is that unstable, no approach to the OPs requirement will work. This is a fairly standard approach to horizontal partitioning. But, you can schemabind the view so the table has to be removed from the definition before altering/dropping it. Basically, you need proper change control/testing. 
Well I updated my post with what I thought would work, so beware, it's probably hideous. I don't have any output results because I don't want to put something into affect that will have negative effects. 
That would work for the first term but then we would be rostering the other terms early too maybe tho yeah
No need for temp table. Pass the desired schema name to the proc as an optional argument, defaulting to your schema. Use dynamic sql to drop and recreate a table synonym called "yourProcName_stuff" that maps to @schema.stuff. Use the synonym for the rest of the procedure.
Selecting way too many rows and then just filtering it in middleware Also using SELECT * by default (or out of laziness), instead of selecting just the fields you actually need. For a website running this repeatedly, performance all mounts up, and transferring way more data than you need will impact you sooner or later 
Another option would be to rename the base table, and replace it with a VIEW. SQL Server allows inserts/updates/deletions into views, with certain caveats (so assuming these were met, the client software would be none the wiser). 
From what I understand using a view will just complicate things. But I appreciate your help. 
Competent programmers in other languages who believe they're experts at SQL because its 'not a real language'. And then rage quit when folks like me come in and make them look like idiots in a couple of hours because we actually took the time to understand the engine and how it works. Oh, and relational algebra. 
Well, whether or not you should use a trigger can depend on the insert conditions. If you are the one who is inserting data or have access code that's writing into Table A, then you should update it be using a transaction. If you're updating the table because the insert into Table A can come from anywhere, then a trigger is actually appropriate. &amp;#x200B; &amp;#x200B; &amp;#x200B;
The insert will be coming from several end users via the UI. I'd like to catch the insert, count all the inserts from another table they've inserted data into and populate a column on that original insert. As fast as they input it, I want that to be available, so they can see the total via the UI. I just don't know where to layout my syntax. My best attempt is in the post, I edited it in earlier today. 
Something doesn't seem right to me, apologies if I'm off the mark here: you have a table of classrooms [c] and a table of equipment [e]. The joint table [ce] defines the relationship between a classroom and one or more pieces of equipment. If there are multiple rows with the same [ce].classroomID, are you going to concatenate a single [c].equipment from the mulitple [ce].equipmentid values?
I'm assuming you're on MSSQL. I don't see you referencing "INSERTED" at any point. You can think of it as a built-in invisible "Table" that exists within the scope of your trigger. It contains all the records that were captured during your insert. This is how you know which records to count from table B and which records to update in table A.
No indexes, or too many. Assuming that because one database behaves in a particular way that so does another (People who use AUTOINCREMENT in sqlite, I'm looking at you). Treating tables like arrays in a regular programming language and wanting to use dozens or more of identical tables instead of a single one because that's what you would do in $otherlanguage. Failure to do some basic normalization of tables, or understand relationships between data in general. 
This is all on sql server, the records from table b are more than likely already on the table b so when at the end of the day they insert into table a, so I'm not sure they would be there. Can I have the insert table store the records throughout the day, then be initialized when the table a record is created? If so, What would that look like? I appreciate the help, I have to go to bed but I'll reply in the morning if you choose to followup. 
Imma take a much smaller bent.. I see people making a lot smaller mistakes like using &gt; instead of &gt;=, or using like ('buttsex') instead of in ('%buttsex%')
WITH NOLOCK on every query because "it makes it go faster". And then you scroll up and they have SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED at the top. 
one word: null
&gt; Selecting way too many rows and then just limiting or filtering it in middleware (PHP, node etc), simply because it seemed easier at the time or they didn't know how else to do it Oh, my. This. I inherited a steaming pile of code that, among other things, selected the entire table and programmatically looped through each row to decide if they wanted to use it. I put that burden back on the database engine with a proper "where" clause and processing time for that job dropped to well below half of its former run time.
Can you explain what an index column is. Noob hea
There are these hidden objects called indexes that can exist against a table. You can create an index against a set of columns. That the SQL engine can jump to the specific record based on the index. Imagine studying for a test and you stumbled upon a topic you don't understand or forgot. You might consider looking at the INDEX at the back of the textbook for the topic. The index will tell you which page to go to to find the information. You can then jump straight to the correct page. Now imagine not having an index at the back of the textbook. You'd have to skim through the ENTIRE book until you found what you were looking for. One way of optimizing queries is to create an index on a column or columns you might commonly filter/join against. &amp;#x200B; &amp;#x200B;
I would like to share another informative blog post, Visit here r/https://bit.ly/2DWGsd6 
Hi I am using Mssql 
Yes. Business Intelligence is your best bet. Why don't you try taking a sql course on edx.org to see how you'd do? "Querying with Transact-SQL" is open for enrollment right now and you van audit the course for free and it's made by Microsoft. Feel free to reach out if you have questions or need help.
Forgetting commas
What's wrong with that? I understand that it can often lead to not understanding how the DB works, but if you are good with SQL and your use case justifies (or makes things simpler/better by) using an ORM, then why not?
https://www.brentozar.com/training/think-like-sql-server-engine/
Do you mean the 1, 2, 3, ... On the left? That's just part of the grid to number each row. Those numbers don't come from the database.
I'm only a beginner but these seem like truly rookie mistakes. Surely things like the WHERE and SELECT commands are elementary. Are they really as prominent in industry as you make them out to be?
Mistakes like this happen more often when using ORM frameworks such as Hibernate, because they abstract away the SQL.
That's an extremely good analogy. 
&gt;So I looked into it and the problem I will have is that some of the term section numbers will overlap and cause future students to be put into the existing courses so you could end up with like 60 kids in a class and actively getting assigned work the last two weeks of each term. &gt; &gt;Maybe if I coupled it with a CASE statement but that would put a lot of weight on the Query that I would like to avoid.
Thanks, that's what I was starting to work out. I guess my idea of how subqueries worked was flawed.
Had to do a little tinkering to get it to run but it cleaned up nicely. Thanks again. ALTER FUNCTION [dbo].[NetworkMinutes]( @Date1 datetime, @Date2 datetime ) RETURNS INT AS BEGIN DECLARE @NumberofDays int , @NumOfWeekDays int , @Date1isWeekend int , @Date2isWeekend int , @TotalMinutes int , @MinutesBefore9am int , @Date1isAfter6pm int , @Minutes6pmTo9am int , @WeekendMinutes int , @NumberOfWeekendDays int; SET @NumberofDays = DATEDIFF(dd, @Date2, @Date1) + 1; SET @NumOfWeekDays = (DATEDIFF(dd, @Date2, @Date1) + 1) - (2 * DATEDIFF(wk, @Date2, @Date1) + CASE WHEN DATEPART(dw, @Date2) = 1 THEN -1 ELSE 0 END + CASE WHEN DATEPART(dw, @Date1) = 7 THEN -1 ELSE 0 END ); SET @Date1isWeekend = CASE WHEN DATEPART(dw, @Date1) = 7 THEN (10 * 60) - DATEDIFF(mi, DATEADD(dd, 0, DATEDIFF(day, 0, @Date1)) + DATEADD(day, 0 - DATEDIFF(dd, 0, '08:00:00'), '08:00:00'), @Date1) ELSE 0 END; SET @Date2isWeekend = CASE WHEN DATEPART(dw, @Date2) = 7 THEN (10 * 60) - DATEDIFF(mi, @Date2, DATEADD(dd, 0, DATEDIFF(dd, 0, @Date2)) + DATEADD(dd, 0 - DATEDIFF(dd, 0, '18:00:00'), '18:00:00')) ELSE 0 END; SET @TotalMinutes = DATEDIFF(mi, @Date1, @Date2); SET @MinutesBefore9am = CASE WHEN DATEPART(hh, @Date2) &lt; 9 THEN DATEDIFF(mi, @Date2, DATEADD(dd, 0, DATEDIFF(dd, 0, @Date2)) + DATEADD(dd, 0 - DATEDIFF(dd, 0, '09:00:00'), '09:00:00')) ELSE 0 END; SET @Date1isAfter6pm = CASE WHEN DATEPART(hh, @Date1) &gt; 18 THEN DATEDIFF(mi, DATEADD(dd, 0, DATEDIFF(dd, 0, @Date1)) + DATEADD(dd, 0 - DATEDIFF(dd, 0, '18:00:00'), '18:00:00'), @Date1) ELSE 0 END; SET @Minutes6pmTo9am = (@NumberofDays - 1) * 14 * 60; SET @WeekendMinutes = ((@NumberofDays - @NumOfWeekDays) * 10 * 60) - @Date1isWeekend - @Date2isWeekend; SET @NumberOfWeekendDays = @NumberofDays - @NumOfWeekDays RETURN @TotalMinutes - @MinutesBefore9am - @Date1isAfter6pm - @Minutes6pmTo9am - @WeekendMinutes; END;
&gt; Being stingy with column lengths. Last names can have more than 30 characters. On the flip side, making every column ridiculously long "just in case". I'm sorry, no, you do **not** need `nvarchar(2000)` to store a mailing address. I don't even need that much to write out detailed directions from my local post office. Right-sizing your columns makes for better performance and better data quality.
ORMs have a tendency to produce boneheaded SQL unless you carefully manage them.
How about wrong assumptions about how their data is structured? You have to know the rules to play ball. For example maybe a "sales order" has a table that stores the common facts about each sales order, and then another table has the individual lines for each sales order. You'd think you can just use inner join all the time, but some salespeople create sales orders with no lines that represent sales in progress or prospective sales. So everyone reporting on the data doesn't think twice and does the default inner join all the time through whatever front-end application they are using (Access, Crystal Reports, BI, etc) and critical data is getting dropped without anyone realizing it. Second honorable mention for 3-valued logic failures --&gt; True --&gt; False --&gt; NULL
When dealing with large data sets, i've noticed (an often done myself): * Not querying on indexes, and wondering why the query is taking so long * Querying on a partial part of a composite index, and wondering why the query is taking so long * Using an inner join instead of a Left join, and not realizing that their results might be missing data &amp;#x200B;
Agreed. Or use the good ones, like SQLAlchemy. OTOH, even the best ORM gives shit results when misused.
So, I've been trying to get the average rental duration per category of films, and I'm having a hard time with the logic. I suspect I need to use a subquery in order to tap COUNT(f.rental_duration) across c.name, but I can't find any examples of subquery usage like that, or how to even search for this. What's the best way to combine, say, a COUNT for the contents of one table column, then put that into an output which is itself divided by a column from another table? For example: GENRE : TITLE : RENTAL DURATION : RENTAL AVG FOR GENRE ACTION : LAWRENCE OF ARABIA : 3 : 3.57
But then why do my ID number (CompanySID) start from where the grid number end and not from "1"? Is that how it's suppose to be?
The common error is: SELECT * FROM TableA a LEFT JOIN TableB b ON a.id = b.id WHERE b.Column1 = 'Value' That's an implicit INNER JOIN. The best method to fix it is: SELECT * FROM TableA a LEFT JOIN TableB b ON a.id = b.id AND b.Column1 = 'Value' Or sometimes like this, which I think is identical to what your logic is trying to achieve: SELECT * FROM TableA a LEFT JOIN TableB b ON a.id = b.id WHERE b.Column1 = 'Value' OR b.Column1 IS NULL *However, this is not identical to the best method above.* If TableB.Column1 is a nullable field then the second query is *not* identical. If you don't want any records from TableB when TableB.Column1 is null, you'd have to specify something like: SELECT * FROM TableA a LEFT JOIN TableB b ON a.id = b.id WHERE (b.id IS NOT NULL AND b.Column1 = 'Value') OR b.id IS NULL And that logic quickly gets gross once you start adding more conditions to the WHERE clause. 
I remember this comment! I'd also like to get an explanation on why that constitutes a milestone...
You can't reduce something by more than 100%. Unless your San is now generating negative iops. 
First I think I'd do some profiling to figure out what the differences are between the 793K and 733K datasets. If you can figure out something that sets the 60,000 rows apart, then maybe that'll tell you what's going on. You mentioned uncommitted transactions... surely your process SQL doesn't use WITH NOLOCK or READ UNCOMMITTED, right? If so, then immediately get rid of that. The default settings will ensure a consistent table state if an update query was still running in the background. It's possible that the SSIS package has multiple task branches running, and that some piece of the package is still running when your task is started. If that is the problem, that's an easy fix for the offshore team by adding an additional constraint to the start of your task. But if it's a linear process, then all previous data flows and SQL queries should be fully completed by SSIS before your task is allowed to start. 
You can tell him how you want the records ordered. Add "order by companySID" at the end of your query (without the quotes). I suggest you check out https://www.w3schools.com/sql/ first. Good luck!
We have determined that whatever the issue is, it is 100% coming from SSIS. We have (1) large SSIS solution that has (1) DTSX in it. This solution has multiple tasks/modules, and some of those execute other DTSX packages. After the final DTSX package executes there is a standard "green line" going to my task, which is a simple `EXEC Stored.Procedure`. Now the weird thing is that this "glitch" only happens for my process, which is the first one to trigger after the other processes are finished. After my process, there are other simliar task that rigger other stored procedures, and those stored procedures point to some of the same tables that my process is pointing to... but the rows come back normally. We have determined that if we simply remove my process from the SSIS package and trigger it manually, then it will always come back @ 733k. So the 793 has to be a cache, or something with uncommitted transactions, etc. &gt;surely your process SQL doesn't use WITH NOLOCK or READ UNCOMMITTED, right? Absolutely not. &gt;It's possible that the SSIS package has multiple task branches running, and that some piece of the package is still running when your task is started. If that is the problem, that's an easy fix for the offshore team by adding an additional constraint to the start of your task. I have a colleague I used to work with who is an SSIS wizard, and this was her recommendation, although she could not think of how it might be happening beyond saying it casually. &gt;But if it's a linear process, then all previous data flows and SQL queries should be fully completed by SSIS before your task is allowed to start. Agreed. Like I said the strange thing is that only my process (the first) will give a bad row count. Now that it is removed I'm going to check the next task (now the first) and see if it does the same thing or not. 
I just wanted the SID's to start with "1". There are a total of 418 records in this whole table and the CompanySID is starting the count after the left grid numbering. so the CompanySID column goes from 419-836. &amp;#x200B; While creating the table, I used: "CompanySID int NOT NULL IDENTITY PRIMARY KEY"
Hmmm... nested packages, that may be a clue. If instead of using the "Execute Package" task in SSIS, they are executing a process and running DTEXEC.exe manually to fire off the child packages, they may be using DTEXEC in asynchronous mode, and it is returning success immediately and starting your task, when in fact the child package has only started. [here](https://www.mattmasson.com/2012/02/exit-codes-dtexec-and-ssis-catalog/) gives some details, but essentially you need to ensure that they set the $ServerOption::SYNCHRONIZED property to True to be it doesn't return until after the package is finished, if that's the technique they are using to start packages. And if a child package executes it's own packages, then I'd check those as well. 
Yup. I wish I had the time to rewrite the code I wrote as a junior. I could gain performance all over the place.
It was going to be one unit and then another, so in-between editing I forgot to change one of them. Eh. It happens. 
You probably tried to insert the records once before or you deleted them. The id's keep incrementing even for failed inserts so there can never be double values by accident. If you really want them to start at 1 I suggest you drop the table and create it again. There are other ways but that's the most simple one. Personally I wouldn't bother though.
Easy one SELECT s.store_id, r.rental_id, FORMAT(r.rental_date, 'MMMM', 'en-US' ) AS month 
Yeah, I don't know. Our DBA / offshore team have basically given up and decided that the best solution would be to just execute my process manually after their SSIS package finishes.
Well at least for basics you can get this free certificate, in about 2 hours. [https://www.sololearn.com/](https://www.sololearn.com/) It improved my standing among my colleages so they know I'm realiable.
I've started doing the WHERE 1 = 1 for all query development now. It really comes in handy with adhoc stuff.
Sadly this does not work, as I get multiple roleids per user
Ok I will try. Thank you
Have you tried this? SELECT UserName, max(RoleName) FROM #TempUser u JOIN #TempRole r ON r.ObjName = u.ObjName GROUP BY UserName
Cartesian joins Not adequately filtering rows and columns, especially before joins Not using explain plan &amp; tuning before any running code Lack of partitions, indexes, general dB tuning e.g. refreshing stats Not structuring &amp; commenting code properly Lack understanding of their data which makes doing the above more difficult
I am stuck on how to join the business table with the category table. I know I am missing something easy. I would think the business table should have a "business_id" attribute, but it only has "id". SELECT business.name, category.categy FROM business INNER JOIN category ON category.business_id = business.id --That isn't working, what do I do? Thanks- 
I would also assume that category.business\_id = business.id. Being that the datatype of those fields is Varchar you might have to cast those fields as integers to get them to join. Please look at the contents of both of those fields. What do the IDs look like in each table?
are you able to upload the SQLite db for us to check?
select business.name, category.category, hours.hours from business join review on review.id = business.id join category on category.business_id = review.business_id join hours on hours.business_id = category.business_id 
I think this is the right track! ** Business.id** --6MefnULPED_I942VcFNA --7zmmkVg-IMGaXbuVd0SQ --8LPVSo5i0Oo61X01sV9A --9e1ONYQuAa-CB_Rrw7Tw **category.business_id | category.category YDf95gJZaq05wvo7hTQbbQ | Shopping | YDf95gJZaq05wvo7hTQbbQ | Shopping Centers | mLwM-h2YhXl2NCgdS84_Bw | Food | mLwM-h2YhXl2NCgdS84_Bw | Soul Food | mLwM-h2YhXl2NCgdS84_Bw | Convenience Stores | mLwM-h2YhXl2NCgdS84_Bw | Restaurants | So, one business can have multiple category which I get. So looks like I need to remove the "--" from the business.id column and CAST both as integers and then join? Going to have to figure out how to CAST. Thanks for helping me get on track!
load it to a temporary table, and then copy into the permanent table. 
I think this goes both way. I am a competent programmer in multiple languages, with a history going back decades. However, I proudly believe SQL is a Turing complete language. Love talking to DBA's and others who yell at me for "bastardizing" or "misusing" SQL to solve legitimate problems and doing so in an efficient manner. Dynamic SQL and loops come to mind here. You can't throw a stone and not hear someone giving you an opinion about how wrong they are. But they aren't. They have very specific and awesome uses. These uses are often missed by "SQL guys" because they don't have exposure to other languages.
&gt;People who just use SELECT * by default, instead of selecting just the fields they actually need I frequently don't know where the data I need is, and run SELECT * and then cancel my queries to get an idea of the data present in the table, and then to figure out how to select only what is needed for my actual script. What's the correct way about doing this? Just selecting the top 100?
No, full disclosure this is for a question on an online course (coursera). Hope that isn't against the rules. The full question is much more in depth, I am just stuck on how to join these two tables. 
Not sure why there's comments in your key field. Try stripping those out on your join. I'll post the sql below. Also you can't cast a text string as an integer. I only suggested that to remove spaces and leading zeros. Does this SQL work? &amp;#x200B; SELECT business.name, category.category FROM business INNER JOIN category ON category.business_id = REPLACE(business.id,'--','') &amp;#x200B;
Understood, thanks again. 
If you don't need any values from tableB, then I would use a NOT EXISTS query: SELECT a.id FROM TableA a WHERE NOT EXISTS ( SELECT 1 FROM TableB b WHERE a.id = b.fk_id AND b.fk_id IS NULL ) You want to be very careful with NOT IN and nullable fields. If a single value of the column you specify is NULL, the expression will not evaluate to TRUE. For example: select 'OK' where 3 not in (1,2,NULL) That will return no results because it unrolls to `WHERE 3 &lt;&gt; 1 AND 3 &lt;&gt; 2 AND 3 &lt;&gt; NULL`. That last part, `3 &lt;&gt; NULL`, evaluates to UNKNOWN. TRUE and UNKNOWN evaluates to UNKNOWN. So the whole WHERE clause evaluates to UNKNOWN, so no record is returned.
 INSERT INTO PassAF VALUES ( SELECT CASE WHEN DIFAF1 &gt; AllowRepeatErr THEN 'Pass' ELSE 'Fail' END as PassVal) 
I'm not sure what you mean by "Insert into a textbox"... but you can get the query to return a pass/fail string by using a `CASE` statement: SELECT CASE WHEN DIFAF1 &gt; AllowRepeatErr THEN "Pass" ELSE "Fail" END
When I run this query, the results I get is that each user posesses the role that is composed of at least 1 of the objects that the users posseses, and this is just not correct, because roles are defined by ALL of the objects it has access to, this means that Role A is the one that is composed of Obj1, Obj2, and Obj3, while a Role B could have those same 3 objects, but also get access to Obj4, making it a completely different role, Therefore, if User Tom has access to Objects 1-4, he should be Role B, but NOT role A
&gt;Q1 Lookup the `WHERE` clause and Wildcards, specifically `%`. &gt;Q2 I'm not really sure what you're asking, maybe a `JOIN` or `UNION` is what you're after? &gt;Q3 I'm also not sure what you're asking for... If you can provide sample data for the tables and what your desired output should look like we can assist more.
Yeah sorry about that. No the Table is not called PassAF. PassAF is a column name. 
So you want to run a query against an entire table containing `DIFAF1` and `AllowRepeatErr`, then set a column equal to Pass/Fail based on that? Something like this should work then: UPDATE TableName SET PassAF = (CASE WHEN DIFAF1 &gt; AllowRepeatErr THEN "Pass" ELSE "Fail" END as PassAF)
Thank you very much. I'm still relatively new to SQL, and I think the format files and creating XML files are just a bit above my abilities right now. I think I'm going to try the temporary tables...
You don't need to create an xml file though that's just one type of format file out of the two. The command I gave you is the other type which is just a flat text file. that defines how the table you will be bulk inserting into is defined. Anyway whichever method you are comfortable with is good at the end of the day; my option just makes it into a 1 command process rather than two after the initial setup.
Then the thing between INTO and VALUES should be your table name
How can I prove they are doing different things? This is a sizable table. What can I do to prove they are doing different things when they look the same? Identical in every way, shape and form? 
I couldn't find it in what I was googling ... 
He isn't right. It is dynamic SQL which currently handles the logic as pasted, and my intent has always been to create it as a function() to simplify the processs. &gt;Also not entirely sure what you mean by a framework, but I would start by defining SQL variables at the top all dynamic values this query uses (such as @Clause2 etc). This is defined already, but was not relevant to the question being asked here. &gt;Once you've got that working as you want, its much easier to just turn those variables into function parameters, and your query should continue to work My query already works, I just wanted to simplify it by creating a function() that achieves the same results so as to simplify the readability of the script. This process runs a dynamic SQL loop that processes ~60,000 sequences based on specific client customizations. There is no other way to handle the requirements. I mean I could go to my boss and tell him that it's impossible to do what the business is asking for, but that is a lie, because it can be done this way. 
select business.name, category.category from business b join category c on b.id = c.business_id;
interesting... could you please rephrase this in terms of the original data you posted where Obj1 belongs to both Level 1 and Level 2 and how that affects Tom
here's what you posted -- SELECT SEASON , HOMETEAM AS TEEAMNAME , FTHG AS GOALS FOR , FTAH AS GOALSAGAINST , FTHG - FTAG AS GOALDIFF CASE WHEN FTR = H THEN W WHEN FTR = D THEN D ELSE L END AS RESULTS FROM PREM RESULT REPORTING WHERE SEASON ='2012-13' ORDER BY SEASON here's the same query with all eight (8!!) syntax errors corrected -- SELECT season , hometeam AS teeamname , fthg AS goals_for , ftah AS goals_against , fthg - ftag AS goaldiff , CASE WHEN ftr = 'H' THEN 'W' WHEN ftr = 'D' THEN 'D' ELSE 'L' END AS results FROM prem_result_reporting WHERE season = '2012-13' ORDER BY season 
Sure, if I don't explain myself clearly do let me know: In this case, level 1 is a user profile composed of the union of object 1 and 2 permissions, since Tom happens to have both obj 1 and obj 2 in his authorizations, Tom can be said to have a level 1 account. On the flip side, mark has a level 2 account, which can be deduced by the objects he has access to. Usually this would go the other way, when setting up complex business applications with multiple authorizations, you start by defining the objects that make up a role, and then assign a role to a certain user, which ends up assigning said user authorization to the objects defined in his role. But I messed up big time and lost the data of a whole application and found myself having to backtrack so that I can reverse engineer the roles I had previously defined by using the recovered user data.
I didn't write it like how I typed it I just made it easier to type and read but have you solved the problem?
If there were syntax errors in my query then I would deal with that before I posted for help 😂
Hey I wanted to thank you, after using some of the resources you suggested, i ended up writing something that worked. i posted it in the edit on the post if youre curious. thanks again 
Hey I just wanted to thank you, after reviewing the resources you suggested i was able to write something that worked. I edited it into my post at the very top. thanks again.
If you want to just time stamp the insertion, you can just create a column called date_ inserted and give the default value of that column (getdate()). 
You're dumb as shit.
Ah yes, @Error-451 definitely got it on the right track, you'd run an EXECUTE on a system proc already existing. Something like: `DECLARE @table varchar(50) = (SELECT table_name from information_schema.tables where table_schema=''7sqlilabs'' LIMIT 0,1);` `DECLARE @sql varchar(max) = 'SELECT * FROM ' + @table;` `EXECUTE sp_executesql @sql` &amp;#x200B; I don't think using a subquery would work unless you nested sp\_executesql procs with even more variables
You might be able to do a dense rank when querying the table without adding any more columns
Ironically I was just hired into a data analytics job out of college with a Computer Science Degree and Research Psych Degree. I have no certifications as I learned all of my SQL on the job and practicing in localhost DBs I set up myself for practice. Spotfire experience is great to have. I'm just now getting into it but it's an awesome tool and HTML/Javascript/Python experience on top of it would help a ton DBA knowledge will vary across jobs. I know the DBAs where I work do little to no development work. They are categorized as "Infrastructure" who mainly just govern our databases, architecture, and review code written by developers. But it may vary and definitely wouldn't hurt. &amp;#x200B; I doubt the "version" of SQL you are certified for matters as syntax and features are very similar across platforms (I worked in MySql, Oracle, and SQL Server). Different versions may benefit you based on new features and infrastructure, however. I have no certifications besides my degrees and there are a couple books I could point you to if you want to dig in. I'm also happy to work through problems or questions you have about SQL code itself. PM me if interested. Good luck!
I thought a .txt file wouldn’t upload if it is being loaded into a table with an extra column? My .txt file doesn’t have headers.
Is your "Level" column an integer or literally a string saying "Level x?" &amp;#x200B; You could try: SELECT USERS, MAX(ROLEID) FROM table1 JOIN table2 ON table1.OBJECTID = table2.OBJECTID GROUP BY USERS &amp;#x200B;
It Is a literal string, I'll try that it a bit and get back to you
You could maybe add on to your JOIN conditions: &amp;#x200B; INNER JOIN Term AS te ON sp.termID = te.termID AND ((GETDATE() BETWEEN te.startDate AND te.endDate AND term != 1) OR ( GETDATE() &lt; te.startDate AND term = 1)) Unless I misunderstood
Thanks for ur input 🤦
But how do I fix the points problem do u have any clue how to start that query it would be a big help 
I'm not sure what is the scoring column, is it just your GOALDIFF? &amp;#x200B; Whatever your score column is you want to evaluate, do this and put it in place of the GOALDIFF in the order by: &amp;#x200B; SELECT SEASON, HOMETEAM AS TEAMNAME, FTHG AS GOALS FOR, FTAH AS GOALSAGAINST, FTHG - FTAG AS GOALDIFF, DENSE\_RANK() OVER (ORDER BY GOALDIFF) AS Rank
Try using NOT EXISTS
Do you have any examples please? What I am trying to achieve is select the same column (Users.id) 2 times but each column has a different condition.
Yep, just drumming up marketing for another shitcoin ICO
Wow thanks heaps for the advice, I'll tweek it and fire it off and see how it goes!
I have come across other SQL based blockchain, but from the white paper, Aergo's aim is not that users/developers build a complete new blockchain, but build on top of Aergo blockchain. This makes it a lot easier just like building any other app so one doesn't bother building his own hash functions and other complex blockchain stuff. To be honest, this is cool, and I hope to try it out
The score colmn is known as FTR which say 'H', 'D' only that's why I made a result column to show results as win loss draw So I want when each team wins they get 3 points Then I want to calculate the sum of those points and base ranking on the highest total points to lowest total points After that I need to create a @date paremeter to query a specific date and get a snapshot picture of the table and the team that's winning on this day but that's after I get this right hopefully 
There is no score column I need to create one with 3 points after each win and 1 point after each draw that's where I need help first 
The 'likes' table is there to cover the a likes b bit. This is then joined to the 'friends' table twice to find the third student C that is friends with both the students from the like relationship. The "not exists" bit at the end makes sure that a is not friends with b already.
So if you look at the start of the query, it's joining the friends table twice on both of the IDs in the like table. That's just finding all of the friends in the like relationship. Then if you look at the where, it's looking for mutual friends. The main thing is in the not exists. That's ensuring that the like relationship doesn't also have a friend relationship
The columns names need to match the destination exactly ( case sensitive), but the destination table can have extra columns. I do sqlbulk copy and add an identity column and a date inserted. Not sure of your situation whether this is an app or...something where your just importing data. If its the ladder you may want to consider building a SSIS package in visual studio?
No, the likes table gives all ID1 that like ID2. Joining friends then gives a list of people that are friends with ID1. This will essentially be ID3. Joining the friends table again will give a list of people that are friends with ID2. (this is now ID4). By inner joining them we can make a list that only contains people that fall in both ID3 and ID4 categories, essentially making them the true ID3.
The fundamentals here are important to know, this is an educational course.
The fundamental here is the XY problem - don't use a rdbms for your graphdb use case.
It's important to know **why** we don't do something, not just that we shouldn't do it.
 SELECT * FROM user u LEFT JOIN approval a ON u.id = a.id WHERE a.id IS NULL That should give you all users that don't have an approval record.
Does my comment somehow indicate I don't believe this?
Create view as select * from ...
I use `IF EXISTS` to often drop a #table after it's been used in a sequence, e.g.: select * into #table where things select * into #newtable where things if exists #table drop #table select * into #newesttable where mostthings if exists #newtable drop #newtable Google for specific syntax. Not sure what Teradata supports.
Thank you. I fully understood this now! Could not understand yesterday, read this thread and woke up today to understand it immediately
Thank you. I fully understood this now! Could not understand yesterday, read this thread and woke up today to understand it immediately
I don't have much experience with international names, but make sure your data type on that column is NVARCHAR and not just varchar. 
I'm connecting to a Google Cloud SQL instance w/ MySQL 5.7. &amp;#x200B;
Got that advice from a friend at the same time. updated the data type and reinserted... no luck. It's still treating them as equivalent values.
AFAIK, MySQL doesn't have a good solution. You can use the utf8_bin collation, but that is both case sensitive and accent sensitive. There is no case insensitive, accent sensitive collation in MySQL. 
&gt;You can use the utf8\_bin collation, but that is both case sensitive and accent sensitive. There is no case insensitive, accent sensitive collation in MySQL. utf8\_bin worked, thanks! Case sensitive is actually fine as WoW character names cannot include capital letters after the first. 
What you should do is basic normalization of your table. It looks like you have something like: Name, Class, Level(?), SomeSmallValue(?), SomeLargeValue(?) What you could do is have a person table: PersonID, PersonName, ClassID, Level, SomeSmallValue, SomeLargeValue A Class Table: ClassID, ClassName So you'd then do an insert first into the class table: INSERT INTO class VALUES ('Paladin', 'Mage') Then insert the two rows: INSERT INTO person VALUES ('Aprîl', (select classid from class where classname = 'Paladin'), 110, 3, 15145 INSERT INTO person VALUES ('Aprïl', (select classid from class where classname = 'Mage'), 110, 3, 15145 You'd have a primary key for the person, which would uniquely identify someone, even if they did happen to have the same name (which in this case seems to be the problem... sort of). That is more work, so if wanted to just uniquely identify the person, you could still add a PersonID without normalizing, making the table: PersonID, PersonName, ClassName, Level, SomeSmallValue, SomeLargeValue This would allow you to keep the same queries, but you'd not have a problem with duplication of the person name because the primary key woud be the ID.
Taking this even further, generally each Person can have zero-to-many characters, so ClassId and such should be taken off of the Person table in favor of a Character table which would have a FK link to Person.
If this is ill-advised, I’ll blame it on the fact that I’ve not worked with billions of rows, but I’d think a simple: WHERE HOUR(datetime) = 12 would limit your results to the noon hour. 
That's a good point. I was thinking "April" (I don't know how to get those special I characters and I'd rather type out this explanation than move my hand over to the mouse to copy and paste it) was the name of the in-game character... though "character" now that I think about it would have been a better name for that table. 
JetBrains have [a Github repository with some sample DB scripts](https://github.com/DataGrip/dumps) for testing their DataGrip product. The ["Sakila" database](https://github.com/DataGrip/dumps/tree/master/sql-server-sakila-db) is a sample Movie Rentals DB originally written for MySQL; they've ported it to MSSQL. I don't know how up to date the content is, but maybe you could treat getting it up to date as a learning exercise? e.g. use the table structure as a starting point, and refresh the movie lists from other sources?
This is perfect! Thanks for the comment and providing all the links. 
I just saw it. That’s excellent. Exactly what I wanted. You made my night! Thanks again. 
Also check out /r/datasets/ for heaps of great data to work with They allow you post requests too
You should create the schema and insert the data yourself. It's a huge skill to have and it'll give a better understanding of how things work.
Not sure why the downvoted on Stack Overflow. I understand the desire to work with meaningful data. I believe it will help you retain the I formation better. I used PowerShell and a free Weather Underground account to pull down my local weather. Then I created tables to hold my gas, electric and water bills.
Imdb does share a chunk of their data. It's here https://www.imdb.com/interfaces/
In this case, April is both the in-game character and the out-of-game human but that's just a coincidence.
First it always helps to do a select all and paste as values in the csv when you're having onboard issues. Additionally ensuring your saving in a standardized format like utf 8. Then it's all about finesse.. why not import it as a decimal instead of trying to convert it later? 
Is review date a year? If it is make it int instead. It’s trying to convert it to time
The select statement is the picture in the top right corner. I just tried to put it all in one. The picture in the bottom right (Modify Columns) is during the import step. Again .. sorry if I'm making it complicated. During import if I select decimal the 1 digit numbers come out as 3.000 (depends on how many I specify) and the rest (actual decimal numbers) are still NULL.
If I try that while importing, the error message says that he can't convert a string into an int. Strange that he treats it like a string... If it helps, when I try to make it a nvarchar, I get a result of "20:13" insted of "2013" 
IMO - don't ever use the data type FLOAT, causes LOADS of issues. Use decimal. When I have issues with csv's - I open them and save them as excel and then import - often times it saves me from guessing column width. But again, unless you work on calculations for distance to planets, do not use float. https://blogs.msdn.microsoft.com/qingsongyao/2009/11/14/query-on-float-datatype-may-return-inconsistent-result/
I don't see your select statement. Time is not datetime or date for one. Second on decimal it's like decimal(10,3) otherwise it goes to default. 
Sometimes saving columns as character instead of numerical helps to at least get the data into a table. So two thoughts, make absolutely sure you are comparing the same rows. Selecting your data may not return it in the same order as you are viewing it or in your file - so you may be seeing nulls where you *think* there is data, but if you match up the correct row in origin to table they are the same. Second, if your data still isn't correct, on the import make all columns text, like nvarchar(100) or varchar(100), etc. Then see if value are coming over.
Sorry but how do I change the source (CSV)? I use a regular notepad to open it and it's just a bunch of data separated with a ","
Is there anything I can do to not make him convert it into int? Even telling him to use decimal data type doesn't help. He treats the date column as string. Strange considering it's only made of 4 numbers (year). If I try converting it to an int while importing, he won't allow me to change it from a string to an int. 
Import data with the review year importing as a string. Then you can convert it as necessary once its in SQL.
I know you mentioned you are a newbie, but what you are saying I'd bet my salary isn't what's going on: And if I try making that into a nvarchar it turns it into 20:16 where it should only show 2016. I'd be happy to help if you are able to share the file. 
What software are you using for the import? Here I was talking about the rating column, not the date. Any ratings in the destination that are not whole numbers are NULL. This means it's an INT (whole numbers only) field. The date SHOULD be an INT field. Then it will not add the unnecessary zeros. 
Yea you're right. I have no shame in admiting I'm a newbie. Gotta start somewhere... I might be doing something wrong but here are some more pictures: [https://imgur.com/a/OLMnpBF](https://imgur.com/a/OLMnpBF) And if you want the CSV file, I've uploaded it here: [https://ufile.io/7hv5l](https://ufile.io/7hv5l) 
YEAR is a number. Treat it as a int. 
Regular "Import flat file" option in SQL server or the Import and Export Data option. Yes he treats the Rating column as an INT even if I specify decimal(10,3) while importing. It still comes up as NULL for decimal numbers. And he treats the column with the Date as a string. I agree that it should be an INT field but he won't allow me to make it an INT while importing because he reads it as a string.
Are the datatypes on the table your importing to correct? Also, you can open the flat file in excel and define the source columns. 
While messing around with it trough excell first I got it to show the way I want. I really appreciate all the help you have provided. Thank you.
While messing around with it trough excell first I got it to show the way I want. I really appreciate all the help you have provided. Thank you.
While messing around with it trough excell first I got it to show the way I want. I really appreciate all the help you have provided. Thank you.
While messing around with it trough excell first I got it to show the way I want. I really appreciate all the help you have provided. Thank you.
Alter table Alter column data type Truncate table And reimport It’s sql trying to figure out the data types with the data
Yep... Like I said, ensuring your Excel is in a standardized format before ingesting is often far easier... You can usually write or record a simple macro to get it into said format to make your rail as quick as possible
I'm glad you have it working. I looked back at all your images and I don't see any on configuring the actual flat file - your images and statements are focusing on the sql table, but if you don't configure your flat file columns with the correct data types as well, you'll get bad results. So, for future, once you connect a flat file source, you've picked the file location/name, you have to go into Advanced (see first image in the link below) and configure those columns correctly. https://docs.microsoft.com/en-us/sql/integration-services/import-export-data/connect-to-a-flat-file-data-source-sql-server-import-and-export-wizard?view=sql-server-2017
 1. Find the attributes that are neither on the left and right side \&gt; (none) 2. Find attributes that are only on the right side \&gt; D 3. Find attributes that are only on the left side \&gt; A 4. Combine the attributes on step 1 and 3 \&gt; since step 1 has no attributes, it’s just A 5. A is not a candidate key by itself, so test other possibilities: Test AB: AB -&gt; C, then ABC can infer D, so AB is a candidate key Test AC: AC -&gt; B, since C can infer B, then ABC can infer D, so AC is a candidate key Test AD: Neither A nor D can infer anything, so it is not a candidate key Examples like ABD would work as well, but are not considered candidate keys on their own because a candidate key exists in this combination (ex: AB is in ABD) so it would be unnecessary to include ABD as AB is more minimal and is a key itself. &amp;#x200B; These two links help dissect the theory more. I used the first one as a template to walkthrough: &amp;#x200B; [https://djitz.com/neu-mscs/how-to-find-candidate-keys/](https://djitz.com/neu-mscs/how-to-find-candidate-keys/) &amp;#x200B; Video: [https://www.youtube.com/watch?v=9fuJUQJd-A8](https://www.youtube.com/watch?v=9fuJUQJd-A8) 
Go into Windows Services and make sure the Postgres services are running. You might also try disabling Windows Firewall and see if that lets you get through... if it does, you can open the necessary ports.
Big data without knowing java or Scala? I would say Python probably is the best language for Big data. At least that’s what I have seen on job postings and etc. As far as I know python is the most popular language foe that
I’m not in the industry so don’t take my word for it 100% but I’m studying myself in going towards big data and data science. I’m personally learning python right now, and planning on learning Hadoop and Spark 
Stack Overflow is hyper sensitive about the contents posted by the community members, and the community members feels extremely egoistic when people like you and we post questions there. As they say that "no question is foolish", but they have very bullying attitude to those.
 SELECT a.id, a.Param_Value, b.Param_Value, c.Param_Value FROM MyTable as a INNER JOIN MyTable as b ON (b.id = a.id) INNER JOIN MyTable as c ON (c.id = b.id) WHERE a.Param_Key = 'First_Name' and b.Param_Key = 'Last_Name' and c.Param_Key = 'Contact'
Look up the PIVOT function - should do what you need to do! 
Which RDBMS? Each one has its own way.
I think kingdom_wide's answer is the one I'd go with but in the future, what I would google is "making data wide from long" in case that helps. 
&gt; Not sure why the downvoted on Stack Overflow. Because SO discourages "shopping list" questions such as software recommendations, where to find resources, etc. It's meant to be used for actual code questions/problems - debugging, algorithms, etc. - not unlike what you'd put on a whiteboard to hash out with a co-worker. From https://meta.stackoverflow.com/questions/309098/what-should-the-predefined-off-topic-reasons-be-for-stack-overflow: &gt;Questions asking for tool or library recommendations are off-topic for Stack Overflow as they tend to attract opinionated answers and spam. Describe the problem and what has been done so far to solve it.
Perhaps you could use the [Movie DB's API](https://www.themoviedb.org/documentation/api?language=en-US) to pull out a lot of data? 
You should be searching for `TSQL Islands and groups` One article that helped me to start thinking in the correct way is this one; https://www.red-gate.com/simple-talk/sql/t-sql-programming/the-sql-of-gaps-and-islands-in-sequences/
If MSSQL then querying sys.tables and sys.colums will be a workable solution. 
Sorry I am not awake yet! I am using Microsoft SQL server Management Studio
Thanks, I've never used visual studio but maybe I should try. The data that I am importing does not have column headers at all. 
When you say 'solution' are you talking about SSIS?
So you have 40 *.SQL files and you're asking how you can open (1) of them instead of opening them all together? 
Depending on the RDBMS: select column_name from your_dbms_metadata_table group by column_name having count(*) = 1; 
PIVOT or UNION the table against it self once for each column. I've actually found Union to be faster in some scenarios. 
Sql server
Sql server
Thanks anymore detail you can provide on creating the query? Not sure how to go about setting this up
No, if your count is the outer query, you won't be receiving the entire dataset over your connection, just the result of the count. I'd expect to see two table scans, and (depending on how they're indexed) either a hash match and hash aggregate or a merge join (and possibly a sort, depending on indexing) and a stream aggregate. 
Dumb question but lets say I have no indexes, versus having (1) field indexed, versus having all fields indexed. Will an intersect/except perform differently? I am not going to index anything to improve for this example, I am just testing some new code that I'm deploying and want to see the counts for where there is an intersection &amp; when there is an except, with the hope that they are always 0 so I know my new code has not modified the results of the table.
Seems like the best way to go to me. We did a lot of testing with INTERSECT recently and it's the most efficient way to diff a table that our BI team could find. By orders of magnitude like 5x compared to joins, if I recall correctly. And I don't see of another way to speed up a count(*) function. Seems good to me.
I didn't expect I would, but I'm not interested in doing a deep anlysis. I have a fairly complex and long process that I am doing significant modifications to. I have a copy of the results saved to a table, so as I'm making changes I'm rerunning the process and then comparing the two tables. There are differences, but I can explain them, and they are as expected based on the changes made. The query takes a bit of time since the table is large, but this isn't something I do often enough to really care about optimizing it. It was just a curiosity I had while sitting here waiting for the results to come back.
If you have no indexes, then SQL would have to scan both tables, and either sort them both for a merge join, or use a hash join that would require building a hash table out of one of your tables. Either way, it's going to need a fair amount of memory to work with. If enough memory isn't granted, you'll get spills to disk, and the query will take a while. If you've got a Clustered index the same on both tables, then it can do a more efficient scan and merge join, followed by a stream aggregate, so it doesn't need to have all the data in memory at once, and you don't spill to disk. I'd expect a single column index to be irrelevant to the query, with the possible exception of a unique index, in which case you *may* get a scan of one table, with a seek on the unique index of the other, with a lookup to the base table for the other columns when a match is found. 
When you say a clustered index but then go on to talk about PK/GUIDS... are you saying something like a clustered index on a datefield would be helpful, but not as helpful as having a GUID... and just randomly indexing some other field would probably not be useful at all?
I think our biggest table in the exercise had ~250m rows. JOIN and other methods came in around 35 minutes. INTERSECT was under 10. Of course, YMMV with the amount of columns, hardware, table sizes, etc. I am genuinely curious to see if there's a more efficient method though.
Just thinking outside the box... if you had a GUID on both tables you might be able to do a UNION ALL where you insert an integer 1 for the top of the union, and an integer 2 for the bottom of the union, and then `select pk from union_table group by pk having count(*) &lt; 2` and join back to the table(s) to see what rows are involved. Beyond that I can only see using a join or an intersect/except.
Why the need for non proprietary? Oracle streams could work if the db is old enough.
TBH - been working in this industry for 20+ years. During those years the least helpful people are the ones with certificates but lack real world experience. When you ask them why they are a (programmer/network admin) they say something like: looking for a change in career. Really good IT peeps have stories like: I always loved programming so I wrote this app once to see if I could do X, Y, Z. Sorry if this wasn't the answer you wanted to hear. I'd advise you to: 1) Take a few classes, if you are able. Either in-person at Community College, in person at a conference or online (if that's your only alternative) and figure out what you are interested in - as far as development, databases, network, programming, etc. 2) Make your own project - write an app, create a database, do SOMETHING you enjoy to get hands on experience. Document your stuff on a blog or print out examples you can take to an interview. I find that the best indication of success is being VERY interested in the topic/language rather than having a certificate. Certificates on resumes, to me, means you can take a test. We gave someone a chance recently that had PM experience and taken a few of the SQL Server db tests. He was worthless and let go within 3 months. Good luck with your future, for real.
What DBMS are you using? If your Event table has a date you want, your DBMS might have a function like DATEPART, where you can pull out the Year or Month component from a DATE or TIMESTAMP. 
I'm curious what you're tracking that will definitely only happen once a month.
We need to know what RDBMS you're using. That's why the posting rules say "When requesting help or asking questions please prefix your title with the SQL variant/platform you are using within square brackets". I'm not following your thinking here. You haven't adequately described what you're trying to accomplish and you haven't provided any information on what your tables look like. What does your event table look like? Can you post the DDL and possibly some sample data? Doesn't your event table already have a datetime for when your event occurs? Can you provide a sample query of what you need to run? What's wrong with: SELECT * FROM EventTable WHERE YEAR(EventDate) = 2018 AND MONTH(EventDate) = 12 Or: SELECT * FROM EventTable WHERE EventDate &gt;= '2018-12-01' AND EventDate &lt; '2019-01-01' If you really need to quickly index by date and year and need a column, then why not modify the event table and add a computed or generated column? Most RDBMSs allow indexing on computed/generated columns. 
Downvoting on Stack Overflow seems to be a common thing for some reason. The community is not as nice as Reddit.
Keep your id as the PK, then add either a datetime field to store your month/year, or add separate fields for month year and then put a compound index it. Using month/year as a PK may seem like a good idea at the moment, but requirements inevitably change and when you later need to store a 2nd event (or a different type of event) then suddenly you've designed yourself into a horrible corner
No.
Periods
You could add a surrogate key. This is an otherwise meaningless value that serves to make each row unique. In MySQL tables, you'll often see an `id` (or similar) column defined as: `id` INTEGER NOT NULL AUTO_INCREMENT When you insert a new row, the database automatically assigns a new value for the `id` column. If you make `id` your primary key, this means each row will be unique even if all the other columns are duplicated. You can then add a [covering index](https://blog.toadworld.com/2017/04/06/speed-up-your-queries-using-the-covering-index-in-mysql) for the other columns instead of having them in the primary key.
It depends what you want to do. Most entry level data analyst roles require it. If you want to get into database management or design you're going to need to know more about database structures, you're going to need to know how to script, you're going to need to know about data security. Etc. 
Exactly. If you can't uniquely identify a row, you aren't even in 1NF!
I thought this was strange but didn't have a term for it. The source table for all my data from I can tell does not have any way to uniquely identify a row. 
How set are they on the design? Is the person who did it still around ? You should probably check. Sometimes debormalized databases are designed intentionally for performance reasons... But often they are the result of lack of formal training. Check out the wiki entries on database normalization if you aren't familiar with it. Data makes a lot more sense when you think of it in such a manner
I can change my table. That shouldn't be a problem, but it would be a lot better if the source table had a unique key. 
Oh yeah 100%. Try to get it into 3NF too if you can (where your attributes are only related to the key). So if you have a person table and they hvae an attribute that doesn't apply to the table, put it in only that table and then tie them together.
Thank you!
Thank you very much! This mixed with the above helped get me what I need. Thank you!
Hi If you want the desired output without any alteration you should do [SQL Backup Repair](http://www.databasefilerecovery.com/sql-backup-repair.html)on immediate basis. It will definitely help you.
&gt; strata scratch hey, How to get code? i can't use money for this, is there any student discount?
What are you doing now?
Maybe try to get in Business Intelligence support or something, i've been on an internship in BI for 3 months and it taught me SQL on a decent level. Now i have no problems using it in my SW job 
You need to understand your data, that's not the sort of thing a script is going to be able to do perfectly.
Depending on the issue, there are several things you can do: Check if you can/have enabled instant file initialisation. Otherwise when you restore, windows has to zero out the entire space that will be used by the data files, and this can be time consuming. If restoring a log is taking a long time, make sure your log isnt ridiculously oversized, and that you don't have a really large number of VLFs. There's some guidance on sqlskills.com about this. If it takes a long time to restore a log chain, try adding differential backups to your backup/restore process to reduce the number of restores required for recovery. If disk I/O during the restore is an issue, make sure the backup isn't on the same set of disks as the database you're restoring, and potentially look into the performance of the disks themselves. You can also tweak maxtransfersize and buffer count in your restore command. There's plenty of info on Google for all of these options.
Well, can I somehow get back data that have common, for example, e-mail types? Like all objects on the server that contain data with '@' ?
Our database is around 90GB and the logs are around 20GB. I was planning to increase the VLFs size to get faster recovery speeds for the price of more storage. Thank you for that good explanation. 
Memories of /u/stonetear You break down your searches. The fields are going to have patterns of where data will be stored and you then search on those patterns 
No I do not have that option
The second concept is much, much worse in reality than you think. You want to have a table with a column for each possible item type? According to your description that would require a table with 300 columns. This is wrong. Your first approach is the correct one. SQL databases were created to deal with inserting A LOT of records into tables, querying them and returning the values. Go with this approach. OR you could have a json column which will contain the inventory for each player. But as always, this approach has its pros and cons as well. It makes it much easier to query database-side, because the DB must return only one row (unless you want to expand it using db functions, which adds cpu usage). Then the json values would hae to be processed client-side - it's up to you if it's a good or bad thing.
I'm leaving my job, you can take mine. Databsse analyst
You'll have to query every text field on every table in every database. If I'm your DBA and I see something like that fire off in my production environment without warning, we're going to have a chat.
What version of SQL? Sometimes you don't have to normalize. The only issue I see is if someone updates their JSON client side and somehow saves that then they can just randomly obtain it. You could just store a JSON of what the player has in an NVARCHAR(MAX) field.
As long as your tables are properly indexed with a fill factor, I highly doubt you'd hit any limitations. I used to help run an private mmo and it worked fine with 3k concurrent please.
At my last job we hired entry level database analysts. Basic SQL was one of the main requirements.
 select t1.country as "COUNTRY", CONCAT(SUBSTRING(t1.MANAGER,1,1), '. ', SUBSTRING(t1.MANAGER,LOCATE(' ',t1.MANAGER)+1)) AS "Manager Initials", t2.country as "COUNTRY", CONCAT(SUBSTRING(t2.MANAGER,1,1), '. ', SUBSTRING(t2.MANAGER,LOCATE(' ',t2.MANAGER)+1)) AS "Manager Initials" from matches m inner join team t1 on m.firstteam = t1.teamid inner join team t2 on m.secondteam = t2.teamid where t1.country = 'CROATIA' or t2.country = 'CROATIA';
Second one, hundreds of columns, is horrible. Do Player ID and ItemTypeID.
Your question is too generic to answer. What are you working on that has you concerned with how to speed up recovery?
Restoring a 50GB database from one server to another one but the ETA from MSSQL is about 6 hours.
How much do you enjoy coding? Are you interested in other languages or just SQL?
u/evisceraze, were you able to solve your problem?
If you decide to go the BI/BA route, learn Excel and at least the basics of Tableau. Beyond that... it really just depends on what whoever hires you is using. Could be Python, VBA, R, SAS, one of a dozen different BI packages... 
Ah sorry, totally forgot about this thread. Yeah I was able to, i created a unix script and used sqlplus to achieve my goal. Thanks a lot!
To reword your problem, if the camera is not an Axis camera, then the model information is found in XML file A. If the camera is an Axis camera, then the model information is found in XML file B. Is that correct? How do you know when the camera is an Axis camera? Is there a flag in dbo.hardware for camera type?
this may sound rude but I promise I am not trying to be. Literally type sql+analyst into indeed.com for your city and you will see tons of postings for Jr Data Analyst that make 30 - 50k / yr. It's not a ton of money but for entry level its pretty good. Good Luck.
Very true, but a lot of them get back to SQL quickly, I use a lot of PROC SQL in SAS, SQL connections in python or R and a lot of Tableau/BI back ends are supported by SQL, it is probably the best common denominator.
Higher Education is a good one also, fairly rewarding using data to try to support student success identifying students likely to drop out and disparities between race/gender in performance and faculty pay.
I'm not a huge fan of the json approach. Any time you want to modify the amount of any item you have to access the amounts of all items. What if you modify two items simultaneously - do you have two copies of the json payload floating around client side? 
Agreed. I mentioned JSON columns because they give some powerful options at the cost of some other complications.
&gt;This worked perfectly and I was able to get the results I needed using the Case statement. &gt; &gt;Thanks so much for your help!
Great! You're welcome!
Good grief... Nailing an xpath query on an xml file and table you don't have on the first time ?! That's VERY impressive. 
Is it all one line? Is this in a batch file?
That makes sense. Our local university pays half what my health care industry pays for similar analysis work but I image I'd feel more fulfilled. If I lived in a big college town like Chicago or Boston I bet education is the way to go.
Thanks, I think I will go with the first way
I love postgresql, you won't find argument here
Basically the header is five rows down. But the dates covered are two rows down. So I can't skip the first five rows, I have to put everything into a staging table. But if I run a conditional split, it errors out on the blank row. The bottom line is the csv is just an absolute mess. I think the script task is my answer. Just have to figure out how to get the damn thing to load even so I can script it.
Top one is better. It's more of a reference table so I assume you will be using 3 other tables that define each of those columns. The bottom one is worse because it's much less flexible if you want to add a new item.
dbo is the schema name. The schema is like the container that your objects (tables, views, etc) live in. The syntax `dbo.myTable` is used to show that you're referring to the `myTable` object in `dbo`. dbo is just the defacto standard for a default schema name. You can create other schemas and name them differently if you want.
I also recommend against using schema names as a logical separator of your tables (as in your patientInfo example). In regular "application code" such as Java or .NET, it's recommended to split up your files and classes into directories or projects/packages for the sake of re-use and overview, but in my experience, with SQL, splitting up tables into schemas "for the sake of", just makes it a hassle to work with. * It makes big queries more troublesome to read and write, since every table has an extra prefix, for example: Customer vs. Customer.Customer * Tables that span multiple business entities, such as a CustomerAccountRelation table - do you put it in the Customer schema or in the Account schema? It creates confusion and ambiguity. * Some error messages don't show the schema name, just the table/view/object name, and since tables or views in two different schemas can have the same name, it can cause confusion. I'm not saying it's the end of the earth if you use schemas like this, but so far, I've seen zero benefits of such a naming convention. It only makes sense to me if you want to build a system where you have per-schema permissions. That way, a certain user or group of users can be set to only be allowed to work with objects in a certain schema.
"dbo" stands for database owner. It's the default schema created in every database. If you don't create another schema, every object is automatically created using the dbo schema. That is, each user in a database has a default schema, and dbo is the default default schema for new users. The dbo schema is created and owned by the dbo database user (a built-in, default user), which is a member of the db_owner database role. Logins that are a member of the sysadmin server role always operate as though they're the user `dbo` in each database. This is why when you run `select user_name()` and your login is in the `sysadmin` role, you get `dbo` as your output. If you don't understand the difference between database users, database roles, server logins, server roles, and how they all relate, then you will want to start reading [here](https://docs.microsoft.com/en-us/dotnet/framework/data/adonet/sql/sql-server-security) or [here](https://docs.microsoft.com/en-us/sql/relational-databases/security/authentication-access/getting-started-with-database-engine-permissions?view=sql-server-2017). Schemas are both an organizational tool and a security partitioning tool. Very few databases uses them, in my experience. AdventureWorks2012 is a good example of a database that uses schema. 
&gt; dbo is the default schema name in SQL Server, so it's implicitly used when you don't specify another Only if the user's default schema is `dbo`.
A foreign key would work. The foreign key would require that a the key exist in the foreign table on insert. You may also want a UNIQUE key, to indicate that no more than a single instance is required. 
It stands for the original developers of sybase, David, Brian and Oliver. Sybase then branched off into SQL Server. 
Yes I think you're probably right. I don't use Oracle, but even on MySQL it's difficult to guess the exact performance implications without seeing an execution plan for both
&gt; That way, a certain user or group of users can be set to only be allowed to work with objects in a certain schema. And, implied in what you said, is that permissions granted at the schema level not only apply to all objects in the schema (obviously) but to any FUTURE objects created in the schema. Like you said, probably a zero-to-small benefit for most, but once in a while it's very nice.
How do you handle a situation where a user buys 3 different products in one purchase? Right now it would appear as 3 separate purchases.
Thanks very much for your help.
If you are storing raw transactions, then store the date (or datetime if you can). Grab all the useful data available to you, and then worry about reporting and aggregating it later. 
Ok. Even if you do day, you still have to aggregate by day. Why not use datetime? So are you interested who the customer is as well? Does it matter to you if a customer purchased multiple different products at once? 
I agree with others about the datetime in transaction. What about an id column on Transactions? Industry isn't really tied to region, right? That's more of an attribute of a shop, so you might want a region table.
If you go through the GUI installer, you can output a config file at the last step instead of actually performing the install IIRC, may be of some help.
Where does it place the config file at? I've tried looking for the ini file afterwards and cannot find it. 
Thank you so much for the point outs! 
guess peeps are cranky today. 
Even if you only had 10 item types, it's wrong because it scales so poorly. You'd be altering your table each time you modified the game to add a new item.
do you need quotes around all of the strings? like /UIMODE="Normal"
Do you control all the software which will access the tables? If so, you could revoke all permissions on the tables and provide an API to update/insert/delete table A using stored procedures only. Then you just make sure your stored procs update table B as needed
Will this not be possible if I can't revoke all permissions to all the tables? Because I don't think they'll provide me this highly elevated access.
You could do it without revoking access, but then you cant guarantee other software isn't manipulating the table... meaning your audit records in Table B is potentially unreliable (ie, if you cant force all access via stored procs, its not worth pursuing this idea) 
&gt;&gt;"dbo" stands for database owner. I believe it stands for database object.
Great answer
I actually tried this before doing the command in my original question. I looked at every screen and it was not listed anywhere. But if you do it with Server 2012 Express it does show it on the 'Ready to Install' screen. On the 2012 install it puts it under C:\\Program Files\\Microsoft SQL Server\\110\\Setup Bootstrap\\Log\\&lt;timestand&gt;ConfigurationFile.ini, I tried finding this under the the path for when I did the 2016 install and it was not there. &amp;#x200B; So apparently 2016 doesn't show the configuration path file like 2012 does. So is there something different with getting the configuration file from a 2016 install?
Check this out. https://stackoverflow.com/questions/19963735/understanding-header-and-detail-tables
Do your own homework
&gt; SQL Server Express Edition setup does not create a configuration file automatically. The following command will start setup and create a configuration file. &gt; &gt; SETUP.exe /UIMODE=Normal /ACTION=INSTALL [https://docs.microsoft.com/en-us/sql/database-engine/install-windows/install-sql-server-using-a-configuration-file?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/database-engine/install-windows/install-sql-server-using-a-configuration-file?view=sql-server-2017)
I did try both folders, and nothing.
A foreign key would definitely work in this case as long as the dates are UNIQUE. However, referential integrity must be maintained and the first table cannot be lost/altered to the point of changing that, unless you lose the Foreign Key. &amp;#x200B; If you absolutely have to have this criteria I would create a new table that also contains the dates and have table 1 and table 2 reference this "master" table so that if table 1 is altered in some way, the master and table 2 will be unaffected
At least intermediate. 
I could use all what you listed but I feel like a beginner. 
Me 2 man. Like seriously a beginner. I read a lot here and I feel like I know nothing. So much to SQL but it makes me eager to learn more 
&gt; make sure you prefix your post title next time with [MSSQL] This is noted. &gt; but what makes you want to avoid CDC I was asked to translate the CDC process into SQL query. I'm not sure the details behind it but that's currently what they're looking for. &gt; Stored procedures that write to the log table Do you have reference link that I can as a base for my script to make it a stored proc?
I see. I'm sorry if I'm sounding a bit thick but I rearranged my original structure to this: Shop: {s_ID,Name,Region,Phone_Num} Customer: {c_ID,cName} Product: {p_ID,pName} Order: {order_ID, date} Orderline: {orderline_id, order_id, c_ID, s_ID, p_ID, Quantity, Price} Does this look better? &amp;#x200B;
There are some things you can't do across schemas! One of them is using a function from one schema in a computed column definition of a table in another schema.
TIL about otherDB..mytable. Thank you!
Thanks so much for your detailed reply!
This may sound basic but are you allowed to maybe try and connect to an instance your colleague regularly connects to on a regular basis to see if maybe it’s just your installation? I haven’t connected to oracle in awhile but is your tnsnames updated?
Data Analysts is a good starting point. If you want to go further you can start getting into Data warehouse and BI.
If you're not writing to the table and space isn't a concern, then you can have as many indexes as you want. It won't harm the performance. Order of the indexed columns is paramount. If you index on A then B, the index is useless for querying on B alone. If you're working with large tables sometimes it makes sense to extract just the data you need into a temp table (with indexes) and then query that. Alternatively, a covering filtered index could do something similar.
Google "use the index Luke". The explanations are far better than anything that I could reply with.
When Blizzard made WoW they went with the second option for spell effects, so they had a hard coded limitation of three effects per spell. They later redid the tables using the first option.
Code has been solved now the SSRS report hopefully 👍
It’s for reporting purposes. I work at an NGO where basically all this info is stored on Excel and it is a huge hassle trying to piece all this info together. I thought of creating a database to store all this info in so we wouldn’t be so confused all the time. Thank you (and everyone in this thread) for the advice. I have a better idea about how to organize my database now.
&gt; but this doesn't have the desired result it does exactly what you said you wanted, so maybe rephrase what you want another way? also, since you have `where title like '%&lt;keyword&gt;%'`, the last ORDER BY condition is redundant
There are other important factors. Depending on the RDMBS you use of course. I'm most experienced with SQL Server, so I will present an example for it, I don't know if it works for any other engines. When working with views, change inner joins to left joins, if you can Imagine the following view on your data: CREATE VIEW report AS SELECT customers.name as customer_name , products.name as product_name , sales.period , sales.amount , sales.qty FROM sales INNER JOIN customers ON customers.id = sales.customer_id INNER JOIN products ON products.id = sales.product_id Now let's say you're only interested in sales by period, maybe you even have a perfect index on the sales table like `CREATE INDEX ix_period_amount ON sales (period) INCLUDE (amount)`. A query like this: SELECT period , SUM(amount) AS total_amount FROM report GROUP BY period Will be slow as fuck and won't use your index. Why? Because these `INNER JOINs` to customer and products table have to be resolved, despite the fact you're not using any data from those tables at all. Now, if you change `INNER` to `LEFT`, you're golden. SQL Server will optimize these joins away, knowing you don't need them. [Here's a working example](https://dbfiddle.uk/?rdbms=sqlserver_2017&amp;fiddle=79f6ab21edfbeee2d193e7eb3aa4ec54), scroll down to see plans. You won't really see a difference in execution time there, because fiddle has a timeout, but if you ran this on your own server with no limit on the seed insert to the sales table (resulting in ~500k rows), you'd see a difference (on my puny laptop it's 6s vs 1.5s). In the example case, switching from INNER join to LEFT join was OK - there was an FK between the sales and products and customers tables, we know these rows exist. Sometimes, an INNER join works as a business logic, so you can't just switch to a LEFT join. I wanted to write more about joins, and statistics, and some other stuff but I've been writing this for a couple hours now as I'm busy at work, so I'll just leave it as is.
Yeah, the more you keep reading the more you realize there's a lot more to learn. 
Most often used by me for the following: If Object_ID('tempdb..#TempTable') is not null drop #TempTable
It’s a great website, first came across the author by his videos on Vimeo and on YouTube 
Besides indexing, using a data warehouse design where read operations are optimized can be really useful for Data Analysis if you're not already on one. Table partitioning can also help when the tables get too large. Lastly, tabular cubes and in-memory tables can help if you have sufficient RAM on your server.
Depending on how large or read-intensive your data is, consider looking into data warehouse design. Even if you won't use it here, it sounds like something you might be interested in. 
 order by case when promoted = 1 then 0 else 1 end, created_at desc, Why not `order by promoted desc, created_at desc`? Don't throw extra logic in when it's not necessary - especially if all you're doing is reversing something that the database already knows how to reverse.
Ahh this looks ripe for injection. What company are you writing this for again, Sony? Lol 
Have you considered breaking it into two separate queries and using a UNION?
No I haven't. Would you know what that sort of query would look like? I'm not an expert on SQL. Thank you.
Columnstore tables for the win. In MS SQL, just one index to create and it's amazing for the types of queries that data analysts tend to do (small numbers of columns over millions of rows). Terrible for OLTP, but great for a warehouse that gets batch updates. Because, yes, the order of the columns in a traditional index *is* important, and easy to screw up so that you end up with a tons of indexes that never get used. Otherwise, using whatever tool your DBMS provides to examine execution plans. Your DBMS will pick the indexes for you (yes you can try to force it, but DON'T), and the execution plans will point out what were used and how they helped. Once you get used to reading execution plans, you can often quickly figure out ways to rephrase the query to make it faster. This is particularly valuable for very complex analytic queries that may involve multiple levels of derived tables with a tons of joins.
That nicely shortens this, thank you: IF OBJECT_ID('tempdb.[dbo].[#myquerytable1]') IS NOT NULL DROP TABLE [dbo].[#myquerytable1] 
I did this once with MS SQL Server. I got estimated execution plans for each SQL statement, and parsed the execution plan XML for fields and tables.
A Link from the same page u/MyOpus posted: [https://docs.microsoft.com/en-us/dotnet/framework/data/adonet/sql/ownership-and-user-schema-separation-in-sql-server](https://docs.microsoft.com/en-us/dotnet/framework/data/adonet/sql/ownership-and-user-schema-separation-in-sql-server) While Objects are mentioned, DBO refers to DataBase Owner and the dbo schema which is the default schema for objects and is owned by dbo. Sometimes we all get confused, but in this case dbo. refers to the dbo schema, owned by dbo (database owner user). There are Database Objects, no doubt, but that is irrelevant and confusing in light of OP's question of what is the significance of dbo. &amp;#x200B;
I did this and I am not seeing a configuration ini file I do see a xml file with the settings ,but do config file. Is there a way to use the xml? 
I just tried with and without quotations and same result :/
I decided to just remove all the data conversions and import the file as raw. Then did all of the updates and ETL in SQL. I never could get the Conditional Split to work on the blank row because of the ragged nature of the header. The weird part was I even had to remove the ',' (comma) from the delimiter. After that the data at least went in (all of it) and I was able to parse and clean.
I've reused that code a hundred times, but never thought about the .. before. Interesting!
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/jXRouHh.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20e7j27vs) 
So if I run that command you gave me it does do a config file but with the defaults. Is there a command to do this as a custom install? 
SELECT E.FirstName, E.LastName, COUNT(*) AS OrderCount FROM Employees AS E INNER JOIN Orders AS O ON E.EmployeeID = O.EmployeeID GROUP BY E.FirstName, E.LastName ORDER BY E.FirstName, E.LastName
[https://www.mssqltips.com/sqlservertip/2783/script-to-create-dynamic-pivot-queries-in-sql-server/](https://www.mssqltips.com/sqlservertip/2783/script-to-create-dynamic-pivot-queries-in-sql-server/)
This may help for sql server: https://docs.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-exec-describe-first-result-set-transact-sql
Not sure. I'm no expert, I just did it recently and the config file worked, but it was the full SQL Server version. Maybe try it through a batch file so that you can confirm that it's all in one line? I get the impression that this is an issue with the Express version. Maybe it just doesn't have those features or they are called something else?
As you point out a table valued function is not going to work in this way, because it takes a single record as input. A few options of various difficulty: * CLR Aggregate function * Stored Procedure taking a DataSet variable * dynamic SQL Stored Procedure taking a query string * Storec Procedure which queries a prepopulated #temp table * probably some others
 select create table #t (col1 varchar(10)) insert into #t values (null), (''), ('Hello') select col1 ,case when col1 is null then 'No' when col1 = '' then 'No' else 'Yes' end col2 ,iif(isnull(col1, '') = '', 'No', 'Yes') col3 -- shorter one from #t drop table #t &amp;#x200B;
Can you have a function that calculates (2) values and returns both of them, or would you need to break it into (2) separate functions?
It's just not possible to tell from what he pasted, since he just 'paraphrased' the SQL. If instead he had posted the code showing parameterised queries being used to show its not vulnerable, then the first people would ask is for, is the raw SQL instead of his middleware code anyway
This might help you. https://stackoverflow.com/questions/10404348/sql-server-dynamic-pivot-query
A table value function is a function you can call in place of a table that returns a result set. Off the top of my head the issue here is the way they are attempting to pass a result set titnthe function.
You can pass a table variable as parameters in 2008 and up you just need to define a [user-defined table type](https://docs.microsoft.com/en-us/previous-versions/sql/sql-server-2008-r2/bb522526(v=sql.105)) first. I'm not sure that is the best answer here but it let's you do exactly what you're asking for.
Could you tell me more about CLR types? It seems like you're saying I could select dbo.function(argument) and get 2 columns? Is that correct?
what data type is col1? instead of null you might want to try ```case when col1 = ' '...```, my suggestion is assuming the variable type is a string
CASE WHEN ISNULL(Col1, ‘’) = ‘’ THEN ‘NO’ ELSE’YES’ END
Case When ISNULL(coll, ‘’) = ‘’ Then ‘No’ ELSE ‘Yes’ END I think this would work
You beat me to it
&gt; &gt; I want it to say this: Case when expression is Null, then ‘No’, when expression is blank, then ‘No’ ELSE ‘Yes’ &gt; &gt; I can’t get it to work because of the NULL, SQL gives me an error. I can either do a case with the null or a case with the string. But not with both. Yep, inline-if (IIF) is just syntactic sugar for your statement.
The more I frequent these forums the more I realize how little I know. I thought “BLANK” and NULL was the same thing. That is the absence of a value. And that a single white space is a character that takes up space. I simply don’t understand OP’s question at all. 
Case when Coalesce(col,null,'') = '' then ... Etc.
I would recommend writing it this way in case you have a variable length empty string. ISNULL(LTRIM(RTRIM(columnName)),’’)
I can't speak for Linq, but in standard SQL you need to COUNT(O.employeeId) rather than COUNT(*). Otherwise employees with no orders will have orderCount of 1 instead of the correct 0. 
Good point, thanks. Corrected.
Buy data from social aggregators. 10K/month would cost you almost nothing. Use their schema.
`select int(field) from ..`
Access doesn't have CASE statements.
Isn’t as a function does not work in MS Access. Try: cint(field) 
it worked for me.. `SELECT int("01"), "01"` returns `1,01`
Any software that claims to be able to recover from corruption when SQL Server can't is lying. I don't see any applicable use for this over attempting recovery options through SQL Server itself. If those fail you are going to be reverting to backups.
sorry about that. Could you use the same logic using a switch function?
do you have a timestamp in the table? Or a row number? SELECT MAX(timestamp), AccountNUmber, FROM Table GROUP BY AccountNumber Put that into a #Table, or CTE Or subquery and join the table to that based on timestamp or row# Or, you put the complete orders into a #Table and the voids into a #Table and join them together to see how many were reentered.
Nvl or casewhen
select * from ( select account ,status ,row_number() over(partition by account order by status) as rowNumber from orders ) t where t.rowNumber = 1
SELECT * FROM ( SELECT Account, Status, RANK() OVER (PARTITION BY Account, ORDER BY Status ASC) AS StatusRank FROM Purchases ) AS p WHERE StatusRank = 1;
If the table only has those two fields (or at least if those are the only two fields you need to consider in your query), then this would work: SELECT Account, MIN([Status]) AS MinStatus FROM [tablename] GROUP BY Account
Case when column = 'YES' then "YES" ELSE "NO" end 
Glad I could help. Look into cardinality estimates and how joins affect them. When you have an inner join, depending on the logical operation, the cardinality estimate (the expect number of rows coming out of a logical operation can vary). So let's say you have a source table with 10 million rows and a dict table with 10, and you inner join between them. Depending on a lot of factors and technical stuff I don't have the time to explain now, cardinality estimator can decide there will be only 1 or 10 or 100 or 1000 rows coming out of that query, wherein you know it will be 10 million because that's just a dictionary table. But this means now that the memory grant will be for say 100 rows, instead of 10 million, and it will slow down to a crawl. LEFT JOIN want affect cardinality much because these rows in the other table do not need to exist, so we know all rows from the "left" table will be returned. That's most likely what happened with your query. This can be best battled with indexes and/or statistics (e.g. a bigger/full sample statistic on the column you're joining from and an index on the column you're joining to), but sometimes you're just stack with a plan that's silly and you have to work your way around it. 
If you wanted to use VBA you could use a SELECT CASE statement, but Access SQL doesn't support that. You have to chain together IIF's, which work but it's a pain in the butt, especially when it comes time to make changes. And yet, oddly, Access SQL handles PIVOTS better than MS SQL Server. Go figure.
CLR types are .Net classes serialized into binary. You can store multiple values in the class and expose them as methods. It works a bit like the geography and geometry datatypes.
Whats the triggers name? Is it like "users"?
Without providing us with some DDL, it is difficult to test what you are trying to accomplish, and thus any DML we send back will have to make many assumptions. With the vague question, here’s a vague response: Left join customer orders to inventory and use an isnull function on the inventory quantity to return a zero when not found in the inventory table. 
You could join the table to itself: with voided as ( select account ,case when t1.status = 'Complete' or t2.status = 'Complete' then 'Complete' elif t1.status = 'Void' or t2.status = 'Void' then 'Void' else 'Neither' end as status from t1 inner join t1 as t2 on t2.account = t1.account ) select account ,status from t1 where account not in (select account from voided) union select account ,status from voided v That should get you the whole table minus those rows that are in the CTE, which are those that have a voided label. It then unions to that table with the ruleset: If there is a completed status, then it is completed. Failing that, if there is a void (bearing in mind it will stop at the first case statement if there are any completed), it will return 'void'. Basically that means it'll be void if either or both are void and there are no 'complete.' Failing that, it'll return neither. This table is then unioned to the table minus itself, resulting in only distinct values from the CTE that have your fixed status of "complete" if either order is complete.
A lot of these responses seem very involved, but this below is just joining the table on itself to give you Accounts that have any "Complete" status attached to them. SELECT PO1.* FROM Purchase_Order PO1 INNER JOIN (SELECT ACCOUNT FROM Purchase_Order WHERE STATUS = 'Complete') PO2 ON PO1.ACCOUNT = PO2.ACCOUNT
This is close: DECLARE @s varchar(128) DECLARE @len int SET @s = '123456789 123456789 aaa123456789 123456789 ' SET @len = 30 SELECT -- DEBUG @s as fullstring , LEN(@s) as totallen , CHARINDEX(' ', REVERSE(SUBSTRING(@s,1,@len))) as spindex , @len - CHARINDEX(' ', REVERSE(SUBSTRING(@s,1,@len))) as leftlen , LEN(@s) - @len + CHARINDEX(' ', REVERSE(SUBSTRING(@s,1,@len))) as rightlen -- THE JUICE , LEFT(@s, @len - CHARINDEX(' ', REVERSE(SUBSTRING(@s,1,@len)))) as Name1 , RIGHT(@s, LEN(@s) - @len + CHARINDEX(' ', REVERSE(SUBSTRING(@s,1,@len)))) as Name2
What DBMS? It will depend on that and what kind of auditing is enabled
Ok awesome.. and what in scenarios where an account has just a Void, I need to pull that record. How can I do that when joining the table to itself? 
my God don't get me started on MSSQL Pivots. I've been working on making some legacy hardcoded pivots into dynamic ones, would not recommend.
I'm thinking make it a LEFT JOIN. Then in your select use a CASE WHEN P02.STATUS = 'Complete' THEN P02.STATUS ELSE PO1.STATUS END as STATUS &amp;#x200B;
Sorry for the late response, so instead of an INNER JOIN, which gives us the data that matches, we'll do a LEFT JOIN, so that it keeps all of the original data. Then the WHERE clause is where we'll take out accounts that has any attached "Completes" and leaves us with accounts that only has "Void". And if you want to see all of this in one data set, you just add a union between this query and the other one I posted. SELECT PO1.* FROM Purchase_Order AS PO1 LEFT JOIN (SELECT ACCOUNT FROM Purchase_Order WHERE STATUS = 'Complete') AS PO2 ON PO1.ACCOUNT = PO2.ACCOUNT WHERE PO2.ACCOUNT IS NULL
Right. This is one of the things that I've tried without much luck. The other problem is that even if I could get the test cases to work I don't see how I could make something like that work in the context of a subquery or cross apply.
So here's what I'm trying to accomplish ultimately. select pv.PatientID ,(select Alpha from dbo.cusLM( (select PatientID from PatientVisits where PatientID = pv.PatientID) ,(select VisitMonth from PatientVisits where PatientID = pv.PatientID) ,(select BMI from PatientVisits where PatientID = pv.PatientID) ) as Alpha ,(select Beta from dbo.cusLM( (select PatientID from PatientVisits where PatientID = pv.PatientID) ,(select VisitMonth from PatientVisits where PatientID = pv.PatientID) ,(select BMI from PatientVisits where PatientID = pv.PatientID) ) as Beta from PatientVisits pv returning something like this PatientID --- Alpha --- Beta 001 1 1 002 .5 2 ... I recognize this doesn't work because each of those subqueries would return multiple values of course, so I'm trying to find another approach. 
The CLR aggregate can be called within queries, but it's a can of worms if you've never worked with it before. Otherwise, the stored procedure #temp table solution probably fits most naturally in a script. SELECT stuff INTO #LinearRegressionInput FROM tables; EXEC CalculateLinearRegression; SELECT stuff FROM #LinearRegressionOutput;
I'm not sure why that wouldn't work but can't set up a test to check at the moment. You could try selecting the data you're after in the TVF. create function dbo.cusLM(@LowID INT, @HighID INT) returns table as return WITH some_table(GroupID, x, y) AS ( SELECT PatientID, VisitMonth, BMI from Patient where PatientID BETWEEN @LowID and @HighID ), -- linear regression query /*WITH*/ mean_estimates AS ( SELECT GroupID ,AVG(x * 1.) AS xmean ,AVG(y * 1.) AS ymean FROM some_table GROUP BY GroupID ), ... You'd then just select from it like this passing the range of ID's you're after. select GroupID, alpha, beta from dbo.cusLM(0, 3) It's not reusable for any set of data obviously but if that doesn't work then there's a larger issue. 
Perhaps its just what I'm used to, but personally I think I'd prefer to have something that could be called directly within queries. I mainly work with SQL via R/RStudio, so the select/exec/select process wouldn't really be an improvement over just doing it all in R. can of worms, here I come I guess
If it returns multiple values then you need to address that. I don't understand your data or your tables so it's difficult to visualize what you're doing. Why can't you calculate your distinct alpha per patientid, or do you want all the alpha's for the patientid and then to analyze these to come up with an aggregate, or average, or distribution of them?
No. You still operation DBAs to keep the system running performet. We're going nowhere. 
Did Lego render civil engineering useless? 
That's an interesting approach! I would have to write a separate function for every X/Y combination (for instance if I wanted Weight instead of BMI, or Years instead of months), but it might get the job done, at least for regressions I need most frequently. I'll try this out.
How are you going to get all the data to the format where it is simple enough to drag and drop? How are you going to troubleshoot issues when they come up? Software is definitely going to be important for self service BI but SQL will not die as a profession.
Why would you need to hire someone to develop queries when you can just use the software now? 
Yep it's not ideal but it might work. If I get the chance tomorrow I'll do some testing myself and see if I can't get it working for a table variable. 
&gt;Basically, with this software, there is no need to have SQL developers on the staff - as the software does everything for you. LOL. That will work for lots of reporting. Good luck if you have to do more advanced analytics, models, or ETL work. I **hate** the software you're talking about, because it is largely useless for any kind of serious work. It is just so much easier to do the work in SQL, then point a reporting technology like SSRS, Tableau, etc., to your table/view.
 select account,status from table A where max(case when status = 'COMPLETE' then '1' else '0' end) = (select max(case when status = 'COMPLETE' then '1' else '0' end) from table B where a.account = b.account); I think that will work. 
Well, take ETL and data warehousing as an example. Let's say you have 5 different source systems, and you have reports that require data from all these different sources. You need to get the data into one central data warehouse in order to properly write reports that join it all together. Unless some really robust GUI tools come out in the future, SQL will remain one of the key tools to get the data to the central data warehouse. 
Fair enough. Would you agree that just knowing how to write basic, or even intermediate queries is now obsolete then? Being very advanced in SQL IE at the MCSE level will net you a job. 
First, writing SQL extends outside of the BI sphere, as any transactional application with a rdbms is going to have SQL queries run on it. As long as applications are using a rdbms as a backend, there is SQL involved. ORM can come into play for smaller apps, but there more complicated the query, the more likely someone is going to be writing SQL. Second, even in BI, SQL is a fundamental tool to have. If you have a we'll set up data warehouse, you can do a lot with BI tools without it, but it's just a matter of time that some exec asks a question that isn't simple to answer, and being able to write SQL will come in handy in those scenarios. Basically, SQL is used in many different areas, not just BI, and while it shouldn't be the only tool you have, it is still considered a standard and will be for a while. If you know SQL, it's a good starting point, but there's always more to learn.
From a pure reporting perspective, I would agree to an extent. But once you start using some of these tools, you will see how difficult it really is to do any sort of true manipulations. Like pivoting data, etc. I don't think SQL jobs will suffer that much, but I do think that business analysts will be expected to be more technical.
The post mostly refers to query development - ie a Business Analyst role that writes basic queries for data analysis. BI software programs such as these render that role obsolete. 
You keep repeating yourself like you weren't actually expecting an answer to your question, but rather are telling us that SQL is dead and we should all pack our bags and go home. Again, these basic queries are great if you have a well structured data warehouse and/or extremely simple transactional tables. If there is any complexity to your data what-so-ever, the tools to generate these queries fall apart **quickly**. I manage both Domo and Microstrategy platforms with a 5000+ user base... you can try and tell me I'm wrong all day, but the statement you're repeating has not held any truth in my organization.
You're vastly overestimating the competency of a workforce. Part of an analysts job is to analyse, not just pull the raw data, I'm unaware of any job that solely pulls data.
Are you including UID in your query before the pivot? If not, then you'd get one row.
SQL syntax starts out simple. Almost anybody could learn the words ("select", "from", "where", "and", "or", etc.). It's the logic that is challenging and the drag-and-drop programs don't help much. What if you want all the sales in Maine and Oregon, including Portland, Oregon but excluding Portland, Maine, except that you should include the Portland, Maine sales of one particular product in the month of June? That's going to defeat some of these SQL assistant programs. You need an analyst just to understand the question, much less to program it in SQL.
Because once the architecture work is done on the back end of the BI platform and/or the data warehouse tables are created to support a report, the BI platforms allow for thousands of users to navigate through a set of data. You're talking apples and oranges here - the general drag/drop of random data from multiple tables - "Build your own report" functionality tends to fall apart very quickly unless your users have a very good understanding of the data. We, and most organizations that I've seen, use BI platforms to provide users access to carefully curated and architected reports rather than carte blanche roaming of everything data.
If I'm understanding you, you have a database that stores passwords in plain text, and the app front end also has SQL Injection vulnerabilities? If my understanding is correct, then there is nothing you can do to secure it other than turn it off.
I don't know about other systems, but if this is SQL Server the drop, and the user/app will be logged in the default trace. Depends how long ago it happened whether or not it's still in the retention period of the trace.
Works well! This is a good short-term solution for some of the more common regressions I tend to run. Thanks!
It’s called self-serve BI. While the presentation layer is now in the hands on the average user, the whole preparation of this data still needs to be done so that it’s performant.
The day NULL started to make sense to a friend is the day he heard NULL meant "unknown" In your case, a blank string (empty string) or space is a known value, thus is not NULL. This is why any numeric operation with a NULL have NULL as a result. 
So would you confidently state that obtaining the MCSA 70-761 and 70-762 certification is a good move for the future? I’m currently studying for the 70-761 as I enjoy working with data, but id rather quickly change course if employment opportunities will be scarce. 
Sure, the platform is far inferior to a Database administrator. But it can replace a Business Analyst that was hired to write simple to intermediate queries on the DB for analysis in a heartbeat. 
So, basically, you've got a database vulnerable to SQL Injection but you can't make any changes to the architecture? &amp;#x200B; When it comes to security, you have to implement the entire suite of things for it to be useful. You could hash the passwords but that would require an architecture change on the front end to allow users access to accounts. &amp;#x200B; You're at an impasse until you can employ a two/three-step security solution.
I can't personally speak to any of those certifications, as no one I know holds them.
Seriously man, multiple people in this thread who are actually working in BI professions have given you real, contradictory evidence, yet you continue to repeat the exact same, incorrect, statement like you've just been brainwashed by a sales conference.
This is the simplest solution that meets OP's requirement.
TL;DR . . . . no
https://giphy.com/gifs/friends-2OP9jbHFlFPW
Ok, you need to include the UID, as I said above, to get your desired output.
Whoa, I missed that part I am sorry, I added it and it worked like a CHAMP, thank you very much!
Is this what you are looking for? select UID,Name,Address,City,State,color from ( select UID,ANSWER,QUESTION from UserData )d pivot ( Max(answer) for QUESTION in (Name,Address,City,State,color) )piv;
Oh man, I totally missed your response. Oops.
Use SSRS unless there’s a reason it has to be a table.
I was wondering where this was going. Brilliant. 
I will have to look up tabular cubes and in memory tables. Thank you for the great info
This is great info thank you very much. I will try switching some inner joins to left joins to speed up my query execution times.
I just checked this website out and it's like the bible of indexing. Great resource thank you very much.
do you know how to do this with excel?
Thanks!
SQL is a declarative language. This means you tell it what you want, and the engine in most cases will choose how to get the data for you. There are some caveats but I won't get into that. The engine interprets your SQL code and generates an execution plan. This plan is exactly how the engine retrieves data. being able to see the execution plan helps you to understand where your bottlenecks are and how you can improve your query. In most cases, the first thing people look for whether or not the execution plan utilized available indexes or not since indexes typically have the most impact on query performance.
Thank you very much. 
It's just sad when it returns something
This is great! Just learning SQL now and the explanations really help
Lets be honest here, at the point AI is good enough to interpret user intention then it'll be advanced enough to replace the user. Most of business intelligence isn't about writing SQL it's about understanding the underlying data and predicting what someone wants. 95% of the job is acting as an interface for people who don't need (or want) to understand the raw data but need to know what the data means to make decisions. The other 5% is knowing how to write SQL. TL;DR - most of business intelligence isn't writing SQL it's interpreting the data for non-experts.
I work in a school district and have had to teach myself SQL from the ground up, I am constantly amazed by the simple solutions to super complex problems. I have not run into a problem that I have been unable to resolve with SQL yet. My new favorite is to use a nested select statement but only return 1 column of data, you can couple it with a NOT IN or IN to basically turn it into an array which is so useful.
Explain plan is correct. https://stackoverflow.com/questions/30070910/how-do-i-view-the-explain-plan-in-oracle-sql-developer First answer shows how to view in SQL Developer. Toad also has the same visual feature.
That is cool. Good post.
This is very useful. Thank you very much.
You add the NewId int column as an identity column. Then you'll have to drop the clustered index/primary key on your Id column. Optional, rename NewId. Then create a new clustered index listing NewId as the primary key. This can take a long time depending on the size of the table! If there is a clustered index, dropping and recreating it causes the table to be sorted and reorganized on disk twice. 
I actually just started a blog and my first post is about this! [https://wordpress.com/stats/day/datastrides.wordpress.com](https://wordpress.com/stats/day/datastrides.wordpress.com)
Link wants me to sign in. I don't think that's the right link?
[I think this is what you're looking for.](https://www.youtube.com/watch?v=dQw4w9WgXcQ)
not op, but try [https://datastrides.wordpress.com/](https://datastrides.wordpress.com/)
I used to use that as an interview question for people claiming to know SQL
Yet, you still need to go the whole "ROW_NUMBER() OVER (..." way to delete all except one of the duplicates, right? Unless you select into a new table that is... Or am I overlooking a commonly used method here? It would be convenient if MSSQL had ROWID pseudonyms like Oracle has...
Still keeping your nested select, try to convert that WHERE IN or WHERE NOT IN to an INNER JOIN or LEFT JOIN WHERE the left table row's ID is null. I think it's optimized better.
Disable that trigger and script it against the new table. I assume your not destroying any data, so assuming you can get a clear window, alter table disable constraint all, move your data, rename tables, and reenable all constraints. You might have to play with it a little tti get it to work.
hey its me, ur unique key constraint
I'll try to do it but it seems veeery complicated :( 
Thanks! This is it...
I used to teach prep classes for the 2012 platform. One of the first things I did was reassure my students that they made a good choice by pursuing this route for their career. We are now living in a time where companies are hosting more data than ever, leaning on it for data driven decisions. Often, students would have doubts, especially those who are just beginning in this space. I would say stay the course, if you actually want to develop on SQL Server. But, be passionate about it. If you don't love what you do, the next 25 years will be difficult. I, personally, am a operational DBA in the enterprise space with more than 25 years of professional experience. I personally believe that ops style work is the long game, and will make more money. To support my argument, I oft point out that in the SDLC, maintenance is the longest part. And this is where ops DBAs come in. Of course I am biased. So take this advice with a pinch of salt. 
please confirm... in october 2018 you want fiscal year 2019, right?
SONNOVA!!!
Start looking into [window functions](https://community.modeanalytics.com/sql/tutorial/sql-window-functions/). Even a more neat trick once you get the hang of it. Personally I also got confused and mixed up and annoyed working with group bys - especially in bigger queries. Window functions remove the hassle of grouping - letting you selectively pick aggregates. Also allows super easy rankings of the duplicates with ROW_NUMBER(). I feel confident saying window functions were the biggest jump in my effectiveness with SQL. Good luck!
Just busted out loud laughing, thanks for a great ending to the week.
You're welcome. Now if you'll excuse me, I've got some integers to de-concatenate before I can go home.
I think it can depend on the situation. But a row number isnt a bad way to get around it. Here is a post with a few other ideas: https://stackoverflow.com/questions/18390574/how-to-delete-duplicate-rows-in-sql-server Also, a lot of times in a production system the duplicates will be based around a certain event or mistake that can make targeting them easier. For instance, all duplicates were inserted from the same file, but the file loaded once, and then again. So all 'good' rows are in ID range 100-149, but the duplicates are in rows 150-199. So you can just run a delete against that ID range.
I'm not too bad with window functions. I can use over, partition by, order by, withing group, dense rank, rank, lead, lag, first value, last value and listagg. I can also use the row and range with preceding and following. I'm stuck however understanding using range with between/and. I just can't wrap my head around how it works yet. Will keep practicing at home until I get it.
Backup/restores, create users, views/triggers. Probably a select top n query.
Some basics: SELECT, FROM, JOIN - This is pretty crucial. You should be able to traverse at least 3 tables (if you're tested for it). This is the skill you'll use the most in most SQL jobs UNION, UPDATE, ALTER - These are extra tools that are important to. It's important to be able to manage your database, merge queries and manage data in tables. &amp;#x200B; INFORMATION\_SCHEMA, Engine specifics, connecting to databases - These are administrative tools that are valuable in any job. Basically, your goal should be to demonstrate competency then go above and beyond to demonstrate data-specific knowledge (if applicable). All of the timed tests I've taken have been pretty straightforward: SELECT from this\_table JOIN that\_table ON keypair WHERE \[condition\] GROUP BY \[col\] HAVING \[value condition\]. You're demonstrating your ability to turn verbal query into a SQL query, basically. 
That's about what I thought. My only worry is the fact that i'll also be tested on basic programming. What the hell do they even mean by that? I've only ever studied specific languages on my own.
We do code tests at my job for anything over entry level. We expect the tier 1s to learn on the job. But if you want to move into a higher tier or VIP support, they give us a 5-10 question general knowledge test. It helps that we aren't expected to do anything other than SELECT statements. Other teams have write access, so the reqs are much higher for them.
I agree - that's confusing on their part. Did the job posting list any specific languages? If not, it seems like a weird thing to test, unless it was really broad concepts like if/else, for loops, etc that weren't language specific.
If you move the data to SQL and use Access as the front end, then your data is less vulnerable to loss, at least to some of the problems Access presents with large data files. 
how's your backups of that 1.3Gig file?
I us it to compare thousands of record's normally personIDs not something that I would want to write out by hand and usually I need to be dynamic like for error reporting.
Typically, you move to SQL Server when you have a system that requires multiple people / transactions running concurrently. Access does not scale well to workloads where there is more than one person active at a time. &amp;#x200B; The question that the business needs to answer is "How much of the profit is tied to a few access files?" If it's some or a lot, then I would recommend that they look at some type of enterprise relational system. It's often like finding whole departments that run their daily operations on excel spreadsheets that they've shared from their Laptop.
We backup daily to a local NAS.
It's entirely dependent on how much data goes through that database. One of the issues we have where I work is that Access databases do not truncate themselves and they have a hard limit of 2 gigs once you hit that everything that uses it dies. A while back this was happening on a near weekly basis at some of our stores so we migrated to a sort of hybrid mode, the majority of tables are MS SQL 2014 based and are setup as linked tables. which the more dynamic tables remain in access, we still have to regularly go in to compact and repair (which you should consider doing to see if that 1.3g database will go down) but it's once every couple of months instead of once every other week. Once issue you will encounter however is field differences and SQL code difference. Access uses -1/0 for bit fields for instance and #01/02/2018# dates in code. Text fields in access also don't care if you do **where mytextfield = 'Bob'**, MS SQL will bitch at you and make you use **where mytextfield LIKE 'Bob'** Linked tables also tend to misbehave if you do things like SELECT A ,(Select count(C) from X WHERE X.t = Z.w group by Y ) ,B FROM Z The query will either take 5 hours or it will just shit itself. So if you don't have access to the source of the software you may be in for some issues. So we had to modify a lot of our programs to talk straight to MS SQL and only our main VB6 executable now uses the access tables. At least until we get the green-light for a full .NET rewrite. So, is it doable, yes. Will you run into issues, probably. Should you at least start off with a compact/repair, most definitely.
“...Just a chance of loss..” is huge. In my opinion, moving to a backend SQL and creating an intranet has its benefits. I’ve found it easier to implement multiple users and create reports and query the DB. Then again, I am a SQL user and I am sure a heavy Access person will say ‘No way man, Access can do all that.’ Just my experience.
It's faster for everyone though. We can see the candidates who can't solve SQL problems and it removes 80% of the candidates without having to schedule calls.
No, the only language they mentioned was SQL. That's about it
I've had this a few times. Strikes me as so odd, to physically write down a query... Takes much longer, lots of scratch
I should note too, that when they review, they usually spend 2 seconds staring at it and say you passed. That's healthcare though... 
Yes. :)
First I would make both the RoleID in Roles and the userID in Users be Unique and Auto Generated. So you would only need to do INSERT INTO Roles (RoleName) VALUES ('Administrator'); INSERT INTO Users (FirstName,LastName,RoleID) VALUES ('John','Cena',1); Now if you know what your role names are but you don't want to remember what the ID is you could do. INSERT INTO Users (FirstName,LastName,RoleID) VALUES ( 'John' ,'Cena' ,(SELECT RoleId FROM Roles where RoleName = 'Administrator');
You either need to change the database from master to your database(top left, see how it says master) or call the tables with yourdatabasename.dbo.tablename
may need to use the `USE` statement first, to select the database
Yes, I agree -- but selling the idea to people who will say they haven't experienced any loss so far isn't always easy. Giving them things like "It's going to be faster" makes the idea more appealing, if you understand what I mean.
No one has reported any corrupted files to me, so I don't have a response.
What is that section for?
which section?
oh yes ofcourse the id are unique and auto generated. But that's good so it inserting data or deleting does not change even if there is a foreign key column. So, the foreign key column only helps in JOINS and relations also..
&gt;Should I build into the ETL the creation of indexes on my big tables? Performance is good until multiple large table scans happen in the joins, amongst other things. Yes, obviously it would be best to test out if creating the index prior to inserting the data is more efficient than after, though. Start with a few indexes that have the biggest impact (joins, large scans that select few rows but run frequently). &gt;Yes I know i should be using SSIS instead of agent jobs This would probably speed things up a lot since if you can load them in parallel... if the tables do not rely on each other and you're currently running them in serial in the TSQL jobs, I'd start by converting it to an SSIS job running in parallel.
If you know what your are doing it probably takes like 45 mins to do.
That master section you mentioned.
Yes, but unfortunately I didn't know enough about databases to know if it was a real risk or not, which was why I came here to find out!
Join's Store procedures 
the box is there so people can easily switch from one database to another without needing to rewrite anything. Like if they have a testDatabase and RealDatabase that are the same but one is use to do test so you don't fuck up live data. you can pick testDatabase, write code, test the code and if it does what you want it to do, just pick RealDatabase and run the code again to apply it to the live one. By default the selected database will always be *master*. unless you do one of the things I mentioned in my first response.
Thank you so much!
probably just general programming knowledge like basic data structures, loops, if/switch statements, try/catch, basic vbscripting/batch scripting etc. It's possible they have some dynamic sql or use an environment like SAS where a little experience with both would be helpful. If you don't know what a variable is or can't get through something like basic VBA or modify a batch script then they probably are looking for someone more rounded
https://www.oracle.com/webfolder/technetwork/tutorials/obe/db/sqldev/r30/SQLdev3.0_Import_Export/sqldev3.0_import_export.htm
It might get you more job interviews, if the company is looking for someone with that particular cert. If you're already in the field, probably not. If you've never been paid to do SQL stuff, then it can't hurt. A lot of certs are product-specific. So you'll have to choose what kind of cert you want. If you're not choosing a platform and targeting something specific, you may want to choose something general that is not specific to any one vendor. 
Frankly I doubt this. I have interviewed for countless positions that required a degree, required a certificate, etc., and ultimately none of it matters. This is most especially true if you are working with a recruiter. What matters is your work experience. If you don't have work experience then get certifications. Having both experience and certifications is a good thing, but it ultimately isn't going to get you a job over someone else. That is going to come down to how well you interview.
You need to clarify your question, brother, because I have no idea what you're asking. Maybe post the code and explain what you want to do?
I don't have any certs... never held me back, and I don't plan on getting any. But op is entering the field as a math major. It's more related than being, say, a political science major, but it's the classic catch-22 of not being able to get hired because you don't have experience, and not getting experience because you can't get hired. Having a cert would at least prove that op can write some sql and pass a test. Gotta start somewhere!
I wish I could give you the questions lol 
I see, I have no relevant experience (work or otherwise) in SQL so at least a cert would look better? I was considering a programming bootcamp too (not just for SQL but learning Python, Javascript, CSS, HTML so just getting a foot start with a host of languages) but it would cost tenfold that of CC and doesn't offer anything something like a certificate. But it looks like it would be more reasonable to do the cert in SQL (with my lack of work experience) and study other languages on the side.
Thanks for your responses. I do have to start somewhere and my lack of experience hurts me, so I think I may do the cert path. However, it wouldn't start until January of next year since the fall semester started some time ago. Quite a while away, I guess I can at least start self-teaching and learning other languages.
I'm a political science major! :)
If the tables have no indices *at all*, then inevitably most queries are going to perform poorly. But due to the sort of queries you are running, would adding a comprehensive set of indices turn your reporting system suddenly convert your reporting system to high performance?... well it's very difficult to say, but probably not. I would run SQL profiler and start capturing a workload of the sort of reports people are running on a day to day basis. There will no doubt be some obscure ones where indices cannot help much, but typically there will be some routine ones which are run often, and you can focus on optimising those (either via indices or trying to optimise the queries). Either way you should probably be doing some sort of index maintenance after repopulating all the tables, and you could also consider pre-populating the cache by running some common queries after your import completes, so that the first reports aren't just having to hit slow disk to get the data. 72GB is a nice amount of RAM, so you may aswell take advantage of it in offpeak hours 
Why do both though, what's the advantage of having people do testing off site? And how do you know that people who did tests offsite didn't cheat?
I disagree with your "probably not" statement. I don't think I've ever seen a system that has to do full table scans for every query perform better than one with at least a clustered index on a column included in a join or filter clause. Profiler, for a newb, is probably overkill. Check plan cache and you can get queries and the plans in your cache - way easier to figure out query patterns. https://www.sqlshack.com/understanding-sql-server-query-plan-cache/
Thank you!
That way we can rule out a lot of candidates without having to schedule calls with the team. They have to install docker, extract a tar.gz and then write queries that run in the docker instance - none of it is simple to cheat if you really know nothing. The onsite questions are more casual and simpler but still test the knowledge. I much prefer this to open ended assessments (find some stuff in our data) that can take an unlimited amount of time, or just qualitative questions that can be bullshitted. 
How is that not simple to cheap if there is another person who knows what they're doing? This makes no sense to me.
Ah ha, caught you! The jig is up, turn in your sql license! Bake em away, toys!
Which RDBMS?
microsoft sql server
Works!
QuickSQL.oracle.com Basically just follow the directions on how to make a table, but use the /insert ### command. It will generate insert statements with random data. It’s been great for me to make test data lately.
The table i mentioned above plus one that is basically a dump containing a field to join to on the code. I plan to normalize the data down the line, but there's no way i can with the time allotted before the demo. 
Show us what you've tried first. Otherwise, do your own homework
The someone else who mentioned sargable was me ;) I've been working in SQL Server since 1999 and in my opinion Profiler, especially to someone not very experienced, is like turning on a fire hydrant to get a drink of water. Great usage if you know what you're looking for but on an active reporting system you'd have to wade through a ton to figure out where your pain points may be, but yes - it's a good tool for a specific purpose. I find that most of what I need is held in the plan cache. I've worked specifically in reporting systems, and ETL, for a good majority of that time, so I was just throwing out my opinions on good focus points for starting out. Good to hear your points though. Glad to see so many trying to help others in here. Have a good weekend! 
The fact you as the dev are unable to clarify when talking to other devs means THEY need to goddamn clarify. They can't just say "Oh we need it to borgaslorch the piddlefuddle" and expect you to figure out what the fuck they actually want. Also without sample data how are you supposed to test or even demo?
Okay cool! Thank you!
Like the other person said, it depends on what SQL you're using. This could be an option for MS SQL, but you would need to change the date format according to your needs, within a string. Then concatenate the calculated year onto that string to complete the date '01-04-' + DATEPART(yy, GETDATE()), start tax year '31-3-' + (1 + DATEPART(yy, GETDATE())), end tax year
When i get next to my computer in the morning, I'll mock up some sample data. The crazy thing is I'm not the dev; I'm the dba. But I'm neglecting that duty in favor of development due to the tight deadline. I'm surprised I'm being downvoted. I'm trying to make chocolate milk out of shit right now. 
I'll mock up the tables in the morning. 
&gt; Bake em away, toys! he's going to be delicious soon enough
Hey - see below: SELECT data.ID, data.[Week of], data.Amount FROM data INNER JOIN weeks ON data.ID = weeks.ID GROUP BY data.ID, data.[Week of], data.Amount HAVING count(data.[Week of])&lt;=weeks.[weeks to return] ORDER BY data.ID, data.[Week of] DESC; It gives me an error: https://i.imgur.com/XjvAOsE.png I guess my HAVING statement is wrong (I know nothing about SQL, trying to learn by myself). What I'm trying to accomplish with the having statement is to make it count how many weeks each ID has, compare it to the count specified in the 'weeks' table, and return only the specified number of rows in the result.
Hi Dan\_au, you can give it a try once. [http://www.data-recovery-solutions.com/sql-viewer.php](http://www.data-recovery-solutions.com/sql-viewer.php)
&gt; The crazy thing is I'm not the dev; I'm the dba. You're splitting hairs. You're the developer for this exercise. If you can't explain why you're trying to solve, you don't understand it well enough to solve properly. If we're "being given the same requirements" you are, you need to go back and get better requirements.
Everyone knows real experience trumps all. The question is how does someone with no experience get into a role so they can gain experience? Does a cert help? 
They don't care. Testing is cheap, there's no tangible limit and what they can throw at their applicants.
Sure but people still do it
There are millions of developers of SQL in the world, and there are not so many developers for Solidity . The choice of Aergo in favor of SQL will greatly simplify the search for specialists and lead to changes in the blockchain world.
Yah, more developers, more progress, more adoption. Also A complete Open source structure so that unlike other blockchains, aergo doesn't need hardforks but more focus is laid on development and growth.
Its a little hard to tell from you unformatted code (put four spaces infront of each line to make reddit understand it). However, it seems like you're trying to do a pivot on the data, the format for which varies from database to database.
A pivot statement might be what you need
Are you using Oracle SQL Developer or SQL Server Developer Edition. I don't know much about Oracle, but if you are using SQL Server, the table list doesn't refresh automatically. You have to manually refresh the list before you'll see any new tables you've imported.
Oh, I see. So, no not directly. You can either do one case statement per column: select transactionNo ,code ,case when [conditions] end as PayGroup ,case when [conditions] end as PayType Or create a lookup table with what your codes are and inner join to it: select t1.transactionNo ,t1.code ,t2.PayGroup ,t2.PayType from t1 inner join lu_table t2 on t2.code = t1.code And then your lu_table would have a primary key (code), and its related types (PayGroup, PayType, etc.)
Can you test out an excel file you make yourself with like 1-5 lines of data? If you can do it with something you make, then its probably something wrong with the file. If it doesn't import, then its something wrong with your procedure.
I'm using Oracle SQL Developer and I have been refreshing the table manually but it doesn't show. For some reason SQL immediately drops the table after it's imported.
Just tested out another excel file and it worked fine. Maybe I have an issue with the excel file which I downloaded as a dataset from a website.
That woudl be my guess. Can you load in Excel and export as a CSV? That should eliminate any problems with the format conversion, but might still have issues if you have malformed data.
exporing as CSV did the trick. Appreciate your help
I got this working, I pretty much had to hard code the configuration file from scratch and run the exe with the config file argument in the command. 
You have the students and what enrollments they have. Join that to a count grouping of sections (by section ID) with 100-level courses. 
You could do select top 1 and order by the count descending.
I don’t do tests, but I used to think be ashamed of asking basic technical questions to candidates until I got burned by hiring some really technically incompetent people. And these weren’t entry level people either. Now I have no shame in asking. A few of my SQL questions are: What’s the difference between a left join and an inner join? What’s the difference between where and having? Give me an example of when you had to use a cursor?
Ignoring the outer select (as in remove it) , you can leave the FROM, WHERE and GROUP BY the same. Add an ORDER BY COUNT(blah) DESC and amend the SELECT to SELECT TOP (1) Student_Id
I think you ought to use a single quote (') over a double quote ("). SELECT * FROM Customers WHERE [Last Name]='Vázquez'; However, no idea what DB you are using. But give that a try first.
Nothing in your description relates a farmer to livestock or crops (and thus their ROI).
You might also lean the data science and data wrangling aspects of Python, as that is becoming, or has by now surpassed, R in data science.
For me it was being comfortable with raw data, I had a good background in SQL to start with and I utilized that plus raw data analysis plus good analytical skills all rounded off by good communication and presenting skills. Then there are the visualization tools such as SSRS, Tableau and power BI that you should know at least one of. Others will have differing views but if I could only pick one tech skill to open the door to data analysis it would be SQL.
My favorite is probably the StackOverflow database. It's a large data set so it really let's you practice your query tuning.
Various cities have data portals for civic data (crime, real estate maps, valuation(?), Tax Increment Financing districts., etc.,) Chicago Data Portal: [https://data.cityofchicago.org/](https://data.cityofchicago.org/)
With your background, you should already be qualified to become a data analyst. You won't need SQL certification but you should definitely know SQL. Just go and apply, there are a lot of openings. Once you do that for awhile, you might consider looking into business intelligence if you like the data visualization aspect of things. If you want something more academic, you can start moving toward data science. 
I would love to query Spotify for my listen stats!
Managed to work it out! I was expecting to simply get all three records with that last name. The issue was more on the professor’s side since he wrote a typo on the instructions to follow.
The way to solve this is to think of a way to perform the aggregations in one pass- have a look and see if you can consolidate the where statements in your sub queries into a case statement and go from there
This is how I would have wrote the query. Well done.
Are you asking how to create the tables? &amp;#x200B; It sounds to me you want to create and Entity table and a Car table. The Car column in Entity 1 would be a foreign key referencing some unique Car ID In a Car table. Then within that Car table you can have model, year, etc. tied to each unique Car ID.
Access has a 2GB hard limit on file size. When your database reaches that point, it'll stop working. Stability issues start creeping up before you hit that limit, however. A move to SQL Server will give you: * Better performance * The option for point-in-time recovery * Separation of data, UI and logic * Improved security and protection from accidental data deletion * A data storage engine that's designed for concurrency, robustness, safety, and protection from corruption As things stand right now, your users are one bad keystroke away from losing the whole database. All it takes is deleting the file from the network share. If properly set up, that isn't possible with SQL Server. But when you perform your migration, you can't just do a one-for-one copy of everything. Your Access DB is probably in need of better data types, foreign key constraints, and other features that users of modern, mature database platforms take for granted. Take the opportunity to make the database better. Make a copy of the Access database, install SQL Server Express Edition somewhere, and experiment with making the move. Prove that it'll work and demonstrate the value.
Use row_number() to sort the AB table and the C table by whatever column it is that relates A and C. Label that column Sort1. Add a second column that has a 1 hard-coded for the AB table and a 2 hard-coded for the C table and call it Sort2. Union AB and C, order the results by Sort1, Sort2. 
Thanks. This may be the way to go. One additional challenge I learned is that one of C's null columns may actually need to contain A's ID, since C is nested / grouped by A.
Makes sense. BETWEEN seems cleaner too. Thanks!
Echoing the first comment and throwing on an additional layer, you don't mention what flavour of SQL your DB is running?, I don't really understand what you're looking to do, you can sort of normalise your data using a cross apply to turn columns into rows as such: `DECLARE @table table ( [timestamp] datetime, [temp@2m_mean] float, [temp@2m_stdev] float, [temp@10m_mean] float, [temp@10m_stdev] float)` `INSERT INTO @table` `VALUES` `('1/1/2018 00:00', 15.23, 0.12, 15.37, 0.10),` `('1/1/2018 00:01', 15.25, 0.11, 15.38, 0.10)` `SELECT [timestamp]` `, [instrument_id]` `, [mean]` `, [stdev]` `FROM @table temp` `CROSS APPLY (` `VALUES ('2m', [temp@2m_mean], [temp@2m_stdev]),` `('10m', [temp@10m_mean], [temp@10m_stdev])` `) CrossApplied ([instrument_id], [mean], [stdev])` &amp;#x200B; You can do as many columns as you wanted in the cross apply, you could even get very fancy and use dynamic sql to convert the column names to variable names for you. I have to do this regularly in a poorly designed reporting database to get data out in a useful format for SSRS... no table should have 800+ columns. &amp;#x200B;
Hi, in October I would want the 2018/19 tax year, which would be 01/04/2018 - 31/03/2019 (for reporting purposes, its actually 06/04/2018 - 05/04/2019 in the UK) So the 1st April and 31st March date would be fixed and the year would amend depending on current date
Have you restarted the Postgres service (on Windows)?
Unless they are going for a DBA role they should not touch backups/restores/SQL users. Views are just SQL code so not sure what there is to learn about them and triggers are nearly always bad practice. The only time a trigger would feature in a test I would have in an interview is a trick question where it looks like it could be done with a trigger but another way exists. I would fail those who used a trigger over another mechanism because in every instance I have ever seen them used a better method existed that the dev did not think of or could not be bothered doing. I have yet to see a legit usage for a trigger in the real world and I have seen a lot of triggers. &gt;Probably a select top n query Right click select top 1000 on a table? Like how to you test competency for that? Time the task? Check to see if they misclick? I'd expect its more syntax (joins, where clauses etc) and a bit of theory (ACID compliance, order of execution) rather than stuff like backups.
Well they do [have an API](https://developer.spotify.com) that provides some listening data, but doesn’t seem to provide an actual play-by-play listening list. For that, going forwards at least, I’d suggest turning on scrobbling to LastFM, which definitely lets you get at that data. I’ve never done it, but chances are their GDPR data access request tool gives you that data as well. 
Personally I’d use Faker to generate legitimate random data of exactly the type your application uses if that’s what you’re concerned about. I mean joining tables on a small int is different than doing it on an arbitrary text string, etc. so every scenario is going to be different.
Thanks. I'll check that out. At least that way I can see if my application will work with dummy data.
Please be careful with `BETWEEN`, especially when working with `datetime` types. You may get more than you bargained for. https://sqlblog.org/2011/10/19/what-do-between-and-the-devil-have-in-common
Not sure if sarcasm or... Most apps are surprisingly boring when it comes to the data they store. User data, customer data, employee data, order data, blah blah blah. Particularly with LINQ it’s *really* easy to screw up a query and end up enumerating the entire table - something that’s fine with the 10 fake customers you’ve added and then suddenly drops off a plateau when your UI guys spin up some front end tests that fake the process in a loop and you’ve suddenly got 10 million to query over. Depending upon exactly what you’re storing, how you’re storing it, how you’re querying it, etc. it’s also valuable to have truly random data to test with. Even things like index distribution and partitioning can easily seem like a non-issue if you load up a real dataset because it wasn’t actually testing what you thought it would... though of course that’s a valid test in and of itself, just of different aspects. 
Well the reason I would be doing it is because I want to make an app and don't want to sit for hours coming up with dummy data just to see if it works. My app would have to communicate with a database so that would be my intention for the data.
Well then I definitely recommend Faker. In the time it’d take to find an appropriate dataset, download it, parse it, and shove it into your app you can fake exactly what you need. Very useful. 
Thank you for this, I guess a little digging on my part could have uncovered this. 
You’re quite welcome. Definitely turn on scrobbling if it’s something you really want to do ongoing, it’s soooo much easier. 
Sorry about that. I can't post exact code because of company policy, but here's the gist of the existing code that already exists. &amp;#x200B; Select insertdate, source, etc, cast(myXML as XML) myXML2 tbl.col.value('citycode','varchar(4)') as city from myXMLTable XT Cross apply XT.myXML2.nodes(//\*.segemts/) Outer apply XT.myXML2.nodes(//\*.Sessions/) tb(co) &amp;#x200B; So what I'm tryng to do is that over the next year, I'm going to be adding numerous fields to this node. However, I don't want to constantly be altering my query and the table that it inserts into. Instead, I'd like everything under a certain node to be added to a table as one field per row. Is there any way of doing that dynamically? &amp;#x200B;
Can you explain why the 'userA' column isn't sufficient? It sounds to me like stripping the @x_y_z will give you userA, so you already have the information you want?
Maybe something like.... Cursor the employees Inside cursor -, look up region description -, get region manager where region description Is that what you are asking for? 
In many ways, yes. However, in my case, I was hoping to make it all work within a self contained query using Query Manager instead of creating an SQR for it. If I can make it work within the query itself, this will enable me to update multiple queries we have with the region manager. But functionally yes, that is exactly what I'm looking to do, but within the constraints of query manager.
Can you provide some sample data and what version of SQL you're using? &gt;This would all be happening on one table - no joins. You can join a table to itself... chances are that's all you need to do: SELECT e.EmployeeName, e2.EmployeeName as Manager FROM Employees e INNER JOIN Employees e2 on e.RegionDescription = e2.DepartmentName
could you also please confirm your platform? "sysdate" sounds like Oracle 
You need to group by your case statement. Is that what you're saying? 
You'll more than likely need to wrap the entire `CASE` statement in the `SUM` function. ... SUM( CASE WHEN ... THEN ... ELSE ... ) as Column
I haven’t used PostgreSQL that much but a quick search reveals you might want to check out POSITION and LEFT. Use POSITION to figure out how many characters in is “@“. Take this result, subtract 1, and the resulting number is the number you want to use within LEFT
OP, use the `split_part()` function. [string function documentation](https://www.postgresql.org/docs/9.1/static/functions-string.html) for example: split_part('userA@x_y_z', '@', 1)
Oh I see, that makes more sense.
Honestly, total brain freeze on my part. Because I'm joining based on a description (not optimal, obviously), and because I was limiting my query set to only a few people, I actually just discovered there is a typo in the specific region I was using as my test subject. Basically yes, doing an inner join on the description (as you have above) is the solution. But now I'm left with a large subset of people who are dropping from the results because there isn't a match. Of course, no problem. I'm just going to create a union and a "does not exist" sub query. I'll put an expression along the lines of "region not matching" or something, and I can use that to start cleaning up the data. Point is - thanks! You were exactly correctly. ;)
This works perfectly. Thank you.
 , '$' + CAST [field as varchar] AS MyField
 where open_date between getdate()-180 and getdate()-90 This assumes that 6 months is 180 days long, and 3 months is 90 days long. If you want to work with whole months you will need to use a `DATEADD()`.
`CONCAT('$', CAST([field] as varchar))` doesn't work? Why do you want to use the function?
I was hoping to get it by whole months. Again, I am very new to SQL (2weeks) so bare with me here Where open_date between getdate(), Dateadd(month, -3) and getdate(), dateadd(month,-6)
It works just fine for me. I tested it with both INT and DECIMINAL datatypes. https://i.imgur.com/VrS6UDG.png
This is MS-SQL? What version? I just tried this code and it worked for me: CREATE TABLE int_tmp (int1 INT) INSERT int_tmp values (1), (2), (3) SELECT CONCAT('$', CAST(int1 AS VARCHAR(100)) ) FROM int_tmp DROP TABLE int_tmp
Give this a look and let me know if you can't figure it out. https://stackoverflow.com/questions/34656367/sql-get-first-day-of-month-3-months-before-current-month
Are you doing just a straight select? Or are you doing an Insert Into Select From statement? That error implies that it is successfully concatenating the dollar sign onto the figure and then it is trying to insert that value into an int column, perhaps?
Thank you!!!
Here is another link: https://stackoverflow.com/questions/1520789/how-can-i-select-the-first-day-of-a-month-in-sql Basically in the example I gave you above you are going to replace GETDATE()-n with some kind of logic. Here is an example of code I have in one of my processes that gives me a "whole" 12 months based on the first of the month. I suppose you would just have to tweak the values a bit for your purpose. Req1stDistrDt BETWEEN DATEADD(m, DATEDIFF(m, 0, getdate()-366), 0) AND DATEADD(m, DATEDIFF(m, -12, getdate()) - 1, 0) 
Clever observation that escaped me.
I’m doing concat on a value returned from a sub query.. 
Hi yes sql server. I’m doing concat on a value returned from a sub query 
I use 'replace' in SQL Server, e.g. " SELECT REPLACE(user,'@x\_y\_z','') " to remove offending characters. Note that the third parameter is not a space, but an empty string (two single quotes). I'm no PostgreSQL wizard, though, so YMMV... or the car might not even start.
Can you post a more complete version of the query. Including if it the results of the query are being inserted into another table? I think that would be critical in helping you solve this.
A GROUP BY clause would solve this for you (order by added also for chronology): &amp;#x200B; SELECT YEAR(RequestDate) AS Year, MONTH(RequestDate) AS Month, COUNT(DISTINCT RequestNo) as ReqRegistrations FROM dbo.Request WHERE RequestDate BETWEEN '01-JUL-2017' AND '30-JUN-2018' GROUP BY 1, 2 ORDER BY 1 ASC, 2 ASC ;
Thanks I'll give that a shot in the morning.
That's perfect! Thanks for that! 
Left joins and coalesce()?
Use the money data type? Cast(field as money)
1. No SSDT. The warehouse will come later, it will be an extension of the Atomic database...and the warehouse will be in the Strategy database. Its all data driven, and really awesome to see what a table can actually do, and the patterns that run them. 2. thanks, make all my beats =).
Holy shit. You ever thought about building an ERP system?
Great stuff
Kinda sort of. I have thought about trying to learn Angular and apply the same patterns to it to create a UI. But ...I am building the poor man's SAP. Master data management is all the systems. This **data** management system... these videos.. are going to hold anything someone gives me. When I design the app my wife wants, it will sit right next to AW, WWI, FEC data, and w/e this evolves into. Its just a multi tenant concept. First.. i have to get the systemMain database working because every table has 11 views attached to it, and i need a way to automatically generate my functions and procedures for change management and data governance. I need something to build my tables by just adding a row, to ensure defaults, clustered indexes, fks, pks are maintained by the SystemMain tables (and error messages). When you see vides sprout up that begin with Definition.. that is what i will be doing. Im super passionate about MSSQL. I can read C#, but i haven't done any .net dev since '03. Im a just a specialist.
I’m not at work right now so I can’t test it but it looks like this would work to me. SELECT DATEADD(m, DATEDIFF(m, -3, gettdate()) - 1, 0) , DATEADD(m, DATEDIFF(m. -6, getdate()) ,0) Does that look right?
All the power to you, dude. Those are big plans and some solid architecture. I've got some side project building "small data" and "big data" pipelines using open source data. Eventually, put a BI stack and some C# on top of it. At the moment, plenty of your material is over my head. But, I'll definitely be watching your videos and trying to keep up. Eventually, our subject matter may cross! When you get to the world of SSDT I would love to hear your opinion on \[biml\]([http://bimlscript.com/](http://bimlscript.com/)). You've said you want to keep things contained in SQL, so this may not fit your plans, but I use it at work to script package and cube creation. It's a mix of XML and C#. I use mainly because I don't know of a way to script the same using SQL. Don't let this go to your head or anything, but you are easily in the 1% of SQL Engineers. This is some crazy stuff, if you're not doing consulting, you should be. &amp;#x200B;
Is this query part of a subselect? It sounds like you're trying to convert the already concatenated number into an int at a later stage.
It’s part of the outer query. I’m querying a sales Column from a sub query. 
I guess I will find out tomorrow and check back. Thank you for your help
I looked up biml. I saw this: [http://bimlscript.com/walkthrough/Details/3112](http://bimlscript.com/walkthrough/Details/3112) Yea, This is the abstruse item i was talking about. A concept can have conditional child concepts that could be other containers for concepts. a concept could be a GetTypeId, or a change management procedure. So if we have already defined a DataSet (table, view, file) then we have DataPoints (columns, elements, attributes), and we can use those Keys and point them to Processes (jobs, insert/update/delete procedures, functions). But... if you can imagine that a record is just a thing, it can be an action or a container, we can create orders with pass/fail conditions. So.. based on what I see on this one page alone, i think its pretty neat to use XML to drive conditions. Anything anyone creates is awesome.
thank you =)
If you want Cell where available, otherwise Home: Select coalesce(c.ContactId, h.ContactId) as ContactId, Coalesce(c.PhoneType, h.PhoneType) as PhoneType, Coalesce(c.Phone, h.Phone) as Phone From Phones c Full outer join phones h On c.ContactId = h.ContactId And c. PhoneType = 'Cell' And h.PhoneType = 'Home'; Probably not very performant on a big data set. You'll want an index on your version of CustomerId or PhoneType depending. 
There is a great MS SQL course on edX.org which you audit for free. 
Postgresql is great and free (open source). Highly standards compliant Sqlite is also a good choice as it's heavily used in mobile apps
Sqlbot is really good
This is actually exciting content. Thank you for taking the time to make it and share it! Looking forward to your updates!? 
One of the best online course. For beginners and for free. With examples and online exercises. [http://www.studybyyourself.com/seminar/sql/course/?lang=eng](http://www.studybyyourself.com/seminar/sql/course/?lang=eng). 
Not really sure whats stopping you from just importing all the data into tables via bulk loads which can even be queried using format files if your csv's are always the same. Which I would assume since you want to do joins they have to have an established format. But I guess if you absolutely have to make life difficult but doing database stuff without an actual database you could try [Apache Drill](https://drill.apache.org/)
Thanks for your response! I'd never heard of 'bulk loads' before, but it sounds like it may do just what I want. Apache Drill seems interesting too...These definitely give me a place to start researching. I appreciate it!
I already explained it to someone else once before so I don't feel like getting into it at 3 am but google the msdn article for format tables. (Or look though a page or two of me being an asshole to people and find the response about bulk loading a made a week or so back on here.) TL:DR it's a flat file (or an xml) you can create from elevated command prompt on an sql server that specifies how a CSV relates to a table in terms of fields and therefore makes it so you can query a CSV as you bulk load. 
Khan academy has the best SQL introduction series on the web. I learned a lot from it. https://www.khanacademy.org/computing/computer-programming/sql
It depends on the indexing of the tables involved, and the volume of data being returned, and whether or not uniqueness is guaranteed. My go-to method is to examine the tables and the context involved. What works well for one design may be poor for another. 
&gt;SqlServer &gt;dealing with Excel and CSV files and their interaction (or lack therof) with databases Right click on your database name in object explorer &gt;Tasks &gt; Import data Data source: Microsoft Excel Destination: SQL Server native client (version) The rest is easy to figure out. You can set this up as a job aswell, this is all inbuilt and works fine, a few times I have had to fiddle with the file to deal with some minor issue but I have never failed to get a file in using this method. Saying that there is no support for excel or csv in sql server is just not true.
duly noted
In my experience, SQL job interviews are less "show me some of your past work" and more hands on questioning with you writing queries or diagrams on a whiteboard. It's pretty difficult to test expertise, beyond looking at your resume, which often is going to be jammed full of half-truths. Maybe hitting up a SQL PASS summit or two would give you some ideas.
Thanks for the kind words. This is what I love to do. I have the next 7 planned. And will update this thread with the new videos as I progress. I'm hoping to record on Friday evenings and Saturdays, make my music on Saturday and upload on Sunday. Not sure how many I will get each week, my min goal is 3 so I'm not under pressure and making glaring spelling and programmatic errors like I have been. "Live streaming dev" is a new thing for me, but I will adapt.
\+1 for the edX recommendation. You don't have to pay the $99 for the certificate and can do the coursework for free. The SQL courses are mostly done by Microsoft so the quality should be good. I would start with this one.. r/https://www.edx.org/course/querying-data-with-transact-sql
This works just fine for me... create table #MyTable (MyInt int) insert into #MyTable values (1),(2),(3) select concat('$',MyInt) from #MyTable
Pandas or spark can do this. Pandas easily handles small files (up to about 4-5 million rows on my macbook) and spark scales with your instance.
Thanks for your input! I'll take a look at it--seems like a useful tool for working with data in flat-files.
You can use `.read_csv()` with arguments to specify types, converters (i.e., string to datetime), and compression (Pandas can natively import delimited compressed files, like gzip), among other things, then use [`.join()`](https://chrisalbon.com/python/data_wrangling/pandas_join_merge_dataframe/) on your input data. Just remember that `df1.join(df2)` implicitly joins on `df2.index`, so you either need to specify `on` (`df1.join(df2, on='column_to_join_on')`) or set the join column as df1's index. Either way the join column needs to be df2's index ``` df1.set_index('join_column', inplace=True) df2.set_index('join_column', inplace=True) df12 = df1.join(df2, how='left') ``` 
You want to store the images, and some associated markdown? The images are easily storable as blobs, or maybe compressed blobs, but I am not understanding what markdown you want to store. If the markdown you speak of has a fixed schema, just convert that to fields, if it's not fixed then you'll need to store that as xml or some string representation. &amp;#x200B; Do you want to search on the contents of the markdown? That seems to be the tougher question. 
That is a pretty expensive way of doing it. I'd use the union OP suggested first. 
 WHERE open_date &gt;= dateadd(month, datediff(month, 0, getdate()) + @NumberOfMonths, 0) Specify a negative number for months prior. Specify a positive number for months in the future.
Microsoft used to have a utility called SqlParser or LogParser that did that. I don't know whether it's still supported. 
There used to be SQL Server Pro many years ago, but that went digital and then folded, and I've not heard of anything since.
Sorry, I’m concating a SUM. 
Hi sorry, I’m concating A SUM function 
It turns out that the CASE When StopTime = '00:00' was making SQL not recognize that StopTime was being used as an aggregate, even when I added StopTime to the part before the Else (SUM(DATEDIFF(MINUTE,TU.StartTime,TUStopTime)+1440). I ended up having to use StopTime (and StartTime) in a SUM *as part of the case* and it quit throwing me that error. Once I made the CASE be based on an aggregate function it recognized that it didn't need to be in the GROUP BY. Case When convert(numeric,SUM(DATEDIFF(minute,TU.StartTime,TU.StopTime)))&lt;0 Then convert (numeric,SUM(DATEDIFF(minute,TU.[StartTime],TU.[StopTime]))+1440)/60 Else convert (numeric,SUM(DATEDIFF(minute,TU.[StartTime],TU.[StopTime])))/60 end as iexMins
You'll need to post the entire query in order for me to help you any further. 
there's a guy on twitter who regularly publishes a collection of SQL Server articles called "SQL Server Daily" https://paper.li/DenisGobo/1477832473#/
When you view the report on the Report Manager (report server) does it display/print correctly?
Also - as far as I know it's still documented that COALESCE on 2 values could have integrity issues...should stick with ISNULL for 2 values, COALESCE for more than 2.
Thank you! As I said, I am very new - if each user is different, how would this work e.g. my users are BLOGGS, SMITH, JOHN, PETERS some with the @xyz, some without. The column name is "User"?
Are both attributes on the same row? &lt;CustomerID, HomePhone, CellPhone&gt;? Select CustomerID, COALESCE(NULLIF(CellPhone, ''), HomePhone) as Phone The NullIF is just a protection on the chance that someone entered a blank string. You could also trim CellPhone if you are worried about ' ' as an input. If the table is &lt;CustomerID, PhoneType, PhoneNumber&gt; I'd use RowNumber if you can guarantee the desired PhoneTypes are match Alphabetical Order (A-Z or Z-A). So if there was (Cell, Home, Work) and you wanted (Cell -&gt; Work -&gt; Home) you'd have to add some logic to assign an ordered number. CASE WHEN PhoneType = 'Cell' THEN 1 WHEN PhoneType = 'Work' THEN 2 ELSE 3 END as Order WITH Ph AS ( Select CustomerID , PhoneNumber , Row_Number() OVER(Partition By CustomerID Order By PhoneType ASC) AS RowNum FROM .... ) SELECT * FROM A WHERE RowNum = 1; 
Can you screencap the pivot fields in excel? you may be indexing values as a column.
As in you have the SUM within the CONCAT statement, or you have the CONCAT within the SUM? If you just posted your entire query here, we'd all be able to help a lot easier.
Due to company policy, I can't screen cap or upload anywhere (even if I blur out information. Image sites are blocked). But I don't believe I have indexing anywhere. The data is only export to Excel in order to be read in a report-like form. No data is being manipulated within Excel
If normalized, CustName shouldn't be in your table. Normalization doesn't want repeated values. One of normalization's benefits is if you want to update CustName, you only have to update one record. In your table you would have to update CustName on several records. Another objection is if your table should exist at all. Monthly balance can be calculated by adding/subtracting all the transactions less than some date. Storing monthly balance separately is duplicating this information. It runs into similar problems if a transactions are changed, balances may also need to be updated.
Brent Ozar has a nice weekly email. Full of interesting bits and pieces about SQL along with the odd random article about something else.
CustName shouldn't be the the column header? And the monthly balances are all pulled from a file from somewhere else a day after the file is created (so today will be the file from yesterday) so the transactions wouldn't change.
Any injection attack using double quotes (") instead of single quotes. SQL injection is pretty much a solved problem. Don't try to come up with your own solution to it, just follow industry standards. https://www.owasp.org/index.php/SQL_Injection_Prevention_Cheat_Sheet
It's common and useful to have denormalized data, such as balance by month, but it's still not a normalized structure. A clear tell that something isn't normalized is if one record is updated and another record has to be updated to stay consistent.
Customer name can be a column in your results, but shouldn't be in your balance table. If a customer changes their name, you should only have to change it in one place (the `Customers` table, for example), and then your balance table would only have an ID that references the `Customers` table to look up the actual name. You'd join the `Customers` table to the balance table. Usually if your story starts with "I receive this file of compiled data from another source..." that data is probably not fully normalized.
If you have VM software, another option is to download the free version of Microsoft SQL Server and run it in a Windows VM -- for the love of god don't install SQL Server on your your main OS.
Thank you for this! I’m currently trying to self-teach and anything helps. Just discovered this community, any other resources?
What if you change the format of your balance month to include the first day of the month? Or change to reporting balances per-day instead of per-month?
So you are saying we have a single table like this: | PersonID | PhoneType | PhoneNumber | | :--- | :--- | :--- | | 1 | 0 | 5555555555 | | 1 | 1 | 5555555556 | | 2 | 0 | 5555555557 | | 2 | 1 | 5555555558 | | 3 | 1 | 5555555565 | | 4 | 0 | 5555555575 | | 5 | 0 | 5555555595 | | 5 | 1 | 5555555655 | | 6 | 1 | 5555555855 | | 7 | 0 | 5555555955 | | 8 | 0 | 5555557555 | | 9 | 0 | 5555559555 | | 9 | 1 | 5555565555 | | 10 | 1 | 5555855555 | Simplistically there can't be more than (2) numbers (Cell/Home) per person, or duplicates, and we're saying 0 is Cell, and 1 is Home. And you want the "easiest" way to do it, correct? My first guess: cte as ( select personid, count(*) c from table group by personid ), select personid from cte a inner join table b on a.personid = b.personid and a.c = 2 and b.phonetype = 0 union all select personid from cte a inner join table b on a.personid = b.personid and a.c = 1 and b.phonetype = 1 You could do it with a (2) joins and a case statement, which might be easier to read. You could also use a `row_number()` which is very handy when it comes to deduping sets before you join. After reading other responses I like what /u/abbbbbba did best. It's just real simple to read and understand what's going on but it does require the "phone type" to be Cell, Home, Work (alphabetical order of C, H, W, or, 0, 1, 2) and would need some modifications if it were Home, Mobile, Work, or Business, Mobile, Residence. I think for a "theory" question the best answer is that it depends on the data structure and then give examples like this as reasons why it depends --&gt; which illustrates how and why it is important to structure your data in such a way that conforms to the regular business practices. Arbitrarily you can imagine a company whose sole job is to query a table of everyone on the planet to accomplish the task you have in mind, and for whatever reason they need to query this many times a day. Since "cell" is the preferred method of doing business, then having your data mapped in such a way that lets you most efficiently query your data will be beneficial. If you have multiple data sources which are being brought in to a single table, then you can see how having an array of values such as: Cell, Home, Work, Business, Mobile, Residence, Office would change the way you need to solve this problem, whereas if you only have to worry about Cell, Home, Work, and you always know C will come alphabetically first... then you can leverage that structural fact to solve your problem, whereas if that is you are not paying attention to how your data is structured you cannot take advantage of it, cannot simplify your query, etc.
Say the monthly balance is calculated at the end of every month, and stored in a table. TransactionID|CustomerID|TransactionDate|IsCredit|Amount -:|-:|-:|-:|-: 1|1|2000-01-01|1|100 2|1|2000-01-09|0|60 3|1|2000-01-15|1|100 4|1|2000-01-23|0|120 5|1|2000-02-01|1|100 7|1|2000-02-12|0|90 6|1|2000-02-15|1|100 7|1|2000-02-28|0|60 CustomerID|ReportingMonth|Balance -:|-:|-: 1|2000-01-01|20 1|2000-02-01|70 Now for whatever reason, we change the amount of a transaction. UPDATE Transactions SET Amount = 90 WHERE TransactionID = 4; To keep our data consistent, we'd have to update CustomerBalance as well. UPDATE CustomerBalance SET Balance = Balance + 30 WHERE CustomerID = 1 AND ReportingMonth &gt;= '2000-01-01'; These kinds of cascading data changes are one of the things that normalization is intended to prevent. A normalized CustomerMonth table might have something like if CustomerLevel [Silver/Gold/Platinum] was evaluated monthly or something subscription related. If the business rules that determine CustomerLevel changes, you wouldn't change historical levels, so it's not the kind of thing that cascades. Another way to think about it is if you ask the question, "What level was Customer#1 a year ago?" It's not something that you calculate (because business rules could have changed), but it's an actual attribute of that Customer and that Month.
https://www.databasejournal.com/
Could you elaborate on this?
Thanks. This was a super helpful post. 
Oh! So for it to be normalized , the table must have the *specific* related data to it?
The most common form of normalization is based on the table name and the data stored. (1nf-dknf) So... technically if your post-pivot structure is named, and keyed, accordingly then it is still normalized.
I do have separate tables for this data. tblCust only has Customer information and so on. I should have mentioned this in the original post, though it slipped my mind! I apologize for any confusion! Thank you for your input!
Well, damn. I believe I have that. The tables shown are results of joining one table (contains customer number, balance, and date) to another (contains customer name, type, etc). on the customer number.
It was in former MS documentation that COALESCE in MSSQL had potential for data integrity flaws on 2 values. I'll try to find it when I get home, but it's very possible it's been resolved by now. I remember it being an issue because ISNULL wasn't (still isn't?) ANSI standard. I can't seem to find any notes or warnings in documentation now so I'll edit my original comment for now.
So, normalization - as I thought I had achieved it with the related tables already - wasn't necessarily as thought when creating my Pivot. The pivot is just sent to my C# program that then sends it to Excel as a "report". It's just supposed to show the average balance per month (all taken from other reporting sources). In the link I provided to my StackOverflow question, the first comment mentioned Data Normalization. I wasn't sure how that applied to what I have as the tables being joined to each other are already normalized. 
There are too many flavors of what is considered normalization. For instance, I would never have a customers table, just a group table, because I see no difference between a customer, a template of data, or a sub group. I need a 3 tier hierarchy of types to classify a group. But I would also not have a transaction table because transactions are just a particular type of events. And a currency structure to hold the values. All in all, to model all of your data I would have, at minimum, 14 tables because I model the concept rather than the specific. But the link that individual provides just references 1-3. Not dknf (6 or 7), so... I would ignore it.
My data received is imported every day, for the previous day of final balances instead of transactions. I don't care about the transactions nearly as much as I care for the final amount at the end of the day. The data received isn't even made my company either. It's a file we receive from another who's reporting on it and we use that data. (Well, we write our in-house programs to read it and send it to our SQL Server, which is what I'm dealing with atm). I was just confused as to why data normalization was the top thing brought up on my question, as I believe I have all my tables normalized **up to** the pivot table, all to give it the "report look".
Gotcha. Thanks for the help! Step one: find out how to get noticed on SO lol
Try Count(distinct case.... And use the Id else null in your case expression.
Looking forward to hearing your remarks. Thank you.
i see a pattern. And SQL loves patterns. The answer is yes, but you might want to model every piece of the name.
I thank you for taking the time out of your day for this.
After some reading, it seems to be the case that in MSSSQL, `COALESCE` will automatically cast all values to the same type based on type precedence: [COALESCE vs. ISNULL](https://www.itprotoday.com/software-development/coalesce-vs-isnull): &gt; The data type of a COALESCE expression is the data type of the input argument with the highest data type precedence. If all inputs are the untyped NULL literal, you get an error. &gt; &gt; The data type of an ISNULL expression is the data type of the first input. If the first input is an untyped NULL literal, the data type of the result is the type of the second input. If both inputs are the untyped literal, the type of the output is INT. So that can end up with unexpected results. This surprised me, because I'm most familiar with PostgreSQL where calling `COALESCE` with mismatched types will result in an error.
Fantastic series. Please keep it up. I'm a diehard fan of ssis but am particularly intrigued by the idea of not using it to build a DW or BI solution. 
Thank you! Its going to be some time before I get there. In a few videos I will have my relationship shape up, then we go back and change the tokenized scripts to code for our table creation... to pull the relationships based on its shape (columns required, indexes, defaults, foreignkeys,views,functions,procedures) Then I'll get into the domain and shared attribute shapes (modeling people and groups, and name, currency,events,contact,settings,references rcfts). Once those are completed, I should be near 90% on the system and I'll start a new playlist for importing adventure works so I can demo migrate,input, and atomic databases... but that's still a good 20 videos away. But when you think about it... it's less than a 40 hour week to spin up a system (if you dont have scripts) that will create and model anything a business needs. Which is where aw and wwi come into play.
Oh, I assumed you meant "from 3 months ago to now or from 6 months ago to now". If today is October 15 and you want open_date between April 1 and July 31, then do this: WHERE open_date &gt;= dateadd(month, datediff(month, 0, getdate()) - 6, 0) AND open_date &lt; dateadd(month, datediff(month, 0, getdate()) - 2, 0) Logically this is "where open_date is on or after the first day of the month six months ago and open_date is before the first day of the month two months ago". 
Updated with error message 
If it's base64, just use a text column in sqlite.
I think you basically have it... UPDATE temp2 SET(columns from temp2) = (columns from temp1) from temp2 join temp1 on temp2.id = temp1.id WHERE(columns from temp2 are in columns from temp1) AND temp2.ID= temp1.ID
You can use Python to get apis. Just use ‘requests’ or use Beautifulsoup to scrape the info. If the website has JavaScript rendering or you have to login, click buttons, etc then you can use Selenium 
You can extract the column names from INFORMATION_SCHEMA.COLUMNS and create an on the fly update string that you can then execute. Something like this. select * into temp1 from &lt;table1&gt; go select * into temp2 from &lt;table2&gt; go Select t1.COLUMN_NAME INTO #Cols from (SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = N'temp1') as t1 join (SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = N'temp2') as t2 on t1.COLUMN_NAME = t2.COLUMN_NAME where t1.COLUMN_NAME &lt;&gt; 'ID' declare @sql varchar(1024) set @sql = (Select COLUMN_NAME + ' = temp2.' + COLUMN_NAME + ',' AS [text()] From #Cols For XML PATH ('')) set @sql = 'Update temp1 Set ' + left(@sql,len(@sql)-1) + ' From temp1 join temp2 on temp1.ID = temp2.ID' --PRINT @sql exec(@sql) go drop table temp2 drop table temp1 drop table #Cols
Sorry, I had too many questions in there and none of them came out clearly. Say I have two tables: 1. the wide-format one in the OP. Call it data\_table 2. a table of sensors, like below. Call it sensors\_table &amp;#x200B; |key|column\_name|sensor\_type|sensor\_height|statistic| |:-|:-|:-|:-|:-| |0|temp@2m\_mean|temperature|2|mean| |1|temp@2m\_stdev|temperature|2|stdev| |2|temp@10m\_mean|temperature|10|mean| |3|temp@10m\_stdev|temperature|10|stdev| &amp;#x200B; My goal is to use information in sensors\_table to select a subset of columns from data\_table. For example, filter the sensors for height == 10, then return the data columns that correspond to those sensors. Where I'm struggling is in getting SQL to select column names that depend on other query results. My full (planned) implementation has a few other tables similar to sensors\_table, but they fit together in a normal row oriented format and work with basic queries. It is only the wide-format data tables that are giving me a headache.
Yeah, this is "a way" but I'd use a SP with temp tables and joins myself.
thanks for the feedback!! I assumed that having a little networking knowledge could work on my favor and or if I get into some kind of networking position the data management/ automation side of things would be very beneficial &amp;#x200B; overall I see them as working together very well but I haven't worked in IT in years
Thank you for this. I knew there had to be something more than just "isNull is coalesce with only 2 parameters". 
Waves. Hi there. I made my bones way back in the day programming in COBOL, and RPG, with a bit of C, Java, cough, FORTRAN, PERL, and little snippets of Javascript, HTML, Java, and &lt;insert language here.&gt; -- Python and R are pretty interesting to me currently. *Anyway,* I made the transition from programming, and spent some time with hardware, networking, and security. I also know telephone systems pretty well. I love analytics. I love it. I fucking love it. My background in the other areas is such a huge benefit. If we're looking at phone data, for example, then I have a ton of experience with how they are set up, how to break them, etc. I can't stress this enough. I think I've mentioned it before in another thread but I am a world class expert when it comes to doing analytics as it pertains to a certain telephone system that is provided by a top tier communications company. It was a total accident. Our company installed it. Our C-level executes demanded certain requirements. Then using the data set that their hardware produced, their 600 page manual, and getting on calls with their engineers we were able to statistically analyze certain events that, up until that point, not something the vendor ever thought about doing. It was a very cool experience. You definitely know the company I'm talking about, and they would look at me in meetings with their own people to explain how we were interpreting the data. There are a lot of challenges in this field. If you excel at it, and do well, then you will inevitably end up working with people who do not understand the technical complexities of things that you understand, and your greatest challenge will be learning how to effectively communicate these things to them. 
Thanks to everyone for all of these great tips
Thank you very much for the info. Much appreciated 
Run this code and look at the result: SELECT split_part("User", '@', 1) AS user , split_part("User", '@', 2) AS domain FROM &lt;tablename&gt;;
Yup
There are better ways (Python, curl), but you can use this (might be easier) - https://stackoverflow.com/questions/33449/can-you-call-a-webservice-from-tsql-code
Yep, this is legal. Things in the select don't affect the filtering per se, but I think using certain aggregate functions as select columns could change how the query optimizer fetches the data and hence the speed. Intuitively, the way to remember it behaves this way is that if you had "HAVING count(*) = 10", it would be really goofy of SQL to demand you fetch a column that's just the number 10 for every row.
Levenshtein distances are great for string comparisons but this example of using them in addresses is highly flawed in one major way: addresses are sequential. "123 Main St" compared to "124 Main St", and "923 Main St" has a difference of one character yet are distinctly different and cannot be equated. Address parsing is a very hard problem to solve even halfway acceptably. SQL is not suited to this task at all. 
Yup, just tried it.... it works.
What I actually do at work is use the row_number() method because we do have multiple numbers per person, even of the same Phone Type. It is slightly slower than the other methods, but not much, and actually uses less IO. -- FULL OUTER JOIN -- Surprisingly not as slow as it seems. Returns multiple results declare @Results1 table (PersonId int, PhoneType varchar(10), Phone varchar(20)); insert into @Results1 select Coalesce(c.PersonId, h.PersonId) PersonId , Coalesce(c.PhoneType, h.PhoneType) PhoneType , Coalesce(c.Phone, h.Phone) Phone from (select PersonId, PhoneType, Phone from Phone where PhoneType = 'M') c full outer join (select PersonId, PhoneType, Phone from Phone where PhoneType = 'H') h on c.PersonId = h.PersonId GO --UNION ALL / NOT EXISTS -- Fastest for my system. Returns multiple results declare @Results2 table (PersonId int, PhoneType varchar(10), Phone varchar(20)); insert into @Results2 select PersonId, PhoneType, Phone from Phone m where PhoneType = 'M' union all select PersonId, PhoneType, Phone from Phone h where PhoneType = 'H' and not exists (select 1 from Phone where PhoneType = 'M' and PersonId = h.PersonId) GO --ROWNUM -- Slowest on my system, but gives the answer I usually want, which is 1 number per customer declare @Results3 table (PersonId int, PhoneType varchar(10), Phone varchar(20)); with Phones as ( select PersonId , PhoneType , Phone , ROW_NUMBER() over (partition by PersonId order by PhoneType desc) RowNum from Phone where PhoneType in ('M', 'H') ) insert into @Results3 select PersonId , PhoneType , Phone from Phones where RowNum = 1; GO 
They mention in the post that the algorithm is useful when working on CRMs or system similar to that. So it is helpful with a small set of addresses that belong to different people and it's very unlikely that contiguous addresses are entered. This is useful when trying to detect scams. The department of defense uses this algorithm if I'm not mistaken. 
If your data set is small and disparate I guess this is a "close enough" solution. Perhaps my background dealing with real estate data makes me bias toward more verifiable solutions. 
I’ve never really hated dealing with data and addresses until I had to deal with international addresses. How any humans have made what we have made and done what we have done and we can’t get “red door” out of a Bangladeshi address will never cease to amaze me. 
No kidding. We send packages to Taiwan sometimes and I still have no understanding of the addresses I write on those boxes. Check out Carmel By The Sea on Wikipedia if you want to see some funky addresses in the US.
yeah, and it's 2018, so everyone uses google maps API to handle addresses
Thank you so much! That is perfect
Thank you so much! That is perfect
You'll need to use dynamic sql to build a list of field names from the sensor table into a query. How you do this depends on the version of SQL in the database. In sql server you could do: `DECLARE @dynamicsql nvarchar(max)` `DECLARE @columns nvarchar(max)` `SET @dynamicsql = N'SELECT xxx FROM data_table'` `SET @columns = N'timestamp'` `SELECT @columns += N', [' + COLUMN_NAME + N']'` `FROM column_name` `WHERE sensor_height IN (2, 10)` `SET @dynamicsql = REPLACE(@dynamicsql, 'xxx', @columns)` `EXECUTE sp_executesql @dynamicsql` &amp;#x200B; Please note the above is quick and dirty and you should properly look into dynamic SQL and understand it before you run any of the code. &amp;#x200B; If you have any questions on the above I can try to answer them.
Last paragraph. 100%. 
Do you have a formula for EMA?
For a EMA14 Initial SMA: 14-period Sum / 14 Multiplier: (2 / ((time-period) + 1) (2 / (14 + 1) EMA = Multiplier * (Close price - EMA(previous day)) + EMA(previous day) The problem I have is I don’t know how to calculate the initial period that uses the SMA14 then the subsequent calculations uses the lagged values of the EMA
To query a varchar you need single quotes- you can escape single quotes with two in a row.. so like, insert into customers (id, first, last) values (1, 'fred','o''grady')
Look into window functions. In particular, aggregate window functions would probably be helpful. https://drill.apache.org/docs/aggregate-window-functions/ https://www.red-gate.com/simple-talk/sql/learn-sql-server/working-with-window-functions-in-sql-server/
Thanks so much, I wasn't aware of dynamic SQL. I'll do some digging.
I did this for a 10 day EMA. My query is [here](https://www.dropbox.com/s/vxxjr0afdpxwabp/EMA.sql?dl=0).
Do you have a google sheet or something with the formulas? Because this is not at all a good explanation of how you calculate what you want. Like, "period Sum" or "time - period", how are we supposed to know what that is? Also, what's you database? As in the engine - MySQL, Postgres, SQL Server, Oracle, etc.?
I'm a BI architect for a medical billing company with multipayer claims. What are you having trouble with?
Once you postgres you can't regress... because we won't let you. but seriously I have no idea. You're not trying to run that from a zipped folder are you?
Try using Revo uninstaller
Looks like I might have just solved it. I think all I need to do is run a cursor on the column names for my temp table found in tempdb.sys.columns What I have currently looks like this, and I believe it will work: DECLARE @ColumnName nvarchar(max) DECLARE @SQL nvarchar(max) DECLARE MY_CURSOR CURSOR LOCAL STATIC READ_ONLY FORWARD_ONLY FOR SELECT DISTINCT Name FROM tempdb.sys.columns WHERE object_id = object_id('tempdb..#temp2') OPEN MY_CURSOR FETCH NEXT FROM MY_CURSOR INTO @ColumnName WHILE @@FETCH_STATUS = 0 BEGIN --Do something with ColumnName here SET @SQL = 'ALTER TABLE #temp2 ALTER COLUMN ' + @ColumnName + ' NVARCHAR(MAX)' print @SQL FETCH NEXT FROM MY_CURSOR INTO @ColumnName END CLOSE MY_CURSOR DEALLOCATE MY_CURSOR I haven't tested this yet, but based on the print statement, this should work once I replace "print @SQL" with "exec @SQL". Let me know if anyone else figures out a better way to accomplish this though!
[Yay, more blog](https://i.imgur.com/JF5gB4C.png)
Ok, I read up a little bit on what these EMAs and SMAs are ([link](https://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:moving_averages)), it's fairly simple. Here it is, with comments: ;WITH SMA AS ( -- calculate SMA for each row, across the entire range SELECT stock_id , dt , price , SUM(price) OVER (PARTITION BY stock_id ORDER BY dt ROWS BETWEEN 13 PRECEDING AND CURRENT ROW)/14 AS sma FROM reddit_9ozhkk WHERE stock_id = 2 ), -- Recursively; first row, here from 2018-02-01, will use SMA as the EMA -- then, recursively, each row from a subsequent date will calculate the EMA using the formula -- Multiplier is 2.0000 / (14+1) -- = 0.133333333333333 -- If SMA value is removed from the SELECT statement, it will run much faster as the database will -- only need to calculate one value for the rather expensive window SUM EMA AS ( SELECT dt, price, sma, CONVERT(DECIMAL(10, 4), sma) AS ema FROM sma WHERE dt = '2018-02-01' UNION ALL SELECT curr.dt, curr.price, curr.sma, convert(decimal(10,4), calc.ema) as EMA FROM ema previous INNER JOIN sma curr -- This join will only work if there are no gaps inbetween data points -- Otherwise, add a ROW_NUMBER() OVER (PARTITION BY stock_id ORDER BY dt) AS rn -- and join on that ON curr.dt = dateadd(day, 1, previous.dt) CROSS APPLY (SELECT 0.133333333333333 * (curr.price - previous.ema) + previous.ema as ema) calc ) SELECT * FROM EMA OPTION (MAXRECURSION 0) [Setup script for a working example can be found on pastebin](https://pastebin.com/R8WFyS3P).
Far as I know, BETWEEN uses the ending values. So it will take the full 24 h of 1 jan 2017 and same with the 2018 one.
I mean, that works, but I would never in a million years qualify a recursive CTE as "fairly simple".
Yep, we need more details. 
You can add counts together from subqueries. For example I have an employee table that stores active/terminated status. I can have two queries, one for counting active, and one for counting terminated, then add those counts together. select (select count(*) from employee where employment_status='A') + (select count(*) from employee where employment_status='T') from dual; Could you do something similar? select (select distinct count(*) from foo where something='processed') + ( select distinct count(*) from foo where something='unprocessed) from dual; --or select from nothing at all if using microsoft
&gt; If you're not specifying the time, it's inclusive from 00:00:00 to 23:59:59. no, sorry 
BETWEEN is inclusive. But why wouldn't you just refer to the docs for this? It takes 3 seconds of Googling
Have you tried reading about normal forms and the normalisation process?
&gt; the primary key for this table is bcid Nope. bcid "101" appears twice. Definitely not the primary key.
one table for Books one table for Rooms one table for Bookcases one table for Books to Bookcases one table for Bookcases to Rooms dunno what capacity refers to. Case probably. i think that will get you to 3nf &amp;#x200B;
applying a function like CAST means the index, if any, on that column cannot be used so it is ~not~ best in terms of performance
I'm not sure that applies when the data types are in the same family. I've never seen casting a datetime to date affect performance, even in joins.
I avoid cursors where possible. Think of data always in terms of sets of data instead of individual rows. You are on the right track though with using sys.columns. I'm going to avoid commenting on updating all columns in a table to nvarchar(max) and assume you must do this for some hairball third party software problem. Anyway, try this: create table #temp2 (column1 varchar(10), column2 varchar(25), column3 varchar(50)); declare @sql nvarchar(max); select @sql = COALESCE(@sql +'; ','') + 'ALTER TABLE #temp2 ALTER COLUMN ' + Name + ' NVARCHAR(MAX)' from tempdb.sys.columns where object_id = object_id('tempdb..#temp2'); SELECT @sql; EXECUTE sp_executesql @sql; drop table #temp2;
If it is in single quotes, you are looking for the literal string Bob, or the value inside of a field. The second example (in most cases) would be comparing a column to a column named bob and return results where values in both columns match, irrespective of the value in those columns.
What is this, MS Access?
I have never combined claims data at the claim line level before and I am just not sure how to combined the data, between like 3 payers who have different data sets. When you need to start from scratch on a claim line file that you have not integrated yet. What would be the first couple things you would do? 
DaySmart software required the latest version of SQL, it allows us to customize reports using SQL coding, but 123pets support people don't support custom reports . I printed the dbschema, and have been experimenting 
Things in the select statement do indeed play a role in what goes on. Imagine that you have an index on TABLE for columns (A, B) and your select statement is SELECT A, B, C FROM TABLE. Well the index has the A, B information since that's how the index was made, but the C column will need to be pulled from elsewhere. Not every database uses this kind of scheme for indexes, but MS-SQL is a notable user of the "Covering Index" and PostgreSQL has also recently included the covering syntax as well, as opposed to logical pointers to data.
SQL is a query language. Do you mean MS SQL Server, the database program? Can you post a screenshot of what you're doing? If it allows you to modify the report SQL, it should just be a page of text that you can edit.... nothing with drop down boxes or whatever.
I will for sure in the morning
Yeah, PGSql can query JSON: https://www.postgresql.org/docs/9.3/static/functions-json.html
More or less, it does everything a doghouse could need, scheduling, credit card processing, payroll, and it has a ton of built in reports, but I couldn't find a report to modify a second period of time for a second column, to easily compare the two