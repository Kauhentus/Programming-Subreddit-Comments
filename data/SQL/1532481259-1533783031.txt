It’s probably enough. I’d also want to know what flavor SQL. By sending Pre-Work, I’d expect it to be done ahead of time. At the very least, they are giving you a chance to memorize your answer. It’d be embarrassing to mess up when you’ve had time to figure it out. 
Fortunately it’s not my job to fix, but it’s making one of my main job responsibilities impossible, so I need to address the issue to stakeholders. I’m just to figure out a temp solution for myself. 
So many reasons.
Bro - just use GUIDs in each database instead of integers. Then they should be unique across DBs
You are every DBAs nightmare!
Or you could use the [`uniqueidentifier`](https://docs.microsoft.com/en-us/sql/t-sql/data-types/uniqueidentifier-transact-sql?view=sql-server-2017) type.
Just don't cluster on the GUID.
unless you default newsequentialid
You're welcome!
Storing date/time as text? Eewwwwww!
Was gonna say that. Is that not exactly what uniqueidentifier is for?
Odd and even does work if you're sure the schemas align and you only have to have a bijection. However, if you're considering another database, it's probably better to just create IDs like \`&lt;DatabaseIdentifier&gt;-&lt;PrimaryID&gt;\` like \`NewYork-130312\`. Does that make sense? After that, you can look into grouping/clustering to see if you can merge/de-duplicate records.
From my POV, tableau and things like powerBi are very “cool” but a lot of bigger or older companies seem to want old school printable and email reports on a set schedule. Tableau and other dashboard things aren’t the best at that. That said there are some BI packages that focus on report writing or at least incorporate it heavily. Sundas BI, Yellowfin, Logi. Pivoting slightly. I would guess your first job would be one of the other. What I mean is it would be report writing that is heavy in report writing with SQL queries or it will be more stat heavy. Python is ok for stats and is improving every day but if you work for a company with a mature database you won’t need a lot of the packages. If I were you I’d focus on stats theoretically from a language agnostic perspective. Like understand why you might use one model over another (and get the basics like confidence intervals) and keep picking up sql on the side. sql is super useful in most jobs you seem to be looking at but won’t necessarily be taught whereas stat classes are very good things to be taught in a university setting. Places like datacamp can catch you up on how to implement models but it can’t tell you as intuitively what blue is. 
From my POV, tableau and things like powerBi are very “cool” but a lot of bigger or older companies seem to want old school printable and email reports on a set schedule. Tableau and other dashboard things aren’t the best at that. That said there are some BI packages that focus on report writing or at least incorporate it heavily. Sundas BI, Yellowfin, Logi. Pivoting slightly. I would guess your first job would be one of the other. What I mean is it would be report writing that is heavy in report writing with SQL queries or it will be more stat heavy. Python is ok for stats and is improving every day but if you work for a company with a mature database you won’t need a lot of the packages. If I were you I’d focus on stats theoretically from a language agnostic perspective. Like understand why you might use one model over another (and get the basics like confidence intervals) and keep picking up sql on the side. sql is super useful in most jobs you seem to be looking at but won’t necessarily be taught whereas stat classes are very good things to be taught in a university setting. Places like datacamp can catch you up on how to implement models but it can’t tell you as intuitively what blue is. 
Good initial steps. Make sure you have a read-only account, and are logging on to a non-production version of the database. If you're looking for some very hands-on "learn-by-doing" practice problems, that teach basic to advanced SQL with well-designed, real-world practice problems, check out [SQLPracticeProblems.com](https://sqlpracticeproblems.com/#utm_source=r1&amp;utm_medium=r1&amp;utm_campaign=r1). I developed it after teaching a SQL course where the material I had to to teach from was poorly structured and academic. Afterwards, the students emailed me, saying they needed practice problems, so I developed the course! Contact me (email in the FAQ) if you have questions. Also, redditors get 30% off the Professional package (discount code reddit30). 
Run a SELECT COUNT DISTINCT(customer\_nbr) FROM accounts\_table WHERE loan\_type = 'xxxxxxx' in the second question.
For the third question, the table, I would probably use COUNT(CASE WHEN days\_past\_due &gt;0 AND days\_past\_due &lt;=30 THEN customer\_nbr ELSE NULL END)...
thank you very much :) 
No worries.
It would help if the dialect of SQL was specified, but this would be perfectly valid if using eg. SQLite
I assume you're not working in an environment where other processes could be locking data, or changing it between executions? A few questions: How are you checking the number of results returned? Are you using SQL Management Studio to do your querying? Can you post the query?
Yes this is SQL Management Studio on Windows Server 2012. And yes this is a live table which can get new records, but the same thing happens to non-live tables. `SELECT TOP 10000 [Resource_ID]` `,[Organisatie_ID]` `FROM [OMRP_SJCN].[dbo].[RAP_WERKNEMERS]` I removed some columns to shorten the query example, but the effect is the same on either. While typing my response I just now tested it again and somehow it seems to be working again, the question is for how long. The server is not running much else except for a XAMPP server, which is on a different port.
Except newsequentialid can stop being sequential [when you reboot your server](https://docs.microsoft.com/en-us/sql/t-sql/functions/newsequentialid-transact-sql?view=sql-server-2017) (still 100% unique, and sequential within that uptime period, but the seed value could be lower than your previous values. making it entirley non sequential :()
I'm curious, why is clustering on a GUID bad? I would assume it's performance related; but, would be curious for a more detailed reason. 
Sorting GUIDs is pretty much as bad as it can get.
My bet would be on isolation level being the cause. Run the below and see what it returns: SELECT CASE transaction_isolation_level WHEN 0 THEN 'Unspecified' WHEN 1 THEN 'ReadUncommitted' WHEN 2 THEN 'ReadCommitted' WHEN 3 THEN 'Repeatable' WHEN 4 THEN 'Serializable' WHEN 5 THEN 'Snapshot' END AS TRANSACTION_ISOLATION_LEVEL FROM sys.dm_exec_sessions where session_id = @@SPID
Thanks for helping, the query returned: Readcommitted
Column width (which bloats every index and join on that table) and typically very random/unsorted.
&gt; typically very random/unsorted. Ah, that's the bit I wasn't thinking through. Thank you.
How many records in the table total? Are there other processes updating the data or is the DB relatively quiet when you were running your queries? 
One table has about 30k the other has about 12k. These are in different databases on the same server though. The 30k one is the most active, the 12k one is mostly inactive and will be updated about 5-10 times a day. Mostly near the start or end of a workday. Both have the same issue
how do i go about connecting using this?
Depends what flavour SQL server they are on. You really need to be speaking to the person who set this work.
Try this method `SELECT` `t.colA,` `t.colB` `FROM (` `SELECT colA, colB FROM tableA` `UNION` `SELECT colA, colB FROM tableB ) AS t` `ORDER BY` `t.colA DESC,` `t.colB ASC;`
Others have said it - sounds certain to be the case -- it's an isolation level issue. If you want to read potentially dirty data you can add WITH (NOLOCK) and you should get more consistent results.
So you want to see each distinct comment text for each ItemCode? If that is the case then it's possible but you'd have to do like a sub select inside your select statement to hash all that together.
Where exactly are you seeing it? As a field name? In an execution plan? In a stored procedure?
I dont even know if its sql server. I dont know what type of db it is. they just gave me that information in a pre designed format that made it look like they are used to sending it out and having people not aks questions.
Either give us more information to work with or &gt; **You really need to be speaking to the person who set this work.**
Thanks so much this reply is exactly what I was looking for. &gt;If you can convince your IT folks to install it for you, most people running MS SQL Server will use "SQL Server Management Studio" (SSMS) to connect to their MS servers. MS has a whole bunch of other tools though too; more info here : https://www.microsoft.com/en-us/sql-server/developer-tools Nice! First, fortunately it's already installed on our server. I'm in the mix of day-to-day IT things around these parts, so I should be able to do what needs needs doing once I know what I don't know. &gt; Yes, you can run SSMS, or some other tool locally on your workstation and connect to the SQL server instance if your network infrastructure will allow it. If your IT folks are really on the ball, your workstation might not be able to 'talk' directly to the database on the ports it would need. Good to know! Going to try getting it setup from my workstation. I've got the server's administrative login information. &gt; Really depends on your infrastructure. If you are in a small shop, I'd worry about the IT folks giving you full write access to a production database. Do you know what type of rights your account will have? Is there only the one instance of the database, or are their clones, etc? We're a smallish regional branch of a larger organization. As I mentioned, I've got access to admin login, but we outsource most of our server setup and management, so I've probably got a little more access than is safe, hence wanting to take steps to be cautious. I know we've got regular nightly backups in place.
You could do that. Not ideal but a potential solution. If you acquire a 3rd company then you'll be in trouble though, but as an outsider I don't know the odds of that happening. Are both databases on the same server or seperated servers? Because there are other options if they're on the same server.
Microsoft uses this as an official reserved word in the realm of c++... https://msdn.microsoft.com/en-us/library/z54t9z5f.aspx millitm: Fraction of a second expressed in milliseconds. There's 1000ms in a second, so that value should be between 0 and 999. Probably can't be 1000 because that's not a fraction at that point, technically. But where you seeing it in SQL? I'm not an expert at Microsoft's DB, but it's not a reserved word as far as I know. 
Is this MSSQL? Have you looked at LEAD and LAG?
Yes, MySQL. I actually am not familiar with LEAD and LAG
Yes it is in Microsoft SQL Server Studio. It is one of the columns between DateAndTime and Tagindex given to me after I do a query. 
I meant MSSQL, Microsoft SQL Server. It looks like MySQL has something similar depending on the version you're using https://dev.mysql.com/doc/refman/8.0/en/window-function-descriptions.html 
What's your query? Because it's likely a field stored on the table and you'll need to ask whoever created/maintains the database about it.
Ah so use a sub-query? I didn't think about that. I'll research that a bit as I've never used them before, only heard about them
Thank you for the reply. Unfortunately, my syntax does not support INTO #temp statements. [https://prestodb.io/docs/current/sql.html](https://prestodb.io/docs/current/sql.html) This is the syntax I am using. How would your query be adjusted to represent this syntax?
You must use IN when the subquery could return multiple values. The way to know if it will never return multiple values is if the subquery is based on a unique index or primary key. In your final example, the use of = is only appropriate if VendorCopy.VendorName is a UNIQUE index column (which is entirely possible)
Now that MySQL supports (8.0) the rownumber(), you can use everybody's favorite rownumber = 1 method.
That makes A LOT more sense. Thank you!!
Well you could use a CTE. WITH Tempcte AS ( SELECT i.id as invoice_number , i.subscription_id , i.amount_due / 100.0 as amount , date_format(i.date - interval '4' hour, '%m/%d/%Y %T') as invoice_date , date_format(c.created - interval '4' hour, '%m/%d/%Y %T') as charge_date , i.closed , i.forgiven , i.paid from invoices i join charges c on c.invoice_id = i.id ) SELECT CONCAT(t.ID,t.subscriptionid) AS '1' , CONCAT(LEFT(CONVERT(varchar, t.invoice_date,112),4),'- ',LEFT(CONVERT(varchar, t.invoice_date,110),2)) AS '2' , CONCAT(LEFT(CONVERT(varchar, t.charge_date,112),4),'- ',LEFT(CONVERT(varchar, t.charge_date,110),2)) AS '3' , (SELECT LEFT(CONVERT(varchar,t1.invoice_date,110),2) FROM #temp t1 WHERE t1.id = t.id) AS '4' , (SELECT LEFT(CONVERT(varchar,t1.charge_date,110),2) FROM #temp t1 WHERE t1.id = t.id) AS '5' FROM Tempcte t
Our warehouse isn't on 8.0 or above.
I dont have a MySQL instance to test but you might try a correlated subquery with limit 1.
 SELECT ItemCode , (Select ST1.[COMMENT TEXT FIELD] + ',' AS [text()] From dbo.[DATABASE THAT HAS ITEMCODE] ST1 Where ST1.[Item Code] = ST2.[Item Code] --&lt;WHICHEVER TABLE HAS ITEM CODE IN YOUR FROM CLAUSE ORDER BY ST1.[Item Code] For XML PATH ('')) FROM MAS_GFC.dbo.SO_SALESORDERHEADER INNER JOIN MAS_GFC.dbo.SO_SALESORDERDETAIL ON MAS_GFC.dbo.SO_SALESORDERHEADER.SALESORDERNO = MAS_GFC.dbo.SO_SALESORDERDETAIL.SALESORDERNO INNER JOIN MAS_GFC.dbo.CI_ITEM ON MAS_GFC.dbo.CI_ITEM.ITEMCODE = MAS_GFC.dbo.SO_SALESORDERDETAIL.ITEMCODE 
How might that look? What if the lowest positive integer isn't the first record? 
Would `delete from TABLE where Initials IS NULL` not work? Or do you want to only delete the records where `Initials is NULL` **and** there's another record matching where `Initials is not NULL`?
You capture `Origin` in the initiation of your recursive cte, and carry if forward in each iteration. WITH [cte] AS ( SELECT [child].[Key] , [child].[ParentKey] , [child].[ParentKey] AS [Origin] FROM [dbo].[TableName] AS [child] WHERE [child].[ParentKey] IN ('A') UNION ALL SELECT [child].[Key] , [child].[ParentKey] , [parent].[Origin] FROM [cte] AS [parent] INNER JOIN [dbo].[TableName] AS [child] ON [child].[ParentKey] = [parent].[Key]) SELECT * FROM [cte]; http://sqlfiddle.com/#!18/6a4be/4
Thank you! Will take a look
This is exactly what I was looking for. Thanks you!
A correlated subquery will look like another (select...) in your main query. You should be able to add whatever conditions you need in subquery.
Can you provide more details of what you are actually trying to do? Generally, a MERGE is just an INSERT when the row doesn't exist, combined with an UPDATE where it does and can easily be rewritten as such. 
Sorry, I know how a subwuery looks. What I'm unsure off is the logic on how to only select what I menttioned in the op
Gee. I'm making a suggestion on how "to find the closest job title an employee had to when a ticket was created". If you need to find the smallest value of X, use min(X)
Here is a pastebin of what I have when I'm creating the view after added your above sub-query. When I select top 1000 rows for the view, I receive an error saying the sub-query is returning more than one record, which I would think would be what I want. It wouldn't let me use the ORDER BY since I'm creating a view https://pastebin.com/wb2LM3Ju 
I've found an SSIS lookup is a clean way to route inserts and updates appropriately. The updates can even pick up a clustered index surrogate key in the lookup (for a more efficient update).
Good explanation! Just a small clarification to prevent misunderstanding: It is not required to have a UNIQUE or PRIMARY KEY constraint on the column to be able to use the "="-operator. The query also works on column without these constraints, as long as the data itself allows it (i.e., only one row fulfills the WHERE condition). In all cases where multiple rows fulfill the WHERE condition the query will instead fail and return an error. The UNIQUE/PRIMARY KEY is simply a way to make sure that the query never returns an error instead of a result. However, there can be situations where getting an error (and correctly handling it afterwards) is preferable.
Yes, what I probably should've said is that it's only guaranteed to be safe (from error states) to use = under those conditions
You left out this part from the sub select: ORDER BY ST1.[Item Code] For XML PATH (''))
&gt;Would delete from TABLE where Initials IS NULL not work? Only because there are some records where it's ok for Initials to be NULL and there aren't duplicates. &gt;Or do you want to only delete the records where Initials is NULL and there's another record matching where Initials is not NULL? This would be ideal. &gt;Are you allowed to have a record where Initials IS NULL and there are no other records for that Code? Yes. Thanks!
Hey - welcome. I wouldn't worry too much about the flavor tag - maybe move it up earlier in the post with an edit - but I very rarely see it used. I think you've psyched yourself out and are over-complicating things. This is solved with a simple sub-query. I made a little test for it: create table #worksegment( employeeid int ,timein datetime ,timeout datetime ) create table #employee( employeeid int ,name nvarchar(100) ) insert into #employee values (1, 'A') ,(2, 'B') ,(3, 'C') insert into #worksegment values (2, dateadd(hour,-2,getdate()), getdate()) ,(1, dateadd(hour,-4,getdate()), getdate()) ,(2, dateadd(day,-1,dateadd(hour,-2,getdate())), dateadd(day,-1,getdate())) ,(3, dateadd(day,-1,dateadd(hour,-4,getdate())), dateadd(day,-1,getdate())) Then it's a simple NOT IN select * from #employee e where e.employeeid not in (select eToday.employeeID from #employee eToday join #worksegment wsToday on eToday.employeeid = wsToday.employeeid where (cast(wsToday.timein as date) = cast(getdate() as date) or cast(wsToday.timeout as date) = cast(getdate() as date)) ) I went ahead and used the current day, but you can change that by changing the "cast(getdate() as date) to the date you want. Sorry I didn't get to format it as well as I would have liked, a little rushed. I hope this gets you on the right track and gets you what you need! Good luck, and feel free to ask questions.
Good response. Now I'm gonna complain about this book teaching this way and giving these examples. I think instead of = or IN this book should be recommending an INNER JOIN. Students like OP could easily be confused on which syntax to use and why they couldn't use = this time, especially depending on how much access or knowledge of the design of the tables being used. INNER JOIN always would work (like IN would) but I feel makes more sense to read (personal opinion) and is less lines of code.
I made some assumptions here but this should get you started. Assumption 1: If there are duplicates with all NULL initials delete only one of them. Assumption 2: If there are duplicates with multiple initials don't delete any of them I had to do the delete in two parts, not sure if someone could come up with a better way but here's a start... IF OBJECT_ID('tempdb..#your_table') IS NOT NULL DROP TABLE #your_table CREATE TABLE #your_table ( ID INT IDENTITY (1,1), Code VARCHAR (10), Initials VARCHAR(3) ) INSERT INTO #your_table (Code, Initials) VALUES ('ABC123', 'XYZ'), ('ABC123', NULL), ('CBA123', NULL), ('CBA123', NULL) SELECT * FROM #your_table AS yt -- This will delete any duplicates where one of them has initials DELETE yt2 FROM #your_table AS yt INNER JOIN #your_table AS yt2 ON yt2.Code = yt.Code AND yt2.Initials IS NULL WHERE yt.Initials IS NOT NULL -- This will delete any duplicates where both of them have NULL initials DELETE FROM #your_table WHERE ID IN ( SELECT MAX(yt.ID) FROM #your_table AS yt WHERE yt.Initials IS NULL GROUP BY yt.Code HAVING COUNT(*) &gt; 1 ) SELECT * FROM #your_table AS yt DROP TABLE #your_table If there are multiple duplicates you would need to run this multiple times until all of your dupes are gone.
A full outer join is effectively the same. If the new data is null then it's been deleted. If the old data is null it's new. If both sides are populated but different data is updated. If both sides are populated and identical there is no change
Gonna post another comment on here in case you just want to get what you have working ASAP. It looks like you just have your selects backwards - your datepart check needs to be in the not exists part - you want all employees that don't exist in the list of employees that clocked in today. You're selecting a list of employees that worked today that don't exist in the list of all your employees. Swap your WHEREs and you should get the results that you expect.
I added that and it worked! Wasn't exactly what I wanted so I modified it using SALESORDERNO instead of the item code and that did what I was looking for. You are a lifesaver! Thank you so much. For anyone else that is wondering: (Select ST1.COMMENTTEXT + ',' AS [text()] from MAS_GFC.dbo.SO_SALESORDERDETAIL ST1 where st1.SALESORDERNO = MAS_GFC.dbo.SO_SALESORDERHEADER.SALESORDERNO ORDER BY st1.ITEMCODE For XML PATH ('') )COMMENTS
I'm essentially just writing a MERGE statement, but under the circumstances we have been told to not use the MERGE statement to perform the same actions
I would have to Join on all fields to detect changes right?
Do you have data already that needs to be queried, or is this all greenfield? If the latter, what version of SQL Server are you running? Have you considered writing the changes to a history table instead of piecing it together via queries?
Unfortunately, we are not using Ssis:)
It already exists in production, and has for many years. Unfortunately my hands are tied regarding table structure and architecture. I have a dynamic SQL script that does it, but it's hardly effective (takes 2+ minutes to run). I've made it this far: SELECT * FROM TableName t1 WHERE NOT EXISTS -- eliminate sequential records ( SELECT 1 FROM TableName t2 WHERE t1.Company_ID= t2.Company_ID -- same company AND t1.NewAddress_ID = t2.OldAddress_ID -- next address in sequence AND t1.ID &gt; t2.ID -- IDs are sequential ) This will get all the starting points I need for the entire table. The next step would be to recursively self join X times, as needed, to the table for each starting point I have. 
So I simplified it these ways: Select * from tcp_Employee.Employee Where not exists ( Select tcp_EmployeeWork.WorkSegment.EmployeeRecordId from tcp_EmployeeWork.WorkSegment WHERE ((datepart(yy, tcp_EmployeeWork.WorkSegment.TimeIn) = 2018) and (datepart(mm, tcp_EmployeeWork.WorkSegment.TimeIn) = 07) and (datepart(dd, tcp_EmployeeWork.WorkSegment.TimeIn) = 06)) ) Zero results and Select * from tcp_Employee.Employee Where tcp_Employee.Employee.RecordId not in ( Select tcp_EmployeeWork.WorkSegment.EmployeeRecordId from tcp_EmployeeWork.WorkSegment WHERE ((datepart(yy, tcp_EmployeeWork.WorkSegment.TimeIn) = 2018) and (datepart(mm, tcp_EmployeeWork.WorkSegment.TimeIn) = 07) and (datepart(dd, tcp_EmployeeWork.WorkSegment.TimeIn) = 06)) ) 74 results I chose the date 2018-07-06 because I know the correct answer for that date is 1 result. Where did I misinterpret your instructions?
To add to this, IN combined with sub-queries can result in sub optimal execution plans in some cases. INNER JOIN is a better suggestion, but consider that it gives you access to the columns in the joined table. This is particularly useful in cases where you want to output or join using these columns, however in the example above, a better pattern would be EXISTS(). EXISTS is more flexible than IN as it allows you to use any number of expressions whereas IN is limited to a single set of values, but more importantly, it "shortcuts" in the optimizer as soon as it finds a match, which saves resources over an INNER JOIN. In your example: UPDATE ic SET ic.TermsID = 2 FROM dbo.InvoiceCopy AS ic WHERE EXISTS (SELECT \* FROM dbo.VendorCopy AS v WHERE ic.VendorID=v.VendorID AND v.DefaultTermsID = 2);
haha yeah I will definitely be using a better type. Anything else that you would change?
Without more insight into your structure the best that I can offer is to select distinct tcp_Employee.Employee.RecordID in your 2nd one, and see how many show up. You're saying you have 1 employee that didn't work on 7/6/2018? Or is there some other flag we need to set that we're missing. Is there some flag saying who *should* be working on a day?
Is this a homework problem? One way to tackle the change detection part of the problem is SELECT * FROM source EXCEPT SELECT * FROM destination However in most real world cases you'd avoid doing it this way because comparing two full tables like that is quite costly.
I'm not terribly sure what you are trying to do here. Maybe I'm just slow after lunch.
Also, a sub query can only return a single value expression for any comparison operator (I.e. &lt;, &gt;, &lt;&gt;, etc.)
SQL that is executed needs to know how many columns will be there and you can have a variable number. What you can do is 1) take a maximum number of changes 2)generate a dynamic SQL for pivoting your data to the required number of columns 
SQL92 I believe? Database manager would be PrestoDB mostly but for simplicity just can think it would be MySQL. Do you happen to see anything in the design you would change?
Can't you use All? I've only seen it done once so I don't remember the specifics of how it works. 
 Select distinct RecordID from tcp_Employee.Employee Where tcp_Employee.Employee.RecordId not in ( Select tcp_EmployeeWork.WorkSegment.EmployeeRecordId from tcp_EmployeeWork.WorkSegment WHERE ((datepart(yy, tcp_EmployeeWork.WorkSegment.TimeIn) = 2018) and (datepart(mm, tcp_EmployeeWork.WorkSegment.TimeIn) = 07) and (datepart(dd, tcp_EmployeeWork.WorkSegment.TimeIn) = 06))) Returns the same 74 results.
I was also thinking a NOT IN would work. SELECT [WHATEVER] FROM tcp_EmployeeWork.Employee emp WHERE emp.RecordID NOT IN ( SELECT work.EmployeeRecordID FROM tcp_EmployeeWork.WorkSegment work WHERE [DATE FILTER STUFF] )
No, it's a real situation we are in. I've always used Merge for insert,updates,deletes but we have orders to not use a Merge statement. 
While this is logically true, you'll still need 3 separate statements to do the DML: Delete, Insert and Update. In that case, using the FULL OUTER JOIN as input for all 3 statements is inefficient - you are better off just tuning each statement based on it's needs. UPDATE A SET A.\[Columns\]=B.\[Columns\] FROM dbo.TableA AS A WHERE EXISTS(SELECT \* FROM dbo.TableB AS B WHERE [A.ID](https://A.ID) = [B.ID](https://B.ID) AND HASHBYTES('sha1', (SELECT A.\[non-key, not meta data columns\] FOR XML RAW)) &lt;&gt; HASHBYTES('sha1', (SELECT B.\[non-key, not meta data columns\] FOR XML RAW))); \-- Provided you specify key columns of clustered index, statement will be SARGABLE. HASHBYTES will consume some CPU but this solution should perform against even large data sets INSERT dbo.TableA (\[columns\]) SELECT \[columns\] FROM dbo.TableB AS B WHERE NOT EXISTS(SELECT \* FROM dbo.TableA AS A WHERE [A.ID](https://A.ID) = [B.ID](https://B.ID)); DELETE A FROM dbo.TableA AS A WHERE NOT EXISTS(SELECT \* FROM dbo.TableB AS B WHERE [A.ID=B.ID](https://A.ID=B.ID));
The ALL means that the entire column returned by the subquery must satisfy the comparison operator.
So you're trying to show which employee didn't clock in that day, right? Try this: SELECT tcp_Employee.Employee.FirstName ,tcp_Employee.Employee.LastName ,tcp_Employee.Employee.EmployeeId ,tcp_Employee.Employee.WorkStatus ,tcp_Employee.Employee.RecordID FROM tcp_Employee.Employee LEFT JOIN tcp_EmployeeWork.WorkSegment ON tcp_Employee.Employee.RecordId = tcp_EmployeeWork.WorkSegment.EmployeeRecordId AND tcp_EmployeeWork.WorkSegment.TimeIn &lt;= '06/07/2018' AND tcp_EmployeeWork.WorkSegment.TimeIn &gt; '06/08/2018' WHERE tcp_EmployeeWork.WorkSegment.EmployeeRecordId IS NULL
By the way, when using exists or not exists, you don't actually need to select anything. You can just say where not exists (select 1 from ... ) where exists (select 1 from ... )
So you need to do a row number on existing numbers and order by initials. Then you delete anything that is &gt; 1. Something like this: IF OBJECT_ID('tempdb..#your_table') IS NOT NULL DROP TABLE #your_table SELECT [ID] , [Code] , [Initials] , ROW_NUMBER() OVER(PARTITION BY [Initials] ORDER BY [Initials] DESC) AS Del INTO #your_table FROM view_items ORDER BY [ID] DELETE FROM [WHATEVER] what WHERE what.INITIALS IN (SELECT yt.Initials FROM #your_table yt WHERE Del &gt; 1 AND yt.CODE = what.CODE AND yt.ID = what.ID AND yt.Initials = what.Initials)
I'm sorry for asking again but I got one other simple question, if you don't mind: In a case like this SELECT name from world where area&gt;ALL (SELECT area from world where continent='Europe' and area&gt;0) Isn't the subquery being evaluated once for each row in the main query? Why is this not considered as a correlated subquery whereas the example in the OP is?
I'll test it out later on. I've got school to go through first. 
How many employees are there? How many worked on that 2018/7/6? Run your sub-query, how many results do you get? Is that how many you expected to have been working that day? Also: I'm not a big fan of the way that you're checking your date...I think you're scanning the table 3 times. You also could have issues of overnight shifts in the future. I think you should go ahead and get that changed before it rears its head later. I'd change it to WHERE (cast(tcp_EmployeeWork.WorkSegment.TimeIn as date) = '7/6/2018' or cast(tcp_EmployeeWork.WorkSegment.TimeOut as date) = '7/6/2018') I don't have the exact column name of TimeOut - that might be wrong, but this checks whether either the TimeIn or the TimeOut fall on that day, fixing overnight shifts.
Use DATETIME or TIMESTAMP. There are nuances to which one to use depending on whether you want to store time zone information in the field, so read up on how they work.
In this case the subquery runs just one time. The database temporarily caches the subquery results and compares those results to the rows in the main query. The reason this is not a correlated subquery is that there is nothing tying (correlating) rows together from the two result sets. There is no comparison like x.continent = y.continent. The subquery you posted here can run completely independently of the main / outer query. And in fact the database will run it just once.
There are a number of issues with your queries 1. The alias of the column should appear after the column item is selected (e.g. SELECT item alias). EX: SELECT SUM(balance) balance FROM accounts; 2. CASE statements require a THEN condition (e.g. SELECT CASE WHEN item CONDITION THEN returnthis 3. Count is a function of SQL and thus requires parentheses afterwards. As such, if you are counting Distinct Items it should appear as count(Distinct item) 4. In terms of getting your items into individual columns, you need to put a comma before each sum to declare a new column. Right now your fourth query will generate an error because you only have 2 columns AND the alias in the wrong place. It should look like SELECT balance, SUM(.....) Under_30_days_past_due, SUM(....) 30_to_60_days_past_due, etc.
So there are 134 employees, of which 66 worked that day, 1 did not and did not call in, and the rest were on vacation days. I changed the date selection as you suggested and the results are still the same but I imagine the execution time must have dropped by 1/3 since there is one less statement to validate.
Similar results to what I already am getting. Lots of people that clocked in but not the ones that didn't clock in. :(
thank you, thank you, and thank you. 
I don't think I ever use `=` I always use `IN`, even if I only expect one result. I don't think I have ever compared the query plans though. Is there a big impact?
Like any function, it's not a good idea to use it to wrap a column in your predicate because it makes it non-sargable. If you aren't familiar with what "sargable" means you can google it.
I would recommend a `eToday.employeeID is not null` in that query's WHERE clause. NOT IN does not mix well with NULL values. 
Can’t you just push a button?
This has been my life lately. [Started a job a while back and my first week was a nightmare](https://i.imgur.com/e5dWc0k.png). 3 months in and I'm about ready to claw my eyes out. You can't make shitty tables faster. In fact, shitty tables run really fucking slow doing some of the most easiest queries. Unless your table has less than a hundred rows, it should have a clustered index and primary key. Your data should never be denormalized with the exception of if you are bulk loading into a table that has been truncated and reloaded to be processed in some ETL process. Not every field needs to be nvarchar(4000) and nvarchar(max). Use proper datatypes. Use unique key constraints. Use foreign key constraints.
Hey! That ETL flow diagram for our EDW is proprietary!! How'd you get it!
Oh look, it's our EDW.
give us a hint, please... maybe show the four tables?
I'd love to say it's just the EDW. It's production tables. I about had a mental break down when asked to break the granularity of a table that is periodically truncated and reloaded that is completely denormalized. Support:*"Take nth row and cross apply this"* Me: *"Why?"* Support:*"We need to have this comma seperated value as one value per row"* Me:*"Why?"* Support:*"So it renders on the front end as such"* Me:*"Why?"* Support: *"Because the front end guy wants it that way."* Me:*"You are telling me you want me to change the ETL process to cross apply this field after I split it against the entire source because the frontend guy can't split this field on the front end?"* Support: *"Yes."* Me:*"Are there any reports against this dataset?*" Support: *"Yes."* Me: *"How are we supposed to report off this dataset after breaking granularity?"* Support: *"Is that going to cause problems?"* Me: *"Yes"* Support: *"Can't we just fix the reports to accomodate?"* Me: *"I'm not answering that question because it's 10000% more work to do that then the front end dev just doing his mother fucking job and splitting the field. WE SHOULDN'T EVEN BE STORING THE VALUES AS COMMA SEPARATED VALUES! WTF PEOPLE!"* Support to development manager: *"/u/Thriven is advising against making these changes."* Dev Manager: *"Why can't we just do this?"* I'm losing it man. 
I'm relatively new to SQL as well. What I might try to do is write a subquery with your conditions built in and then join on the subquery. But like someone else mentioned it depends what your conditions are and how your data is. 
Yes, you can use ALL and ANY with the comparison operators &gt;, &lt;, &lt;&gt;, = etc.
Couldn't you just do SELECT e.FirstName , e.LastName , e.EmployeeId , e.WorkStatus , e.RecordID FROM tcp_Employee.Employee e WHERE NOT EXISTS (SELECT 1 FROM tcp_EmployeeWork.WorkSegment ws WHERE ws.EmployeeRecordId = e.RecordId AND CAST(ws.TimeIn AS DATE) = '2018-07-06') ?
In addition to what everyone else here said, you could theoretically make = work like this: UPDATE InvoiceCopy SET TermsID = 2 WHERE VendorID = ( SELECT TOP 1 VendorID FROM VendorCopy WHERE DefaultTermsID = 2) I think the way I'd do this myself is: UPDATE ic SET TermsID = 2 FROM InvoiceCopy AS ic JOIN VendorCopy AS vc ON ic.VendorID = vc.VendorID WHERE vc.DefaultTermsID = 2; This uses the join to limit the affected update rows, so basically you're building a query that returns all rows you want updated, then updating the value in a column for that entire query.
The tables are huge, and they deal with different aspects of items sold by the company. I'm just trying to write a query that can join the pertinent information. I guess my question was just asking I'm general... Are if statements even a thing? Can cases be used to execute a joint, or to not execute one? Sorry if not being helpful.
But how are those conditions built in? For instance, in table one there is a column that let's call priceID. If priceID is zero, then I don't want to join that element (with primary key ItemID) with the price table, because it won't have a row there. So that's my condition, if a column entry is zero, but I'm not sure how to execute a subquery based on a condition.
SELECT * FROM table WHERE priceid &lt;&gt; 0
No that's not what I mean. So given a ItemID say (900100) I query table one for this ItemID, also retrieving the priceID. What I would like to do is join this resulting query with a query from the price table ONLY if the priceID is non zero. Hopefully I'm making sense. Your query is retrieving all items with priceID nonzero, but I'm getting info about a single object and joining it with another table if a condition is true. I just don't know how to execute a join if and only if a condition is true. The reason why I'm doing joins is eventually there will be multiple items with the same itemID, so a join will be necessary.
You write "I only want the first one to execute if a condition is met (based on the data type in one of the columns) ". Are you sure you mean data type, instead of value? Because the data type (Int, Date) of a field won't change... If you're looking for some very hands-on "learn-by-doing" practice problems, that teach basic to advanced SQL with well-designed, real-world practice problems, similar to what you're trying to solve, check out [SQLPracticeProblems.com](https://sqlpracticeproblems.com/#utm_source=r1&amp;utm_medium=r1&amp;utm_campaign=r1). I developed it after teaching a SQL course where the material I had to to teach from was poorly structured and academic. Afterwards, the students emailed me, saying they needed practice problems, so I developed the course! Contact me (email in the FAQ) if you have questions. Also, redditors get 30% off the Professional package (discount code reddit30). 
Yes I do mean value, sorry about that. Thanks for the offer but I don't want to pay for a course right now.
I’ve only used if statements to see if a table exists before trying to truncate it. I’ve never heard of using it for joins. Not sure if I understand completely, but you can join using multiple conditions. ie: inner join on a.id = b.id and (a.price &gt; 100 or b.quantity &gt; 5). This would be identical to creating a cte ... ;with a as (Select * from a where price &gt; 100) , b as (Select * from b where quantiy &gt; 5) Select * from a inner join b on a.id = b.id I would do a quick QC to make sure, but this should be right.
Search YouTube for “the expert” ...why didn’t you try transparent ink?
I see what you're saying about multiple conditions on the inner join, but is there a way to not do the join at all? Say for instance I am executing joins A, B, and C across four tables, but I only want to do join A if a condition is met (based on a value in the first table, which would cause there not to be a row in the second table, so the whole chain of joins would become null because the first join returns null), but I always want to execute joins B and C based in the data from the first table (regardless if the data from table 2 is joined in or not).
Maybe I'm not understanding. What end result are you trying to achieve? Based on this explanation it sounds like you could write your whole query and just say at the end WHERE priceid &lt;&gt; 0
join B and C first, using inner joins, then join A using left outer join
The end result is a table that is generated by an itemID. That table is generated by a sequence of joins after querying 4+tables. One of those joins needs to only be executed if a condition is met, which I detailed above. There are thousands of items that had nonzero priceIDs, but I'm writing a script that takes a few itemID as an argument and executes this series of joins. One of those joins only occurs if that specific itemID has a nonzero priceID. Sure I can do where and get that priceID, but I'm trying to program a sequence of joins.
Ohh okay that's an interesting idea, thank you. Didn't think about it like that. I'll try it.
I feel pretty confident that in T-SQL I could get that exact result by just adding the WHERE clause at the end unless I am seriously missing something. At the end of the day you're trying to exclude a value from a query so your joins will create your data set for you and your where clause will add conditions on what you want to be displayed at the end of your dataset. 
Sounds like this shop is a fucking joke. Keep job searching and jump ship ASAP.
For the curious.... source of photo: [Miracle as forklift truck driver walks out of factory unhurt after being trapped under tons of CHEDDAR CHEESE for nine hours after massive shelving collapse](http://www.dailymail.co.uk/news/article-3576799/At-one-person-missing-warehouse-collapsed-Shropshire.html)
I swear if I didn't know any better I'd think you were talking about my environment.
Ok so let's have racks as tall as the building will allow, because we don't want to bother checking the height of the boxes. Also, checking for duplicate serial numbers on the boxes is too hard so don't bother. Remember, don't pack the boxes tight, shelve the boxes so they take up the most room on the shelves!
nvarchar(max) is a SQL meme.
Check out Brent Ozars website. His stuff is available online and I’ve found his classs to be pretty informative. I’m pretty big into getting SQL books and just reading through them too. Itzik Ben-Gan and Ben Nevarez are both really good to read.
I realize that every table should always have a primary key, but I'm having trouble making one for the Results table. It is for a school assignment. I think I'm pretty close, but it just seems a bit off to me. Some advice would be greatly appreciated, here is the assignment: Design a database for a bicycle race, which will have riders from various countries (you’ll need a country table). Each rider will also be on a team. The teams will have a name, a sponsor, and a country. The riders on any given team may be from any country, not necessarily the same as the team country. The overall race consists of a number of stage races, each with a name, date, start and end points, and total distance. Each stage will also have a type (mountain, sprint, time trial …) so you will need a table for stage types. And of course we’ll want to store the results of each stage, so we’ll need times for each rider in each stage. And we’ll want enough info to determine to the overall winner based on lowest cumulative time, the team with the lowest cumulative time, and overall young rider winner (the rider under a certain age with the lowest cumulative time). 
How many minutes before you eat some cheese? I'd say like 15 min max. 
Well said!
Just skip the SELECT and only do the UPDATE. If you are doing it this way because you need a value from the row you are updating, read up about [RETURNING](https://www.postgresql.org/docs/current/static/dml-returning.html)
For the Results table, you have two main options for the Primary Key: - Create a composite key (a primary key consisting of more than one column) using Rider and Stage. Both columns together make up the key. - Add a new unique column like ResultID that will be the primary key column. There are some advantages and disadvantages to each, and some people have preferences for one method over the other.
Cool. I saw it once when I took a more formal online course in sql (I learned everything I know on the job from Google and ready NG other people's stuff), but I never really needed to use it. Wasn't sure if I were remembering it correctly. 
Hey, thanks for the input. Yeah, I was actually about a ResultId on there. I’ll probably go with that. What about the CumulativeTime fields in the Team and Rider tables. Do you think it makes sense to have them there, or would another, maybe Cumulative table make sense?
I would recommend against having any cumulative time columns. I think that cumulative times should be ascertained at run time when that query or report is executed.
Yeah, that makes sense. Thanks for your help!
Can you make a view like you did before but with all of the big data databases in it?
From the sidebar: *If you are a student or just looking for help on your code please do not just post your questions and expect the community to do all the work for you. We will gladly help where we can as long as you post the work you have already done or show that you have attempted to figure it out on your own.*
 1. Prepare a list of offices sorted by country, state, city. - No u. 2. How many employees are there in the company? - too many to count. 3. What is the total of payments received? - 1 million dollars. 4. List the product lines that contain 'Cars'. - Automobiles. 5. Report those payments greater than $100,000. - Dr. Evil. 6. List the products in each product line. - Make me. 7. How many products in each product line? - 12 8. What is the minimum payment received? - SELECT * FROM your_house WHERE your_mom IN 'me' 9. What is the average percentage markup of the MSRP on buyPrice? - 300% 10. How many distinct products does ClassicModels sell? - Chevy 
You need a table for Race, with a foreign key to Stage. CumulativeTime probably shouldn't be on Rider or Team. You say you need enough info to determine CumulativeTime, but that can be done with a query rather than being stored and updated.
The CumulativeTime makes sense, I will take it out of my database. I’m open to your suggestion, but what do I need the Race table for that isn’t already in the Result table?
Your default schema is pointed to 'text'. Based on your output log, the Employees table resides in the ClassicModels schema. If you do: SELECT COUNT(*) FROM ClassicModels.Employees; What do you get?
&gt; Design a database for a bicycle race Oh, I see. The entire database is single use for one race. You'd never do that in the real world, but it wouldn't be an academic problem if it tried to be practical. :P
Oh my god how i forgot about this video. So good.
Rather answer this - how do you store results of the same course (2 different tries) in your structure?
6 years, 3 jobs. I feel like this is just the way it is.
+1 for exists, but instead of * I always put a fixed value like a 1.
The ship you're looking for is currently not available, please leave a message.
I don't see how that's possible; you'd either be getting nobody(suggesting there's a record of some kind for non-clocked-in employees) or only the employees with no record for the day in question. We're doing a left join and the WHERE clause says: only return records where the left table's id is NULL, i.e. the join found no match. Adding date criteria to the JOIN limits the left table to the date you want. It definitely won't return *more* records than you want unless there's something awry with the way your db is designed.
SELECT Replace(.... FROM dbo,blah Is generally Ok. select.... from table1 t join table2 t2 ON replace(somecolumn....) Or Select * from table t where replace(column) Are both generally best avoided if possible due to non-sargable reasons.
Thanks very much for the reminder. I had forgotten about RETURNING. But that won't help with what I'm working on now. I have a task that must be performed, but there are two different ways to select the records that will be inputs to the task. So, I do something like this: CREATE FUNCTION do_something_from_integer(integer) begin SELECT input_record FROM input_table WHERE some_integer_field = $1; PERFORM some_task_using_input(input_record); end; CREATE FUNCTION do_something_from_text(text) begin SELECT input_record FROM input_table WHERE some_text_field = $1; PERFORM some_task_using_input(input_record); end; CREATE FUNCTION do_something(input_table) declare calculated_value integer; begin calculated_value = calculate_value(input_table.input_field); update input_table set output_value = calculated_value where input_key = input_table.input_key; 
Wow, first thing I thought was, "I hope nobody is trapped under all that." Can't even see where the truck is buried.
In order to do that, I suppose I would make an Attempt field in the Result table, but the design is supposed to be similar to the Tour de France, so there wouldn’t be multiple attempts in this particular schema
Thanks for your input. :-)
Well doing it in two steps seems unavoidable then based on that design. But if the concern is efficiency due to having to do two lookups, you could capture the ctid of the row you selected so that you can use it in the update.
At least in MS SQL Server, that advice is moot. Years and versions ago, the advice of the SQL CAT (customer advisory team) was to use * in this instance alone because it gave the optimizer the most options for indexes. That has since become unnecessary because SQL ignores whatever you put there anyway. If you choose to use a literal/constant, it's basically just a style decision. Regarding other DBMS systems, I can't speak. 
We have an OLAP SQL Server database that is black-boxed to us and is used as a 'proprietary' reporting database for a financial application we have. In total, it's size is around 600GB. We are running it on 12 cores and 128GB of RAM and the company who built it is still blaming resources as the cause for slowness during the ETL processes on it. :(
What's got you stuck?
It really depends on where you work or personal preference. I have two team leads over my department and each has his own preference to how our code is standardized. I personally like the looks of [Field] vs "Field" but since I also work in Vertica it makes more sense for me to use "Field" due to Vertica not recognizing brackets.
Honestly just piecing everything together. I barely understand the material. It's an online class so I don't do well with that. I'm trying to focus and use the text book but I'm having a hard time focusing.. Just had a death in the family so my mind is a bit scattered (not to get too personal, but you know) 
So short answer, nothing specific lol... Just the whole thing. I need my hand held for this one and I'm willing to pay for it xD
Sorry for your loss. Unfortunately I'm not much on DBMS so I won't be able to help. I was just looking to get some context on your issue in case someone else came in and could give you some help. 
 SELECT * FROM tcp_Employee.Employee emp WHERE emp.RecordID NOT IN ( SELECT work.EmployeeRecordID FROM tcp_EmployeeWork.WorkSegment work WHERE (cast(work.TimeIn as date) = '7/6/2018' or cast(work.TimeOut as date) = '7/6/2018') ) Still coming up with 74 records on this one.
&gt; SELECT work.EmployeeRecordID Try SELECT DISTINCT work.EmployeeRecordID. 
I just wanted to follow up: I went into work and tried this first thing, and it worked like a charm. Thanks again. Now I see the benefits of those kinds of joins.
Same 74 results as all the other variants. :(
The same 74 records.
Are you sure that's not the right answer?
I'm willing to take a look with over the assignment in more of a tutoring sense. I will not do the work for you, however. Shoot me a PM if you're interested.
Are you seriously asking people on the internet to do your homework for you in exchange for pay? Doesn't this fall under certain definitions of academic fraud?
It seems like you should be able to do something like this: SELECT "ID" , "ITEM" , "DIVISION" , "SOMEFLAG" , "PRICE" INTO #temp FROM [TABLE] SELECT [ID] FROM #temp WHERE "DIVISION" = 2 OR "SOMEFLAG" = false
The problem with that is the cases where I don't have division 2 or someflag = false, or if I have rows that have one but not the other. The prioritization falls through.
So you want Division = 2 , SomeFlag = false and then whatever else that doesn't have either or those as long as its 1 row per record?
The highest priority is 1 row per item. The "preferred" rows are "division = 2" and "someflag = false"
Make granularity guide you: 1 row per item: select main_T.item from &lt;table&gt; main_T group by main_T.item if the item exists in division 2: max( case when main_T.division = 2 then main_T.id) d2_id if there's a record with someflag = false: max( case when main_T.someflag = false then main_T.id) f_id otherwise we dont care: max( main_T.id) nvm_id get the records: join &lt;table&gt; another_T on another_T.id = coalesce( d2_id, f_id, nvm_id) 
Yes, look up linked tables in Access
Check out the following post on StackOverflow: https://stackoverflow.com/questions/14523816/load-data-infile-variable-into-infile-error
I'm still not sure. I'm getting that file from another website and it's usually in the format of input_20180726.csv after it's unzipped.
 SELECT "ID" , "ITEM" , "DIVISION" , "SOMEFLAG" , "PRICE" INTO #temp FROM [TABLE] SELECT [ID] FROM #temp WHERE "DIVISION" = 2 OR "SOMEFLAG" = false UNION ALL SELECT [ID] FROM #temp WHERE [ID] NOT IN (SELECT MIN([ID]) AS [ID] FROM #temp WHERE "DIVISION" = 2 OR "SOMEFLAG" = false ) ORDER BY [ID]
Hello ichp. I appreciate you answering my question. What do you mean by "look up linked tables in Access"? or How would I do that? Thanks
Google it
Got it. Thank you.
Yeah we are pushing 300gb but it's because everything stores values along side keys. Pkey Fkey1 Fkey1 value Fkey2 Fkey2 value Fkey3 Fkey3 value Keep in mind, they are all guids stored as varchar (50). These are foreign keys but there arent any foreign key constraints. The foreign key values are just one property on that foreign table and none of them are consistent. Reports are all written using these as identifying criteria. The values are stored as nvarchar (4000) so indexing these makes for huge slow indexes because what should be a 50mb index turns into a 1.3gb index. Repetitive tables because they are so huge and bulky and unmanaged heaps with no indexes they must store data in chunks. dbo.Object &lt;- new records written from app dbo.Object_Live &lt;- days worth of records from app dbo.Object_Hist &lt;- cold storage because once it hits this massive forget reading it ever again. Everything is a heap. All processes take hours to run. I have been told ,"this is big data, it's going to be slow no matter what." My goto is ,"go to Google, type something into that search bar. How long did it take to retrieve search results over googles petabytes of storage? You can't even join two 500mb tables because they are basically spreadsheets stored on a sql server." When you treat your data like garbage it will be garbage.
My guess would be SHA('password') is your issue. 
I agree. I would do 3 separate statements for completing the operations, however they just asked to identify the different states without a merge in which case a full outer join with a case in the select would give you a status.
Do you know how I would make it work with that statement or a statement of the sorts? The professor wrote that part and I’ve never inserted a encrypted password. This is supposed to allow values to be inserted from a java program. Do you think that ‘SHA(‘password’) ‘ is giving me an error because I’m doing it in sqlworkbench instead of java? I was just trying to test my triggers and sorts
case inside max! Now that's something I've never seen. #TIL o7
 ; with base as (select * , case when division = 2 then 0 else 1 end Sort1 , case when someflag = 0 then 0 else 1 end Sort2 from t) , ranked as (select * , row_number() OVER ( partition by item order by sort1, sort2) rw_num from base) select * from ranked where rw_num = 1 
I'm not familiar enough with Java or SQLWorkBench to answer that. The syntax is causing the error because you have part of it between '' and not all of it. If the whole password is "SHA('password') then you are going to have to do something about the ''.
is insert into user values('NameHere', SHA('password'), '23', 'male', '847-222-3854', 'basketball', TRUE); the actual line or did you replace the actual data with generics? Because when I test it as it is now in sqlfiddle set to MySQL 5.6 it works perfectly.
That’s really strange because the only thing I changed is the name field but I made sure to keep it that exact format :(
can you post the actual line you tried to run? also can you try running the line you posted here?
I don’t know if you’ll notice my last edit so I’ll comment so you see :)
Thanks for the advice. I figured out it was an error in my trigger causing this to occur. Thank you though, I appreciate it!
what did you change to resolve it?
&gt; If the whole password is "SHA('password') ... SHA() is a command... you never put a command in ''
You could do: SELECT *, CASE WHEN tbl.Foo = 'bar' THEN 1 END as CasedColumn FROM tbl You'd get everything in the table, plus an additional aliased column at the back end with your case logic. 
This. But I want to add that it is usually best practice not to use SELECT * and to list out every column.
If your table definition isn't going to change, you can do the right-click - select top 1000 thing to have the system generate a query that names every column, and then just adapt that. If you want one that changes when the table definition changes, and you're okay having cased and uncased versions of the same column, you can do what Mindflux mentioned. Remember that you need to give your case column a different name than your source column. If you want one that changes when the table definition changes, AND you don't want the un-cased version of the cased column, it's technically still possible but more annoying. You'd need dynamic SQL that gets the column names from the information schema and loops over all of them, returning straight selects of those columns unless they're one of your exception columns, in which case you return your case statement instead.
You can learn the Syntax quickly but learning where to use it needs experience
Sharepoint has the ability to connect to a SQL Server table and allows for the filtering of the table from the site via External Content Types. Never set anything up like this myself, just know it exists. Perhaps that's the direction you're needing. 
Awesome. Will take a look - Thanks!
Microsoft's standard tool for this is SSRS, which is essentially a web server that runs reports that you define. You can save a report on a variety of formats, including CSV. IIRC, some versions/editions of SSRS can be embedded in SharePoint, but I am not a SharePoint guy. If your users are competent with Excel, that would work as well and you could skip the server configuration. Excel can connect to SQL and run queries.
I think the excel is great idea - but that also means they have to open/manipulate a sheet. I was hoping to create a easy interface that can be accessed anywhere and becomes user friendly. But i will look into SRSS - thanks!
You may be looking for a "business intelligence" tool in which you can publish data sets for users to select fields. If you are already a Microsoft shop, pretty Microsoft PowerBI is fairly inexpensive. If you have the coin tableau or microstrategy would be better. 
I have Tableau desktop....i'm asking very nicely for Tableau server ATM...will look into it more as Tableau is VERY new to me.
This is kind of dead but I was searching for a similar question. Basically why should I use an editor? When I am writing something bigger where I might not know the best way to go I will run the query many times testing things out. Using an editor would require constant copying and pasting into the Vertica app to run it. 
You've got a whole lot going on in your question. I often try to break down building my whole package into manageable steps, especially when I'm trying to do something new or complicated. If might take longer and require you to reenginner a little bit as you go, but in the long run, you run a higher chance of accomplishing your goal with less errors. Climb the mountain a step at a time. I recommending working inside out. First build the tasks that will take the part of the file name you want to build your report or whatever. Next build the part that loops through all of the files in that folder. Then build the part that loops through all of the folders. Each of these steps might require you to do some heavy googling, but I think you'll find you'll accellerate through as you go along.
You can't have * (get everything) and then followed by another column, as you have in the 1st example; however when you add the alias to the * you are telling oracle to pull all columns from that table and then generate the count, as in the 2nd statement. I'm surprised the partition clause works, as you seem to have a superfluous left bracket, I'd expect it to look like this: select j.*, count(*) over (partition by a, b) cnt from Temp j;
Unless I am very much mistaken UPDATE ic SET ic.TermsID = 2 FROM InvoiceCopy AS ic LEFT JOIN VendorCopy AS vc ON ic.VendorID = vc.VendorID WHERE vc.DefaultTermsID = 2; Your left join is essentially pointless as the where clause is requiring VC to have a value. Looks like a waste of a left join. Left joins are slower than inner joins. Joins are processed in the logical order of execution of statements BEFORE the where clause. This means your statement is doing all the work of a LEFT join then applying a where clause that essentially shapes the data into an inner join. Why bother with the left join, its just a pointless waste of resources.
&gt; Remember that you need to give your case column a different name than your source column. False. Atleast in a raw select in Microsoft SQL Server. I'm sure it will complain if you try it in a view or proc though. &gt;You'd need dynamic SQL that gets the column names from the information schema and loops over all of them Don't do this. Its awful design. 
Have you tried running your query against an actual database? Or something like SQL Fiddle to see what the results from your query are? If you see what your query produces, it will help guide you as to what needs to be corrected.
Interesting article but I've had a different question/concern about the field. I worry that building a career deep in SQL as a developer will be almost too niche and would make you less marketable. Most of my job requirements are database work and SQL with a little HTML/Javascript here and there. I worry I will become too specialized. Is this a real concern in the area?
Here's what I would do: &gt; Let us say that I have a variable named incident. The variable incident is part of the filename in a folder. I would create a FOR EACH LOOP container that dynamically assigns the file name to a variable and then inserts those variables into a staging table. You would have a TRUNCATE table task prior to this task, based on an IF CONDITION that files exist in the folder. If there are no files, you probably don't want it to truncate. [Example of looping through and grabbing file names.](https://stackoverflow.com/questions/38151342/ssis-how-to-loop-through-files-in-folder-and-get-pathfile-names-and-finally-e) [Example of IF CONDITION logic.](http://www.rad.pasfu.com/index.php?/archives/11-Implement-If-condition-in-SSIS-package.html) &gt;My goal is to iterate through each file in the folder, extract part of it, and pass in the extracted part into the incident variable. I'm not exactly sure what you mean. Do you mean that each file needs to have data imported into individual tables with the filename? Or do you mean you need a piece of the data in the file to append to the filename? [Example of importing files dynamically.](https://www.mssqltips.com/sqlservertip/2874/loop-through-flat-files-in-sql-server-integration-services/) &gt;The variable incident is manually passed in for each run of the package to generate some report. The staging table can have additional columns to help you process and keep track of the records used. Likewise, the staging table allows you to pull back or assign the filenames to the variable between packages. &gt;Next it needs to through some steps and generate a report. Once it is done generating the report the processed file must be moved to a different folder. I need to keep on repeating these steps for each file in the folder. This would be another FOR EACH LOOP but moving the file. [Example of moving the files dynamically.](https://www.tutorialgateway.org/move-multiple-files-using-file-system-task-in-ssis/)
I do IT reporting for a large company. I build reports in Visual Studio (.rdl files) and store them out to a Sharepoint site where the reports can be run or run then exported to Excel by the consumer. This may be the best solution for you now if you can get a Sharepoint site set up and are familiar with report building. If not, Microsoft PowerBI or Tableau are great options. I personally find the latter, Tableau, to be more intuitive than PowerBI which may help the consumers with running their reports or exporting to excel/datasets.
It's because I've actually read that left joins can out perform inner joins. I haven't measured this myself though. Maybe I'll reexamine that assumption. 
\&gt; I worry I will become too specialized. Is this a real concern in the area? There are a lot of jobs that require SQL specialization, the real questions are: 1. Do you want to specialize in SQL? Does this interest you? 2. What is the availability of SQL jobs in your area? 3. Do SQL jobs pay what you need in your area? Worldwide there are a lot of SQL jobs and they typically pay well. SQL is not going anywhere anytime soon, but you will see a lot of changes. Either way I feel confident it will be a good path to pursue if that interests you, but do expect the landscape the change. Realistically, the landscape will change no matter what you land in for technology. 
it looks ok to me, maybe there are some null codes in the set? use "= (select max(code) ...)" instead of &gt;= ALL(..)
Another option are tools like phpmaker, which present you a page to select database tables and so on, and another page to design a simple website.undet the hood these tools generate PHP code, which is a programming/scripting language. I think you'd have to setup a Webserver for this though.
If the search criteria isn't specified, then normally you'd just exclude it altogether from the WHERE clause If that's not possible (long time since I've even touched Access), then you could probably do something like... And ((IsNull(Forms!Search!sex_comboform) or ([Clients].sex=Forms!Search!sex_comboform)) 
dbeaver https://dbeaver.io/
You shouldn't need another license. They only recently moved to a subscription model, but existing licenses allow you to use the same version you had.
What's the error?
first, what error is it throwing? from looking at your code, you have syntax errors, like missing underscores, or adding CarID when the column in table car in ID
1. those are terrible names. use names that describe what the thing actually is 2. you are violating first normal form. the "data" table should have a foreign key to jobs table
Thank you. I’ll try it out soon!
This should work If isnull(forms![form].combobox) or forms![form].combobox="" or forms![form].combobox="all" then Display all records Else Display combo box parameter End if
I use SQL databases for web development and excel reporting, works great. Much better than access IMO. 
I am confused about databases in the sense or should I use mysql or ms sql server or SQLite , like can I use any of them of what is the difference ?
&gt; I’m completely stuck on whether I need to use a sub query, a straight select, or a join with a sum. try each if something doesn't execute properly, or returns the wrong results, then post your query here
 You can do both with just about any SQL database. MS SQL Server integrates with Excel easily.
MSSQL is $2500-$7500 per server depending on how you license it for commercial usage. Totally free for development work. Great features and ecosystem, tightly integrates with excel, and works in every programming language I've ever tried. MySQL is often used for web applications due to the free licensing. It's fine, and it's mature. If you are looking at this I also suggest you look at PostGRES and focus your efforts there. SQLite is appropriate for small database style structures, typically it's used as a backend for applications used by a single user on a single computer.
&gt; MSSQL is $2500-$7500 per server depending on how you license it for commercial usage. Express Edition is free even for production usage. If you can stay within its limits, it's fine to use. Standard and Enterprise Edition are licensed *per core*, not per server. Unless you're in a cloud environment, in which case it's rolled into your monthly bill based on usage, performance level you've paid for, etc. &gt;MySQL is often used for web applications due to the free licensing. It's used because it's free and easy, so everyone had it on their hosted accounts since the beginning of time and it just sort of became a de facto standard offering. Postgres is just as free and a better product overall.
Honestly, I'd just spin up an AWS RDS instance and be done with it. Use it for as long as you need then dump it. Having it there will make your life easier anyway; setting up infrastructure and back-end is part of the process and learning how to use it in a quasi-production setting would be beneficial.
A lot of those will depend on your use case and platform. More context would be helpful.
I'm going to say you don't "need" to use subqueries, for example. You can, when they are helpful - and they often are :) Anywho, let the output ("I need the column from this table, sum of this whole column from that table and a flag") and granularity ("My answer needs to have 1 record per this and that attribute") lead you - if your output columns come from different tables, most likely it's going to be a join. If your output granularity is different from your data granularity - it's going to be a 'group by' most likely (and everything that's not part of your output grain will need to come with sum/avg/min/max/etc.). If you cant get to your output from your data in one jump - go ahead, use subqueries as your "stepping stones"
Wouldn't it be a considered a parameter of the *function* `LIMIT` when described at a high level? The argument in the specific case of `LIMIT 7` would be `7`, but only because that is the actual value passed into the *function*. Or I'm way wrong, who knows.
Your guess is as good as mine! Parameter makes sense but that's only an assumption I'm making based on how argument and parameter are often used interchangeably. Maybe it's good enough, just wanted to know if I'm not forgetting a more accurate term. Is the `LIMIT` clause considered a function at a high level of abstraction? What about `FROM` and `ORDER BY`? In any case, it doesn't seem right to call something like `LIMIT` a function when there are entities like `LENGTH()` and `UPPER()` that are most decidedly "functions"
In SQL-speak at a high level I suppose, it would be considered a \`SELECT LIMIT\` statement. In reality, it's generally referred to as a SQL statement with a \`LIMIT\` clause. Within that, \`LIMIT\` has parameters or arguments (depending on how you are talking about it). Looking at it from a programming side, I imagine you could consider it a function (\`LIMIT\`) within a class (\`SELECT\`)? That may make the usage of parameters and arguments in text more contextually accurate. I think you'll get your point across either way. You'll always have someone tell you it is wrong....that's how SQL goes. :D
Excel is part of the Microsoft stack so I would recommend T-SQL (SQL Server) which is also Microsoft
SQL is based on relational algebra and calculus so are logically the same across most relational database products. There are some syntax differences between say Oracle and SQL Server but if you understand the logic that underlies the queries you want to make you can easily find the correct syntax by looking at each product's documentation. Its pretty easy to pick up the basics of SQL but it takes time to learn the different products and what features they offer. But, you specifically mentioned you want to learn SQL and not so much about databases so any old website that teaches you coding syntax will do. 
Postgresql is nice and standards compliant. Open source so you could look at the code of you wanted
I would definitely recommend T-SQL (Microsoft SQL Server flavor of SQL). You can download Sql Server Developer Edition for free and start projects right away. Sql Server is heavily used in the enterprise world
Makes sense. Thanks!
Okay that's good to know. I was worried I might be wasting time learning the wrong one. Thank you!
I recommend this free online course https://mva.microsoft.com/en-us/training-courses/querying-with-transact-sql-10530
Yeah, its really no big deal transitioning between the different types of sql syntax and databases. For example, when i first studied database design we used an oracle database for our projects and workshops. Then in another course we used python scripts and sql to talk to a SQLlite database and then for another course project i used a mysql database for a web app. The big difference between these products are usually features that you probably wont understand unless you get more into intermediate to advanced database design topics.
Something like this should work. The point is that the Jobs table shouldn't duplicate data that is stored in other tables, it should just reference the ID of the row in those tables that have the data you are looking for. CREATE TABLE IF NOT EXISTS PrimaryData( id int NOT NULL PRIMARY KEY AUTO_INCREMENT, name varchar(32) NOT NULL, path varchar(260) NOT NULL ); CREATE TABLE IF NOT EXISTS SecondaryData( id int NOT NULL PRIMARY KEY AUTO_INCREMENT, name varchar(32) NOT NULL, path varchar(260) NOT NULL ); CREATE TABLE IF NOT EXISTS Event( id int NOT NULL PRIMARY KEY AUTO_INCREMENT, type varchar(32) NOT NULL, ); CREATE TABLE IF NOT EXISTS Job( id int NOT NULL PRIMARY KEY AUTO_INCREMENT, primary_id int NOT NULL REFERENCES PrimaryData(id), secondary_id int NOT NULL REFERENCES SecondaryData(id), event_id int REFERENCES Event(id) ); 
This is a really thought-provoking question! If you were to consider the LIMIT token to be an Operator, then the value 7 would be an Operand.
If you're looking for something to just play around with SQL Server Express is great for that. I wouldn't use it as a productions database that you rely on due to its limitations. If you're looking for a good, free, production level database, I would look into PostgreSQL.
Best bet is to go to a job site and search for SQL related jobs in your area, then start learning the ANSI standard + built in functions and features of whatever is predominant in your area. While a lot of it is very similar between 'flavors' it doesn't hurt to be focussed on what is available in your area 
You are not missing anything - in the end it's a matter of personal preference. Both commercial and open source database engines are used for projects of all shapes and sizes. I would recommend you to base your selection on the stack that you plan to use for your web app. In excel you can access any data as long as you have proper adapter and I believe that they are available for all major db engines. 
Microsoft refers to that as an argument in books online. 
[I found this though a google search](https://perishablepress.com/mysql-magic-find-and-replace-data/): UPDATE `player_analytics` SET `server_ip` = REPLACE( `server_ip`, `81.19.212.44:27010`, `185.251.226.56` ); But when I tried it out, it gave the error #1054 - Unknown column '81.19.212.44:27010' in 'where clause' How would I fix this? 
Pay closer attention to what kind of quotes you’re using. Backticks and single quotes mean different things, you can’t use them interchangeably. 
Lord, please help us. Just look up a standard update statement at w3 schools. Update &lt;table name&gt; Set server_ip = '&lt;new server ip&gt;' Where server_ip = '&lt;old server_ip&gt;' And before executing you made a db backup and / or made a table backup. Mass updates are dangerous! 
Hahaha who needs backups. Hell, why even use a where clause? Wait... [use a where clause dummy](https://www.brentozar.com/archive/2017/12/can-prevent-deletes-inserts-without-clause-running/) 
Off the top of my head not sure if any is more efficient, but option 7 is: `code`count(case when mybitfield = 1 then 1 end) Since count ignores null, it converts the 0’s to null and counts... Just another option, again no real info on performance right now, on my phone so not really able to do my diligence. 
Well, if you need more: SUM(IIF(My it field,1,0)) should work...
Use a simple UPDATE statement. Should be very straightforward.
I solved it. I looked at this problem from too moch Pythonic(pandas) viewpoint. SOlution is to work with SELECT (CASE table."ALLOW_MAC_ADDR" = 1 THEN 1 END) AS "ALLOW_MAC_ADDR" AND then you get dummy columns from this piece of code.
This is a good suggestion! Thank you!
How do Windowed Functions differ from just temp tables? It looks to me like they do similar tasks. Windowed functions must be better for performance?
MySQL should suffice for what you want but you can also install MS SQL developer edition (MS SQL is more a marketable tool) . This allows you to have a database upto I believe 4GB in size. Plenty for playing around. MS SQL has a data import tool, dump everything into a CSV and import it. Pretty sure MySQL also has a tool for it. Otherwise just script the inserts. You can use functions or stored procedure to do the fancy math to get the desired results. They are basically scripts saved in the dB so you don't have to run the lengthy script each time. To install MySQL on MAC: https://dev.mysql.com/doc/refman/5.7/en/osx-installation-pkg.html
They are completely different approaches. I don't see how a temo table can give me lead or lag value? Could you elaborate?
This could just be me misunderstanding which part of the code in the video’s examples are the Windowed Function, so bear with me if I’m being stupid... but here we go! The examples of code in the video seem to be operating on 2 fronts: With ( - - insert some form of select statement with a group by clause to aggregate some data. This aggregation will inherently have some field that can be used in the Lead or Lag function later, like a date or some other sequential field. ) AS AggregatedDataAlias Then once you have the data aggregated, you can query from it, including the Lag or Lead function: Select *, Lag([Field you want to compare]) over (Order by [Date Field]) From AggregatedDataAlias I completely understand how the Lag or Lead function is pulling data out of the previous or next row. That makes perfect sense. I am more curious about the With() statement where you aggregate the data in the first place. Is there a benefit to use the With() statement versus just creating a temp table there, and then referencing the temp table when you run the Lag/Lead functions? Apologies for being unclear! Also, great video!
Ah that was it. I tried to copy and paste the example code but it didn't let me, so typed it in manually. Managed to get the data all changed correctly with no errors 
Ah what you mean are the WITH ( sql query ) expressions. Those are called CTEs (common table expressions). They are not scope of that video, I just use them to get the sample data I need in order to show the windowing functions. CTEs have nothing to do with it and you can apply the windowing functions (lag,lead,sum,first_value,last_value) also on regular tables. CTEs are not like temporary tables. The statement does not get persisted and is evaluated every time the CTE is used. For instance: WITH CTE AS ( .... ) SELECT * FROM CTE as a JOIN CTE as b ON a.id = b.id would evaluate CTE 2 times and not take data from some kind of cache ( other than the usual SQL Server cache mechanism ). Maybe I will do a video about CTEs in the future. Thanks for your feedback.
Ah gotcha, great explanation! Thanks for walking me through that!
What's the frontend programmed in?
That seems irrelevant. 
What would be the business need to create 10,000 tickets in a ticketing system at one time?
Are these like event entry tickets? Do they need to be created individually or can you just create an event record with a num tickets and maybe a num remaining field to show how many unclaimed tickets there are. The best solution really depends on the problem you're trying to solve. 
SELECT DATEPART( wk, '1-1-2008') -- returns 1 SELECT DATEPART( wk, '12-31-2007') -- returns 53
This is the right answer, my work has a statistics table for each time period (StatWeek, StatMonth, StatYear, etc.) and for StatWeek we use this method. Insert the date into a column for the record, and use this datepart to populate WeekNo.
SSMS with SSMSBoost add-in
SSMS + ApexSQL's [SQL Complete](https://www.apexsql.com/sql_tools_complete.aspx) is great.
SQL Complete crashed SSMS regularly for me. I suspect it had something to do with my unorthodox production instance overwhelming it.
As an additional note since the original poster mentions a configurable start to the day of week, this will be very helpful. Use it to set the First Day of the Week, before doing the statements. [https://docs.microsoft.com/en-us/sql/t-sql/statements/set-datefirst-transact-sql](https://docs.microsoft.com/en-us/sql/t-sql/statements/set-datefirst-transact-sql)
I'd advise against it. There's nothing wrong with it in and of itself, but most likely, you'll forget you used it or someone else will be editing your SP later and they won't realize it's being used and shit won't make sense. Someone will end up with set datefirst ptsd and feel like they have to check the value of @@datefirst before every date operation. Just say no. I care about you, dear redditor. I really do. 
Heidi SQL
Also true, been on that end of the debugging mess myself. But that is how my current workplace does it. I actually added a test for the stored procedures that looks for that command because UT going missing if you use it every where is bad. 
I built myself a hacked together python daemon that did this for my team. Read "jobs" from a DB and executed them against the target DB at a given time and then could email results as well.
Datagrip is really good at inspection and will autocomplete down to the field level.
I'd recommend using "iso_week" instead of "wk". iso_week uses Thursday to decide which year the week belongs in. It considers the "week" to be Monday - Sunday, so Thursday is the mid point of the week. It makes it so Week 1 has 7 full days in it, where "wk" only does it when the first day of the year happens to be Monday. select DatePart(iso_week, '2008-01-01')
Perhaps use Windows Tasks Scheduler to run your queries/procs using sqlcmd.exe? I use it to run ad-hoc file imports via batch files. sqlcmd.exe -S ServerName -E -d DatabaseName -Q "exec dbo.StoredProcName @Parameter1 = 'Value';" &gt; Results.txt 
Dbeaver is nice if you are familiar with eclipse IDE. You can get it as an eclipse plugin or as a stand alone. It supports many database systems; the features depend on each database.
Thank you! I’m having trouble deciding what info goes in what tables currently but I got everything ready to go software wise 
If you're looking to calculate Monday as the start of the week, you could use this: declare @TaskDate datetime = '2018-07-29'; select WeekId = @TaskDate - (DatePart(dw, @TaskDate-1)-1); This is short in length and gets the job done. However, it is not immediately obvious what it does if not commented/documented, and works with datetime but doesn't work with date.
/u/ComicOzzy has t he right idea. However, you may also not have task scheduler on your server. Without getting into the politics of your organization, if you are the BI team, then for all intents and purposes, you should own the BI server. If you are running your DW on a shared SQL Server, then best practice dictates that you put it on a dedicated server of which your team owns SQL Agent permissions.
SSMS + Redgate SQL Prompt
I suggest Navicat 
Prompt makes such a difference 
Could it be that there are null rows in the table? Try SELECT * FROM atmos_data ORDER BY timestamp DESC NULLS LAST LIMIT 1
Tried does not work. :/ 
You mean running the statement directly against the server does not work?
no via the script not directly in the shell/console 
Does it return data when run directly?
no 
to add to this running the same on pgadmin 3 returns a positive result and displays the data. wonder why it is broken through script.
Are you sure the execute method returns a collection? I thought you had to execute the query and then fetch results - cur.fetchall() will give you a collection.
this solved it,should have done it this way. Thanks
In psycopg2 which you will be using, curr.execute returns none. You need to use curr.fetchone() ir curr.fetchmany() after calling curr.execute() to retrieve your data. &gt; The method returns None. If a query was executed, the returned values can be retrieved using fetch*() methods. http://initd.org/psycopg/docs/cursor.html 
yup this solved it,thanks. 
No problem, it's confusing behavior at first. Not intuitive
:-) 
Something like this? SELECT dbo.ContractEntityCode(M.EntityRef) AS 'Entity' , E.Name , M.Number , M.Description , M.Created , M.WIPLimit , UF.FullName AS 'Fee-Earner' , UP.FullName AS 'Partner' , D.Description , SUM(M.WIpLimit) AS total FROM Matters AS M JOIN Entities AS E ON M.EntityRef = E.Code JOIN Users AS UF ON M.FeeEarnerRef = UF.Code JOIN Users AS UP ON M.PartnerRef = UP.Code JOIN Departments AS D ON UP.Department = D.Code WHERE M.Created &gt; EOMONTH(DATEADD(mm, -1, GETDATE())) AND M.WIPLimit &lt;&gt; 0.00 GROUP BY D.Description , rollup (( dbo.ContractEntityCode(M.EntityRef) , E.Name , M.Number , M.Description , M.Created , M.WIPLimit , UF.FullName , UP.FullName ))
That's the exact thing I'm trying to do I've been trying to figure out the syntax for ROLLUP for ages! 
[removed]
May I mention the poorest man's version of scheduling one-off scripts: WAITFOR TIME ? 
Completely agree with your take, but the way our BI group is structured, there is the IT side which maintains the ETL processes, nightly batch refreshes, etc., then the side my team and I are on work more closely with the business users by building out reports via a few different means (Tableau, Business Objects, or sql queries exported to excel). I’m always pressuring my boss to open up more conversations about scheduling. If we’re constantly searching for ways to get around current restrictions, we should find a better solution. I imagine they’re worried about the scheduled jobs getting out of hand and increased risks of blocking if we’re not aware of all the simultaneous jobs running. 
Sounds great in theory. But our warehouse disables users from about 7pm until midnight-ish while the system does a full refresh. I think I’d be logged off if I tried this overnight, plus I’d probably show up on quite a few “long running” query audits if it was executing for hours haha
I would have seperate tables for teams, scores, stats (team stats and player stats seperate tables). You can dump everything in one table but it's a horrible practice, read up on Normalization - for now try to use it as a guild line not a rule book. Let me know if you have any questions regarding this and send me what you have so far. 
Depends on what the data looks like, and what you want the return to look like - any of those could be applicable. If all the data you need to group by for aggregation is in one table, you don't need a join. If it's not, then you need a join. If you want to transform data in one table before aggregating based on data in another, then you can use a subquery and join to that. I'd suggest just mocking up some data, or downloading a sample database (e.g. [Wide World Importers](https://github.com/Microsoft/sql-server-samples/tree/master/samples/databases/wide-world-importers) for MS SQL) and making up some requirements to practice with.
Thanks! More reading for me!!!
C#
yes entry tickets, i was thinking of the same logic but i was told it was better to create the tickets needed than to keep a number of tickets column
what would be your solution
yes they can be created individually
Thank you for this. My career is leaning towards report building and data analytics, and it interests me a lot and I love working with SQL and databases in general. I'm fresh out of college with this job and enjoy it, but my concern was if I focused to heavily in it it would eventually become "niche" and not be marketable. But you make good points. Thank you
Similar experience: I had a conversation like that with a "team lead" that was actually a consultant for handful of onsite developers. he gets angry and in my face. I laugh, #walkaway, and talk to my boss discover my boss and that "team lead" are friends, my life at office goes to shit. Start job searching, quit with 2 weeks notice. last day of employment was 3 days before huge go-live. my friends that WERE there, i gave documentation and tips for my stuff, so they knew. Project release was a CF. Later I find out: that a "senior" developer wrote a SSIS that migrated data from a remote server to another remote server through his laptop AT home. with a 10Meg upload cap. 
Checked with the HR department this morning. They did not give me complete information. It turns out that there were 4 people out that day. Just one "no call, no show". Once I dug into the other fields in the table for employees I found that HR has not been filling out employee data completely and after I said "I'm done until you all fix your inputs". They filled out the extra fields in the employee records and now we have clear data to select from. So ultimately here is the query that returns the correct information: SELECT e.FirstName , e.LastName , e.EmployeeId , e.WorkStatus , e.RecordID , e.Status FROM tcp_Employee.Employee e WHERE NOT EXISTS (SELECT 1 FROM tcp_EmployeeWork.WorkSegment ws WHERE ws.EmployeeRecordId = e.RecordId AND CAST(ws.TimeIn AS DATE) = CONVERT (date, SYSDATETIME())) and e.status = 0 and e.workstatus = 2 Thank you for your help on this!
Happy to help! Good luck on your SQL adventures! :D
I will say breadth is the new depth, people seem to want folks who can do a little of everything and have the ability to learn. Data and databases is a HUGE industry with MANY career options, you will not feel overly niche and will be marketable, but it's important to remember location. Data in itself is huge, but there are not nearly as many career options as development and administration, this comes and goes in waves of course. So keep the idea of location in mind. I can find Data jobs locally, there are always a few new ones each week. If I lived in a tech hub though, I could tie my resume to a rock, throw it, and land a job. Different locations will cater to different job markets, but obviously this isn't solely an IT job issue or niche problem, it's just how it is. 
I wrote a post on pivot and unpivot a while back, please see if this helps you: https://www.reddit.com/r/SQL/comments/8wmsyc/pivot_and_unpivot/ 
&gt; https://www.reddit.com/r/SQL/comments/8wmsyc/pivot_and_unpivot/ Thank you I will take a look
Create a function that substrings a new_id(). Check that's it's not in the table already and retry until you find a unique one?
Try Jenkins + Powershell. Even though it might seem too much to tackle at first, it's a pretty damn good tool. I created this [article](https://nvarscar.wordpress.com/2018/05/03/special-agent-jenkins/) a while back, hopefully you'll find it useful.
create a sequence that starts with 1 million
What database platform?
How are you identifying which line gets Pivoted ? SQL won't know if you don't specify. 
Well that's where I was stuck but somehow what you just made made me realize I can consolidate the columns to be simpler. If I nix the Title column and combine yr code and trm code I can do ACC 101 201403 10 ACC 101 201501 5 This would be much simpler to pivot. Thank you for kicking my brain somehow :p
I'd probably have the application handle it then
SSMS + Redgate for just doing straight DB Dev/Admin work. Really digging Falcon SQL client for doing some data exploration / pre-liminary visualization work.
Can you install a side SQL server on your desktop or a group VM or something? Could schedule the jobs to kick off from there.
platform? because joined update syntax is different depending which database you're using
 [AutoID] [numeric](7, 0) IDENTITY(1000001,1) NOT FOR REPLICATION NOT NULL
DB2
Your current attempt updates the whole table A. Is that what you've intended? If not, then add the 'where' part (or write the 'from' table expression if that works on your platform)
 declare @AdditionalTime Time = '01:15' declare @CurrentTime Datetime = '2018-07-30 13:15:00.000' PRINT convert(nvarchar(12),cast(@CurrentTime+@AdditionalTime as time),100)
Your friends answer is the correct one, since it allows for additional verification steps to occur (*example: ensuring that the minutes are not larger than 59*). It is also important to note that you can always format the output to look how you want once you have the accurate value; T-SQL is optimized to work on well defined data types, and it's good practice to use the built in functionality to reduce your chances of errors. * **2018-07-30 14:30:00.000** can be formatted to look like **2:30 PM** * **2:30 PM** cannot be formatted to look like **2018-07-30 14:30:00.000** Now this looks suspiciously like homework, so I'm not going to provide a solution. Rather, some of the tools you need to attempt it: -- Create an SQL variable and set it to the current tiime -- Note: It is generally not needed to do this, as in practice this will likely be a column value from a table DECLARE @currentTime DATETIME; SET @currentTime = GETDATE(); -- Return the current time, an hour into the future SELECT DATEADD(hh,1,@currentTime) -- Example using a table SELECT yourDateColumn AS originalDate, DATEADD(hh,1,yourDateColumn) AS modifiedDate FROM yourTable 
You'd be much better off putting the TIME to a DATETIME because it avoids truncation. 
There's a few options here. The easiest (in SQL Server 2008 and beyond) is just to cast it as a `time` type. `cast(AdditionalTime as time)` This relies on having error free input that's in the right format (it looks like HH:MM works, as well as HH:MM:SS). You can cast `AdditionalTime` as a time, add it to the CurrentTime (or `getdate()`), and then cast the result as a time, to strip out the date. I like to store durations in seconds, as it makes storing and doing math much easier. Formatting can be handled when the data is output. But that probably goes beyond the scope of what you're asking for. 
You're doing a hybrid of implicit / explicit joining... do one or the other, not both: Explicit: SELECT * FROM STUDENT INNER JOIN MAJOR ON student.major = major.major Implicit: SELECT * FROM STUDENT, MAJOR WHERE student.major = major.major
&gt; I'm seeing some answers say that there's load balancing with read-only replicas, but some people say that it's not really load balancing. This is not true load balancing. What is happening is the primary is carrying the transactions over from A to B and B is your read only. This means that when an application just needs to read or see something, it is querying server B. So server B is taking the memory / cpu / disk hit and server A is not. &gt; I'm also wondering if the read-only replicas mean that they are literally in a static state from when you create them or if you just write to the primary and it copies it to the replicas under the AAG settings? The latter, it is carrying the data over. You can do snapshot replication to a server where it would remain static until later. You can also bring over the database in the AG and then remove the DB from the AG. This leaves a static DB on server B. (This is the same as taking a backup and restoring it on the server.) &gt; Is there a better option for load-balancing, or is this really the only way to do it and is it actually load balancing or is it just there in case the first server can't be used at the moment? Can you define what you need to load balance? In SQL Server, saying load balance would be a high level concept. Are you experiencing issues with user or data concurrency? Is there not enough memory or threads? Is there io pressure? Besides load balancing the server itself, you need to get specific with SQL Server. Tell us where it hurts or what your end goals are and we can give you some more insight. 
I tried that, It then says major doesn't exist.
does it exist? can you do `EXPLAIN ANALYZE SELECT * FROM MAJOR` http://sqlfiddle.com/#!17/a8ec2/9
It's a column inside the student table.
Nvm professor gave me old version of assignment. Thanks
So you're looking for time-based triggering of a SQL Server T-SQL stored procedure?
I hope it isn't a problem bringing it up here. I didn't really know where else to go, and I've found Reddit quite useful in the past. 
We are not /r/domyhomeworkforme Show that you've made some effort towards solving the problem yourself first.
Do your readings, pay attention in lecture, and participate in lab?
Even better, setup and attempt it in SQL Fiddle: [http://sqlfiddle.com/](http://sqlfiddle.com/) I personally dislike courses taught like this as it doesn't help build understanding. 
I'm gonna be honest i'm pretty fresh on all this. My company wants to load balance for their database pulls (if that makes sense?). Basically it's going to be housing the database for our POS system. While I don't have the full details, as far as I'm aware it's going to log transactions, hold customer information (customer accounts), inventory, and item information. So a typical use would involve pulling the customer data, followed by the part information, and then after completing the sale would note the transaction, and adjust the inventory for that store.
&gt; http://sqlfiddle.com/ Thank you. I had no idea this even existed. /u/Daakuryu I wasn't trying to to offend anyone. I'm genuinely struggling with this. I only brought it here because I felt like I needed some help. 
If it isn't homework, what is it? Looks like either that, an exam question or an interview question.
I'm trying to do some exam prep. 
You can use case statements in join. `SELECT` `TSHIRT` `FROM SHIRTS` `JOIN PANTS ON` [`PANTS.ID`](https://PANTS.ID) `= CASE WHEN PANTS.COLOR = 'BLUE' THEN` [`SHIRTS.ID`](https://SHIRTS.ID) `WHEN PANTS.COLOR = 'YELLOW' THEN SHIRTS.ID2 END` If you only have one condition, of course you can just do this: `SELECT` `TSHIRT` `FROM SHIRTS` `JOIN PANTS ON` [`PANTS.ID`](https://PANTS.ID) `=` [`SHIRTS.ID`](https://SHIRTS.ID) `AND PANTS.COLOR = 'BLUE'`
Would you mind elaborating a little? I do have a separate laptop I’ve kept around after they upgraded us a while back. How could I use SQL server on that machine to schedule jobs? After reading some posts here I think I like the idea of using windows task scheduler to run some batch files, but if there’s a better way to do it from SQL server I’d love to know. 
1) select s.name from Student s inner join Attends a on s.matric = a.student inner join Course c on a.course = c.courseNo where c.name = 'SPM'; You use table joins ON various relationships to link Students to Courses. Because you want to pull data from Students (the student name) while filtering on the Course name. So Courses is linked to Attends is linked to Students.
I'm totally going to be a dick here but whatever, If you're genuinely struggling with those questions and aren't just being lazy, drop the class. This is basic Joins and Where you should have pretty much mastered in the first week.
Is the POS in house or an application that you guys install and it uses SQL Server? Typically an AG setup is good for high availability and it helps allow the database "pull" (read is a better term) data as it can do so from the readonly connection while using the primary server for data modification. The "pulls" however, need to be report-esque in nature. The last sentence you described to me is an online transactional processing database. Meaning it will read and write with an emphasis on writing to disk. If the software is not in house, you'll be in luck because they should help you with specs. If the software is not in house, you really should have a data architect or senior DBA help you out with this while contracted to your company to help. Helping design and scale out an enterprise wide POS system is not a small task and honestly it could take a team of data professionals depending on what the scope is. Go to brentozar.com and download his things. https://www.brentozar.com/first-aid/ Check out the following in order: 1. Worksheet - High Availability and Disaster Recovery Planning.pdf 2. Worksheet - Hardware Sizing.xlsx 3. Worksheet - First Responder Checklist.pdf Now that you have seen those, read these: 1. eBook - Google Compute Engine - How to Build an Always On Availability Group.pdf 2. eBook - Google Compute Engine - Disaster Recovery.pdf I think the first 3 things filled out will greatly help you help us and you'll be happy you have those filled out when it comes time to working on this. 
I'm working at it. I'll get there eventually. 
c) is tougher one. It starts the same as a) in that we want to select a list of students: select s.name from Student s But now we want to restrict the data based on the name of the teacher of the course.... so create the joins between Student to Attends to Teaches in order to get the staffNo column. select s.name from Student s inner join Attends a on s.matric = a.student inner join Teaches t on a.courseNo = t.courseNo We have the joins, the relationship between the data has been established, now to figure out how to exclude any student where they Attend more than one class with the same teacher. There are a lot of different ways to do this... what sorts of SQL functions have you been using/are you familiar with? Have you used EXISTS and NOT EXISTS? COUNT? DISTINCT? 
c) is tougher one. It starts the same as a) in that we want to select a list of students: select s.name from Student s But now we want to restrict the data based on the name of the teacher of the course.... so create the joins between Student to Attends to Teaches in order to get the staffNo column. select s.name from Student s inner join Attends a on s.matric = a.student inner join Teaches t on a.courseNo = t.courseNo We have the joins, the relationship between the data has been established, now to figure out how to exclude any student where they Attend more than one class with the same teacher. There are a lot of different ways to do this... what sorts of SQL functions have you been using/are you familiar with? Have you used EXISTS and NOT EXISTS? COUNT? DISTINCT? 
We haven't used exists, but I'm familiar with COUNT &amp; DISTINCT. Should I familiarise myself with EXISTS/NOT EXISTS as well? 
An hour in and you haven't posted a single line of code in response to being told to make some effort towards solving it yourself. So... I'm doubtful.
There are different ways to do it. I'd probably do something where I take the count of distinct teachers for each student, then filter out the results where the number of distinct teachers is &gt; 1.
That's okay, it doesn't bother me. Just don't hold your breath because I'm about to head to bed!
I'm going to try and use GROUP BY () &amp; COUNT to see if I can solve the final part. Thank you so much for your help. 
Sure thing.
This is very ghetto-rigged, but I’d probably use CHARINDEX and SUBSTRING if I had to do it in a pinch and couldn’t get XML Parsing to work.
I'll take a look at those in a bit. They'll likely have someone helping us at the beginning but my boss wanted me to get a grasp on how it all works. It is not in house, just being customized. I'm fairly certain they'll have someone helping us set it up (likely a team) but he understandably wants me to have a grasp of how it works so that in the future we won't need outside help. They're also definitely helping us with specs and such, and I believe the systems themselves are already ordered as well as the software. My company really runs like a small family business still despite having grown quite large. We have people running systems that are critical to daily operations for the entire company entirely by themselves. As part of this transition I'm becoming one of those people.
I had to sort out something similar recently. Here you are: DECLARE @stuff xml = '&lt;cm:ContactEmail xmlns:cm="http://schemas.company.com/webapp/ContactManagement/V01/ContactManagement.xsd" Version="1.0" Type="Business"&gt; &lt;cm:Url&gt;First.Last@company.org&lt;/cm:Url&gt; &lt;/cm:ContactEmail&gt;' ;WITH XMLNAMESPACES('http://schemas.company.com/webapp/ContactManagement/V01/ContactManagement.xsd' as cm) SELECT R.i.value('.', 'varchar(256)') as [Email] FROM @stuff.nodes('/cm:ContactEmail/cm:Url') R(i) 
I learned from Simon Allardice's course on Lynda (now LinkedIn Learning) and it covered not only the basics of sql but fundamentals of database design principles. Really laid a nice foundation I think. But it's not free. I had a subscription through my school at the time. 
Free beginners course through Stanford. Not sure if its any good though. https://lagunita.stanford.edu/courses/DB/SQL/SelfPaced/about
Can't guarantee this will work at your organization, there are a specific set of circumstances that you need to have going on. You'd need the ability to create a linked server between your side server and the main server. Since you'd be running stuff in an Agent job on your local machine, the service account on your local machine will need access to the prod machine. * Install SQL Server on personal machine/private vm/etc. * Set up linked server to prod server from your new server. * If you can create a stored procedure on the prod server with the code you want to execute, that'd be ideal. * Create agent job on your local machine that executes some local code that then executes the remote stored procedure "exec [linkedServerName].databaseName.schema.procedureName" Your IT dept may not like you doing this, so use caution when doing it without discussing it. Rogue SQL servers that access production servers are a security risk. Ideally your IT dept would be willing to set you up with this kind of playground where you can alter SQL Agent jobs, and still have access to the production data you need, remotely. An alternative I'd be asking for is for them to create a SQL Agent job for you, that you don't have access to, that just runs a single stored procedure at a safe hour, that you do have access to. Then you'd be able to just put a series of batch code in that SP. Good luck! Dealing with politics and procedures is the least fun part of the job.
May I suggest being more specific with your questions and showing what you want tried so far? People are willing to help, but nobody is really looking to do your homework for you. 
Try this: SELECT ,customer_id ,count(*) FROM ORDER WHERE purchase_date &lt; DATEADD(month, -2, GETDATE()) AND order_id IN ('97','43','54') AND GROUP BY customer_id having count(*) &gt; 5
Hi u/SloRomci , attempted it, but it not providing me the accurate data. I know a couple of the customer has more than 5 counts, but wasn't included in the list.
Give this a shot. If you have a physical tally table rather than the CTE, that's even better. declare @start date = '2017-01-01'; with tally as ( select row_number() over (order by (select null)) - 1 as n from (values(0),(0),(0),(0),(0)) a(n) cross join (values(0),(0),(0),(0)) b(n) ) ,intervals as ( select dateadd(mm,n,@start) as interval_start ,dateadd(mm,n+2,@start) as interval_end from tally ) SELECT COUNT(*) AS PurchaseCount ,t.customer_id ,i.interval_start ,i.interval_end FROM yourtable t JOIN intervals i ON t.purchase_date &gt;= i.interval_start AND t.purchase_date &lt; i.interval_end WHERE t.order_id IN (97,43,54) GROUP BY t.customer_id ,i.interval_start ,i.interval_end HAVING COUNT(*) &gt; 5
Try this variation: SELECT ,customer_id ,purchase_date FROM ORDER WHERE purchase_date &lt; DATEADD(month, -2, GETDATE()) AND order_id IN ('97','43','54') GROUP BY customer_id having count(*) &gt; 5
I've always liked w3schools.com for my online learning needs. https://www.w3schools.com/sql
I usually see everything funneled through an application server to the database, and if you can trust the application to be the gatekeeper for writing to your DB, then direct creation might be fine. If you're going to have other tendrils hooking into your DB, then you might want a FIFO queue to prevent data loss.
GETDATE does return "now", but that was surrounded by DATEADD, taking away two months. Your original query was for 2017, the above date math was for this year.. should hopefully get you the results you want!
&gt;Would making essentially an infinite loop be detrimental in anyway? Bad design. Infinite loops like that to wait tend to use CPU resources. Either a scheduled task or a SQL agent job would work fine except... &gt;I don't want to use any SQL servers like T-SQL So why are you here exactly? 
So the remote server provides c5 and c6, and the local system provides c7 and c8, and an object's identity is based on c2, c3, c4? And does (c2, c3, c4) act as a unique key for the objects - ie, you would never want to store more than one object with the same value of (c2, c3, c4)? Because if it is a unique key then you could use an outer join. -- What you want the local table to contain -- once the REST response has been saved select l.c1 , c2 , c3 , c4 , r.c5 , r.c6 , l.c7 , l.c8 from Local_table l full outer join Temp_table_with_remote_response r using (c2, c3, c4) Ok, this is not going to solve the problem of actually performing the merge, but if it returns the desired result - what you want the local db to contain after the merge - then it's a start. 
If the XML for each row only has one url value for the email then you can just do the following. SELECT Col1 ,Col2 ,[data].value('(cm:ContactEmail/cm:Url)[1]', 'nvarchar(max)') as [Email Address] FROM ContactMethod * If [data] is not stored as an XML type you'll need to cast it to XML first. * The first argument for value() is an XML xQuery so should be easy to fix if I've gotten it wrong for your data. * You might also need to define the XMLNAMESPACE as show in the other answers. 
Ah ok, that makes a lot more sense then. I'd begin looking at what high availability means to SQL Server as well as what disaster recovery means. I think those resources I gave you are a good start to that path, but I'd also begin looking at how SQL Server allocates resources. For example, I'd use an AG setup in a cluster for high availability with the possibility of log shipping or periodic backup restores to another server in the same environment (relatively speaking) that a router can switch between for disaster recovery. For scaling out, I'd be measuring resources and workload on the server and seeing what needs to be tuned. Odds are that partitioning and query tuning will be out but you can still look at indexing and statistic maintenance with the side thought of resource governor. If they have the specs and hardware in place and you can't do much from the data architecture standpoint, it seems like you'll really just need to focus on the administration and maintenance pieces. If your work is willing to pay for training, it wouldn't be a bad idea to take some classes but otherwise there's a lot of good material online to learn from. Ozar has a lot of great pieces you can dive into and SQL Central has great stairway articles. 
This query will display customers who have made purchases before 2 months ago. It does nothing to group the purchases in intervals of 2 months. 
I always recommend https://www.w3schools.com/sql/. It is free and a fantastic resource. 
Something like this? SELECT o.customer_id , PurchaseCount = MAX(pc.PurchaseCount) , LastPurchase = MAX(o.PurchaseDate) FROM [ORDER] o CROSS APPLY (SELECT PurchaseCount = COUNT(*) FROM [ORDER] o2 WHERE o2.customer_id = o.customer_id AND o2.purchase_date &lt;= o.purchase_date AND o2.purchase_date &gt;= DATEADD(MONTH, -2, o.purchase_date)) pc WHERE o.purchase_date &gt;= '2017-01-01 00:00:00' AND o.order_id IN ('97','43','54') AND pc.PurchaseCount &gt; 5 GROUP BY o.customer_id P.S. - A table named ORDER? Please tell me you don't actually have a table named ORDER...
[Stuff for xml path](https://stackoverflow.com/questions/31211506/how-stuff-and-for-xml-path-work-in-sql-server) might be your ticket. Pretty sure you can include an Order By for your path_label_id in the stuff's subquery (assuming that is important).
Xml path is what you want. Filtering on the PathLabel\_Id could happen in the inner STUFF subquery. SELECT ptl_1.Path_Id , STUFF(( SELECT '-' + ptl_2.LabelValue FROM PathToLabel ptl_2 WHERE ptl_2.Path_Id = ptl_1.Path_Id ORDER BY ptl_2.LabelValue FOR XML PATH('')), 1, LEN('-'), '') AS Status FROM PathToLabel ptl_1 GROUP BY ptl_1.Path_Id ORDER BY Path_Id; Check out his sqlfiddle: [http://sqlfiddle.com/#!18/d2b63/7](http://sqlfiddle.com/#!18/d2b63/7) If you were on SQL Server 2017+, you could use STRING\_AGG, which simplifies things a bit.
Tbh still refer to this sometimes when I'm having a brain fart
Using FOR XML PATH('')) in this manner works but is undocumented behavior that could possibly change. An alternative that works would be to join twice on the same table: select PTL1.LabelValue + '-' + PTL2.LabelValue from PathToLabel PTL1 JOIN PathToLabel PTL2 ON PTL1.Path_ID = PTL1.Path_ID AND PTL1.PathLabel_ID &lt; PTL2.PathLabel_ID
Why do you think "for xml path" is undocumented? https://docs.microsoft.com/en-us/sql/relational-databases/xml/column-names-with-the-path-specified-as-data?view=sql-server-2017 
Super close... here's how it happens in MSSQL: update A set A.CLIENT_TRACK_NUM = A.TRACK_NUM from A inner join B on B.X = A.X and B.Y = A.Y and B.Z = A.Z Let the join return the records you want to update, and then update the whole result set.
STUFF For XML PATH is the magic on this one! The working code: SELECT ptl\_1.Path\_Id, STUFF(( SELECT '-' + ptl_2.LabelValue FROM PathToLabel ptl_2 WHERE ptl_2.Path_Id = ptl_1.Path_Id AND (PathLabel_Id = (SELECT PathLabel_Id FROM PathLabel WHERE Name = 'Second Set') OR PathLabel_Id = (SELECT PathLabel_Id FROM PathLabel WHERE Name = 'Third Set')) ORDER BY ptl_2.LabelValue FOR XML PATH('')), 1, LEN('-'), '') AS Status FROM PathToLabel ptl_1 GROUP BY ptl_1.Path_Id ORDER BY Path_Id; Thank you so much! Problem solved and I learned something new.
Thanks - This is working if I make it a subquery. Does it need to be a subquery? ;WITH XMLNAMESPACES('http://schemas.company.com/webapp/ContactManagement/V01/ContactManagement.xsd' AS cm) , cteContactMethod AS ( SELECT ContactId , Data FROM LCNWPRODUCTION_REPLICATED.dbo.ContactMethod WHERE ContactMethodType = 'Email' AND Active = 1 AND [Primary] = 1 ) SELECT c.ContactId , [Email] = ( SELECT R.i.value('.', 'varchar(256)') AS [Email] FROM c.Data.nodes('/cm:ContactEmail/cm:Url') R(i) ) FROM cteContactMethod c
Start by reading up on window functions 
It is but the usage for string concatenation to create csv data etc is not and could be changed by Microsoft. It works well but the purpose is to create xml. 
SQL Server 2017 includes STRING\_AGG, but previous versions require STUFF FOR XML PATH.
This is what is commonly known as an islands and gaps problem. Google that and you can find many solutions for them. Something like: ;WITH yourtable AS ( SELECT ColorGroup = DATEADD(DAY, -DENSE_RANK() OVER (PARTITION BY [Color] ORDER BY [Date]), [Date]) , [Hours],[Type],Color, [Date],ID FROM (VALUES (10, 'A', 'Black', CAST('2018-07-31' AS DATE), 1) , (10, 'B', 'Black', CAST('2018-07-30' AS DATE), 2) , (10, 'A', 'Black', CAST('2018-07-30' AS DATE), 3) , (10, 'B', 'Red', CAST('2018-07-29' AS DATE), 4) , (10, 'A', 'Blue', CAST('2018-07-29' AS DATE), 5) , (10, 'B', 'Black', CAST('2018-07-28' AS DATE), 6) , (10, 'A', 'Red', CAST('2018-07-27' AS DATE), 7) , (10, 'A', 'Red', CAST('2018-07-26' AS DATE), 8) , (10, 'A', 'Red', CAST('2018-07-25' AS DATE), 9) , (10, 'B', 'Blue', CAST('2018-07-24' AS DATE), 10) ) v([Hours],[Type],Color, [Date],ID) ) SELECT [Type], Color, TotHrs = SUM([Hours]) FROM yourtable GROUP BY [Type], Color, ColorGroup HAVING SUM([Hours]) &gt;= 20 
What version of SQL Server are you using?
Thank you so much for the example and common naming convention of the issue. This helps tremendously! 
2008 R2
Happy to help! Good luck on your SQL adventures! :)
It's been a while since I've done this but try this: SELECT Type, Color, SUM (Hours), FROM Table WHERE (Color = BLACK) AND (((LAG(Color) &lt;&gt; BLACK) AND (LEAD(Color) = BLACK)) OR ((LAG(Color) = BLACK) AND (LEAD(Color) &lt;&gt; BLACK))) GROUP BY Type, Color HAVING Hours &gt;= 20 
If there are two groups of consecutive occurrences of black, do you want to group them separately? I'm not 100% sure that this is what you're asking but you can try: SELECT Type, Color, SUM (Hours) FROM Table WHERE (Color = BLACK) AND ((LAG(Color) = BLACK) OR (LEAD(Color) = BLACK)) GROUP BY Type, Color HAVING Hours &gt;= 20
So I ran this on my actual dataset, already knowing what results should be and it as spot on! Thanks again, this is a life savor and a great script for me to come back to as example. 
So, I do have one more question. Do you mind if I PM you my actual code? I need to filter out Types that have had any color other than black used after the consecutive 20 hours, as this actually eliminates the problem. The longer black is used after 20 hours, without using another color, the greater the problem. 
Sure, no prob
I WISH i had access to STRING\_AGG - that definitely would have made life easier
There are various ways to do this, what database are you using?
If it's MSSQL you could use EXCEPT
I am trying to update column \[VIEW\] to 'Lecturer' if column \[PEOPLE\_AMOUNT\] contains '1'. And then update the rest of the of column \[VIEW\] to 'Audience'. I have doubts if this statement can achieve that...?
Should be able to just do 2 statements: UPDATE tbl_video SET [VIEW] to 'Lecturer' WHERE [PEOPLE_AMOUNT] LIKE '%1%'; Then... UPDATE tbl_video SET [VIEW] to 'Audience' WHERE [VIEW] &lt;&gt; 'Lecturer';
Have you tried? Such a sweeping update should be testable somewhere. Fork a copy of the table.
Thank you! it works like a charm. and yes it is [order] lol... much appreciated for your help!
Hi u/StopThinking, thank you for your input. I have not attempted it as I find the syntax very confusing (I am new to SQL). I will try to do some study on the suggested syntax. Much appreciated
If were talking SQL server, then you have a variety of options: A SQL Agent job with a schedule. A script with a WAITFOR TIME. A sqlcmd command scheduled with task scheduler. And for other platforms, any command line SQL tool and a task scheduler will do the job. 
Happy to help! Good luck in your SQL adventures!
'%1%' will find 10-19, 21,31,41,111,1001 etc in addition to 1, so you might want to change how you check for a 1. maybe '% 1 %' which would enforce a space either side?
Update table a Set a.column1 =decode (a.people_amount,1,’Lecturer’,a.column1)
I think he's using microsoft, not oracle. But if it were oracle, the decode will not do the LIKE comparison for '%1%' so a CASE statement would be better. Also, this only sets Lecturer, but does not set 'Audience'.
I replied above to show how it can be done in a single update with no where clause needed since you are updating every row. Just a few other comments: - "VIEW" is a horrible name for a column and is likely a reserved word. - Also, "PEOPLE AMOUNT" is probably supposed to be number of people, not amount. "Amount" of people is like how much the people weigh. A large person is a greater amount than a small person. - Are you saving a numeric value as a string? Why do you have to do a LIKE '%1%' ?? - The entire update is dubious. I would question whether this was really the business rule to make the update based on finding a 1 in a string. I would also question defaulting all the others this way
You have query A and query B. Just run a set operation like a MINUS. If you do A minus B it will show you what is in the first query but not the second query. If you do B minus A it will show what is in the second query but not in the first. Both will exclude rows that exactly match. If you want to get the full results do this: (SELECT ID, Date, Type FROM A MINUS SELECT ID, Date, Type FROM B) UNION ALL (SELECT ID, Date, Type FROM B MINUS SELECT ID, Date, Type FROM A) 
You could UNION all the queries into a single select (or create a view which applies the UNION, then query the view).
The person with excel could add a data connector for those five results in individual workbooks, then have a final one for the pivot table with defined data sources from the other five workbooks. That way nothing needs to get exported/summarized repeatedly, just run a data refresh. If the results have the same column data, the warehouse database could be just have an automated job to pull from all five with an additional column for source table. Without knowing the requirements of why these processes are occurring, it's difficult to build good procedures for them. 
&gt; with an additional column for source table/query this
Some sample data would be helpful, as I'm not really following how your expected results differ from what you're getting. If there are multiple change dates for a user within the given timeframe, you should be receiving multiple results, right?
you want only one date for each MU_ID and AGNT_NM? SELECT sch.[MU_ID] , sch.[AGNT_NM] , MAX(dt.[DATE]) AS latest_date FROM ... GROUP BY sch.[MU_ID] , sch.[AGNT_NM] or perhaps `[DATE]` is inappropriately named and actually contains **datetime** values, you there are multiple of these **per day** SELECT sch.[MU_ID] , sch.[AGNT_NM] , CAST(dt.[DATE] AS DATE) AS date_no_time FROM ... GROUP BY sch.[MU_ID] , sch.[AGNT_NM] , date_no_time
SOLVED. you da bomb.
Just to clarify - MAX is going to return the highest result, versus the lowest as in your example. Swap it with MIN for the results to match your example.
You'll need to find the delta. Inner join the table to itself on your patientID. So something like, with outer as ( select c.personid ,c.claim_date date1 ,c1.claim_date date2 from claims c inner join claims c1 on c1.personid = c.personid and c.claim_date != c1.claim_date ) select personid ,date1 ,date2 ,datediff(days, date1, date2) from outer This assumes SQLServer, but there are ways of doing it in PGSql and Oracle too that are very similar (possibly identical, though I know for sure you'd have to modify the datediff function.
select the row **once** at the beginning based on OrderRefNum. Get all relevant columns you need. Then run through your conditions. Give that a whirl and see what you come up with.
formatted: SELECT student.sname, course.course_number, grade_report.grade FROM rearp.student, rearp.course, rearp.grade_report WHERE ( grade_report.grade = 'A' AND course.course_number = 'COSC' ) OR ( grade_report.grade = 'B' AND course.course_number = 'COSC' ) ORDER BY course_number; 
Thank you that is much more readable. 
This is one of the rare occasions where I would endorse a table variable to avoid reading that `orderstransaction` table four times. declare @Results table (status int,expiry datetime, amount decimal (10,4)); insert into @Results select status,expiry, amount from orderstransaction where refnum = @orderrefnum; if (@@ROWCOUNT = 0) begin select @output = 1 return end if (select status from @results) &lt;&gt; 100 begin select @output = 2; end IF (DATEDIFF(MINUTE,@date,(select Expiry from @Results)) &gt; = 0) BEGIN select @Output = 3 return END IF (select Amount from @Results) &gt; @amount BEGIN select @Output = 4 return END
Simplified the WHERE clause: SELECT student.sname, course.course_number, grade_report.grade FROM rearp.student, rearp.course, rearp.grade_report WHERE grade_report.grade IN ('A','B') AND course.course_number = 'COSC' ORDER BY course_number;
isn't there a faster alternative heard that conditions in sql are bad..but thanks so much
Good call. That's much better.
Are you sure the course numbers are "COSC" exactly, and not "COSC-101" or something like that? You are doing equality checks. Just being sure!
Yes I am certain I am looking for COSC exactly
You're querying three tables but not telling it how to relate any of them. What is the common column(s) between the tables?
Yes.... rather than just list the 3 tables, you need to identify how the data in the tables relate to each other. That means either using JOINs or including the relation in the WHERE clause (ie: WHERE student.studentID = course.studentID) SELECT student.sname, course.course_number, grade_report.grade FROM rearp.student INNER JOIN rearp.grade_report ON student.studentID = grade_report.studentID INNER JOIN rearp.course ON grade_report.courseID = course.courseID WHERE grade_report.grade IN ('A','B') AND course.course_number = 'COSC' ORDER BY course_number;
How about something like: SELECT c1.* FROM claims c1 WHERE EXISTS (SELECT 1 FROM claims c2 WHERE c2.patientId = c1.patientId AND c2.claimDate &lt;&gt; c1.ClaimDate AND ABS(DATEDIFF(DAY, c1.claimDate, c2.claimDate)) &lt;= 21)
Maybe use CASE statement?
It depends on when/how they're being used. If you're given a blanket "conditionals are always bad", that might not be the best person/piece of advice to listen to. Given that you're testing four very different conditions here, other logic that attempts to bypass using an `if` statement will probably perform worse or be a lot messier to code &amp; maintain.
This is good code, except should not the order of the when statements be be reversed to match the OC's code, so that @output is 4 if the amount is greater than @amount even if the other criteria is met?
no, because he has RETURN inside each of the IFs. Just like the CASE statement, it would stop at the first true condition
Oops, you are right, I missed that.
I have fixed the table name [order] in the query above, so it should run without any changes. But I'll also break it down for you a bit since, if I'm reading your requirements correctly, this query will suit your needs and the others posted here will not. First, we declare the start date in a variable since it is referenced in the query multiple times and you may want to change it in the future... DECLARE @start date = '2017-01-01'; Then we use a Common Table Expression (CTE) to create a tally table, which is just a list of integers (0 to 99 in this case). Don't worry if you can't read the syntax, it's just generating a series. Here is the `SELECT` outside of the CTE syntax that you can run standalone... SELECT row_number() OVER (ORDER BY (SELECT NULL)) - 1 AS n FROM (VALUES(0),(0),(0),(0),(0),(0),(0),(0),(0),(0)) a(n) CROSS JOIN (VALUES(0),(0),(0),(0),(0),(0),(0),(0),(0),(0)) b(n) n| - 0| 1| 2| 3| ... Then we use the tally table to generate another CTE containing the intervals that you want to evaluate (Jan/Feb, Feb/March, March/April...). Here is the second `SELECT` outside of the CTE syntax (you still need the variable and the first CTE) that you can run to see the intervals... DECLARE @start date = '2017-01-01'; WITH tally AS ( SELECT row_number() OVER (ORDER BY (SELECT NULL)) - 1 AS n FROM (VALUES(0),(0),(0),(0),(0),(0),(0),(0),(0),(0)) a(n) CROSS JOIN (VALUES(0),(0),(0),(0),(0),(0),(0),(0),(0),(0)) b(n) ) SELECT n ,dateadd(mm,n,@start) AS interval_start ,dateadd(mm,n+2,@start) AS interval_end FROM tally WHERE dateadd(mm,n,@start) &lt; getdate() n|interval_start|interval_end| :--|:--|:--| 0|2017-01-01|2017-03-01| 1|2017-02-01|2017-04-01| 2|2017-03-01|2017-05-01| 3|2017-04-01|2017-06-01| 4|2017-05-01|2017-07-01| 5|2017-06-01|2017-08-01| 6|2017-07-01|2017-09-01| 7|2017-08-01|2017-10-01| 8|2017-09-01|2017-11-01| 9|2017-10-01|2017-12-01| 10|2017-11-01|2018-01-01| 11|2017-12-01|2018-02-01| 12|2018-01-01|2018-03-01| 13|2018-02-01|2018-04-01| 14|2018-03-01|2018-05-01| 15|2018-04-01|2018-06-01| 16|2018-05-01|2018-07-01| 17|2018-06-01|2018-08-01| 18|2018-07-01|2018-09-01| 19|2018-08-01|2018-10-01| Then we `INNER JOIN` your table `order` to the `intervals` CTE resulting in the original query I posted. I also removed the ticks around your IDs in `order_id IN (97,43,54)` because presumably (I could be wrong) they are integers and you are creating work for the engine by passing them as strings.
This is a horrible query.
Just cast the integer to char or vice versa: join course on section.course_Number = cast(course.course_number as numeric) and section.dept_id = cast(course.dept_ID as numeric)
That's what I may need to do. This entire access database, and the roughly 20 queries that came along with it, I all inherited from the previous IT staff. Haven't convinced them of alternative methods of pulling the data they need in the way they need yet.
I mean stuff like this: IIf(IsNull([PhysicalAddress1] &amp; [PhysicalAddress2] &amp; [PhysicalCity] &amp; [PhysicalState] &amp; [PhysicalZIP]),'','(P) ' &amp; IIf(IsNull([PhysicalAddress1]),'',IIf(IsNull([PhysicalAddress2]),[PhysicalAddress1],[PhysicalAddress1] &amp; Chr(13) &amp; Chr(10) &amp; [PhysicalAddress2])) &amp; Chr(13) &amp; Chr(10) &amp; [PhysicalCity] &amp; ', ' &amp; [PhysicalState] &amp; ' ' &amp; [PhysicalZIP] &amp; Chr(13) &amp; Chr(10)) &amp; IIf([SameAsPhysicalAddress],'',IIf(IsNull([MailingAddress1] &amp; [MailingAddress2] &amp; [MailingCity] &amp; [MailingState] &amp; [MailingZIP]),'','(M) ' &amp; IIf(IsNull([MailingAddress1]),'',IIf(IsNull([MailingAddress2]),[MailingAddress1], [MailingAddress1] &amp; Chr(13) &amp; Chr(10) &amp; [MailingAddress2])) &amp; Chr(13) &amp; Chr(10) &amp; [MailingCity] &amp; ', ' &amp; [MailingState] &amp; ' ' &amp; [MailingZIP])) AS ChurchAddrApptsAddressBlock What is this even trying to do?
I formatted your code in Notepad++. Hope this helps a little bit. SELECT CurrentChurchData.ConferenceName ,CurrentChurchData.STATUS ,\[ChurchName\] &amp; IIf(IsNull(\[ChurchStatus.Code\]), '', ' \[' &amp; \[ChurchStatus.Code\] &amp; '\]') &amp; ' - ' &amp; Right(\[ChurchID\], 3) AS ChurchAddrApptsChurchNameNumber ,CurrentChurchData.SameAsPhysicalAddress ,IIf(IsNull(\[PhysicalAddress1\] &amp; \[PhysicalAddress2\] &amp; \[PhysicalCity\] &amp; \[PhysicalState\] &amp; \[PhysicalZIP\]), '', '(P) ' &amp; IIf(IsNull(\[PhysicalAddress1\]), '', IIf(IsNull(\[PhysicalAddress2\]), \[PhysicalAddress1\], \[PhysicalAddress1\] &amp; Chr(13) &amp; Chr(10) &amp; \[PhysicalAddress2\])) &amp; Chr(13) &amp; Chr(10) &amp; \[PhysicalCity\] &amp; ', ' &amp; \[PhysicalState\] &amp; ' ' &amp; \[PhysicalZIP\] &amp; Chr(13) &amp; Chr(10)) &amp; IIf(\[SameAsPhysicalAddress\], '', IIf(IsNull(\[MailingAddress1\] &amp; \[MailingAddress2\] &amp; \[MailingCity\] &amp; \[MailingState\] &amp; \[MailingZIP\]), '', '(M) ' &amp; IIf(IsNull(\[MailingAddress1\]), '', IIf(IsNull(\[MailingAddress2\]), \[MailingAddress1\], \[MailingAddress1\] &amp; Chr(13) &amp; Chr(10) &amp; \[MailingAddress2\])) &amp; Chr(13) &amp; Chr(10) &amp; \[MailingCity\] &amp; ', ' &amp; \[MailingState\] &amp; ' ' &amp; \[MailingZIP\])) AS ChurchAddrApptsAddressBlock ,IIf(IsNull(\[PhoneNumber\]), '', 'Ph: ' &amp; \[PhoneNumber\] &amp; ' ') &amp; IIf(IsNull(\[FaxNumber\]), '', 'F: ' &amp; \[FaxNumber\]) AS ChurchAddrApptsPhoneBlock ,IIf(IsNull(\[ChurchEmailAddress\]), '', 'Email: ' &amp; \[ChurchEmailAddress\] &amp; IIf(IsNull(\[Website\]), '', Chr(13) &amp; Chr(10))) &amp; IIf(IsNull(\[Website\]), '', 'Web: ' &amp; Mid(\[Website\], InStr(\[Website\], "#") + 1, InStrRev(\[Website\], "#") - InStr(1, \[Website\], "#") - 1)) AS ChurchAddrApptsInternetBlock ,ContactAppts.SortOrder ,IIf((\[ContactAppts\] ! \[ChurchAddrApptsYearsNameRole\] = ','), '', \[ContactAppts\] ! \[ChurchAddrApptsYearsNameRole\]) AS ChurchAddrApptsYearsNameRole ,ContactAppts.OrdinationStatus ,ContactAppts.PastorCode ,CurrentChurchData.ChurchID ,ContactAppts.LastName ,ContactAppts.FirstName ,ContactAppts.MiddleName ,CurrentChurchData.WorshipAttendance ,ContactAppts.ConferenceAppointment FROM ChurchStatus RIGHT JOIN ( CurrentChurchData LEFT JOIN ContactAppts ON CurrentChurchData.ChurchID = ContactAppts.ChurchID ) ON [ChurchStatus.ID](https://ChurchStatus.ID) = CurrentChurchData.STATUS WHERE ( ( (CurrentChurchData.STATUS) NOT IN ( 5 ,6 ,7 ,8 ) ) AND ((ContactAppts.ConferenceAppointment) IS NULL) ) OR ( ( (CurrentChurchData.STATUS) NOT IN ( 5 ,6 ,7 ,8 ) ) AND ( (ContactAppts.ConferenceAppointment) NOT IN ( 1 ,14 ) ) ) OR ( ((CurrentChurchData.STATUS) IS NULL) AND ((ContactAppts.ConferenceAppointment) IS NULL) ) OR ( ((CurrentChurchData.STATUS) IS NULL) AND ( (ContactAppts.ConferenceAppointment) NOT IN ( 1 ,14 ) ) ) ORDER BY CurrentChurchData.ConferenceName ,\[ChurchName\] &amp; IIf(IsNull(\[ChurchStatus.Code\]), '', ' \[' &amp; \[ChurchStatus.Code\] &amp; '\]') &amp; ' - ' &amp; Right(\[ChurchID\], 3) ,ContactAppts.SortOrder ,ContactAppts.LastName ,ContactAppts.FirstName ,ContactAppts.MiddleName;
This would be fun to rewrite and make efficient. I hope they allow you to do that!
May be costly for them, really all of them need/could use rewrites. 
Just convert the integer to a varchar ON a.varchar = b.convert(varchar,intfield) 
Greenplum (https://greenplum.org/) is an open source parallel processing platform built on top of PG. Beyond a solution like this, you are just load balancing - the balance of a query will only run on a single server. That's my understanding, at least - someone with may experience may be able to correct me.
Great, that Greenplum looks promising. Do you happen to know if Presto or Hive have this capability out of the box?
I'm honestly not familiar enough with the mechanics of either of those to give a confident answer. I do know that Hive is distributed, but I'm not sure at what level, or what complexity.
Awesome, thanks for your help!
If you're using a tool that can do CTEs (common table expressions, very, very useful), you could do the below, it should be very easy to understand: ;with PostsViewdByUser1 as ( Select distinct post_id from View where user_id = 1 ) ,PostsViewedByOtherUsers as ( Select distinct post_id from View where user_id in (2,3,4) ) Select * from Posts Where post_id not in (Select post_id from PostsViewdByUser1) and post_id in (Select post_id from PostsViewedByOtherUsers) If you're looking for some very hands-on "learn-by-doing" practice problems, that teach basic to advanced SQL with well-designed, real-world practice problems, similar to what you're trying to solve, check out [SQLPracticeProblems.com](https://sqlpracticeproblems.com/#utm_source=r1&amp;utm_medium=r1&amp;utm_campaign=r1). I developed it after teaching a SQL course where the material I had to to teach from was poorly structured and academic. Afterwards, the students emailed me, saying they needed practice problems, so I developed the course! Contact me (email in the FAQ) if you have questions. Also, redditors get 30% off the Professional package (discount code reddit30). 
Thanks a ton. It totally worked. Now what if I want to sort it by View.created\_at? Don't worry, this is my last question. :-)
Most databases now can use window functions, which will allow this to be solved very easily. For instance, if you have data like this: PatientID ClaimDate 1 2018-05-01 1 2018-05-10 1 2018-06-15 1 2018-07-20 2 2018-01-01 2 2018-01-19 2 2018-02-15 2 2018-03-15 2 2018-03-20 ... you could do a query like the following: ;with ClaimDataWithDaysBetween as ( Select PatientID ,ClaimDate ,DaysAfterPreviousClaimDate = datediff( d , Lag(ClaimDate, 1) over (Partition by PatientID Order by ClaimDate) ,ClaimDate ) From ClaimData ) Select * From ClaimDataWithDaysBetween Where DaysAfterPreviousClaimDate &lt;= 21 If you're looking for some very hands-on "learn-by-doing" practice problems, that teach basic to advanced SQL with well-designed, real-world practice problems, similar to what you're trying to solve, check out [SQLPracticeProblems.com](https://sqlpracticeproblems.com/#utm_source=r1&amp;utm_medium=r1&amp;utm_campaign=r1). I developed it after teaching a SQL course where the material I had to to teach from was poorly structured and academic. Afterwards, the students emailed me, saying they needed practice problems, so I developed the course! Contact me (email in the FAQ) if you have questions. Also, redditors get 30% off the Professional package (discount code reddit30). 
 If you're looking for some very hands-on "learn-by-doing" practice problems, that teach basic to advanced SQL with well-designed, real-world practice problems, similar to what you're trying to solve, check out [SQLPracticeProblems.com](https://sqlpracticeproblems.com/#utm_source=r1&amp;utm_medium=r1&amp;utm_campaign=r1). I developed it after teaching a SQL course where the material I had to to teach from was poorly structured and academic. Afterwards, the students emailed me, saying they needed practice problems, so I developed the course! Contact me (email in the FAQ) if you have questions. Also, redditors get 30% off the Professional package (discount code reddit30). 
You dont mention what flavor of SQL you're working with. but, I'm not sure of a SQL dialect where this is valid syntax: Student.Name_Last Not Like ('') But, you shold probably just say Student.Name_Last &lt;&gt; '' if you're not going to provide a pattern with wildcard characters. The main way to improve query performance would be to add covering indexes to the referenced tables. But, its hard to say without understanding the pain points of the query. I find it hard to believe that a student database would have enough records to run into query performance issues.
THANK YOU! works like a charm and your explanation did guide me through in understanding better. much appreciated for your help :)
True it wont compare %1% in the statement i provided but he is sure that value will be 1. Also if we had to compare it to %1% in decode function you ca do something like this decode(instr(a.column1,'1'),0,'Audience','Lecturer') Decode also provides default value option. Say if all the matches fail then it will set it as the default provided. Somehow I am a fan of decode function :) 
Joins missing is a flaw but that should not give no rows found. Try running only this: Select * from student, grade_report, course ;
Good question, I often ask myself where I stand in relationship to the rest of my SQL peers. Where I work, I think understanding grain, complex queries/stored procedures, indices, execution plan, and being able to articulate and implement SQL functions is 'intermediate'. 
Create a view 
Study dimensional modeling also if you want. Star schemas, dimensional tables, fact tables, etc..
Disclaimer: I am by no means an expert, just someone who's written a few stored procedures. * I don't think you need to declare `@` variables, just start using them. If you want a variable that you're only going to use inside a stored procedure, then declare it without the `@`: declare EmployeeID int; * You can use the `select into` syntax for setting these variables: select ID into EmployeeID where TaxID=whatever; * You can also use `set` to assign an expression to a variable. I don't know if you can use the `set` keyword to assign a variable out of a select like you're doing in your code, I've always used the `select into` method.
If you're doing just queries I'd consider that pretty entry. Intermediate SQL should be dealing with database structure, architecture and maintenance. Though, then the only difference between intermediate and expert may be age and experience (not that that's a bad thing). Things I'd expect from an intermediate SQL guy: * Laughs when I ask him to do a simple view function that groups an n X m table. * Rethinks about it when I tell him the table in question is 200GB. * Rethinks working for me when I tell him he has to do it with 8GB of ram. * Knows what triggers are and how they're useful. * Acknowledges that they're rarely used in full-stack development roles but are useful to know. * Can explain what regex is and can do a simple regex filter (still boggles my mind sometimes) * Knows how to define SQL view and window functions. * Knows basic to advanced administration in their favourite database CLI/GUI/Client (for instance, in MSSQL, I'd expect an intermediate guy to be able to tell me something I don't know about SQL Server (I'm more of a Postgres guy but who cares?).
Damn... guess I'm still solidly a beginner then!
Check the following web site [http://www.studybyyourself.com/seminar/sql/course/?lang=eng](http://www.studybyyourself.com/seminar/sql/course/?lang=eng). The course, along with examples, is quite easy to follow. You can submit exercises as well. Everything for free. 
Everyone one of the folks posting answers here so far is out their god damn mind. I think you’re all confusing the role of a dba and someone who writes sql to describe a data set. Who knows though, maybe the question was actually directed toward an intermediate level of relational database understanding. 
It depends. Posts and views are 1-n, so it's unclear how to sort post by multiple views. You can for example sort by the last view: select * from [posts] p left join ( select v.[post_id], max(v.[created_at]) as [created_at] from [views] v group by v.[post_id] ) lastView on lastView.[post_id] = p.[id] where exists(select 1 from [views] v where v.[post_id] = p.[id] and v.[user_id] in (2, 3, 4)) and not exists(select 1 from [views] v where v.[post_id] = p.[id] and v.[user_id] in (1)) order by lastView.[created_at] or by first view, if you replace max() with min(). Also a question what to do if post doesn't have views at all. In my query I use left join, so posts without views will remain, but they don't have corresponding lastView.\[created\_at\], so will be ordered as nulls. At least SQLServer orders nulls at first place in ascending order. You can "fix" that by replacing order with `order by isnull(lastView.[created_at], '%some very big or small date constant%')` or if you just don't need posts without views, replace `left join` with regular `join` (which is inner by default)
What is advanced? Imo being able to just query is beginner, anything past that is intermediate
And what if you call your SP from SSMS? If result is ok, seems like you have error in C# code. If you use plain [ado.net](https://ado.net), maybe you just forgot to set right Direction for output parameter of your command. And also. Technically stored procedures are mainly used to return datasets (tables), not single answers with output parameters. But you can, if you need it hard.
That street magic with dateadd and dense\_rank() works only if you have no gaps between days in table, am I right?
I personally dont use SQLite, but i'm pretty sure you can do what your asking with Group_Concat() so something like this? SELECT a.parentID, GROUP_CONCAT(a.childID,",") AS childrenIds FROM Table a GROUP BY a.parentID
Yes, you are right on both points. This helped me make it all work. Thank you very much! 
I know right? Ive learned about some of the dba things listed here but I have no experience with them since I am not a dba so im unfamilar with most of it.
yes!!! that worked. Thank you very much
Interesting I guess I should learn that side of SQL more then, got any resources you suggest looking into? Not a DBA so I cant practice them at work.
Google “tally table”.
Thanks for the resource. Just another question because 897 seems like a lot of rows to be missing; could a trailing space on the end of the number I'm grouping cause issues? 
- Windowed Functions - Triggers - User defined scalar functions - Table variables (how and when to use) - Temp Tables (how and when to use) - PIVOT / UNPIVOT - Ability to understand an execution plan
Its hard to say but when looking at the sql it is normal that grouped sql return less values. That is the purpose of group by to group same values into 1. The second sql to me says retun all distinct ZPC_MODEL_SKU which have count &gt; 0 and return the count. So if multiple rows have same ZPC_MODEL_SKU that would mean that row count would be less. Only time it would be the same it would be if ZPC_MODEL_SKU would be unique in all rows.
You should go line by line and see where the error is. Check first if either @TaxID or @JobNum are NULL, then @EmployeeID, finally @PhoneNumberID. This will let you know which part of your code is not working. For example: @TaxID nvarchar(50), @JobNum nvarchar(50), @ReturnID int = NULL OUTPUT declare @EmployeeID int; declare @PhoneNumberID int; --set @EmployeeID = (SELECT [ID] from tblEmployees where [TaxID] = @TaxID); SELECT @TaxID; SELECT [ID] from tblEmployees where [TaxID] = @TaxID; --set @PhoneNumberID = (select top 1 [ID] from tblPhoneNumbers where [ID] IN (select [PhoneNumberID] from tblJobs where [JobNum]=@JobNum) and [EmployeeID]=@EmployeeID order by [PhoneNumberDateTime] Desc); SELECT @JobNum, @EmployeeID; select [PhoneNumberID] from tblJobs where [JobNum]=@JobNum; select top 1 [ID] from tblPhoneNumbers where [ID] IN (select [PhoneNumberID] from tblJobs where [JobNum]=@JobNum) and [EmployeeID]=@EmployeeID order by [PhoneNumberDateTime] Desc; --set @ReturnID = (select [ID] from tblJobs where [PhoneNumberID]=@PhoneNumberID and [JobNum]=@JobNum); SELECT @PhoneNumberID, @JobNum; select [ID] from tblJobs where [PhoneNumberID]=@PhoneNumberID and [JobNum]=@JobNum; This will tell you which of your subqueries and/or variables are causing the issue.
You would have to reload the file in every day to have it reflected in SQL. That could involve setting up a SQL job that is scheduled to run an SSIS package every morning that loads in your file again.
I think we need some more detail here. A quantity of what? What do you specifically want the stored procedure to do, and what data columns to do you have to work with. Are you talking about creating a stored procedure to generate replenishment orders to refill stock? If this is a homework question, just add the whole question.
Doing it w/DATEADD only works if your "islands" are contiguous regions of some regular interval. If your data has intermittent frequency but you're trying to find ranges of contiguous values you can use ROW_NUMBER() ordered by your date . E.g.: ;WITH grouped AS ( SELECT ColorGroup = ROW_NUMBER() OVER (ORDER BY [Date]) - DENSE_RANK() OVER (PARTITION BY [Color] ORDER BY [Date]) , [Hours], [Type], Color, [Date], ID FROM (VALUES (10, 'A', 'Black', CAST('2018-09-01' AS DATE), 1) , (10, 'B', 'Black', CAST('2018-08-15' AS DATE), 2) , (10, 'A', 'Black', CAST('2018-07-30' AS DATE), 3) , (10, 'B', 'Red', CAST('2018-07-29' AS DATE), 4) , (10, 'A', 'Blue', CAST('2018-07-29' AS DATE), 5) , (10, 'B', 'Black', CAST('2018-07-28' AS DATE), 6) , (10, 'A', 'Red', CAST('2018-07-27' AS DATE), 7) , (10, 'A', 'Red', CAST('2018-07-26' AS DATE), 8) , (10, 'A', 'Red', CAST('2018-07-25' AS DATE), 9) , (10, 'B', 'Blue', CAST('2018-07-24' AS DATE), 10) ) yourtable ([Hours],[Type],Color, [Date],ID) ) SELECT [Type], Color, StartDate = MIN([Date]), EndDate = MAX([Date]) , TotHrs = SUM([Hours]) FROM grouped GROUP BY [Type], Color, ColorGroup HAVING SUM([Hours]) &gt;= 20 
Why do you need the Excel intermediary? You could write an ssis package to pull the data directly
At least you know there is more you don't know so keep on learning!
How?
Right. If anything, missing joins should result in spamming data due to the resulting cartesian product.
I agree with /u/highhimscott. The problem is people see "SQL" in this subreddit and they automatically assume you mean Microsoft SQL Server. That drives me nuts. If I'm looking for someone with intermediate SQL experience, I'm looking for SQL query stuff. I want someone who knows the proper syntax of updates, inserts, deletes - all of that basic stuff. This includes properly joining tables (left join, inner join, etc). I am also looking for experience of things such as SUBSTRING, COUNT, MAX, DISTINCT, GROUP BY, SUM, date manipulations, CEILING/FLOOR/ROUND. If someone wants PL/SQL or TSQL experience, that should be specified in the requirements.
 CREATE TABLE #t1 ( val float) GO INSERT INTO #t1 VALUES (RAND(0)) GO 100 SELECT COUNT(*) FROM #t1 DROP TABLE #t1 
Exactly. There are multiple rows in the table that have the same ZPC\_MODEL\_SKU. There are rows in the second query with counts &gt; 1. If you sum the counts, you'd get the number of rows in the first query. There is no missing data here.
Yes. You don't really want a real-time connection between the two and using Excel as an intermediary is clunky at best. SSIS, a PowerShell script, or a custom application which pulls the data and then loads it directly into the database will be a much better way to go. You *could* do it with Excel, but now you're into scripting within your worksheet and having everything tied in with an extra, unnecessary application.
&gt; The problem is people see "SQL" in this subreddit and they automatically assume you mean Microsoft SQL Server. That drives me nuts. What you described as what you "look for" is applicable to SQL Server though, and generally any RDBMS.
That's a really long explanation, and I'm probably not the best person to answer that. Google "ssis get data from HTML". First couple of results are pretty good. There's even a YouTube video titled "ssis html table source - Web scraping without coding..."
I'd consider myself intermediate and I have nearly a decade of experience using SQL and am currently employed as a DBA. Unless you are really really good 1.5 years puts you at basic. I have met only 3 people I would consider advanced, 2 of which were very experienced DBAs and the other a mct accredited trainer who trains DBAs. 
Would this be a good use case for a trigger?
Some of the answers I disagree with, I mean stuff like knowing regex is not useful for anyone working on a decently designed DB. However to be intermediate level you should really be able to know about the performance side of things. Being able to figure out syntax and write queries is still basic. It's one thing to be able to pull out the results set you need. Its another entirely to do that in an efficient and sargable manner. For that you need to understand indexes, statistics, query plans, different types of join. If your queries produce the correct result set but scan a million rows to produce 5 records then you are very much at a basic level because you need someone else to go over your work to check its not going to kill the live server when deployed. You need an understanding of the performance side of things to be able to write queries that are not going to cause major issues.
Your sub-query: Select job From Faculty Where Job not in ('Administrative','General Services','Human Resources') Two things. First, I'm assuming that faculty is everyone. You probably have duplicates in that sub-query. Change it to select distinct. Secondly, sub-queries frequently screw up your query plan and make you utilize indexes poorly or not at all. Change it to a CTE ; With subx as ( Select job From Faculty Where Job not in ('Administrative','General Services','Human Resources') ) Select bla bla bla *** As others have said, converting your ID from INT to VARCHAR is going to ignore any indexing you have. I wouldn't be surprised if this query has a lot of table scans.
LIKE can cause you to read the entire field. (Sometimes). Avoid using LIKE when looking for exact string matches. Just use = or &lt;&gt; as Mr GustaDerp suggests
Dude. Never use select * .. your first line, change it to If not exists (select 1 from ..) Is OrdersTransaction a really big table? It sounds like a big table
Excel 2016 has a function called PowerQuery. You can mess with the settings to have the workbook refresh the query whenever it opens. Another option is to write some VBA that contains the SQL and connect that to the event of opening the workbook
These are applicable to SSMS, but the converse is not true; that is, not everything that you'd want to know for SSMS is applicable to SQL in general. I think that's the point being made here
Querying tables, joining tables, reading/creating data structures is all the same between systems. What you're referring to is T-SQL which is Microsoft's specific flavor of SQL. The overall theory is the same, the specific approaches are different.
Just keep messing around and force yourself to do things you don’t think you will be able to do. Just don’t do anything that will accidentally mess up the data. 
I've gone through at least 3 or 4 regex tutorials and I "get it" but there's some regex wizardry that still blows my mind.
This is exactly what I was looking for and reflects some of the responses on here, thanks for that. I am actually working toward getting into a data engineer position like your flair shows.
My company calls me a "Data Engineer" but I don't consider myself one, I feel more like a DBA. Data Engineering to me is working with big data, python, SQL, and just creating ETL and proper data architecture. To a degree, similar to a data architect. Some companies may have those roles separated, others merge them. I've done some work with Python and I've worked with tables in the past as large as 6TB, but it feels the same to me as most jobs are. I've actually blogged about what my day to day is like and why I felt the jobs I have are very similar too. The number one thing you can ever do for your career and development will be networking. Start attending data and technology meetups, even online meetups are great. (SQL Server has one called Group By.) The second thing you can do is continue developing yourself. This is broad, incredibly broad. You can achieve this from posting online to figure out direction to apply yourself (like you did), certify, blog, podcast, volunteer, help people online, work on a portfolio of projects, read, watch tutorials, experiment, etc. The main thing I would recommend is to create a way to document and showcase this. When an employer asks me questions about how I document things or what my skill set is like or other applicable questions, I refer them to my Github / blog / stack overflow / reddit and anything else they may be interested in. By taking the second advice I gave you and creating a visual story from it, it becomes substantially more valuable to you than just the results themselves. 
Thanks for the detailed response, got a link to your blog? Would like to read it. I am trying at the moment to sharpen my SQL and Tableau and then expose myself to Python.
It does actually, I was supposed to look at Explain Analyze and see the query plan and work off of that. 
There's rarely just one option, but this is one. You have to keep it mind that Excel is designed to be used by non-developers. And you're working now with a language as a developer--Excel has some logic to grab new results every day. And you can automate data to be loaded daily in a similar fashion. As a developer, you have the power to build a whole bunch of stuff. In doing so, you lose somewhat other people solving your problems for you. Excel thinks we might like a daily refresh, so it does it. But in SQL, we are always specifically, in great detail, telling the computer what we want to happen. And if we don't tell it something, it won't. It isn't a manual update btw. You're just automating the thing that you did once already to happen on its own every day. 
Can you approach it from the other way around, import the data into a db table and get Excel to read that data out? You could look at using SSIS to bring the data in, run under an agent job to automate it.
Not using subqueries is a start.
Ooh :) what approach would you use for updating it daily? 
You should know how to manipulate data - grouping it in various ways, making calculations, join multiple datasets in various ways, pivoting, writing fairly complex stored procedures, using cursors. Basically, be able to do all the stuff in SQL that one would do with pivot tables and vlookups in Excel. Or maybe I am describing advanced level SQL. If so, ignore stored procedures i guess. At least the complex variety that might run into hundreds or thousands of lines.
You have to use subqueries for some things though?
After you’re comfortable with the basics, I’d say start looking into joins and unions. The ability to work with multiple tables is a MUST. Then start looking to how to make procedures and views. There is decent money to be made in making reports so just kinda learn as you go 🤷🏻‍♂️
Since you're using SSMS, I would create an SSIS package that can load the file into SQL and schedule it to run daily in a SQL job. As someone else mentioned it would be better perhaps to load the data directly from the website to a table, but while a little clunky, this method is probably a bit easier to execute and can still give you what you need.
Expanding on my own comment (MS SQL based but extrapolate for other DBMS's, focused on data analysts, not DBA tasks, nor DB architect tasks): SQL Student: - SELECT 'HELLO WORLD' SQL Beginner (has enough skills to be useful when knows all the below): - SELECT [Columns] FROM [Table] WHERE [CRITERIA] ORDER BY [Columns] - JOINS (INNER and OUTER) - Difference between a table and a view - GROUP BY, SUM, AVG, MIN, MAX, COUNT, HAVING - Execute stored procedures SQL Intermediate: - Windowed Functions - Triggers - User defined scalar functions - Table variables (how and when to use) - Temp Tables (how and when to use) - PIVOT / UNPIVOT - Ability to understand an execution plan - Author basic stored procedures - CASE SQL Advanced: - Deep execution plan understanding and optimization (most important!!) including impact of partitioning and compression. - Common Table Expressions (could be intermediate?) - CURSORs (teaching these too early to programmers often leads to bad habits IMO) - Sophisticated SP authoring with IF's and WHILE's, @@RAISERROR etc, including dynamic SQL - User Defined Types - User Defined Aggregation Functions - User Defined Table-valued Functions - Use of system tables/DMV's - Integration with non-SQL code (CLR or R in MS SQL) - Spatial Data Types - Design of indexed views, and SCHEMA_BOUND. - Graph Tables - XML queries - Full Text Catalog use - Importance of statistics - Filtered indexes - FILESTREAM data type
It's a use case at best. But I wouldn't call any need for a DML trigger a good use case.
Ok, that sounds good. I was doing some joints and unions on the course but I appreciate I rushed through a lot Procedures and views. I will bare that in mind. Many thanks
In what way can I mess up the data? Am I not reading it when I do SQL?
I don't think there's a way to do that. Why not just do the inner SELECT, sans the FOR XML part, into a temp table, store @@ROW_COUNT in a variable, and then set @tableHTML with the sub-select from the temp table?
Or instead of a trigger run the procedure from a SQL job on a schedule.
&gt; everything tied in with an extra, unnecessary application which will just bog it all down This...and being one copy-paste from Word/Outlook away from making a huge mess.
Few options here but you pretty much answered you own question. Your where filters cause the total set to be the same as your desired result. One trick would be to remove the filter from your where statement and put it in a case select. Something like: `sum(case when ([Clients].doa) Between Forms!Search!datebegin_textform And Forms!Search!dateend_textform then 1 else 0 end) / count(1) .` 
An API isn't a thing you can upload or download. You program against it, send requests/data into it and get data back from it.
&gt; the ability to use a functional api to isolate the table from downstream users Are you referring to some sort of abstration or base table obfuscation like limiting users to accessing data via views and procs? 
 Depends on how you write your query. Remember this very important flow: FROM WHERE GROUP BY HAVING SELECT ORDER BY Your FROM is the first thing your SQL engine would parse. If it has sub queries that has crap like (SELECT * FROM TABLENAME) it's gonna be a bad time. If I don't have a choice but to use subqueries, I develop a table with indexes that I can populate first and then use that table as a JOIN straight up rather than a subquery. NEVER use functions. You don't want your current cursor to switch between indexes. If you ABSOLUTELY have to use subqueries, go with a temp table variable or a common table expression and join that in instead. This way your SQL engine isn't grinding each time you perform JOINs. Especially if you're dealing with data &gt; 10,000 rows. Again, this is just my 2 cents from all from personal experience of working with SQL as a DBA for the past 8 years.
I meant I didnt have a choice since I dont have access to create tables, I can only query from existing ones. I usually have to a subquery to pull some data then I use joins to get the final results.
Agreed, that's where it gets tricky. Subqueries are what I call the lazy-man's way of generating relationships between tables. If there's no straight relationships, you try to create them first. Using dynamic columns that you can populate based on the key fields. Again, this is all going to depend on the schema of a database. Are you dealing with an ERP like SAP?
1200 connections to a POS system should be easy to handle if the app and DB are designed properly. A cashiers at a POS will not hammer the database the way reports or automated services do. Setting up a read only Replication or Availability Group would allow reports to do what they do, and minimize contention.
I guess it's possible a temp table might help I've never used one before though so I'm not exactly sure. 
the latter
none of what i listed was dba stuff. it's just basic sql competency.
&gt; I just wanted to be sure that if I did do NLB that it would still be able to cluster. I basically just wanted to be sure it wouldn't cause issues especially as most information I've been seeing (see: almost all) says that the load balancing isn't typically necessary. I think the message to take away from this is that clustering behind NLB shouldn't be a concern because NLB is not something that people put in front of SQL Server in the first place. &gt; The application being designed properly would be a worry of mine. While we are able to customize a lot of it, the application is third party. So I wouldn't be able to say with any certainty that it WOULD be designed properly, and if it wasn't, if we could change that. A couple points: 1) Talk to the vendor about scalability. **Now**. Load balancing, availability groups, all of it. Find out what they recommend. Don't just rush into load balancing, Always On, clustering, etc. without consulting them. 2) Just because the software is customizable doesn't mean that you should go nuts doing so. I've been down that road several times. You may find yourself being unsupported by the vendor (because they can't support the changes you've made) and/or upgrading to a new release really means re-implementing your customizations. Seen both happen. If the application is designed to be customizable, you do it *completely* within the parameters the vendor gives you - done that way, you'll have a fighting chance on upgrades. &gt; 1 at a remote location in case our main office burned to the ground or something. What's your plan for keeping the DR site ready to spin up? &gt; No one here has ever worked with SQL servers to this extent. Bring in a consultant who has. Listen to what they say and actually do what they recommend (I've seen many dollars spend on consultants, only to watch management ignore everything they just paid for). PM me if you need names. Yes, it'll add a few bucks to the project budget. But it'll be a **lot** cheaper than having to undo mistakes made after you go live. &gt;As a result, if it seems the definitive answer is that we simply shouldn't be doing it, then that is as valid an answer as any. I'm not saying that you "definitively shouldn't be doing it" but I *am* saying that you don't have enough information about the application itself to make a decision. And you won't until you've consulted with the vendor *and* put a production workload on the system.
Sure, [here's my post](https://jonshaulis.com/index.php/2018/07/04/what-is-the-average-day-of-a-dba-part-1-of-4/) talking about my average day. It's a part of a series, but this is the average typical day piece. You can see [all of the posts here](https://jonshaulis.com/index.php/blog-posts/) and see if any are more interesting to you. SQL is a solid skill as is Python, both are usable independently and together. I haven't seen that same flexibility with Tableau but it is definitely an in demand skill to have as well!
Yeah deal mostly with sap but we have a couple of other tables outside of it
You are but there are also update statements, drop table, etc...
I can post the explain in the morning. There are no indexes as far as I'm aware. Tickets has maybe 80k, work_items has 800k, days has a couple hundred and users has a couple thousand.
&gt; There are no indexes as far as I'm aware. That's a large part of your problem right there. 
/u/WhoahCanada Your guess was pretty much correct. I recommend creating a table that only your app uses. Create a clustered index if you have more than one key field. Dump data into it and use that to JOIN to your dataset.
Agreed. If your company permits, the first thing you need is an index. Simplest example: Imagine searching through a dictionary for a word without indexing the page numbers of the first letter. How are you going to find it in a short time?
I really appreciate it. I’ll try it tomorrow when I’m at my internship. They don’t have anyone who has tried to do this, so I’m sort of learning "on the job. " Thankfully, they’re supportive of it. Haha, I had an inkling that my answer was in the question, but I just didn’t know how to go about it. Thanks again! 
I'm not sure if I totally understand the question but have you tried using AND? SELECT * FROM table WHERE customer_id = 5 AND customer_order_number = 10 or if there's a lot of order numbers SELECT* FROM table WHERE customer_id = 5 AND customer_order_number BETWEEN 100 AND 500 idk if this answers your question. if it doesn't, tell me why and I'll try again
could you also post the combined query?
Thanks! Does it have to do with the query only having to perform the first one once with a temp table rather than multiple times every time the second calls back to the first?
I don’t really get it. Like the other guy said, you can filter with matching criteria with an AND. I noticed that you mentioned that these columns exist in 2 tables. I’m not sure if this is the issue, but could it be that you need to join the tables? If you haven’t learned joins yet, I would recommend looking into them, as they may address the issue. 
Based on there being no indexes on the tables, and an ad-hoc query that's been running for 8+ hours I doubt the company even has a DBA.
It's incredibly easy, so it's worth a shot. After the list of things you select, before FROM, put INTO #TABLENAME. Name it whatever you want. And it acts like a normal table only using the info you put into it.
Thanks for replying and sorry for my bad explanation! I'll try again. I have a list like this. Customer_id, customer_order_number Eg 0001, 1001 0002, 99985 0125, 1001 I want to get the information from a table for the customer that matches their order number. Customer ids are all unique but different customers might have the same order number. Currently I have used something like below. Select * from table1 Where customer_id in (list of ids) and customer _order_number in (list of order ids) But this gives me other customer_ids that match the order_numbers. 
I have used joins a little. Table 1 contains lots of information so I am trying to find data for only those that also exist in table 2.
Sorry for lack of information been writing from phone and it crashed so forgot to write that part again when I redid it. Hopefully everything makes more sense now. 
how are you generating the list of IDs? another select or just a hard coffees list of IDs?
Different sql I think. Just given a hard list of ids and order ids bassically. 
I think you mean something like this? (works in Db2): select * from syscat.tables as t where (tabschema, tabname) in (VALUES(‘DB2','CUSEXT’) , ('DB2','CUSMAS') , ('DB2','CUSTHIER') , ('DB2','SHIPPING_LOCATIONS') ) with ur 
I don't have MySQL experience, but what on earth does that MySQL even *mean*? If I have a red bowler hat, green bowler hat, red cowboy hat, and orange cowboy hat, and then select all columns, grouping by hat type - what's the "color" of the bowler hat row? How to do it in MSSQL would depend on what you're expecting to happen.
This allows you to specify pairs of values in a list. I think that's what you meant, but not sure.
I never understood why MySQL allows code like that to run - no other RDBMS does this. I'm not even entirely certain what it does, but my GUESS is that it would be a select distinct?
&gt; I never understood why MySQL allows code like that to run For the same reasons MySQL shipped with really bad defaults for many years, or has a history of doing [other weird things](https://stackoverflow.com/a/332213/1324345) instead of throwing errors or at least warnings?
What do you get when you run that second query in MySQL? How does MySQL even know what to do in that case? For that matter, why are you using `GROUP BY` without an aggregate function?
Check the fiddle example I posted in my edit to play around with it. As OP pointed out, it groups by the requested columns and then returns the top row of data for the non-grouped columns.
So if I'm looking at tables I didn't create, I can run a group by clause to see if there are other relationships with the data I haven't identified, ie. other columns that are pulling down the same value on every record
"top" based on what criteria? Results are not ordered unless `ORDER BY` is specified.
A `limit 1` without specifying an `order by` yields nondeterministic results. You are not guaranteed to get the same result every time.
When you're learning a tables schema you don't always use an aggregate function. These are tables I didn't build or have documentation on. So just looking at what's in them, and the relationships between the columns
It appears to be based on the order in which the records are inserted into the table based on my limited testing. I'm not sure that's entirely important for OP, however.
I would suggest doing something like this instead, using a count for your aggregate function at whatever level you're trying to analyze. http://sqlfiddle.com/#!9/cd737b/5
Yeh, I don't care what order they're in, I'm just pulling them out as an example of what data is in the same record where this group by happens. I'm not using this to create/update/delete/hail cthulu
Thanks sHORTYZ, judging from the responses I guess the answer is no to the initial question.
No thanks when everything's working, all hell when anything's not. 
If you're planning on learning SQL only first then your end job will probably be a data analyst, with the possibility of moving on to be a data scientist. SQL is a great one to learn and the role of a data analyst is highly sought after in industry at the moment. Every front end system was probably built using multiple languages but there's only one language that can pull data from the database and that's SQL, and that's every business in every industry in every country in the world. Honestly, learning SQL will never end because it only takes one email from a stakeholder to make you challenge yourself and do something new. But how long will it take to learn SQL to be ready to get hired for your first role I would says months probably, get on YouTube and follow a 12/15/20 part tutorial on how to start. Then start taking quizzes, testing yourself etc Have a side project going or something that you'll be able to speak about in an interview, even if it's analysing the performance of your favourite sports team, something you can speak about in an interview that wasn't learned directly from a book. W3schools will be a great reference and dataquest.io has a good SQL course, check Udacity as well. To get that first job might take a year but you'll learn a lot there and after 2 years you'll move onto something else and in 5 years you'll be in a great position. In short, it'll take a few months to a year of learning and learning and learning. Good luck
Thanks for the detailed answer it helped so much. So you’re saying that I could get a job as a data analyst by mastering SQL only, as well as having some projects in my portfolio, is that right. My career path is to become a full on developer but I was thinking of starting as a data analyst. We did cover some SQL as part of my Diploma, does this mean I could learn it in less than a year? I’m familiar with some of the basic concepts such as rows, columns, tables, keys, normalizing databases, select statements, join statements, stored procedures
There's no guaranteed order.
Go to Microsoft virtual academy and get free training. They will give you free SQL server and Azure. You can download their database, create reports, practice queries. You can also learn SSIS as well. All for free. 
Try [Codecademy](https://www.codecademy.com/catalog/language/sql)
/u/pranavrules / /u/alinroc \- Here are the indexes, [https://imgur.com/a/UBnLd3Y](https://imgur.com/a/UBnLd3Y). Does that help at all?
Not a homework question lets say am creating an even website. An organiser will need to sell a number of tickets, which will all be created at once..there are org who may misuse this and allocate more than the ticket they'll sell . I'll have a max num of tickets that can be sold per ticketclass e.g early bird @ lets say 20k i'll prolly have a minimum that i will create on creation @ maybe 5k and then increase it till maximum ticket per class is reached based on ticket sold
here's a great explanation from an earlier version of da manual -- http://download.nust.na/pub6/mysql/doc/refman/5.1/en/group-by-hidden-columns.html here's a great explanation from a later version of da manual explaining how this behaviour is now part of SQL99!!! -- https://dev.mysql.com/doc/refman/5.7/en/group-by-handling.html also, note this comment from the later version -- &gt; If ONLY_FULL_GROUP_BY is disabled, a MySQL extension to the standard SQL use of GROUP BY permits the select list, HAVING condition, or ORDER BY list to refer to nonaggregated columns even if the columns are not functionally dependent on GROUP BY columns. This causes MySQL to accept the preceding query. In this case, the server is free to choose any value from each group, so unless they are the same, the values chosen are nondeterministic, which is probably not what you want. Furthermore, the selection of values from each group cannot be influenced by adding an ORDER BY clause. Result set sorting occurs after values have been chosen, and ORDER BY does not affect which value within each group the server chooses.
instead of WHERE customer_id IN ( this list ) AND custoner_order_number IN ( that list ) you need to do this -- WHERE customer_id = 1 AND custoner_order_number = 23 OR customer_id = 2 AND custoner_order_number = 47 OR customer_id = 3 AND custoner_order_number = 56 etc.
I make API requests from external services all the time in SSIS. Plenty of examples are about for getting data from various API's using either SSIS componentnts or Script Tasks.
Check the following web site [http://www.studybyyourself.com/seminar/sql/course/?lang=eng](http://www.studybyyourself.com/seminar/sql/course/?lang=eng). The course, along with examples, is quite easy to follow. You can do exercises as well. Everything for free.
Thanks for your reply I'll take a good look once home! 
Thanks! this makes perfect sense. I guess I didn't think of it since its a pain to write... But I could just make a script/macro to auto write and change the information into the format I need! 
That's purely coincidental and depending on a number of factors, may vary from run to run. Again, unless you specify in `ORDER BY`, there is no *actual* ordering of the results. In SQL Server, if you omit `ORDER BY` the ordering of a resultset can change just by adding the `WITH NOLOCK` hint to a query, or by making the query go parallel.
If the database software knows when to use index and what index to use, why can't it suggest what indexes I should create directly?
Bad bot, get this crap out of here.
&gt; but I appreciate I rushed through a lot &gt; Procedures and views. You can't rush learning. You need to understand the theory behind it. SQL is very different from something like VBA and if you employ the same coding styles you will find your queries take forever to run. Keep watching videos, reading and most importantly actually writing SQL, there is no shortcut for this.
So basically I want my end result to look something like this. bananas | yellow bananas | green
Interestingly .. moving the second query into the first has sped things up greatly. 28 mins to run versus hours before. Not ideal but workable nonetheless.
Uze OR or use FruitColor IN('Yellow','Green')
If youre talking about SQL Server look into Database Engine Tuning Advisor, under Tools in SSMS. Dont mindlessly copy and paste though but should give you some ideas.
Won't work, IN will return anything that's either yellow OR green and that's now what I want.
Not to mention they can track the frequencies and query duration to make a more informed recommendation than us human picking through logs to understand the situation.
I am using Postgres. My googling couldn't find anything like that for postgres. I don't understand why postgres doesn't provide this. Indexing is like the No.1 thing for database tuning.
Sorry, misread probably. Try this: SELECT FruitType FROM fruit WHERE FruitColour IN ('Yellow', 'Green') GROUP BY FruitType HAVING COUNT(*) =2
28 minutes is still *far* from "ideal". That query should be running in seconds, not minutes.
You can use “exists”, if it’s oracle (not sure about others), can’t write query script from phone, but idea is “(exists FruitType=our fruit type and color =green) and (exists FruitType = our fruit type and color = yellow)”
It wouldn’t work if FruitType + color combinations are not unique 
Then change = 2 to &gt;=2, to get the 'at least yellow and green exist together'. /u/andrey_zotov solution should work fine as well.
Sorry, but &gt;= can give us wrong result if there are 2+ rows with f.e. FruitType1 and green
This would work slowly
Go to r/datasets download a data set that you can relate to. start asking questions and try answering them with queries. 
Try This: CREATE TABLE #FRUITS ( FRUIT\_NAME VARCHAR(100), FRUIT\_COLOR VARCHAR(100) ) INSERT INTO #FRUITS (FRUIT\_NAME, FRUIT\_COLOR) VALUES ('Banana', 'Yellow'), ('Banana', 'Green'), ('Apple', 'Red'), ('Apple', 'Green'), ('Strawberry', 'Red') SELECT F1.FRUIT\_NAME, F1.FRUIT\_COLOR, F2.FRUIT\_COLOR FROM #FRUITS F1 INNER JOIN #FRUITS F2 ON F2.FRUIT\_NAME = F1.FRUIT\_NAME WHERE F1.FRUIT\_COLOR = 'Yellow' AND F2.FRUIT\_COLOR = 'Green' If you want to see all the fruits that have more than 1 color: SELECT F1.FRUIT\_NAME, F1.FRUIT\_COLOR FROM #FRUITS F1 INNER JOIN #FRUITS F2 ON F2.FRUIT\_NAME = F1.FRUIT\_NAME WHERE F2.FRUIT\_COLOR != F1.FRUIT\_COLOR (If there are more than 2 colors for a fruit, you'll need to use a distinct to get only one row row per color.
Yes, there is a huge number of Jobs available in the market. Why should we not look at the proper statics - 33,360 SQL jobs were posted in the year 2017 20% SQL Developer Jobs May Increase by the year 2022 As per concerned about salary there is a huge scope of increment on avg salary with learning opportunities. Static ref - (https://www.janbasktraining.com/online-sql-server-training) 
Start with T-SQL. It's not the easiest, but will text you there most concepts. I recommend the Kudvenkat channel on YouTube. You'll pick up sooo much. Honestly though, until you're using it everyday in your job, the learning will be slow
&gt;These queries have 25+ joins and sub queries nested within the joins. That does not sound good. Start by breaking them apart. Its exactly the same as simple joins and querys... just more. You cannot possibly hold all those joins in your head at once so you gotta understand pieces at a time until you know enough to understand the bigger picture. &gt;Did our IT department write these queries in a few hours? Likely an initial version was created in a few hours and it got changed, added to and grew over time as the business added requirements. &gt;Any tips or advice on how I can bring my SQL skills to this level? Experience.... keep coding!
That's correct. Also the fact that your engine doesn't have to cross-map all the columns it finds in your subquery against your main query. You can shorten the # of rows that go into the temp table too..
OK, so you want to increase the allocation of tickets between categories as certain categories start getting sold out. While a trigger could work, I'd recommend against it because then every time a single ticket is sold, the allocations will readjust, which may not be necessary. And triggers can be a pain. I would probably use a stored procedure as a scheduled job on SQL Agent, so it runs every day or every hour or whatever your business rules dictate. Obviously how the query actually does the reallocation is entirely up to your business rules. Let's say you have a table with CATEGORY, MAX_ALLOCATION, INCREMENT, SOLD, and CURRENT_ALLOCATION UPDATE tblAllocations SET CURRENT_ALLOCATION = CASE WHEN INCREMENT * (1 + FLOOR(.5 + (SOLD / INCREMENT))) &gt; MAX_ALLOCATION THEN MAX_ALLOCATION ELSE INCREMENT * (1 + FLOOR(.5 + (SOLD / INCREMENT))) END Using your early bird example, this should give everyone their initial increment (5000), then when they've sold 2500 of them, it'll provide another 5000, up until they've reached 20000. Obviously not tested.
Yes you are right. I answered OPs question - no further knowledge of how the table looks. I assumed FruitType+Colour would be unique.
second to that. take it subquery at a time. besides, given that you query against raw data, 25+ joins sounds like very heavy data pull. you should build statistical tables that gather data like daily from this mega join. and then query against this table instead. 
If you are using SSMS, use the display estimated execution plan button. It will show you how the optimizer is going to execute your query, more importantly it will show any major missing indexes and key lookups.
I know you say Postgres, but I don’t think you have a good grasp on indexes based on your initial post and comments. I’d strongly suggest watching this series. It definitely helps you understand how indexes work from a different perspective (and Brent is funny as hell). https://youtu.be/ACzguQ-AT-c
It's not an automatic thing because most DBAs are able to determine which indexes to create.
&gt;Did our IT department write these queries in a few hours? Some dummy at my job accidentally deleted a query that took someone three months to write last week. She even laughed about it. Thankfully we found a backup after some searching. As someone else said, queries usually start off small and get more and more advanced as needed. It's not terribly out of the norm to see huge queries that took a long time to write.
Everyone else is spot on, wanting to see the plans and indices. I just want to point out that sub-selects are almost always a bad idea. Additionally, OR's in a WHERE clause can cause a lot of slowness.
There are advancements to this, but indexes are actually quite complex. There are people out there who make a living specifically consulting on indexes. The more you learn and understand them, the more you'll see why it's not quite automated yet but I do foresee this being something that can become automated eventually. Indexes are give and take. I create an index, my queries use the index, those queries are faster. Neat! Well, what else happens when I create an index? Indexes need to be maintained under the hood or else they become worthless. So SQL needs to append and fix those indexes when updates / deletes / inserts occur. So now there are at least two places those actions need to take place. Add in more indexes and more and more and more and eventually, you run into problems with resources. What about disk space? Indexes can take up a lot of space. What about other types of indexes? SQL Server has columnstore indexes with their own sets of woes that are great for analytics but with a lot of gotchas. What about locking / blocking / deadlocking / and user concurrency? What happens when a query modifies an index due to data manipulation when another query executes that requires the index for querying? Primary keys? Constraints? These are types of indexes under the hood but primary keys aren't always clustered indexes, perhaps you should be using a different clustered index from your primary key? It depends on how the table is used and how to allocate the best resources for the business needs. The priority and needs of a business drive the work you perform and queries created which in turn gives priority to X,Y, and Z queries or processes. Let's add in the fact that there are also full text indexes to worry about too. There is A LOT to consider with indexes and there is SO MUCH to learn about them. Choosing proper data types, proper columns to index, whether to include columns or disclude, or create multiple indexes to help with bookmark lookup deadlocks, or even fill factors depending on your pages and how data works in your database. Now, a lot of the stuff I bring up is kind of high level. There is a T-SQL book called T-SQL Querying where the author goes in depth about indexes and physical and logical structure, but he only scrapes the surface after 300 pages. You are totally right though, I'd love to get some index automation and we've actually come a long way. I think the primary issues are that there is a lot of give and take when it comes to indexes and the priorities and decision of the business needs to drive that. No piece of software currently can decide this is the best course of action for your business. We don't care if the data warehouse load suffers at night, we only care it performs during the day. With advancements in AI and technology, there is a reasonable plausibility this will become automated in the future, but that will be a little ways away.
Third on this. Subquery by subquery is the way to go. If you can't join in, it's likely a subquery. Write that subquery, run it, verify the results and then determine where and how you're going to merge that in with the rest of the query or main query. It's a process. I've had reports that required up to or just over 300 lines of code and it's really about chunking those subqueries and joins and not only making sure they run and pull the right data but that they are also easy to troubleshoot and modify if needed. It really comes down to experience. Play with some data and see what works and what doesn't.
Just change your query to | HAVING COUNT(ZPC_MODEL_SKU) &gt; 1 and you will see all the duplicate SKUs. 
I was hoping this would work but it doesn't. It just duplicates the values and where clause just doesn't work.
SQL Server does this. If there's a really obvious missing index, it'll show up in the execution plan as a missing index suggestion. But it's stupid about it sometimes, and misses things other times. I've seen databases where someone took every missing index suggestion and implemented it - result is lots of borderline duplicate indexes and some that were just pointless. That said, SQL Server 2017 has a feature called [Adaptive Query Processing](https://www.sqlshack.com/adaptive-query-processing-in-sql-server-2017/) which is *supposed* to do better with handing situations with sub-optimal indexing. If you have a corpus of commonly-run queries and understand how indexes (and the engine) works, you can do as good a job if not better than the software can in most cases. Your job is to give the database as much information as possible so that it can make the appropriate decisions about how to run the query.
Where is he wrong?
SQL Server tracks this information in missing index DMVs. As noted by others it does sometimes show them in execution plans, but you can find the whole set in system views. It tries to give you an idea of how many queries will be affected by a specific recommendation, and what the impact to the query will be, so you can judge if it's worth creating or not. The problem with tools that give you tuning recommendations is that they're only as good as what you feed into them - people frequently use the DTA to tune single queries at a time, which results in unnecessary indexing bloat. They may help individual read procs, but they'll adversely affect your writes. You really need to consider the entire workload, and what queries are critical and which aren't, the frequency they run, the impact to the server, etc. It's very difficult for a tool to do this for you. It's well worth spending the time getting a good understanding of how your database engine works, and then it will be apparent to you when writing your code what you need and what you don't. 
Try Codewars , solve katas ranging from basic to advanced
Yeah, I do appreciate that. I rushed through to see what it can do and looks like. I definitely need lots of repetition. I can do things but I don’t fundamentally understand why they work which is what I will be focusing on before progressing. Good to know that I need to review my approach to it too Thank you
Yeah, that’s the catch 22 though. Can’t get a job until you know it, jobs only want people who know it. Hopefully I’ll slowly pick away and learn enough to get a starting job
Thanks! I have been googling this topic and read a dozen articles for past few days but all of them are just scratching the surface. Also another good resource is [Use the index Luke](https://use-the-index-luke.com/) recommended by another commenter here(For some reason he deleted his great comment :( ).
In general too many indexes won’t hurt read performance, what they will do: Increase insert and update time, since the index has to be updated. Increase disk usage. 
Google’s BigQuery has massive free db with tons of public data to create analyses with
This is exactly what I wanted, now is there any other way to write this? As I MUST use a sub-query
&gt; Some dummy at my job accidentally deleted a query that took someone three months to write last week. Deleting a file is just sloppy. Not using version control for something that takes you three months to write on the other hand…
True. Get some big sample data sets. Write some impressive, clean code that demonstrates your understanding of the basics. Design an analysis project around it
https://community.modeanalytics.com/sql/tutorial/introduction-to-sql/ I moved to this site after taking the basic SQL course on code academy. 
Oracle does. Their documentation for 2 Day DBA and 2 Day Performance Tuning Guide covers the basics of it I believe. Too many indexes can be bad for an operational environment. So, where you're inserting a lot of new records as opposed to a reporting environment, where you're worried about generating reports for users for data that is already available or bulk loaded. So, like a lot of things with SQL, it depends on what you're trying to in your business process/personal goal.
You can format your code by adding 4 spaces at the start of each line. Much better readability. :) CREATE TABLE #FRUITS ( FRUIT_NAME VARCHAR(100), FRUIT_COLOR VARCHAR(100) ) INSERT INTO #FRUITS (FRUIT_NAME, FRUIT_COLOR) VALUES ('Banana', 'Yellow'), ('Banana', 'Green'), ('Apple', 'Red'), ('Apple', 'Green'), ('Strawberry', 'Red') SELECT F1.FRUIT_NAME, F1.FRUIT_COLOR, F2.FRUIT_COLOR FROM #FRUITS F1 INNER JOIN #FRUITS F2 ON F2.FRUIT_NAME = F1.FRUIT_NAME WHERE F1.FRUIT_COLOR = 'Yellow' AND F2.FRUIT_COLOR = 'Green' SELECT F1.FRUIT_NAME, F1.FRUIT_COLOR FROM #FRUITS F1 INNER JOIN #FRUITS F2 ON F2.FRUIT_NAME = F1.FRUIT_NAME WHERE F2.FRUIT_COLOR != F1.FRUIT_COLOR
Thanks. I’ll do just as much, the analysis bit is easy for me as that’s what I’ve done historically but writing good clean code will be the challenge 
Read more about your DB engine, look at your execution plans, and test various indexing strategies with approciate caching. If you understand something of how query plans are derived then indexing strategy becomes much easier.
Depends on your local job market, but SQL alone seems a bit weak for an analyst role - most work with Python/R, Hadoop etc. In that context SQL is typically used for extracting data in the required format rather than any serious analysis. You also need to understand statistics, and probably to be able to explain them to people who don't.
sqlpracticeproblems.com
&gt;backup after some searching. Only having one definite copy of any code, on what is presumably a prod server, kind of suggests a bigger process issue. Most of us have accidentally deleted stuff or otherwise screwed up. That's why we use multiple environments and source control.
It's just a lot of smaller queries. There's no course for it. Just try to apply what you've already learned in a logical manner. Start at the bottom, figure out what each subquery does, write a lot of notes, draw diagrams.
You can also give each tenant its own schema within a single db.
You could CROSS JOIN sys.all\_columns to itself and generate a ROW\_NUMBER(), but it'd be better to use a numbers table.
what happens when you run the subquery by itself?
It looks to me like you're missing a closing parenthesis. There's an open parenthesis before SELECT, another after COUNT, but only one closing parenthesis, after CONSULTANT\_NAME. No, I'm probably wrong. There's a closing parenthesis after DIRECTOR\_ID on the second-last line. Try pulling that SELECT query out and running it separately to see if there's a syntax error in it.
Customers - The main source of your data. Batch - Jobs that manipulate the data generated by customers. Steve - The DB user who constantly forgets how to use a where clause. 
I guess that’s really my question. At my work, the developers BUILT the ETL process and then were promptly laid off. (I know it’s stupid, it’s a whole thing.) So now I run and maintain the ETL process (making corrections if something breaks), but they gave me an “analyst” title, which I don’t feel is accurate. I wasn’t sure if there was a level between analyst and developer, which is where my current role would fit. 
I'm the developer of [SQLPracticeProblems.com](https://sqlpracticeproblems.com/#utm_source=r1&amp;utm_medium=r1&amp;utm_campaign=r1). It's great if you're looking for some very hands-on "learn-by-doing" practice problems, that teach basic to advanced SQL with well-designed, real-world practice problems. I developed it after teaching a SQL course where the material I had to to teach from was poorly structured and academic. Afterwards, the students emailed me, saying they needed practice problems, so I developed the course! Contact me (email in the FAQ) if you have questions. Also, redditors get 30% off the Professional package (discount code reddit30). 
Fucking Steve! And that makes sense, with the batch being automated. The batches at my job are not automated (as the data comes in differently from each new client). I mentioned in a replay to another comment that I feel I fit somewhere between dev and analyst since I have to build a custom ETL process (or modify an existing one to work) for each new client, but didn’t actually build the warehouse. Just trying to get a feel for this inbetweener role. 
Yes SUM(CASE WHEN X=Y THEN 1 ELSE 0 END)
 If you're looking for some very hands-on "learn-by-doing" practice problems, that teach basic to advanced SQL with well-designed, real-world practice problems, similar to what you're trying to solve, check out [SQLPracticeProblems.com](https://sqlpracticeproblems.com/#utm_source=r1&amp;utm_medium=r1&amp;utm_campaign=r1). I developed it after teaching a SQL course where the material I had to to teach from was poorly structured and academic. Afterwards, the students emailed me, saying they needed practice problems, so I developed the course! Contact me (email in the FAQ) if you have questions. Also, redditors get 30% off the Professional package (discount code reddit30). 
You're missing a closing parentheses 
I'm the DBA for a multi-tenant system that's one DB per tenant. The problem with this model is that when you become successful, you'll find yourself managing hundreds or even thousands of databases. Need to do aggregate reporting across all the tenants? You're querying *every* database. Need to push out a schema change? You're running that on *every* database. Changing schema per-tenant becomes a management nightmare when you do one DB per tenant. One table per tenant in a single DB? You've got the same sort of problems as above, just in a single DB but spread across many tables.
&gt;end) That's creating a new column for me? I don't want a new column. Just a new row/account under that column (Account\_desc)
I think there is confusion on what you are trying to do. You want to actually add a brand new column to the table that automatically sums two other columns based on certain conditions? This is called a computed column.
Standard ANSI SQL won't let you produce a "TOTAL" record and append it to the end of your results. But some DBMSs support this function. What database/DBMS are you using?
No, I don't want to add a new column. I want to add a new account under the column Account\_Desc. For example, I want to add a new account description that would the sum of "Total Sales" and "Food Cost". I'd want the same conditions in my where statement to still apply. 
You could accomplish that with a bunch of Union statements of you really wanted to. Depending on what flavor of SQL you are using you can look into cube/roll-up functions, but I'd recommend sticking to unions. 
I see your point but this is really a horses for courses argument. Almost all the drawbacks you listed sound like advantages to me. Say you are running enterprise software as a service. You have release 1.0 which uses schema 1.0. You are planning to release 2.9 which uses schema 2.0. Only. Guess what, 60% of your users don't even agree to your release date. They want more time. Some want more time than others. Some other don't even want to upgrade as it is too expensive for them. You see why IE6 is still hanging around?? So how are you going to handle all this in a multi tenant database? To your point, if you have thousands of customers, obviously a single tenant db is going to be a huge administrative issue. You could still manage it with automation although it will be a big engineering effort. But hey, those thousands of customers are likely paying you enough moolah for all that extra engineering work. And conversely, with a multi tenant db, you will still run into the same level of complexity of problems with thousands of customers. Say you do a release. Something breaks bigtime for 10% of your clients. Now what? You're up shit creek without a paddle. Your option is to rollback to the "last known good state". But that destroys the upgrade for 90% of your client base.
So you are just attempting to insert a row in the table? If that's what you're doing you'll need to be a little more clear with the conditions for the insert into account_desc. Can you add a grid to your post that mimics the table? 
remove the `(` between `COUNT` and `DISTINCT`. it is spurious and incorrect.
The problem is , I don't know where to insert the indexes. That is whats messing me up.
The think the closed parentheses that some people said I'm missing is the one after "Group by v.DIRECTOR_ID". I'm able to run the query as a standalone query but when I copy and paste it into the original query I keep getting the same error again. Not sure why
&gt;insert into TABLE_NAME (Month, Year, Acct #, Account Desc, Amount) --this should be a list of all the columns in the table &gt;select 'August' /*or whatever value you want for month*/ as month, 2018 /*or whatever value you want for year*/ as year, 8675309 /*or whatever value you want for acct #*/ as acct, 'acct 12' /*or whatever value you want for acct description*/ as acct_desc, sum(amount) as amount &gt;where acct_# in (/*all the acct number amounts you want to sum, comma separated*/) &gt;group by month, year, acct, acct_desc
The query runs with no problems when I run it by itself
 Select distinct FruitCode, FruitColor From table Where fruitColor in (‘green’,’yellow’)
I'm surprised to hear that. You have t.director in your select list but not in your group by. That should throw an error regardless of whether it's embedded in a subquery or standalone.
Probably you'll want some tables to track tenants and versions and maybe planned move dates, and have a nice little UI, and we'll need to version the table version tracking scheme as well...
Job titles mean nothing. Sometimes the job title goes with the role, sometimes not. Sometimes an "Analyst" is actually a developer who builds solutions. Sometimes it's someone who is just a customer of the DW and writes reports.
So for whatever reason, the entire query worked when I copy and pasted it into a blank query page. Solved!
Sigh yeah...
For \#3, the current version of Sqlite (3.24) added new syntax to INSERT called [UPSERT](https://www.sqlite.org/lang_UPSERT.html) that lets you insert or update an existing row depending on if there's already a record with a unique key or not.
I'd say definitely one DB per tenant. This gives you several advantages: Flexibility. A tenant leaves as a customer? Drop the DB. A tenant has a data issue and needs a restore? You're only affecting that tenant, not everyone else. You can have different maintenance windows for each one. Scalability. One DB each means you can easily scale out onto more servers. In a single DB, this requires some redesign to work. If a particular tenant has significantly different performance requirements, you can move them to more appropriate hardware, dedicated disks, etc. Rolling out schema upgrades doesn't have to affect everyone at once. Rolling out upgrades slowly helps when you hit issues: would you rather have a single customer calling you up with issues or all of them? Security. Each tenant only has access to their database. Sure, multiple DBs require more admin overhead, but tools like Powershell make working with multiple databases/servers not a big issue.
Ah. I don't know what you are then. Job titles and duties can get pretty fuzzy. I've heard of developer/analyst titles though, perhaps that fits.
Ive been Steve one or twice. Don't forget Randal the Vandal who thinks that functions in joins and cursors are fine cause they still return data. 
if you look up the syntax for the COUNT function, you will find that it is `COUNT(...)`, not `COUNT ...)` so you are dead wrong, sire or madam as the case may be 
&gt; Don't think of the primary key as a value, think of it as a promise you and the database are making to each other. The database is promising to store rows based on the primary key and have consequently really fast key lookups. In return, you're promising to never return duplicate values in that column. If you do, the database is allowed to freak out and light your app on fire. In this case, if you can make that promise (you have a UID field), you should. the most delightful and accurate description of PKs i've seen in a long time bravo!!
|| |:-| |||| |:-|:-|:-| ||| |||| |:-|:-|:-| ||||
Everyone gets the same version of everything on each release, or new features are put up alongside old and then migrated by changing configuration settings. Feature flags everywhere. I didn't create it, I'm just managing the databases and trying to prevent future messes from being made.
3 depends on your specific project requirements. Do you just want to store the most recent version of the data for that record? Or do you want to retain a history of previous versions? What's commonly done in databases is to have separate loading zone (LZ) and reporting zone (RZ) tables. The raw, scraped data will be loaded into the LZ table, then another SQL script will apply the desired logic to update the data in the RZ table using the LZ data. If the RZ table should just contain the most recent version of a record, you can delete the old record from the RZ table where there is a newer version in the LZ table: delete from RZtable where (RZpkcol1, RZpkcol2, RZpkcol3) in (select pkcol1, pkcol2, pkcol3 from LZtable); insert into RZtable (col1, col2, col3, col4, col5, col6) (select col1, col2, col3, col4, col5, col6 from LZtable); 
Oh wow. That’s brilliant. I’ll check it on Monday, as I currently have moved country and don’t have access to a laptop/pc, when I’m at work! Seems very much what I’m looking for
&gt; the developers BUILT the ETL process and then were promptly laid off. Yup. And without adequate documentation of the data flows.
I think you're thinking about your data in a way that isn't conducive to SQL. There's a difference between presentation and how to store day in am efficient manner (normalization). If you add a row to your table, it becomes part of whatever that table represents but a total row really isn't. What I would do is add a calculated column that is always the sum of that row. In other words, a column that has the same number for every row that is the total. In doing so, when you go to present the data, you can pull it easily and you can also use it for percentages within the data. So Select id ,Num_col ,(select sum(num_col) where [...insert your conditions here...]) As total_col (Forgive my formatting I'm.on a phone) You may also use a view, which are better for presenting data,or at least doing what you want as it calcafed every time the view is viewed. Create view v1 as Select id, num_col from t1 Union Select 'Total' as Id, sum(num_col) from t1 where [conditions] 
 LEFT JOIN (SELECT COUNT(DISTINCT x.CONSULTANT_NAME) PAID_AS_POINTS, v.DIRECTOR_ID, t.DIRECTOR FROM GENEALOGY_REPORT x LEFT JOIN RECRUITING_MODEL v ON x.CONSULTANT_ID = v.CONSULT_ID LEFT JOIN COMPENSATION_TITLE t ON x.CONSULTANT_ID = t.CONSULTANT_ID WHERE x.BATCH_DT &gt; t.Director AND x.PAID_AS_TITLE LIKE '%DIRECTOR%' GROUP BY v.DIRECTOR_ID) GEN1 ON a.CONSULTANT_ID = GEN1.DIRECTOR_ID You're only grouping by DIRECTOR_ID... you need to group by both the v.DIRECTOR_ID and t.DIRECTOR when using an aggregate function like COUNT
Sometimes you get unprintable characters (newline, etc) if you copy paste from sources like MS Word or Skype chats. I end up having to delete a few characters then type them back and the query will work.
This sort of defeats the purpose of SQL though doesn't it. You want to keep unaggregated data in your tables and then aggregate them outside the table.
&gt; You're only grouping by DIRECTOR_ID... you need to group by both the v.DIRECTOR_ID and t.DIRECTOR when using an aggregate function like COUNT. not true, this is MySQL if ONLY_FULL_GROUP_BY is turned on (which it is by default in the latest versions) then yes see https://dev.mysql.com/doc/refman/8.0/en/group-by-handling.html 
Again, management becomes an issue. You've got a 1000 tenant databases, each potentially on its own code base. Imagine managing the SDLC for 1000 different code bases. Setting up test cases for a 1000 different code basis. 
I pay zero attention to certifications. I do pay attention to personal projects. Put an interesting SQL related personal project on a resume, I will ask a out it, and know in about 30 seconds if you have basic SQL proficiency. I don't speak for the industry though.
Thanks, I appreciate the tips. I will look into a personal project. Just hard to know which carries more weight, the cert or your actual knowledge. I think people go to school more for the credential than the actual knowledge. 
Agreed. The tables themselves should not be used in this manner, for various reasons. The table should not "tell the story" so to speak. You can throw views on top of it if you feel so compelled to get what you're after, or you can pull it out in another place and pivot/aggregate to your heart's content.
This was my introduction to SQL, and I loved it: [https://sqlbolt.com/](https://sqlbolt.com/)
Think about your question. If a person has a certification and doesn't know how to join two tables properly, what would have more merit?
Great Site.. Any idea on advanced SQL topics like performance tuning,stored procedures and you know taking it to another level learning sites?
Love the idea about a personal project. How does one start that?
Sure but the other way can be a problem too. What if you know it inside out but don't have the credential? Might not even get to the interview. 
I am guessing just come up with a practical problem that sql can solve then do it. But curious as well. 
True, in that case I'd argue that whoever the interviewer is has no idea what they're looking for, or it's not a good team.
Seconded. /u/jc4hokies, any recommendations on what a SQL personal project might be? I could make a database schema for a database for a small web-application I was thinking of. But that's the extent of my idea so far lol.
Why do you assume you would have 1000 codebase? Does Microsoft support 1000 versions of IE and Office? But it also doesn't mean they only support 1.
1. Pick a dataset that interests you. Sports, games, stocks, weather, horse races, whatever. Here are some sites for inspiration. https://registry.opendata.aws/ https://cloud.google.com/bigquery/public-data/ https://www.kaggle.com/datasets /r/dataisbeautifull 2. Download it and load it into a local database 3. Ask the data questions; use SQL to get answers 4. Try to make queries faster 5. Download related datasets 6. Link related data together, and reorganize things so that queries are easier and faster 7. Try to get advanced features working, like partitioning, change tracking, full text indexing, and spacial data \#1-3 is all that's necessary for basic SQL proficiency, but even experienced professionals can benefit from personal projects. /u/apowerseething u/MightBeJerryWest
If you want the database engine to reinforce referential integrity (primary/foreign key relationships, etc), then you need to declare the relationship. As an example, if I have a table A where there's a column named CompanyId which contains the primary key, then for table A I would declare CompanyId as the primary key. The exact syntax for doing so depends on the database you're using. I then have Table B which has a column named CompanyId (primary &amp; foreign keys often have the same name, but not always) which is a foreign key. I can then declare that as a foreign key (syntax again varies) and indicate the table/column it relates to. If I've defined it in the database, then an attempt to insert into Table B with a CompanyId value that doesn't exist in Table A will fail. Likewise, if I've configured cascading deletes, deleting the row with CompanyId 5 from Table A will automatically delete all the corresponding rows from Table B with CompanyId of 5. The alternative is to not declare the references in the database and have the application itself maintain referential integrity. Regardless of how you do it, when you insert a row into Table B, you have to provide an appropriate value for CompanyId - it won't automatically populate for you.
So if you've got Table1.ID, in Table2 you have a column called Table2.Table1_ID, and you use your chosen RDBMS's relationship syntax to create that foreign key so the RDBMS will enforce the relationship. Then when you're inserting data, and adding a row to Table2 that includes data defined in Table1, you'll lookup the corresponding row in Table1 before inserting to Table2, and insert the Table1.ID field into Table2.
I commonly use while loops for anything that's not set based. Backups, index maintenance, gathering metadata, and creating dynamic SQL are all tasks I commonly use loops for. 
When you create the tables, you include a foreign key constraint. create table table1 someCol numeric, someOtherCol varchar(50) constraint primary key(someCol); create table table2 col1 numeric, col2 varchar(20), col3 varchar(20), constraint foreign key (col1) references table1(someCol); 
Thank you. I don’t really do much of that besides occasional dynamic sql, so maybe that’s why I’ve never really come across a need for it. 
I've used it to break up big deletes. DELETE TOP(100000) FROM Table WHERE Date &lt; '2000-01-01'; WHILE @@TRANCOUNT &gt; 0 DELETE TOP(100000) FROM Table WHERE Date &lt; '2000-01-01';
They can can often replace, and are sometimes much more performant than, cursors.
This is excellent! 
I thought about publishing all my projects in a blog setting so folks can browse them.
a menu for a program uses a while loop.
Another use for them not already mentioned is to call stored procedures if you don't have time to rewrite the code to use set logic. Surprisingly this is often faster than using a cursor.
This might help: https://stackoverflow.com/questions/10757169/location-of-my-cnf-file-on-macos
CCL has SQL like syntax with a report writer component tagged on. Unfortunately the best resource I have found is existing scripts, and at times the UCern site (though the value of that site is dubious at times). 
I just use Notepad++ then copy/paste the queries into DBVisualizer
Do I have to download two programs? I currently have PostgreSQL + PGAdmin 4. 
Cursors. FETCH NEXT FROM MY_Cursor INTO @VAR1Number, @VAR2DateTime, @VarLongText WHILE (@@FETCH_STATUS &lt;&gt; -1) BEGIN
I think it's the table subquery as you suggest. `FROM (SELECT categories.id as catid FROM categories) t` Can you just rewrite it as `FROM categories t` Then use [t.id](https://t.id) instead of t.catid in your subqueries? Alternatively, you could use a CTE to name the columns correctly, then run your query over that instead.
I'm confused as why you don't just directly hit categories and instead use an inline select. Additionally, just for readability (and performance, although this varies from DB to DB) try using some CTE's for your in select's in the select clause. 
Thanks for your reply. The issue with doing it inline (FROM categories t) is that then the references to t.id in the subqueries after SELECT won't work. Do you know of any ways around this?
Thanks for your reply. The issue with doing it inline (FROM categories t) is that then the references to t.id in the subqueries after SELECT won't work. Do you know of any ways around this?
You'll need to use [t.id](https://t.id) in the other subqueries. I'm guessing you used t.catid in the subqueries, which won't work.
Is the ID column from categories a unique identifier in that table? If you say "yes, it's supposed to be" I then say "are you sure?" Check. I think you'll get bad results here if not. You might want to slap a distinct on that subquery.
Comma missing between type' &amp; Tran? 
 FROM (SELECT categories.id as catid FROM categories) t JOIN categories c ON t.catid = c.id Is the categories c join required? I don't see table c being used anywhere in the query.
Yep its unique, checked with count and count distinct
Thanks for the detailed response. Its backend data from my own online store and my inventory on ecomm sites. I'm trying to better understand stats from different categories' inventory. I want a dataframe with all category ids (there are dozens) along with computed facts pulled from items and categories tables. The categories table was joined on itself as I couldn't pull ids directly from categories as for some reason I wasn't able to directly reference these aliases in the other subqueries. On the ROUND() columns, why would I need a group by for SUM and COUNT if I'm just looking to return a single value? Wouldn't it default to no groups and give me a single value response?
(BTW, formatting your queries makes it easier for people to read and thus to help you - everything on one line is hard to read :) ) In your inner query, you're grouping on Transaction_ID, Account_Name, Account_Date and Transaction_Type You then join it to the outer table on Transaction_Id. This means you're going to have multiple dates that match to each row from Transaction_Detail Run your inner query by itself and look at what you get. Then mentally match each row to its corresponding row in the first instance of the table and you'll see what I mean. BTW, there's no reason for a self-join on this query.
You should probably post your code. 
Subbie's gonna have an out-of-body experience when he learns windowed functions. 
You need to remove the transaction type from your Group By statement. That's what is causing the separate lines on this statement.
But if I remove it from the GROUP BY I get an error message that it is not included in the aggregate function... 
 SELECT SUM(if.sales) FROM items i WHERE [...] Is this part really correct? You are doing a sub-select on the "items" table here, but you are summing an attribute from the "itemfacts" table. Another thing is that you are using the same alias "i" for the "items" table of the outer query and for the "items" table of your subselects. I'm not sure if this is a real problem in this case but at the least it is a bit confusing and makes the query harder to understand. 
Ok then. I don't know if this is a one off thing, but I'd still pop a distinct or a group by on that sub query just to be safe in the future if it isn't a one off thing. Is it possible for you to post something anonymized to sql fiddle?
Without seeing your code it's tough to help. I'll offer the input that there is usually a better way to deal with data that does not involve a cursor.
Try this: WITH GL_Sums AS ( SELECT gl.PFT_ID , gl.GLFTD_CNTX_NO AS Account_Number , CAST(gl.GLFTD_CNTX_DTM AS DATE) AS Account_Date , gl.GLCOAT_TYP_CD AS GL_Account , cast(gl.GLFTD_GL_SENT_DTM as DATE) AS Date_Sent_To_GL , SUM(gl.GLFTD_AMT) AS GL_Amount FROM GENERAL_LEDGER_FNCL_TRAN_DTL gl WHERE GLFTD_CNTX_NO = '151501084206' GROUP BY gl.PFT_ID , gl.GLFTD_CNTX_NO , gl.GLFTD_CNTX_DTM , gl.GLCOAT_TYP_CD , gl.GLFTD_GL_SENT_DTM HAVING SUM(gl.GLFTD_AMT) != 0 ) SELECT gl.Account_Number , gl.Account_Date , T.FNCL_TRAN_TYP_NM as "Transaction Type" , cast(F.PFT_DT as DATE) as "Transaction Date" , F.PFT_ID as "Transaction ID" , F.PFT_AMT as "Transaction Amount" , F.PFT_CMT as "Comment" , gl.GL_Account , gl.GL_Amount , gl.Date_Sent_To_GL FROM GL_Sums gl FULL OUTER JOIN POLICY_FINANCIAL_TRANSACTION F ON F.PFT_ID = GL.PFT_ID FULL OUTER JOIN FINANCIAL_TRANSACTION_TYPE T ON F.FNCL_TRAN_TYP_ID = T.FNCL_TRAN_TYP_ID
Still didn’t work. I had to change the joins to LEFT JOINS since i was getting thousands of results. But the -185 and 185 still is not summing.
Do they have different PFT_IDs?
[SQLBolt](https://sqlbolt.com/) is pretty good for learning. I've been going through [Hackerrank](https://www.hackerrank.com) for more practice but some of the questions seem awkwardly worded so far, still helpful however. Good Luck!
Still didn't work. I think because for the 11151000 account, a value exists in the Transactions Type table for one entry, but doesnt doesnt exist (NULL) for the other entry. Is it possible to subquery the SUM and then join it? This is what happens when I just subquery the SUM: This is exactly the result I want (but without the zeros - my HAVING clause takes care of that). How can I query just this, and then join it with the other details?
&gt;Still didn't work What was the result? &gt;Is it possible to subquery the SUM and then join it? That's what it should be doing. 
Sorry, I see the issue now. Give me a bit to take a look.
SET SERVEROUTPUT ON; DECLARE v_cname CHAR(20); v_make CHAR(10); v_model CHAR(8); v_cyear CHAR(4); v_color CHAR(12); v_trim CHAR(16); v_ocode CHAR(4); v_odesc CHAR(30); CURSOR prospect_list_cur IS SELECT prosp.cname ,prosp.make ,prosp.model ,prosp.cyear ,prosp.color ,prosp.trim ,prosp.ocode ,opt.odesc FROM prospect prosp LEFT OUTER JOIN options opt ON prosp.ocode = opt.ocode; BEGIN /* FOR each_prosp IN prospect_list_cur LOOP DBMS_OUTPUT.PUT_LINE(v_cname || CHR(9) || v_cyear || ' ' || v_color || ' ' || v_make || ' ' || v_model || ' ' || v_trim || ' ' || v_odesc || ' ' || '('|| v_ocode ||')'); END LOOP; */ DBMS_OUTPUT.PUT_LINE('Name' || ' Want' || CHR(10) || '____________________ ________________________________________'); OPEN prospect_list_cur; LOOP FETCH prospect_list_cur INTO v_cname ,v_make ,v_model ,v_cyear ,v_color ,v_trim ,v_ocode ,v_odesc; EXIT WHEN prospect_list_cur%NOTFOUND; IF v_ocode IS NULL THEN DBMS_OUTPUT.PUT_LINE(v_cname || TRIM(v_cyear) || ' ' || TRIM(v_color) || ' ' || TRIM(v_make) || ' ' || TRIM(v_model) || ' ' || TRIM(v_trim)); ELSE DBMS_OUTPUT.PUT_LINE(v_cname || CHR(9) || TRIM(v_cyear) || ' ' || TRIM(v_color) || ' ' || TRIM(v_make) || ' ' || TRIM(v_model) || ' ' || TRIM(v_trim) || ' w/' || TRIM(v_odesc) || ' ' || '('|| v_ocode ||')'); END IF; END LOOP; CLOSE prospect_list_cur; -- */ END; /
Could you go more into depth about that? I assumed I needed the cursor because of iterating through all the entries. 
So do you need the transaction details at all? Should totals by account number be contained in two lines if there are actually two separate transactions with details? What if there are two with details and one without?
Why does the 185 come up with the 11151000 GL account?
 SELECT c.id AS catid , AVG(i.price) as mprice , AVG(isc.ratingcount) as mreviewcount , AVG(isc.rating) mreviewrating , AVG(i.sellercount) as msellercount , (SUM(if.sales)/SUM(isc.ratingcount)) as reviewability , AVG(i.acccount) as macccount , AVG(i.varcount) as mvarcount , ROUND(100.0 * (SELECT SUM(itemfacts.sales) FROM items i JOIN itemfacts ON i.id = itemfacts.id WHERE i.categoryid = c.id AND i.responserank &lt;= 5) / (SELECT SUM(itemfacts.sales) FROM items i JOIN itemfacts ON i.id = itemfacts.id WHERE i.categoryid = c.id AND i.responserank &lt;= 20), 1) as salesconcentration , AVG(if.sales) as msales , AVG(if.revenue) as mrevenue , SUM(if.revenue) as revenue , 0 as allrevenue , ROUND(100.0 * (SELECT COUNT(*) FROM items i WHERE i.categoryid = c.id AND i.brand LIKE 'Amazon') / (SELECT COUNT(*) FROM items i WHERE i.categoryid = c.id), 1) as amzndom , AVG(if.fbafees) as mfbafees , ROUND(100.0 * (SELECT COUNT(*) FROM items i WHERE i.categoryid = c.id AND (i.title LIKE 'refill' OR i.title LIKE ' pack')) / (SELECT COUNT(*) FROM items i WHERE i.categoryid = c.id), 1) as refillability , (STDDEV(i.weight) / NULLIF(AVG(i.weight),0)) as mstdev , 0 as reviewabilitypct , 0 as macccountpct , 0 as mvarcountpct , 0 as dimstdevpct --INTO categoryfacts FROM categories c JOIN items i ON c.id = i.categoryid JOIN itemfacts if on i.id = if.id JOIN itemscrape isc on i.asin = isc.asin GROUP BY c.id
Imo cursors are bad because you are only processing one row at a time. The power of SQL is the ability to process millions of tires at a time. They have a place, but I generally see them used because the developer didn't understand set based logic and used the cursor because that's what they knew. I don't follow what you're trying to do. It looks like you have a data set and are iterating through it and printing the data set to the console line by line.
I apologize in advance for having to use CCL
SQL operations studio
Consider using a `SELECT` instead of a `CURSOR` like so... SELECT prosp.cname ,CASE TRIM(prosp.cyear) != '' THEN TRIM(prosp.cyear) || ' ' ELSE '' END || CASE TRIM(prosp.color) != '' THEN TRIM(prosp.color) || ' ' ELSE '' END || CASE TRIM(prosp.make) != '' THEN TRIM(prosp.make) || ' ' ELSE '' END || CASE TRIM(prosp.model) != '' THEN TRIM(prosp.model) || ' ' ELSE '' END || CASE TRIM(prosp.trim) != '' THEN TRIM(prosp.trim) || ' ' ELSE '' END || CASE TRIM(opt.odesc) != '' THEN 'w/' || TRIM(opt.odesc) || ' ' ELSE '' END || CASE TRIM(prosp.ocode) != '' THEN '(' || TRIM(prosp.ocode) || ')' ELSE '' END FROM prospect prosp LEFT OUTER JOIN options opt ON prosp.ocode = opt.ocode;
You would be correct. However since the table is a concatenated unique key of all the options and the name, it doesn’t mean that there is always going to be a value for every column. 
Would I be correct to assume this is something I’d use inside a for loop to go through all the entries? I totally understand what you’ve written. Just not entirely sure how I should implement it yet!
No, that SELECT will give you all of the data at once. No loop needed. 
 If you're looking for some very hands-on "learn-by-doing" practice problems, that teach basic to advanced SQL with well-designed, real-world practice problems, similar to what you're trying to solve, check out [SQLPracticeProblems.com](https://sqlpracticeproblems.com/#utm_source=r1&amp;utm_medium=r1&amp;utm_campaign=r1). I developed it after teaching a SQL course where the material I had to to teach from was poorly structured and academic. Afterwards, the students emailed me, saying they needed practice problems, so I developed the course! I've also interviewed lots of job candidates who were supposed to know SQL (many were even certified) and got some spectacular fails when asking them some basic real-world problems. I think the value of certifications has decreased because there are so many brain-dump sites out there, that list all the questions. Contact me (email in the FAQ) if you have questions. Also, redditors get 30% off the Professional package (discount code reddit30). 
I'd post some simplified sample data data (create table and insert statements), with SQL that reproduces your problem. That will help people figure out exactly the data your working with, and provide better help. If you're looking for some very hands-on "learn-by-doing" practice problems, that teach basic to advanced SQL with well-designed, real-world practice problems, similar to what you're trying to solve, check out [SQLPracticeProblems.com](https://SQLPracticeProblems.com). I developed it after teaching a SQL course where the material I had to to teach from was poorly structured and academic. Afterwards, the students emailed me, saying they needed practice problems, so I developed the course! Contact me (email in the FAQ) if you have questions. Also, redditors get 30% off the Professional package (discount code reddit30). 
If I'm understanding the question correctly, you can aggregate for all values in the CTE which then gets filtered down the the relevant values when you join the CTE to your main data. (Omit the reference to t.id in the CTE then join on it) You will need to check that the query plan doesn't suffer tho. Is most of the situations I have used the query optimiser has been pretty smart.
Oh I gotcha. I could put this into a process and just call the process right?
You may get more traction over in /r/BusinessIntelligence
This is a fascinating question. My first thought is that SQL is probably not suitable for analysis, other than aggregations. However, SQL + Power BI or Tableau or some other visualization tool, might do the trick. 
1. what do u plan to do with it (other then what's listed) 2. it has nothing to do with security 3. it is irrelevant 4. indexing is best done by what you're searching for/accessing. &amp;nbsp; What are you trying to solve?
The simple answer is 'no'. The little bit more expanded version is that although all major SQL dialects have enough support for general programming, SQL was not built to provide for efficient general programming. At the first blush, I would say Oracle with its packages provides the most structured version of the general programming environment. You can read about PL/SQL if you are interested in this as a general subject.
I'm solving for Uniqueness across tables the UUID can be used to identify where the exact record is..also my types of tables i have Transfer History,Sales Invoice,Deposit History,Withdraw History tables. the keys will be exposed and the data along side it can be updated. the overkill part is that i might have UUID and id to update a record but i also think a UUID might work better i don't want someone randomly guessing the exposed keys.
&gt; Is it just too much data? It sounds like you have a small database at just 2.5 million records. There are probably issues with your database design or your queries (or both). Can you give us some more clues so we can help? What database? What are your tables and indexes? What is your query?
I think you are overthinking it. Primary keys do not need to be unique across tables. Primary keys do not need to be exposed to users. A more creative or complex key does not add any security benefits. 
You might want to look into [Window Functions](https://docs.microsoft.com/en-us/sql/t-sql/functions/aggregate-functions-transact-sql?view=sql-server-2017). I linked the MS SQL version, but I think every flavor has an equivalent. It also removes the need for GROUP BYs, which I think end up just muddling the water for newer users, anyway. It looks like the problem is a combination of your GROUP BY logic and maybe some NULL values. Try changing your sum line to SUM(ISNULL(gl.GLFTD_AMT,0)) OVER (PARTITION BY gl.GLCOAT_TYP_CD) as "GL Amount", and commenting out your GROUP BY lines and your HAVING lines. IF you really need that logic, you can add it in a WHERE line. Let me know if you have any questions!
Can you explain what you are trying to do in SQL that you are having problems with? 
I'm not sure what you mean, but you shouldn't need to write further SQL around this. It should be ready to consume by the application per your original post.
Do it as a join. It's more sargable, selects are processed later than joins. Doing it as a sub select can result in a sargable query so long as its really simple but often it results in a row by row scan.
You are right, UUID _can_ be used to provide uniqueness across different entities, true; and I think understand that you are making your IDs a part of your domain data by making them exposed/available/usable. I'm still not sure I understand why you *need* uniqueness *across* entities though. I.e. what is the situation where you _know_ the identifier but _dont know_ (or dont care for) the context (what table/business area this record belongs to). I also dont understand what do you mean by 'possible unwanted change' - could you describe this scenario?
What's a good resource to learn this knowledge?
sql doesn't seem to like the not equals, as it says its expecting FROM.. 
&gt; You can use that to find places where you can extract data for another users ID That is your real security problem. You should not be able to do this in a well designed system.
Do your problem sets have solutions? 
Try `&lt;&gt;` instead of `!=`. I wasn't sure of your RDBMS.
It was also missing `WHEN` keywords. I have edited the query above.
How are you currently loading it? For a free solution, I'd suggestion getting Pentaho (PDI) and setting up a proper ETL job for your various steps. It's a fairly easy, graphical ETL tool that can handle all of the steps you have outlined above. https://sourceforge.net/projects/pentaho/
There is something wrong with your syntax as it is seeing the column definition as the alias instead. I see you are missing a `TRIM` on your first `CASE`. Maybe that's it? You'll have to fiddle with it until it works as you have the SQL engine and the data, of which I have neither.
It depends on what software you are using, but in MySQL a simpler option is to build a stored procedure which performs all this conversion for you, a method I’ve used before is to load the file into a raw table unedited, then delete the lines you don’t want based on your filtering criteria(building repeatable logic into the process) You can then insert into your “real” table, using the transform options in SQL. If the time to load is the issue, removing indexes from your table and adding them back post load sometimes removes lots of overhead, as there’s only one instance of index update rather than per row. A better option is generally to get the data in the transformed manner the way you want it to remove security issues or just unnecessary processing, but I’m guessing that’s not possible based on your post. If you are working in sql server, an integration services package may be able to handle this for you- there are similar solutions for other software too.
I would definitely look into PDI as I suggested above and move this into a proper ETL flow - you'll thank yourself later, especially if this is something that will be ongoing.
I'm interested in trying out stored procedures. How would this process work? Would it look into txt file, then convert 2008-11-30 to a DATE in the first table?
Agreed but its not obscurity. Its pretty much mathematically impossible to guess a GUID, its very easy to guess an int. You are stopping brute forcing on user ids completely so the only way to leverage a 'get me data for this user' method lacking an 'is current user' check would be to somehow find out another users ID. That would only work if the GUID was exposed elsewhere in the app or a MITM attack or something. Thats the difference between a critical security issue and medium. You wouldnt need another exploit for an int but would for a GUID. It really comes down to how trustworthy your front end devs are and how large your application attack surface is. 
I used SQL Fiddle to mock up some data in Oracle since I'm not running it locally. http://sqlfiddle.com/#!4/96ae0e/1 It seems the problem was the empty string comparison. Oracle treats empty strings as NULLs (I guess?). So I edited the query to use `IS NOT NULL` and it seems to work.
I was just thinking that. you rule!
Personally, instead of trying to do it all in sql I'd whip up a perl script or whatever that takes the raw data and inserts just what you want to keep into the database, transformed appropriately.
Do you do basically the same things to the data every time? If there is a consistent set of rules, I'd probably build a Python script that loops through the file and does these things automatically.
What "mathematically impossible"?... GUIDs/UUIDs are generated via well known and understood algorithms. Once someone sees one they will get the 'static' and 'parameterized' parts. If your system is not very busy well this only means that there will be more holes (no records found) in the sequence but that's why we need computers to brute force through ALL numbers representing milliseconds.
Yes, the format is: Problem Expected Results Hint And there's an answer section at the end of the book. Also, in the FAQ at the bottom, you can download some sample questions.
A correctly rolled GUID on a system where the attack has no access to the CPU clock cycle etc is cryptographically very secure to guessing. If the implementation is using only data that can be accessed from the web then yes it makes it possible to predict. &gt;Thus, for there to be a one in a billion chance of duplication, 103 trillion version 4 UUIDs must be generated. [Source](https://en.wikipedia.org/wiki/Universally_unique_identifier#Collisions) I'd say those numbers are high enough that its basically mathematically impossible. As an example of how incrementing ints can cause a security issue on a system I worked with the records had states and moved through them. Only certain state changes were allowed and it had code to stop an incorrect state (think complete back to draft). It also had code to detect if several incorrect state changes occured in a row for that record and it would put it into an error'd state to stop any further attempts to move it. This code had no 'is the current ID the same as the requested state change ID' check. A rolling integer brute forcer could attempt several incorrect state changes for every single number between whatever range the IDs appear to be in and have completely killed that system (it was patched). This sort of potential attack vector would be completely stopped by using GUIDS. I am not suggesting for a second that we all roll GUIDs instead of INTs just for security reasons. I am just pointing out that GUIDS do offer a security advantage over INTs as IDS in some situations and make some attack vectors largely useless without some sort of other exploit or data source.
That pretty much is classic textbook security by obscurity. If your system will give up another user's details to me if I can correctly guess their User ID (anyone's ID in your system), then that it a huge security flaw. The correct way to implement security is to first establish a security connection using credentials that will only work for my account. Then, limit my access to information that only pertains to my account. If you have a wide open API that will give me anyone's data if I provide their ID, that's awful.
'Duplication' and 'collision' for implementation 'as intended' means nothing for 'cryptographic security' - 'security' part is protecting against 'incorrect'/'malicious' use. Your particular example implies attacker ("brute forcer") to have access to the interface that does 'state change' - *that* is your security breach, not the ability to iterate through a couple hundred thousand integers. 
Does this HAVE to be done using SQL? I would probably create a batch script (Python) to pull out each record `where Category like '%|%'`, apply some regex to get the individual values, then create and execute the INSERT statement.
Ok then, let's assume for the sake of argument that your GUID algorithm is solid. Each GUID is unique and not guessable. Congratulations! But then you still have the same security problems you are trying to solve with creative key generation (which actually does not improve security). The GUID can still be discovered. Does it appear **anywhere** in your application (client side or back-end)? Do you still have a wide-open API that lets me get someone else's data if I find out the GUID? Do you store this GUID as a hash digest using a cryptographic one-way hashing algorithm like you would store a password? Is it salted? Do you make sure this value never goes to the client? No you don't do those things. Because primary key values are not credentialing tokens. Keys are not used for authentication.
karwin, celko, the manual, and practice
Unfortunately yes. I was able to do a query that sort of gives me what I want (it'll split the strings on |, but i have to link it to my CategoryDescription table and then update my MovieCategories table). I'll update my original post with the query that seems to put me in the right direction. 
 sum(cast(total_capacity as integer))
&gt; GUIDs/UUIDs are generated via well known and understood algorithms. Once someone sees one they will get the 'static' and 'parameterized' parts. Even moreso when you use algorithms like Comb or Flake to generate time-ordered GUIDs, i.e. what everyone should be doing when using a GUID in a clustered index.
Yes, I know. I am trying to check that the row I want to copy won't add a row that conflicts with an existing primary key.
Generally three steps, Load data local infile /load the data to your raw table/ Delete the invalid columns Select the remaining rows into your real table, applying the transformations you want to do, so you can cast as date the date fields for example If the file is consistent in its format, you should be able to build some repeatable code and store that in a stored procedure to execute.
&gt; Thus, for there to be a one in a billion chance of duplication, 103 trillion version 4 UUIDs must be generated. &gt; &gt; Source &gt; I'd say those numbers are high enough that its basically mathematically impossible. You are conflating two separate things. One is the likelihood an algorithm generating a duplicate GUID across separate machines. The other is the likelihood that I can guess another GUID your system has generated. &gt; a system where the attack has no access to the CPU clock cycle etc is **cryptographically very secure** to guessing You really don't know what you're talking about.
It depends on the complexity of pulling the data, but sometimes I’ll need to write some PL/SQL or T-SQL to automate pulls or get exactly what I want. Rule of thumb, write it in SQL until you can’t. Having a query/code that shifts from SQL to whatever flavor of proprietary programming SQL your database uses is costly because it has to pass the results back and forth between the two SQL engines the database uses. 
Postgres has ways of handling key conflicts on insert if you don't want an error. https://wiki.postgresql.org/wiki/UPSERT
How would you check for invalid columns? 
Deleted my old comment as I remembered MySQL is a PITA. Integer is not a CAST or CONVERT'able format with MySQL... I would suggest DECIMAL(0) instead, depending on how you want to handle the values. What kind of input values are coming in with this column, and how do you want to handle rounding if you're moving from decimals to whole numbers?
Unfortunately INTEGER isn't a valid data type for CAST or CONVERT in MySQL... because why would they adhere to standards?
&gt; Having a query/code that shifts from SQL to whatever flavor of proprietary programming SQL your database uses Can you elaborate? What language does MSSQL (for example) have beyond its SQL? Do you mean CLR (.NET), or do you mean non-data related commands such as BACKUP DATABASE or such?
[T-SQL](https://en.m.wikipedia.org/wiki/Transact-SQL) 
Cool! That's exactly what I need! Thanks very much!
Its probably not the most exciting resource, but have you covered all the basics in the Teradata docs? [https://www.info.teradata.com/HTMLPubs/DB\_TTU\_16\_00/index.html#page/SQL\_Reference%2FB035-1148-160K%2Feqp1472241406820.html%23](https://www.info.teradata.com/HTMLPubs/DB_TTU_16_00/index.html#page/SQL_Reference%2FB035-1148-160K%2Feqp1472241406820.html%23) I'm still learning a lot of this myself, but focusing on SQL instead of SQL with R or Pandas has improved my understanding of the engine. I was writing merely functional SQL to extract data to be manipulated in Pandas. I'm still learning how to write great SQL :) What do you think of these practices? Is there business logic in the application that should be in the database? Best path - are you using the best choice for each task? Have you evaluated multiple paths and can you explain your choice with performance tests? Its rare, but cursors may be the best choice for a task! Error handling - No surprises, are all outcomes covered ? Data integrity and concurrency - depends on the requirements of your analysis and app (e.g. are dirty reads ok?) Getting technical - Can you use a natively compiled stored procedure or function in place of those compiled at runtime? Are you using table hints optimally? Version Control and testing - How are your procedures, functions, or views versioned, tested and documented? Security - Procedures and views can also support your security objectives.
&gt; You are overthinking it. Primary keys do not need to be unique across tables. Not necessarily. You may very well need (or at least benefit from) keys being unique *across instances of the same application*. There are cases where you might need to merge data from several databases, and many RDBMSs use UUIDs for replication, so you'd have to store them anyways even if they're not selectable fields. You also have the benefit that, technically, you can generate your UUIDs anywhere, even in your application. I can easily imagine a situation where a gaming site might have several game servers running separate instances for performance reasons, but that data needs to be aggregated into an OLAP database or some other sort of data warehouse. 
But that's different. Even with multiple instances, shards, or standby applications, a key to a row in a table does not need to be unique across different tables representing different business entities. That's the situation OP is going after.
You've done the hard part. Now you have a list of movieID -&gt; category name, you can put it in a temporary table and use that to: * populate your table of Categories from the distinct list of category\_names, adding a category id * join with the Categories table to populate a link table of MovieID -&gt; CategoryID
Your code removes the internal newlines. This is a different behavior than the one I've described.
TRIM already works the way you want it to: SELECT TRIM('\n' from ' \n \n foo bar \n \n ') AS TrimmedString; Returns foo bar
Why not use a regular expression? regexp_replace(s, '(?:^[[:space:]]+)|(?:[[:space:]]+$)', '')
Change the tool preferences?
That's what I did, there's a tab called NLS but it didn't help
There's differences. I'm not as familiar with the ANSI standards as I should be. A couple differences off the top of my head are: 1. Implicit Auto Commits for Microsoft SQL Server. Not so for Oracle. 2. Date functions are different. 3 .Oracle has the weird empty string is a null character. 4. Oracle Dual table versus Microsoft doing select 'value'; 
If you can access a secure record having just an ID, then you've already failed to secure your system. If you're worried about missing something as important as a `isCurrentUser` check, then the chances are you're going to be leaking some important UUID somewhere. You can at least write tests to validate the former, but the latter is much easier to miss. What more, assuming the latter is secure is more likely to result in undesired behavior when a new dev picks up this code, and doesn't know that these decisions were made with "security" in mind. Remember, at this point you're trying to compete with people actively trying to break your system. They are the sort that will happily dig through hundreds of pages of data to find something useful. In such a scenario, resources are best spent ensuring you have a good QA infrastructure that can validate your security solution, instead of trying to find clever solutions to marginally increase security by hoping some data elements don't leak. I use UUIDs from a different type of security consideration; preventing side-channel attacks. If you see incremental ID 150, that is sufficient to tell you how many other records of this type are in the system. If you're running a business, and your competitor just created a client account with that number, now they know how bit your client list is. 
&gt; GUIDs/UUIDs are generated via well known and understood algorithms. Once someone sees one they will get the 'static' and 'parameterized' parts. The *vast* majority of UUIDs are [version 4 UUIDs](https://en.wikipedia.org/wiki/Universally_unique_identifier#Version_4_\(random\)). Those have 6 fixed bits and 122 completely random bits that are not based on timestamp or the generating system in any way. A v4 UUID looks like" `xxxxxxxx-xxxx-4xxx-Nxxx-xxxxxxxxxxxx` where `N` is always the value `8`, `9`, `a`, or `b`. The remaining `x` are all random (usually cryptographically random). Most RDBMS generates version 4 UUIDs with their built-in UUID functions. Oracle is an exception where `SYS_GUID()` returns a sequential value that *approximates* a UUID. That is: you should generate your GUID in your application. I know that .Net's Guid.NewGuid() generates v4 GUIDs. [Python's doc](https://docs.python.org/3/library/uuid.html) basically indirectly says "use uuid4()". Java usually generates UUIDs with `UUID.randomUUID()` which generates -- you guessed it -- a v4 UUID. PostgreSQL and a lot of other languages use some form of the uuid-ossp library, which does let you specify your version (1, 3, 4, or 5), but I would suspect that most people reading the doc would select v4 since they all say, "this exposes private information," with v1, while v3 and v5 require submitting a namespace additional argument. 
You could always set in the database session the way you like it and then check 'Skip NLS Settings' in the tool preferences.
This is a good point and it took me a while to see why I missed that (note that I've changed the example string in the original post). However, moving forward, is it possible to do the same with ' \r\n \r \n foo bar \r\n \r \n ' ?
&gt; I'm still not sure I understand why you need uniqueness across entities though. I've run across a bunch of scenarios where uniqueness across tables is desired. Mostly these scenarios exist at the interface of multiple systems, where the idea of "separate tables" may not necessarily be relevant; things like serializers/de-serializers, client side GC, and generic data stores. Also, situations where you might want to use tools like closure tables across multiple tables are much, much easier to implement if your all tables have universally unique keys. In all the above scenarios using UUIDs can simplify your algorithms, at the risk of that one in a trillion chance of a UUID collision. 
If you wanted to use RegEX: REGEXP\_REPLACE(@mystring,'\[\\r | \\n | \\s\]','',1,0); [https://dbfiddle.uk/?rdbms=mysql\_8.0&amp;fiddle=f15042b4ca65ec467766d3be4a48d480](https://dbfiddle.uk/?rdbms=mysql_8.0&amp;fiddle=f15042b4ca65ec467766d3be4a48d480) [https://dev.mysql.com/doc/refman/8.0/en/regexp.html#function\_regexp-replace](https://dev.mysql.com/doc/refman/8.0/en/regexp.html#function_regexp-replace)
Just tried it, doesn't work
I guess this is the way to go. However, my version of MySQL is somewhat old and doesn't have (AFAIK) natively the `regexp_replace` function: SHOW VARIABLES LIKE "%version%" version 5.6.39-83.1 Any workaround idea for that?
Your code seems to remove all newlines, not only from the edges. This is a different behavior than the one I've described. However, I do agree that this this seems to be the way to go!!! Unfortunately, my version of MySQL is somewhat old and doesn't have (AFAIK) natively the `regexp_replace` function: SHOW VARIABLES LIKE "%version%" version 5.6.39-83.1 Any workaround idea for that?
does date even store timezones?
This is the best answer. Microsoft literally provides all the tools to learn basic SQL for free.
https://asktom.oracle.com/pls/asktom/f?p=100:11:::::P11_QUESTION_ID:5011677391274 DATE doesn't support timezones. 
Yeah, that doesn't do what he wants. (Besides removing spaces and newlines from in the middle, it also removes | characters...)
I thought older versions of MySQL had regexp support (Though a different library, come to think of it, that might not understand the RE I used) since like the 90's. I might be wrong though. If it doesn't have the function... upgrade or use one of the other solutions?
What's your intended goal here? Do you want to fix your data in the tables, or are you trying to format data for output?
&gt; upgrade or use one of the other solutions? 1. It is a shared server, therefore upgrade is not an option. 2. Currently there is no other solution besides yours (which, btw, would be a very elegant solution if it weren't this version issue)
Yes I learned this too, DATE stores upto seconds but not the time zone.
&gt; What's your intended goal here? Develop a robust DB as much as possible independent from the application (o course, within my limited resources). Therefore, it is valuable to make the DB clean itself the data before inserting into the table instead of only expecting the application to do so.
What is your rdbms (database)? How are you loading the data? Seems like a long time to load the data.
I assume load takes the longest? You could potentially limit your columns im source file before load to speed that part up. Typically "best practices" (they may vary) state load all columns into character datatypes in a "staging table", 1:1 format of file. Then transform data from Stage to its final place "Target table".. again this varies by application (can have more hops if needed). But if performance is the issue then try to reduce file size before the load. 
*can be made And if you're an Oracle employee that's what you'll do pretty much all the time. And your code will be formatted like a 5 year old typed it. And if you ask them what a "house style" is, they will give you a blank stare. 
Your requirements are actually pretty lean, despite your diction implying otherwise. You still can't go wrong with postgresql, with well-supported node.js libs (a few to choose from) and public cloud compatibility from AWS and Google if you want to host it there. However, that's not the cheapest option if you do not need high availability; I recommend Hetzner's cloud servers, the smallest of which probably wouldn't meet your 1000 ops/min unless it's all basic CRUD reads but the next step up might. What storage requirements do you have? 2 GB? 200 GB? You said no big ETL jobs, but what about complex `select` or broad `update` queries?
Postgres is still a good choice.
The peak time will be super basic CRUD stuff, as I'm OK delaying the expensive operations a few seconds since they'll all be admin stuff. Broad update queries will only happen in "oh shit" scenarios when I need to patch stuff up, and select queries usually won't top 2k rows on a table of no more than 10k Hetzner looks cheap and good, but I'm based in the US. Not sure if that matters since I won't be doing a whole lot of back-and-forth. 
Fun fact: Redshift primary keys are merely informative, and by default will not enforce PKs when loading tables.
Hmm. What about `regexp_substr()`? Does your version have that?
 Error SQL query: Documentation select REGEXP_SUBTR() MySQL said: Documentation #1305 - FUNCTION REGEXP_SUBTR does not exist 
`regexp_substr()`
 Error SQL query: Documentation select REGEXP_SUBSTR() MySQL said: Documentation #1305 - FUNCTION REGEXP_SUBSTR does not exist
 Error SQL query: Documentation select REGEXP_SUBSTR() MySQL said: Documentation #1305 - FUNCTION REGEXP_SUBSTR does not exist
I'm using Dream report
So, those 2.5million records are for only one "tag" and there are about a 100 tags. I'm using microsoft sql server, the tables are quite big with many columns. As for the indexing i don't think there are any as we use wonderware historian to collect this data and I dont think it creates indexes, and then dream report gets the data with queries. The query is just a "select" with an INNER REMOTE JOIN also.
There is a need for the per-second granularity, but as we are using wonderware historian to collect the data, I think I might be able to make a "summary tag" that that doesn't have as much data and that way I could make the Dream report reporting software collect data from that summary tag. The reporting software isn't running that many queries at a time, I tried to check the times and I'd say about 4 queries almost at the same time. I'm using Microsoft SQL server, Historian for collecting data to database, and Dream report to make the reports. There actually is a "INNER REMOTE JOIN" in the queries, the system which I'm testing on shouldn't need the remote join as I'm running everything on my Virtual machine, but in the field I think it might be needed, and also I have no idea where I would change the join as dream report is running the queries after it was setup. The query is basically something like this: SELECT \[DateTime\], \[TagType\], \[Value\], \[Quality\] FROM \[Tag\] INNER REMOTE JOIN \[History\] ON \[TagName\] WHERE \[TagName\] = 'tagname' AND \[DateTime\] &gt;= 'date1' AND \[DateTime\] &lt;= 'date2'
Could it be that it becomes this slow when it does the "REMOTE INNER JOIN" it have to go through all of the 2.5million records, because it does go through all of it on the join, right? And then because of so many tags and queries like this it goes through it becomes slow?
its actually very unlikely to get to a point that an UUID will collide. "1 billion UUIDs every second for the next 100 year" for just 1 duplicate.
&gt;s I made a plan for one of the slow queries, how should I get it to you? just a picture of the graphical plan or?
what i mean by unwanted changes is in this article https://medium.com/lightrail/prevent-business-intelligence-leaks-by-using-uuids-instead-of-database-ids-on-urls-and-in-apis-17f15669fd2e
There is a lot of so-called [modern SQL](https://modern-sql.com/) which is quite powerful but can be subtely different between Oracle and SQL Server, particularly when you are dealing with non-current versions (typical in most enterprises). You get hit by things like LISTAGG not being available and various functions being different.
The end goal is to have a persistent table to dump data into as an intermediate table before applying them to another master table. Teradata made this possible with Global Temp, which stores definition, but doesn't delete them after. When you say I could just 'truncate it at the end' what does that mean / entail ? 
Use PostgreSQL!!! 
I could. What's the logic behind a temporary table if you can just use a permanent table then?
I can't really give you a justification for using global temp tables. I think their usage is weird. But session specific temp tables are easier to justify. E.g you want to do something that should be limited to your session and not be visible to other sessions.
Try https://www.brentozar.com/pastetheplan/, or post the plan on a file share.
Interesting insight. I guess you could have a stored procedure that you could run that could go through all the permanent tables you have that are meant to be used for dumping and clear them out with truncating right?
[I thought this looked familiar](https://www.reddit.com/r/SQL/comments/8yhz07/top_10_sql_interview_questions_and_answers/) All the criticisms noted there still apply.
&gt; As for the indexing i don't think there are any This would be my focal point. Look at the columns that are used to join and filter and see if some indexes could help.
If you are able to select data in desired format, then easiest way would be to just create new table based on that: CREATE TABLE new_table AS (SELECT * FROM old_table); Later just rename tables and that's it. Or you just insert those rows you selected in the same table and later delete all obsolete rows (e.g. all rows containing Delimiter). INSERT INTO your_table (col1, col2) SELECT something from your_table WHERE something like '%thing%';
That article doesn't have anything to do with unwanted/unauthorized changes. What it basically boils down to is having 'guess-able' data points leads people to guessing stuff. So, giving out incremental IDs (UI/extracts/whatever) allows guessing people figure out velocity and max sizes of identified population. It's a valid concern if you have it and you should implement non-sequential IDs (or UUIDs) for external references to obscure that. It doesnt impress me much though - as with many 'look at this thing' things. There are other ways to gauge your org so you'll need to be consistently aware and in search of guess-able data and guard against most or most important circumstances. It is not a bad thing - but given how bad sometimes actual security (unauthorized access) implementation is, I would want to gauge that first and foremost. Now, I believe the sequential IDs (if you are on MSSQL, sequential GUIDs give (or used to) better performance than IDENTITY) will be much better for you clustered keys - so if you decide that you do care about exposing those, you'd want to still make your PKs sequential, and your FKs will reference those; UUID/GUID columns will be only needed for entities referenced externally or observably (UI/extracts/ETL/APIs/etc.)
Despite /u/da_chicken's conviction of V4 being in widespread use and Wikipedia naive math, RFC recommends 47-bit random generation, which is quite a drop from 122 bit. Also, pretty much all 'random' number generators are pseudo-random and some are bad enough to have a guessable pattern in a sensible number of sequential tries. Performance of the truly random V4 would be my concern for a high-volume system. The question and suggestion is still the same - determine what are legitimate/visible threats (if this is threat-based), prioritize them and address by priority.
It would be a lot easier to reason about this if we got more of your requirements and usage. :) But having a landing table or staging table is a common thing. You're talking about many such tables? Do you have many master tables? What work is to be done on the data when moving from landing to master? How often? There is a command called truncate [tablename] that truncates all the data. It's faster than a delete. But on the other hand. I'm more partial to moving data first and then deleting only the data that I've moved from the landing table. That way you could move things in batches and you could adjust the batch size to something that works for your hardware. 
That’s what we do... we have a scratch database where everyone has full access. 
Unfortunately, raevnos's answer doesn't help me. INSERT ON CONFLICT only came in with PostgreSQL version 9.5, and my customer uses PostgreSQL 9.3. So, since I can't use an ON CONFLICT clause, what is the best solution?
Get some big data set (sport if you like it, music, movies etc.), load it into DB and start analyzing. Don't think about SQL, think what you want to get from this data, e.g. find all NBA champions and add their ranking year before and year after (you can use Lead &amp; Lag for this). This is just simple example. Best way to learn is when you need to answer questions.
In this scenario, it sounds like it doesn't need to be visible to other users. I normally use a permanent table as a staging table (adding "staging" to the table or schema name to indicate it's just for data staging). Truncating a table in SQL Server is like deleting all the rows from a table, without as much logging. It's technically an `alter` command. `truncate table &lt;table_name&gt;` I typically set up a SSIS package (Microsoft ETL product) to wipe the staging table, move data into the staging table, merge the data (often through a stored proc so that I can edit it directly on the server) and the wipe the staging table again (to prevent hoarding data I don't need). Depending on the ETL product, you can even just include a command to create the temp table on the fly. I've done this when writing Python scripts to ETL out of Hadoop. Hope that helps! 
It also depends on the position and company/department you are applying to. Some business people don't have any technical skills, and when you create SQL report for them they think you are God. On the other hand, IT nerds like to ask about advanced staff even if you are never going to use it in your career. Don't stress, just start learning and after some time star applying for jobs.
I assume you can reach the email server since you use SSMSBoost? I want to say SQL Mail is independent of SQL Agent, at least you'll be able to test! I would append the[ SQL linked here in stack overflow](https://stackoverflow.com/a/13305615/5149122) at the end of your query after you plug in the appropriate information.
You should read up on memory optimised tables, (specifically with schema_only durability) they are tables that can be used globally - like global temp tables however the data/structure is stored in memory instead of on the disk
&gt; Table names that start with ## are visible to all users/connections, and are dropped when the last connection that touched it is closed. Just tested this. ##tables will close when the following two conditions. 1. The connection that created the ##table is closed. 2. There are no open transactions referencing the ##table. Merely referencing a ##table does not seem to keep it open for the duration of the connection. --- --Session 1 CREATE TABLE ##Test (ID int); INSERT INTO ##Test VALUES (1); - --Session 2 BEGIN TRANSACTION; SELECT * FROM ##Test WITH (HOLDLOCK); --Close Session 1 WAITFOR DELAY '00:00:15'; SELECT * FROM ##Test WITH (HOLDLOCK); COMMIT TRANSACTION; SELECT * FROM ##Test; ---- ID| -:| 1| ID| -:| 1| Msg 208, Level 16, State 0, Line 6 Invalid object name '##Test'.
Would that work though for an Azure SQL database? 
Sure. Go for it.
Well yea but how 😆?
Load take about 30 minutes. Updates take long too. I was wondering if there were better way to reduce times. 
Here is my example database I am building to practice. Take note the "GUI" table will be the one I want to access as the pretty and functional section for the user. http://imgur.com/bx99x4R
Here is my example database I am building to practice. Take note the "GUI" table will be the one I want to access as the pretty and functional section for the user. http://imgur.com/bx99x4R
Can you guarantee that the temporary data fits in RAM? What if it doesn't? 
It's easy to avoid inserting rows that already exists. Your solution isn't that clean because it's possible the concatenated cover + area will match an existing record even though cover and area separately do not. But you can just use "where (old_public.cooling_cover.cover, old_public.cooling_cover.area) not in (select . . . from cooling_covers)" or even better, "where not exists (subquery selecting from cooling_covers)" That won't help you if old_public.cooling_cover has dupes itself, though. You can try "insert into . . . select distinct", but that may not work either since you could have a cover and an area duplicated in old_public but not some other column. If that's the case, you need to decide what data from old_public you actually want and work from there. 
The way you describe teradata global temps is literally just a regular table that auto truncates, so why get hung up over a name? Just use a table and truncate it.
All you need is SQL Server's SSIS tools. Well, plus TeraData's ODBC driver installed. SSIS is extremely powerful and can handle pretty much any transformations you need to do while the data is in flight.
Sounds like you're creating somewhat contrived example in order to practice SQL. Nothing wrong with that, but make sure you have a great understanding of real-world SQL down as well. If you're looking for some very hands-on "learn-by-doing" practice problems, that teach basic to advanced SQL with well-designed, real-world practice problems, similar to what you're trying to solve, check out [SQLPracticeProblems.com](https://sqlpracticeproblems.com/#utm_source=r1&amp;utm_medium=r1&amp;utm_campaign=r1). I developed it after teaching a SQL course where the material I had to to teach from was poorly structured and academic. Afterwards, the students emailed me, saying they needed practice problems, so I developed the course! Contact me (email in the FAQ) if you have questions. Also, redditors get 30% off the Professional package (discount code reddit30). 
Because I know the other associates and even the slightest inconvenience will cause them to be confused and either forget to do that, or do it at the wrong time and mess up the data massively. That's really the problem in a production environment.... 
Thanks very much for your advice
Thanks a bunch, i am defiantly at the relative beginning stages, but I am quickly picking up the fundamentals and ideas. Aldo yed this is a example one omly to see if I am correctly making promary keys amd foreign keys that can work with each other.
&gt; The end goal is to have a persistent table to dump data into as an intermediate table before applying them to another master table. Teradata made this possible with Global Temp, which stores definition, but doesn't delete them after. This general pattern is usually called a staging table. The table can be temporary or permanent, but this process is most commonly called staging data. There's a couple of patterns you could use in SQL Server. First, just create a permanent staging table and use that. You'll need to delete the table contents before or after each use (I recommend before) so if you need multiple concurrent users of the staging table, this is not appropriate. It also means you're logging your inserts into the staging table in your database's transaction log, but that's not often a significant concern. it is by far the easiest method, and frankly, seeing what the data looked like at the end of your import process is really useful for debugging errors. Second, create the temp table manually on each connection. Simply put, it's only one extra statement. It also means that your import process contains all the information about the import including the staging table's schema. Third, create or use an existing permanent table as a template. You can then execute `SELECT * INTO #StagingTemp FROM TemplateTable WHERE 1 = 0`. This will create a temp table using the same schema as the base table. The WHERE clause here just serves to exclude any data so that the table is created empty. The only real benefit here, however, is that you don't have to maintain the CREATE TABLE statement in your import process. You can even create a table from a query with JOINs or VIEW if you want. I've even seen someone create a VIEW like `CREATE VIEW dbo.StagingTemplate AS SELECT CAST(NULL AS INT) AS [Id], CAST(NULL AS VARCHAR(30)) AS [LAST_NAME], ...` with no `FROM` clause at all and use that as a template, but, honestly this kind of a silly amount of complexity to go through in order to avoid the first or second pattern above. The above two patterns are what the RDBMS was designed to do to accomplish this task. &gt; When you say I could just 'truncate it at the end' what does that entail? He means you execute `TRUNCATE TABLE StagingTable` to delete all data in the table. It's equivalent to `DELETE FROM StagingTable`, but TRUNCATE is minimally logged so it executes comparatively quickly. Essentially it just tells SQL Server to just make the table empty instead of telling SQL Server to delete all the rows from the table. The only downside is that it's DDL so it requires the ALTER TABLE permission. 
It's close, but the data remains in a memory optimized table between connections. The schema_only durability means that the system never writes the data to disk, but it persists as long as the instance is running.
Here: [Installing Teradata Tools To Use with SSIS](https://www.mssqltips.com/sqlservertip/3363/installing-teradata-client-tools-to-use-with-sql-server-integration-services/)
yes my PK are sequential. the UUID is just an access key for end users
[rubber ducking](https://en.m.wikipedia.org/wiki/Rubber_duck_debugging) 
**Rubber duck debugging** In software engineering, rubber duck debugging or rubber ducking is a method of debugging code. The name is a reference to a story in the book The Pragmatic Programmer in which a programmer would carry around a rubber duck and debug their code by forcing themselves to explain it, line-by-line, to the duck. Many other terms exist for this technique, often involving different inanimate objects. Many programmers have had the experience of explaining a problem to someone else, possibly even to someone who knows nothing about programming, and then hitting upon the solution in the process of explaining the problem. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/SQL/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Which podcasts *have* you tried? Any particular flavor of SQL?
I just grabbed a few of the most popular on the iTunes store. As far as particular flavor I dont really know enough at this point to really answer that. The videos and lessons I am working through are covering all the basics I assume. I have watched the people I will be helping out do a few things, but most of it, up to this point, has still been over my head.
If the SQL questions for the interview has basics like "what is a primary key, what's the difference between Truncate and Delete statements", etc., then just search online for SQL interview questions, and you'll be fine. On the other hand, many interviews actually involve needing to solve real world problems with SQL. If you're looking for some very hands-on "learn-by-doing" practice problems, that teach basic to advanced SQL with well-designed, real-world practice problems, similar to what you're trying to solve, check out [SQLPracticeProblems.com](https://sqlpracticeproblems.com/#utm_source=r1&amp;utm_medium=r1&amp;utm_campaign=r1). I developed it after teaching a SQL course where the material I had to to teach from was poorly structured and academic. Afterwards, the students emailed me, saying they needed practice problems, so I developed the course! Contact me (email in the FAQ) if you have questions. Also, redditors get 30% off the Professional package (discount code reddit30). 
Thanks, guys! I am completely self taught and I know I must misuse a lot of functions. Appreciate the help! Maybe this is something better off for Tableau as my goal is to have the data connected to Tableau for automated reporting.. ? In other news, if anyone knows of any worthwhile training for either SQL or Tableau - please share! 
Thanks for this answer. I should've mentioned in this case, I'm plugging the output into a visualization program, so I need 6 separate series, hence the `'#' as motor` which I use to create the series
the q values are the maximum and minimum values for the motor's angle as read out by the encoder over time. in this case I want to separate into 6 series so I can use a visualization tool to chart them. 
Sure, you can subtract the two sums to give an overall variance at the aggregate level. Or you can subtract the two columns at the detail level and then get a sum of the variances. (Those numbers will not necessarily be equal.) You are already doing something like this: sum(x) as Total_Actual_Amount Just do this: sum(a) - sum(b) as Total_Variance Your group by will remain the same. Also, when all else fails, you can also surround your entire select statement with another select statement to do anything you need to do: select (a - b) as Total_Variance from (select...group by...) 
I've done this quite a few times myself in here.
So like what are the ones you have listened to called? I'd be interested to know so that I can tune in.
If you only need to move data, SSIS is Microsoft's go-to tool. If you have a lot of tables to migrate, you want to look into BIML before trying to code a lot of transforms by hand. Googling "teradata sql server migration tool" finds some interesting hits too, especially if you have code to migrate. Disclaimer: I know nothing about Teradata other than it seems to pop up on Reddit a couple of times a year
&gt; I just grabbed a few of the most popular on the iTunes store. Do they have names?
I tried SQL server pain relief: office hours and Dear SQL DBA. 
One DB per tenant if you want scaleability.
Two of my favorites. 
https://itunes.apple.com/us/podcast/groupby-free-sql-server-training/id1196061826?mt=2
There is no direct equivalent that I know of but SQL Antipatterns by Bill Karwin is a great book to prepare for interviews.
This worked for me: DECLARE @J VARCHAR(MAX) = ' { "sold_price": "Sample", "auction_date": "Sample", "address": { "state": "State", "street": "Sample", "number": "Sample", "suburb": "Sample", "postcode": "Sample", "country": "Sample" }, "REA_Agent": "Sample", "sale_date": "2018-08-03 00:13:04+00:00", }'; SELECT JSON_VALUE(@J,'$.address.state')
It's not always going to be the same amount of columns though... There will always be at least 2 results but it can go up to 8. Would that still work?
If you build it like /u/bitterjack's description, yes.
If I understand you correctly you wanted to know how to get the nested values.
Correct(ish) I want to get all of the values in my JSON file into a table. The ones that are not in the additional nest are loading into the table fine, but "state", "street", "number" etc are not showing when i run Select\* From BT1 Example: STATE NUMBER STREET SUBURB POSTCODE PROP_TYPE SOLD_PRICE SOLD_DATE NULL NULL NULL NULL NULL Apartment 857000 3/08/2018 NULL NULL NULL NULL NULL House 1401000 28/07/2018 NULL NULL NULL NULL NULL Apartment 442000 28/07/2018 So anything that is under the "Address" nest, does not seem to come out like the rest, I assume because i've missed an additional piece of code that brings the nested data out. I hope that makes sense, im still learning the terminology. 
Ok.. I showed you how to access nested values. Like this: '$.address.state'
&gt;Thanks Mauna, I will have a look tomorrow - but thank you :)
On mobile so can't format tabs or check any of the syntax, but here goes. It's separated into subqueries so you can see what each step does. I'm particularly worried about the outermost query needing a -1 or +1 in the STUFF function index argument. Try adding one of those if the 'and' placement doesn't look right. SELECT CASE WHEN COALESCE(MyString2, '') &lt;&gt; '' THEN REVERSE(STUFF(REVERSE(MyString2), CHARINDEX(',', REVERSE(MyString2)), 1, 'and')) ELSE NULL END AS MyString3 -- replace last instance of comma with 'and', by reversing it, finding the "first" instance, replacing that using STUFF, then reversing it back FROM ( SELECT CASE WHEN COALESCE(MyString1, '') &lt;&gt; '' THEN LEFT(MyString1, LEN(MyString1) - 2) ELSE NULL END AS MyString2 -- remove last comma and space FROM ( SELECT COALESCE(NULLIF([Column1], '') + ', ', '') + COALESCE(NULLIF([Column2], '') + ', ', '') + COALESCE(NULLIF([Column3], '') + ', ', '') + COALESCE(NULLIF([Column4], '') + ', ', '') AS MyString1 -- concat columns, however many you need. Empty string/NULL columns will be ignored. FROM TableA ) AS Subquery1 ) AS Subquery2
I will add some table aliases to make it a bit easier to read. The pure SQL way to do it is this: SELECT r.redditUser, COUNT(*) AS "RemovedPostCount" FROM ActionLog a INNER JOIN RedditPost r ON a.postID = r.postID WHERE a.postStatus = 'Removed' GROUP BY r.redditUser ORDER BY r.redditUser; Although, depending on your data, you may need: SELECT r.redditUser, COUNT(DISTINCT r.postID) AS "RemovedPostCount" FROM ActionLog a INNER JOIN RedditPost r ON a.postID = r.postID WHERE a.postStatus = 'Removed' GROUP BY r.redditUser ORDER BY r.redditUser; Aggregate functions and GROUP BY are an advanced beginner topic, although you're writing a JOIN here so you should be able to grasp them. GROUP BY says, for each unique occurrence of the columns listed in the GROUP BY clause, do the aggregate function once. That means that every column in the SELECT clause has to either also appear in the GROUP BY or be one that's in an aggregate function (like, say, `SELECT CustomerId, SUM(Qty * Price) AS "TotalCustomerOrderValue" FROM Order GROUP BY CustomerId`). So, you can't include `RedditPost.title` in the above query because you'd have to either GROUP BY it (which would give you the number of posts removed per user per title) or you'd have to aggregate the titles somehow, which of each post because you're going to be combining those posts together into the count. The query engine wouldn't know which title to use. You can get around that with the GROUP_CONCAT() aggregate function in SQLite: SELECT r.redditUser, GROUP_CONCAT(r.title) AS "RemovedPostTitles", COUNT(*) AS "RemovedPostCount" FROM ActionLog a INNER JOIN RedditPost r ON a.postID = r.postID WHERE a.postStatus = 'Removed' GROUP BY r.redditUser ORDER BY r.redditUser; But bear in mind that GROUP_CONCAT() is not a standard SQL function. Don't avoid using it by any means, but just be aware that it won't exist in every RDBMS or will have different names or different functionality in other RDBMSs. Also, rule of thumb: You almost never want to use SELECT DISTINCT. Overusing it is a very common beginner pattern. DISTINCT is expensive because it forces the query engine to sort the entire result set and then remove duplicates. 
This. One of the best parts of open office spaces: cross-pollination and problem solving.
thanks man!
Kudos to you know typing that out on mobile. It gave me a headache just reading it. 
More public dataset repositories. https://registry.opendata.aws https://cloud.google.com/bigquery/public-data https://www.kaggle.com/datasets
Cool, thank you, I will look it up. :)
That's awesome, thank you so much. I'll have to do some more research to understand it all, but this looks exactly what I need.
T SQL is the dialect of SQL used in Microsoft SQL Server. It is the same language with some additional functions. 
The way you've written it, it's looking for root level nodes called "state", "number", "street" etc. But those fields are actually all nested inside the "address" node, so you get NULL returns. You just need to qualify your field names.
Both Microsoft and Oracle have Database Development certifications you can do. [Microsoft's MCA - Database Developement](https://www.microsoft.com/en-us/learning/mcsa-sql2016-database-development-certification.aspx) [Oracle Advanced PL/SQL Certifications](http://education.oracle.com/pls/web_prod-plq-dad/db_pages.getpage?page_id=654&amp;get_params=p_id:228#tabs-1) My advice would be to look at Indeed/Monster/LinkedIn/etc. to see what the primary RDBMS is that's used in your area. Most regions, at least in the U.S., are either Microsoft or Oracle regions. 
 WHERE mydate BETWEEN LAST_DAY(CURRENT_DATE - INTERVAL 2 MONTH) + INTERVAL 1 DAY -- first_day_last_month AND LAST_DAY(CURRENT_DATE - INTERVAL 1 MONTH) -- last_day_last_month 
Not sure I understand correctly, but I think I'd just use sys tables instead. If you're talking about dynamic SQL then you do not want to do that if at all possible, it's painful to develop and considered poor practice if reasonably avoidable.
Thank you for the reply
Don't use Postgres but other ways of handling this include using JOINS or perhaps [MERGE](https://www.postgresql.org/message-id/attachment/23520/sql-merge.html) statements. For the Join method, you would LEFT JOIN your Source table to the Target table on the proper keys, then only include records where the Target keys are NULL.
I'll help out a bit. SELECT CASE WHEN COALESCE(mystring2, '') != '' THEN Reverse(Stuff(Reverse(mystring2), Charindex(',', Reverse(mystring2)), 1, ' and')) ELSE NULL END AS mystring3 -- replace last instance of comma with ' and', by reversing it, finding the "first" instance, replacing that using STUFF, then reversing it back FROM ( SELECT CASE WHEN COALESCE(mystring1, '') != '' THEN LEFT(mystring1, len(mystring1) - 2) ELSE NULL END AS mystring2 -- remove last comma and space FROM ( SELECT COALESCE(NULLIF([Column1], '') + ', ', '') + COALESCE(NULLIF([Column2], '') + ', ', '') + COALESCE(NULLIF([Column3], '') + ', ', '') + COALESCE(NULLIF([Column4], '') + ', ', '') AS mystring1 -- concat columns, however many you need. Empty string/NULL columns will be ignored. FROM tablea ) AS subquery1 ) AS subquery2
Typically certs (the ones that hold value) in SQL are meant for users who have a few years of experience and want to test their chops or shorten that gap in knowledge. You have the other spectrum of users who want to get into database and try for certifications as another way to stand out while working towards a SQL primarily job. It sounds to me like your work is offering you an education stipend which is amazing! I personally think you should use this investment on things not exactly related to certs, but maybe you decide it is best to go with certs. The certs themselves are usually pretty cheap. If I want a MS SQL cert, it is $165 per exam and you need to pass two exams to obtain your MCSA if you are on the 2016 path. This would leave you with $6670 left for the year. And to be very blunt, those exams are hard. They aren't impossible, you can cram for them in a month and pass, there are shortcuts, but they aren't meant to be achieved via short cuts. The paper you hold at the end of the exam is worthless, it's all of the experiences and education you went through and gained that is worth something. With all of that said, /u/SQLSavant had a great point, you need to pick a flavor and figure out what you're going to do. I would not recommend flipping between database platforms unless you have to for work unless you have years of experience under one already. With one picked, you can look at the exams listed and then look at the material necessary to pass the exam. I'm sure googling the cert will bring up many resources you can use. With your money, I'd probably invest in a streaming platform like Lynda or Pluralsight or perhaps someone you find in the industry who has specific tailored videos to your interests. I would also subscribe to Safari books online as you will have every technical book you'll need to buy, saving you more money for other things. With X money put aside for the cert exams later in the year along with a little money for practice tests and retakes, I would then look at training's either online or yearly that you can attend. Once you know a little more about the flavor, we can give you more specifics. Oracle World and SQL PASS are two events I'd love to attend, but with that money you may be able to break it up into multiple local conferences. 
Use "120" instead of "12". I'm pretty sure 12 is yymmdd
If you want YYYY-MM format, use 23 instead of 12. What other problems are you having?
&gt;Is it possible to subquery the SUM and then join it? Absolutely. Just join to it in an aliased subquery, e.g. `SELECT` `baseTable.AccountNumber` `,baseTable.Column1` `,baseTable.Column2` `,transactionSums.TotalTransactions` `FROM` `baseTable` `LEFT OUTER JOIN (` `SELECT` `AccountNumber` `,SUM(TransactionAmount) AS TotalTransactions` `FROM` `TransactionTable` `GROUP BY` `AccountNumber` `) transactionSums` `ON baseTable.AccountNumber = transactionSums.AccountNumber` However, doing this would break your HAVING clause. Without a GROUP BY, HAVING will (if I read the docs properly) aggregate that entire field and operate on the single aggregated value. You could either move the HAVING clause to inside the subquery, or use `WHERE transactionSums.TotalTransactions &lt;&gt; 0` on the outer query, depending on whether you want to exclude accounts with transactions summing to zero from the results or just have them return a NULL total.
Use a CASE statement- something like SELECT FORMAT([table].[column], 'yyyy-MM') , 'Column Name For Reference' , Pct = SUM(CASE WHEN [your where condition] THEN 1 ELSE 0 END)/COUNT(*) FROM [table] GROUP BY FORMAT([table].[column], 'yyyy-MM')
I'm confused -- why do I need to use my WHERE condition? I want to do the % as the specific COUNT I am doing over the total records for that given month.
120 is yyyy-mm-dd hh:mi:ss.
The way you phrased it I thought you were wanting the ones satisfying the where condition as a % of all records for the month. That would do that, however it *could* include some which do not satisfy the where condition (they'd show 0%). Perhaps try aliasing your table and then doing a sub-select. Basically the same query without the where clause, joined to the original on whatever columns they have in common. Something like: SELECT FORMAT(t1.[column], 'yyyy-MM') , 'Column Name For Reference' , Pct = COUNT(*) / cts.Ct FROM [table] t1 INNER JOIN (SELECT ym = FORMAT([column], 'yyyy-MM'), Ct = COUNT(*) FROM [table] GROUP BY FORMAT([column], 'yyyy-MM')) cts ON cts.ym = FORMAT(t1.[column], 'yyyy-MM') WHERE [your where condition on t1] GROUP BY FORMAT(t1.[column], 'yyyy-MM'), cts.Ct
23 returns yyyy-mm-dd for me on Server 2016.
Correct, but the NVARCHAR(7) will limit the answer to the first 7 characters, the YYYY-MM part.
Yes I do want the WHERE condition count as a % of total COUNT for that month
Well, I didn't consider that properly! Nicely done.
Alright, then what I said initially should work- you move the WHERE condition into the SUM(CASE [where] THEN 1 ELSE 0 END) and use HAVING SUM(CASE [where] THEN 1 ELSE 0 END) &gt; 0 to exclude any where none meet the WHERE condition
Like u/d_r0ck said, the [EOMONTH](https://docs.microsoft.com/en-us/sql/t-sql/functions/eomonth-transact-sql) function is your friend. It will return the last day of the month of a date value, so gives us a convenient GROUP BY option SELECT EOMONTH([column name]) as Month, SUM(Amount) as TotalAmount FROM dbname GROUP BY EOMONTH([column name]) If you want the first of the month instead, you can use the function's options to go back a month, plus add a day... SELECT DATEADD(dd,1,EOMONTH([column name],-1)) as Month, Sum(Amount) as TotalAmount FROM dbname GROUP BY DATEADD(dd,1,EOMONTH([column name],-1)) ... or you can use the pre-2012 compatible code by playing with a combination of DATEDIFF and ADDDATE: -- Adds (number of months between the beginning of time and now) to (the beginning of time) SELECT DATEADD(mm,DATEDIFF(mm,0,[column name],0) as Month, Sum(Amount) as TotalAmount FROM dbname GROUP BY DATEADD(mm,DATEDIFF(mm,0,[column name],0) 
This is where I am so far. I am definitely making a dumb mistake, but I can't figure it out. SELECT FORMAT(\[table\].\[column\] , 'yyyy-MM') AS 'YYYY-MM' , 'Kiosk-Mall' AS 'Lead Source' , COUNT(\*) AS 'Monthly Total From That Lead Source' , Pct = SUM(CASE WHEN \[your where condition\] THEN 1 ELSE 0 END)/COUNT(\*) FROM \[table\] WHERE \[table\].\[other column\] = 'Kiosk-Mall' GROUP BY FORMAT(\[table\].\[column\], 'yyyy-MM') ORDER BY FORMAT(\[table\].\[column\], 'yyyy-MM');
Perhaps you want something like this? SELECT FORMAT([table].[column], 'yyyy-MM') AS 'YYYY-MM' , 'Kiosk-Mall' AS 'Lead Source' , COUNT(*) AS 'Monthly Total From That Lead Source' , Pct = SUM(CASE WHEN [table].[other column] = 'Kiosk-Mall' THEN 1 ELSE 0 END)/COUNT(*) FROM [table] GROUP BY FORMAT([table].[column], 'yyyy-MM') HAVING SUM(CASE WHEN [table].[other column] = 'Kiosk-Mall' THEN 1 ELSE 0 END) &gt; 0 ORDER BY FORMAT([table].[column], 'yyyy-MM');
I don't bother do doing the convert. SELECT DATEADD( mm, DATEDIFF( mm, 0, \[column\]), 0) FROM \[tablename\] GROUP BY DATEADD( mm, DATEDIFF( mm, 0, \[column\]), 0) This can easily be converted to day, hour or year. You can even do in 15 minute increments if you wanted. You way works, but I use this for all of my date groupings. It makes it easy for me to identify what's going on simply by looking at the first parameter in DATADD/DATEDIFF. 
When I go to group things by month, I create a new data field that is DATEPART(YEAR,datefield) \* 100 + DATEPART(MONTH,datefield) and then group on that. It scales well to large datasets.
I got this error: Msg 241, Level 16, State 1, Line 23 Conversion failed when converting date and/or time from character string.
Definately this. It is easily the most flexible approach and when I spent some time reading about it a while ago was one of if now the most efficient approaches too.
SQL is the generic word used to describe the language used to interact with databases. ANSI SQL is the standard, but each database vendor has their own flavor based upon it (T-SQL for SQL Server, PL/SQL for Oracle, etc.), with tweaks &amp; extensions. SQL Server is Microsoft's relational database platform. The difference between SQL Server and SQL is the difference between England and English.
Ok, this is rather difficult to diagnose without specifics. Is [table].[column] a DATE/DATETIME field? It sounds like it might not be, in which case you might have a value which can not be converted to a date. Also, I don't know which version of SQL Server you're using. FORMAT() is a newer function. You might try using DATEADD(MONTH, DATEDIFF(MONTH, 0, [table].[column]), 0) to get the beginning of the month- that works pretty much anywhere.
So when I'm using SQLite during a codecademy training, that's just a different source, similar to SQL Server and My SQL? So if My SQL, and Sequel Server are English (in your analogy) are T-SQL and PL/SQL dialects of individual "languages"? Does it matter when beginning which language or dialect you use in this case?
Damnit bot! The autocorrect is too strong now, it's just gunna stay like that
Damnit bot! The autocorrect is too strong now, it's just gunna stay like that 
SQLite is an RDBMS. Very lightweight, great for embedding in applications, and is very close to the ANSI standard. Yes, they're dialects. Think of it like T-SQL is American English and PL/SQL is English English. The dialect you learn on doesn't matter _too_ much, as long as you understand that if you change engines you'll go through an adjustment period as you try to figure out what "bangers and mash" is, or why the Coke you ordered in South Carolina tastes suspiciously like Pepsi.
Sorry, I'll clarify. "Run as query", meaning I opened a New Query, copied over the text from the agent job (exec [sproc_jobname...], variables, \\sharename\output.csv). On the old server, this runs without a problem. On the new server, it runs just fine if I store it locally, or if I drop it in a few low level folders on that \\sharename\. It's when I get into deeper folders that it begins to crap out on me. However, both servers are running the job as the same account. Both DBs are owned by the sa account. The SQL service account is a domain account that I've specifically granted permission to the folder structure in question, but it won't even drop it at the base folder. I can't figure out which account it's authenticating with on the successful run (old server). The Owner on both sides is DOMAIN\SQLsvcaccount.
Correct. Everything is rooted in ANSI SQL. SQLite's dialect is the closest to it AFAIK.
I had to read another article for what you said to make sense, but i did figure it out in the end - so thankyou !
Ok so both the SQL Server Service and SQL Agent Service are running with the same account on both servers, right? Have you tried running the source of the sproc as a separate script? What exact line cause the access failure? Otherwise since it is a stored proc check the EXECUTE AS in case it’s is coded to run as some unique account. If this doesn’t bear fruit, the only other idea I have is the OS... as the only difference perhaps win 2016 has added a security feature that is causing the problem?? But I am not a win16 expert so couldn’t help past that.
Learn python. I work in an accounting department, to help accountants develop reports. I picked up python to help with everyday tedious task. Automated different types of reports. Schedule task to run for things I don't want to remember. Python is your ultimate toll. Automate the boring stuff is where you shoud start, you will not regret it. 
Can you elaborate? What should the case statement look like for this scenario?
SQL can be really useful for creating custom reports. The SQL most people use is just a small subset of the language. It's basic SELECT stuff, joins, where filters, etc. Luckily, it's not too difficult to learn. But the more advanced stuff (e.g. subqueries) can be useful too.
Why don't you use month(column name)||year(column name) as month, instead? And group by "month"? 
It's not SSMS but you could use PowerShell to accomplish this with some coding. [Here is code for a notification.](https://mcpmag.com/articles/2017/09/07/creating-a-balloon-tip-notification-using-powershell.aspx?m=1) It would be possible to do it with that and an Invoke-SqlCmd call.