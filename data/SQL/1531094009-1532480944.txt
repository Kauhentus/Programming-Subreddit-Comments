Thank you for your response and thank you for pointing me towards the sub, I'll definitely take a look. Could you tell me a little bit more about what you do? I get the idea conceptually but what do you do day to day?
Is there such a thing as "the SQL guy" in application development? Would that fall under one of those three categories? Business intelligence?
Currency needs to be in Double Quotes "Currency"
ok, same error shows https://imgur.com/a/v4C3j6M
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/TwAl5wU.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20e2139c1) 
Try Lowercasing your function to format 
Interesting! So do you sell databases to company's? I've been wondering, since our company will be transferring to a new system soon, whoever sells us the system is going to have to work with us to customize it to our needs. Sounds like there will need to be someone very familiar with the inner workings of their product, be able to make adjustments to both the back end and the front end, and then do training. Sounds non stagnant and exciting, would kind of be a dream job if it exists! Is that what you do, does that exist?
Also if that doesn’t work try the CCur function but not sure if Access can use that in SQL or if it’s just for VBA.
https://www.reddit.com/r/SQL/wiki/index
okay, i just did, same error. https://imgur.com/a/DSxkWJJ this type of error is on a few more of my currency type of questions, and it also is giving the same problem
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/ponF2Ik.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20e2141lx) 
No, I'm in the casino industry. Database Marketing is email, mobile, and snail mail campajgns to acquire, retain, and reactivate customers. I've seen companies that do what you describe but the roles are usually separated from my experience (i.e. developer, trainer, account manager).
Thanks for responding! What do you mean when you say "and soft skills to deal with investigators, etc" My biggest interests that would be relevant is making (woodworking, metal working, etc.) so I think working in a fabricating/manufacturing environment would be cool. Outside of that, my other area of expertise is commercial photography but I'm not sure how that would be relevant. I really like consumer products, that's what I've worked with for all of my professional career. 
So I guess you are dealing with tons of records, but is that a SQL heavy job or more marketing focused? 
I don't know what any of those things are, so I've got some googling to do! Sounds like graphic design in a way?
I work for a health system, so I'm a report monkey as I'm pretty junior level so basically I work a ticket queue where report requests come in with the issue or problem people are trying to solve, I meet with them and talk about it to better understand what they actually want vs. what they put in the ticket (lol) and then go back, write the report (SQL), provide an initial dataset, we comb through the data set to weed out the things we both inevitably didn't consider. Rinse and repeat. I worked as an application support analyst prior to this so I have a deep understanding of the software we report on, which has made learning SQL much easier. Long-term my department is moving towards utilizing predictive analytics to better understand health issues and intervene prior to problems happening, or just generally use predictive analytics to drive business decisions/processes (supply chain for example - right stuff at the right place at the right time). I hope to move into this type of work as I build my skills and move from an analyst level to a developer level.
Yes and no. Each of those 3 career paths make you the SQL guy. I've been the SQL guy in support for 8 years which puts me at the top of support but demand is underwhelming without the associated skill in those career paths. Basically this is one skill in 3 careers that require more than one skill. I get to fix things until DEV can fix the code patch, but in looking at the market no one will hire (quickly) without more skills and those are the careers they are looking for. Ive had a few nibbles but they are basically for the same kind of deal (Application Engineer). DBA and PowerBI are fairly easy to do if you are willing. I want to get into DEV and that's freakishly hard where I am (requires a person to basically start over from the bottom with a huge pay decrease as an intern-equivilant position, else find a company where they promote from within and recognize talent and hard work and then work in IT with support). I'm working up from within now (for the 3rd time). Fingers crossed no bad luck this time (Im going to be so pissed if another great manager of mine gets let go)
Thanks for your response! So what would be an example of practical use? The only reporting I'm familiar with would be from websites and email campaigns but I imagine this goes way beyond user interaction...
That sounds amazing
If you're looking for some very hands-on "learn-by-doing" practice problems, that teach basic to advanced SQL, check out [SQLPracticeProblems.com](http://sqlpracticeproblems.com/). I developed it after teaching a SQL course where the material I was required to teach from was very dry and academic. The students didn't get real-world practice, and thus didn't get a good foundation in the SQL that's used most commonly. Afterwards, they emailed me, saying they needed practice problems, so I developed the course! Email me (email is in FAQ) for a reddit discount code. 
That's enough. Show that you can provide business value and that you can learn the business side.
50/50 although I do have a senior analyst that spends more time in sql. We develop our own queries, stored procs, jobs, ETL processes using our CRMs data access layer. It also can involve some BI and predictive analytics, we just started using Python in MS SQL Server so we're diving into that now.
I actually like this course from Udemy. Great instructor and he shows you how to set up a free PostGreSQL database https://www.udemy.com/master-sql-for-data-science/
That's a very broad question and varies case by case but some days it's simple and only a few joined tables and sometimes there's dozens of tables and queries within queries that I have to get help from senior people on, so it just depends. I like it because it is like putting puzzle pieces together and I like the variation and constant learning.
I mean, adventureworks is still worth using.
Go to this website: [https://www.sololearn.com/Course/SQL/](https://www.sololearn.com/Course/SQL/) and/or download Sololearn from the app store. It's a good place to get your feet wet. iOS: [https://itunes.apple.com/us/app/sololearn-learn-to-code/id1210079064?mt=8](https://itunes.apple.com/us/app/sololearn-learn-to-code/id1210079064?mt=8) Android: [https://play.google.com/store/apps/details?id=com.sololearn](https://play.google.com/store/apps/details?id=com.sololearn) Aside from that, I bought *SQL in 10 Minutes, Sams Teach Yourself.* You can probably find it used, but if you get one that doesn't have the code in the book used, you can get a discount of the online version of the book which has videos and you're able to try the code in the book on the playground. I paid $10 for that because I bought the book. It was originally $40ish, I think.
Joins are a very fundamental part of SQL - you may want to get some more practice with them. If you're looking for some very hands-on "learn-by-doing" practice problems, that teach basic to advanced SQL, check out [SQLPracticeProblems.com](http://sqlpracticeproblems.com/). I developed it after teaching a SQL course where the material I was required to teach from was very poorly organized. The students didn't get real-world practice, and thus didn't get a good foundation in the SQL that's used most commonly. Afterwards, they emailed me, saying they needed practice problems, so I developed the course! 
This should work for you: `;with FirstPurchase as (` `Select` `RowNumber =ROW_NUMBER() over (Partition by PersonID, ManufacturerGroup order by PurchaseDate )` `,PersonID` `,ManufacturerGroup` `From Purchase` `)` `,PurchaseTotals as (` `Select` `PersonID` `,ManufacturerGroup` `,BenefitTotal = sum(Benefit)` `,FirstPurchaseDate = min(PurchaseDate)` `,CountPurchases = count(*)` `From Purchase` `Group by` `PersonID` `,ManufacturerGroup` `)` `Select` `FirstPurchase.PersonID` `,FirstPurchase.ManufacturerGroup` `,PurchaseTotals.BenefitTotal` `,PurchaseTotals.FirstPurchaseDate` `,PurchaseTotals.CountPurchases` `From FirstPurchase` `join PurchaseTotals` `on PurchaseTotals.PersonID = FirstPurchase.PersonID` `and PurchaseTotals.ManufacturerGroup = FirstPurchase.ManufacturerGroup` `Where FirstPurchase.RowNumber = 1` Feel free to check out my course, [SQLPracticeProblems.com](http://sqlpracticeproblems.com/). I developed it to give real-world, hands-on practice in SQL. There's lots of problems similar to yours.
Check the following web site [http://www.studybyyourself.com/seminar/sql/course/?lang=eng](http://www.studybyyourself.com/seminar/sql/course/?lang=eng). The course, along with examples, is quite easy to follow. You can submit exercises as well. Everything for free. 
Check the following web site [http://www.studybyyourself.com/seminar/sql/course/?lang=eng](http://www.studybyyourself.com/seminar/sql/course/?lang=eng). The course, along with examples, is quite easy to follow. You can submit exercises as well. Everything for free. 
Go to this website: [https://www.sololearn.com/Course/SQL/](https://www.sololearn.com/Course/SQL/) and/or download Sololearn from the app store. It's a good place to get your feet wet. iOS: [https://itunes.apple.com/us/app/sololearn-learn-to-code/id1210079064?mt=8](https://itunes.apple.com/us/app/sololearn-learn-to-code/id1210079064?mt=8) Android: [https://play.google.com/store/apps/details?id=com.sololearn](https://play.google.com/store/apps/details?id=com.sololearn) Aside from that, I bought *SQL in 10 Minutes, Sams Teach Yourself.* You can probably find it used, but if you get one where the scratch-off discount code is intact, you can also get a discount for the online version of the book which has videos and which allows you to try the code in the book on the playground. I paid $10 for that because I bought the book. It was originally $40ish, I think. Also, remember, there are different Database Management Systems that require SQL to be written a certain way for it to function. The aforementioned book does a good job of talking about several of them. And anyway, once you get a good grasp of the basics, it's not that difficult to look up what the version is for the DBMS you're working in. Good luck.
I own a copy of your book from Amazon. Interesting problems and I like how it has hints with it (even it would be more challenging and educational without them). What I disagree is the emphasis you give in using Microsoft SQL Server instead of MySQL or PostgreSQL. Yes I am aware SQL Server can be installed in Linux but how many newbies are aware of that? Plus **I think** it is a time limited installation so not much long term value for a student. I am posting this because you offer Microsoft SQL only in the basic version and you require $99 for the MySQL version.
erm... it constantly crash lol
Wow thank you verry much. It’s so smart - I mean learn by doing. Thanks for help. Everyone!
r/therewasanattempt
Just remove , A1.benefit, A1.purchase_date From your GROUP BY and it should work fine.
[https://sqlzoo.net/](https://sqlzoo.net/)
Are you sure that's correct? Looking at your example desired output, all you needed to do was remove , A1.benefit, A1.purchase_date From your GROUP BY. 
Express and dev versions of MSSQL are free and not time limited.
It sure is. I use it for my T-SQL practice from time to time.
So: - You have an excel file. - There is a database. - You want to replace the excel file in the database (what?) - You get errors. - You have problems with arabic language. - You tried a tool that came with SSMS. &amp;nbsp; - What are you even trying to do? Excel file, database... - Do you expect us to guess the errors? - Do you expect us to guess the problems with the arabic language? - Do you expect us to guess what you *tried* to do in the import/export tool? 
I really like this series: http://www.sqlservercentral.com/stairway/ I would start with [Beyond the Basics](http://www.sqlservercentral.com/stairway/104497/) and move to advanced T-SQL after that, but what's cool is there's a lot of "how to be a DBA" type stuff in there that will really push you ahead of your peers if you're doing any kind of analytics with data mgmt.
Could you post the code, or a simplified version?
SELECT ISNULL(Convert(varchar(50),LocationName),'Total') AS 'td','' ,Convert(varchar(50),Convert(Decimal(15,1),ActualBudget)) AS 'td','',Convert(varchar(50),Convert(Decimal(15,1),Budget)) AS 'td','', Convert(varchar(50),Convert(Decimal(15,1),Variance)) AS 'td','' FROM (SELECT MAX(ServiceDate) AS [Monthdt] FROM Vw_Executive_ADC_ADA) B CROSS APPLY ( SELECT Convert(decimal(15,1),AVG(ActualADCUnitTotal)) AS ActualBudget,Convert(decimal(15,1),AVG(ADCBudget)) AS Budget,LocationName, Convert(decimal(15,1),(AVG(ActualADCUnitTotal) - AVG(ADCBudget))) AS Variance FROM Vw_Executive_ADC_ADA C INNER JOIN OceansLocations D ON C.location = D.Location WHERE ServiceDate = [Monthdt] GROUP By LocationName WITH ROLLUP ) A order by ((case when LocationName is null then 1 else 2 end)), Variance DESC
I haven’t used mgmt studio, but in case someone else has, it sounds like you’re trying to recreate an excel sheet as a table in the database, but you’re getting import errors. 
Hard to help you here without any specifics, can you tell us the steps you're using? The exact errors your getting? Are you trying to import as a new table? Add to an existing one? What version of SQL Server are you using?
/u/Yuvuz_Selim is right. When asking for help you need to provide specific information about what's trying to be accomplished, what system you're using, what errors you're receiving, and what you've tried up until this point. For example: when you said that you were trying to replace the excel file in a database, this was confusing but I assumed from context that you had an excel file and were trying to import its data into a database table (like MySQL or something). But I have no way of knowing if that's true. Also, without an error message there really isn't much we can do to help (there are simply too many possibilties).
It's not easy to read it like that. But I guess in your totals line you want to sum the columns that are being calculated using AVG? Using the ROLLUP won't do that, it doesn't alter the aggregate function, only the group(s) it's applied to. You could split the query into various steps using CTEs (a WITH clause) allowing you to get the averages and then to sum them. With Param_Month as (select max(ServiceDate) as MonthDt from Vw_Executive_ADC_ADA) , Budget_Stats_1 as ( select t.LocationName , t.ServiceDate , avg(t.ActualADCUnitTotal) as ActualBudget , avg(t.ADCBudget) As Budget from Vw_Executive_ADC_ADA t join OceansLocations o on (o.location = t.location) join Param_Month p on (p.MonthDate = t.ServiceDate) group by t.LocationName , t.ServiceDate ) , Budget_Stats_2 as ( select t.* , ActualBudget - Budget as variance from Budget_Stats_1 t ) , With_Totals as ( select LocationName , sum(ActualBudget) as ActualBudget , sum(Budget) as Budget , sum(Variance) as Variance group by LocationName with rollup ) --- select ISNULL(Convert(varchar(50),LocationName),'Total') AS 'td' ,'' ,Convert(varchar(50),Convert(Decimal(15,1),ActualBudget)) AS 'td' ,'' ,Convert(varchar(50),Convert(Decimal(15,1),Budget)) AS 'td' ,'' , Convert(varchar(50),Convert(Decimal(15,1),Variance)) AS 'td' from With_Totals
Look into coallations. https://stackoverflow.com/questions/3560173/store-arabic-in-sql-database
Also, anyone with a .edu email can get a free enterprise copy of the latest sql version from Microsoft. 
You may have to create the table with correct coallation before importing
I suggest using nvarchar instead.
Thanks vagara. The pro pack actually contains a whole additional set of practice problems, in addition to a lot of other features, so it's not just the MySQL version. 
&gt; I am reading The Practical SQL Handbook and they use them in the WHERE clause put the book down back away carefully never look at it again
The join conditions should go in the join! It's much clearer. And with outer joins you don't really have a choice.
Haha do you not like it? I think its pretty good this was the first thing that made me scratch my head
Well thats the thing. In this example there is no join condition they just list the two tables in the FROM and put author id 1 = author is 2 in the WHERE. Ive always done an inner join when self joining on a table
SQL-92 released in 1992 set the SQL2 standard for platforms to support JOINS, UNIONS, etc. Major rdbms platforms support these while some even dropping engine support for the early WHERE joins. You can still perform these joins but the engine will treat it like a cross apply and will cartesian the results prior to filtering. You write your joins specific to the platform. If you are writing for MSSQL you best write to the highest SQL complaince and they give no craps about backwards compatibility.
Interesting. Maybe that is why. This book was written in 1998.
Maybe the book was published at a time when support for ANSI join syntax was not as widespread? 
That's like reading a book on VB6, following the examples and wondering why VB.NET is throwing errors about your code... When it comes to programming, don't read books that are considered ancient texts.
I just checked it out because it was one of the few in my company library. The reviews on it seemed pretty good even for todays standards. I am more than halfway through and this was the first thing that seemed off. I learned a lot from the book so far overall so its not that bad. I should have just ordered the most recent edition of it.
Placing your join conditions in the where clause is the old fashioned way of doing it before the JOINs were around. You would write all of the tables you need to "join" in the from clause and separate them by commas, then include the conditions for the mess of tables in where clause. The newer and more accepted way of writing the syntax is to keep the join conditions with the respective join. This helps others who may need to interpret/review your code as it is much clearer to see the purpose of the join.
Analytics and data science are great! I've wandered through a few different position in my time using SQL and Access so feel free to PM me if you'd like to chat more.
That's their point: the "author id 1 = author id 2" tells SQL how to link the two tables (it *is* the join condition), so the JOIN criteria should be right there with the table they relate to (as part of the FROM clause) and not buried in the WHERE clause where it's harder to find and edit/dev.
My favourite method is to use a window function for this. 2 advantages: you only need to list each column, no need to write out a comparison; and it had no problem with nulls. Select * from ( Select t.* , Sum(1) over (partition by col1, col2 ... ) As n From table_with_dups t ) Where n &gt; 1
This is not the best way. If you have n records duplicated on the columns you're interested in, then a join will return n*n records. This could get heavy if you have big data or a lot of dupes. If you want all of the duplicate rows returned, then /u/toms-w solution is the way to go. if you want one row per duplicate set, then replace toms-w inner query with a regular ol' "... sum(1) from ... group by" .
Yes, a group by can be more useful if you're sure your duplicates are duplicates : in my case the duplicates often arise because of wrong expectations, eg I'm joining on some but not all of the columns, and wrongly expect them to form a unique key. In this case is helpful to be able to see all the columns in the table, as they might account for the repetition.
3rd advantage: it's almost always faster than joining a table back to itself. 
If Hitler used SQL I'm pretty sure he would put his join conditions in the WHERE clause.
Database types are notorious for complaining about sub-optimized queries. A self join to find duplicates is fine, keeping in mind the efficiency concerns noted here and the need for proper indices. And yes, analytic functions are wonderful. Put it another way, do you always use a steak knife when a blunter knife is working fine?
Yeah this seems to be the case from all the responses. Its good to know at least.
ve vill not let you join anysing! joining ist verboten! papers please!!! 
Need a propaganda poster to accompany thjs
OP came here to learn, they were offered advice. CTE and self joins when a window function is most likely easier to write and more efficient? Work smarter, not harder.
By soft skills I mean communication, oral and written. Are you good at “customer service”, which is kind of an odd thought in my area, data for research. Can you manage a relationship, because we work with a lot of doctors and nurses, and many are awesome, but many are pushy/demanding/always changing the scope of work/talking down to you because you aren’t an MD. 
Does it connect via IP address or the VIP of your cluster?
That is interesting. I hadn't tried to connect via IP, I always use names. When I tried to connect via VIP using SSMS the connection failed. This seems strange to me because they are going to the same location. connection string is SSMS: VirtualServer.FQDN\\InstanceName Sucess VirtualServer.VIP\\InstanceName Failed
Would you mind to point me towards an RDBMS that has dropped support for join predicates placed in the WHERE clause? Thank you!
do what does this mean? How do I fix this?
I’m also learning and self-taught! 
It might just be my age, but I never even considered putting the join condition in the where clause. In fact, I try to make my where clauses non-existent when possible. Join conditions contain my table filters, and if I need to put a WHERE on my first table, I generally opt to make a CTE. Makes readability *much* better.
Neither have I. This was the first time I saw it.
Stackoverflow, has real world problems ranging from simple to complex. Along with answers and the reasoning behind the answers. Also you can download their q and a database 
Maybe this is too simple but I usually do a GROUP BY HAVING. SELECT Column1, sum(*) FROM Table GROUP BY Column1 HAVING sum(Column1)&gt;1. 
You can do this with a lag function, syntax is something like this: Select column_name, current_status, lag(current_status)over(partition by column_name order by log_date) as previous_status From table Where... Sorry for lack of formatting typing from phone.
I usually use COUNT with GROUP BY. ` Select &lt;insert columns here&gt;, [count] = Count(*) From [table] Group By &lt;insert columns here&gt; Having [count] &gt; 1` 
 Thanks for your response. I keep getting the previous status as null using the below query Select key, status as current_status, lag(status)over(partition by status order by CREATE-TIME) as previous_status From SCHEMA.TABLE_NAME Where key = 'TTRRYYY' and STATUS_QUANTITY &gt; 0 and status= 'XXXX';
If you want to get status/previous status by key then it would look like this: Select key, status as current_status, lag(status)over(partition by key order by CREATE_TIME) as previous_status From SCHEMA.TABLE_NAME You'll want to remove the filter on status, this will restrict it to only 'XXXX' statuses which won't necessarily be the previous status. Are you trying to get a dump of all statuses/previous statuses for a given key? Or just the latest status along with it's previous? If the latter something like this should work: with last_status as ( select distinct key, status, create_time, lag(status)over(partition by key order by create_time) as previous_status, lag(create_time)over(partition by key order by create_time) as previous_create_time from SCHEMA.TABLE_NAME where ... --Insert predicates here for best performance, don't filter on status ) select distinct key, first_value(status)over(partition by key order by create_time desc) as current_status, max(create_time)over(partition by key) as current_status_time, first_value(previous_status)over(partition by key order by previous_create_time desc) as previous_status, max(previous_create_time)over(partition by key) as previous_status_time from last_status The with statement basically gives the whole dump of status with previous status, the bottom portion will grab just the latest and previous by key, so one row per key. There's other ways to do this with sub-selects, I prefer OLAP/window functions if the DB supports them (this kind of thing can be a real PITA in MySQL which has no OLAP functions).
The additional value of the windowing function is that you can return the entire row, not just the duplicate columns. 
I'm guessing the table looks something like the below: ItemStatusID ItemID Status 1 1 order placed 2 1 order shipped 3 1 order received 4 2 order placed 5 2 order on hold 6 2 order invoiced 7 2 order shipped 8 3 order placed 9 3 order shipped 10 3 order received (with ItemStatusID incrementing as the new statuses are created). Some SQL like this should work for you. This is assuming you can use CTE and window functions (Row\_Number is a window function) in Oracle, I'm pretty sure you can, but you may need to change the syntax slightly. This works in SQL Server. ;with ItemStatusOrdered as ( Select RowNumber =ROW_NUMBER() over (Partition by ItemID order by ItemStatusID desc ) ,ItemID ,ItemStatusID ,Status From ItemStatus ) Select LastStatus.ItemID ,CurrentStatus = LastStatus.Status ,PreviousStatus = PreviousStatus.Status From (Select * From ItemStatusOrdered where RowNumber = 1) LastStatus join (Select * From ItemStatusOrdered where RowNumber = 2) PreviousStatus on LastStatus.ItemID = PreviousStatus.ItemID If you need some practice with this type of problem, do me a favor and check out [SQLPracticeProblems.com](http://sqlpracticeproblems.com/). I developed this course to give real-world, hands-on practice in SQL. There's lots of problems similar to yours. Contact me (email in the FAQ) for a Reddit coupon code. 
&gt;...after 2 days, i'm being told that the tables are still stale. Who/what is telling you this? Other users? Oracle metrics? Trust, but verify. &gt;Now i'm directionless as to what to do more? Is the data in these tables being constantly updated/modified? If so, that could lead to stale statistics. To find when a table was last analyzed, run this: SELECT TABLE_NAME, LAST_ANALYZED FROM DBA_TABLES WHERE OWNER = '&lt;OWNER&gt;' ORDER BY LAST_ANALYZED DESC NULLS LAST; &gt;How do i check the stale status of tables myself personally? There are multiple ways to check for stale statistics. One quick possibility: SELECT TABLE_NAME FROM DBA_TAB_STATISTICS WHERE OWNER = '&lt;OWNER&gt;' AND STALE_STATS = 'YES'; You can also check out DBMS\_STATS.GATHER\_SCHEMA\_STATS. One of the options is "LIST STALE". &gt;... how do i deal with stale tables if the above procedure is wrong? Check the STALE\_STATS value in DBA\_TAB\_STATISTICS after you run "GATHER\_TABLE\_STATS". If STALE\_STATS is no longer YES, then you can simply run the command periodically via a scheduled job. I would recommend a scheduled job with some intelligence to determine which tables to gather stats on, rather than blindly gathering stats on all tables. Tables in which data does not regularly changed do not need much attention.
I have a video that may help. I will send the link in the am if that is ok. It seems that the security for your virtual sql name is not correct or has gotten askew. You need to check you dns entry and make sure your cluster has full rights to it. It is confusing.
Mostly explicit joins, occasionally implicit, depends on my mood and what other developers I'm working with. If it's 2-3 tables joined on single column PKs I'll use the implicit join syntax, but most of the time I break it out with explicit joins. When you get into 7-8+ table joins the implicit syntax is just dumb and error-prone (partial Cartesian products).
On running the DBA\_TAB\_STATISTICS and DBA\_TABLES query you mentioned it is telling me ORA-00942: table or view does not exist 00942. 00000 - "table or view does not exist"
Should be an interesting round of blog posts. Good idea to read through them next week. 
That will give you the same info. It just means you don't have privileges for the others. 
&gt; SELECT * &gt; FROM **** &gt; WHERE ****_activity_date &gt;= DATEADD(D,-7,GETDATE());
Thank you! But for some reason DATEADD isn't working on PL/SQL Developer; is there any alternative? Thanks again really!
we've had candidates young and old... tech interview cuts out approx 95% of candidates. we look for experience with: OLTP schema, OLAP schema, ETL, OLAP cubes, scaling and efficient handling of "large" data (not "big data", but large enough that you need to tune it). usually the biggest issue is dealing with large data... at small data, hardware addresses inefficiencies... "big data" is usually handled outside of SQL (except for SQL PDW / Azure SQL DW)... but with large data, you need to know how to handle the data as quickly and efficiently as possible - querying/accessing, transferring into warehouse, processing, etc. Some of the answers are academic in nature and system agnostic, others are platform/technology specific (MSSQL ColumnStore in 2012 vs 2014 vs 2016).
plslql developer sounds like you are using oracle. so you can always of course google how you substract days from a date in oracle. Ill tell you a secret, just use minus. sysdate - 7
Sorry for not clarifying; still very new to this! So it would be something like activity_date &gt;= sysdate -7 right?
yes https://imgur.com/a/dycoPmZ
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/WL8fQje.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20e24is0j) 
Thank you so much!
The nested SELECT Statement essentially returns and Array (I know SQL does not have arrays) and I had no idea I could do that!
I've never created a portfolio but it's something I feel I should do, so would be interested to hear others opinions. Normally for interviews, they've presented a problem and asked me to show how I'd fix it, simple joins, how to create objects etc.
Your best bet is to 1) Create the tickets with the event, and assign the tickets to a user once they purchase it. 2) To avoid assigning the same ticket to multiple users, you want to make sure you don't commit a ticket to a user too early. The easiest way to fuck this up is the following: I. Client A adds a ticket to his cart II. Server checks for next available ticket. Finds ticket 5796 III. Client B adds a ticket to his cart IV. Server checks for next available ticket. Since Client A hasn't checked out, 5796 is still the next available ticket, so now Client A and Client B have the same ticket Now both clients are butting heads on who owns that ticket, and without server side validation, the last user to check out will be the one to own it. However, with server side validation, you'll end up with a poor user experience ("What do you mean the ticket isn't available anymore? Is the event sold out?"). In short, the server should never trust what the client is telling it, and the client should never know more than it needs to know. 3) I find that whenever I'm considering "How do I update the state of these rows after X amount of time or under Y conditions", I usually find the best answer when I abandon that train of thought. That kind of question usually leads to clunky and unapparent solutions. The reason being: you want to change the state of your data based on an observation of its current state, when all you really need to do is make the observation, right? So when you're reserving a ticket, you'll change the status to reserved, and update some DATETIME column, maybe ReservedTime. Then, instead of excluding all reserved tickets from tickets available for purchase, you only exclude tickets that have been reserved for less than 10 minutes. You may want to implement logic to prioritize purchasing tickets that have not been reserved, but that's optional
Thanks so much
Almost nobody cares about your GitHub portfolio. It's a lot more important to just know SQL, be able to talk about how it works, and explain how you'd solve various problems with it. You should also have an understanding of how DB's work (indexes, triggers, views, stored procedures, execution plans). There are differences between the specifics on the different DB's, but the ideas are generally the same, but don't try to learn them all. Become an expert in one database rather than a novice in 10 of them. The other thing to keep in mind is that SQL is not enough. If you want to be a DBA, you have to know DB's. If you want to be an analyst, you need to know Excel and at least the basics of programming (probably Python or VBA, it just depends on the company you end up with). If you want to work with Big Data, you better be good at R or SAS (and Excel). 
Lately I have been compiling homework for interviews and adding them as my portfolio. One assignment was: " I want to open a pet shop and I want to have my own inventory/point of sale system. I plan to sell pets, toys, supplies, and food. Please create a DB definition that will allow me to track purchases, inventory, vendors, holiday sales, and taxes. I plan on being very successful and selling online in the future". In a day turn around I gave them: 1. A project plan outlining the scope of the concept. 2. Loadtest performance on the database. 3. ERD diagram. 4. Documentation created from Redgate toolbelt. 5. The entirety of the SQL that created the database. I found tax tables online, created a holiday / date time calendar, set up some indexes and constraints, etc. I put that on my blog to showcase it as "Here is something I did in my spare time for an interview. This is a proof of concept database for the assignment purpose and this is what I was able to produce in this quantity of time." Another homework piece that I was given was three questions. There was a piece of SQL with three tables. Two tables were combined with the SQL to make a third table and I was supposed to give them hypothetical answers to reverse engineering the solution correctly while handling little gotchas. I wrote about six pages of explanations and information which described the scenarios I could see and what solutions I would use for each scenario. Afterwards I included about 35 pages of load and performance tests using query plans and stats to identity the likely best queries (I had to write a different way to solve each problem, I found 9 problems. So I wrote 45 queries altogether and measured them / compared them all.) and I attached a programmable query script so they just had to change a variable to run each test simulation locally to reproduce the results. Something I'm working on is a PowerBI resume. [I've talked about that before.](https://www.reddit.com/r/SQL/comments/8pmoea/what_type_of_project_should_i_upload_to_my_github/e0drn1t) Something else I've been doing is blogging. (I guess I mentioned that already.) This gives them more insight into my code style, technical prowess, interests, and gives that extra "oomph". A lot of people get hung up on what to write about. "Everyone writes on indexes, I'll just be adding one more pointless article to the mix." The point of the blog is not to just teach and help people, but it's to help illustrate YOUR knowledge. If they go out and see you wrote 10 articles on indexes, they may ask a followup question or two in the interview but they know you know indexes now. This is especially helpful if you are writing about things they need help with. Another benefit I didn't realize until now is that I can make my own email address. My email has its own domain so it comes off looking nicer and cleaner than a Gmail. (I have a [firstnamelastname@gmail.com](mailto:firstnamelastname@gmail.com) too, but I really like my own address.) Honestly though, the most important thing you can do is network. Go to meetups, go to SQLSaturday, go to everything. Network, network, network. You're in college still, so make use of those connections and internships if you can. The degree helps you not be auto filtered out of jobs when applying, the internships and connections you make now are what get you into the field and achieve entry level experience. Experience + degree = your next job. All of that stuff I mentioned prior is really nice to have, but it's honestly not going to make as much of an impact as networking. Certifications are another plus that can help you. I personally believe if you truly study and learn for them, they can help you when you are at work as it fills in the gaps where you may lack knowledge. Sometimes they are valued, but I don't think I've ever had a job offer sway one way or the other because of a cert. The fact I have the cert is meaningless, the fact I have the blog is meaningless. What they are looking for is that I'm the TYPE of person who will go out to improve myself and my career, further my education and skills without them requesting. They know they are getting a professional because this is the type of action they see professionals take. That's my opinion anyway. Speaking is another great thing to add. You can add slide decks online and create a portfolio that way. For another job interview, (the same one for the database project actually) I created a High Availability slide deck and real time demonstration for a 1 hour slot. I was using a RaspberryPi hooked up with my phone + laptop. It was a simulation of making a change to a table in a highly active OLTP environment without downtime. The point being, I was able to demonstrate a solution using another database platform that wasn't my primary platform as the position was for a completely different RDBMS than SQL Server. The slide deck afterwards talked about the Blue / Green deployment method and a comparison between the two. The last thing is helping folks online, just like this. I am primarily active on Reddit and StackExchange on breaks or after hours at home / weekend to log some time helping others. When you can teach people, you illustrate you know the information and you are reinforcing that material. Again, this is not going to be a huge deal either. The important pieces is that you are learning and exposing yourself to new problems you may not see so that if it comes up in the future, you have a solution to that problem. People who get siloed in the same job don't get that chance to grow and expand as easily, being online helping others is that opportunity. TLDR; Network. Go to meetups. Go to groups. Get online and talk to people. Make connections and network. 
1. [T-SQL Fundamentals should be your first read. This will teach you how to use SQL and SQL Server.](https://www.amazon.com/T-SQL-Fundamentals-3rd-Itzik-Ben-Gan/dp/150930200X) 2. [After you can walk around in your database, you are ready to learn how to design databases. Ralph Kimball has a great methodology.](https://www.amazon.com/Data-Warehouse-Toolkit-Definitive-Dimensional/dp/1118530802/ref=pd_sim_14_8?_encoding=UTF8&amp;pd_rd_i=1118530802&amp;pd_rd_r=4e043012-844f-11e8-9453-e5e28017db9d&amp;pd_rd_w=1dF0E&amp;pd_rd_wg=kFnTA&amp;pf_rd_i=desktop-dp-sims&amp;pf_rd_m=ATVPDKIKX0DER&amp;pf_rd_p=7967298517161621930&amp;pf_rd_r=T8K6SGSNTXSMQZ00YBH2&amp;pf_rd_s=desktop-dp-sims&amp;pf_rd_t=40701&amp;psc=1&amp;refRID=T8K6SGSNTXSMQZ00YBH2) 3. [Now you are ready for the more advanced intricacies. A lot of this will go over your head, you may want to try learning on your own before hitting Itziks next book.](https://www.amazon.com/T-SQL-Querying-Developer-Reference-Ben-Gan/dp/0735685045)
where did you get the NBA dataset? just finished up a SQL 101 course and would love to use an NBA data set to continue practicing! 
Thank you! 
How old is considered ancient? I'm looking through the sql cookbook and it was published in 2005. It's old, but it still like a useful reference for the fundamentals.
Thanks! I'm prepared to do a lot of these things but haven't really converted them into "portfolio" material yet (they're topics/projects I can talk at length about at the moment), so [saved]. One hole in my game is the performance testing you mentioned. Do you have any good leads on resources for teaching myself more about it?
The database I designed was a relatively basic proof of concept that integrated some complex ideas. The core of the application would be inserts with minimal updates and then a delete / clean up process could potentially take place as I had temporal tables set up to give us a cube analysis later. Since it's basic inserts most of the time, I wanted to test what the impact of the temporal tables and constraints would be on my database. Of course this was on a small i3 laptop with 8GB ram and a SSD, so if the results are decent, I'll take that as a win since it should scale. So how I tested was taking Redgate's data generator and then loaded up all the tables at once and attached the run time sheets. To generate 100,000 records in each mapping table, it took about 5-7 seconds. To generate 1 million records on my primary table for purchases, it took about 1 minute and 43 seconds. Just to stress really quickly, that's not a true performance test. For a quick proof of concept though, this should be a decent enough test to show that it can perform pretty well. If I had something more than 2-4 sentences to work off of, I could give better concrete processes. But really quickly, let's get back to performance testing and how you can get yourself up to speed. I actually made a post series recently and one of those posts I talked a bit about performance. Steve Jones asked me if I could add more insight to what I'm looking for specifically and some methodology. Here's more or less my simplified philosophy: (Baseline) - (Current Performance) = (Success Results, determines if actionable items are necessary.) The common theme is a baseline. You need to be able to compare to SOMETHING. The SOMETHING should be a defined goal driven by the business. Here's an example: Our users need to access the database through a web application to send and update records. Ok, so we know we will have concurrent users, there will be a web and a db layer with ideally another in-between layer to act as a buffer for the DB. I would follow up and ask: - What is an acceptable amount of time to wait when sending a record? - How about when updating? - Both of those questions I would assume the answer would be instantly to 10 seconds. Maybe instantly to 3 seconds if they need to do frequent data entry. - If two users interact with the same record, how do we decide what is correct? - What are common fields they will be using to decide or find a record to update? - What other business logic is occurring? The time gives us a baseline. Maybe it's ok if it takes 1 hour to update an order. (It probably isn't, but you get the idea. Establishing a service level agreement gives you something to be held accountable for and you can measure your success. Likewise it sets an expectation and precedence for policy going forward for the customer.) Either way, we now know the baseline. How the application and users interact with the database will help us determine our isolation level. This can impact performance. Knowing the fields they will search to update will allow us to create better indexes. Keeping them minimal is ideal since we are based on primarily inserts. Due to the updates, we probably will need a clustered index to prevent RID lookups. Business logic can bog things down and extra processing takes extra time. The questions will differ and the circumstances will vary, but this gives us enough to work with. So let's just say our query is taking 10 seconds and the baseline is 3 seconds. 10 Seconds Performance - 3 Seconds Baseline = (7) seconds that need to be shaved off the current performance. We know we need to tune that process. Cool, we know what we need to tune but how do we tune it? With a similar methodology, let's measure what we are waiting on for those 10 seconds. Wherever we see wait time, we can apply tuning techniques to it. We're aiming for the biggest bang for the buck here. Here, I'd say use the right tool for the job: 1. DMV's - Wait times - Index use 2. Extended Events 3. Query Plans 4. IO statistics and time 5. Query Store I know this is me talking about performance tuning, not load testing. (I would argue performance testing, performance tuning, and load testing are all three different things.) You use a lot of the same ideology though amongst all three, you have your baseline and current performance, from there you dictate the outcome with an actionable item. Tools I use for load testing (first three are free): - HammerDB - Custom T-SQL scripts - BCP - Redgate tools - Apex tools If I'm testing and setting the load time thresholds for my own processes, I'll use my custom T-SQL that loads and unloads tables. (Typical ETL development.) I'll log the results, run times, etc and the other tools help me capture the time to compare. If I want to see what the raw unprecedented power of the DB can do so I can benchmark the DB specifically, HammerDB and other tools like Redgate and Apex are what I use. I'll use other software to stress test the disk as well and someone should be looking at the other aspects. (Host performance, network, etc.) So what does this mean for learning? I think just learning how SQL Server works and focusing in the core technology is the best bet. If you understand how clustered indexes or non-clustered indexes impact a query or affect the structure of the table, you can plan accordingly. [Here's a cheat sheet to get you started.](http://web.swcdn.net/creative/infographics/1403_Confio_SQL_Server_Tuning_Infographics_8_5x11.pdf) If you are pretty experienced, I'd recommend to start by checking out [Itzik SQL Fundamentals](https://www.amazon.com/T-SQL-Querying-Developer-Reference-Ben-Gan/dp/0735685045/ref=asap_bc?ie=UTF8) or [Pro Internals](https://www.amazon.com/Pro-Server-Internals-Dmitri-Korotkevitch/dp/1484219635/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1531246040&amp;sr=1-1&amp;keywords=pro+internals). If you are kind of new, I'd say start off with [T-SQL Fundamentals](https://www.amazon.com/T-SQL-Fundamentals-3rd-Itzik-Ben-Gan/dp/150930200X/ref=asap_bc?ie=UTF8) first. If you are not going for T-SQL specific, pick a database platform and stick with it for a while. Google around for some experts on that database, I'm sure they will have a suggestion list of books to read. There are some generic books like Database Design that are agnostic and most of the information applies across the board. Designing your database appropriately will definitely impact performance. You can do this without books, you can google for performance testing, performance tuning, or load testing and I know you'll get thousands of hits. Most of them will have great information and there are forum or slack channels where people are happy to help walk you through problems or concepts. Kendra Little, Brent Ozar, Paul Randal, Kalen Delaney, and Pinal Dave are fantastic people to google right now. You will take months to read through all of their free content on performance tuning. 
&gt;Hello All, &gt; &gt;First let me extend my appreciation for any one able to assist me in looking at this problem. &gt; &gt;I have a 2 node SQL (SQL 2016) cluster (Windows Server 2016) with a freshly installed named SQL instance. From my application server (Windows Server 2012 R2), with SSMS I can connect directly to my new SQL instance using the connection string in the following format (VirtualServerName.FQDN\\InstanceName). Unfortunately if I create a SQL Alias (TCP) using that same connection string, I'm unable connect to that instance using SSMS from that same application server. I have confirmed TCP is enabled network configuration on this instance and that firewall is configured to allow connections on all ports to and from the SQL instance executable (sqlservr.exe). &gt; &gt;This isn't my first time creating a configuration like this. I have 5 other named instances installed on this 2 node SQL cluster and all 5 of those instances I can connect using an alias from the same application server. The only difference I can find between the instances is that 1-5 were created using shared drives and the 6th was created using CSVs since I ran out of drive letters. There may be another configuration difference but I can't recall making other changes to get an alias to function correctly. Its just so strange that SQL connection string works in SSMS but then doesn't when configured with an alias using that same connection string. &gt; &gt;Has anyone else experienced similar issues? &gt; &gt;Best, &gt; &gt;Curtis &gt;Hello All, &gt; &gt;First let me extend my appreciation for any one able to assist me in looking at this problem. &gt; &gt;I have a 2 node SQL (SQL 2016) cluster (Windows Server 2016) with a freshly installed named SQL instance. From my application server (Windows Server 2012 R2), with SSMS I can connect directly to my new SQL instance using the connection string in the following format (VirtualServerName.FQDN\\InstanceName). Unfortunately if I create a SQL Alias (TCP) using that same connection string, I'm unable connect to that instance using SSMS from that same application server. I have confirmed TCP is enabled network configuration on this instance and that firewall is configured to allow connections on all ports to and from the SQL instance executable (sqlservr.exe). &gt; &gt;This isn't my first time creating a configuration like this. I have 5 other named instances installed on this 2 node SQL cluster and all 5 of those instances I can connect using an alias from the same application server. The only difference I can find between the instances is that 1-5 were created using shared drives and the 6th was created using CSVs since I ran out of drive letters. There may be another configuration difference but I can't recall making other changes to get an alias to function correctly. Its just so strange that SQL connection string works in SSMS but then doesn't when configured with an alias using that same connection string. &gt; &gt;Has anyone else experienced similar issues? &gt; &gt;Best, &gt; &gt;Curtis &gt;Hello All, &gt; &gt;First let me extend my appreciation for any one able to assist me in looking at this problem. &gt; &gt;I have a 2 node SQL (SQL 2016) cluster (Windows Server 2016) with a freshly installed named SQL instance. From my application server (Windows Server 2012 R2), with SSMS I can connect directly to my new SQL instance using the connection string in the following format (VirtualServerName.FQDN\\InstanceName). Unfortunately if I create a SQL Alias (TCP) using that same connection string, I'm unable connect to that instance using SSMS from that same application server. I have confirmed TCP is enabled network configuration on this instance and that firewall is configured to allow connections on all ports to and from the SQL instance executable (sqlservr.exe). &gt; &gt;This isn't my first time creating a configuration like this. I have 5 other named instances installed on this 2 node SQL cluster and all 5 of those instances I can connect using an alias from the same application server. The only difference I can find between the instances is that 1-5 were created using shared drives and the 6th was created using CSVs since I ran out of drive letters. There may be another configuration difference but I can't recall making other changes to get an alias to function correctly. Its just so strange that SQL connection string works in SSMS but then doesn't when configured with an alias using that same connection string. &gt; &gt;Has anyone else experienced similar issues? &gt; &gt;Best, &gt; &gt;Curtis &gt;Hello All, &gt; &gt;First let me extend my appreciation for any one able to assist me in looking at this problem. &gt; &gt;I have a 2 node SQL (SQL 2016) cluster (Windows Server 2016) with a freshly installed named SQL instance. From my application server (Windows Server 2012 R2), with SSMS I can connect directly to my new SQL instance using the connection string in the following format (VirtualServerName.FQDN\\InstanceName). Unfortunately if I create a SQL Alias (TCP) using that same connection string, I'm unable connect to that instance using SSMS from that same application server. I have confirmed TCP is enabled network configuration on this instance and that firewall is configured to allow connections on all ports to and from the SQL instance executable (sqlservr.exe). &gt; &gt;This isn't my first time creating a configuration like this. I have 5 other named instances installed on this 2 node SQL cluster and all 5 of those instances I can connect using an alias from the same application server. The only difference I can find between the instances is that 1-5 were created using shared drives and the 6th was created using CSVs since I ran out of drive letters. There may be another configuration difference but I can't recall making other changes to get an alias to function correctly. Its just so strange that SQL connection string works in SSMS but then doesn't when configured with an alias using that same connection string. &gt; &gt;Has anyone else experienced similar issues? &gt; &gt;Best, &gt; &gt;Curtis You may want to check this. I have tried to link the videos, but I am not sure if you have Microsoft services. The videos are about 1/2 down dealing with the listener. https://ms-pes.learnondemand.net/CourseAssignment/285780 https://docs.microsoft.com/en-us/sql/database-engine/availability-groups/windows/create-or-configure-an-availability-group-listener-sql-server?view=sql-server-2017
Also you may want to check the SPN's for the server https://technet.microsoft.com/en-us/library/bb735885.aspx
Great answer. I'm familiar with the majority of those resources and the concepts you've outlined will go a long way helping me to track down the info (I also realize and appreciate that you're over-answering for the folks passing through). I do a lot of tuning but, for one reason or another, haven't ever needed to document the impacts in any formal detail. I came up in a pretty lax shop, which was great for someone like myself who wants to learn and do more, but the flipside is I'm less confident in my testing and documentation. Started a new gig recently and expect to build those up here shortly. Thanks again.
Documentation doesn't get the attention it deserves, kudos to you for trying to increase your skill set in it! I can't count the times I wish I had a document on something or how many documents I've personally created. I'm glad you liked my long rambling! I try to think out and structure my writing when I post on my site, but when I post on forums my thoughts tend to be more sporadic and random. At the very least, I try to make it readable and informative. 
You’re right - SQL is definitely a different beast than something like JavaScript or C++. Why are you trying to do this? From what I have seen a function in SQL is typically used to encapsulate a set of business rules and not necessarily act as something that builds SQL statements for you. I’d advise that you not do this if you can avoid it. There are a lot of gotchas with dynamic SQL not the least of which it being very hard to troubleshoot.
What version of SQL are you using? Might not be the most efficient, but you could use a CTE to pull only fruits from the price table having 1 matching record after grouping them, then use that in there where clause of the join to limit the results... ; with FruitsWithOnePrice (Name) as ( SELECT Name FROM Price GROUP BY Name HAVING COUNT(Name) = 1 ) SELECT f.*, p.Price FROM Fruit f INNER JOIN Price p ON f.Name = p.Name WHERE p.Name IN (SELECT * FROM FruitsWithOnePrice)
Maybe something like this (I’m on my phone so the formatting may be funky) Select * From a join (Select b.fruit From b Group by fruit Having doubt(fruit) = 1) b2 On a.fruit = b2.fruit
Tables A, B, and C are structured similarly, but have different columns. I am building an ETL process to validate the loads to those tables, and the "Select case" part of what I wrote is applicable to all 3 tables (provided different columns). So I want to avoid creating 3 separate validations processes for each table (or create 3 separate variables).
Know enough to talk about it. Know your join's. Really it's not the SQL you have to know, that is the easy part. Learning your database, knowing your data. Etc
More than a couple of years old means it's not up to date on any new techniques and syntax that may have been implemented so while the fundamentals may be a useful reference you're pretty much better off taking any reference they give with a grain of salt and pulling better info directly from the dev of whichever flavor of back end you're working with. Not saying the book is a complete wash but if you take OP's example for instance, the code it's showing him is not only outdated it's a nightmare for legibility/code debugging.
Could you elaborate more on "learning your database and data"?
Not entirely sure about Oracle syntax, but you could just nest switches according to what your particular comment here is. At least, from my MSSQL days: Case When table1 AND blah Then Case When Then End When table2 AND blah ... End 
I will agree with the difficulty of troubleshooting, but as far as dynamic SQL is concerned there are still many possibilities that would prove just as efficient as long as OP remembers that most things SQL are run best by using precompiled sources, or are specifically tailored to endpoints that are made for said dynamic SQL. Indexing, using stored procedures, etc. can make the dynamic SQL thing not so bad. However, if OP is thinking about using SQL to look at stuff outside of the database itself... That's a nightmare.
That was one of my first attempts, but I think with the way Oracle handles SQL, it validates if all columns exist on a table, so if I reference a column that doesn't exist on table A (**even** though the code will never reach that case statement) it still throws an "Invalid Identifier" error.
&gt; "learning your database and data"? Just some examples: data types, how they're structured, what they're used for database engine, functions, stored procedures, indexing, execution plans, how the database is stored
[I basically agree with this post.](https://softwareengineering.stackexchange.com/a/181657/256008) At the end of the day though, I strongly believe networking is your best way to land a job. If you can perform all the actions with relative ease in the beginner section, I'd say that's what you need for entry level. 
Basketball Reference is where I got all of my information. It’s incredibly current and even has ABA vs NBA data if you want to go that far back!
Oh I absolutely agree about the joins. I always put joins in their own section separate from the where. It's just much easier to read that way. 
Never had a portfolio. Maybe I should? I have a github with a couple of random, unfinished projects but when I show it to potential employers I'm like "Please don't judge". 
fyi most people will just call them subqueries
I seen ones where people make their own website and put there projects and links there. Msg me if u want to see examples
Agree 100%. SQL is definitely awesome. Just watch a video about how SQL engines work and you can really start to appreciate how much work it saves you. People take SQL for granted, but it is truly a godsend. I would absolutely not want to live in a world where I had to write procedural code to do the things that SQL does effortlessly.
Hi, Yes! SQL is Awesome! You have a perfectly sound solution, however I have the following recommendations for refactoring your query. 1. Since `FirstTable` has `groupID`, there is no need to join to `SecondTable` in your WHERE clause sub query. Eliminating unnecessary joins helps queries run faster. I believe the following shorter query should work. USE OurDatabase GO DECLARE @usergroup INT = x; SELECT ugm2.groupID , ug2.[name] , COUNT(ugm2.groupID) AS counts FROM FirstTable AS ugm2 INNER JOIN SecondTable AS ug2 ON ugm2.groupID = ug2.groupID WHERE ugm2.groupID != @usergroup AND ugm2.userID IN ( SELECT DISTINCT ugm.userID FROM FirstTable AS ugm WHERE ugm.groupID = @usergroup ) AND ugm2.groupID IN (list OF GroupIDs to look in) GROUP BY ugm2.groupID , ug2.[name] ORDER BY counts DESC 2. It looks to me like you are using SQL Server, which has support of Temp Tables. These can be used to hold records in memory for the duration of your session, or till dropped. Using temp tables helps avoid using nested sub queries, which I personally find harder to debug. Here is the above version refactored to use a temp table. USE OurDatabase GO DECLARE @usergroup INT = x; SELECT DISTINCT ugm.userID FROM FirstTable AS ugm INTO #users WHERE ugm.groupID = @usergroup SELECT ugm2.groupID , ug2.[name] , COUNT(ugm2.groupID) AS counts FROM FirstTable AS ugm2 INNER JOIN SecondTable AS ug2 ON ugm2.groupID = ug2.groupID INNER JOIN #users AS u ON ugm2.userID = u.userID WHERE ugm2.groupID != @usergroup AND ugm2.groupID IN (list OF GroupIDs to look in) GROUP BY ugm2.groupID , ug2.[name] ORDER BY counts DESC DROP TABLE #users 3. My personal favorite solution for a problem like this is to use analytic functions. I have found them incredibly powerful in my few years using SQL for data analysis. USE OurDatabase GO DECLARE @usergroup INT = x; SELECT g.groupID , g.[name] , sum(1) counts FROM ( SELECT DISTINCT ugm.groupID , ugm.userID , ug.[name] , max(case when ugm.groupID = @usergroup THEN 1 ELSE 0 END) OVER (PARTITION BY ugm.userID) has_usergroup FROM FirstTable AS ugm INNER JOIN SecondTable AS ug WHERE ugm.groupID IN (list OF GroupIDs to look in, including @usergroup) ) g WHERE has_usergroup = 1 GROUP BY g.groupID , g.[name] ORDER BY counts desc
Not OP but this helped me out, too. Thank you! 
Should you consolidate the tables into one? Yes. Your life will get easier if you can do that. As long as it makes sense to group the data together. I have written a bunch of dynamic SQL before with Oracle, so maybe I can help give some pointers. But first I see some errors in your declare section. I'm guessing AN_IS and A DATE are just placeholders, because for now the block won't even compile with those there. More tips: - use alternative quoting mechanism so you don't ever have to do double single quotes or quadruple single quotes ever again - always put you assembled text into a variable and then just do execute immediate on the variable. The reason? You can comment out the execute and print the variable instead as you troibleshoot. It is too easy to build the string wrong. 
how about wrapping the outer switch in if/else logic? now that i think about it, if you're moving an entire table at a time, this would probably be more performant.
You said if my man. I definitely agree that there are uses for dynamic sql but I feel like it’s a bier beware type situation. Especially if something as basic as not indicating an objects owner can atop plan reuse 
This is amazing
Relevant: [No, You Don't Need ML/AI. You need SQL.](https://cyberomin.github.io/startup/2018/07/01/sql-ml-ai.html)
So yes I am not currently using the second table but I plan too in the future I am making some modificatione to pull more data later on. As far as temp tables I generally avoid using them for the same reason as you recommend using them I have a hard time trouble shooting them. I know nothing about analytic functions tho I'll look into them tomorrow when I get back to the office.
When I need to do this I used RANK OVER PARTITION BY with a sub query. It works really well and scales up as you need it.
Where do you download the databases?
Thought so. 
You seem to be always selecting from StageTable, whereas I'd expect it to be table A or B or C. And if there are only 3 tables to select from then it would be easier to write it out as a static UNION ALL expression. Or even abandon the one-by-one approach and just write a view that gives you the validity of every file in the 3 tables.
Thanks so much you guys! I will work on that 
...which is technically wrong, but people do it anyway. A subquery returns a single value, what OP has is actually an inline view.
In the last code block what does the then 1 else 0 do?
&gt; Just watch a video about how SQL engines work and you can really start to appreciate how much work it saves you. How can you leave us hanging like that? Um, what was the video? 
Lol. Sorry. Here is one such video but Richard Hipp, the creator of SQLite -- (click here)[https://m.youtube.com/watch?v=Z_cX3bzkExE]. And keep in mind that SQLite is one of the most simple SQL databases out there. It would likely take quite a whole video series to describe any of the common client-server DBs -- PostgreSQL, SQL Server, Oracle, etc.. Point being is that they do a ton than people take for granted. Good stuff.
Depends on what job it is. Are you looking for a DBA role? BI report writer?
As a data warehouse developer I couldnt really disagree more. Knowledge of multiple different approaches to any one issue is far more important to knowing the data. Especially if its an enterprise warehouse, knowing all the databases and data would fill your brain pretty fast. To land a job you need to prove you've got good problem solving skills and are a fast learner. Knowing the data isnt going to be a possibility before you get the job. 
Yes they are placeholders, they will be real legitimate values once I finalize this piece to actually be working appropriately. Thanks for the heads up about the Q operator, did not know of it. I was able to figure I needed the double/quadruple quotes via your debugging method listed in the 2nd point.
sort of the same idea: what job in this career field has the lowest technical knowledge entry point? i'm trying to career pivot into this field and really just want to get my foot in the door to start the real learning alongside people i can learn from. should i mostly be looking onto data analyst positions?
Dynamic SQL is almost always a bad idea unless you have very good reasons to do it. Can you redesign so you don't have to do that? In this case, your select into looks wrong to me. According to: https://docs.oracle.com/cd/E11882_01/appdev.112/e25519/selectinto_statement.htm#LNPLS01345, it should be: `SELECT INTO v_var FROM table WHERE....` You've done: `SELECT blah from table where blah INTO v_variable` I'm not sure it's the cause of your problem, but it doesn't look right.
Here are a couple links to get you started on analytic functions and aggregate functions with the OVER clause. [https://docs.microsoft.com/en-us/sql/t-sql/functions/analytic-functions-transact-sql?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/t-sql/functions/analytic-functions-transact-sql?view=sql-server-2017) [https://docs.microsoft.com/en-us/sql/t-sql/queries/select-over-clause-transact-sql?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/t-sql/queries/select-over-clause-transact-sql?view=sql-server-2017)
Unfortunately, I was unable to access the first link even though I’m a Microsoft Partner. I did resolve the problem though. Again, thank you for taking the time to reply to my issue. *Summary of problem:* Unable to connect to TCP alias of SQL instance from application server. But can connect directly to SQL instance from application server using SSMS using the connection string **server.fqdn\\instance.name.** In addition, I learned that I could not connect to the SQL instance using SSMS using the connection string **xxx.xxx.xxx.xxx\\instance.name.** SQL instance is hosted on a two node SQL cluster. *Error:* Cannot connect to **alias.name** A network related or instance-specific error occurred while establishing a connection to SQL server. The server was not found or was not accessible. Verify that the instance name is correct and that SQL Server in configured to allow remote connections. (provider: TCP provider, error: 0 – The wait operation timed out.) (Microsoft SQL Server, Error: 258) The wait operation timed out *Troubleshooting:* What I learned is that when using the **server.fqdn\\instance.name** it was, in fact, connecting to the server using named pipes. When I used the connection string **xxx.xxx.xxx.xxx\\instance.name** I was, in effect, forcing it use TCP and it was failing, just like my TCP alias. I confirmed this by using the connection string **tcp:server.fqdn\\instance.name** in SSMS and observing that the connection also failed. So, I figured my issue was the windows firewall because the problem was narrowed down to being TCP related. But I already had a rule in my firewall for this connection. Like all my other working firewall rules it pointed sqlservr.exe executable of the instance so it would automatically open the port of the named instance when a port was assigned at time of service start (see more information on dynamic port assignment when using named instances [here](https://docs.microsoft.com/en-us/sql/sql-server/install/configure-the-windows-firewall-to-allow-sql-server-access?view=sql-server-2017).) *Solution:* Well turns out, when I created the new SQL instance instead of using shard drives (e:, f:, g:, h:, i:…), like I had on previously working clustered SQL instance and alias, I had switched to CSV (cluster shared volumes) because I had run out of drive letters on this cluster. Instead of getting drive letters, CSVs get a folder under the c: drive (c:\\cluserstorage\\volume1, c:\\cluserstorage\\volume2…). I discovered that the firewall rule I created pointing to c:\\clusterstorage\\volume01\\mssql13.xxxxx\\mssql\\bin\\sqlservr.exe was not working. Although I couldn’t find documentation I suspect that the windows firewall doesn’t support executable under c:\\clusterstorage. So instead I [manually assigned the SQL named instance a static port](https://docs.microsoft.com/en-us/sql/database-engine/configure-windows/configure-a-server-to-listen-on-a-specific-tcp-port?view=sql-server-2017) and opened the firewall to that specific port on each node in the cluster. Now the TCP alias work as expected from the application server. Please let me know if you have any questions or feedback. Cheers
The CASE statement returns a 1 if the `groupID` matches `@usergroup`, else it returns 0. The maximum of the 1s and 0s for each `userID` returns a 1 if the `userID` has at least one `groupID` that matches the `@usergroup` otherwise it returns a 0. 
^ CTE is the solution to this. Or at least, it's what I would do.
Your question seems like more of a system design question rather than sql question. Are you asking for help with SQL queries to give you the info you need?
Here's another way to do it. SELECT * FROM TableA WHERE ( SELECT COUNT(*) FROM TableB WHERE TableA.Name = TableB.Name ) = 1 And then you can join to TableB.
What part of the Syntax is confusing
Are you getting this question from Treehouse? 
 Select title, year --The results will be a column named title, and a column called year. from movies --The table in which these columns reside is named movies. where year % 2 = 0 --% is modulus I presume, so year mod 2 gives the remainder of a value (year) when it is divided by 2. That's going to be zero for even years and 1 for odd years. Is the modulus operator where your confusion lies? In cases where year mod 2 is zero, (even years), it will return the title and the year. 
year % 2 = 0 this is telling sql to check the remainder of year divided by two. if it is anything other than 0, it's odd. if it is 0, then it must be even.
Sqlbolt. 
% 2=0
Yes that's where the confusion is at. I totally didn't get what % 2 =0 represents.
Thanks for sharing - will have to give it a shot!
That's a modulus function. It returns the remainder after division. So what that is saying is "where year divided by two has remainder zero" meaning it's an even number.
The % is a symbol (bitwise? i'm unsure) that looks at the remainder only when you divide by the number following. &gt;SELECT title, year FROM movies WHERE year % 2 = 0 In English it translates as, "Give me the movie titles and year released for all even years." 2000 / 2 = 1000, remainder 0 gives us % 2 = 0 2001 / 2 = 1000, remainder 1 gives us %2 &lt;&gt; 0
I guess I can see why your priorities are different. I'm more of a analyst, I do report building, KPI's, etc.
% means the remainder after division (it's called the modulus operator) so an odd year divided by 2 will result in a remainder of 1 causing the where clause to fail.
Ahh, good ole FIZZBUZZ.
FIZZBUZZ :) 
SSIS, Make a nice ETL to move the data you need.
The standard Microsoft tool for ETL is Integration Services (SSIS). If your upstream databases are not well built (no primary keys == not well built, at a minimum they should have surrogate primary keys based on identities), I'm unaware of any tool that is going to easily fix the problem for you. You might be able to get Change Data Capture (CDC) to work for you but, IIRC, CDC doesn't need a primary key but it needs some kind of unique index or it can't track which rows changed. Another feature that might help you is Availability Groups, which could get a copy of the database onto the correct computer in a more timely way. Another possibility is to look at differential backups of your source databases. Restoring a a full backup + a differential backup is often faster than restoring a full backup + dozens/hundreds of transaction log backups. I have never seen your environment, but my shoot-from-the-hip guess is that moving from "restore full backups every night" to "something better" could be a **lot** of work. It might not be worth while. If you can't get CDC or AGs to work for you or if you can't fix/change the design of the source tables so you can efficiently find inserted/updated/changed rows, you may have to continue fix the problems by throwing hardware at them. Good luck. 
How long have you been using SQL or coding? Just curious because that symbol is the same in every language.
More of a system design question.
You said that you created some sort of form to input data. But what about doing the same thing for a report? How did you go about creating the form? Did you use something like Ms access?
Wish I could change the source databases. The main problem one was designed by a multi billion dollar company and is fed into by their front end (I cant make any changes without breaking it), but its terribly designed with very nondescript column names, inconsistent naming, tons of unused columns, etc... Makes me cringe and It took a ton of time to learn my way around it. On the up side, job security I guess lol. 
Look in the bright side: my current clients would not spend for a 4TB PCIe drive. 
Mixed bag lol. I can buy whatever I need within reason and they'll happily send me to any training I want to go to (I've done 3 this year)... but my salary is bleh.
&gt;Hi, &gt; &gt;I have a query that inserts into a temp table from various tables with basic joins/left joins. This temp table ends up with about 35 columns and about 60k records. When I do a select \[column 1,..,column 31\] from this temp table it takes about 4-5 minutes to run which is way longer then it should right? also, I am specifying the column names and not doing a select \* For 60k rows it's definitely slow. Also depends on the joins. Are you joining on a range? If you check your execution plan do you have some type of Cartesian product that's being filtered? &gt;I've tried tried to create the table and do a insert into select from and creating nonclustered/clustered indexes and both seem to help just a little (shaves off about 30 seconds of run time but i did run it during off hours where server use is low). I've also tried to use a table variable instead of the temp table but performance is the same. Some questions - what else can I try to speed it up? Also if i'm creating an index on the temp table and i am not using those columns in my where statement in subsequent query from the temp table - does indexing actually help? Please let me know if I was not clear or if I need to provide any additional questions! Thanks in advance! The SQL Hokey Pokey is a very long song. *You put your data in. You take some data out. You put some data in, then you shake it all about. You do the SQL Hokey Pokey and you run your sequel engine into the ground, that's what it's all about!* It's a catchy tune and everybody knows it but it makes dbas cringe.
So the actual query runs in a decent amount of time, it's just selecting from the temp table that's taking a lot of time? I'm assuming you're running SQL Server? If so you might have a I/O bottleneck, possibly from a tempdb that is too small or has too small of an autogrowth factor. If the temp table contains text/varchar(max) columns I would suspect this to be a culprit. Would need to see some code. A screenshot of the (actual) execution plan would also be helpful.
Thanks for the reply! The insert into temp table actually is running fine, it's the select from temp table that is taking a long time
Yep! Just the selecting from the temp table and it's SQL Server. I'll try to get an execution plan but we are locked out of it
Have you tried just working with query result set directly and running the query repeatedly? If you're not manipulating data in the temp table this might be a better way to go. SQL Server will cache the query plan and the data will be fetched from the page pool in RAM unless the database has a lot of activity from other queries.
VBA ... Here come downvotes 
&gt; Please let me know if I was not clear or if I need to provide any additional questions! Post your SQL and your table definitions.
Indexing columns you aren't using in your where clause is useless. Index based on your where clause. Use an order by for that field when you do your insert (order by your first index and make it clustered.) Make absolutely sure your joins and where comparisons have exactly the same data type on each side. If you need to do a conversion do it on your inserts ahead of time or do it on the right side. Do not convert indexed columns in your joins/wheres. If more than one index (in where clause), order the biggest filter first in your where clause (i.e., use the filter that will pull back the largest set first and make sure the second index/filter merely a subset of that first one. Order them that way.) Take your query apart and run it one join at a time and one where clause at time. (In case you find query plans hard to read.) That might pinpoint your problem. Finally, inner joins are more efficient. If you can rearrange your process to use only inner joins it should make it faster, since some left joins will be seen as nested joins by the query optimizer.
I am manipulating it after i do the insert with some update statements, will see if i can somehow work it into the initial insert query
Thanks for the tips! I figured indexing on columns not used in where is useless. I'm not actually having any performance issues on the query where I'm inserting into the temp table, its the select from the temp table (not using a where clause) that is taking a long time to run. I will take your advice on trying to convert the left joins to inner joins though - any efficiency counts!
I will post one first thing in the AM! Thanks!
Awesome catch and writeup. We never had this because we keep the ports set in the firewall manually as a setup step. 
If i understand you right, you are assembling a result set by running multiple statements into a temp table, and once done you are running a simple select to pull back all the result rows to your client machine? If that's correct, and you have latency only in the final step, then your bottleneck is likely on the network connection getting all that data back to your client machine or rendering it in your client Dev tool. 30+ columns coming back and 60k rows is just shy of 2 million values, if you've got a couple long text elements in there if a couple hundred bytes mixed with dates, integers, etc you're suddenly spooling back tens or hundreds of megs of data, throw in protocol overhead and a gig isn't unreasonable. With a mediocre net connection, or a client machine without a good amount of RAM, that can take a lot longer than you'd expect. 
It's definitely a good idea to filter out as many records as possible before inserting into a temp table, big temp tables in general can be problematic because of the way they are implemented in SQL Server. Instead of messing with the tempdb to improve performance with the temp table you might just try creating a regular table for staging your data and delete everything out at the end of your stored proc.
Unfortunately, this is going to be a procedure called from a dashboard and dropping/recreating a table every time it is called is not ideal. Appreciate the great insight nonetheless!
Correct. Some good info here, appreciate it. Will definitely try your test case first thing tomorrow.
I meant create the table outside of the proc and just fill/delete data as needed. I'm just trying to rule things out. The fact that your somewhat complex query (as you describe) runs fast but a simple query from a temp table runs stupid slow is what makes me think it's something specific to temp tables, usually a tempdb config. I just recently fixed a similar problem at work with code that was creating temp tables and found out the tempdb for the server was too small and installed on an HDD volume instead of the RAID SSD volume the DB files lived on.
Yea, I think it might be something with the temp table. Question though - I replaced the temp table with a table variable instead and performance was the same. Do table variables occupy the same space on disk as temp tables?
Yes, tables variables are also created in the tempdb, more info [here](https://blog.sqlauthority.com/2009/12/15/sql-server-difference-temptable-and-table-variable-temptable-in-memory-a-myth/).
Ah ok thanks for the link!
I think you’re missing a semi colon right before your commit.
Think of % as a division sign where you throw away the whole number and keep just the remainder.
A join is overkill, a NOT EXISTS would be enough. set status_x = ( SELECT count(*) from patient WHERE NOT EXISTS (SELECT 1 FROM temp_1 WHERE temp_1.id = patient.id AND temp_1.id2 = patient.id2) AND NOT EXISTS (SELECT 1 FROM temp_2 WHERE temp_2.id = patient.id AND temp_2.id2 = patient.id2) ; If temp_1 and temp_2 are materialised tables you can try adding an index to their IDs too. If that's still not fixed it enough post your full process, including how you've created temp_1 and temp_2.
Generally, XSLT or XQuery are more suitable than SQL for this, and some databases let you use them to update XML values stored in db tables. Could you be a bit more specific?
Oracle, Sql, Aws Database Service provider [http://dbshift.com](http://dbshift.com) dbShift provides best data migration services in USA, Singapore and Europe and help you to migrate your database safe, secure and without any loss of data. 
not exactly a mySQL answer but if you can connect your dB to a BI tool like Tableau or AWS quicksight, you could build the reports you need for those people. 
If you have nothing else at hand, you could try getting hot and heavy with VBA and put it in Excel. You could also try hooking up your DB to Microsoft PowerBI. But yeah I'm not aware of any MySQL client that does this on its own.
Oracle tell you exactly what is wrong. Oralce got a commit command when actually it was expecting a ; your execute immediate statement (this includes the using) needs to end in a ; 
a) records stored in a database are in a set, so they're unordered and they will be returned in whatever order they're read from the DB. If the table doesn't get updated/reorganized, you'll *generally* get the records returned in the same order each time the same query is executed. b) Yeah.... you'd want to do something like a "PARTITION OVER topic_id", which would add a row number counter to each set of records with a common topic_id. Then you could specify to only return records with the row_number between 1 and 20. That would give you 20 results for each topic_id. c) If you want 20 records total, but 3 topic_ids, you'd want to select 7 records from each topic (as described in b), then limit the total result set to 20. So your subquery with the PARTITION OVER would return 21 records, but you'd only display the first 20. 
You are missing a semicolon after the execute immediate sratement. Why are you using execute immediate? Just write the insert statement. Insert into my_table(my_column) values (varMyvar); Absolutely no need for dynamic SQL. Variable references are treated as bind variables. 
Maybe I am misunderstanding but why not just auto increment the rows when you put them in the db to get the rows. He is the one entering the topics, if you have 30 do 1-10 topic 1, 11-20 topic 2 etc. For MYSQL its is going to pull it based on your PK column if you do not order by so if you have 40 results auto incremented and you limit by 10 you will get records 1-10 if you did ORDER BY DESC you would get 40-30 and so forth. If you ORDER BY any column and LIMIT then it will do the same whether in number order alphabetical, date or whatever. for instance table people id name 1 bill 2 dave 3 alec Select \* FROM people ORDER BY name DESC LIMIT 1 will return id name 2 dave MYSQL does not have partition over If you know the questions you want to present just do this. Obviously you can edit this better, its just an example you could add a rand with a count and sub query to pull a random 6 from topic id 1-10 a random 6 from 11-20 and random 7 from 21-30. Although I prefer not to do this in sql, sounds like the end user is selecting the values , so whatever you are using php etc., its a bit more seamless to just grab the random values through php. SELECT \* FROM question\_list WHERE topic\_id = 'topicId1' AND AUTO\_INCREMENT column BETWEEN 1 AND 6 UNION SELECT \* FROM question\_list WHERE topic\_id = 'topicId3' AND AUTO\_INCREMENT column BETWEEN 11 AND 16 UNION SELECT \* FROM question\_list WHERE topic\_id = 'topicId3' AND AUTO\_INCREMENT column BETWEEN 21 AND 27
If OPs end users requirements aren't that demanding I'd also add in the free option of google data studio as well. Connects to MySQL easily and google are constantly adding features like crazy. Tableau or Quicksight (Mainly Tableau) have a pretty hefty price tag on them at any kind of enterprise level, Looker and Sisense are two really good BI interfaces as well. 
Thanks so much for your solution, that sounds like it should work. The only issue I have with autoincrement is that there's no way of automatically updating the AI value if I delete questions, is there? 
Cheers, I'll look into this! 
As a Team lead of a Data engineering team I'd normally check for knowledge of these minimums with SQL. --Analyst Aggregation functions and how to use them Joins Where conditions Awareness of Window Functions --Data Engineer Primary Keys and Indexes, when and how to use them Data modelling (e.g what is a star schema) Query Optimisation Plus I'll also be pretty pleased if someone shows some awareness of formatting and consideration that others will have to work with their queries in the future. 
UNIONs would work, but if the topic list increased, it would take more work to modify the query: (SELECT * FROM question_list WHERE topic_id = 'topicId1' LIMIT 7) UNION (SELECT * FROM question_list WHERE topic_id = 'topicId3' LIMIT 7) UNION (SELECT * FROM question_list WHERE topic_id = 'topicId3' LIMIT 6);
His main point is that someone competent at their job will know how to deconstruct and work with data and different approaches even if you took them from working with financial data and plonked them in front of a load of retail data. Knowing the data simply comes from time served with your company but that experience with one set of data isn't as transferable as analytical skills. 
That's not too much of a problem because I'm generating the SQL query using foreach loop so it's flexible in terms of how many topics there are
&gt; If you want to work with Big Data, you better be good at R or SAS (and Excel). I'm not sure what you mean by big data here (&gt;1GB vs &gt;1TB) But working in the field most companies working with Big Data (e.g King Games who make candy crush) their engineering is done in Java/Scala/Python and their Analytics are done in Python. I don't think I know anyone using SAS outside of really stodgy old corporates and R is getting rarer these days. I'd be very sceptical if you could even open an excel file containing anything close to a big data dataset. Saying that you are 100% right, just knowing SQL on its own is only going to open up very very basic jobs versus knowing the infrastructure and/or a programming language to go with it. 
You're looking for a reporting platform. You create a report which has one or more queries behind it and parameters for the user to enter/select, then (usually) host it on a web server and turn the users loose. SQL Server Reporting Services and Crystal Reports are the big names in that on the Microsoft side, [BIRT](https://www.eclipse.org/birt/) is one option that's open source.
Only look at Tableau if you have shipping containers full of money that you'd rather spend than burn to heat your home in the winter months. For basic reporting, it's obscenely expensive and probably not the right tool anyway.
Your query fix will only return 7 values. Limit works the same way that order by with unions all or nothing.
I should have mentioned that I am hosting my website with LAMP. Are there similar options that you know of for LAMP?
Have you personally tried Google data studio? Does it generate effective reports and it is easy to use? Also, I’m hosting my website with LAMP. 
Can you not install the BIRT server components (reporting engine, presentation engine) alongside?
Jasper Reports is FOSS, though not as user friendly as Tableau
to include something that isn't there requires an **outer join** CREATE TABLE buckets ( lo_value INTEGER NOT NULL , hi_value INTEGER NOT NULL , bucket VARCHAR(9) ); INSERT INTO buckets VALUES ( 0 , 100 , ' 0 - 100' ) ,( 101 , 400 , '101 - 400' ) ,( 401 , 800 , '401 - 800' ) ,( 801 , 1000 , '801 - 1000' ) ,( 1001 , 999999 , '1001 and Above' ) ; SELECT B.bucket , COUNT(O.QuantityShares) AS number_of_orders FROM buckets B LEFT OUTER JOIN OrderTransactions O ON O.QuantityShares BETWEEN B.lo_value AND B.hi_value GROUP BY B.bucket ORDER BY B.bucket 
Do you know of any other way? I know for a fact that what I currently have is the correct code except for my line beginning with ELSE
Freelancing is almost a no-go, unless we're talking about consulting, which will require at least a few years of experience. You didn't mention what you did in your past business analyst role, but as a BI analyst, my jobs is 100% querying + report building through SSRS. It's not the end-all-be-all of working with SQL, but it's in demand and pays well, and fairly low-stress. For MS SQL, install SQL Server Express, SSMS and a sample db like Adventureworks. For learning the fundamentals, there are good online resources like w3schools, sqlbolt.com, and sqlzoo.net
&gt; I know for a fact that what I currently have is the correct code not true your CASE expression will ~never~ return anything in the 1001-or-over bucket because *there aren't any rows in that bucket*
Do the results have to be displayed vertically? (one row for each "bucket" value)? Could the results be a bucket in each column? SELECT SUM(CASE WHEN O.QuantityShares between 0 and 100 THEN 1 ELSE 0 END) as "0-100", SUM(CASE WHEN O.QuantityShares between 101 and 400 THEN 1 ELSE 0 END) as "101-400", SUM(CASE WHEN O.QuantityShares between 401 and 800 THEN 1 ELSE 0 END) as "401-800", SUM(CASE WHEN O.QuantityShares between 801 and 1000 THEN 1 ELSE 0 END) as "801-1000", SUM(CASE WHEN O.QuantityShares &gt;1000 THEN 1 ELSE 0 END) as "1001 and Above" FROM OrderTransactions O;
I am pretty sure I can but what I heard about BIRT is that it is more or less designed for web apps built with Java; though, there is a way to communicate between PHP and Java. I thought you may know something similar to BIRT but for apps that are built with PHP - i.e. I do not want to deal with Java.
It's not elegant, but it should produce the results you want. (SELECT '0-100' as "Bucket", SUM(CASE WHEN O.QuantityShares between 0 and 100 THEN 1 ELSE 0 END) FROM OrderTransactions O) UNION ALL (SELECT '101-400' as "Bucket", SUM(CASE WHEN O.QuantityShares between 101 and 400 THEN 1 ELSE 0 END) FROM OrderTransactions O) UNION ALL (SELECT '401-800' as "Bucket", SUM(CASE WHEN O.QuantityShares between 401 and 800 THEN 1 ELSE 0 END) FROM OrderTransactions O) UNION ALL (SELECT '801-1000' as "Bucket", SUM(CASE WHEN O.QuantityShares between 801 and 1000 THEN 1 ELSE 0 END) FROM OrderTransactions O) UNION ALL (SELECT '1000 and Above' as "Bucket", SUM(CASE WHEN O.QuantityShares &gt;1000 THEN 1 ELSE 0 END) FROM OrderTransactions O) ORDER BY "Bucket";
[This is what the table should end up looking like](https://imgur.com/a/ke7P7Yk) My code currently produces a table that does not include '1001 and Above'
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/qKoXNqp.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20e29pmm1) 
https://www.reddit.com/r/SQL/comments/8yblwk/oracle_need_help_with_including_results_with_a/e29pevy/
##r/PostgreSQL --------------------------------------------- ^(For mobile and non-RES users) ^| [^(More info)](https://np.reddit.com/r/botwatch/comments/6xrrvh/clickablelinkbot_info/) ^| ^(-1 to Remove) ^| [^(Ignore Sub)](https://np.reddit.com/r/ClickableLinkBot/comments/853qg2/ignore_list/)
Sure but it's not exactly commonly used. At least in my world.
For fairly some reports I would recommend Excel (someone mentioned VBA, but for simple reports there's no need). There plenty of drivers out there to connect Excel directly to MySQL using the standard data connection tools. If you need something more complicated there's power query and/or power pivot.
I created a 2nd temp table with the same structure as my 1st temp table, did an insert into 2nd from 1st and it was instant. Then I do a select * from 2nd temp table and it's faster then my original select from 1st. I've been running different benchmarks on both queries comparing the speeds and so far the 2nd option has beaten out the 1st option in all cases. The thing I don't get is that since I am inserting into the 2nd table from the 1st and selecting from the 2nd - wouldn't that also be same as selecting from the 1st? What's the difference here?
You could do it in SQL using CTEs (or a temp table probably wouldn't be too bad, since you're storing 2 rows) or by simply running two selects and `UNION`ing the results... the approach will vary based on the version of SQL you're using... for MSSQL/TSQL: ; with firstTwo as ( SELECT TOP 2 * FROM TABLE ORDER BY column ), nextRows as ( SELECT * FROM TABLE ORDER BY column OFFSET 2 ROWS FETCH NEXT 48 ROWS ONLY ) SELECT * FROM firstTwo UNION SELECT * FROM nextRows ORDER BY column
There's plenty of work for SQL out there. But as with everything else, finding the work is the challenge.
I have - it's pretty simple. I completely forgot about it but that is probably your best bet
very true. I forgot about data studio, I mainly use quicksight since it falls under the co budget for aws cloud so it slips under radar
Cool thank you for the response. I will give this a shot! I am using POSTGRES btw
Are the data types in the other tables exactly the same for every column? What platform are you on? In Oracle, for example, it's as easy as two insert statements for your tables in the format of: insert into new_table select * from old_table where your_conditions_that_grab_those_5_rows 
Not that I'm aware of... you'll need to use TPT (teradata parallel data transport) probably..
Zero: select count(*) from A inner join B on false N x M: select count(*) from A inner join B on true
well done illustration of min and max in practice, there will be some inner join condition, so the real answer is somewhere in between ;o)
Yeah I'm the head of BI for a global 100-200 person company and we're using it to provide all of our basic dashboards for none-technical people (mainly because CEO was too tight to stump up the 60K for Looker). It isn't yet able to do more sophisticated analyses that paid for options can do (e.g Cohorts was one we couldn't do fully dynamically). But for basic dashboards and marketing overviews It's fine and pretty easy to generate them. Also google is really taking on feedback and releasing features constantly. 
I'm still confused how Tableau became so popular,for paid enterprise solutions Looker and Sisense are both cheaper and able to parse data in more sophisticated way, and both companies are a lot more competitive right now because they're trying to overtake tableau. Tableau just looks the prettiest but that's the main advantage I can see outside of more people have used it. 
Learn how to use SQL. The theory behind the SQL standard is very interesting but I rarely actually use it in work. To be useful you need to know how to write a query and how to optimize a query. 
When I move to new job I was going to get PyCharm pro and Datagrip, if datagrip is anything as good as PyCharm was when i finally started using then I'll be pretty pleased. 
You can just create index with CREATE INDEX ON &lt;table&gt; (&lt;columns&gt;) It'll define a name automatically Also having the index name is useful a) query planning it helps you b) primary key definitions get used in ON CONFLICT ON CONSTRAINT &lt;pkey_name&gt; DO (NOTHING|UPDATE SET)
The term is cardinality and can be represented by two bars around your set (table) names, such as |A| and |B|. |A (inner join) B| &lt;= min (|A|,|B|)
Reporting Analyst and Data Analyst are fairly common career paths. Learning SSRS and Visual Studio is pretty much mandatory. Also, deploying a report server can be extremely useful. Typically you will also want to brush up on your Excel skills because many company’s continue to replace manual reports with SQL when possible. Also, you should probably think about developing another language with SQL. For example, I learned VBA which complete set me apart from other Analyst at my company. I was able to turn large excel reports into SQL inputs eliminating most of the manual entry they were going to have to do. Whatever path you choose to go think about developing at least one more language. Pairing a query language with an objective oriented language will pay off. 
Tableau: * Looks pretty * Purports to make everything easy to put together * Is pushed by consultants They've got a swanky looking office in the Fremont neighborhood of Seattle. Or at least they did 4 years ago. So they've got that going for them, which is nice.
Protip. Learn SSAS on Azure and SSIS on Azure. Ez jobs
So overall, this is is pretty good! There are a few business rules you might want to define so that your data model can support those use cases: My questions are marked in red numbers 1) Do you think that a double would be appropriate for a painting name? Usually names are human readable strings 2) Can a painting belong to more than one customer? If yes, then you'll need to add a new table between Painting and Customer to represent that M-to-M relationship. 3) Why does a timesheet have multiple paintings? Also, does an artist work on these paintings? You might want to track who is spending time on a specific painting. You'll also need a new table to track artists. 4) I'm not sure what the purpose of the SourceImage serves. The relationship seems strange to me. Unless there's more than one SourceImage, you could just include the SourceImage.URI as an attribute in your Painting table. Lastly, the relationships between Painting, Palette, and Color are a bit off. I am assuming a painting can have only one palette. And for a single palette, it can have many colors associated to it. For that, you'll need to add an associative entity to represent that relationship (in blue. Everything marked with green checks makes sense. Good job! [Marked up ERD](https://i.imgur.com/wy0fuix.jpg)
Truth. This isn't yet the sweet spot, but it's the future.
Exactly. Look towards the future and you'll make major bucks
Perfect, thanks. I will definitely start looking at it.
Thank you for your response first of all. This whole process has been a humbling experience. As for your questions: 1) First of all, I meant to put name as a string (lol my bad). I will have to look into a double word format. I am going to implement this in an sqlite database, so I will have to do research as to whether or not this exists in an SQLite structure. It is noteworthy that artists are weird cats. Take for example, Jackson Pollock. His painting [No. 5, 1948](https://en.wikipedia.org/wiki/No._5,_1948), is titled "No. 5, 1948". 2) After posting this I discovered what a relationship table is. I can see now that my ERD obviously is lacking a couple. In addition I am somewhat confused on the directionality of the arrows. Which is a perfect lead to... Number 3) A painting should have 0 to many time sheets associated with it. If the artists walks away and then continues painting, the database should record a new time sheet for this. I believe I need a relationship table for this? 4) It is common for artists to use more than one source image to paint/draw from. I would like to be able to have that ability in my database. I have to again, come to the lingering confusion I have, of the relationship table. Given, that each painting has to have at least one image, and as many as the device can handle, how would I keep track of this? Lastly, a painting can have 0 to one palettes. Each palette can have 1 to 10 colors. May I add to my questions with, is an associative entity basically a relationship table? 
Very nice. ERDs are a puzzle to themselves and take time and experience to understand all the options. I do not know what level of ERD your class expects, and what parameters have been set on the design. For examples; an Artist table; an intermediary Sales table rather than a direct painting to customer link; the timesheet linked to an artist and the painting. The Customer table (call it a Contact table) could be used for artist and customer with an identifier for either (a contact_type field). As you see it can start to get tricky. But go with what the class is expecting.. they might be suspicious if it is too good !! 
OP, there is no way we can help you if you don't provide us with a description of the process that you try to model! From this, we should be looking for your "core business concepts" which help in defining the entities. Broadly speaking, transactions are often many-to-many relations between these entities. 
The other advantage of having at least good understanding of SQL and Object Oriented Language(s) is the ability to choose the appropriate technology for the task. I've see it many times where SQL developers will create gargantuan Stored Procedures with large run time to perform something that would be much better served as an application, function, or method. Equally, witnessing the lengths some programmers will go to avoid a connection to a DB is breathtaking!
very much a DBA-oriented article (e.g. BCP) -- most SQL jobs are not DBA also this -- &gt; When two tables are joined together, the inner join will return the values that are the same in the rows. On the other hand, outer joins will return the fields as well, same in the table. and this -- &gt; [PK] is a unique key with defined values and it cannot be NULL. This is able to identify all the records to get maximum possible outputs by putting minimum efforts only. you simply must try harder 3/10 is the best i can rate this 
No worries on the data types! Those vary depending on the database technology you use. I only picked that one out because it really didn't make sense to use a floating point as a name. Typically, attributes that give a name to an entity is a string-like datatype to make it human readable. So even though a painting might be called "No. 5, 1948", you'll still want to use a string-like data type. I'm not sure exactly what you mean by a "relationship table", but I think we are talking about the same thing when I say "associative entity" (or sometimes referred to as a "cross table"). Basically, the purpose of an associative entity is to capture the many-to-many relationships between two entities. The blue table I drew in my original response is an associative entity. The easiest way to identify those tables is seeing that there are at least two "many" Crow's foot symbol is attached to the table. (Go look back my my drawing and you'll see it). To help with reading ERDs, here's a small [example](https://i.imgur.com/Ttc2nez.png) that uses two tables and their relationship. There are two diagrams, but they are the same tables and cardinality. The only difference is I try to demonstrate how you read a relationship in both directions. You'll hopefully see what I mean after seeing my drawing. For number 3, your relationships are backwards given what you said. For timesheets, consider this: You have an Artist who works on one or more paintings. For each painting, instead of having multiple time sheets, you can have only one Timesheet per painting, but in order to capture multiple occurrences of time spent, you can have another table called "Duration" relates to only one Timesheet, and that Duration table captures each occurrence of time spent on a painting. I can draw this out to demonstrate what I mean if you'd like. For source images to paintings, your relationship is backwards also. 
Thank you for coming, we will not be taking your application any further. Gonna have to get up a lot earlier in the morning to pull the wool over my eyes. I do find these top 10 interview questions type blogs disconcerting. 
These formulas are **all** wrong, all for the same reason - not understanding the cross join/cartesian product. How the heck this gets upvoted? For example, | a |x| b | &lt;= |a| * |b| (or | a theta b | &lt;= |a| * |b|)
&gt;WHAT IS DIFFERENT BETWEEN SQL AND PL/SQL? SQL is an ANSI standard language definition for interacting with databases. PL/SQL is a vendor-specific extension and proprietary variation on the language. &gt;Here, is a complete list of steps involved in creating a scheduled job in the SQL. This creates a job that does literally nothing. The job needs at least one step and none are defined. This "complete list of steps" is incomplete. Also assumes that the interviewee can only operate a GUI. You can create a job via T-SQL and PowerShell/.NET (through SMO) &gt;This command is popular when processing a large volume of data. When we reproduce, delete, or alter a large number of data files, tables need to be restructured and this is done with the help of UPDATE_STATISTICS Command in SQL. Updating statistics does **nothing** to "restructure tables." You don't even have to run it against a table, it's usually run against one or more indexes. &gt;When two tables are joined together, the inner join will return the values that are the same in the rows. On the other hand, outer joins will return the fields as well, same in the table. I don't even understand what this attempting to say. &gt;HOW CAN YOU COMPARE THE TRUNCATE AND THE DELETE COMMANDS? Overlooks that if constraints exist on the table, you may not be able to `truncate` If I got these answers to these questions in an interview, I'd probably move on to the next candidate.
&gt; I do find these top 10 interview questions type blogs disconcerting. I think they're fine if it gets the reader to think critically about their own knowledge base and explore the answers more to get a deeper understanding. This particular blog post does not do these things.
The answer for Q6 is incorrect. If the TRUNCATE statement is contained within a TRANSACTION it can be rolled back. Strangely Microsoft documentation states that TRUNCATE TABLE cannot be run within a transaction which is also incorrect, see: [https://sqlperformance.com/2013/05/sql-performance/drop-truncate-log-myth](https://sqlperformance.com/2013/05/sql-performance/drop-truncate-log-myth) and [https://blog.sqlauthority.com/2010/03/04/sql-server-rollback-truncate-command-in-transaction/](https://blog.sqlauthority.com/2010/03/04/sql-server-rollback-truncate-command-in-transaction/) I don't want to come down too hard on another person's work but this really isn't a great list and as interview prep I'd say it's misleading and unhelpful.
Clearly written by someone whose primary language is not English, but if the interview were conducted in English I would immediately pass on a candidate who answered t this way about outer joins. Even if they know what they mean and just can't express it properly in English, I can't hire such a person. 3/10 is generous.
I'm in a somewhat similar position, but I'm going for the MTA Database Fundamentals mainly to stand proof for what I can do. A lot will say you don't need a certification but really it depends on where you live/will be working and what the demands are. If you can do it via uni/college and have the means to comfortably do so, by all means do it. I've been doing it on my own and it has been a bit of an uphill struggle with just Google. More than happy to help where I can (even as a study buddy)
Yeah, I found tableau very frustrating to learn, coming from sql, and I know it’s very expensive too.
Get the cert. And don't wait to start learning and applying for jobs until after you get the cert. Honestly you don't really *need* a cert or IT degree to get into BI reporting. Experience in electrical engineering should be a good indicator for potential employers that you have strong math skills. Use the many online resources and get familiar with writing complex SQL and go from there.
Thank you!
Check out [SQLPracticeProblems.com](http://sqlpracticeproblems.com/), I have some great material there for learning practical, hands-on SQL. Contact me (email in the FAQ) for a Reddit coupon code. 
Interesting article. I have been wanting to learn more about Kubernetes. I should really get on that. 
&gt;Alongside the more clear advantages, it offers effortless node and process scaling, health checks, configurable eviction, and a declarative and eventually consistent model. Sounds good! I wonder if there are any more tools besides Kubernet that accomplishes this?
This was my first thought too- the question requires more context to give an accurate answer. The number of rows in table C will vary based on the conditions of the join (yes, even with an inner join). It could exceed the total number of rows in table A and B combined depending on the join conditions. As done above, consider using a join where your ON clause compares two fields and uses an operator other than =.
What database are you working with?
No, you're the one who is confused. You're right that the cardinality of a cross-product is |A| x |B|, but at no point did I do a cross-product. Give me time to shower and get my stuff done and I'll go through them one-by-one.
Oracle SQLPlus, sorry. I will update the post.
If you have the string_split function on your platform, you dont need loops - just analyze every value you get from the split and convert to lower/upper bound fields (if it's a single value lower and upper bounds will be the same). the result is a "not exists" away after that. Or your other favorite tool.
As someone who's very new to SQL and looking at it as a possible job opportunity, can you unpack what you mean here?
Update: I found this in my course notes, which is used to create a view that allows a user to see their own employee info. I believe I can create a view to do what I want, and then grant it to manager? CREATE VIEW myinfo AS SELECT * FROM employee WHERE db_userid = USER; GRANT SELECT ON myinfo TO PUBLIC; 
Learn the Azure side of SQL. So don't just learn on-premises SSAS, learn SSAS on Azure as well. Same with SSIS. These technologies are cloud facing and are very much so the future. If you learn these now (early in their lifespan) you will be able to find those speciality positions when the entire world is moving towards these solutions
First and foremost, Thank you for all of your help. After doing some further investigation and completing a full Udacity nano degree on databases basics in SQLite, I think I am starting to understand associative relationships. I have attempted to redo my ERD with these hopefully implemented in the correct manner. Before we go over the ERD let me give you some more context to the problem. The class this is for is Senior Capstone for BS in Mathematical Sciences with emphasis in Computer Science this fall. This database will be implemented in a program. As for my background, I have mainly worked with data analysis problems and OOP languages, even some functional languages. Somehow, I managed to go a full three years of school with little to no database exposure (hints im trying to learn it all before the class starts). As for the relationships: A painting can have zero to many customers, meaning a user does not have to define a customer, or they can define a group (if a group wishes to pay for a painting). A customer can have one to many paintings. A painting can have zero to one palettes (the user doesn't have to define one, but can only define up to one) A palette can belong to zero to many paintings ( a palette can be defined but never used, or a palette can be used over and over agian) A palette can have one to 10 colors (There is a whole other reason why this number is caped to 10). A color can belong to zero to many palettes A painting can have zero to many time sheets (I would prefer to store the time sheets with a start and finish because I would like to implement the start and finish in a calendar like way, I guess it would be possible to have delta time and a start time, but it seems more convenient to not have to compute this). A time sheet can have one and only one painting, A painting can have one to many source images (meaning there has to be at least one source image). A source image could belong to one or many paintings. A painting could have zero to many progress images but a progress image must have belong to one and only one painting. Now, as far as the artists table goes, we will assume that the user is the artists. Meaning there no need for an artists table? I have redone a lot of relationships and now I am bit confused which ones need indexed. Let me know what you think of this ERD: [NEW ERD](https://drive.google.com/file/d/1PuJtlh_VJHWn5SLWvHGJimk7hxkqqh2l/view?usp=sharing)
Thanks for the tip. I'm still really early in learning and the more I know the more I realize just how much I don't know. Is there a specific OOP or procedural language you would recommend to pair with SQL for this particular field?
If you’re looking for a free option, you could start with the basic tutorials in w3 schools: https://www.w3schools.com/sql/ You can “try it” to run their queries and play with the data they present to you. I often turned here during college for HW before I thought I’d be using sql heavily. Now I used it daily! This is good if you haven’t had any experience with SQL. 
Given all of your specifications, I've marked up the new [ERD](https://i.imgur.com/t0V6427.jpg). The new stuff and some of the relationship correction look good! There are a few things I want to touch on though: 1) When you start to create attributes that are suffixed with numbers is an indication that this should be broken out into a child table that fan capture this data. Not only is it cumbersome to type out number1, number2, ... numberN, but it isn't scalable. What happens if you need to capture more numbers than what you originally define in the table? You'd have to add a new column for those unique use cases. My suggestion, as I lightly mentioned above is to break this out into a child table that uses the ColorID as a FK to tie back to that the Color table. 2) It loks like you might be capturing the RBG values of a color a given color. This isn't a bad idea, but I would consider storing the hex value instead. It's only one attribute to capture rather than three. Plus, there are tons of libraries that can work with hex much easier than RBG values. Worth the consideration. 3) From your assumptions above, you said that a palette could have zero to 10 colors, yet you don't have a table or relationships to represent that data structure. I drew in blue what would help you capture that specific rule. One last note: your naming conventions have been pretty good and consistent across the board. The only thing I would suggest is to improve your cross table names. Typically, these tables use their parents table names in CamelCase to implicitly denote that it's a cross table. For example, you have two parent tables: Paintings and SourceImages. For the cross table that represents the M-to-M relationship should be called PaintingSourceImages unless there's a more intuitive name that can describe that entity. You're getting super close to have a really solid ERD, and better yet, a solid data model! 
Tons of online resources. sqlbolt really did it for me, I liked the real-time feedback.
I learnt using this originally and SQL is now 80% of my job. It's pretty good! 
w3Schools is really good not just for SQL. Albeit, it's lacking in the admin behind SQL but, in terms of pure querying, it's really good. If you want an expose on database administration may I recommend: [http://programming-motherfucker.com/become.html](http://programming-motherfucker.com/become.html)
There's also sqlzoo and hacker rank 
Www.upwork.com and similar services ...
I have not. Always thought about it. Maybe you can find some work in remote ok? https://remoteok.io/
Are you in the United States? Try looking at some of the larger community colleges in your state that may offer a SQL or Certificate certificate. In about 30 seconds I found this page (for California) that may gives some leads or ideas of what to look for. https://cvc.edu/programs/
run this -- SELECT "Fruit Type" , "Month , SUM("Number of Sales") AS Sales FROM SalesDB WHERE Type='Fruit' AND "Year"='2018' GROUP BY "Fruit Type" , "Month and then do a pivot (if your database supports that syntax) or reformat it in your front-end language
A few comments on the post: * Using the term 'update' under Atomicity is a poor choice of words; Atomicity applies to all actions that modify the database, this includes INSERT / UPDATE / DELETE / etc. I understand the intention, but the target audience of this post if obviously new individuals unfamiliar with RDBMSs. * What does rolling back a transaction mean? Does it effect the whole database? A basic explanation of transactions / commits is, in my opinion, essential for understanding A.C.I.D. * Far more than programming errors can cause Consistency issues. Consistency is more about allowing the database to be modified in only the allowed methods/defined rules of the database. * Isolation is somewhat close. An explanation of locking is a good place to start. (example: Only one person can take a folder out of filing cabinet and change it any time, but anyone else can look over their shoulder while doing it). * The use of programming error under Durability is a completely different meaning than the one used under Consistency. One is referring to system stability, the other is referring to an end user interface/API/etc.
I would recommend fiverr.com. First reason, I got my first project within a week. The community forum is really alive and lots of experience sharing goes there. I've been on UpWork and Freelancer for months now with no project, so definitely a different story on Fiverr. Second, if you use my referral link I get a chance to earn 100 bucks. That would really help me in upgrading my potato PC. PM me if you'd be interested in that. But first do take a look at all these options. These are all my personal experiences and may differ from person to person. Lastly, keep trying even if you don't get projects in the beginning, like I said, it was a few months before I got my first project. Goodluck!
The admin side is something I wish they focused on more throughout my college career (IT undergrad). A lot of focus on queries, not as much anything else. So you’re absolutely right!
Thanks, that works. Any ideas on what I could do to provide a field showing Sales per days in the month? I can add days per month as a column for each month but SUM("Number of Sales")/"Days per month" AS "Sales per Month" Doesn't work.
What language variant are you using? The double quotes for a literal in the SELECT is confusing me when added to the double quotes for object names. Try expanding the example below, and ignore the `order by`, which is only there to make it match your oddly unordered output precisely: http://sqlfiddle.com/#!17/47199/7
DBA.stackexchange.com always has remote work ads on it. Right side banner.
The problem is that you are not comparing or referencing any of the attributes of the "Likes" table from DELETE-statement. You are only use the "Likes" tables L1 and L2 from inside the SELECT-statement. This means that the SELECT-part acts completely independent from your DELETE-part and the SELECT will return the same result for each row! In this case you said that the SELECT is non-empty, so for each row of "DELETE FROM Likes" the EXISTS-condition returns TRUE. Therefore, all entries are deleted and your table is empty.
Thought this was going to be a bit more exciting, not gonna lie
I'm not positive, but the question might be about sets vs multisets.
LOL, you should cross post this in /r/programmerhumor
I am in UK
What, no [Big Data](https://youtu.be/E8b4xYbEugo)?
Hi , and sorry for not specifying myself I have a dbo.x in database in sql I have a program that it's database based in MS sql The software has a backup and restore application , it export a .Bak file extension I didn't know how to read inside , what ever i found a website called .bak to excel , i got a new zip file that includes all the dbo.x files I edited my excel target with excel but now i need to" replace" dbo.x (the excel file) with the one in the database Any help ?
I tried sql import and export tool It didn't work , it's giving me tons of errors 
Done 😎
Why the __c suffixes?
Just guessing but could the relationship have to do with a self join? When you join the table on itself. 
When you say reinsert, do you mean update? Or do you mean insert the new value, leaving the old value intact in the table? Or do you mean delete out of the old record while inserting the adjusted record?
I doubt you'll find a video for something that specific. Here's something to get you started. But as /u/FoCo_Sql said, are you updating or inserting new? Also what DB system you're using impacts which functions work. SELECT ColumnA,SUBSTRING(ColumnA,2,99) as NewRow FROM Table1 WHERE things = stuff
That's the convention for custom object and field names in Salesforce. No idea how/whether that applies here though.
 INSERT INTO yourtable ( userid , vendorid , dataid , datacolumn ) SELECT userid , vendorid , 2 , SUBSTRING(datacolumn FROM 3) FROM yourtable WHERE userid IN ( /* list of 30 userids */ ) AND vendorid = 1 AND dataid = 1 
Using tsql and its going to be an insert
Using tsql and its going to be an insert
uhhh you mean you're not sure how to create a database? check the docs, there are examples. you'll probably want to write your schema alongside your program, so itll really be dependent on what you're doing.
not sure about the 0,5 -- you may have to use 0.5 other than that, yeah, the same
ok, thanks :)
This was just what I needed! Thank you so much. 
Sorry to bother again what would I good to find this syntax and do you suggest any good resources? 
https://docs.microsoft.com/en-us/sql/t-sql/statements/insert-transact-sql?view=sql-server-2017
Here is a database agnostic source. We don’t really like to write queries for assignments, as it’s detrimental to learning, but this should get you going. https://www.w3schools.com/sql/sql_count_avg_sum.asp
Thanks I was at that website before. I know how to display one value, but not sure how to display both at the same time. That is where I'm stuck. This is the code that I used to display each course without duplicates. select distinct dept_id from course; 
Understandable. Why don’t you show us what you have so far. I think you might be making it more complex than it is.
I think I need to do the sum of all the occurring values, since there are only 16 distinct values that represent the departments, they occur 200 times. Is there a way to display dept_id column with the sum of all values from the number column? Not sure if I worded that clearly.
Yep, count(courseNumber) Then group by dept_id :)
https://www.reddit.com/r/SQL/comments/8ywo1n/the_database/ This is part of the data base. The count() then group by clause didn't display the sum of the number column that each dept_id occurred with. It just gave me the sum of all the times the dept_id occurred.
Could you do this by using a nested sub query in the from statement?
use sum(courseNumber), not count. Count counts, sum sums... for lack of a better way of explaining it :)
I think I got it. I used select distinct dept_id, sum(number) from course group by dept_id; The Format Your Code on the right side of this Reddit post helped me out xDD. Thanks for helping me out as well, I appreciate it.
I've seen some SQL Saturday presentations, and some msft presentations (power bi) on publicly available datasets. If you're looking for something like pretty resume builder, maybe go with like CDC, Census, or some other public large dataset.
Good point!! I was thinking count distinct for some reason. I’m on vacation and trying to keep my mind off work :)
If you're looking for some very hands-on "learn-by-doing" practice problems, that teach basic to advanced SQL in a logical, well-structured manner, check out [SQLPracticeProblems.com](http://sqlpracticeproblems.com/). I developed it after teaching a SQL course where the material I had to to teach from was poorly structured and academic. Afterwards, the students emailed me, saying they needed practice problems, so I developed the course! Contact me (check the FAQ on the website) for a 25&amp;#37; Reddit discount. 
Kind of a standard suffix that we use at my company. Not totally sure why or if there is a specific reason for it 😊
I’m a Salesforce Admin. Reppin’ my technology 😎
You actually don’t need the distinct there - the group by tells the sql engine to roll the aggregate functions up to the level of what columns you have included - it will only return a single row for each distinct id. 
Also, SQLite can use any file extension, fyi
I'm so glad I'm not in college anymore. The way this stuff is taught is so aggravating. If someone asked me that in an interview I wouldn't even want to work there!
Thanks. But i realized that those were class numbers...not amount of classes per department. I felt stupid haha.
It is! I am taking this as a 6week accelerated online course over the summer. It's been a struggle for me to say the least.
just grab any sql dataset. simple tables with simple inserts are sufficiently portable to work even though they're for a different backend
This was the answer I was looking for. Thanks!
Wording on homework problems is stop tedious. PatientEncounter is an attribute of MedicationOrder. Joining PatientEncounter doesn't change the granularity. B
Do you mean you are taking a primary key Dept\_Id from Table1 and a primary key Course\_Number from Table2 and add them into another table? Or do you mean you want to add those two columns as primary keys? Tables can only contain one primary key, but you can add those keys from other tables as a reference to another table as foreign keys. This creates a constraint that guarantees data consistency between the two tables and it creates a reference that can help people understand how the data correlates. If you wanted the combination of two columns to be a primary key, you can cluster the primary key similar to the syntax you used. When you create or alter by adding the primary key, SQL will typically create a clustered index under the scenes with a unique constraint. You can also specify the syntax in the alter statement if you want to, but it isn't necessary. There may be a time though where you want the primary key to be a non-clustered index and create a clustered index on other columns, you would want to specify the syntax in that instance. 
You should try both ways, or look it up in the documentation of the DBMS of which you are working. You'll be doing that A LOT as you start a SQL career. 
All keys are all constraints. A primary key is a unique constraint, and a foreign key is an exists constraint for a specified value in another table. The thing that makes a primary key different in implementation from a unique constraint is a primary key is usually (read almost always) the clustered index. Clustered indexes are one per table and is a bit of a misnomer, since it's not an index but the order the table is sorted on the disks. The syntax for a composite key can be found here: [https://www.w3schools.com/sql/sql\_primarykey.asp](https://www.w3schools.com/sql/sql_primarykey.asp) 
&gt; You start a query with Medication and add Patient using a left outer join Why? If every MedicationOrder has a PatientEncounter, then there's no reason to use a left outer join. Just use an inner join. Anyway... if you start with Medication and you do a left outer on Patient (where only one PatientEncounter can be associated with a MedicationOrder), then the original granularity is one row per medication (because Medication is the base/driver table, Patient is the joined table). And adding the left outer join where it's a one-to-many doesn't change the granularity, it's still one row per medication order.
It let me add both as PKs with the syntax I said above...not sure if it was supposed to work or not but it did.
The later, assuming that you want the combination of the two values to be unique and not the individual values themselves. For example it would allow each department to have their own course "101", once per department but with multiple "101"s throughout the table, while also allowing departments to have rows for each course they offer rather than only one.
Yup, it's functionally identical to a unique constraint, if you add two columns (as you did in the syntax above) then only the combination of the columns needs to be unique. To go a bit beyond the question, This is one of the reason why most people shy away from composite primary keys, in the example above you could have 5 departments with 5 courses apiece, and the maximum number of rows wouldn't be 25, it would be 125 since any of the course could appear for any of the departments. So the constraint is probably less effective than desired since the main idea behind a composite key is to enforce a one to one relationship. In a normalized schema course and department would be in separate tables with an FK on the course table to it's department, since a department can exists without courses but every course must have a department. The biggest issue with composite keys goes back to the Clustered index. Remember I said that clustered index is the order a table is sorted on the disk, so it's imperative that inserted values be in that same order so that new rows are appended to the back of the files on the disk. If they are not, the DB will have to resort the table on the disk with each insert. So say you clustered is on department and course, and someone adds a new course to the art department called "Excessive Disk IO". every course with a department name latter in the alphabet than Art (so math, philosophy, underwater basket weaving) will all have to be moved on the disk to allow this new art course to be added in the correct order. This is why most of the time you'll see the Primary key on a identity column.
A bit of terrible naming choice here. Medication - 1 instance of tylenol of specific dosage, for example, Medical Order = could be many instances of tylenol of the same dosage. Anywho, usually relationship implies mixed granularity, like so: G(Medication Order) = ( G(Patient), G(something else)) &amp;nbsp; If you use a proper key join (G1(Patient), G(something else) AJOIN{G1=G2} G2(Patient) you will get (G(Patient), G(Something else)), i.e. you're getting the same granularity as your original medication order table.
Go for a practice oriented tutorial. The following resource, for free, may help you [http://www.studybyyourself.com/seminar/sql/course/?lang=eng](http://www.studybyyourself.com/seminar/sql/course/?lang=eng). 
Something like this: SELECT hero , COUNT(*) FROM match WHERE match_id IN (SELECT match_id FROM match WHERE hero IN ('hero1','hero2','hero3','hero4') GROUP BY match_id HAVING COUNT(*) = 4) AND hero NOT IN ('hero1','hero2','hero3','hero4') AND radiant_win = 1 GROUP BY hero ORDER BY COUNT(*) DESC; 
first you need to concat the string '-' and then the second part of the string, soo you will need to concat twice. What are the two field names you want to concat? If they are "startStationName" and "endStationName", the select should look something like this SELECT CONCAT(CONCAT(STARTSTATIONNAME,'-'),ENDSTATIONNAME) AS PATHID FROM ...
You're probably aware but that's a bit of an oversimplification. A clustered indexed table doesn't actually literally mean that every item needs to be physically moved after an insert. Rather, a "page" of disk will be allocated betwren existing ones and only some of the data needs to be physically moved. 
First of all, SQL is a language. this is not Microsoft SQL Server. So we are not sure what system you are using, whether it is SQL Server Oracle or whatever. Each of them have different syntax. Furthermore I've never even heard of CURDATE. I use GETDATE(). Are you using MySQL? If not, there's your problem. Works in:MySQL 5.7, MySQL 5.6, MySQL 5.5, MySQL 5.1, MySQL 5.0, MySQL 4.1, MySQL 4.0, MySQL 3.23 
Which rdbms? 
You won't need to do this unless it's an Oracle db. T-SQL and MySQL both support CONCAT with N variables.
\&gt; Would like to find out eventually what caused this. Most likely it never was there. What caused it? You might well ask why your SQL interpreter has \*any\* particular command or feature that you want. It all comes down to the design specifications; it's not a felony to incompletely adhere to the \[ISO standards\]([https://modern-sql.com/standard](https://modern-sql.com/standard)), or Microsoft Syntax. AFAIK nobody adheres to any one public standard since these standards omit a lot of features we are all grateful for. 
I've never used MySQL personally, just MSSQL. W3Schools claims that a CURDATE() function does exist though: https://www.w3schools.com/sql/func_mysql_curdate.asp.
Yes but like I just said it's my sequel It's not Ms sequel. Look at the URL even.... You're trying to use something made for my sequel in Microsoft sequel which will not work.....
The space in `CURDATE ()` is the cause of the error. Should be `CURDATE()`
Ask someone to write the same query without looking at your code and compare results. Also throw in sample data, compare aggregate statistics like sum and average (do all the true values plus all the false equal all the values, or are we losing nulls somewhere). Get out a pencil and run some logic by hand. Let the QA testers find errors (it's their job).
You need to edit this now and remove the login information. At the very least, it needs to be given out on a PM basis. 
It's probably not a good idea to publish your admin password here, it probably opens up your server to a number of attacks.
It's a sandbox, and the data in there is trivial. I could learn just as much from an attack as a useful insert. Besides, they'd be attacking an Amazon trial, not my actual systems.
&gt; I need to get off of developer to update VmWare that this SQL server hosts. Wait...what? Why is this? You can change editions by re-running the SQL Server installation but to do that on Core, you'll need to do an unattended (sort of) install via command line. See http://www.eraofdata.com/sql-server/installation/upgrading-sql-server-editions/ (it *should* be applicable to newer versions as well). **HOWEVER**, downgrading editions is a rough path and may not always work. By going from Dev to Standard, you are downgrading editions - Dev is the same as Enterprise, just licensed differently. But you shouldn't be having to do this unless you're promoting your "test" instance to a production one. I wouldn't recommend doing that, as test servers tend to have terrible things done to them and the configuration might be wonky.
If what you're looking for is practice writing SQL, check out SQLPracticeProblems.com. I developed this course to give real-world, hands-on practice in SQL. The Professional package has a module specific to MySQL. Contact me (email in the FAQ) for a Reddit coupon code. 
"probably"?
You need to be worried about more than what someone might do to your data. You have to consider what someone will do by **impersonating you** with those credentials.
Our Test Environment is on VMware vSphere 6.0, I need to upgrade them to 6.5, then 6.7 to test functionality of new versions prior to deploy. vmWare doesnt support Developer versions of SQL. The 6.0 to 6.5 upgrade will not run as it sees a version mismatch. Our Production Vmware SQL Databases are running on Standard so I want to match that. Im guessing the guy who set this all up used developer because of licensing rather than functionality. 
The query should be written to satisfy business requirements. The business which produced those requirements should be able to provide test data along with expected results to execute in your test environment.
You don't have a dev or test server to play with? Usually if the database isn't too big I restore a copy of it onto a test environment where It'll be safe if anything breaks and you can get more accurate results as close to production as possible. Other than that, you can do what bigfig stated as well.
Seems a lot like it! Thank you for your response. Except I'm afraid it's not using the radiant_team column which I would think is necessary to filter if they won along or against that set of heroes. I think it just needs a little more tweaking but this is a huge step forward, thanks again!
As others said, having an environment where you can validate the results against known values is a very good thing. If you have an existing query and are refactoring it, you can use EXCEPT as a sanity check. SELECT NEW QUERY EXCEPT SELECT OLD QUERY; SELECT OLD QUERY EXCEPT SELECT NEW QUERY; You should get no results returned if they are the same. The reason for checking both is to make sure there are no results in just one set and not the other, not just that the row is different.
&gt; vmWare doesnt support Developer versions of SQL. That's news to me. I've run dev edition under multiple versions of VMWare. VMWare shouldn't even be aware of the software installed on the guest OS. Are you sure they're not talking about a trial or otherwise hobbled version of *Windows*? Can you point to the documentation you're looking at? &gt;Im guessing the guy who set this all up used developer because of licensing rather than functionality. Don't waste licensing dollars on Standard Edition for a non-production instance.
They are pushing back on me for the Developer version not being supported because of this Matrix https://www.vmware.com/resources/compatibility/sim/interop_matrix.php#db Its really them kicking it back to me - The Vcenter Update Wizard fails prechecks with the error "Incompatible Source vCenter Server Database version. Resolution: Make sure your database version meets the vcenter database requirements". While I agree it should work fine, its an out for them to push back on me. Basiclaly from their standpoint I should change versions or move the databases to a different server. Seems like at this point my options are to downgrade which is probably as much work as a new install. 
Yes its great I would recomend any course by Jose Portilla, I finished his SQL course and I am now doing the Python one. The SQL course really helps introduce basic operators, aggregate functions and joins and gives examples of how they can be used in a business setting. He also goes into string functions and date functions but I have found the best way to learn those is to learn them as and when I need them in my queries. I've gone from knowing nothing about SQL or any language 3 months ago to being able to write fairly complex queries that are 400 + lines long with confidence and a large part of that is due to this course. I would say daily practice is also key though.
The one I mentioned in the question is actually from him. It’s $9.99 right now. Thanks 
I'm confused here. Are you talking about the SQL Server database that vSphere holds its management/performance data in, or instances of SQL Server that will be run on your guest OSes? If the former, then no, you shouldn't be using Dev Edition in the first place because it's not development usage. But Express Edition should be sufficient to handle those requirements (it's just storing perf data). If the latter, then I don't know what the people who created that matrix are smoking. Because Dev Edition runs perfectly fine on VMs managed by VMWare.
Yeah, that’s where the problem really lies. The functional groups tend to make report requests without a clear understanding of what they expect the results to be
It seems unlikely that he's using MSSQL if he's looking up MySQL functions. We don't know for sure what his RBDMS is though. 
I was confused and thought you were the OP
If I have an already previously validated reference example. SELECT COUNT(*), CHECKSUM_AGG(CHECKSUM(*)) FROM ReferenceQuery; SELECT COUNT(*), CHECKSUM_AGG(CHECKSUM(*)) FROM NewQuery;
All good! Hopefully he answers soon.
If they don't know what they want then how are you going to know? You have to be firm and make sure they understand that you need to know what they're looking for.
Is this more like it: SELECT hero , COUNT(*) FROM match WHERE match_id IN (SELECT match_id FROM match WHERE (radiant_win = 1 AND hero IN ('hero1','hero2')) --Played with these heros OR (radiant_win = 0 AND hero IN ('hero3','hero4')) --Played against these heros GROUP BY match_id HAVING COUNT(*) = 4) AND hero NOT IN ('hero1','hero2','hero3','hero4') AND radiant_win = 1 GROUP BY hero ORDER BY COUNT(*) DESC; 
Yup. Same everywhere.
&gt; Thanks for the reply! The insert into temp table actually is running fine, it's the select from temp table that is taking a long time Sorry for the super latent reply but yes, that is expected. Imagine you go into a friends apartment to move them and everything is in boxes, organized, labeled (Clustered and Non-Clustered Indexes). He even knows he has roughly 10 boxes from the office, 20 from the kitchen, 15 from the bedroom (Statistics). As you all move the boxes from the apartment into the moving truck you basically open each box up and gently dump all the contents into the truck. This takes a very short time because you are just moving boxes without any thought process to it. Now you arrive at your destination and your friends says ,"bedroom stuff boxes with blue tags go into storage and red tags go into my new apartment." *"What do you mean blue tags? Were those on the boxes?"* *"YES! They were all categorized and indexed? Did you just dump all my stuff into a HEAP in the back of the truck? Now I have to go through it all again. What did you guys do?"* Now your friend who had organized all his stuff now has to reidentify all these items in the back of the truck. Now only does he have to identify blue tagged items vs red he also has to identify what is "bedroom" furniture. There are times to do temp tables but they should never be in a report stored procedure. I cannot think of a case where someone must do a temp table that cannot be done in a single select statement. When you do temp tables you are also doing serial transactions. A single thread is your choke point. If you use CTE's or inline queries you are allowing your SQL engine to run processes in parallel. 
I think what OP is saying here is that people are asking for something without knowing what the answer is, e.g. what was our revenue last year? Not that the people don't know what they're asking for. Though, that's pretty common, too.
I'm assuming you are referring to quality assurance... how do you know that the results are correct? Well, I take sample data at random points, and I try to validate it. I also know my business really well, so the returned results will either make sense to me, or they will stand out as erroneous. Give me an example of what you are wanting to QA. I'll see how I might go about it.
Over the past year I've been writing unit tests for stored procedures and views using [tSQLt](https://tsqlt.org). It formalizes what everyone is echoing here, the idea of comparing output against known correct output from test data. It's got some really useful procedures built into it to help with this sort of testing to help with tests getting too complex due to dependencies and things like that. Another huge benefit of unit testing in general is that you can run all of your tests every time you change anything, thus catching any unexpected changes as a result of your change.
Udemy courses should always be around $10, so don't get fooled by the discount amount. That's just their marketing ploy. I completed that course of Udemy and thought it was really good. I had some SQL experience going in, but still found it really helpful and it was definitely worth $10 to me.
Ok thank you because it’s saying 4 more days left at this price. 
Yep that’s pretty much it exactly. Except it’s stuff that’s really specific like how many left-handed fiddlers took the #16 bus last year on a Tuesday? No one has any clue what the answer is but we really do have to know...generally for legal reasons. It’s health care so we get all sorts of weird stuff come down from the state
I had to do this when I was sneaking in bug fixes but had to make sure a portion of the code stays the same. Ran them along side and just use this to compare outputs. 
The 'team_player' columns are what is known as a repeating group. They need to move to their own table. It's a process called normalization. See if [this](http://graphdatamodeling.com/resources/rettigNormalizationPoster.pdf) helps.
A couple of small things: * PRIMARY KEY implies UNIQUE. You don't need both on a single column. * [AUTOINCREMENT](https://www.sqlite.org/autoinc.html) is usually not necessary with Sqlite.
Yeah, I'm in a similar position on the regular at my job. I'm guessing you've recently started at this place or position? It was a weird transition for me going from a place with QA to a place where my reports needed to be correct out of the gate, with directors as the customers. It gets better, though. As for advice, I would say try to come up with a couple different ways to get the answers and compare results across the two queries. After a while, you'll get more comfortable and hopefully have a repository of queries you're confident do what they're supposed to. Further, the more you work with the DB, the more comfortable you'll be doing ad hoc stuff. Good luck!
--I've read what you posted, not sure if i understood how to implement it here correcty though. Is this better? CREATE TABLE `match_data` ( `match_id` INTEGER PRIMARY KEY, `map_name` TEXT, `datetime` TEXT, `date` TEXT, `time` TEXT, `wait_time` TEXT, `match_duration` TEXT, `viewers` INTEGER DEFAULT 0, `team1_score` TEXT, `team2_score` TEXT ); CREATE TABLE `player_data` ( `player_steamID3` INTEGER PRIMARY KEY, `profile_link` TEXT, `nickname` TEXT ); CREATE TABLE `scoreboard` ( `match_id` INTEGER UNIQUE, `player_steamID3` INTEGER UNIQUE, `ping` INTEGER, `team` INTEGER, --1 or 2 `position` INTEGER, -- 1-5 `kills` INTEGER, `assists` INTEGER, `deaths` INTEGER, `mvp` INTEGER, `hsp` INTEGER, `score` INTEGER, FOREIGN KEY(match_id) REFERENCES match_data(match_id), FOREIGN KEY(player_steamID3) REFERENCES player_data(player_steamID3) ); I am not sure about the player_data table, considering that you can change your nickname or profile link.
select v5 from c join b on b.v2 = c.v2 join a on a.v1 = b.v1 where a.v3 = whatever and b.v4 = whatever There's tons of ways to do it really.. but there is the super basic to help you with the logic.
A primary key is a unique constraint. That can't have nulls. And you can only have one per table 
Typically, these are a suite of problems that can be solved at several layers of the stack, and there are other orchestration solutions, like DCOS. You could use something like supervisord for management of process scaling, Managed Instance Groups on AWS/GCP for node scaling, nginx for health checks, etc. What Kubernetes tends to uniquely offer is all of these capabilities with minimal gotchas, in a way they can interoperate with each other, all in one place, expressable via configuration rather than tons of code, while working on any provider within reason. That's a package deal that automatically adds tons of bang per buck and allows you to focus on the software instead of everything beneath it, while providing flexibility as needed on the lower layers.
A few years ago I started a new job and had lots of problems with proper data validation of reports. I quickly found that users did not trust the reports and I needed to win back that trust. I also found dozens of work orders that were closed with the 2nd to last comment: "Sent copy of the report to requestor for review" Then a week or two later: "Having heard no reply, I consider this WO complete." SMH. Eventually I found part of the problem was setting proper expectations. Many requests were time sensitive and a formal report fully developed wasn't what they wanted. Solution: Create a new category of Work Order called Data Dump. I set expectations that the dumps were quick and dirty, but could be changed to formal reports of the requestor needed it and could wait X days per our standard SLA. Second key change was to start labeling all reports DEV with the deployment date until the report had been data validated by the end user. Then when I needed to close a work order due to lack of communication from the requestor, the report would have the mark of shame on it. In the future when a request would reference a DEV report, I would bring the old WO notes to any meetings about it and get buy in from the user and management to have the report properly validated. I'm very detailed oriented, and triple check any reports before deployment. It took a while, but this helped users see their necessary role in validating any report change requests. As for how? Typically in Excel unless layout and printability is vital, then PDF. Also, I develop in productions so we don't have to worry about Dev/prod sync, just production impacts. If a user has a data validation issue I always demand an example record that I can use to reproduce the issue and resolve it. Finally, most of my report issues today are garbage in, garbage out. First thing I can validate is the report data matches the system of record. After that I'm searching for the business process that changed and wasn't updated. Good luck! 
You are awesome! I wasn't going about this quite the right way, and I got too hung up on many other methods of doing this, but starting from the last variable and working backwards helps where I before I was running in circles. Thank you so much!
You got the idea! The scorecard table is what we call an intersection table, where you have a single row for each combination of match and player. Changing the nickname or profile link doesn't make any difference to the scorecard.
Since your wrote the report and understand the data model, you could mock up test data and make sure your output is as you expect. This is your chance to try and think up the crazy edge cases you can, and see that your query handles it appropriately.
No worries! Happy to help!
Still, it doesn't take into account the column radiant_team
Since our outer query is winning heros, radiant_win = 1 are on the same team as our hero, and radiant_win = 0 are our opponent heros.
That's great! What's your strategy? Just to through these courses once?
It’s giving contradictory results I think. I’m not sure how to test it now that I think of it.
Yeah I went through the SQL course once and at the sane time kept a kind of notebook in excel with notes about use cases of each function and example syntax. On top of that I am able to practice everyday as I have been using SQL to develop my own scripts at work.
Try and relate it to set theory, with regard to the join operators - there are loads of diagrams online. For example, if you want to exclude rows from table A that don't have a corresponding value in table B, that's an inner join. Else it's a left outer. Then, where your limitations are dependent upon the specific value, rather than just the presence of a non-null one, use WHERE predicates.
You went through this particular course? Did you find it very helpful? I am not new to sql. But would still consider myself a beginner because I have been away from it for years 
Yeah its provides a great foundation, maybe a little easy if youre not a newcomer though
&gt; How do you ensure that the data you’re gettin out of a query is what you want? I give the report to the user that requested the report and say, "You need to verify this data to be sure it's accurate and what you expected. You'll need to spot check several to look for issues. If you find any inaccuracies, however minor, you need to let me know immediately what you're getting, what you're expecting, and what you think might be missing." I make the report match the agreed upon spec or description. It's up to the user to determine if the results match what they expect. Especially with our financial system, only the finance office people really understand the data well enough to know what they're looking at is correct. Reports typically consist of combinations of six different codes plus some dollar values, and I don't know the codes well enough to know what they all refer to, so to me it's often just a bunch of arbitrary codes. If I make a coding error (which is fairly rare) or if what I understood the spec to be isn't really what's desired or wasn't complete (which is very common), then corrections are made. When people complain that I should just know if it's right or not, my typical response is, "I can present the data in any number of ways. I want to present it to you in a way that makes the most sense for your needs and for the needs of everybody else. I don't know what you're trying to accomplish with this report, so I need your help with that." It usually take three to five revisions for complex reports to get them working the way the users want with all the relevant data that they need. Yes, I have the luxury of having the people who request data from me knowing what they want. 
Is it a problem that JOIN really means INNER JOIN? Would you be accidentally limiting the results that way? Or is that specific to MySQL? 
When you install MySQL on the computer, you set up the login then. Did you install MySQL yourself?
Just reinstalled mysql and got it working. Thanks
Cheers! 
Is logical you get less records You join the start end date of 1 record against the min/max over multiple records. The only id's that will hit is probably the records with only 1 records or if the multiple records has the same date Why dont you only join on your id? Why do you use a subquery, or is this a simplefied query with a lot less columns?
This would definitely limit your results because of the join. That’s good for this query though. However, if you were trying to find rows based on v3 ‘OR’ v4 then you’d probably want to set it up much differently.
Just like any other permissions scheme - your user can't read from that column. It could be because it's data is sensitive, deprecated, duplicative, or any other reason.
 select id, name, place, min(start_date), max(end_date) from tbl_people group by id, name, place;
Ok great. I’m also looking into python because I want to get a job. Not sure if learning sql will land me a job vs python 
If I do this, I get a lot more rows, but a lot of blank ID fields... I had already tried this, and the sub query seemed to remove this issue
How do you get blank IDs? Are there records in your table with null IDs?
It seems like a mechanism to prevent `SELECT *` queries. You're meant to name your columns and leave out `NoSelect`.
No I just checked the table, and there are no id fields that are NULL. There are many other fields, but I don't care about them. I also should say that the id is not the primary key/actual id number it's the person's id number. Each person has an id and it's unique to them.
We use Visual Studio database projects. We use the comparison tool to identify differences, and generate base scripts which we modify for performance.
Oh, I didn't know Visual Studio had that capability built in. I'll give it a try, thanks!
Cool thank you for the reply!
I have been working in tech for nearly a year now and my manager advised me to learn SQL first and then possibly VBA as they are more useful in an entry level role as opposed to Python.
But why would it be called noselect? 
That makes a lot of sense, but damn, why the hell would it be necessary? I guess it would be a way to prevent people writing procedures that include "insert into . . . select * from product" which would then break when a column is added to the product table, but it's a painfully ugly solution to me.
Don't underestimate the passion of some to hate specific syntax.
Because the designer though it would indicate to users that they shouldn’t select from there.
Ha Ha. But why would it be needed??
Doesn't Apex have that capability? We use Redgate Compare / SQL Source Control / Snapper to work with our source control and automate the creation and deployment of scripts and changes once they are reviewed. Apex is a direct competitor to Redgate I believe and their tools should be very similar. 
Sounds like DBAs trying to force a development team to stop writing bad code. Another option would be a computed column defined as 1/0. 
Thanks 
I have several colleagues that do both web and data. They are paid less then people like me that specilize in data. Web is sexier, but there are more people doing it. By specilizing you become better expert and more exclusive.
It’s a field-level security that blocks regular user selects. For all I know it could be credit card number, trigger word for hypnotic suggestions, or url to nudes. Does not matter, it’s just data users should not have access to.
It's absurd to store something sensitive in a table that is queried by users who shouldn't see it and calling the column "noselect." I mean, surely no one with the tiniest pride in their work would do that, right? I think the real answer is what the other guy posted about disallowing select * from queries.
Absurd but not impossible. And we cannot know the reason without asking the designer or having access to the field in question, so the point is moot.
Maybe it contains nothing at all, but the designer adds this to every table with no privileges to prevent "SELECT *" queries across the board. 🤷
&gt; That makes a lot of sense, but why the hell would it be necessary? Because someone abused it, didn't listen when they were told to stop, and so they lost that privilege. People who had query access kept writing SELECT * queries and not specifying columns. Those queries take extra time for the engine to run, almost always indicate laziness in a production query, and lazy query writing often means stuff that looks like SELECT * FROM Table JOIN Table JOIN Table JOIN Table for 50+ fields when all the program actually needs are 2 or 3 fields from 1 or 2 tables which means you've got a *huge* waste of memory, disk IO, and network IO to return all those results. 
Redgate tooling uses the same framework as Visual Studio (With SSDT) uses. That is called DACfx, and is the frame work to create and update a database. We use a custom PS script to deploy it, which took all of a day to write and test. Using that method, you can pipe Octopus variables all the way down into your individual objects, by virtue of the SQLCMD variables it enables.
We don't currently use SSRS, but we use stored procedures for absolutely everything. It's quite useful for a database to be self aware of all code that runs against it.
I would point to a Stored Procedure, for compartmentalization.
If you embed queries (or use embedded data sources in your reports) - I hate you. By doing these things you are hiding code...among other evil things. And - if you have to change the structure of your database? Guess what...you are going to be combing through all of your reports looking for code that references that database. Just don't. Just fucking don't.
`DATE(col)` will yield a valid date, or else NULL so use this when inserting values -- `STR_TO_DATE(DATE(col), '%m/%d/%Y')` this ensures that dates stored are valid or NULL for display, you could use `COALESCE(DATE_FORMAT(col,'%Y/%m/%d'),'Unknown')`
I’m doing the Stanford lagunita SQL course rn and it’s great.
ISDATE()
Lol our EHR vendor modified a lot of table structures about 10 months ago and that happened.
If you store your database schema and stored procs in source control with validation like VS, when you make underlying table changes or view changes it will let you know when you break stuff. Also provides you with migration scripts.
That shit is the bane of my existence. And why I will never be unemployed :)
If you're looking for some very hands-on "learn-by-doing" practice problems, that teach basic to advanced SQL in a logical, well-structured manner, check out [SQLPracticeProblems.com](http://sqlpracticeproblems.com/). I developed it after teaching a SQL course where the material I had to to teach from was poorly structured and academic. Afterwards, the students emailed me, saying they needed practice problems, so I developed the course! Contact me (email in the FAQ) if you have questions. Also, redditors get 30% off the Professional package (discount code reddit30). 
People who hard code queries are scum of the earth.
Don't forget how much easier it is to manage the permissions by just allowing execute rights against the stored procedure instead of allowing all that access to the underlining tables or views. 
Absolutely use SPs as much as possible. Assuming you aren't needing to make changes to the report design itself, using SPs makes updating queries much quicker because you don't need to load the rdl files into report builder or visual studio. Just keep SSMS open on your desktop and alter the procedures for all your reports right there. What's even better is you don't need to redeploy the report to SSRS after the query is changed. Just refresh the report and boom, your query changes will be reflected in the output. Writing your SQL in SSMS is also much easier from a formatting standpoint as well.
Absolutely use SPs as much as possible. Assuming you aren't needing to make changes to the report design itself, using SPs makes updating queries much quicker because you don't need to load the rdl files into report builder or visual studio. Just keep SSMS open on your desktop and alter the procedures for all your reports right there. What's even better is you don't need to redeploy the report to SSRS after the query is changed. Just refresh the report and boom, your query changes will be reflected in the output. Writing your SQL in SSMS is also much easier from a formatting standpoint as well.
I'm shocked by the unanimous support for stored procedures here. Usually there's one or two who prefer to use embedded queries. That said: I'm with /u/mmfonline. Unless there's no way you can do what you need to do with a stored procedure *or* you have a wild database implementation that makes stored procs a mess to maintain (or requires copious amounts of dynamic SQL), go with the stored procedure.
Even if you have some (probably wrong) case for using dynamic SQL (another evil...on par with triggers, not quite as evil as RBAR) - FFS embed it in a stored procedure. Then it is findable and gets backed up with the database as well. Sorry - old school DBA and Architect. Low tolerance for bad ideas. And some ideas will always be bad.
Could you use the Row_Number function?
I researche dit. It seems row_number() *requires* you to use the Over keyword. row_number Over (Order by x,y,z) which doesn't really solve my problem. Because after ordering it by x,y,z, there's no guarantee the data will otherwise be in the original order, right? I need the row numbers from the original table..
You realize that you can query the ssrs code and return the queries from the XML. We have a set of code for it at work, we call it the query query. It works like a charm.
I will be the one to defend ssrs embedding. Here are my reasons: 1. Biggest for my current company: altering an ssrs report is not a big issue and doesn't need IT to do a full implementation cycle to release the updated report (as it's an Fing select query), but an altered SPROC is a change to the underlying database structure and therefore must go through full IT change controls and cycle. 2. I never liked that to track a SPROC based report back, I would have to find what sproc the report used by querying the catalog and then dig into the sproc in the DB to figure out what they were doing. Also, it was less likely that the dev would go down some nasty sproc rabbit hole that goes multiple procedures deep passing variables all the way making it nearly impossible to debug in a short time.
I would suggest trying some free resources before you commit to anything. My personal favorites were w3schools.com/sql and sqlbolt.com
If the request is quick and one time, I use text/embedded queries in my reports. Things like 'select something from table where column equals one and only one value. If it is a report that uses similar data sets of reports that others have asked for, I make it a shared data set or pull it from a stored procedure just to save time for future usage. The pain I have discovered is when the query or report results don't make sense, you have to change the query in the report rather than on SSMS.
And why not use a layer of report views? 
OP is using MySQL
Another vote for store procedures here. I recently moved from an EHR that heavily relied on stored procedures to a business app with embedded queries and I just yell “fuck” all day long.
Check the following web site [http://www.studybyyourself.com/seminar/sql/course/?lang=eng](http://www.studybyyourself.com/seminar/sql/course/?lang=eng). The course, along with examples, is quite easy to follow. You can submit exercises as well. Everything for free. 
I am wholly in the SPROC camp but for one use case (which is far too common in my current place): `WHERE variable IN (@inputlist)` I know you can use dynamic SQL to get around this but it's a lot easier to just embed the query.
1. Basically boils down to your internal IT processes. Not relevant. Your company is applying a release process to SQL objects but not report deployments. If your company had standardised release processes for all types of things then this would matter at all. It's not a point in favour of embedded SQL and is actually an issue with your companies lack of standard IT policies across all types of deployments. 2. Really??? Just fire up profiler, run the report and you've got your SP call ready to debug. If your SP params are easy to figure out you may not even have to do that. It takes about 10 seconds to change a alter proc statement to declare the parameters instead, copy over the set param values from a trace window and then run. I do this all the time and can only think the issues you have with debugging procs come from a lack of experience and familiarity with them. I cannot think of a good reason to use embedded SQL except for having to deal with legacy messy code in which you are already dealing with bad practice. Even then your probably going to be better off wrapping the whole lot in a proc anyway. However I can think of many many reasons why SPs are the better choice. Does anyone have a good reason to use embedded SQL? Cause I cannot see any.
That is one of the big headaches I've found and we have a lot of those
You can use a split function in a SP to achieve the same thing. "WHERE variable in (select * from ufn_split(@variable, ','))
I forgot to put that when I made this post, so I edited in MySQL.
I know - but I would prefer the code to reside in the database it is pulling data from.
&gt;o about how SQL engines work and you can Sorry about the delay, I read this the other day and shared it with all my co-workers. Super interesting read!
You can use ROW_NUMBER OVER () (nothing in the parens)... maybe that'll get what you want. I don't get how it's not ordering the way you want unless your order by statement isn't in the correct order. (eg: human error?) 
This is awesome, it took a little searching to find the custom split function you referred to but I'm going to give it a whirl and hopefully be able to fully convert this mountain of reports to SPROCs (I've pasted the code below in case someone else needs it): `CREATE FUNCTION [dbo].[ufnSplit]` `(@RepParam nvarchar(max), @Delim char(1)= ',')` `RETURNS @Values TABLE (Item nvarchar(100))AS` `BEGIN` `DECLARE @chrind INT` `DECLARE @Piece nvarchar(100)` `SELECT @chrind = 1` `WHILE @chrind &gt; 0` `BEGIN` `SELECT @chrind = CHARINDEX(@Delim,@RepParam)` `IF @chrind &gt; 0` `SELECT @Piece = LEFT(@RepParam,@chrind - 1)` `ELSE` `SELECT @Piece = @RepParam` `INSERT @Values(Item) VALUES(@Piece)` `SELECT @RepParam = RIGHT(@RepParam,LEN(@RepParam) - @chrind)` `IF LEN(@RepParam) = 0 BREAK` `END` `RETURN` `END` Taken from [https://mahalenilesh.wordpress.com/2013/08/18/t-sql-getting-multi-value-parameters-to-work-with-stored-procedures/](https://mahalenilesh.wordpress.com/2013/08/18/t-sql-getting-multi-value-parameters-to-work-with-stored-procedures/)
I'd like to think I'm proficient at sql so take this as you will! You may want to structure your conversation like this. 1) what is a database / relational database -give basic definition of it 2) how does a database work ( in terms of storing data and accessing it) -i know my company still thinks excel files are databases so maybe describe how an actual database differs from "excel databases" 3) how it benefits the company performance, organization, etc 4) maybe go over basic select statements or some joins after gauging their aptitude from points 1-3 Hope that was helpful or at least gave some ideas to think about. Best of luck man!
I'd focus on theory rather than actual syntax and usage. The concept of header and detail tables, and how they have a "one-to-many" relationship, is a good place to start. I'd talk about the philosophical *why* of a database - which is to facilitate the quick retrieval (and storing) of data. Having a paper trail is good but you can't quickly sort or extract information from it. Up until the late 1950s (IIRC) the US stock exchange had to be closed for an entire day each week just so the accountants and number crunchers could keep up with all the trades. Once they switched over to mainframes they were able to stay open that extra day.
I can insert invalid date values, though when I update it using STR_TO_Date, I get the error 'Incorrect datetime value: "Homer"', not Null. 
Apex does have that capability, but it's very limited and buggy. For example, when exporting the resulting compare to XML, it cuts off the Sync Script for no apparent reason. There's also no way to load an object filter file... I gave up on that method.
This might be the answer I'm looking for. I'll check it out, thanks!
see second line of my reply
I'll look into using STR_TO_Date function during initial loads, Load data local infile.
I have two suggestions then, you can try Redgate Compare pro / source control and see if that works. I have found it to be very reliable and works well, my biggest complaint is how slow it is to use but I'm 95% sure it's a hardware and hardware configuration problem. The other option is SSDT. My understanding is SSDT is used by Apex and Redgate, those two just slap a nice cover on it and make it easier to use for a fee. So SSDT may be able to accomplish what you want for free.
You should have (at a minimum) 2 instances. Ideally you would have at least 3; however, you can work with 2 depending on your exact situation. Make a backup of your production data during downtime when there are no transactions. This is now your development environment. All Dev should be done there. If you can't make a third environment for user Testing then conduct all testing in Dev. Once you have completed all tests and ensured there are no critical bugs, deploy to Prod (the original database that the users were using). Development can take place in the Dev environment anytime that you like, without risk of causing problems to the users. The deployment to Prod will either take place after/before hours or it will need to be taken offline temporarily during business hours for the deployment. Theoretically the deployment should take no more than a few minutes. 
Do you have a good place to get started with this? Google doesn't seem to show me what I'm looking for at quick glance.
If there are slides or handouts, maybe something like this to show people how the various joins work? [https://www.codeproject.com/Articles/33052/Visual-Representation-of-SQL-Joins](https://www.codeproject.com/Articles/33052/Visual-Representation-of-SQL-Joins)
How do you deploy structural changes from the development database to the production database in a few minutes? The data in the dev would be behind the data in the prod, so I wouldn't be able to do a straight export.
You can work out how to apply the changes in your Dev environment and don't have to sweat too much if you lose data. Once you've got everything worked out, these changes are stored as scripts. If you have a Test environment, you can test the scripts there. Assuming all goes well, run the scripts in Prod. You're not worried about moving data from Dev to Prod, rather executing changes to the structures as scripts. 
&gt; changes are stored as scripts Maybe this is where MySQL will come in handy, because to my knowledge, Access does not store changes as scripts.
In short, I don't believe so. You are probably going to have to schedule a time to update the database. You will have to have exclusive locks on the actual database file to update any table or UI structures. You could maybe script Access DB changes in VBA but I'm not sure about that and you would still need exclusive access to the database to run the script anyway. If you were using linked tables to some sort of SQL server you would be able to generate scripts to update your tables in a few seconds then open the Access DB and update all the linked tables through the linked table manager but that would still require exclusive access to the Access DB for a short amount of time.
I would highly recommend using MySQL if possible. Access can be great for certain things but if you will be continuing development it may be best to move into a RDBMS. You'll definitely find plenty of resources online for guidance in your tasks since what you are doing will likely have been done hundreds of thousands of times in that RDBMS. That's not to say that you can't do this in Access, though. If you would like to create a structured process for deployment you can build a VBA program for deploying your scripts. Theoretically you can save your sql statements in files in a designated directory and have your VBA program load the file and execute every statement. If you are handy with VBA or aren't interested in that method you could also manually execute each statement in the script but that would take a bit longer to complete and you also run the risk of user error by copying/pasting incorrectly or perhaps skipping a statement. But the good news is that you have options here.
That's a legit story/example thanks!
Right on, useful!
Yeah that's pretty much my plan for now... Thanks friend!
&gt; linked tables to some sort of SQL server I am already planning to: 1. Normalize and rebuild the table structures from my Access database into MySQL 2. Rebuild my macro of hundreds of queries that populate the tables and update fields in MySQL 3. Link these MySQL tables to Access and continue using Access as a front end program as the forms are pretty useful. Seems like I just need to incorporate another layer as laid out by /u/onex0907 and complete everything I laid out above and I'll be set. In the meantime, seems it's simplest to continue my routine of working off hours in chunks to make the QOL improvements that are needed day to day.
I use a powershell script that scrapes through the files and tells me the files that match the string I'm searching for. Visual Studios has a similar feature too. 
I definitely want to move into MySQL and will begin doing so as soon as possible. Small business constraints make it difficult, hence my search for a bridge solution that keeps me from working off hours like I've had to do.
yeah, if you're loading data, copy it as is into a "landing" table first, then use INSERT SELECT to extract from the landing table into a new table, applying edits such as `STR_TO_DATE(DATE(col), '%m/%d/%Y')` in the SELECT
Take a look at [DATEPART](https://docs.microsoft.com/en-us/sql/t-sql/functions/datepart-transact-sql?view=sql-server-2017). If you're looking for week number: `SELECT DATEPART(WEEK, GETDATE())`
So can I replace the GETDATE with my date, what I have labeled as “exampledate” above?
Exactly. :)
Be careful with this, as you likely want year and week. SELECT DATEPART(WEEK,yourDateColumn), DATEPART(YYYY,yourDateColumn) Otherwise you will group the first week of every year together. 
Assuming you have VS installed: Install DACfx on you machine. Install SSDT (SQLServer Data Tools) extension/SDK/whatever for VS. Create a database project. There's also a way to point it at an existing database and "extract" the project from it. Once complete, create and save a publish profile for you application (right click the project and publish, it allows you to save the settings). Once you have a publish profile, you can build to create a .dacpac, .dll, and .publish.xml which, together, allow you to deploy and upgrade. It also has support for detecting drift (DB changed since last deploy), existing data (to prevent data loss), and more. One of the more fun things you can do is reference the DACfx DLL in a C# project and load your dacpac. This let's you do fun stuff like format-police yourself and enforce coding standards. I have a project that checks all keywords are capitalized, all FK's have a backing index, sprocs don't call sprocs, etc... There's a lot more to it, but the basic steps are just to install DACfx and SSDT, and start working on making your DB into a *comprehensive* project.
Thanks! SOLVED
Gotcha, thanks!
Build the structure in mySQL and then do the data migration with Talend jobs. Should be much faster and better structured than doing it by macros.
Are you saying to use Talend as a one time import to the new MySQL database, or to fully replace my current macro structure? I currently use different macros in Access to populate and scrub my sources of data into my tables. I was planning on re-writing it all in PL SQL (or whatever the equivalent is in MySQL) as I need to run the queries at least a dozen times a day.
Script out your changes in the development instance and apply to the “production” instance. There are some frameworks that do this for you. Django and SQL Alchemy are two python libraries that manage the database. In addition to decoupling your code from the back end, it also creates versions of your database that you can store in a source code repository. Additionally, you can start on a new database entirely and reapply all the previously saved scripts. eg if you switch from MYSQL to Postgres, or if you want to apply only the latest changed from development to production. Access is probably one of the least scalable databases for exactly the problem you’re having: you can’t make changes while users are using the data. IIRC user updates also lock the entire table rather than a single row (though maybe that’s only on older versions). The best thing you could do is change your database platform. Stop using access and use something like Postgres, SQL server, or MySQL. The first two follow SQL standards more closely than MySQL, so there’s fewer esoteric commands. If you can’t change your platform, create file backups of your database regularly in case t gets corrupted (access DBs almost always do corrupt). When making changes, create a copy, modify it, then overwrite the production copy (assuming the production copy has been backed up). If possible, use scripts to modify your database so that you can apply that script right before deploying. That way your users don’t lose data. 
MySQL is better than access, but it still has more limitations than Postgres. If you’re planning to scale this application up I would recommend using the best open source DB available. 
Yeah, for non technical users, I'd just focus on #2 and #3 and forget #4 ( or maybe throw a few examples of pseudocode selects - I always use movie examples because it's universally easy to follow what select director from movies where title = 'Jurassic Park' means - and then just point them to a good online primer if they want to learn more) Particularly I'd focus on advantages like Centralized Storage and maintenance of data Good security practices, roles, etc. Multiple users accessing data at the same time without impacting each other Able to join different datasets together Scalable Able to work with sets (set theory) so can quickly retrieve or update very large datasets Easy to connect back into data-driven software like Excel, Tableau, PowerBI, etc. 
Why is Postgres better than MySQL and any other open source option?
https://www.2ndquadrant.com/en/postgresql/postgresql-vs-mysql/
Talend as a one time import for initial migration. If you keep reusing Access as your frontend using pl sql procedures is propably the best solution. Otherwise I would consider writing my own api as a backend in .NET or java to get rid of access. But it depends on the scale of your application and how flexible you have to be.
One old school way is to do all your edits and inserts into another table. And when you're done, swap the two tables by renaming one to another.
for the data shown, that query will always produce two rows what was your question again?
The DMVs will tell you almost everything about what's going on deep under the covers of your SQL Server. What, specifically, are you looking for? What do you mean by "how they impact SQL Servers?" Do you mean negative performance impact when you're querying them?
I only have a half dozen users and we are in a small business with few resources. My IT department don't currently have the capacity to work on this with me, so this is a one man operation and I've never messed with .NET or java, plus I've only worked with SQL in the class room. Perhaps it is something to upgrade upon in the future, but I think the most important thing for me is to normalize the banckend and get it in a better environment than Access.
I think I have an inkling as to why this is an old school way lol.
Not really the negatives of the dmv, but how it can improve query performance times.
&gt; How do you deploy structural changes from the development database to the production database in a few minutes? You don't. You get buy-in from management that maintenance windows are necessary, schedule a downtime window for maintenance, and notify your users before each window. Then during your maintenance window you deploy and test to confirm your changes before bringing the application back online and notifying the users that maintenance was completed. Your downtime window should be at least three times as long as you think it will take, but always plan at least two hours. If you're back up before then, nobody will mind and you'll look professional. People will be pissed if you underestimate, so don't underestimate. If that means you've got to come in early in the morning or late at night or on a weekend to have enough time to do the work, that's what you do. Welcome to IT. If you're lucky your boss will let you shift time to get time off to balance it out. Even if you're going to do it at 3 am on Christmas Day, however, you still email your users that there is scheduled downtime because some idiot will always complain otherwise. Some idiot will *still* complain, mind you, but at least then you can point to how they were informed of the downtime should have planned appropriately. If they still complain, you pass them off to management (which is why getting management buy-in is necessary). Also, always do a full backup before you begin any maintenance window. If something goes pear-shaped, you can restore the backup you made and push the changes off until the next window. If your employers tell you that you need 100% 24x7x365 uptime, they don't understand what they're asking. 99% uptime is still three *days* of annual downtime. 99.9% annual uptime is still 8 hours of downtime annually, and 99.9% uptime is going to cost a lot more than a small MS Access application. The best applications in the world struggle to meet 99.9% uptime. 
DMVs don't do anything to improve (or degrade) your regular user query times. They provide information about SQL Server's operation itself.
If you're staying with Access consider using MS SQL instead. If your needs are modest (DB less than 10gb and performance not impacted by having only 1gb or ram) you can get away with the free Express edition. You'll likely find the migration from Access to MS SQL much easier than to MSSQL, and then you've got a path forward via PowerApps for future development to get away from Access completely.
1. Create a foreign key constraint on the `Department` field on `Employees` which references the `ID` field on `Departments`. 2. Populate `Departments` from your reference data. 3. When you load `Employees`, to populate `Department` you'll have to do a `JOIN` or `SELECT`(depending upon import implementation) to get the `ID` value from `Departments` that corresponds to the value in your source data for `Employees` 4. If you have data in your source for `Employees` which doesn't appear on `Departments`, it'll throw an error because you're attempting to violate the foreign key constraint. If you really want to go down the full normalization hole, or have a need to associate an employee with more than one department, remove `Department` from `Employees` altogether and create an intermediate table which lets you link `Department` to `Employee`. That'll just have two columns, the Employee ID and the Department ID.
[Do you have a date table?](https://www.brentozar.com/archive/2014/12/simply-must-date-table-video/) [You should think about setting up a date table](https://sqlsunday.com/2018/07/16/sql-server-calendar/) which breaks out the weeks your company uses to track things as they may differ from the regular calendar. For example, if your company's fiscal year starts on July 1 and you need to do this grouping by the week of the fiscal year instead of the calendar year.
it's easy to miss but you need the driver. if you happen to be using jetbrains IDE (IntelliJ), open the database connectivity thing and click the green plus, pick SQL server, and the pop up will say "missing drivers, want to install them?" or something. I'm at work right now or I'd be more help, Google "how to install SQL server driver for Java"
If you are going to be doing this, you definitely do not want to be using access
MySQL is to PostgreSQL as PHP is to Python. The core issue most DBAs have with MySQL relates to it's fundamental design: originally, it wasn't interested in being a well-designed RDBMS. MySQL's ease of use -- which is what made it popular -- partially comes from not caring that it was doing things badly or wrong. That didn't really change until MySQL 5, which isn't *that* long ago. MySQL 8 is much, much better, but it still has a fairly poor reputation overall. Also, people hate Oracle. PostgreSQL is very much concerned with "the right way" of doing things, and it has had support for some very advanced features that are very nice. When you're very familiar with an RDBMS, PostgreSQL feels much better to use than MySQL. Realistically, if your application is an MS Access app, then either RDBMS will be fine. You're not going to be using many of the advanced features, and as long as you're looking at MySQL 8 and PostgreSQL 10, it's largely a wash for small applications. 
Well aware of that, I've just used the tools most accessible to me and am already game planning normalization for a transfer.
I'm actually curious to know why? What are the drawbacks of this technique? Genuinely asking - I have not implemented this strategy but am curious. 
My current backend sits at 200MB, but it's 6x in size compared to a version only 3 months older (and yes I compress regularly). I am a bit confused by your acronyms (MS SQL vs. MSSQL?), but I assume you're comparing MySQL to PostgreSQL. Based on my reading's [here](https://www.digitalocean.com/community/tutorials/sqlite-vs-mysql-vs-postgresql-a-comparison-of-relational-database-management-systems), PostgreSQL seems to be a better fit for me due to its support of concurrency, larger support of date/time fields, and superior data integrity.
I'm far from the best person to answer this question, but it just seems risky.
Thanks for the links! PostgreSQL seems to be a better fit for me due to its support of concurrency, larger support of date/time fields, and superior data integrity. However, MySQL is portrayed as easier to work with as a direct import from Access. I'll keep reading.
Sorry, fixed my acronyms. Mssql is Microsoft SQL Server. It's comparable to postgres in the areas you've listed. Worth evaluating, particularly if you might want to look into future development where the wider ms ecosystem could be handy. Of course the biggest difference is that it is closed source and will require licencing once you get past the express size limit.
you need to load the driver with ' Class.forName("com.microsoft.sqlserver.jdbc.SQLServerDriver"); ' and the driver jar needs to be in your CLASSPATH
The person who first suggested this technique was a seasoned DBA. And lots of people after that have suggested this too. 
Fair enough, I am admittedly a n00b.
Surprised it's not mentioned but the best basic one out there in my opinion is https://www.w3schools.com/sql/default.asp You should have all the essentials nailed in a month of evenings. 
strings need quotes default 'vertical'
A couple of things: * Are you running the setup with administrator privileges? * If you downloaded the Release 1 for 12c, did you make sure you included both file downloads in your extract directory and extracted the second file after the first?
I'm not quite sure I understand the question. How do you want to determine which of the records in table Second that you want to return? If you need some practice with this type of problem, check out [https://sqlpracticeproblems.com/#utm\_source=r1&amp;utm\_medium=r1&amp;utm\_campaign=r1](https://sqlpracticeproblems.com/#utm_source=r1&amp;utm_medium=r1&amp;utm_campaign=r1)[SQLPracticeProblems.com](https://sqlpracticeproblems.com/#utm_source=r1&amp;utm_medium=r1&amp;utm_campaign=r1). I developed this course to give real-world, hands-on practice in SQL. There's lots of problems similar to yours. Contact me (email in the FAQ) if you have questions. Also, redditors get 30% off the Professional package (discount code reddit30). 
From the data given, there's no way to make that determination.
Oh my gosh. That was it, thank you so much.
Try the below: Select ID ,name ,place ,min(start_date) as first_start_date ,max(end_date) as last_end_date From people Group by ID, Name, place; If this doesn't work, then try changing your sample table above to match your real table more closely. If you're looking for some very hands-on "learn-by-doing" practice problems, that teach basic to advanced SQL with well-designed, real-world practice problems, similar to what you're trying to solve, check out [SQLPracticeProblems.com](https://sqlpracticeproblems.com/#utm_source=r1&amp;utm_medium=r1&amp;utm_campaign=r1). I developed it after teaching a SQL course where the material I had to to teach from was poorly structured and academic. Afterwards, the students emailed me, saying they needed practice problems, so I developed the course! Contact me (email in the FAQ) if you have questions. Also, redditors get 30% off the Professional package (discount code reddit30). 
Yup, for both It's really weird, because I don't get any error messages, so I don't expect to have any corrupt archives. When I think about it now, the only thing that comes to mind is some kind of boot error within the program, that is the only think I can think of to explain why I wouldn't get any message explaining which exception I am facing, but that can't be it! If this was the problem everyone would face it and the internet would be raging with complaints I am puzzled
Also make sure that the SQL Server is configured to allow TCP/IP connections. I think by default it is just named pipes and TCP/IP is disabled. 
If you have a firewall running then allow port 1433 to connect.
In your union, your second query is SELECT DISTINCT '5. 1001 and Above' AS "Bucket", 0 AS "Order Count" FROM ORDERTRANSACTIONS O This row doesn't exist; you added the "distinct" because you got 150 instances of it back. That happened because you selected from a table with no criteria, so got one output row per existing row. Instead, just do SELECT DISTINCT '5. 1001 and Above' AS "Bucket", 0 AS "Order Count" without a table. You're selecting constants, that should work fine. In your case statement, you also should test both the upper and lower bounds (i.e., WHEN O.QUANTITYSHARES &gt; 100 and O.QUANTITYSHARES &lt;= 400) I'd move the GROUP BY up to the first query prior to the union, since that's where the aggregation needs to occur.
 Not really an old school way. This is legitimately what MIcrosoft SQL server does to this day behind the scenes. You make a table change in the GUI. It creates a table with TMp_ in the name with your changes, copies your data from the old table into it, then drops the old table and renames the tmp table to the old name. 
Are you running mysql from console?
Thanks for taking the time out to help! I gave it a shot and it looks like I am getting the error message "ORA-00923: FROM keyword not found where expected". The data is pulling from the ORDERTRANSACTIONS table, but guessing the FROM line needs to be relocated? Here is the latest shot of the code: SELECT "Bucket", COUNT(\*) AS "Order Count" FROM (SELECT CASE WHEN O.QUANTITYSHARES &lt;= 100 THEN '1. 0 - 100' WHEN O.QUANTITYSHARES &lt;= 400 THEN '2. 101 - 400' WHEN O.QUANTITYSHARES &lt;= 800 THEN '3. 401 - 800' WHEN O.QUANTITYSHARES &lt;= 1000 THEN '4. 801 - 1000' END AS "Bucket" FROM ORDERTRANSACTIONS O GROUP BY "Bucket" ORDER BY "Bucket") UNION ALL SELECT DISTINCT '5. 1001 and Above' AS "Bucket", 0 AS "Order Count"/
Okay Oracle needs a table. Add a last line of “from dual” and your error should go away
Wow, cool! 
Try writing a second stored procedure similar to this: SELECT sp.FIELD1, sp.FIELD2, SUM(ot.FIELD3)... FROM (EXECUTE FirstStoredProcedureName) sp LEFT JOIN OtherTable ot on ... I believe that since the first stored procedure returns a table, you can join to it just like you would any other table.
Pretty sure you just need an: ELSE '5. 1001 and above' If you don't specify an else and none of the conditions match it should kick back a null.
I don't think so, it's from a website, codio, that's used by the class I'm taking 
I'm not an Oracle DBA, so there may be some way to log out the console execution prior to the OUI launching - but the only other thing that I can think of is to just ask if you're running the OUI from your root drive and not from some Downloads folder or anything of the sort sitting on a secondary drive. After that, not sure I can offer any other suggestions. 
Check out COUNT and GROUP BY. https://www.dofactory.com/sql/group-by
will not generate the desired 5th bucket, because there are no transactions over 1000
check out the exact same homework question here -- https://www.reddit.com/r/SQL/comments/8yblwk/oracle_need_help_with_including_results_with_a/
In addition to the count and group by recommendation, if you can change the format of the "DATE" field to a date format you will thank yourself in the future. It appears in it's current structure that it is not a standardized fact format and would take some massaging of the data to me able to order cronologicly as opposed to alphabeticly as the system will think it is a string. There is not currently a function to convert full month names to the standardized format. An additional issue would be seen if in two different years the event occurred on the same date, May 25 2017 and May 25 2018, as there isn't logic to differentiate the two.
Postgres would be something vaguely like Select date_trunc('DAY',timestamp) as time_stamp, count(distinct name) FROM ( Select --convert timestamp field to an actual timestamp from text to_timestamp("Timestamp",'MM/DD/YYYY HH24:MI:SS') as timestamp, "Name" as name From table_name)t Group By 1 Order By 1 desc Whatever syntax you want to go for you broadly want to convert that text timestamp into a valid timestamp for whatever DB you're using then aggregate by the day field of it and count the number of distinct names (or just count them if you're sure there are no duplicates). You might need to do something to pad the single digit months and/or days out as well. 
Would something like ROWNUM work for you? https://www.techonthenet.com/oracle/functions/rownum.php
Unfortunately not, because rownum = 200 would give no result set and i guess the where clause with rownum = 200 would be triggered before sorting, so its wouldnt be the right 200th value as i need the value after sorting. 
You want to add rownum to your first subquery. No need for a double nested query. select T2.* from (select count(X), rownum as rn from Y where ........ group by X order by 1 desc ) t2 where T2.rn = 200
I assume i will need the rownum in my group by clause in the subselect, in that case i would get always 1s from the count? 
Assuming your column names, as terrible as they are, are correct: SELECT "Date", COUNT(DISTINCT "Name (Last Name, First Name)") FROM UnnamedTable WHERE "Date" = ? GROUP BY "Date" 
 select col1, col2 from ( select ROWNUM, col1, col2 from Y order by myCriteria; ) WHERE ROWNUM = 200; 
What does the value in QUEUE represent? Is that the rider's position in the queue? Starting at 1 and incrementing?
SELECT DISTINCT title, email FROM employees
Yes the QUEUE represents the riders position but they are inserted out of order in the table. 
I get the same problem as in another replay already. If I select the rownum in the subselect I need to but it in my group by clause since I have a count in there. Because of that the data is wrong. It looks like that: rownum count(X) 1 1 2 1 3 1 4 1 5 1
Row number is an unreliable way to query the data. Why not query a unique id?
select queue,Name from tableName Order by Queue DESC; That whats comes to mind for me so feel free to try it.
Is count(*) the only value you're returning from the data set?
It is, yes. Although its not a count(*), its a count on a number-field its that makes any difference. 
Yes, i only want a single record. I want the count of the X-th(200 in that case) highest count of entries.
You want to select the count(x) from a bunch of different data sets? select count(x) from set1; select count(x) from set2; select count(x) from set3; Is that the 2000 data sets you'd mentioned? Take the count from each of the 2000 sources, then find the 200th highest?
Select ID, name, queue, max(weight)[weight] from table where weight &lt;=1000 Group by id, name, queue
I did edit a bit of a backstory into my post. maybe that help a bit to understand my problem. 
In your original description, you have `select count(x) from Y...` is Y a variable representing 2000 separate sets of data (tables), or is Y a single table?
Y is a single table.
Really have no idea why other commenters are making this difficult, this is just a simple group by. SELECT [Date], COUNT(*) FROM [Table] GROUP BY [Date] 
Assuming MSSQL - but I'm pretty sure every flavor has a corresponding function: I'd recommend checking out [window functions](https://docs.microsoft.com/en-us/sql/t-sql/queries/select-over-clause-transact-sql?view=sql-server-2017). It's a little wordy but just go down to the examples. You'll probably want to convert that "Date" column into actual date datatypes if they aren't, but ultimately unnecessary for this practice. `SELECT` `Date` `,COUNT(*) OVER (PARTITION BY Date) AS NameCount` `FROM` `YourTable` `--WHERE Date = 'May 06'`
You could use sql like this. The key is to find the running total of the weight, using a window function (the sum, with the over clause). Then I just show the highest Queue number, using the Top keyword. This is written for SQL Server, but most databases have this functionality now. ;With WeightTotal as ( Select * ,RunningWeightTotal = Sum(Weight) Over (Order By Queue) From ElevatorPeople ) Select Top 1 FirstName From WeightTotal Where RunningWeightTotal &lt; 1000 Order by Queue desc If you're looking for some very hands-on "learn-by-doing" practice problems, that teach basic to advanced SQL with well-designed, real-world practice problems, similar to what you're trying to solve, check out [SQLPracticeProblems.com](https://sqlpracticeproblems.com/#utm_source=r1&amp;utm_medium=r1&amp;utm_campaign=r1). I developed it after teaching a SQL course where the material I had to to teach from was poorly structured and academic. Afterwards, the students emailed me, saying they needed practice problems, so I developed the course! Contact me (email in the FAQ) if you have questions. Also, redditors get 30% off the Professional package (discount code reddit30). 
I wrote a post about it recently. https://www.reddit.com/r/SQL/comments/8wmsyc/pivot_and_unpivot/ 
Sounds like you want someone to write the query for you. That's not going to happen. You need to atleast attempt it and post the query for edits, here.
Hey thanks this is exactly the write-up I was looking for. Clearly explains in detail the type of info I need. Really appreciate it!
Guy above linked me to an exact write-up I was looking for. I tried a few things but the few sites that I had referenced before weren't the type of clarity I wanted. The actual data is much more complex than what I wrote in the post so I simplified it to get a clear concise response.
Read about: Count - Calculates the number of rows in the context of the query. You could count a name column. Group By - for aggregates like count, sum, etc. it identifies how to group the days. In your case by Date. You may also need Distinct (if you have duplicates by name for a date) and MAX (if people signed up multiple times and you want to only use the sign up for their latest Timestamp).
OK sounds good, thanks! I tried this and there are a few kinks to work out, but it might work.
This definitely will not work in most SQL platforms. You'd be much better off with user defined functions that return a table. 
Looks like you've got a homework assignment here, so I'll give you an idea to springboard but leave finalizing the proper select giving you the proper answer to you. You're going to be wanting to calculate a running total column. Several ways to go about that, but a google search or 2 should get you on the right track. The fanciest and most inclusive way of doing it involves the [Lag Function](https://docs.microsoft.com/en-us/sql/t-sql/functions/lag-transact-sql?view=sql-server-2017), but hardly seems necessary for a more simple problem like this Give this a try: select ID ,NAME ,WEIGHT ,QUEUE ,(select sum(WEIGHT) from Table where QUEUE &lt;= t.QUEUE) as RunningTotal from Table T order by QUEUE ASC Then you just go ahead and cut it off at 1000. You could nest it in another select and set it where RunningTotal is less than or equal to 1000, but you'll have issues with having an order by in a sub-select. I'll leave that for you to work through, plenty of google sources for works around to that, too. Good Luck
Did you try looking at google and stackoverflow?
So will there be 2000 values for x? select count(x1) from Y; select count(x2) from Y; ....etc? 
Minor point out that Queue should be ascending, 1st in line gets on the elevator. And it should be a list of all the Names that get to ride on the elevator (so list of names with queue position less than or equal to runningtotal of 1000). Also, very minor and nitpicky of me, but best practices has moved to [surrounding your top expression with parentheses](https://docs.microsoft.com/en-us/sql/t-sql/queries/top-transact-sql?view=sql-server-2017). Awesome help. Not so awesome linking to your own for-profit practice problems.
I would upsert the results of both stored procs into a new table. 
Yes, although the 2000 is just a randon number the amount of values will differ.
I'm with you - Until somebody sat down and explained it to me I had to google pivot table examples tons of times because none of the stack overflow posts explain how it works, just post it. Helps so much having somebody explain why it works so you learn how to use it yourself instead of just copy-paste. Don't feel bad asking for help. Also...probably the majority of posts on this sub are people asking for help and getting straight unexplained code answers to text questions.
I would go with the analytical functions, check the https://docs.oracle.com/en/database/oracle/oracle-database/12.2/sqlrf/DENSE_RANK.html#GUID-BB66F574-09DF-4594-87A4-ABD83E8DC3FE
What flavor of SQL are you using?
I see the error got resolved already. Welcome aboard, you'll run into a lot of weird errors like this one! Its a pain usually, and we'll be here when you need help!
I'd do it as a join - it allows to use the having clause (probably what the teacher was going for in this case)
Many thanks! Yea I've definitely had to utilize a lot of notes for small things like forgetting a space, a new line, or a semicolon. Thanks for the support!
Yeah, I think a self join is a much better and cleaner solution. But felt like this was a beginner SQL question and I wanted to make sure (s)he understood what was happening. I actually went back and forth on that 5th column being a joined table and this on which was easier to follow, but I thought less lines to read was easiest. I was probably wrong. But I also wanted to do as little as possible while still actually helping him get the answer he needs/wants, so they work through it and still manage to learn. 
Agreed. In T-SQL You can execute a proc into a table that has already been defined E.g Create table #Test (id INT) insert into Test execute proc You cannot do a select into though, you need to create the table first. Alternatively alter the proc so the last statement executes into a table rather than just returning a select.
Because programmers need to be clever more than they need even to be right.
For simplicity's sake, the first thing I would do is to have your script loop through the values for X and insert the count(x) into a separate table. Then run your rownum = 200 query against this new dataset. for( String x_var : X){ insert into newTable (x_count) (select count(x_var) from Y); } Then: select a.x_count from ( select rownum, x_count from newTable; ) a where a.rownum = 200; 
&gt; can a natural key be a better alternative than an int for lookup yes not always, but yes
Very few things in the real world areas unique and as stable as you'd like when modeling data. It's possible for a natural key to be a better choice than a surrogate, but it's extremely rare. The downsides of surrogates are that they can waste space and occasionally cause extra lookups in query processing. Downsides of natural keys is that they're frequently not actually keys, in that there are collisions, alterations to the value over time, etc. In addition, when used in FK constraints they end up needing to be copied to all referencing tables, which if the natural key is long can also be a significant waste of space vs. a single 4 byte int.
An attribute that is external to the DB can change, even if the data owner swears up and down that it never will. By definition you have complete control of the surrogate key, and in fact you should resist revealing it externally. To contrive an example, a widget composed of parts a, b and c might be known as widget a-b-c until one supplier changes part c to d, then you need to decide if the ID you are using for dates prior to the switchover should change or not (it's the same thing in all other respects). If they change then what of linkages to child tables that would otherwise be accurate? If you have a surrogate primary key it's a non issue. I don't see any other obvious concerns using one over the other.
 /r/techsupportgore 
Don't forget steering wheel, a circular device used by criminals to steal cars on streets, truck routes and even interstate highways.
TIL: I know cyber-trickery.
Lot of dumb things in this story, including that this [SQL injection was reported back in 2016](https://arstechnica.com/information-technology/2016/08/officials-blame-sophisticated-russian-hackers-for-voter-system-attacks/). But the ending takes the cake: &gt; "In the wake of the breach 2 years ago we installed new firewall hardware and software we devoted personal specifically to cyber security to shore up the system to make sure it wasn't going to happen again" said Dietrich. &gt; The SQL ploy has been used by hackers around the world to gain access to corporate databases, banks and government agencies. The keystroke trick usually exploits vulnerabilities that are commonly present in databases that allow unauthorized access. Nevermind the "keystroke trick". It's that the problem is described by the reporter -- and more importantly, was described to the reporter by the state officials/experts -- that this was a problem with the database itself, or even with the database access controls. The problem was that unsanitized code was read directly from a public web form. Though yeah, it definitely didn't help that admin tables had no access control and that admin passwords were stored in plaintext. 
Those devious bastards using SQL. SQL should be outlawed.
To be fair, when I lived in Chicago 10 years ago ABC/WLS-TV News sucked. Mark Giangreco was a douche. NBC was much better. 
And if "8 of 10" data breaches are due to SQL Injection, when they shored up their system security 2 years ago, why weren't steps taken to mitigate SQL Injection attacks? Guessing they just forced everyone to install McAfee and claimed they were secure.
If you outlaw SQL, only outlaws will use SQL. I always wanted to be [an Outlaw](https://www.youtube.com/watch?v=R82OM5tzcrk).
Tomorrow's headline: Donald Trump signs executive order banning SQL
FWIW, the Board of Elections published a breach report in 2016, and it does seem they aren't completely clueless (other than not testing their systems of course): &gt; Additionally, the server logs showed the database queries were malicious in nature – a form of cyber-attack known as SQL Injection. SQL Injections are essentially unauthorized, malicious database queries entered in a data field in a web application. SBE programmers immediately introduced code changes to eliminate the vulnerability.
Never underestimate the power of stupidity in public officials. I really think the ancient Chinese civil service exam approach should be updated when dealing with information exposed to the public. To ensure that basic basic basic security features like preventing SQL Injection attacks is understood AND they can implement the fix BEFORE people are allowed to manage said information. Not sure what is going on at Illinois but the people there in management positions are operating like they are from Somalia.
CNN: "Muh Russia" uses SQL. 
NoSQL!!!!
Right, like I probably skip using a surrogate key on [ISO 639-1](ISO 639-1) language codes like "en" or "fr", but that's quasi static lookup data anyhow, and the chore of de-referencing would drive me nuts when debugging.
Sorry, I was on my mobile. If you have SQL 2016 it is a native function. https://docs.microsoft.com/en-us/sql/t-sql/functions/string-split-transact-sql?view=sql-server-2017
I always knew SQL was trouble
#\#NotMySQL
Oy... honestly, why don't these reporters contact an expert before they write an abomination of a sentence like that.
We must hit them back. Quick, someone call little Bobby Tables!
That's it, I am changing my job title from "Developer" to "Cyber Trickster."
Right - in my current company, our main application is running on a 100+ terabyte db with a bit over 300 tables. Exactly *four* of those have non-identity "natural" keys: Language, Us State, Timezone, and a config table with internally managed key-value pairs for app settings. The first three only really exist to enforce data integrity in referencing tables and to populate dropdowns in the app, and the last isn't referenced by any FKs anyway.
You actually don't want a pivot here. You want 3 sum case variables sum(case when transaction_date between 1/1/2007 and 12/31/2007 then 1 else 0 end) as CY2007 count Group by transaction ID Careful about mixing up Calendar year and fiscal year. 
I found out I was a cyber-hacker today. Guess I better turn myself in.
Note: the paragraph in question has been changed: &gt; SQL, an acronym for Structured Query Language, is a database programming language. An "SQL injection" is a common piece of cyber-trickery used to illegally gain access to government, financial, business and private computers. Experts estimate that 8 of every 10 data breaches occur as a result of SQL injection. It does honestly look like an editor screwup, i.e. the attempt by the editor to streamline all that "technical mumbo jumbo".
To add on to this. If you’re using netbeans you can right click on service and add the driver. It will load without having to type any code. Here’s the SQLlite driver if you’re using netbeans. http://plugins.netbeans.org/PluginPortal/faces/PluginDetailPage.jsp?pluginid=16018
SQLoki
(after joining t3...) LEFT OUTER JOIN ".$wpdb-&gt;prefix."postmeta as t4 ON t1.post_id = t4.post_id AND t4.meta_key = 'NO_LIST' WHERE ISNULL(t4.meta_value,'') &lt;&gt; 'on' AND t1.meta_value = $c_post_id and t2.post_type='STATE' (continue as is from here)
Wait until they hear about what little Bobby Drop Tables has been up to.
Wait till this investigator finds out I use SQL to access their patient medical records.
They must have updated the article or you misquoted for karma but the article now reads - &gt; An "SQL injection" is a common piece of cyber-trickery used to illegally gain access to government, financial, business and private computers. 
LOL "Trump announces the new Space Wars V2 Defense System to shoot down SQL attacks"
They added a note at the end explaining the change.
They changed that after the submission: https://twitter.com/matt_kiefer/status/1019987687548379136 
The most robust solution is going to be using analytical functions like `row_number()`: --Returns 200-th most frequent value of X in TABLEY select X, CNT from ( select X, CNT, row_number() over (order by CNT desc) RNK from ( select X, count(1) CNT from TABLEY where ... group by X ) ) where RNK = 200 If you're on **12c** , you can use the new `FETCH` and `OFFSET` keywords: --Returns 200-th most frequent value of X in TABLEY select X, CNT from ( select X, count(1) CNT from TABLEY where ... group by X ) order by CNT desc offset 199 rows fetch next 1 row only ; 
The use case described in the stackoverflow example could be handled as a Slowly Changing Dimension (Type 2). I would include an "is_current" flag.
These are the same people who write anti-gun stories and don't know the difference between a clip and a magazine, or what "automatic" actually means.
Declare @lastday as datetime Set @lastday = your query
It's going on my resume right now, "Experienced cyber trickster"
Next up “nobody need assault SQL” Illinois bans SQL IDEs
How to become a SQL expert: 1) Do your own homework.
What have you tried?
Thank you for all your replies!
I bet those cyber criminals also use coding and algorithms we keep hearing about.
Don't go down that road. Every day, I turn all kinds of tricks, one by one (although I call them my "relations" in polite society). If you just want a good view of a FULL JOIN, that's cheap, but I charge extra for INSERTs and GROUP BYs, and even more for full dumps. (If you're into something super obscure and hardcore, like Tutorial D, we'll have to negotiate. I seem to recall that one of their principles is "every hole has to be filled," and that makes me nervous.) Being able to use hash and gin helps me a lot, but I feel even worse for the people just getting into the biz. They have NO standards these days and will just stick anything anywhere. I mean, I know everyone likes a little strange dict now and then (or maybe something extra wide), but most of the new generation don't even bother to CHECK what kind of weird objects they're shoving in, and SOAP is basically a thing of the past.
They're on to us! Better shred my SQL pocketbook that's sitting at my desk. 
Reporters never contact experts. If it's an article on a topic you know about, it's almost always awful. For example, the recent stories about the kids trapped in a Thailand cave? Most of the technical SCUBA diving detail in those stories was wrong, dangerous, or dangerously wrong. Remember that when reading stuff you don't know about.
I agree with LetsGoHawks. I'll give you a hint though: check the chapters about HAVING and CASE in your course book.
Try a Row_number partitioned by customer ordered by purchase date, in either a derived table or cte. Then filter to row number =1, which will give you the first purchase row for each customer (assuming one product per purchase, otherwise try the rank function).
Also, the year/convert isnt great for date maths, so try just &gt;= '20160101'
[removed]
Varchar
Given varchar is half the size on disk per character the answer should be fairly obvious but your comparing two different datatypes created for two different purposes - one supports Unicode and one does not. Its like comparing the performance of integers and decimals. Its a pointless comparisson. The real questions you need to ask are: what are pre-existing data types used? if its mostly nvarchar for instance use that. what data are you storing? Do you need to store weird characters? IE chinese or russia language? Then use nvarchar If theres no pre-existing types and your just storing alpha numerics then use varchar. Also is there a reason you could not type 'varchar vs nvarchar performance sql' into google yourself? Seems like a very easy to answer question if you put half a thimble worth of effort into typing into google.
Thanks. Figured it'd be simple. 
And
And o
And of
And of
They corrected it, thankfully. Still cringe-inducing. &gt;SQL, an acronym for Structured Query Language, is a database programming language. An "SQL injection" is a common piece of cyber-trickery used to illegally gain access to government, financial, business and private computers. Experts estimate that 8 of every 10 data breaches occur as a result of SQL injection.
You could create a variable that is “today” minus 6 weeks, and then have the query pull data greater than the variable. Not exactly sure what you mean about displaying the data by weeks, but it sounds like you should like into GROUP BY. And you can “pivot” within the query, but I always have to look up examples online and then play around with the query and look at the results. I’m not familiar enough with it to explain it without examples to look at. Anyway, the only database I’ve actually used in a professional setting had all kinds of funky date types, that I was forever converting, so someone with more knowledge may have a more elegant solution.
I'm always in favour of calendar tables for this kind of reporting need. Just make a table of dates, pre-populate with as many years as you feel is necessary,and you can produce all the attributes you like for reporting periods, reporting weeks, how to label dates for different displays/aggregations, etc. Then it's all centralised and reusable across reports, and makes reporting queries far simpler.
You could hack it by putting a lot of date functions together, but I think you'd be best off creating a calendar/date table in SQL, and using that in your report. That'a a robust solution, and once you have the calendar table, you can use it for multiple reports without having to repeat all kinds of complex date functions. Search for info on calendar/date tables, there's plenty. If you're looking for some very hands-on "learn-by-doing" practice problems, that teach basic to advanced SQL with well-designed, real-world practice problems, similar to what you're trying to solve, check out [SQLPracticeProblems.com](https://sqlpracticeproblems.com/#utm_source=r1&amp;utm_medium=r1&amp;utm_campaign=r1). I developed it after teaching a SQL course where the material I had to to teach from was poorly structured and academic. Afterwards, the students emailed me, saying they needed practice problems, so I developed the course! Contact me (email in the FAQ) if you have questions. Also, redditors get 30% off the Professional package (discount code reddit30).
What version of Access? You should just be able to change the field type to date/time in the source table. Remove the control/ field from the form and add it again. Newer versions of Access automatically give you a date picker on forms.
Yes.
Further to a\_s\_clark's comments, you might use a CTE because you want the columns are correlated. This is similar to an inline view, just easier to read and reusable in the query. I have put together an example from your data. [https://www.db-fiddle.com/f/3VpPMqQcncy1X8cGrb4A8M/13](https://www.db-fiddle.com/f/3VpPMqQcncy1X8cGrb4A8M/13) a\_s\_clark also makes a good point that you should never try to apply a function to a query restriction since it is virtually guaranteed to defeat any index you might have (unless, of course, you have created a function index). Even if you don't have an index, it's a bad habit that might bite you in the butt later as your application grows.
By this same logic keyboards and operating systems are similarly liable. Dang keyboards... emboldening hackers since... IDK the implementation of ARPANET.
Ignoring performance for a moment - which is the appropriate type for your requirements? `nvarchar` stores two bytes per character, so on-disk and in-memory it uses twice as much storage. So if that's a significant concern **and you have no need to store non-ASCII characters**, `varchar` may be the way to go. A greater concern for performance is performing conversions (especially implicit conversions) between data types - joining two tables where the field is `varchar` on one and `nvarchar` on the other. Conversions aside, you shouldn't see a significant performance difference either way in most cases.
But I don't think I'll need to support unicode characters 
Sorry, even adjusting for english not being your native language, this was hard to understand. I think what you are trying to achieve is to get minimum date per id (cohorte) and create N rows (4 in your example and 25 in your actual task) and have an 1 indicator on first K rows, where K is the total number of records per id in the original table. You can do this, for example, by getting the min date and count of records per ID and then unpivot to the required number of records (use a sequential number to order your "periode" records). The indicator ("periode") is a case expression on the sequential period num and the number of record per id. How to unpivot: https://www.reddit.com/r/SQL/comments/8wmsyc/pivot_and_unpivot/ 
&gt; What version of Access? I suppose it's a current version. I have the Microsoft 365 subscription. I've only starting learning SQL for about 3-4 weeks, and have only dabbled in Access for about half of that. I'll look a bit more closely at that then. Thank you!
I had to do something like this in the past, which involved setting up some VBA behind the button click to do the inserts and trigger a refresh of the output tabs. 
you can do it with VBA
I’ve used vba before so I know I can do it, but I don’t know sql yet, which part of this process is just vba and which part is sql, many thanks.
I’ve used vba before so I know I can do it, but I don’t know sql yet, which part of this process is just vba and which part is sql, many thanks.
you know what's better than "easy steps" books? da manual https://dev.mysql.com/doc/refman/5.5/en/create-table-foreign-keys.html you have to specify the target column -- CREATE TABLE shipments ( s_no CHAR(3) , p_no CHAR(3) , qty INT NOT NULL , PRIMARY KEY (s_no,p_no) , FOREIGN KEY (s_no) REFERENCES suppliers (s_no) , FOREIGN KEY (p_no) REFERENCES parts (p_no) );
The problem is some of the lab tutorials provided doesn't work half the time, there's also some other questions i would like to ask that doesn't make sense to me, is it ok if you can PM me as I'm really struggling ?
Couple of things look odd. First: a.current_year = b.current_week Are you meaning to join on the year being equal to the week? And second, there's no mention of UPC in the join - which I imagine means that all values in the same week of this year and last are being returned.
Thank you so much! i have changed some of it but prior_AMT doesn't seem to work. select a.UPC10, a.POS_AMT, a.current_year, a.current_week, b.POS_AMT as prior_AMT from ( select UPC10, current_year, current_week,SUM(POS_AMT) as POS_AMT from [STAGING].[NETBENCH_ULTA_SALES_UPDATED3] group by UPC10, current_year, current_week )a LEFT JOIN ( select UPC10, current_year, current_week, SUM(POS_AMT) as POS_AMT from [STAGING].[NETBENCH_ULTA_SALES_UPDATED3] group by UPC10, current_year, current_week )b ON a.current_year = b.current_year + 1 and a.current_week = b.current_week
I'm fixing some of that from the guy before me. Might be worth looking into business intelligence (qlik, powerbi, tableau) or an ssrs report. It can be done but when I started I found a bunch of problems and have advised against such a system because it can get out of hand fast.
I'd change your query to have 2 well-named CTEs, that can be tested easily and **separately**. Then, join the CTEs. Something like this: ;with CurrentYear as ( Select UPC10 ,current_year, ,current_week ,SUM(POS_AMT) as POS_AMT From [STAGING].[NETBENCH_ULTA_SALES_UPDATED3] CurrentYear ) ;with PreviousYear as ( Select UPC10 ,current_year as PreviousYear ,current_week as PreviousWeek ,SUM(POS_AMT) as POS_AMT From [STAGING].[NETBENCH_ULTA_SALES_UPDATED3] PreviousYear ) Select CurrentYear.UPC10 ,CurrentYear.current_year ,CurrentYear.current_week ,CurrentYear.POS_AMT ,PreviousYear.POS_AMT as PreviousPOS From CurrentYear join PreviousYear on PreviousYear.PreviousYear + 1 = CurrentYear .CurrentYear and PreviousYear.PreviousWeek + 1 = CurrentYear .CurrentWeek and PreviousYear.UPC10 = CurrentYear.UPC10 If you're looking for some very hands-on "learn-by-doing" practice problems, that teach basic to advanced SQL with well-designed, real-world practice problems, similar to what you're trying to solve, check out [SQLPracticeProblems.com](https://sqlpracticeproblems.com/#utm_source=r1&amp;utm_medium=r1&amp;utm_campaign=r1). I developed it after teaching a SQL course where the material I had to to teach from was poorly structured and academic. Afterwards, the students emailed me, saying they needed practice problems, so I developed the course! Contact me (email in the FAQ) if you have questions. Also, redditors get 30% off the Professional package (discount code reddit30). 
So first step, type that title into bing/google/duckduckgo
Give us the tables and what you've tried so far and some nice person might give you some advice.
We can better help you if you A) provide us with information about the tables you're using and B) show us the queries you have written thus far and let us know where you're stuck.
Hi, thanks but it doesn't work at all. Does it work on microsoft sql server? 
You made a brand new account for this? No credibility, no tables, no help.
you probably want to join on UPC as well, and I'd drop the current year = current week specification--it doesn't really make sense. select a.UPC, a.current\_year, current\_week, a.POS\_AMT, b.POS\_AMT as prior\_AMT, a.POS\_AMT + b.POS\_AMT as TotalAmount from \[tablename\] a inner join \[tablename\] b on a.current\_year = b.current\_year + 1 and a.UPC = b.UPC \--and a.current\_week = b.current week Uncomment (remove the --) from the above if you want the sum of sales for this week last year and this week this year.
You can but I wouldn’t, as others have stated look into other ways, we started using power bj and it’s so much better. Excels a thing of the past.
It can be done with VBA and ODBC. I write many reports that have a 'refresh' button that calls a query to an oracle database to odbc and puts the results on the sheet. One trick you can use for this to eliminate a lot of the headache of custom code, while at the same time adding more complexity to the process and additional software requirements, is to create a pass-through query in Access that stores the exact SQL you want to run. Then you simply add data to your sheet using Access as your connection, and VBA for the refresh button is as simple as refreshing your data source. In my case Access is the program that is doing the ODBC call behind the scenes. Do all of your clients have Access installed? If so it opens up more SQL possibilities. 
That worked but if i select the new table it shows empty set? Im guessing because it doesn't have data in the new table but how do i show the joined tables? 
not sure what you're asking here could you rephrase?
sorry, i don't do PMs... just post your questions here
I've got a different questions, are you available to help me?
if they are related to this thread, sure if they are about different tables, please start a new thread
What do you have so far? (Did you at least try to write queries for these?)
Yh didn't know where to post. I'll make a new post.
Try /r/domysqlhomeworkforme
1. Select Count(1) from... where date between ... Group by [key]. 2. Select product, avg(revenue) from... Group by product, order by 2 3. Similar as 2. 4. Use Window function 
Yeah, it's called the "Refresh Data" button? Why make fake buttons to do what's already native in Excel? Create a data connection for your query, put it in a table, create a pivottable off that. When you hit Refresh, it automatically reruns your query and populates your table. Even if you have 10 queries, same thing. One button, 10 refreshes, or you can pick and choose to just refresh one query if you want.
No. 1) Non relational databases and relational databases fulfill different roles, and one is not better than the other, so one will not make the other obsolete. 2) Massive companies like Microsoft and Oracle pour money into developing SQL DBMS's, and will not simply drop it. 3) Cloud computing just means computing on someone else's computer. That computer, if working with large amount of data (probably will be), will likely be support by a SQL backend
Keep in mind that this is the same industry where "COBOL and the mainframe are obsolete" has been uttered for over 20 years. Yet they're both still around and both going strong. At work, I make use of both Redis and SQL Server. They both have their place in the world, and they serve entirely different purposes. Microsoft announced Cosmos DB last year (really just a re-branding and enhancement of Document DB) and while it can serve *some* of the purposes of SQL Server/Azure SQL, it's not a replacement for it. Even Microsoft will tell you that. SQL databases aren't going anywhere any time soon.
Thanks fauxmosexual, here is the database my professor gave us. [https://www.w3schools.com/sql/trysql.asp?filename=trysql\_select\_all](https://www.w3schools.com/sql/trysql.asp?filename=trysql_select_all)
No.
It's worth adding that BigQuery pretty much uses SQL syntax. Maybe SQL administration declines in value as compared to cloud administration but the language is not going anywhere anytime soon.
Yes it does work on SQL Server. CTEs (common table expressions) work on most major DBMS. MySQL just got them in the latest version (8.0). If you're using an older version, just switch to derived tables just switch to derived tables as you have above - a little harder to read, but kind of similar in functionality.
Have you attempted the question at all or are you looking for someone just to do all your homework for you?
This is awesome
We use SQL in the cloud, too, dude ...
Yes i have, i've completed 16 of the easier questions, just stuck on these last 4. We've just started working on SQL yesterday and the professor is confusing as shit. 
Make an attempt and post where you get to.
it works now! Thanks! 
Works now!! Thanks!
Alright, it'll have to be tomorrow, I'm working til 2am tonight😖
Just going to add that SQL is a great foundation skill. Don't get discouraged. Even if you don't end up using SQL directly, it can get you into a mindset about how you need to do work with data.
Can you please indicate where to add the "from dual" line?
The "from dual" would be the last line in the script posted above. BTW, if you want to the know the purpose of dual, [read this Wiki page](https://en.wikipedia.org/wiki/DUAL_table)
**DUAL table** The DUAL table is a special one-row, one-column table present by default in Oracle and other database installations. In Oracle, the table has a single VARCHAR2(1) column called DUMMY that has a value of 'X'. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/SQL/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
As others have said, no SQL will not become obsolete in any foreseeable future timeline. That being said, I do very much wish there was an alternate standard developed for the relational model which was developed from scratch, filling in all the weak points in SQL. SQL is awesome, and I am working in it the majority of my work week...there are some pretty strange decisions / gaps in the standard that make things harder than they need to be though.
No and no. 
What weak points do you mean? I’m guessing I just haven’t used it long enough to have found them.
Too much sql data out there.
You may want to test in a [fiddle](http://sqlfiddle.com).
And usually each SQL won’t explain why they didn’t implement a specific relational algebra filter, so for each x feature it could be either “we didn’t want to spend 100s of man hours building something &lt;1% of our users use,” or “we were philosophically opposed to x feature, because it (conflicts with other feature || is janky || other reason).”
Ive been hearing that for years and it was funny when I heard it and still funny after I learned how NoSQL works (anyone who ever said that has no working knowledge what each one does)
No. Even systems that did not support SQL in the beginning are now starting to support it. E.g. recently Elasticsearch: https://www.elastic.co/blog/elasticsearch-6-3-0-released Further, SQL is not limited to the relational model anymore. SQL-92 was the last standard the embraced the purely relational model. SQL:1999 added non-relational concepts such as arrays, compound types and the like. For the avoidance of doubt: this was really added to the SQL language itself—not an procedural extension like PL/SQL.
noSql will die. Elephants and dolphins will not
Look into attunity replicate, it has a free express version to experiment with. https://www.attunity.com/products/replicate/platform-support/ Familiarize yourself with CDC (change data capture) Pm me if you want to chat more about it.
Just out of curiosity, why don't you want to use replication? Sounds like just the sort of task it was intended for.
Depending on the size of the company, you’re either hacking something together, or completely out of your league. Rather than asking here, you should be consulting with a software architect. 
No. 
HEY thanks for the reply but i have been thinking lets say 1. I choose the method of creating a ticket whenever am creating an event e.g an organiser may need 10,000 tickets to be sold won't the insert be abit expensive on the database? 2. Can't I use the same analogy of PRODUCT - ORDER - ORDER DETAIL..Maybe i can have a TICKET CLASS of type lets say EARLY BIRD quantity column maybe 10000 tickets..and i can be locking the quantity e.g a USer may purchase 1 ticket..i delete that from the quantity and maybe create a lock of time in the order table if it expires i can return the quantity Can this Method work? THanks 
I know you did explain but i wanted to ask if this other approach can be valid
If you’re talking MS SQL Server, merge replication is a native feature that could do this. 
The ticket rows are preloaded into the DB. The interface blocks off all tickets marked as sold. When user purchases their tickets, a lookup is done to see what matches the user's choice, and what is still available. The system claims the rows, which are then ineligible for purchase to anyone else looking. If the first user backs out, or the lock expires before purchase, the ticket rows become available again. If the purchase goes through, the row is updated to reflect that it's no longer available permanently.
If someone releases a dramatically better language, maybe SQL's almost the oldest language you can name. That survivor bias exists because this language is dramatically difficult to replace. I don't forsee this happening in the next 15 years.
 This is what I have so far, any thoughts? 1. select CategoryID, count(\*) as Products, avg(price) as Average\_Price from \[products\] 2. select CategoryName, count(\*) as Products, avg(price) as Average\_Price from \[products\] 3. SELECT \* FROM \[Products\]Order By Revenue ASC
 This what I have so far, and here is the database I'm working with. [https://www.w3schools.com/sql/trysql.asp?filename=trysql\_select\_all](https://www.w3schools.com/sql/trysql.asp?filename=trysql_select_all) 1. select CategoryID, count(\*) as Products, avg(price) as Average\_Price from \[products\] 2. select CategoryName, count(\*) as Products, avg(price) as Average\_Price from \[products\] 3. SELECT \* FROM \[Products\]Order By Revenue ASC
Yessir, I figure it's cheaper then paying $60 per hr for a tutor. Here is the database I'm working with. [https://www.w3schools.com/sql/trysql.asp?filename=trysql\_select\_all](https://www.w3schools.com/sql/trysql.asp?filename=trysql_select_all)
This is what I have so far and here is the database. [https://www.w3schools.com/sql/trysql.asp?filename=trysql\_select\_all](https://www.w3schools.com/sql/trysql.asp?filename=trysql_select_all) 1. select CategoryID, count(\*) as Products, avg(price) as Average\_Price from \[products\] 2. select CategoryName, count(\*) as Products, avg(price) as Average\_Price from \[products\] 3. SELECT \* FROM \[Products\]Order By Revenue ASC
Can you write it out using the table names from this database so I can get a better understanding plz. [https://www.w3schools.com/sql/trysql.asp?filename=trysql\_select\_all](https://www.w3schools.com/sql/trysql.asp?filename=trysql_select_all)
Sorry, I was vague for the same reason as everyone else (not to hand it to you on a plate). Generic help: * Count, Avg, etc. are aggregate functions. You need GROUP BY clause to use aggregate functions. You can only display columns that are either specified in GROUP BY or have Aggregate function. * Joins are easy. You use (INNER) JOIN when the information you seek is in both tables, LEFT JOIN if it doesn't have to be in the right table. * Normally you can ORDER BY column name, but with aggregate function you would repeat yourself, so you can order by column number. * You can limit the number of returned rows with the TOP [number] after SELECT, but you also need ORDER BY. P.S. While you can do #4 with a window function, you don't have to. Normal GROUP BY should work here as well.
what college class is this?
 select 'homework' from mainTable
I won't give you the answers, but I've given questions on each to try to help you step towards the answer. I'm happy to help you step through this. &gt; Write a SQL query that displays the number (count) of orders placed during December 1996. I don't see a query above that filters on date. Think about how you'd determine if an order is placed in a given date range. &gt; Write a SQL query that lists the products' CategoryID and the average price of the products in each category. Order the results by the category with the highest average price to the lowest. So you have select CategoryID, count(*) as Products, avg(price) as Average_Price from [products] You have two aggregates -- count() and avg(). What do you want to calculate the average *on*? What clause tells an aggregate how to group rows and determine what you want to count() and avg()? You don't want to average *all* the prices in one shot, you want them by category. How do you tell it that? (Hint: Look up SQL aggregates) &gt; Write a SQL query that lists the products' CategoryName and the average price of the products in each category. Order the results by the category with the highest average price to the lowest. Get the query above this one working, then how would you convert Category ID to a Category Name? &gt; Write a SQL query that lists the name of the 5 products generating the highest revenues along with the revenue generated by each. How would you calculate the revenue for each? Think about what you're trying to do, then translate it into SQL. What elements do you need and from what tables? Then, how do you join those tables together to get what you're after?
It's not really an assignment, it's just lab sessions but since uni is ended i can't ask the tutor since they aren't available and his own lectures aren't that helpful. The problem is i can't find anything similar that is in the format like select column and then do match words with characters and then name them using AS. I figured out the summation one just had to use IN to specify different id.
That doesn't make sense. No trolling please.
It's not clear from your question what you're trying to do or what actual problem you're having.
As long as the columns are in the right order you can just edit the table in SSMS, copy the rows from Excel, and paste in SSMS.
&gt;edit the table how can i edit ? the table has more than 400k of rows and i can see the only option is "edit top 200 rows"
i have an excel that includes the missing data in ms sql server database 
It sounds like you're trying to import directly from your excel file into the target table and running into various column restrictions. Try importing your raw data into a temporary table first, and then copying from there to the target table. (You know how to copy rows from one table to another, I assume?)
Read a guide on SSMS. These are some intro level things and if this is for work you don't want to be messing with things you do not understand. It will help you greatly. Edit Top 200 shows the top 200 rows. The last row is where you can manually add days or in this case paste it. There's a Show Query button if you want to remove the 200 restriction or other things.
give it a try, OP, and we will help you get over any errors you might encounter
I don't understand the last question, does it want me to do different commands for min, max? Or to output all of them?
This should be easy. Is there somewhere you can post more detail and sample data etc.? 
Desc TARGET_TABLE and share. Share a sample of data types from the excel file.
Let me know if this is correct... You have an Excel file with 800 rows of data. You would like to import that Excel file into an MS SQL database.
one SELECT query producing a single row with three columns
Reddit messes up numbered lists so I assume those are for questions 2, 3, and 4. For the second one you're on the right track: you just need a GROUP BY and ORDER BY clause. The group by is to tell the query what you're averaging or counting over (in this case category ID) and what order you want to see the results in - avg(price). The category name question is trickier because the name isn't in the product table. It's the same query, except you need to JOIN the category table to the products table. Again you want to include the categoryname in the GROUP BY. The final one is much trickier: revenue doesn't exist in the database so you need to calculate it. Join Products to OrderDetails, then multiply the quantity from the OrderDetail table to the price in the Products table - that's your revenue. On top of that you need to then sum that calculation, with a GROUP BY the product name from the products table. Finally, you need to use ORDER BY on that sum, and start the query with SELECT TOP 5 to limit it the first five. 
I mean, as others have said here, replication sounds like the best option. A web service, etc just sounds like you're trying to reinvent the wheel. Depending on what you're doing with that data, a linked server could be another option, but it really depends on what you intend to do.
 select * from mainTable order by itemId asc; select itemID, ('brand.Name mainTable.Model briefDescription.Name') as AvailableDeals, round(mainTable.Price, 2) as RRP, case when price &gt; 800 then round((price * 0.93), 2) when price between 350 and 800 then round((price * 0.95), 2) else null end as SalePrice from mainTable; select itemId, sum(price) as "Calculated total" from mainTable where itemId in (31,27,24,2) group by itemId; select max(price) as "max", min(price) as "min", avg(price) as "avg" from mainTable; If you understand these queries (actually understand them, not just blindly copy them), the only other main part of SQL to learn are JOINS.
Nosql database solutions will die much quicker than Sql will.
Internets. Serious business.
A lazy way I'm fond of is to use a feature on SQL fiddle: Go to http://sqlfiddle.com Change the option in the top left from MySQL to MS SQL Server Click on Text To DDL near the top. Paste your data from Excel into the new window, putting the name of your actual table in the table name box, then click Append to DDL. It will create a CREATE TABLE statement - ignore this part. You want the INSERT statement with the 800 VALUES. You'll probably have to fix the naming to make sure you're inserting correctly, then you can just run that statement on your SQL server to insert the values. *** BONUS TIP *** If your table has an identity column, you're going to want to start your query with SET IDENTITY_INSERT [your table name] ON
SQL isn't for everyone
While I loathe Replication especially Merge Replication with SQL Server... it sounds like a good fit - assuming you're using MS SQL Server. Another option might be CosmosDB, depending on if it's a home built app and if you're on Azure.
Dude check information_schema.columns to build a dynamic Sql statement! It’s the wave of the future!
DUDE GOOGLE DAT DML SWEETNESS
Here ya go 1 open ssms 2 type a sql statement 3 hit f5 4 profit
Sorry, i realise the question is worded wrong. Im confused how to use the correct syntax commands, i don't understand the discount part, i can't find similar code or examples.
everything after "NOTE:" is one line in a CASE statement. 
Dude buy the beginner sql book by itzik Ben Gan. All the info is there.
That's makes so much sense, i actually figured the summation one since all i had to do is use IN function and add the id in brackets. It's just how the question is written get confusing since it's kinda does gives hints. Thank you so much i appreciate it. 
That's dead interesting. Do you mind if I ask how you know? I'm asking purely because I work for a separate software company that provides ticketing that specifically does not preload tickets on the DB (they are created only when a ticket is bought/"created").
SQL, not even once.
+1
What is script and why does it cause multiple connections?
This is an eye opener , I'll try it Thank you so much &lt;3
its a python script that initialises and runs the flask server,boots up the sensors etc 
Kinda true , if i want to create new table ill have to take the whole table with around 400k rows add to them the 800 and I should import that file to the database But my software didn't read the new table because the old one has primary and secondary key and I'm really so new to sql
Well , this sounds like a good idea I'm sorry but I don't know how to copy rows from a table to another Can you tell me how ? Or what to read? Or where to start? Thank you.
then why i, who's not writing any script now, have multiple connections (or high connections) so I was automatically blocked as a bot? do you know why?
Hmmm, away from my desktop so can’t test this but I would expect the join to be a.current_year = b.current_year + 1 and a.current_week = b.current_week and a.UPC10 = b.UPC10 let me know if that doesn’t fix it!
I believe you need to add the “else” part on your case statement. So after your first condition you would have ... “else 0 end) as commission”. This would be where you add your different levels of commission.
I'd suggest a sub query, cte or temp table. Then use a case statement once you have a set with the account and number of transactions.
SQL snapshot isolation my friend, good article on it [here](https://www.brentozar.com/archive/2013/01/implementing-snapshot-or-read-committed-snapshot-isolation-in-sql-server-a-guide/)
Try this case statement: CASE WHEN COUNT(O.TRANSACTIONSID) &gt;=10 THEN COUNT(O.TRANSACTIONSID) * 5 ELSE COUNT(O.TRANSACTIONSID) * 10 END AS "Commission" Hope it helps!
Yes this worked perfectly! Thank you for the help!!!
IRL: press F6. "GODDAMN DEBUGGER!"
https://www.coursera.org/lecture/sql-for-data-science/module-introduction-GAA9h
So to be honest, if you don't have any knowledge of SQL at all you *probably* shouldn't be messing around with this -- certainly not if it's a crucial table you're trying to import to. If these are skills you need for your job or whatever, look into an introductory MS SQL course. But to answer the question, it would look something like this: INSERT INTO dbo.YourTargetTable (col1, col2, col3) SELECT col1, col2, col3 FROM dbo.YourTemporaryImportTable For the first list of columns (in parentheses) you specify the columns in the target table you are writing *into*, and for the second, the columns you are reading *from* in your temporary import table. 
I should add that I'm looking to do it this was so I can filter the information in PowerBI by having for example users that do Workout A and then seeing what the totals and ranking which was the most popular Workout B for those users. That way the hard coding of the Row number is important.
 SELECT ... FROM ... WHERE futuredate &gt; CURRENT_DATE
have a small test database which is representative (i.e. includes all varieties of data, including outliers) check by inspection
A couple of things.. if you only expect to have exactly one record of something, verify that your result set only contains one of them. Also, if you don’t expect joins to change the number of records in your result set, select row count with and without your join and make sure they match. 
OP is using SQLite, not SQL Server.
With MySQL you will have to use a variable to achieve the same outcome with the row_num http://www.mysqltutorial.org/mysql-row_number/
Thank you for guidance
Thank you for your input and explanation. Syntax works like a charm!! :) i will do further digging into CTE
"Golden transactions" Where you have a single record (or small set of records), which you trace through the ETL process and verify the resulting values match your manually-computed/expected values.
Here's an easy way to start to see if there's missing records. `select count(1) from my_table ` then compare to `select count(1) from my_table join another_table on my_table.id = another_table.id` You should see the same number of rows as in your first query if you are not missing any join keys. There's a lot more to do here - happy to give suggestions if you have examples of certain types of discrepancies you'd like to check. 
The migration towards cloud computing? As in, Amazon Redshift, Google Bigquery, Microsoft Azure SQL Server? They all use Standard SQL... Shit, Redshift is technically a column-store NoSQL database, and it's based off Postgres 8.0. 
Not just pretty much, right? BigQuery has wisely stepped in favor of #standardSQL which IS SQL syntax. 
+1
can also use NOW() instead of CURRENT_DATE in the case where you care about future entries which are still of the current date, but past the current time. 
Interested what you include in your load tests/resources for load tests? 
I'm quite comfortable with querying data. Any pointers to resources on how to actually set up the DB? I.e. how to create rollup/aggregation tables for analysts, doing so on automated basis, data replication pipelines, etc.
I am interested in open biohacking information if you could provide some links? 
Select *, Row_number() over ( partition by [distinct list of fields that create a primary key] order by 1) as row into #test From [table] Select * From #test Where row &lt;&gt; 1 Alternatively just start counting * vs counting distinct * and see if you get the same values
Setting up a database is a lot different than the analytics side. The requirements for an OLTP database are vastly different than those of a reporting database. Depending on the scale of the database, it usually makes sense to have two separate systems: the OLTP system and the reporting/BI system. 
r/https://www.youtube.com/watch?v=b2F-DItXtZs You should watch this video
Sorry for the late follow up. I didn't download "databases" per se but pulled data via .csv from places like [https://www.baseball-reference.com](https://www.baseball-reference.com) and [https://www.pro-football-reference.com](https://www.pro-football-reference.com) and created SSIS packages to import the data into my SQL databases. I haven't found any databases that do the same however I will admit that I haven't looked either. I was also working on building SSIS packages at the time so I wasn't looking for databases at the time.
It would be helpful to show the execution plan you're actually getting. We're flying blind without it, so here are my guesses: First, I'd write this as one flat query instead of a bunch of UNION ALL queries. My guess is that the query is doing a table scan six times. I would expect this query: select max(q\[1\]), min(q\[1\]) , max(q\[2\]), min(q\[2\]) , max(q\[3\]), min(q\[3\]) , max(q\[4\]), min(q\[4\]) , max(q\[5\]), min(q\[5\]) , max(q\[6\]), min(q\[6\]) from motor\_values to do a single scan of the whole table just once, computing all six aggregates as it went. Second, you can create indexes. Finding the min of an indexed column means getting the first value of the index; finding the max means getting the last element in the index. Those should be O(1) operations. Your second question just uses the output of this query and inserts it somewhere else. INSERT INTO Dailyrecords (MotorNumber, MaxQ, MinQ, When) SELECT '1', MAX(Q\[1\]), MIN(Q\[1\]), GETDATE()) -- ... and more columns ...
I'm not sure what you mean by "open" biohacking info. Please clarify and I'll help if I can. I personally use data that I keep from sources like Fitbit, Muse (meditation device) , FitnessNotes/Fitocracy (Android App/WebApp) etc. and have that data stored out via IFTTT or Microsoft's Flow. If there is any data out there for biohacking, you can check [Quantifiedself.com](https://Quantifiedself.com). I think you can find links in the Forums via search. There is another site that has non-specific data for biohacking but I'm struggling to find that link. Your Google Fu might be stronger than mine right now but it's out there. Maybe QuantifiedSelf can point you in the right direction. Good Luck.
Exactly what are you trying to determine? What exactly do the values in the `q` array represent? I would do it like this: SELECT max(q[1]), min(q[1]), max(q[2]), min(q[2]), max(q[3]), min(q[3]), max(q[4]), min(q[4]), max(q[5]), min(q[5]), max(q[6]), min(q[6]), FROM motor_values; If you *have to have* it in two columns, then dump the above into a temp table and make the resulting query a bunch of UNIONs. You can try a subquery or WITH expression or VALUES statement perhaps with a LEFT LATERAL JOIN, but I'm not that familiar with PostgreSQL syntax for this type of operation. 
Look up TRUNCATE TABLE and INSERT INTO TRUNCATE TABLE dbo.Things; INSERT INTO dbo.Things (Col1,2,3) SELECT Col1,2,3 FROM dbo.FixedThings;
Select Update (check mark) Update (execute) Select
I know you're asking for truncate, but if you do not need to keep indexes, primary keys etc, I'd use SELECT INTO `DROP &lt;table&gt;;` `SELECT * INTO &lt;table&gt; FROM &lt;original table&gt;;` It's been noticeably quicker than INSERT INTO for me when the tables are on the heavy side
Not familiar with Postgres personally but can you qualify and partition table by the column and return top values ascending, then flip it and return descending? You could do it in either 1 union or joining 2 derived tables that way... Or possibly just run a dense rank over a partition of the data in a formula and return top and bottom ranks by value range? (Rank ascending and descending and then filter to quantity you want in where statement)
Bcnf: customer id -&gt; first name, for example (or it appears so). Generally, folks mean bcnf when they say 3nf.
Ok firstly your usage of () brackets wrapping your joins is rather.... odd.... and as far as I know pointless. You want either a not exists check for the same but a later date or a left join for the same where its null. 
Have you considered using Power Query and Power Pivot to build out a data model? I think Power Query is called Get &amp; Transform in Excel 2016 and later. 
Looks like you have good suggestions already about truncating the table and inserting back into it. You might need a basic introduction to tables. You have the table definition and the data contained in the table, which are two different things. Think of a table definition like a bucket. You define the bucket, how big it is, what color it is, what it can hold, who has access to it. Think of the data (the rows in the table) like the stuff in the bucket. You can fill up the bucket with data. When you truncate the table, you are emptying the bucket, but not destroying the bucket. (You remove the data - the contents - but not the definition of the table.) Then you can fill it back up again with new data without having to define it all over again.
I don't know which one it violates, but it does violate the fact that some people don't have phone numbers, some have multiple phone numbers, and some phone numbers are shared by multiple people
Like it says.. "Spreadsheet is full". Make sure the Excel version is set to whatever the most recent is, and make sure you're using .xlsx for the file extension. I personally prefer to export to a flat file and then import to Excel.
use CREATE TABLE and define columns with the appropriate data types What database are you using?
This is exactly what the delete step in a MERGE process is for. I like to employ staging tables for this. 
It’s not useless, as it used to force joinorder, but nowadays the query optimizer is smarter than that. Also, it’s a syntax still used by various query wizard.
Personally I would do it in two stages using CTEs (a WITH clause). And use table aliases. Or even three stages, using the first stage to calculate sd18_top_r - sd18_top_l, just to avoid repeating it all the time. 
Oooh thats quite interesting. In my simple test the query plans were the same but I can see uses for this in rare scenarios.
So what I think I understand from your setup here is that the Excel file indicates what orders are left, but the database table has orders that need to be removed and ones that are left. If it is possible to put the Excel file into a database table then you can just use a NOT EXISTS type statement. Let's say your Excel file gets loaded into (stage\_table\_1) and your orders are in (order\_table\_1) and the key between them is (pk\_col\_1, pk\_col\_2) then: DELETE FROM order_table_1 a WHERE NOT EXISTS ( SELECT 1 FROM stage_table_1 b WHERE a.pk_col_1 = b.pk_col_1 AND a.pk_col_2 = b.pk_col_2 ); Of course, my understanding of your issue might not be clear though.
Including word table in the table name doesnt serve much purpose and unless you do it in every table you start having inconsistent naming. I am not aware of any technical issues that this could cause, but typically I think its best to avoid it for non technical reasons. I would go about it by finiding a more appropriate name for the column - typically the column you want to name same as the table actually contains more detailed information so different name could/should be use. E.g. dbo.customer - column customer_name rather than just customer dbo.licence - column licence_id rather than just licence etc etc 
Primary key = LicenseID and tablename dbo.License.
It gets pretty redundant having table in the table name because it’s implied. But because stored procedures and tables share the same name base a lot of people use “sp_StoredProcedureName” as a convention.
&gt; It gets pretty redundant having table in the table name you should see the mess when people put "column" as part of the column name CREATE TABLE tbl_customer ( col_customerid INTEGER NOT NULL PRIMARY KEY , col_customername VARCHAR(99) , col_customerphone VARCHAR(15) ); but wait, i can hear you saying, people don't stick "col_" at the front of the column name oh yeah? so why do they stick "tbl_" at the front of the table name? IT'S JUST AS SILLY 
wouldn't it be called `futuredatetime` instead of `fururedate` if that were the case?
use plural name for tables, like "licenses" &amp;dagger; use specific names for columns, like "license_id" or "license_body" &amp;dagger; Joe Celko, who helps write the SQL standard, suggests using plurals, so you should too https://www.safaribooksonline.com/library/view/joe-celkos-sql/9780128007617/B9780128007617000012.xhtml
Generally I agree, and it's not a naming convention I use, but tables share the same object level as VIEWs and differentiation between them can be import in some cases.
i love joe celko here's what he had to say about those prefixes -- &gt; 5) Prefixes on variables and schema objects. In the early days, the compilers were close to the hardware and needed all the help they could get. In Fortran, all variable names that started with I through N were of type INTEGER and the rest were floating point numbers. In BASIC, variables that started with a $ were strings. &gt; &gt; This is the year 2000; the ‘60s are over! Things like "str_firstname" or "tblPayroll" are redundant. The syntax of SQL takes care of the allowable datatypes for you and the compiler. &gt; &gt; What you are doing is trying to expose the physical storage choices in your logical data model all over again. And you are making your code hard to read. Try to read "Paris in the Spring," "**nounParis prepIn artThe nounSpring**" and see if the prefixes make it easier to understand; now imagine that was a 20 word sentence with subclauses. -- https://web.archive.org/web/20090201181713/http://www.intelligententerprise.com/001205/celko1_1.jhtml
“Used to force join order”, as in it’s probably no longer the case with modern execution optimizers. You’ll have better luck with query hints.
No, it's confusing.
or dbo.Licenses, plural. To indicate the table contains data for many licenses.
This is what I had also considered. Thanks for validating my idea!
thanks for the feedback. I was thinking this, but I thought it was a little 'raw'. Now I have the confidence to build it!
Make the code match your business logic. If you know that 0001 - 9801 does not equal -9800 how do you know that? I am guessing that you can tell (or you are guessing) based on the number ranges. What if you did something like this: when the first number is under 1000 and the second number is over 9000, then add 10000 to the first number before subtracting. 
Make the code match your business logic. If you know that 0001 - 9801 does not equal -9800 how do you know that? You can tell based on the number ranges. So code for that. What if you did something like this: if the first number (build) is lower than the second number (supplier), then add 10000 to the first number before subtracting. So 0001 - 9801 becomes 10001 - 9801. If that condition is not true, then subtract like normal. 
There are certainly times that it makes sense to do so, and I can't think of any reason why it would cause problems. Much more common complaint I have is when tables have columns named things like Order, which is an SQL keyword that forces me to put brackets around the column name every time I use it.
Makes sense as it would revert if the output was now a positive value.
Asking this question means you probably need to go back to reviewing the design. Putting Table/tbl or Column/col doesn't make sense because anyone touching that database should know the difference. If that means you would have License.License then you need to think about what that column is meant to store. Primary key? Call it LicenseID. Text for the name of the license in the row? LicenseName or LicenseDescription are a good start, but think about if that shouldn't be a text column. Instead, maybe it should be LicenseTypeID which is a foreign key for the LicenseType table's primary key. That way any changes you need to make to a specific license type, like renaming the LicenseName from 'Multi St' to 'Multi-State License' can be make to one row in the LicenseType table until an update run on multiple rows in the License table.
Thanks
Thank you
I disagree. Plural names are good for ORM, but of course your table is gonna have multiple rows, it's a table. Would you rather have table People or Person? Also for the primary key, the table prefix is redundant as well. You should have only ID column for your table, the other foreign keys will have tablenameID columns.
No if anything SQL will become more commonplace
It appears you're concatenating a space in there: RIGHT(Name, len(Name) - CHARINDEX(',',Name)-1)+' '+ LEFT(Name,CHARINDEX(',',Name)-1) Should maybe be: RIGHT(Name, Len(Name) - CHARINDEX(',',Name)-1+Left(Name,CharIndex(','Name)-1) ? 
I would start by running a query that just returns your possibilities, so you can make sure you *know* what they are and account for them. select name, charindex(',', name) from table where name is not null Run that and see what you get. Also, your case statement is likely going to barf when name is null -- charindex() likely doesn't handle null well. Or, it may just return null. You'll need to consult the documentation for the DBMS you're using.
When I use the code you've got, names without a comma end up as a null result but the query executes fine. 
https://i.imgur.com/iu7e6Ua.png
Yea, it's strange. There must be a particular value that is triggering the error. I'm guessing it's caused by values that have LASTNAME, but no firstname. Testing that right now
 The only thing that stood out was that there are records where we have LASTNAME, but a blank/null FIRSTNAME. I'm going to see if that was the issue 
Man your username is tripping me out. I have a buddy I've known since the 2400 baud dialup days who went by "Harvester Of Sorrow" back then. Then a few years back we started up an IRC channel on #freenode and he spun up a bot called CaptObvious. 
Oops. I copied the query from a SELECT statement and forgot to remove that for my example
What if I told you that was me? ... .. . It's not though :P And it turns out the issue was on records that had a comma but no firstname. The workaround just added to set those to null. Thanks for looking into it. 
&gt; it turns out the issue was on records that had a comma but no firstname I was about to ask you about that. :) Glad you got it figured out!
Just make all the views start with "vw_" or something. If it doesn't have "vw_", you know it's a table :)
Your question is a little ambiguous, so here's my best guess at what you're trying to do: SELECT t1.Department , t1.ID, t1.Date, t1.Sum , t2.ID, t2.Date, t2.Sum , Difference = t1.Sum - t2.Sum FROM yourTable t1 INNER JOIN yourTable t2 ON t2.Department = t1.Department WHERE t1.ID = [xx] AND t1.Date = '[yyyy-mm-dd]' AND t2.ID = [yy] AND t2.Date = '[yyyy-mm-dd]'
 SELECT * , LAG(transactionid,runningexclusions) OVER (ORDER BY transactionid) AS lastapplicabletransaction , LAG(decline,runningexclusions) OVER (ORDER BY transactionid) AS lastapplicabledecline FROM (SELECT * , SUM(exclusionflag) OVER (ORDER BY transactionid ROWS UNBOUNDED PRECEDING) AS runningexclusions FROM #windowed) AS a ORDER BY transactionid;
There may be a cleaner way, but this should work. SELECT customerid , transactionid , exclusionflag , decline , transactiondate , LAG(transactionid,lookback) OVER (PARTITION BY segment ORDER BY transactionid) AS lastapplicabletransaction , LAG(decline,lookback) OVER (PARTITION BY segment ORDER BY transactionid) AS lastapplicabledecline FROM (SELECT * , ROW_NUMBER() OVER (PARTITION BY segment ORDER BY transactionid) - 1 AS lookback FROM (SELECT * , SUM(exclusionflag^1) OVER (ORDER BY transactionid ROWS UNBOUNDED PRECEDING) AS segment FROM #windowed) AS a) AS b;
2016 still has row limits, albeit higher than previous versions. OP, maybe try chunking the data and loading it into different sheets. Or exporting to a flat file.
I'll take Joe Celko's advice over yours, no offense Also, most SQL keywords are singular, so there's less chance of a conflict with user defined names. Can't call your table `user` but `users` works You can't take advantage of natural joins without the prefix on primary and foreign keys. Also you need to rename everything called `id` when you join multiple tables
I know, it’s what I do ;)
Which RDBMS? Is this Oracle/MySQL (from using := as your assigner) or some other platform? Is it just an anonymous block or part of a procedure?
Without knowing what RDBMS my instincts tell me that wrapping that second else if predicate in an extra set of parens - ie else if ( (A) OR (B) OR (C) ) - will probably help out
 select case when Reason = 'Journal' then 'Narrative' when reason in ('FX Wire', 'Third Party Payment - Wire', 'Tax Payments') then Reason || ' - ' || Narrative else Reason from myTable;
Is there only one level of subposts?
 select a.id, a.title, count(*) from tag a left outer join tag b on a.id = b.tag_id and b.type = 'subpost' where a.type = 'post' group by a.id, a.title order by a.id, a.title; Does this work?
If i understand your question correctly you need to format a large csv file. The records in this csv file will be exported to a database. The file is too big to edit the formatting manually. You dont know where to start. Is this correct? If i am understanding correctly then what we have is not an SQL related question. There are many programming languages like Python or C# that have modules that allow you to edit large csv or xls files algorithmically (to clean up the formatting of datasets). These same programming languages are then used to extract data from the spreadsheets and put them in a database. So essentially, you write a program that executes all the functions you require. Perhaps you should clarify with your team what exactly they need you to do with the file but it sounds like something SQL cannot do. Sorry for formatting, typing on my phone. 
If i understand your question correctly you need to format a large csv file. The records in this csv file will be exported to a database. The file is too big to edit the formatting manually. You dont know where to start. Is this correct? If i am understanding correctly then what we have is not an SQL related question. There are many programming languages like Python or C# that have modules that allow you to edit large csv or xls files algorithmically (to clean up the formatting of datasets). These same programming languages are then used to extract data from the spreadsheets and put them in a database. So essentially, you write a program that executes all the functions you require. Perhaps you should clarify with your team what exactly they need you to do with the file but it sounds like something SQL cannot do. Sorry for formatting, typing on my phone. 
I think it's more natural to use plural names for tables because I'm thinking about sets.
If that's true, then why `LicenseID` as a column name, and not ID? When qualified, you'd have `License.LicenseID`. Wouldn't `License.ID` be preferable? For foreign keys, you'd still qualify. `License.OwnerID`, for example.
I see it done both ways. Sometimes in the same database, which I am not a fan of.
this is why the baby `${load data infile "./data/religious_folks.sql"}` recommends unit testing the first hand-written case would have failed, and you'd have to contrast expectation with actuality up front
You are not creating two tables. You are selecting data from a single table but you are giving it two different aliases (x and y). By using different aliases, you can link the queries together. Essentially the part in the parenthesis (the subquery) runs once for every row processed by the main query. You are saying something like this... - look at every row in the World table one by one. There is a row for every county in the world. - for each row, read the table again, and see if it is as big or bigger than every other country... - but only compare country size (area) if it is in my same continent. - return the row from the main query only if the row I am on is bigger (or as big) as ALL other countries in my continent
We use `nvl(end_date, date'9999-12-31')`
It looks like that sql only queries the tag table there. Sorry, maybe I didn’t explain the toxi tag scheme well. What happens is that the tag table contains all the possible distinct and unique tags, the post table contains all posts. It’s the post_tag table that contains their relationship. For example, post id 3154 can be tagged with european_politics (tag id 1) and cats (tag id 3), so the post_tag table would look like this: id_pk, post_id, tag_id 1, 3154, 1 2, 3154, 2 Where id_pk is the primary key id. So unfortunately, querying only the tag table won’t be able to yield the post count we’re looking for. 
Yes, thankfully, only one level. A sub post will never be referenced by another subpost. 
I've done that before but... I dunno... It just feels more prone to issues, like the one you mentioned. 
Thanks, I'll have a look at doing it in two stages.
something like this? select t1.id as tag_id, t1.title, count( pt.id) as post_count from tag t1 join tag t_all on t_all.id = t1.id or t_all.tag_id = t1.id left join post_tag pt on pt.tag_id = t_all.id where t1.type != 'subpost' group by t1.id, t1.title
I only briefly read your post, but it sounds like you want to combine the top 10 results for each Grade, right? If so this link may be be helpful: https://support.office.com/en-us/article/use-a-union-query-to-combine-multiple-queries-into-a-single-result-1f772ec0-cc73-474d-ab10-ad0a75541c6e
Funnily enough that was one of the resources I used. It was probably the closest of the lot, showing the right amount of members. Sadly I wasn't able to order them properly. It seems you may only use one order thing, and I don't know why, but it either gave me a range of values, or just the lowest. (I tried alternating between decending and sending)
Union
Typically when I'm performing a load test, it's to see how fast I can get the records into the DB / maintain data consistency / utilize as few resources as possible. I've used tools that just bulk load "fake" data as a modest baseline, it gives you an un-optimized "how fast can I put things in approach". The best approach I've found is to use the application and "crank the dial" in testing. How you test is going to vary by application. I've seen some designed in serial fashion, others are done in parallel. I've taken a task that was designed for serial input but was using parallelism and re-designed it for serial input. Example: App executes 10 procedure calls inserting or updating 10 records into a table row by row. Each one calls a trigger which updates the table. I re-designed it to shovel all records into a staging table, then update / insert / delete from the staging table in a single transaction vs 5000 transactions. It's not executing them in parallel anymore but the process went from 5 hours to 20 minutes. In the end, I guess you could say it's tuning and design based on the goal and expectations and then running the application in testing mode to see if it hits those numbers. If I need an application to have sub-second results, that's not hard to design. It's going to cost the company some money though on SSD's, ram, cpu, and licensing. We'll need to get OLTP set up and likely need to work into data archive and partitioning. When I'm testing how fast things happen in the DB, I'm trying to minimize anything I can that doesn't have a downside. (Picking the best data types, doing things in sets, filtering data at the correct levels, etc.) Once all of the bare bones things you can do are finished, it comes down to the give and take. If I add this index, how does this affect writes? How does it affect reporting? Will this impact free space necessary? If I don't add the index, what kind of lock escalation may occur? I don't have any one tool I use and it depends on shop to shop. I'll measure the results of how the application performs and see where we have waits, analyze the query plan, review the indexes / statistics and application logic. See what the biggest bang for the buck is for refactor and go from there.
Thank you so much. This make it much easier to understand now.
You could go wide and combine sql exercises with programming in C#, Java, or Python. I'm also going for the MCSA and started to make notebooks to practice and review. Python has useful libraries for finance domain experts. [https://i.imgur.com/YU2hDaK.png](https://i.imgur.com/YU2hDaK.png) [https://i.imgur.com/d3aQNfE.png](https://i.imgur.com/d3aQNfE.png)
A little more about correlated subqueries: Correlated subqueries use a type of table expression, derived tables, where the inner/outer queries are dependent on each other. So in MySQL Workbench, you can't highlight the inner query and execute it alone because the predicate depends on the outer query. The derived table vanishes after the query is executed. SQLZoo does warn us about the ALL predicate "To gain an absurdly detailed view of one insignificant feature of the language, read on. We can use the word ALL to allow &gt;= or &gt; or &lt; or &lt;=to act over a list..." Maybe it would be easier to see an example of a correlated subquery without using ALL. The example would perform poorly because three subqueries are executed per continent. Using ALL is a more efficient, but is not easy to understand at first. `select distinct x.continent` `, (select` [`y.name`](https://y.name) `from world y` `where y.continent = x.continent and y.area = (select` `max(z.area)` `from world z` `where z.continent = x.continent) ) as 'name'` `, (select max(z.area)` `from world z` `where z.continent = x.continent) as 'area'` `from world x` 
This gave some very promising results. Using your SQL, the european\_politics total count gave me: 139 The count I received for each individual category was: 102 + 33 + 4 = 139 However, this double (or incrementally added) counts for posts that contained more than one tag. Example: a post that was tagged european\_politics and england\_politics would be counted once in 102 and once in 33. This is a very good starting point though. Thank you! I will look deeper into this route.
Simply do count( distinct then
This is so dependant on the app interacting with the database and what fields you are including. We don't exactly spec database schemas on demand here. If you want to write one, post it here and talk about what software you'll be using to interact with it we will throw b in our two cents.
is that like a homework type question they gave you? I'm pretty sure you'd need a database name too.
Judging by your LOAD DATA INFILE command syntax, you are using MySQL. In this case, I would suggest using Python to split the file up by the number of records. [This](https://stackoverflow.com/questions/16289859/splitting-large-text-file-into-smaller-text-files-by-line-numbers-using-python) solution on Stackoverflow worked for me with modifications. I don't think there is a better way to do it without splitting the file up first. Or..you could be patient and wait for it to load, then do: SELECT * FROM temptbl1 LIMIT 1000;
Let me contact IT to have python on my desktop. Heidisql and Commandline is what I got so far to work with.
Can anyone explain me step by step about this stored procedure and can provide me with an easier example with a dummy data?
&gt;select t1.id as tag\_id, t1.title, count(\*distinct\* pt.id) as post\_count from tag t1 join tag t\_all on t\_all.id = t1.id or t\_all.tag\_id = t1.id left join post\_tag pt on pt.tag\_id = t\_all.id where t1.type != 'subpost' group by t1.id, t1.title Interestingly enough, adding distinct to [pt.id](https://pt.id) gives me the same numbers as without distinct. I'm not sure why. This following query also yields me the same result: select t1.id, t1.title, count(distinct post_tag.id) as count from tag t1, post_tag where post_tag.tag_id IN (select tag.id from tag where tag.id = t1.id or tag.tag_id = t1.id) and t1.type = 'post' group by t1.id, t1.title order by count desc and this query gives me the result i'm looking for: with Q1 as (select distinct post_tag.post_id, t1.id, t1.title from tag t1, post_tag where post_tag.tag_id IN (select tag.id from tag where tag.id = t1.id or tag.tag_id = t1.id) and t1.type = 'post') select Q1.title, count(Q1.title) as count from Q1 group by Q1.title order by count desc EXCEPT! It took over a minute for the result -.- We're getting closer! Note: I think I may have found out the reason why the distinct does not work: |post\_id|tag| |:-|:-| |3120|european\_politics| |3210|cats| |3210|england\_politics| What we want is the unique combination of post\_id and tag, but because they are already unique, though we know that england\_politics should really be counted as european\_politics, it's not coming back as distinct.
Have you literally just pasted the code of one of your companies procs on the internet?
We don't know your schema, we don't know your data, and this post is barely legible (and even appears syntactically invalid, meaning parts of the code are missing from what was posted). It's a stored procedure that calculates some stuff in several steps and returns a table. 
K now that I'm not on the phone - if you need to count distinct posts, do a count( distinct pt.post_id)
...why not ask them? If they haven't given a database name then presumably you will only have access to whatever databases the supplied log in does But it seems like it should be enough to me
Update: That last complicated query can be condensed to: select count(distinct post_tag.post_id) as count, t1.id, t1.title from tag t1, post_tag where post_tag.tag_id IN (select tag.id from tag where tag.id = t1.id or tag.tag_id = t1.id) and t1.type = 'cast' group by t1.title order by count desc Unfortunately, it does not reduce the processing time. 
Try using the IGNORE DUP KEY option on the primary keys, and not removing them for your load? The new rows will insert and duplicate PK values will be silently discarded. 
This query is still the closest. Here is the simple fix that made it work: instead of count( [pt.id](https://pt.id)), which i finally realized didn't count the duplication of the posts, we needed to change it to count(\*distinct\* pt.\*post\_\*id). The final solution is: select t1.id as tag_id, t1.title, count(distinct pt.post_id) as post_count from tag t1 join tag t_all on t_all.id = t1.id or t_all.tag_id = t1.id left join post_tag pt on pt.tag_id = t_all.id where t1.type != 'subpost' group by t1.id, t1.title Thank you so much for your time and patience.
I've never heard of this. Assuming this is going to ignore the record that's trying to be inserted? If so, that will work. I'd obviously want to keep the newly created record and remove the old one. Especially if there are more columns that have been updated as opposed to just the UpdatedBy column. All other columns are assumed important. 
So if you get existing PKs, you want to keep the newer version of the row? In which case, I'd suggest loading a staging table and using a MERGE to insert new rows and update the values of existing ones in the main table.
You're fired.
If you mean the below, it's possible, but I don't think I need a staging table to do this. I would just do this between source and destination. Though, if it's the below, that's a hell of a lot of code to write for hundreds of tables with many columns :( MERGE Target.dbo.Table1 AS [Target] USING Source.dbo.Table1 AS [Source] ON [Target].[PKColumn1] = [Source].[PKColumn1] AND [Target].[PKColumn2] = [Source].[PKColumn2] WHEN MATCHED AND ([Target].[DataColumn1] &lt;&gt; [Source].[DataColumn1] OR [Target].[DataColumn2] &lt;&gt; [Source].[DataColumn2]) THEN UPDATE SET [Target].[DataColumn1] = [Source].[DataColumn1] , [Target].[DataColumn2] = [Source].[DataColumn2] WHEN NOT MATCHED BY TARGET THEN INSERT [Target] SELECT * FROM [Source]
&gt; I recommend fixing this: LINES TERMINATED BY '\r\n' &gt; To: LINES TERMINATED BY '\n' This worked. No freaky spaces between each character, quotes left intact, or rows, and rows being loaded. What does leaving out '\r' do?
You're running your ETL by removing the PK constraint on the table then adding new data??? 
and protect the integrity of your data? GASP!
You know nothing John Snow
Because it's WAY faster, that's why. This thing would chug along for an hour without doing so. I've actually tested it. Without PK or indexes the inserts are faster. 
..... step away from the database, sir. No one needs to get hurt. Just walk away.
Also, you may want to do some research on this: https://tinyurl.com/gntt68e
Desired outcome, Destination matches Source... If a record is new, it's inserted, if it's an updated recorded, the same record in the Destination is updated with the columns that do not match. If the record exists in Destination and not in Source, it should be deleted from Destination. Like I said, I don't know the best way to go about this. I could truncate all the tables and just bulk load it, but it takes too long.
&gt; INSERT INTO Table1(Column1, Column2) &gt; SELECT Column1, Column2 FROM Table 2 &gt; EXCEPT &gt; SELECT Column1,Column2 FROM Table1 Can we talk about this atrocity? Since you have an UpdatedBy column, I assume you also have an UpdatedDateTime column to. How about your select all records where the UpdatedDateTime greater than your last ETL into a staging table. Then MERGE that data set into your destination table. Insert new records and update existing records.
You could use Change Data Capture to achieve that. However, I would recommend taking a step back and asking yourself "WHY AM I DOING THIS?" Also, when asking for assistance, post some DDL so we can model the issue. 
I think this is the best way to go about it, I was just having a hard time. Thanks for being so nice...
Waaaait a minute. I just realized that it sounds like you just want a COPY of this table to exist in some other location. I'm guessing you want it in another database? Is this to offload a reporting workload? If so, you have chosen the most complicated approach possible. Things that can help you: Transaction Replication CDC Database Mirroring Database Snapshots Hire a DBA and have him help you. 
OK now I feel bad. What you are doing is scanning both of your tables, inducing HUGE amounts of IO. Dropping constraints and indexes, only to re-add them is the wrong approach to better the performance of a bad solution design. Honestly, I think you should look at this comment as your real solution... &gt;Waaaait a minute. I just realized that it sounds like you just want a &gt;COPY of this table to exist in some other location. I'm guessing you &gt;want it in another database? Is this to offload a reporting workload? &gt;If so, you have chosen the most complicated approach possible. &gt; &gt;Things that can help you: &gt; &gt;Transactional Replication, CDC, Database Mirroring, Database &gt;Snapshots &gt; &gt;Hire a DBA and have him help you.
That's exactly right. We need a reporting database. Problem is, we're on SQL 2008 R2. I don't think those options are available to us. I decided to try to script it out to a destination in an express version of SQL Server and since I only need the rolling years worth of data, we're under the 10 GB limit. Also, my example is just one table, but there are close to 200. Most having at least 20 columns, if not more. Most are transactional and have millions of rows for just this year. That's why I want to append and not truncate and reload. 
SQL 2008 R2 supports Database Mirroring, Snapshots, Change Data Capture, and Transactional Replication. I would configure database mirroring for the database to a "reporting" server and schedule snapshots to be run on whatever frequency you'd like the data available for reporting (once an hour, once a day, etc.). It would be the fastest solution available for you that requires the least amount of effort to configure. 
Like I replied to the other dude, I tried that. I want to order them, but I can't seem to find a way to achieve that. I'd order them in the main table, but as the points are split up and are connected to the members though a relationship, I don't know how that would work.
Do not use SQL Express for any of this. You're setting yourself up for failure and frustration by moving in that direction -- which is evidenced by the fact you had to post here looking for help. Do it once and do it right. If you really have to use Express, re-evaluate your current role and consider posting your resume to find something else. 
Delete from the dest where record with dest PK exists in source, then insert into dest. Leave the PK as it is.
Your getting a bit of stick. Dont worry about it. We all start somewhere. You learn a bit of syntax then code yourself a mile down into the depths of some unholy design madness only to realise that a much simpler method exists. I once wrote 200 lines of code that my boss at the time replaced with 3 characters. Dunno what the other person said as they deleted the comments but its feasible with dynamic SQL. Not at SSMS right now but you would use the information schemas to dynamically generate a merge statement. It would be fun to write I guess. But honestly there are better ways of doing this. I mean is taking a backup and shipping over transaction logs not feasible? Or sql replication or just plain old copying the tables? You can script that up easy enough with the import wizard. 
Thanks for that. That's pretty much where I'm am. I know enough to be dangerous, but at the end of the day I'd like to complete this for my company. The snide comments are taken in stride. Everyone starts somewhere. Problem is the destination is on sql express and has a 10GB database limit. That's why I wanted to script it out to only the rolling years data. Because timestamps will very per table. 
too bad this is on windows, on linux this is as straightforward as a `tail` or `awk`. If you plan on doing a lot of text processing I'd recommend maybe installing the linux subsystem for windows or cgwin.
I'll get access to a Linux Virtual Machine soon. Hopefully by the end of this month. I'm stuck on Windows 7.
10x data ninjas don't have time for constraints!
I know what you mean, but realistically I'm not going to do that and the company isn't going to budge on express. We're using SAP Business Objects to report, and the only problem is the 10 GB limit. There are no other issues with using express and it is a huge cost savings. I will look into the above options to see if I can control the amount of data loaded based on timestamps of the rows in the tables, otherwise I have to script it out unfortunately. I have a feeling that's the case. And if so, I will just have to figure it out. 
The DB name is quite often the same as the username
Is it not faster inserting millions of rows to a table that doesn't have PKs and Indexes, as opposed to them having them? The time saved rebuilding them is worth it. 
You could try out my software called Musoq (https://github.com/Puchaczov/Musoq), Basically you would like to do the query like 'SELECT * FROM #csv.file('path/to/file/csv', ) TAKE 1000' with additional switch -qs "/path/to/out/file". It should copy first 1000 rows to external file.
Just start with basic statements, SELECT stuff FROM table Then move on to more complex stuff SELECT stuff FROM tableA JOIN tableB ON column.tableB = column.tableA Get an idea for a set of data you'd like to explore and create it out of SQL for yourself. 
If your company isn't willing to spend money on a viable solution, then it may not be as important as we think it is. It would most likely cost less money to implement another server and utilize the mirroring approach than to pay out your rate for however long it's going to take you to build out some custom hack. But, I digress... Use a time based delta approach -- it's been the standard procedure for things like this for decades. 
All of this can certainly be done in SQL. Is there a particular part of it that you're having trouble with or would like help with? I don't have time to do it all but could answer one or two questions!
Appreciate the tips. The funny part is, I'm familiar with the statment you're describing. My trouble is what software I should use, and how to set it up to interface with the server. 
Thanks for the reply. I will try to explain as best as I can. I have this query which finds the max charge date and invoice date. However, it does not show me all records for that customer. It will only show me the max records. SELECT concat(charges.invoice\_id, invoices.subscription\_id) as helper, date\_format(max(charges.created) - interval '4' hour, '%m/%d/%Y %T') as max\_charge, date\_format(max(invoices.date) - interval '4' hour, '%m/%d/%Y %T') as max\_invoice FROM charges JOIN invoices ON charges.id = invoices.charge\_id group by concat(charges.invoice\_id, invoices.subscription\_id) I want to be able to join this somehow with the baseline query so that each record from the baseline query has 2 extra columns corresponding to the max dates for each record. Then from there I mostly need help making a new column that finds for each record the previous invoice id that came before it based on the previous max invoice month (current record max invoice month - 1 month) and subscription\_id. If the same subscription\_id in the list of data output has the same previous max invoice month, then return the previous invoice id. 
I work in SQL server a lot and then copy/paste my results to excel. You can also query directly from Excel or PowerBi. Probably tableau too. 
 INSERT INTO Table1(Column1, Column2) SELECT Column1, Column2 FROM Table 2 EXCEPT SELECT Column1,Column2 FROM Table1 That is basically saying get me all the data I don't need to exclude it from the data I do. As in pull every single row from both tables and compare it. If this is across two different servers via linked server or something then one has to send the entire table to the other server I believe. Its very bad cause its utilizing just about every resource to the max. 
Something like this: SELECT i.id as invoice_number, , i.subscription_id, , i.amount_due / 100.0 as amount, , date_format(i.date - interval '4' hour, '%m/%d/%Y %T') as invoice_date, , date_format(c.created - interval '4' hour, '%m/%d/%Y %T') as charge_date, , i.closed, , i.forgiven, , i.paid INTO #temp from invoices i join charges c on c.invoice_id = i.id SELECT CONCAT(t.ID,t.subscriptionid) AS '1' , CONCAT(LEFT(CONVERT(varchar, t.invoice_date,112),4),'- ',LEFT(CONVERT(varchar, t.invoice_date,110),2)) AS '2' , CONCAT(LEFT(CONVERT(varchar, t.charge_date,112),4),'- ',LEFT(CONVERT(varchar, t.charge_date,110),2)) AS '3' , (SELECT LEFT(CONVERT(varchar,t1.invoice_date,110),2) FROM #temp t1 WHERE t1.id = t.id) AS '4' , (SELECT LEFT(CONVERT(varchar,t1.charge_date,110),2) FROM #temp t1 WHERE t1.id = t.id) AS '5' FROM #temp t
\r is the carriage return escape character. \n is new line. You are now only starting a new line, and no longer performing an extra action in the formatting.
&gt; So, how do I set myself up to access it and run queries? If you can convince your IT folks to install it for you, most people running MS SQL Server will use "SQL Server Management Studio" (SSMS) to connect to their MS servers. MS has a whole bunch of other tools though too; more info here : [https://www.microsoft.com/en-us/sql-server/developer-tools](https://www.microsoft.com/en-us/sql-server/developer-tools) &gt;Would I do this from my office workstation? Yes, you can run SSMS, or some other tool locally on your workstation and connect to the SQL server instance if your network infrastructure will allow it. If your IT folks are really on the ball, your workstation might not be able to 'talk' directly to the database on the ports it would need. &gt;Is this a dangerous or inappropriate idea for any reason? Really depends on your infrastructure. If you are in a small shop, I'd worry about the IT folks giving you full write access to a production database. Do you know what type of rights your account will have? Is there only the one instance of the database, or are their clones, etc?
Inserting into a heap (a table without indexes) is typically faster (assuming absolutely no indexes), but it has downsides... https://www.red-gate.com/simple-talk/blogs/reasons-why-you-may-not-want-to-use-a-heap/ If your source 2008 R2 Server is running Standard/Enterprise you could use SSIS on Standard/Enterprise to push data to Express - some functionality may be limited, but there should be plenty to write your own Slowly Changing Dimensions (I'd add my own checksum column to compare to determine D U or I rather than comparing every value against every value for every row every time).
Probably not the best way about it. At my work we have a different database for AU and NZ, so we have an extra column set to a “CountryCode” and have the unique key across both of those for reporting and data interrogation.
Just to clarify, are you saying that all customer don't have a unique identifier or customers from the two database don't have similar identifiers. If it's the first option are the names spelled the same, it so you can group each customer by name and assign a unique identifier. If it's the second option, can you try to group customers by name? 
Why not just add a field called 'clientid'? 
You could generate GUIDs for your unique identifiers. Not sure which RDMS you're using, so that may not be supported as the primary key. But, you can treat a GUID as a primary key. We use MS SQL, and that does not have a GUID datatype, so you have to store it as a string.
Doesn’t NEWID() generate a GUID? 
Our databases are based on location. Database A = Connecticut and Database B = Massachusetts. We have many customers who have accounts in both states. I don’t manage our have full access to our databases, and have never merged two databases before, so if this is a newb question, I apologize. Would that require syncing a field in two different databases? Is that possible? I don’t know how we’d use the unique key across both without manually entering it in once. I’ve always just relied on auto generated primary keys.
yes it does. But if you want to store it in a table, you have to use a VARCHAR datatype.
Create a hash key and use that for uniqueness [Like this.](https://www.red-gate.com/simple-talk/sql/t-sql-programming/intelligent-database-design-using-hash-keys/) You may still need to hash a composite or even use differing hash per database, but will give you uniqueness.
Try to separate the concepts of the primary key on the table and the unique identifier you're trying to create. What if you had 3 databases instead of 2? Your solution stops working. The primary (sequential INT or BIGINT) key that comes on the table by default is your clustering key, and just happens to work well as a unique identifier in single-DB environments. When you have multiple DB's, you need to use a Globally Unique Identifier (GUID or UUID). You can generate GUIDs on each DB separately and not worry in the slightest that there will be any clashing. When you need to communicate across DB's for the client, you use the GUID. For your queries on the local DB, you can use the ID for optimization purposes. We implement this same concept to share several entities across several platforms. We have a DB that serves as the primary data warehouse, and our clients pay for access to that DB. They can also submit updates and create new entities in our DB, and that can propagate to our other clients. The only reliable way to get *truly unique* identifiers for entities across N number of DBs is through GUIDs.
Awesome. This sounds very promising. I’ll look into it tomorrow at work. Thanks! 
Ohh... customer merges. Shit. I thought you wanted to be able to have them in one DB like a warehouse to report off. You are in for pain and misery. Sorry buddy. A few years ago we merged almost 100 individual store databases into one. Still finding duplicates 5 years later. The best you can do is try match customer details and then work from there. As for your customer identity. If it’s only two locations you could have an identity column at each site growing in opposite directions from 0?