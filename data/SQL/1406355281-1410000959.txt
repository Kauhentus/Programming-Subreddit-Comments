if its just 4000, you can get the lat/longs manually http://www.torchproducts.com/tools/geocode
I can confirm it was not running at the time you posted, but is up again since about 2 hours. Sorry for that, this was a really rare event. And thanks for letting me know!
 $alphagram = implode('', sort(str_split($my_string))); Then substitute it on the SQL. You could technically [create your own MySQL C function](http://dev.mysql.com/doc/refman/5.6/en/c-api-plugin-functions.html) but it is a bit cumbersome. Doing it in a MySQL routine may also not have the desired performance, but it certainly is possible.
This was shared on [StackOverflow](http://stackoverflow.com/questions/24972977/sql-query-how-to-get-users-and-their-three-latest-posts): select * from ( select u.username, p.title, row_number() over (partition by u.id order by p.post_time desc) as rn from users u join posts p on u.id = p.user_id ) t where rn &lt;= 3 order by u.username; But I use SQLite quite frequently and would be curious to see it without window functions.
Thanks.
as soon as you drop 'the DB' and create 'the DB' anew, it is a different database. It has the same name but it is a different security object. In effect, I think, you're asking for the CREATE ANY DATABASE server privilege, which would allow you to create a DB with any name and have CONTROL over it, which will include dropping that DB. PS. having said that, I don't understand why'd u ever need to drop the DB. And if that's not needed, all you need is to have the DBA create your DB and grant you the db_owner role in that DB.
Why do you mention windows auth? This has no relevance. Drop the database only by a script and not by the GUI. So if the jerks happen to use your database name, they get what they deserve.
You could use [LIMIT](http://www.sqlite.org/lang_select.html#limitoffset) clause for sqlite, it'll also support pagination with an OFFSET parameter. I don't think hitting the user table on it's own would be a significant increase in overhead though, and once you have the user object loaded into your client you could then more easily page through results from the post table without the overhead of a join to the user table. Most other SQL dialects will have a similar concept for pagination as well.
Yep that was the best answer I found on stack overflow
Yeah see, the main concern here is my anxiety. I don't want to screw up servers.. And doing these things on a Friday gives me an awkward anxiety over the weekend in case I've done something crazy like drop the wrong database. The reason I dropped it is because it is not live yet, and with all the schema changes it's easier to drop it.. I really should stop that habit and just do a schema compare. Btw, I've never found a nice way to remove the existing connections for a DB so I can rename it. Any suggestions? (So I don't have to drop so much..)
My opinion, I would never give production access to create/drop a database. If you can't script your changes and test them in other environments, I don't want you making production changes. Especially through the GUI. Everybody makes mistakes - that's what change control is for. Also, if I have to, I could give out DBO rights. Then you can drop and create any object in the database - but only in your database. There's seldom reasons to drop and create the same database on a regular schedule - I usually see it when someone is being lazy with scripting or ETL. But that way I know you cna't drop anyone else's objects in their own databases. I'd probably resource govern the heck out of it - shared server resources mean that you need to play nice - so I'd make sure you couldn't take up all the CPU on the server with a unintended cartesian join. Next - taking the bait, I would be adamant about windows auth, too. I much prefer it for many reasons. Some of them: * Makes password reset, group management, etc the job of whoever manages the AD. I prefer to do other things than manage users. Security? Sure. Users, meh. * I hate seeing "userId=;Password=" in clear text in configs. Trusted connections make that more secure as well as eliminating deployment snafus (did I update the connection string?). * A little more difficult (not too much) to share users. If everyone comes in using the same account, auditing, resource groups, etc all before a little more difficult. * There are more, but that's enough get explain my preference.
Flip the db to the single user mode. Management studio will ask if you need to drop existing connections. Don't forget to flip it back.
ALTER DATABASE MyDb SET SINGLE_USER WITH ROLLBACK IMMEDIATE will kill all connections to a database, but the next connection will take up that single user. Better hope that's you (I.e. Run a quick use statement in the same batch. ).
2014 came out in april. What do you mean by upcoming? This version focuses on in memory technology. Last version was for BI. For t-sql I'd like some way to perform relational division and a group concat function.
If your table is large, you may get better performance writing it slightly differently: SELECT id FROM categories WHERE NOT EXISTS ( SELECT TOP 1 1 descendant FROM category_paths WHERE depth &gt; 0 ) Why? [Because Aaron Bertrand said so](http://sqlperformance.com/2012/12/t-sql-queries/left-anti-semi-join).
I'd like a way to create an empty clone of a table including keys, defaults, etc.
i was referring to more from a developer side. new functions. I would also like to see a more friendly approach to cursors and loops like in oracle. I've never used any of these but they would be a great way to bring others into sql who come from a more procedural language. right now, cursors and loops are generally forbidden 99% of the time in t-sql for obvious performance reasons. 
also would like to see the ability to use expressions in a where clause without having to use sub queries. hoping something like this would be valid down the road: select orderid, year(orderdate) Year from table1 where year = 2014
you need to join the team and player tables then put AVG(height) in the SELECT clause then add a GROUP BY clause for the team then an ORDER BY clause as for the top 10 part, you neglected to say which dbms you're using, so the answer depends
have you learned sub queries yet? It simplifies things. I think this will work. ----------------------------- select top 10 SubQ.TeamName from (select team.Name as TeamName, avg(Player.Height) as PlayerHeight from team inner join player on player.team_id = team.team_id group by team.name) SubQ order by PlayerHeight desc 
I'm not familiar with that particular product or its setup, but I'm guessing it has a small proc that is building and populating the tables for you. for the first one, it's trying to store a date value in a date data type column. since it's complaining, I'm guessing the date is trying to go in as 04/24/2014 when the system is expecting 24/04/2014 or something. So check to see if there are any pre-setup things you have to do like "set date format "yyyy/mm/dd" or something like that. the second error is complaining that a "parent" record does not exist that a child record is pointing to. (AKA foreign key) Ex, if you have a a department table with two values "Accounting" &amp; "Marketing" in it and an employee table which has a record that points back to the department table but says "IT", then you'll get this error. It is probably caused by the FIRST error, where the date was wrong. that insert was rejected, so the record didn't make it into the table. Now some other record is trying to point back to it. These are all educated guesses. Hope I didn't cloud the issue too much. 
subquery isn't actually needed here, it adds noise to the signal plus, you should really have the average in the result set
I didn't think it was the most efficient query since the category's id would be checked against every value in the table generated by the sub-query, but I'm not the most knowledgeable on the subject so I didn't say anything. I took a quick look at that article you linked but I'm not sure what to make of it. [Edit] I am blind, I didn't see that you wrote an alternative query. I'll try that. [Edit #2] TOP doesn't exist apparently.
Just detect for null.
it was the fact that they specifically didn't ask for the average in the result set that i did it that way. This is similar to the problem where you'd want to sort departments by average salary, but not actually expose the salary. 
good point
You can't have a default value for associative arrays (index by collections) unless you use a function to create one. function EmptyNumberTable return dbms_sql.number_table is vResult dbms_sql.number_table; begin return vResult; end; procedure patch( v_id in integer, v_name in varchar2, v_version in integer, v_sql in varchar2, v_duplicates in dbms_sql.number_table default EmptyNumberTable) .....
I could be wrong but i think the SUM(Fields!WOtotal.values) seems to be returning zero with the sum function. I'm not familiar with the system you are working in but I think if you use your system's equivalent to ISNULL combined with NULLIF you can force the null and zero values to be something that does not error, for example 1. Edit: oh I forgot if ssrs doesn't have a built in ISNULL or NULLIF functions you are stuck using an IF around just that SUM() function before the divide by \.
=IIF((MAX(Fields!Meter.Value) - MIN(Fields!Meter.Value))=0,"N/A",(SUM(Fields!WOTotal.Value) \ (MAX(Fields!Meter.Value) - MIN(Fields!Meter.Value)))) 
You don't have to use TOP. Your SQL implementation may use the LIMIT syntax, or you omit that and just SELECT 1 FROM
I don't like our chances of getting a Python script into production. Thanks for the tip, however.
It's my understanding that SSIS uses the bulk insert statement anyway. Looks like I'm going to have to do something similar regardless. Thanks, and I'm stealing &gt;I find this about as helpful as a shoe to the head.
 SELECT id FROM categories WHERE NOT EXISTS (SELECT 1 FROM category_paths WHERE depth &gt; 0 LIMIT 1); Gives no rows.
Awesome! I had no idea this was possible. Thanks!
Still getting the same error. This seems like it wouldve worked too
oh, cool! yeah that definitely will do the trick if the order by is supported like that. (i.e. allow to reference it when not specifically calc'ed or pulled in the select) I've been working with a Teradata warehouse a lot for the past few years and we.subquery.*everything*. It's kind of a "best practice" for that engine. But yes, your solution above is leaner and cleaner. 
Ugh, it's because I'm stupid. We need to use a correlated subquery and I forgot to do anything to correlate the parent query and the subquery. Can you try something like this (assuming I'm understanding the ancestor/descendant fields properly)? SELECT id FROM categories AS c WHERE NOT EXISTS (SELECT 1 FROM category_paths AS p WHERE p.descendant = c.ID AND depth &gt; 0)
If that doesn't work, then it must be a defect and you need a better reporting tool. I do wonder about why you are using the wrong slash for division. EDIT: I looked it up and this is expected behavior since SSRS evaluates all arguments regardless of what is used. The solution is ugly. =IIF( (MAX(Fields!Meter.Value) - MIN(Fields!Meter.Value)) = 0, "N/A", (SUM(Fields!WOTotal.Value) / IIF( (MAX(Fields!Meter.Value) - MIN(Fields!Meter.Value)) = 0, 1, (MAX(Fields!Meter.Value) - MIN(Fields!Meter.Value))))
Can't you do that in SSMS by selecting the table and choosing 'Script Table as Create' ? Or use Tasks&gt;Generate Scripts?
Well, a brief read confirmed that 'not exists' would be doing just fine in this particular case within SQL Server. But thanks for the link anyway. In this case, MySQL, this change might not have any effect whatsoever or an opposite effect. 
SSRS treats `NULL` as `Nothing`, which it coerces to `0`, so you need to `IIF` around that.
Yeah, you can. But then you have to manually modify the script to change the names of the table, PK, indexes, FKs, etc. It would be nice to just have a T-SQL command like "CLONE src AS tgt" and it would update all those values according to some fixed convention. A friend of mine said that Oracle has a similar feature and it was something she really missed in SQL Server.
I just edited the 1st message with the correct order of the errors as shown in the log. Anyway regarding the date error, i figured the same you did, but looking around in preferences (of the oracle sql developer) i didnt find any option to edit the date form, neither checking on google for some clarification. As i said i just started, but shouldnt the date format be coded 'inside the database'? or is it just an option can be set in the compiler (sql developer)? 
&gt;Am I on the right track and/or what am I doing wrong? Are you getting the required/expected results from the query? If you are, then you're on the right track.
Have you checked your data and schema to validate that the results you're trying to get are possible? IOW, do these tables really do what you think they do? Is there a foreign key relationship between `QA.QA_REQUESTER` and `Staff.STAFF_EMP`?
There is a foreign key relationship. However, realizing I was testing on an old version of the database, I have come to the conclusion the query is working as it should. 
So...bad/missing data?
Missing, yeah. X was not in the reference table so that's why there were no results. 
yeah for sure, the setup should have taken all that into account. It's not very robust if it can't handle a different system date format. I'm not sure it you can see the script that is running. It would be helpful to see the actual inserts it's trying to run. As for the foreign key violation, the first error, that is very odd. Maybe the tables are being populated out of order. Sometimes, running the setup a second time fixes this. sorry if these are vague answers, but I don't know enough details to give you the proper help. 
Google has a geocoding API that you can use for up to 2500 requests per day. It will translate addresses to coords and provide a bunch of other data (municipality name, etc.) as well.
Very ugly, but worked perfectly! Thanks!
If it was me, I'd add a "moved" field. Compare current location with last saved location when the truck ping comes in then update moved with the current time if the location is different. 
Not really a beacon of efficiency, but this may do it, I named your datetime column "checkin" for this example. SELECT DISTINCT vehicle, MAX(checkin) LastPing, DATEDIFF(hour, MAX(checkin), GETDATE()) AS HoursAtGivenLocation FROM vehicles WHERE location LIKE '%dallas%' GROUP BY vehicle HAVING DATEDIFF(hour, MAX(checkin), GETDATE()) &gt; 24
I thought about this too. The problem I think is that I can't guarantee I'm only getting 1 new update per truck when I go to insert new data. I could get many updates at once and would have to come back to this.
This seems to bring back everything that was in Dallas over 24 hours ago I think...looking at the results it brings back it's definitely not the right data. I need everything that's currently in Dallas and that's been there (without leaving) for over 24 hours.
Does the vehicle periodically check in even if it hasn't left its location, or does it only check in when it changes locations?
I'll get updates even if it hasn't changed locations. 
See if this works? Its effectively trying to emulate a GROUP BY, without the limitations of it select latest.vehicle, latest.datetime, DATEDIFF(hour, history.datetime, CURRENT_TIMESTAMP) HoursAtGivenLocation from [table] latest LEFT JOIN [table] history ON ( latest.vehicle = history.vehicle and latest.location NOT LIKE '%Dallas%' and history.datetime = ( select TOP 1 last.datetime from [table] last where last.vehicle = latest.vehicle and last.location NOT LIKE '%Dallas%' and last.datetime &lt; latest.datetime order by last.datetime DESC ) ) where latest.location LIKE '%Dallas%' and DATEDIFF(hour, history.datetime, CURRENT_TIMESTAMP) &gt;= 24 and latest.datetime = ( select TOP 1 latest2.datetime from [table] latest2 where latest2.vehicle = latest.vehicle and latest2.location NOT LIKE '%Dallas%' order by latest2.datetime DESC ) Kinda hard to write complex queries with no data, will probably have a few syntax errors :-)
 SELECT t.vehicle, MAX(datetime) AS LastPing, DATEDIFF(hh, MIN(datetime), GETDATE()) AS HoursAtLocation FROM [table] t INNER JOIN ( SELECT vehicle, MAX(datetime) AS lastoutside FROM [table] WHERE location NOT LIKE '%dallas%' GROUP BY vehicle ) vo ON t.vehicle = vo.vehicle WHERE t.datetime &gt; vo.lastoutside AND DATEDIFF(hh, vo.lastoutside, GETDATE()) &gt; 24 GROUP BY t.vehicle EDIT: Fixed some syntax, it's hard to write without tables to work against. 
I wouldn't go down this route. Suddenly you're left having to manually update bitfields to keep your dataset coherent. Stuff like this is what wastes an afternoon debugging, when an update fails 6 months down the line Far better to just import raw data and you can query everything you need from that
I should have clarified something. The location may say something like: "In Dallas, TX On I-75" OR "3.2 Miles North of Mckinney, TX, In Dallas" or something along those lines (I don't have any control over this field). So each location there is distinct however for my purposes they both count as being in Dallas since both have location like '%Dallas%'. I do think you may be onto something here using the CTE....I'm going to see if I can make it work.
Yeah I wanted to avoid this for the same reason. I just want a query that works.
oh yeah that makes it a little more difficult...this should work, but you could get unexpected results if you're looking for something more generic. you wouldn't need a CTE for that i don't think, unless you wanted a list of all these locations. if you just need the vehicle, you could just take the query inside the CTE. if you needed the locations, you'd need to join back to [table] ;with single_loc as ( select vehicle, sum(case when location like '%Dallas%' then 0 else 1 end) as [location], MIN([datetime]) as [first_datetime], MAX([datetime]) as [last_datetime] from [table] where [datetime] between DATEADD(HOUR, -24, GETDATE()) AND GETDATE() group by vehicle having sum(case when location like '%Dallas%' then 0 else 1 end) = 0 ) select distinct vehicle, location from single_loc sl join [table] t on ( sl.vehicle = t.vehicle and t.[datetime] between sl.first_datetime and sl.last_datetime ); 
As a side note, there are big advantages of using this and big disadvantages; for example: * You can determine all the categories for a row by joining it to itself, which is extremely efficient: *** SELECT cc.title FROM category c INNER JOIN category cc ON c.lft &lt;= cc.lft AND c.rgt &gt;= cc.rgt *** * When inserting a new element, the table may have to be rebuild; especially when adding high level parents. From the example above, adding "Children" to "Clothing" would require a whole new range to be built for it under clothing. 
As is it doesn't bring back any data I know I have at least 1 truck that fits the criteria.
Hmm, ok. I'd need some test data to see whats wrong. I may play around with it more later. Does your test vehicle have a ping prior to the current location (or could it be NULL?)
All I can think of is that your 'group by' clause is combining two amounts. Are you sure your ID's are unique?
Ahh you're right it doesn't have any location other than the one I'm currently querying for (I only have a a handful days of data in here). I'm going to build some other test data real quick brb.
I'm working in MS SQL I don't know if those functions translate. My locations look like this: 18.1m SE of El Paso, TX and 5.9m NE of Guadalupe, Chih On I-10 62.2m N of Winston-Salem, NC and 0.3m SW of Christiansburg, VA On I-81 Yonkers, NY and In Suffern, NY On I-287 97.2m W of Richmond, VA and 7.4m NE of Lexington, VA On I-64 6.6m SE of El Paso, TX and 0.7m SE of Socorro, TX On I-10 Chicago, IL and In Lombard, IL On I-355 87.6m S of Lubbock, TX and 0.6m E of Big Spring, TX On I-20
This brings back every truck in the table it's not working. I added in the final WHERE clause LOCATION LIKE '%Dallas%' but still get back every truck in the table. 
This was my end result after importing the table from a CSV file. SET SERVEROUTPUT ON; BEGIN FOR X IN (SELECT COURSE_NUMBER, TUITION_AND_FEES FROM TEST3) LOOP UPDATE DL_COURSES Y SET Y.TUITION_AND_FEES = X.TUITION_AND_FEES WHERE Y.COURSE_NUMBER = X.COURSE_NUMBER; END LOOP; END; This just made my life so much better. Days of work now takes thirty seconds... I could cry. 
Well, that makes stuff WAY more complicated. I assumed that 'location' was a fixed term, not a quasi-random string generated by some sort of GPS device for good 'human readability'. I don't think you can solve that problem easily with MySQL, unless you're willing to guess that if the location string contains the words El Paso, it's actually near El Paso. However, as you can see, there's two locations in your list that say they're near El Paso, but they're obviously different. You can also hope that if a truck is not moving, the string does nog change. At all. You'll have to find out how the string are rendered, otherwise you'll never be sure about that. If I would have to attack this problem, I would try to get coordinates in stead of strings. I assume these are auto generated from location data, so the coordinates are available. Then I would make the assumption that if the truck location changes by less than 1m (or any other cutoff, depending on how far trucks normally move), it's stationary. You'll then have to calculate the distance between each next location and determine if the truck has actually moved or not. I would not try to do this is SQL, though, but get out some Python/Ruby/PHP/C++ or whatever. BTW: thanks for wasting my time by only now adding the information that 'location' is not straightforward. ;) Also: GROUP_CONCAT doesn't 1:1 translate to MSSQL, but there are alternatives as discussed [here](http://stackoverflow.com/questions/451415/simulating-group-concat-mysql-function-in-microsoft-sql-server-2005).
* You need to use a single subquery with joins to insert this data. I've done this below, assuming all joins should be INNER. * Only use the `VALUES` keyword when you're actually typing out the values. It work work with a subquery. * In your target table field listing, you left out vendor desc. I've assumed a target field name of `investment_vendor_desc` below. Adjust accordingly. # INSERT INTO supplier_investment (house_id, month_seq, fy_month_seq, prodid, investment_supplier_id, investment_vendor_id, investment_vendor_desc, investment_fy_supplier) ( SELECT m.house_id, m.month_seq, m.fy_month_seq, m.prodid, p.supplier_id, v.vendor_id, v.vendor_desc, s.fy_supplier FROM monthly_Sales m INNER JOIN product p ON m.prod_id = p.prod_id INNER JOIN vendor v ON p.vendor_id = v.vendor_id INNER JOIN supplier s ON p.supplier_id = p.supplier_id );
Huh, I would think that would actually not even be returning all of the trucks, since it misses any truck that hasn't ever been outside Dallas. This works for me in a limited test: SELECT t.vehicle, MAX(datetime) AS LastPing, DATEDIFF(hh, MIN(datetime), GETDATE()) AS HoursAtLocation FROM [table] t LEFT JOIN ( SELECT vehicle, MAX(datetime) AS lastoutside FROM [table] WHERE location NOT LIKE '%dallas%' GROUP BY vehicle ) vo ON t.vehicle = vo.vehicle WHERE ((t.datetime &gt; vo.lastoutside AND DATEDIFF(hh, vo.lastoutside, GETDATE()) &gt; 24) OR vo.lastoutside IS NULL) AND t.location LIKE '%dallas%' GROUP BY t.vehicle
I agree with this. In such cases, I usually remove the grouping and add the IDs into the SELECT to quickly see where the duplication is coming from. Something like: select 'Account Setup' as Type , 'Paid' as Status , a.cust_id as ID ,b.cust_ID AS cust_account_ID ,c.invoice_no , datepart(mm, a.charge_date) as Month , a.total_amount as Amount from #invoice_charges a inner join #cust_account b on a.cust_id=b.cust_id inner join #cust_invoice c on a.invoice_no=c.invoice_no where b.product_class_code like '%prepay%' and YEAR(a.charge_date)=2014 and a.charge_desc='account setup fee' and c.invoice_fully_paid_yn='y' 
I would suggest having a look at the contents of the temporary tables individually before you do your final select. It's possible that you have duplicate data in them and your final select is OK.
Thanks all! I think you are correct, I have duplicates in my temp tables. Now I have to figure out how to only take each one once...
It's possible that you are reading an invoice twice. It is hard to tell without knowing the underlying relationships and schema. However, lets try to get anything with a duplicate invoice number. It may identify the accounts causing the issue in the join. DECLARE @Year INT = 2014; DECLARE @YearBegin DATETIME = CONVERT(DATETIME, @Year + '-01-01'); DECLARE @YearEnd DATETIME = DATEADD(DAY, -1, DATEADD(YEAR, 1, @YearBegin)); SELECT 'Account Setup' AS [Type], 'Paid' AS [Status], a.cust_id AS ID, DATEPART(MONTH, a.charge_date) AS [Month], SUM(a.total_amount) AS Amount, a.invoice_no AS tmpInvNum, COUNT(*) AS tmpCount FROM #invoice_charges AS a INNER JOIN #cust_account AS b ON a.cust_id = b.cust_id INNER JOIN #cust_invoice AS c ON a.invoice_no = c.invoice_no WHERE b.product_class_code LIKE '%prepay%' AND a.charge_date &gt;= @YearBegin AND a.charge_date &lt; @YearEnd AND a.charge_desc = 'account setup fee' AND c.invoice_fully_paid_yn = 'y' GROUP BY a.cust_id, DATEPART(MONTH, a.charge_date), a.invoice_no HAVING COUNT(*) &gt; 1; Research [sargable](http://en.wikipedia.org/wiki/Sargable). You can see an example here where I replaced the function on the left side with two sargable operators.
Did this query not work?
thank you, I re wrote it as INSERT INTO supplier_investment (house_id, month_seq, fy_month_seq, prodid, investment_supplier_id,INVESTMENT_SUPPLIER_NAME, investment_vendor_id, investment_vendor_desc, investment_fy_supplier, INVESTMENT_AMOUNT) ( SELECT distinct m.house_id, m.month_seq, m.fy_month_seq, m.prodid, p.supplier_id, S.SUPPLIER_NAME, v.vendor_id, v.vendor_desc, s.fy_supplier, sum(M.EXTENDED_AMOUNT - M.EXTENDED_DISCOUNT) FROM monthly_Sales m INNER JOIN product p ON m.prodid = p.prodid INNER JOIN vendor v ON p.vendor_id = v.vendor_id INNER JOIN supplier s ON p.supplier_id = p.supplier_id where M.HOUSE_ID = 59 and month_seq = 169 group by m.house_id, m.month_seq, m.fy_month_seq, m.prodid, p.supplier_id, S.SUPPLIER_NAME, v.vendor_id, v.vendor_desc, s.fy_supplier ); getting a PK violation now, but that is way closer than I was before.
I think this is getting partially there, how could we add in the amount of hours it's been at the given location?
Oops, the PK violation is my fault (probably, if all the PKs are tablename_id). That last join should be: INNER JOIN supplier s ON p.supplier_id = s.supplier_id I had it joining product to itself and it was creating a cartesian product with the supplier records.
Wait I think this is close...in the subquery at the bottom (latest2) you set the latest.datetime = the last time the truck was at my location. This means the query returns location showing the last datetime it was in Dallas not the actual current location...
I commented out the piece that limits latest2 to current location and it looks like it works.
Thanks for the reply. I've got no issues emulating the categories and subcategories at this point but what I'm really trying to do it get a count on assets by category so the GROUP BY statement is really messing me up. 
assuming the list of locations isn't needed ;with single_loc as ( select vehicle, sum(case when location like '%Dallas%' then 0 else 1 end) as [location], MIN([datetime]) as [first_datetime], MAX([datetime]) as [last_datetime] from [table] where [datetime] between DATEADD(HOUR, -24, GETDATE()) AND GETDATE() group by vehicle having sum(case when location like '%Dallas%' then 0 else 1 end) = 0 ), last_loc as ( select vehicle, MAX([datetime]) as [last_datetime] from [table] where location not like '%Dallas%' group by vehicle ), first_time as ( select t.vehicle, MIN(t.[datetime]) as [first_datetime] from [table] t join single_loc sl on ( t.vehicle = sl.vehicle ) join last_loc ll on ( t.vehicle = ll.vehicle ) where t.[datetime] &gt; ll.last_datetime group by t.vehicle ) select sl.vehicle, DATEDIFF(HOUR, COALESCE(ft.[first_datetime], sl.[first_datetime]), sl.last_datetime) from single_loc sl left outer join first_time ft on ( sl.vehicle = ft.vehicle );
edit: derp. Thank you :P
Glad it helped. I would do your update purely in SQL rather than PL/SQL (as you've done), as it will be faster. Use a correlated update and don't forget to commit : update DL_COURSES y set y.TUITION_AND_FEES = (select TUITION_AND_FEES from TEST3 t where t.COURSE_NUMBER = y. COURSE_NUMBER); commit; Also, take a look Oracle's SQL Developer, it's a free tool for doing development / management against Oracle.
yeah i found it, but thanks for checking back!
That looks like it does the trick! Great job thank you! I'm happy I asked for help this definitely required some fancier skills I appreciate your help and time. 
I just commented out the second line of the WHERE clause not the whole thing. Do you think that'll work? It looks like it might but the scenario you're talking about I'm not sure if I have enough data to check for that. 
Oh I see what you mean. So you're saying we need to remove the location check entirely form the lastest2 subquery, that way it limits the 'latest' row to its current location, rather than just when it was last in Dallas I'd agree with that now I look at it 
So now its working, but there are two final issues Im aware of with this query... 1) It works out the HoursAtGivenLocation, by the time between [the latest ping, now its in Dallas] and [the ping from when it was last not in Dallas] This isn't technically accurate. It should be showing the difference between the FIRST time it was in Dallas, and the current ping. This is doable but its a whole new subquery being tacked onto it 2) I dont know if your data is ever likely to contain more than one location, something like 'Leaving Houston' -&gt; 'Houston to Dallas Highway' -&gt; 'Entering Dallas' The 'Houston to Dallas Highway' location entry could cause some odd problems I think. I dont know how you'd work around this without assigning an actual location id field, and having some logic to infer it from the description as opposed to simply using LIKE 
Awesome I appreciate your help here. I sent you some gold. I've created a stored proc with it and I'll be testing this week, I'm pretty confident it'll get me what I'm looking for. Thanks again kind SQL master.
Use my second response to pull all the categories then just group by on those, I think something like this would work. SELECT COUNT(c.id), cc.title FROM category c INNER JOIN category cc ON c.lft &lt;= cc.lft AND c.rgt &gt;= cc.rgt GROUP BY cc.title
I deleted my comment as Id replied in the wrong order but yes, I think removing the latest2.location clause is all thats needed Thanks for the Gold :-) 
Regarding the time difference I see what you mean although I think it'll suffice for our purposes. That difference should be relatively small. The second issue could be a problem in some instances although the main implementation of this will actually have a custom string in location that says "CompanyName Yard" to denote that it's in the truck yard of this company (or within a short distance to it). So I'll use that for the initial implementation which should be very precise. I may want to extend it out to be able to show where trucks are sitting in a given location anywhere in the country but that's still TBD. At that point I may just have to build something in .Net that uses the longitude/latitude coordinates instead. 
Sweet, thanks for the gold!
Welcome! 
Ok cool, I wasn't sure if the arrival vs departure time would be an issue or not As it happens I fixed it anyway, although as always it makes the query more complex... In short, the 'arrival' subquery scans back from latest to find the first time it entered Dallas, and then uses that datetime for the HoursAtGivenLocation DECLARE @location varchar(50); SET @location = 'Dallas'; select latest.vehicle, latest.datetime, latest.location, ISNULL(arrival.datetime, latest.datetime) arrival_datetime, history.location prev_location, DATEDIFF(hour, ISNULL(arrival.datetime, history.datetime), latest.datetime) HoursAtGivenLocation from [table] latest INNER JOIN [table] history ON ( latest.vehicle = history.vehicle and latest.location LIKE '%'+@location+'%' and history.datetime = ( select TOP 1 last.datetime from [table] last where last.vehicle = latest.vehicle and last.location NOT LIKE '%'+@location+'%' and last.datetime &lt; latest.datetime order by last.datetime DESC ) ) LEFT JOIN [table] arrival ON ( arrival.vehicle = latest.vehicle and arrival.location LIKE '%'+@location+'%' and arrival.datetime = ( select MIN(arr.datetime) from [table] arr where arr.vehicle = latest.vehicle and arr.location LIKE '%'+@location+'%' and arr.datetime &lt; latest.datetime and arr.datetime &gt; ( select TOP 1 last.datetime from [table] last where last.vehicle = latest.vehicle and last.location NOT LIKE '%'+@location+'%' and last.datetime &lt; latest.datetime order by last.datetime DESC ) ) ) where latest.datetime = ( select TOP 1 latest2.datetime from [table] latest2 where latest2.vehicle = latest.vehicle order by latest2.datetime DESC ) and latest.location LIKE '%'+@location+'%' and DATEDIFF(hour, ISNULL(arrival.datetime, history.datetime), latest.datetime) &gt;= 24 Fun problem anyway :-)
I'm having trouble getting my brain to finish this one properly, but if you're on SQL 2012 here's an approach you can take. By partitioning on the vehicle to create a sequence number, we know the vehicle hasn't moved since the last ping. And doing the LAG inside the CTE allows us to easily get duration since last ping. You may have to take this and throw it into a temp table to finish it off. WITH PingCTE AS ( SELECT vehicle, location, LAG(pingdatetime) OVER (PARTITION BY vehicle ORDER BY pingdatetime) AS PrevPingDate, LAG(location) OVER (PARTITION BY vehicle ORDER BY pingdatetime) AS PrevLocation, ROW_NUMBER() OVER (PARTITION BY vehicle ORDER BY pingdatetime) AS Sequence, pingdatetime AS PingDate FROM mock_data) select p1.PrevPingDate, p1.PingDate, DATEDIFF(hh,p1.PrevPingDate,p1.PingDate) TimeSpentInLocation, p1.vehicle, p1.location FROM PingCTE p1 INNER JOIN PingCTE p2 ON p1.vehicle = p2.vehicle AND p1.location = p2.location AND p1.Sequence = p2.Sequence + 1 ORDER BY vehicle,PingDate
No, there isn't. I would think of a CTE as a temporary view. It's a black box to the query that uses it, just like a view. But you can just turn the CTE SQL into a subquery.
Based on your description, you _count_ duplicates regardless of the approval and the name so I adjusted your subquery slightly. The second one removes any instances of an approved duplicate: SELECT a.Address1, a.Name FROM vw_StoreCompanies a WHERE a.Deleted = 0 and EXISTS ( SELECT 1 FROM vw_StoreCompanies b WHERE a.Address1 = b.Address1 and b.Deleted = 0 HAVING COUNT(*) &gt; 4 ) and not EXISTS ( SELECT 1 FROM vw_StoreCompanies b WHERE a.Address1 = b.Address1 and b.Deleted = 0 and b.Approved = 1 )
I'm not able to test it right now, but it looks like exactly what I was looking for. Honestly I'm not sure why I didn't think to try that! Thank you so much for your time and for the help. I really appreciate it!
you were my first and you'll always be my first
This post covers equi, non-equi and self-joins within the context of the the INNER JOIN clause.
Not working :( I think the issue is that it's not properly grouping duplicates in the query. I tried adding GROUP BY Address1, Name into the exists and not exists area, but it's still not getting everything grouped together. The not exists part that you added seems to remove 1 instance of an approved company, but leaves the others that didn't properly group up. I'm not sure if it makes a difference, but not all of the duplicate entries are close together in the database. I've been messing with the query for an hour and am getting nowhere :( any other ideas? I'm seeing multiple entries for companies that are approved in my results, when I shouldn't see any of them at all
Your inner cursor seems redundant. I do not understand why you are creating a cursor off the same data that is in your outer cursor.
It is redundant for the time being, but it won't be when i'm done. I'm trying to structure a couple of things in order; Offline all DBs, change paths for some, physically move some, online all DBs. At this time it's just some print statements for my own logical processing, and i'm trying to figure out specifically why the cursor doesn't continue.
Your first cursor (movedb) FETCH statement is outside the WHILE loops, so it will fetch only 1 set of records.
This part looks suspect: header.echd_number = detail.ecdt_number I don't know your field names but I'm guessing your detail table will have a echd_number field. That's what you'd want to join on. So header.echd_number = detail.ecdt_number would become header.echd_number = detail.ec**hd**_number
I'm only actually doing tasks in the inner loop at the moment though, print alter and print convert. 
You will only get one print "ALTER statement" here because the bolded code will only return 1 row: OUTER LOOP START DECLARE dbpath as cursor FOR **SELECT @database_ID, @file_id, @name, @logicalname, @newpath** OUTER LOOP END A variable can only hold one value. When you FETCH NEXT through the movedb cursor, it will over write the values in @database_id, @name, ect but they still each just hold one value. This is why I said that opening a second cursor is redundant. 
I can see why that would be confusing, but that JOIN is actually correct. Within the software system in question, the *echd* part of *header.echd_number* is simply a way to prefix the name of the columns in such a way that they correlate with the table name. This is necessary as the database was designed in a sloppy fashion, and the code in their software probably references columns without specifying table names. The *ecdt* is detail, and the *echd* is the header table. They are the same field, just named differently across tables. Needless to say, I hate working with this system and everything about it sucks. EDIT: A little more explanation 
Oh, i think i get it. So in my nested cursor I have to call everything again and I basically can't reuse variables declared in the outer? Here's what im trying to accomplish 1st cursor define databases and paths 2nd nested cursor offline all dbs 3rd nested cursor alter db statements 4th nested cursor move files part of 1st cursor online all DBs close 1st cursor 
You can remove detail.ecdt_act_amt from the group by clause. The SUM is already grouping by that field. 
Yeah it was obvious what you were trying to do but yes, as soon as you declared your first cursor you were going to get an output of define 1st databases and paths offline 1st database alter 1st db statements move 1st db files ect ect define 2nd database and paths ect ect Thats just how the cursor was defined. **What you can do if you want is use a table variable and just create your dataset.** DECLARE @movedb as TABLE ([database_id] tinyint,[name] varchar(30),[fileid] tinyint, recoverymodel varchar(30)..... ect ect) INSERT INTO @movedb Select db.database_id, db.name, mf.file_id as fileid ---add the rest of the select statement you used to populate the movedb cursor in your original code Then just start at your dbpath cursor and do DECLARE dbpath CURSOR FOR SELECT [database_id], [fileid], [name], [logicalname], [newpath] from @movedb OPEN dbpath FETCH NEXT FROM dbpath into @database_id,@fileid, @name,@logicalname, @newpath For each cursor just reuse the @movedb table variable. That way each cursor creates a statement for the same db's in your movedb temp table.
Well done. I like the order used for introducing each of the concepts. So many join examples only ever use just one column from each table--nice to see it done with two columns in the tables! A follow up article on the different types of joins should be brief after this--the syntax is largely the same...just need to show how LEFT OUTER &amp; RIGHT OUTER return different result sets.
your @param1 that you defined in @ParamDef isn't anywhere in your @SQLString. This is what you're passing @order_id to with the @param1 = @order_Id in the sp_executesql line. Should likely be something like DECLARE @SQLString NVARCHAR(50) = N'EXEC dbo.StoredProc @param1' see the examples in [this article](http://technet.microsoft.com/en-us/library/ms175170) for examples of passing parameters. Here's my test code that should show you the issue. Change @order_id to anything other than a 1 to show the effect. DECLARE @order_id INT = 1 DECLARE @SQLString NVARCHAR(50) = N'select 1 where 1 = @param1' DECLARE @ParamDef NVARCHAR(50) = N'@param1 NUMERIC' EXECUTE sp_executesql @SqlString , @ParamDef , @param1 = @order_id edit: formatting, example and fixed the link to the article.
Thanks guys, it looks like your suggestions worked. Now to deploy the finished report to SSRS..
Thank you for your help, I really appreciate it and I like your idea of a table variable so that I can reuse the input. This is what i came up with in the end. All i have to do is change my in db names when i choose to, change my destination path, and then my prints to executes. https://gist.github.com/anonymous/d9aac0a8a2016405436d
Is it that closed paren in `article_content`='".$content."')??? I dont see an open 
Looks good. I'm glad it worked out for you.
You need to remove a foreign key from either Table1 or Table2 to form a valid relationship. Also, Table2 and Table3 share the same primary key, which means it's likely that you don't need to be storing Site_ID in both tables. There's a good chance this isn't normalized form.
It's very possible that I work with a different version than you, but I was under the impression that you needed to create [volatile tables](http://teradatasql.com/how-do-i-create-a-temp-table-in-teradata/) in teradata. Have you tried this? 
also useful if you're consistently using a certain column to group by when aggregating. 
How would I actually execute those statements that are in @movedb now that they're in order from a set based operation? I had just copied and pasted from the other cursor, you can ignore the xp cmdshell.
Were you planning on the query to eventually Execute the line statement rather than just print it? If you were intending to just use it to build a statement change your result output from ["Results to grid" to "Results to text"](http://www.mssqltips.com/tipimages2/2795_SSMSToolBar.jpg) and execute the query and it will just give the results in text that you can copy/paste from. If you want it to execute each line, I would probably just stick to the cursor you wrote and change it from print to Execute ('ALTER DATABASE SOMETHING') 
change AND to a comma, and lose the unbalanced parenthesis
2 and 3 aren't quite the same. Contact_ID and Cont**r**act_ID
Yes I'm going to be changing the prints to execs and running the whole thing, the prints are just so I can see the order and verify syntax. Thanks for taking the time to do it in a set though, still learning so every bit helps. I was close to changing the whole thing to a msforeachdb script, so I could have probably used your output with that, but I'll probably just stick to the cursor. Thanks again
&gt; if you want to look up people on dorchester drive, you need an index on address Sorry to be a pedant (but then again, if databases don't encourage pedantry, I don't know what does), but depending on how the address is stored, an index probably won't help there. For example, if the address is split up (as is fairly standard) into `varchar` (or `char`, or there `n` equivalents) fields `addr1, addr2, city, state, zip, country`, an index on `addr1` won't help with `LIKE '%dorchester%'` (pedantry on pedantry: if you know you're looking specifically for dorchester drive a whole lot, look into filtered indices). An index on `city` though will help with `WHERE city = 'New York'` or even `WHERE city LIKE 'New%'`. On the other hand, if the address splits the house number into its own field (`number, street_name, addr2, city, state, zip, country`), then an index on `street_name` will help find dorchester drive.
Hmm. What is considered to be a 'duplicate'? The 1st subquery is looking for the duplicated Address1 values ("duplicate entries that have more than 4 matches on the "Address1" field"). 
You are absolutely correct, sir! I'll implement that tomorrow morning.
I've seen this simplified in the past by, for e.g., adding `read_by_user_1` and `read_by_user_2` as boolean columns to the `replies` table.
My comments are based on MSSQL as that is my strong suit. 1. There are a few type of indexes, we are primarily concerned with CLUSTERED and NONCLUSTERED. 2. CLUSTERED INDEXES define the physical layout of the data it self. A clustered index IS the base table it self, unless you don't have one defined, then that is a HEAP. By default when you indicate a PRIMARY KEY, you have just defined your clustered key, you can override this, but that is beyond this post. Your Clustered Index key will always be included in all NONCLUSTERED indexes so it has a way to relate back to the actual data. If you were looking at a row of books in the library. Dewey Decimal numbers are your primary key. They define an ordered way to physically place the book, this is a clustered index. Now imagine that sale they have every now and then where all the old books are just tossed into a giant bin with no order and no method to the madness. This is a heap. Now imagine all the work it takes to go through every single book in the heap to find two. That is a lot of work. Please use CLUSTERED INDEXES. 2. Nonclustered indexes are used to help queries navigate the data more efficiently. Think of these as the card catalog in the library. When you want to look up all books by Hemmingway, you don't want to go to every book and open it up and look at the author do you? No you want something that already has that information defined and organized for you. On that card catalog you can have all types of useful information, you can order those card catalogs by whatever information makes most sense. You could order them by Author, then Title, or Year, Author, Title or any other combination of data about that book. These are your indexed columns. You also have the option of including extra data. For instance if we want to add the publishers name, but we know we'll never search on it, we can add that to the card catalog as well. We don't worry about how that's sorted because we won't be using it to do lookups, we'll just be referencing that data once we've found that specific card. Using included columns like this allows us to answer all the portions of our query, without having to go to each book to pull that extra information, thus saving work. Imagine if you go through the card catalog and you have 4/5 pieces of information, but for that last piece you have to go to the book shelves, pull each book, extract that ONE piece of information. Wow that would be a lot of work when I could just add that extra information to my NCL index. One thing to remember about queries is that each field that you index and include is a separate copy of the original data. So having too many copies of the data is a bad thing. Imagine some corrective page were sent from the printing company and you need to update the data on a single book and part of that they updated the published year. Any index that contains that day now has to be corrected as well, which takes time. Too many indexes are bad, there is a fine art to index tuning. Now imagine that library shelf again (SQL calls this a page), with the books all squeezed in. YOu get a brand new book that needs to go into that shelf. When you ask SQL to add that new book, it will figure out where it goes and realize there is no room, it will take all the books on that shelf AFTER the new book and move them to a new empty shelf at the end of the library. This is called a page split. Now imagine you are going to every book and reading the Dewey decimal numbers when you hit this shelf there's a little note saying, ohh yeah, this second was moved way at the end of the library go there, then come back here and finish reading all the numbers. This is the cause of index fragmentation. When a DBA says they are doing index maintenance, they are essentially shifting all the books to put them back in order. With that we can also adjust how full we make each shelf row, this is called Fill Factor. If we leave a little bit of room, we don't have to shift all those books do we? We can simply add the book in place nad walk away, quick and simple. There are some considerations to worry about with this as its wasted space, but that's for another library lesson. Basically, indexing is just as important as they layout of your schema. Poor layouts and/or poor indexing strategies will give poor performance. 
Enjoy your SQL injection hacks! This problem, as well as the security vulnerability, would be prevented if you used [prepared statements](http://php.net/manual/en/mysqli.quickstart.prepared-statements.php) instead of string concatenation. http://xkcd.com/327
[Image](http://imgs.xkcd.com/comics/exploits_of_a_mom.png) **Title:** Exploits of a Mom **Title-text:** Her daughter is named Help I'm trapped in a driver's license factory. [Comic Explanation](http://www.explainxkcd.com/wiki/index.php?title=327#Explanation) **Stats:** This comic has been referenced 291 times, representing 1.0257% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcdcomic/)/[kerfuffle](http://www.reddit.com/r/self/comments/1xdwba/the_history_of_the_rxkcd_kerfuffle/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_cjbyh00)
Seems pretty solid (Aaron Bertrand definitely knows what he's talking about), as long as you take his caveats into consideration and account for them in your process. He has workarounds for the major gotchas. The proc created in the script only produces the `sp_rename` queries, it doesn't actually run them. So you'll be able to review everything to make sure it's safe before you do it.
This is a query I had saved months ago that I used to perform something similar to this with indexes(used it to generate SQL and pass it to EXEC). Hope it helps. *** SELECT TableName = t.name, IndexName = ind.name, IndexId = ind.index_id, ColumnId = ic.index_column_id, ColumnName = col.name, ind.*, ic.*, col.* FROM sys.indexes ind INNER JOIN sys.index_columns ic ON ind.object_id = ic.object_id and ind.index_id = ic.index_id INNER JOIN sys.columns col ON ic.object_id = col.object_id and ic.column_id = col.column_id INNER JOIN sys.tables t ON ind.object_id = t.object_id WHERE ind.is_primary_key = 0 AND ind.is_unique = 0 AND ind.is_unique_constraint = 0 AND t.is_ms_shipped = 0 ORDER BY t.name, ind.name, ind.index_id, ic.index_column_id ***
Thank you for your detailed response!! Could you explain what the difference between single and composite indices are? If I do a composite index, say on author and publisher, is it the same as doing two single indices on author and publisher, or am I creating an index that "combines" author and publisher? So, essentially, I want to use indices for things that I am accessing a lot?
Have it return the value in a column with a flag and error code or just use a table and INSERT to it.
Using logic from [TRY...CATCH](http://msdn.microsoft.com/en-us/library/ms175976.aspx), I'd do something like below. Or if you have a huge spaghetti mess of code and don't want to put the effort to put try catches every where you could use [Extended Events](http://sqlblog.com/blogs/davide_mauri/archive/2013/03/17/trapping-sql-server-errors-with-extended-events.aspx) to capture the errors too. PRINT 'dbo.ErrorLogging'; GO ----Optional reset --IF OBJECT_ID('dbo.ErrorLogging','U') IS NOT NULL -- DROP TABLE dbo.ErrorLogging; --GO IF OBJECT_ID('dbo.ErrorLogging','U') IS NULL CREATE TABLE dbo.ErrorLogging ( ID INT NOT NULL IDENTITY PRIMARY KEY, EntryDate DATETIME NOT NULL, EntryLoginName NVARCHAR(128) NOT NULL, ErrorNumber INT NULL, ErrorSeverity INT NULL, ErrorState INT NULL, ErrorMessage NVARCHAR(4000) NULL, ErrorProcedure NVARCHAR(128) NULL, ErrorLine INT NULL ); GO PRINT 'dbo.LogError'; GO IF OBJECT_ID('dbo.LogError', 'P') IS NULL EXEC ('CREATE PROCEDURE dbo.LogError AS SELECT NULL'); GO ALTER PROCEDURE dbo.LogError AS BEGIN INSERT INTO dbo.ErrorLogging ( EntryDate, EntryLoginName, ErrorNumber, ErrorSeverity, ErrorState, ErrorMessage, ErrorProcedure, ErrorLine ) SELECT GETDATE(), SUSER_SNAME(), ERROR_NUMBER(), ERROR_SEVERITY(), ERROR_STATE(), ERROR_MESSAGE(), ERROR_PROCEDURE(), ERROR_LINE(); END; GO BEGIN TRY SELECT 1/0; END TRY BEGIN CATCH EXEC LogError; END CATCH; GO SELECT * FROM dbo.ErrorLogging; GO 
A composite index will combine them, but by sorting on first one then the other. So, if you have a composite index on two fields, `fldA` and `fldB`, in that order, that index will help for filtering on just `fldA` and for filtering on `fldA` AND `fldB`, but it will not help for filtering on just `fldB`. Similarly, swapping around the order of the fields in the index would make it help for `fldB` alone, but not `fldA` alone. In the case of a table with two indices one on each column, a query filtering on both will likely use just one index (which one will be used will be determined by whichever the planner thinks is most useful). A further caveat: even if you are always filtering on both fields, the order of the fields in the index may matter. For example, say `fldA` will take values 1, 2, and 3 more-or-less equally, and `fldB` can take any `int` value between 0 and 100000, spread out more-or-less evenly. Let's further assume our typical use case will be `WHERE fldB = @b AND fldA = @a` or `WHERE fldB = @b GROUP BY fldA`. In that case, the index on *first* `fldB` *then* `fldA` will be a lot more useful as it can eliminate far more on the first pass, as it were.
Hope it works out well for you -- SSIS afaik uses either OPENROWSET or some internal stored procedure magic to do the insert. Regardless, it goes very fast if the table is just a heap. With indexes it can slow down a bit, but there are ways around that, and even then, it's still quite fast. I can do a quarter million wide rows into an indexed table per second on my dev machine.
You execute the stored procedure, then execute the T-SQL that it produces (copy/paste, pass into `sqlcmd` or `invoke-sql`, etc.)- **after looking it over**.
Its not free but its pretty cheap if you buy the old data. http://www.armchairanalysis.com/data.php $35 for all stats from 2000-2012 They also have sample data there so you can see what you are buying for $35. Its all csvs as well.
moved on, using a powershell script now, using Select * from $DB.dbo.sysfiles to populate a variable to loop through and create a full statment to restore with. This is working now
Thanks so much for the answer. Everything I have found online has explained it in such an overly complicated way. Thanks for putting an end to my probably 2 hour hunt thought numerous resources today!
Ow and you don't happen to be a active member of stackoverflow do you? I have also asked this question on there and have then aswered the question myself linking back to this post on reddit. You are more than welcome to answer it and claim your rep :) http://stackoverflow.com/questions/25044097/view-resolution-definition/25044637#25044637
Couldn't you just add a single bit column to replies_read_by whereby you could just set it true if the reply was read by the user? Would make querying easy too; just get all the replies where read_by user X is false. Also, just a side note; do you have a user table with id's that you could use in place of a string for username? IE: replies_read_by would just look like this id | reply_id | user_id | read ---|---|----|---- 1 | 1 | 1 | 0 2 | 1 | 2 | 1 3 | 3 | 1 | 1 
I know. It was me that answered the question. You are more than welcome to provide it and I will mark it as the answer :)
Misread what you said. Sure, I'd love points on Stack. I can barely do anything at the moment on stack overflow due to low points. :P
Wow that looks very elegant I was hoping to give it a go but unfortunately I'm on 2008 at the moment :( Thanks for your time though I appreciate this sub.
Here's the basic syntax for the first half of your question: SELECT * FROM [XTABLE] INNER JOIN [YTABLE] ON [XTABLE].[VALUE] = [YTABLE].[VALUE] INNER JOIN [ZTABLE] ON [XTABLE].[VALUE] = [ZTABLE].[VALUE] For the second half of your question you could easily doing that by using an ORDER BY clause and only retrieving the first record (you can use TOP if you're in SQL Server or a Fetch 1 First Rows Only if you're in another DB). You could then return the id of that value via a select and pass that into a where clause in your joined statement above.
guttermonkey's answer is correct for the first issue. On the second, that would work, but a simpler approach for a newbie might be to use a GROUP BY clause on the person, and select the MAX(visit date) for each person. something like: SELECT person,MAX(visit_date) FROM [xtable] GROUP BY person This will give you the highest value of visit_date for each person. (One pitfall to avoid: if you want other values in your result along with the person, add those values to your GROUP BY as well. Anything that isn't a calculation, like the MAX() or other functions, must be in the group that defines the calculation.)
Good point although I found when I was starting that GROUP BY confused me for some reason compared to doing an inner query.
Thank you, guttermonkey. I tried your select statement and I get an Ambiguous column name error on the first line of my select statement. Can I use the same value for [XTABLE].[VALUE] in join 1 and 2? Here is a sample of my query. SELECT Player AS 'Player Name',... FROM Player (nolock) INNER JOIN Account (nolock) ON Player.Player_ID = Account.Account_ID INNER JOIN INFO (nolock) ON Player.Player_ID = INFO.Account_ID
That worked markusrtk. Thank you for the help. I think my other issue is with the inner join is that the columns that I was using do not correspond to one another. 
Thanks Guttermonkey. I think that my original second inner join was on the wrong column. 
You're welcome, thanks again for Gold I don't use MSSQL so much these days (MySQL now), but I cant resist a good SQL puzzle :-)
You can still use it on 2008 if you remove the LAG lines. Then you'll need to do a DateDiff in the bottom query to get the number of hours between pings.
&gt; WHERE 'Center_ID' = 'A' OR 'B' OR 'C' OR 'D" This won't work, for two reasons: * SQLite's quote management is really lenient, but in this case, it'll match anything where the *string* 'Center_ID' matches the *string* 'A' [ie, never] * And your syntax for OR isn't correct, so it'll also match anything where 'B' [ie, everywhere] * And because you OR'd it together, you'll get *everything* For your WHERE clause, you need WHERE "Center_ID" IN ('A', 'B', 'C', 'D'); Then to answer your actual question ['all data from all customers who ever called one of these call centers'], a subquery is the easiest way to go: SELECT * FROM call_data WHERE Customer_ID IN (SELECT Customer_ID FROM call_data WHERE "Center_ID" IN ('A', 'B', 'C', 'D') );
You need to specify that you want the maximum gdp in Europe. SELECT name FROM world WHERE gdp &gt; (SELECT max(gdp) FROM world WHERE continent = 'Europe' ) 
Thanks this worked! Do you mind if I run one more by you? I am such a noob at SQL
Thanks!Same table, this is the question : "Find the largest country (by area) in each continent, show the continent, the name and the area" Not sure how to even approach this one since multiple countries have the same continent
Here you go. select w.* from world w join ( select continent ,max(Area) as maxArea from world group by continent ) m on w.continent = m.continent and w.Area = m.maxArea; Note: if two countries happened to have the exact same area and they are also the largest in their continent, this would show both. I'm signing off, but good luck with the rest of your questions!
Thanks for all your help! :)
great resource SQLzoo ....bit of advise ...google "SQLZOO" and the question plenty of sites have answer pages :) may be helpful to you.
Wow thanks man!Going to try this now, the most annoying thing is not having answers!thanks again for the tip :)
Ill try this out when I reach the office later, thank you. I'm hoping this will give me the list of customers who contacted one of my centers and show me their whole history. I did this manually in Access but it took me hours. We had about 200,000 unique customers who contacted us since June 01 but about 40 percent of them had a repeat call within 30 days. Often these calls were made to another call center (not A B C or D) which is why the file size is so big. 
i know ...i had to learn it for my new job ...was running around like WTF lol ....there was one on the joins that wouldn't show as correct even though it is so just be aware :)
Thanks man, just by the way, are there any other online SQL resource you used?
I'm not sure what you mean by ''. An empty string is not a `smallint`. Are you talking about `NULL`s? In that case, just make the column `NOT NULL` with no default.
CREATE FUNCTION dbo.fnCalcDistanceKM(@lat1 FLOAT, @lat2 FLOAT, @lon1 FLOAT, @lon2 FLOAT) RETURNS FLOAT AS BEGIN RETURN ACOS(SIN(PI()*@lat1/180.0)*SIN(PI()*@lat2/180.0)+COS(PI()*@lat1/180.0)*COS(PI()*@lat2/180.0)*COS(PI()*@lon2/180.0-PI()*@lon1/180.0))*6371 END above should maybe work in mySQL, does mySQL have a GEOGRAPHY data point similar to MSSQL? if so: try something liike DECLARE @orig_lat DECIMAL(12, 9) DECLARE @orig_lng DECIMAL(12, 9) SET @orig_lat=53.381538 set @orig_lng=-1.463526 DECLARE @orig geography = geography::Point(@orig_lat, @orig_lng, 4326); SELECT *, @orig.STDistance(geography::Point(dest.Latitude, dest.Longitude, 4326)) AS distance --INTO #includeDistances FROM #orig dest
Basic questions: What SQL are you using? How much data do you have? I would expect that query to run really slow for large datasets. Having a Select inside a Select can kill performance. I would have gone with a self join. Something like this: Select fst.ID, fst.insert_time, sec.update_time, datediff(mi, fst.insert_time, isnull(sec.update_time,current_timestamp)) as 'Hours Open' From (select id, insert_time from service_req_history where version = 1) fst left join (select id, update_time from service_req_history where version = 2) sec on fst.id=sec.id order by fst.id desc ;
Thanks! I have some work to, but soon you'll see those two article you envision.
I'm in the process of taking my older posts and re translating them to SQL Server. This one starts from the beginning. This is for people that don't even know what the SELECT statement is used for...
Is there a way to import only a few columns from the raw data when I originally import the table? There are probably 50 columns in the raw data but I need about 8 of them, maximum. I was trying to google for a solution and I saw that someone used Python to strip the columns they didn't need and then they imported the smaller file into SQLite. Is there a way to just import select columns originally? I don't know Python but I do have it installed if the code is easy.
Why? Example: Suppose the gdp-values of 3 european countries are 5, 10 and 12. gdp &gt; ALL(5, 10, 12) gdp &gt; MAX(5, 10, 12) IMO, both statements are equivalent or am I overlooking something?
Let me make sure I understand this. You have a situation like this: CREATE TABLE #tmp ( fld smallint); INSERT INTO #tmp (fld) VALUES (''), (1), (7); And when you run `SELECT * FROM #tmp`, you're getting: fld ------ 0 1 7 Basically, you want MS to impliment one of the comments [here](http://connect.microsoft.com/SQL/feedback/ViewFeedback.aspx?FeedbackID=260762). In the interim, you'll need to handle things like this in the application layer. My other thought though is that if 0 is not a valid value for the column, you can add a `CHECK` constraint to enforce `fld &lt;&gt; 0`. 
Yeah idk, maybe it's a SQL Zoo issue? I've never used it.
The issue is that zero is valid, but space isn't. I was having an problem because checking for space equates to true when the value is zero. But it's irrelevant because space isn't allowed for that datatype. The implicit conversion was throwing me off.
Ah! Yeah, OP mentioned that, I'd forgotten.
Well, that's quite awesome! Thanks
Where are you checking for space?
I'm just doing a validation check "Select * from table where column = ''. Anything with a zero value gets returned. 
I would write it like this: SELECT srh1.id, srh1.insert_time, srh1.update_time, DATEDIFF(mi, srh1.insert_time, ISNULL(srh2.update_time, srh1.update_time)) AS MinutesOpen FROM service_req_history AS srh1 LEFT OUTER JOIN service_req_history AS srh2 ON srh1.id = srh2.id WHERE srh1.version = 1 AND (srh2.version IS NULL OR srh2.version = 2) AND DATEDIFF(mi, srh1.insert_time, ISNULL(srh2.update_time, srh1.update_time)) &lt;&gt; 0 
That's because SQL Server coerces empty (and space only) strings to 0 when evaluated in a numeric context. There's nothing you can do about it until they decide to implement an `OPTION STRICT` or something. Implicit type conversions between numeric and non-numeric types is, in technical jargon, gross.
I think you might be missing that '' is not stored in the database. When the previous inserts happened. It actually converted them to 0. When you are running your query it is converting your '' to 0 ***before*** it hits the actual stored data. There is no way to differentiate if a 0 was actually a 0 or a blank string when it was inserted. If it was '' at the time of insert it converted it to 0 before putting it in the database.
Don't bother. Are you asking because of performance issues? If so, put some indices on there. I'd suggest starting with an index on customerid
This sounds like a job for `CROSS APPLY`: SELECT c.cust_id , a.vendor_name , c.date_connected FROM customers AS c CROSS APPLY ( SELECT TOP 1 vendor_name FROM customers WHERE cust_id = c.cust_id AND date_connected &lt;= c.date_connected AND vendor_name IS NOT NULL ORDER BY date_connected DESC) AS a
My pleasure. One caveat: `CROSS APPLY` can be kind of slow.
It worked and only slowed me down ~3 seconds - I'll take it. Thank you again.
This is a recursive calculation and can be described mathematically : g = greatest(..) b{n} = g(a{n} + b{n-1}, 0) = g(a{n} + g(a{n-1} + b{n - 2}, 0), 0) =&gt; g(a{n} + g(a{n-1} + g(a{n-2} + g(...., 0), 0), 0) So as an example, it can be expanded as follows : b{5} = g(a{5} + g(a{4} + g(a{3} + g(a{2} + g({a{1}, 0), 0), 0), 0) You could do this with recursive common table expressions (CTE) but the poster says he's using 10g which doesn't support that. The two solutions that work are both implementing the required recursion : one using MODEL and the other a user defined aggregation function. I'm not sure you could do this with analytics, because the "greatest" operation needs to be applying to the history of values.
You could use aggregate functions for both getting your most recent date and eliminating NULLs: SELECT cust_id ,MAX(vendor_name) vendor_name ,MAX(date_connected) date_connected FROM customers GROUP BY cust_id This works for the sample you provided, but depending on what your actual data looks like it may not be ideal. I would expect it to perform better than the CROSS APPLY.
My first idea would be a SP, depending on how your application communicates the actions to your SQL. You could easily have it take the last input, and update the other database with the same information. A trigger could work as well, but they are generally not used anymore, as they hurt performance quite a bit. Which SQL version are you using?
Thanks for the info , currently we are using 2008 R2 though we are moving to 2012
That won't work if the customer has had more than one prior vendor name unless they happen to be in increasing alphabetical order over time. Consider this case: cust_id vendor_name date_connected ------- -------------- -------------------------- 283504 Zebra Sales 2013-01-01 00:00:00.000 283504 Direct Sales 2013-11-27 00:00:00.000 283504 NULL 2014-07-01 00:00:00.000 Your query would yield cust_id vendor_name date_connected ------- -------------- -------------------------- 283504 Zebra Sales 2014-07-01 00:00:00.000 The way to do it with a `GROUP BY` would be to select `MAX(date_connected)` and then join back to the table to look up the `vendor_name` on that date.
That's why I said "depending on what your actual data looks like it may not be ideal". It's not clear to me from the original question whether data like this could exist nor what the desired output would be if it did. Thanks for the downvote though.
I downvoted you because your answer was wrong: without very strict assumptions about the data, it will give incorrect results. To be clear, it will be wrong, not simply "not ideal." This is not just a failure to be robust.
So your assumption that the most recent vendor_name should replace the NULL is more correct? Give me a break. The question was vague and I qualified my answer with a warning.
Yes, using the most recent non-missing value is a more reasonable assumption than pulling the one with the highest sort value based on the column's collation.
I would also suggest to go with using a procedure to just update the records in the other DB. That's a much better solution that using a trigger.
IIRC, in Access, you can do e.g. `SUM(IIF(tblClient.Gender = 'M', 1, 0)) AS Males`.
SELECT gender, COUNT(*) as qty FROM tblclient GROUP BY gender;
Aah that's a shame, but hey atleast it's something I guess huh
I am having the same problem. I have been looking all over for the last two days. If I do manage to find anything with my google-fu, I will be sure to pass it on. 
That would require multiple queries, one for each criterion set.
restore a backup?? check brent ozar
If you are facing any difficulty with project server after fully server crash, drive failures, fault in saved database, accidental file deletion, etc, then you can take help SQL Server Repair Toolbox of this http://www.sqlrepair.sqlserverrepairtoolbox.com/ tool can efficiently deal with all corruption issues and further restore entire project information, like project resources, task schedule, tasks, custom fields, etc from different project workspaces.
So you want (Not America and Not Frank). So that means you do want (America and Bob), and you want (Asia and Frank)? Where 1 = CASE WHEN a = 'America' THEN CASE WHEN b = 'Frank' THEN 0 ELSE 1 END ELSE 1 END
Ahh... you are right. I didn't notice the "not a list of records" part. 
Thanks! This worked perfectly, you rock! o/\o
What do you mean? I caught one error, but not sure if it makes sense.
This is a nifty trick to use in SQL Developer, but it will not work in Access.
&gt; My work is assigned by the last two digits of the "Request Number." It should be split up into separate columns. That is bad.
Which part? The "trick" is in the algorithm to do the matching, not the specific syntax. I gave you my whole script to show how I tested. The actual `SELECT` query is all you should need and and it should work with Access once you put in the correct table names. You may need to change how the string parsing is done to make Access happy. Or, ditch Access and get a real database.
I think I started working too early or I'm going crazy. I had seen the "Cross Apply" which worked in SQL Developer, but does not in Access. And if only it was possible to use a real DB. I can do some things easier in SQL Management Studio or Oracle SQL Developer, but that's not an option for the project I'm working on.
Ah I see. That would be a simple update to pull the 2 digits from the right.
 SELECT t.Y, CASE WHEN COUNT(t.X) BETWEEN 0 AND 100 THEN '0-100' WHEN COUNT(t.X) BETWEEN 101 and 200 THEN '101-200' WHEN COUNT(t.X) BETWEEN 201 and 300 THEN '201-300' ELSE '-' END as X from mytable as T GROUP BY Y 
I had originally written it as a `cross apply`, then re-evaluated &amp; tested the version you see now.
ahh gotcha
If you can't use a real DB, at least fix the schema that you do have in Access. As I said before, what you have right now is a disaster.
Can you run this? *** USE YOUR_DATABASE_NAME GO ALTER DATABASE YOUR_DATABASE_NAME SET SINGLE_USER GO DBCC CHECKDB('YOUR_DATABASE_NAME', REPAIR_REBUILD) GO ALTER YOUR_DATABASE_NAME SET MULTI_USER GO *** Edit - Obviously take a copy/precautions before doing this. 
It is also worth noting that you may have to restore a backup (as supermario182 mentioned) then run the above as a just in case. 
Thanks I'll try that out!
Well, that makes the join a little cleaner, but it makes the schema even *worse* because now you've got duplicated data that you'll have to maintain and &amp; verify consistency on. In SQL Server, you could make Term Digit a computed column (persisted or not) to eliminate the maintenance/verification.
So you aren't backing up your database on the regular? If not, then why do you care that your data file is corrupted? 
This works perfectly! Thanks a ton! The only problem is that I cannot generate a report because (error message) "Multi-level group by clause is not allowed within subquery." No matter though, the query gives me everything I need!
Thanks a ton. I manually checked and everything matches up. Huge time saver. I can see other applications for this too, like instead of Call Center I can replace that with Reason for Call so I can get the full history of any customer who eventually called in about X issue Thanks again 
Using SQL Server, target has to be a single table. The source may be many tables. 
Doing a Select Into will create a table on the fly with the columns specified in the order listed. EXample Select name.FirstName, Name.LastName, Address.AddressLine1, Address.AddressLine2, Address.City, Address.State, Address.Zip INTO NameAndAddress from Name Inner join Addresss on name.AddressID = Address.AddressID This statement will create a table called [NameandAddress] It does NOT modify the Tables [Name] or [Address]. Doing a select into is useful many situations, but over-using it tends to create sloppy SQL code and other issues down the line. Nothing I think you need to worry about though. 
I am a bit confused by your question but I will give it a try. I am going to assume this: your data table is like this: Create Table myTable (X int) Assuming the Table has 100 Rows of Values 0 to 1000 ---------------------- Now let's take a look at your first attempt. SELECT t.Y, CASE WHEN COUNT(t.X) BETWEEN 0 AND 100 THEN '0-100' WHEN COUNT(t.X) BETWEEN 101 and 200 THEN '101-200' WHEN COUNT(t.X) BETWEEN 201 and 300 THEN '201-300' ELSE '-' END as X from mytable as T GROUP BY Y Issues- There is no Column Y in the table The Case statement is counting the number of Rows in the table, not the number of rows between a given range. Depending on how you want the results to look you could do it a few ways. Ask 4 Developers how to do something and you will get 12 answers :) You want a result that looks like this: 0-250 251-500 501-750 751-1000 ------ -------- -------- -------- 10 30 52 8 select sum(case when x between 0 and 250 then 1 else 0 end) as [0-250] ,sum(case when x between 251 and 500 then 1 else 0 end) as [251-500] ,sum(case when x between 501 and 750 then 1 else 0 end) as [501-750] ,sum(case when x between 751 and 1000 then 1 else 0 end) as [751-1000] from #mytable here the case is returning a 0 or 1 depending on if the number falls within the range, 1 for each time the number falls in the range, then summing up those 1's gives you the total number of values. If you wanted the results like this: Band Count ------- ------------ 0 - 250 10 251 - 500 30 501 - 750 52 751 - 1000 8 you could do this: select case when x between 0 and 250 then '0-250' when x between 251 and 500 then '251 - 500' when x between 501 and 750 then '501 - 750' when x between 751 and 1000 then '751 - 1000' else '' end as Y, count(*) from #mytable group by case when x between 0 and 250 then '0-250' when x between 251 and 500 then '251 - 500' when x between 501 and 750 then '501 - 750' when x between 751 and 1000 then '751 - 1000' else '' end 
There's a Firefox plugin that's really good, called sqlite manager. The icon is a blue disk array, just search for it in Firefox plugin screen 
It assumes John is a column because you are not placing the word in single quotes, which would mark it as a string. This is not a problem when using a number as a search term since MSSQL doesn't require quotes to understand its type.
dbo.Followup_Min is the actual table. @MINSearch is a temp table.
For what it's worth, you already know if @Search1 is null. You don't need these comparisons and, as a bonus, they avoid the completely un-SARGable garbage of ISNULL('blah', column)
Well, you are having issues because that's a poorly designed schema that WordPress is using. You'll need to connect each number of location together. What vendor of SQL is this for? Try something like this SELECT p.post_title AS Store Name, address.meta_value AS Address, state.meta_value AS State FROM wp_posts p LEFT JOIN wp_postmeta address ON address.post_id = p.ID AND address.meta_key LIKE 'store_location_%_address' LEFT JOIN wp_postmeta state ON address.post_id = state.post_id AND LEFT(address.meta_key, 17) = LEFT(state.meta_key, 17) AND state.meta_key LIKE 'store_location_%_state' WHERE p.post_type = 'stores' AND p.post_status = 'publish' ORDER BY p.post_title ASC
If I recall, when I first started writing it I couldn't get it to work without it. I'll give it a try when I get to work. 
If you're going to be using dynamic SQL like this, I highly recommend reading [The Curse and Blessings of Dynamic SQL](http://www.sommarskog.se/dynamic_sql.html), it will save you a lot of performance headaches. If you're going to be at the PASS Summit this fall, I also recommend attending [my session on Dynamic SQL](http://www.sqlpass.org/summit/2014/Sessions/Details.aspx?sid=6071). **Edit** - the reason I suggest this is that you can avoid your column name errors before, and you don't have to worry about attempting to prevent SQL injection with your own code because you can't. In the meantime, try something like: ALTER PROCEDURE [dbo].[FollowupSearch] @Criteria nvarchar(50) = 'NULL', @Search1 nvarchar(50) = 'NULL', @Search2 nvarchar(50) = 'NULL', @Search3 nvarchar(50) = 'NULL', @Search4 nvarchar(50) = 'NULL', @Search5 nvarchar(50) = 'NULL', @Search6 nvarchar(50) = 'NULL', @Search7 nvarchar(50) = 'NULL', @Search8 nvarchar(50) = 'NULL', @Search9 nvarchar(50) = 'NULL', @Search10 nvarchar(50) = 'NULL' AS DECLARE @Command nvarchar(max) SET @Command = 'SELECT ID, Timestamp, Part, ETA, TruckInfo, ULOC, DLOC, StdPacksOnHand, MIN, LastMIN, MINStatus, Count, CSN, CountTimestamp, Counter, Comments, ClosingComments FROM Followup_MIN WHERE 1 = 1 ' + CASE WHEN @Search1 IS NOT NULL THEN ' OR (' + @Criteria + ' = @s1) ' ELSE '' END + CASE WHEN @Search2 IS NOT NULL THEN ' OR (' + @Criteria + ' = @s2) ' ELSE '' END + CASE WHEN @Search3 IS NOT NULL THEN ' OR (' + @Criteria + ' = @s3) ' ELSE '' END + CASE WHEN @Search4 IS NOT NULL THEN ' OR (' + @Criteria + ' = @s4) ' ELSE '' END + CASE WHEN @Search5 IS NOT NULL THEN ' OR (' + @Criteria + ' = @s5) ' ELSE '' END + CASE WHEN @Search6 IS NOT NULL THEN ' OR (' + @Criteria + ' = @s6) ' ELSE '' END + CASE WHEN @Search7 IS NOT NULL THEN ' OR (' + @Criteria + ' = @s7) ' ELSE '' END + CASE WHEN @Search8 IS NOT NULL THEN ' OR (' + @Criteria + ' = @s8) ' ELSE '' END + CASE WHEN @Search9 IS NOT NULL THEN ' OR (' + @Criteria + ' = @s9) ' ELSE '' END + CASE WHEN @Search10 IS NOT NULL THEN ' OR (' + Criteria + ' = @s10) ' ELSE '' END + ' ORDER BY Timestamp DESC ' EXEC sp_executesql @Command, '@s1 NVARCHAR(50), @s2 NVARCHAR(50), @s3 NVARCHAR(50), @s4 NVARCHAR(50), @s5 NVARCHAR(50), @s6 NVARCHAR(50), @s7 NVARCHAR(50), @s8 NVARCHAR(50), @s9 NVARCHAR(50), @s10 NVARCHAR(50)', @Search1, @Search2, @Search3, @Search4, @Search5, @Search6, @Search7, @Search8, @Search9, @Search10 ; RETURN 
Will this work? http://stackoverflow.com/questions/19241312/get-current-week-days-from-monday-to-sunday
Thank you for the feedback. I'll definitely be giving that article a read through.
WHERE DATEPART(WEEK,GETDATE()) = DATEPART(WEEK,smalldatetime) Is this what you are looking for?
Hard code it, don't run functions on database columns. Create a dates table that has the data generated for many years on an auto rebuild schedule.
Needs to include year as well otherwise will include results from that week in previous years. 
Can you explain a little further please? 
I think he means to have a master table with a date column and then columns for every possible date related piece of data you would ever want. So like, first day of that date's week, first day of that date's month, etc. The DBAs at my company set up a table like this and it's pretty useful/easy to just join to it. Edit: So in your case you could join current_date to it and get the current week (the Sunday previous to your current date).
Definitely a data problem, not a sql problem. The data has both a "0" and a "1" in the meta_key for postid 514, for both addresses, thus both states get returned for both addresses. With the fields supplied there are no additional ways to filter the data, other than to simply choose the first of each occurrence. I'd be looking at the other possible fields in the data sets to see if there's a better join key. I'd bet there's a key combination available that would simplify it. It may also be that the max integer within the meta_key is the most recent version of the address/city for that id. In which case you'd have to find the max value for each id, then join that to get the correct results (which, if true, is horrid db design. Never hide a version key inside a string).
Which RDBMS? Sidebar....
"smalldatetime" implies microsoft sql server but yeah, ignoring that sidebar suggestion bothers me too
We use it differently at my company, you have a table like you mentioned, but then we have columns in there with a Y/N value to indicate whether that date is CurrentWeek, CurrentMonth, PreviousWeek, etc... and then we join the target date to that value and then filter WHERE CurrentWeek='Y' for example.
Select * FROM &lt;table&gt; WHERE &lt;smalldatetimeColumnName&gt; &gt; DATEADD(WEEK,-1,GETDATE()) Just tested it. Works like a charm. Hope it helps
HI - I thought about that, Can I post to both reddits? I don't want to spam. The reason I only post to one area, is that my posts focus more on SQL and less on the specific DBMS nuances. Also, if the posts were more about DBA stuff, like replications, backups, and clustering, I would go that reddit instead.
Yes, you can cross post. All your screenshots were SQL Server specific. Side note, I hate using AS when making an alias. 
We use Oracle where I work. I had an argument with people at work last year because I was told they had some tables that I needed access to available only in "SQL". I didn't understand why that meant I couldn't have access... Of course they're loaded into SQL. well apparently we also have a MS SQL Server environment that no one bothered to tell me about. And when people said SQL, that's what they meant, otherwise they'd have said Oracle. FFS. I am still bitter about that not being told that. 
And you have the ability to go back in time to find data based on week sets. Always build in flexibility where it costs nothing!
Taking all of the suggestions, I got the following to work: SET DATEFIRST 1 SELECT * WHERE: DATEPART(WEEK,smalldatetime) = DATEPART(WEEK,GETDATE()) AND DATEPART(YEAR,smalldatetime) = DATEPART(YEAR,GETDATE()) The few test queries I've ran appear correct. Anyone see a potential issue with this approach? Thank you for all the responses! 
Microsoft SQL Server - apologies for not mentioning 
Sometimes we have to do some things we're not proud of when working Access.
My pleasure. Which database system are you using?
 SELECT "name" AS successors FROM monarch START WITH "name" = 'James' CONNECT BY PRIOR m_id = predecessor; 
It depends on what the result should be. Your query returns only the direct successors of James. A recursive query will also return the successors of James' successors and so on.
This could be the case for DML statements. However, changing the database structure with an DDL statement will always automatically issue a commit by the DBMS itself.
Thanks for the feedback - I'll cross post. That is great. The good thing about AS is it is optional... :)
Latest article to help beginners sort query results.
Thanks! Is there a good place where I can pratice these kind of queries? I'm struggling to find a good place to practive SQL for an exam :/
You should add a note that `TOP` is SQL Server syntax, and other RDBMSs require [different syntax](http://stackoverflow.com/a/595155/1324345)
SQL Server? Oracle? MySQL?
I'd start with the MS Press books for 70-461 and 462. There's also MS virtual academy courses for both. Pluralsight.com is also a great resource for SQL.
DBA = Database Administrator, so knowing T-SQL isn't always a part of the job. If you are a true DBA as opposed to SQL Dev, I would focus more on server administration, maintenance schedules, disaster recovery, security policies, upgrade roadmaps, etc.
I completely agree with this. You can be a whiz at querying and manipulating data, but do you know what to do when applications aren't running quickly or efficiently? What's your first reaction when a user calls in and says the system is slow? Make sure to read up on exactly what /u/Elfman72 mentions and also take a look at indexing [[click here]](http://blog.sqlauthority.com/2011/01/03/sql-server-2008-missing-index-script-download/). Pinal Dave has a great website that's full of good information. Also take a look at Brent Ozar's website for [performance tuning](http://www.brentozar.com/sql/sql-server-performance-tuning/) and [indexing](http://www.brentozar.com/sql/index-all-about-sql-server-indexes/). Both are great resources and the more you can learn about the 'other' parts of SQL Server, the more well rounded and better off you'll be. Also, get familiar with the SQL Server Logs to see what's going on with your server and learn about temp tables. I wish I learned about temp tables when I first started as a DBA as they've helped tremendously in getting data the way you want it to look without much work involved.
I don't use SQLITE, I use Oracle's SQL. I'll give it a try, though. Select customerid, sum(case when calldate &gt; sysdate - &lt;timespan = 1 for 1 day back&gt; then 1 else 0 end) as call_count from call_table where customerid = &lt;current customer id&gt; group by customerid ; Replace the &lt;&gt; sections with whatever variables you want. 
Quite right sir.
i tested the following, and it worked perfectly -- CREATE TABLE recipes ( recipe_id INTEGER UNSIGNED NOT NULL AUTO_INCREMENT , result VARCHAR(192) NOT NULL , product_count INTEGER UNSIGNED NOT NULL , machine VARCHAR(128) DEFAULT 'crafting-table' , PRIMARY KEY (recipe_id) , KEY result (result) /* , CONSTRAINT result FOREIGN KEY (result) REFERENCES items (unlocalized_name) ON DELETE NO ACTION ON UPDATE NO ACTION */ ) ENGINE=InnoDB DEFAULT CHARSET=utf8; CREATE TABLE oredict ( name VARCHAR(192) NOT NULL , PRIMARY KEY (name) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; CREATE TABLE oredict_ingredients ( recipe_id INTEGER UNSIGNED NOT NULL , oredict_name VARCHAR(192) NOT NULL , amount INTEGER UNSIGNED NOT NULL , PRIMARY KEY ( recipe_id, oredict_name, amount ) , INDEX oredict_name_idx ( oredict_name, recipe_id, amount ) , CONSTRAINT recipe_id FOREIGN KEY (recipe_id) REFERENCES recipes (recipe_id) ON DELETE NO ACTION ON UPDATE NO ACTION , CONSTRAINT oredict_name FOREIGN KEY (oredict_name) REFERENCES oredict (name) ON DELETE NO ACTION ON UPDATE NO ACTION ); so maybe it worked for me because i didn't use the "mcrd" database prefix? not sure p.s. note how i declared the intersection table ~without~ an auto_increment -- it's not needed and hampers performance also notice how both the PK index and the oredict_name_idx index include all three columns -- this will ensure that any query using this table will use a **covering index** for good performance
You probably need to create in index on oredict.name. For future reference, you should know about `SHOW ENGINE`. http://dev.mysql.com/doc/refman/5.1/en/show-engine.html In mysql command line, run: show engine innodb status\G And look for the heading like this: ------------------------ LATEST FOREIGN KEY ERROR ------------------------ It gives you full details on what went wrong. ---- \-edit: Looks like /u/r3pr0b8 resolved the specific error I pointed out by adding that index. &gt; notice how both the PK index and the oredict_name_idx index include all three columns
Postgres, but any helps!
At first I was confused by your statements because I'm not used to seeing the comma as the first character of each line. How does the auto increment hamper performance?
I think it may work if you disable pre validation on the package. It might just be confused by the bit change. I often have to run my packages a few times in visual studio because it hangs during the validation. I personally haven't run into your specific issue so it's just a guess. 
I tried that too. Thanks, though!
Second [PluralSight.com](http://PluralSight.com)
it stores the rows in create sequence... better they should be stored in sequence by recipe, that way when you retrieve a recipe, all the "oredicts" are located together on the disk
There's no reason that your query shouldn't work as you have it typed. In MSSQL: select count(distinct type_desc) from sys.objects Returns 8. select count(distinct type_desc) from sys.objects where schema_id=4 Adding the predicate, now returns 2. 
Distinct has its place and usefulness, but don't use it to cover up bad code. In the above query, think about why an object_id would appear on this table more than once? Would the same ObjectID be ready multiples times at the same time? *SHOULD* the same object id be ready multiple times at the same time? Can you eliminate the duplicates by filtering an additional column in the where clause? One of my co-workers would "select distinct &lt;columns&gt;" instead of figuring out why his join/where clause was causing "duplicates". It eventually lead to performance issues, and inconsistent counts when compared to other queries. It's possible you have a legit reason to use Distinct. In my experience, it's not needed most of the time. Edit: Change your where clause to isnull(y.IsReady,'') &lt;&gt; 'Y' and it should be fine.
Yeah I think it works in that case, if ANY of the objects in the distict groups have a desired schema. But it doesn't work for negative where I want to count of distinct groups where none of the objects match the desired schema . e.g.: "where schema_id &lt;&gt; 4" I think this is because if I have Object A and it has Schema_IDs of 3, 4, and 5. It will show a match for my criteria of "&lt;&gt; 4" when it checks the schema 3 and then say "Yup... this one matches, so count it" even though it does have a 4. Am I making any sense?
I'm not really sure if I understood you correctly. Do you want to count the number of times a 'Non-Y'-entry exists for each different object_id? If so, you could use the GROUP BY clause: SELECT y.object_id, COUNT(*) FROM y WHERE y.IsReady != 'Y' GROUP BY y.object_id; Edit: Added object_id to the select part.
I'm not an expert in joins, and I could just be completely going about this wrong as you state, but just so I'm clear... In this case they aren't actual duplicates. I'm querying a database of files related to physical objects (it is NOT a database of objects), but for each object there are often many files. Each file will have different properties. In this case the field I'm looking for will be different for each file. I want to know how many objects have none of their related files in a particular state.
That seems like it should work other than that I am looking at the time between calls, and not between the call and today or sysdate. 1) Pythor calls on 7/27/2014 8:00:00am 2) Pythor calls on 7/29/2014 9:00:00am 3) Pythor calls on 8/5/2014 12:00:00pm So I would be looking at all calls if the CustomerID = CustomerID (Pythor) and then I would check on the difference from the current call from the previous call. For example, you called in on the 27th and then 2.04 days later you called on the 29th and 7.13 days later you called on the 5th of August. I would then sum up the total repeat calls within a certain defined range (I normally use like 4 hours, 1 day, 3 days, 7, 14, 21, and 30 days to identify trends with certain call drivers to see if a high percentage of customers are calling us back within a certain period of time). 
I want to count how many different (distinct) Object_IDs have no version where IsReady is 'Y' 
Check out w3schools. Great beginner tutorials. http://www.w3schools.com/sql/ 
So you are saying you want to exclude any object that has schema_id = 4 even though they may have other schema_ids? WHERE OBJECT_ID NOT IN (SELECT OBJECT_ID FROM [SomeTable] WHERE schema_id = 4) That might be what you want.
Try this: SELECT COUNT(*) FROM (SELECT y.object_id, COUNT(*) FROM y WHERE y.IsReady = 'Y' GROUP BY y.object_id HAVING COUNT(*) = 0); The inner query counts the number of 'Y'-entries for each different object_id and the HAVING clause leaves only those that have a count of zero (meaning the object_id has no 'Y'-entry). The outer query then counts the number of remaining rows (object_ids). DISTINCT is not needed in this case because the GROUP BY clause already aggregates all rows with the same object_id into a single row. Edit: Aww. I just realized this won't work. The WHERE part will already eliminate all object_ids that have no 'Y'-entry. So they can't be counted afterwards... It would have worked if we were looking for more than zero entries. How about this instead? SELECT COUNT(*) FROM (SELECT DISTINCT y1.object_id FROM y AS y1 WHERE NOT EXISTS (SELECT * FROM y AS y2 WHERE y1.object_id = y2.object_id AND y2.isReady = 'Y'); The innermost query selects the 'Y'-entries for each object_id of table y. The middle query selects only those object_ids that did not have such an entry. And the outermost query counts the number of rows.
You are actually not to far off. You are not taking into account NULLs. The value NULL is not a value and thus cannot = or != anything. It is also not BETWEEN or IN anything. By simply checking for NULLs as well you are set. Here are two ways to do this. I prefer the first as it avoids an OR. In complex queries OR can kill performance. SELECT COUNT(DISTINCT y.OBJECT_ID) AS "Number not ready" FROM y WHERE ISNULL(y.IsReady, '') &lt;&gt; 'Y'; SELECT COUNT(DISTINCT y.OBJECT_ID) AS "Number not ready" FROM y WHERE y.IsReady IS NULL OR y.IsReady &lt;&gt; 'Y';
Thanks, I see that I was missing the issue with NULL and this does solve that part of the problem, however I think I have another problem. This returns a count that is way too high. I think it's returning the count of Object IDs where any file with that object ID has a N or NULL IsReady flag. In this database there are multiple files for each object ID, and only one of the files is going to be marked IsReady. I want to count how many ObjectIDs have NONE of their related file marked IsReady. Your correction to my error, still has my other problem of it seeming to be counting how many ObjectIDs have at least one file that is not marked IsReady. Which is not what I want.
Good point with the NULL value issue. However, this query won't give the expected results in this case. As I understood it, each object_id can have multiple entries, some being 'N', NULL or 'Y'. This query will count an object as "not ready" as soon as it reaches the first 'non-Y' entry, even when another 'Y'-entry for that object_id exists.
OK, how's this one: select first.customerid, sum(case when second.calldate - first.calldate &lt; 30 then 1 else 0 end) repeat_30_days, sum(case when second.calldate - first.calldate &lt; 21 then 1 else 0 end) repeat_21_days, sum(case when second.calldate - first.calldate &lt; 14 then 1 else 0 end) repeat_14_days, sum(case when second.calldate - first.calldate &lt; 7 then 1 else 0 end) repeat_7_days, sum(case when second.calldate - first.calldate &lt; 3 then 1 else 0 end) repeat_3_days, sum(case when second.calldate - first.calldate &lt; 1 then 1 else 0 end) repeat_1_day, sum(case when second.calldate - first.calldate &lt; .3333 then 1 else 0 end) repeat_4_hours from call_table first inner join call_table second on (first.customerid = second.customerid and first.calldate &lt; second.calldate) where first.customerid = &lt;pythor&gt; ; You can drop the where clause and add "group by first.customerid" if you want counts for every customer, but that may run a very long time depending on your customer base. Also, it would get much more complicated to fix this, but you might over count with this query. If I call at noon, 1 PM, and 3 PM; all on the same day, how many 4 hour repeats is that? Most people would say 2. But this query is going to count 3. The 1PM and 3PM calls are both within 4 hours of the noon call, and the 3PM call gets counted again since it was less than 4 hours since the 1PM call. Add in a 3:45 PM call, and the count goes to 6! Off the top of my head, I can't see an easy way to fix that unless you have something like the LEAD and LAG functions that are available in Oracle SQL. I have no idea if SQLite has them, nor how the work if it does.
Another way to do this without dipping into string manipulation is: WHERE NOT (table.a = 'America' AND table.b = 'Frank')
Then you want to use: SELECT COUNT(DISTINCT Y1.OBJECT_ID) AS "Number not ready" FROM y AS Y1 WHERE NOT EXISTS ( SELECT * FROM y AS Y0 WHERE Y1.OBJECT_ID = Y0.OBJECT_ID AND Y0.IsReady = 'Y' );
Thanks a bunch. When I do this in Excel, I am only comparing two rows of data at a time. It is a simple yes or no if you called in again within a period of time from the PREVIOUS call. This period of time is rolling as well and each one refreshes the 'timer'. So if you call in on the 15th and then the 30th, that would be a repeat hit in 30 days. Now your new timer is from the 30th and you can forget about the 15th. This goes on and on. For 'First Call Resolution', we need 30 days to pass in between time stamps (720 hours). Does it change anything if we are only comparing the 'two rows' at a time since the window is rolling? The metric only cares if the customer ID repeats on the report again within 30 days. I want to break that down into different time periods though so it is more actionable. Since June we have 211,000 unique customers that we will be running this for - we will not be selecting individual customers. 
This is just a stab at it, and just guessing. Do you have a 32-bit version of Jet 4.0 installed? It's pretty much 100% chance you do already but may be worth verifying.
Hi - I have another post for SQLite that mentions the LIMIT clause. This post is specific to SQL Server.
I'm guessing that it's creating a huge temp table to to some part of the query, and that's what's killing you. Can you show the query, and the output of "EXPLAIN QUERY PLAN {query}" ?
if you need a field in the table that is a unique value to identify that record, add it as an [http://msdn.microsoft.com/en-us/library/ms186775.aspx!](identity column).
The ID field that identifies stops on the routes will have several rows of the same number before changing. I think an identity field wouldn't allow that. Right?
No, I mistook your question. looks like you are missing a table. if you can post an example of what you are trying to do when you get to a computer, I will take another look.
You definitely want to avoid looping in SQL. In very simple terms, it sounds like you need a tables with one-to-many relationships for route, location, and stop. Each route can have multiple locations. Each location can have multiple stops. If stops repeat, what is it about each stop that differentiates it from previous stops? Is it what's delivered? When it happens? There's going to be an additional column, possibly a foreign key from another table, to further identify and differentiate the rows in your stops table.
&gt; You have not understanding how predicates work. I'm pretty sure that's an understatement. It is quite confusing as any background I have is with more straight forward basic programing (simple C++ things) and not the complextity of databases, but I need to do some complex searches. There are certain logic things that come easy to me having a little programing background, but I know I'm missing some key fundamentals. The database is of information about photographs. One of the pieces of data is an object ID of what object that photograph is of (which relates to a completely different database... this database is not the record of authority for objects... it is only about the photographs). So yes it is a headache of many-to-many or many-to-one relationships. I did obfuscate my code a bit by leaving out joins that show too much in the way of custom field names and I have a feeling that's just making it harder for you and everyone to see what I'm screwing up. Thanks. I have a lot to learn. Maybe I'll see if there are any good resources on Lynda.com or something. Basics first. Thanks again. Edit: formatting
Can't help more with your understanding, but here's a good visual on set theory which is the basis for most database work. [http://blog.codinghorror.com/a-visual-explanation-of-sql-joins/](http://blog.codinghorror.com/a-visual-explanation-of-sql-joins/)
OK, can we try this one? select customerid, sum(repeat_30_days) repeat_30_days, sum(repeat_21_days) repeat_21_days, sum(repeat_14_days) repeat_14_days, sum(repeat_7_days) repeat_7_days, sum(repeat_3_days) repeat_3_days, sum(repeat_1_day) repeat_1_day, sum(repeat_4_hours) repeat_4_hours from (select ct.customerid, case when customerid != lead(customerid) then null when lead(calldate) - ct.calldate &lt; 30 then 1 else 0 end repeat_30_days, case when customerid != lead(customerid) then null when lead(calldate) - ct.calldate &lt; 21 then 1 else 0 end repeat_21_days, case when customerid != lead(customerid) then null when lead(calldate) - ct.calldate &lt; 14 then 1 else 0 end repeat_14_days, case when customerid != lead(customerid) then null when lead(calldate) - ct.calldate &lt; 7 then 1 else 0 end repeat_7_days, case when customerid != lead(customerid) then null when lead(calldate) - ct.calldate &lt; 3 then 1 else 0 end repeat_3_days, case when customerid != lead(customerid) then null when lead(calldate) - ct.calldate &lt; 1 then 1 else 0 end repeat_1_day, case when customerid != lead(customerid) then null when lead(calldate) - ct.calldate &lt; .3333 then 1 else 0 end repeat_4_hours from call_table ct order by customerid, calldate ) group by customerid ; The inner query is using lead and an order by clause to pull the current call, and the calldate of the call immediately after (for that customer). The Case statement pulls that into a 1 or 0 for each time frame. The outer query then sums up all those calls to see how many repeats qualified for each bucket. I can't really suggest any particular book. Just lots of practice and Google when you need something you can't figure out. And of course, reddit ;).
I put together a mock table to describe what I'm talking about. http://i.imgur.com/oB0VB9i.png What I want the query to produce is the column named "stopID". What I keep getting are columns "x" where it will number rows within stores and "y" where it will number the stops correctly, but if there's another stop on the same route that is the same store, it assigns it the same stopID to both stops. Any ideas?
I don't really understand how you intend to do this when there isn't any difference in data between records 123 and 789. Same employee, same date, same store, same activity. You need *something* to be able to tell them apart if you plan on adding that stop_id field. 
Those records are for illustrative purposes only. The number, type, and duration of the activities will be different. Why does it matter, there should be something in SQL that just acts as a pure counter operating off of the event that a record in a particular field changes from one row to the next. 
What's the size limit of your tempdb? You could try compressing the tempdb and restarting the sql instance to free up some space for a start and then identifying what is filling it up is the next step. Your most recent select may just be pushing it over. I would check which other procedures and queries you have running that are getting too large for the processing to be done in memory which would cause them to dump into the tempdb. A quick google of sql tempdb filling. Should give you some results to aid with identifying and managing these queries 
What's the schema of the source data? 
This might actually be the answer. If I grouped on the employee/date/store/arrival time field then I could uniquely identify stops on the route and then update these stop IDs back to the table. So simple. / thread. 
Question your assumptions. Do you REALLY think that a 33GB SQLite database is okay? Use the right tool for the job.
&gt; CONNECT BY That's Oracle syntax, PostgreSQL doesn't support "connect by" recursion. OP will have to use recursive common table expression.
Out of curiosity, are your `Employee`, `Store`, and `Activity` fields `varchar`s or `nvachar`s? Is there a reason you're not using `int`s?
Thanks for the hint, you are right. With recursive common table expression it could be done with: WITH sucessors(suc_m_id, suc_name) AS ( SELECT m_id, "name" FROM monarch WHERE "name"='James' UNION ALL SELECT m_id, "name" FROM sucessors, monarch WHERE predecessor = suc_m_id) SELECT "name" FROM sucessors; However, I only tested this in Oracle. :)
That's good, I think, and a team captain is a team member as well. I don't like that it's not perfectly normalized keeping the `id` in memberships and `CaptainID`. But I think protecting that with code would be simpler than what I currently have.
Whats in your SSIS config file?
Head to Introduction to databases in coursera. There are video lectures and practice qns dedicated for recursive queries 
Step 1.) Create a new table - Roles. Sample columns/data: ID|Title |Description|MemberLimit ---------------------------------- 1 |Captain|Queen bee |1 2 |Member |Drone |50 Step 2.) Add a column in your memberships index corresponding to Roles.ID. Step 3.) Include the logic for checking Roles.MemberLimit against the existing role count for that team before adding a record to membership either in your code or a stored procedure. Step 3b.) If the project scope warrants it, consider dropping Roles.MemberLimit in favor of a separate roleID/teamID/MemberLimit index, so each team can have customized limits (co-captains, for example). Step 4.) ???? Step 5.) Enjoy scalability.
I've always shyed away from creating a table with only a few rows. Maybe that's a mistake. Since the rolesID would only map to two possible states (co-captains couldn't happen, the "team category" would change in a way that would preclude this.) what would be the difference between keeping an id and a boolean? I didn't know about stored procedures though so I will look into those as a protection for data integrity.
Thanks so much 
the OrderDate was from me copy/pasting from the test db I was using and not changing all the parameters for my reddit post. Sorry about that.
Did it work for you?
I'm assuming you're using SQL Server for the purposes of this response, but the general logic should hold for other DB types... In terms of your use of the date diff function, it looks right to me. That said, can you say with absolute certainty which date (a.OrderDate/b.OrderDate) will always be larger? If not, you'll need to use an ABS() to check against the absolute value, otherwise DATEDIFF() will return a negative if they're in the wrong order. For example: --Returns 106 SELECT DATEDIFF(DD, '2014-05-01', '2014-08-15') --Returns -106 SELECT DATEDIFF(DD, '2014-08-15', '2014-05-01') --Same as above, but absolute value, so ABS(-106) = 106 SELECT ABS(DATEDIFF(DD, '2014-08-15', '2014-05-01')) In your case, the first thing I'd try is: SELECT a.PatientID, a.TestDate, a.TestScore, b.TestDate, b.TestScore FROM Tests a JOIN Tests b ON (a.PatientID=b.PatientID) WHERE ABS(DATEDIFF(dd,a.OrderDate,b.OrderDate)) &gt;= 90; Hope this helps. 
Do you need the data to move in real time? If not, I recommend using a SQL Agent Job to synchronize the data in multiple databases. This makes it more flexible in case one of the databases moves or changes. Just have it run every 5 minutes or something.
what version of SQL server?
Everything. Which details do you want? 
2008 R2
Having a Captain flag isn't correct as it doesn't enforce your data / business rules. You could easily have more than one or no captains through some erroneous situation. Either add a Captain Id to the team (nullable with on delete set null referential integrity to your people table) or add a new table "team captains" (team is the pk, captain Id not null on delete cascade to teams and people), which is a one to one off of team table. The second is probably the most correct in data terms as a team can exist before members join and isn't defined by a captain.
What does your query look like? Something like: SET DATEFIRST 1 with cte as (etc etc etc)
Look into the `PIVOT` statement. You'll need either a temp table, a CTE or a subquery in your `FROM` with a `ROW_NUMBER()` to produce a value to pivot on. You will only be able to handle a fixed number of columns from Table 2. Alternatively you could do a CTE and series of subselects, which might perform a little worse but be far easier to maintain (IMO). If you have a large number of matching rows in Table 2, the performance of this approach will be more of a problem.
What's the error? Is it complaining that the with has to come first in a batch?
That's probably what I would do too. I hate pivots.
Post your config file after you remove all security info
This worked in a different DB, figured out another problem with my practice DB that I hadn't anticipated, dates were stored as varchar, which meant datediff didn't work at all.
there is no way I know of to directly capture the offending row. but if you explicitly name your constraints it should be much easier to figure out which one is being violated. when you let the sever pick the name you get that ugly autogen name that doesn't tell you anything.
Ahh, ok, thanks. For now, there's only one constraint per table, so the table name is sufficient for that purpose, but I'll keep that in mind. I think I may have stumbled on a solution... would like opinions on this... - create the tables without the constraint, with additional columns called "KeyNum int" or something - Insert as normal, with KeyNum int = COUNT(*) OVER(PARTITION BY [key values that should be unique]) - select into a varchar(max) [maybe?] WHERE KeyNum &gt; 1, capturing the relevant data - attempt to apply the UNIQUE constraint after, it will catch an exception if any exist... - Can use the information that was previously selected to display exactly which data points caused errors I think this is the lowest overhead solution?
Remove the constraint and add an extra step that identifies and handles the duplicates appropriately. I have to deal with this kind of crap all the time. I generally start with an import table with no restrictions and all varchar columns then apply my rules to that. Sometimes I throw the violating rows away, other times I reject the whole import. It really depends on what these errors mean at a higher level.
IMO you should always sanitize before going to the DB. Building out the connection and transferring the request is expensive and you don't want to waste time with it if you know the data will blow up the proc. If you want the most efficient solution I would sanitize before you go to store. Building temp tables and doing a ton of inserts will be slower than say just iterating over your collection of data and validating it. In my experience having the SQL server do error handling beyond checking for bad inputs is a pain, because, as you are experiencing now, you have to jump though a few hoops to get good feed back for the user. I like to apply the KISS method with a vengeance when it comes to db work. SQL is just more difficult to maintain than some code in a repo somewhere, to me.
This goes along with a style I've heard that takes ETL and turns it on its ear to ELT. In more plain english, use a staging table with no constraints on it and THEN massage the data as needed.
Do a select and join the temp tables using the fields found of each constraint to the table to be inserted into and [output](http://msdn.microsoft.com/en-us/library/ms177564.aspx) the results to another temp table or logging table. You can also remove them from the data set. Include which constraint it violated in the logging output. Either continue with insert with good data or end the SP and cancel the batch. 
FWIW, this data is ONLY being read, and used to create invoices, which have to be correct for legal reasons, so there's very little danger of writing bad data to the DB, we do write some "comments" to an invoice which should only happen if everything is successful, which is why we rollback the transaction if anything bombs.
Unless I'm missing something, there is nothing special about that query. You shouldn't use a Stored Proc for pure SQL. Change it to a Table Valued Function and ensure you've run the query profile and created all the requested table indexes. EDIT: I just noticed the timing difference. How many rows &amp; columns is this returning? I have no idea what the expected overhead for SSRS is, but it really shouldn't be more than query execution time plus 5 seconds (for around 10k rows). You might try a [better reporting suite](http://www.actuate.com/info/ftype-download-free/?gclid=Cj0KEQjwx4yfBRCt2rrAs-P5vtkBEiQAOdFXbUHt0kopKKQ-co_tu1fawdLfbvl0eVXYSPGcfc8tDIoaAm2-8P8HAQ). But I'm biased. 
This is all /r/sqlserver and has nothing to do with writing SQL. Also, you should use [BIRT](http://www.actuate.com/info/ftype-download-free/?gclid=Cj0KEQjwx4yfBRCt2rrAs-P5vtkBEiQAOdFXbUHt0kopKKQ-co_tu1fawdLfbvl0eVXYSPGcfc8tDIoaAm2-8P8HAQ). ;)
Thank you 
Try taking the results of the split into a temp table and use that instead of using the split directly.
ok. So I'm assuming that you're using a Windows account and not a SQL account. if you're using a SQL account, you won't be able to access the folder or network path that the Access DB is stored in. But you probably aren't doing that. So another thing to try, is to make sure that the SQL server is a trusted destination or network path. There's more substance on MSDN here: http://social.msdn.microsoft.com/Forums/sqlserver/en-US/c250a81f-5d25-4ba8-9639-3464c524fb35/importing-access-2010-tables-to-sql-server-2008-r2?forum=sqlintegrationservices But basically, you set this up in MS Access 2010, by clicking 'Database Tool' -&gt; "SQL Servers" -&gt; Use Exisiting Database -&gt; Click Next ... It should prompt you for a 'File Data Source' ... -&gt; Click "New". follow the remaining steps to create a new File DSN and your database import should work. I tested it using MS Access 2010 (32-bit) and SQL Server 2008 (64-bit). Your DSN file should be similar to the following: [ODBC] DRIVER=SQL Server UID=abcuserid DATABASE=testData WSID=Machine1 APP=Microsoft Office 2010 Trusted_Connection=Yes SERVER=Machine1\SQLEXPRESS Description=DSN for testData on SQL Server It's probably going to be slightly different steps for MS Access 2003, but relatively similar. Give this a shot. Or, if you already have, check back in and we can keep investigating.
Hey hey now. There ain't nothing wrong with MS SQL. Its a functional out of box software that's easy to set up by the novice administrator. Just because he said this isn't a SQLServer subreddit doesn't mean we don't cover MSSQL. It's just the programming side that we talk about in this subreddit, not the server administration. 
That code is shit. It should be rewritten. I'll see if I can help some tomorrow but there are obvious SQL no-nos that tell me that there probably isn't much reason to justify its complexity.
You think that is bad, I inherited a system that uses a permanent table TableA_MOD as a staging table to the actual table TableA, with views joining them together here and there. The _MOD table has 12 triggers acting as constraints, several with: select count(2) from _mod where value=value having count(2) &gt; 1 To insert into this table, there is a select top 5000 * cursor that loops over each one, capturing the error and logging it. This proc runs every 5 minutes. Ohh and since there is no IDENTITY, they have select max(id) from TableA_MOD (TABLOCK, HOLDLOCK) just to throw a few kinks in there. Nothing like a few deadlocks an hour. And since this system is being rewritten into a HBase (no clue why) I'm just to 'tend to it' as there is no reason to do a rewrite before the rewrite. Tell me why your hired a SQL Developer/DBA again.. Sorry rant over. 
I'd be interested in what you are referring to exactly. I assume the Where statements and would love some other input.
Ok, what am *I* missing? What's terrible about using # tables to preprocess data before output?
I think $hostname_cribs should be "unix:/var/log/mysql/mysql.sock" or something like that since when you specify localhost you're trying to connect using the networking stack but you want to use a file socket.
EDIT: I originally saw this post when I was just about to leave work. Last night after a few beers I was like ,"Ugh, I gotta be straight with this guy but I'll see if I can help tomorrow." I had to put it on GITHUB because it was beyond Reddits 10000 character limit. Keep in mind, I could only test this for syntax. [1st Solution](https://github.com/jmurrayga/Reddit/blob/master/reddit1.sql) The first solution removes the nested correlated subqueries. I'm thinking that your original code can't hash the tables due to the way your correlated subqueries are written and dbo.split is called 1-3 times per row data. [2nd solution](https://github.com/jmurrayga/Reddit/blob/master/reddit2.sql) The second solution attempts to combine your aggregate queries. I removed the table variables and just put them inline. I don't know if they will aggregate exactly like your original code but its an attempt at reducing the redundant sub selects you have going. I also alias'd your table names. If you go with solution 1, you should probably do that as well. Using 3 part names is lengthy.
Nothing at all, its a valid route to take if you data is not being validated in the application, unlike the cluster that I'm stuck with above. 
Don't believe you can use localhost on godaddy. http://support.godaddy.com/help/article/3323/locating-your-database-connection-strings I haven't used godaddy in a while but I'm pretty sure you'll get a MySQL host name thats pretty long like dbmysql.godaddy-1.com:8271. 
I actually fixed it and pushed my updates up to github. My original solution for reddit2.sql didn't work. I flubbed the aggregates. Please take a look at it again. EDIT: I fixed it again. I noticed that I was not summing referral type and status over the rest of the group bys.
Well... it's *supposed* to be validated, and we're putting more checks in to make sure that it is, but we've come to the conclusion that we have to validate at the time of running this proc as well, as a double check. My experience with users, admins, and everything else has been that data muse be validated at every possible stop, there's always an issue otherwise.
So I am having issues with the joins and Split function. I realize you cannot see the report but the split function will send in multiple values to be "split" if it is "All" and it appears to be returning the first value only. Is there a way to join multiple values if there is more than one value coming out of the Split function? I've tried constructing a case when or if statement down there. EDIT: It returns one no results if All is sent through.
Can you assume there's only one PENDING and one COMPLETE per FileID (eg because of a unique index)? If so, you can JOIN the table to itself.
reddit1.sql or reddit2.sql?
Reddit2.sql
Yes, there is only one of each. I never thought about that.. I'll try that out and I don't see why it wouldn't work. Thanks!!
So, actually it works now. There should be a left join on ECSTEAMS instead of an inner join. I also added a != all as you have in the other ones. THANK YOU!
Ding ding ding! This is the correct answer!
Use window functions! Observe: SELECT *, DATEDIFF(second, modifieddate, pendingDate) AS pendingSplit FROM ( SELECT FileID, status , modifieddate , LAG(modifieddate) OVER (PARTITION BY FileID ORDER BY modifieddate) AS pendingDate FROM DataTable WHERE status = 'COMPLETE') http://msdn.microsoft.com/en-us/library/hh231256.aspx
I've done this using some XML feature of MSSQL (sorry at home ATM). Try googling for concatenation string and for XML path or something. Edit: here it is. http://sqlandme.com/2011/04/27/tsql-concatenate-rows-using-for-xml-path/ Another option may be to pivot the table and create a column for each possible value. 
The xml path approach is how I usually go for this. 
using XML path will probably get you there. Another option if you want to go overboard is using CLR's. http://www.mssqltips.com/sqlservertip/2022/concat-aggregates-sql-server-clr-function/ I normally compile &amp; install 2 CLR's: one to concatenate a column together, and another to split them back up based on a delimiter. You'll probably only see a benefit to using CLR's if you start concatenating/splitting large volumes of data.
Something like this works for a recent report I created in DB2, using Toad UPDATE TableName SET Fieldname = (Select listagg(TypeOfChange), ',') WITHIN GROUP Order by TypeOfChange If this almost helps I can Ty to be more specific later when I'm not only on my phone. Essentially, I've used multiple sub queries to pull the various changes that could have happened to a record. Those are selected into a temp table where the only field that's different is the TypeOfChange field. This consolidates them into one field, and concatenates the different TupeOfChsnge values Hope it helps
[Here]( http://www.postgresonline.com/journal/archives/191-String-Aggregation-in-PostgreSQL,-SQL-Server,-and-MySQL.html) is an article detailing various approaches.
Have been using a CLR concatenation aggregate for years now. At this point, I don't know how I ever got by without it.
This is one of the quality of life changes I've been wanting for a while. The stuff for xml approach is so bulky for a fairly common need. It would be awesome they could incorporate an overload of concat() into their window functions.
Do you get better performance this way than FOR XML PATH('')?
Depends on the volume of data. Randomly throwing out numbers, but when you get into the 100's of thousands of records, it may be faster than XML path.
pretty decent article, from a quick everview
You'll just have to join the users table to the games table twice. We can do that by aliasing the tables when we join them. In this example I aliased them as p1 and p2 SELECT g.id, g.p1, p1.name as p1_name, --replace .name with whatever the player name column is on users g.p2, p2.name as p2_name --replace .name with whatever the player name column is on users FROM games g INNER JOIN users p1 ON p1.id = g.p1 --Or replace p1.id with whatever the id is on the users table INNER JOIN users p2 ON p2.id = g.p2 --similarly replace p2.id with whatever the id is on the users table
This is awesome. Thank you.
If less than 100% of the games have a player 2 (like if they selected single player) and you need to know this, then use a left join instead of inner join on p2 Otherwise you're good to go. Nice work, duckpants
I know a few people that remote in to work, and you're totally correct that you don't have to be physically present to write code. All the people that I know that do this got the job worked there for a few years then got permission to move out of state and continue to work. I'm curious what others say as rumor has it east and west coast jobs pay 30% more than positions in the middle of the USA.
This is a good clarification. I assumed there would not be a game that only had one player, but I suppose there could be
 [dbo].[Split] ( @RowData NVARCHAR(MAX), @Delimeter NVARCHAR(MAX) ) RETURNS @RtnValue TABLE ( ID INT IDENTITY(1,1), Data NVARCHAR(MAX) ) AS BEGIN DECLARE @Iterator INT SET @Iterator = 1 DECLARE @FoundIndex INT SET @FoundIndex = CHARINDEX(@Delimeter,@RowData) WHILE (@FoundIndex&gt;0) BEGIN INSERT INTO @RtnValue (data) SELECT Data = LTRIM(RTRIM(SUBSTRING(@RowData, 1, @FoundIndex - 1))) SET @RowData = SUBSTRING(@RowData, @FoundIndex + DATALENGTH(@Delimeter) / 2, LEN(@RowData)) SET @Iterator = @Iterator + 1 SET @FoundIndex = CHARINDEX(@Delimeter, @RowData) END INSERT INTO @RtnValue (Data) SELECT Data = LTRIM(RTRIM(@RowData)) RETURN END 
I worked for IBM for three years. I recently left to work for a retailer supporting an inventory management tool .. I'm making some BI tools and helping improve pricing models and supply chain. Anyway. At IBM I worked from home first three days a week, then four. Its challenging doing any kind of advanced data modeling or product design remotely. America is so full of old guard mindsets that need to see whiteboards, people who need to print out documents and point and grunt at figures. Working remotely was a blessing because I got to watch my baby grow into a toddler. But it was a serious headache because I found myself explaining the same things over and over in different ways. Some people just don't read, and need to have things read to them. You'll find this in every region, and in every industry and business size. So I'd advise caution when seeking remote opportunities. Being a code monkey and working from home is great. Set your own schedule and work without being bothered. But mid and senior tier positions will be difficult. The other difficulty was the hours. I put in about 60 hours a week. I started about 6am, worked until 6pm, ate dinner, did a few chores, put my son to bed, and did final documentation review or generation in the wee hours. I frequently responded to emails and even calls or texts on weekends. Including Super Bowl Sunday and Mothers Day. I was on salary so ... wasn't much I could do to recoup this. My sex life diminished. My wife and I are much closer now that I leave the home eight or nine hours a day. This wasn't my first job with telecommuting opportunities. And my frustrations were common in previous telecommuting jobs. There's a mindset among most senior managers that if you're working at home they're doing you a big favor so you owe them more hours. There's a lot of benefits to it. But go in with your eyes wide open. Read up on glass door. Be ready to sacrifice a lot of personal time in exchange for never having to put on a tie or sit in traffic. Good luck.
Woops, I kinda misread the question, I guess :D My bad
Here is my latest post on basic SQL stuff.
Since you are talking about SQL-Server-specific subjects, you should let your readers know about character ranges in the LIKE condition. Aside from that, good article, it's nice and easy to read and understand.
Google
Before you start trying to "query SQL in a webpage", get a handle on the language itself. And then get a handle on how to [make that webpage safe](http://xkcd.com/327/) because you'll probably get it wrong the first time.
[Image](http://imgs.xkcd.com/comics/exploits_of_a_mom.png) **Title:** Exploits of a Mom **Title-text:** Her daughter is named Help I'm trapped in a driver's license factory. [Comic Explanation](http://www.explainxkcd.com/wiki/index.php?title=327#Explanation) **Stats:** This comic has been referenced 297 times, representing 1.0006% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_cjn5y7x)
Yes, PHP, Java, JavaScript, Python, Ruby, Go. Use what you want. There's free learning material a google search away.
Here is the JNDI datasource doc for Tomcat. http://tomcat.apache.org/tomcat-7.0-doc/jndi-datasource-examples-howto.html
Thanks for bringing up some good points on the potential ugly side of working remotely. It might not be worth it to me if I'm forced to work outrageous hours. I'll definitely need to tighten up my negotiation and convincing skills. :P Appreciate the response!
The handling of NULL values by COUNT does not depend on the WHERE clause! It depends on whether an expression or the asterisk is used for counting. Oracle documentation - COUNT(_expr_) vs COUNT(*): &gt; If you specify _expr_, then COUNT returns the number of rows where expr is not null. &gt; If you specify the asterisk (*), then this function returns all rows, including duplicates and nulls.
Hi - Thanks for the feedback. I considered talking about ranges, but didn't. I'll go back and add them in when I get a chance.
Ahh, that got me! Do you know if there a particular logical reason why this is? Regardless, thanks a bunch for setting me straight :)
Sure thing! :) However, I have no idea about the reasoning behind this.
At the risk of doing *too much* of your homework for you, I'd recommend double checking your id fields.
May you please elaborate? They is no risk involved :-)
The risk involved is me, a guy that does this for a living, encouraging a lack of troubleshooting skills from eventual co-workers/underlings. I'll say that there's a big difference between one of your first lines and one of your last lines, though. Try formatting your statement so it's easier on your eyes. A newline after each comma, tab in on open parentheses, etc. You'll get it. I believe in you. :) 
If I asked you 'how many numbers do I have: 1, 2, 3, 4' you'd say 4. If I asked how many numbers do I have now: 1, 2, 3, 4, null, null, null' there are still only 4 numbers there, null isn't a number. On the other side, the '*' signifies a record (think %rowtype). A record can have all of it's fields be null, and not itself be null, so counting every record, even those that have null fields. This holds true even if you have a table with a single column, the record exists, the value it contains is null. 
OMG If it is this easy, just kill me now. I will only ask, will this command work equally well in PHP? I have read so much about parameterization and avoid injections but since this will be a private function of a class I guess it is pretty safe (no user inputted data is used int the command)? and I am using MySQL at the moment and transitioning to MariaDB. Thanks so much I will try it out tomorrow!
You should look to see if this is a problem that's already been solved. @@identity is a value in t-sql which returns the last value auto-incremented by the column last inserted to. E.g. if you insert a new value into a table, and an identity property has been implemented on the id column, it should *automatically* increase the values inserted 1...2...3..4 etc, and @@identity or its equivalent is what the value is of the row you've inserted. I'd stay away from doing an update id = id+1 because it's bad practice - if you have a table of magazine reviews it probably will never matter, but if you ever design a system which later comes under serious load people will curse your name. 
Thanks for the tip. I will look into that. Honestly this is an app that will have at most 5 people logged in at a given time and only interfacing with their own courses so should be little if no overlap. I would just rather do it right than dirty. Not sure auto increment will work though because the insert is on table reviews and the update is on table courses. Thanks for the input!
SQL should look pretty much the same regardless of the front end language you are using to call it. In this case, the only thing you should need to parameterize is in your where clause.
&gt; I will only ask, will this command work equally well in PHP? Your PHP interpreter and SQL database are independent. From PHP, you send queries (instructions) to the database to do things. The database doesn't know or care where the query came from or who wrote it, it just sees a query, parses it, executes it, and returns a resultset or status code/error message.
Setting aside the fact that `select *` is generally discouraged, how is this different/better than: select c.courseID, c.Name,c.Description, count(*) as review_count from courses C join reviews R on c.courseID = r.courseID group by courseID I did both your version (after correcting for the `select *`) and mine with a decently-sized data set on SQL Server and they performed identically (although with different execution plans). Test both ways, check index usage, execution plans and I/O stats, etc; I prefer avoiding correlated subqueries where I can, just for readability.
yes you are missing something there's a definite syntax error in there hint is in aburger's first reply
This worked for me. SUBSTRING([stringfield], 1, LEN([stringfield]) - CHARINDEX('-', REVERSE([stringfield]))) EDIT: Formatting for Code EDIT2: based on bwalbs use of LEFT(), you can simplify my snippet to LEFT([stringfield], LEN([stringfield]) - CHARINDEX('-', REVERSE([stringfield])))
As a comma suffix user, may you comma prefix users suffer an agonizing torture in whatever hell you end up in.
Left of the rightmost hyphen?
I'm going to assume you are using T-SQL. One of the basic things you must realize is that NULL = NULL will always return false. Saying something is NULL does not mean it is empty. So in your where clause diag_num = NULL will always be false and so nothing would be deleted. You can use IS NULL to delete rows, so your code would look like : DELETE FROM sorted WHERE diag_num IS NULL
just add an or statement to compare to an empty string DELETE FROM sorted WHERE diag_num=NULL or diag_num = '' 
you cannot compare null to a value. since you mention phpmyadmin I asume you work with mysql although this rule applies to all SQL variantions. http://dev.mysql.com/doc/refman/5.0/en/working-with-null.html
anger and hostility are sure signs of small-mindedness and parochialism http://stackoverflow.com/a/118363 http://www.thatjeffsmith.com/archive/2010/12/trailing-or-leading-commas/ http://forums.devshed.com/mysql-help/722501-error-sql-syntax-check-manual-corresponds-mysql-post2493961.html#post2493961
I believe you are just asking to max over skill level and group by the rest. I also aliased your tables and changed the right outer join to a left outer join and re-arranged/combined your where statement. Many of times the SQL server optimizer is smart enough to choose an execution plan correctly based off a where statement, however, it can sometimes chose an execution plan incorrectly because your where statement is out of order. SELECT WH.USER_DOC, AH.SERIAL_NO, AH.DESCRIPTION, AH.MODEL, WT.HSN, WT.LSN, WC.CARD_NO, HM.MEMO_PAD2, HM.MEMO_PAD3, WC.CARD_DEFERRED, UL.F_NAME, UL.L_NAME, MAX(WCSO.SKILL) as Skill1, MAX(WCSO.SKILL) as Skill2, MAX(WCSO.SKILL) as Skill3, MAX(WCSO.SKILL) as Skill4 FROM USERLIST UL INNER JOIN WOMNT_CARD_SIGN_OFF WCSO ON UL.CCODE = WCSO.CCODE INNER JOIN WOMNT_CARD WC ON WCSO.PARENT_CARD_DOC_NO = WC.DOC_NO INNER JOIN WOMNT_HDR WH ON WC.PARENT_DOC_NO = WH.USER_DOC INNER JOIN AIRCRAFT_HDR AH ON WH.PARENT_DOC_NO = AH.DOC_NO INNER JOIN WOMNT_TC WT ON WH.USER_DOC = WT.DOC_NO LEFT OUTER JOIN HEADER_MSG HM ON HM.DOC_NO = WC.DOC_NO WHERE (WOMNT_HDR.USER_DOC = @WORKORDER) AND (NOT (WOMNT_CARD_SIGN_OFF.SKILL IN ('010','012','014','015'))) AND (WOMNT_TC.TYPE = 2) AND (HEADER_MSG.DOC_CATEGORY = 'MC') AND (NOT (WOMNT_CARD.CARD_DEFERRED = 'y')) AND (NOT (WOMNT_CARD.CARD_NO = 'admin')) AND (WOMNT_CARD.INCLUDE_CARD_IN_LOG_BOOK = 'Y') GROUP BY WOMNT_HDR.USER_DOC, AIRCRAFT_HDR.SERIAL_NO, AIRCRAFT_HDR.DESCRIPTION, AIRCRAFT_HDR.MODEL, WOMNT_TC.HSN, WOMNT_TC.LSN, WOMNT_CARD.CARD_NO, HEADER_MSG.MEMO_PAD2, HEADER_MSG.MEMO_PAD3, WOMNT_CARD.CARD_DEFERRED, USERLIST.F_NAME, USERLIST.L_NAME 
 LEFT([string name], 12)
Whoops; wasn't too sure on the specifics.
I have a couple clarifying questions. What version of SQL are you using? ~~Can more that two skills be associated to a "job"?~~ I guess the better question is, how many skills can be associated to a given "job"? I believe what you are asking for can be done using PIVOT and ROW_NUMBER if they are available in the version of SQL you are using.
i've already solved it, it was a little more complex than that, had to just concat a string in a view and use that as the join as opposed to doing all that in the where clause. I generally don't like to mess around with those.
somebody took it seriously, BECAUSE I LOST VALUABLE KARMA WHEN THEY DOWNVOTED ME !!!! ;o)
Size your transaction log correctly.
This method looks perfect for what I am needing, and seeing the statement made it "click" as to why it is better off this way. Thank you very much for your input!
Right on, thanks for the reply! I think I have just been complicating things in my head and setting some false limitations on the way things work.
/u/gsxr_jason's query is valid, and there may be some cases where it's better than my method - it will depend on a number of factors.
Yeah, I have tested both and both work fine for me. like I said before my db is so small relatively that I think worrying about efficiency is not worth it in this case, but I am always trying to learn better methods. Thanks again for your input!
I'm totally agree with you. Unfortunately finding advance or even intermediate courses are really hard. everyone is just making an easy introduction video. I hope they follow their course to more informative content.
How did this compare to w3schools?
Some people prefer video tutorials rather text based tutorials (w3schools) for start. I personally prefer the video tutorial for starter but they mostly never touch the advanced materials. Also this course is an introduction to Oracle SQL however the weschools is a general SQL which you can use it anywhere (MS SQL or MySQL). Again this is my opinion and I maybe wrong. You can always watch the preview videos to get a better understanding. 
count() is a function, so you have to put the column you want to count in it. Assuming you are using MSSQL: SELECT C.Name, count(S.SupplyName) as SupplyName FROM Customer as C inner join Supplies as S on C.CustomerID = S.CustomerID WHERE S.SupplyName = 'Paper' GROUP BY C.Name Anyway you would probably need the Order table for something like this. So what's happening in this query? You want to select a count of orders per customer. You have to count the orders and group by customername and make a join if you want the name perse and can't live with just the customerid.
To be on the safe side, I would do the GROUP BY over Name and ID. Otherwise customers with the same name get grouped into one. SELECT [...] GROUP BY C.CustomerID, C.Name;
Is it really a problem to load it in one transaction or does that just depend on his situation - like if he's in danger of running out of space on his log drive?
Are you taking TLog backups throughout the day? Can your ETL process be rerun to makeup for failed database? Can the business sustain downtime for the rebuild/reload? If so, SIMPLE might be for you. Bulk logged is really just deferring the logging of those operations until the next backup time, rather than logging the changed operations, it leaves a pointer in the log file to the pages of that operation. One of the side effects of this is that your TLog backups larger, and since it has to copy all those pages from the MDF during backup, more IO intensive as well. [http://social.msdn.microsoft.com/Forums/sqlserver/en-US/763daa39-b4e6-4308-8410-8964273ef325/tlog-backup-size-in-full-vs-bulk-logged-recovery-model](http://social.msdn.microsoft.com/Forums/sqlserver/en-US/763daa39-b4e6-4308-8410-8964273ef325/tlog-backup-size-in-full-vs-bulk-logged-recovery-model) 
Yeah...our data is "not so great". It about a second. 
Man, don't start me on bad data. The guys who wrote this home grown membership system didn't properly write the system. It's filled with triggers and cursors because they didn't know how to write the logic in the front end application to maintain various tables.
If there are foreign key constraints you should update those foreign keys to point to the proper record before deleting the primary record.
How can I do that via script?
 delete from ForeignTable where PrimaryKey in ( select PrimaryKey from Persons where Something=Something ) delete from Persons where Something=Something
If your interest in learning is motivated by your bottom line, just make a jump to project management instead because you will eventually hate your career path if you continue as a developer.
lol! I'm not ready to throw in the towel on contributing anything useful to society just yet. 
Broadening your skillset is a good thing. Is 85k their opening offer? You probably have some wiggle room, if so. 
As a point of reference - the Tableau conference is 50% larger than the Professional Association for SQL Server conference. Almost no investments have been made to SSRS in the last 2 releases of SQL Server. Most BI improvements are in the Power* product family. Product specific knowledge isn't all it's cracked up to be. If you want to keep tuning SQL, people always have the need for the ability to tune code. If you want to make mad money, learn SAP.
you could use a cursor to move it row by row. 
Thanks for the response. I am horrible at explaining something that i'm envisioning. maybe a simple way to explain it is a record needs to be reviewed every 30 days, between the day it lists, and when it becomes inactive at day 100. based on active records, how many will qualify for review next week on Monday, Tuesday etc. 
Yeah, you need a date dimension table or a "tally" table to do this in a set based manner. Here's a mockup: --use a tally table mockup to project the next weeks worth of dates with dt as ( select t.Tally ,dateadd(day, t.Tally, cast(getdate() as date)) as [Date] from ( select 0 as Tally union select 1 union select 2 union select 3 union select 4 union select 5 union select 6 union select 7 ) t ) select dt.[date] ,datename(dw, dt.[date]) as [DOW] ,ASP.Assigned_Date ,datediff(day, ASP.Assigned_Date, dt.[date]) as DiffInDays from dt cross join dbo.ACCOUNT_SPINFO ASP WITH(NOLOCK); 
I apologize for all of the spellings errors, posting this from my cell phone. I also wanted to note that to some reading this it may seem like RPG would be the obvious choice since I currently work in AS/400 shop. Well my employer already has three amazing RPG programmers and with that said all three have off handily said not to bother learning RPG. Basically they said if the bosses catch wind that's what I want to do I'll end up maintaining some seriously old and sloppy code. I have been playing around with free form RPG with embedded SQL. Also I career wise I don't think RPG is right move. 
If there is duplicate data and you have foreign keys in other tables referencing that record, the foreign keys need to be updated. You need to create a table with the mappings (preferably an actual table) that has the following columns. (Lets say its members and referral data. Referrals have a FK relationship to members. Members has duplicate data.) 1. Member Key being kept. (OriginalMemberID) 2. Member key being deleted. (DuplicateMemberID) Then join your referrals table on the member key being deleted in an update statement Update REF SET MemberID = OriginalMemberID from dbo.Referrals Ref INNER JOIN dbo.MyMappedKeys MK on ref.MemberId = MK.DuplicateMemberID Once that is done, you can delete the duplicate members. Keep in mind, Members might have more foreign key relationships. Thats why its good to keep your mapping of keys around in an actual table so you can reuse them.
I think you want a Windowed Function and PARTITION BY DATEPART() instead of all the case datediff's. 
One example is we keep our length of minutes for visits in varchar..in multiple rows for the same visit. 30 + 30 + 30 = 90 or 60 x 2 = 120. Depends on how they enter it. We use internal use codes in Epic...we're not really "on" epic. It's just weird. What's the front end made out of?
SELECT = projection (pi) INNER JOIN = theta-join WHERE = selection (sigma) ORDER BY can not be represented in relational algebra since it is based on sets. Sets don't have an order. Renaming (rho) is needed whenever the same attribute names are used in different relations. For your example query the sequence of operations would be something like this. Sorry for the lack formatting and formal symbols, I'm a little short on time. _rename_ PeopleDetails.PersonID into PersionID2 result1 &lt;-- _theta-join_ Properties with PeopleDetails on PersonID = PersonID2 _rename_ StaffDetails.StaffID to StaffID2 result2 &lt;-- _theta-join_ result1 with StaffDetails on StaffID = StaffID2 _rename_ Addresses.AddressID to AddressID2 result3 &lt;-- _theta-join_ result2 with Addresses on AddressID = AddressID2 _rename_ Cities.CityID to CityID2 result4 &lt;-- _theta-join_ result3 with Cities on CityID = CityID2 result5 &lt;-- _selection_ on result4 with Rent &gt; 500.0 final\_result &lt;-- _projection_ and _renaming_ of the wanted attributes of result5 You should take a look at the wikipedia article on relational algebra: http://en.wikipedia.org/wiki/Relational_algebra
Wrap parentheses around your first where clause and your second where clause then try using or.
I typed it all out then thought your second query was a subset of the first query so I deleted it. I then realized the field names were slightly different so I lost all my hard work formatting it.
Is there a way to do what I'm asking? Just curious
There might be some formatting errors but you could do something like with table as ( query 1 union query 2 ) select * from table where whatever
Don't be taken aback by the formal symbols. If you understand the SQL query you will understand relational algebra if you put your mind to it. It's just another way of writing the query. Maybe this will help a little more: http://rat.cs.panam.edu/in-action/tutorial/ The tutorial gives a lot of examples for SQL queries and their equivalent in relational algebra.
thanks 
Uh didn't 2012 add Powerview? 
85k for SSMS, SSRS and SSIS is kind of low. I mean, maybe if you live in the midwest or something thats a fair salary. I make quite a bit more than that in the Pacific Northwest. If its a contract job, you should push back. Hard. Tell the agency you want more. Say it after you nail the interview. By this point the company/client has told the agency they want you and they've already negotiated a .. I don't know, $250/hour fee. Once you're sure you're in, tell them you want more. At that point the headhunter can either give you slightly more money, or piss off the client and force them to restart the recruitment process. To answer your specific question, yes SSRS and SSIS are important tools. ETLs and business intelligence reports are critical skills for a data developer or analyst.
no, only if you also learn sharepoint and the company is willing to buy a sharepoint license.
Single-quotes are for strings. So this is always false. &gt; WHERE 'group'='newgroup' What you meant was [note the double quotes around the identifier] &gt; WHERE "group"='newgroup' Additionally, group is a reserved word. Rather than messing around trying to quote it to make it work for you, just suck it up and use a [different word](http://thesaurus.com/browse/group) for it.
Instead of that huge CASE just use: select datename(weekday,ASP.Assigned_Date) Why are you first calculating the difference between two dates and then just getting a weekday out of it? [Stop using NOLOCK, locking is good!](http://www.barkingaardvark.com/sqlblog/?p=50) I also think your second case can be made simpler and faster for the indexes: WHEN cast(GETDATE() as date) - ASP.Assigned_Date between 23 AND 29 THEN 'X' 
Where do you live? 
I was in a similar boat as you a few years ago, and yes you should be worried that nextgen may become obsolete quickly. Now for the good news, the other ehr systems out there are also just as broke. My solution was to break off from one organization and do consulting across lots of hospitals and broken ehrs. Trust me, that's a good place to be in if you can find your way there.
Which is expensive as Fuck, especially if you go with Knowledge Lake as a vendor set up. 
Oh, well yeah. From what I understand, most upcoming functionality in the BI toolset will require Sharepoint integration. Sucks, but that's Microsoft for you. At least Sharepoint licensing did change with 2013, so it could potentially not be as expensive as in the past. Even so, you are correct.
NYC
I don't think it necessarily has an expiration date. Even if the EHR system is replaced, you'll be the point man, so your job is secure. Whether you know it or not, you've accumulated a LOT of expertise about EHR and you are very useful. Now I'm saying, you *job* is secure. It might not be as fulfilling as you like, but you'll have a job. Depends on how techy you want to be in your future. I'd have *no* problem leaving a job for a 10% cut if the work was more interesting at the new place. 
Dude. I'm in the same boat as you. Learning ssms and ssrs right now. 
Fixed. Thanks =)
&gt; Instead of that huge CASE just use: select datename(weekday,ASP.Assigned_Date) Worked brilliantly! Thank you &gt; Why are you first calculating the difference between two dates and then just getting a weekday out of it? Ignorance and what I believed was grouping values by an alias. &gt; Stop using NOLOCK, locking is good! I should clarify up front that I'm an operations guy and self taught in SQL. I find it fascinating and if I'm able to get to data to bring back to excel, i can quickly look for things without adding to IT's workload by opening tickets that may or may not lead to anything. I use NOLOCK at the direction of our IT team. Until I read the article you linked I did not fully understand what it did other than placate others. &gt; I also think your second case can be made simpler and faster for the indexes: WHEN cast(GETDATE() as date) - ASP.Assigned_Date between 23 AND 29 THEN 'X' Looking into this. Thanks for the info!
I'd go with /u/Youraagh. The Excel button and Office-style presentation hints strongly at it being a MS product.
Your SQL server is running under the local service account NT AUTHORITY\SYSTEM. In most production systems. This is usually a dedicated service account like 'MYDOMAIN\PROD_SQL_SVC'. Then when you need to appropriate access to databases, local or network shares; you give read permissions to the database and write to folders. I believe you can add (if its not already in the sql servers users list) a user of NT AUTHORITY\SYSTEM. Give it read access to DatabaseName. Then make sure the folder you are writing to that NT AUTHORITY\SYSTEM has write access to the folder.
~~I have also tried the sa user but that still doesn't work.~~ ~~NT Authority\System does have login for the server, it is a sysadmin. The database in question it also has access too along with being a dbowner.~~ ~~Edit: Looking in the server logs I'm getting this as an error for when using the SA account or the NT Authority\System Account~~ ~~Error: 18456 Severity: 14 State: 38 -which basically says it doesn't have access to the database but both of them are db owners of that database.~~ Solved. I'm stupid. -_- 
For my curiosity what was it?
I spelled the database wrong... ._.
Could be a [toad](http://www.quest.com/toad/) install using an SSMS look
I was thinking 2008 with only the query and result windows shown...
Thanks for all your comments! I watched more videos and at one point she says she is using SQLite.... Which doesn't help me because when I downloaded it, it is only a command line tool... Argh! Getting the SQL environment that you want is easier said than done :)
Eh...85k does sound low honestly for that kind of work in NYC. It would be awesome for where I live but the cost of living is a lot lower here.
You need to use the [ROW_NUMBER](http://docs.oracle.com/cd/B19306_01/server.102/b14200/functions137.htm) analytic function to rank by unit_id in a subquery, then select the top ranked records. Use `NVL2` to null time_peroids where the val is null, thus sending them to the end of the ranking. SELECT unit_id, time_period, val FROM ( SELECT t.*, ROW_NUMBER() OVER (PARTITION BY t.unit_id ORDER BY NVL2(val, time_period, null) DESC NULLS LAST) rnk FROM temp_tbl t ) WHERE rnk = 1 ; --tested in [SQL Fiddle](http://sqlfiddle.com/#!4/11050d/4) Note that when using the test data, unit_id 4 and 5 still return null because no non-null value is available. You can filter further with a `WHERE` clause to get rid of those if you want.
Without any data or a way to see how you want the data to look in the end, I'm kind of stumped. Does this need to be an UPDATE statement or do you want it to be a SELECT only statement? If it's just a select, do this: SELECT [old number] AS 'new number' , [other information] FROM pineapple WHERE [old number] = '12345' When done this way, you're just rebranding the old number as the new number. No need for a while loop or case statement if it's done that way. If you want to do an update, just set your field called [new number] = [old number] Like I stated earlier, if you provide more info like what your data looks like and how you want your end result to appear, I can help out more.
The order you're sorting these things in and starting point is super important here. You need at least 2 CTEs. In the first CTE you're going be assigning a sequential row number based on your sort order. In the next you join your first CTE to itself on [row number] = [row number] + 1 and select the sum of your number out. I need the basic schema of your pineapple table to go any further. At a minimum I need the column you're summing, the pk column and column(s) you're sorting by. I can write you up the skeleton of the query if you give me that.
 SELECT distinct t.unit_id , FIRST_VALUE(t.time_period) OVER (PARTITION BY t.unit_id ORDER BY NVL2(val, time_period, null) DESC NULLS LAST) TIME_PERIOD , FIRST_VALUE(t.val) OVER (PARTITION BY t.unit_id ORDER BY NVL2(val, time_period, null) DESC NULLS LAST) VAL FROM temp_tbl t ORDER BY t.unit_id Gives the same result as the other response, just no subquery if that's an issue for you. 
Select only, the data format I'd like to see is as follows: - So, the data I'd like to track back would look something like this: - OldNumber, NewNumber, Other data 100 103 95 100 70 95 10 70 10 10 So to do this manually I'd be running the query: - select * from pineapple where new number = '103' copying the old value into the new value from the result and repeating until I hit the number where they are the same. I don't really get what your AS is doing in this context either, but I think that's an aside. Many thanks again. 
I'm on mobile right now. I'll come back to this later tonight. What is output you are expecting from your sample data? 
&gt;while loop that puts the new number value from the old query and runs it again, until new number = old number. Okay, I need more detail on what you want your end result to be? The way you are explaining it, you want your loop to compare the old number and new number columns and when they don't match, make them equal? If that's true, it's just a plain update statement. Can you add a column to your sudo-table example which shows your proposed end-result so we get a better idea?
Apologies if I wasn't clear, that would be the output I was hoping for. Although the matching number without the steps would be sufficient too. The table is just many variations of these numbers following that sort of pattern.
Thank you for taking the time to try and elicit the relevant information from me, really appreciate the gentle, helpful attitude this sub has. That sudo table was the end result. If you imagine running the query select * from pineapple where new number = '103' Then again on 100, then again on 95 all the way until 10, you get what I am doing manually at the moment. I just want to automate that process. I don't need to change the data, it's simply for information. 
Not as part of SQL Server. That's Office.
I'm still having trouble figuring out what you mean by "until newnumber = oldnumber"? I've got something which will do as jdawg suggested and basically "shifts" the whole column down (or up if you put Sequence-1 instead)... I don't think it's what you're looking for though. Create Table Pineapple (Oldnumber Numeric(9), Newnumber Numeric(9)) insert into Pineapple values (100,103 ), (95 ,100 ), (70 , 95 ), (10 , 70 ), (10 , 10 ) Relevant code: ;with cte as ( select Oldnumber,Newnumber,ROW_NUMBER()over(order by newnumber,oldnumber) as sequence from Pineapple) select A.Oldnumber,B.Oldnumber -- Note first one is from original CTE, but the second one is from the joined CTE which, because of seq = seq+1 is "shifted" up one. from cte A full outer join cte B -- Notice here I used a FULL outer join so you can see where it's "missing" data. on A.sequence = B.sequence+1 Note: Edited to add my table creation.
Oh yeah, I probably should have specified that you can alter that to do an update statement too... Make sure to add a new column so you don't delete any data! Also note that I put the new column into the original WITH statement's SELECT list so that I could reference it in the UPDATE statement. alter table pineapple add NewField numeric(9) ;with cte as ( select Oldnumber,Newnumber,ROW_NUMBER()over(order by newnumber,oldnumber) as sequence from Pineapple) Update A -- Note first one is from original CTE, but the second one is from the joined CTE which, because of seq = seq+1 is "shifted" up one. set A.NewField = B.OldNumber from cte A full outer join cte B -- Notice here I used a FULL outer join so you can see where it's "missing" data. on A.sequence = B.sequence+1
I'm now completely lost as to what you're trying to accomplish. Could you give me some sample input data with the expected output?
Perfect! Thank you!
Honestly, I thought in ASP.net pagination doesn't require multiple queries. It performs 1 query and when the first 50 to 100 rows are returned the website renders the gridview. The query continues to run for additional pages on the server side. It may do the top 50 and 100. How other scripting engines do it could be totally different.
 SELECT pu.post_id FROM post_users pu GROUP BY pu.post_id HAVING MAX(CASE WHEN user_id = 123 THEN 1 ELSE 0 END) = 1 AND MAX(CASE WHEN user_id = 456 THEN 1 ELSE 0 END) = 1 Here is with the join SELECT p.id, p.title FROM posts p INNER JOIN post_users pu on p.id = pu.post_id GROUP BY p.id, p.title HAVING MAX(CASE WHEN user_id = 123 THEN 1 ELSE 0 END) = 1 AND MAX(CASE WHEN user_id = 456 THEN 1 ELSE 0 END) = 1
Thanks! I'll try that.
How about this... SELECT p.title FROM post_users pu INNER JOIN posts p on pu.post_id = p.id WHERE pu.user_id in (123,456) GROUP BY pu.post_id, p.title HAVING COUNT(DISTINCT pu.user_id) = 2 
Quite like this version. Easy to read. Edit: actually, this doesn't work. It returns posts where user_id is EITHER 123, 456, where two users are associated, where the other user can have any user_id. Edit2: Or does it? I can't be sure. Could you confirm whether my above statement is correct or not?
add this to the having clause: and count(user_id) = 2
MSSQL then? Would you elaborate on what closetime and opentime are and what you are trying to accomplish between the two. I need more situational data to determine the right suggestion.
The SQL to do this is trivial, but you have to set up the supporting data correctly. This should give you an idea of how this might work. --DateDim contains one row per date --HourDim contains 24 rows with hour values of 0-23 --both contain flags that specify what hours should be counted select count(*) as NumberofWorkHours from DateDim d cross join HourDim h where dateadd(hour, h.[Hour], d.[Date]) between @StartDate and @EndDate and d.Workday = 1 and h.WorkHour = 1;
Look at the WHERE line: WHERE pu.user_id in (123,456) The GROUP BY clause will do a sort operation on the result set. It cannot sort or aggregate *anything* that doesn't satisfy the WHERE clause. There is no other user id that goes into the COUNT operation.
It would work. Post user can only be the 2 values in the IN clause so if count distinct results in 2, you can be sure it's only those 2 values.
Trying to get a true turn time report based on hours of business. This data is from a ticket tracking system. Closetime will be the datetime the CSR closed the ticket. Opentime will be the datetime that the ticket was opened. The turn time will be the average hh:mm:ss that a CSR took to close their tickets. We will be looking at the CSRs that have the highest turn times and determine if there are training issues causing the times to be excessive. I do not want to have a CSR that just happens to get an issue that has to stay open all weekend, for example, on a consistent enough basis to have their turn time reflect negatively. Edit: Essentially, for every 24 hours that the ticket is open, I would like to remove 12 from calculation. Except for weekends and holidays, which I would remove the whole 24 hours from the calculation.
Hey, I got to work and my colleague had beaten me to this, using a totally different method. I really appreciate all your help and will endeavour to make a better supported/clearer post prior to asking for help again. 
Thanks for the explanation.
You're welcome and thank you for this thread as well. It's always fascinating to see in how many ways the same basic logical problem can be broken down. 
I'm surprised there are so many different ways. Might have to crack out the benchmarks!
You can connect to the postgresql server, but you can't run psql. Is that right?
Ah, it makes me so sad when I see this. I hope you have shell access to the box! You're going to need to make some space. Best practice is to have separate partitions for root, your data, and your WAL files. I'm going to assume that everything is on the same partition for you. That means your first port of call is to clean out some logs on the server. SSH in and delete a few logs, or nuke some other small data. Second, if you can live without the data use TRUNCATE instead of DELETE. DELETE won't actually reclaim disk space (for reasons that are rather tedious to recount at the moment), though (eventually) it will leave you with more room in the database. This is roughly because when you delete a row, Postgres just goes in and marks the row as no longer needed to make room for later overwriting. There are good reasons for this but an extended description of MVCC is probably a little overkill here. Anyway, to make a shameless plug, as an alternative to solving these problems I might recommend considering our product Heroku Postgres in the future. Our whole deal is that we do this kind of stuff for our users so that they don't have to learn it. :) 
I've been working as a programmer analyst for the past two years dealing heavily with SQL(PL/SQL) so feel free to use me as a resource if you would like. Always glad to help!
Thanks. In my database, the list can be very long (over 30,000 records) and the client doesn't know the page number or the total number of records. I will keep using the current implementation.
I'm down, in CST time zone (Chicago) and new to SQL. my experience is making simple inserts, updates, etc beginner things On my phone, care to provide more details? Also, out of town this week, not sure if that will impact anything. 
My first reaction is to start up python and do the counting off-db because I can't think of an obvious way to group on arbitrary substrings like that. Also doing millions of individual inserts is slow because of the overhead, but a massive insert values statement , or a load data infile is much faster (because the overhead is in the transaction start/end and not the individual bits of data sending down the wire.)
I found a way: http://www.marcogoncalves.com/2011/03/mysql-split-column-string-into-rows/ But it creates a temporary-table with the data. Maybe there is a way to get the data for each row, probably creating a temp_table for each row, because if I keep 1-temporary-table for all the rows it may grow too big.
Normalising those 3000 integers per row is still going to be a hell of a lot faster on anything you plan on doing with that data. A couple of million rows is nothing to be afraid of, index it, and write your queries to be sargable and you're good.
the primary key is a cluster-key of 3 columns, and it will be repeated for each of those 'millions of rows', toooooooooooooo much overhead
I'm up to ex6 and unfortunately I won't be home today til late. I might continue to work on it tonight but for sure tomorrow. PM'd details
I think the question you should be asking is why does your table have 30+ columns that define a key. Consider creating a surrogate key of 1 column and using that as the foreign key used in other tables.
Also, the answer to your question is "no"
yes I suspected there is no way of doing this using basic JOIN/GROUP BY queries. However I hoped that a clever combination of INTERSECT and JOIN might work. I think /u/sephv1 suggestion is satisfactory for my purpose.
Natural Join is what you're looking for.
You could do a NATURAL JOiN between the tables if the columns are all named the same.
Yes, the solution though was apparently simplier than I thought. I just had to resize the droplet.
Pretty cool that you can do that. It wasn't very long ago that a full disk made DBAs start pulling their hair out.
Thank you for the useful feedback, especially on how DELETE versus TRUNCATE works. Resizing the droplet fixed my problems, but I will definitely look into Heroku to prevent crisis like this in the future.
I think a few of the queries work fine for only 123 and 456, but I've gone ahead and removed that restriction, anyway.
And if the RDBMS supports it. And if there are no shared non-key column names.
Yep. I rarely use it. It reminds me of MOVE CORRESPONDING from my old COBOL days.... Didn't use that either...
I didn't use it for more than just the class. If i remember, if's shareware so you have to pay for a license eventually. Then at work i use SQL Server Management Studio so it's a lot more powerful.
Reminds me of what it might be like when/if we begin to store/retrieve memories as data, and query our past. If only this was reality! `SELECT name, address FROM people WHERE gender = "f" AND attractiveness &gt; 80 AND sexual_standard IS NULL`
Read the chapter on INNER JOIN again. (assuming this is for school)
That's what I thought but I don't know how to apply it correctly in this case to get the results I wrote in the picture. For example I've tried doing an INNER JOIN from A -&gt; A_B -&gt; B and do a WHERE to get the As I want but it's not the correct query to get the results I need. I can't think of the right way to apply JOIN so I get the desired result. &amp;nbsp; Example db to test with: http://sqlfiddle.com/#!2/dcf59/18
Shouldn't this table be called `person`?
I'll try to give you a push into the right direction without writing out the code. You learn a lot more by thinking it trough yourself: Getting all the A's that have a link to any one B is trivial (I hope, otherwise read up on joins). A little detour for bit of Syntax you might be missing : EXISTS --example : SELECT * FROM sometable a WHERE exists ( SELECT 1 FROM someOtherTable b WHERE b.id = a.id ) The exists clause will check if the subquery will return any row. Any row at all, the data the row contains is irrelevant. You are allowed to reference the outer tables inside this subquery, which makes it very useful. I could go into performance considerations, but in my opinion Mysql sucks big time on query optimizing (and a lot of other things) so I'm not even sure it would apply, and anyway judging by the question, you proberbly are not yet on a level to have indepth discussions about excecution plans. So to make it short, exists is not a performance killer, it will not go trough the entire table, it will / should only go on until it hits the first row matching the condition. Now, back to the issue. &gt; if I pass B id 0, 4 it should return any A which has only links to B id 0 and/or 4 So you have to get all the A's that DO NOT have a link to any B not in the List of ID's you are searching in B. So, you write a subquery, getting you all the links A has to B, where B's ID is not in the list you specify. Then you exclude all the A's that have any such link, while also having a link the the B's which you filter on. If you want, I can type that out for you, but try to get it working yourself first please. best regards 
I prefer plural named tables.
What's he looking for? Schema only or data too? What's he hoping to achieve in the end? If you want it to be text you're going to have to either export all the scripts to create the tables, indexes, views, procedures, etc. or come up with some piece of software that can interpret something like an xml representation of the schema into those scripts. Same for the data. I would start by looking into Redgate Sql Source Control.
There's probably a few ways to do this may depend on how many tables you have to backup, how big those tables are and how often you want to back them up. The approach I'd probably take without further clarification is to use an ActiveX Script to connect to the database, query the specified tables and write CSV files. Then use 7Zip's command line interface to compress them all into a single archive at high compression and push the results to a specified location. If it's a smaller amount of data I'd consider pushing it into an access database as well.
 SELECT * FROM PEOPLE WHERE CONGRESSMAN = TRUE AND (UNRESOLVED_AFFAIRS_CNT &gt; 0 OR SECRETLY_GAY = TRUE) 
I wonder if the query planner would be smart enough to optimize that to SELECT * FROM PEOPLE WHERE CONGRESSMAN = TRUE ^(My apologies to the decent congresspeople out there)
Generally, if you are putting your data onto GITHUB it would be the scripts necessary to recreate a database. Data may be included but this would be the data only necessary to get the application up and running or default values you want included in the app (Default admin user for initial login, default group level permissions for application). Right click on database. Hover over "Tasks" [Click on "Generate Scripts"](http://www.mssqltips.com/tipImages2/2500_image001.png) Through the Wizard you can select the objects you want to generate scripts for. Pretty much, select everything to script out the whole database. [If you want data too, click on Advanced.](http://www.mssqltips.com/tipImages2/2500_image004.png) [Choose Schema and data](http://social.technet.microsoft.com/wiki/cfs-filesystemfile.ashx/__key/communityserver-components-imagefileviewer/communityserver-wikis-components-files-00-00-00-00-05/0537.AzureDelivGd_5F00_Fig12_5F00_AdvScriptingOptDialog_5F00_b.jpg_2D00_550x0.jpg) 
There should be an inner join or union joke in there somewhere...
&gt;This exercise was sent to me by an employer that I'm interested to work for. *Edit - Erased because I'm not helping someone unqualified get a job if they don't legitimately understand it.* 
Your query does not attempt to identify a particular reader, as per your requirement. It does, though, look as if the joins are correct to help you achieve what you need. If that was all the confirmation you were looking for then yup - looks good so far! Otherwise, maybe look into the MAX() and COUNT() functions to move you forwards. HTH
This, this is why we do not test queries on production.
I came in expecting this to be somebody used to MySQL who just doesn't get how big boy databases work. Turns out it's actually a list of my biggest pet peeves. Great list.
The whole lack of "nulls first" clause in ORDER BY has always been a thorn in my side.
If I understand you correctly, something like `select * from table where floor(FLAT_FEE) &lt;&gt; FLAT_FEE `
That worked. Can I also use FLOOR in an UPDATE/SET? UPDATE TABLE SET FLAT_FEE = FLOOR(FLAT_FEE) WHERE FLOOR(FLAT_FEE) &lt;&gt; FLAT_FEE;
You animal!
It's also why step 1 is always take a backup. 
Yeah it's just a built-in function.
&gt; When declaring UNIQUE constraints on columns, SQL Server treats all NULL values as unique. This means you can't have more than one NULL value in a unique column. ......... annddd
here is how I would do this, if you only want the information of table a returned : SELECT * FROM a WHERE EXISTS ( --this will filter all a's that have a link to any of the selected b's SELECT 1 FROM A_B WHERE a.id = a_b.id_a AND a_b.id_b in (0,4) ) AND NOT EXISTS ( -- this will filter out all the a's that have a link to any of the NOT selected b's SELECT 1 FROM A_B WHERE a.id = a_b.id_a AND a_b.id_b NOT in (0,4) ) If you need the information of table a and the linked table b, replace the first exists expression with inner joins : SELECT * FROM a INNER JOIN a_b ab1 ON a.id = ab1.id_a INNER JOIN b ON ab1.id_b = b.id WHERE b.id IN (0,4) AND NOT EXISTS ( -- this will filter out all the a's that have a link to any of the NOT selected b's SELECT 1 FROM A_B WHERE a.id = a_b.id_a AND a_b.id_b NOT in (0,4) )
It violates the SQL standard and means that you can have two records that cause a key conflict without being equal.
Look into SSDT, you can reverse engineer your db and create a VS project for it - check in/out of any source control system you desire. Alternatively RedGate has a database source control add-on for SSMS.
Maybe the standard needs to be rewritten.
Why?
Can you create a view and do your joins on the 30+ keys once? Then reference that view when you need it without having to retype your joins?
To cover all scenarios you will probably end up needing to do a scalar function, passing it a open and close time.
office.microsoft.com/en-001/access-help/access-wildcard-character-reference-HP005188185.aspx
If you add an id column to user_skill and project_skill you can do it with a fairly straight forward count distinct. Select username, description, count(distinct user_skill_id), count(distinct project_skill_id) from project Join project_skill on project.id = project_skill.project_id Left join user_skill on project_skill.skill_id = user_skill.skill_id Left join user on user_skill.user_id = user.id Group by username, description Having count(distinct user_skill_id) &gt;= count(distinct project_skill_id) http://sqlfiddle.com/#!2/21a713/1/0
Thanks! Not sure if it's 100% correct though - this shows that John can build PHP websites - yet he's missing skill ID 3... http://sqlfiddle.com/#!2/3cb113/1
For a small number of possibilities like this, a case statement is easy enough: select tbl_data.name, sum(case when tbl_data.decision = 'Yes' then 1 else 0 end) Yes, sum(case when tbl_data.decision = 'No' then 1 else 0 end) No, sum(case when tbl_data.decision = 'Maybe' then 1 else 0 end) Maybe from tbl_data group by tbl_data.name; **edit**: Apparently Access doesn't have case statements. Sorry about that. Try this (untested, but should work)? select tbl_data.name, sum(iif( tbl_data.decision = 'Yes' , 1 , 0) ) Yes, sum(iif( tbl_data.decision = 'No' , 1, 0)) No, sum(iif( tbl_data.decision = 'Maybe', 1 , 0)) Maybe from tbl_data group by tbl_data.name; If there are more possibilities, this gets annoying very fast. Then you'll need to google 'pivot' for your version of SQL, since it isn't standard anywhere. 
Wow... I thought Case was SQL standard. 
Sorry for the rubbish title, just realised I never came back to it after writing the post.
Yeah access doesn't follow standards in many ways, sadly. I should have said, rather than nested iif, there's the SWITCH function that acts similarly. 
Many thanks to both. Got it working with something like: SELECT tbl_data.name, SUM(IIF(tbl_data.decision = "Yes",1,0)) AS Yes, SUM(IIF(tbl_data.decision = "No",1,0)) AS No, SUM(IIF(tbl_data.decision = "Maybe",1,0)) AS Maybe FROM tbl_data GROUP BY tbl_data.name;
Would that work better than the IIF option in my reply above? http://www.reddit.com/r/SQL/comments/2e2gn3/access/cjvgh81
yeah, sorry, I tried to reply to your post but was on a tablet and somehow replied to OP. ANYWAY, no, I don't think it would work better than your Iif option in this case. I think where SWITCH takes over is if there are more than a few responses, in which case it's easier to type and keep track of the parentheses and all, like CASE. 
[I think this will do what you want](http://sqlfiddle.com/#!2/568f6/1/0). SELECT DISTINCT project.description , user.username FROM project CROSS JOIN user LEFT OUTER JOIN project_skill PS ON project.id = PS.project_id LEFT OUTER JOIN user_skill US ON user.id = US.user_id AND PS.skill_id = US.skill_id WHERE NOT (PS.skill_id IS NOT NULL AND US.skill_id IS NULL)
I often shrink databases when moving from PROD to DEV. You just have to be very careful when running performance tests the first time. 
If you are moving files from PROD to DEV, do a shrink and a backup. No point in restoring a database and the .ldf file is needing 75gb of free space for a brand new transaction log or an .mdf file needing 120gb for 80gb of data. Make sure though after the shrink your growth settings are correct for the database based on how much data is loaded or created in that database on a regular basis. Edit: more detail Shrinking has no performance increase and can in the future cause a performance decrease which is why its generally frowned upon. If your database grew to 120gb at one point and then some tables were truncated, there is probably a reason it should stay at 120gb. Shrinking the database will slow loads and large inserts later when the sql server will have to increase the size of the database for the new data based on its growth rate. If your growth rate is at 100mb and you are loading 2gb of data into the database, sql will then have to expand the database 20 times to accommodate the new data.
Thanks for the help so far... Here's an update. I'm pretty sure (~ 99%) that access uses * instead of % as Like "%" returns no finds and like "*" returns all the files. For whatever reason single quotation marks yeild the same results as double. To make matters worse/more confusing if i do: SELECT * FROM Table WHERE (Name like "S*") It'll bring back Sara... WHAT THE HECK?! 
If all else fails and you are using a "Full" database recovery model, you can switch the recovery model from full to simple. Then run shrink. After this is done switch it back to full. This should free up the space.
UPDATE!!! So for whatever reason its reading the DeptName as a number. Thanks for the help everyone!!!
She uses MySQL and SQLite mainly (Postgres sometimes) but it's always mentioned which one and why. It's all about SQL Standard and differences between each DBMS are explained. This pretty client is Database Master and i was very comfortable using it. Unfortunately you can only use trial version (sth about 30-day limit) unless you buy the soft of course. It's excellent for learning purposes. Light and intuitive.
If you are going to shrink be sure you run index maintenance once you are done as shrink will fragment the hell out of everything.
&gt; Wow... I thought Case was SQL standard. it most definitely is... Access isn't 
 select U.username, U.ID, Group_Concat(Distinct US.skill_id), Group_Concat(Distinct PS.skill_id) from user U join user_skill US on US.user_id = U.ID join project_skill PS on PS.skill_id = US.skill_id where PS.project_id = '1' Edited the syntax to work from Group_Concat. I think it's what you're looking for. **Alternatively** this way will provide you exact matches with no need for a stare and compare: select * from (select U.username from user U join user_skill US on US.user_id = U.ID join project_skill PS on PS.skill_id = US.skill_id where PS.project_id = '1') as Username, (select Group_Concat(Distinct US.skill_id) as User_Skills from user U join user_skill US on US.user_id = U.ID join project_skill PS on PS.skill_id = US.skill_id where PS.project_id = '1') as User_Skills, (select Group_Concat(Distinct PS.skill_id) as Project_Skills from user U join user_skill US on US.user_id = U.ID join project_skill PS on PS.skill_id = US.skill_id where PS.project_id = '1') as Project_Skills where (User_Skills = Project_Skills) group by Username
... and NULL represents an unknown/undefined/missing/[include your term of choice] value. Thus it is incorrect to say that the NULL in row 1 represents the same value as the NULL in row 42 (or any other NULL). That's why NULL = NULL returns false. So it cannot be said to violate a uniqueness constraint.
For some reason the left join isn't doing what it should. Not sure why.
"Don't shrink" should really be "don't shrink regularly" or "don't shrink on a schedule". If you shrink a database that is just going to regrow to the same size, you've done nothing but waste time and resources. That being said, there are plenty of good reasons to shrink a database, and this is one of them.
/r/SQLServer ??
what do you need?
Will PM.
ok DBA full time and might be able to do something on the side if it's not too time intensive
This will be fucking easy for you then. Got Skype?
can you email me, have to put my kids to bed soon and just waiting for them for a minute or two it's my username at gmail dot com
Check Reddit PMs.
It appears /u/tcme has this under control, but for those looking: Put a little description in the post. Hours expected, amount of work, etc. Helps me decide if it's moonlighting work or "holy shit, you should hire somebody" work.
Also the DBMS you're using - I'm a huge MSSQL guy but a novice at MySQL. 
Ha, well, in that case, MySQL would make you tear your hair out. I started on MySQL, but have since become very adept at MSSQL (the whole package--SSIS, SSAS, SSRS, etc), and now when I have to work on MySQL, I'm like, "WHAT DO YOU MEAN YOU DON'T HAVE WINDOW FUNCTIONS?!"
Allow me to introduce you to PostgreSQL. Or MSSQL's inbred redneck fat cousin, SybaseASE (aka T-SQL syntax with no window functions).
Afraid not - this query against Project #2 should only show one user, john : http://sqlfiddle.com/#!2/e2bc9/1
At least it doesn't use FoxPro!
Perfect answer. The db team I'm consulting with had the "don't shrink" attitude and came around when I explained it about this way. 
I don't think that the client (SSMS, SQLCMD, etc.) passes the filename to the SQL instance, so the instance doesn't ever know what the name of the file is. The instance just sees a batch of TSQL commands that needs to be run. "Scripting variables" are special variables that are interpreted by SQLCMD or SSMS running in SQLCMD mode. For more details go [here](http://msdn.microsoft.com/en-us/library/ms188714.aspx) It does not appear that the scripting variables defined by default provide a file name or path, which would have provided exactly what you want. If you are using SQLCMD to run your scripts via CMD.exe or Powershell, you *could* set a scripting variable to the name of the file and then use that variable in the script. Or, similarly to Example D on that MSDN page, you could set an DOS-style environment variable and refer to that in your script. Admittedly, both of those 'fixes' would be a clunky work-around and not a clean solution.
print object_name(@@procid) will get you the stored procedure name (but will return null outside of a stored procedure). You can get the script filename (if your total calling command is &lt;= 128 bytes) by setting it manually with SET CONTEXT_INFO %1 in the calling script. Then you can retrieve it with SELECT CONTEXT_INFO() The above assumes a Windows client. In an operating system with a shell, like linux or OSX, that would be $0 instead of %1.
You can't get the name of the .sql file because it doesn't contain a context(if you are using a context call, **xilanthro's** method looks like it will work). They are just text that are analyzed by the database engine and ran. What you can do however is pull what queries ran and pipe the results to a file: *** -- Show recent queries that ran -- Recommended to use the WHERE to limit datetime range SELECT qstats.last_execution_time AS [Time], qtext.TEXT AS [Query] FROM sys.dm_exec_query_stats AS qstats CROSS APPLY sys.dm_exec_sql_text(qstats.sql_handle) AS qtext --WHERE last_execution_time &gt; DATEADD(hh, -1, GETDATE()) ORDER BY qstats.last_execution_time DESC ***
Reworked... how about this? select U.username, U.ID from project_skill PS join skill S on S.id = PS.skill_id join user_skill US on US.skill_id = S.id join user U on U.ID = US.user_id where US.skill_id in (select group_concat(distinct PS.skill_id) from project_skill PS where PS.project_id = '2') Group by U.username
I found this page on microsoft's website for the different certifications for SQL Server and I was curious as to whether these certifications were: a. good for learning sql OR b. worth obtaining to put on the resume to show sql knowledge / proficiency 
Interested to know this too. From what I've heard so far, experience typically counts for far more than certs.
Looks like you're missing a single quote before "select". You have: &gt; call csvwrite ('~/extract.csv', select * from smalltable a, bigtable b, where a.uniqueid=b.uniqueid'); I believe you need: &gt; call csvwrite ('~/extract.csv', **'**select * from smalltable a, bigtable b, where a.uniqueid=b.uniqueid');
Try this. Select * from smalltable a left outer join bigtable b on a.uniqueid = b.uniqueid 
Thanks for the thorough reply! I have been looking at some analytics consultant positions within the company I work for and all of them have T-SQL / SAS knowledge listed as a requirement so I was looking at different ways to obtain that specific requirement.
Just for the CV. You don't learn a huge amount studying for the Certifications in my experience, and I know other people who became certified who still only have a basic understanding of sql server. If you just want to learn the SQL Language, dive in and get your hands dirty. Immerse yourself in forums and read SQL Server Performance which is a great book. 
This didn't help, but yes I had intended the single quote to be there. It was there in several attempts.
Thanks, but this gave me &gt; call csvwrite ('~/extract.csv', 'select * from smalltable a inner join bigtable b, on a.uniqueid=b.uniqueid'); Syntax error in SQL statement "SELECT * FROM SMALLTABLE A INNER JOIN BIGTABLE B, ON[*] A.UNIQUEID=B.UNIQUEID "; expected "identifier"; SQL statement: select * from SMALLTABLE a inner join BIGTABLE b, on a.uniqueid=b.uniqueid
you added a , after "bigtable b" which is incorrect syntax on an inner join you may also want to add a ; at the end of your statement. call csvwrite ('~/extract.csv', 'select * from smalltable a inner join bigtable b on a.uniqueid=b.uniqueid;');
Interesting.... something else going on then other than your syntax.
Thanks! I made changes to my initial query and failed to take out the , I can't look at it for a bit due to meetings, but I'll give it another go tonight. Thanks very much for your time!
Sorry. I don't really know much about H2. The only other thing I can think to try based on the documentation is to include an explicit option (character set or delimiter). The example I saw for CSVwrite syntax is: &gt; CALL CSVWRITE('test2.csv', 'SELECT * FROM TEST', 'charset=UTF-8 fieldSeparator=|'); I would expect that last parameter to be optional, but if it is not or the default values are missing or something? Small likelihood of being the problem, but easy enough to be worth trying, IMO.
Country a and Country b are the same table. The second select is a field in the first select. So basically you're saying: Count up how many countries have a surface area that's larger than this country (plus 1), and then show me that number in a column called "Rank". Then show me the name and surface area of each country. So if you had three rows in country: Country A 5 Country B 9 Country C 13 You would return: 1 Country C 13 2 Country B 9 3 Country A 5 There are 2 countries that have a larger surface area than Country A, and so on... EDIT: I changed the return code, because I remembered it's sorted DESC.
What server?
2012
Nope :( http://sqlfiddle.com/#!2/fa9f0/1 Peter should not be listed. I'm leaving it for now and solving it in a few lines of PHP; at least I have confirmation it wasn't a simple problem to solve :)
Yes. I have it capped at 27GB. That's how much memory sqlbufferpool reported it was using.
1. Not really. 2. Do you have working experience ? If no, you should get certified for your first job. Those certs are there for people who have no experience can use them to prove they know this much. 
From personal experience, I would say get SQL experience as /u/marri3d4life mentions, and then go after the cert if the company you work for incentivizes it. For instance, the company I work for gives a hefty bonus for acquiring the SQL cert while employed with them, as we are a very SQL-heavy shop, and the knowledge needed for the cert goes a bit deeper than your usual query slinging.
Thanks for the help :)
What's your full table schema? If there's multiple hotels per visit number, and multiple visit dates per visit number, without other data to join on, you won't be able to get the min data/hotel combo - it's a many to many relationship. Is there a customer ID, or anything else that's part of the needed info? 
There is only one hotel per visit. You can join on visit number.
Your initial query is not optimal. For each country (each row value) you are causing SQL to perform an additional query. I can't imagine a remote possibility where the SQL optimizer would be able to hash that correlated sub query. So if you have 1000 rows in the table, 1000 additional queries must be run against that table. The problem with this is that people learn how to code this way against small databases that eventually grow, or they learn to code this way and move to another company with large databases and what used to be a difference of 2 seconds vs 10ms (non-optimal vs optimal) goes from 20 minutes to 3 seconds (non-optimal vs optimal). Honestly, considering the grain of the table is probably one row per country. You could probably do RANK or ROW_NUMBER. SELECT RANK() OVER (ORDER BY SurfaceArea DESC) AS Rank, Name, SurfaceArea FROM Country ORDER BY SurfaceArea SELECT ROW_NUMBER() OVER (ORDER BY SurfaceArea DESC) AS Rank, Name, SurfaceArea FROM Country ORDER BY SurfaceArea I believe both should give you identical output.
Okay, assuming there is a hotel ID in both tables, it's a basic join with a criteria containing a subselect. create table VisitDates (VisitID numeric(9),VisitDate date,HotelID numeric(9)) --truncate table VisitDates -- Commented so you don't delete your own table, oops! insert into VisitDates select 1,'01/01/2010',2 union -- Here is earliest date select 2,'02/01/2011',1 union select 3,'03/01/2012',1 union select 4,'04/01/2013',2 union select 5,'05/01/2014',3 union select 5,'01/01/2010',1 union -- Here it is again. select 1,'02/01/2011',1 union select 5,'03/01/2012',1 union select 2,'04/01/2013',2 union select 1,'05/01/2014',3; create table Hotel (HotelID numeric(9),HotelName varchar(40)) insert into Hotel select 1,'Hotel A' union select 2,'Hotel B' union select 3,'Hotel C'; select H1.HotelID ,H1.HotelName ,V1.VisitDate ,V1.VisitID from visitdates V1 join Hotel H1 on V1.HotelID = H1.HotelID where V1.VisitDate = (select MIN(V2.VisitDate) from visitdates V2) Edited to fix a few things. - Also I misunderstood the structure of your tables... This may be incorrect.
If you really felt so inclined, you could do a join on a subselected table (I forget the proper term for it)... It's just as confusing though. select H1.HotelID ,H1.HotelName ,V1.VisitDate ,V1.VisitID from visitdates V1 join Hotel H1 on V1.HotelID = H1.HotelID join (select MIN(VisitDate) as Min_VisitDate from visitdates) V2 on V1.VisitDate = Min_VisitDate The crux of your problem isn't the join, it's the establishing of the comparison value to compare your VisitDates.VisitDate field against. Another alternate would be to set up a variable and set the variable equal to the select MIN() part... It's all doing the exact same process though. declare @min_date date = (select MIN(VisitDate) from visitdates) select H1.HotelID ,H1.HotelName ,V1.VisitDate ,V1.VisitID from visitdates V1 join Hotel H1 on V1.HotelID = H1.HotelID where V1.VisitDate = @min_date
So, you're saying it was just a style choice on the writers part, and in general the old-style are "softly depreciated"?
Wouldn't an inner join be better for showing you if a relationship exists? It would return only results with relationships? DECLARE @RelationshipCount INT = NULL; SELECT @RelationshipCount = COUNT(*) FROM Quotes q INNER JOIN QuoteOrderLink qol ON q.QuoteID = qol.QuoteID IF @RelationShipCount &gt; 0 BEGIN RETURN 1 END ELSE BEGIN RETURN 0 END If you want to return a single table row just change RETURN X to SELECT X EDIT: You probably want a WHERE clause on the select like WHERE q.QuoteID = ??? Otherwise if ANY relationships exist you will get a '1' 
It's not clear what you're trying to accomplish. If you're just trying to return a list of Quotes, along with a check for existence of a QuoteOrderLink for each quote, you can do something like this using the CASE function: SELECT *, CASE WHEN EXISTS (SELECT * FROM QuoteOrderLink qol WHERE q.QuoteID = qol.QuoteID) THEN 1 ELSE 0 END AS HasQuoteOrderLink FROM Quotes q 
Since no one seems to have really answered your question yet: SELECT q.QuoteID, sum(CASE WHEN qol.QuoteID IS NULL THEN 0 ELSE 1 END)AS qol_count FROM Quotes q OUTER JOIN QuoteOrderLink qol ON q.QuoteID = qol.QuoteID GROUP BY q.QuoteID EDIT: This will give you a total count of relationships. You could also add another case to only return a max of 1 if any number of cases exist: SELECT q.QuoteID, CASE WHEN sum(CASE WHEN qol.QuoteID IS NULL THEN 0 ELSE 1 END) &gt; 0 THEN 1 ELSE 0 END AS qol_count FROM Quotes q OUTER JOIN QuoteOrderLink qol ON q.QuoteID = qol.QuoteID GROUP BY q.QuoteID 
 select q.quoteID, relationship_exists_ind = case when ql.quoteID is not null then 1 else 0 end from quotes q outer join ( select distinct quoteID from quoteOrderLink) ql on ql.quoteID = q.quoteID
Do you need to display any columns from QuoteOrderLink table? If not, here's one solution: SELECT *, CASE WHEN EXISTS (SELECT * FROM QuoteOrderLink WHERE QuoteID = q.QuoteID) THEN 1 ELSE 0 END 'Relationship' FROM Quotes q
I've never tried this method. I've always imported. I'm getting different opinions on migrating databases from 2008R2 to 2012. This guy says you can't attach or restore, and you instead need to import. http://dba.stackexchange.com/questions/65122/restore-master-database-from-sql-server-2008-r2-on-sql-server-2012 This guy says you can totally do a restore and has some kind of technique to walk you through the process. I'm at home on 2012 and don't have access to 2008R2 right now so I can't test this. http://sqlmag.com/sql-server-2012/migrating-sql-server-2012 At my last job, I had to help migrate our data from 2008R2 to 2012. Here is what we did: - obtained new server with superior hardware, and installed 2012 on the server - created backups of all 2008R2 databases - manually imported every database from server A with 2008R2, to server B with 2012 - ran all user/security permission scripts on 2012 - set all 2008R2 databases to offline mode - modified every configuration file for web services to point to server B - tested like mad - treated any issues with applications like a bug, troubleshooting and fixing it like a bug - refusing to rollback to 2008R2 in spite of business unit demands it was slow, tiring, painful, and very situation-specific work. But it kept problems to a minimum. From what the gentleman in the first article says, there are a lot of features to 2012 that don't exist in 2012, as well as features that have been deprecated. So it makes sense that restoring or attaching might not work. The 2008R2 MDF files could be missing some crucial components, or contain unsupported features that you might not be aware of. I'm a developer and BI Analyst, not a DBA. So maybe its possible to successfully attach or restore your entire set of databases from one version to another. It seems unrealistic to me though. Good luck
Running SQL as admin.
Can't right now. I did confirm the paths match up if that's what you're looking for.
You may still have to reset permissions manually. My sysadmin had the worst time with copying MDF or even zipped BAK files from one 2k8 R2 box to another, having to fix permissions so the user running the SQL service can see the files and restore/attach. Dealt with a ton of client-submitted databases so we went through this dozens of times.
Stack overflow is saying the same thing. They recommend importing. http://stackoverflow.com/questions/15163938/how-do-i-convert-a-sql-server-2008-mdf-file-to-sql-server-2012 One suggestion might be to install SQL Server 2008r2 on your laptop or some other device, then attach the mdf, then import from your laptop to the server. However, these guys claim to have a couple of workarounds. http://www.sqlservercentral.com/Forums/Topic1233485-391-1.aspx sp_attach_single_file_db @dbname='reddit', @physname='S:\AMBERDATAFILE\reddit.mdf' ^ where "reddit" is your db and file name This syntax might work as well. CREATE DATABASE Reddit ON (FILENAME = N'C:\Folder_with_MDF\reddit.mdf') FOR ATTACH_REBUILD_LOG Again I don't know if these will work. I'm a developer / BI Analyst and haven't done formal DBA stuff in a very long time. Remember to work with copies of your MDF in case any attsmpts to restore corrupts your MDF. Good luck.
Any idea what specifically the fix was? Thanks!
I think they first had to reset the file's owner (and folder owner if copying a whole folder), then remove any inheritable permissions and inherit permissions from the parent folder.
Does that account have access to the files? Can you try running the service under another account?
Do a substring on the first char of TaskID to match anything starting with 'B', or whatever...
The issue is you have a loop with this design and you will be limited to where the worker is also the project manager. You will want to alias the user table for separate lookups for worker and project manager. SELECT t.WorkerID, p.ProjectID, p.Name AS [Project Name], pm.Username AS [Project Manager Username], p.DueDate FROM project p JOIN task t ON ( p.ProjectID = t.ProjectID ) JOIN user pm ON ( p.ProjectManagerID = pm.UserID ) JOIN user w ON ( t.WorkerID = w.UserID ) WHERE t.WorkerID = @WorkerID
Dumb question. Forgive me if this is patronizing but just want to check all options. It needs to be on the C:/ drive of the machine you're executing from. So not YOUR C:/ drive but the server's If you're already doing that then again sorry for patronizing you and I'm out of ideas right now.
Ha I'll check with the host to ensure this. Thanks!
Try calculating the traits and interests scores in two separate CTEs then joining on the personid, adding them up and sorting on the sum. Not sure if that will help or not, but it shouldn't take long to rewrite it that way.
Great suggestions, guys, I'll try and see what works best.
I saw a link to this months ago and could never find it again. I need to start using ok board or something. Anyway, I love the way this works. Dependency based systems are so much better than automated or ordered ones. 
Oracle folks are the worst, because oracle was one of the last to get ANSI-style joins and their old style syntax is terrible. Select * From person p, department d Where d.department_id = p.department_id (+) Gross. (In case that isn't clear or I forgot the syntax: Select * From person p Left join department d on (p.department_id = d.department_id) 
What do you mean the client doesn't know the page number? This is what offset is for. SELECT id, name FROM students ORDER BY id ASC OFFSET 5 LIMIT 5; Or SELECT id, name FROM students ORDER BY id ASC OFFSET $n * ($pg - 1) LIMIT $n
Windowing functions are basically magic. 
If a table is going to be established for each type of relationship, then I think this approach does simplify queries. The query I provided will remain the same regardless of how many new relationships are added. Otherwise, the query will likely need to be updated to add additional joins/unions each time a new relationship is added. If every relationship functions the same and has the same attributes, then I think a simpler approach would be more appropriate. With the approach you suggested, I would still add a relationship_types table and a relationship_type_values table that are associated by IDs. This would normalize the database and make it easier to maintain. It would also prevent the need to identify categories and their associated options in the application code. This simpler approach is a bit constraining though and starts to fall apart when you need to store additional fields that are unique for each relationship association. The value of the approach I suggested is that each relationship can have its own unique set of additional attributes. Both approaches may be appropriate depending on the needs of the application.
&gt; Do a substring on the first char of TaskID Don't do this and don't suggest this.
I think we're largely in agreement, and I think the beauty of this approach is that you can start simple and add complexity later. In general, i'd look very hard at a solution that requires a different table for each category, because I'm currently working in a database that requires 9 joins to get from a lead to their address. If you use a foreign key to the category table you can have basic stuff in there and even add a JSON bag of attributes of they don't change much. 
He's joining a table with a function result. He should use a LATERAL join optimally. He's probably just doing it for brevity. To join a table and function result w no common key, you would add join func() on true , which is kinda verbose. 
I wouldn't worry about it too much. It's no worse than your subselect. Just make sure you have an index on account id. 
http://sqlmag.com/blog/debugging-nested-stored-procedures-sql-server-profiler not sure if this is what you are looking for
 create proc runcolor @color nvarchar(10) as begin if @color = 'red' exec runred end if @color = 'blue' exec runblue end end ^ something like this? you want to know who executed which color and when they executed it, right? you can document this stuff into a logging table pretty easily. just insert the three records (datetime, user, parameter) into the logging table. if you have a lot of people executing it, be careful of collisions.
Specifically, I want to log errors, so I'd be inserting into an Error Log table in the CATCH block. Since each SP has its own set of parameters, I was hoping there was an @@PARAMS or anything similar that would capture that for me, rather than put all that together manually for each proc.
Take?
There are samples in the mysql website: http://dev.mysql.com/doc/sakila/en/ http://dev.mysql.com/doc/employee/en/ I also like this one: http://www.artfulsoftware.com/infotree/queries.php
[Adventure Works](https://msftdbprodsamples.codeplex.com/releases/view/125550) is the go to sample database most schools use. Although I don't know about web stuff.
I posted [this list](http://www.reddit.com/r/SQL/comments/1x8gpa/im_looking_for_a_good_dataload_to_play_with/cfcm6x6) about six months ago. 
OP is looking for SQL Server sample DBs.
oooops, sorry.
I'm a little late to the party but I hope it still helps. :) I tried the following and was successful for all of my test cases: SELECT u.username, p.id, p.description FROM user u, project p WHERE NOT EXISTS ( SELECT ps.skill_id FROM project_skill ps WHERE p.id = ps.project_id AND ps.skill_id NOT IN ( SELECT us.skill_id FROM user_skill us WHERE u.id = us.user_id ) ); Explanation: The innermost query retrieves all the skills of one specific user. The middle query then goes through the list of skills of a specific project and selects only those skills that are NOT IN the previously selected user's skills. The outermost query then selects only those combinations of users and projects that did not miss any (NOT EXISTS) of the project's skills.
Man mysql is really hard when you are used to sqlserver. Not sure what the query plan on this thing will be like but I think it should work. http://sqlfiddle.com/#!2/328fea/16 Give me all the projects and users for where there are no skills in the project that the user doesn't have. SELECT p.description ,u.username FROM project as p CROSS JOIN user as u WHERE NOT EXISTS ( SELECT * FROM project_skill as ps WHERE project_id = p.id AND ps.skill_id NOT IN ( SELECT us.skill_id FROM user_skill as us WHERE us.user_id = u.id) )
Is that the reason why? I try to hold my tongue about Oracle developers because I'm sure there are great ones, just not the ones I've worked with. I have one who used the companies DBA and became the systems architect, who wrote most of our ETL code and I and a coworker have spent the last year rewriting all of her code from non-compliant ansi code to code that runs on MSSQL systems. We implemented a Code Review process just to stop the code from being put into production. My belief is that the oracle optimizer and the Embarcadaro suite is why she writes such crap code. INSERT INTO table1 SELECT 'A', field, 'something1' , field3, ltable.description from table2, ltable where field4 = '1' and ltable.value = table2.value UNION SELECT 'B', field, 'something2' , field3, ltable.description from table2, ltable where field4 = '1' and ltable.value = table2.value UNION SELECT 'C', field, 'something3' , field3, ltable.description from table2, ltable where field4 = '1' and ltable.value = table2.value I believe Oracle will realize shes an idiot and turn the unions into a hash and do a single read/write. However, MSSQL will perform 3 reads and a distinct/sort over a Cartesian product. Her code will take 45 minutes to an hour to run in an MSSQL environment, after rewrite into a case statement it takes about 5 minutes. INSERT INTO table1 SELECT CASE WHEN FIELD4 = 1 THEN 'A' WHEN FIELD4 = 2 THEN 'B' WHEN FIELD4 = 3 THEN 'C' ELSE NULL END AS A_FIELD, Field, CASE WHEN FIELD4 = 1 THEN 'Something1' WHEN FIELD4 = 2 THEN 'Something2' WHEN FIELD4 = 3 THEN 'Something3' ELSE NULL END AS A_SOMETHING, FIELD3, ltable.description FROM table2 inner join ltable on table2.value = ltable.value WHERE field4 in ('A','B','C')
I believe you can configure something like Wordpress to function with sqlserver, see the instructions [here](http://wordpress.visitmix.com/development/installing-wordpress-on-sql-server). This would provide you both a web application and a database from which to work with. The downside is, is that you would need to supply content though I believe there are some sample sites you can download and import to populate your wordpress site, you'd have to google for that though. as /u/Mamertine said though, Adventure Works is the go to sample database if the web application isn't necessarily a requirement. 
I think WordPress might be the way to go. Pretty much looking to conduct DBA tasks on the database and seeing how the web application handles. I'll definitely look into the link you provided thanks. Want to learn to be a DBA through practical learning similar way as to how I learnt how to write SQL scripts
I always built the report feed in stored procedures - it gives a ton of flexibility to use temp tables, use multiple passes for joins/parameters instead of throwing them all at SQL and hoping for the best, etc. One caveat, however, is that for some inexplicable reason, if the original params are used in where clauses, it can run really, really slow. The only work around I ever found was to reassign the value to a new parameter in the stored proc. Ex: CREATE PROCEDURE SSRS_Can_Be_Dumb @Param DATATYPE AS DECLARE @Param_Local DATATYPE SET @Param_Local = @Param do some sql
This. It's considered bad form in most cases to write data to a table that is computed from other parts of the database (with some notable exceptions...like aggregates in a BI system)
If you have a more robust database, you can also make a stored function to get the actual_hours on the fly. However, OP probably just needs a View. 
I also have better luck using something like: Where 'ALL' in (@Param) or table.column in (@Param) Declaring your parameters in SSRS should resolve the parameter sniffing issue. 
Thank you! Reading into it now.
Try setting nulls to "" (blank text) in your export. 
Recently, I had a case where I wrote a view that gave me counts of claims based on indexed fields in another extremely fat table. BatchTable ( BatchID int, ClaimType char(1), Processed bit ) BatchOutput ( BatchID, Claim, ~~~~360 columns Here~~~~, Errors ) BatchID and Errors are indexed. The view needed to report how many claims were in the batch and how many errors were found. Initially it was written as SELECT b.batchid, b.claimtype, b.processed, COUNT(*) as TotalClaims, SUM(CASE WHEN bo.ERRORS is not null THEN 1 ELSE 0 END) as TotalClaimsWErrors FROM BatchTable b inner join BatchOutput bo on b.batchid = bo.batchid GROUP BY b.batchid, b.claimtype, b.processed ORDER BY b.batchid DESC Because the view populated an ASP.net gridview with pagination I found it to execute much faster and return results using correlated sub queries. SELECT b.batchid, b.claimtype, b.processed, (SELECT COUNT(*) from batchoutput bo where bo.batchid = b.batchid) as TotalClaims, (SELECT COUNT(*) from batchoutput bo where bo.batchid = b.batchid where errors is not null) as TotalClaimsWErrors FROM BatchTable b ORDER BY b.batchid DESC However, I didn't dig into the execution plan. I kind of suspected it would go faster due to the amount of claims in batch output. 1. It may have been faster because it may have hashed the correlated subquery and joined it to the dataset as a 1 for 1 hash. 2. It may have been faster because it the per row execution cost against a small index far outweighed growing and aggregating the dataset back down. I'm radically opposed to correlated sub-queries but they can serve a purpose in rare occasions.
You gave me an idea, sort of. I ran this code and now it works fine. UPDATE project SET projectname = '' WHERE projectname IS NULL
MSSQL &gt;= 2008?
Yes, sorry, R2.
Try the NZ command in your select [MS ACCESS: NZ FUNCTION](http://www.techonthenet.com/access/functions/advanced/nz.php) [coalesce alternative in Access SQL](http://stackoverflow.com/questions/247858/coalesce-alternative-in-access-sql)
What does your execution plan look like? If you're not adept at reading Execution Plans you can use [SQL Sentry Plan Explorer](http://www.sqlsentry.com/products/plan-explorer/sql-server-query-view) to have it show you graphically where a few of your problems are.
Just a thought: SELECT b.batchid, b.claimtype, b.processed, bo.TotalClaims, bo.TotalClaimsWErrors FROM BatchTable b INNER JOIN (SELECT BatchID, COUNT(*) AS TotalClaims, SUM(CASE WHEN bo.ERRORS is not null THEN 1 ELSE 0 END) as TotalClaimsWErrors FROM BatchOutput GROUP BY BatchID) bo on b.batchid = bo.batchid ORDER BY b.batchid DESC
You have a table that has game, player, score. Write a query that lists the to ten players, with scores, for each game. You can use window functions, but in databases that don't have windows or are a bit older, correlated subqueries are the way to go 
Let me know if this works for you. I removed the temp table, changed it to an exists. Removed the correlated subquery using same dataset. Removed a bunch of erroneous isnulls on your sum statements, if its null its not counted. DECLARE @StartDate datetime = '2012-01-01' DECLARE @EndDate datetime = '2012-04-01' SELECT DATEPART(mm,L.createddate) AS MM, DATEPART(yy,L.createddate) AS YY, isnull(L.source_code__c,'None') AS SC, isnull(L.campaign_code__c,'None') AS CC, isnull(L.responsechannel__c,'None') AS RC, COUNT(DISTINCT L.id) AS IDs, ISNULL(SUM(CASE WHEN Y.cancelled = 'N' THEN Z.subcount ELSE 0 END),0) AS Submitted ISNULL(SUM(Z.setcount),0) AS Settled, ISNULL(SUM(Z.setproduction),0) AS Production FROM [dbo].[TABLE1] L WITH (NOLOCK) LEFT JOIN [dbo].[TABLE2] A ON L.convertedaccountid = A.id LEFT JOIN [dbo].[TABLE3] O ON A.id = O.accountid LEFT JOIN [dbo].[TABLE4] Z ON O.policy_number__c = Z.policyno WHERE L.createddate BETWEEN @StartDate AND @EndDate AND (L.firstname &lt;&gt; 'TBD' OR L.firstname &lt;&gt; 'XXX' OR L.firstname IS NOT NULL) AND EXISTS ( SELECT NULL from [dbo].[TABLE9] WHERE recordtype NOT IN ('Partial','Test','Duplicate') AND leadroute &lt;&gt; 'PARENT') ) GROUP BY datepart(mm,L.createddate), datepart(yy,L.createddate), L.source_code__c, L.campaign_code__c, L.responsechannel__c ORDER BY datepart(yy,L.createddate), datepart(mm,L.createddate), L.source_code__c, L.campaign_code__c, L.responsechannel__c edit: removed alias from exists subquery. changed alias from y1 to y. Added isnulls back in.
Yeah that would have worked as well. Essentially, I would be doing what the optimizer would be doing and hashing the aggregate before the join (only using the indexes). Another reason why I went the route of correlated was before it was a view, it was just a query I gave to the analysts and they were using a top 20 in it before we moved it to the asp.net application and it became a view. Top 20 with the correlated greatly reduced what data had to be hashed. Your solution is very good though.
Have you looked at [pragmatic works](http://www.pragmaticworks.com/PragmaticWorksHome.aspx)? One of the tools included in their workbench is DOCxpress. I'll take a look at their documentation tools for SSIS. The only thing about DocXpress is its extremely verbose.
Top 20 potentially (I'd guess likely) turns the join solution into nested loops instead of hash. Removing the group by from the outer query gives it some flexibility how it makes the join. I've even seen the reverse, where a subquery is executed as a hash/merge join. Subqueries are joins, but are often written such that they must use nested loops.
For the wrapped ISNULLs, if the sum was across all NULL values it would also return NULL. I believe he wanted the result to always contain a value in either case.
Good point. Added them back in.
This is great! Thanks. I haven't tried it yet because I seemed to have fixed my problem like I mentioned in the other comment, but if my solution stops working then I'll try the NZ function.
Can't you just use this as your query? So you don't have to worry about future NULLs: SELECT ISNULL(projectname,'') [projectname] FROM project
The Oracle optimizer is really, really good. And there are mountains of people on old-ass versions of Oracle, because it's solid and production-critical, so upgrading is slow. But unless you mis-translated into fake tables, that shouldn't cause a cartesian product. It will cause a distinct sort. Sometimes you also get weird typing behavior with that kind of query - if field4 is an int and you're passing it in as a string some combinations of query/driver do a type miscast and do a table scan in order to convert types.
You can do a substring of (1, end of string) of the column, where the string begins with a single quote; maybe like this: update table set column = substring(column, 1, length(column) - 1) where column like '''%'; I'm not sure what db you're using, so I cant give more exact syntax on the substring and length functions (I only know Oracle, but I imagine looking them up is simple enough). ninja edit: for the trailing single quote change the numbers for the substring and the like clause should use '%'''
If it was a "file" originally, your import tool should have something called "Text qualifiers". So if it is a CSV like 'apples','oranges','banana,strawberry' Text qualifiers allow you to import that as A B C apples oranges banana,strawberry If you specify a text qualifier on import it will remove the single quotes. Other wise use what crazytogrammer said or use REPLACE(field,'''','') 
Which database are you using?
Big thank you. Finally got back to working on this today and it's exactly what i needed.
Thansk jzknuckles! 
Not having a computer science background or a college degree means that you're going to have to go (well) above and beyond learning 'basic SQL' with a 'little analytics'. I support a couple large database applications in my work, and going in, I knew very little SQL. What got me the job was not any specific fluency, but rather the fact that I was college-educated and also that I had spent a half a year after college working on real-world development projects at a freelance capacity. The reality is, unless you're going to be hired as a SQL developer or database administrator, neither of which are entry-level positions, employers are looking for computer science foundations. Every job is different in terms of the technologies they implement, and it's constantly evolving. So, while SQL fluency is a start, I think that it would be best for you to aim for either an associate's degree or an advanced portfolio. I also recommend that you take a look at Microsoft's and Oracle's SQL certifications.
What do you want to DO with SQL? Are you shooting for DBA? Maybe a developer? Too unclear. I would not hire an accounting clerk who 'knows sql' for either role unless you would let me drill your teeth a bit becuase I 'know some dentistry'. Dont try to apply for programming jobs outright if youre not trained. If you want to get into programming, take a new accounting job with a company that will let you shadow their dev team or DBAs with some of your time. I took-in an employee from out accounting dept. once like that and he's great. Only other tip i have is don't learn sql for its own sake. Learn it to accomplish something that makes a company money and you'll have a lot more opportunity. Maybe you could learn about automating accounting? Most major accounting programs have a nice API. You could mess with that to get familiar with RDMBSs via your existing knowledge of accounting since they're all just databases anyhow. Source: Software dev. I manage a small team in a financial services company building specialized internal software 
well Tableau and SQL you could start learning BI good BI people get paid well. I would start looking at SSAS and SSIS aswell. also dont pigeon hole yourself on only a single database platform. 
My first IT certifications were SQL-centric, though I'd been doing IT and programming as a hobby for nearly a decade prior to getting certified. T-SQL was actually pretty easy to learn syntactically for me, extremely so in comparison to full-fledged programming languages. If you have no scripting or programming background and are unfamiliar with basic concepts like procedures, variables, etc, I don't know how easy it would be to just "pick up". Don't let that discourage you, everyone starts somewhere, I just wouldn't be making any short-term career decisions based on your ability to learn the SQL platform or T-SQL scripting. If I were you, I'd try to find an entry-level IT job somewhere. This is generally help-desk, but avoid call-center helpdesks as you'll become a hamster in the customer-service wheel and that is NOT what you want. What you DO want is to become surrounded by technology and immersed in the IT atmosphere. IT is largely knowing enough basic concepts to be able to research the correct answer or solution. For instance, knowing T-SQL is not going to help you if you don't know how to create an ODBC connection to actually connect to the DB. You don't even need to know how to do that without aid, you just need to know that you need to do it and have a basic enough understanding to google your way to a solution. Being part of any IT is going to give you the ability to absorb a wide range of knowledge, and become familiar with the concepts that will make you a successful IT member. At the root of IT, in all its forms, is logic-based troubleshooting. If you can learn basic troubleshooting skills, basic program-flow and logic, you will eventually reach a point where you can say "Yes, I'd like to go get SQL certified" or "I like Networking, I'll go for my CCNA." But to just say "I'm going to learn SQL" and then expect, even if you shelled out several hundred or thousand dollars to get a MCSA cert in SQL Server, that any IT company is going to hire you as a DB analyst with no prior IT experience is, while not impossible, very unlikely. So get into IT, if thats what you want to do, and then decide if SQL is what you'd like to specialize in. Edit: I forgot to add, if you do get into IT, please remember this one thing above all else. Your job, from now until the day you retire, is to **always be learning**. Every day. If your company is bringing in new technology, ask questions, try to learn at least a little about how it works, why its necessary. Don't shrug and write it off as someone elses problem because it falls outside your scope. Subscribe to some technology blogs, or listen to the occasional podcast. Find a programming language you enjoy using and stay up to date on its evolutions and iterations. IT changes daily, and if you don't change with it, you will wither on the vine. Everyone who has been in IT for more than a few years knows "that guy", generally an older person in the department, who decided to stop keeping up and who was kicked out the door when the last legacy server he supported was forklifted out of the department. Don't be that guy. Develop a passion for learning about technology, for being around technology, and your career will start to fall into place. 
If you're wondering why you're being downvoted, its because you're extrapolating out your IT bitterness (clearly you've had some crap jobs, I have too man, I totally understand) and stating that they apply across the board. I do believe IT burnout is a real phenomenon, but could apply to almost any "skilled" labor position. I've certainly had jobs where I was treated poorly, but I have to tell you man, there are ideal jobs out there as well. I'm in one now. 40 hours, every week. I think I've done maybe 5 hours of OT in 5 months while being exempt. Awesome pay. Great atmosphere. Appreciative management. So don't fill the community pool with your piss-water. Instead, I'd say you probably need to find a better work environment and maybe shed some of the subliminal bitterness you're injecting into the conversation. 
&gt; the tools included in their workbench is DOCxpress. Hi, Thanks for the reply. I'll give it a try. Battman
Interesting links nontheless. I do use mySQL too.
The most logical progression for someone in Accounting to use sql is to move to an accounting focused role where you are analyzing data with sql because the data sets are too large for Excel or access. In most companies this would be titled as senior accountant or senior financial analyst. Both of those jobs are going to have bachelor degree minimums. You're better off working towards an accounting degree than becoming sql focused right now. 
Throw out all of those options. They're all terrible and prone to failure. Microsoft has already done your work for you: 1. [SQL Server Replication](http://msdn.microsoft.com/en-us/library/ms151198.aspx) 2. [Database Mirroring and Replication](http://msdn.microsoft.com/en-us/library/ms151799.aspx) 3. Please consider [AlwaysOn](http://msdn.microsoft.com/en-us/library/ff877884.aspx) What's your use case? Why are you trying to replicate your database? Redundancy? One for reporting, the other for transactions? What?
Not arguing, simply asking for your thought process... why?
Your third option is the best for normalization. Normalized data usually feels right, but as you point out, require some extra work when querying data. Unless you have a huge amount of data, the performance loss doing JOINs on normalized data isn't going to be noticeable. Side note: I think using a JOIN or an IN query would be more readable than a sub query.
- We have 3 applications that should run on different locations. - Data must be propagated back to a central location on at least a daily basis. - The 3 applications uses different tables, but uses some common tables (Clients, HR ...) that can be changed on the central location only. - The 3 locations must be independent in some way : they must work even if there is no internet or any type of connection. That's quite all that matter for the replication (I think).
Surely a teams table, plus a separate table for each league would be better from a normalisation POV?
The term you are looking for is non SARGable: http://sahlean.wordpress.com/2013/12/09/non-sarg-able-predicates-1/ (reference link: https://www.google.ca/search?q=sql+server+non+sarg&amp;oq=sql+server+non+sarg)
What you actually want isn't replication or AlwaysOn. You'll probably want to do batch jobs with [SSIS](http://technet.microsoft.com/en-us/library/ms141026.aspx) (free w/ SQL Server), and use [Master Data Services](http://technet.microsoft.com/en-us/sqlserver/ff943581.aspx) for your common tables. These are the types of systems I build as a consultant, so if you want any in depth help, feel free to PM me. You have a lot of research ahead of you, though.
Three tables: 1. competitions 2. teams 3. team_competitions #3 is your man-in-the-middle, containing FK links to both competitions and teams. Ergo you can have a one to many relationship back and forth. For reporting, create a view that your report developers can use which joins these together and de-normalizes the data.
An Example: SELECT team.name AS team_name , competition.name AS competition_name FROM team INNER JOIN team_competition TC ON TC.team_id = team.id INNER JOIN competition ON competition.id = TC.competition_id This presumes your team_comepetition table looks like: CREATE TABLE team_competition ( team_id int NOT NULL , competition_id NOT NULL , CONSTRAINT PK_team_competition PRIMARY KEY (team_id, competition_id) , CONSTRAINT FK_team FOREIGN KEY (team_id) REFERENCES team (id) , CONSTRAINT FK_competition FOREIGN KEY (competition_id) REFERENCES competition (id)) (this is Sql Server syntax by the way) This is a completely normalized schema and will yield the best performance you can get (with appropriate indexes that I didn't include).
arrghhhh. Any ideas as to what is causing it to not work?
Revert to Mavericks? Consider this a lesson in what to do and not do with beta operating systems.
I have what you did rewritten and running, but the subquery has been removed and there is no join on Y (Table5). I've been told by our IT/NB divisions that the subquery is necessary until next year to calculate that value.
Glad to be of help.
Sorry, but you wont find a new job in SQL by just learning some basic SQL with analytics thrown in. Employers want either a background in SQL, meaning you are commercially experienced, or someone with an education in SQL to some degree. At the moment you have neither of these things, and after learning the basics of SQL ... you will still have neither of these things. I honestly don't mean to put you down/off the idea, because if you start down this path and you enjoy it, you will probably come to love it, it all depends how serious you are about it. If you are serious about it, then you have two options. 1. Take some official courses on the subject area, the kind of courses I'm referring to are paid for courses, so you wont be able to just do some self-learning and snag a job. I mean in theory yes, but realistically, it's not what employers are looking for. Courses would be MS certified, like https://www.microsoft.com/learning/en-gb/sql-certification.aspx , or even non-MS certified but still fairly well respected, like maybe Pluralsight's, http://www.pluralsight.com/training/Courses . 2. If you would rather not pay for courses with no guaranteed future (this is how I'd go about it in your shoes), then I would suggest you do some self learning (try places like http://www.sqlservercentral.com/stairway/ ), and look for a job in a non-sql position but with a company willing to train you up into a sql position. If you have no educational background in SQL then the best route is to get experience through a company willing to put you on track. Either way, you will be looking at a junior position, when comparing education vs experience, one is not necessarily better than the other, its more about what you can do, expect practical tests in an interview to show off your skillset.
So DBMail is working for other users, correct? The user in question is a member of the public and databasemailuserrole in MSDB? Can you paste the exact error?
Exactly what i was going to say, need the exact error, as the permission might be on the profile.
[CTE](http://technet.microsoft.com/en-us/library/ms190766%28v=sql.105%29.aspx)s. Really useful for breaking up complex queries plus they let you write recursive queries. They frequently perform a lot better than a single giant query too.
Windowing functions.
Learn to love temp tables. Windowing functions.
* Search arguments (SARGs) and why they are important * Window functions * Query plans and optimization, indexing * A thorough understanding of temporal data types (ie datetime) and the functions that act on them. For example, why DATEDIFF(year, birthday, CURRENT_TIMESTAMP) is likely to give you off-by-one errors when calculating age, but DATEDIFF(day, birthday, CURRENT_TIMESTAMP) / 365.25 will not * Basic DBA tasks such as backups and permissions and how to set up a test/dev environment so you can experiment and learn and not ruin everything forever * The laws and regulations governing your location and industry relating to data protection and privacy * How to run Profiler to get a good overview of your data * Views, functions, and stored procedures and why having consistent, centralized logic will save you a world of headache when 30 reports all need to change based on the way a particular metric or business logic is defined 
boot camp?
OVER PARTITION BY, MERGE upsert, PIVOT, DMVs, nesting "and"s and "or"s for multiple aspect optional variable searches... Ranking searches (ex: search via tags and having the one with the most matching tags come up first) COALESCE, ISNULL, ORDER... they seem so simple, yet you can create very complex analysis queries using these. Master them. Self joins, date correlation optimization, indexed views, indexes with included columns. These are rarely used, most likely because few people understand them. Get a good understanding of these and you'll be a SQL god.
Don't know how I missed that especially when referencing Y.canceled in the case statement for the sum. I would just left outer join it. I also removed the nolock on L table that I had left in. DECLARE @StartDate datetime = '2012-01-01' DECLARE @EndDate datetime = '2012-04-01' SELECT DATEPART(mm,L.createddate) AS MM, DATEPART(yy,L.createddate) AS YY, isnull(L.source_code__c,'None') AS SC, isnull(L.campaign_code__c,'None') AS CC, isnull(L.responsechannel__c,'None') AS RC, COUNT(DISTINCT L.id) AS IDs, ISNULL(SUM(CASE WHEN Y.cancelled = 'N' THEN Z.subcount ELSE 0 END),0) AS Submitted ISNULL(SUM(Z.setcount),0) AS Settled, ISNULL(SUM(Z.setproduction),0) AS Production FROM [dbo].[TABLE1] L LEFT JOIN [dbo].[TABLE2] A ON L.convertedaccountid = A.id LEFT JOIN [dbo].[TABLE3] O ON A.id = O.accountid LEFT JOIN [dbo].[TABLE4] Z ON O.policy_number__c = Z.policyno LEFT JOIN [dbo].[TABLE5] Y ON Z.policyno = Y.policyno WHERE L.createddate BETWEEN @StartDate AND @EndDate AND (L.firstname &lt;&gt; 'TBD' OR L.firstname &lt;&gt; 'XXX' OR L.firstname IS NOT NULL) AND EXISTS ( SELECT NULL from [dbo].[TABLE9] WHERE recordtype NOT IN ('Partial','Test','Duplicate') AND leadroute &lt;&gt; 'PARENT') ) GROUP BY datepart(mm,L.createddate), datepart(yy,L.createddate), L.source_code__c, L.campaign_code__c, L.responsechannel__c ORDER BY datepart(yy,L.createddate), datepart(mm,L.createddate), L.source_code__c, L.campaign_code__c, L.responsechannel__c 
As most people have said, learn window functions inside and out. I'm pretty sure CTEs are just glorified subqueries that don't add any real additional functionality other than being able to clean up your queries (correct me if I'm wrong here) but window functions make what used to be super complex and bad performing into something quick and easy to do.
You are going to have to start at the bottom as a jr data analyst. If you have the passion for SQL, it can be an awesome journey. SQL reinvigorated my love for development. It bridged so many gaps in my knowledge but I went further with it. It consumed me. The thing about SQL is, not everyone has that love for it. If you have a passion for it you will rise to the top quickly.
Excellent. Thanks for the link. http://stackoverflow.com/questions/799584/what-makes-a-sql-statement-sargable Not being a SQL server guy this helped me out too (found using your reference link)
This is basically how I found my current position working with SQL Server. I started off straight out of college as an IT Auditor and was able to become a jack of all trades, doing work with SQL, C#, etc. This I was able to translate to work experience on my resume and get a data analyst position which entailed heavy SQL Server and SSIS usage (though nothing too complex outside of never having used SSIS much). I was very fortunate that my boss was looking for people that didn't necessarily have the experience, but the right problem solving mindset and basic skills. I believe his view point is that you can always learn a new tool/language/etc but changing how you analyze problems and solutions is more difficult. It is why some people make excellent engineers but terrible sales people and vice versa (yes I realize some people are good at both). Edit: I graduated with a BS in Computer Information Systems which, at my college, is a business degree.
It's probably just the particular use cases I've personally run into where I've taken some monster of a query and broken it up with CTEs, but in that experience, I've definitely seen a performance increase pretty often. One of those patterns is using multiple joins with different parameters to the same CTE to replace multiple subqueries. I sometimes see much better plans out of logically equivalent queries when using CTEs vs a crap ton of joins in a single statement too. I don't understand enough about plan optimizer black magic to guess why that might be. 
Agreed, most of the time, your CTE will generate the same execution plan as your original monolithic query.
Other than the recursive functions of CTE's you are correct, they make things more readable aka pretty.
This is really interesting and is similar to my path. Can I ask you to put some timelines around this? Also when you say Junior Developer - is this programming or SQL related?
Looks like you're adding data to the @results table, but not actually selecting it. select databasename, custodian from @results
When we interview people we ask them to describe how you might remove duplicates from a table. Everyone scoffs at the exercise at first however you'd be surprised at how many people struggle with this simple task. If you've never done it, it might be good to figure out how to do it before someone jacks up your data and you have to fix it in minutes.
* Windowing functions. * Common Table Expressions. * Cube, Rollup and Grouping sets. * Pivot and Unpivot * XML DB (XQuery, XML Docs etc.) * Full-text Search * Binary Large Object (BLOB) handling
it's actually standard SQL, not just SQL Server glad to see you did not include an IDENTITY or autonumber column in this many-to-many table -- that's always wrong the PK automatically creates one of your two needed **covering** indexes -- the other one is also a 2-column index, but with the columns in the reverse order than the PK thus, all queries accessing this table can use only an index (no access to the table at all)
separate table for each league? no
This is not optimal. An empty text field is not the same as a null text field and if this is not a small project that's always under your complete control you're not going to be sure that having blank isn't going to create problems down the line. Even if it's just a tiny project for your own edification, empty fields should be null, and change that null to the value you want to appear in the presentation, not in the table.
GREAT SUCCESS. Turns out you need to download the Java SE Development Kit 8 for it to work. Heres the install. Run that and SQL Dev will magically start working. http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html EDIT:changed hardlink.
Got the solution. Check my post.
And table variables.
This is a question my senior production DBA began asking when we were looking for junior DBAs. This is a question we ask in the face-to-face interview so we can watch how they respond. So far out of about 8 interviews nobody has had a good response. Granted these are young guys with only a year or two of experience but I expected better. We don't expect a perfect answer but it usually is a good opportunity to see how logically they think. When I do this I typically load the dups to a temp table, delete the dups in the source and the select distinct from the temp to insert back into the source. 
http://use-the-index-luke.com/ This is a great site to learn some more advanced stuff. That said, use the index and don't use a function on an index column in the WHERE clause if you can solve it another way.
you use row_number() and partition by all the column you need to consider for duplicates. Then delete all rows that does not have row number 1.
Your model reflects your DB, so when you reverse engineer you only have View DDL from the data dictionary, so there is no concept of PKs FKs as views are unmaterialized queries, i.e. a chunk of SQL nicely packaged under an object name. The underlying tables are possibly in another schema or your user login doesn't have privileges to describe the underlying tables and referential integrity. Oracle Data Modeler is doing the correct thing with what it can. What I would do is try to get access to the underlying tables (ask your DBA to grant relevant privileges), reverse engineer them, then do again with the views. Then you might find you have the correct relationship information between views and tables.
Thanks for the response. It makes perfect sense. Once I've got this figured out I gotta start looking into best practices for using subviews to break the large clump of data into readable chunks. Lots of free time at work this week. 
Learn how to use the APPLY operator.
Sorry to piggyback on your post (I didn't want to create another learning topic), but as someone that is applying for some positions out of town, what would be the easiest way to gain at least a basic understanding of SQL. I've gotten my MBA and have a pretty solid resume which should get me the interview, but I don't want to lie in the interview so I would like to at least be familiar with SQL so I can say I'm not in the dark with the software. Thanks guys :)
A while loop would do this, but likely isn't reasonable if your data set gets overly large. Likely the better solution is to generate a table with all calendar dates [like this](http://social.technet.microsoft.com/wiki/contents/articles/22776.t-sql-calendar-table.aspx) and then join to that table like the following: select a.Name, b.Date as Arrival, b.Date as Departure from TravelInfo a join DatesTable b on b.Date between a.Arrival and a.Departure Edit: Removed some wording I had in place regarding the year... It would be part of the date so was likely useless.
You'd generally have the cube process scripted to take place after the tables were loaded. The cubes only need the data in the tables to be present when they're being processed though, after that anyone accessing them is hitting the cube data structures and not the underlying data. If you're just kind of winging it right now, I'd heavily suggest that you take a look at the following courses on Pluralsight: [Integration Services Fundamentals](http://pluralsight.com/training/courses/TableOfContents?courseName=ssis-basic) [Advanced Integration Services](http://pluralsight.com/training/courses/TableOfContents?courseName=ssis-advanced) You'll save yourself a lot of time in the end if you start with a nice templated approach to the ETL, and data warehouse design. (Source: currently re-writing my own ETL and dw using templated approach).
This may actually be irrelevant. Lets focus on one cube and lets state that this cube holds one area of interest at day (date) granularity. You would be 'refreshing' this cube every day so, right after you repop your tables, you would do the same to bring your cube up to date. You would do this by adding new data for the date that just completed. Of course, if you had a different time period, say weekly, you would refresh this cube weekly instead. 
problem here is that the data in providex is not exactly stamped in a way that I can easily determine changes... an order may be entered today, but shipped in 3 months (fashion industry so bookings are quarterly based on seasons) the order will go from an open to closed status but the order date remains 08/27/2014, same goes if a line is added or cancelled from the order in a future date. The ERP in question is a really good system for managing apparel, but its database can use some improving, ProvideX can use MySQL and I've been begging for the devs make the leap.
Not sure about Access but in SQL it would look something like this. SELECT A.ID FROM List A LEFT JOIN List B ON A.ID = B.ID WHERE A.Rate &lt;&gt; B.Rate You may be able to accomplish this in Access. I take it you are using a query builder? Take a look at the SQL it generates and see if you can't attempt something like above.
When writing an UPDATE statement in Oracle 11g or DB2/400... SQL Server has an excellent (but sadly non-standard) [UPDATE FROM](http://msdn.microsoft.com/en-us/library/ms177523.aspx) syntax for using UPDATES with JOINs and/or CTEs, which I love it for.
You could build a range of dates to join to using a recursive CTE as well.
You could use inline queries or use GROUP BY or use CTEs.
You will want to include a.rate and b.rate in the select. No point in just having the company id returned (in the vein of meaningful data).
Sounds more like you hate software and want instead to learn the Microsoft garbage. You'll find no help here for your kind.
Which SQL server? Why do you not say? Are you so clueless that you don't know?
I'm no "expert" by any means but if your SQL supports Group by then: Select Name, Min(Arrival), Max(Departure) From TravelInfo Group By Name
I had to look ProvideX up... So maybe everthing talked about in here happens AFTER your devs export data to MySQL db as you mentioned. If you could end up with a daily export in an rdbms, you could would be in a better spot... process wise, usually the cubes you are referring to are way down in workflow. Another alternative would be to process system information from logs and into db but now things got a lot more complicated. G'luck.
even better is a generic **numbers** table CREATE TABLE numbers ( n INTEGER NOT NULL PRIMARY KEY ); INSERT INTO numbers VALUES (0),(1),(2),(3),(4),(5),(6),...; SELECT t.Name , t.Arrival + INTERVAL n.n DAY AS Arrival , t.Arrival + INTERVAL n.n DAY AS Departure FROM TravelInfo AS t INNER JOIN numbers AS n ON t.Arrival + INTERVAL n.n DAY BETWEEN t.Arrival AND t.Departure the numbers table can be used in many different situations (google for more info) 
This as well and depending on what else you're doing, a numbers table might be useful in many other situations. My company deals more with actual dates so that was my first suggestion. Worth noting in the example above is that INTERVAL is an Oracle/PL-SQL Keyword (Please correct me if I am wrong). You could do this in SQLServer either just adding the number value (in the above example it is n) to the Arrival date (make sure this is a date (or related) data type if you use this method) or using dateadd().
This would not provide what the OP was describing. OP wanted a single row for every day between the arrival and departure from the original data set.
Looks like it works under MySQL (possibly others) but not SQLServer or Postgre. SQLServer reports &gt;Incorrect syntax near '5'.: SELECT '2002-04-14 21:31:01' + INTERVAL 5 SECOND Postgre 9.3.1 reports &gt;ERROR: syntax error at or near "5": SELECT '2002-04-14 21:31:01' + INTERVAL 5 SECOND [SQL Fiddle for testing](http://sqlfiddle.com/#!6/d41d8/21090)
try it with single quotes around the numeric quantity -- it's a string constant in the standard
That wasn't it. Care to link to ~~the standard from which you're seeing it or provide~~ a fiddle example of it working in sqlserver? I did all of these things above, if you're going to say I'm wrong I'd appreciate at least some helpful information that supports what you're saying. Edit: I see the interval data type added in [SQL-92](http://en.wikipedia.org/wiki/SQL-92) but can't find any examples outside of Oracle/PL-SQL. So really, just a fiddle example of it working under sqlserver.
Thanks for posting this question! I love learning new tricks! 
Have them just named userdefined[n] have a few text or date. Then allow the user to "change the name" in the app which essentially just changes any labelling that references it.
I dont know if adding unlimited fields directly by sql is a good idea btw. 
What you are looking for is called an EAV model. It will allow you to store your column labels in one table and the vales in another that. Then you can use a mapping table to define the relationship between them, you can read more about it here... http://en.m.wikipedia.org/wiki/Entity%E2%80%93attribute%E2%80%93value_model
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Entity–attribute–value model**](https://en.wikipedia.org/wiki/Entity%E2%80%93attribute%E2%80%93value%20model): [](#sfw) --- &gt; &gt;__Entity–attribute–value model__ (__EAV__) is a [data model](https://en.wikipedia.org/wiki/Data_model) to describe entities where the number of attributes (properties, parameters) that can be used to describe them is potentially vast, but the number that will actually apply to a given entity is relatively modest. In mathematics, this model is known as a [sparse matrix](https://en.wikipedia.org/wiki/Sparse_matrix). EAV is also known as __object–attribute–value model__, __vertical database model__ and __open schema__. &gt; --- ^Interesting: [^InfinityDB](https://en.wikipedia.org/wiki/InfinityDB) ^| [^Resource ^Description ^Framework](https://en.wikipedia.org/wiki/Resource_Description_Framework) ^| [^Attribute-value ^system](https://en.wikipedia.org/wiki/Attribute-value_system) ^| [^Semantic ^Web](https://en.wikipedia.org/wiki/Semantic_Web) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+ck4dgmd) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+ck4dgmd)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
I do. It is certainly is not. 
Why not just start by practicing ways to do inserts?
Yes, i have practiced INSERT commands for some time. It's tiring when i'm working with much more tables having foreign key references.
How do they access this table? Is it through a gui?
Have to use postgresql.
This looks solid to me - will try that - Thank you
Postgres has hstore, and you can store key value pairs that are pretty easy to query. EAV is kind of a bad idea in any situation I think.
That is sometimes the painstaking part of playing with data. In a real world example if I am testing with a blank schema I would insert rows in tables as needed. 
my code always has proper indenting and wrapping... a sample (without validation) is below: CREATE VIEW vew_AllStaff AS /* 20140830 kb54 ... this is a comment describing external dependencies, change history etc... */ SELECT tblStaff.StaffID,tblStaff.Field1,tblStaff.Field2,tblStaff.Field3,tblStaff.Field4, tblAssignments.Field1,tblAssignments.Field2, LastName + ', ' + FirstName as CalcName FROM tblStaff LEFT OUTER JOIN tblAssignments ON tblStaff.StaffID=tblAssignments.StaffID WHERE tblAssignment.Active=1 You can develop your own style that makes sense to you... but the key is to be consistent. I assure you that your style will develop over time. Also... whenever possible follow consistent naming conventions (tbl* for tables; vew* for views etc). And lastly... comment that bitch!
I'd recommend checking out Joe Celko's book *SQL Programming Style*. You might disagree with his particular recommendations, but he gives a reasonable starting point and (in places) attempts to provide some evidence why his suggestions are best. I don't agree with a lot of his arguments, but I found it useful in a team environment to have a copy in a team environment where you could refer to when you wanted to settle disputes. Random rules/observations from my own experience: - Standard headers for all Stored Procedures, Triggers, etc - giving the author, date, description, etc. Views as well, although in SQL Server you have to put the comment within the statement for it to be retained. - If it's a database I'm designing I try to maintain some sort of consistency for naming columns - particular if they're keys just to reduce the amount you need to rely on your memory - especially if intellisense is not available 100% of the time. I use schemas quite heavily to segregate objects in as a sort of namespace. - Where I'm creating objects in an existing database (which is probably true 90% of the time) I try to keep consistent with the database I'm working on. Sometimes this is maddening because the database will have (imo) ridiculous conventions but I've found from experience that it's much better in the long run just to go along with it. So if the database records user names in a column called "PRIMARY_USER_ID" then that's what I'll do too - all caps as well, as much as it grates. Of course, some systems aren't even internally consistent and I have worked with business critical databases which switch conventions *in the same table* for column names. In those cases, it's a choice based on which is the most common and least stupid. - Whitespace is obviously a personal preference, but as with any other programming - keep consistent (although again, I am inconsistently consistent - I will try to stay consistent with the environment / system I'm working. This doesn't always work, obviously). - On whitespace / indentation - automatic SQL formatters never quite layout code 100% right, but they're an absolute godsend if you're working with code written by someone else or generated by some system/script. I use SQLInform (one off licence fee) for those purposes - useful to me because I work with a variety of SQL variants. Auto generated code in particular is often utterly unreadable before being reformatted (and even then, often will need cleaning up to remove ten thousand excess parentheses). - Unless there's a good reason not to, I like to have an individual stored procedure for each report / application dataset (rather than using a single stored procedure for lots of things). That allows flexibility later on - if someone wants to adjust the contents of a report then in most cases I can just adjust the procedure without worrying about anything else breaking something else. If there's a piece of business logic which is commonly used (e.g. some sort of CASE statement) then I prefer to put that in a view, and refer to that in a stored procedure. On databases I've created I've aimed for the application to not touch any table directly. Again, this makes schema changes easier to manage and simplifies security a bit. - I agree with Joe Celko about avoiding redundant prefixes - there's no need to prefix tables with "tbl_" - for the same sort of reasons that the Hungarian naming convention isn't recommended for programming in Java, C or whatever. One thing I've never managed to do is keep (non-production / adhoc) SQL scripts properly managed. I write a *lot* of SQL and none of the approaches I've used for managing misc files has worked to my satisfaction. Right now I use the SSMS Tools pack to store my SQL history. That allows me to find the scrap of code I end up needing later - most of the time.
that actually wouldnt fly in my shop, not at all. CREATE VIEW vew_AllStaff AS /* 20140830 kb54 ... this is a comment describing external dependencies, change history etc... */ SELECT sta.StaffID ,sta.Field1 ,sta.Field2 ,sta.Field3 ,sta.Field4 ,ass.Field1 ,ass.Field2 ,CalcName = LastName + ', ' + FirstName FROM tblStaff sta LEFT OUTER JOIN tblAssignments ass ON sta.StaffID=ass.StaffID WHERE ass.Active=1
If they both have the exact same columns you could Union them. SELECT a, b, c FROM ... UNION ALL SELECT a, b, c FROM... After doing this only read the first row. If the first select is empty the first row will be the result from the second query. If the first select was not empty you can ignore the second row returned from the second query. EDIT: However I think doing two separate queries is probably better. Why do you want to combine them?
The reason I want to combine them is not to have a redundant work (don't really like doing copy/ paste into the script if i can make a bigger script and run it all at once and I like to automate as much as possible of my work). Plus it keeps my work a bit interesting, since i am rather a novice. And it with union all is doesn't really work if I have a result from the first script it might get a bit confusing.
Sorry for not being very clear. I will try to reexplain. I have 2 scripts (Scrip A and Scrip B) I want to combine them so the result is like this: If Script A doesn't retrieve any results run Scrip B If Script A retrieves results run only scrip A and ignore Script B. At the moment I could not find any kind of help on the web (except for union all / and case) which do not really work for my particular problem.
Point taken, and it's a good one. I understand what you're saying but it is a fairly small project so it's not a big deal.
Unfortunately they're the type of host that expects you to simply trust that they know what they're doing and won't discuss details of their ops. This question is partially just to help my understanding of the mechanics f the situation. If at some point after a transaction fails due to running out of space and a later transaction goes through because some space freed up at some point, is there any way for the instance to know there are transactions missing between the 2? How could it if there was no room on disk to record this info? Is there a mechanic that prohibits this future transactions after the first failure to keep data integrity? I mean, if a write failed, I wouldn't want another one to go through knowing the failed write could be needed by the executed write.
Still not entirely sure what you're setup looks like, but maybe do a select * from ScriptA into a temp table. If @@rowcount = 0 then run ScriptB else select * from #temp
You can try something like this (this was done using T-SQL): IF OBJECT_ID('tempdb.dbo.#test_table', 'U') IS NOT NULL DROP TABLE #test_table; IF OBJECT_ID('tempdb.dbo.#select_output', 'U') IS NOT NULL DROP TABLE #select_output; CREATE TABLE #test_table ( a INT, b VARCHAR(30), c DATETIME ) CREATE TABLE #select_output ( query VARCHAR(10), a INT, b VARCHAR(30), c DATETIME ) INSERT INTO #test_table VALUES(1, 'testrow1', GETDATE()); INSERT INTO #select_output SELECT 'SCRIPT A', a, b, c FROM #test_table WHERE a = 2 IF @@ROWCOUNT = 0 INSERT INTO #select_output SELECT 'SCRIPT B', a, b, c FROM #test_table WHERE a = 1 SELECT * FROM #select_output Edit: Formatting
then add a Limit 1 at the end. You will only ever get 1 result
Thanks for this. I will try to give it a shot tomorrow (not very good with T-SQL) and see how it works.
You could make a stored procedure that does a count() of the return. Then an if/then statement to run the next query if it is zero: create procedure spGimmeResults as begin declare @total int SELECT @total = Count(id) FROM tblSampleData WHERE a = b if @total = 0 begin SELECT id FROM tblSampleData WHERE a = c end else SELECT * FROM tblSampleData WHERE a = b end So it pretty much starts out by finding out if there is a result for the first query (a=b). Then it checks if it's 0. If it is, it will run the second query (a=c). If there was a result, then it just does the original select statement as the last step (select * where a=b)
Ahh ok, and would these be returning similar data from the same table?
There are 2 tables (mainly) from which the data is retrieved. One would be an "archive", and the second one would be the "active table" to get an understanding. and I have 2 scripts for this that I want to unify. 
 Select 1 as order, a, b, c FROM ... UNION ALL Select 2 as order, a, b, c FROM ... ORDER BY order LIMIT 1 something along these lines could work. If not. I think you are asking too much from sql itself. Write a function that executes the queries how you want and use that function everywhere you need it. Sometimes its bad to try and do too much with a single query.
This would have been my choice as well problem with this is that I get only one result (that being a location) so I can't really order by a name of a city.
Going to try this tomorrow and see how it works. Thanks again.
You could do it this way: SELECT COALESCE( (SELECT Field FROM query1 WHERE stuff = 'True'), (SELECT Field FROM query2 WHERE stuff = 'Different') ) --At least that should work. haven't tried to use subqueries in Coalesce.
I'm pretty sure UNION ALL will preserve the correct order. However, I added a hacky solution to fix it even if it does not. Notice the "1 as order" SELECT 1 as order, location FROM ... UNION ALL SELECT 2 as order, location FROM ... ORDER BY order ASC LIMIT 1 You might have to make the union an inner query for limit/order by to work. However you are basically hardcoding in the order you want and then ordering by that. You should be able to modify your queries to work this way.
 Declare @Result varchar = ( **Enter Select query A here** ) if @Result is NULL begin set @result = ( **Enter Select query B here** ) end Select @Result 
I use SQL Server Express 2012 on my personal laptop. It works well enough. what setup errors are you running into?
I've downloaded the Express package and I see in my start menu: SQL Server Import and Export Data and Configuration Tools. I choose the Import/Export and it asks me to choose a data source then I am lost as to how to import a data set. I've downloaded a training set that is on my desktop but every option I select brings me to "The connectionString property has not been initialized (system.data)" ....frustrating...
you should have SQL Server Management Studio. I suspect you might have missed a couple steps while setting it up. here's an instructional video. http://youtu.be/vng0P8Gfx2g
thanks man, i'll give it a look!
SQLzoo.net has pre-made practice datasets in the cloud that you can practice on. Might be the easier way to go. Good luck at your interview!
You have to insert things in dependency order. e.g If you have a driver table and a journey table with a FK to a driver, you have to populate any driver that you want to reference in a journey first. There are ways around it, like deferred constraints, where referential integrity is checked at commital time, but that is generally used for special cases, not to circumvent developer laziness.
Yeah, thankfully it looks like we do that here at least some of the time.
I agree that using a date table would be the easiest.
xkcd sucks
 create proc_embed1 @param1 int as begin if @param = 1 begin select 1 end end else select 2 end create proc_embed2 @param2 int as begin exec proc_embed1 @param2 end This is a really simple version of what I think you're referring to. Yes, you can have a stored procedure call another stored procedure, and use the results of the stored procedure as the parameter. It totally works. Its not even necessarily bad design, as long as you document everything really well, but be mindful of collisions and timing issues.
I'm real sorry considering you took the time to reply but I don't understand. Do you have an example you could link me to?
You can use the IN operator: SELECT [...] FROM ASSET_DATA_10 WHERE USER_67.FULL_NAME IN ( SELECT USER.FULL_NAME as FULL_NAME FROM USER WHERE ((EMAIL like '%Reddit.com%')) ) UNION [...]
NTEXT supports Unicode character encoding, anytime you see an N in front of a data type, Unicode support is what it's indicating. Also, NTEXT is deprecated (it still works but will be eliminated at some point), NVARCHAR(MAX) is the way to go.
&gt;I usually use nchar() for strings but my prof. says it's not good cause of the limited string lenght. Yes, `char()`, `varchar()`, `nchar()` and `nvarchar()` have limitations (unless you're using `max` for the length, then the limitation is identical to `text`/`ntext`), but **that's not a bad thing**. Use the appropriate data type for the job. If you're storing a username or an email address (`n`)`varchar()` **is** good. If your professor is flat-out telling you that these data types are "not good" without proper context, he's not doing you any favors. The difference between `ntext` and `text` is that `ntext` stores Unicode data, while `text` does not. But you should also be aware that [these data types are deprecated](http://msdn.microsoft.com/en-us/library/ms187993.aspx) so you should move away from them to prepare yourself for the future. FYI, your question isn't really about SSMS, but SQL Server itself. SSMS is just one means by which one can access, manage &amp; query SQL Server.
In all cases, n* means the encoding type is Unicode, and plain means the encoding is whatever the database is sort to. They're different in other ways, too, though. Text and ntext have (effectively) unlimited length, but are stored separately from the main table as a specific data set. This adds certain limitations, like not being able to use them in distincts &amp; group bys. Char(50) and nchar (50) can each store up to 50 characters, but Nchar is space padded and nvarchar only uses the amount of storage for the length of strings required. Varchar(max) and nvarchar(max) will store strings use the amount required, and if the string is &gt; 4 GB switch to the separate apt rage type text uses. Varchar(max) or nvarchar(max) is great for when your data has no enforced length limit.
An additional note...it's probably not important, but ntext takes up approximately twice as much storage space as text, since each character takes up 16 bits instead of 8.
Thank you. That worked perfectly. The references I found for IN on Google searching initially all were working on the same table so I didn't know if what I wanted to do was possible.
You could... Make a table to show the relationship between tables? Or a spreadsheet. Can you give a little more context? How a program uses the tables and whether they relate to each other or not is something the db can very well be unaware of.
Junction Tables, aka Bridge Tables, quite often consist of only two columns, each containing the Primary Key of the tables they are bridging. If you see a table like that, there's a very good possibility it's a junction table.
ok. thanks for the answer.
Glad to hear it. After I re-read your question, wasn't sure if my answer was what you were looking for. Another thing to note about bridge table is that they are used to cover a many-to-many type of relationship. For example, consider people and phones. A person can have many phone numbers where you can reach them - a home phone, a work phone, a personal cell, a business cell, etc. But you can also reach many people thru the same same phone number - they may all have the same work phone, for example, or all the members of a family can be reached thru the same home phone. There's a lot more on the net re: this topic. Cheers!
We made a cheat sheet for SQL date functions that you can check out [here](http://blog.hiquery.com/2014/04/13/sql-cheat-sheet-mysql-date-functions/).
Change your &amp;lt; for a &gt;=. This will return only records that are newer or equal to 365 days ago from today. Edit: If this is for invoicing, look at flattening the date, if somebody ordered a year ago at 3pm bit you run the report at 2pm they won't return. For 2008(12?) and newer and T2.docDate &lt; DateAdd(dd, -365, cast(getdate() as date)) Older and T2.docDate &lt; DateAdd(dd, -365, cast(getdate() as varchar(12))) 
If this is an ETL, can we assume the amount of space used during the ETL is similar for each run? Shrinking the log file is likely futile since it will simply grow again during the next run. If any of the applications I were to manage had this type of outage, I would likely have lots of questions to answer to my management as this would all be considered downtime until I resolve it. The fact that you have this problem on a hosted platform indicates potential issues with the way the hosted company is monitoring and managing your SQL server. 
If you want all customers regardless of time, but only the sum for the last 365 days On phone so work with me here. Select Customer, Sum(case when datediff(dd,start date,end date) between 0 and 365 then value else 0 From my table Group by customer 
Sorry, its a nightmare posting on my mobile SELECT T0.cardcode AS 'Code' ,sum(Case when Datediff(DD,T2.docDate,GETDATE()) BETWEEN 0 AND 365 THEN T2.DocTotal ELSE 0 END) AS 'Total' FROM dbo.OCRD T0 LEFT JOIN dbo.OINV T2 ON T0.CardCode = T2.CardCode WHERE T0.cardType = 'C GROUP BY T0.cardcode 
Thanks, totally worked. Is that's a more efficient way than a subquery? 
Its not a subquery, and will barely impact performance at all In this case (excuse the pun), CASE just evaluates each row, and instead of SUM(field) its using SUM(generated value) 
What he said, why would you create a table without an index, that's day 1. Just wait until you have a heavy load database and need to tune it. Good design is essential. 
I'm assuming these were created in MySQL. I've inherited this so bare with me :). Would you mind doing an ELI5 of how I'd go about doing an index for the table?
Subscribed immediately through pocket casts, thanks!
Sure. Let's start with what an index is. An index (also called a key) works like the index in a book. That is, it helps the database find records faster. On top of that, one can require that an index be *unique*. That is, no other record can have that same value (or set of values for a mult-column index). A typical use case for those might be on a table of people (e.g. customers, members, patients, whatever), you might assign a unique key to the SSN field to ensure that each person only has one entry. Now, there is a special kind of unique key called a *primary key* (PK for short). A PK serves as the primary means of addressing a particular record in the table. There are two types of PKs: 1. **Natural keys** - A natural key is one that is intrinsic to the data, meaning that there is a column (or group of columns) in your table that should be unique to each record, and we're going to use that to refer to the record. Going back to our people table, using the SSN field as a primary key (please, please, please never do this) would be a natural key. 2. **Surrogate keys** - Sometimes there's no good (or convenient) natural key, so the solution becomes to add a new column to the table with some arbitrary value that is unique to each record. In our people table, you might assing each customer a unique number, `customer_id`, so you don't have to refer to them by their social security numbers. Some people prefer natural keys, and some prefer always (or almost always) using a surrogate. Personally, I take it case-by-case. However, every table needs a primary key so that you have a way of addressing individual records. In your case, because I know nothing about your data, we're going to add a surrogate key. It's been a few years since I've been forced to use MySQL (and I've mostly repressed those memories), but I think this should do you: ALTER TABLE tbl_name ADD COLUMN row_id INT(10) UNSIGNED NOT NULL PRIMARY KEY AUTO_INCREMENT; Your table will now have a new column, `row_id` (you might want to make that something more descriptive, like `customer_id` or whatever), that auto increments between each row and has uniqueness forced on it by the PK requirement. I would also recommend seeing if you can decide on a natural unique key and adding that to preserve data integrity. I hope that help.
OP is using MS SQL Server, so MySQL cheat sheets are only going to confuse matters.
How much of your homework do you want people here to do for you? What have you done so far, and where are you getting stuck with it?
Please post the code and I'll be glad to help analyze and do what I can
&gt; The original standard declared that the official pronunciation for "SQL" was an initialism: /ˈɛs kjuː ˈɛl/ ("es queue el"). http://en.wikipedia.org/wiki/SQL#Standardization
Looking at the 1st query you already completed, it appears that you didn't follow explicit instructions of only selecting Name and Gender. This might be a better answer: SELECT p.Name,g.Description FROM People p INNER JOIN Gender g ON p.Gender=g.Gender ORDER BY p.Height
**Completed Query #2** "The number of athletes who have participated in at least one tournament." This question isn't asking for a result set, but a simple aggregate of how many athletes have have participated in at least one tournament. Query should maybe start with... SELECT COUNT(p.Id)
I and most other developers and businesses I've ever worked with say sequel, but you do hear S.Q.L. occasionally. I think the former is easier to say too.
"S Q L", except in the context of "SQL Server", for who knows what reason.
Whichever one you use, you are wrong. That seems to be the general feeling I get.
&gt; The ID and name of all athletes who currently hold more than one record, along with the number of records they hold. This one is tricky since I don't have the sample database, so I suggest testing this thoroughly. SELECT p.Id,p.Name,COUNT(s.Record) AS RecordsHeld FROM People p INNER JOIN Results r ON p.Id = r.PeopleId INNER JOIN Sports s ON r.SportId=s.Id WHERE r.Result IN (s.Record) GROUP BY p.Id HAVING COUNT(*) &gt; 1 
Ugh. "Sequel" makes me cringe.
Interchangeable as far as i'm concerned. Don't really get the people offended by the latter term.
Not seeing the difference
Wtf, where did you hear that? It would be "ese cu ele".
Well, one problem is that IBM sells a product called SEQUEL: http://www-304.ibm.com/partnerworld/gsd/solutiondetails.do?solution=14167&amp;expand=true
The sample database helped, thanks. The first query I wrote was returning athletes who held the best record AND had more than 1 result. I overlooked this when I copy and pasted my previous answer. All you have to do is remove the join to the Sports table and the WHERE clause. Much better results! SELECT p.Id,p.Name,COUNT(r.Result) AS RecordsHeld FROM People p INNER JOIN Results r ON p.Id = r.PeopleId GROUP BY p.Id,p.Name HAVING COUNT(*) &gt; 1
It is possible. Depending on how deep you want to go you will eventually start implementing an [approximate string matching](http://en.wikipedia.org/wiki/Approximate_string_matching) method like [n-gram](http://en.wikipedia.org/wiki/N-gram) analysis.
&gt; The ID and name of each athlete that has at least two results in the high jump, their best result, along with the difference between the best and worst results; the second-to-last column should be named “rest” and the last column should be named “range”. If you can format the result to show only two digits after the decimal point, all the better. Was able to tackle this one now having the sample database. I don't understand why the last two column names were requested as rest and range, but whatever... SELECT p.Id,p.Name , MAX(r.result) AS rest -- Athlete best score , CAST(MAX(r.result)-MIN(r.result) AS DECIMAL(18,2)) AS [range] -- High score minus low score FROM People p INNER JOIN Results r ON p.ID=r.peopleID WHERE r.sportID=0 GROUP BY p.Id,p.Name HAVING COUNT(*) &gt; 1
My librarian friend pronounces it as "Squeal".
Thanks! I just hope you are making a legitimate effort to understand this. Otherwise, avoid a career in databases because this is pretty basic stuff.
Since Q is pronounced as "koo" (as in cuckoo) in Polish, I've been pronouncing it as "Skool" (school). I've been frowned upon a lot.
Skimming over this, it looks like you could easily replace most of this with a [recursive UDF](http://www.shubho.net/2011/02/recursive-function-example-sql-server.html) with a parent/child model or just passing your recursive level as an Integer and incrementing it. Frankly, this looks awful and has no ability to adjust for less or more levels.
Thanks! I knew it was inefficient as I was writing it, but I needed to get it to run before I could try and make it more efficient Edit: its going to take me some time to understand this function. heh
ESS-QUE-ELL
Sorry I've been away. Just on your #3 don't do a select * if it's homework since they're only asking for name and gender, not all. I'll look through everyone else's comment to see what else is missing.
Inner join the table on unique id and datediff the two create dates smaller then your accepted difference. This will give you only the rows where the create dates are too close.
I call it Squirrel.
My teacher at my introductory database course talked about this in class today. He said that 'if you are an older person and worked at IBM when sql was being invented, then you can say 'sequel' (because it was called 'sequel' back then by them). Otherwise it's not cool.' 
From a Cuban American who worked for a banking software company based in Miami.
D'oh! We're developing a MS SQL server date functions cheat sheet as well. 
And XUL is zuul.
can't inner join on unique id, that would be unique, thus nothing would join. You do get points for using datediff though. 
Es-K-El, as opposed to Es-Queue-El?
Hipster... I can't help but call it Sequel, just faster to say that instead of S.Q.L.
His Spanish was perfectly fine; that's why I wrote "mispronounce". He was just being a cheeky bastard. He was found dead in his hotel room a few years back. Probably heart attack. He was a cool guy. 
Kal-el?
This did it: select student_id, semester_number, min(create_timestamp), max(create_timestamp), datediff(day, min(create_timestamp), max(create_timestamp)) from table group by student_id, semester having datediff(day, min(create_timestamp), max(create_timestamp)) &lt;30
Well, that escalated quickly... I'm sorry.
Sqwell
Correct. I write SQL for Sequel Servee
And the very next sentence... &gt; Regardless, many English-speaking database professionals (including Donald Chamberlin himself) use the acronym-like pronunciation of /ˈsiːkwəl/ ("sequel"), mirroring the language's pre-release development name of "SEQUEL". Also, the actual initial version of SQL was actually called SEQUEL (Structured English Query Language), but was changed to SQL because of trademark violations.
**Seek**wull
Personally: * ess-queue-ell * see-quell-server * em-ess-ess-queue-ell * tee-ess-queue-ell * an-see-ess-queue-ell * my-ess-queue-ell * post-gres-queue-ell * no-ess-queue-ell Basically, I say the letters unless it's SQL Server. Every Microsoft SQL person I've ever encountered, including their own employees, say "sequel" when referring to SQL Server.
This is what one of my professors said as well.
Why do you want to be competent in sql? What's your goal? Administer a server? Create an application? Aiming to put SQL on a resume? You can take a look at [triggers](http://msdn.microsoft.com/en-us/library/ms189799.aspx) and [stored procedures](http://msdn.microsoft.com/en-us/library/ms187926.aspx). Both are pretty fun and useful.
I call it the database thingy
As of right now, it is part of my job. Currently, I'm just using it for data retrieval for trend analysis. A lot of the business analysts and systems analysts above me are consistently using SQL, so I figure I need to improve my capabilities to reach there level. As of right now, working in the back office in financial services...
If you are only looking for SQL for querying, here are some things/keywords you should know about IMO. You may already know about some/all of this stuff and the list is by no means complete or in any order of importance. - different types of JOINS (INNER, NATURAL, OUTER) - set operations (UNION, INTERSECT, MINUS) - handling of NULL values - the IN and the EXISTS operator - nested queries / subqueries - recursive queries
It's faster to say Sequel, 33% query improvement.
Hi. Sorry for the delayed response. It's been a couple of days. And I wrote that code on my phone while getting ready for bed. So I just wanted to clarify that yes, you can use a stored procedure to obtain a value, then pass that value as a parameter to *another* stored procedure, within the stored procedure. In other words, just as you can say "SELECT" or "UPDATE" or "DELETE" inside a stored procedure ... you can also say "EXEC" inside of a stored procedure. So say you have one stored procedure that documents your poop type. Soft, firm, or both. You have another stored procedure that looks for the most recent food you ate, and executes the stored procedure to update your poop type. CREATE TABLE Foods (FoodStuff CHAR(25), FoodDate DATE) GO; CREATE TABLE PoopType (PoopType CHAR(4), PoopDate DATE) GO; CREATE Procedure log_PoopType @PoopType CHAR(4) AS BEGIN INSERT INTO PoopType VALUES (@PoopType, GetDate() ) END GO; CREATE PROCEDURE inspect_poop AS BEGIN DECLARE @MaxFoodDate DATE SET @MaxFoodDate = ( SELECT MAX(FoodDate) FROM Foods ) DECLARE @NextPoop CHAR(25) SET @NextPoop = CASE WHEN ( SELECT FoodStuff FROM Foods WHERE FoodDate = @MaxFoodDate) = 'Nasty Big Mac' THEN 'Soft' WHEN ( SELECT FoodStuff FROM Foods WHERE FoodDate = @MaxFoodDate) = 'Bran Muffin' THEN 'Firm' ELSE 'Both' END EXEC log_PoopType @NextPoop END GO; So we start by creating two tables. One to track the food you ate, and one to track the poop you poop. Next we create a stored procedure that simply updates the PoopType table, with the PoopType (soft, firm, or both), and the PoopDate. It's a pretty simple stored procedure. Now. After that, we have a table that inspects the most recent food you ate. It creates a temp variable called @NextPoop, that inspects the most recent food you ate and determines what your next poop will be like. If the most recent food you ate was a "Nasty Big Mac", then your @NextPoop value will be "Soft". If the most recent food you ate was was a "Bran Muffin", then your @NextPoop value will be "Firm". Otherwise, it's both. The last statement in the "inspect_poop" stored procedure, is to actually execute the log_PoopType stored procedure, and update it with our next poop. This is how you can pass the results from one query to the next. It's a totally valid and common thing to do. Also, my wife is kind of happy that I used poop as a tutorial.
Paraphrasing Markus Winand (Indexing Guru) from a previous reddit post. *The two most important additions to SQL in the last 20 years were window-functions (the OVER-clause) and Common Table Expressions (CTEs—the WITH clause). The first being the way more important one. If you do an SQL tutorial today without covering window functions, then you are doing it wrong* I recommend reading up about these. Referred Post with other some good suggestions for taking SQL learning to the next level. http://www.reddit.com/r/SQL/comments/2bkdl0/to_all_learning_sql_what_sources_are_you_using/ 
Yes, hence I'd summarize: S-Q-L: right. SEQUEL: wrong :) In my reception, SEQUEL is mostly used by SQL Server users. Which raises another question which I've never found the answer to: I guess Microsoft has the right to declare how their product is to be pronounced. So, I guess SEQUEL Server is indeed SEQUEL Server.
Yes, sorry I meant fulfilment_detail for the first one.
we'd have to see some specifics - column headings at least, if not actual data - but it sounds like the fulfilment_detail table probably does contain enough information that you could add a primary key. That'll also speed up DB access quite a lot, since it won't have to scan the entire table any time someone queries it.
SEQUEL is [something else](http://en.wikipedia.org/wiki/SQL#History), call it SQL.
#####&amp;#009; ######&amp;#009; ####&amp;#009; Section 1. [**History**](https://en.wikipedia.org/wiki/SQL#History) of article [**SQL**](https://en.wikipedia.org/wiki/SQL): [](#sfw) --- &gt;SQL was initially developed at [IBM](https://en.wikipedia.org/wiki/IBM) by [Donald D. Chamberlin](https://en.wikipedia.org/wiki/Donald_D._Chamberlin) and [Raymond F. Boyce](https://en.wikipedia.org/wiki/Raymond_F._Boyce) in the early 1970s. This version, initially called *SEQUEL* (*Structured English Query Language*), was designed to manipulate and retrieve data stored in IBM's original quasi-relational database management system, [System R](https://en.wikipedia.org/wiki/IBM_System_R), which a group at [IBM San Jose Research Laboratory](https://en.wikipedia.org/wiki/IBM_Almaden_Research_Center) had developed during the 1970s. The acronym SEQUEL was later changed to SQL because "SEQUEL" was a [trademark](https://en.wikipedia.org/wiki/Trademark) of the [UK-based](https://en.wikipedia.org/wiki/United_Kingdom) [Hawker Siddeley](https://en.wikipedia.org/wiki/Hawker_Siddeley) aircraft company. &gt; --- ^Interesting: [^Microsoft ^SQL ^Server](https://en.wikipedia.org/wiki/Microsoft_SQL_Server) ^| [^SQL:2003](https://en.wikipedia.org/wiki/SQL:2003) ^| [^IBM ^SQL/DS](https://en.wikipedia.org/wiki/IBM_SQL/DS) ^| [^Language ^Integrated ^Query](https://en.wikipedia.org/wiki/Language_Integrated_Query) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+ck8uewm) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+ck8uewm)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Read other people's code and try to understand it. I'm learning so much (and learning the importance of white space and the shift key).
I'm surprised nobody has mentioned this, but working towards a certification is a great way to ensure you get wide exposure to various SQL concepts. Consider getting some study material for the SQL Server 2012 Querying 70-461 exam.
That would also do the trick... although you should consider if there are 3 entries for a student.
We can't use CTE's and MERGE in our environment. Programmatically they are awesome, performance-wise they tend to be nothing but trouble
I think you'll need recursion for this.
You could technically have an IF statement to see if the column name exists (in MSSQL it would be in the systables table, not sure about Oracle. However, this is really bad practice. It's better to have all columns created and just fill the FINAL column with NULL value.
This hurts my brain, the scripts for the column and the procedure to query it should be written in that order. If the column doesn't even have a name how would you script for it. 
OR... Just have the guy create his database structure before he decides to query off of it. Seriously - naming columns in something that elementary shouldn't be a decision you put off.
Thanks very much! The view works like a charm. I would still have to have the owner edit the view but at least the script portion is completed. Off topic, I noticed your pivot query and got me thinking that I **could** do an UNPIVOT. it doesn't solve my situation, but at least I understood more of the PIVOT functions thanks to your suggestion :) Could be useful for other situations I have. with unpivot_math_scores as ( SELECT * FROM math_scores UNPIVOT ( scores FOR exams IN (quiz as 'Q', test as 'T') ) ) SELECT student, avg(scores) FROM unpivot_math_scores group by student; Much appreciated.
The same way I store any other source code. It's just another part of the project. Anything needed to create the database items needed by the application gets put into source control right alongside the rest of the app.
Which database?
Subquery in the select is probably not the way to do this. Rather do something like the following (I'm using SQL Server): create table salesinformation ( CustomerName varchar(50), Value int, Qty int, OrderDate datetime, Item varchar(200) ) insert into salesinformation (CustomerName, Value, Qty, OrderDate, Item) values ('Cust1',1,30,'07/05/2014', 'RedBike') , ('Cust2',1,30,'08/10/2014', 'RedBike') , ('Cust1',1,30,'08/15/2014', 'RedBike') , ('Cust1',1,30,'02/15/2014', 'RedBike') declare @startdate date = '08/01/2014', @enddate date = '08/31/2014'; select a.CustomerName, a.Item, SUM(a.QTY), MAX(b.YTDQTY) from salesinformation a join (select CustomerName, Item, SUM(Qty) as YTDQTY from salesinformation z where z.OrderDate between '01/01/2014' and @enddate group by CustomerName, Item) b on a.CustomerName = b.CustomerName and a.Item = b.Item where a.orderDate between @startdate and @enddate group by a.CustomerName, a.Item Edit: the MAX(YTD) is just to avoid having it in the group by...
Probably not the professional way to do it, but on my dissertation we maintained a teardown.sql script and a setup.sql script which could be ran against the db in order to create or tear down the db. The setup script contained test data for our integration tests too. However we all used the same azure db. Therefore versioning was never an issue. 
We primarily use SQL Server
Same here. I save all of my SP, Triggers, Schema changes, etc... to .sql files as part of my project...overwriting the old file when I make a change. I then use Git to track the changes to each file.
Making some assumptions by what you mean by update the data... Could likely do something like the following (MSSQL) insert into junctiontable (user_id, follower_id) select new_user_id, new_follower_id from new_userdata a where not exists (select 1 from juntiontable z where a.new_user_id = z.user_id and a.new_follower_id = z.follower_id) then if you wanted to remove any that wasn't in your new data set you would do something similar with a delete statement. MSSQL allows you to use the merge command, but I'm not sure if there's a specific equivalent in MySQL (which I'm guessing you're using... feel free to correct me if I'm wrong) EDIT: a word
I'm not too familiar with MYSQL so take what I'm saying with a grain of salt. MySQL's INSERT ON DUPLICATE KEY checks for primary and key constraints. You must have the constraint setup on the table for the check. Lets say 'a' is user_id, and 'b' and a key constraint has been created that does not allow duplicates of 'a' and 'b'. INSERT INTO table (a,b,c,d,e,f,g) VALUES (1,2,3,4,5,6,7) ON DUPLICATE KEY UPDATE c=3, d=4, e=5, f=6, g=7; If there is a duplicate on a/b the other columns (c-g) will be updated and a duplicate values wont be inserted. This avoids having to do composite keys as other SQL systems usually require you to do. If you don't have any columns other than a/b (user_id, follower_id) then use: INSERT IGNORE INTO table (a,b) VALUES (1,2) If a duplicate is found, it wont insert it.
I've used both the VS/SSDT and TFS/git approach, and RedGate source control with arbitrary vcs. The RedGate tools are nice because of flexibility in syncing specific objects to different targets, but the SSDT project is better for refactoring - SSDT had also become a little less rigid about syncing specific changes in the newer versions.
&gt; If I download new data and want to update my table, this scenario comes up often do a complete DELETE for that specific user_id (or follower_id, as the case may be) and follow this with INSERTs
We wrote a console app that looks at a directory and runs migration scripts against a database. A table in the DB keeps track of what scripts have been ran. Simple naming convention to determine the order of files and they get ran in a transaction per subdirectory. One of my co workers presented this at the Heartland Developer Conference today. Source will be in got hub next week if anyone is interested. 
I use a program called sqitch (www.sqitch.org). It works on top of git and really helps with overall organization and the deployment process. 
You might try looking into linuxy operating systems. They generally are rather smooth in the i18n department.
I think given a more general situation you are right. But given that the situation you suggested just doesn't match the the scenario the OP suggested I don't think its really a big concern here. If people want to stretch the time period outside the year involved in the YTD then you have to ask why are they comparing it to YTD. If they are looking at YTD sales data and then comparing to a value that is outside the YTD period then the comparison doesn't have a great deal of value. If they want to compare data from last year to this year (EG YTD till august last year compared to YTD to August this year) then that is really a very different report to the one the OP requested. I think for this particular situation the query does a pretty good job of being as efficient as it can be. 
The specific situation would be the second semester. Example below: Semester1 01/01/2014 Semester2 06/01/2014 Semester2 05/01/2014 your min and max would pick up 01/01/2014 and 06/01/2014 respectively.
Bring up Environment Variables, then look for PATH under System Variables and edit that one. Hit the right arrow key so that you're typing at the end of the string, and then append `;"C:\your\sqlite\install\directory\"` If the existing PATH already ended with a semicolon, you can leave that out, but if there wasn't one in there, it needs to be the first character you add. Any command prompts you already have open when you do this will not get the new setting, you have to close them out and open a new instance of `cmd` for the updated PATH to take effect.
Oh totally. :) I wasn't trying to take anything away from your answer which obviously works just great. I just thought I would add a different approach as I always find it interesting to see how different people approach the same problem and just how many different but possibly valid ways there are to do the same thing.
Oracle 11g supports virtual columns and [computed columns]( http://viralpatel.net/blogs/oracle-11g-new-feature-virtual-column/ ). That is the correct way of doing things, as in general DB rules, calculated fields shouldn't be stored.
This is what I do.
I agree with everything you said. Its always interesting to see how different people approach a problem :)
Another alternative.. seems so obvious now. Thanks for responding - sorry this response is so delayed.
Thanks - the CASE is really all I needed and I forgot all about it - should have seen that coming. I appreciate you taking the time to help!
Just as shawnc said Here is step by step how to do that on multiple OS's thanks to sun systems **Windows 8** 1.Drag the Mouse pointer to the Right bottom corner of the screen 2.Click on the Search icon and type: Control Panel 3.Click on -&gt; Control Panel -&gt; System -&gt; Advanced 4.Click on Environment Variables, under System Variables, find PATH, and click on it. 5.In the Edit windows, modify PATH by adding the location of the class to the value for PATH. If you do not have the item PATH, you may select to add a new variable and add PATH as the name and the location of the class as the value. 6.Close the window. **Windows 7** 1.Select Computer from the Start menu 2.Choose System Properties from the context menu 3.Click Advanced system settings &gt; Advanced tab 4.Click on Environment Variables, under System Variables, find PATH, and click on it. 5.In the Edit windows, modify PATH by adding the location of the class to the value for PATH. If you do not have the item PATH, you may select to add a new variable and add PATH as the name and the location of the class as the value. **Windows XP** 1.Start -&gt; Control Panel -&gt; System -&gt; Advanced 2.Click on Environment Variables, under System Variables, find PATH, and click on it. 3.In the Edit windows, modify PATH by adding the location of the class to the value for PATH. If you do not have the item PATH, you may select to add a new variable and add PATH as the name and the location of the class as the value. 4.Close the window. **Windows Vista** 1.Right click My Computer icon 2.Choose Properties from the context menu 3.Click Advanced tab (Advanced system settings link in Vista) 4.In the Edit windows, modify PATH by adding the location of the class to the value for PATH. If you do not have the item PATH, you may select to add a new variable and add PATH as the name and the location of the class as the value. 
Does every item go 7 levels deep? This seems like if there isn't 7 joins deep you'll never get your [QTY] value. Does this occur often?
Whats the primary key on the table?
thanks
I'm using mac but I'll check if there is an equivalent syntax 
What table is QTY and YIELD in? They aren't specified in the subselects which tells me they are in one table and not the other. Is this MSSQL, ORACLE, MYSQL? I'm assuming MSSQL or Oracle.
This is my attempt to do a recursive CTE Please read the comments. declare @ver int set @ver = 40 declare @item varchar (max) set @item = 'RU10110' ;WITH list (item,partno,qty,yield,depth) AS ( --THIS IS THE ANCHOR PART OF THE RECURSIVE QUERY THAT GIVES STARTING POINT SELECT a.item, a.partno, ISNULL(b.qty,0)/ISNULL(b.yield/100,1) as QTY, --IS QTY and YIELD IN ppbmt or ppmhd? 1 as Depth FROM dbo.ppbmdt a INNER JOIN dbo.ppbmhd b on a.item = b.item INNER JOIN dbo.iciloc c on a.partno = c.item WHERE a.item = @item and b.version = @ver and a.item not like 'BUDNEW%' and c.loctid = 400 --THIS STARTS THE RECURSIVE JOIN THAT JOINS THE CHILD ITEMS TO ANY DEPTH UNION ALL SELECT a.item, a.partno, ISNULL(b.qty,0)/ISNULL(b.yield/100,1) as QTY, --IS QTY and YIELD IN ppbmt or ppmhd? ROW_NUMBER() OVER (ORDER BY .item) + 1 as Depth FROM dbo.ppbmdt p INNER JOIN list l on l.partno = p.item and p.version = @ver and l.partno not in ('END','END_') INNER JOIN dbo.ppbmhd b on a.item = b.item ) SELECT MIN(l.partno) OVER (ORDER BY l.DEPTH ASC) as Level1, MAX(l.partno) OVER (ORDER BY l.DEPTH ASC) as MAXLevel, MAX(l.depth) as MAXdepth --If you are curious to see what depth it went to ROUND(SUM(COUNT(*)*AVG(l.QTY)),8,1) as QTY --This should be more accurate than rounding first then multiplying FROM list l
That's precisely what analytic functions with windowing are for. You can have row or range windows. [Here's an article for Oracle]( http://www.orafaq.com/node/55 )
Business Intelligence is the industry you are looking for. Dimensions that change are called [Slowly Changing Dimensions](http://en.wikipedia.org/wiki/Slowly_changing_dimension). There multiple types of slowly changing dimensions. These are usually chosen during the modeling portion of the star schema. &gt;An example of this would be, "I want to see all donations of $50,000 or more over the past 10 years by the State they lived in at the time of the gift." That's a very simple version. You would need a type 2 or type 6 or even a type 2/6/3 slowly changing dimension. When you write your query: SELECT state, donator, donation, FROM donator inner join donation on donator.id = donation.donatorid inner join donator_addresses on donator.id = donator_addresses.donatorid and donation.date between donator_addresses.start_date and donator_addresses.end_date WHERE donation.date &lt; DATEADD(year,-10,GETDATE()) 
Awesome, I am actually on route to aspen at the moment. But I will validate these queries on Monday
Weird. @@rowcount is a signed integer so maybe it's overflowing somehow? My suspicion would turn toward the typecast though, since it's probably more likely. Maybe try casting to BIGINT first and then cast to VARCHAR(20)? 
I've never seen this. Nothing from your query would cause these results. Perhaps some critical detail was lost in your effort to provide an abridged version of the code for us. Or the issue is stemming from something outside the context of what is visible to us from your post. I would almost suggest checking your database integrity, to ensure there is no corruption anywhere. To aid in debugging, you might try using the OUTPUT clause of the UPDATE statement to capture the updated rows in a temporary table, then count the number of rows in the temporary table. For example: DECLARE @temp TABLE ( SSN CHAR(9) ) UPDATE A SET stuff = B.Thing OUTPUT INSERTED.SSN INTO @temp FROM Table1 A JOIN Table1 B ON A.SSN = B.SSN AND A.Field = 'A' AND B.Field = 'B' AND A.Stuff is NULL SELECT COUNT(*) FROM @temp Edit: formatting
Thanks, I'll go have a look.
The answer greatly depends on how the datamart (or warehouse) has been built. If the DM is Kimball-style and assuming it is built with these kind of requests in mind, you'll have DateGiven_Dim (or rolename) and Demographic_Dim, MyRelevantDonationAttributes_Dim (all measured with the fact/grain). Your query will them become simply a join by Dimension keys and a condition on DateGiven year. If your DWH is Inmon/Imhoff style and you have a proper Data Mart built, see above. If not and you're starting with the 3NF schema, it all depends on which data relationships were modified to allow for historical change and you'll need to use a given date to resolve many-to-many relationships to required cardinalities (1-to-many or 1-to-1). 
I understand makes perfect sense that it is using the column from the first table, but still surprised that it wouldn't error. I've since told the person who wrote this to alias their columns. Edit: think i get it, the sub query is literally saying select 3 from tableb. In Your example
BI system manager here. This is a great answer. Just a quick note...you need more than just a slowly changing dimension table. You need aggregates who have that particular dimension keyed off the SCD dimension table (assuming you have both a SCD and non SCD version of that dimension.
I think some of the following you'll know already, but just want to back up a bit and you may find some good concepts/technologies to look into... In terms of MS SQL Server (terminology may be different in other platforms but I'm sure the concepts are similar)... It's already been mentioned that this is referred to as BI (Business Intelligence). To expand on that, the main tool you'd want to use for this sort of thing is a Data Warehouse (and more specifically, multi-dimensional cubes, using MDX and SSAS in the SQL Server world). In a Data Warehouse, generally it's just a regular SQL database, however you intentionally organize it differently. Usually people mean 'transactional database' when they mention a SQL Database. These are optimized for capturing data and keeping storage use optimized. When it comes to pulling and aggregating data to report on, a 'data warehouse' is more optimized. The data is de-normalized... it takes up more space, but is much faster to pull relational data because you generally join less tables and are more able to hone in on the data you want. The organization of 'fact tables' and 'dimensions' really helps you slice and filter data in complex ways, much faster than in a transactional database. Taking it further, multi-dimensional cubes and SSAS will improve this efficiency even more, at the cost of even more storage requirements, as well as more time putting data into the cubes. Cubes can be configured to pre-calculate your data aggregations as the data goes in, so if you want, say, sums and averages of sales figures in a particular product line and geography in a given period of time, but only on Sunday mornings when the purchaser is a female and has been a customer for atleast a year... well you can get that very fast, even with very large datasets, because it's already basically been calculated. I'm not an expert on multi-dimensional cubes yet, so please forgive me if that's not quite correct. But it seems like that's pretty inline with what you are looking for so I have a hunch this is what you want to look into. FYI you don't use normal SQL syntax to query these cubes, in SQL Server the language is called MDX. Another tool you might look into is Powerpivot for Excel. It is meant to link to SSAS (SQL Server Analysis Services) and can be a very useful tool, especially if you have a data person on one end providing the database requirements, and a 'power user' on the analysis side that wants to slice and analyze the data with a friendly GUI. It also helps a ton for making visualizations based on complex data analysis. 
Not quite. Because the column c_id does not exist in tableB the way the statement is executing could almost be written like this: delete from tableA where c_id=c_id But that isnt exactly true and i will show you why.... Oracle is doing its best to try and resolve the c_id column in the subquery and is resolving as the tableA column. Below are two explain plans. 1 with the column in tableB and 2 without the column in tableB. **1. Explain plan with tableB.c_id** |Operation|Options|Object|Rows|Time|Cost|Bytes|Filter Predicates| Access Predicates | |-------|------|------|----|----|-----|---|---------|-----------| |DELETE STATEMENT|||5|1|6| 130||| |DELETE ||TABLEA ||||||| |**HASH JOIN**| SEMI || 5|1|6| 130| |**"C_ID" = "C_ID"**| |TABLE ACCESS| STORAGE FULL|TABLEA | 10| 1| 3| 130||| |TABLE ACCESS| STORAGE FULL|TABLEB | 5| 1| 3| 65||| As you can see in row 3 Oracle is doing what you are expecting, it is joining the two tables in memory on the C_ID column and eliminating matching rows. **2. Explain plan without tableB.c_id** |Operation|Options|Object|Rows|Time|Cost|Bytes|Filter Predicates| Access Predicates | |-------|------|------|----|----|-----|---|---------|-----------| |DELETE STATEMENT|||1|1|6| 13||| |DELETE ||TABLEA ||||||| |**FILTER**||||||||**EXISTS (SELECT 0 FROM "TABLEB" "TABLEB" WHERE :B1 = :B2)**| |TABLE ACCESS| STORAGE FULL|TABLEA | 10| 1| 3| 130||| |**FILTER**||||||||**:B1 = :B2**| |TABLE ACCESS| STORAGE FULL|TABLEB | 5| 1| 3| 65||| Because the c_id column does not exist in tableB oracle is changing the subquery into an exist statement that always returns true and is binding to the value of tableA.c_id. :B1 and :B2 while named differently are referring to c_id in tableA. So you can view that predicate as "tableA.c_id=tableA.c_id" or always true. So as long as there is at least 1 row in tableB and c_id does not exist then all rows will be deleted. If you delete all rows from tableB then no rows will be deleted because the exist statement will always be false.
You are more then fine starting with your localDB. If at any point you find you are limited (most likely months down the line), make the change then
Are you having performance problems with the query right now? Can you give me some context as to when this will be run and how often and why? 
If you want to recover your corrupt SQL Server Database files and fix the any type of error then you have need a SQL recovery software, which repairs &amp; recovers MDF file including entire elements such as tables, trigger, view, functions etc. http://www.repair-sql.net/ 