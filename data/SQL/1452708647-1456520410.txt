For what kind of role? DBA or Developer?
Developer but would like DBA 
I doubt that I qualify for DBA position at the moment so would need to start entry development first.
Forgive my naïveté, but couldn't you just order by salary desc, and where row number = N
I guess this really depends on what you define as a new or returning customer but I would go for something like this for a returning customer HAVING COUNT(orders.customerid) &gt; 1 And add something similar to this to your where clause And customerid in (select customerid from orders where orderdate &lt; GETDATE()-14) --Must have ordered prior to 14 days ago 
si
No silly is writing inefficient queries that are not scalable.
THIS is EXACTLY what I'm looking for, BUT...the guy only did an intro. Do you know who this is? Is he like a teacher somewhere? Can I find more of his work somewhere else? This is so genius! https://www.youtube.com/watch?v=HgoM1I4yEFo
who told you that lie ?
The best is to learn by practicing. Try out the following web site. There are both course and exercises you do at your pace. http://www.studybyyourself.com/seminar/sql/course/?lang=en. Watching video might work to get an overview. If you want to anchor thing though, the best way is by trying and doing.
Awesome, thanks! Will give it a whirl in the morning!
I made a game, the Schemaverse. https://github.com/abstrct/schemaverse Just have fun with a project.
**SSMS** First thing you should do is get [SQL Server Management Studio](https://www.microsoft.com/en-us/download/details.aspx?id=42299) **AdventureWorks Database** and then install the [AdventureWorks](http://blog.sqlauthority.com/2014/08/10/sql-authority-news-download-and-install-adventure-works-2014-sample-databases/) sample database. This will allow you to have access to actual data with realistic relations. **Other things to do:** * Learn ANSI SQL - Check out [this](http://javarevisited.blogspot.com/2015/06/5-websites-to-learn-sql-online-for-free.html) site for some ideas * Since this is a Microsoft product, there seems to be less open-source learning tools out there for MS SQL specifically, but you can start with ANSI-SQL and go from there. * Get engaged in the SQL community * Try to answer SQL-related questions 
Thanks for the reply! I've already been playing around with some stuff similar to your suggestions. "Last Wednesday" means the previous Wednesday. So if today is Tuesday, then it would be 6 days ago. If today is Thursday, then it would be yesterday. Ultimately, I'm trying to get two sets of data. * 24 hourly samples for "last Wednesday" * 24 hourly samples for the average of each hour (hourly profile) for all Wednesdays in the last 60 days
I don't have a good understanding of that role, so I assumed you can install DBs, tune DBs, and of course also use DBs.
For me and for my database the best friend is http://sqlbackupandftp.com I like it because the price is really good and this tool is easy in use. P.S. I found promo code (SQLBACKUP-40) for SqlBackupAndFtp with 40% off.
What about when you aren't Inserting or Deleting? Like if you just need info from X.coulumn1 to equal y.coulumn2?
If you know that you will never be doing inserts or deletes and are only going to do updates, there isn't a significant difference. If there is a chance of business needs changing down the line to where you sometimes will be doing inserts or deletes (or both) along with updates, then merge wins because you only have to add that part of the statement vs starting over.
Hard copies. Print everything out.
Personally, I would only use Shrink DB if I knew for certain that the DB doesn't regularly grow above a certain size, using some analytic tool. That being said, there are some other options to look into first: * Data compression * Reorganize/Rebuild indexes * Remove duplicate/unused Indexes I would research these first, before doing Shrink DB unless you are absolutely sure you have excess space. Good luck on your migration.
Well it depends a bit what your current skill level is, this is prety advanced stuff you made. Most starters have trouble with normal querys let alone dynamic stored procedures.
As someone who manages a DB that backs up 24TB this made me chuckle. ha but seriously 30GB is a baby database
Cool, thanks! Gonna try it here in just a bit, will report back. =)
What you're looking for is a pivot table. This problem can be a pain, particularly in MySQL (a few other db engines have a pivot table function built in). I'd recommend [this](http://stackoverflow.com/questions/33101722/mysql-pivot-in-same-table) post. The solution given by r3pr0b8 is a good solution, but be aware that you need to know all of the values in one of the columns ahead of time, and the size of the query grows with the number of values. When I've run into this problem in the past, I've taken the cowards route and compiled the table outside the script (in spread sheet or with a scripting language).
You want the DATE = (select max(DATE) from .... where DATE &lt; TODAY and the weekday(DATE) = Wednesday). (If today is Wednesday, do you want today or a week ago?)
That's actually fairly relevant, because my end goal is to import the data into Excel using the MySQL connector and PowerQuery, then create a pivot table there using the data. Basically this is a data set that we need to review every month with the most current data, and this seemed like the easiest way to do it without a Crystal license. I'm open to any other suggestions, though!
&gt; I make scheduled backups with the help of third party Tool. Why? There's no need, in the vast majority of most cases. Use Ola Hallengren's maintenance solution or MinionBackup to create Agent jobs, schedule them, and get it all taken care of within SQL Server. Backup compression (usually cited as the main reason people go with a 3rd-party tool) is available in Standard edition since 2008R2 (maybe earlier).
The article says this &gt;So when you are in the habit of using them, be prepared to review all your UPDATE statements when moving to Oracle, DB2, Sybase, MySQL, or even a different Microsoft database! I've been working with SQL Server everyday for 15 years. I've worked with MySQL and Oracle a couple times a month for years as well. I've never seen a situation where a system changed out it's underlying database and had to worry about ANSI conformity. Eliminating all proprietary syntax reduces performance in aggregate. I don't use MERGE that often. I should use it more often when it makes sense to do so. In some areas, I am stuck in my ways. My advice to you is to keep up with the new features and syntax as much as possible. Keep the tools that help, and just keeps the ones that don't in the back of your mind.
As someone that backs up your mom every night, made me chuckle.
I worked in a large insurance company and we had the centralized approach. We had one team of report writers that handled all requests from all departments. In this structure, basically all your customers are internal business units. I loved working this way, as I got to get my hands in all different aspects of the business, but beware. It is important to have someone who can prioritize the work, and gather the requirements from the business, otherwise it can be result in more meetings and therefore less reports. My 2c
You are correct, 2008R2 is when compression came to standard. We were stuck on 2008 for years, and hated not having compression. I was *this* close to getting 3rd party just to get compression. Now on 2014 and damn its nice having such smaller backups :) So the guy could be stuck on an older version for some reason, that would be a valid reason for it. Technically could you class something like MinionBackup/Hallengren's scripts as 3rd party? I know they're only scripts, but there's a lot of work and code gone into them, and they do a lot of the heavy lifting to turn 'backup is a thing' into an actual usable and maintainable function. Some might class them as a little bit 3rd party, even if its still just sql server doing the actual backups. 
 select ts::date as dt, sum(value) as value from your_table where datediff('day', ts, now()) &lt;= 60 and extract(dow from ts) = 3 group by ts::date;
I actually know people who say that playing the game helped them get a job. Just playing helps to learn basic SQL and then a fun way to start experimenting with your own pl/pgsql (which the game uses to script actions), or write external database apps too. 
thank you for the reply, not everyone bothers to do that happy SQLing ;o)
~~Er... Developer I guess? Programmer would be a better term. I convert legacy data to our software using straight SQL.~~ Edit: wrong topic. Hah!
Select bread from kitchen where crust = 'cut' and toppings = 'peanut butter' and 'jam';
Brillant!
Just spin up an instance of something you'd be comfortable using (I'd recommend MySQL or MSSQL Express) and write the queries to import all the files into a single table, assuming they are a uniform standard; otherwise use multiple staging tables and clean up the data first. From there it's just trial and error / Googling to find what you need it to do. Without knowing what the data looks like it's hard to answer the question. *P.S. - If you want a **really** ghetto solution, create a single excel file that has connectors for each other file/etc as a separate worksheet, then have a macro aggregate them together.*
&gt;Definitely try to avoid splitting single table updates that look like they would have to tablescan anyway Not sure what you mean by that Could you provide an example from my code? &gt; If this is an sp, so (presumably) is intended to be run multiple times, why not prebuild at least some of the aggregations in separate tables (passes 1 and 2, for example)? Because we want to be able to change the date range and the event list (0) is the default but we have an event code or every holiday but I will likely split this into two or maybe 3 SP to be honest. - One that Fetches/crunches all the data. - Possibly One that sets all the ignore flags - One that does the simulation. (might be merged with the flag part not sure yet.) Right now it's one big block because I'm trying to get every piece down and functional without having to tab through stuff. That way we can mess around with the simulation on one data set multiple times and if we want to expand it we just start from the top again. That's why the drop at the end is commented out, I've been running the new code on the existing data and resetting only when I mess something up. 
&gt; but could you explain it to me like I am 5 Basically does the data in all the separate locations look the same? In the same style of format. For example: *** Spreadsheet #1: | ID | Firstname | Lastname | DOB | etc | etc Spreadsheet #2: | ID | DOB | Firstname | Lastname | Primary Location | etc | etc *** You'd have to account for the date of birth, firstname and lastname being in different locations when you import them. Or that some might have more information than others (ex: Primary Location). &gt; use multiple staging tables This basically means that when you import the data from the separate excel files each will get it's own (temporary) table instead of being all imported into a single data set. Then clean up and combine those separate (temporary) tables to populate a single data source. Essentially this gives you the ability to clean up or modify the data in a way that you control before utilizing it to produce results. &gt; go to different folders because they go to different people When you import them, add extra data that contains who it needs to go to that can be included/filtered when you go to do the final portion. Honestly the best way to do this kind of thing if you are unsure is just to set something up and try it. Obviously making isolated copies that cannot touch the originals; working with data is the most effective way to become comfortable with it and develop an understanding of how it links together. Many of us are accidental DBAs who started out as something completely different and got thrown into a completely arbitrary project. 
Ahh, okay, well yes, for the most part the style of each spreadsheet is more or less the same in terms of formatting. And yea, that is what I want to do, as the data is very sensitive and auditors apparently hate when they screw with the live data and obviously, I want to learn on some dummy data. Is there a quick and easy way to basically make a bunch of pointless, dummy data in multiple spreadsheets besides going in manually and entering in data? Thank youi very much for the help!
[GenerateData.com](http://www.generatedata.com/) works fairly well for that kind of task. 
He has a great career ahead of him.
&gt; SELECT FirstName, Lastname, City &gt; FROM Customer &gt;Where (City = 'Brazil') &gt; AND|OR (FirstName = 'Frank AND LastName = 'Ralston')
SELECT * FROM Costco Am I doing this right? 
`AND|OR` isn't a SQL keyword. It's one or the other. It should be *really* obvious which one you need.
Brazil isn't a city. Well, it's possible that there is a city somewhere named Brazil but for this exercise, I'd wager that you're looking for the **country** of Brazil.
Omg. I love you. &lt;3
This statement is true.
Select * from sandwiches You're now dead from sandwich dataset
Y'know, they've got this crazy new technology called screenshots...
No imgur on the work laptop. Firewalls and such ya know.
Use mysqldump and ssh/sftp. It can be done with a one-liner: http://www.cyberciti.biz/tips/howto-copy-mysql-database-remote-server.html 
Why unix? Do you like VI?
What happens if the FTPSEND records are not received by the vendor, or are somehow incorrectly processed? Are they able inform you to request a resend? If so, I'd assume you'd have to keep all the packages that are sent via FTP, since you're truncating the source table and the data would no longer be available from there. If it were me, the FTPSEND table would never be truncated, but instead each newly inserted change would be datestamped, so you've got an audit of every record. Then there's an argument for changing your data-transfer method from FTP to a webservice. Create a service that exposes the FTPSEND table, and allow the vendor to query it supplying a date range to get all the changes between those dates. Taking it one step further, if the webservice could accommodate the processing of a receipt message from the vendor, you could process it and potentially perform any truncation then, if table size is an issue.
Thanks for the suggestion. Didn't think to try that however can't connect to my MySQL db get errors. Have you had l this working yourself? Tried looking on google but everything refers to the odbc or ado.net connectors. Thanks again 
Ended up installing an old copy of SQL server 2000. Created a DTS package and all is working as it should. Can copy data both ways and create tables on mysql no problem . Would like to get it working in 2008 or 2014 but this will do for now. If anyone comes up with any suggestions please let me know. Thanks
Many DBA's are usually database developers who have progressed into a DBA role. Any decent DBA should routinely be checking the stored procs and queries the other devs are writing, and then optimising them (where needed). For this you obviously need to understand SQL, but also the impact of SQL in terms of performance
Thanks, it's clearer now.
The side bar has a link to the wiki which has some good resources. https://www.reddit.com/r/SQL/wiki/index
I don't know of any solution that can be done directly from SQL. I do however have a script in Python that'll read the database and dump it into epub format. Let me know if you're interested. I need to know a few things to finish it 1. File naming scheme? What happens if two stories have the same name(there are many)? 2. Dump all epubs into a single folder or any type of folder structure you want? I'm using some crappy epub reader called Cool Reader just to test to see if I could get it working, it looks ok but I've no idea what it'd look like on your device. [You can see a sample file here with 7 chapters.](https://mega.nz/#!zgBUxIyQ!hM6kraotDrEGDNtWEclYfqSA0eglMEptWHTL9Tqg_wo) i choose one at complete random, I've no idea what it's story is about.
Insert Into table1(coulmn1,column2,etc,) Select coulmn1,column2,etc From table2
This automatically appends the table, correct? I won't delete whats already there? 
seriously, thank you!!! I was not expecting an answer so fast!
Can you put it in a code block? Reddit formats the asterisks away and we can't tell what your actual code is. 
I'm drunk, but still smh. Jesus, this guy should not have access to whatever it is he needs to modify...
Seperate to the error all the values &gt;10000 will be evaluated in the &gt;2000 if test.
so you mean that the formula will be applied twice? with one would go first and with one will go last?
Sounds perfectly reasonable to me! We've done similar things when needed. When shrinking, we just keep an eye on things a little more closely (mostly active processes) to ensure the additional IO isn't causing any abnormally long query's, etc... It's easy to cancel the shrink and it doesn't cause any sort of rollback so it's an instant 'fix' if things go south. And as you say, redo the indexes after - the shrink is a physical data move, so stuff is all out of order afterwards.
999 DBAs just cried out in dispair... and suddenly they were silenced.
If I am understanding correctly, I would make the ID column an identity column. This way the ID will be incremented internally and you never have to worry about it. What SQL platform are you working on? EDIT: I was not understanding when I responded the first time. Essentially, the scenario is: you get a new order with a new customer. We will say that this order has multiple cakes. Your question is how do you insert to orderscakes?
Why do it in a subreport? Why not bring in the image as a column in the main dataset if they're at a 1:1 IF CASE field=1 THEN blobData ELSE null
&lt;snark&gt; It sounds like the problem you're experiencing is that you're trying to use MS Access as a database &lt;/snark&gt;
I would make sure the sub (including margins) is smaller than the item containing it.
Ok, thank you. What SQL platform are you using? I would insert into the tables with autoincrement first, then grab the last inserted ID and save it to a variable (depending on the SQL platform this can be accomplished using @@Identity or something else). I would avoid incrementing the id variable yourself. Here is some pseudocode: Insert to Customers Insert to Orders Save last inserted orders ID to variable For each cake in this order Insert to Cakes Save last inserted cake ID to variable Insert to OrdersCakes with two saved variables It's difficult to give the best answer without knowing everything.
 if P &gt; 2000 then SET precio= (P*0.05)+(P*0.12)+P; also change if P &gt; 2000 elseif P &gt; 10000 to if P between 2000 and 10000 that will prevent values greater than 10000 to trigger the 2000 if you still need the &gt; 10000 if though.
He's a jive turkey
Fixed it.
Val2 integer as (val1 % 10)
What don't you like about the INSERT INTO method?
Not sure, but they're called generated columns or computed columns if you want to do more research.
Personally I'm anal retentive on the idea of not doing the "TRUNCATE &amp; INSERT" method if there is a rhyme or reason to what is getting loaded into the Target table for IO/Networking reasons. So my one question is, is the Target DB getting the same data plus any new data loaded since the last time the Target DB was loaded? On an note, ETL is a lot of times done by stored procedures and a job on the server. SSIS is much easier graphically and driving paths of error and success. Personally works better for managing transporting of data but also making it more precise and not just blanket wiping and reloading of data which takes up IO and bandwidth. 
nice, thank you, will test it out right now.
 DELIMITER // CREATE PROCEDURE facturar(fecha date,cedula VARCHAR(9),cantidad int(3),peso VARCHAR(10), precio varchar(10)) BEGIN DECLARE P float; declare total float; set P=0; insert into ventas VALUES(fecha,cedula,cantidad, peso,''); set P=peso*4; if P between 2000 and 10000 then SET precio= (P*0.05)+(P*0.12)+P; elseif P &gt; 10000 then precio= (P*0.10)+(P*0.12)+P; else precio= (P*0.12)+P; end if; select concat("Total de envio=", precio); END; // New code, same problem. #1064 - You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near '= (P*0.10)+(P*0.12)+P; else precio= (P*0.12)+P; end if; select concat("Total' at line 9 also thanks a lot for pointing those out. edit: facepalm, forgot the "set" in elseif and else. Thanks a lot boy, you solved my problem, if i could i would give you gold.
I agree. Try merge.
Do note that it is not recommended to do this with a large number of rows, but it does come in handy at times :)
Hadn't looked at merge yet. I'll deffinetly look into it. The move is taking produciton data and putting it on a reporting database. Don't have to worry about messing up existing stored procedures.
Be careful about high availability(H.A.), replication.... as people seem to forget that a very common disaster scenario is some process or person messes up data. In the case of replication that messed up data is rapidly copied to the other server. So replication is only a partial solution and for some problems not a solution at all. Backups are far more important. I like to do log shipping and such as that allows a point in time recovery. Any complex solution you are thinking about doing shouldn't be done at all if you don't have a way to test it and test it on a regular basis. Some H.A. solutions require ongoing maintenance and may be much more costly in the long run than expected and may not be worth it to the business. Occasionally a H.A. solution will actually be the cause of downtime. 
The primary benefits of nonclustered indexes are that you can have hundreds of them per table if necessary, and they're "cheaper" during write operations because less disk IO is involved. In general, you want to have a clustered primary key comprised of one or (very few) more columns whose values will rarely change. Columns that are frequently updated should be covered by nonclustered indexes instead. This gives your common select queries the performance boost of the clustered index, without slowing your updates way down. I'm not sure about disk space, I don't think I've ever thought to try a comparison!
Ok, you have it partially correct. A clustered index is indeed not officially a separate index but a declaration to have the system store the primary data pre-sorted by the indexed columns. This reduces disk IO on writes marginally as only the data pages need to be updated. It also dramatically improves read performance when the indexed columns are used and when you are secondarily reading columns outside the index as there is no "hop" required going from the indexes row to the data row (given that in this case they are the same). As to space, you are indeed correct that the clustered index, not being a separate index, means that it takes up no more physical disk space. However, the downside is that by definition you can only have one (you can't store the data pages multiple ways obviously - well you *could* but that would bring in other challenges). This is where non-clustered indexes come into play. Non-clustered indexes are stored separately. Conceptually they are like new tables. They will only containing the data for the columns defined in the index and are sorted by those columns. They also contain a record pointer (different database implement this different ways) that allows quick(er) access back to the row in question. Here is the important thing to understand regarding performance. For most RDBMS's, if your query ONLY queries from columns that are in the index than no secondary lookup to the source data pages is required. It makes you query as fast as querying things by the clustered index except obviously you are using the columns from the index. However, if you include even a single column from outside the indexes list of columns than that record pointer needs to be used to fetch the source row and read the data. In fact, some systems allow you to store non-sorted data columns in the index just so you can avoid that secondary lookup. These are all about read efficiency, especially when foreign keys are used; you want to ensure that both sides of the FK relationship are indexed to make joins work as efficiently as possible. However, the downside is that more indexes (and more columns - indexed or just stored as I mentioned) means a **whole lot more** disk IO [/u/shaunc is unfortunately incorrect on this]. As a result, you take a large hit on write performance. Sometimes you just don't care but sometimes you change your whole database design to optimize for write performance - which usually means reduced read performance. As with lots of things in computer engineering... it's a trade off. I should also call out that how systems decide which indexes to use is also something you need to pay close attention to. You want to do query plans with any query that will be coded into a system (one-off queries obviously don't matter). You, at times, will be surprised that an index you thought was important isn't ever being used and dropping it can be a boon to perf. Alternatively, dropping the wrong index can take some critical query that used to run sub-second to all of the sudden take 5 minutes. 
Not to be rude but much of your information is incorrect. I have posted a (lengthy) description back to OP that lays out how things work - in case you are interested.
 SELECT m.msgid , m.status , ml.description FROM ( SELECT msgid , MAX(timestamp) AS latest FROM messagelog GROUP BY msgid ) AS mx INNER JOIN messagelog AS ml ON ml.msgid = mx.msgid AND ml.timestamp = mx.latest INNER JOIN message AS m ON m.msgid = ml.msgid of course, there will also be a solution using OVER and PARTITION BY... 
Yepp, you'd better use a window function. with lastDescriptions as ( select distinct MsgID, last_value(Description) over (partition by MsgID order by TimeStamp rows between current row and unbounded following) as lastDescription from MessageLog ) select m.MsgID, m.Status, d.lastDescription as Description from Message as m join lastDescriptions as d on m.MsgId = d.MsgID; Read about last_value window function here: http://blog.sqlauthority.com/2011/11/09/sql-server-introduction-to-first-_value-and-last_value-analytic-functions-introduced-in-sql-server-2012/ And be careful with that OVER clause and default window definition (rows/range between...).
Of SQL? Your question is the same as "tell me the limitations of HTML or XML". SQL is just a "Structured Query Language". I suppose the only limitation is that it is generic and many relational databases have their own esoteric functions to streamline complicated tasks. 
Are your statistics up to date? Sounds like it's trying to do it in memory and failing.
You're going to want to look into grouping. Use the GROUP BY clause with the AVG(). For example: SELECT DATEPART(MONTH, ReadTime) AS MonthNumber , AVG(NewMeterValue - PrevMeterValue) AS AverageUsagePerMonth FROM MeterReads WHERE ReadTime &gt;= '20150101' AND ReadTime &lt; '20160101' GROUP BY DATEPART(MONTH, ReadTime); This filters to only last year. If you want more than one year you will need to add year into your group by. You could swap out the month for Customer Id or whatever column depending on what your schema is and what you are trying to accomplish. If you find some of this still running slow it is because you don't have proper partitioning or indexing. That is a more advanced topic you may want to bring in a consultant for that because it would take you a few months to learn what they are and how to use them well.
Thank you! This is helping me wrap my head around how SQL works. I am coming to realize how powerful SQL can be. My modeling software also uses SQL queries but the majority of those I stole from forums, line traces, water aging, etc. I don't know how people just sit down and figure this stuff out.
You're using a 3TB Access database? Some people have no luck at all! Actually, this doesn't surprise me, I also used to work at a large utility company and their systems were so unorganised it was unbelievable! Glad I got out of there. 
Do any of the SPs access tables, views or table-value functions that use SELECT * to get their columns instead of direct column references? If so, try replacing the * with a list of the actual columns in that object. I've been burned before by the DB object changing with a newly inserted column and can throw off what SQL Server is expecting to come back. 
Remove MEProductivityB.Employee from the order by
done. It still creates a list for each employee though.
Yes, both SP's have select statements that insert into the new table from existing tables... And at the very bottom the the report SP it has a " SELECT * FROM #TMP" which is what i think you're referring to... I'll try re-writting that part with the table column headers instead and will update this post. I can't exactly work on this during office hours because i don't really have a testing environment to reproduce this report in which is a major pain... Thanks dude!
I appreciate the help. This is what I was going for, though I do have one question: is it possible to run a query that will look through multiple columns at once without listing out all the column names? In other words, is there a way to search through all columns and drop those with entirely null values without having to write out each column name? My background is in Stats and we regularly have 500+ columns, many of whom can be entirely missing data. It helps to hone in on those that only have values.
I think your problem may be here: &gt;FROM MEProductivityA INNER JOIN MEProductivityB ON MEProductivityA.MonthGroup = MEProductivityB.MonthGroup Based on your desired output, it looks like you have more than one Employee per MonthGroup - try Joining on both the month group AND the Employee in that last query 
PHP or Python come to mind. Or you could probably create a stored procedure in MySQL to accomplish this.
Thanks!
exams for $150/ea of course, it depends how many times you need to take them before you PASS :)
 SELECT COUNT(columnOne) columnOneCount, COUNT(columnTwo) columnTwoCount FROM myTable;
This is awesome! I didn't know you put AND in there to combine innerjoins... Well it mostly works now. It's returning a slightly different number of minutes, but it's kinda close. I will work more and come back if I get stuck.
I think everyone gets *halfway* to solving your problem, but the actual solution can differ depending on what you need to do. **Case 1: You only care about the most recent value** Keep the MessageLog table for audit purposes only. Add Description and an UpdateDatetime column to the Message table. Simple, easy. **Case 2: You do care about other values and/or only want to make one insert to update the status** (instead of an update then insert). In this case, don't store the status on the Message table. Query the MessageLog table for the status you need. **Both solutions will require some changes to your existing schema:** 1. Do not use "Timestamp" as a column name as it is a reserved keyword in several RDBMS's and can have a completely different meaning as well. Let's use UpdateDatetime instead. 2. "Status" is also a reserved keyword, so "MessageStatus" is probably more appropriate. 3. Get rid of the MsgLogId column. It is a row identifier, a waste of space, and a terrible clustered index. 4. Your primary key is MsgId, UpdateDatetime - use it as such. Now we can do things the easy way: SELECT LOG.MsgId ,LOG.UpdateDatetime ,LOG.Status ,LOG.Description FROM MessageLog LOG WHERE LOG.UpdateDatetime = ( SELECT MAX(sub.UpdateDatetime) --Replace with MIN if you need the first status. FROM MessageLog sub WHERE sub.MsgId = LOG.MsgId ) We're using the clustered index, so things work fantastically. Also, because we have the right index we can take advantage of it when using windowing functions. Normally a windowing function has to sort the data, but no sorting is required if we use the clustered index for our partition and sorting criteria: LAG(MessageStatus) OVER (PARTITION BY MsgId ORDER BY UpdateDatetime) This [SQL Fiddle](http://sqlfiddle.com/#!6/84665/1) should illustrate everything nicely. There is one caveat when using a non-sequential clustered index, and that is page splits. Because you'll be potentially inserting records into pages that are already full, the existing rows will need to be shifted to accommodate the entry. If this gets to be an issue, you can decrease the fill factor of the clustered index to provide room for new rows.
That seems to be a trait within the sector, the place I worked was the exact same, we had data in lots of different systems, by trying to actually do anything with it was a nightmare! Out of interest, where are you? I'm in the UK.
Hi PStyleZ, Thanks, that helps--I'm now able to get two tabs. However, I think I'm using an older version so I do not have the page name option... but other users in my organization (who I'm assuming are on the same version) have been able to create RDLs with custom names for the worksheets. I'm on Visual Studio 2008 and using SQL Server Business Intelligence Development Studio, so I'm not sure why I don't have those options.. but they are not appearing. Any ideas?
I don't get why those offset functions are dumped in the middle of a post that's supposedly about windowing functions. It's like this is just a dump of documentation from somewhere with little to no editing.
Anyone you would recommend? Online vs in person?
It's hard to say without checking the execution plan. It could be comparing every possible set of rows before filtering to sensor1 not NULL, or it could skip NULL sensor1 before performing the join. Either way, that's just how SQL works. You say what you want, and the DBMS will figure out the way it thinks is best. As long as you're getting the result you need, and it's not taking too long for you, you don't need to fiddle with it. Or someone call me out if I'm spouting nonsense.
Your best bet is to look at the execution plan and look for a full table scan, which will take the highest percentage of resources. If you are familiar with indexes, build one, otherwise, run the optimization plan on a regular basis and let SQL determine the best way to execute. Also, read Brent Ozar's blog and search 'missing index'
Additionally, you can do a count and group by (not order by) and test performance. I'm not 100% sure why, but this has been incredibly effective for me in the past.
Please recommend a reputable source instead of some junk WP site a 13yo built.
You may need to check your syntax, which I mentioned above is non-standard in many cases.
Open your report in report builder. Make sure the tablix you are selecting has a parent group. The available options change depending on the object type, i.e. if you had selected a text box instead. Let me know if that's not clear? 
Take a look at sqitch: http://sqitch.org/
Great, thanks for the info!
Thanks for the advice. I actually have a number of conditions in the actual WHERE clause but I'll see if I can throw them all into the join. Thanks!
One vote for liquibase here
I'm not really sure, we use asset management software like Maximo and oracle too, and I can only access the database when in a specialized desktop through Citrix. I don't know if that changes things but I looked it up and you're right, 2Gb is the cap for Access so there must be some tom foolery there. This site is super helpful though I appreciate it. W3 Schools was alright but I was spinning my wheels there pretty quickly. 
You know what's interesting is there are no standards for hydraulic modeling in the U.S. but there are in the UK. So whenever I go through a round of calibration I always use the WRc's (might not be called that anymore) standards as a sensible goal for calibrating the system. The UK takes modeling more seriously than we do I suppose.
Came accross an underlying issue when trying to add only 1 of the columns to the table...When i would try to execute the SP after adding the column ( The column is just Zip/Postal codes, it's a VARCHAR(10) ) to the Create Table statement and the select statements, i would the get the error : String or Binary would be Truncated. [SQLSTATE 22001] (ERROR 8152) Null value is eliminated by an aggregate or other SET operation.(ERROR 8153)
hey thanks a lot for this really appreciate it.
Does your EventDate include time? could you filter on that between the dates of 'Alarm Zone Armed Away' and 'Alarm Zone Disarmed'?
Exactly. And the LEFT JOIN allows it to return data from the source table even if they don't exist in the joining tables.
[Red Gate](https://www.red-gate.com/products/oracle-development/source-control-for-oracle/). 
This is dirty and nasty and it was a ton of fun to solve. Here's a recreation of your problem with a working solution: http://pastebin.com/DbAqCcFx Thanks for the brain teaser :)
oh, okay. I didn't quite understand what it was you were doing. would an aggregate function like `group by employee_id` work?
**ISSUE RESOLVED** Was a measly column index order in the create table...Made sure the index order in the Create table part of the SP was the same order as the columns in the select statement and everything worked like a charm after that.
As someone else said you could start with MTA. It is 115 and only have to take one exam vs the 3 exams for 450 for the MSCA. Start basic and work your way up. That is actually exactly what I am doing.
I'm glad you had fun! I'm gonna look over your code and see what you came up with :) I forgot to mention that I'm using SQL Server MSE 2005, so I'm not able to do multi inserts. I'll take a crack at breaking them into single insert into's, then use union all, though. I'll let you know how it goes! 
&gt; filtering between them Sorry, I guess I'm still not understanding what exactly you're trying to do. You want to get all the times any employee comes in and leaves after hours, so what's there to filter?
I want to grab the Date/Time, Name, Code, Event Description and Event Location of every event that occurs after hours. So, if the building is armed, and someone opens a door, I'd want that recorded, but not any events that occurred while the building was disarmed. I found a table that may hold more of the info I'm wanting, so I'm parsing through that, real quick.
Unless you know a specific flavour of SQL that will give you the best chance of landing a job (Oracle, MySQL, Postgres) I'd suggest going with Microsoft SQL Server. You could start by: - downloading [SQL Server Express](https://www.microsoft.com/en-gb/server-cloud/products/sql-server-editions/sql-server-express.aspx). Make sure you install management tools - SQL Server Management Studio (SSMS) is the software you'll use for writing and executing queries. - downloading and installing the [AdventureWorks sample database](http://msftdbprodsamples.codeplex.com/releases/view/93587) so you have some data to play with - do the http://sqlzoo.net/ exercises - design and create your own basic database e.g. for an online shop. Plenty of examples out there. Then you can say "yes, I have experience using SSMS to query and manipulate data in SQL Server 2014 databases using T-SQL". You can definitely get to a basic proficiency in a few months if you have time and dedication - but you learn so much more on the job, especially from senior developers and DBAs, that the sooner you can get to work with them the better. Good luck!
That's... Why didn't I think of that? I'm not entirely sure how to go about that, though. Maybe something like if EventDescriptionID = '10' set @eventStart = (SELECT EventDate FROM Events) if EventDescriptionID = '14' set @eventEnd = (SELECT EventDate FROM Events) where EventDate between @eventStart AND @eventEnd With the select statement prior to the where clause?
&gt;I wasn't selecting them directly because I didn't know if adding a new table to this database would cause any issues with its stability. The query doesn't add a new table, its just a query. Whereas literally at the top of your pastebin, you're adding a temp table. How many rows are we talking? You're gonna want some kind of Declare @day datetime Set @day = (first of last month code above) While @day &lt; (end of last month) begin Delete rows from #temp where day(EventDate) = @day and EventTime &lt; (select EventTime from # temp where day(EventDate) = @day and event='Armed') - - same where time greater than disarmed Set @day = @day +1 End
I'm not sure either. I'm more of a query person. Honestly, with a problem like this, you can spend 15 hours designing/optimizing a query or you can spend 15 minutes writing a simple program to process the result set of a simpler query to give you what you want. In SQL, I would just pull all the relevant data (joined appropriately) and order it by date/time. Then in some other language, I would skip over any result rows after an after-hours alarm disable. Then I would have a discussion with your DB engineers about adding a column to indicate whether a door was opened during a period when the alarm was disabled so you could make it much simpler.
Honestly, that's probably what I'm going to do. I'll export it as a TXT or REPORT file, then filter from there using something with a bit more combustibility. Thanks a ton for your help :)
I'm gonna be honest, I completely forgot I was making a table in my pastebin. It's been a long day :P
Is report builder different from the general environment where I'm building and creating a report in BIDS? If not, I'm still way confused. Let me elaborate--I'm trying to get a report showing one tab of active clients and one tab of suspended. I've built a report to return their first name, last name, ID, state, and county. I also have a column being returned in the query telling me if they are active or not, and I'd like each results set to show up on a different tab when exported to excel. However, I'm not seeing a number of the options I should be seeing according to [the pagination reference linked from the stack overflow article](https://technet.microsoft.com/en-us/library/dd255278.aspx). I didn't have a parent group, but I assumed that would need to be the "active" column, so I added it, then deleted the column but kept the group. None of these selections have the options they should according to the site. If I select report and view properties, no InitialPageName is available; there's not even a name field in the 'page' section, or anywhere else. I'm also not seeing the Disabled, ResetPageNumber, or PageName properties. When I select the tablix as a whole, no group options, but I see a pageBreak at least under General. When I select tablix members/parent groups, I get page break options in the group section, and setting it to between for the parent group has given me the two separate tabs when I export. However, no name options exist. Weirdly, too, my column headers go missing on the page break. I suppose to fix the headers issue, I could create two tables driven by two separate datasets and set the pagebreak to be at the end of the first tablix, but the name problem would remain an issue. Any ideas? Could it be a permissions issue? Oh, and thanks for trying to assist!
First off, at least based on the query you provided, you don't need to join to `record_set`. Notice that your first query would return the exact same results as: SELECT id AS record_id, issurvivor, record_set_id AS set_id FROM record The only reason you'd need to join to `record_set` is to limit only to `record_set_id`s that are in that table. ____ Going off your screenshots, you want to duplicate the "survivor" row, and order them as "survivor, retiree, survivor, retiree", correct? In my opinion, the better way to show the relation between a Survivor and its two Retirees would be to display them all on the same line: set_id | survivor_id | retiree_1_id | retiree_2_id ------|-----------|------------|------------ 10001 | 1 | 2 | 3 To do that, you'd need to "self join" `record` three times: SELECT s.record_set_id, s.id AS survivor_id, r1.id AS retiree_1_id, r2.id AS retiree_2_id FROM record s JOIN record r1 ON s.record_set_id = r1.record_set_id JOIN record r2 ON s.record_set_id = r2.record_set_id WHERE s.issurvivor = 'Survivor' AND r1.issurvivor = 'Retiree' AND r2.issurvivor = 'Retiree' AND r1.id &lt; r2.id -- this keeps it from displaying the same id for both retireees ; If you insist on the "survivor, retiree, survivor, retiree" structure, it gets a lot more complicated: SELECT id, issurvivor, record_set_id FROM ( SELECT *, ROW_NUMBER() OVER (PARTITION BY record_set_id ORDER BY issurvivor DESC) rnk FROM record UNION ALL SELECT *, 3 rnk FROM record WHERE issurvivor = 'Survivor' ) c ORDER BY record_set_id, rnk, issurvivor DESC ; 1. Inside the subquery, we select all the records and rank them from 1 to 3 within a record_set_id. The Survivor gets rnk = 1 and the Retirees are assigned rnk = 2 and 3 randomly. 2. Then, we UNION back to only the Survivors and assign them a rnk = 3. That way, have "survivor, retiree, survivor, retiree" with ranks 1,2,3,3, respectively. From there, it's a simple matter of ordering. Again, I'd highly recommend the first method over the second. When you're first starting out, it's best to think of each row of data as completely independent of any others. This doesn't always have to be true, but it's a good rule to follow until you know enough to break it. SQLFiddle test data: http://sqlfiddle.com/#!15/70b72/3 
I'm assuming I'm working in report builder. Basically, if I open a rptproj file/RDL file in Business Intelligence Development Studio, I see something that looks like the typical SSRS environment. I wish I could share screenshots, but as you guessed it, any hosting site is going to be blocked here. Anyways, if the official term for the regular SSRS environment is the report builder, that's where I believe I am, because I'm building reports here, and a lot of the fields and things seem to match--it's just a lot of them are missing. I kind of wonder if it's because the actual report scheduling and dissemination is handled by another member, so maybe it is just a restriction. I'll check up on that and send my DBA what I have so far just to see if that's the case. Thanks again for all your help--may have more for you tomorrow!
no problem. you should consider using a more structured file format like csv.
You can alter tables and add columns. The update them with a value. Ex. Alter table YourTable Add ColumnName varchar(50) Update YourTable Set ColumnName = case when Blah then Blah else Blah End
Pm if interested. I'm self taught but don't mind showing the ropes. I'm also used to remote work and training.
I can see how it does your head in - but there is a way out! C is definitely the right approach here. Abstract logic/pesudacode is: 1) Starting with first field check if the first character is " 1a) ---NO PATH--- 1a1) the next comma you come across will be the delimiter for the field. Split to next column on that. 1b) ---YES PATH---- 1b1) detect the row end by having [x]", where [x] is any character that is NOT " (e.g. 5", valid but not 5"",) (try a regex expression) 1b2) Now you have end of row, strip first and last character (which is ") &amp; replace double "" with single " to parse true values in the middle. At least I think that would work :) Some tips on regex to get started: http://stackoverflow.com/questions/13441952/regex-only-match-if-character-is-not-included-directly-before-desired-string
This is correct. OP - you'll find there are multiple ways to achieve the same thing in SQL, each with pros and cons under different scenarios. 
I work in data analysis for an insurance broker. We have monthly reports that go out to clients. Some are fine, but we have a few with over 250k rows of data with about 20 columns each on the back-end that are being pulled through with SUMIFS functions to the front end of the report that will then be shown to the client. We work on a cloud and each user is allocated two cores from the processors. I can't even open the damn file in under 5 minutes. Then I have to turn off formulas until I've made all my changes or it will lock up on every cell. I'm learning SQL so I can find a better way to handle reports like this one and take it to my boss and try to get leverage to move some of our nastier reports over to query from an actual database...
I don't really understand the issue or what you're trying to do. are you mapping users to another table? I don't think there's a way to dynamically set a from clause in a query. I think you'd be better served working with result sets in a higher level language.
And this is why our company's file layout guide for our customers suggests using pipes instead of commas for files that are delivered to us. I think our old solution was to either check to see if the " had a comma on either side or was the first character or the last via a Script Task and clean up any other "... Its almost easier to simply open it with excel, change the character used from a comma to a pipe, reexport then load.
Sorry for formatting. SELECT * FROM mytable WHERE date_field &gt;= '20160101' AND status = 'Not catalogued' is the query and GROUP BY item_id ORDER BY quality LIMIT 1 ...is what I'm thinking I might have to add to get the desired result
You can do this with the ROW_NUMBER() function or a self-join: The Row_Number assigns a row number within each group specified by PARTITION BY. In this case, "HD" comes before "SD" alphabetically, so it will be given row_num = 1. You than just select the first record from each group. SELECT title_id, quality FROM ( SELECT * ,ROW_NUMBER() OVER (PARTITION BY title_id ORDER BY quality) AS [row_num] FROM my_table ) AS [a] WHERE a.row_num = 1 The self join makes an HD version of the table, and an SD version of the table and joins them. The COALESCE function shows 'HD' if it's in the [hd] table, or defaults to the 'SD' from the [sd] table. SELECT title_id, COALESCE(hd.quality, sd.quality) FROM my_table AS [sd] LEFT JOIN my_table AS [hd] ON sd.item_id = hd.item_id WHERE sd.quality = 'SD' AND hd.quality = 'HD' 
Cheers, I'll give them both a go
I don't want to give the actual tables we are using. Any idea how I can knock something better together?
Sorry i don't remember exactly what that notation is called. Does it matter? What's not clear from my diagrams? There's hundreds of tables within the schema and a lot of them, especially the ones in question, have a lot of columns.
I don't see any issues syntactically, but this won't work in MySQL. MySQL doesn't have a ROW_NUMBER() function. That's only in Oracle PL/SQL.
Use a replace on the join FROM table1 LEFT JOIN table2 ON replace(table2.SurveyID,'MAX','') = table1.survey
Makes sense - more dynamic -thank you for the response!
I did your examples in the latest SSDT for 2014 package and it worked fine. Maybe upgrade to SSDT and try that? https://imgur.com/a/DSBPd Edit: Yeah looks like it was a [bug in previous versions](https://social.msdn.microsoft.com/Forums/sqlserver/en-US/a1cea026-545d-4548-9358-97a56fca90f2/ssis-bug-when-handling-csv-files-with-commas-embedded-in-quoted-strings?forum=sqlintegrationservices).
You could do a substring, although a replace, especially in this case is easier. If I needed to use a substring here for whatever reason, I would do SUBSTRING(SurveyID,3,LEN(SurveyID)) or a RIGHT like the commenter below said would work as well RIGHT(SurveyID,LEN(SurveyID) - 3) Might need to cast different data types but they should all work the same.
Yeah that's the biggest issue I could think of. You'd have to go through your InfoSec department because I'm sure all of the data would have to be encrypted properly and security measures implemented. Unfortunately I wasn't involved in that process so that's where my knowledge kind of ends, but we have clients involved in healthcare and yes it was a very rigorous process to get all of our data (in MSSQL) how they wanted it. It was worth it in the end. Best of luck!
 SELECT sametable.urefid1 FROM ( SELECT a.urefid , a.jobstart , MAX(CASE WHEN a.recordchangetime &gt; a.recordcreatetime THEN a.recordchangetime ELSE a.recordcreatetime END) AS latest_time FROM ( SELECT urefid , max(jobstart) AS latest_start FROM sametable GROUP BY urefid ) AS sub2 INNER JOIN sametable as a ON a.urefid = sub2.urefid AND a.jobstart = sub2.latest_start ) AS sub1 INNER JOIN sametable ON sametable.urefid = sub1.urefid AND sametable.jobstart = sub1.jobstart AND ( sametable.recordchangetime = sub1.latest_time OR sametable.recordcreatetime = sub1.latest_time ) 
do you know if this will work with SQL server 2008 R2? 
See my comment history for VBA
you're asking a company politics question me, i would love a reporting role... the challenge is to deliver what they want, when they need it, and make it as accurate as possible the secret is to show them where it's coming from i find diagrams help
Can *you* concentrate it? Instead of writing six different scripts for six different programs, trying warehousing the data and write only one script for one program. If the data required isn't realtime, it might be worth the extra effort to simplify your reporting.
This is the right answer. Create a warehouse or ODS (operational data store) database that gets data ETLd into it on a periodic basis (based on user need and source system data update rates). Right all your reporting and dashboard scripts against the warehouse/ODS. This means, in your case, that you have 6 ETL processes that you need to create to populate the ODS. Each of them can be tested independent of the other's so you can track down bad data (like your extra month). In the ODS tables, include columns for the following metadata to help with troubleshooting: * source system - what ETL process created this record * ETL date - when was this record created in the ODS * source create date - when was the record created in the source system * source update date - when was the record last updated in the source system
data--&gt;from other sources--&gt;ODBC makes it super easy depending on your set up
I am going to assume after a match is over it cannot change. What may be worth while is to compute everything and preserve it into a table or use a materialized view. Then when it comes times to query you select the precomputed data and then union that to data which has not been computed yet. This would be a substantially smaller result set and should perform quickly enough. When looking for fast speeds on the web I try to precompute and store as much as possible since disk space is so cheap.
It is the same test. Only difference is online someone is watching you via web cam and in person someone is watching you
Figured it out. Thanks for the push in the right direction.
There is generally never a reason to archive data when there are so few rows. If you look at the query plan, there are too many rows in that scan (#5) The query they pasted also seems to be missing a qualifier on highestachievedseasontier which is indicated in the query plan.
Short answer: Yes, Microsoft Certification can get your foot in the door. What was your graduate studies in? And when you say SQL Developer, what specifically is your interest? Are you interested in designing the software that will communicate with the database (there is a huge, HUGE demand for people who can do this correctly)? Are you interested in developing SSIS/SSRS packages, along with database tuning and optimization of the server? I'd suggest applying for junior dba/sql developer positions, while pursuing certification. Until you get your certs under your belt, I would highly suggest going and becoming an analyst to practice your SQL in a work environment.
Sure. It's pretty common for a beginning SQL developer to think in terms of rows and columns. Unfortunately, this usually results in handling SQL tables like 2-dimensional arrays, iterating through and doing individual processing against each row. (I've seen it happen.) SQL works best when you stop thinking in terms of rows and columns, and start thinking in terms of sets of entities which have defining attributes. Each "row" is an entity, which is identified by a key value. then, there are additional attributes, ("columns") which hold information that describes the entity in question. Customer Address Table| CustomerId (Key)| Name (Attribute)| City (Attribute)| State (Attribute)| Zip (Attribute) ---|---|----|----|----|---- **Entity**| Cust1| Acme Co| Richmond| VA| 23218 **Entity**| Cust2| Emca Inc| Houston| TX| 77019 So when you do something as simple as SELECT * FROM CustomerAddress WHERE State = 'VA' you are extracting a set of entities (Customer Addresses) that have the same State attribute value of "VA." This is mathematically considered a "set" of objects. So to take that a step further, you can take two sets and join them together. If you want to know the total value of all orders shipped to companies in Virginia, you might do something like this: SELECT SUM (Orders.Qty * Orders.UnitCost) FROM Orders INNER JOIN CustomerAddress ON Orders.Customer = CustomerAddress.Customer WHERE CustomerAddress.State = 'VA' We have two sets here. A set of all orders, and a set of all customers in Virginia. If we combine the two sets, we get a set of all orders that were placed for customers in Virginia. We do a calculation to combine two attributes (Qty and Unit Cost), and then SUM them up into a single number. That gives us total cost for all orders we sent to Virginia. In a procedural world, we would have to do something like loop through and check each order to see if it was a Virginian customer. If they were, we would have to multiply the two attributes, and stuff the result into a subtotal variable somewhere. It seems like a small thing, but when you get into more complicated data sets (think recursion), working with sets of data instead of row-by-row is almost always faster.
This was a bug. It may be fixed in ctp 3. You could upgrade to that or switch to 2014 express to learn on. There is also a work around mentioned there. You could log in with SQL auth (e.g.: "sa" and the password you created at install).
I've tried updating to see if there was a fix, but that was a whole other fiasco. I may just take your advice and download 2014 and start over. Thanks for the input.
Sorry I edited, just as you replied. Try SQL authentication to do your diagrams. It should work.
It was originally set up to log in as the current account user (me), so I'm guessing I would use my windows account password? I'll give it a shot. Thanks. Edit: HOLY SHIT that worked! Thank you, I appreciate your help!
Correct.
http://dev.mysql.com/doc/refman/5.0/en/string-functions.html#function_insert Not to be confused with inserting into a table, this inserts into a string. so you can use it whenever your string is below 7 characters to add a 0 after ARCH. The less elegant solution is to mess with substring and concat.
This will likely serve you best if you need your numbers to be fixed length (i.e., with leading zeroes): http://stackoverflow.com/questions/11165104/adding-a-leading-zero-to-some
This is good but I keep getting the following error and I can't figure out where to group it at. Column 'sametable.urefid' is invalid in the select list because it is not contained in either an aggregate function or the GROUP BY clause.
This is crazy as I have never seen a "with ordered as". Any idea how I can include this in a SQL update statement to flag the "correct" correct? 
These are definitely the best resources. * Under $50 * Way more in depth than is required for the basics; but the solid foundation for the Microsoft trick questions. * Sample questions give you a good idea of what the test is going to be like. 
Nice and clean.
You're right about the relationship. I guess I was thinking about my particular usage when I typed that out. 
please show your query sql 
sorry here you go SELECT Employees.EmpCode, Projects.ProjectPath as Code, Sum(TimeSheetItems.WorkHrs) as 'Total Hours' FROM Employees INNER JOIN EmployeeUDFS ON Employees.EmpID = EmployeeUDFS.EmpID INNER JOIN TimeSheets ON Employees.EmpID = TimeSheets.EmpID INNER JOIN TimeSheetItems ON TimeSheets.TimeID = TimeSheetItems.TimeID INNER JOIN Projects ON TimeSheetItems.ProjectID = Projects.ProjectID INNER JOIN TimeSheetOriginals ON TimeSheets.TimeID = TimeSheetOriginals.TimeID AND Projects.ProjectID = TimeSheetOriginals.ProjectID Where Employees.EmpCode = 'JOHNDOE' and ProjectPath = 'V' and Year(TimeSheetOriginals.WorkDate) = '2015' Group by Employees.EmpCode, Projects.ProjectPath, TimeSheetItems.WorkHrs Order by ProjectPath 
Or just account for that fact and calculate the LEN() at the same time you perform a RIGHT(). Would look like FROM table1 LEFT JOIN table2 ON table1.survey = RIGHT(table2.SurveyID,LEN(table2.SurveyID)-3) Just pointing out that you can handle the variable length nature of an autoincrementing field in a similar fashion. Replace would likely still scale better though if any other affixes were ever identified as you could just add them to the replace. Edit: Just read down the thread a bit, /u/LeaveMyBrainAlone suggested exactly this... so uhm.... what they said. Edit2: Didn't realize something this far up on /r/SQL was a couple days old. Apologies ;\
AKA intersection table.
Just want to note that in the schema building section of your code, you entered 8 twice for each date. It should only be 8 total for each date. I received this error message using sqlfiddle You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'nvarchar(1), TotalHours decimal(8,2), WorkDate datetime ) insert i' at line 2
not that it should make a difference, but your original post said TSQL/MSSQL, and that's a mysql error
Hi PStyleZ, That's actually exactly what I did. Got on the horn with IT. Turns out for some reason, all the other Devs had 2008 R2 and I didn't. WOMP WOMP. So much trouble for one little missing package! Thanks again for all your help. If I could upvote you more than once I would. You're super friendly and willing to assist--not something you find often in non-technical subreddits ;)
The double entry was intentional and was meant to highlight exactly what your current issue is. See how many entries the code returned? Edit: Try this with your code... remove the group by and the SUM() and see what is returned. I'm willing to bet that there are 4 rows almost identical to what I had in my example.
Will do, presuming I can :)
Ok, gotcha. So it's possible to have: UREF|JobStart|RecordCreateTime|RecordUpdateTime| :--|:--|:--|:--| 1|2015-01-25|2015-01-25 08:30:00|2015-01-25 9:30:00| 1|2015-01-25|2015-01-25-09:00:00|2015-01-25 9:15:00| Without a refactoring, you'll need to use the solution /u/roveo put together. If you have the ability to redo the table, let me know and I'll show you how to do it in a manner that avoids the need for such complex queries. Roveo's query gets you the right result, but such operations require the table be sorted. If the table is small, probably not an issue.
Got it. 
At the risk of being pedantic: mirroring is not the same as having a backup. Mirroring will roughly double your disk usage.
Hmm i thought mirroring will let me have an exact copy of my database in another server that i can use if my principal fails? But if it will double my disk usage then is my best choice log shipping? About how much will log shipping use up in terms of disk usage? 
Mirroring will use roughly the same amount of storage as is required for the original set. I say roughly because there may be some overhead - housekeeping data, or some such - but also because there may be some optimisation possible in the mirroring process. But roughly the same storage requirement as the original data set anyway. As for a mirror not being a backup, it might sound like pedantry but it's similar to the many discussions elsewhere about how RAID is not a backup. Certainly a mirror provides resilience against hardware failure, but think about data being corrupted at application level, or deleted by a user, or whatever. You can recover these failures with backups, but not from a mirror. I really don't know how log shipping in MS SQL works, I'm not an MS SQL guy. 
Yeah I used my phone as a hotspot so my mobile was connected to 4G and it let my pc connect.. then I turned my wifi back on computer and i'm getting the error about connection timed out
You might check your joins as they may be replicating data. Easiest way to check is just to select everything and identify what is happening with the joins. Once you identify what is causing the multiple rows, you can address that.
Unfortunately the table was created long before me and I cannot make any changes. Thanks for the help.
I can see needing to denormalize when working on a Warehouse or even Mart platform. I would prefer to work with a handful of powerful tables rather than one large fact table. Normalized tables can be abused(FIT IN ALL THE COLUMNS!), especially if the person building them is not RDBMS inclined or used to working with just spreadsheets.
The other way around, it's 100 chairs from wh2 -&gt; wh1. Warehouse 1 has orders for 970, but only 870 in stock. I need the output to be something like this: id | quantity | product_option_id | other_line_items_total ---------|----------|----------|---------- 1 | 900 | 2 | 70 | 2 | 70 | 2 | 900 | If I can get this working correctly, I can ALSO join the warehouses tables and compare each line item to the available inventory in other warehouses to predict transfers, like this: id | quantity | product_option_id | other_line_items_total | warehouse_1_inventory | warehouse_2_inventory ---------|----------|----------|----------|---------|------- 1 | 900 | 2 | 70 | 870 | 108 2 | 70 | 2 | 900 | 870 | 108 I suppose you could take it a step further and do another calculation grouped by product_option_id but I need the line item ids as well and that would condense the query down too much.
I think you're missing a GROUP BY in your subselect. GROUP BY product_option_id, order_line_items.id, order_id
A single column index is an index consisting of a single column. A composite index is an index consisting of more than one column. What values you want to be unique determines which you should use. Do you want the slug to be unique and the display_position to be unique, or do you want the combination (composite) of the two to be unique?
Agreed, although I am not sure what the source columns were. The best way to deal with any date is to use the correct data type, lol.
Like the local catholic priest you gotta hammer it into the newbies while they are still young and impressionable
The source column is RECORDED_DATE and the data is stored in that column as shown in the post. Please disregard if I misinterpreted what you meant.
I think you should start by building a running total from the current date based on orders received. Sometheng like with demand as ( select prod_id, warehouse_id, date, sum(qty)over(partition by prod_id order by date) as cum_sum ) select stuff
That sounds like the default port for MySQL 3306, for SQL Server it would be 1433 default. Can you ping the server you are trying to connect to? How about telnet w/port ?
I'll try this tomorrow. Thanks for the follow up.
Can I ask why you want date and time split into different columns? Most people would want to keep the date and time values stored together in the database (in a "datetime" column), then only separate them out when needed, like when you're working on a particular type of report. /* store it as a datetime */ declare @MyTable table (OrderID int, OrderDate datetime); insert into @MyTable (OrderID, OrderDate) values (1, getdate()); /* separate it when you need to */ select convert(date, t.OrderDate) JustTheDate, convert(time, t.OrderDate) JustTheTime from @MyTable t; /* https://msdn.microsoft.com/en-us/library/ms187928.aspx */ select convert(varchar(8), t.OrderDate, 114) TimeAsVarChar from @MyTable t; 
It's also been referred to as a "pivot" table. Although I've only seen that naming in the PHP/Laravel ecosystem.
Are you actually trying to connect INSTANCE and not just a SQL server? If you have set up an instance your connection string is "COMPUTER_NAME\INSTANCE_NAME". Verify you can ping the FQDN of the SQL server. Telnet on port 1433 to confirm you can establish a connection.
Thanks for the advice, much appreciated! Especially the SSMS advice, this is exactly what I was looking for in the last part of Q5.
Totally taking you up on this, check your inbox - thanks!
Use [] to denote a column name, like tablea.[itemlist]. 
there is a reason 1NF was invented, to discourage people from putting more than one value inside a column SELECT a.* , b.itemdesc FROM tablea AS a LEFT JOIN tableb AS b ON ',' + a.itemlist + ',' LIKE '%,' + b.itemid + ',%' this will work but the performance will suck majorly
Oh man I know, I am not for this practice either. I had to use this in a similar query recently, but with @parameters. I have attempted your versions but it is getting hung up on the percent signs: *Conversion failed when converting the varchar value '%,' to data type int.*
What version of MS SQL? &gt; To add to this, I am having to do this in a sqldatasource in VS2010. Never used it but couldn't you use it to first get the CSV data into a variable and pass that variable to the WHERE IN as a second statement?
 ON ',' + a.itemlist + ',' LIKE '%,' + CAST(b.itemid AS VARCHAR) + ',%' 
I've been chunking on it for a while and I think I may do something similar: My tentative plan is: - Keep (as you said) the canonical version of the database (ddl + seed dml) in one location. - Add a separate location that actually has scripts, organized by labeled feature/release, that can be executed by the release automation (you know, when I get to "release automation" and continuous integration in my spare time.) That way at least I'll keep things separate, and if I have edits to the "users" table structure I don't have to blow away the whole thing every time. I think the next thing down the list is deciding what constitutes seed data. I mean, I'd like to keep my data in the database, rather than source control, but that may just be naive. 
I'd only want to have lookup tables in source control. Everything else should go in a database backup. Backup/restore is optimized for faster recovery than just insert/update/delete (typically).
Well THAT'S interesting. Thanks for the pointer. I'm gonna check that out. 
You need to first transpose the comma-delimited list into a set of rows, and then join or filter on that: create table #TableA (itemList varchar(max)); create table #TableB (itemID int, itemDesc varchar(255)); insert #TableA values ('1,2,3'), ('8,9'); insert #TableB values (1, 'Item 1'), (2, 'Item 2'), (8, 'Item 8'), (11, 'Item 11'), (101, 'Item 101'); SELECT b.itemDesc FROM #TableB b WHERE b.itemID IN ( SELECT Split.a.value('.', 'INT') AS ParsedA FROM ( SELECT CAST ('&lt;X&gt;' + REPLACE(itemList, ',', '&lt;/X&gt;&lt;X&gt;') + '&lt;/X&gt;' AS XML) AS xmlItemlist FROM #TableA) AS Transposed CROSS APPLY Transposed.xmlItemlist.nodes ('/X') AS Split(a)); Basically, you are first turning the comma-delimited lists in each row of TableA into valid XML by doing `CAST ('&lt;X&gt;' + REPLACE(itemList, ',', '&lt;/X&gt;&lt;X&gt;') + '&lt;/X&gt;' AS XML)` (this assumes there are no spaces around the comma - edit this line if there are.) Then you are performing an XML transform with `CROSS APPLY Transposed.xmlItemlist.nodes ('/X') AS Split(a))`, and then finally using XPath you're pulling out the values from each XML node with `Split.a.value('.', 'INT') AS ParsedA`. Wrap this into a WHERE clause, or make it a derived table for the JOIN condition as required. On large tables performance will be bad, because XML stream is processed in small chunks. See [this article](http://beyondrelational.com/modules/2/blogs/114/posts/14617/delimited-string-tennis-anyone.aspx) for the details. TL;DR version is to pull out the XML into a variable, build a table with transposed values separately, and use that table in subsequent queries.
OR is there a way to use the BETWEEN function for two columns that are VarChar that are set up like dates. Example the column is Date and type VarChar and I want a WHERE Date Between '01-10-2016' and '01-15-2016';
 with letters as ( select userid , case when count(distinct letter) = 1 then 'Pure ' || max(letter) else 'Mixed' end as user_class , count(distinct letter) as letter_count from table group by userid ) select userid, user_class, letter_count from letters where user_class = 'Pure A'; -- here you choose your group || is string concatenation in Postgres, you can replace if with whichever your DB engine uses. Also this query handles cases when you suddenly have a C, D and E added to you letters. If you're using MySQL (poor you), replace CTE with a subquery: select userid, user_class, letter_count from ( select userid , case when count(distinct letter) = 1 then 'Pure ' || max(letter) else 'Mixed' end as user_class , count(distinct letter) as letter_count from table group by userid ) as letters where user_class = 'Pure A'; -- here you choose your group
 with letters as ( select userid, count(distinct letter) as distinct_letters, min(letter) as min_letter group by userid from your_table ) select distinct 'Pure A' as label, t.userid, letter from your_table t inner join letters l on l.userid = t.userid where l.distinct_letters = 1 and min_letter = 'A' union all select distinct 'Pure B' as label, t.userid, letter from your_table t inner join letters l on l.userid = t.userid where l.distinct_letters = 1 and min_letter = 'B' union all select distinct 'Mixed A and B' as label, t.userid, letter from your_table t inner join letters l on l.userid = t.userid where l.distinct_letters = 2 
yes, with caution, because i think mysql will let you update columns directly but me, i would never trust myself... so i would alter the table, add a date or datetime column, then update the table and set the new column to the function on the old column, then CHECK MY WORK, and then alter the table to drop the old column, then alter the table again if necessary to rename the new column to the old column's name
If you're not opposed to buying a commercial product, Red Gate's SQL Source Control is pretty good. I've used it on projects with a dozen people with good results. The main problem we had was certain people just not bothering to check their database changes in. You can work directly in the database and detects the changes you've made. It uses their Schema Compare engine for deployments and it's pretty smart. The only place it has problems is non-nullable columns with no default since it can't possibly know what you want in there. You have the option of writing scripts to run at different points in the deployment but I would usually just make those changes manually before doing the deployment.
Unfortunately at this stage I'm pretty much tied to "however I want to set up svn." But people keep raving about Red Gate.
No MSSQL support :(
"\\\\prdfilsrv01\\department\\bp\\vendor\\loadfile_"+ (DT_STR,4,1252)DATEPART( "yyyy" , getdate() ) + RIGHT("0" + (DT_STR,4,1252)DATEPART( "mm" , getdate() ), 2) + RIGHT("0" + (DT_STR,4,1252)DATEPART( "dd" , getdate() ), 2) +".txt" Output: \\prdfilsrv01\department\bp\vendor\loadfile_20160127.txt
Gold in the morning if it checks out. That's crazy looking.
Lazy coders
So basically I want to remove all "groups" of "code" that are blank in result. So if a number in "code" is blank in result, it would filter it out. The thing is though, I want this to apply to the entire groups. So if an entire group (say all the 253s) are blank, than it filters them all out, but if even a single one of them has a number. It would keep it.
I don't see how casting to a varchar makes it easier to do use a between filter. Any reporting solution worth its salt should be able to format dates however you want them too. But good on you for living up to your username!
Thanks
&gt;&gt; What I'd like to know is if there are any good reasons for storing dates as varchar rather than date or datetime. &gt;no Agreed. Date time data types exist for a reason. Use them.
A truncate command also resets the identity value of a table. Obviously it nukes all the data in the table as well.
Errr..If I'm reading this right then you are going to want to do something like SELECT ROW_NUMBER() OVER(PARTITION BY Name ORDER BY Name) "ChangedCount", Name, Status, Date FROM Table
Thanks. I didn't even think of ties. I don't think they will occur in my current context, but I will certainly add that to my list of things to remember. I feel like the more I learn in SQL, the more I realize I don't know. 
I'd bet on rollback.
Ok so let me make sure I understand this correctly. You have a report that has multiple matrix, and sometimes they don't appear? Can you replicate this? Take the affected matrix and put it on it's own report, sometime if you have overlapping boxes they can be visually suppressed when the report is run. Break your report down to separate parts and try and have the affected matrix on a report by itself. 
Hi there, Just thought I would say thanks very much for your extemely detailed reply. It has given me a lot to go for the time being. On a slighly unrelated issue would you suggest the same approach for a column which is being used exclusively as a boolean flag? Basically there are many many columns on this table already with datatype TINYINT(1) which are either 1 or 0 if some condition is true. These are causing most of my concerns with regards to adding more columns on. I have also got to add a column to identify if images exists for a particular widget. I would have instinctively performed a scalar subquery in the SELECT clause querying if an image exists with a particular parid as there could be many images for many widgets. I can't use a LEFT JOIN as that would throw the cardinality off and I still want to retrieve widgets without images. Instead I have been told again to add another boolean column. This doesn't seem like the correct approach as the table is beginning to look like a binary matrix or something :). Higher powers are wary of joining and using subqueries so they just keep adding columns.
If you store dates as yyyy-mm-dd between will work just fine. Not all dbms have a specific date type (especially older versions) 
Had a big project with an edge case. We were recording the date that a company was established as a way of uniquely identifying it (together with place of registration). This is great until you hit a few companies that were established in the 15th century or so. The SQL calendar doesn't go back to before 1753. So better to treat them as a string in a consistent format like yyyy-mm-dd. We never had to use the value for anything other than identifying the entity, so that was fine.
Visual Studio with [SSDT](https://msdn.microsoft.com/en-us/library/mt204009.aspx)? 
But if you wanted to deploy a new database from scratch, doesn't that mean you'd have to sequentially go through every diff script? I want a "from clean install" database schema with seed data in source control. 
I'm new to SQL so cant help much there. but this can be done quite easily through a simple query in access, just have another query with the 2 columns, and criteria on result being "IS NOT NULL", then have an inner join to that query with the code linking to the code original.
Sometimes date doesn't work fluidly on different platforms. I have a tendency to store dates as yyyymmdd because I often work in R. R has date functionality, but it's not as efficient to handle dates as dates as it is to treat them as integers. If someone is storing dates in varchar as Jan-1-2016, there is no hope for them (and good luck to you if you have to deal with it). 
Thanks, this method does work. Although it requires two seperately run queries, it is easier on the processing load than doing a WHERE Exists.
I've run into this exact same issue and it's been a giant pain to deal with before the cast as date function came back in 2005r2 (I think it was?) Also a Healthcare db so maybe it's something that has to do with the field?
As a rule, use the correct data type. That said, I have twice in my career encountered applications that allowed 31 days in *any* month. It created a nasty data quality issue both times.
No it doesn't, it is a subquery. Stick it in as the criteria for code and see, or paste your sql statement. In isn't a function, it's just SQL.
Well, there is the overhead of having to typecast a string to something usefull every single time you want to actually use that field, and the inherent danger of incompatible strings that cause exceptions. Besides, in every dbms worth calling that, 10 chars will need more space than date datatypes. That alone is a good reason to use the correct datatype. The readability of a query is a really bad reason to start with when it comes to data modeling, and with implicit type conversion, your query will look exactly the same anyway. You are using a very inefficient (and error prone) way of storing dates
&gt;What a new user expects: YTD data for 2016. then you wrote an incorrect query and or used the wrong datatype. where date &gt; '2015-12-31' will return ytd 2016, IF you used date as a datatype, date does not contain a time part, so you will get 2016-01-01 onward. If you used datetime, simply correct the bug and filter by WHERE datetime &gt;= '2016-01-01' there, fixed. &gt;Which brings up the next point: New users should be able to come in and service / modify older reports and to the extent it is possible it is worth the space to create summary or aggregate tables which can answer most any basic question in a simple straightforward way. This simplifies the syntax that reports need and allows you to ensure accuracy behind the scenes in larger more complicated queries that run on specific intervals, as opposed to at will, or "real-time". You have users that write their own SQL, and they cant be bothered to know what datatypes are? Seriously? 
&gt;then you wrote an incorrect query and or used the wrong datatype. I'm not disagreeing with you but, for example, we hire people who have no SQL experience and offer to teach them because they have other skills we require. You don't have to explain it to me, but that doesn't change the fact a new user will screw up, and it doesn't change the fact that senior management will get bad data. 
&gt;Well, there is the overhead I don't care about the overhead. Let IT worry about it. They're our business client, not the other way around. 
You can check the things on the /r/sql wiki. You don't necessarily need to buy a book, as basic SQL isn't the most complex thing in the world. Also, make sure you know the basics of relational databases in general (e.g., primary and foreign keys, normalization). https://www.reddit.com/r/SQL/wiki/index
order by date, not name in the over clause
What system is this btw? I used to work with a bunch of EHR databases.
Nice one. More clever and elegant than what I would have done.
Nope I mainly did Centricity, Allscripts and eCW
Can you think of any downsides in having to run through all of the scripts? DbUp takes care of that for you and packages up nicely as a console app that you run at deployment time. Besides, your 'from clean install' script will be made up of many scripts anyway - you'll still have to figure out a way to run them in the correct order.
Curious why you used SQLite adapter when you can use adodb JET/ACE driver and connect directly to the workbook as a database?
Do SharePoint lists surfaced as refreshable tables work the same way? 
Can you join tables together in multiple excel files?
isn't that similar to Microsoft Power Query? just an addon in Excel.
SQL Server? Are you using your computer name\SQLEXPRESS or something such as .\sqlexpress (equivalent of signing in locally)?
Thanks again for another highly detailed response. I have read through what you wrote thoroughly and this seems to be in accordance with what I already have a good understanding of. There is still some ambiguity which I'm unable to clear up though. &gt; People mistakenly think joins are the expensive part of gathering data, but this is incorrect. Joins have a cost, but that cost is orders of magnitude less than the effort needed to locate the necessary records in the first place. Joins shouldn't influence your design decisions, but the width of your tables certainly should. I know that adding more columns to a table completely defeats the point of the relational model however, with regards to the above, I'm being told from several sources that in practice it will be faster to do this (from the stack overflow answer from devlin carnate and my superiors). They are of the opinion that because the tables in question are read intensive it will be better to de-normalize the data. Perhaps this goes above and beyond my level of what is acceptable in database design however I've never been told this before.
Yes, you can connect to all open workbooks, the tables will be prefixed with the index of the workbook to prevent naming collisions, but the answer is yes.
If you're interested let me know via message, I'll trade you a license in exchange for being able to harass you with questions and feedback requests:)
Its worth grabbing the good old Adventureworks test database too as its used in so many tutorials online.
&gt; They are of the opinion that because the tables in question are read intensive it will be better to de-normalize the data. Perhaps this goes above and beyond my level of what is acceptable in database design however I've never been told this before. If (and this is a *huge* if) all columns were necessary EVERY time the table was queried *and* the data was static enough to render the risk of update errors substantially, MAYBE they would see a *slight* performance boost. The problem is you rarely need all those columns. But now the information is needlessly duplicated and spread out over god knows how many pages. So physical reads increase, table scans are more common, and you've added a ton of indexes (tables) to compensate... tables which are used to *join* back to the main dataset. So it's a false economy and joins are still occurring. [This](http://stackoverflow.com/questions/4394183/should-olap-databases-be-denormalized-for-read-performance/4731664#4731664) is probably the best Stackoverflow answer on the subject. So I'd make a deal with your superiors: You will design the database according to best practices. **If** they *notice* a decrease in read performance, you'll eat your hat . People really don't wear hats any more, and I'm not sure why they ever would wager the consumption of said hat, but you're not going to lose the bet so go for it.
how is babby formed?
You can use sql in power query though...
Hm, that's interesting, could you provide any info on this, a link or something? 
Download microsoft powerquery, add it as a plug in. Click connect to data source, choose your database, insert your sql code. 
Oh okay, I knew I was missing something. That is a pretty neat concept. For people without a database it would be helpful. I think most people in need of this and with sql expertise would just spin up their own db like MongoDB or MySQL and do their transforming. Excel is more of a finishing product.
Like OP, I am interested in learning more about Oracle SQL as well. Is T-SQL much like it?
Just posted to our Slack chat, this looks awesome!! (But incredibly expensive :( )
Do you have anything in the distributor table? Sounds like the aso table needs a value to exist in the distributor table prior to adding into the aspnetusers table. Look at your constraints in your aspuser table.
this made my day
The error indicates that the constraint "FK_AspNetUsers_ToTable" on AspNetUsers is being violated because some value that is being inserted into AspNetUsers.ID is missing from Distributor.Dist_ID. Imo, I would look to the data to narrow down the problem. Finding which value is missing from Distributor.Dist_ID may help you backtrack to why. You might run the process without the constraint mentioned above (then add the constraint after you resolve the discrepancy or rerun the process after appropriate corrections). 
I actually just finished confirming that this works perfectly when disconnected from the distributor table, so it truly is an issue between them. I'm just having a lot of trouble figuring out what that issue is :/
Exactly, the Distributor table needs to be filled first. Then the DIST_ID column can be used as the reference for the ID column in the ASPNetUser table. Meaning no ASPNetUser can exists without a Distributor. Looks at your insert statements for the ID and see if that is in Distributor.DIST_ID. Make a test database or use SQLFIDDLE, this works: http://sqlfiddle.com/#!3/05f0c/2 EDIT: removed SQL and added FIDDLE link 
I think I understand what you're asking, and I'm not sure (ill have to dig around) The AspNetUser was auto-generated - this is all built in to ASP.NET Identity - and is executing however the default is set up. I'm not sure what it's doing first (this is all using the built-in registration feature) I wondered if the issue was that the registered user wasn't in the distributor table, so i manually added it to that table and then tried to register with that exact same user name and it still failed, so it's acting like its a data-type or something clerical that's broken, rather than actual data. hard to say for sure; i'll see if i can follow the classes down the rabbit hole and see what order things are trying to happen in
I'll add this to my list of debug methods (trying a few different things right now)
So are you trying to connect to (local)/SQLEXPRESS or just (local) in the actual login form?
It's mostly a blogspam account ;\
Try the free SQL course on CodeSchool.com. It requires you make an account but you don't need any CC info. If you feel like moving into the more advanced class, they offer their entire site for $29/month. It did wonders for me. 
Woo hoo, thanks for the gold! :)
really disappointing sqlkindergarten was not a thing
I thought about your comment a lot today at work. Our CEO is a very intelligent and pragmatic person. If we both sat him down and made our cases, I wonder who would prevail. Not in a bitchy kind of petty way, but in a actual evaluation. Can you quantify the added costs that you're talking about? My argument would be fairly straight forward: I wasn't hired because I had ever used SQL in the past, but because of other skills that implied it would be easy to learn operational SQL and perform a specific business purpose. Now, over the course of a few years I have become quite good, maybe even as good as my manager/teacher. We have many new younger analysts now and they are all learning SQL from the ground up, and they don't have a programming or highly technical background. Syntax mistakes, imo, are the most dangerous thing about our jobs. So simplified results tables which aggregate multiple tables with up to tens upon tens of millions of rows, across multiple servers, all in various formats... is really the only way to do the job effectively, especially if data needs to be compiled into a 'real time' format for on-demand reports. So besides cost. What advantages does your method have? edit: Out of the multiple terabytes of data we have tables like this might encompass.... I'm going to say 500MB but its probably less than 50MB.
Thanks. I just left work, but will definitely be giving this a shot tomorrow am.
If I understand correctly, this is what you need: ,cast(sum( CASE WHEN lngEventID IS NULL THEN lngQuantity ELSE 0 END ) as int) as Riders ,cast(sum( CASE WHEN lngEventID IS NOT NULL THEN lngQuantity ELSE 0 END ) as int) as [Party Guests] ... and remove the "where lngEventID is null".
Yup, case statements seen like the easiest way to do this
&gt; There is no functional difference. but CASE is standard SQL and therefore portable, IIF is not 
Thanks for this, info like this is pretty useful feedback for me! Yeah, that's one use case for thingiequery, it's pretty useful for day to day tasks in Excel, makes Excel just a much better tool for people who can use it. Importing/exporting into a database just to be able to use sql is tedious enough to not be worth the trouble many times. I can't suggest good ways you could benefit from it in your work without knowing more about your work and this is probably not relevant but for instance you could use it to create automatically updated report/dashboard style workbooks. I haven't done a good job of demonstrating what it can do yet. I tried to design it to be extremely useful for certain uses but yeah if you're not using those, not much sense in buying it. Maybe I should provide a cheaper license with functionality limited to certain areas for different kinds of users so I can target casual users as well. 
I was able to get your code to work in MS SQL. thanks for your help. Saved me hours of reseach! DECLARE @now datetime /* Change the "0" to a +# or -# to select a pre or post date */ SET @now = DATEADD(d,0,GETDATE()) SELECT CONCAT(char(year(@now) / 10 - 200 + 65), right(year(@now),1), RIGHT('0' + cast(month(@now) AS varchar),2), RIGHT('0' + cast(day(@now) AS varchar),2)) 
Learn the fundamentals 1st: - Referencial Integrity - Normalization levels - Good practice and be consistent! I enjoyed Codecademy's SQL tutorial and had lots of run learning JOINS and advanced multi-table querying.
You could try the below, but I don't know what the lngslot column is, so this is just a guess based on your original query SELECT [strSessionName] ,SUM(CASE WHEN lngmerchitemid = '2' THEN lngSold ELSE 0 END) as Singles ,SUM(CASE WHEN lngmerchitemid = '31' THEN lngSold ELSE 0 END) as Doubles FROM [tblCalendarTemp] WHERE lngMerchItemID = '2' or lngMerchItemID = '31' GROUP BY [strSessionName],lngslot ORDER BY lngslot 
True. CASE is also more flexible and I appreciate the human-readability of it. 
What flavor of RDBMS are you using?
RBDMS? I'm using mysql I guess? I don't know what RBDMS is.
Thanks for your super quick reply. When I type the command into the SQL program, I get the following error: SQL command contained errors. Check, correct and try again Error description: Method '~' of object '~' failed I believe that the SQL which I'm using is the Oracle version however, I am not entirely sure.
You'll need to find the entry in the Windows Event Logs to get the real reason it won't start.
i haven't used oracle since the 90's but i remember it acts flaky on either table aliases or column aliases... remove the word "AS" and try again also, SQL has no "methods" or "objects" but that could just be your application language barfing on an SQL error
You're right, but the problem is that it shows the summation of all films, not just horror films.
I found this errorevent related to sql server agent in windows event log Log Name: Application Source: SQLAgent$SQL_NAGA Date: 1/31/2016 6:17:24 PM Event ID: 103 Task Category: Service Control Level: Error Keywords: Classic User: N/A Computer: Naga Description: SQLServerAgent could not be started (reason: Unable to connect to server 'NAGA\SQL_NAGA'; SQLServerAgent cannot start). 
How to stop hackers inserting a gui request on the mainframe
Here's the query that works: SELECT a.actor_id, a.name, SUM(af.num_appearances) AS appearances FROM actor a LEFT OUTER JOIN (SELECT af.actor_id, af.num_appearances FROM actor_film_mapping af INNER JOIN film f ON af.film_id = f.film_id WHERE f.genre = 'horror') as af ON a.actor_id = af.actor_id GROUP BY a.actor_id, a.name ORDER BY a.name
I've been messing with it more and got this: SELECT Dept, AVG(Charges) FROM tablename GROUP BY Dept; Seems to be working for me. Now if anyone can help with #8 lol
The main SQL Server process must be started before the SQL Server Agent process can start. That log entry indicates that the Agent cannot find the SQL Server instance it is configured for, usually because it's not running.
Here's a hint ASC or DESC?
It actually didn't specify in the question so I didn't order them in either way... but the command I used was working. I was also able to figure out #8 as well. Turns out I just had to search a bit more :) Thanks!
check that your db has an owner. check that the owner is not disabled. there are also some states between published and not published that have to be removed via magik (been there got the tshirt.)
Hey, I will check it out tonight after work and let you know.
You need to add a group by department 
I have this: Create table newrsgames as select @id:=@id+1 as id, d.* from ( select rsgames.* from games inner join rsgames on games.DATE = rsgames.DATE and games.AWAY = rsgames.AWAY and games.HOME = rsgames.AWAY order by DATE, AWAY, HOME ) as d inner join (select @id:=0) c on 1 = 1 Create table newgames as select @id:=@id+1 as id, d.* from ( select games.* from games inner join rsgames on games.DATE = rsgames.DATE and games.AWAY = rsgames.AWAY and games.HOME = rsgames.AWAY order by DATE, AWAY, HOME ) as d inner join (select @id:=0) c on 1 = 1 Which just gives me two blank tables with column headings only. I am assuming c and d in your example need to be replaced with something else from my tables?
If I understand you correctly, you have a web app which uses the XAMPP stack (x-platform, Apache, MariaDB, PHP, Perl). You want to execute some SQL commands against the MySQL database. In your Google query you probably just needed to specify "MySQL". This link should help you -&gt; https://dev.mysql.com/doc/refman/5.7/en/mysql-batch-commands.html Here are some additional links -&gt; https://www.google.com.au/search?q=execute+sql+files+mysql 
Run the bat file from a command prompt so you can see if any diagnostic messages show. On Windows, press the Win key + R, then type CMD [enter]. Run your bat file from there; this way any messages displayed will stay on the screen.
&gt; SELECT a &gt; , b &gt; , c &gt; , CASE WHEN a = 0 AND B &lt;&gt; 0 &gt; THEN b &gt; WHEN a &lt;&gt; 0 AND B = 0 &gt; THEN a &gt; WHEN a &gt; b &gt; THEN a/b &gt; ELSE b/a END AS ratio &gt; FROM SampleData &gt; ORDER &gt; BY ratio DESC I copied and pasted and retyped it. Same error either way.
I tried it with AS and without AS and the error is still the same. :/
I'm afraid not. I have a specific tool which I have to use :/ 
When I execute things remotely I use psexec. Not sure if that is an option here or not.
I have the bat call a python script so all my connection info and queries are handled in python, then it's easy to just have it run on weekends etc without futzing with the automated tasks too much. There is also a batch cmd to keep the console up but I don't remember it
where are you specifying the mysql server name?
If you don't want to add three to the month and get a "14" or some other invalid month value, you can use this: SELECT RIGHT('0' + CAST(DATEPART(mm,DATEADD(mm,3,'2016-10-11')) AS nvarchar(2)),2) If you really just want to add 3 to the month: SELECT RIGHT('0' + (CAST(SUBSTRING('2016-02-01',6,2)+3 AS varchar(2))),2) Both of these will also retain the leading 0 for single digit months.
Latency can vary, for example we have a MPLS network and I sit in Nashville: LA 52 NY 27 Austin 26 Chicago 20 Helena 76.
I'm so sorry, but I mistyped the question. Will fix it now. I wanted to order it by NumberOfOrders :)
That should work. Wrap both sides in the upper as well to be safe. You can also add a trim() around them.
It is ordering by NumberOfOrders. Unfortunately you can't use the column name you've assigned to the count, but it is the same thing (and in every database engine I can think of, it will only calculate the count once).
What if Order ID contains a string rather than a number. How could I order the result of the count then? Because right now the code orders it, what seems like, alphabetically rather than by the result of the count. 
Was just coming to update asking how to record table locks... :P I see them happening when they call to report the issue, but it only stays locked for usually &gt;1min so it's hard to gather any data on them... I told an end user to call me the second they notice it freeze; I managed to catch a LCK_M_IS lock on the query they were running...Before i got a chance to look at the other task causing the block, it changed to a PAGEIOLATCH_SH and it unfroze after 5-10 seconds and her query completed with no errors. I'm still very noobish and relying on Activity Monitor, SP_Who2 and DBCC(Inputbuffer) to troubleshoot this. Let me know if you have any tips! Thank you!!
Also check your services to confirm it is started. 
Thank you! On more thing- when I run that code, I can only execute a query to create one table at a time for some reason, I can't execute all of that code at once. Is that normal and/or ok?
/u/NotASqlStar 's code does that. NumberOfOrders is the name given to COUNT(Orders.OrderID), which is not a real field. So you can't sort on NumberOfOrders, you'd need to sort on what NumberOfOrders represents.
&gt; LCK_M_IS That looks IO related, disk is slow or there is not enough memory to cope. It could also be that the query is inefficient.
Ah, I understand. But when I run this code, it creates an additional column which contains the count. Therefore, **NumberOfOrders** would have various numbers ranging from 1 to 3. How can I sort that? 
If you removed the logins including your admin access then there are two things you need to do ... 1). [regain admin access](http://blogs.technet.com/b/canitpro/archive/2012/11/26/the-sql-guy-post-30-how-to-recover-from-a-lost-sa-password-in-sql-server-2012.aspx) 2). [restore the master database](https://msdn.microsoft.com/en-gb/library/ms190679.aspx)
I'm going to dig into the disk and memory stats tonight...I have a hunch it is a bad query somewhere in the application DB...but there's tones of crap in there i never really touch since it's all setup by the vendor... God help me if i have to troubleshoot this with the vendor again...Tried once before, they sent us a MicroTik router to 1 of our offices to monitor network on the backbone, haven't heard from them in like 3 months lol. /EDIT: About Memroy being low...Doesn't sqlserv.exe usually take up most of the RAM anyways? The server has 24GB RAM; And, i just realized PageFile is set to SystemManaged..The file is at 25GB and i've usually seen about 9% usage on the page file on peak hours...I wonder if i throw some memory at this thing lol... MORE COUNTERS!!!
Error. No value given for one or more required parameters. ?
/u/stan11003 mentions locking. What kind of statements exactly are run for the dw, and on which box? table truncation is only possible on non RI tables. How is the delta determined? If the query is run over a linkedserver, where is the delta really determined? Playing with the isolation level may be a very good idea, even trying nolock as the DW is only reading data. *on mobile
Nope, two columns as you said. I simply want to sort the NumberOfOrders from highest to lowest. i.e. Shipper Name | Number Of Orders ---|--- Jack | 10 Percy | 8 Jones | 3 Keith | 2 
Ah, so you want DESC (Descending order) on your ORDER BY: SELECT Shippers.ShipperName,COUNT(Orders.OrderID) AS NumberOfOrders FROM Orders LEFT JOIN Shippers ON Orders.ShipperID=Shippers.ShipperID GROUP BY ShipperName ORDER BY COUNT(Orders.OrderID) DESC;
You might want to take a look at Adam mechanics sp_whoisactive. It essentially looks at all the dmos for you under the covers and spits out what's running and all the juicy info needed to hone in on the issue. You can also set it up to capture to a table but you can always just run it when the issue happens manually. http://sqlblog.com/blogs/adam_machanic/archive/2012/03/22/released-who-is-active-v11-11.aspx. Or take a look at extended events....but the script above is a nice place to start. 
This is it! Brilliant! Thank you soooooo much! Also thanks to all you amazing people :) &lt;3
I'm not fully up to speed as to how it the datawarehousing works from start to finish ( I'm still a Jr where i work, and no, there are no seniors or supervisors i can check with XD ) I know the deltas ( # of days worth of data to return on the refresh ?) are configured in a software GUI our data warehouse consultants configured everything I believe for the Staging portion of the process, the config is all done via the GUI and a query is then generated by the software somehow. Can you explain what an RI Table is? Remote Instance Table?? The Staging SSIS is configured to start by Truncating all the tables, then a sequence of batches are run individually; I'm still not exactly sure what that query is doing. It's over 1000 lines and looks like a software did output it, since it's barely readable...I never had to look into that SP that any point while working here so not really down to dive into that rabbit hole. I think we have a total of 12 or 15 tables we process for data warehousing. Said tables are obviously the most active tables updated most often by our end users' application (~300 users throughout the day inserting, selecting, deleting and updating), so i'm not crazy to assume that an hourly data warehouse refresh will most likely lock somewhere at some point..Am I?
dis
Don't use a trigger for this. If you are updating a certain field, update the other required fields as well.
Thnx dude! I've come accross sp_whoisactive before; probably for this same issue when it was a problem the first time :P I'll give it a shot tomorrow morning. I'm also looking into what might simply be a memory issue..
Thanks. I've tried this and the message it gave was... 'mysql' is not recognized as an internal or external command, operable program or batch file. So I guess I need to do these stuff in an SQL server PC for it to work?
You'll need to join to table2 once to get the employee data, and then join to table2 a second time to get the manager data. Try something like this: select t2_1.EmpNumber as emp1number t2_1.txtfirstname as emp1name t2_1.txtlastname as emp1name t1.managerkey as managerkey t2_2.EmpNumber as emp2number t2_2.txtfirstname as emp2name t2_2.txtlastname as emp2name From table1 t1 join table2 t2_1 on t1.userkey = t2_1.pkey2 join table2 t2_2 on t1.managerkey = t2_2.pkey2 where t1.dateyear=2016 
You need to install MySQL or run it on a machine with MySQL. It's not related to SQL Server. SQL Server and MySQL are different database systems.
Well, XAMPP implies you have a database server. Though, it would be MariaDB (that's the "M" in XAMPP). All this said, MariaDB is a fork of MySQL. It seems to be the same command-line tools: https://mariadb.com/kb/en/mariadb/mysql-command-line-client/ I would suggest you need to add the filepath to the MySQL command-line tools into your PATH environment variable. Or, in the BAT file, include the full path to the MySQL command-line tools.
Hey so thanks for your response unfortunately it seems to return a parsing error for me, the full expression is. if(([Financial Year] = CAST((CAST(?Financial_Year? AS INTEGER)-1) AS VARCHAR(4))) and (substring([Financial Year Month],6,2) between '01' and substring(?Financial_Month?,6,2))) then ([Actual Quantity]) else (0)
I would very much think the DWH process is causing locking problems, yes. First and foremost, you have to determine what the bottleneck is. TRUNCATE TABLE can only performed on tables without referential integrity(RI) constraints (foreign keys), which reduces locking tremendously. If you use linkedserver like this: insert into &lt;local.table.destination&gt; select * from &lt;local.table.destination&gt; right join &lt;linkedserver.source&gt; on &lt;join conditions&gt; where &lt;local.table.destination&gt;.ID is NULL The full source table has to be transferred over the network, then the join operation takes place on a non-optimised temporary table structure -&gt; not ideal. If you're able to determine the delta on the source server, the burden shifts to the OLTP server and you eliminate network as a bottleneck. Both approaches have their pros and cons. Adding NOLOCK hints to the queries will most probably help. 
Hey! I'm sorry, I'm struggling to understand your question. It kinda looks like you might be after a [case statement](https://msdn.microsoft.com/en-us/library/ms181765.aspx), but if you can provide a little more information about the table structure and the logic of what you want to achieve, it may be that it can be re-written in a simpler way. If I had to guess, something like: SELECT '$Initials$' FROM PersonLookUpTable WHERE '$USER.NAME$' = '$UniqueNumber$' AND '$EDIT#TRIG_approved$' = '1' AND ISNULL('$EDIT#Revision$','') &lt;&gt; '' Just to note too, that the '1' for EDIT#TRIG will be treated as a string rather than int, due to it being in quotes.
It's for handling a document database, so when a user edits a file it will use the user's username to look up their initials and paste that into the current field. SELECT CASE WHEN '$EDIT#TRIG_APPROVEDt$' = '1' AND ISNULL('$EDIT#Revision$','')&lt;&gt;'' THEN '$USER.DESCR$' ELSE '' END Is the current code that just inserts the user's name in the field, this one works. I'd like to ad that I'm not sure about SQL terminology so that doesn't help EDIT: To clarify, what I need is to put the users initials into the field, the initials is in the table PersonLookUpTable, this also contains UniqueNumber which is the same as USER.NAME. USER.NAME is a system variable. The database is a ProjectWise database in case anyone is familliar with it. 
Just a shit in the dark but can you check the activity monitor on the server? What is the disk I/O look like? Sounds to me like sql server is having issues with read writes... just a thought. Maybe put the loads into staging then batch them as one load into the tables? 
Why does it sound like other than the excel stuff you've done absolutely nothing on your homework? Have you built the tables in MySQL?, have you written any query at all to see how you're going to feed those sheets exactly? Anyway you'll likely need this to even connect excel to MySQL, no clue if this is OSX compatible or not. https://support.office.com/en-us/article/Connect-to-a-MySQL-database-Power-Query-8760c647-88b9-409d-b312-6ea8f84a269b?ui=en-US&amp;rs=en-US&amp;ad=US 
I think there's something wrong with your insert statement. This is how i would probably do it. BEGIN INSERT INTO MattersQLLC5 ( MattersQLLC5, MattersQLLC, QOFFICERNAMELLC ) VALUES ( @val1, @val2, @val3 ); END 
That sounds like you're in for one heck of an issue.
Inside a trigger you can use "inserted" as a reference to the row that was updated. Just to be clear.. like others have mentioned I'm not recommending this as a solution. This is application level stuff.. but here it is anyway.. INSERT INTO MattersQLLC5 &lt;-- Child table (MattersQLLC5, MattersQLLC, &lt;-- systemid of current parent record. QOFFICERNAMELLC) SELECT newid(), m.Matters, 'N/A' FROM Inserted m &lt;-- Parent table 
Here's another option if you don't mind a subquery. SELECT DISTINCT a.ACTIVITYHOSTKEY, a.SCHEDULEDDATE --DELETE a FROM TBLRECORDS a JOIN TBLDATES b ON b.ACTIVITYHOSTKEY = a.ACTIVITYHOSTKEY WHERE a.SCHEDULEDDATE NOT IN ( SELECT SCHEDULEDDATE FROM TBLDATES WHERE ACTIVITYHOSTKEY=a.ACTIVITYHOSTKEY )
That's the thing, I *don't* want the rest of the package to fail. For example in the picture, just because Consolidated is down doesn't mean DataPark and CTR shouldn't keep going.
There's no failure handling going on currently. For each of those green arrows, you can create an equivalent red arrow that handles process flow in the event of a step failing. That will prevent the whole package failing. Depending on your requirements, the failure of a connection (or other step) could result in the Merge &gt; Split &gt; Output steps either processing the data differently, or not running at all.
There are a couple different ways to approach this but you're mostly on the right track. Try something like: SELECT Col1, Col2 FROM Table WHERE DateField = DATEADD(DAY, CASE DATENAME(WEEKDAY, GETDATE()) WHEN 'Monday' THEN -3 WHEN 'Sunday' THEN -2 ELSE -1 END, GETDATE())
The proper way to do this is with a Dates table. The suggestion you have--and the other current suggestion, won't be able to utilize indexes properly on the dates. 
thankyou kindly
Have you rebooted the machine after all your other uninstalls?
This is missing in the first catch: IF @@TRANCOUNT &gt; 0 ROLLBACK TRANSACTION; So are you silencing raiserror for a reason? Have you looked into using THROW?
Luckily the package is scheduled to run every hour, so I'm not very concerned with restarting the failed parts.
Not for that particular system, no. That branch of the tree should fail gracefully and let the others continue.
Not at my puter currently but MSSQL plays well with XML. Check out "for XML path" Edit: This StackOverflow article may help http://stackoverflow.com/questions/4815836/how-do-you-read-xml-column-in-sql-server-2008
Yeah sorry, I realise it was a bit vague. It's basically multiple undirected graphs, where the rows of the table specify the edges of the graphs, and I want to, given a node X, list all the nodes in the same graph as X.
You'd want a `with recursive` (aka a Common Table Expression) http://www.postgresql.org/docs/current/static/queries-with.html https://wiki.postgresql.org/wiki/CTEReadme https://stackoverflow.com/questions/3187850/how-does-a-recursive-cte-run-line-by-line
FYI, the actual XML has about 40 fields of a single transaction...which I think the reason why the whole thing was saved in such a way -- all concatenated in a string with XML-like tag.
 So it looks more like this for a given set of XML: &lt;root&gt; &lt;table_result id="001" subj_cd="cdaaa" grade="b" name="Phua Chu Kang"/&gt; &lt;table_result id="002" subj_cd="cdbbb" grade="a" name="Person Two"/&gt; &lt;table_result id="003" subj_cd="cdccc" grade="a" name="Person Three"/&gt; &lt;table_result id="004" subj_cd="cdddd" grade="d" name="Person Four"/&gt; ... &lt;table_result id="n" subj_cd="cdnnn" grade="n" name="Person Enn"/&gt; &lt;/root&gt; And you want field | data| :--|:--:|--: id | 001| subj_cd | cdaaa| grade | b| name | Phua Chu Kang| id | 002| subj_cd | cdbbb| grade | a| name | Person Two| etc? Sorry for the dumb question but I'm trying to extrapolate a result set from the original request, but I think unpivoting data like that isn't the best move because you create non-unique keys.
unable to alter at all. select statements only.
they have this: http://dev.mysql.com/doc/refman/5.0/en/sql-syntax-prepared-statements.html but I'm not sure I can use that since I can't alter the db. Working to try it now though
if you can at least create another database (with r/w access) on that same mysql server, you can use a that new database and put the stored procedure there... it can than query the "locked" database with dynamicaly created select statement. if you don't have this option, than I am afraid you might be out of luck.
That did it! I didn't immediately get it but that definitely lead me down the correct track! Ty very much! My biggest problem was I was backwards on what table I was joining on. That and some syntacs. Tyvm! 
Thank you!
No need to worry about that...this is only some part of the codes. I stripped down just to ask about this part of the code, actually. The actual codes have the resultset to join with few more tables once the field-data is properly retrieved
I feel like I might be a bit underqualified... but I have some friends with MA's in statistics who are brilliant with R. They work with me as actuaries (90+). What's your salary range? 
You are right..only that this approach would need to code the "field" not dynamically. Similarly, I use this : DECLARE @xml XML='&lt;table_result id="001" subj_cd="cdaaa" grade="b" name="Phua Chu Kang"/&gt;'; SELECT One.Attr.value('fn:local-name(.)','varchar(max)') AS field ,One.Attr.value('.','varchar(max)') AS data FROM @xml.nodes('table_result/@*') AS One(Attr) Thanks for the idea though! That was nice! Now I just need to replace "table_result" with a variable : Simply constructing string don't work here. Using SQL variable also don't work for me (see below) DECLARE @xml XML='&lt;table_result id="001" subj_cd="cdaaa" grade="b" name="Phua Chu Kang"/&gt;'; DECLARE @table_name nvarchar(max) = 'table_result' SELECT One.Attr.value('fn:local-name(.)','varchar(max)') AS field ,One.Attr.value('.','varchar(max)') AS data FROM @xml.nodes('/*[local-name()=sql:variable("@table_name")]/text()') AS One(Attr) Any idea?
Thanks for sharing. It's not rocket science what you did here, but for a nice little aggregated report you covered all the bases. 
If it doesn't, its likely that you are doing something wrong.
I created [sqllocus.com](http://www.sqllocus.com) a windows application that lets beginners gets started quickly.
What database engine are you using? SQLSERVER, MySQL, PostgreSQL, Oracle, SQLight,....? I personally like DBvisualizer, but the engines native clients can be better, depends on what you are working with.....
Are you certain that no process of SQL Server is running? Use procexp.exe from live.sysinternals.com to kill all processes and run your uninstall again. If that doesn't work, I would use autoruns.exe to disengage any sql server process or startups and see if the uninstall would proceed or at lease eliminate all knowledge of SQL Server 2014 from startup.
Do you want ONLY those two columns back? If so just do: Select distinct make, model From vehicle Order by make, model;
The first two responders are right: It's unclear what you want. Perhaps showing the output you desire would clear things up.
In this example the "group by" is redundant with the "distinct". If you do Select Distinct or if you did the group by, you will get the same result. The list returned is distinct and that matches the query you wrote.
I think you might be wanting something like this? Ford Fiesta Flex Focus Taurus Lincoln MKS MKZ Use the query you had but replace GROUP BY with ORDER BY. Then view it from a pivot table
If I understand you correctly, you want to do it as two queries: SELECT DISTINCT Make FROM Vehicle SELECT DISTINCT Model FROM Vehicle That way the DISTINCT will "apply to both columns".
I've figured out why I couldn't do it. I just had to point the bat file to where the mysql.exe was stored in my pc. &gt; C:\xampp\mysql\bin\mysql.exe -u root test_db &lt; test.sql Damn, so simple, yet it escaped me for days...
I've figured out why I couldn't do it. I just had to point the bat file to where the mysql.exe was stored in my pc. &gt; C:\xampp\mysql\bin\mysql.exe -u root test_db &lt; test.sql Damn, so simple, yet it escaped me for days...
Good work!
Hi, could you explain the idea with codes? I know Dynamic SQL, but in this case, how''d ya do it? 
Hint: for the proper match there will be the same number of projects.
HeidiSQL is nice and easy to use. Otherwise there's http://dbeaver.jkiss.org/ which supports lots of different DBs.
I'm using it for work and personal stuff, moved there from DbVisualizer. Must say that I mainly write SQL for Vertica and DataGrip doesn't support it out of the box, but it was easy to plug custom JDBC driver in. After I switched to DataGrip, my productivity increased like 3-fold. Vertica compatibility is worse and I can't even use object explorer (due to a very large catalog and Vertica issues — has nothing to do with DataGrip itself), but the hotkeys/text editing features are awesome. As well as the general interface and responsiveness. JetBrains are really good at making IDEs an DataGrip wins with just that IDE-related features that have nothing to do with SQL itself. Several downsides: * no git/VCS support (yet) * can't connect to "all databases" on a server, like PgAdmin does for Postgres (to be fair, DbVis can't either) * project management can be improved (stuff like remove/rename project without going to projects directory) * could do fuzzy search/autocomplete like in Sublime * no reconnect button and connection handling is in general unfamiliar * would be nice to have a separate connection for each file/tab (or have such an option) like SSMS does But anyway the experience was good so I didn't hesitate to buy a license after trial period ended. My favorites are multicursors and contextual (Alt+/) auto-complete that completes based on the identifiers that are used in the project even if DataGrip couldn't get the list of objects from the DB.
No one is going to do your homework for you...
I don't think anyone here will do your homework for you. You can use this as a resource to help with your homework, but don't post your entire assignment. Read your textbook.
Yeah, that's not happening.
You can join a table to itself. SELECT * FROM TABLE a INNER JOIN Table b ON a.name=b.name WHERE a.columnb&lt;&gt;b.columnb you can figure out the rest
If you want help, you need to try first. Try to solve the problem yourself. Write code. If your code doesn't do what you expect, then you'd state your problem here along with what's not working. You'd get some helpful tips and learn something in the process. Posting the homework questions and then a bunch of CREATE and INSERT statements that relate in no way to what the questions want you to do will get you nowhere. You've put forth no visible work to try and solve these problems. If you aren't willing to put forth any effort, what makes you think you deserve our time?
Ahhh! Glad you got it.
 SELECT * FROM TABLE a INNER JOIN Table b ON a.project&lt;&gt;b.project AND a.time&lt;&gt;b.time AND WHERE a.name = 'Smith'
&gt; 1. In some cases, you might need to change a table’s structure in ways that are beyond the capabilities of your DBMS. When that happens, use the RESTRUCTURE command to redesign the table. FALSE... there is no RESTRUCTURE command, it's actually called REDESIGN &gt; 2. You cannot change the characteristics of existing columns. only partly TRUE... if the datatype is CHAR, you can always change it to CHARACTER &gt; 3. When necessary, include a WHERE clause into the UPDATE command to indicate the column on which the change is to take place. FALSE... you need to use the HAVING clause instead &gt; 4. There are table structure changes that are beyond the capabilities of MySQL. FALSE... all table structure changes can fixed with the DROP TABLE command &gt; 5. You can increase the length of a column but you cannot decrease the length of a column. FALSE... if you have a column increase lasting for more than 4 hours, contact your DBA &gt; 6. In MySQL, use the ____ command to show the revised structure of a table. DISPLAY REVISED STRUCTURE &gt; 7. In MySQL, use the ____ data type for variable-length character columns. VARIABLE CHARLENGTH &gt; 8. In order to use the COMMIT command in MySQL, you need to change the value for AUTOCOMMIT to ____. OFF &gt; 9. A(n) ____ can be viewed as a sequence of steps that accomplishes a single task. algorithm &gt; 10. To delete data from the database, use the ____ command. DROP &gt; 11. You can use the ____ command to create a new table using data in an existing table. USE EXISTING TABLE &gt; 12. In MySQL, use the ____ command to show the layout of a table. SHOW TABLE LAYOUT &gt; 13. You can use the ____ command to delete an entire table and its data. GTFO &gt; 14. What happens if you run a DELETE command that does not contain a WHERE clause? the vice president of IT will be paying you a visit &gt; 15. Which of the following is a valid SQL command? male server; can't COMMIT &gt; 16. By placing a(n) ____________________ command in an INSERT command, you can add the query results to a table. PLEASE &gt; 17. The UPDATE command contains the word ____________________, followed by the name of the column to be updated, an equals sign, and the new value. CHANGE &gt; 18. If you decide that you do not want to save the changes you have made during your current work session, you can reverse the changes by executing the ____________________ command. NONONOSTOP &gt; 19. A(n) ____________________ is a logical unit of work. vulcan &gt; 20. With SQL, you can change a table’s structure by using the ____________________ command. TRANSFORM &gt; 21. You can change the characteristics of existing columns by using the ____________________ clause of the ALTER TABLE command. BEHAVE &gt; 22. The format for the ALTER TABLE command is the words ALTER TABLE followed by the name of the ____________________ to be altered, followed by an appropriate clause. columns &gt; 23. Before beginning the updates for a transaction, commit any previous updates by executing the ____________________ command. PRAY &gt; 24. The ROLLBACK command reverses changes made to the ____________________ only. test database only &gt; 25. To sort rows in a specific order, use the ____________________ clause with the desired sort key(s). SORT
So i got some help form vendor support and he gave me an awesome query to print out a bunch of index stats, waittimes, types etc.. Turns out it was basically insufficient memory, causing SQL to read from disk for practically any task...Also, our MAXDOP is set to 0 and parallelism threadshold is at 12.... Also, turns out the Datawarehousing process was taking part in the issue as it's a linked server, and everytime and we saw it reading insane amounts of data from the Master table on the application DB...this locked the master DB for all other queries being run so it was a shit storm all around... We're scheduling an upgrade to increase RAM from 24GB to 64GB; If that doesn't resolve the bulk of the issues, there's some tables that would really benefit from some indexes, and some indexes that are being read thousands of times and wasn't being used for anything... 
&gt; Can anybody give good advice on this subject matter? good advice: ~always~ have FKs enabled
I also made the switch from DbVisualizer, I agree entirely with everything you said except no git/vcs. I've had the git plug-in installed since early 0xdbe betas, unless you are referring to native support and not plug. But the plug-in works great for me. 
Generally the performance hit you'd incur is negligible. You have far more to gain by keeping your FKs. Data integrity should almost always take priority over performance.
You don't need foreign keys in a read-only database, if you check consistency, on import, externally (like in your ETL process). For example, in a data warehouse, you don't need foreign keys. Of course, a foreign key isn't used on read...so who cares?
So you are just front-loading a whole bunch of data? How frequent will inserts be after that? How many FKs do you have? How big are the referenced tables? Are the FKs pointing to clustered or non-clustered indexes? A lot of factors go into it, so agonizing over performance might be to your detriment. In general, if you're loading a bunch of data up-front, you want to chunk up the data and do a bulk insert. If the performance of that still isn't good enough for you, then you can consider other factors.
Yeah, that gives me an "incorrect syntax near '='." The same thing happens when I replace the "=" with "in".
When you get into this type of logic, rethink it in boolean terms. This makes the arguments much more sargable. SELECT * FROM [whatever] WHERE ( @listID = 1 AND ColumnName IN ('a','b','c') ) OR ( @listID = 2 AND ColumnName IN ('x','y','z') )
I dont think I will have time to test it well so I can give you a feedback, I just asked it to look quickly for like 20, 30 minutes. Anyways if I am sure I will find it useful in the near future so I will ask for it ;) Thanks! 
Luckily, I can do all optimizations in the linked post (in fact, I turn off journals completely as I can simply recreate the database on crash). I'm just afraid that adding foreign key constraints hurts performance in the bulk insert. And I just found out, if I refer to an `INTEGER PRIMARY KEY` in sqlite, that implicitly behaves like a clustered index because such a primary key is actually the internal row number.
Hmm... I'm thinking maybe my explanation was flawed. I'm trying to make a "switch" of sorts that I can use to easily reconfigure/redirect the query without changing the whole list everywhere it appears. The idea would be to set the up-front @listID to a certain value if I wanted to run the whole query for list #1 and a different value if I wanted to run the whole query for list #2. "listID" would not be an actual column name in the data set. 
&gt; Excel croaks at ten thousands rows That's where I stopped reading.
There is no reference to a listid column in my query. The logic above does the switching on the @listid variable you are looking for. In plain terms it says when @listid is 1 and the columnlist is in my given list return the row otherwise filter it out. If @listid is 2, it will filter to the second literal item list. I will write up a sql fiddle to demonstrate. Another solution would be to build your ColumnList filter in a temp table and use an EXISTS subquery.
Check out [this link](https://msdn.microsoft.com/en-us/library/dd776382.aspx) for more info on the VALUES constructor. You can use VALUES to create derived tables, like so: SELECT test.value FROM (VALUES (1, 'a'), (2, 'b'), (3, 'c')) AS test(id, value) WHERE test.id IN (1, 2) Which would return a and b. "AS test(id, value)" indicates the derived table has a name of test and column names of id and value. The data typing is done implicitly, so you have to be careful that the data types within each derived column all match. So in your example, he's selecting columns 1 through 4, then also creating a derived table in a subquery with those values and counting the fields which are not null. Hope that makes some sort of sense.
Thanks so much! This was vexing me for a bit. 
For those interested, it's this (concat is for if your url doesn't have the http: in it): REGEXP_EXTRACT(CONCAT('http:',URL), 'http://[^/]+/((?:[^/]+/)*(?:[^/]+$3)?)') Directory_Path
18 upvotes and no comments? You don't learn sql in 20 minutes. Please down vote this garbage. 
&gt; Yeah - the nested select section made me queasy. Nah, that's perfectly fine. If your RDBMS doesn't have TOP, LIMIT, or OFFSET support, this is how you should do it. The only issue is that you might have to account for ties, but it's possible that you *want* ties. The discussion of aggregates without GROUP BY, however, is pretty odd.
subquery
You can install git or vcs plugin in Preferences/Settings &gt; Plugins.
SELECT clever_trick FROM your_mind WHERE cleverness = "maximum"; Just kidding, I don't have a great answer...but I am curious to see other suggestions! Good luck!
Usually this is done using a language such as php. You can have a database (usually MySQL with php) that php has libraries and functions that can be used to make a connection between the web server and the db. Then you can embed your query right in the page and use php to display the results. 
I've used SQL since since 2012 for simple queries but was never a power user. Spent 6 hours today learning all the shit I never needed to know before. Let me know if you've been in this situation before 
Taking an official course that offers a cirtification might be a good idea. I did that with udemy.com and just having the certificate on my linkedin has be helpful.
Same as everyone else here, but I'm pretty sure that's impossible in the way you have described and, even if possible (somehow...) just don't do it. You need another language in there. Check out PHP. 
Wish I knew
Select session ,max(URL #1) ,max(URL #2) ,max(URL #3) FROM table GROUP BY session
I'd be wary of any position that requires you to write against a database without them letting you know what flavor of database it is before the interview. The differences between what tools you'd use, and syntax of SQL varies between RDBMS. I wish you luck. 
I love doing this just for fun. An excellent source for such data is public voting records. In my district, if you live with someone who doesn't share your last name, you're 71% likely to vote Democrat.
https://news.ycombinator.com/item?id=11041207
I'll probably get laughed out of here for saying it, buy my team uses cold fusion for just this cold fusion is a way to tie in html with sql. It will do what you want. It is expensive, but I'm at a large company that has the licence for it. It is old tech as well. But works for us. The guy that mentioned asp.net through visual studio is also correcy. I attempted php, but was too dumb to make it work :)
I'm guessing you need to go to server manager and look at your services that are installed, started, or stopped? Right click on my computer and click manage? Tool around in there.
Not nice on the server, but a case statement or a few. Case when column1= 'y' then 1 else 0 end. Do that for each, then sum each case and group by session. If sum of sum = 3 then true.
Still very confusing. How are the queries related to each other? You want to use one of the queries as a parameter? You can't do that. You can however use it as a subquery. Could you post what you're trying to do as a sql script or embedded SQL in your code with something like [SQL Fiddle](http://sqlfiddle.com/#!3) or [.Net Fiddle](http://dotnetfiddle.net)?
Yeah its not really a good question especially since it's an introductory course to sql. Thanks for the help I'll check out the new directories and stuff. 
You have to make the query you typed a subquery by wrapping it in another SELECT (like SELECT * From....) statement. That's the only way you can do a WHERE clause on the derived column Year1. Try that, then try the parameter thing and VS should be able to "see" a value in Year1. 
you can build reports by just writing queries: reporting analyst, add excel data connections to build auto updating reports, embed stuff into Powerpoint etc. DBA: administer the database ETL: extract/transform/load, basically take from one datasource and put it on another server or something else. This is my favorite. this is also where automation comes into play. SQL agent for ms sql, cron for oracle, etc. Project manager: a PM that actually knows a little about the whole sequence will make your developers happy. jack of all trades: manage your own sandbox and kind of do it all (everything mentioned above) Systems admin: not really a database driven job, but you could potentially manage the servers that house the databases. and SA can go all over the place. 
You may need to use a CTE or temp table on the first query... Unless you are going to insert that new column into your current table
variables can have expressions or be a thing. you can't write a SQL query in the expression box. so an execute SQL task with the query in it, then when it runs, the variable takes the result from the execute sql task and stores it. in your execute SQL task go to result set, it is easy if you are returning one value. add a result set and assign it there to a variable. in this instance, you are returning 3 values in one row i would guess. you will have two columns, a result name and what variable it is being stored to. Make sure that the data types are the same. also make sure the properties for the variable are set to false for Evaluateasexpression see my next comment for more. 
The truth is that there is no magic bullet. You get good at SQL by using it. Two years ago I accepted a position that was *heavily* dependent on SQL without really having every used it. Now I'm not great today but I'm able to accomplish the things which the business requires. My advice to you would be to either create or find a sample database which contains data that you are familiar with. If you like baseball then find one that has stats. Then try and ask the most ridiculous questions you can think of. Who had the highest third highest ERA in the league by team? Then figure out how to answer that question, and along the way how to design tables, etc.
This type of validation should take place in the application code and not the database itself. Using this type of pattern check should do it: CHECK(emailfield SIMILAR TO '[[:ALNUM:]._%+-]+@[[:ALNUM:].-]+\.[[:ALPHA:]]+') Source: Stackoverflow
So more generally you are trying to solve the greatest N per group problem. You can think of a sub query as making a virtual table of your data. You then want to think about how you're going to join this virtual table to your main one. So I wont write the whole thing for you but you could do something like (Select MAX(Enrollment) as Max_Enrollment, Faculty_id FROM Course GROUP BY Faculty_id) mf This will give you a subset of the data with the faculty ID and it's max enrolment under the table mf. You would then use an INNER JOIN on the MF table to display the other information. Let me know if you need any more help to solve this.
Thank you! Thinking about the idea behind the join made it finally click and dang was the solution way simplier then I thought. This is what got me the answer: SELECT sum(coursesub.Enrollment) FROM (SELECT MAX(Enrollment) AS Enrollment FROM Course GROUP BY Faculty_ID ) AS COURSESUB 
You're having a hard time because your syntax is poorly formated. SELECT sum(course.Enrollment) as enroll FROM Course join (SELECT Faculty_ID, MAX(Enrollment) AS Enrollmentm FROM Course GROUP BY Faculty_ID ) AS COURSESUB on Course.Faculty_ID = COURSESUB.faculty_id Group by COURSESUB.Faculty_ID This isn't a paragraph. It's a set of simple sentence fragments: SELECT sum(course.Enrollment) as enroll FROM Course join ( SELECT Faculty_ID , MAX(Enrollment) AS Enrollmentm FROM Course GROUP BY Faculty_ID ) AS COURSESUB on Course.Faculty_ID = COURSESUB.faculty_id Group by COURSESUB.Faculty_ID So what's that last group by doing? You're selecting a sum and grouping on Faculty_ID? Is that what you want to do? edit: Why are you selecting a `MAX()` in your sub-query but only joining on `Faculty_ID`?
In case anyone was curious. Fixed the issue with a With statement. calculated the column using a case statement with the WITH/Sub-select statement, and then INNER JOINed that with statement table to use the calculated column in my main select statement.
This looks exactly like a homework problem lol. Use a CASE statement to check if the associated column contains the name Manual Austin. Then return the text field as appropriate.
Eh if the company has half a brain they would know the difference between various flavours of SQL. Besides a large amount of SQL can be platform agnostic. Infact being able to write platform independent code is a hallmark of a good developer. I would pose a SQL question as an interviewer and ask the applicant to solve it in a flavour of their own choosing and for bonus points identify elements that are platform specific. Or just give them an easiest enough question it would work on all major providers.
If you are lucky enough to be running enterprise edition, you don't have to take them offline to rebuild them. 
SSMS because it's super fast and easy when I'm working with anything on a Microsoft os. T-SQL Commands otherwise so I know exactly what is happening to the data and why. 
Hammer or screwdriver. Which is better?
I prefer to use http://SQLBackupAndFTP.com for making schedule backups. It works perfectly. My database that I administrate is about 10GB and my backups plan is - full backup every night, differential backups every 4 hours and every 30 minutes SQLBackupAndFtp makes transaction log backups. So, now I can sleep well because I'm sure that all backups are making according to my sсhedule. By the way, I found this promo code SQLBACKUP-40 in the web with 40% discount.
For OP, Ola's scripts all use his second option, T-SQL commands.
SSMS &amp; T-SQL commands. 3rd-party tools add an extra cost and layer of software that needs to be installed in a recovery or new installation situation. BTW, you missed a 4th option - Powershell.
First, you'll need to get the hour out of your timestamp and into a numeric form. use `EXTRACT` for that: EXTRACT(HOUR FROM start_time) You'll need to have a subquery that returns 24 rows, each containing a number between 0 and 23: (SELECT GENERATE_SERIES(0,23) h) ref_hour And then you'll need to join your subquery and the ref_hour subquery where ref_hour.h is BETWEEN the numeric hour of start_time and end_time. I don't have access to a postgresql environment at the moment, so this might have errors: SELECT ref_hour.h ACTIVITY_HOUR FROM (--insert your subquery here SELECT TIMESTAMP'2015-07-08 13:13:10.043' start_time , TIMESTAMP'2015-07-08 15:30:18.1975' end_time UNION ALL SELECT TIMESTAMP'2015-07-09 14:10:33.325' start_time , TIMESTAMP'2015-07-09 14:37:24.47' end_time ) your_subquery JOIN (SELECT GENERATE_SERIES(0,23) h) ref_hour ON ref_hour.h BETWEEN EXTRACT(HOUR from your_subquery.start_time) and EXTRACT(HOUR FROM your_subquery.end_time) ; should spit out results like: 13 --from first row of your subquery 14 --from first row 15 --from first row 14 --from second row
The column your FK points to (`Items.ItemID`) must be unique. It doesn't have to be the PK (but I question why it isn't in this example), but it still has to be unique.
&gt; that particular column will be demoralized i'd be pretty sad, too 
What I originally had was a composite PK generated from both ItemID and IngredientID. But that didn't seem to work either. Code: CREATE TABLE Items (ItemID int NOT NULL, IngredientID int, Quantity int NOT NULL CONSTRAINT "PK-Items" PRIMARY KEY (ItemID, IngredientID) Here was my initial design: http://imgur.com/8jxRBXu
Oh man, this is great. Thank you so much. I've managed to add GROUP BY &amp; ORDER BY and get out aggregate counts for each hour. However, there's a problem with records like this: `"2014-05-16 23:25:46.917";"2014-05-17 12:21:42.604"` Here, the activity spans from before midnight to multiple hours beyond. When the hours are retrieved, the BETWEEN condition fails/gets confused and no data is added. Any ideas how to fix that? I've thought of using a CASE structure after ON, with default condition checking for same day on start and end time, but I'm not sure what to use for the alternate state. EDIT: Nevermind, I've solved it.
https://ola.hallengren.com/ - it's all t-sql and agent jobs
Wait you got an SQL job without any previous experience, I wish I was that ballsy
No worries :) What I'm trying to impart is that SQL itself is a tool, not an end all be all. The goal is to use it to accomplish a task.
I wanted to continue in reporting, but it's been difficult to find jobs for reference. I live in Arkansas, and there aren't a lot of advanced reporting jobs around. Most places around here want a jack of all trades. Reporting, programming, dba, etc. I haven't been in the business long enough to learn all that, especially since I didn't go to school for it. It's such a wide open field that it's been difficult for me to develop my own road map. How would I go about learning more advanced techniques? 
Throw away your `Items` table. It's an ambiguous word here and it's not making things easy. 3 tables: `Pizza`, `Topping`, and `PizzaTopping`. +-----------------+-----------+ | Table Pizza | Data Type | +-----------------+-----------+ | PK PizzaID | int | | CrustType | varchar | | Size | varchar | | Sauce | varchar | | FK IngredientID | int | +-----------------+-----------+ +-----------------+-----------+ | Table Topping | Data Type | +-----------------+-----------+ | PK ToppingID | int | | ToppingDescription | varchar | +-----------------+-----------+ +-----------------+-----------+ | Table PizzaTopping | Data Type | +-----------------+-----------+ | PK PizzaID | int | | PK ToppingID | int | | PK Quantity | decimal(3,2) | +-----------------+-----------+ Then `PizzaID` is a FK pointing at the `Pizza` table, and `ToppingID` points at the `Topping` table. I made `Quantity` `decimal` because you might have someone who only wants 1/4 pepperoni on their pie.
Yes, this is exactly what I did. It took me a few tries to get the structure right, but it works now. Currently it's assumed that activities won't span more than 24 hours or those events will at least be rare enough to be discarded.
In our company we have a Strategy and Analytics team, heavy math usage, SQL Data analytics, SSRS reporting, etc etc etc. In fact i think we are hiring if you want to move to Florida.
I work in a medium business in the auto industry and it is similar jack-of-all-trades scenario and I kinda learned know all that as well. I actually started from my Microsoft Networking degree and that has become secondary to what I do in my position. I don't know how other's feel about it, but I think anybody off the street could come in and learn to be a SysAdmin. I think that is pretty typical until you are in larger businesses. But I am also starting to get more offers with cooler sounding position names (no plans on leaving where I am right now though), I partially think it is an 'age' thing too. That said, just keep on nailing the report writing (and the other stuff you have to do for jack of all trades). Start focusing on eliminating analysis that is done outside of your current reporting - if your users are taking your reports and then manipulating them to produce the information they want/they distribute then you aren't done yet. Also, if you have nothing keeping you where you are, take a look at that other guy's job offer :)
Enterprise edition lets you rebuild online. If not, take it offline to rebuild it. We rebuild a huge one weekly, it's on an EMC VNX SAN configured in RAID 60 (not ours, it's managed services) and capped at 5000 IOps. This is the only DB that resides on that disk. The table is just over 1 billion rows and is 1.6 TB in size, it's a clustered index. On it's worst day it took 1 day 20 hours to rebuild. On average it takes about 10-15 hours to rebuild it. If it's not above 30% fragmented we re-organize it, which runs on average 4-10 hours. These will vary between hardware of course, but hope it gives you an idea.
That appeared to work. Thank you so much for your help!
As far as I know, this is the most-correct way to do it on SQL Server. Alternatively, you could write a function that uses a fast-forward cursor. In my experience that still performed extremely fast for concatenating a 60,000 row result set. One thing you most certainly SHOULD NOT do, is something along the following lines: SELECT @myCSVList += ',' + ColumnName That *is not* supported and will absolutely break in different ways, depending on how the system decides to optimize your query.
thank you... i even tested it ;o)
Can you give me an example? I'm pretty sure I brute forced the `ISNULL` in every possible place. It didn't work.
Mind explaining what that is? Right now I'm trying to fill my resume by overseeing the implementation of a portal (LookingGlass). So lots to learn there, like some pseudo-sharepoint stuff, but that's about it for me at the moment. 
Ehh it's not so much the syntax that bothers me, it's just that this application of FOR XML isn't what it was designed for. The fact that this is arguably the best way to do it is what's frustrating.
So Microsoft offer certifications for a range of their various platforms, Server / Exchange / SQL etc. In order to earn a certification you need to sit a series of exams. [There are 3 to earn your MCSA in SQL Server](https://www.microsoft.com/en-au/learning/mcsa-sql-certification.aspx). After which you can sit a further two to expand this to an MCSE in Data Platform or Business Intelligence. These are generally well regarded industry certifications, with the caveat that they are obviously Microsoft centric. The great thing about earning an MCSE is that it gives you a lot of direction on how to learn and develop SQL as opposed to randomly trying to run some queries. I would suggest it's an excellent certification to get, but it does take a bit of work. I think it took me around 2 months studying for each exam while working full time, though I wasn't focusing 100% of my attention on earning it ASAP or anything. The below is a sample quiz as to the type of questions you might face. There is a lot of good public study material on this content so I wouldn't stress if it seems hard at first. http://www.sql-server-helper.com/free-test/sql-question.aspx
Yes - the MS exams. Thanks for the info I will forward then along when I get home. I know she was searching for a while and all the prep courses were thousands.. Not sure if there is 3rd party prep or any study materials to help her. I'll take a look through these links tonight. Thanks again 
Just curious, do u have access to sql server reporting services? If so, you could, you could let SSRS do the pivoting for you in a matrix and avoid all that code up there. Is this being used in a application? 
oh ok
This looks great. Thank you. She's gonna start with the trial. I assume you used it in the past. Worth it?
Well done. In my experience, the best way to protect the database from the failure is to make regular backups. I'd recommend you to make backups with the help of a third-party tool.
I'd use a sub query, WHERE UserID IN (SELECT UserID FROM ... WHERE ... 'welder' or 'Joiner')
I'm using http://sqlbak.com/ and I'm happy. The price is not high, even it has a free plan. The benefit is that SqlBak is SAAS, I can manage my backups from any place.
I highly suggest Stanford's databases course: https://lagunita.stanford.edu/courses/DB/2014/SelfPaced/about
 UPDATE User SET TxtField1 = CASE WHEN ISNULL(HasWelder, 0) + ISNULL(HasJoiner, 0) = 2 THEN 'Valid' ELSE TxtField1 END, TxtField2 = CASE WHEN ISNULL(HasWelder, 0) + ISNULL(HasJoiner, 0) &lt;&gt; 2 THEN 'Invalid' ELSE TxtField2 END, FROM User U OUTER APPLY (SELECT TOP 1 1 AS 'HasWelder' FROM JobProfiles JP WHERE JP.UserID = U.UserID AND JP.Title = 'Welder') HW OUTER APPLY (SELECT TOP 1 1 AS 'HasJoiner' FROM JobProfiles JP WHERE JP.UserID = U.UserID AND JP.Title = 'Joiner') HJ You can split it into two less-convoluted updates that just do EXISTS instead of outer applies (one that hits valid and one that hits invalid) but this will let you do both in one. Rather than doing the sum thing you could also go off of HasJoiner IS NOT NULL AND HasWelder IS NOT NULL for valid and HasJoiner IS NULL OR HasWelder IS NULL for invalid. 
The only problem is that the years can change and vary in quantity (2-4 years, depending reporting period).
That's so incredibly convenient, but I'm on SQL Server. My mistake though. I should have specified.
Thanks! I appreciate all the responses 
Thanks this is great
&gt; I really just want some sort of string aggregate function. Oracle, PostgreSQL, MySQL, DB2, and SQLite all have it. Really? I did not know DB2(400) or Oracle had such things.. EDIT: Sure enough: Oracle: http://docs.oracle.com/cd/E11882_01/server.112/e41084/functions089.htm#SQLRF30030 DB2/400 is bastardized, arguably to the point of being unusable, and does NOT have the LISTAGG function that DB2 LUW has. 
Post your sample data and I'll have a look
Substring is what you need in this case, but this is really a terrible data layout for a database. If it is a 1 time thing then I guess substring and lengths and a lot of back and forths. If you import this then you would have a single row where the delimiter separates the string into columns. If you want to use * as a delimiter that is fine, but in your case I would have the file be a carriage return instead of the *, and another delimiter that creates a column for name, another for age, etc.
In other words I would want to see something like the below Name,age,sex Kevin,22,m Matt,18,m Amy,30,f If this is in an excel file right now, I would do a data validation delimiter of a star. Then copy pate with transpose then data validate again where the delimiter is a space. Add column names and then import. 
 It doesn't sound like you've looked at the book for more than 10 minutes yet. If you come here with an actual problem and show us you've out some effort in to figuring it out, we are usually able to show you which page of the internet your answer is on.
I'd be happy to help you but as /u/drunkadvice said look at the course, look at the problem and post a question. If you really get stuck we can communicate over Skype. Basic and introductory SQL is actually very easy, it's one of the more straight forward languages and concepts to understand. I really enjoy sharing knowledge and helping people learn but any type of IT/programming takes time. Further SQL is a language where you build on concepts you learn. If you can't do an inner join, you won't be able to understand sub queries, CTE's etc. 
Sure so a part of solving any computer problem is breaking it down into little steps and combining it together. So you want to: Do a select statement that returns their last name and first name. Concatenate the last name column, a comma, a space, and the first name column. Limit the select statement with a where clause to only show last names that start with M to Z Sort the results by last name in ascending. I am assuming this is MSSQL and you have access to a SQL express or equivalent? Try solving each of those individual steps and let me know where you get stuck 
So your other two steps is how to concatenate, see this example [here](https://msdn.microsoft.com/en-AU/library/hh231515.aspx). And how to limit your query by a where clause, see example [here](https://msdn.microsoft.com/en-au/library/ms188047.aspx).
Unhelpful error messages from Oracle for $500.
Your CASE returns a single value. If you want it to restrict the number of rows returned, you need to compare it to another value (and add a WHERE after the join). WHERE clauses always have to have statements that can be evaluated as either true or false. So you might have = 'no' directly after END. Also don't alias it - unless you intend to return it as a column in your recordset, in which case(heh) the CASE statement goes between the SELECT and the FROM. You also need another WHEN before the second T.genre and get rid of the T.genre immediately after the CASE.
also DBMS_OUTPUT not DBMS_OUTFIT
Thanks! I didnt notice this, I have corrected it, but still getting an error. Error(17,1): PLS-00103: Encountered the symbol "DECLARE" Error(22,4): PLS-00103: Encountered the symbol "end-of-file" when expecting one of the following: ( begin case declare end exception exit for goto if loop mod null pragma raise return select update while with &lt;an identifier&gt; &lt;a double-quoted delimited-identifier&gt; &lt;a bind variable&gt; &lt;&lt; continue close current delete fetch lock insert open rollback savepoint set sql execute commit forall merge pipe purge 
What are you trying to do? Are you trying to get JJJ into your results? If so put the case statement in the SELECT part of the query, if you are trying to limit the results of your query based on the case statement, then you need an AND before CASE and something to compare it to after END.
What this guy said. Remember, no FUNCTION under WHERE! Doesn't matter the RDBMS platform they always force a FULL TABLE SCAN for that JOIN. 
Add a / on new line after both END statements (Function and Declare)
Holy shit, that worked! I was told you didnt need those because it was an older thing Oracle used to do, thanks!
If a field is used in the select section but not aggregated on, you need to add it to the group by section.
Everything you need to know about Oracle in one statement: SELECT 1 FROM DUAL WHERE '' IS NULL; Simply put, don't use it. Edit: This variation is even more fun: SELECT 1 WHERE '' = '' -- This statement is false 
How to recover an oracle backup to a new host: Spend 2 days in rman wondering what an ST_TAPE is. Give up and pay $1000s for an expert to do it.
This worked like a charm. Thank you! 
In other news, Google is a global search engine leader
Without knowing your system I can't give you the exact syntax, but you may have to list out the columns and put the ucase statement on whichever columns you want to capitalize. 
Thank you. Yeah I wanted to be lazy and say SELECT * and also say UCASE() all in one; but it doesn't work that way. I'l have to identify columns one by one. 
ive done this exact thing :D QUALIFY row_number() OVER ( PARTITION BY store ORDER BY value desc ) &lt;= 5; you need to mess around with 'store' and 'value' so that its looking at the right fields.
this query returns employees whose boss's boss doesn't have a boss
ohhhh...thanks! I was so stuck trying to figure it out. This is all I needed!!! =-D
Works fine if you add forward slash, which executes the most recently executed SQL command or PL/SQL block which is stored in the SQL buffer. It acts like "RUN". CREATE OR REPLACE FUNCTION ship_calc_sf (p_qty IN number) RETURN NUMBER IS lv_ship_num NUMBER(5,2); BEGIN IF p_qty &gt; 10 THEN lv_ship_num := 11.00; ELSIF p_qty &gt; 5 THEN lv_ship_num := 8.00; ELSE lv_ship_num := 5.00; END IF; RETURN lv_ship_num; END; / **Function created.** DECLARE lv_cost_num NUMBER(5,2); BEGIN lv_cost_num := ship_calc_sf(12); DBMS_OUTPUT.PUT_LINE(lv_cost_num); END; / **11**
If you always wanted your address formatted like that in future, you could either create yourself a view with the formatting you want and just refer to that in the future or update the table in place.
TitleID | Title | StudioID -------|-----|-------- 001 | Iron Man | 001 002 | Thor | 001 003 | Captain America | 001 004 | Superman| 002 **Without the WHERE clause:** SELECT T1.Title, T2.title FROM Titles T1 JOIN Titles T2 ON T1.StudioID = T2.StudioID Title | Title -----|----- Iron Man | Iron Man Iron Man | Thor Iron Man | Captain America Thor| Iron Man Thor | Thor Thor | Captain America Captain America | Iron Man Captain America | Thor Captain America | Captain America Superman | Superman 1. The same record can join to itself, giving you "Iron Man, Iron Man". 2. You have the same pair twice, but in opposite spots ("Iron Man, Thor" and "Thor, Iron Man"). By adding the WHERE clause, you ensure that neither of these happen. **With the WHERE clause:** SELECT T1.Title, T2.title FROM Titles T1 JOIN Titles T2 ON T1.StudioID = T2.StudioID WHERE T1.TitleID &lt; T2.TitleID; Title | Title -----|----- Iron Man | Thor Iron Man | Captain America Thor | Captain America Edit to add technically, the WHERE should be part of the join for reasons I can't verbalize: SELECT T1.Title, T2.title FROM Titles T1 JOIN Titles T2 ON T1.StudioID = T2.StudioID AND T1.TitleID &lt; T2.TitleID; 
Thanks a bunch! I'll experiment with other relational operators to see what type of information is rendered. 
Thanks.
&gt; return a single column I seem to have missed this phrase... Thank you so much for your help. Just out of curiosity what is the `st.name`? Is that a view? 
That's just what I picked for the table alias. In the FROM clause I have `FROM sys.tables AS st`, although I left the AS out above (it's implicit). You then use the alias to refer to the table instead of the whole table name, which is helpful in some cases (e.g. with fully qualified table names or very long ones) and mandatory in others (e.g. when joining a table to itself).
This is a good answer and the correct use of dynamic SQL, which is what the question is trying to lead the student towards.
I know, but I can't quite figure it. Should I use an outer join instead, but along with the self-join?
The recursive cte section. Oh joy
Hint: what does the error read exactly? 
I fixed the query, but I still get an error. I can't get it. SELECT Sa.Lastname, S.Studioname FROM Salespeople Sa INNER JOIN Studios S ON (Sa.SalesID = S.SalesID) SELF JOIN Salespeople SS ON (Sa.SalesID = SS.Supervisor) AND SS.Supervisor IS NOT NULL;
pretty sure Eployee was just a typo
there is no keyword SELF use INNER instead 
Are you sure? I have been using a SELF JOIN repeatedly.
oh man..you're right...I feel like an idiot....darn...
Thanks. I've actually used Substring as well, but the problem is that there will be Japanese characters in the file, and each Japanese character is like 2 or 3 bytes each, while an ordinary English character only contains 1 byte. So far, I observe that Substring uses [per byte] instead of [per character]. So if my range is 1 to 2, I can pick up [Jo] but I can't pick up any Japanese characters at all...
I'm aware that an excel version is far easier than what I'm working on at the moment. Hopefully, I can convince them to use excel instead.
I wish all my SQL was that easy.
Select SS.lastname, (etc)? 
I figured it out. Thanks! 
The paragraph above it pretty much sums up my networking class's subnetting assignments.
There is a name for storing data you could calculate; caching. Admittedly this is an in-table cache, but the usual caching rules apply; you're essentially trading off time for space, you save CPU cycles, in exchange for disk space. The trade off isn't free though as you now have a cache to maintain and preventing that going stale or drifiting out of sync may be tricky. Whether it's good practise really depends on how much time is saved by the cache, how often that time is saved, how much overhead it is to maintain and query ergonomics. (Overhead would be influenced by who can write to the table over time etc, ergonomics being how much easier your job becomes with the extra column) Depending on the complexity of the query and the context, your decision will vary wildly. Run some sample queries, measure times, make a choice.
TL;DR: It depends.
Welcome to the world of SQL!
I think it looks much more complicated than it is because of the wording.
&gt; It depends. [SQLSkills](http://sqlskills.com/) has a company T-shirt with this silk-screened on the back. Actually, it says something like &gt; It depends &gt; ....is only the beginning of the journey 
Yeah, I can't figure out what's noteworthy about this post. It's not a stupid exercise, it's not brutally difficult, it's not redundant, it's a good stepping stone demonstrating self joins. The best I can figure out, is that the textbook uses comic sans, and is really inconsistent with it's use of quotation marks.
The first time I ran into self-joins with a textbook style HR table, I thought I'd never run into one in the real world. But, I have a custom software that I built for HR that does this very thing. Every day it audits a one-size-fits-all HR table for over 200 different scenarios that will cause problems, and several of them have to do with things like "Find all employees who report to a terminated supervisor" and "Find all employees that report to themselves" and "Find all supervisors that are flagged to have employees who have no employees". When writing all these queries, I feel like I'm back in college. 
I believe the keyword in SSMS is "persisted" when using a calculated column. "A computed column is a virtual column that is not physically stored in the table, unless the column is marked PERSISTED." https://msdn.microsoft.com/en-us/library/ms188300.aspx
I love it. The only SQL question I've encountered for which the answer does not begin with "It depends" is "Does SQL need more memory?". That answer is always, of course, "Yes".
Purely from a normalization standpoint, your statement is true. However, for the sake of performance it's sometimes better to denormalize. For example, if I have to do a somewhat expensive calculation every time I retrieve the row, and I retrieve it a lot more than I write it, then it makes sense to store the result of the calculation. As others have said, "it depends". 
It's not too strange. In T-SQL, dividing two integers returns an integer. 11/10 evaluates to 1, and the ceiling of 1 is 1. Try SELECT 11/10
I disagree. The question was, "is it bad practice" &amp; the answer is straight forward. There are corner cases where an experience person deviates from the norm. I agree and do it as well. This doesn't make it a "good" practice. If a new driver asks "is it bad practice to speed in front of a police officer" the answer is yes. There are certainly corner cases where it's necessary and acceptable. Agreed. But this doesn't change the answer to a very basic question.
In this example it's also "bad to sort in SQL" (there is no order in a set) and the use of a function in the ORDER BY because you cant use an index this way. I'd choose the lesser of 2 evils here.
I'll add one more to the "it depends side." Here's an example: money. Right now I'm working with a database, and invoice amounts are determined in a calculable way (e.g. I can just do a join on the number of products you purchased). Maybe prices change down the line. I often find myself making a receipts table which includes the order info as well as the quoted price. 
What's the plan of the full big query look like?
It's all about explicit and implicit data type conversions, or in this case, lack there of.
I don't get it.
We do something similar with financial data, we are a multi currency business but report internally in EUR and GBP so we have the value amounts converted and stored in the table along side native currency. Storage is cheap time isn't .
Yeah, buy the training book and study it. Nothing worth having comes easy. I set 2-3 hours of study time after work and 8 hours every weekend. 
I love hearing this! I'd recommend getting the 70-461 certification. It will force you to study in order to pass the exam and you will have an excellent foundation to build upon. Best of luck!
 -- Also, doubling quotes would likely throw errors as it would result in unexecutable SQL. Just use parameters, it is why they exist. Edit: [Give this a read for lots of information](http://www.sommarskog.se/dynamic_sql.html)
Depends on the flavor of sql a bit. Basically pass the following in the field ';Drop Table dbo.Students;
To be fair, they exist mainly so queries don't have to be hard parsed with each execution. 
I just wanted to say that I got rid of it today by just restarting my computer. But now I know how to fix it next time, thanks!
It looks like you're trying to write dynamic SQL. While it's a fine thing to learn but, for the love of God, please don't use that anywhere where someone else will have to decipher what it's doing. There are way better ways to do whatever it is you are trying to do.
Some RDBMSs can be configured to cache parameterize &amp; cache ad-hoc queries.
I do this as usual!
&gt; This is especially noticeable for very large databases, in the order of tens or hundreds of GB. Tens of GB is "Very large"?
Tsql and plsql are very similar, you should be able to transition easily (I worked in a place with an Oracle backend with Microsoft BI layered on top for 5 years). No local temp tables so find something else (ctes for queries is what I did). You do have global temp tables. Window functions are relatively new to SQL but have been in Oracle for a while. If you haven't used them, learn them. Oracle has some nice functions (listagg and trunc are ones that come to mind that SQL doesn't have). But overall, it's all the same. You shouldn't have very many problems. 
Working with dates in psql is so much nicer. select sysdate as now, sysdate - 5 * (1/86400) as five_seconds_ago, sysdate - 5 * (1/1440) as five_minutes_ago, sysdate - 5 * (1/24) as five_hours_ago, sysdate - 5 as five_days_ago, trunc(sysdate) as today, trunc(sysdate) - 1 as yesterday, trunc(sysdate, 'MM') as first_of_the_month, trunc(sysdate, 'YY') as first_of_the_year, trunc(sysdate, 'MM') - 1 as last_day_of_previous_month, trunc(trunc(sysdate, 'MM') - 1, 'MM') as first_of_previous_Month from dual 
&gt; OP - as stated - does not work with sql. He is trying to learn it. If he's just starting, it's the perfect time to start building proper habits. If he manages the query properly (as a prepared statement), he wouldn't have this problem in the first place. So, the choices are: A) Help him do it wrong B) Tell him he's doing it wrong, and give him the right way (prepared statements). &gt;Edit: Are you Java guy or .NET guy? (I may regret this edit later...) Both, plus a "SQL guy".
I guess it depends on how many columns you want and the amount of rows you anticipate having. Keep in mind you can have different primary and clustered keys. Your primary key can meet your design logic (using your example, 45gtk0) and your clustered key can meet your performance needs (e.g., sequential integers). I don't see an issue with having non-numeric characters assuming you're not going to be doing a bunch of joins on that table. However, keep in mind it's just a ton easier to have a primary key that auto-increments. That said, take what I say with a grain of salt. I'm a DBA not a developer.
Not that I'm aware of. You can't modify the procedure in question? What I've done before (with mixed results, mind you) is to have the calling procedure set a flag in the database with the key values of the record it is inserting. The trigger then checks that flag table and ignores any processing on the inserted records which match. At the end of the trigger it can then clean out that flag table.
Okay, so we have the backslash escape, which isn't active in MS-SQL, some coding errors and more evangelism. What else?
can you show me how the backslash escape would work similar to the example above for the password? Before, I just did 'OR '1'='1 as the password and it worked. Would you escape one of the doubled single quotes with a backslash and be able to perform the 1=1 operation again?
don't use triggers.
I am a small time developer and don't use as many big words as /u/artsielbocaj but I would use what they suggested with two separate keys, where the primary key is the standard auto-incremented int for ease of use and speed of joins, but then generate a string for url generating of public facing objects. 
Curious how long this post will last...
another one day old blog spam account... awesome.
PL/SQL and T-SQL are quite different. Whereas in the former cursors are your go-to objects, they are almost always the worst in terms of performance in the latter. Also, the syntax is quite different. No horrible @ signs for variables and all that. Logically they are similar in that they provide procedural extensions SQL, but that's where the similarities end. But yes, you can transition. I went from T to PL too. 
If I'm correct, hashids.org provides a library to map an integer to a random string. It does not help with how to store them efficiently in SQL.
Here's the bigger question: How fast do you plan to insert? And how do you plan to handle the collision? Generating those values and guaranteeing uniqueness can be an expensive proposition. Regarding your stack overflow comment, there's nothing that says you cant seed your starting value to some arbitrary value. 
I think the point is that if you can map it to an int efficiently enough, you don't have to store it in the database at all. 
Can you use them, yes. Is it as efficient as int, no. (Honestly what I would do is store it as an int but have a function that would allow you to convert Base36 ID strings to Base10 int. Since you would need disk/CPU to search an index to look up a record.... or just compute its PK.)
It's not updating links. It's just concatenating HTML links to a random subset of varchar(max) fields in your tables. He's hoping your database uses varchar(max) or nvarchar(max) fields to store html fragments. My guess is that it's using indexes to look for tables instead of views since indexed views are so rare. Now, why he's dong that instead of `o.xtype = 'U'` I have no idea, but people doing SQL injection attacks are often not the brightest bulbs in the pack. He's only looking for tables with clustered and heap indexes, though, so maybe he thinks that he only needs to worry about tables that have primary keys? In any case, [it looks like boiler plate SQL injection](http://stackoverflow.com/questions/32305661/find-the-tables-affected-by-sql-injection). 
If you don't mind me asking, how did they get the injection in? Btw, I agree with /u/da_chicken regarding what it does. 
You already have the table structures, so you don't have to put any thought into that part. You can create them in any order that you like, it doesn't make a difference. 
You can store a huge amount of data in "tens of GB". Depends on if you have few large image/xml columns that take all that space or loads of text/int columns.
That makes sense, thanks. I was probably just over thinking it. 
Just because it is a lot of data doesn't make it a very large DB. EMR systems are often in the tens to hundreds of GB for a clinic, let alone hospital size. There is just a lot of data that gets generated in the medical world. I wouldn't consider the local community health center to have a 'very large database' when they have 50GB of history at their disposal. That's normal. That's not even counting the 100s of GB of image data stored elsewhere.
Explain to me how posting a 3 word reply "Don't use triggers" without any context, justification or reasoning is helpful or good advice? Triggers are a core part of a SQL database and exist for a reason. Blindly telling people not to use them doesn't help them understand the scenarios that they should and shouldn't be used. Further have you ever worked with ERP's? Should I just tell Microsoft to remove all their triggers from Dynamics GP / AX / NAV and that they should just re-engineer their entire database solutions? Sometimes also there are design constraints, both technical and political that don't allow you to follow the best practice approach. To address the second part of your post having a computed column as part of an inline function is considered a poor choice due to the performance hit. As I understand this is because SQL will be unable to optimize the scalar function and generate only a sequential plan. I don't see how this relates to a topic specifically covering altering triggers?
... That's fascinating. What was your query for this?
Thanks for the reply, I'm considering learning it, as just my marketing degree hasn't gotten me any calls for jobs I want and after reading Quantitative Analyst job descriptions, seems like something I'd love to do.
No, the cmdlets use SMO.
This is actually really common in the real world.
ECHO and HEADING, I did not expect heading to suppress row count but that is being used for other reasons. Reading over the manuals once more I'm not seeing any other applicable commands. There is no NOCOUNT and that option is not recognized.
First, I would immediately make the change to [explicit joins](http://sql-plsql.blogspot.com/2011/02/explicit-implicit-sql-joins.html). &gt;Author (AuthorID, Lname, Fname) &gt;Book Author (ISBN, AuthorID) &gt;Books (ISBN, Title) If you're just doing joins, choose any one table to start from. Let's choose 'Author' SELECT * FROM Author Ok, now we need to join book author to get the ISBN SELECT * FROM Author AS a JOIN Book_Author AS ba ON ba.AuthorID = a.AuthorID Now if we need to join a 3rd table we need to find a key that one of the previous tables has. In this case, Books needs to be joined on Book_Author as they share the ISBN. SELECT * FROM Author AS a JOIN Book_Author AS ba ON ba.AuthorID = a.AuthorID JOIN Books AS b ON b.ISBN = ba.ISBN The rest is just cleanup now to format the data how you want. SELECT a.AuthorID , a.LNAME , a.FNAME , b.TITLE FROM Author AS a JOIN Book_Author AS ba ON ba.AuthorID = a.AuthorID JOIN Books AS b ON b.ISBN = ba.ISBN
&gt;the soup by clause The tastiest clause
That got my script working, but I still need the results in ascending order by author's first name. Can I still use an ORDER BY to get them in the desired order or do I use something else? Appreciate the help, thanks.
Put the ORDER BY as the last clause. Example is on the side of the subreddit.
Thanks for breaking it down like that, it makes a lot more sense. Aliases b and ba don't work for me. I assume I can just use different letters. Also getting an error in line 6 (AS a) saying the command is not properly ended.
You should be able to use whatever you want as an alias. Try removing the 'AS' when aliasing the table names and end the query with a ';'
I didn't realize I needed to include AUTHOR.FNAME in both the GROUP BY and ORDER BY clause. Here is what I have now and it works: SELECT AUTHOR.FNAME || ' ' || AUTHOR.LNAME AS "Author", COUNT(BOOKAUTHOR.AUTHORID) AS "Books written" FROM BOOKS INNER JOIN BOOKAUTHOR ON BOOKS.ISBN = BOOKAUTHOR.ISBN INNER JOIN AUTHOR ON BOOKAUTHOR.AuthorID = AUTHOR.AuthorID GROUP BY AUTHOR.FNAME || ' ' || AUTHOR.LNAME, AUTHOR.FNAME ORDER BY AUTHOR.FNAME; Thanks again.
&gt;Just because it is a lot of data doesn't make it a very large DB. Like I said, depends on your definition of "large". Some people will look at the amount of storage, some will look at the amount of data. The writer of this article is probably of the latter.
that GROUP BY is redundant for two reasons -- constant strings never change, and FNAME is in there twice try it again with -- GROUP BY AUTHOR.FNAME, AUTHOR.LNAME 
furthermore, you don't need to join to BOOKS at all for this query
It doesn't matter. You can create all the tables, then create the constraints afterwards. It may be more convenient to do it in a particular order if you're using the table designer, but I don't personally know any SQL Server professionals who use SSMS's designers as they're slower, clunkier, and have been known to have bugs in them. Plus, you can't throw GUI clicks in source control or email them around; you *can* do that with script files.
Its winter here. Soup by is far superior to group by (and does the same thing)
Nevermind, thought he wanted a title too. Carry on.
He does need Books because without it the COUNT() would always be 1. For sake of semantics, it would probably be clearer if he did something like COUNT(Books.ID).
Oh, I know. I have to use a university's server for class. I have no idea why.
After rereading the OP, you're right. I missed that Bookauthor is a one-to-one match to Books. It would make more sense to have Books.AuthorId instead of a separate BookAuthor table, but given OP's current structure you're totally right.
A Check constraint needs to be a boolean expression. So think about how you would write the same expression for say a WHERE clause where it would also be evaluated as a boolean. Extra hint for this case you probably want to look into IN . 
So I need to code it in a way so that if anything other than M, F, m, or f is entered, it returns an error. Hmm. I know how I would do that in VB but not SQL.
Not like that no ... Just what you would put in the IF statement ... Lets say its a check constraint on Age ... and no on is ever allowed to be more than 120years old ... a boolean expression that evaluated to true for ages less than 120 is Age&lt;120 so the check constraint is CHECK (Age&lt;120) (on my phone so apologies for any formatting errors)
If you really want to do it this way then ,you have the tables round the wrong way. You should left join the table with more strict restrictions to the more general table. So: SELECT b.source , COUNT(a.source) / COUNT(b.source) AS pct_source FROM tweets b LEFT JOIN tweets a ON (b.tweet_id = a.tweet_id) AND a.source LIKE 'IFTTT' GROUP BY b.source HAVING COUNT(a.source) &gt; 0 ORDER BY 2 DESC; Or you could do it with a case statement on one table: SELECT b.source , COUNT(CASE WHEN b.source LIKE 'IFTTT' THEN b.source ELSE NULL END) / COUNT(b.source) AS pct_source FROM tweets b GROUP BY b.source HAVING COUNT(CASE WHEN b.source LIKE 'IFTTT' THEN b.source ELSE NULL END) &gt; 0 ORDER BY 2 DESC; There are all sorts of other methods you could use and depending on what your data looks like they may be faster or slower. But either of these should at least get you the results you are looking for. 
This is meant to be a higher level forum designed to help people learn and apply SQL. You're clearly just a child who is here to troll for attention, I am no longer going to give it to you 
Thank you. For some reason, I still get "1" as the output for pct_source. Is this a rounding problem with SQLite?
&gt; This is meant to be a higher level forum lol right "hey guys, I'm having problems with my homework, what's the difference between select and update?
The only time table creation order matters is when you're creating tables that use foreign key constraints *if* you create those constraints at the same time you create the table. You won't be able to create a foreign key constraint to a table that doesn't exist yet. So, if Department looks like this: CREATE TABLE [dbo].[Department] ( [Department_Name] VARCHAR(15) NOT NULL, [Department_Number] INT NOT NULL, [Manager_SSN] CHAR(9) NOT NULL, [Manage_Start_Date] DATE NULL, CONSTRAINT [PK_Department] PRIMARY KEY CLUSTERED ([Department_Number]), CONSTRAINT [FK_Department_Manager_SSN] FOREIGN KEY ([Manager_SSN]) REFERENCES [dbo].[Employee] ([SSN]) ON DELETE SET NULL ); Then you can't very well create this if `[dbo].[Employee]` doesn't exist. You'd get an error that the table referenced doesn't exist. Note that there's another error here that will prevent this statement from executing even if `[dbo].[Employee]` exists. I accurately followed the spec sheet given. However, since I don't know if that's part of your assignment, I won't tell you what the error is. Also, just a side note and totally not your fault, but it is HORRIFICALLY BAD DESIGN to use a social security number as a primary key. Not because it won't work, but because there are *tremendous* legal issues around SSNs since it can be used for identity theft. In every finance system I've used, the SSN is not used as a key. It's simply impractical.
What he's saying is the software you're using (SSMS) to talk to the server (SQL Server) can be run from your local machine, probably. There's no telling how the school has their network configured though, so unless you fully understand what you're doing it's probably not worth investigating yet. If you stick with this, over the next few months you'll think back to this exchange and realize you were silly. :)
Huh, that's all I had to do? Thank you so much! But what exactly does this constraint do? I thought it would return an error if anything other than M, F, m, or f were entered when inputting data for this attribute but apparently I'm wrong according to someone else on this thread. 
I'm a developer rewriting an existing ERP in C# along with SQL server. Part of my job is reverse engineering the existing business rules, untangling the existing mess, to disambiguate and reconcile literally everything in the system, to come up with better data models (the database schema was a soup of bad pratices with no referential integrity, indexes spanning across all columns in very wide tables with tons of triggers, no nulls just always pushing default values, dates stored as varchar, etc.) and elegant object relations based on sound principle of design that will satisfy the business requirements of features, performance, reliability but also external constraints like organizational or government standards related to POS and payment systems, etc. 50 % of my job is writing tests, implementing and documenting. 40 % is spent on modelling, analysis and remodelling. 10% is mentoring, teaching and helping others. 
Thanks for the advice. I agree with them too. In graduate school I had to use other programs to analyze data. I am trying to re-create those papers using SQL coding. 
&gt;But it is useful to apply a DBCC SHRINKDATABASE operation after a table was dropped or truncated because such operations create lots of unused space. I'm sorry, but what? IMHO this should only be considered in cases where a **large** table (as in a double-digit percentage of the total size of your database) is dropped *and* you aren't anticipating further significant growth of the database. &gt;If you need to make it regularly If you're **regularly** shrinking your databases, you need to seek some serious guidance in managing your environment. Especially since at the beginning of this blog post, it's stated: &gt;Also, we recommend not to change the AUTO_SHRINK database option from False to True, unless you are sure that you really need it. But I guess if you're doing it on a schedule with this company's software that they're advertising, it's OK?
###How to run `DBCC SHRINKDATABASE`. 1. Don't 
It's okay, this account, along with /u/JohnNilson, /u/2goor and /u/JoeGoldstein (I assume there are more) are just spam accounts. The mods dgaf though, so we keep getting this type of crap here and in /r/SQLServer (/u/Lincoln_burrows is a prime example there) from brand new accounts. [Proof that the mods don't care](https://www.reddit.com/r/SQL/comments/45ja9b/sgo_to_mes/) Edit: Add /u/LarryKeenan to the list...
Always run a DELETE or UPDATE as a SELECT first. ALWAYS ALWAYS! 
I've been doing database work for 20 years. I can honestly say the only time I've ever seen real problems is when there was no backup. Having said that I have at least twice seen huge problems because the customer thought they had a backup, but no one had ever tested it. A marketing company was adding new hard drives to a RAID 5. They gave the tech two new drives and said rebuild it and reload the data from backup. I told the tech before he went to see the customer to make sure the backups worked. Customer said,"Absolutely." They didn't want us to waste the time to test. Long story short he got the drives working, went to restore the backup and no data. Went back three months into a tape archive and every single tape was blank. Company went out of business a short time later was the last I heard. They managed to make it about six months trying to literally rebuild everything from scratch.
How is the user connecting to the instance? If it's through a non-Microsoft application, could you confirm that they can log in using SQL Server Management Studio? Another approach for more information could be to run a trace using Profiler to confirm that the login seen matches your expectations.
While we're at it, write your `WHERE` clause **first**.
Avoid "highlight"/"selection" executions as well. I can't tell you how many times someone has come to me and said, "I forgot to highlight the where clause" or "I forgot to highlight just the code I wanted to run and executed the whole script on accident." These are one of the main reasons why DBAs are such permission nazi's.
&gt; I almost feel like it would be better to start from scratch. I want to but I'm not allowed, my boss want our existing clients to continue to operate seamlessly while substituting all the legacy parts with my new architecture, so the reverse engineering process is very real since : * Nothing is documented in the legacy system. * Variable names in the code are misleading. * The code itself was built with the same kind of professionalism who were probably pasta advocates (ex : core business logic dependent on the UI, core depending on plugins). * Database columns could have been randomly named since you have no idea what the data in there is supposed to really represent. * The legacy test suite has been abandoned for years so the specs can't be recovered. * I also have to migrate the data from the legacy system into the new system while fixing corrupted/inconsistent data from existing clients. * I absolutely cannot break support for existing plugins and features. &gt; How long would it take for someone who knows very little about C#. C# is too specific since any developer can become operational within 1-2 weeks of trying out a new language, it mostly depends on your general experience with programming, if you already know your data structures, design patterns, etc. it should be quick, otherwise, just getting comfortable with OOP and also C# could take a while depending on your innate capabilities. Starting from scratch should be easier while having access to domain experts can speed a lot of things since my approach is usually domain driven. &gt; but a significant amount about SQL server It may help you to avoid pitfalls of database programming, although I'm very much against putting logic into the database and if you have no other choice, you could leverage your existing knowledge by offloading some computing to the database. &gt; For instance, creating a very basic point of sales system. No less than 6 months. More power to you if you can pull it off before that but I have strong reserves stating otherwise.
I don't have much time to read this at the moment but I'm really looking forward to reviewing this when I get a chance. Out of curiosity, does this address the ability to compress backups as well?
If you go to a SQL Saturday where David Klee is talking, he sometimes does a "war stories" session where, instead of a presentation, discussion goes around the room where people get to tell stories just like this (and he's got plenty of his own). It's fun.
This is good advice and we will see if we can reverse-engineer the procedure but it will be difficult as the one is encrypted and I do not believe we can view it at all, but I will ask around with the guys. 
I never run an UPDATE or DELETE without enclosing it with BEGIN TRAN/ROLLBACK TRAN :P
Is this a Windows-integrated or a SQL user?
Ran somebody else's code that took down the server. Everybody gets one... Right?
Windows-integrated...
I work at a lawfirm we keep notes on everything that is entered into a FILE. This would start with our software we use to put the notes in... Then they are put in a sql database. Then those notes are backed up nightly as most places need to do... Well a new guy dropped the Active table! It took 7 days to fully restore the table. One year... At a time :) 
Is this the only user with that setup? Were you able to confirm connectivity from that machine?
I do this. I write all but the delete/update table portion, check it, then add the delete or update 
It is the circumstances of the spam. These are brand new accounts with no post history. I understand spam happens and if it is from a couple week/month old account, then so be it. But these same users people keep.on.posting. So while removing a post or something is nice, another one will pop up. I'm happy to hit the report button (you can probably guess which reports are mine) but much of this spam could simply be prevented by disabling these accounts from posting by doing [some checks](https://www.reddit.com/wiki/automoderator/full-documentation#wiki_karma.2Fage_threshold_checks). I understand that people have jobs and being a mod isn't one of them, but understand that this isn't "one or two" posts slipping through. So when I keep seeing the same sites linked by the same people... there are only so many conclusions one can draw. &gt;I do not understand your "proof that the mods don't care" because when you click on it, you will see that link was actually removed by me 1day ago (while your post is 4hrs old). My comment there is 2 days old (as is my report) and [I don't see any indication that it was deleted](http://i.imgur.com/qKyfJR5.png). It doesnt show up on the post list, but I have to logout to see that because I reported it, so I apologize for bringing that up as an example. Anyways, thanks for the reply, I'll ease up and just keep using the report button. 
 SELECT convert(datetime, '23 OCT 2016', 106) -- dd mon yyyy Adjust the string to this date format and it should work. http://www.sqlusa.com/bestpractices/datetimeconversion/
You may be able to use SQL Profiler to at least trace the parameter names.
Thanks. I set up an automoderator rule for accounts which are &lt; 1 day old, but there was a typo, which I fixed either yesterday or the day before. We'll see if that stops it. It certainly is a frustration to me. For reference, I'm in GMT+10h. The other moderator is, I believe, in the U.S. but is unfortunately not highly active so I am hoping to leverage automoderator a lot more than was previously the case.
How about you have a crack at these yourself and we'll point out where you've gone wrong? This seems awfully like you've copy/pasted a homework assignment question.
Based on what I saw in Andy Klee's LinkedIn profile, I'm guessing there's no relation, or ay most a distant family connection.
For the first one, you wouldn't include the data type and WITH ROLLUP doesn't do anything if you've only got one group. You probably also want to display the game name. Select Game_name, Sum(Amount) From Revenue Where Date between #2015-01-01 and #2015-12-31 Group By Game_name That is by game. To get revenue by person, by game: Select ID, Game_name, Sum(Amount) From Revenue Where Date between #2015-01-01 and #2015-12-31 Group By ID, Game_name The second one you have is tricky. At the moment, your query would be counting all people who have any activity in that game in that month, not just the people who have started in that month. So first you would make a subquery that would give you the earliest date per player per game: SELECT ID, Game_name, Min(date) AS InstallDate FROM Activity This would give return you one row per person per game, with the earliest date they played that game. You can then slap that in parenthesis and give it a name, and treat it like any other table in your query, like so: Select game_name, count(id) FROM (SELECT ID, Game_name, Min(date) as InstallDate FROM Activity) AS Installs WHERE InstallDate between '2015-01-01' and '2015-01-31' GROUP BY game_name 
Yep, put a text file on the desktop and gave security permissions to that service account with no issues. I really can't figure it out.
This doesn't happen so much anymore since they took #temp tables out of BOL, but you still see a *lot* of really large tables being built in TempDB, overrunning the disk space available, and *bringing down the entire server* when it happens. Kids, don't do big stuff in #temp tables. Listen to uncle amaxen. 
If you guys need extra help on the mod team, don't be afraid to ask. I'm on GMT+1 so together we can cover the globe ;-)
Databases are easy: 1. Give full access rights to everyone, otherwise they will hassle you until they get them. Database permissions are far too complicated 2. Testing is for wimps, you are an SQL god, why use select when you really need to use delete or update Backups are boring (take too long) and why bother checking them? 3. Transactions really slow things down 4. Always keep your old SQL in SQL Developer/Toad/Whatever (good)and re-execute the statement you want from the middle of the block (bad)..... 5. Databases are multiuser, don't worry about reorganizing the DB during peak production. Seen all of these.... What fun, what overtime fixing them.
Here's one way.... select * from ( select company, avg(sales) avrg from tablename group by company order by avrg desc nulls last ) where rownum = 1 Another approach... with data as ( select company, avg(sales) avrg from tablename group by company ) select * from data where avrg in (select max(avrg) from data)
Have you read [the documentation](https://msdn.microsoft.com/en-us/library/ms189049\(v=sql.120\).aspx#TsqlProcedure) for how to create a FK constraint on a table? You have several significant problems with your data model: * [SSN is a poor choice for a unique identifier](http://databases.about.com/od/specificproducts/a/primarykey.htm) * Your gender field is too narrowly-scoped * `NAME` can **easily** be more than 15 characters (`Christopher` alone is 11, which means their surname would have to be 3 characters or less) * `RELATIONSHIP` can easily be more than 8 characters long (and should reference a whole other table to ensure that you're consistent with the descriptions of relationships).
&gt; You have several significant problems with your data model: Tell me about it. But this is the data model I was given by my professor to code. I've only been studying SQL for about 3 weeks and even I know this would never work in a real world scenario. I've already run into several logic errors and have had to change things around just so that it would work. About that link, though, I forgot to say that I'm using SQL Server. Transact SQL has fairly different syntax doesn't it? 
I guess you need to find some 3dr party tool that can do it.
&gt; I don't have much time to read this at the moment but I'm really looking forward to reviewing this when I get a chance. Out of curiosity, does this address the ability to compress backups as well? Yes, it is!
In what way do you want to "manage" them? Making sure that the backups run properly? Consolidate storage for backups? Other? Can they write the backups to a shared location on the network?
This. Make a primary directory and have sub-directories for each system with permissions specific to each.
which column in Query2 corresponds to your customer/dealer?
Buyer5mil in Query 2 corresponds to Dealer in Query 1 Thanks!
okay, here's your subquery i wasn't sure what "top 50 by location" means, so i simply slapped TOP 50 onto your outer query, so you'll have to work the rest out yourself, or offer a clarification SELECT TOP 50 dbo.DailyReport.locationcode , dbo.DailyReport.buyer5mil , dbo.DailyReport.elig , dbo.Auction_locations.Division , dbo.Auction_locations.Subdivision , SUM(dbo.DailyReport.Count) AS SumOfCount FROM dbo.DailyReport INNER JOIN dbo.Auction_locations ON dbo.Auction_locations.initials = dbo.DailyReport.locationcode AND dbo.Auction_locations.Division IN ('National','Local') WHERE dbo.DailyReport.buyer5mil IN ( SELECT tf.dealer FROM dbo.[tappy file] AS tf LEFT JOIN dbo.[SF Mappings] AS sf ON sf.[Customer ID] = tf.dealer WHERE tf.total_purchases &gt; 40 AND tf.Price250 &lt; 200 AND tf.NumClosedFactory / tf.total_purchases &lt; 0.5 AND tf.current_member IS NULL AND sf.[MOL MEGA ID]) IS NULL GROUP BY tf.dealer ) AS subquery GROUP BY dbo.DailyReport.locationcode , dbo.DailyReport.buyer5mil , dbo.DailyReport.elig , dbo.Auction_locations.Division , dbo.Auction_locations.Subdivision
I've seen similar weird oddities with windows accounts, typically it's something in AD. I use these two cmds to isolate if it's AD or something SQL. Run them on a working similar account, then run them on problem account, compare. xp_logininfo https://msdn.microsoft.com/en-us/library/ms190369.aspx SUSER_SID https://msdn.microsoft.com/en-us/library/ms179889.aspx
A compound join is a join containing several columns joining two tables.Compound joins are useful where multiple columns generate a unique key to join to another table. Sometimes a relational database has a unique key, which is a combination of two or more columns. In other words a FK to PK link contains multiple columns, so the join involving the tables is compound (multiple columns).
so an FK is just a constraint. define the column SSN and it's data type. then add the constraint as you have and then add the PK constraint as well. 
Right, I figured it out. I was doing my foreign key completely wrong. 
this is what that gets me Only one expression can be specified in the select list when the subquery is not introduced with EXISTS.
Yes. A folder for each machine, with subfolders for each database.
Sounds like the database is on a sql farm at your school. Set up team viewer on a machine there you control and hit the network that way, or if your school has a vpn use that.
Okay, given PCI compliance, that is the worst example to utilize in a teaching environment. You NEVER EVER want to utilize SSN as a key reference. First off I would recommend using a GUID for your primary key and depending on what version of SQL you are using to ensure that the ssn is always encrypted in the particular table.
please do a SHOW CREATE TABLE so we can see your datetime/timestamp column
Amen. I usually run a select first, see number of rows affected. then put it in a BEGIN TRAN/ROLLBACK TRAN, and make sure the number of rows is the same.
That is where I'm struggling and was able to get what you provided, thank you. I am trying to use the HAVING function with no luck.
I feel a little dirty just writing this, I feel like the problem here is really one of DB design - at the very least there should be a date/time for votes. But to do this in a simple if kludgy way it might look something like.... SELECT u.vote_number, count(u.id) FROM user_votes u INNER JOIN (SELECT userid, max(id) as lastvote FROM user_votes GROUP BY userid) x on x.userid = u.userid and x.lastvote = u.id GROUP BY vote_number We're putting a lot of faith in the identity field not ever having been reset or manually updated though. 
exactly my point about the anti-pattern
/r/domyhomework
Running just EXEC xp_logininfo 'troubled user'; gives me "could not obtain information about windows NT group/user 'troubled user', error code 0x5. Running SUSER_SID nets me results that DON'T match what is inproduction. However, the same goes for every user that I checked (meaning all of the working service accounts that are accessing different databases on this instance.) Is that to be expected in a VM replication situation? To have differing SUSER_SIDs? In production they all show up as 3 digit values for all users that I checked, in the DR site, they are all showing up as long ass hexadecimal values, starting with 0x0.... 
Try this if Oracle. Select * from ( select * from table order by employment_date desc ) where rownum = 1 I don't know what you're studying, but this will work. 
Edit: Figured it out. I used SELECT * FROM tablename WHERE employment date = (SELECT MAX(Employment Date) FROM tablename);
Thank you sir. I figured it out. Would you mind having a look here: https://www.reddit.com/r/SQL/comments/4607pa/oracle_queries_help_please/d01kovy
yup!
Try this little marvel https://yoursqldba.codeplex.com/ 
I need to be sure that backups run properly and sometimes change database backup plan. 
okay, my mistake... i used an older version of the subquery (copy/paste from my text editor) the subquery you want is the one here -- https://www.reddit.com/r/SQL/comments/462zxu/sql_server_getting_out_of_my_paygrade_writing/d022ma0 it has only tf.dealer in the SELECT list and GROUP BY
If I could paraphrase what I just read,you are trying to fit tee "realized" metric. My question would be - why, if you have the actual distribution data? Btw, looking at the data, I would guess it is more akin to power distibution. Anywho, since you want to calculate by months, why not create a matrix of # of leads by created month by appDate month, and then you can calculate totals, percentage to totals and running sums off of that? I do believe windowed expressions have been available in 2008r2.
Create a Dev environment and test it
AFAIK, you'll need to do something like DateVal = CAST(RIGHT(DateStr, 4) + '-' + CASE SUBSTRING(DateStr, 3, 3) WHEN 'Jan' THEN '01' WHEN 'Feb' THEN '02' [etc...] WHEN 'Dec' THEN '12' END + '-' + LEFT(DateStr, 2) AS DATE)
&gt; Can i convert a string to a date and then filter by the date all in my where clause? yes and if you could [please go and read the sidebar](https://www.reddit.com/r/SQL/comments/45yzz1/how_can_i_do_this_date_transformation/d018iil) about identifying which platform you're using, we might even be able to tell you which builtin date function to use
 SELECT customer_id , customer_comment = (SELECT STUFF((SELECT ',' + c2.customer_comment FROM customers c2 WHERE c2.customer_id = c1.customer_id FOR XML PATH(''), TYPE).value('.', 'varchar(max)'),1,1,'')) FROM (SELECT DISTINCT customer_id FROM customers) c1
okay, you'll be using the CONVERT function WHERE CONVERT(DATETIME,DateString,106) &gt;= '2015-01-01' AND CONVERT(DATETIME,DateString,106) &lt; '2016-02-01' note how the upper end of the 12-month range is open, i.e. using strict "less than" -- this is preferable to using BETWEEN and trying to nail the last datetime value of the period
Is there some way i could do something similar to below WHERE CONVERT(DATETIME,DateString,106) &gt; Today()-365 so i wouldnt have to insert new date ranges everytime i ran the query
There are multiple ways to do this, parameters/ dateadd&amp;datediff to get lasst full year or last full 12months etc If you haven't got an answer in 10mins when I get home I'll post the sql 
sure ... &gt; DATEADD(YEAR,-1,GETDATE())
Formatted: *** BACKUP DATABASE myDatabase TO DISK = 'D::\myDatabase.bak' WITH COPY_ONLY *** I believe /u/2goor answered the question, but it is also possible to mirror on a backup operation as well: *** BACKUP DATABASE myDatabase TO DISK='Z:\SQLServerBackups\myDatabase.bak' MIRROR TO DISK='X:\SQLServerBackups\myDatabase.bak' ***
The only guaranteed way to know if a backup can be restored is to restore and consistency check.
A test restore is the only way to be sure! It's good practice to test your backups along with your recovery plans; not everyone has the time, resources or budget though. If you're in a pinch, you can restore the backup to the same instance, just under a different database name. It's also a good idea to integrity check the database before backup and after restore, that way you can check for corruption that might exist in the database already, or is being introduced due to a problem in your backup media (this is pretty rare, but again - good practice) If you're a total newbie with support responsibilities, I would suggest running through a restore scenario and documenting the steps - you'll thank yourself later ;) Some people get really fancy and have automated routines that ship their backup files to their recovery servers and restore them on a regular basis ... Assuming it's SQL Server here btw :)
&gt; Ok so more generally you don't ever want to reply on some combination of business logic inside SQL. I don't know how your application handles payments etc, but what if someone processes $0 payment or a negative payment etc your view will not always account for that. Do you mind elaborating a little more on this? I mean, this is how my data comes out of the database, so I'm not sure what else I can do to flatten it out for Excel...
I would need the source code to recreate a new procedure to replace it with something that does its job
&gt; I want to report on each procedure a patient received on a given date of service. &gt;I would like to report on one line per patient and date of service. You're probably in for a world of hurt unless you specifically limit the available procedures. Essentially what you're doing is a pivot. You're taking a tall list of services and trying to show them all on one line per patient per day. This is easy when there are only one or two services you're looking for, but gets harder for each additional one, and if the list is variable or bigger than like 10, it's going to get really ugly. Just think about the Column Header names and that will get you started thinking about what needs to have happen.
A backup is useless if you can't restore it. How do you know if you can restore it...until you attempt to restore it? Restore your backup(s) to a non-production instance **regularly** to both practice the process and validate that your backups are good. Test the various recovery scenarios, and make sure that you can meet your Recovery Time Objective while restoring to your Recovery Point Objective. https://www.brentozar.com/archive/2015/09/why-rpo-and-rto-are-actually-performance-metrics-too/ https://www.brentozar.com/archive/2011/12/letters-that-get-dbas-fired/
I'll check it out. Right now I'm [here](http://pastebin.com/0EKdfYH9). The [data](http://www.filedropper.com/sampledata2) is in the correct format but I think I need to unpivot and then group by and sum the days. Thanks so much.
oh, yikes... I didn't realize that was what you were trying to do. Well, I'm still not sure what you're trying to do. Where do the percentages fit in that? You've got a column for every day, and you want a row for every month? Then each cell would have the % complete of the total submits for the month? 
I didn't realize what I was trying to do originally either, and was just trying to hack and bash my way through the SQL until it worked. Your approach is obviously a lot more elegant and I'll learn a lot from it. Thanks!
Never seen stuff before, this is great
So, probably not the right tool for the job but I'm using SQLite. *&lt;pauses&gt;* - once everyone stops laughing I'll explain. I'm a multidimensional developer and I stepped into this project recently to help out and I noticed the cube was full of 0's (bad) that I need to replace with #missing (good) and this is my primitive way to do it. I'm good with scripting and have stumbled my way through SQL on my own out of dire necessity. I prefer not to learn graphical apps, but if there's a way to code in 2008R2 (it's on my laptop) I could do it there, if it's quicker. Each SET query is taking about 15 minutes to complete. 
To be honest, I'm really not at all familiar with SQLite, but hopefully someone else here is. Are you having to re-run this regularly? 15 minutes is a lil' slow, but doesn't sound too terrible for what you're having to do, especially if you're working through some kind of proxie system like Lite. If you're only having to do this one go around (or possibly as a scheduled task for weekends or something), then I think the time is reasonable as long as you assure you're not getting new data written over while you're trying to complete the process. If that were the case, you could consider setting up a temp table and moving things over / purging, so you can drastically reduce the data set each go around. Your essential problem is that these updates are going to necessarily require an entire table scan each time they run to see if they meet your update conditions and may be fragmenting the table / messing with the indexes (which is costly for the next batch) in the process. It would be technically possible to use a bunch of CASE statements and boolean logic in your UPDATE/WHERE to do this in smaller batches, but I think the risk of coding it improperly outweighs doing it piecemeal for the kind of performance you're currently experiencing. But - hey - I'm just some random guy on Reddit and I haven't seen your system or data. 
That's really interesting, thanks for the info. In this case I am sure -- info can only go in and come out one way -- but it's good to keep in mind for the future.
I'm not sure if this matters in your case, but one quick thing to remember is that setting Match1="" isn't the same thing as making Match1 null.
Thank you so much for the advice "remove inset, delete and update permissions from the account/log-in" - that was perfect. Much appreciated, thanks again!
instead of executing 50 seperate statements, you can do this in just one update. Somthing along the lines of UPDATE AZ_0 SET BegBalance = case when BegBalance = "" then NULL ELSE BegBalance end ,Match1 = case when Match1 = "" then NULL ELSE Match1 end ,... Currently, you are scanning the entire table for each column ,unless there is an index, but there won't be on all columns (I hope). If you do it in one statement, you will have a full table scan, and a scalar expression on each row, that should be one hell of a lot faster than what you are doing right now. 
You can try to move the filter conditions that use Sin and Tin parameters to the where clause. 
Look at Galera
I don't quite understand?
 PARAMETERS Sin Short, Tin Short; SELECT StudentSparesBackup.ID , StudentSparesBackup.StudentID , StudentSparesBackup.SpareSlot , StudentSpares.ID FROM StudentSparesBackup LEFT JOIN StudentSpares ON (StudentSparesBackup.SpareSlot = StudentSpares.SpareSlot) AND (StudentSparesBackup.[StudentID] = StudentSpares.[StudentID]) WHERE (((StudentSpares.StudentID) Is Null) AND ((StudentSpares.SpareSlot)="IsNull") AND ([Sin] = StudentSparesBackup.[StudentID]) AND ([Tin] = StudentSpares.SpareSlot) ); 
Compete your sum operations inside of sub queries and then join into the outside most select statement. They must be summed separately. 
CASE, of course, thank you! That's exactly what I need.
&gt; I only have read access to the database, so I can't create the lookup table for categorizing the procedure codes. You can still create a temp table, which will likely perform better than a CTE. &gt;SQLServer is calling code and cat invalid column names They aren't reserved words, so something else must be going on. Wrap the column names with square brackets (`[code]`) to safely use them as identifiers.
This is really useful, thank you! 
Appreciate your reply, Thanks
Sure, let me know if it works. In the first select statement i see if forgot the comma after C.outcome so i added it.
To be even more concise, you can use NULLIF([yourcolumn],"") instead of the CASE statements
What about 'between' is evil? Is it better to use something like where date &gt; '2016-02-16' and date &lt; '2016-02-20'?
Are you using Microsoft SQL Server? If you are then it comes with a tool called profiler which you can use to gather the exact statements that are running against the server. 
Thanks! I'll look there.
Integer subtraction does not work with the DATE data type, only DATETIME.
The difference between what you wrote and what between does is that BETWEEN is an inclusive comparison, and yours is exclusive. If you wrote yours like this: where date &gt;= '2016-02-16' and date &lt;= '2016-02-20' It would behave exactly the same as between. There is nothing objectively wrong with BETWEEN, the author of that article just has a personal dislike for it. His complaints amount to: "It's an inclusive comparison and some people may not realize that besides the fact that between is almost always used as an inclusive comparison in common language" "The value on the left is assumed to be smaller than the value on the right so you have to validate your user input just like you would have to do if you were using &lt;= and &gt;=" And finally "standard date comparison rules apply meaning that datetime values without a time specified are assumed as having a time of midnight, just like every other method of datetime comparison"
"If you are including actual code in a post or comment, please attempt to format it in a way that is readable for other users. This will greatly increase your chances of receiving the help you desire." Please format your code (see sidebar)! By putting 4 spaces at the start of each line your SQL command can be made much more readable.
[No it doesn't.](http://i.imgur.com/BFZmpfq.png?1)
What date does that integer translate to? Is that a sequence?
you asked for the easiest, but you neglected to mention which platform you're running so here you go -- SELECT FROM_UNIXTIME(1418580338) AS mydate mydate ------ 2014-12-14 13:05:38
I believe its in seconds.
Second second
i don't use SQLite but [da manual](https://www.sqlite.org/lang_datefunc.html) says this should work -- SELECT datetime(1418580338, 'unixepoch') 
I think I would be able to answer basic interview questions, but my problem is getting the interview. 
So I must of accidently deleted the close parenthesis? Thanks. It seems to not hitting an error. Now how to define it properly. Thanks for your time.
AH okay thanks for that.
What you are looking for is an insert / update trigger. I'm not sure what database platform you are using otherwise I could help a little further. the trigger will "trigger" when a user attempts to insert or update when they do that you can take the value for birthdate and compare it against sysdate to get their age. if it doesn't match you can raise an error other wise you are good to go. 
So your experience with SQL is literally that you have completed courses from Code Academy and Khan Academy. I presume they offer some form of certification? You can add those two as a line in your qualifications. If you really wanted to demonstrate to someone that you understand SQL then you build build or make something, and then slightly exaggerate about it on a resume. I.e. Build a web service that logs the IP of someone visiting a page to a SQL database and sends an email with the details of that IP. Now embellish slightly on your resume and write about how it was a project for a local community. This is just one random example. From what you have written the extent of your knowledge is that you have done an online course. So if you wanted to be truthful write on your resume that you did an online course and let them determine if if they think that adds value. Really though while those courses are ok, I wouldn't hire someone if that was their only qualification. I would be looking to get some industry recognized certifications like an MSCE in BI or equivalent depending on your preferred flavour of SQL. I would be backing that up by finding a real world problem and using SQL as one of the tools to help solve it. If you're sole plan for your career is to say I can write some select and update statements and maybe read an execution plan you're really not doing yourself any favours. You're painting yourself as a simple tool or a cog, and any potential employers will treat you as such. You want to present yourself as some type of professional who is able to solve business problems using a variety of tools. SQL would then be one of the tools you would say you are proficient in using. 
hmm ok :) will try and do that, thanks!
This is great advice. Your skills are in demand, don't get discouraged. That BS in math will serve you well through out your career. My first job was landed through waiting tables, of all places. I have an accounting degree and a somewhat regular customer mentioned a technical problem. I answered using the knowledge I had obtained from a database class I took during the pursuit of my degree. Turns out he owns a law firm and offered me a job a few days later. Fast forward and my next career step came from a technical recruiter, TekSystems. Post your resume on Dice, Monster, Linkedin and they will find you. Let me know if you have any questions. You got this.
&lt;triggered&gt; It's easy to find a guide through Google, but here's the easy way: In Object Explorer, drill down to Security -&gt; Logins. Check your user list, the "main" login of yours should be there. If you right click on Logins, you can create a new login. Guessing you'll want SQL Authentication (Mixed Mode ties into Windows Active Directory for logins, SQL is just a regular, standalone login). From there you can give permissions (or you can create roles and assign them to the logins) and set passwords and everything. Once you get everyone connected through their own accounts, change the password on the "main" account and never give it out again. Make sure no one besides your department knows the sa password as well.
I know there are people who hire report developers, but knowing the ins and outs of SQL will get you farther than knowing SSRS alone. The report will only perform as good as its SQL. I'd suggest rounding yourself out if you want to open up more opportunities going forward. That being said, SSRS isn't a bad launching point. 7 years is a good number and you should be able to find a place that will let you ramp up on SSIS. In my opinion, SSRS is disrespected by devs which results in many reports that look like a toddler put them together. If you're truly a great report dev, you can make your report work fast AND look beautiful. If you have that going for you, you can run with any report tool. SSRS may not be around forever, so if you have the eye for a good report, you're set. 
Security is your first concern. All those people sharing the same high-priv login is no bueno. At the very least, if you *have* to stick with SQL authentication, create three for read, read+write, and read+write+DDL, add them to the corresponding database roles in each of the databases, and start handing those out to the appropriate parties. If you have many databases, this is going to turn into a combinatorial nightmare. Try to get AD going and use security groups. Once everyone can get their reports, disable the old one and spin the password for it. Harden the rest of your stuff. Create admin logins for you and your boss, and use those for administration. Change the sa password and disable the account. From a performance perspective, given his "kill everything that runs long and see who screams", i'm guessing there's no index / statistics maintenance. Keeping index fragmentation and statistics up to date can help immensely. Then you'd want to look at actual indexing strategies for your worst-performing queries, and start tuning those. Look at your storage configuration - are your data, log and temp files on separate drives / LUNs? Have you enabled Instant File Initialization for the service account? Put antivirus exclusions in place? Honestly, is it worth it? It sounds like a veritable museum of what not to do ~~wrong~~. If you've got the stomach for it you, it might be fun to tighten that whole system up and own the performance benefits, because it sounds like you'd have carte blanche to do things the right way, since I don't think your boss would recognize "the right way" if it bit him. If the current situation is causing you heartburn, with executives yelling at you all the time, maybe it's not worth it. EDIT: You may want to look at some of the scripts that the Big DBAs have created, that can help you a lot in figuring out what's going on and what to fix. Specifically, look for Adam Machanic's "sp_whoIsActive" script and Brent Ozar's "sp_blitz" and "sp_blitzindex" or whatever he called it. they will both give you some quick insight into your weak points, and Brent's stuff contains things like URLs of blog pages he's written discussing each of the findings. You will not regret using these tools.
&gt; Honestly, is it worth it? It sounds like a veritable museum of what not to do wrong. If you've got the stomach for it you, it might be fun to tighten that whole system up and own the performance benefits, because it sounds like you'd have carte blanche to do things the right way, since I don't think your boss would recognize "the right way" if it bit him. If the current situation is causing you heartburn, with executives yelling at you all the time, maybe it's not worth it. I can't tell if you're telling OP to bail out entirely, or burn the whole thing to the ground, nuke the site from orbit, salt the earth, and rebuild it all properly. I'm on board with either approach.
SQL*Plus... you poor bastard. 
Gotta throw in what will be (most likely) an unpopular opinion. Before you start fixing anything, get a bit more general information from the horse's mouth. Start here: https://technet.microsoft.com/enus/library/bb500155(v=sql.110).aspx Basically, read every node on the 1st and 2nd levels down from that url (e.g. Features &amp; Tasks -&gt; Databases, Features &amp; Tasks -&gt; Databases -&gt; Tables, Features &amp; Tasks -&gt; Databases -&gt; Indexes, etc.) 
I live in Orange County. There are plenty of jobs available (Irvine) but I lack the experience required to get any of them. I've had a few interviews for data analyst positions, but they did not ask many technical questions. 
The best way to get into the industry is to get a paid intern position through your school.
It's hard to tell from what you described what the problem is. Can you post the CREATE TABLE and CREATE TRIGGER scripts for both tables/triggers? And what platform are you using? EDIT: "Is it a good way?" If you are enabling the trigger only when you are about to insert data a certain way, then probably not. Trigger should for the most part be "always on" from a design perspective, because whatever the trigger is doing, it's so important that it's being done at the database layer. If it's something that is only needed for "some" database inserts, then just include that as part of the insert query batch. Past that, I'd still want to see the trigger to get some idea of what you are trying to maintain.
If this is the case, why are you using stored procedures to handle the relevant data operations. It's much better than triggers and allows you to easily report problems as well as be able to rollback if something went wrong. Triggers are dirty, their legitimate use count is (*very*) low; even those are being worn away by new features/releases that address those situations.
If your data is truly *that* variable, I'd suggest multiple columns with the appropriate data types. It sounds like you're using it as a "catchall" column instead of putting your data in more appropriate, separated places.
the problem is the sp still causes the trigger to fire, encapsulating the sp in another sp will allow me to add context_info but will not fix the problem of the sp that I cannot modify firing the trigger. At least that is how I imagine it would work. 
Your situation sounds very similar to where I was a while back. Like, 10 years ago I knew very little of SQL. I have an art degree but had a couple of years of CS (Pascal) in HS, barely passed a C++ class my first year of college when I thought I could major in electrical engineering, and had done some work programming in ActionScript. So I had some cursory knowledge of programming and given enough time I could figure stuff out, but I certainly couldn't write a query without doing some googling. I got a job where the COO of a startup wanted standard reports to be emailed out weekly. Knowing nothing, I connected to our ERP system via Access and used that to figure out the SQL I needed, and then used VBA to automate the querying, generation and delivery of the reports in Excel and set up scheduler tasks to open the Excel files and kick off the VBA scripts. Needless to say, that quickly got really hard to maintain, and by then I was writing the SQL without needing Access's visual designers. I thought, "there has to be a better way to do this, surely someone has figured this out already". That's when I discovered SSRS. I was primarily a report developer for the next 4.5 years, with a couple DW projects well in development when the company got over-leveraged, bought out, and then downsized. For the past 5 years my title has been "ETL Analyst" in the development team of an amazing medium-sized company. The company does a lot of acquisitions and we get a lot of data with each of them, in varying formats, that needs to get into our ERP system. So, I write scripts to load that data, as well as scripts to do mass-updates for things that would take too long to do from the front-end of the ERP system. I also write ETL processes for a number of in-house and 3rd party applications that we use. The work can be a little dry at times, and the artist side of my brain does sometimes miss the reporting side of things, but I love what I do. My advice would be to keep getting better with SQL. Thinking in the set-based manner which is required to write good SQL is not something every programmer can do. It takes a certain kind of brain. My coworkers, who are better application programmers than I'll ever be, think I'm some kind of wizard because I can write a single elegant query to do what they are trying to do with a hundred lines of code and cursors or whatever. SSRS and PowerBI are fun, but they come and go- already it seems like in many was MS is abandoning SSRS to PowerBI, and whatever company you go to in the future may use some other reporting package. But almost anything you do is still going to involve SQL. From there, if you're going to stay in the SQL Server world, I'd recommend getting familiar with CLR. It can be pretty powerful when you need custom aggregates or other more advanced functionality. And I second the comments about ignoring SSIS. SSIS is annoying as hell to work with and anything it can do you can usually do in SQL. The difference is, when something's not working, you can scroll through your SQL script and see what's doing what pretty quickly. With SSIS you can be clicking and clicking, opening and closing dialogs and scrolling through lists of parameters for ages to diagnose what's going wrong. Fuck SSIS.
Depends on what it is, if like /u/alinroc said you are tying to use one field as a catch all, don't do that it's stupid; One entry on a form = one field in the DB. If on the other hand it's just one entry you are pulling but you don't want to limit how much people can write in that field (like a comment box for instance) then at that point you want to use Varchar(Max) as it will store up to 8K in the row itself and switch to blob storage if it exceeds that.
Most SQL engines support a VARCHAR data type, which is essentially a variable length string field. There is also a newer VARGRAPHIC type that has some advantages over VARCHAR, but potentially requires more memory. There is a point at which very long VARCHAR fields become unwieldy and start to negatively affect database performance. If you are truly storing very long text fields, look into the CLOB data type, which is specifically designed for this purpose.
I got moved up from helpdesk to Jr.DBA after constantly mentioning that i'm learning SQL on my own time. The 3rd party application we use inhouse is based on MS SQL; I was already somewhat familiar with the software. I was in the helpdesk team for about a year or so. Originally, we had 1 IT Director who was also the inhouse DBA/Business Analyst/sysadmin. He left and got replaced by a new IT Director/sysadmin + another another guy who was a Business Analyst took over the SQL Role...he had just as much experience as me in SQL; 0 , but much more business experience and know how, plus he worked in the field the company is in. Then that person ended up leaving and only trained me for a few months before heading out, so i basically took over the DBA/Business analyst role, starting with 0 experience, with only some knowledge about SQL. I learned almost everything i know in the field working on PROD servers and just reading up articles online and getting into different SQL Forums and such...Mostly outsourced support to our vendor, but i basically just started writing new SSRS reports, fixing old ones and documenting a lot of crap no 1 knew about. I mainly do support for the 3rd party application now for the entire company. Took me about 6 months to be able to carry my own weight. I learned a lot troubleshooting things with Vendor support too. This all happened in a span of 2 years or so; It's my first job, not only in IT, but first job out of school, period. I finished an associates degree in Computer Support in 2012-2013, that's all. I guess i just had a stroke of luck...Also, i pretty much shined really bright compared to the rest of my team...they were really comfortable were they were..i was starving to do and learn more every day. So i guess my suggestion is to just keep learning as much as you can ( Setup a test server on your home PC and give yourself some homework). You'll have to start from the ground up like i did; If you're already employed in tech support, see if your company uses any SQL based applications, and just start studying that on your own time so when you approach them for an open position, you'll already have an upperhand. They're more likely to promote an existing employee and give them a raise than to bring in a certified DBA who will ask for a much higher salary.
Now seeing the updated information, I can confirm that this would 100% be better handled without triggers. *Edit - As a side note, SQL Server 2016 will also allow you to define row level permissions. :)*
I work on a Microsoft SQL Server and I would use NVARCHAR(MAX) for something like this. NVARCHAR lets me store Unicode data. 
Dude used an XKCD comic without attribution. No bueno.
VB really? If you want to make your code difficult to read for beginners (which I guess this is targeted for) why not use mindfuck? 
Also it's little bobby tables, which gets referenced ad nauseam on the technical parts of reddit.
Never use triggers. They only lead to problems due to their 'hidden' nature, and can cascade into extreme performance issues if you alter the table in a wrong way accidentally. What if I need to update every row in this table for some reason? God save us all Why are you writing business logic in the database? Where, if, declare, when, then, temp tables? This is a job for an external program, such as a .NET service that will perform these complex updates together with EF/LINQ functionality. A database is not a logic engine. It's made for set based operations and data storage. Don't bend it in ways it's not really made to do. Just because you can doesn't mean you should. Temp tables and Declares leads to big IO cost and will hog down the system and take resources from other operations. The SQL query optimizer cannot optimize DECLARE since it's set at runtime. This costs you heavily in execution time. Where, if, for loops takes big CPU cost. SQL is not an optimized language for logic. This must be a top priority in your mind all the time you work with databases. You're being very selfish and shortsighted with this design, taking the easy way out which will cost you in the long run.
This is really cool! It would be quite useful to have some more regex stuff implemented in Google BigQuery. It would be way more efficient being able to construct the bigram model purely out of the `reddit_comments` table as opposed to the `reddit_extracts.words_bigrams` table. I can't seem to think of any way of doing this on BigQuery with the existing supported functions (ie: there is no `string_to_array` function). Anyone think it's possible/have an idea?
That is too bad. I hope you can move towards refactoring that mess. 
This. I use information_schema all the time to generate script headings.
I would make some composite primary keys, such as assetID &amp; PurchaseDate, in the purchasedetails table, otherwise if you buy the same stock multiple times you're going to have a primary key violation. Same with selling table. Asset type should have assettypeid as the primary key and assetid as the foreign key to asset table. 
Or... exec sp_help 'table name' Lists out column info, primary key info, identity column info, and a few more pieces of table info. I'm a fan. 
If you can't read VisualBasic code snippets and understand exactly what it's doing, you're *really* in the wrong business. 
This spits it out in a way you can just write selects &amp; inserts, with commas in the write places. It saves me a bunch of time.
&gt; Asset type should have assettypeid as the primary key and assetid as the foreign key to asset table. No, the Asset Type should not have a foreign key to the Asset table, it's the other way around. Asset should have assettypeid as a foreign key referencing Asset Type.
&gt; Asset T Thanks. That helps me understand also
This error means that you are trying to restore a backup with wrong Log Sequence Number (LSN). Before to restore make sure that you restore the backup that should be restored with the help of T-SQL Command - RESTORE HEADERONLY FROM DISK = 'backup.bak' Also, I'd recommend you to read this article - https://sqlbak.com/academy/backup-chain/ to understand, how to find out which next backup must be restored. 
Re read the question. It's supposed to be OR, not AND (your query on lines 27/28 is correct). Also, it says &gt; 1990, not &gt;= 1990.
Figured it out. It was the suggestion from /u/zaozo with case sensitivity. My line on 27/28 was incorrect due to a capital E in 'epic'.
Yes sorry - sql server 2012. The entire query is tied to a single schema. Most tables contain a PK and an FK. Some only contain their PK such as tables that only store an account name. The execution plan looks decent until it takes this huge turn on a Hash Match (left outer join) costs 2% and then it heads into a wave of hash match (aggregate) cost 16% - parallelism cost 2% - another hash match (aggregate) cost 16% - parallelism cost 2% - Hash Match (inner join) cost 8% splitting into two streams. Both start parallelism 1% and one ends with clustered index scan (clustered sessions) cost 15% and the other Index Scan (nonClustered) cost 7%. It also states there is a missing index in the sessions table (impact 17.1629) I'm really worried about indexing that table as it is utilized in so many areas of the database.
I'm interested in where you were going with this but unfortunately the 2nd query is throwing errors on the join calls
You could also try something like this: CREATE #DENOMINATOR TABLE ( [count] [int] [not null] , [hardcode] [int] [null] ) INSERT INTO #DENOMINATOR select count(distinct se2.ServiceProviderID) AS 'Count' from msbconnect2.dbo.SessionStudents sst2 Join msbconnect2.dbo.Sessions se2 on se2.ID = sst2.SessionID where sst2.AccountStudentID = acs.ID and se2.date = racd.sessiondate and se2.AccountServiceID = acsv.ID Then below try this: JOIN #DENOMINATOR B on B.HardCode = A.HardCode OR B.HardCode IS NULL
A `TOP` without `ORDER BY`? He likes rolling the dice w/ his donuts, eh?
use single quotes around strings: ```where mood in ('epic') or released &gt; 1990```
thank you for the reply. I'm a bit confused about these options as the where condition still calls the acs / racd / acsv table nicknames. Yet the query where that was contained is left out so the error appears saying that it doesn't know what acs / racd / acsv data elements are. I'm trying to figure out how to link it all back together but I can't seem to get past this. Any ideas?
I'm more blown away by the fact that the dev_desk contains a specific column for "donuts". Which means for every object in dev_desk, it reserves the possibility that is partially a donut. I mean, he's using "select top 5 donuts" like a where clause. 
One sec. I'm dumb. Didn't read closely enough.
stsv is the stateservices table and is used in the numerator. you know I didn't personally write this query and I figured it was plugging in the a.ID whenever the value was null but obviously if it's forcing 366 it can't be processing the isnull. good eye thank you for catching that.
That had me laughing out loud at work. I want to hear about the fresh and very tasty stapler.
In what numerator? I'm not seeing it being used anywhere... Anyway, here's what I got: ;WITH ProviderIDCts AS ( SELECT sst.AccountStudentID , se.AccountServiceID , SessionDate = se.Date , ProviderIDCt = COUNT(DISTINCT se2.ServiceProviderID) FROM msbconnect2.dbo.SessionStudents sst INNER JOIN msbconnect2.dbo.Sessions se ON se.ID = sst.SessionID GROUP BY sst.AccountStudentID , se.AccountServiceID , se.Date ) SELECT Account = a.Name , District = sd.Name , ServiceProvider = sp.LastName + ', ' + sp.FirstName , Total = SUM(racd.allowed / pic.ProviderIDCt) FROM msbconnect2.dbo.Accounts a INNER JOIN msbconnect2.dbo.AccountDistricts ad ON ad.AccountID = a.ID INNER JOIN msbconnect2.dbo.StateDistricts sd ON sd.ID = ad.StateDistrictID INNER JOIN msbconnect2.dbo.EDIRARemittances rar ON rar.NPI = sd.NPI AND rar.CheckAmount &gt; 0 AND rar.RADate BETWEEN '7/1/2015 12:00:00 AM' AND '6/30/2016 12:00:00 AM' INNER JOIN msbconnect2.dbo.EDIRAClaims rac ON rac.EDIRARemittanceID = rar.ID AND rac.void IS NULL INNER JOIN msbconnect2.dbo.EDIRAClaimDetails racd ON racd.EDIRAClaimID = rac.ID AND racd.allowed &lt;&gt; 0 INNER JOIN msbconnect2.dbo.StateStudents ss ON ss.MedicaidID = rac.MedicaidID INNER JOIN msbconnect2.dbo.AccountStudents acs ON acs.StateStudentID = ss.ID INNER JOIN msbconnect2.dbo.SessionStudents sst ON sst.AccountStudentID = acs.ID INNER JOIN msbconnect2.dbo.Sessions se ON se.ID = sst.SessionID AND se.date = racd.sessiondate INNER JOIN msbconnect2.dbo.ServiceProviders sp ON sp.ID = se.ServiceProviderID INNER JOIN msbconnect2.dbo.AccountServices acsv ON acsv.ID = se.AccountServiceID INNER JOIN ProviderIDCts pic ON pic.AccountStudentID = acs.ID AND pic.SessionDate = racd.SessionDate AND pic.AccountServiceID = acsv.ID INNER JOIN msbconnect2.dbo.StateServices stsv ON stsv.ID = acsv.StateServiceID and stsv.name = racd.CPTCode WHERE a.Name = 'TXWICHITAFALLS' AND a.ID = ISNULL(a.ID, 366) GROUP BY a.Name, sd.Name, sp.LastName, sp.FirstName ORDER BY 1, 2, 3 Ideally, you'd load ProviderIDCts into a temp table or something, but for the purpose of keeping it a single query I'm leaving it a CTE.
ran it and the db locked up a minute or so later. i'm going to index the sessions table as i believe that is the main thing choking this up but i'll write back with an update. thanks so much for all the querying / help!! ;)
Just add in some `WITH (NOLOCK)`'s and you should be fine unless you're worried about historic data being updated as it runs.
Is it common to write column names in camel case? I have never actually been anywhere that does.
Psh, not me. Which version completed in 20 sec? `WITH X AS ()` or the `#TABLE` approach? Down from how long?
The very original query I posted took 20 seconds. I actually kept getting syntax issues with all the suggested queries today and I'm executing the fix on the original query for today to stop our website from crashing so frequently. Later this week I'll block out some time to work with the queries you sent over to get the query to work even faster. Mostly I plan on reviewing what you did from a fundamental standpoint so I can learn your process instead of just copying your work. I'm sure it's just a nickname or table name that's the main syntax issue.
What I have always seen is Column_Name. Part_No, Customer_No, etc. Interesting when you run into different practices out there.
If you're down to 20 sec with an index then (imo) your original query is fine. I just deconstructed the original query two differing approaches, which might not be as fast. You could deconstruct your entire join set into indexed #tables and then compile your query and it might not run as fast as the original. A lot of that depends on the makeup of your data, how much there is, etc.
I just realized I'm in SQL not SQL server...camel case is pretty standard in the Microsoft world. What are you coming from?
I get a web front end of MS SQL to do reporting on. 
I'm assuming they denormalized for practical reasons based on the requirements of a dev desk. Realistically, it only needs columns for coffee, donuts, and adderall. It's unlikely that those requirements would change before the system were obsolete. 
When you do not have a lot of people using your system, try updating the statistics for the tables involved... For example: UPDATE STATISTICS msbconnect2.dbo.Accounts UPDATE STATISTICS msbconnect2.dbo.AccountDistricts UPDATE STATISTICS msbconnect2.dbo.StateDistricts UPDATE STATISTICS msbconnect2.dbo.EDIRARemittances UPDATE STATISTICS msbconnect2.dbo.EDIRAClaims UPDATE STATISTICS msbconnect2.dbo.EDIRAClaimDetails UPDATE STATISTICS msbconnect2.dbo.StateStudents UPDATE STATISTICS msbconnect2.dbo.AccountStudents UPDATE STATISTICS msbconnect2.dbo.SessionStudents UPDATE STATISTICS msbconnect2.dbo.Sessions UPDATE STATISTICS msbconnect2.dbo.AccountServices UPDATE STATISTICS msbconnect2.dbo.StateServices UPDATE STATISTICS msbconnect2.dbo.ServiceProviders Then I would change the line 'and rar.RADate between' to: and rar.RADate &gt;= '2015-07-01 00:00:00.000' and rar.RADate &lt;= '2016-06-30 00:00:00.000' You may also want to try handling the ' and a.ID = IsNull(366, a.ID) ' another way. It's hard to recommend a solution without knowing more about what it is doing but to see if it is impacting performance try commenting it out and running the query.
Varies from one group to another. No camel casing in my databases (although objects are not named with case sensitivity anyway). 
Isn't comparing bool to 1/0 redundant? (at least in pgsql it is)
So, to confirm: When you say "I need to remove the alternative genre from my table, since it is 0." you mean "If a genre has no artists, it should not be included in results"? Short answer: Inner join, not left join. [Simple reference chart of join syntaxes](http://www.codeproject.com/Articles/33052/Visual-Representation-of-SQL-Joins) 
Unfortunatly, turns out the stapler was fresh but not tasty. It did had a special occasion, though. When somebody stapled his tongue trying to figure out whether it was tasty or not.
Have a look here: https://blogs.msdn.microsoft.com/sqlreleaseservices/2016/01/29/tls-1-2-support-for-sql-server-2008-2008-r2-2012-and-2014/
I'd like to see quick summary statistics on queries or tables similar to the describe function in the pandas library on python. 
It should use curly braces and be less wordy.
Remove auto shrink
One dialect to rule them all.
You mean like min, max, avg, etc? In Oracle DBMS_STAT_FUNCS.summary can give you that info: https://tonguc.wordpress.com/2008/10/04/understand-your-data-with-oracles-in-database-statistical-functions/
y
Requiring a WHERE clause for all queries, with just something like "WHERE TRUE" would probably save a horrifying amount of money from reducing the amount of people who accidentally delete all the rows in a production database by typing DELETE FROM mytable and forgetting to add the WHERE clause ...
If he was one of our developers he would have made the field a varchar with the values YES or NO... &gt;&lt;
Shrinking databases is bad enough, it causes huge overhead and a heck of a lot of fragmentation as well. Now imagine that lot but automatic.
Look up "insert into select". You can do an insert where it inserts each row to a table from a select statement. 
Thanks!
You never posted the error.
Ambiguous column name errors when there is an inner join on the same named column. Dufuq?
There is no logical operator there though. Is it a AND? Is it a OR? Is it a NAND?
https://www.brentozar.com/blitz/auto-shrink-enabled/
Microsoft marked the functionality that allows you to [skip the terminating semicolon on each statement as deprecated a decade ago](https://www.brentozar.com/archive/2015/12/give-your-t-sql-a-semicolonoscopy/). No signs of them yanking the rug out for exactly that reason - the amount of code that would have to be fixed is **staggering**.
Can you call one function to get the list of summary statistics in db2?
So somebody has a 1GB database, that randomly expands to 50GB, and then back down to 1GB for the rest of the week. Autoshrink is not going to be too much of an issue for them.
Since you are using C# I assume you are doing .NET. Have you looked into [Entity Framework](https://msdn.microsoft.com/en-us/data/ef.aspx)? Or some similar ORM.
Don't know why we can't have multiple dialects for the same functionality. For example when querying Teradata there is a Teradata mode and an ANSI mode.
I tried going the ess kyoo el route for a while but nobody else says it that way.
I get it now, this was the solution, thanks a bunch select ID, "CS-001" course_id, 1 sec_id, "Autumn" semester, 2009 year, "NA" grade from student where dept_name = "Comp. Sci." Plus an insert 
Should we start calling NASA "N-A-S-A" too? and RAM "R-A-M"? and jpeg "J-P-E-G"?
I have not I will take a look. Thank you
Doesn't the FOR syntax have an implicit fetch in Oracle? You should just do: My_var:=HisName and get rid of the FETCH.
FOR does an implicit fetch and populates the variable, so you don't need to issue a FETCH. DECLARE CURSOR names_list IS select His_name from Database.table_Name; BEGIN FOR rec IN names_list LOOP -- Do something with rec.His_name... END LOOP; COMMIT; END;
One quick question. I'm trying to link the tables, and have a small issue. When I take the pk of cylinder_info and set it to the fk of product_info it updates every rows fk. For example, if just row 100 should be set to 1, all 100 rows get set to 1. Can you take a look? string updateProductInfoCylinderPK = "UPDATE mydb.product_info SET cylinder_info_cylinder_id = @cylinderInfoCylinderID"; //get pk of last entry of cylinder_info cylinderID = command2.LastInsertedId; string cylinderInfoPK = cylinderID.ToString(); MessageBox.Show(cylinderInfoPK); } // set product_info.cylinder_info_cylinder_pk to cylinder info primary key using (MySqlConnection connection3 = new MySqlConnection(constring)) using (MySqlCommand command3 = new MySqlCommand(updateProductInfoCylinderPK, connection3)) { command3.Parameters.AddWithValue("@cylinderInfoCylinderID", cylinderID); connection3.Open(); affectedRows = command3.ExecuteNonQuery(); Console.WriteLine("Affected Rows: {0}", affectedRows); }
They're completely optional. ANSI_QUOTES work just fine. FYI, MySQL uses non-standard \`identifiers\` as well :P
I use MSSQL with SSMS. I spend a fair chunk of time highlighting a chunk of code in the editor, and hitting run, which runs just what I highlighted. It prevents DELETE FROM mytable WHERE LastActivity &lt; dateadd(dd,-30,getdate()) if I accidentally highlight 1 row too few.
Change `update` syntax so the `where` clause comes before the `set` list, and doesn't work if you leave it out.
You want an exhaustive list of all built-in aggregate functions? Like any RDBMS, the answer to that lies in the reference manual and [this is the section](https://www-01.ibm.com/support/knowledgecenter/SSEPEK_11.0.0/com.ibm.db2z11.doc.sqlref/src/tpc/db2z_aggregatefunctionsintro.dita) you are looking for. If you wanted to be clever, you could likely query INFORMATION_SCHEMA to find an exhaustive list of *all* aggregate functions, including user defined.
If you want all rows, whether there are artists in that Genre or not, but you don't want Alternative for some reason, then you would move the don't include alternative down to a where clause since it's not really a join condition. If you don't want any genre that does not have an artist in that genre (and alternative is your only example of this), you'd simply make it an inner join instead of a left join as /u/PuppiesForChristmas suggests. SELECT T.Genre, COUNT(A.ArtistName) FROM Titles T LEFT JOIN Artists A ON (T.ArtistID = A.ArtistID) WHERE T.Genre != 'alternative' GROUP BY T.Genre;
How about setting up views for them?
This returns PLS-00382: expression is of wrong type
oh, so true -- at least until they adopted settings to be more standards-compliant e.g. ONLY_FULL_GROUP_BY 
 select * without &lt;the columns I don't want to see&gt; from &lt;table&gt;
Sorry. HisName.his_name. HisName is a row variable.
It started out as Sequel, it's not like it came from a vacuum. Just part of its illustrious history.
You bastard you.
CodeSchool calls it Sequel, also it's more fluid and fewer syllables. 
&gt; select * without &lt;the columns I don't want to see&gt; from &lt;table&gt; A small workaround that I will use is: SELECT Things, I, Really, Want, To, See, * FROM ... That lets you see specific data first followed by the entire row.
Lol.. My point is that if an acronym can easily be pronounced, it usually is
It's extra work for the server, and is less logically pure than just inner joining. ("join, exclude some members via a HAVING calculation, then present results" vs "exclude those members innately via join, then present results") When you work with bigger datasets, scalable performance should be a product of elegant logic as much as tuning other factors. (tenths of seconds in Dev/unit test are an eternity when you deal with multiterabyte or billion row set production sets)
This is one I would really like. Here's a table with 20 columns, and if you want 17, you have to write out 17 instead of "select * without &lt;3 columns&gt;".
Sql server should be able to tell me exactly what fucking column is truncating when inserting. I prefer not to have a wild goose Chase. Oracle updates with joins... Wtf Oracle? 
No expert but I would probably aggregate by deptdesc and count(sku) and stick a join in there for good measure - an inner join in this case as you're only really interested in the top value(s). Then group by the deptdesc and order the results descending by the count(sku). Probably better solutions out there.
It's saying column sku is ambiguous.
Ah instead of sku do count(sku) in the first line 
That sounds very unimportant to anyone developing, and even less on systems where indexes are being defragmented frequently. Best practices, sure, but not even close to making it to a top 1000 issues list.
It's vitally important for anyone who sets it, developer or DBA. There is simply no need to use it and in fact there have been various items flagged to try to remove the auto shrink code. As for index defrags, perhaps there wouldnt be the need to do so many if auto shrink was turned off? Also, having to do regular defrags because of high fragmentation itself hints at a problem. Hopefulyl though, any devs wont have the ability to turn the setting on in the first place!
Is there any real reason why you are doing this? You should be doing multiple update statements to cover your various logic points of manipulation.
insert into Table (ID, name, number, month, year, grade) select (newid(), name, number, month, year, grade) from Table idk why you would want to do that, though...
 select deptinfo, count(sku) from deptinfo inner join skuinfo on skuinfo.dept = deptinfo.dept order by 2 desc
Extra commas! SELECT 1, 2, FROM mytable; Should be valid, so it doesn't break my fancy editing: SELECT first_name, last_name, FROM people ; 
By the way, your terminology is a little confused. There is no such thing as a "Teradata Viewpoint Query." Viewpoint is a "systems management and monitoring" tool, not a query engine or client. In the future, just write "[Teradata] &lt;your title/question here&gt;".
At least six Full Backup, Differential Backup, Transaction Log Backup, Copy-Only Backup, File and Filegroup Backups, Partial Database Backup.
I think you need a mock problem to work on instead of just meaningless tables with keys. You might look into MySQL sample databases. Or build something simple like an inventory tracking system. &gt;Take row_id of second table and insert (using update) into FK column of first table by sorting by most recent datetime(is this the best way to ensure I'm linking the correct records?) This sounds like it would be terrible form in most cases, but it would depend on the specific problem you were trying to solve. Typically you would probably want to know the exact primary (or candidate) key of the row you are trying to update.
&gt; 4.Take row_id of second table and insert (using update) into FK column of first table by sorting by most recent datetime(is this the best way to ensure I'm linking the correct records?) not sure what you're doing here but sorting is probablt wrong
your problem is here -- WHERE skuinfo IS NULL; you need to specify a column here, not a table
I'm actually working on a very small database for my first work project. A unit comes in, gets all kinds of tests done, the results of the tests get captured. I just have a table for product_info with partnumber, serialnumber, customername, then a table for test data (all related tests). 
&gt; I'm trying to figure out how many skus are within the skstinfo table but NOT in the skuinfo table. No Join necessary. He's getting ambiguous columns since both tables have sku and with a join it doesn't know which sku to count. SELECT COUNT( sku) FROM SKSTINFO WHERE NOT EXISTS ( SELECT 1 FROM SKUINFO WHERE SKSTINFO.sku = SKUINFO.SKU )
The less elegant way to do this.... SELECT 'Time Period 1' as tLabel, COUNT(DISTINCT(users13.UserID)) AS tCount FROM [database].Users users13 JOIN [database].UserStuff userstuff13 ON users13.UserID = userstuff13.UserID JOIN [database].UserStuffTypes types13 ON userstuff13.TypeID = types.TypeID WHERE users13.Flagged = ('Y') AND (userstuff13.date &lt; (~~enddate goes here~~) AND userstuff13.date &gt; (~~startdate goes here~~)) AND types13.criteriaID IS NULL UNION SELECT 'Time Period 2' as tLabel, COUNT(DISTINCT(users23.UserID)) AS tCount FROM [database].Users users23 JOIN [database].UserStuff userstuff23 ON users13.UserID = userstuff23.UserID JOIN [database].UserStuffTypes types23 ON userstuff23.TypeID = types23.TypeID WHERE users23.Flagged = ('Y') AND (userstuff23.date &lt; (~~enddate goes here~~) AND userstuff23.date &gt; (~~startdate goes here~~)) AND types23.criteriaID IS NULL
Allow an alternate SELECT syntax. Use FROM as the key word. Everything else would remain the same. FROM {table names with JOINS} SELECT {column stuff here} WHERE {selectivity here} The other clauses would follow just as they are now. Reason: This is how the SQL server internals have to reconstruct your query anyway (according to several books). Thinking about the select clauses in this order helps me design fast performing queries from the outset.
 SELECT COUNT(a.sku) FROM SKSINFO a JOIN SKUInfo b ON a.sku=b.sku WHERE a.sku NOT IN (SELECT sku FROM SKUinfo); Would get a count of 0 because you're doing an inner join on the table you're then doing a "NOT IN" on.
This one makes the most sense as there may be more than one item with that same highest price and this would return all of them.
First, I'm not the OP, but the OP requested &gt; I'm trying to figure out how many skus are within the skstinfo table but NOT in the skuinfo table. SELECT COUNT(a.sku) FROM SKSINFO a -- First table that contains *all* the skus JOIN SKUInfo b -- second table that contains *some* of the skus ON a.sku=b.sku -- that you are performing an inner join on, so -- limiting your return set to only those rows -- in both tables. WHERE a.sku NOT IN (SELECT sku FROM SKUinfo); -- Then you add this additional criteria to only return rows -- that don't exist in the joined set which is already limited to -- only those rows in BOTH tables Thus the causes the query to return 0.
 SELECT COUNT(a.sku) FROM SKSINFO a -- You pull all SKUs from the main table JOIN SKUInfo b -- Join on the table you want to check if SKUs are in ON a.sku=b.sku -- Inner join will work because SKSINFO contains all of the SKUs -- he is not interested in SKUs that exist in SKUInfo but not SKSInfo -- This is basically a vlookup. WHERE a.sku NOT IN (SELECT sku FROM SKUinfo); -- Then this tells is to remove anything that exists in SKUinfo, only leaving SKUs that are IN SKSINFO but not SKUInfo 
 SKSINFO(A) SKUInfo (B) Sku sku First First Second Third Third Fourth Fifth Fifth Results of your join before the where clause First Third Fifth COUNT WHERE NOT IN ( SELECT sku FROM SKUinfo ) on this result set returns 0. Where are you getting the idea that: - he is not interested in SKUs that exist in SKUInfo but not SKSInfo That's EXACTLY what he **is** looking for and you're saying he's not interested in it?
&gt; Oracle updates with joins... Wtf Oracle? Could you expand on that?
The SQL LEFT JOIN returns all rows from the left table, even if there are no matches in the right table. So the result of the join should be a.sku ................. b.sku First ................. First Second ................. (Null) Third ................. Third Fourth ................. (Null) Fifth ................. Fifth you seem to be thinking of an inner join?
Umm... you didn't do a left join. You did a join, which is an INNER JOIN If you can point out where the word LEFT appears in any of the SQL you've posted in this thread I'll happily retract my complaint. Also, doing a LEFT join is pointless and you may as well have shortened your SQL to SELECT count(sku) FROM SKSINFO a WHERE sku NOT IN (select sku from SKUinfo); Which would return the exact same result as if you add the spurious left join to SKUInfo (b).
(facepalm) My data studio automatically defaults JOIN into a LEFT JOIN if nothing else is specified. Should have seen that coming. And yep your shortened version works
So.. you're query should always return all those who attended a meeting on the date that this query is run? SSRS implies MSSQL Server, so GetDate will return a datetime, not a date. So if SQL 2008 or higher, you can just convert it to a date and not a datetime, presuming the begin and end dates are also datetimes convert( date, Begin_Date) = CONVERT(date, getdate()) -- the meeting began the day the query was run. 
What exactly does the inner join do?
I'm not sure if this is ANSI or if it will require some syntax changes for MYSQL DELETE takes WHERE EXISTS ( SELECT 1 FROM course where takes.course_id = course.course_id and course.title like '%word%' );
Others have answered, but an easier way to do this might be a use something like: sum(case when date between START and END then 1 else 0 end) as count You have a count(distinct) - so you might have to rejigger the query a little bit so that you're starting with a set of unique user-date pairs. Using the sum(case when...) approach allows you to create as many sum(case) statements as you want for different groupings of dates.
Here is the query: DELETE takes FROM takes as T INNER JOIN course as C ON T.course_id = C.course_id WHERE title like '%certain word%' "inner join" is what makes 2 or more tables connect, in the script above the table [takes] is connected to [course] by the field [course_id] that exists in both tables. 
try to add the database name in the query, like: FROM DBname.takes as T INNER JOIN DBname.course ... 
Same error
Try to use DELETE T instead of DELETE takes
I'm not D_W_Hunter but since I'm here :D He used the "SELECT 1" because for the clause "EXISTS" you only need to return anything, so you could write anything in the select inside it, "SELECT A", "SELECT *", "SELECT POTATO" ... it is just common use to write select 1 in "exists" clause.
Error Code: 1175. You are using safe update mode and you tried to update a table without a WHERE that uses a KEY column To disable safe mode, toggle the option in Preferences -&gt; SQL Editor and reconnect. 
Did you use only "DELETE T" or you used DELETE T FROM takes as T INNER JOIN course as C ON T.course_id = C.course_id WHERE title like '%certain word%'
 delete T from takes as T inner join course as C on T.course_id = C.course_id where title like '%database%';
I have no idea then. That must be something more specific to MySQL, I'm a MSSQL dev. 
&gt; Is there any real reason why you are doing this? Because it is my job. &gt; You should be doing multiple update statements to cover your various logic points of manipulation. No I shouldn't. Why on gods green earth would I do everything manually instead of a procedure to handle it? Have you ever worked a day in your life? You don't turn a automatic procedure into a manual process; that slows it down and increases points of failure. You don't update and commit every step of alteration, that goes against the core points of Java object manipulation. You know, the shitty thing about posting to here is all the assholes who post non-solutions to try and show how smart they think they are, but this may be the first time I've seen a person offer an objectively worse solution instead of one that is just unnecessarily complicated
Compare sql server syntax to this.. http://stackoverflow.com/questions/2446764/update-statement-with-inner-join-on-oracle
* Find a niche for someone you know, where there is potential for other clients * Build an app for them * Charge them what they can afford to license (not own) it. * Support it, so that it works * Ask for an Reference and who they think would also buy the app. * $$$
Getting a return of 0 with this one as well.
Thank you.
The shortened version worked, I just had to swap places with SKSINFO and SKUinfo. Thanks.
I am currently located in Los Angeles.
I did something similar for a small company that sold products on Amazon. But never thought about doing it for SQL. I have been taking mini courses whenever I have free time on codecacademy and it has been really fun. Your advice is exactly what I was looking for, to get some real life projects under my belt. I appreciate your help.
It's exactly the kind of thing you should do. Real-world is great, but if you're struggling to find work or need some extra practice, hunt down some free datasets (like the U.S. Census) and build interesting reports. Coding is an art--you should develop a portfolio for your resume. 
You're dumb.,
I feel similar but I'm thinking it's my familiarity with SSRS and lack of with Tableau. Time will tell. 
Tableau and Power BI are meant for quick and easy development, not so much super customization. You can create something from scratch with a few clicks that looks way better than SSRS charts. Then consider the interactivity in Tableau and Power BI and now SSRS looks really old and static. SSRS will always have its place, but it's lagging when it comes to dashboards. But supposedly 2016 will have a lot of changes. Check it out if you haven't. 
SSRS definitely has more flexibility, and with the coming features in 2016 (specifically the Datazen acquisition) I think it'll give Tableau a run for the money. That said, I haven't found either meet my needs for a dashboard platform. They're both fairly sales-centric, or otherwise limited in interactivity and functionality. For example, spatial data - if you have anything other than the most basic spatial data requirements for tableau you're looking at doing a fair bit of hacking to even get your coordinates available to the application, and even then there's some very restrictive limitations on what you can do. SSRS fares a bit better with easily integrated SQL Server spatial datasets, but the styling options are horrendous, and nothing near what something like Mapbox can do. Sadly, they don't support any third party tile servers like Mapbox with the SSRS spatial reports - you're stuck with Bing. Lately, I've just been doing the dashboards directly in JavaScript. The learning curve is steep, but the flexibility is pretty amazing.
the product ships out with a new part number and serial number. This is for a semiconductor repair center, it's a fairly complicated process to explain via text. Essentially the only thing that remains traceable long-term is the serial number of a sub-component, so the part_info table is actually my head table with the product_info table reporting to it. (this was recommended by the DBA at my very large company after a looooong sit down meeting). 
My experience in the IT world is that no one really cares about degrees. They care if you can do the work. If you understand the basics of SQL maybe it's time to look for a new job. Sift around on indeed.com until you find something that piques your interest.
I wouldn't say no one cares about degrees, but in the grand scheme of things, if you know SQL and are a hard worker then you can still get your foot in the door and land a job. With no degree your resume may not get past a some HR people, but if the IT people are looking at your resume and like what they see they'll be more than willing to bring you in for an interview. I would update your resume and make sure it looks great! There are some subreddits (I forget exactly) like itcareers or programmingcareers where you can post your resume and they'll give you some tips. Have a friend or trusted co-worker look it over as well. Then like /u/mamertine said, hit up indeed and start applying to jobs that sound interesting to you. Chances are 95% of them are going to 'require' that degree, but just apply anyways. If you've been working for a while that will mean significantly more than a piece of paper that says you graduated from some university.
Yup. I do consulting for many industries and honestly, I just find out what the report will be used for, the format it needs to be in, and the desired results and I translate that to a SQL dev. The pay is insane. 
Any way to implement what you have in C# in T-SQL? That would certainly speed it up. You can also try a CLR function if the code is too complex for migrating to T-SQL. You can try wrapping all of your UPDATE operations in a transaction, then commit everything all at once. Keep in mind that if you have full logging enabled on the database, it might overflow the transaction log. If you can't set simple logging, you can split it up into batches. Your C# app could use threading to commit the batch of say 10000 rows to the database and keep working on parsing the rest of it while the database processes the updates. If the fields you're updating are indexed, disable the index at the beginning, rebuild it after you're done. If you update table set value = newValue where id=x, make sure you have the id column indexed. One more idea, based on your plan B. Create two identical tables to the one you want to update. Let's call them switchDelete and switchInsert. Insert everything into switchInsert from your C# app, then perform partition switching: first from your original table to an empty switchDelete them between the original (now empty) and switchInsert. Switching partitions is instant. Although I'm not sure if that would even be necessary, since you only have 300k rows and you could just update original from your temporary insert table, assuming correct indexes it should be fast enough. 
Something like this maybe. I'm grabbing the data set, putting it into a temp table, then setting the max timestamp from data set and then pulling all info related to that stamp. There's definitely better ways to write this, this was what my lazy approach would be. SELECT n.id, mm.meeting_type, mm.begin_date, mm.end_date into #trythis FROM Name N INNER JOIN ((Order_Meet om INNER JOIN Orders o ON om.ORDER_NUMBER = O.ORDER_NUMBER) INNER JOIN dbo.Meet_Master MM ON Om.MEETING = MM.MEETING) ON N.ID = O.ST_ID WHERE begin_date &lt; GETDATE() and end_date &gt; getdate() AND mm.meeting LIKE '%AM%' declare @Figureoutmax datetime2 set @figureoutmax = (select max(begin_Date) from #trythis) select * from #trythis where begin_date = @figureoutmax 
Gonna try this after lunch break; Thanks!
This is my rule of thumb. Tableau for dashboards and maps. SSRS for list and more spreadsheet data reports. Also SSRS is better at sending people emails each day by using a data driven list. I also find the user group and data permissions a pain in Tableau. Our Tableau guy told me that he had to write a scrip to update AD groups.
&gt;Lately, I've just been doing the dashboards directly in JavaScript. The learning curve is steep, but the flexibility is pretty amazing. Resources please? I use SSRS fairly regularly and love how (without a ton of parameters) you can get really clever with the SQL. For example I wrote a neat little hackjob of a report which creates a staging table and then uses it to update the parent table before emailing out (internal) a raw CSV subscription of any changes from one week to the next. In SSIS it would have been a lot bulkier, but it does its thing and in order for SSIS to do the same then we would have had to work with networking/IT in order to connect it to our outgoing email server, assign credentials, etc. When it comes to using a lot of parameters though things just fall apart quickly. Getting it to look clean/presentable/centered is a huge pain in the ass. Working with it's exclusive syntax (e.g., `=TODAY()` instead of `=GETDATE()`) or trying to get numbers to be formatted correctly, etc., is another chore. Normally I find the best solution is to use SQL exclusively, create a 'holding table' which is updated periodically from a stored procedure for the SSRS report to sit directly on top. This is still limited for doing loops, or dynamic pivots, etc. 
Maybe we can help you format it using SQL so that you don't need to do it in c#? Otherwise... could you dump the records to a text file and then do a BULK INSERT. This would be much faster!
&gt; Resources please? To date, mainly [Mapbox](https://www.mapbox.com/), [leaflet](http://leafletjs.com/), and [D3.js](https://d3js.org/). But I've been moving towards more re-usable pieces via web services in [hapi.js](http://hapijs.com/), leveraging SQL via [knex.js](http://knexjs.org/) and building interfaces using [React](https://facebook.github.io/react/) / [Redux](https://github.com/reactjs/redux). 
That's about 8,035 rows per second. Not bad.
This. Additionally, if you can afford it, drop or disable non clustered indexes not needed for the update just before you do it then add/rebuild them when the update is completed. This will significantly reduce IO / processing time. Obviously do this during a maintenance window. Alternatively/additionally do the updates in batches so you don't lock the table up too badly while they're being updated. I find in most cases batches of 1000 or 10000 work well.
&gt; Setup the script to run in batches so your transaction log doesn't fill up (or make sure you are backing up the log periodically to flush it) Even if your TLog is getting backed up regularly (we use 15 minutes), you'll want to batch the transactions so that there's something *to* flush. &gt;In our case we disabled 2 non-clustered indexes and shaved about 20 hours off the update This is a very good piece of advice. If you're updating a significant portion of the table, those index updates will drag you down. The other thing you can do is disable auto-update of statistics. If you're updating the *clustered* index, you'll still take a performance hit though because of all the rearranging. If the CI isn't your PK, you can disable that one as well. If the CI is your PK, I'd advise against disabling that.
Yes, you are correct. I was just referring to the format you retrieve, not how the server store on the disk. I shouldn't have used the word "store", "display" would fit better what I truly meant :P
it would return 0 if all the SKUs in SKSINFO were also in SKUINFO, otherwise it should return the skus in SKSINFO not in SKUINFO.
Assuming your table that has badly formatted data has badly formatted data in one column and not all columns. 1. Create a work table, use a temp table or just make a table you will later drop. This table should contain your main table's primary key and the field that needs to be fixed. 2. Enhance your program to read from the table with badly formatted data and insert into this work table. run through enough rows to see if that boosts your speed to where you want it. If yes.. proceed, when done, update the main table from the work table (during non-peak usage time, this is likely to churn for a bit). If it's okay for the table being fixed to be in a transitory state (you can tell which rows were bad, so if the fix has to be stopped mid-way you could pick up where you left off) then you may want to update 20-30,000 rows and commit. If this has to be all or nothing, then do all 300,000 rows as one transaction. Make sure your transaction log space is sufficient. If your speed is still unacceptable, write your two columns out to a text file instead of the work table and bcp the whole text file into your work table after the application is done and then update the main table from the work table.
It's *really* not pretty. At least the only way I can come up with is very brute force. Medians are just weird. Most often I see it defined as, when values are sorted, if the list contains an odd number of items, the median is the middle one. If the list contains an even number of items, the median is the average of the two middle values. So... your list is all the weight values in your test table for a given dog. There's no *easy* way to get the nth row of a data set using MsAccess. Lets say your test table has a key of Dog_ID and there are 24 rows for dog_id 1234 You then need to average the weight value for element 12 and 13 from that list ordered by weight. SELECT TOP 1 Weight FROM ( SELECT TOP 12 Weight FROM TestDogs WHERE Dog_ID = 1234 ORDER by WEIGHT DESC ) mid1 Similarly you get the 13th SELECT TOP 1 Weight FROM ( SELECT TOP 13 Weight FROM TestDogs WHERE Dog_ID = 1234 ORDER by WEIGHT DESC ) mid2 So, the median becomes the sum of results of these two queries divided by 2 Problems: Access's implementation of "TOP" does not choose between equal values, so if the dogs weight was the same for two middle entries, the top 1 **could** return 2 rows. So you might have to add distinct to your count &amp; query to eliminate duplicate weights which would also then make it so you don't have a true median edit: dog_id can't be a unique key
The next thought I have is to run these, may help identify the SQL blocking. SELECT req.session_id ,sqltext.text ,blocking_session_id ,ses.host_name ,DB_NAME(req.database_id) AS DB_NAME ,ses.login_name ,req.status ,req.command ,req.start_time ,req.cpu_time ,req.total_elapsed_time / 1000.0 AS total_elapsed_time ,req.command ,req.wait_type FROM sys.dm_exec_requests req CROSS APPLY sys.dm_exec_sql_text(sql_handle) AS sqltext JOIN sys.dm_exec_sessions ses ON ses.session_id = req.session_id WHERE req.wait_type IS NOT NULL --WHERE req.wait_type = '?' go sp_who2 select session_id, status, command, blocking_session_id, wait_type, wait_time, last_wait_type, wait_resource from sys.dm_exec_requests where session_id &gt;= 50 and session_id &lt;&gt; @@spid; select r.session_id, status, command, r.blocking_session_id, r.wait_type as [request_wait_type], r.wait_time as [request_wait_time], t.wait_type as [task_wait_type], t.wait_duration_ms as [task_wait_time], t.blocking_session_id, t.resource_description from sys.dm_exec_requests r left join sys.dm_os_waiting_tasks t on r.session_id = t.session_id where r.session_id &gt;= 50 and r.session_id &lt;&gt; @@spid; SELECT sqltext.TEXT, req.session_id, req.status, req.command, req.cpu_time, req.total_elapsed_time FROM sys.dm_exec_requests req CROSS APPLY sys.dm_exec_sql_text(sql_handle) AS sqltext Otherwise perhaps look at the query plan?
I need the Median unfortunately! Not so simple haha
Ahh wow that is pretty complicated! Thank you for your help though! Is there a way to do this without adding an ID? I would think you could do something like the following SELECT MIN(Weight) FROM (SELECT TOP 50 PERCENT Weight FROM TEST order by Weight) **somehow group this by dog** SELECT MAX(Weight) FROM (SELECT BOTTOM 50 PERCENT Weight FROM TEST order by Weight) **somehow group this by dog** Divide these by 2 and then VOILA would get the Medians for each dog group- can't figure out how to write this though!!
It's pretty simple if you use VBA.
ya that's fair
Thank you so much for the help. I realize now that what I'm trying to do can be accomplished in T-SQL (and should). I've updated my original post with the logic my C# app does. Maybe someone could give me a hint on how I can wash the string in T-SQL just like I do it in C#? Anyway, lesson learned! haha! The whole complex with c# .net app is due to lack of experience and not knowing enough T-SQL.
Question I would ask an in an "Intermediate-level" query writing test would be things like... "What is something that you *can* do in a CTE but you *can't* do in a Subquery. "What's the difference between a 'normal' index and a Clustered Index. How much space does a Clustered index take on disk. What's the difference between a function that returns a table and a procedure that returns a table (the same question can be asked regarding scaler functions/sproc, but wording it this way doesn't trip people up with a "function returns value, sproc returns a table", because that's not the point). Write a query on the board/paper that strips the file name off a path. I never judge on exact syntax, but understanding how to nest functions and how to parse strings is definitely something that comes from writing a fair amount of SQL. Does this help?
Yeah, this is pretty much the way to do it alright. I've never had to use it on larger tables but it does work pretty well with a a few hundred records. I would be a little critical of your function, however, in that it is limited to use on one table only. I have a similar one myself in a module that I use in most projects. The function accepts a table name or SQL string; the name of the field to be grouped; the grouped field value itself; and the field which the median is to be calculated on. So, the query itself would look like (Table name is 'tblDogs'): SELECT tblDogs.Dog AS [Type of Dog], fMedian("tblDogs","Dog",[Dog],"Weight") AS [Median of Weight] FROM tblDogs GROUP BY tblDogs.Dog; 
Can someone post answers to these questions please? I am curious how well I did but ashamed to put up my crappy answers 
That's a good approach, also asking questions that show they know knowledge of inner workings of SQL. Or as you said, if they know ideas or methods to write things that can solve problems. Last interview I nailed, they asked if I had any questions about the job or environment. So I asked questions such as "In this environment, how did the previous DBA's structure the servers? What form of normalization is commonly used or set up? What type of performance logging and backup methods do you use?" Kind of just knew what type of questions they would ask me, and I asked questions to find out what the job would be like here by showing knowledge and skills in the kind of questions they would probably ask. It was more like just a chat with people who were enthusiastic about SQL. It was also hard to explain to current employers why I had a 3 1/2 hour dentist appointment.
Completely agree with this. I have been doing SQL/Data Analyst roles for close to 20 years now. I consider myself to be quite good at SQL but I haven't had the need to use A LOT of the proposed questions in that list. Yes, you do need to have sound fundamentals but I have found that having a real sense for the data and the business needs are much more important than remembering what default port SQL runs on. *Edit: spelling
Yeah I work in SQL all day and some of these questions are very niche and not a question I would think of as a judge of ability for a would-be SQL developer. I feel that good questions revolve around problem solving or concepts, not "how do you delete a constraint?" I think a good question would be like: "I have a report on our website that would call a stored procedure which requires me to dynamically group by a certain date range, insert those contents into a temp table and then select out the contents to be displayed. How would you go about doing that?" Or something like that. Or "I have some reports that continually reuse the same block of code, and it's costly to generate those sections every time the reports are generated, and causes the queries to run slowly. What is a potential solution to this?" I actually like this one better because it requires them to think critically but also know about the concept of caching/other solutions, which is a bit outside of the box when it comes to SQL and the mark of someone who is not a beginner.
Also a good question to ask before you accept the job, sit down and realize all of their tables are 1NF...
wouldn't you just carry the dog ID up from the base? SELECT Dog_ID, MIN( tWeight.Weight) , MAX( bWeight.Weight) , (MIN( tWeight.Weight) + MAX( bWeight.Weight) / 2) as MedianWeight FROM ( SELECT TOP 50 PERCENT Dog_Id, Weight FROM TEST order by Weight) tWeight, ( SELECT BOTTOM 50 PERCENT Dog_ID, Weight FROM TEST order by Weight) bWeight WHERE tWeight.Dog_ID = bWeight.Dog_ID
Doesn't help that several of these questions are pretty poorly written. 12 is particularly egregious.
Cool thanks!
The forum thread suggested it, I honestly don't know either way. It's something I think is worth a try. 
I think I just threw up in my mouth a little bit. Do people actually do this??
;)
This might be SQL Server only but this one is a real performance booster: &gt; We are undergoing a sales tax audit. Given that we have [SalesOrder], [Address], and [States] tables that must be joined in which clause of the SELECT do you place the filtering for sales orders that were billed to addresses in Iowa? There are two answers. One will yield poor performance and the other good performance. Your clue is what word was ***missing*** from the question.
&gt;Maybe someone could give me a hint on how I can wash the string in T-SQL just like I do it in C#? declare @dirty varchar(20) = 'ABC_1999_555_01' ;with cte as ( select @dirty as dirty, CHARINDEX('_', @dirty) as delim_1, -- third parameter is the start position, which we set to one greater than -- the previous delimiter CHARINDEX('_', @dirty, CHARINDEX('_', @dirty) + 1) as delim_2, CHARINDEX('_', @dirty, CHARINDEX('_', @dirty, CHARINDEX('_', @dirty) + 1) + 1) as delim_3 ) select SUBSTRING(@dirty, 1, delim_1 - 1) + SUBSTRING(@dirty, delim_2 + 1, delim_3 - delim_2 - 1) + '-' + SUBSTRING(@dirty, delim_1 + 3, 2) + '_' + RIGHT(@dirty, len(@dirty) - delim_3) as Clean from cte Clean ------------ ABC555-99_01 This would work just fine as long as you know how many delimiters you have. You would need to do a recursive query if there were no known maximum number of delimiters.
If you have a SQL 2005 backup file, you should be able to restore it to a SQL 2012 server. Have you tried that? That does not mean that your software is compatible with SQL 2012. Only the vendor can say that for sure.
 reverse(substring(reverse(path), charindex('.', reverse(path)) + 1, charindex('/', reverse(path)) - 1)) Didn't look anything up, so I'm about to go execut this and see whether it worked :P