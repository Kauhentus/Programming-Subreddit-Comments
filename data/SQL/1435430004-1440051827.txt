[This is what I see in the config manager](http://imgur.com/7yM9zPT) In the event viewer I got this: "Login failed for user 'MicrosoftAccount\xxxxxx@gmail.com'. Reason: Could not find a login matching the name provided. [CLIENT: &lt;local machine&gt;] " 
Would be a good idea to escape single quote characters in the data if there's any chance your data has them.
You didnt add your account as an admin during the setup. Did you do mixed mode, and if so did you set a password for the sa account?
Dataset: http://i.imgur.com/wjfzW1o.png Results: http://i.imgur.com/LVDDlqB.png Like /u/tikketyboo said, Joining on both categories is because the route's NUM is not really unique, and neither is company, so it's trying to separate the data out a bit better. With the data that i've supplied, you can see that if you just joined on NUM, you would have company AND and ALI clashing due to their buses having the same number. The self-join basically just lets us work with **2 of that table**. The first where clause specifies that our first table, table a , requires a stop to be 3. 3 on its own: http://i.imgur.com/UtB794r.png The second where clause specifies that our second table, table b, requires a stop to be whatever stop 'LONDON ROAD' is. 4 on its own: http://i.imgur.com/fkX6uCP.png (the blue one gets dropped from the end result) If you just had two ANDs with no self-joins: WHERE A.STOP = 3 AND A.STOP = 4 you would get 0 results returned, because there can only be one STOP per record/row. If you were to use an OR and no self-joins: You would get if they had been to 3 or 4, but not 3 and 4. http://i.imgur.com/sdklUcz.png
I've used W3 school's little example DB to show some people the basics. It skips over a lot of the fundamentals and gives you some data and a query window... which works well for some people. http://www.w3schools.com/sql/trysql.asp?filename=trysql_select_all
It would probably not be a big job to do it automatically: https://stackoverflow.com/questions/9201239/send-e-mail-via-smtp-using-c-sharp - shows how to send an email using c# http://www.dotnetperls.com/sqlconnection - shows how to make a simple call to a database using C# You'd need to add some error handling, modify the query, and put in some timer mechanism to piece it all together. A junior developer should be able to help you put that together pretty easily. I picked out C# since it sounds like a microsoft environment, but lots of people would do this is python or ruby in a linux environment.
This is something that you could do with SSIS very simply. Execute SQL task to query all the accounts you want to email into an object variable, then loop through the object using a for each loop to send the email and write a "email sent" record to the DB (assuming you'd want a record of the email being sent). Should be a half-day job to get something like that set up and scheduled.
http://www.brentozar.com/archive/2014/06/trace-flags-1117-1118-tempdb-configuration/
Microsoft's best practices mention that you should have a tempdb file for every CPU core. One might see decreasing gains when the number of TempDB files exceeds the number of CPU's. From https://support.microsoft.com/en-us/kb/2154845/en-us: &gt; As a general rule, if the number of logical processors is less than or equal to 8, use the same number of data files as logical processors. If the number of logical processors is greater than 8, use 8 data files and then if contention continues, increase the number of data files by multiples of 4 (up to the number of logical processors) until the contention is reduced to acceptable levels or make changes to the workload/code. 
I understand now. You should be able to use linux cmd line tools to get the blog posts out. There are probably some examples out there in googleland, or post a single "INSERT" line and someone should be able to give you the cmd.
Is this possible without actually running a SQL server, or will I need to install MySQL to do it? I was hoping there might be a visualization tool or something to browse the data in the backup, but I'm guessing that's not likely, what with it being a database and not "visual" content. Edit - Never mind, got it. I was able to open the database with geany, find the "INSERT INTO" section for the posts, and then I can copy out the post data, clean up bad tags and save it as HTML and I get a clean text output. Thanks for your help.
Got it sorted out. Requires a bit of clean up, but it's definitely working out now.
The gains are partly around SGAM file contention, which you alleviate with multiple files.
I'd recommend SQLite. It's super quick to set up and easy to interface with on the command line
Without doubt the stupidest article I've seen this week. Any sane person in the world would use bulk insert from a CSV file (that you can produce from Excel) or import export wizard. It's shocking this guy really believes he's onto something here.
Why anyone would pick MySQL as a replacement for Oracle when you have PostgreSQL available to you is unfathomable. It's obviously being done for cost reasons, because no-one in their right mind would do it for any other reason. MySQL is so functionally poor compared to Oracle, something which the OP will soon discover when he makes the switch.
I don't believe they are related .. a mirror will ALWAYS be in "Restoring" mode, as it is constantly shipping logs to it once the mirror is running. Note: While the mirror is 'down'/not replicating, logs on the primary cannot be truncated. I've had a problem on a server with low disk space where a failure on the network connection at the mirror machine caused the primary to quickly run out of space, as the logs could not shrink. Was painful to recover from. So the restoring is normal. Getting the endpoints to actually connect can be a fine art, and most of the MS documents are very sketchy on the details. 2005 certainly was not very good at giving meaningful errors. I've not tried again since with 2008 or newer. 
Thanks to your information I found this article that explains SGAM: [SQL Server Tempdb and Latch Contention](http://logicalread.solarwinds.com/sql-server-tempdb-latch-contention-w01/#.VZE1nnMo7qA). It makes sense to me now why multiple files can speed it up. Seems like this could apply to all write heavy file workloads.
Nah you should never use preexisting tools when you can do it manually with a ton of copy pasta.
WM_CONCAT is undocumented and unsupported by Oracle, it is an internal function for Oracle's own use. It SHOULD NOT be used in production systems as it could be removed at any time and any patch-set. Incidentally, it has been removed from the latest 12c version. My advice, when you move to 11gR2, migrate WM_CONCAT occurrences to [LISTAGG](http://www.techonthenet.com/oracle/functions/listagg.php), which is supported from 11gR2 onwards and far more flexible anyway.
Does this have to be done with SQL? If you can give me a sample of the table (ie. 2-3 rows) I'll gladly craft a python script to fix this for you. You could put an extra column in the table, named "dateFixed" set to a boolean value or something, the python script could pull every instance with an incorrect date, sort the date out by using the position of the characters (assuming that's constant) and update the value in the table. Just have the script run a query against the table at a set interval, depending how soon the values need to be fixed.
The advantage of this technique is you can produce script and once the script is generated you can run the same script at multiple servers like test, development and production etc. 
Back in the office this week, and went to look over this again. Out of curiosity, did you get this working properly? Did it return the results you needed, or did you have to tweak it further? Just curious if you're able to share an update, hope all else went well either way. Thanks!
Yes - but that has no relevance here. OP is using a bigint to store decimal date representations so that the only valid values in the 100s and 1000s columns would be valid month numbers, for example, so {01..12}. That is not Unix time. Unix (or POSIX) time looks more like this: https://en.wikipedia.org/wiki/Unix_time . In SQL, SQL Date types are the efficient way to store date values, surprisingly. ;)
try something like: select a.cons_numb from (a) where a.cons_numb not in (select cons_numb from (b))
The alternative solution would PROBABLY (as much as it is a pain in the ass) be to use where prod_desc like '%product1%' OR prod_desc like '%product2%'...etc. That'd be the approach I would utilize.
That's what I'm currently doing, but the problem is then I can't group them by product. Any suggestions there would be much appreciated!
oh I understand. You have prod_desc correct? select count(*), prod_desc from tbl where prod_desc like '%prod1%' or prod_desc like '%prod2%' group by prod_desc Edit: It looks like you may have a product column too. Use select count(*), product. Then group by the product column, not the product description. Hope that clarifies things.
Also, here's an example I found on stack overflow, worked well when I tried it. declare @date date set @date = '19901124' select @date --your date format select CONVERT(varchar(10), @date, 101) --new date format
For reference, this is called an anti-join.
Be careful with NOT IN and NULLs. If cons_numb is nullable, you should write this: select a.cons_numb from a where a.cons_numb not in (select cons_numb from b where cons_numb is not null) Or use NOT EXISTS instead.
Which SQL does this use? I would like to learn SQL. However i have seen several differnt ones like: mysql, sqlite, etc
Yes, it's possible. You'll want to select productiondate, sum(productionquantity), rejectdate, sum(rejectquantity) from production inner join reject on productiondate = rejectdate groupby productiondate, rejectdate. See what that gives you.
SELECT p.productionDate as a, SUM(p.productionQuantity) as b, r.rejectDate as c, SUM(r.rejectQuantity) as d FROM production p INNER JOIN reject r on p.productionDate = r.rejectDate GROUP BY p.productionDate, r.rejectDate The output: a / b / c / d 2015-06-26 / 45189 / 2015-06-26 / 814 2015-06-27 / 30954 / 2015-06-27 / 360 2015-06-29 / 29448 / 2015-06-29 / 1455 My production data spans from '2015-06-22' to '2015-06-30' and my reject data spans from '2015-06-26' to '2015-06-30' It seems to only display data that have data in both dates.
-- This query rolls productionDate and rejectDate into one column -- IsNull can be used to change null qty to 0 select coalesce(p.productionDate, r.rejectDate) as dt, sum(p.productionQuantity) as productionQuantity, sum(r.rejectQuantity) as RejectQuantity from production p full outer join reject r on p.productionDate = r.rejectDate group by coalesce(p.productionDate, r.rejectDate) order by coalesce(p.productionDate, r.rejectDate) 
Oops, I am mistaken. /u/trip_2020 is correct, sorry. I'm a bit sleep deprived and for some reason I thought you wanted returns on only dates that existed in both. In that case, you will want a full outer join like /u/trip_2020 and /u/moosies said. 
I've run into this problem before; it's doing the Cartesian product and it's screwing your end result up in the process. You'll need to use a subquery in your join IIRC; it's REALLY strange and it's happened to me once. Try using a subquery within the join.
No idea what Netezza is. I've only used MSSQL. Hope this helps: CREATE TABLE #PRODUCTS ( ProductName VARCHAR(255) ) INSERT INTO #PRODUCTS VALUES (..), ... (..) SELECT P.ProductName , ( SELECT COUNT(*) FROM tbl T PD ON T.ProductName = P.ProductName WHERE T.prod_desc LIKE '%' + P.ProductName + '%' ) AS Count FROM #PRODUCTS P Assuming that product names are unique. If not, use product id instead. 
Seems to work now! I'm not sure what I did really, I just took one of the formats of my pivot and somehow it worked... :/ SELECT * FROM (SELECT p.productionDate, SUM(p.productionQuantity) as qty FROM production p GROUP BY p.productionDate) p FULL OUTER JOIN (SELECT r.rejectDate, SUM(r.rejectQuantity) as qty FROM reject r GROUP BY r.rejectDate) r ON r.rejectDate = p.productionDate Thanks for the help
The inner query should get executed only once, so it's pretty much optimal the way you had it.
Hmm, I see. Seems like there could be a way to use qualifiers to get around that. I use SSIS to import pretty much everything, and I've certainly handled a similar issue with it. If I have time to dig into one of my packages, I will let you know how I handled it.
I had to do this a couple weeks ago, and used method 1. $CSV = Import-CSV &lt;FilePath&gt; | Select-Object -Property &lt;only the properties you want to import&gt; $CSV | Export-CSV &lt;NewFilePath&gt; This method worked for me, except I needed to do some other manipulation of the data before it would load (stripping semicolons and dropping the excessively long fields) I also had to run it through notepad++ to remove the quotation marks from the .CSV that powershell added. 
About the only conclusion we've come to around here is that we're just going to have to remove that column. Everything else should work. And it pisses me off so much.
If you can, would you be able to send me a small bit of sample data from the CSV file? You can change values to whatever, so long as the format remains the same. I don't want you to send me confidential info for my sake and yours :)
I think the easiest way to imagine it is: 1, Y, N, Bob,Roberts, 13:30, East So you can see, I have a lot of integer, string, etc. stuff, no problem. But Bob,Roberts is...one column. We only (in this example) have columns A-F. For now, I'm just going to work around it by manually deleting the name column.
 select MANAGING_FACILITY "Managing Facility", PO_TYPE "Po Type", PO_NUMBER "Po Number", HOME_OR_FIELD "Home Or Field", REVISION "Revision", REVISION_DATE "Revision Date", COMMODITY_CODE "Commodity Code", DESCRIPTION "Description", PO_NOTTOEXCEED_VALUE "Po Nottoexceed Value", PO_RELEASED_TODATE "Po Released Todate", TOTAL_VALUE "Total Value", COMMITTED_VALUE "Committed Value", AWARD_DATE "Award Date" from AD_PO_LOG_V where TRUNC ("FINAL_CLOSEOUT_ACTUAL") is null and (( REVISION_DATE &gt;= ADD_MONTHS (TRUNC (SYSDATE, 'MM'), -12) and REVISION_DATE &lt; ADD_MONTHS (TRUNC (SYSDATE, 'MM'), 1) ) or ( AWARD_DATE &gt;= ADD_MONTHS (TRUNC SYSDATE, 'MM'), -12) and AWARD_DATE &lt; ADD_MONTHS (TRUNC (SYSDATE, 'MM'), 1) and RELEASE_NUMBER IS NULL )) 
Thank you for the reply. What I am looking for is three sets of the fields, joined together. So, Managing Facility, PO Type, etc. will be repeated three times and joined together in a table that has 39 columns.
You would only normally pivot the data like you want to do, if the data for each record was related, are they?
How are they related? What is the relationship between the three sets of records?
Awesome--didn't know the syntax surrounding IN. That's certainly helpful. I'd use temp tables or bulk insert, but it turns out I don't have those rights :). Thanks for your comments!
Sounds like you just want to have UNION ALL between your three queries and run it as one statment.
That revised query should work fine. The original issue stems from the JOIN ON columns (ProductionDate &amp; RejectDate) each having multiple rows in their respective source tables for a given value. From the numbers, we can even make some guesses about how the data looks in the tables before aggregation. Looking at 2015-06-27 as a hypothetical example... ...we can determine the number of rows in the reject table (with date = 2015-06-27) using our productionquantity sum values 1. Sum(ProductionQuantity) = 5159 2. Sum(ProductionQuantity) with the Join = 30954 3. 30954 / 5159 = 6 4. Therefore, we can deduce that there are 6 rows in the reject table for the given date ...we can also determine the number of qualifying rows in the production table using the same method 1. Sum(RejectQuantity) = 24 2. Sum(RejectQuantity) with the Join = 360 3. 360 / 24 = 15 4. Therefore, we can deduce that there are 15 rows in the production table for the given date Taking this a step further, [example:](http://i.imgur.com/8qFineI.png) * Query A1: Hypothetical example of the contents of the Production Table for the given date * Query A2: Hypothetical example of the contents of the Reject Table for the given date * Query A3: Expected result of the sum of ProductionQuantity in A1 * Query A4: Expected result of the sum of RejectQuantity in A2 * Query B1: Without the aggregation options, this is the result of joining without the subquery. As you can see, for each row in @Reject, the row in @Production is repeated. Calculating the sum of the ProductionQuantity column (with all those repeated values) is how the 30954 value was created. Same for 360 being the sum of all those repeated RejectQuantity Values * Query C1: The bad result that comes from the join demonstrated in B1 * Query C2: The expected result. **Basically, it is the result of joining A3 and A4.** * Query C3: A second example of C2 with the date columns consolidated into a single column.
Scripting languages simply aren't an option here. I'm going to try the table when I have time, though. I mean, this was already basically just a staging table anyway (to be used as a source for later joins), so I didn't want to use a staging table for a staging table :)
Thank you! I will give this a try and respond with any questions.
Thank you! I will give this a try and respond with any questions.
Any chance of fixing the source file? Generally easier to get the source in a nice consistent state than hacking around the problems further down the line. Otherwise notBenstar's solution is probably the best bet.
It's a CSV, so yes :(
Microsoft Access treats anything in square brackets as a literal. I suspect you have something like [ ( ] happening somewhere in that mess of code. One approach is to bracket out half the SQL code and test. If it fails when you run one half, and not the other half, you have narrowed down where the problem is. Repeat by reducing the SQL until you get right to the problem.
it's not an SQL application, it's a text editor called [ultraedit](http://ultraedit.com)
and the formatting is done by me with manual indents (albeit with multi-line capability) as well as search-and-replace (to change commas to new-line-plus-n-spaces-commas)
OK thanks for the advice. I'm still learning SQL and its going to be very important for my next career step, so this kind of feedback is good.
I've done so much of the same work the /u/clockworkcow mentioned, I had to laugh reading his description. In the users scenario, I'd probably opt for using sublime text and a regular expression for the csv. Maybe I am looking at your example data too literally, but if you are looking for a comma that comes before a space character and after more than one letter character. Might be even easier to adjust those Y N columns into Boolean 0 and 1's. Then you are just looking for a comma that is between letter characters. I am sure there are more elegant ways to do it, but I'd start by killing any space character after a comma, then find all my single letter n or y that are followed by a comma and alter them to a 1 or 0 followed by a comma. Then I'd take the first instance of a comma falling between 1 or more letter characters and modify to my tastes. 
Haha, you're definitely looking at the data too literally :) But that's a good idea, thank you.
Well, that solution, while good, probably wouldn't work for our application. We do a randomly-activated 'emergency' schedule where we ping things every 15-30 minutes. We could try to schedule around it, but we already have at least 1 automated process on a schedule running another part, and I don't have full access to these things (I'm obviously not a DBA, I just dove into this project).
Homework?... Aggregation approach : select A, B, C from MYTABLE where (A, B) in (select A, max(B) from MYTABLE group by A) Windowed analytic approach : select t.A, t.B, t.C from ( select A, B, C, row_number() over (partition by A order by B desc) RNUM from MYTABLE) t where t.RNUM = 1 **EDIT** : If MYTABLE's PK is not a composite of A, B then aggregation approach : select top(1) A, B, C from MYTABLE where (A, B) in (select A, max(B) from MYTABLE group by A)
Use [**SCOPE_IDENTITY()**](https://msdn.microsoft.com/en-us/library/ms190315.aspx) It returns the last IDENTITY value produced on a connection and by a statement in the same scope, regardless of the table that produced the value. As the scope is your statement and in your session, it must be the value you want.
What exactly would the syntax be for that in this case?
Examples in the link
 you can use the output clause https://msdn.microsoft.com/en-us/library/ms177564.aspx 
Well. Shit. 
D'oh, yeah that's it. Great, thanks for your help!
This is by far the best and safest method of retaining referential integrity when doing inserts.
I'm not 100% clear on what you're asking, but I think this should work: select SupplierCode from table where SeqNum in (select min(SeqNum) from table group by user, ID)
[Google Geocoding API example using The White House's address](https://maps.googleapis.com/maps/api/geocode/json?address=1600%20Pennsylvania%20Avenue%20Washington%20DC). Though keep in mind there are limitations to how frequently you can use this service (2500 requests per 24 hours I think) so if you have a large dataset then you might want to consider something like what r3pr0b8 suggested. 
here's a way to do it with ROW_NUMBER WITH cte AS ( SELECT ID ,[User] ,[DateTime] ,ActivitySubType ,ActivityType ,SupplierCode ,SeqNum ,ROW_NUMBER() OVER (PARTITION BY ID,[DateTime] ORDER BY SeqNum) AS rowno FROM your_table ) SELECT ID ,[User] ,[DateTime] ,ActivitySubType ,ActivityType ,SupplierCode ,SeqNum FROM cte WHERE rowno = 1 
Thank you! 
I ran into this problem when I was trying to implement this as a dev environment. Didn't have time to figure out what caused the issue if you do find out can you please PM me? 
Are you certain that there aren't more than 26 occurrences of the same Invoice ID? If so, you can use CTE and the ROW_NUMBER() function to get what you are asking for. WITH temp_tbl AS (SELECT ID, InvoiceNo, ROW_NUMBER() OVER (PARTITION BY InvoiceID, ORDER BY ID) rownum FROM your_table_name) SELECT ID, CAST(InvoiceNo AS VARCHAR(10)) + CHAR(rownum + 64) InvoiceNo FROM temp_tbl I'm adding 64 to rownum to get the ASCII values starting with "A" and then appending it to the InvoiceNo.
This isn't an easy question actually. This type of string concatenation isn't something that SQL is terribly good at inherently, though there are ways. Honestly, a temporary table that is updated is probably one of the better approaches, but if that's fully off the table something like this will work, but at a performance cost. -- USE A CTE TO GET A DISTINCT LIST OF PK NUMBERS. ;WITH DISTINCT_PKNUMBERS AS ( SELECT DISTINCT [PK#] FROM TABLE_NAME ) SELECT [PK#] = PK.[PK#] -- USE A SUB-SELECT TO CONVERT RECORDS FOR THIS PK TO AN XML CONCATENATED STRING. -- USE THE STUFF FUNCTION TO TAKE OUT THE FIRST CHARACTER, SO ONLY SEPARATORS ARE LEFT. ,[LICENSE_STATES] = STUFF( ( SELECT [text()] = '/' + LS.[LICENSE_STATE] FROM TABLE_NAME LS WHERE PK.[PK#] = LS.[PK#] FOR XML PATH ('') ) ,1 ,1 ,'') ,[LICENSE_NUMBERS] = STUFF( ( SELECT [text()] = '/' + LN.[LICENSE_NUMBER] FROM TABLE_NAME LN WHERE PK.[PK#] = LN.[PK#] FOR XML PATH ('') ) ,1 ,1 ,'') FROM DISTINCT_PKNUMBERS PK Basically you get a distinct list of PK# values from your record set. Within that set, you use a sub-select statement to go and get an XML interpretation of every row, appended together with the '/' character. This XML concatenated string is then parsed with the STUFF function to take out the first character, so only the '/' characters between records are left. I've had to use similar hacks when I was required to make a view show concatenated notes within a scroll window for an OLTP system. Not pretty, but it is effective. Good luck!
I'll try to clarify a bit: For records that are over 40 GB, they would be excluded from the result set. I'm only concerned with grouping individual "Persons" together until their sum of GBs hits as close to 40 GB. My bad with Person 8 and 9. I edited the original post and took that out. Conceptually, yes that does make sense. And I think that would work too. On paper it seems like that would accomplish nearly exactly what I'm trying to do. I'll have to write something up tomorrow when I get the chance. Thanks for the reply!
Just wanted to comment that these are all excellent solutions to this problem (which is typically referred to as a "Top Per Group" query), but be mindful that while they will all give you the results you expect, some will out-perform others in different situations. One of the best articles I've ever read on this was by Bob Hovious on SQL Server Central. The article is called [T-SQL: Why "It Depends"](http://www.sqlservercentral.com/articles/T-SQL/69481/). You may not be able to read it without a free registration to SQL Server Central, so I'll briefly quote the findings below. Essentially, he puts forth three standard approaches to Top Per Group. 1. MAX() GROUP BY 2. ROW_NUMBER() 3. SELECT TOP (1) He extensively demonstrates and tests each approach across results sets with 1K, 10K, 100K and 1M rows in sample tables. His graphs speak to the findings eloquently, but here were the results demonstrated in raw numbers. **Execution Time in Milliseconds** Technique | 1,000 Rows | 10,000 Rows | 100,000 Rows | One Million Rows ----------|------------|-------------|--------------|----------------- Match MAX() | 489.6 | 515.18 | 937.06 | 15526.08 Row_Number() | 4.28 | 70.3 | 2121.12 | 63764.18 Top (1) | 6902.94 | 6911.56 | 7066 | 8479.36 **Execution Time in Milliseconds *With* a Non-Clustered, Non-Covering Index** Technique | 1,000 Rows | 10,000 Rows | 100,000 Rows | One Million Rows ----------|------------|-------------|--------------|----------------- Match MAX() | 2.22 | 12.94 | 113.96 | 1120.96 Row_Number() | 4.48 | 49.9 | 2169.44 | 61470.84 Top (1) | 1.92 | 10.6 | 99.22 | 975.18 Essentially, depending on the volume of records and whether you had an index, one version of the solution could significantly out-perform the others. As an example: * ROW_NUMBER() was a consistent winner without an index and for 10K rows or less, by a significant margin. * TOP (1) won out if you had an index in all situations, though it was very, very close to MAX() with the same index. * Without an index MAX() was better up through 100K rows, but fell off compared to TOP (1) at 1M rows. Fascinating findings, and wonderful article. It really helped me to understand just how much the approach and performance will vary depending on your environment, data, and design.
Which DB server are you using? On Oracle, binds are used in triggers when you want to refer to the value after the triggering(`:new`) or value before the triggering event (`:old`). The code you have written doesn't make much sense and you'll run into [mutating trigger/table problems](http://stackoverflow.com/tags/mutating-table/info). Furthermore, your queries are wonky - you're fetching into the same variable as you are using in the where clause?? That'll cause the query to fail and you'll run into `NO_DATA_FOUND` errors 
;with recursive as ( Select top 1 ID, amount From #worktable Where amount &lt; @limitValue Union all Select top 1 ID, amount From recursive r Where amount + (Select sum(amount) from recursive) &lt; @amountlimit ) Select * from #work w Inner join recursive r On r.ID = w.ID Ill have to test that tomorrow. It will probably need some tweaking. 
I have little experience with CTE's, so I might take this approach just to see if I can get it to work. Honestly I barely have a grasp of how recursive CTE's work so it will probably be a decent challenge for me. Thanks for the reply.
Definitely the approach to use
I think you should be able to do this recursively. I haven't written recursive sql myself, yet but there are plenty of resources online. Other than that you need to look at different tree structures maybe a closure table? Nested sets or maybe an adjacency list will also do the job.
Thanks, that looks promising.
Brilliant, yet bonkers. I love it.
still haven't figured out how to make it work
This. If it is a heavy query a self referencing foreign key will drastically speed something like this up. (ParentID REFERENCES ID) 
There is no easy way to solve this, it's a version of the [Knapsack Problem](https://en.wikipedia.org/wiki/Knapsack_problem) which is NP-Complete. *AKA - You can be [crafty](http://bayen.eecs.berkeley.edu/bayen/?q=webfm_send/266) but there are limitations.*
Same, MySQL 5.6.23 throws me lots of syntax errors when I try to run it.
&gt; Applies to: SQL Server (SQL Server 2008 through current version), Azure SQL Database. I was stuck on 2005 until just a year ago, but I'm still ashamed to admit this is the first I've even heard you could do this. 
Awesome!! I tried yours yesterday but must have had the syntax wrong because I could never produce results. This seems to be working perfectly!!!! Thank you.
It's a cool game but I keep losing. How do I get past level 2? Is it best to fight the dragon or level up first? 
I decided to approach the problem by throwing random chance it. As a giant game of guess and check, calling it "calculating" the best configuration is probably a gross misnomer, but for practical purposes I think it approaches the right configuration with a reasonable degree of accuracy. That is of course with the caveat that "optimal" is defined as a sequential increase in the amount of unused space per grouping. Imo though, "optimal" is kind of a fuzzy term. In the context of DBs on the least possible number of drives, optimal could be greatest free space per drive while fitting all dbs into the minimum number of drives (an even better answer would be correlating most free space to predicted growth per db, but in this hypothetical that data doesn't exist). In the context of burning files to physical media, free space per "disc" is probably irrelevant. Lowest number of "discs" would be optimal and many possible "optimal" solutions could exist. Disclaimer: There wasn't even a single performance consideration in making this. I just tried to dump out a theoretically accurate result set. http://pastebin.com/W4BsQXyb The below result set was generated using the above script with @iterations = 10,000 TotalSize | PersonList --------|--------| 39.84 | Person 1, Person 9 39.78 | Person 2, Person 7, Person 8 28.01 | Person 10, Person 4
Thank you so much for the reply. I've never used the stuff function or the XML Path stuff, so this might be difficult, but you've given me some great information to point me in the right direction. Thanks again for taking the time! Once I get a chance to test your method, I may reply with further questions.
Awesome--really appreciate your reply as well. I will certainly try to do something like this and may have more questions. If I do, I'll definitely reply again. Thanks again for taking the time to assist!
You could do a distinct on the people id, or do a not like on the phrase "Emergency Contact" in whatever field is associated with that.
If you are on ms sql, you could use hierarchyid type to code/capture your hierarchy. A syntactically simple a&lt;b is the 'isaDescendant' test then. https://msdn.microsoft.com/en-us/library/bb677290.aspx
Thanks for pointing me in the right direction.
Hi - if this has only changed since an application update, then you need to look at the internal SQL Server performance before making hardware changes. EG. If a query has changed and now could do with a different index elsewhere that's missing, it could result in a higher than previous number of reads from the database. Cumulative effect could be visibly increased load on your disks. Equally it could increase tempdb usage for sort spills and cause contention on allocation pages (this is what you were looking around with multiple tempdb files) however this is a really specific problem, you need to see if you are actually getting contention on these pages prior to adding files or you won't see any benefit TLDR: do some performance analysis in the database first as this could well be what's causing your disk performance changes and it's possible to fix at source. If this is not something you can do yourself I would get someone in to have a look
What is your question? Subqueries are used very, very often in SQL.
It's basically create two subscribes in a from statement, but after looking at the database there really isn't a sub query one would use in a from statement. I could send you the ERD to take a look at it if you wish. 
Join two tables and use that in your FROM clause. There's your subquery.
As other commenters have mentioned, what you're looking for here is an index. To be more specific, you're looking for a "clustered index" - clustered just means that the data is physically sorted on disk according to the index. Non-clustered indexes achieve similar results by caching information about the data in a separate data structure, but they aren't quite as performant as clustered indexes and they take up additional hard drive space. As you can imagine, a table can only have one clustered index. A primary key is a type of clustered index, so I'd recommend you get to the result you're looking for by adding a PK to your table.
just use PostGIS it's already got this built-in
Since I need to group by those columns to get the correct data, don't I need to add them into my select statement? That is why I'm thinking I need it to be a subquery. 
Nope, you're going to be perfectly fine selecting just what you need in this case.
EDIT: nevermind, I believe that this answers my previous question: CREATE TABLE waypointIdToPathIdMap ( waypointId, pathId, indexInPath, PRIMARY KEY (pathId, indexInPath) ); end EDIT Thanks. So one can't implement the equivalent of Java's 'nested' (for lack of a better term) Comparator? To explain: I'd like to keep the waypointIdToPathIdMap table sorted by pathId, but if their pathIds are equal, I'd like them to be sorted by indexInPath, rather them reading all waypoints for a given path in and then sorting by their indexInPath each time, although the nLogN sort wouldn't really kill me. - - - - - name: waypointIdToPathIdMap fields: waypointId pathId indexInPath - - - 
Why not just SUM the MAX row number for easy patient. Since the partition is within the filtered period that should be the total number of occurrences. 
Also FYI - when they say legacy, they will be referring to the version of the database engine - not SSMS, which is backwards compatible (eg. you can use SQL2012 Management Studio to point to a SQL2005 instance) If they have any 6.5 instances you should just run away :)
You could set up a vbs script ran from task scheduler to automate backups. :( [This could also be a good read](http://itknowledgeexchange.techtarget.com/sql-server/the-least-expensive-sql-server-2012-high-availability-solution/)
Thanks, will try at tomorrow and check the results. 
I encourage you to investigate why that is your only option. If it's cost that's a concern, a LAMP stack is cheaper and more efficient. If they are stuck on a Windows server, you can still install Postgres or MySQL. The bottom line is that high availability, flexibility, and performance are simply not terms that would be associated with those database options. If you need those things, you're looking in the wrong place.
There is no built in feature in Express for HA or DR but both can be achieved. You are able to cluster Express for HA, but not through a wizard, you have to add it as a generic service. It isn't cluster aware but it should work. You can also do log shipping to accomplish your DR, but again that is outside of the actual product since there is no agent. Task scheduler + batch/powershell scripts will allow you to take log backups and copy them off to another location where they can be recovered. Why not use azure instead of doing it in express? The cost would be pretty low if it isn't a lot of data and then you'd have HA built into your SLA. 
You're welcome! I wish I had found your article 5 yrs ago ;)
how did you do the backup in your attempt ?
MSSQL is a product that MS makes money from... Express Edition is the *starter* edition... HA is a cost decision (not just software, but hardware as well)... which is also why HA and scaling (partitioning/etc) features are focused on the Enterprise Edition.
Thanks, this is a good start!
&gt; a table can only have one clustered index. Using the INCLUDE clause on an index creation can make a table have, effectively, multiple clustered indexes.
The table should generally hold the primary keys for each entity. Check out the Wikipedia article on [junction tables](https://en.wikipedia.org/wiki/Junction_table) (which have about a dozen names, I always have called them "join tables"). It has a diagram to illustrate an example many-to-many relationship between actors and movies, and the junction table that would hold that relationship.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Junction table**](https://en.wikipedia.org/wiki/Junction%20table): [](#sfw) --- &gt; &gt;In [database management systems](https://en.wikipedia.org/wiki/Database_management_systems) following the [relational model](https://en.wikipedia.org/wiki/Relational_model), a __junction table__ is a database table that contains common fields from two or more other database tables within the same database. It is the standard way of creating a many-to-many relationship between tables. &gt;In most database systems, only one-to-one or one-to-many relationships between data tables can be created directly, usually by utilizing [foreign keys](https://en.wikipedia.org/wiki/Foreign_key). The Foreign Key (FK) usually is a [Primary Key](https://en.wikipedia.org/wiki/Primary_Key) (PK) of another table and thus a unique constraint. &gt;A junction table maps two or more tables together by referencing the primary keys of each data table. In effect, it contains a number of foreign keys, each in a many-to-one relationship from the junction table to the individual data tables. The primary key of the junction table is typically composed of the FK columns themselves, but may employ its own PK as well. &gt;==== &gt;[**Image**](https://i.imgur.com/EaotaDh.png) [^(i)](https://commons.wikimedia.org/wiki/File:Mapping_table_concept.png) --- ^Relevant: [^Many-to-many ^\(data ^model)](https://en.wikipedia.org/wiki/Many-to-many_\(data_model\)) ^| [^Bloor ^Street](https://en.wikipedia.org/wiki/Bloor_Street) ^| [^Wisconsin ^Highway ^44](https://en.wikipedia.org/wiki/Wisconsin_Highway_44) ^| [^Route ^26 ^\(Uruguay)](https://en.wikipedia.org/wiki/Route_26_\(Uruguay\)) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+csthz01) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+csthz01)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](/r/autowikibot/wiki/index) ^| [^Mods](/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Call ^Me](/r/autowikibot/comments/1ux484/ask_wikibot/)
This type of table goes by many names, but at a minimum it needs to hold the primary keys of each of the associated entities. If it makes sense to do so, you can also store things here that are about the relationship itself, such as start and end dates or which user created it or something. This is exceedingly common and there will be many examples, tutorials, and explanations of this online.
Your plan is to denormalize this data, which is just bad. Make a table of professors with their appropriate columns, unique per professor. Make another table with the course data, unique per course, then import just the 2 columns you listed here as a third table. Then you can create a query, view, report, etc. That will display the data you want how you want regardless of how many courses a professor has.
Your query looks correct to me, assuming that the lack of a comma between `A.EXAMPLE_COLUMN` and `C.COLUMN_I_NEED` is just a typo in your post here. Does `EXAMPLE_COLUMN` actually exist on `TABLE_A`? &gt;error because I am trying to join Table A and C using columns from B and C You aren't joining A to C. You're joining A to B, and B to C. Which is completely normal and common. What is the actual error text you're getting (copy &amp; paste it, anonymize names like you've already done here)?
In your example you forgot "as" - is this your real problem? FROM TABLE_A AS A LEFT JOIN TABLE_B AS B ON A.CURR_AFF=B.CURR_AFF 
For future reference, before you copy/paste code to Reddit, indent it with four spaces. This marks it as code so it doesn't apply the normal markdown. SELECT A.EXAMPLE_COLUMN ,C.COLUMN_I_NEED FROM TABLE_A LEFT JOIN TABLE_B ON A.CURR_AFF=B.CURR_AFF LEFT JOIN TABLE_C ON C.PROV_CK=B.PROV_CK Looks like you may have forgotten to alias your tables. You need to put `A` after `TABLE_A` in order to reference the field as `A.CURR_AFF` instead of `TABLE_A.CURR_AFF`. SELECT A.EXAMPLE_COLUMN ,C.COLUMN_I_NEED FROM TABLE_A A LEFT JOIN TABLE_B B ON A.CURR_AFF=B.CURR_AFF LEFT JOIN TABLE_C C ON C.PROV_CK=B.PROV_CK 
Very cool didn't know about the 4 spaces that's super helpful. And in my example code for simplicity I left a lot of syntax out. But in real code it is there, I updated as it seems many people were confused
Good point, thanks.
Assuming there's actually a need to change GUIDs, you can create a mapping TABLE_A2C( GUID_A, GUID_C), populate it whenever you load table_A and use it to load x-refs in TABLE_C
The only reason I would want to change them is because of the possibility of duplicates. Should I worry about that?
&gt; **PRACTICE** 1000x This. I've experienced too many companies that said "We have backups"; but had never been restored/tested and were corrupt. When it came time to use them they were out for days instead of what should have been an hour or two. 
natural join and equi join are different have you tried google? 
Don't use natural joins, they are an abomination of assumption. Always declare your joins explicitly.
no i am so new to the internet i did not get the freakin idea to ask google first ... YES i did and I don't get proper answer for a natural join of 3 tables, don't know how to write it down, I'm only getting errors
my question was not "what is better to use" but how does a natural join of 3 tables look like?" 
Not necessarily a pivot situation, can also be done with CASE, though less scalable SELECT CustomerID, IsNull(SUM(CASE WHEN Product='A' THEN Quantity ELSE 0 END),0) AS A, IsNull(SUM(CASE WHEN Product='B' THEN Quantity ELSE 0 END),0) AS B, IsNull(SUM(CASE WHEN Product='C' THEN Quantity ELSE 0 END),0) AS C FROM sometable GROUP BY CustomerID
Since OP is being so adamant about natural joins I'm guessing this is for homework or something. There's really no reason for new developers to use these joins anymore.
okay, it looks like this -- SELECT curly.foo , larry.bar , moe.fap FROM curly NATURAL JOIN larry NATURAL JOIN moe 
I think there will be performance problems with the latter method. Neither will be speed demons, but the latter will be worse.
What was the error that you got when you tried that? See http://stackoverflow.com/a/28406477/1324345
It says: [Error] Execution (20: 18): ORA-00932: inconsistent datatypes: expected NUMBER got DATE In excel, simply subtracting =today()-entrydate would give you the difference in days between the two as a number. That's the kind of thing I'm looking for.
Unfortunately I found that while netezza is based off of postgresql they do not support pivot tables :\ Thank you for the help though!
It is likely the ENTRYDATE is VARCHAR2 and not a DATE. This code works: with data as (select to_date('01/01/2014','MM/DD/YYYY') entrydate from dual) select ((sysdate - entrydate)/365) as years_open from data; This code generates the same error you are getting: with data as (select '01/02/2014' entrydate from dual) select ((sysdate - entrydate)/365) as years_open from data; 
I get the following error with ENTRYDATE higlighted: [Error] Execution (20: 40): ORA-01843: not a valid month 
I just tried this: TO_DATE(ENTRYDATE, 'MM/DD/YYYY') AS Date_Entered, And got the following error: ORA-01830: date format picture ends before converting entire input string
That means your date string is longer than the format you specified. It probably has time, so your date mask needs to include everything to match it.
You could do what you've outlined. You could also join to your item table 9 times. Let me ask you though: why transpose your drop item records? If the drops would be captured as individual records, the query (and the whole formula) would be much simpler.
 SELECT MD.Name, MD.Id, P1.sell_price*MD.Drop1Per + P2.sell_price*MD.Drop2Per + P3.sell_price*MD.Drop3Per + P4.sell_price*MD.Drop4Per + P5.sell_price*MD.Drop5Per + P6.sell_price*MD.Drop6Per + P7.sell_price*MD.Drop7Per + P8.sell_price*MD.Drop8Per + P9.sell_price*MD.Drop9Per AS MobPrice FROM mob_db MD INNER JOIN sell_price P1 ON P1.ID = MD.Drop1Id INNER JOIN sell_price P2 ON P2.ID = MD.Drop2Id INNER JOIN sell_price P3 ON P3.ID = MD.Drop3Id INNER JOIN sell_price P4 ON P4.ID = MD.Drop4Id INNER JOIN sell_price P5 ON P5.ID = MD.Drop5Id INNER JOIN sell_price P6 ON P6.ID = MD.Drop6Id INNER JOIN sell_price P7 ON P7.ID = MD.Drop7Id INNER JOIN sell_price P8 ON P8.ID = MD.Drop8Id INNER JOIN sell_price P9 ON P9.ID = MD.Drop9Id If a dropId column can be null then turn the INNER JOINs into LEFT JOINs and use COALESCE. The ugliness comes from having nine drop columns in your mob table. I suggest splitting it up, one drop per row: mob_table: MobId MobName DropId DropChance Then you can do: SELECT MobName, MobId, SUM(DropChance*SellPrice) FROM mob_table, item_table GROUP BY MobName, MobId ORDER BY MobId
Your mob_db table design is incorrect and not normalised. Your drop records should be one per drop instance, not 9 per record.
Out of curiosity, what was the scenario here? Are you analysing an existing game? .. based on parsed logs? Manually entered data? Or did you have access to the mobs' actual drop tables/rates? Or are you developing a game, and are devising a system to track whether the item drop system is equitable? 
The following will find all the columns in a database with a name like 'CompanyCode'. USE yourDatabaseName GO SELECT syst.name AS tableName, SCHEMA_NAME(syst.schema_id) AS schemaName, sysc.name AS columnName FROM sys.tables AS syst INNER JOIN sys.columns sysc ON syst.OBJECT_ID = sysc.OBJECT_ID WHERE sysc.name LIKE '%CompanyCode%' ORDER BY tableName, schemaName; As a side note, you can use the above to generate dynamic SQL, but I'd highly recommend doing it manually until your understanding of SQL grows. 
The query /u/Coldchaos posted just gives you the list of tables where the "CompanyCode" column appears. Just loop through this list (one of the few instances where cursors are acceptable) making updates via dynamic SQL: declare @tableName varchar(255), @schema varchar(255), @sql varchar(max); declare cur1 cursor local for SELECT syst.name AS tableName, SCHEMA_NAME(syst.schema_id) AS schemaName FROM sys.tables AS syst INNER JOIN sys.columns sysc ON syst.OBJECT_ID = sysc.OBJECT_ID WHERE sysc.name LIKE '%CompanyCode%' ORDER BY tableName, schemaName; open cur1; fetch next from cur1 into @tableName, @schema; while @@fetch_status = 0 begin set @sql = ' UPDATE ' + quotename(@schema) + '.' + quotename(@tableName) + ' SET CompanyCode = REPLACE(LTRIM(RTRIM(CompanyCode)), '' '', '''') WHERE CompanyCode LIKE ''% %'''; exec (@sql); fetch next from cur1 into @tableName, @schema; end close cur1; deallocate cur1; Hope this helps.
Yeah, I was reading the docs and it looks quite impressive, it just feels like too much for this. I probably will use it if I can't find a strait forward vanilla solution. (And math is fun)
Off the top of my head this would be my rough guess for what you're looking for: SELECT id, ST_Distance(thing::geography, ST_Buffer(ST_Point(search_lon, search_lat)::geography, 1000)::geography) as distance FROM thing_table WHERE ST_Intersects( ST_Buffer(thing::geography, 100), ST_Buffer(ST_Point(search_lon, search_lat)::geography, 1000)) it should look for all the points with a 100 meter radius that intersect your search points with a 1000 meter radius (assuming the column thing is a postgis point).
You're not wanting a join, but a union and you want the fields not to match in this union....excepting you probably have to have a couple of these matched from the two tables (but not showing) to allow for where clauses, sorting, etc. 
SYSDATE returns current date/time for the server, in the server's local time; CURRENT_DATE returns current date/time for the server, in the client's / connection's local time. As CURRENT_DATE has to be converted, it could be mutable. For example if a query spanned an adjustment to daylight saving time. I imagine this is why you are getting your warning, it has to treat CURRENT_DATE as a parameter with a potential range which would obviously affect explain plans.
Don't know your DB platform, so making some assumptions, but something like this : select me.SUBJECT, me.SENT_TO, me.TIMESTAMP as TO_TIMESTAMP, cast(null as varchar) as SENT_FROM, cast(null as date) as FROM_TIMESTAMP from TABLE_ME me union all select you.SUBJECT, null, null, you.SENT_FROM, you.TIMESTAMP from TABLE_YOU you order by me.TIMESTAMP, you.TIMESTAMP
Thanks for the tip. I will give this a shot! Edit: it is telling me that the second order by element, you.TIMESTAMP, cannot be bound. 
Ive worked on similar systems. Someone needs to rationalise those fields, turn them into dates and create a view on top to maintain app legacy. e.g. -- THE_TABLE (ID integer, DESCR varchar2(..), ADATEUNIXSTR varchar(..)) insert into FIXED_THE_TABLE (ID, DESCR, ADATE) select ID, DESCR, UnixStrToDate(ADATEUNIXSTR) from THE_TABLE / drop THE_TABLE / create view THE_TABLE as select ID, DESCR, DateToUnixStr(ADATE) as ADATEUNIXSTR from FIXED_THE_TABLE / Create instead of trigger on THE_TABLE to convert ADATEUNIXSTR on inserts / updates.
&gt; SELECT players.playerID AS 'PID', players.fName, players.lName, matches.playerID, COUNT(matches.playerID) AS 'gamesPlayed' FROM players RIGHT JOIN matches ON players.playerID=matches.playerID WHERE PID = $pid GROUP BY PID, players.fName, players.lName Try this: SELECT players.playerid AS 'PID', players.fname, players.lname, Count(matches.playerid) AS 'gamesPlayed' FROM players INNER JOIN matches ON players.playerid = matches.playerid WHERE pid = $pid GROUP BY players.playerid, players.fname, players.lname This should give you the following columns +---+---------------+---------------+-------------+ PID | players.fname | players.lname | gamesPlayed | +---+---------------+---------------+-------------+ 
One thing I noticed is you were using a column alias in the GROUP BY clause. Typically this will not work in most RDMS due to the order of processing. See this answer from Stack Overflow below: SQL is implemented as if a query was executed in the following order: 1. FROM clause 1. WHERE clause 1. GROUP BY clause 1. HAVING clause 1. SELECT clause 1. ORDER BY clause For most relational database systems, this order explains which names (columns or aliases) are valid because they must have been introduced in a previous step. Also you were selecting matches.playerID and then doing a COUNT on it. Assuming you included matches.playerID in your GROUP BY clause, this would have given you a count that always = 1. That being said, I don't know what you are trying to accomplish. 
So the main issue here is the overlaps. Since we have no guarantee about what kind of rules the overlaps follow, we have to account for everything. I'll give you a "cheating" strategy first using a numbers table and a way of dealing with the overlaps later. For this method, we're basically just going to count discrete minutes that are covered by the date ranges. -- Create Numbers table and fill for a day's worth of minutes (1440) create table Numbers ( Number int primary key ) set nocount on; declare @i int set @i=0 begin transaction while @i &lt; 1440 begin insert into Numbers select @i set @i = @i + 1 end commit -- actual code to get active/inactive time once numbers table is created/populated declare @earliestStartTime datetime, @activeTime int, @inactiveTime int Select @earliestStartTime = MIN(StartTime) From ActiveTime select @activeTime = COUNT(DISTINCT N.Number) From ActiveTime AT, Numbers N Where N.Number &gt; DATEDIFF(MINUTE, @earliestStartTime, AT.StartTime) AND N.Number &lt;= DATEDIFF(MINUTE, @earliestStartTime, AT.EndTime) Select @inactiveTime = DATEDIFF(MINUTE, MIN(StartTime), MAX(EndTime)) - @activeTime From ActiveTime Select @activeTime ActiveTime, @inactiveTime InactiveTime 
On both what? As I said, I have a local machine, which I'd like to populate 'some' database on. I can do what I like with this machine. But ideally it's going to be done with python and into some database. I'd like to migrate this locally populated database into my Bluehost account, which apparently has MYSQL set up. PHPmyadmin is indeed set up on this service. I mean, I *could* just populate the online DB with my local python script, but I'd rather do it locally, so that I can do it more quickly.
If you are relatively new to this, you are going to find yourself getting great results using the technique written about here: http://leehblue.com/sync-local-and-remote-databases/ 
http://www.made2mentor.com/2011/04/calendar-tables-why-you-need-one/ If you have a table of all available times, you can simply left/right join to it and then aggregate the Inactive (IE NULL activity) and then subtract that from the project's start and end datediff. Something like this (not at all tested): SELECT DateDiff(m, min(P.start), max(P.end)) - (sum(case when P.start is null then 1 else 0)*30) activeMins FROM CalendarHalfHours T LEFT JOIN Project P ON T.time between P.start and P.end 
Perhaps I should explain my use-case a bit better. My python script currently aggregates a few different music APIs to allow browsing of artists/albums. What I'd *like* to do is modify it a bit to store these into a local database (with either crawling, or just cacheing general usage). Once the DB obtains plenty of artists/albums/etc, migrate the DB over to my web host/php, and have it as a general look-up database. Ideally the python script would be much faster to store the data locally than to continually send insert requests to the remote database. I think the best case would be to quickly crawl the services, cache the content, and then upload the end DB. I'm thinking possibly up to a million artists, a few million albums, and a relational table (to show related artists). Would that take up more than 2mb? I figured that it'd be quite a large db, but it's hard to say. As I said, the aggregation is all done via python. But I'd like the end result to look up data with typical MYSQL/Php/phpmyadmin. I'm just not sure what the best way to get that conversion would be. Again, ideally I'd like it to be stored locally at first, so that I can do local development, rather than it possibly stressing the server (which I'm using for other things as well). Thoughts?
Well the idea is to make a DB locally. And once I'm finally ready, then upload it to my Phpmyadmin/MYSQL/PHP setup. Not migrating data from one host to another (which would be as simple as exporting on one phpmyadmin and importing in the other).
Perhaps avoid the `JOIN` altogether? SELECT *, Count((SELECT 1 FROM matches WHERE playerID=$pid)) AS gamesPlayed FROM players WHERE playerID=$pid; I've just started learning SQL so there is a decent chance this is completely wrong. I haven't tested it either because I'm on mobile, but I think it works. I'd appreciate if you let me know how you get on using it! Also, I'm assuming `players.playerID` is a primary key. EDIT: You'll need to do SELECT *, (SELECT Count(1) FROM matches WHERE playerID=$pid) AS gamesPlayed FROM players WHERE playerID=$pid;
I was trying sub queries last night but couldn't get it but I will give this a go, thanks for the help. I'm also pretty new to SQL.
 select p.playerID,p.fname,p.lname,p.phonenumber,p.rating from players p join (select playerID,count(m.playerid) from matches group by playerID) m on p.player_id =m.player_id where p.playerId=$ID
ORMs are OK for rapid prototyping of shallow Web Applications which aren't data heavy and just need to get up and running fast. Other than that they're not really all that great. 
Thank you kind stranger, I knew a sub query was going to be the simplest solution I just couldn't quite get it at 2am haha, this did work, although I changes it to count(playerID) instead of count(1).
Many thanks Moosies - I'll spend some time to go over your solution.
As a side-note, I would love to see how to apply Tabibito-san in this case (I just do not see how to convert ranges to ordinals here). Here's a MS SQL statement that will give non-overlapping segments and relevant following gap segments: PS. Added a version with LEAD() instead of OUTER APPLY for illustration purposes: with src as ( select * from (values (1, cast( '01/01/2015 8:00am' as datetime), cast( '01/01/2015 9:00am' as datetime)), (2, cast( '01/01/2015 8:15am' as datetime), cast( '01/01/2015 9:00am' as datetime)), (3, cast( '01/01/2015 8:30am' as datetime), cast( '01/01/2015 9:30am' as datetime)), (4, cast( '01/01/2015 8:30am' as datetime), cast( '01/01/2015 9:15am' as datetime)), (5, cast( '01/01/2015 9:15am' as datetime), cast( '01/01/2015 9:45am' as datetime)), (6, cast( '01/01/2015 10:00am' as datetime), cast( '01/01/2015 11:00am' as datetime)) ) as mytable( id, start_t, end_t) ), -- 0. ensure start_t is the key src_Clean0 as ( select min( id) id, start_t, max( end_t) end_t from src s1 group by start_t ), -- 1. exclude fully covered. src_Clean1 as ( select * from src_Clean0 s0 where not exists ( select * from src_Clean0 s1 where s0.start_t &gt; s1.start_t and s0.end_t between s1.start_t and s1.end_t ) ), -- 2. remove overlaps -- 3. add non-coverage src_Clean2 as ( select s1.id, s1.start_t, end_t = case when s3.start_t &lt;= s1.end_t then s3.start_t else s1.end_t end, gap_start_t = case when s3.start_t &lt;= s1.end_t then NULL else s1.end_t end, gap_end_t = case when s3.start_t &lt;= s1.end_t then NULL else s3.start_t end from src_Clean1 s1 outer apply ( select top 1 s2.* from src_Clean1 s2 where s2.start_t &gt; s1.start_t order by s2.start_t )s3 ), -- Using LEAD() -- 2. remove overlaps -- 3. add non-coverage src_Clean3 as ( select s1.id, s1.start_t, end_t = case when lead( s1.start_t) over (order by start_t) &lt;= s1.end_t then lead( s1.start_t) over (order by start_t) else s1.end_t end, gap_start_t = case when lead( s1.start_t) over (order by start_t) &lt;= s1.end_t then NULL else s1.end_t end, gap_end_t = case when lead( s1.start_t) over (order by start_t) &lt;= s1.end_t then NULL else lead( s1.start_t) over (order by start_t) end from src_Clean1 s1 ) -- select * from src_Clean2 select * from src_Clean3 
I use micro ORM's such as Dapper and Petapoco to reduce boilerplate code I have to write to get my data out of the DB and mapped to my DTO's. They're great for that. I write all my own T-SQL in the form of stored procedures and always have done. For my own way of working, I see no issue with ORM's. Its when you lean on them as a crutch to save you having to actually learn anything about the database which is bad. 
Well my client is a survey agency that takes in mass amounts of info on people. She feels Excel isn't an easy way to filter through to pinpoint an exact target audience she needs to survey. And thank you for the reply :)
&gt; She feels Excel isn't an easy way to... It's difficult to respond without knowing the exact problem she's having - but I feel she's wrong. What did you mean when you said that Excel is so limiting?
I guess what I'm looking for is another program for her so she filter through her data, but with a bit more user friendly. As you can tell this woman is technologically impaired. 
ORMs are fine for a ton of stuff. For the rest yep, write your own SQL. In any case it's best to know SQL well to properly use an ORM.
&gt; As you can tell this woman is technologically impaired. You are as well. To reply to specific posts on reddit you should be hitting the reply button. Additionally, you are not really giving us any information on what it actually is you are trying to do. If you have an excel spreadsheet that has column headers, you can highlight them, go to the data ribbon, then hit filter. If there is some other reason this won't work you need to provide details.
We're a fair complex data driven web app, and the ORM works well enough for just about all the use cases. Its only when we need to do those complex joins that we use SQLAlchemy, which is still an ORM. Writing raw SQL still seems like a bad idea.
Pivot Tables in Excel seem like the perfect tool for this. Without knowing why Excel is the wrong tool, I'd say Access might be helpful so you can handle the backend and she can have buttons that are easy to understand.
My apologies I am new to the reddit society, and also I agree that I am impaired when it comes to Excel. Though I was just wondering if there was a simpler program for her to cycle through contacts to pick certain demographics of people.
Terrific! Though she was having the problem of importing the data from Surveymonkey to Excel and it wouldn't add to the pool of contacts she has. Instead it would just make it's own subcategory for that survey. How do you make a pool of contacts I can filter through?
&gt; recode the wheel. i lulz'd at this one. :)
Because codebases should be under source control; and databases need not be if you're using a declarative ORM... like SQLAlchemy's.
nevermind, I resolved it. the left outer join of tblinvoice needs to have the where statement included in it, otherwise it returns a null that needs to be compared against. SELECT tblcases.jobreferenceno, tblcases.clientcd, tblcasedetail_1.insurednm, tblcases.policytypecd, tblcasedetail_2.reservevl, tblcasedetail_1.sitevisitdt, tblcases.iadt, tblcases.prdt, tblcases.frdt, tblcasetextds.remarktx2, tblcases.closedt, tblcases.casestatuscd, tblcases.caseopendt, tblcasedetail_2.anticipatedfeevl, tblcasetextds.jobreferenceno AS Expr1, tblcases.recoveryfl, d.companynm AS 'client3', d.countrynm AS 'brokercountrynm', tblcasedetail_2.jobreferenceno AS Expr2, tblsumtimespent.totalminutes AS 'c_totalminutes', COUNT(tblinvoice.billno) AS 'c_noofbills', tblcases.allocatedtostaffid, tblcases.excldsvcstd, tblsumtotalexpenses.sumsumtotalexpenses AS 'c_totalexpenses', tblcountpdffile.numberofcases, tblcases.motor_lodcount, tblcases.motor_lodsettled, tblcases.donotcountfl FROM tblcases INNER JOIN tblcasedetail_2 ON tblcases.jobreferenceno = tblcasedetail_2.jobreferenceno INNER JOIN tblcasedetail_1 ON tblcases.jobreferenceno = tblcasedetail_1.jobreferenceno INNER JOIN tblcasetextds ON tblcases.jobreferenceno = tblcasetextds.jobreferenceno LEFT OUTER JOIN tblcountpdffile ON tblcases.jobreferenceno = tblcountpdffile.jobreferenceno LEFT OUTER JOIN tblinvoice ON tblcases.jobreferenceno = tblinvoice.jobreferenceno and (tblinvoice.invoicestatuscd IN ('ICLOSE', 'POST', 'PRINT')) LEFT OUTER JOIN tblsumtimespent ON tblcases.jobreferenceno = tblsumtimespent.jobreferenceno LEFT OUTER JOIN tblsumtotalexpenses ON tblcases.jobreferenceno = tblsumtotalexpenses.jobreferenceno LEFT OUTER JOIN tblclient AS d ON tblcasedetail_1.brokeragentcd = d.companycd WHERE (tblcases.allocatedtostaffid = 'AWK') AND (tblcases.casestatuscd IN ('CA', 'IA', 'PR', 'FR')) GROUP BY tblcases.jobreferenceno, tblcases.clientcd, tblcasedetail_1.insurednm, tblcases.policytypecd, tblcasedetail_2.reservevl, tblcasedetail_1.sitevisitdt, tblcases.iadt, tblcases.prdt, tblcases.frdt, tblcasetextds.remarktx2, tblcases.closedt, tblcases.casestatuscd, tblcases.caseopendt, tblcasedetail_2.anticipatedfeevl, tblcasetextds.jobreferenceno, tblcases.recoveryfl, d.companynm, d.countrynm, tblcasedetail_2.jobreferenceno, tblsumtimespent.totalminutes, tblcases.allocatedtostaffid, tblcases.excldsvcstd, tblsumtotalexpenses.sumsumtotalexpenses, tblcountpdffile.numberofcases, tblcases.motor_lodcount, tblcases.motor_lodsettled, tblcases.donotcountfl ORDER BY tblcases.jobreferenceno
What prevents you from putting your Stored Procedures in SC?
shitty source control
Its my pleasure to receive your suggestions and comments.
Stores procedures are not entirely useful. Having the SQL in my code facilitates deployments and version control and with parameters an ad-hoc query is just as fast and safe as an SP.
Nothing, it's just more complexity when you are deploying plus you are not getting much in terms of advantages by using stored procedures.
It is more of hassle, I'll give you that. I always cringe a bit when I realize it's time to write a stored procedure. But I do like the abstraction you can get from them. I often put the mission critical stuff in the database as stored procedures rather than in the code. That way if a developer with less SQL experience starts working on the application, I'm comfortable that they won't break anything vital. Funny that I'm saying this. When I started working more seriously with SQL I fought SPs pretty hard. Now here I am advocating them. Life is strange that way I suppose...
Here you go solution in Oracle, but easily transferred to MS SQL or whatever: with qry as ( select 1 as ID, to_date( '01/01/2015 08:00am', 'dd/mm/yyyy hh12:mipm') as start_date, to_date( '01/01/2015 09:00am' , 'dd/mm/yyyy hh12:mipm') as end_date from dual union all select 3 as ID, to_date( '01/01/2015 08:30am', 'dd/mm/yyyy hh12:mipm') as start_date, to_date( '01/01/2015 09:30am' , 'dd/mm/yyyy hh12:mipm') as end_date from dual union all select 6 as ID, to_date( '01/01/2015 10:00am', 'dd/mm/yyyy hh12:mipm') as start_date, to_date( '01/01/2015 10:30am' , 'dd/mm/yyyy hh12:mipm') as end_date from dual ), grp_starts as ( -- Assign 1 to each row that starts a group and 0 to every other row. select start_date, end_date, case when start_date &gt; max(end_date) over (order by start_date, end_date rows between unbounded preceding and 1 preceding) then 1 else 0 end grp_start from qry ), grps as ( -- Assign a group to each row using a running sum of the 1s and 0s. select start_date, end_date, sum(grp_start) over (order by start_date, end_date) grp from grp_starts ), merged as ( -- Merge overlapping groups select grp, min(start_date) start_date, max(end_date) end_date from grps group by grp) select 24 * active_days as active_hours, 24 * (total_days - active_days) as inactive_hours, 24 * total_days as total_hours from ( select sum(end_date - start_date) active_days, max(end_date) - min(start_date) total_days from merged) If your time sample exceeds the max end date, add that difference to TOTAL_DAYS
Put Query 1 in Query 2 where you need the results as a subquery.
if Query 1 (Q1) looks like this: select id from &lt;table&gt; where x = y; and Query 2 (Q2) looks like this: select name from people where id = "result from Q1" you can sum the whole thing up like this: SELECT name FROM people WHERE id = ( SELECT id from &lt;table&gt; WHERE x = y); There are obviously other way's to pull this off, for example you could inner join the 2 tables to get the same results. But if you're new at this I figure this would be the simplest approach.
no, you cannot inner join the tables to get these results inner join expands over a two dimensional cartesian space you don't use that to reduce a space you already have
Unless you plan to limit the &lt;table&gt; subquery to returning one row.. You'd need to use WHERE id IN ( ... ) rather than WHERE id = ( ... )
you're going out of your way to display that something is possible which is a bad idea, in a thread where someone's giving that as advice directly to a novice.
Correct, completely forgot to mention that.
It is effectively the same amount of work yes. Without knowing more about what you're trying to accomplish it's hard for us to point you in the right direction. The above is a super simplified demonstration of subquery usage. Please note what /u/pease_pudding said about needing to use IN instead of = if Q1 is returning more than one row.
MySQL is the worst possible choice for hierarchical data as it doesn't do Recursive CTE's, unlike every other database
Using join to perform a filtering is a bad idea. . &gt; If a novice learns to solve every problem using subqueries You'll notice I actually explicitly told him that subqueries are not the way to do this, and taught him the right way instead. . &gt; You seem to think JOIN is a bad idea Using an expansion to filter is a bad idea, yes. . &gt; but have yet to offer anything other than hunch and very cryptic musings Either that or [you just didn't look](https://www.reddit.com/r/SQL/comments/3cstm6/help/csyoso2). Also, there's nothing cryptic about "expands a cartesian space;" that's SQL 101. Please stop downvoting for disagreement when you don't even know what I've said.
This tutorial will explain the use of check constraints. It will also explain how to define check constraint at column level as well as table level with Create Table Clause.
The join criteria is not filtering. That why theres a where clause. &gt; Please stop downvoting for disagreement when you don't even know what I've said. I played no part in your downvotes
Well, you are still pushing sort of a nonsense approach. Joining means filtering one table, filtering the other, generating a temp table from those, then returning that. Putting them in a where clause means a single walk of an existing table, and no temp table. Please consider filling a table with a large volume of random data and `explain`ing both strategies. It turns out there is an ***enormous*** difference. I get that you're convinced, but this is SQL 101, and easily measured.
For reference/FWIW, a lot of items are quite delicate, and since I don't need most of the things right now, it'll be most efficient for me to pack it all up in preparation for a house move that's happening down the track. I'll be bubble-wrapping quite a lot of stuff so finding things will not be trivial, and an inventory map will let me know for certain that item X is in box Y and making the effort to pull box Y apart *will* find me that item - I won't need to open random boxes and waste effort. A relational database will not solve my problem; it provides a fundamental springboard which I will indeed be building upon, but not nearly the full architectural depth I'll require to implement what I want.
Oh, wow. TIL, thanks for that info. I admittedly wanted to use SQLite because - as its authors state - it's zero-configuration and just works (initialization requires a single line of PHP code, vs. a whole server setup and so forth for MySQL). I had no idea my choice of SQLite was the reason I discovered recursive CTEs :P wow.
I'd first try tuning the MERGE query - Run an explain plan to see if there are any indexes you can create to improve the performance of the match condition(s). Properly tuned, it should give better performance than reading the data into a PL/SQL varray and manually processing it. If tuning the MERGE query doesn't do the trick, try using an explicit cursor so that you can create a varray using the cursor rowtype, and then bulk collect the data from the cursor into the varray (code is untested): DECLARE CURSOR c1 IS SELECT plx_val.item_mstr_skey AS item_mstr_skey, plx_val.mbr_skey AS mbr_skey, plx_val.peer_grp_skey AS peer_grp_skey, plx_val.spend_amt AS plx_spend_amt, plx_val.hco_qty AS plx_hco_qty, plx_val.bnchmrk_qty AS plx_bnchmrk_qty, bm_val.mbr_lpp_bnchmrk_price_amt AS bm_mbr_lpp_bnchmrk_price_amt, bm_val.last_bnchmrk_dt AS bm_last_bnchmrk_dt, pi.price_index AS pi_price_index, plx_val.bnchmrk_ind AS plx_bnchmrk_ind FROM plx_val INNER JOIN bm_val ON plx_val.item_mstr_skey = bm_val.item_mstr_skey AND plx_val.mbr_skey = bm_val.mbr_skey AND plx_val.peer_grp_skey = bm_val.peer_grp_skey INNER JOIN pi ON plx_val.item_mstr_skey = pi.item_mstr_skey AND plx_val.mbr_skey = pi.mbr_skey AND plx_val.peer_grp_skey = pi.peer_grp_skey; TYPE c1_array IS TABLE OF c1%rowtype; v_array c1_array; BEGIN OPEN c1; FETCH c1 BULK COLLECT INTO v_array; CLOSE c1; -- Process the data END; / BULK COLLECT will give better performance, but be aware that it will consume more memory. An alternative to this would be to just loop over an implicit cursor (less memory used, but worse performance): BEGIN FOR rec IN (SELECT plx_val.item_mstr_skey AS item_mstr_skey, plx_val.mbr_skey AS mbr_skey, plx_val.peer_grp_skey AS peer_grp_skey, plx_val.spend_amt AS plx_spend_amt, plx_val.hco_qty AS plx_hco_qty, plx_val.bnchmrk_qty AS plx_bnchmrk_qty, bm_val.mbr_lpp_bnchmrk_price_amt AS bm_mbr_lpp_bnchmrk_price_amt, bm_val.last_bnchmrk_dt AS bm_last_bnchmrk_dt, pi.price_index AS pi_price_index, plx_val.bnchmrk_ind AS plx_bnchmrk_ind FROM plx_val INNER JOIN bm_val ON plx_val.item_mstr_skey = bm_val.item_mstr_skey AND plx_val.mbr_skey = bm_val.mbr_skey AND plx_val.peer_grp_skey = bm_val.peer_grp_skey INNER JOIN pi ON plx_val.item_mstr_skey = pi.item_mstr_skey AND plx_val.mbr_skey = pi.mbr_skey AND plx_val.peer_grp_skey = pi.peer_grp_skey) LOOP -- Process the data DBMS_OUTPUT.put_line(rec.item_mstr_skey); END LOOP; END; / **Edit:** It's also worth noting that implicit cursors perform better than explicit cursors.
SQLite is great. It's derived from Postgres, so if you need a database *server* it's a not-so-hard upgrade. Postgres has R CTE too. 
Oh, okay! TIL x2 =P \**Files away for later use*\*
&gt; inner join expands over a two dimensional cartesian space Inner joins don't expand 'expand'. Why would they? Why would the query optimizer bother to care about grabbing records that are filtered out? Are you confusing inner join with cross join? I tried googling to find evidence to back up your stance, and was unable to find it. I also tried googling to try and find something that refuted your statement, but I realized it's so nonsensical no one would bother writing to refute it. Could you please link me where in SQL 101 that states inner joins exhibit this behavior? The best information I can find all pretty much say the below: &gt; Practically speaking, however, the answer usually comes down to performance. Some optimisers suck lemons when given a join vs a sub-query, and some suck lemons the other way, and this is optimiser-specific, DBMS-version-specific and query-specific. &gt;Historically, explicit joins usually win, hence the established wisdom that joins are better, but optimisers are getting better all the time, and so I prefer to write queries first in a logically coherent way, and then restructure if performance constraints warrant this. http://stackoverflow.com/questions/2577174/join-vs-sub-query
I'm assuming this is an array of records. If so, you need to use "bulk collect into"
Is your example accurate? It switches discontinued as well as date items? If so, you have probably selected right to left formatting on the field in the report
You type a whole lot without actually saying anything. This comment chain comes from the original recommendation on how to produce records based on filtering from 2 separate tables. Your solution of using 'and' in the where clause doesn't replicate the functionality of a join or subquery. You have proposed no alternative. You have given 0 attempt at backing up anything you say. You act like this is knowledge so common, yet is completely void of any supporting text. A query doesn't need to perform all 4 x 4 possible combinations to realize that 12 of those aren't viable, and can be filtered out before the join even begins. It doesn't need to fill the Cartesian space. If it was true, and it was always table rows * table2 rows, you could never use join with a table with a million rows , [but clearly you can](http://imgur.com/FIBEL2V). 99.99% of matches are never even considered. Cartesian join: &gt; The Cartesian product, also referred to as a cross-join, returns all the rows in all the tables listed in the query. Each row in the first table is paired with all the rows in the second table. This happens when there is no relationship defined between the two tables. Both the AUTHOR and STORE tables have ten rows. If we use a Cartesian join in these two tables, we will get back 100 rows. I have no idea why you think I'm yelling, especially when you've been the one who has been condescending to everyone else, without even attempting to back up anything you say. I would also like to leave you with this execution of 2 identical result queries, one using a subquery, one that is not: http://i.imgur.com/tL4h0rk.png. If you can explain how there is a performance difference, I would love to hear it. 
&gt; You type a whole lot without actually saying anything. This tone suggests that no matter how much detail I give you, you will choose not to understand it. . &gt; You have proposed no alternative. Two, actually. I even gave you a direct link to them. . &gt; Cartesian join: ... is not cartesian expansion. There are lots of things that use that word. . * Are you confusing inner join with cross join? * I tried googling to find evidence to back up your stance, and was unable to find it. * I also tried googling to try and find something that refuted your statement, but I realized it's so nonsensical no one would bother writing to refute it. * You type a whole lot without actually saying anything. * You have proposed no alternative. * You have given 0 attempt at backing up anything you say. * You act like this is knowledge so common ... * I have no idea why you think I'm yelling I bet. . &gt; A query doesn't need to perform all 4 x 4 possible combinations Nobody said it did. . &gt; It doesn't need to fill the Cartesian space. Nobody said it did. . &gt; If it was true, and it was always table rows * table2 rows Nobody said it was. . &gt; without even attempting to back up anything you say. I don't know why you keep saying this. I gave code examples. I told you how to benchmark this. . &gt; I would also like to leave you with this execution of 2 identical result queries Hooray. Two queries, namely the two strategies I said don't use, neither of which are one of the two I recommended, one of which you were previously holding above the other, and without the table definition, the cardinality, any signatures, or any metrics, you want me to explain the results of the query planner of an unknown version of an unknown SQL engine. Or if I find that boring and not worth my time since you didn't ask until after I already gave up on you because of your tone, you must surely be right, because if you assert something and nobody finds it interesting enough to refute it, then after complaining at great length how other people who gave more context than this gave no alternatives, you think this constitutes a proof. Michael jackson popcorn dot png. . At this time I see no further purpose in continuation. If you'd like a conversation beyond here, try being pleasant. I'm not interested in being told that I have to be first, because I don't actually want a conversation with you.
Okay.
Pivot and unpivot. I've never had a real world use for them, but I could see them being useful at times.
Ooh, I came across those once, when I was trying to do something that they would have been perfect for - my version didn't support them, but they do indeed sound awesome for doing dynamic columns!
Should have clarified further, we have already been through several stages of tuning and have used various optimizer hints as well as specific indexes via the explain plan. Originally it ran in 200s, which we got down to 100s, but that is obviously still not acceptable in a web app. We have also discussed using the bulk collect into, but we may run into memory issues there, so we decided not to pursue that route. Thanks for the notes though.
Depending on your database, there are often also operators that concat strings first + ' ' + last or first || ' ' || last
Surround the alias with single quotes or brackets 
Basing on comments: You may want to pull the results into a MySQL or MS-SQL database (can both run on a local or server). Use ODBC and a startup macro to refresh the sheet each time it is opened as well as a refresh button on the sheet to kick off the macro to refresh Solves 2 problems; too many CSV's and inability to pull the data together from different sheets (CSV's). Maybe one step further; a script that runs on a 15 min schedule to check a folder, if that CSV exists, pull it into the DB and archive/delete it. Now SHE can get the csv and save it to that folder and things will happen automagically
My OCD has to ... SELECT max(time_check_out) ,owner_id FROM daycare_visits GROUP BY owner_id HAVING max(time_check_out) &lt; date_sub(now(), interval 1 month)
you might want to use CONCAT_WS instead of CONCAT CONCAT yields NULL if any of the strings being concatenated are NULL, CONCAT_WS will ignore the NULL strings so you want `CONCAT_WS(' ',firstname,lastname) AS CustomerName`
Even though OP said add a space in the alias, that's not what he/she actually meant. So, technically you're correct, but that wasn't what OP was looking for. 
Python is your hammer, the problem that needs a programming solution is your nail. 
NEW TUTORIAL
Hard to tell. A web developer would need to be familiar with at least a framework or two and an ORM as well as basic select, insert, delete, update queries. A data scientist will need to be familiar with pandas, machine learning packages to scrub and present data. He will need enough SQL to learn some pretty advanced select queries for data wrangling purposes. A system administrator may need to know SQL to admin the databases and python for systems automation (perhaps using tools like ansible) or working with the AWS api.
&gt; Putting them in a where clause means a single walk of an existing table.. You are assuming that OP has control of the database, and can restructure the data into a single common table. There is no reason to believe this is the case. Most likely, the data is located in more than one table, in which case there are only two ways to obtain that data: a JOIN and a SUBQUERY, in which case the former would be more performant. PS - People need to stop downvoting things they don't agree with. I had to expand this conversation to see what was going on and that's annoying as fuck. I know it's a couple days old, but c'mon :p ---- 
Try /r/learnsql. As per our sidebar we do not allow basic SQL tutorials here. The readers are a more advanced audience.
If you're just starting out with sql I would stay away from ORMs. Just write your own select, insert, update, and delete statements. An ORM helps immensely if you're doing bigger things with lots of different kinds of objects but they add an unnecessary layer of complexity that you don't want to deal with when just starting out. 
I'm fairly new at this and wasn't familiar with GUIDs at all so thank you for the patience. I'm largely self taught so I struggle with how I need to say what I mean/need so it makes sense. &gt;What, exactly, are you being asked to validate? I have to prove that the data in the legacy system matches the extracted data in the target system. This is data on legal hold; I'm in pharma so mostly drug trials and manufacturing. SDM exports each table to a flat file and then creates a handful of .xml files to create a schema that maintains the structure of the legacy system.
Then you want something like Red Gate's SQL Compare, Total SQL Compare from Idera, or the comparison features in SQL Server Development Tools. Assuming you're using MS SQL Server. These will compare both the schema and data. You can't just diff the flat files because ordering may be completely different, depending upon how you generate them. And you can't (easily) diff XML with flat-file tools, you need an XML-aware diff tool that can handle interpreting the schema too.
That's 32bit SQL on 64 bit Windows
&gt; (No column name) Microsoft SQL Server 2012 - 11.0.5058.0 (Intel X86) That's 32bit. x86 is the 32bit instruction set, a legacy term named after the original processor to run that instruction set, the 8086. The OS is x64, which is 64 bit.
Even though when i look at the root of the install disk there's a setup.exe - but i also see a x86 directory...?
Has the performance of your queries changed since changing the MAX DOP? Can you measure durations before and after the max dop change using profiler? cxpacketwait &amp; max dop change is a common reaction, high cxpacketwaits don't always mean you have a problem with parallelism, so you might want to adjust that setting back up. Tracewrite would be due to profiler, oledb could be linked servers. Have a look here: http://www.sqlskills.com/blogs/paul/wait-statistics-or-please-tell-me-where-it-hurts/ I think you want to do some work identifying your long running queries in addition to looking at the wait stats
gotcha - thank you
They are practical projects, if you could complete them, I'd say you were proficient with Python and SQL.
Yeah, this is where I ran it from. I've been living in those queries and the SQL Blitz queries. &amp;nbsp; IO Latency - There, but not too bad. Since business hours today: (Physical Disk latency on the DB Drive) Read Requests - min=0 | max=8.12ms | avg=1.60ms Write Requests - min=0 | max=47.57ms | avg=5.12ms &amp;nbsp; Bad Indexes - Assuming so. Just ran this: http://i.imgur.com/l2ITFSn.png Fragmented Indexes - How can I check? Maintenance Task that runs every nights does: Rebuild Index, Update Statistics, Create Missing INdexes, Create Missing Statistics. &amp;nbsp; Locking - So far today: Deadlocks - min=0 | max=0 per sec | avg=0 per sec Lock Timeouts - min=0 | max=8.48 per sec | avg=0.38 per sec LockWaitTime - min=0 | max=169.61ms | avg=5.66ms LatchWaitTime - min=0 | max=966.98ms | avg=23.87ms &amp;nbsp; All the issues occurred after the vendor's Service Pack update. And of course they're saying "Not our fault, has to be something you've chagned"
You beat me to it. How many f-ing times do we have to ask this on this subreddit??!!!!!
Well, I suppose it would help if I included that. Forgive my RDBMS narcissism as I am using Management Studio. 
I like this approach. How does it handle extra data like IDs that we do not have in our DB as well as the weights associated with them?
So the creating indexes every night was a Maintenance Task created by the vendor, which they recommend doing every night. When I had them in there they re-enabled it. Think I should disable it? I know it's crazy to run those that often.
You're probably reffering to SQL Server Management Studio. That is the client application, so the RDBMS is MS SQL Server. In that case, SSMS has a built-in wizard "Import Data" that also takes in Excel files. Right-click on the DB instance and choose Import Data. Then go through the wizard. This is the easiest way to do it. Of course it can also be done via script, but that requires some prerequisites to be met.
We're slowly moving away from the system that requires this code, so I'm trying not to rock the boat too much by changing functionality at this stage. Another dev had already encountered this and had a workaround SP I found out, but I'm mainly curious on what exactly is triggering the optimizer to do what it's doing... The error still occurs when I move the join to the IN statement. Interestingly enough, changing the NOT IN to be a LEFT JOIN, where t.FieldValue IS NULL the error doesn't manifest. Somehow SQL is tricking itself into either removing the PATINDEX clause until after it tries to cast it in the table-valued function, or it's moving the hdr.Account &lt; 9000 to be along side PATINDEX.
What is the aggregation doing?
I would use DBMS_PARALLEL and chunk up the data and process each chunk in parallel using your 12 CPUs. This could sub-aggregate to a table and you could do further aggregation on that. [Here's an example of it](https://asktom.oracle.com/pls/asktom/f?p=100:11:0::::P11_QUESTION_ID:4248554900346593542) 
Even if you simply run ?.... select * from LINKEDSERVER.SOURCE.dbo.LiveAccounts what about using a CTE? with qry as ( select * from LINKEDSERVER.SOURCE.dbo.LiveAccounts ) select * from LINKEDSERVER.SOURCE.dbo.hdr hdr join qry la ON la.Account= hdr.Account ... 
What do you get when you run: SELECT I.ID, I.Weight AS OldWeight, S.TEMPWEIGHT AS NewWeight FROM Item I INNER JOIN Sheet1$ S ON I.ID=S.TEMPID Anything? 
As it stands the table holds checks against data. 20150712, CheckA, ClassX, Item1, OK 20150712, CheckA, ClassX, Item2, OK 20150712, CheckA, ClassX, Item3, NOT OK 20150712, CheckA, ClassY, Item1, OK 20150712, CheckA, ClassY, Item2, OK Would aggregate to 20150712, CheckA, ClassX, 2 Items OK 20150712, CheckA, ClassX, Item3, NOT OK 20150712, CheckA, ClassY, 2 Items OK I have 5 years of data, 20 checks, 10,000 Classes, and up to 1,000 items per class. It soon adds up. The 'Not OK' data needs to be kept in it's full glory, which is simple enough. The aggregation groups by date, check and class to write out a single line where there may have been thousands.
Thanks. I've tried (unsuccessfully) some DBMS_Parallel executions before, but they always locked up the database. The page linked does offer some sensible advice I've yet to find, so I'll give it a detailed look though. 
Actually, I remember now. You should be able to prevent predicate pushdown by using a dummy cross join with the LiveAccounts bit, this makes the optimizer materialize the result of LiveAccounts first. Try this : select * from LINKEDSERVER.SOURCE.dbo.hdr hdr inner join ( select la.*, dummy.nul from LINKEDSERVER.SOURCE.dbo.LiveAccounts la cross join (select '' as nul) as dummy ) on la.Account = hdr.Account where hdr.Account not in ( select t.FieldValue from #temp1 t where t.FieldName = 'Account Code' ) and hdr.Account &lt; 9000 
I am going to take a shot at this but be ready for it to be way off the mark since you post is light on the specifics. If possible what I would do is build out a simple backend API and to put the data into a Staging database on your server, then I would build a simple cross-platform app with [Iconic](http://ionicframework.com/) so you don't have to work to hard to get the App piece running. Once the Data is in the staging DB and corrected by the user via the app you can move the data to "Production". You sound like a backend guy so that why I went for the "heavy" backend and light frontend with Ionic. It also sounds like you are comfortable with SQL so this solution puts most of the work there and the API and "App" are basically just shells to interact with your tables. Once again this could be way off base but this is were my brain went based on your OP.
Hi, I think I'll be able to have a script that takes a backup every hour, I'm just not sure how to push that backup to another server, do you have a link that I can start looking into? Another redditor gave me this for backup: https://support.microsoft.com/en-us/kb/2019698#/en-us/kb/2019698 I can't find how to do the next part. 
Another redditor mentioned that after backing up using the method from the link you gave, that we should be pushing what we backed up to another server, do you have anything regarding that part?
I don't know enough about baseball to understand your other metrics like BB, but you don't really have a straight forward table for determining a win versus a loss so you're going to need to solve this using something like a case statement. Forgive me, I'm not at my database right now so I can't test simple things but here's an example: select a.team, case when a.R &gt; b.R then 'WIN' when b.R &gt; a.R then 'LOSS' when a.R = b.R then 'TIE' else 'ERROR' end as Results from table1 a inner join table1 b on b.matchup = a.matchup and b.opponent = a.team Play around in that direction?
This may be of no help but have you tried using [Count(*) over partition by](http://www.orafaq.com/node/55)? I have a table with about 500M rows and 300 columns and this works wonders. Although, I am not aggregating the whole table. Good luck with this. 
This is pretty much a solid answer. Have just done some research but this ionic seems really easy to use and like it works well on android tablets. Also it looks quick in putting together projects which is something i need at the moment. Not sure about the logistics of having this staging area but I can have a look into that and it doesn't mention much about storing data on the site but I'm sure I'll be able to find some useful help online now I know where to start. 
Works perfectly! Thank you.
The sales team physically go get the data from the customer. So we build conservatories for peoples houses. Current approach 1) Sales team get enquiry from Joe Bloggs 2) Go to house talk about conservatory take rough measurements in notepad and give a rough figure for cost. At this point we take customer details all written into a pad and this is handed to office for marketing purposes and transferred to database. 3) if customer wants to go ahead we go back to there's and do a final measure again all data input in pad 4) Data passed back to office and input into specific formulas to come up with exact prices and Costs 5) Quote generated from this and given to customer 6) If order placed then quote added to folder and production commences on conservatory My current project has been going from data given to office to final quote and then production orders. Make it a bit easier to generate quote, analyse data and track where orders are up to. Saves admin time as they don't need to input data 5 times like they were doing. The only thing I want now is to go directly from sales rep to office system. It means the data is nearly live and means the sales reps don't have to physically send the data in. Less chance of errors through data input. Don't need people in office to input data that the sales rep does anyway. 
I love a challenge. First questions is how are the rows actually organised and are you hitting the index or the actual rows with your aggregation? 
Unless you're doing something really "out there", upgrading the compatibility mode to anything less than SQL Server 2014 *shouldn't* hit you with any weirdness. But that doesn't mean you shouldn't test it for yourself in a non-production environment. If you're running SQL Server 2014, setting the compatibility mode to that version will trigger the use of the new cardinality estimator which *will* impact how your queries execute. This may be for the better or worse; you'd have to test with your own workload. There have also been a few bugs found in the new CE under specific, unusual conditions. BTW, you tagged your post wrong. You're asking about MS SQL Server, not MySQL. Two **very** different products.
Hi, Thanks a lot for your explaination. Currently I don't have an index on that column. What will be the difference between using order by CASE WHEN COLUMN_1 IS NULL THEN 1 ELSE 0 END and order by Column_1 NULLS LAST 
&gt; order by CASE WHEN COLUMN_1 IS NULL THEN 1 ELSE 0 END Isn't this WHEN COLUMN_1 IS NULL THEN 2 ELSE 1? I've never used this syntax before, but I think order starts at 1. I could be wrong. Either way, it would be unlikely that one would be faster than the other -- I would _guess_ that the interpreter would act on these the same way. But I'd be interested in seeing this tested in real life - and whether the result set would be different. Incidentally, why would you bother with the CASE statement at all? You can just use `order by column_1`, which acts like `ASC NULLS LAST` by default. 
I agree that the schema is not ideal. Unfortunately, this is how the information is presented from ESPN's league site, so without an automated way to restructure it, it's the fastest way to store it. :(
That sucks :) See my edit.
Simply beautiful, elegant, and works perfectly. Thank you so much! Have some gold.
Oh dang, thanks.
Maybe you could try to force the query planner's hand a bit with collections. For example make an associative array with keys OK and NOTOK, and values which are nested tables or varrays of the items. So a row might look like 20150815,CheckA,ClassB,{"OK": [item1,item2],"NOTOK":[item3]}. Then in a second pass, convert this to a string, or however you present it. Another thing to look at might be temporary tables, which can be done in parallel, if the data is chunk-able somehow. Other than that, if possible try to have up to date table statistics available, and use the query planner explains to see what the costly parts are. The work will be much faster if you can develop with a subset, say a 10% random sample of rows.
What about this cross join approach I mentioned?
I would suggest using [CUBE / ROLLUP / GROUPING SETS](https://technet.microsoft.com/en-us/library/bb522495\(v=sql.105\).aspx). These allow sub totals and totals for any grouping you desire.
Neither cross join or CTE implementation is preventing it. As well, implementing an IN Subquery still gives the cast error. WHERE ... AND hdr.Account IN (Select Account from LINKEDSERVER.SOURCE.dbo.LiveAccounts) I'm still surprised all of these alternate methods don't work, but pasting the LiveAccounts sql directly in the query prevents the pushdown. 
Where 1? what 
 SELECT count(*) , Book , DATE_FORMAT(creation_date, '%Y-%m-%d') as date , publisher , sum(if(error=0,1,0)) as A , sum(if(error&amp;2!=0,1,0)) as B , sum(if(error&amp;4!=0,1,0)) as C , sum(if(error&amp;8!=0,1,0)) as D , sum(if(error&amp;32!=0,1,0)) as E FROM Sale A LEFT JOIN Book B ON Sale.book(id) EQUAL Book.id; LEFT JOIN publisher C ON B.publisher EQUAL C.id; WHERE 1 GROUP BY publisher , DATE_FORMAT(creation_date, '%Y-%m-%d') , publisher Cleaned up the SQL so everyone can read it a little easier. There are a handful of different issues here and some I believe are due to quick edits as you sanitized your query for posting online. Here are my notes (going from top of query to bottom): * Book (second field in SELECT clause) does not seem to be defined well, I am going to assume you meant to select Book.id * I am not incredibly familiar with MySQL field naming restrictions but I don't think Sale.book(id) is a valid field name. I am going to assume you meant to say Sale.book_id * should remove both ;'s since they signal the end of a query, that will break your query * your WHERE clause doesn't seem completed (as /u/BlackOdder mentioned). What should be equal to 1? Remove the entire clause if it is not needed. I am going to simply set it to 1=1 so it returns all rows and you can replace values with whatever logic you like. * you should be grouping by all non-aggregate fields in the SELECT. you are currently grouping by publisher twice. remove the second "publisher" and replace it with "book" In the end, the new query should look more like this: SELECT count(*) , Book.id , DATE_FORMAT(creation_date, '%Y-%m-%d') as date , publisher , sum(if(error=0,1,0)) as A , sum(if(error&amp;2!=0,1,0)) as B , sum(if(error&amp;4!=0,1,0)) as C , sum(if(error&amp;8!=0,1,0)) as D , sum(if(error&amp;32!=0,1,0)) as E FROM Sale A LEFT JOIN Book B ON Sale.book_id = Book.id LEFT JOIN publisher C ON B.publisher = C.id WHERE 1=1 GROUP BY Book.id , DATE_FORMAT(creation_date, '%Y-%m-%d') , publisher
You have a lot of issues with your syntax. I have modified your query so it is easier to read (see below). If you look at your query, you will find that you have a few semicolons on your joins, which causes the query to end at that point. You will also notice in your GROUP BY statement you have put 'publisher' in there twice. And when you do your LEFT JOIN, you can use the following syntax: SELECT * FROM Sale A LEFT JOIN Book B ON A.book_id = B.id Formatted Query select count(*) as Count , Book , DATE_FORMAT(creation_date, '%Y-%m-%d') as date , publisher , sum(if(error=0,1,0)) as A , sum(if(error&amp;2!=0,1,0)) as B , sum(if(error&amp;4!=0,1,0)) as C , sum(if(error&amp;8!=0,1,0)) as D , sum(if(error&amp;32!=0,1,0)) as E FROM Sale A LEFT JOIN Book B ON Sale.book(id) EQUAL Book.id; LEFT JOIN publisher C ON B.publisher EQUAL C.id; WHERE 1 GROUP BY publisher , DATE_FORMAT(creation_date, '%Y-%m-%d') , publisher If you want to review some SQL basics, this is a good course: http://www.learntosql.com/course/hands-on-sql-for-beginners-select-from-where/
The quick and dirty way: Use [ROW_NUMBER](https://msdn.microsoft.com/en-us/library/ms186734.aspx) and ORDER BY the date asc. Also select the COUNT(\*). Put all of that in a subselect and then use the WHERE clause to filter to where ROW_NUMBER=COUNT(\*)
Sill fail :P
That's the query I was hoping to do. The other comment mentioned scheduled tasks and that seems to be the right path.
If Express does what you need and Agent is the only thing you need Standard Edition for, save your money. Just set up a Windows Scheduled Task or Scheduled Job (PowerShell) to kick off your update.
If you haven't decided on an RDBMS, PostgreSQL 9.3 and Oracle 12c natively support JSON. So you can produce it directly from and save it to database tables.
Did you reboot after changing the nTablesMax value?
Thanks, I'll look it up. The aggregation itself is done at a day level, then an item level, consisting of around 3M rows per day. The script is tested and proven, I just need a faster throughput. I only hit redo log issues if I try to broaden the granularity to maybe monthly or weekly level. Doing it a bit at a time is fine, but the data isn't static. There are circumstances where folks need to re-run tests for historic data with their updated values, or more likely someone feeds in the wrong date, and then the 'completed' dataset is now fecked. 
Perfect! This is exactly what I was looking for. Trying to learn data mining because at my company there is a huge need 
I can't leave a script running in Production that hits performance, especially over a 10-day period. The present plan is to copy it to UAT - aggregate most of it there, then take it back to production, and finish the job. But I still was kind of hoping for a magic button that focuses 100% of resources at the issue. What is the fastest write-out method? If I dumped all this data to a CSV or a DMP would it suddenly turbocharge?
To be honest, a single prod deployment is untenable. I'm building what I can in UAT, using an untouched prod copy, and finishing it off during the deployment green-zone. However, time is not on my side. Today I had to requisition $200K of extra storage, because the growth of the system is out of control. The powers-that-be decree unreasonable timelines and magical powers. Limitations are: * I can't run aggregation scripts in Prod if they take longer than 8 hours. * A performance hit to live environments gets flagged within minutes. * I have 2 days per month where we have downtime. 
Ach.. Unfortunately all the data we need is sat in multiple mssql databases. I wonder if I have it all back to front, whether I should be using something else to access the database that can produce json files? What ever I use it has to be able to run autonomously
JSON support is coming to MS SQL Server 2016. As for now, [**something like this**](https://www.simple-talk.com/sql/t-sql-programming/producing-json-documents-from-sql-server-queries-via-tsql/) may help. Alternatively, write a web-service in .NET (which has JSON libraries) which works off your SQL Server back end.
I've done exactly that, split the quoting stage off from the production stage mainly for analysis purposes but also because we can quote for lots of things and the customer could only want to purchase half, so rather than overwriting the original data it just inserts it into the production tables. It's not so much the programming side of things I am stuck with as I have a good idea of what I want it to do and where data is stored, especially on the desktop side of things where the server is already connected and interacting with the database. It's the whole mobile connectivity and reps can add new data and I've got some good ideas for reports I'm going to set up so the reps can see exactly how their customer base is doing, hopefully meaning we'll target customers a bit smarter. Anyway, off on a tangent. We're currently running an MSSQL server, I know we have a VPN client but I personally don't know how this would work with the tablets as I'm more a software dev than a database admin. I'll have a look into ASP.NET though it keeps coming up in all my research. I had a good look into Ionic and I believe that would be really good for developing a decent app, the reviews I've read so far have been glowing as well and if this goes well I do have some decent ideas for apps I want to work on as side projects, however I really struggled to install it, not sure if my firewall is blocking access or I'm not using the command line correct on node.js. Do you know why I might be experiencing this problem. Thanks for your help so far as well, really appreciated. 
I'm a bit confused by this. it sounds like your saying we can aggregate the correct data and therefore destroy some of the details of that data, but at the same time, the data can be changed. Is the aggregate data still viable for these re-runs? Can you post the aggregation script, maybe there are some things inside that can be tweaked. For instance is it being interpreted on the fly, or is it compiled into the database as a stored procedure. If it was compiled, was the optimizer turned on, is it interpreted on the fly, or is it compiled natively for your platform. How is your data represented in the database. Are you using varchars to represent every class, or are we using numbers.
I also use an ERP system, it has multiple modules for different items like Customers, Ecommerce, Inventory, Accounting, Etc. And the main tables like customers, suppliers, accounts are all created on their own table. Having separate tables seems cleaner to me so that if you want to build reports/views later it would be easier. What software did you go with?
Thanks for the share, just starting to brush up and it looks like there are some great links there.
Thank you I was missing the ON portion of the statement.
rebuild then repoint production to your new environment?
can you do some of the processing on another copy/server? either restore the data from a snapshot/backup, then process everything in that snapshot on the copy, and then copy the data into PROD and only update the data based on data newer than the snapshot... or perhaps just run the query against prod, in small chunks (avoiding the "you're monopolizing the server" thresholds)... store it off in a temp table/etc... then in the actual rollout, use the pre-built aggregates.
This. Also, don't build the DW based on expectation, build it based on need... use best practices along the way, but don't expect that "if you build it, they will come"... this approach will also make sure that you're focused on answering questions.
If i understood your question correctly,this should work..get the best agent for each unique zip-code from zipcoderadius and sa.agent_list, then use zip-code from callz_details_2015 to get the best agent.
Yuck, so I've done something kinda like this, back in the day for a previous past life. The biggest challenge seems to be the "Find which agents are within X distance from Y zipcode". Essentially, because of the shape of zipcodes, this can either be done from some assumed center-point of the given zipcode, or the lat/long boundaries of the zipcode must be a known quantity (which isn't easy because they're rarely squares/rectangles). For mine I assumed center-of-the-zipcode, assigned one pair of Latitude/Longitude and then set to Google to code-monkey some distance functions. Caveat emptor, not all of the zipcode distance functions worked the same, or returned the same results.
I got an error: "Incorrect syntax near 'range' when I replaced the subquery with it. Also made everything that had "T.something" into "could not be bound".
I think you downvoted /u/alinroc instead of answering the question, but it is a valid one. Why is a single person using multiple nicknames? Is there a reason for this as well that we are just unaware of or is that just how you did it? And to solve your issue, you could put a timestamp when you change the persons nickname and select the max timestamp
SQL Server 2008 R2, statement is the same as the post except replaced (select sum(T.QUANTITY*T.UNIT_COST) from dbo.FI_REQ_PO_ITEMS where T.PO_NO &lt;= T.PO_NO and T.PRODUCT_ID = T.PRODUCT_ID) 'Running Total' with "Running Total" = sum(t.quantity*t.unit_cost) over( order by t.po_no range unbounded preceding)
Not sure if i understand you but try something like CONVERT(DATE, left(string, 8))
yes, to no affect
Solved by using CONVERT(DATETIME, LEFT(SHIFT, 8))
Hey, we are [Joes2Pros](http://www.joes2pros.com) and while we may not be free, we offer some pretty great SQL Server Training. You can get your first month for just $1 using code: REDDIT1. 
After doing some research ASP.NET seems good and since I've done a little web development anyway should be pretty straight forward to progress with and goes hand in hand with my SQL Database and existing program. Just a couple of questions I can't seem to find answers for on the internet. Does ASP.NET work when the network is offline, like say for example the user is in a blackspot and can't connect to the Server? Maybe you can recommend what I'd do in this situation, possibly have the web app store it in a SQLite database on the tablet and retrieve it later to upload into the MSSQL database when they have network access? Furthermore, similar to this will I need to install the app on the phone as well as this SQLite database so its features can be accessed at any time as I'm guessing without a network connection if the app is hosted on the server the tablet wont be able to authenticate themselves or even use the system. Sorry if I seem a bit clueless, I'm literally approaching this side of things blind. Thank again.
I've had good luck with Tinder, but in all honesty well done on the SQL, glad you figured it out.
 select * , Streak = RowNumber() OVER ( PARTITION BY Continues, ORDER BY WeekNum ) from ( select WeekNum , Continues = CASE WHEN LastWeek.WeekNum IS NULL THEN 0 ELSE 1 END FROM source LEFT JOIN source lastWeek ON lastWeek.WeekNum = source.WeekNum - 1 ) inner then just filter on Streak
After trying your query I get this error: &gt;SQL Error (1451): Cannot delete or update a parent row: a foreign &gt;key constraint fails (`realestate`.`property`, CONSTRAINT &gt;`property_ibfk_6` FOREIGN KEY (`propertyPropertyTypeID`) &gt;REFERENCES `propertytype` (`propertyTypeID`))
That's not a problem with the query syntax I gave you, that error relates to a referential integrity constraint in your DB which has been set to delete restrict. It's basically saying "I can't delete a property type because there is a property record referring to it". Maybe you really just want to delete properties? If so, replace DELETE p, pt ... with DELETE p ...
You do not have a unique key to the players table. So, your title of: &gt; I have a list of unique player IDs, but variable player names. I need help with a query to only get the first player name per ID. (self.SQL) is not accurate in your current design. You have a player ID, that ID is not unique. If you want it to be unique, I don't really care if you allow players to change their name or not, either you update the name associated with the player ID, or you create a whole new player ID for that person under their new name. Having two records for the "same" player defeats the purpose of having a player table.
If your query already determines those added, you can "create" a column with a constant value. select s.person_pk, s.vendor_id, s.company, s.first_name, s.last_name, s.suspended_date, 1 as added ....
so when guest brings online ticket... is that ticket scanned? is it converted to a "pass"? How do you know whether such an online ticket is valid? can I print my own barcode and show it to "Joe at the front gate" who thinks my printout is legit? is the ticket invalidated for reuse, day after day after day after day? *somehow*, the online ticket should generate the day's "pass"... whether it tracks that association internally, or applies an anonymous voting type of system ("ticket was redeemed, oh by the way, look at this $0 pass") will be a question. But assuming no such association occurs, just join by date... when the execs/etc realize that they want more info (like which online offers are working, which are generating *income* vs which are generating *debt*, etc), they'll realize that the system's *internals* should track the association, even if the "pass" prints the same either way.
I have a feeling I am completely misunderstanding the question. Are you talking about joining on the date? Could we get some column names of the two tables, maybe?
While this is not really a SQL question, you may want to take a look at [LimeSurvey](https://www.limesurvey.org/en/). It's an open source survey platform that could do what you describe, and you would have direct access to the database to query any results you received. If you use one of the big cloud services, bitnami has a [pre-configured setup](https://bitnami.com/stack/limesurvey) for LimeSurvey that makes it incredibly easy to test / get rolling with cloud hosting.
Or add a case statement like case when s.suspended_date is null then 'Active' else 'Suspended' end [Status]
Hmm. Did you check if any of the data flow task sources are referring to project connections that you don't have? The connections could also be called in other types of task ; I.e Sql execute tasks, scripts tasks, etc. 
I can get sum(quantity) easily by grouping by three fields a, b, c. But how can I get the last column value? If possible, can you provide a sample query? Thanks
Your sample data and your question are either incomplete or confusing. I'm not sure what you are trying to do, and the example you gave doesn't make sense since you reused all the data. D3? A1? What? Subquery. That's a possible answer. You may need to do more research.
Which DB?! 
SQL relies on knowing the number of columns at the point of parsing, even when using PIVOT. In your example there is no guarantee there will be only 3 pivoted column pairs. So unless you limit to a fixed number, what you want wouldn't be possible.
select to_date ('2015-07-16 12:12:04', 'YYYY-MM-DD HH24:MI:SS') + 3/24 from dual; The "3/24" part is to add the three hours. Source: http://www.orafaq.com/faq/how_does_one_add_a_day_hour_minute_second_to_a_date_value 
Group by the third_column, count( distinct concat( first_column, '&amp;', second_column)). Instead of "concat" you can use another binary function that gives reasonably unique output.
Thanks for getting back but this adds three hours to the existing time i.e. 2015-07-16 **15**:12:04. I'm looking to get 2015-07-16 **03:00:00**
2005 or 2008 version of concat then: Cast(first_column as varchar(200))+'&amp;'+cast(second_column as varchar(200)) Replace '200' with a sufficient number to fit values in your first and second columns 
... Tried it. That gives me an incorrect syntax error too. Might be something else that is to blame though. It looks sorta like this: SELECT XX.YEAR_MONTH, XX.CONTACT_ID||XX.KNOWN_BY, COUNT(XX.CONTACT_ID||XX.KNOWN_BY) FROM ( [Whole massive multi-joined select statement thing here] ) XX GROUP BY XX.YEAR_MONTH, XX.CONTACT_ID||XX.KNOWN_BY ORDER BY XX.YEAR_MONTH It keeps consistently giving me two error messages. One regarding a syntax error in the first line. One regarding a syntax error by the "XX" lower down. I had a previous query that worked fine where I only had to find unique values of a single column, but now I'm trying to hunt down unique combinations from two columns, the whole thing is falling apart. 
select trunc(to_date ('2015-07-16 12:12:04', 'YYYY-MM-DD HH24:MI:SS')) + 3/24 from dual; This converts the string to a date, then truncates the time portion, then adds three hours. 
That seems to work for concatenating the columns... but now I'm getting a syntax error "near FROM" ... which doesn't even begin to make sense. SELECT XX.YEAR_MONTH, COUNT(DISTINCT(CAST(XX.CONTACT_ID as varchar(10))+CAST(XX.KNOWN_BY as varchar(64)) FROM ( [Functional Select Block Here] ) XX GROUP BY XX.YEAR_MONTH ORDER BY XX.YEAR_MONTH This is still giving me double syntax errors. Only now... "FROM" and "XX" as opposed to the first line and the XX before.
Beautiful!!! works like a charm! select to_char(trunc(to_date (minexec, 'YYYY-MM-DD"T"HH24:MI:SS"Z"')) + 3/24,'YYYY-MM-DD HH24:MI:SS') from Dual; I had to do some tweaking on format, but it seems to work. You're a king amongst men. 
Once you format better (or get a better editor that counts opening/closing parentheses - like a SQL Server 2012, for example :)) you'll see that you've missed 2 closing parentheses: SELECT XX.YEAR_MONTH, COUNT( DISTINCT( CAST(XX.CONTACT_ID as varchar(10)) + +';'+ CAST(XX.KNOWN_BY as varchar(64)) )) FROM ( [Functional Select Block Here] ) XX GROUP BY XX.YEAR_MONTH ORDER BY XX.YEAR_MONTH
DOH! Of all the mistakes to make! ^_^;; Yeah, it'd be nice to get some program updates, but the IT guys in this company are hells of lazy about updating the software. We were stuck using Excel 2003 until last year, for instance. AAAAND it worked. It worked exactly as I'd needed. Thanks! =)
is it || for concat?
(dont know what teradata is, I am just a tSQL developer, so I may not understand what you need or what your systems limitations are.) Couldnt this be done easier using a join? like... SELECT b.* FROM Table_A a INNER JOIN Table_B b on a.Claim_NBR = b.Claim_NBR WHERE a.Example = 'X'
Right. So essentially the Volatile table idea. 'Create a volatile table B that includes the ~7,000 claim numbers, and then inner join that volatile table with the main table. I was just looking if there was an easier way or using a volatile table was indeed the best way. And Teradata has slightly difference syntax than oracle or tSQL but its all the same!
Thanks for the input! Might try this to see which output I like better.
While there are similarities, that's not correct generally speaking. Volatile/temp tables are DB physical objects - they survive a statement but don't survive various other scopes (session, server reboot, etc.) To be used as a source of data, all that data needs to be previously fetched/stored as well - the sql optimizer usually cannot do anything about the fact that all of the data needs to be instantiated in the temp/volatile table and the flow of execution is somewhat static (create temp table -&gt; populate all data -&gt; use temp table data). Subqueries/parts of a statement, on the other hand, do not have visibility past the statement that uses them, the query might be re-written by the optimizer in a different way and the data might be instantiated/accessed on as-needed basis (the optimizer could also choose to create 'ad-hoc' temporary storage for the data). 
&gt;a bunch of 'invalid naming errors It's hard to find fault in this. The result set column names are terribad indeed.
It's extremely difficult to debug someone else's SQL, especially without the tables handy, but here are a few things I noticed. &gt;SELECT net.[Network] ,[School] [School] is unaliased here, which table are you pulling it from? I would prefix its table/alias here to avoid confusion and errors. Further down: &gt; FROM #Schools s &gt;LEFT OUTER JOIN Where # implies that #Schools is a temporary table. Is it populated outside of this statement? Or is it supposed to be a regular table that's been populated already? If it is a temp table, pay no heed to this. Furthermore, your naming schemes could use a lot of work. I would try and cut down the column name sizes significantly to ones more manageable. You can always rename the columns in whatever report or application you're providing the data for (though that isn't ideal, it's a better solution to use short-hand names in the stored proc (at least for debugging purposes here). Good luck! 
Could the cells be formatted as text and not numeric? SQL looks good to me.
I'm doing Khan and [W3](http://www.w3schools.com/sql/default.asp) right now. Two days in and have learned a hell of a lot already. It looks like SQLZoo takes a slightly different approach, so I'll probably mix it up over there as well. Thanks for the link!
select count(distinct column_1 &amp; column_2) from tbl no need for silly formulas just and.
https://msdn.microsoft.com/en-us/library/ms175874.aspx &gt;The names of variables, functions, and stored procedures must comply with the following rules for Transact-SQL identifiers. The first character must be one of the following: A letter as defined by the Unicode Standard 3.2. The Unicode definition of letters includes Latin characters from a through z, from A through Z, and also letter characters from other languages. The underscore (_), at sign (@), or number sign (#). Certain symbols at the beginning of an identifier have special meaning in SQL Server. A regular identifier that starts with the at sign always denotes a local variable or parameter and cannot be used as the name of any other type of object. An identifier that starts with a number sign denotes a temporary table or procedure. An identifier that starts with double number signs (##) denotes a global temporary object. Although the number sign or double number sign characters can be used to begin the names of other types of objects, we do not recommend this practice. Some Transact-SQL functions have names that start with double at signs (@@). To avoid confusion with these functions, you should not use names that start with @@
This is perfect!!! However, i don't know how to implement this in HiveQL... I am working on a Hadoop environment.. Any help in the regard would be much appreciated.. Thanks, btw
The issue you're asking about lies with [# Of Exiting Seniors]. You assign the value SUM(CASE pe.GradeLevel WHEN 13 THEN 1 ELSE 0 END) the alias [# Of Exiting Seniors], which is fine, however you then try to use [# Of Exiting Seniors] further down in your calculations, which is not possible as at the time of evaluation SQL won't know what [# Of Exiting Seniors] is as the aliasing hasn't taken place yet. This probably occurs in other parts of your query but I haven't found them yet. That same line (SUM(CASE pe.GradeLevel WHEN 13 THEN 1 ELSE 0 END)) is what's causing you to require a GROUP BY clause, not the subquery. E* This is how I would have written your procedure. I'm not sure if this will work or not because I don't have your tables or data, so no guarantees. Also I used different column names for my own convenience Edit 2: Now that I think through it again, this code won't work as you're looking at the number of seniors *per school*. I'd go back through and correct it, but I honestly don't have time (I'm at work). Sorry DECLARE @NumberOfSeniors DECIMAL(5,2); DECLARE @DiverseLearners DECIMAL(5,2); SELECT @NumberOfSeniors = COUNT(pe.gradelevel) FROM etccommon.dbo.tblstudentinfo_prev pe WHERE pe.gradelevel = 13 SELECT @DiverseLearners = COUNT(DISTiNCT studentid) FROM etccommon.dbo.tblstudentinfo_prev AS e LEFT JOIN etccommon.dbo.ods_student AS o ON o.studentid = e.[sid] WHERE e.gradelevel = 12 AND spedindicator = 'y' ;WITH info AS ( SELECT d.schoolid, CAST(COUNT(d.chk13_01) AS DECIMAL(5,2)) AS SeniorsOrParentsContacted, CAST(COUNT(d.rdb13_02_y) AS DECIMAL(5,2)) AS SummerSeniors, CAST(COUNT(d.rdb13_03_y) AS DECIMAL(5,2)) AS CreditRecoveryPlans, CAST(COUNT(d.rdb13_04_y) AS DECIMAL(5,2)) AS CompletedIepTransitionPlans, CAST(SUM(d.txt13_05) AS DECIMAL(5,2)) AS AssistedSummerCollegeApps, CAST(SUM(d.txt13_06) AS DECIMAL(5,2)) AS CollegeAcceptances, CAST(COUNT(d.rdb13_07_y) AS DECIMAL(5,2)) AS CompletedFafsas, CAST(SUM(d.txt13_08) AS DECIMAL(5,2)) AS CompletedScholarships FROM datacollection.dbo.stc_transitiontool_data AS d GROUP BY d.schoolid ) SELECT net.network, s.school, s.schoolid, @NumberOfSeniors, info.SeniorsOrParentsContacted, info.SeniorsOrParentsContacted / NULLIF(@NumberOfSeniors, 0) AS PercentOfSeniorsOrParentsContacted, info.SummerSeniors, info.CreditRecoveryPlans, info.CreditRecoveryPlans / NULLIF(@NumberOfSeniors, 0) AS PercentOfCreditRecoveryPlans, @DiverseLearners, info.CompletedIepTransitionPlans, info.CompletedIepTransitionPlans / NULLIF(@DiverseLearners, 0) PercentCompletedIepTransitionPlans, info.AssistedSummerCollegeApps, info.CollegeAcceptances, info.CompletedFafsas, info.CompletedScholarships FROM #schools AS s LEFT JOIN info ON s.schoolid = info.schoolid LEFT JOIN etccommon.dbo.tblschoolnetwork AS net ON s.schoolid = net.schoolid WHERE AND info.schoolid IS NOT NULL
Try using a range in the where clause and check to see if all the numbers are returned. If all of them return then it's probably caused by trying to match against an exact floating point number. WHERE Total &gt; 0 and Total &lt; 999999
Having used Microsoft SQL and MySQL, I can say of the two that Microsoft SQL had better tools for building stored procedures. (Which is how I communicated with the database so as to avoid ORM weirdness/SQL injection.) (And depending on your scale/scope/feature set needed of project, Azure hosted SQL databases can be an awesome option. Just remember that you are effectively sharing the server with others at that point, and while security isn't as big an issue, you at least need to be good performance wise about playing in the sandbox together. Otherwise the connection governor can become your mortal enemy.)
In addition to ichp's comment, it's better to never use anything other than alphanumeric characters, and to avoid using numeric characters in database object names. There is a balance between clarity and brevity with names. You don't want people that read your code to be overwhelmed by the length of your names, but you also don't want them wondering what the names represent.
Okay, sounds like Microsoft would be a better choice with the added bonus of Azure as an option for the future. The scope is really no more than 12 clients accessing the database which contains xrays, lots of notes and specialized forms, along with lots of patient and other information. Typically this scenario runs off of a closed network w/o internet access for security reasons. (Having all of peoples medical records along with their personal information is a HUGE liability) However, I know one of the software options that currently exists runs off of a cloud based solution that stores all information at the developer's HQ, allowing remote access. However, Azure seems like a better platform with better security. Realistically ~85% of its use would be small text files and the remaining would be .jpg files of xrays. 
See what type the column is for those tables in the database. Sometimes it will just default to text instead of numerical, in which case you might have spaces in the data. 
These are the tables that would be required I believe Project (TABLE), PNo EstCost CompDate LeadCId PDesc StartDate Consultant (TABLE), CId Specialty CName HireDate Salary ProjectTeam (TABLE), PNo CId EstHrs Specialty (TABLE), Specialty BillingRate Assigned_Task (TABLE), PNo CId TaskNo HoursBilled 
On mobile, so forgive spelling and formatting. SELECT Name FROM table WHERE Name NOT IN (SELECT Name FROM table WHERE col2 = col3) Or something to that effect. 
Weird - when I got here yours was the only downvoted answer despite being the only one that does what OP actually asked.
Ha thanks! I hadn't noticed. 
Use regular expression replace in SSMS.
Sublime Text with any of the SQL plugins. Nothing beats the multiple cursors to insert commas, highlight the text and place it all in one line. 
Notepad++'s extended find and replace is perfect for this. Find \s replace with ',' ... and then just add (' and ') onto the ends of the string.
Select name From table Where name not in (Select name From table Where col1 =col2) I think that would work
I agree with most of this post. I would choose SQL Server of the two you mentioned. I've grown to appreciate Oracle myself. As a DBA for a medical software development company, I am curious what you are working on. 
Or you could use NOT EXISTS for the same effect as NOT IN. Not really better or worse intrinsically, just different. Consequently sometimes you will get better performance, occasionally worse, but most often essentially the same performance out of the 2 methods. Just an option. SELECT DISTINCT name FROM table t WHERE NOT EXISTS ( SELECT 1 FROM table e WHERE e.name = t.name AND e.col2 = e.col3 ) (As an aside the EXISTS form will almost always be faster than the IN form. Consequently I tend to use NOT EXISTS where I can so that its consistent with where I have used EXISTS everywhere else, even though NOT EXISTS and NOT IN are essentially equivalent)
I believe I read that most modern RDBMS will make either usage equivalent in the explain plan. Definitely worth looking into for op's specific product if the table is ginormous
You are right that in most cases (post SQL Server 2008) the optimiser will make the conversion for you. Still better to be safe than sorry. And EXISTS is more flexible due to being able to match multiple columns. I think its probably mostly just that old habits die hard ;)
The data I grab usually looks like this: Apple Banana Strawberry Bacon With notepad++ regex replace ^ with ' and $ with ', Remove line breaks if desired.
If you prefer Excel... Paste the row. Split to cells. Copy the cells. Paste special transpose so they are now in columns. Place a single quote in preceding cell. Place a single quote and a comma in the following cell. Use a formula in the next cell to concatenate them together. Copy new cell and paste it as values to new column. Copy again and paste into SQL editor as is, or paste special in excel and transpose to horizontal again, then copy and paste that to your sql editor. Remove the trailing comma Quite fast when you get used to it but the notepad++ method is simpler. 
If you go for Microsoft SQL server, then take into account you'll not be able to use Dreamspark forever, and if you create a product and sell it, who will have to pay for the SQL server license? Depending on which version you need, SQL server is not cheap. Just checked: http://www.microsoft.com/en-us/server-cloud/products/sql-server/purchasing.aspx According to that price, you pay minimum 7434usd for a standard edition. I would check on PostgreSQL and on Firebird. Both are open source and very mature and capable database servers. Firebird can also be embedded in applications. If I understand it right, you will store big/huge files inside that database (xrays). I think you should definitely check out the support and performance with that kind of data in the database servers you want to chose from. That should be a major point in your decision. I suppose you will use an ORM for your application. If you do it right, it should definitely be doable to switch to another database at some later point. Of course, it's possible you won't be able to use an ORM for everything if you want to take advantage of special support for big files in those database servers. Personally, I'm not a fan of MySQL and would never use it. And the suggestion of using SQLite is... well, no comment.
Currently I'm in K-12 public education as an Analyst/DBA. I one of the guys who runs the student information system, finance system, and other systems for a largish school district (~8,000 students). I left healthcare IT about 7-8 years ago when I moved home to help take care of family. I was more of a pure Analyst in healthcare, mostly writing reports and supporting apps from the technical side (at least half the group was former nurses). I also did the workstation patch management because the desktop services team couldn't do it. I worked at a larger hospital with ~400 beds. I only worked there about a year and a half, though. Just long enough to get the culture and everything figured out. Oh, and to not spell it "HIPPA" anymore. At the time we were still doing endless meetings about HIPPA compliance and auditing, and this post made me think of it. Up until a couple years ago, I was trying to get back into healthcare IT so I was paying attention to it. Now I'm kind of stuck where I'm at, and I don't pay much attention. I can't imagine the culture has shifted that much, however. In between I did some dev and sysadmin work at an engineering firm on their in-house ERP which ran on MySQL 4.0. That's where I learned to be a lot more skeptical about MySQL, but 4.0 was very stupid. It would silently truncate or modify data, allow invalid dates like February 30th, and just generally make itself known as a nuisance. It was, however, very easy to use and manage... at least by appearance because it often just didn't tell you when there was a problem. 
There are problems with SSMS regex replace. [See my post below.](https://www.reddit.com/r/SQL/comments/3doss4/ms_sql_suggestions_for_format_text_in_preparation/ct7mjgq)
TIL. For OP's use case though it'll work just fine. It appears that after a few hundred lines it is consistently broken. It bothers me when extra tools are the first recommendation for such a simple task that the existing tool will handle it just fine.
Postgres
That's the biggest porblem with Microsoft because to sell it, I'd need a commercial license which is very expense. The question is could I use the others and still be HIPPA compliant. I'll look up PostgreSQL and Firebird and read into them. As for file sizes and performance I will definitely look into that on the solution I go with as a backbone. Thank you for your help!
100+strings!!! Create a table with the values.. If you have them in the text file then import... but don't string them in a IN statement.
I'm not from the US, but from what I can see online, I think you will probably have to have your complete software setup audited for HIPAA compliance by a certified, independent auditor. I don't think that is gonna be cheap. There seem to be some technical guidelines available online for HIPAA compliance (http://www.hhs.gov/ocr/privacy/hipaa/understanding/srsummary.html): * access controls * audit controls * integrity controls * transmission security In the software I work on, I also deal with a lot of personal information. We always implement access control (who can view the data), audit control (who changed what when), and transmission security (data between services is encrypted). (Software is also only available on-site.) Implementing those is not really difficult, but it's best to take those things into account right from the beginning. The only thing we don't have right now, is integrity control, but I assume that could be fullfilled by using some kind of checksum so that one can verify whether someone tampered with the data. 
I would be very careful about taking a patient database home. That is likely violating hipaa and could get the chiropractor in trouble. 
100% agree on hipaa concerns. I would also like to add that even if this wasn't about hipaa and patient confidentiality, businesses don't like you just taking a copy of their database.
Noob here also but I would suggest just copying the format of the DB, maybe there are SQL scripts to recreate it, and just filling it with test data. be sure to ask permission first, I have heard stories of people getting in trouble for copying code to their home computer.
As others have said, do not take any of the data. This is a huge violation and could get you and your chiropractor friend in a lot of trouble. Some EHR systems can be "deidentified", but it's still not a good idea. Probably best just to steer clear of this. 
RED FLAG!!!!! DO NOT TAKE HOME PROTECTED PATIENT INFORMATION/PATIENT HEALTH INFORMATION. I REPEAT, DO NOT!!!!!! I'm sure that there is only ONE computer that stores the DB and all other systems access that server. Taking home PHI data opens you and the practitioner up to a HUGE liability if that data were to be lost to data theft or physical theft of your computer. At best, you would only be allowed by law, a fully redacted version of that database. ( no names, addresses or other potentially identifying information ) I'd be careful as to what you do, How you do it and what cryptic information you allow back out through the programs 
Use an inner join and a when clause filter for Type = A
what about getting a count of all distinct EMP_ID for each Region.
Not at my computer with Oracle right now. I'm guessing the final output will have two columns one with region and one with the number of distinct employees for that region. Also its one statement Thanks :) 
Good point, I guess you use my SQL?
MSSQL mostly. 
I agree with all the comments here; if someone new came to my company and wanted to take a copy of the database I'd be mortified. Add to that patient confidentiality and it is just not a good idea. This said, if your company agrees - so you need to communicate with them - you can recreate the database tables and fields, but not the data, by having SQL Server Management Studio generate a script for you. Right-click the database name, and under Tasks, look for the option to Script the database. You can then recreate the database structure by just executing this script on a SQL Server you have elsewhere, such as at home or in Azure, or elsewhere. You can populate it with dummy data.
Thanks for the info. I had never heard of that and didn't even think of it although it be pretty obvious. I'm going to try to figure out how to set up an SQL server and script the format of the DB to the server I make and add in test data. Now to figure out how to do all of that as I've never done it before. :P
You could try something like this if you are using SQL Server(there are equivalences for others): -- Note: Change datatype to match that of cs-uri-query DECLARE @tempstr VARCHAR(MAX) DECLARE findErrorCrsr CURSOR FOR SELECT cs-uri-query FROM yourTable IF OBJECT_ID('tempdb..#failedToConvert') IS NOT NULL DROP TABLE #failedToConvert CREATE TABLE #failedToConvert (dataz VARCHAR(MAX), datamsg VARCHAR(MAX)) OPEN findErrorCrsr FETCH NEXT FROM findErrorCrsr INTO @tempstr WHILE @@FETCH_STATUS = 0 BEGIN BEGIN TRY CASE WHEN Sign(CAST(dbo.ParseWebLog([@tempstr],'WT.vt_f_tlv=') AS NUMERIC(14,0)))= -1 THEN NULL WHEN dbo.ParseWebLog([@tempstr],'WT.vt_f_tlv=') = '0' THEN NULL ELSE dateadd(ms,abs(CAST(dbo.ParseWebLog([@tempstr],'WT.vt_f_tlv=') AS UMERIC(14,0)))%86400000,dateadd(dd,floor(abs(CAST(dbo.ParseWebLog([@tempstr],'wt.vtvs=') AS NUMERIC(14,0)))/86400000),'01-01-1970')) END AS LastTimeVisit_Standard END TRY BEGIN CATCH INSERT INTO #failedToConvert(dataz, datamsg) SELECT @tempstr, ERROR_MESSAGE() END CATCH END CLOSE findErrorCrsr DEALLOCATE findErrorCrsr SELECT * FROM #failedToConvert
Which flavour of SQL is this?
Firstly, what's the error message? 
Put a SELECT in front of it.
 Select DateName( Month, Date ) + ' ' + DateName( Year, Date ) As MonthYear From SomeTable If you want numeric months, use DatePart instead. Select DatePart( Month, Date ), DatePart( Year, Date ) From SomeTable
year(date_field) &amp; "|" &amp; month(date_field) as [year|month]
If you're indexed on the sort row and not using other parameters OR you're group indexed including the fields with additional parameters the process load is not likely to be intense barring the data pull is larger. 72,000 rows is not significant at all.
Read this then come back https://msdn.microsoft.com/en-us/library/Aa479086.aspx
I know generally you wouldn't but there isn't a single use case for it?
Each user would have their own login to access the data, which got me into thinking of this whole 'database per user' scenario where I could also provide them with read SQL access to their database. As for how many users I can't really answer that myself, the idea could be a total failure and then sure a few dozen, or it could explode and I suddenly have 1000s of users. I would guess initially all going well to be dealing with 100 or so active users and maybe 200-300 more "log on once or twice and use it" users. I wasn't really too worried on the security aspects of doing something like this, I think I understand pretty well on how to keep separate it per user whether its on the same database or different databases. I'm much more concerned about performance and my question is really I guess "how many rows is too many rows in a table". I'm really just trying to design a way of storing this data that will scale cheaply if the idea gains popularity. As for which DB I guess either mySQL or PostgreSQL with leaning more toward PostgreSQL at the moment.
Thanks, I just gave it a quick scan through but looks very helpful I'll read it fully later.
If I understand your question correctly. &gt;SELECT &gt;accountid &gt;, cycleendingbalance &gt;, purchaseamount_a &gt;from [dbo].[table] &gt;where at_creditlimitdelta &lt;0 &gt;and &gt;at_datelastlimitchange BETWEEN dateadd(m,-3,GETDATE()) AND dateadd(m,3,GETDATE())
*yes -3 for before. the problem is i dont want the balance, and spend 3 months before Today's Date. i want it 3 months before the 'at_datelastlimitchange'
What does that table have in it, one record per account or one record per account per day or what?
sorry. the table has one record per account per month.
select accountid, at_datelastlimitchange, sum (case when at_datelastlimitchange = dateadd(m,-3,at_datelastlimitchange) then cycleendingbalance else 0.0 end) as Balance3MonthsBeforeChange from [dbo].[table] where at_creditlimitdelta &lt;0 group by accountid, at_datelastlimitchange
?? still not working 
Use IF EXISTS(select SCENARIO) BEGIN -- do something END 
What was used to make the charts?
Try Cast(left(periodid, 4) &amp; '-' &amp; right(periodid, 2) &amp; '-01' as datetime) or change datetime to varchar if you want it as a string. 
What is your question?
[It's a joke](http://money.cnn.com/2015/07/20/technology/ashley-madison-hack/). 
I use **https://pypi.python.org/pypi/pyodbc/**, just FYI. Why do you want to use Python for this? I'm always down for learning new things, but when you start doing this in production there are other concerns. Scheduling. If you used SSIS or Powershell, you could use a SQL job to schedule it. Using python on windows, the easiest way to schedule it would be a scheduled task, but that functionality has always sucked and it's better to have everything in one place. You could still use a job with python, but then you'd have to get into remote commands because I doubt your DBA would want python on the production db server. Maintenance. It sounds like you're the only one using Python in the office. This causes problems if you're out and something breaks. Python's not hard, but if the script is critical and whoever gets to fix it has to learn Python on the fly, you'll probably get chewed out when you get back. 
A couple of questions for you, first what does your Cpu utilization look like? You are hitting a bottle neck somewhere, which a lot of times I have found to be IO constraints. What I would look at doing is using dbms_scheduler to break up the aggregation, and also to use a tool to measure the resource usage that a subset of the query is using. There is a method outlined in chapter 10 of "Oracle 12c Advanced Plsql techniques" by Michael McLaughlin. There is a very real possibility that you have a bottleneck that won't allow you to work within your time window. Let me know if you need some more direction on using the scheduler. 
&gt; the script just inserts a report(200K rows) into a pre-existing table and then runs an UPDATE Statement over the table, but the process didn't complete and then the log file ballooned to 400GB. Yeah, this does not make any sense unless you have nested triggers enabled or an infinite loop in your script or some kind of SQL injection going on or some serious failure in logic like running `UPDATE Table SET Field = 'Value' WHERE 1 = 1` every step of a loop. It sounds like you're using Python for ETL. I would probably not do that. I would probably use SSIS because that's very easy once you know how to author a package. If I couldn't do that, I'd probably use Powershell. There's nothing wrong with Python, but .Net interfaces very well with SQL Server and the provider has been tested to death. If you know SqlDataAdapter and SqlCommandBuilder it's pretty straightforward to do, but even parameterized statements in a loop with SqlCommand is pretty easy. In any case, whenever I do data imports I always use a staging table. Usually I create an actual table and not a temp table. First because then you can see the data after import but before you merge without cracking open the source file. Second because you can write a view that does all the necessary transformations. All you need to do is remember to start off with a TRUNCATE or DELETE on the table (use the latter if you want the possibility of a rollback, the former will save log space and reset AUTOINCREMENTs). Now all your logic is stored in the DB in a nice, deterministic format. Then you start a transaction and do your INSERTs and UPDATEs. I tend to avoid MERGE because it has some [rare concurrency issues](https://www.google.com/search?q=sql+server+merge+race+condition), but if you do use it be sure to specify HOLDLOCK/SERIALIZABLE locking hints or the SERIALIZABLE transaction isolation level to avoid most of the badness. [Short version is that MERGE unrolls into a bunch of UPDATE and INSERT statements, and doesn't entirely make sure nothing steps on it's feet... including itself.] And as /u/juitar says, develop on a Dev server! 
I'm not sure if this is relevant, but the last command in the sql script is `db.exectute("UPDATE Table SET Last_Update_Date = SYSDATETIME();")` It's not executed inside of a loop and no error is thrown. However, the code seems to freeze on running this command. 
So you have a 200K row report that you're inserting, and after every row you insert you're updating the entire table's Last_Update_Date? That seems very excessive. On an empty table inserting one row at a time followed by the table-wide update that's (200000*200001)/2 = 20,000,100,000 updates! If you're using a datetime2 or datetime2(7), that's 8 bytes of data for every row. 160,000,800,000 bytes ~= 150 GB just in datetime2 values. If you've got your DB set to Full recovery, your transaction log has to remember all of those changes for point-in-time recovery. Even if it's set to Simple recovery, there probably is too much activity for a periodic CHECKPOINT to flush the logs. Even skipping that, I can't imagine needing to track a single date value on every single record. It's essentially Table_Last_Update_Date. You're sure that's right? It's not supposed to be *the record's* Last_Update_Date? Why not just update the value once at the end of the loop? I mean, how many millions of rows is the table if you're inserting or updating 200K rows for a single report? And is there a trigger on the table? Many systems with this type of field have an AFTER INSERT, UPDATE trigger to automatically update the field when the record is changed.
If you're doing ETL use SSIS as it's specifically designed and optimized for what you need.
master?
[Really?](http://i.imgur.com/d2RPMnN.gifv)
Several years ago, Reddit used to only use two tables. Now it uses two tables per *thing* (users, posts, etc). So each table can have millions of rows. It's pretty typical for a heavily used table to have millions of rows. At work on our test server we have one table with 4m rows, one with 1.5m rows, and it looks like 10, or so, more with 500k+ rows. That isn't even counting what is actually on the production server that clients use. You need to remember how efficient computers actually are at processing data. Especially SQL.
Looking for ' with wildcards. Inside the string, the second single quote is an escape character for the first one.
Can you give us your current query? I'd think making it an inner query with row_number() function, and then adding an outer select with "where rownumber &lt; 10" might do it, but it depends how your query is written.
OK, I'm still confused, mostly because this query wouldn't even run... I know that happens when you're not at a computer. :) Did you mean this: SELECT * FROM TOP_50_ITEMS INNER JOIN (SELECT PRODUCT, REGION,AVG(SUM_OF_SALES) AS AVG_SALE FROM TOP_50_ITEMS group by region, product) AVG_REF ON TOP_50_ITEMS.REGION = AVG_REF.REGION and top_50_items.product=avg_ref.product WHERE TOP_50_ITEMS.TOTAL_SALES &gt;= (AVG_REF.AVG_SALE * .80) If so, Id try this: select * from ( SELECT top_50_items.*, avg_sale, row_number() over (partition by region, product order by total_sales) rn FROM TOP_50_ITEMS INNER JOIN (SELECT PRODUCT, REGION,AVG(SUM_OF_SALES) AS AVG_SALE FROM TOP_50_ITEMS group by region, product) AVG_REF ON TOP_50_ITEMS.REGION = AVG_REF.REGION and top_50_items.product=avg_ref.product WHERE TOP_50_ITEMS.TOTAL_SALES &gt;= (AVG_REF.AVG_SALE * .80) ) where (avg_sale*0.80)&lt;=total_sales or rn&lt;=10 I'd also replace the * with an actual field list, so that you can exclude avg_sale and rn from the final result. **edit**: I forgot the 80% rule, so I added that back in. 
In oracle we don't use rownum. Check this article to get you moving along the correct path. https://oracle-base.com/articles/misc/top-n-queries using rank and the average together rather than average in your where clause will always result in giving you the correct 10 results. you probably want dense rank so if a bunch of items are tied it doesn't arbitrarily leave any off the list. 
Were you working as a nurse before moving into SQL work? What is your job title/responsibility now and could you elaborate on what steps you took to get there?
Thanks. Did you have to pick up any other languages or is SQL adequate for DBA jobs? Are there any other technical knowledge you need? E.g. How extensive is your knowledge of statistics?
They haven't been as extensively tested as documented procedures, so there might be edge cases where the proc is not guaranteed to behave as expected. EDIT: phrasing
Definitely NOT a must to learn Python and PHP if you're going the SQL route.
Not OP, but SQL is definitely enough for a DBA role. Not sure what you mean by statistics. You mean like regression analysis-type statistics? Again, not OP, but having a stat background wouldn't hurt, but it's definitely not needed at all. Probably the most important things are experience and certifications. If you're serious about SQL I'd check out study guides for the Microsoft 71-461 exam (Querying MS SQL Server 2012). It'll give you a great base for relational databases in general.
Depends on your end goal. ETL developer will allow you to do more SQL related development and can also help bridge the gap to other development roles. DBA will allow you to do more Server administration tasks and lead to a more server side career. Either works, depends on what you prefer. Though I do believe DBA has a brighter future (higher pay and more opportunities) than an ETL developer. 
Im an ETL dev. I like it. Mostly SQL, some DBA stuff, some IT stuff, a lot of meetings. Some days I do nothing at all. Once you have your processes engineered and automated... not much to do!
You've got your answer with the aliasing, but wanted to also note that you don't appear to need a subquery here. Wouldn't this work? select Accountid, email, AccountEmail as [Type] into Accountemail_temp from Account where Email like '%''%' 
Business Knowledge + SQL = BI. It's not a quick or easy path though.
I started out as a report writer as well, with very limited sql knowledge. It is a great starting point.
I deal with this kind of problem pretty often and as painful as is to accept, using a loop is a hell of a lot easier. You can do it set based, but when you have these kind of rules, it starts getting messy in a hurry and you end with these monstrous queries that are very difficult to understand. Adding more rules causes the complexity to go up exponentially. The real problem shows up when some one wants you to tweak the rules. Maintenance of these queries is a nightmare. In my case, I've found trading a whole bunch of performance is worthwhile because things changes enough to justify the hit. Every once in a while there will be something that's performance critical and then I'll go through the hassle of doing it set based, but the majority of the time, it's a loop where I evaluate the which bucket to put the current item in against all the rules and what I've already done (to keep the buckets as balanced as possible).
What do you mean by using a loop? 
Most report writing jobs in my area pay ~50k/year and only require like 1-2 years of experience.
Hmm, not sure if this feasible unless I'm not correctly understanding the way the WHILE loop works since the data set contains almost 300 stores and 150,000 customers
It helps a ton to know some other languages, but can be done with just SQL.
Okay thanks, I'll give it a shot
 select e.*, e.Counter % numstores from ( select *, Counter = ROWNUMBER() from [employees] ) e should give you a start... basically, Counter simply gives each employee a sequential number (for even distribution)... then % numstores to round robin assign
That is a good start, thanks There is a column in my customers table that has the store number that corresponds with the store that is closest. Using your query, I'm having trouble assigning employees to the customers that only match the store number on the customer table when I join the two together
What version of MSSQL? If possible you might want to consider [utilizing MERGE](https://msdn.microsoft.com/en-us/library/bb510625.aspx), it handles situations like this fairly well. 
Use a LEFT JOIN. SELECT t1.*, t2.col1 email_id FROM Table1 t1 LEFT OUTER JOIN Table2 t2 ON t1.client_ID = t2.client_ID
You'd add... WHERE table2.somecolumn = yourFilterValue OR table2.client_ID IS NULL
You are going to need to provide more details if you want a code-based solution. You are correct that you're going to need to group by Shipping address and name and sum the quantity. I don't understand what you're looking to do with rank and top 1. Once you group by address and name, your result set will only have 1 record per address/name combination.
It would just be a part of the JOIN condition. ... LEFT OUTER JOIN Table2 t2 ON t1.client_ID = t2.client_ID AND t2.col1 LIKE '%@%'
Awesome. Thank you very much. I didn't know I could add that to the end of a join. Still learning. E
Given the information, you'd most likely want to do something like this: SELECT * FROM ( SELECT rn = ROW_NUMBER() OVER (PARTITION BY shipToAddress, Name ORDER BY dateProcessed DESC), * FROM yourTable ) WHERE rn = 1 The above will give you the most recent order for a person grouped by the two things you listed. 
left(CONVERT(varchar(10), cycleenddate, 20),7)
Are you just trying to get the beginning of the month as a date? If so, you could use DATEADD(MONTH, DATEDIFF(MONTH, 0, [your date]), 0) Or, if you want the beginning of the month 3 months prior, you could do DATEADD(MONTH, DATEDIFF(MONTH, 0, [your date]) - 3, 0) So, maybe all together, you'd end up with something like SUM(CASE WHEN DATEADD(MONTH, DATEDIFF(MONTH, 0, at_datelastlimitchange) - 3, 0) = cycleenddate THEN cycleendingbalance ELSE 0.0 END)
Well you don't have to make all the columns text, you can select them and make them different. However, I know what you're saying... You don't want to have to do that every time. I would put my teradata connection, query, and output routine to excel (specifying column formats) in a python script... Just because it's my usual tool of choice. But VB is a way to go as well, certainly. But there is no way to make Excel smarter, unfortunately.
Yes, i dont always have all the attendee fields populated. Just depends on how many show up.
Someone else might have a more elegant solution but this should work: WITH X as ( SELECT DISTINCT Attendee1 as Attendee, COUNT(*) as Attendance FROM Table GROUP BY Attendee1 UNION SELECT DISTINCT Attendee2 as Attendee, COUNT(*) as Attendance FROM Table GROUP BY Attendee1 UNION SELECT DISTINCT Attendee3 as Attendee, COUNT(*) as Attendance FROM Table GROUP BY Attendee1 ) SELECT DISTINCT Attendee, SUM(Attendance) FROM X --WHERE Attendee IS NOT NULL (edit: necessary?) GROUP BY Attendee1
Could probably also write it as: SELECT DISTINCT X.Attendee, SUM(X.Attendance) FROM (SELECT DISTINCT Attendee1 as Attendee, COUNT(*) as Attendance FROM Table GROUP BY Attendee1 UNION SELECT DISTINCT Attendee2 as Attendee, COUNT(*) as Attendance FROM Table GROUP BY Attendee1 UNION SELECT DISTINCT Attendee3 as Attendee, COUNT(*) as Attendance FROM Table GROUP BY Attendee1) as X GROUP BY X.Attendee
Excel is awful regarding zero padded fields, strips them, incorrectly type casts them. Try putting a dummy row after the column headers (must be the first row) which puts a letter in that field, e.g. Z102345. I think Excel then treats the column as a string, rather than numeric column.
if you have a good structure then you can `SELECT` `COUNT` of customers with `acc_handler` (which is the PK of an employee), sort by lowest `COUNT`. ***may require sub-select***, then simply `ORDER BY` and combine with `LIMIT` to first result. Alternatively (what I would do), is to add a field to the employee that I update, with the result of the count periodically. It's then very easy, &amp; even simpler, `SELECT` the employee with the lowest number of accounts handled, use that to assign to the customer. It is not actually round-robin, but it is even load-distribution, and can be combined with an `IN` clause if the employee's need to have certain skills, by using employee PK's from another `SELECT`, on another table mapping employees to skills
You are correct, not very database savvy. I'm a spreadsheet person who is trying to branch out and thought this would be a simple project to start with. In time, I think CODESIGN2's format is probably better. It's difficult to get into the mindset of data in different tables and referencing them together. notasqlstar, I tried both sets of code. The first one wouldn't work as Access was expecting "Delete", "Insert", "Procedure", "Select" or "Update". The second set of code gives me an error saying you tried to execute a query that does not include the specified expression 'Attendee' as part of an aggregate function. 
You could try breaking the query down into two. SELECT DISTINCT Attendee1 as Attendee, COUNT(*) as Attendance INTO NewTable FROM Table GROUP BY Attendee1 UNION SELECT DISTINCT Attendee2 as Attendee, COUNT(*) as Attendance FROM Table GROUP BY Attendee1 UNION SELECT DISTINCT Attendee3 as Attendee, COUNT(*) as Attendance FROM Table GROUP BY Attendee1 SELECT DISTINCT Attendee, SUM(Attendance) FROM NewTable --WHERE Attendee IS NOT NULL (edit: necessary?) GROUP BY Attendee1
What about linking excel directly to TERADATA through power query? https://support.office.com/en-za/article/Import-data-from-external-data-sources-Power-Query-be4330b3-5356-486c-a168-b68e9e616f5a?ui=en-US&amp;rs=en-ZA&amp;ad=ZA#__toc381890195
remember, when you GROUP BY something, then DISTINCT on that something is completely redundant
&gt; remember, when you GROUP BY something, then DISTINCT on that something is completely redundant 
Excel will retain the leading zero's if you put a leading single quote in front of the field value. You can concatenate before you export in sql.
select name , title from ((select mID, rID from Rating R1, Rating R2 using (rID, mID) where (R1.ratingDate &lt; R2.ratingDate) and (R1.stars &lt; R2.stars)) join Reviewer using (rID)) join Movie using(mID);
what is the difference between that and this? If there is any... LEFT(Convert(date,cycleenddate),7)
NULL is not the same as a record not existing. You might try aomething like IF EXISTS (your query) THEN (your query) ELSE (default query)
First, I would avoid using comma joins. Comma joins make it difficult to tell what's going on, and queries of the type you're writing become essentially impossible. Beginning with ANSI-92 (as in 1992) the improved JOIN syntax was introduced, and that's really the only syntax you should use. Both versions still work, it's just that comma joins are more difficult to maintain and less usable. So, your query written with the current JOIN syntax looks like this: SELECT Machine.Name, Daily.Quality, Daily.Down FROM Machine INNER JOIN Daily ON Machine.ID=Daily.ID WHERE Daily.Date='2015-07-22'; Notice how the logic that describes how the tables are connected is entirely in the ON clause, and that all the filters are in the WHERE clause? That's why people find it easier to read and maintain. Now, one important point is that the database actually does the ON clause and the WHERE clause at completely different points. It does the ON clause first, and then later does the WHERE. That's not important right now, but it will be later. Now if you run that, it should be identical to what your current query does. In other words, it's still not what you want. So, let's take a look at one of the nice [join images](http://i.stack.imgur.com/66zgg.png). The INNER JOIN is in the middle. That's not what you want. You want every record from `Machine` even if there are no records in `Daily`. That means you want a LEFT OUTER JOIN, also called a LEFT JOIN. SELECT Machine.Name, IFNULL(Daily.Quality,'10') AS Quality, IFNULL(Daily.Down, 'False') AS Down FROM Machine LEFT JOIN Daily ON Machine.ID=Daily.ID WHERE Daily.Date='2015-07-22'; Now, if you run that, it's *still* not going to work. The reason is because of the WHERE clause. You're requiring that `Daily.Date` have a value, so if that field is NULL -- such as it will be if there's no record in `Daily` -- the WHERE clause will evaluate to UNKNOWN instead of TRUE. What we've done here is created an *implicit* INNER JOIN because we didn't handle what happens when `Daily.Date` is NULL. You must be careful with OUTER JOINs because you can easily do this kind of thing. So, we have to allow `Daily.Date` to be NULL: SELECT Machine.Name, IFNULL(Daily.Quality,'10') AS Quality, IFNULL(Daily.Down, 'False') AS Down FROM Machine LEFT JOIN Daily ON Machine.ID=Daily.ID WHERE Daily.Date='2015-07-22' OR Daily.Date IS NULL; However, there's another way to do this. Remember I said the ON clause happens before the WHERE clause, and that the the ON clause determines how the JOIN works? Well, you can use the ON clause to filter a table before the JOIN even happens. That means you could write the query like this, too: SELECT Machine.Name, IFNULL(Daily.Quality,'10') AS Quality, IFNULL(Daily.Down, 'False') AS Down FROM Machine LEFT JOIN Daily ON Machine.ID=Daily.ID AND Daily.Date='2015-07-22'; Basically, we're telling the query engine to filter the `Daily` table before the JOIN happens, and since it's before the JOIN happens, we don't have to worry about NULLs yet. Now, this only works if `Daily.Date` is a NOT NULL field. If the field legitimately has NULL values, you'd still have to handle them. For that reason, I'd generally use the WHERE clause method above. However, this method can perform better on very large tables because the query engine has to do less work since we're filtering the table before the engine has to join all the records. This is especially true if there's an index on all the fields in question.
sadly, there is only one upvote i can give here what a wonderful, detailed explanation
Fascinating - will play with this today.
Lol, why would you hate people caring enough to try to educate you, or at least open a dialogue to further your skills...
It's basically just an extension to GROUP BY: SELECT a, b, c, SUM ( &lt;expression&gt; ) FROM T GROUP BY ROLLUP (a,b,c); Have a read through the msdn article here: https://msdn.microsoft.com/query/dev10.query?appId=Dev10IDEF1&amp;l=EN-US&amp;k=k(ROLLUP_TSQL);k(SQL12.SWB.TSQLRESULTS.F1);k(SQL12.SWB.TSQLQUERY.F1);k(MISCELLANEOUSFILESPROJECT);k(DevLang-TSQL)&amp;rd=true
Once you are used to writing SQL, just seeing the view designer will make you shiver. I highly recommend not using the clicky clicky parts of SSMS for doing that kind of work. Its still usefull to let it script out stuff for you sure, but I'd highly recommend to get used to the syntax and keywords yourself. In the long run you are so much better off I can't find a methapore
Why? Personally I'd check the disk space they take or have left over, as this is a good metric to monitor in terms of server health and scaling. Is there a reason to monitor this information from the database?
yes, i did, what's a OLE DB accessor
What are you using to interact with Teradata? Many programs have Excel support. Teradata Studio Express is free and can export to Excel. I haven't used that feature myself, but when I export using Toad it respects data types.
You're close, but you got your convert a little wrong: `convert(varchar(7), dateadd(m,-3,at_datelastlimitchange),120)` You need 7 characters (one for the hyphen) and you want format 120, not 101. 101 is mm/dd/yyyy.
In this case you'd want it to be distinct.
If I do something that is not exactly defined, I want there to be errors. I want any potential ambiguity to cause immediate failure so that the problem can be fixed at the earliest juncture. This all goes doubly for a database engine. The last thing I want my database engine to do is "just deal". 
* DBCC CHECKDB integrity checks on your production DBs and restored backups * Index fragmentation * Unused indexes * Disk free space * Database growth * sys.dm_os_wait_stats * Server configuration settings * Failed login attempts * Last restart time EDIT: Jesus, is this how the formatting came out? Sorry, I submitted it from the iPad. Formatted.
Well it's a little academic considering your SQL doesn't work. 
RDBMS' aren't meant to be about being as easy to use as possible, they're about as being as *correct* as possible. Databases aren't merely data stores - They're highly sophisticated systems with a great deal of complexity, all of which is necessary in order to *guarantee* the [integrity](https://en.wikipedia.org/wiki/Data_integrity) and [durability](https://en.wikipedia.org/wiki/Durability_\(database_systems\)) of your data. As an Oracle DBA, I feel obliged to answer your rhetorical Oracle question: &gt; "It limits the amount of ('items', 'like', 'this',) you can put in an "in" statement to 1000. Why?" Because IN isn't meant to be used in that way, put the data in a table and use an inner join! Tables are for storing data, queries aren't. Dynamic SQL like this is going to result in a hard parse each time you execute variations of the query, so you're going to get awful performance because hard parsing is an expensive operation (and this applies to all RDBMS'!).
It doesn't work in Access. It works perfectly fine in SQL.
Well, it makes logical assumptions about what you want that are logical 99% of the time. IE if it sees a number 9.9999384723E5 .... it will parse that as scientific notation, rather than think an E was randomly inserted in the middle of a integer. This is common sense. Also, MS SQL is inconsistent .... like I said, it takes ISO time, and RANDOMLY either appends a "z" (this stands for GMT time) or a +0:00 (this ALSO stand for GMT time, in some symbolic but non-uniform way). The inputs are exactly the same, and I don't feel like wasting two or more hours of my life to figure out some dog dung code that is causing this behavior in MS SQL. Let the "architect" deal with what errors may come. Don't hold my hand and say "it's dangerous to have two queries update at the same time!" --- Uh, let me figure that out. If I want to go 75 MPH in a 60 zone, I'll take that risk.
There are many situations where two or more simultaneous updates on a data table will not affect data integrity whatsoever, given certain parameters about the update data and the table data. AKA if you are updating based on the primary key, and there are no duplicate primary keys in the update data (or the records or identical) --- then there is no danger for data integrity. The only difference is that it's executed twice (or up to 100 times, or more) as fast. &gt;If you rely on implicit conversions on that.... you deserve a paddeling. Eh, to be honest, my 3rd party ETL software is taking decimals and spitting out that scientific garbage ... and MS SQL won't convert it back .. yeah I guess that's my fault, but shit. As for the time, "2015-07-22T10:14:28Z" &lt;---- This bamboozles MS SQL greatly. MySQL can handle that as a datetime just fine. No, I don't like that format but a lot of applications do. And MS SQL will only accept that as a string (yes I can parse it with Java or JavaScript or whatever, but for fuck's sake) .... and then will randomly decide to keep the Z, or half-ass convert it to 2015-07-22T10:14:28 +0.00 .... FFS. Just accept this goddamned extremely common time format, MS-hole SQL. Look, there's something called overengineering and patched-up fuckery and that is MS SQL. It's protections and limitations and paternalism simply waste hours of your life.... while MySQL continues to make me money and "just work" for me just fine. My data integrity, scalability, performance, monitoring, error catching are just fine in MySQL. MS SQL? Fuck that.
Sometimes there's a situation where you receive an arbitrary list of 3,000 items in some Excel document in an email. Quickly inserting that list in an "in" statement is far easier than creating a table on the fly only to delete later, but eh. Oracle accepts (list of 1000) and (list of 1000) and (list of 1000) in any case, so I don't see the big problem, but eh.
oh why do I even bother....... &gt;The only difference is that it's executed twice (or up to 100 times, or more) as fast. &gt;and there are no duplicate primary keys in the update data there can't thats why its a primary key. SQL Server has lock escalation, for a very good reason (memory consumtion on large scale updates). If you are updating a single row, you will quite surely end up with a row lock, which will not block any other updates on the table. So we are fine on that front. If you are running inserts on hundreds of threads on a single table at the same time, you will run into latch contention, which is pretty much the "assign the next value in the identity", and assign "page in file to new row" to each insert. That is normal, and can be planed for and delt with. &gt;Eh, to be honest, my 3rd party ETL software is taking decimals and spitting out that scientific garbage ... and MS SQL won't convert it back .. yeah I guess that's my fault, but shit. it's your own fault to just assume that every piece of software out there treats strings the exact same way. This assumption is wrong, not SQL Server is wrong, you are wrong in your expectation of SQL Server devining your intentions, without your specifiying them. &gt;As for the time, "2015-07-22T10:14:28Z" &lt;---- This bamboozles MS SQL greatly. MySQL can handle that as a datetime just fine. Oh god damn, I'll repeat myself, if you try to convert god damn strings, to god damn datetimes, without giving the converting function the format, you should be beaten. You should be beaten long time, since you are assuming that the rest of the world just assumes the same damn format that you are feeding it. you did not do your job there ! &gt;No, I don't like that format but a lot of applications do. And MS SQL will only accept that as a string (yes I can parse it with Java or JavaScript or whatever, but for fuck's sake) .... and then will randomly decide to keep the Z, or half-ass convert it to 2015-07-22T10:14:28 +0.00 .... FFS. Just accept this goddamned extremely common time format, MS-hole SQL. Oh for fucks sake. MSSQL does not save dates as a god damn string. Nothing does ! Its a numeric offset based on a epoch. If you think that the SSMS grid view representation of a datetime datatype is what SQL Server has as actual data, you should never be allowed to touch a keyboard again. The string representation is depending on your local culture settings! If you do time conversion, do god damn specify the format. The convert(datime, 'whatever', XXX) has the XXX parameter to SPECIFY the format ! &gt; FFS. Just accept this goddamned extremely common time format, MS-hole SQL. ISO 8601, thats what I want as a string for datetimes. If not that, I want the damn definition of the format. Be it a String, a HEX, a Binary, I don't really care, I just want the SPECIFICATION &gt;Look, there's something called overengineering and patched-up fuckery and that is MS SQL. It's protections and limitations and paternalism simply waste hours of your life.... while MySQL continues to make me money and "just work" for me just fine. My data integrity, scalability, performance, monitoring, error catching are just fine in MySQL. MS SQL? Fuck that. There is something like php monkeys should not be allowed to touch data as well. Btw, what currently makes you money, is the very thing that makes me money, a couple of years down the road, when what you did falls on its face and dies, and I get hired to fix it.
Ah OK.
&gt; If you specify a format that does not conform to the string you are feeding it nah the convert function takes a known datetime and converts it TO whatever you specify. The third argument is the format it converts TO. MS SQL doesn't recognize ISO 8601 as a datetime, even though it's extremely commonly used in JSON and XML applications. Why? It's a sack of shit, that's why. Look, the difference is simple. MySQL says drive as fast as you want, know the risks, be careful. MS SQL says for your own protection, we are limiting your car to 20 MPH. It also has a vocabulary the size of an 8 year old with down syndrome. Shit. I can make more money in far less time using MySQL than that turd life-waster MS SQL. That's why the former is used 10,000x as often as the latter in the real world.
Yeah I prefer that way. Now you know that you can group by (all) if you desire to. I'd rather that than ridiculous errors like "you can't make a cluster index because the data is non-deterministic" ---- okay Socrates. I'll learn all the cobwebs of your program on the 12th of Never b/c I have real shit to do rather than chasing rabbit holes.
Thanks, that's a good list.
Use temporary tables, you can even have them persist the rowset for the lifetime of your session in case you need to reuse the data for other queries The problem is that your large query with all its IN criteria is not only going to run slowly, it's also going to evict other queries from the library cache - So you're affecting the performance of other queries too because all those queries evicted from the library cache are going to have to be hard parsed again next time they're executed!
Declare an int at the top (@affected_rows or something). After each statement: SET @affected_rows = @affected_rows + @@ROWCOUNT;
MySQL forgives many sins, but it also breaks many rules too, it has bastardised the SQL standards in so many places. Your comparisons are like saying "I have an old car and I can run it on leaded petrol, unleaded petrol, diesel, ethanol. It's much better than this fussy Ferrari which is restricted to only running on unleaded petrol". No. MySQL doesn't require years of study to master because it allows poor discipline. That isn't a good thing, as that poor discipline doesn't transfer to other RDBMS.
If you utilize maintenance plans within SSMS there is a 'Verify Backup Integrity' option that runs a VERIFYONLY RESTORE. One of those options that defaults to off for legacy reasons but has few reasons not to be enabled; similar to backup compression. 
Once a week is fairly often, even once a month would put you better shape than the vast majority. *(Also makes it more realistic for developers who might be attempting changes.)*
Use case for me when I used this approach is when I needed to decide how much disk space I will need on staging , qa and production. Staging and QA are the instances where you keep resources on tight leash and if your application is e.g. a data analysis app can have huge db size consuming good amount of disk. What happens over time is if you keep on editing your DB or delete some older DB in MySQL case, because we generally do not delete the binary or bin log file the disk space consumed and the real size of DB can have different stats. 
Zip Code is connected to a person but the issue is the double counting with the block codes. Thanks!
I should also mention, we're looking for total counts, not at the individual level. So the overall count for each zip code and the count of how many from 30-54 are in each zip code, not the individual level. 
Seconded. Especially 2) &amp; 4)
sigh ...you are wrong. CONVERT ( data_type [ ( length ) ] , expression [ , style ] ) convert is used for typecasting from anything to anything. As far as MSSQL not recognizing ISO 8601... read the documentation. https://msdn.microsoft.com/en-us/library/ms187928.aspx that would be convert(datetime, '2012-11-07T18:26:20', 126). Why? cause its not a sack of shit, you just have no clue how it works. Mysql is used more often because MSSQL and Oracle cost a lot of money, and cause people like you. Postgree is moving in to claim the opensource DBMS throne at some point thou, so you might actually have to learn what datatypes are. 
&gt;all the nurses with very little technical skills get all the glory, promotions, etc. Just started at a health insurance company, can confirm.
Do you just need to add the zip into your JOIN to block codes then?
Well, it appears I misread what the 3rd argument means. &gt;Is an integer expression that specifies how the CONVERT function is to translate expression. Whoever wrote this clearly has downs. It's vague and actually implies it's the format converted TO, not FROM. It also will change the output (to make it appear like it's a TO) with certain combinations. IE put in a ODBC canonical, use 112 (ISO) --- it will output an ISO. Nevertheless, MS SQL sucks balls. Again, sounds like you're a guy who probably likes to overengineer shit. If it ain't broke, don't fix it.
Or maybe insert the query name and row counts into a temp table.
&gt;Well, it appears I misread what the 3rd argument means. reading the documentation is a widly used aproach to counter that issue. &gt; Whoever wrote this clearly has downs. It's vague and actually implies it's the format converted TO, not FROM. Wrong yet again, you really got a talent for that. When you convert a date to a string, the resulting string representation will follow the format you specify. When converting a string to a date, the format will tell the service how to parse the string in order to convert it to a date. You do know what datatypes are don't you? &gt;Nevertheless, MS SQL sucks balls. Running out of ideas are we? &gt;If it ain't broke, don't fix it. You and me have very different definitions of the word "broken"
A better analogy is --- here's a simple vehicle, a Ford Fusion --- it will get you from A to B, quickly, is reliable, is easily repaired, and has lots of options. Hell, put monster truck tires on it, speed with it, beat the shit outta the brakes, go nuts. Then there's Microsoft Ferrari. It can only be driven using two steering wheels at once. The gas pedal is actually in the back seat for optimization and security reasons. When it breaks down, frequently, you'll have to order parts from Italy, and study the car manual -- which is in German. It will not let you go above 20 MPH, as that is a security risk. You can't just hit the gas, or breaks, and go ... no, you must input AN EXACT DECIMAL speed, (which must be deterministic at ignition time). Under certain optimal conditions, it will get you from A to B faster than the Fusion (provided the trip is across several continents).
Yes but pasting a list inside is faster than creating a temporary table and inner joining. I can simply perform that task faster than you. I, not the computer, will determine if I want to fry the database. and we're not talking about 1 million items, we're talking about a few thousand.
its pretty easy in excel too.. if your names were in Column A, you can use this ="'"&amp;A1&amp;"'"&amp; "," copy it all the way down, just paste it in your query. make sure you remove the comma from the very last one tho. 
1a) gonna disagree with your disagreement 1b) if humans read lower case more easily, then the *important* parts of the sql statement should be lower case -- the identifiers 1c) maybe your tools, but not mine 1d) maybe your query interfaces, but not mine 2) agree 3) agree strongly 4) adj_totally verb_disagree prep_with pronoun_you adv_here... how's your comprehension on that?
&gt; Yes but pasting a list inside is faster than creating a temporary table and inner joining. No it isn't once you set it up (2 minutes). In Oracle I have an external table, mapped to IDs and a certain filename in an Oracle Dir. All I do is paste the Excel CSV into my Oracle Dir, it is automatically a queryable table, instantly. No pasting, completely extensible, no limitations. I simply run this..... select * from A_TABLE where ID in (select SOME_ID from T_CSV_TABLE) / &gt;I can simply perform that task faster than you. I guarantee you can't. Pasting a file into a directory is quicker than pasting a whole load of predicates into an IDE and formatting with commas etc. You have to change your query, I don't. &gt; I, not the computer, will determine if I want to fry the database. and we're not talking about 1 million items, we're talking about a few thousand. That statement is so naive it made me laugh. You know it's a few thousand, but how does the DB know?! You don't understand good practices at all. 
www.poorsql.com
SQL is such an old language (1974) that when it was conceived, most keyboards didn't have lowercase letters, which is why the original standards used uppercase. The original SQL language documentation simply reflected the technology of the time. OK, I agree that systems like SQL Server support Unicode naming, so the 21st century SQL standards probably now reflect that, but the original standards were uppercase, which is why the original DBs like Oracle, DB2, et al. take this approach and why standard SQL is case insensitive (because upper was the default and there are less issues with it). The bit I was thinking on was here.... SQL 92 (section 5.2.13): 13)A &lt;regular identifier&gt; and a &lt;delimited identifier&gt; are equivalent if the &lt;identifier body&gt; of the &lt;regular identifier&gt; (**with every letter that is a lower-case letter replaced by the equivalent upper-case letter or letters**) **Also from Celko's book....** "In the original SQL standard, all letters had to be uppercase Latin, so there were only 26 choices...." "Case-sensitivity rules vary from product to product. **Standard SQL,** IBM, and Oracle will convert regular identifiers to uppercase... " Whilst I respect Celko, I disagree with his reasoning. I write software in several languages and all are based on predominantly lowercase keywords (some camel case, but mostly lower) as syntax is better read lowercase. It aids quick scanning, comprehension and absorption. If I'm working in PL/SQL on packages with 30,000 lines of code I can scan, read, comprehend and absorb code with lowercase keywords far quicker than uppercase. Anyway, thanks for the debate, very interesting.
Blech. I dislike you're style for joins. It's just a jumbled mess that you have to read line by line to figure out. My preferred style puts each table on it's own line with the all the join predicates vertically aligned. The join keyword gets its own line so it's easy to see the type of join at a glance. This makes my queries pretty tall but the way everything lines up, it's really easy to pick out the gist of what's going on at a glance (what tables are involved and how are they joined). Also, I hate underscores (they're awkward to type). Camel case FTW. A quick example: SELECT ST.FirstColumn , ATT.SomeOtherColumn FROM dbo.SomeTable ST INNER JOIN dbo.SomeOtherTable SOT ON SOT.SomeTableId = ST.SomeTableId LEFT OUTER JOIN dbo.AThirdTable ATT ON ATT.SomeOtherTableId = SOT.SomeOtherTableId AND ATT.SomeTableId = ST.SomeTableId WHERE ST.BlahBlahBlah IN ('abc', 'def') OR ( ST.BlahBlahBlah = 'xyz' AND SOT.BloopBleepBlop = 2 ) A far as naming, everything is a table by default. Indexes and keys get prefixes (IX\_, PK\_, FK\_, etc.). Views get the world "View" appended to the name. Stored procedures are generally named after the main table they work against then and underscore and a verb or short phrase indicating what they do. I know nothing incites a religious war more than coding styles and that I'm sure many people will tell me I'm a horrible person for writing my SQL like this. I like it though. It's sort evolved naturally out the last 15 or so years I've been writing this stuff. I work in the healthcare world and I swear no other industry has such a propensity to produce data structures that require such complex queries to answer completely mundane questions. Anyway, I'm curious what people think. People I work with seem to like reading it but dislike writing it. I've been doing it that way for so long I don't even have to think about it.
This is my tool of choice. Simple, readable and effective .
I don't mind your style, it's reasoned and logical, but to me it's just far too "vertical". It's fine with 2 tables and simple joins, but if I applied that to the Enterprise systems I work on (where some queries are extremely complicated, and join 10s of tables) I would be constantly PAGE-UP and PAGE-DOWNing. Anyway, aesthetics and pragmatism are often in conflict, huh!
Redgate had a tool that's customizable to do this. I don't recall if this specifically is the default configuration. Edit: Redgate [SQL Prompt](http://www.red-gate.com/products/sql-development/sql-prompt/) worked pretty well for me when I needed an auto-formatting solution. You could tweak the settings fairly easily to satisfy your preferences. SQL Server only (?) 
I can appreciate the "river", as he puts it, as a concept, but this is painful to read when trying to make sense of actual indentation after ifs, begins, etc.
Apexsql has refactor. [http://www.apexsql.com/free/](http://www.apexsql.com/free/)
 SELECT Z.Zip , SUM(Z.Pct2534/100.0Z.Popl)/2 USPct3034 , SUM(Z.Pct3544/100.0Z.Popl)USPct3544 , SUM(Z.Pct4554/100.0*Z.Popl)USPct4554 FROM (SELECT B.CE21GZIP_Zip AS Zip , B.CE21GZP4_Zip4 , AVG(CE21Pop_Population) as Popl , AVG(CE21PP25_PopAge25_34) as Pct2534 , AVG(CE21PP35_PopAge35_44) as Pct3544 , AVG(CE21PP45_PopAge45_54) as Pct4554 FROM dbo.Census_Blktrc A INNER JOIN dbo.Census_StubFile B ON A.CE21GGCO_BlkGrpCd=B.CE21GGCO_BlkGrpCd AND A.CE21GFIS_StateFIPSCd=B.CE21GFIS_StateFIPSCd AND A.CE21GFIC_CountyFIPSCd=B.CE21GFIC_CountyFIPSCd AND A.CE21GTRA_TractCd=B.CE21GTRA_TractCd GROUP BY B.CE21GZIP_Zip, B.CE21GZP4_Zip4) Z GROUP BY Z.Zip ORDER BY Z.Zip Providing some sample data is probably necessary to help you. An initial guess is that you'll need to format your A &amp; B tables to remove the potential for duplication and then JOIN them. https://www.reddit.com/r/tabled/wiki/table-format
I use Dice.com a lot, and my LinkedIn profile. It's funny, I get leads from all over including Az. I apply to some, but mostly recruiters contact me. I have never heard of Upwork/Odesk. You have similar skills to mine, but I gave up on Java/Javascript a while ago. 
I should add, that I started with database design and development back in the late '80s/early 90's. I began concentrating on data warehouses around' 99/2000 with SAP's BW product. Since then, that's been my specialty. 
Dude, you and I are *sooo* alike. I wrote my first SQL professionally in 1988. Usually combined with some kind of front end to display it all. In 2000 I moved to a BI specialty consulting firm using Essbase, Hyperion, MS SQL and Analysis Services. Today, I'm primarily coding in Teradata for my SQL and Essbase for cubes, because that's what the client uses for their DW. And you mentioned above "I'm not a programmer". dude, you're a programmer. I personally have a huge lofty title on my business card but I always tell people, simply, "I program computers". :) 
My work has SAP HANA and I'm the one who gets to play around with SAPUI5 stuff for creating some apps that allow CRUD operations. It uses OData and server-side JavaScript which is a pain the ass to debug. I didn't take the job to do this, but someone needs to on my team and I'm best qualified for it. I thought it would be easier when I took on this task, so I really only have myself to blame.
I did SAP BW until 2005. I got out when I got tired of going to work on an airplane. I lived out of a suitcase for 8 years. At the time there were few SAP BW projects available in Southern California. I then started doing MS SQL server/BI stack. One of the best decisions of my career. 
I do love me some SQL Server. With the exception of reporting services, I'm a fan of everything Microsoft. Once you go Tableau, you don't go back.
Tableau has some great tutorial videos that show you how to do just about every common task. The only downside of Tableau is it doesn't do text based reporting that well, but its not designed for it because it's an interactive visualization tool.
The only other tool I have any experience with for reporting besides SSRS and Tableau is Oracle BI Publisher. Like everything Oracle does, it sucks. ODI, SQL Developer, BI Publisher, I can't stand that whole stack and I will never work with it again.
IN vs EXISTS is one of those situational things. The key difference is that IN processes an entire set of rows once, whereas EXISTS processes that set of rows one at a time. Given that, there are a few clear-cut cases where you'd use one over another. Take this example. Assume that there is NOT an index on phone_number: select * from walmart_employees a where exists ( select 0 from united_states_citizens b where a.phone_number = b.phone_number ) That's going to suck for you. For every person in the walmart_employees table (a million plus) , the DB is going to do a sequential scan through the us population. Contrast with this: select * from emp a where phone_number in ( select phone_number from united_states_population b ) That's going to do *one* pass through the us population, ideally making a big hash lookup of the whole thing. So in this case you'd prefer the IN. Now assume that SSN IS indexed. This query does one pass through the us population: select * from emp a where SSN in ( select SSN from united_states_population b ) and rownum &lt; 10; But this does an indexed lookup on 10 people: select * from walmart_employees a where exists ( select 0 from united_states_citizens b where a.SSN= b.SSN ) and rownum &lt; 10; So in that case, you'd want to use the EXISTS. In some cases, IN and EXISTS are interchangeable, but sometimes they result in very different performance characteristics. Disclaimer: Modern databases (like compilers) optimize the code you write, so it's some cases the difference I noted will be optimized away.
Say you have something like this in your Products table: ProductID | ProductName | InStock | OnOrder ---|---|----|---- 1 | Name1 | 65 | 50 2 | Name2 | 15 | 5 3 | Name3 | 20 | 10 Your sub query: SELECT OnOrder FROM Products WHERE Instock &lt; 60 would return (5, 10). So your main query would be the same as this: SELECT ProductID, ProductName, InStock, OnOrder FROM Products WHERE InStock &gt; ALL (5, 10) Which would then return all rows of this hypothetical data set, including ProductID 1 where InStock is greater than 65. If you wanted just the rows where InStock was less than 60, but the OnOrder was larger than InStock you would write this: SELECT ProductID, ProductName, InStock, OnOrder FROM Products WHERE InStock &lt; 60 and OnOrder &lt; InStock
With syntax highlighting I don't see a reason to indent main keywords such as FROM, JOIN, and WHERE. They stand out pretty well in the beginning of the line. Is the assumption here that not everyone has syntax highlighting?
Well, SAP BW is a past specialty and I am out of touch with what's been going on in the SAP world. For me I see it as another tool in my toolbox. I guess it would be a stepping stone to where I am today as it gave me the BW knowledge and experience that I needed. 
Most of this is self-consistent and a reasonable style guide [although I disagree with a lot of it for my own personal style]. Being said, there's a few bits that just make me want to reject this entire thing out of hand. &gt; Store ISO-8601 compliant time and date information (YYYY-MM-DD HH:MM:SS.SSSSS). Except there's no TZ in that example. &gt; Avoid Hungarian Notation [...] _num - denotes the field contains any kind of number. [...] _date - denotes a column that contains the date of something. Sigh. &gt; The key should be unique to some degree. *gag*. Anyone coming up with a SQL syle guide should understand why attaching a "mostly" modifier to "unique" is wildly unacceptable
submitted to this very same sub yesterday https://www.reddit.com/r/SQL/comments/3efmn0/sql_style_guide_a_consistent_code_guide_ensuring/
IS DULL
UPDATE jt_jokes SET groan=1 WHERE which_joke='this one'
I prefer TRUNCATE TABLE Jokes
This kills the joke.
Sure. Although because something is difficult doesn't necessarily mean it isn't worthwhile. :)
&gt; break grammatical convention wut? 
It wasn't all that alive to begin with. 
Thanks, personally I'd re-check disk space after dropping the DB; then work from that figure, but at least we both know what each other are doing and why ;)
I'm located in NYC. Can I ask what the role entails and what the qualifications are? Thanks
Thank you. Since I will not have any relevant prior work experience, what do you think would be the best way to show employers that I'm familiar with SQL so I can obtain my first job? 
Anyone?
Thanks sephv1 - I was able to make use of your approach. 
Wrong.
$mail_I'd
Sure, I sent you a PM.
&gt; I mean, the code format is correct, right? no it isn't -- see my other reply 
you generated that code **wrong** this is your code -- INSERT INTO 'my_database' . 'new_table' this is what it should be -- INSERT INTO `my_database` . `new_table` those are backticks, not single quotes however, you should really be coding like this -- INSERT INTO my_database.new_table "escaping" identifier names with backticks should not be necessary if you do not use spaces or special characters inside the identifier names, *which you shouldn't* also, don't put spaces between the database name, the dot, and the table name
I am sorry. I miss typed it. Thanks though 
I have about 15 years BI and over 25 years of database development experience. I am on Dice, glassdoor, LinkedIn, and a few others, I have never used a freelance site, most of my work comes from recruiters finding me. 
I am sorely disappointed that I had to scroll to the bottom, the last of five replies, before anyone mentioned the SQL injection this code is exposed to *and* how to avoid it. Good on ya. Wish I could give you enough upvotes to put you on top of the tread.
Hey, thanks for the concern. I am aware of SQL injection. I am just making very basic thing here. So, I didn't bother about that far, yet. Anyway, thanks for reminding me. :)
I am aware of SQL injection. I didn't bother about it intensionally. But wait, if I understand you correctly, mysql_error returns any error that I make in my sql code? Is that right?
&gt; I am aware of SQL injection. I didn't bother about it intensionally. Well, that's just foolish and I pity the people who have to use and maintain your code later. &gt; mysql_error returns any error that I make in my sql code Yes, it returns errors from the mysql server directly; they usually include the part of the query that was wrong as well.
Ohh.. I guess, you are right. Alright.. I ll consider it. I am newbie and my tutorial provider mentioned it right at the beginning means, it really matters. Thank you. But one thing.. Why post isn't working and get is working?? What's up with that.. Please help me with that...!!!
Ohh.. I am sorry. I was ignorant about SQL injection. But now I realized how important it is even for newbies. It's part of learning, I guess. I ll take your advise. And it's cool about mysql_error() thing. One more thing.. Why my code isn't working for method post.. But works fine with get method.. Please help me with that too.. Thank you so much for your time..!
Use a matrix instead of a table. Turn your dataset into a tall, narrow dataset with a column for your DateTime variable, a column to describe the parameter (ParmA, ParmB, ect), and a column to contain the value.
Have you considered using a batch file to move the files one by one and call the procedure after each move using sqlcmd?
I've seen it around, but was hoping to avoid the batch move, and just stick to the xp_cmdshell. Let me say that I'm not generally opposed, just a little nervous about it, I'm hoping there's a way I can do it without it.
I would just use the xp_cmdshell function. I don't even think you'd have to get too creative. You can pretty much do anything there that you can through the command prompt... even call a batch file as the previous comment suggested. Either way, I would do the best I could to keep the entire process somehow contained within the one proc, or if it's more than one, create one that calls them all. 
This is the proper execution
like you would see here https://www.elance.com/ for example someone may say I need 20 dts packages converted to ssis then people message their estimates and resume etc. Usually these are hourly tasks, I think this is what op is looking for. Or another example would be "I need a postgres database converted to mssql, message me how long and how much" you get the idea. What you are referring to I would personally consider Contract work.
I think you should avoid regressing in technology. I have been progressing with technology in my field the last 10 years but not enough to quite keep up with the actual technology itself. I feel like I'm missing out and will eventually have to invest some time in making up the difference. Don't put yourself in such a position to have to play catch-up in the future.
&gt;Offered Job: On sql server 2000.. You can stop right there. As someone who works on many clients a day, when they say they're eventually moving to new versions, that could mean weeks or years. Some clients get stuck with an app out of development and no money to develop a new one. No use getting experience with 2000, not really marketable experience in the field. 
You're fucking rude. You think you know everything. And are in complete denial over the fact that every other commenter in this thread has corrected everything you said. 
Haha it's really not easier than creating a temp table. Have you ever created a temp table before? It would require like 1 more line of code than your random list of 1000 values. What are these values? If you're looking for matches or mismatches of something so long and obscure, why isn't it stored somewhere in the DB in the first place? 
I'm using a tablix right now. Here's a general idea of what the report should look like if the end-user selects 3 parameters. [I'm new to the reddit thing so the formating below may not work right] Date/Time ParmA ParmB ParmC ------------------ -------- -------- -------- 01/01/15 00:00 10.2 1.8 37.4 01/01/15 01:00 10.1 1.9 41.3 ------------------ -------- --------- ------- So, what i've done in the past is my dataset consisted of the following: Date/Time, Parm, Value I then was able to column group it by parm and row group it by the date/time. Is there a way to do this same thing with my new dataset? Perhaps a post query processing or can I have a query run through a stored procedure that returns a dataset that is like the old dataset I had been using? 
i would do: select * from ( select u.uid, u.uname, u.ulname, u.ufname ,(select top 1 cast(serverlogintime as smalldatetime) from userlogs where userlogs.uid = u.uid order by cast(ServerLoginTime as smalldatetime) desc)) as LastLoginTime from users as u ) as q1 where q1.LastLoginTime &lt;= DATEADD(month, -1, GETDATE())
This is only returning 1 record for some reason. It should be a little over a 30 (based off manually matching users to last logon dates).
Well basic conversion from text to variable could probably be accomplished with ascii(). That's not going to be the short solution you're looking for. You'd probably have to build a custom translation function for that.
Ah okay. Beyond my SQL skills. Thank you!
as valid a reply as any other you're going to get based on the dbms you identified -- use the SUBSTRING_INDEX() function
You're absolutely right. Work is just crushing me atm, and I just submitted this without even thinking about providing y'all info. My bad. I'm going to try the solution the other commenter listed in the stack overflow link.
Try googling for "sample csv data."
http://www.mockaroo.com/ is good for making your own data sets https://archive.ics.uci.edu/ml/datasets.html has some real-world ones Learn how to import CSVs into your db and you're good to go :)
You could take a look at r/datasets.
Surprised nobody has mentioned the [Adventure Works](https://msdn.microsoft.com/en-us/library/aa992075.aspx) database. It's essentially a sample production database that is amazing for learning everything from SELECTs to Execution Plans; has 200MB~ of pre-populated data. Many of the online learning videos for SQL Server will utilize or reference it at least a few times. *Note: This is for SQL Server, you can get the [Express Edition Here](https://www.microsoft.com/en-ca/download/details.aspx?id=42299)*.
This is something I use (sparingly) with SQL Server that converts a delimited string into multiple rows. *** CREATE FUNCTION [dbo].[udf_List2Table] ( @List VARCHAR(MAX), @Delim CHAR ) RETURNS @ParsedList TABLE (item VARCHAR(MAX)) AS BEGIN -- Parses a string @List over the delimiter @Delim to return a table with the values seperated -- Usage Example: SELECT item FROM dbo.udf_List2Table('first,second,third', ',') DECLARE @item VARCHAR(MAX), @Pos INT SET @List = LTRIM(RTRIM(@List))+ @Delim SET @Pos = CHARINDEX(@Delim, @List, 1) WHILE @Pos &gt; 0 BEGIN SET @item = LTRIM(RTRIM(LEFT(@List, @Pos - 1))) IF @item &lt;&gt; '' BEGIN INSERT INTO @ParsedList (item) VALUES (CAST(@item AS VARCHAR(MAX))) END SET @List = RIGHT(@List, LEN(@List) - @Pos) SET @Pos = CHARINDEX(@Delim, @List, 1) END RETURN END *** Notes: * This is a table-valued function, utilizing it on a large scale isn't recommended * If the data sizes / max length for @List &amp; @item are known they should be defined instead of MAX * Querying for your situation would be: *** SELECT item FROM udf_List2Table('first|second|third', '|') ***
Try it with a `select` and find out.
thanks! i cant believe i didnt think of that earlier.
&gt; why isn't it stored somewhere in the DB in the first place? Have you ever worked at a large Fortune 500? This question practically answers itself.
WHERE Var1 IN (1,2,3) OR Var2 IN (1,2,3) This should get it done.
lol nice work buddy. troll grade = A+
https://www.quandl.com/ has a lot of good stuff.
Technically, you want this: WHERE EXISTS ( SELECT v FROM (VALUES (Var1), (Var2)) t(v) INTERSECT SELECT v FROM (VALUES (1), (2), (3)) t(v) ) Or this: WHERE EXISTS ( SELECT 1 FROM (VALUES (Var1), (Var2)) t1(v) JOIN (VALUES (1), (2), (3)) t2(v) ON t1.v = t2.v ) Of course, you might find more concise syntax for your particular problem, such as WHERE Var1 IN (1, 2, 3) OR Var2 IN (1, 2, 3) But this stops being concise if you have tons of vars and tons of `IN` list values.
&gt; you'll find that mysql will accept it (but produce erroneous results) Hopefully, this will soon be a thing of the past: http://bugs.mysql.com/bug.php?id=75896
 WHERE 1 IN (Var1,Var2) OR 2 IN (Var1,Var2) OR 3 IN (Var1,Var2) 
Yes, mathematically it's a [multiset](https://en.wikipedia.org/wiki/Multiset) intersect. Oracle can handle this type of predicate scenario correctly with its multiset operators: create or replace type TNumberList as table of number; / select * from ATABLE where TNumberList(VAR1, VAR2) multiset intersect TNumberList(1, 2, 3) is not empty / However, in my opinion, multiple "in" statements are still preferable.
Nice idea, I keep forgetting about this feature
MySQL is beyond evil though. It'll allow anything, it's the slut of SQL.
New favorite sub!
&gt; r/datasets did not know that existed! thanks!
In a way, yes, intersect and equi-join are similar as only those tuples are retained that are present in both sets. But unlike intersect, join is based on a cartesian product between the two sets, not a simple concatenation of the sets, which means that the resulting tuples of a join will contain columns from both joined sets. So, a join can "intersect" two sets of different type, whereas intersection can "join" only two sets of the same type. In this particular example, none of this is relevant, and a join solution would have worked just the same. I'll add one to my answer. Does that make sense? :)
I'll add for format changes you can do CAST(recievedDate as DateTime) as RecievedDate, CAST(CASE WHEN ISNUMERIC(Amount) =1 THEN Amount ELSE NULL AS INT) as Amount
If it's **only** the names that have to change, create a view that renames the columns via aliases and query the view instead of the table. `select col1 as newcol1, col2 as newcol2`
 select cast(col1 as &lt;new datatype&gt;) as veld1 , col2 as veld2 into NewTable from OldTable 
You'll want to ultizing a [binomial proportion confidence internval](https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval). Link to an implemented SQL solution [here](http://www.evanmiller.org/how-not-to-sort-by-average-rating.html): *** SELECT widget_id, ((positive + 1.9208) / (positive + negative) - 1.96 * SQRT((positive * negative) / (positive + negative) + 0.9604) / (positive + negative)) / (1 + 3.8416 / (positive + negative)) AS ci_lower_bound FROM widgets WHERE positive + negative &gt; 0 ORDER BY ci_lower_bound DESC; *** Looks complicated, but really isn't once you read what it's doing. You might have to do a bit of tweaking to fit that data, but I'm fairly confident that this is the correct direction. 
one thing I'm not sure of: the sp is setting the read transaction level to uncommitted. My understanding of this is that is performs everything with (nolock). What i'm not sure of - because it is executing it as parameterized sql - does that transaction level need to be articulated inside each instance of @Query ?
Thanks, I used something similar to this, due to using Azure I couldn't use SELECT INTO but this link helped for whoever it may help in the future - http://azure.microsoft.com/blog/2010/05/04/select-into-with-sql-azure/
Several questions I would ask : Why is this procedure using dynamic SQL? It probably means the optimizer may have problems with the explain plans for the queries. Your problem may be plan caching. With many users using the system, I expect there isn't enough memory to cache the plans for these tables and they are falling out of the cache.
&gt; any practical reason you'd ever need to have an order by in a view creation Sure. You're using a materialized view (for speed purposes - it's frequently queried but infrequently updated). You want the default behavior to be a certain order for all queries. Also, MySQL allows it. &gt;reorder columns in a table Turns out UI and usability and the principles of Gestalt psychology remain outside of a customer viewing a front-end webpage. No, turns out these principles affect developer productivity as well. I want a default order. Simple. MySQL allows it - is the open source community comprised of idiots over there by your logic? I am not belittling SQL or databases. I enjoy them very much. However, the rationality of MySQL is far greater than MS SQL. One of these reasons is intuitiveness and flexibility. Positive flexibility. The reason why your car has the capability to go 120+ MPH even though usually it may be a bad idea.
&gt; one thing I'm not sure of: the sp is setting the read transaction level to uncommitted. My understanding of this is that is performs everything with (nolock). Yes it is. It's also performing reading dirty reads. Unless you legitimately need to avoid deadlocks more than you need accurate results, this seems kind of questionable. There's a certain set of SQL devs who put `with (nolock)` on everything they do everywhere whether they need it or not. It's suspicious coding behavior -- although I can understand it for a working queue -- but it should not cause performance to degrade. &gt; What i'm not sure of - because it is executing it as parameterized sql - does that transaction level need to be articulated inside each instance of @Query ? Again, I don't think this is related to your issue, but `sp_executesql` inherits the current transaction isolation level. You can verify that with this: SET TRANSACTION ISOLATION LEVEL READ COMMITTED; DBCC USEROPTIONS; EXECUTE sp_executesql @stmt = N'DBCC USEROPTIONS;'; GO SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED; DBCC USEROPTIONS; EXECUTE sp_executesql @stmt = N'DBCC USEROPTIONS;'; GO SET TRANSACTION ISOLATION LEVEL READ COMMITTED; DBCC USEROPTIONS; EXECUTE sp_executesql @stmt = N'DBCC USEROPTIONS;'; GO Besides, it looks like the queries generally specify `with (nolock)` everywhere already. &gt; This stored procedure is part of an application we pay for. Contact the vendor immediately. This is why you pay for the service contract. The absolute last thing *you* should be doing is debugging a stored procedure *they* wrote. &gt; One of the weird things, when I execute the stored procedure directly from sql (with the same paramaters from my trace) it takes 3 seconds. When the same sp is executed from the application, it takes around 5 minutes. This is not that uncommon. The typical answer you'll see describing this issue is "parameter sniffing" or "poor cached query plan". [Here](http://www.sommarskog.se/query-plan-mysteries.html) is an in-depth discussion of the issue. The common reason SSMS performs differently than the application is that SSMS sends a number of configurations when it connects. It sends `SET ARITHABORT ON;` for example, and that's not on by default so unless your application specifies it it won't be used. That means SSMS and your application will very likely use different query plans entirely. This stored procedure also relies heavily on tempdb. It's possible, although it would not be my first thing to investigate, that tempdb contention is to blame. If you were seeing more consistently poor performance then I would look more at this, but what you describe here sounds more like the parameter sniffing/poor query plan problem. Are you using [multiple tempdb files like MS generally recommends](https://technet.microsoft.com/en-US/library/ms175527\(v=SQL.105\).aspx), etc. 
You're right about dirty reads. It's not something we find to be much of an issue - because the database is so poorly designed. Each patient record has its own row, and normally just one (with a gazillion columns). The silver lining of this is that it is very rare to have more than one user/process that will try and manipulate the same row of data.
In MSSQL: If you put TableName, it will create a permanent table. If you put #TableName, it will create a temporary table. If you put ##TableName, it will create a global temporary table.
(From my experience with MSSQL) What defines a temp table is if you prefix it with a "#". a global temp table is prefixed with "##". So [#tempTable] is a temp table, [tblTable] is a normal table, and [##globalTemp] is a global temp table. A temp table deallocates once your current connection closes. A global temp table can be accessed from any connection but will deallocate once everything using it finishes and the original connection closes. Another way to do it would be to CREATE TABLE, then INSERT INTO VALUES.
&gt; I even had to "improvise" to know how to pull out date, time, etc. under SQL Server. oh well. huh? just SELECT getDate()? 
Did you catch the syntax error in your select? Also, when you use the NOT IN statement, it won't return nulls. In your original post, did you forget a parenthesis to combine your OR conditions in the where clause? SELECT users.uid, users.uname, users.ulname, users.ufname, Max(userlogs.serverlogintime) AS LastLoginTime FROM users INNER JOIN userlogs ON users.uid = userlogs.uid AND (userlogs.userstatus &lt;&gt; 'Login' OR (userlogs.userstatus = 'Logout' AND CAST(userlogs.serverlogintime as DATE) &gt;= cast('2015-06-28' as Date))) I guess i'm a little confused by the logic here. Do you want it to display those whom are not logged in and have a last login time greater than a month ago? Is serverlogintime updated with the most recent time they've logged in? if so, why do you even evaluate Login? In case someone's been logged in for more than a month? Why not just join ON logged out users that have been offline for more than a month instead of excluding logged in?
oh really? I use that online resource w3school so the basic one that I saw was sysdatetime(), had to add days to it then convert what i got to date-only to remove the time component. or something like that. in Access it was something like Date() + 7 but yeah, SQL Server doesn't like adding int to datetime so there's dateadd(). so yeah, while I am learning it from a course that teaches Access SQL there are still some things I have to change to get what I want in SQL Server. I guess that's better since I am learning how to combine stuff to make things work.
If you're having trouble getting stuff to work, don't forget you can CAST variables into other stuff. Be careful though. date formats are annoying.
How many rows are coming back? How is the app reaching out to the DB? Linked server? Is it using openquery or select from the linked server? openquery sends the query command and runs it on the server and only gets back the result. if the app is doing a select from the linked server, ALL the rows are downloaded to the client and then the where and any other bling is done on the client.
Not sure what SQL platform you are on, but is there some sort of equivalent to APPLY? &gt; The APPLY operator allows you to invoke a table-valued function for each row returned by an outer table expression of a query. The table-valued function acts as the right input and the outer table expression acts as the left input. The right input is evaluated for each row from the left input and the rows produced are combined for the final output. The list of columns produced by the APPLY operator is the set of columns in the left input followed by the list of columns returned by the right input. https://technet.microsoft.com/en-US/library/ms175156(v=SQL.105).aspx 
the version of windows being run might have a limit on the number of cores the OS is allowed to use (depending on licensing). Check out the BIOS screens when the system boots. The BIOS will tell you the true story.
2008 R2 looks at cores as cpus I believe. Standard has max of 4. Enterprise has 8. What version of MSSQL?
Does the JDBC work if you use the dns shortname? Could be using it's own internal DNS lookup method.
Interesting thought. I'll try that tomorrow. 
http://www.w3schools.com/sql/default.asp Data goes in buckets and you decide how to mix them back together.
You also need to keep track of numa nodes. 
First off there are many flavours to SQL. So the question your asking is hard to answer because we don't know what flavour of SQL your using...once you tell us this we can help you out further. 
You are looking for MSSQL, free download for Management Studio, code in a window like R, you can import csv files (or any type) into a table in a db. I actually find it easier for analysis work to upsize to sql from either excel or Access, this makes your files interchangeable.
I know we have two NUMA nodes. Does that effect it in some way as well?
You can import the CSV into your database and run queries that way. I know the tutorial you're talking about, he tries to make it easy but I think he overcomplicates things. Download PostgreSQL and pgAdmin3, import your csv (or create some sample data) and run your SQL queries through a button on pgAdmin3. That should be the easiest way, IMO.
SQL generally runs as an application that "waits" for incoming SQL requests, typically on a remote server in professional environments. When you installed the XAMPP stack, it installed many applications related to web servers, including MySQL, which I presume is the flavor of SQL you're learning. If so, I suggest looking for MySQL Workbench in your list of applications, which is the default way to interact with data. As for how to use your CSV, I believe you can use BULK INSERT or the MySQL variant (sorry, I'm mostly a MSSQL guy, so the specifics may be wrong but I've played with MySQL a bit). When you load it into a table, then you can query against it and what not. Hope that helps!
SELECT GETDATE() + 7 will work or you can do DATEADD(DAY, 7, GETDATE())
I was going to suggest this as a joke; I don't actually think it's that effective for a serious learner.
The tutorial focuses on just standard SQL unfortunately so I don't know what flavor to use!
I see, is this what you are talking about? http://www.microsoft.com/en-us/download/details.aspx?id=42299 Thank you!
I see, thank you for your help! Downloading now :)
SQL Server Management Studio Express Edition is what you want. I don't recommend using Access. 
I think it's good for the person who doesn't mean to do database as a career... like... the business analysts of the world, for instance, who kinda need to understand it from a high-level perspective.
Yes I do. I also have "Automatically set I/O affinity mask for all processors" checked.
lol, i've made a good living doing stuff about this. IMO, it's almost impossible for you to properly tune for performance without real data. Recompiling it solved most of the slowness - it's down to a few seconds again. I'm tempted to add with recompile to this one. I may also de-paramaterize it. It also hits the patient_encounter table something like 35 times, so I'll probably pull the information into a temp table and use the temp table for the other 34. We are on 5.8.1.28 Hot Fix 125 KBM 8.3.4.1 - Build 4 Microsoft SQL Server Management Studio 10.50.1617.0
Heh, I just realized that Standard edition supports up to 16 cores or 4 sockets but it depends on your licensing. You must have a 4 core cal on that server. Its $2k a core.
Ahh I didn't even think of that. I'll look into that further. Thanks for your help!
I dunno I bought it as a novelty, but I don't necessarily think it would make a lot of sense if you don't already have a grasp of the concepts .... Maybe I'm underestimating it.
Add trunc(viewdate, 'Q') in the select and group by clause and then pivot it like this but only for quarters http://stackoverflow.com/questions/22207709/oracle-sql-pivot-with-multiple-sum-columns
If the table is the same just dump it to a CSV and import it/loop it. Edit: He is probably just entering them by hand to make it take longer.
Do it the same way as above and just drop it into excel and pivot. You'll need to add the same trunc line to your group by.
I need it to be automated, because it's for a web app/site that I'm developing it for automation within the company
Sorry if I wasn't clear. I didn't mean to add quarter as a field into your data's table. I meant creating a whole new table that would have nothing but calendar dates and all the different attributes a date could have(quarter, fiscal year, day of week, month, etc). I'm used to working in MS/Postgres, but this article looks like it covers how to do it for Oracle. http://www.perpendulum.com/2012/06/calendar-table-script-for-oracle/
A super easy way is to use the [sqlite-manager](https://github.com/lazierthanthou/sqlite-manager) extension for firefox. It lets you create a db, import from cvs, run sql and view results.
[We have a wiki that lists a few.](https://www.reddit.com/r/SQL/wiki/index)
If you're just doing an update using a `MERGE`, then just use `UPDATE` with a `JOIN` instead. http://stackoverflow.com/questions/1604091/update-a-table-using-join-in-sql-server
Thanks friend, so I should just create a temp table out of list, and then compare that list to the table? How do I make the updates based on the case? ohhhh. Apply the case to the temp table, then join the temp table to the table? So like... CREATE TABLE #TempTable Randomid INT INSERT INTO #temptable (1700 word list) SET CASE when dbo.stuff.randomid = 5 then 6 UPDATE dbo.stuff2 FROM dbo.stuff as A INNER JOIN #temptable as B ON a.randomid = b.randomid WHERE my brain hurts 
For the current year: SELECT USER, SUM(CASE WHEN VIEWDATE &gt;= TRUNC(SYSDATE, 'YYYY') AND VIEWDATE &lt; ADD_MONTHS(TRUNC(SYSDATE, 'YYYY'), 3) THEN 1 ELSE 0) Q1, SUM(CASE WHEN VIEWDATE &gt;= ADD_MONTHS(TRUNC(SYSDATE, 'YYYY'), 3) AND VIEWDATE &lt; ADD_MONTHS(TRUNC(SYSDATE, 'YYYY'), 6) THEN 1 ELSE 0) Q2, SUM(CASE WHEN VIEWDATE &gt;= ADD_MONTHS(TRUNC(SYSDATE, 'YYYY'), 6) AND VIEWDATE &lt; ADD_MONTHS(TRUNC(SYSDATE, 'YYYY'), 9) THEN 1 ELSE 0) Q3, SUM(CASE WHEN VIEWDATE &gt;= ADD_MONTHS(TRUNC(SYSDATE, 'YYYY'), 9) AND VIEWDATE &lt; ADD_MONTHS(TRUNC(SYSDATE, 'YYYY'), 12) THEN 1 ELSE 0) Q4 FROM TABLE GROUP BY USER
Think of it in terms of an Excel spreadsheet. You have spreadsheet 1 with a series of column names, and spreadsheet 2 with a series of column names. They are only going to match if they have been designed to match. To answer your original question, you should just be able to do a select from database 1, and insert into database 2. As long as you know where you want the data to go, would take 2 minutes to design the query and 3 seconds to execute for 300 rows. Thing is I have no idea how or where the information is stored in your first system, and where it needs to be stored in the second system (i.e. is it just 1 table, or does it span 20 tables of information?)
Oh yeah, the reason is that SQL is slightly universal for data retrieval from databases and non-databases. So unless you want to be stuck with the data you are fed, you should learn SQL. The strategies may differ, but any ERP system should be a good starting point to learn how to write queries. SQL is largely standard with differences due to implementation. Each database engine has different syntax. Best would have to be the engine your market is popular with.
This makes a lot of sense! I'm currently a Financial Analyst as well and right now ai do exactly what you said.... Copy and paste data from excels and manipulate it using pivots, vlookups and some VBA. SQL queries sound like a huge time saver. I think it's a good language for me to learn then... Any recommendations on websites to start out the learning process?
thanks friend.
/u/anaconda1189 has better answers, but I'm glad i was able to help. He said everything I would say perfectly.
I am a Financial Analyst working on an ERP platform and I also do a lot of ETL. I use SQL more than anything else. That's because the ERP system is a DB with a front-end application that is mainly driven by SQL and PL/SQL. When doing ETL between systems, I use SQL and shell scripts to extract and transform and load into staging tables. I also do a lot of VBA for Excel macro development to make everyone's lives easier. I don't use any PHP or Python, but it could be useful for certain things like parsing JSON data, which some newer systems are now exporting. But to me it is no surprise that any FA position would require SQL, more so for the DML, not so much for DDL. So for your purposes, you should focus on how to write more of the SELECT FROM WHERE statements rather than ALTER TABLE stuff. Definitely learn some! It's an ANSI standard so you can take it with you to any platform. 
I personally am using "Head First SQL" to learn, and I am loving it. It's a book that teaches you the actual language. I started out in access and it is a WYSIWYG program, and that's not for me anymore. So the way I'm learning is I downloaded MySQL and have accompanied it with the book. I'll split off at times to further explore a concept, or maybe find another way to achieve the same result. I would recommend the book, learn the basics, and then get a reference book (I don't have one to recommend) to use as you continue working. 
Financial/data analyst here and knowing SQL is essential, you will not have to rely on others for your extracts, it has kept me employed for over 20 years, quick to learn the basics.
Fantastic and exactly what I needed! Many thanks, much better than MS SQL Server for my use :)
This is going to sound dumb, but can you close the rdl and try it again? I can't think of a reason off the top of my head why it wouldn't be there. 
How are you going to handle the link between inventory and orders? You have a widget product. You have 10 in warehouse A (inventory id 1) and 20 in warehouse B (id 2). A customer places an order for 25 widgets. Are you going to link to both inventory records? You can't without a resolution table. I would not link orders to inventory. I would use a "stock" column on products as a counter for the total inventory quantity, so you know you have 30 widgets in stock. Then I would decrement the quantity in the appropriate warehouses when an order is placed. Alternately, if you need the additional complexity, you can keep an inventory transaction table, and link orders to that (via an order id in the transactions table). Order comes in, put in two inventory transactions, one for 20 from warehouse B, and one for 5 from warehouse A, both with the same order id. This will help you chart the quantity as it goes up and down in the various warehouses. If you null the order id, you can track resupply when you put new products into the warehouse. 
As mentioned below.....Orders need to go with 2 tables (Order Header and Order detail is needed) and possibly an interim table to allocate the items from on-hand so that while you are attempting to fulfil the order, no other processes or order can grab an item before you complete the order.......
Oh, I would force the user to make multiple orders. I just wanted to make a basic inventory management system. If you have an alternative solution that would make things much better feel free to tell me. Like the thing is that the other warehouse could be like in another country, and the cost of delivering product X from A to B could be much higher than delivering product X from C to B. That's why I thought it made much more sense, but if you have a fool-proof design that wouldn't complicate the implementation too much feel free to share.
In the table Product, I would make the product(varchar45), idCategory (int), since i am guessing they are coming from the Category table. I would split Order in two, for this i am going to name them OrderAdmin, and OrderProduct. I am showing you what I would do, because seeing a different way can sometimes make things click into place. OrderAdmin, holds the none product details about the order. idOrderAdmin (int) user_iduser (int) order_status (boolean) orderPrice (numeric(15,2)?) # I have no clue about storing money amounts, but stackoverflow says numeric would be the way to go.) Payment_status (Boolean) Submited (Date) Dilivered (Date) # recipient_Name would be in here Recipient_address (text) Comments (Text) Requested_by (Varchar) #i don't know what this is for OrderProduct idOrderProduct (int) Order_idOrderAdmin (int) Quantity (int) idProduct (int) intentory_idInventory (int) With this, you can have more then one product in an order, which i think will be peoples number one issue when looking at your diagram. good luck with your assignment. *Edit: formatting
It's not an assignment, just a personal project. I got carried by my teammates in uni, so I wanted to do the whole thing by myself.
you(the system) creates an ID, when the order is created.
Hmm, personally I'd have "Location" (or similar) table. Then your Inventory table contains columns for Location, Product and quantity.
As u/gtbaddy mentions, you need both the database server and SSMS, which are available in the SQL Server Server Express with Tools package. [Here]( http://www.microsoft.com/en-us/download/details.aspx?id=22973) is a 2008 version. I'm not sure if a newer one is available. There is also SQL Server Express with Advanced Services, which includes what the with tools package contains, but also has Reporting Services.
Capitalism
What instance name did you give it? You can also have a look at Start -&gt; Run -&gt; cliconfg and enable those too.
Instance name is same as the computer name, which I thought should work fine. edit: I'm an idiot. I had to do CompName\InstanceName
It must be MSSQLSERVER in order to only do (local) or a period. If it's anythign else you must do (local)\INSTANCE-NAME-HERE or of course the period.
UNION ALL will be quicker. This is because union is a distinct set whereas adding all will take all rows as they are without filtering duplicates. Is this the best solution ever? Probably not, but we'd need a lot more information about the use case and examples of the query being ran. 
You could create a temp table then query off that and drop the table at the end. 
Because I found it by clicking through a bunch of stories I *did* need the Wayback Machine for, and forgot to strip off the front.
Agree here. I often find Common Table Expressions in particular can remove the need for `UNION`s and improve readability.
I've often found that Common Table Expressions really kill the runtime for queries. Depending on the size of the results in the CTE, they were making a query take 10x as long as dumping the results into a temporary table and then discarding the temporary table at the end.
Let's say I have 3 tables: BuyerDetail as b, OrderDetail as o, GatewayDetail as g. So I want to create a fraud alert/detector based on 3 classifications: 1) b.address!=g.address AND timediff(o.ordertime, b.birthdate)&gt;5 2) g.address in ('USA','UK','NZ') AND g.bin=123456 AND count(o.id)&gt;3 3) o.item in like '%drug%' AND b.name like 'jame%' Obviously the real thing is much more complicated, but I tried to show in this scenario that a simple join in a single select (even multiple selects) is not enough.
I've also used [Wal-E](https://github.com/wal-e/wal-e) for continuous WAL archiving and recovery. It works very well and can back up to S3, Azure blob storage or Openstack Swift. It's also currently development FOSS software so you pay for nothing but a bit of time setting it up and your storage used.
Whoever posted, you're shadowbanned.
Nicer than me! That's a little bit of a crazy request in my book.
You can still do a lot with a "canned" system, especially index tuning and host system configuration.
Meh I had it 'laying around'. A big part of what I do is set up new environments for customers. A big part of that is backing up a DB from another customer and restoring it clean (instead of having to manually create the db... which would take an eternity). I try to help out around here when I can haha.
A lot of enterprise systems have querying abilities against the production system, and a nightly snapshot of data extracted overnight for the regular mass of users and reporters. This is how every system I've ever worked on is configured. Only a few people had access to directly query the live production system, for obvious reasons such as continuity and availability. If the clients do not need up-to-the-minute data, your platform, whatever it is, should be doing a daily extract and populating an RDBMS of your choosing. Having mass amounts of people able to do querying on the live production system is just asking for your resources to become unavailable. On my current system, we dump everything into Oracle databases every night. On my previous system, we extracted flat text into CSV and dumped that into a MSSQL database. If you want something free, dump it into mySQL as a daily snapshot and let the users have at it! 
What methods do you use to dump the data into the Oracle databases? Is it the SSIS tools or something more robust? In-house software?
You need to correlate the update statement, because at the moment the update is thinking it could update one field from many rows. On my phone so excuse the formatting. Your update should be of the form... UPDATE TableA SET TableA.field = (SELECT case when &lt;conditions....&gt; end FROM TableB WHERE TableA.key = TableB.key) In your case, TableA and B are the same table.
This post is on the money. Any one of these bullets could be considered a "best practice". That being said, I want to reiterate his last paragraph. You should not be reporting off your live production OLTP database. Full stop. You don't know what any of those 1000 users querying the database could do. What if they drop in some monster of a query that just devours resources. Handling that means removing permissions, going through a stored proc, or changing the tables. It's a fairly straight-forward set of solutions.
Did you bother reading our sidebar before posting this here?
I'd do something like this in such a case : WITH valuesToUpdate AS ( SELECT ProgrammeID = 23171 , CustFrmDataID = 324616 , newVal = 324617 UNION ALL SELECT ProgrammeID = 23178 , CustFrmDataID = 324662 , newVal = 324663 UNION ALL SELECT ProgrammeID = 23184 , CustFrmDataID = 324693 , newVal = 324694 UNION ALL SELECT ProgrammeID = 23189 , CustFrmDataID = 324720 , newVal = 324721 ) UPDATE p SET CustFrmDataID = vtu.newVal FROM Programmes p INNER JOIN valuesToUpdate vtu ON P.ProgrammeID = vtu.ProgrammeID AND p.CustFrmDataID = vtu.CustFrmDataID ;
&gt; You don't know what any of those 1000 users querying the database could do. What if they drop in some monster of a query that just devours resources Ideally, they aren't running ad-hoc queries, but rather running pre-written reports.
/u/alinroc has great suggestions if you want to stay inside your database. I'm just going to plug [the company](http://www.actuate.com/) I work for. We have a reporting platform, BIRT (Analytics Designer), that you can embed in your website. The BIRT reports can be designed to use a cached DataModel (that refreshes on a schedule). The DataModel uses a columnar storage that lets your users have very fast report execution and then interactive capabilities to rearrange and filter the tables and charts. Basically, it's like having a separate table in the Database, but generated and stored by third party software and it's heavily optimized for reporting. 
Yep, very similar concept. I can't find the whitepaper, but I'm pretty sure we tested faster than SSRS. 
Or use a common table expression.
What RDBMS are you working with, and at what level (DBA? Developer)? If you're using SQL Server and tending toward DBAdom, check out [SQLSkills' Immersion Events](https://www.sqlskills.com/sql-server-training/) &amp; [Brent Ozar's in-person courses](http://www.brentozar.com/training-in-person/)
True. But imagine if 300 of them all decide to run the largest report at the same time. It's still a bad idea.
Planning on testing this today ...thanks for your help!
I don't know what you mean by features, but you're probably going to need to use 2014. It looks a little different but is basically the same. 
yes.I'm a beginner. what version should i install in windows 10?
Thanks a lot man! 
I tried this and it is still give me this error: Subquery returned more than 1 value. This is not permitted when the subquery follows =, !=, &lt;, &lt;= , &gt;, &gt;= or when the subquery is used as an expression. The statement has been terminated. Any ideas? UPDATE Programmes SET Programmes.CustFrmDataId =(SELECT DISTINCT TEMP7.CustFrmDataID FROM TEMP7 GROUP BY TEMP7.ProgrammeID, TEMP7.CustFrmDataID HAVING Programmes.ProgrammeId = TEMP7.ProgrammeID) 
That's nothing like what I said! You're not correlating by the Key field(s) and you're not even using HAVING correctly (which is generally a filter on an aggregation by using an aggregation). Based on your original post, I meant like this (I'm assuming ProgrammeID is the Primary Key) : UPDATE Programmes a SET a.CustFrmDataID = ( select CASE WHEN b.ProgrammeID = 23171 AND b.CustFrmDataID = 324616 THEN 324617 WHEN b.ProgrammeID = 23178 AND b.CustFrmDataID = 324662 THEN 324663 WHEN b.ProgrammeID = 23184 AND b.CustFrmDataID = 324693 THEN 324694 WHEN b.ProgrammeID = 23189 AND b.CustFrmDataID = 324720 THEN 324721 ELSE b.CustFrmDataID END from Programmes b where b.ProgrammeID = a.ProgrammeID) As you are only processing when ProgrammeID is in (23171, 23178, 23184, 23189) I would actually add that as the where clause too, but I left it out for demonstration purposes.
Id definitely break up your addresses into address_1, address_2, city, state, zip. There are a lot of good reasons to do this. - Validation (zip code of format ##### or #####-####) - Reporting (how many orders have we gotten from Houston?) - Portability (if you ever want to switch to another system that uses discrete fields) Not to mention just plain old best practice. You could also create a person_address table to let them have more than one address, but that sounds like overkill for your purposes.
If you're getting certified the one in Florida near Tampa is awesome. https://www.certificationcamps.com/ I know a few people who've gone and I have as well. Highly recommend. Chester Flake is the guy you want on the SQL server stuff if you can get him. But the all seemed pretty good.
Try join correlation below (perhaps your db doesn't support the correlation syntax above). update t1 set t1.columna = t2.expr from table1 t1 inner join table2 t2 on t1.column1 = t2.column1...
http://www.globalknowledge.com/ 
The application is interesting; what it does is pull the data (say from employee and a few other tables) to a staged_data table. Then it creates all manipulations as necessary and either rewrites to the table if needed or the application pulls the staged_data table for a report. 
That sounds ~~easily~~ replaceable with an automated job/script that you could modify and control. *Source: Have replaced a fair number of 'irreplaceable applications' with stored procedures and sql jobs.*
Can't, legacy software. No one touches the code anymore as the newest version is being pushed out. Unless I am misunderstanding you. 
Ok so first you'd need to find out what aspect of it is causing the slowdown Concurrency alone is not the issue. SQL Server is built for concurrency (locks withstanding) Most likely its one of three things.. 1) Database locking (use sp_lock to see what tables have locks on them when the db is slow) 2) CPU or RAM being exhausted 3) My guess is, since the legacy software copies data to temp tables, the slowdown is caused by sheer disk activity as its copying data into its own ready to be reviewed. You may be able to find some workaround for this if you were a dev and a dba, but if you're non technical, realistically its likely to be beyond you Another workaround is to replicate your live database to a different server, and point your legacy software at that instead. You could then use some form of merge replication to brings its changes back into the live db. Again these are quite advanced concepts for a non-dev though
If you are completely against SSIS, use PowerShell and a SQL Server job to do this for you. :) I made a blog [post](http://www.philschwartz.me/posts/1) that involved invoking a stored procedure and exporting results to a csv to be emailed to recipients. You may be able to mold it into your use case. Edit: Also if you choose to use powershell and need any help, let me know. 
Is there a way to download one of these databases to play with it offline?
I do not know about an MDF you can download and attach, but there is a hard way (downloading the XML data dump). I will link to [this article by Brent Ozar](http://www.brentozar.com/archive/2014/01/how-to-query-the-stackexchange-databases/). His instructions are for getting the data into a SQL Server database with schema. If you want the data in another database system, you will have to find tools to help you parse that XML into a schema in that system.
With first solution number 1, remember that some day you may be considered the "original developer". Think of your own balls before shooting another person's. One thing I notice up front is that you are executing SQL based on column data using execute_sql. Are you sure the slowdown is the content of the stored procedure you posted, and not because you're executing SQL inside of a cursor? Or the WHILE loop at the end of the stored procedure. Those are two huge red flags for possible performance issues that depend entirely on the data driving them, which the logic doesn't help show us. Instead of just looking at the total execution time, I'd be generating some 'Actual Execution Plans' and looking at what parts of this process are really hurting. If you do decide to add indexes, there are always a few things to keep in mind: Indexes create copies of the data in question in the index. This can negatively impact performance elsewhere by creating more work during INSERTS or UPDATES. I'm not saying don't add them, but adding a bunch at once is almost never the right thing. It really helps to know both the data and the model in that case. An index like this: CREATE NONCLUSTERED INDEX idx_small ON TABLENAME(a,b,c) Can be used on a query that specifies column a, columns a and b, or columns a, b, and c. It won't be used to Seek on a query that does not have column a. It won't be used to Seek on Columns A and C. It might be used in an Index Scan operation, but that's not as fast as a Seek. As a bonus with indexes like this, if all the data you are accessing (in the SELECT list or WHERE clause) is in the Index, it won't actually go to the table data at all- it does an Index Seek and is done. It also would be rendered useless by another index "ON TABLENAME(a,b,c,d,e)", so don't create a bunch of overlapping indexes. Lastly, indexes only really work if the query is [SARGable](https://en.wikipedia.org/wiki/Sargable). You can add all the indexes in the world to a table, but if the query isn't written to use the index correctly it won't do you any good. Before you make any changes to the logic or the indexes, get that Execution Plan, and really look at what is happening. It might surprise you where the lack of performance is really from.
Here ya go: https://technet.microsoft.com/en-us/library/JJ856598%28v=SQL.110%29.aspx
This is an interesting approach, and you are correct that I'm wanting to convert it to base 36. Yesterday I asked myself "If only I could represent 0-9 A-Z using base 10..." I did manually code a 36-rank conversion mapping for each character position, even though I know it was not the smart way of doing it, and resulted in several hundred lines of code. I'm going to see if I can modify this code to fit into my model. Thanks for the link. 
huh, only 2k seconds? We have nightly jobs / stored procs that regularly take 90min to run and no one gets woken up unless things take more than 3hrs.
This code is slightly beyond my understanding (for now, anyway, because I've never used keep, dense_rank, first, last, etc) but I can tell you that in all my common scenarios I've tested, this code works for each set of "f" data that I plug into it. I think this is probably the answer I'm looking for. I had already figured out the regexp_like syntax, but I was trying to split the sort into number/nonnumber then try to union them together while maintaining order then selecting the 1st row off of that, but I couldn't seem to maintain order and my non-numeric sort wasn't that great anyway. But this solution is doing everything I need so far. Thanks, and it is a very clever way of doing it! 
I had started to do something like this last night, except I spelled out each "rank" for each value, resulting in 36 lines of code for each of the five positions. It technically works but it looks like crap and I'm afraid I might get laughed at if any of my co-workers see it. It looks like this code is a more compact version of that. I'm going to test performance of this vs another crafty solution that was posted and see which is better. Thanks for posting this. 
No problem. Windowing and analytic functions are definitely something you're gonna want to look into eventually. They're incredibly powerful. This article's pretty good at explaining them: http://www.oracle.com/technetwork/issue-archive/2013/13-mar/o23sql-1906475.html
sqlcmd is one of the management tools that's part of the SQL Server installation. You would need to install these on your computer to run it. Basically it's a commandline interface, you're connecting to DBSERVER and DBNAME, running the script movement 3-6-12-MO.sql and outputting the results to movement_report.csv If the script is missing, then you have a problem :)
lol, us too, i'm sure. this is a stored procedure that 'quick-loads' a form so that the doctor doesn't have to fill it out, though. It's used in real time.
Ahhh, ok so things used in real time have completely different requirements then. the concern then makes sense.
I didn't have him as a teacher I had Suzy, who was also great. But Chester would poke his head in and crack jokes every couple of hours. For OP: This is a very intense course don't expect to go see the sights in Tampa, you will literally be in a classroom all day and studying all night.
There may be a better way, but this is what they are now familiar with. Thanks for the link. 
Update, for those interested: First, I realized I was calling the script from the wrong folder. Once I fixed that.... I was able to install the ODBC driver [link](https://www.microsoft.com/en-us/download/details.aspx?id=36434) and SQL Command Line Utilities [link](https://www.microsoft.com/en-us/download/details.aspx?id=36433) and then I was able to run the script.
Not full time. I freelance on the side. I love writing SQL. It's therapeutic for me. Great sense of accomplishment when its complete. The $$$ would be based on the size of the query and complexity of the rewrite. It blows my mind when companies turn it down after the a $500-1000 quote. They'll pay an internal employee $30-40 an hour and that employee will get bo where on it in a month because SQL is not their thing. I'll make it go from non functional to optimal at a fraction of the cost.
If its a once off then this (and probably others including your 36 line case statements) solution should be fine. But if you need to sort in this order all the time then you probably want to look at virtual columns or the like so that you can index them and sort with some semblance of speed.
I export much larger datasets than that with no problems using a generic PL/SQL package procedure. Millions of rows with several hundred columns. I suspect it's your coding approach to the problem that is the issue. Incidentally, my routine is a few hundred lines of code. Yours is 3,500, what?!!!!
That's pretty cool. Someone with the know would realize 500 or 1000 is nothing for a complex query. I'm sure you have tons of repeat customers. How's business been lately? Where do you find your clients? My SQL knowledge isn't there yet, but that would be nice to do some day.
SQL is used a lot in Finance because SQL is well suited for transaction data. No-SQL is not really suited for the Finance field because the consistency of data is not guaranteed. You can't get rid of SQL because data is nowadays the most valuable asset of companies. As a consequence, every soft in whatever programming language it is written needs to access to some data. You can learn SQL by yourself since it s a high-level programming language. Here a good resource : http://www.studybyyourself.com/seminar/sql/course/?lang=eng.
Here a nice resource : http://www.studybyyourself.com/seminar/sql/course/?lang=eng.
If that is an option, 100% it should be done. Sometimes convincing management is an uphill battle, even though it can be a read-only snapshot of your live environment as well.
I've seen them run out of table space before. We had a similar issue. 2500 line insert I inherited that was 20+ union all as an inline view that was aggregated. I rewrote it with a staging table instead of views and it made a huge difference in performance.
Yeah, I'm going to go with the others here and say that this amount of data should not take this much time and certainly should not take 3,500 lines of code to extract 250 columns of data unless you're aggregating data from several hundred tables. How is your extract running? Are you using cursors? Can you rewrite them to be set-based operations? Are they nested cursors? Do you have joins missing primary keys so you're creating accidental partial Cartesian products? Honestly, I'd love to see this proc on pastebin or something. I'm really struggling to understand what it is you're doing that could be using so much memory and require so much code for such a small amount of data. Are you running the query in SQLPlus, or are you getting these errors in, say, Toad? 
&gt; There is probably a better alternative. Yeah, this seems like a basic report. Even if there's no application reporting tool, SSRS would work. Still, this will get them back working.
You're looking for Merge Replication.
I know what you mean, and for a traditional analyst type role that would be true. But our team is a lot more operational as opposed to analytical and works with a lot of lists from vendors, importing from tons of different sources, etc. so knowing about data types is important. To give you an example, I would like an analyst to know that if you import an excel file and SQL decides to make the data type float or real that you would need to convert it to a non-approximate data type before doing any sort of calculation on it.
In the eternal pattern of posting questions online, I figured this out within 20 minutes of posting my question. Here's the problem: When I put the "Current.Run_Month" qualifier in the where clause, it explicitly filtered out the very results that I'm looking for which is "April not in May." I moved those run_month assignments in to the join itself and got the right result. Updating my post now. 
The easiest thing I could think of would be to generate two importable reports. Load them both into some staging tables on a dev database server and perform a FULL OUTER JOIN on the results, then look for NULL rows. *Not particularly familiar with Informatica, but have utilized this successfully a few times.*
Ah, I understand what you mean now. My apologies :)
The data that is provided is in conceptual form. I need to write it in a logical form. Such as : Gamer (userid, first_name, last_name, email). But I do not know how to write the others
Check out this article: http://sqlmag.com/business-intelligence/logical-modeling In particular these two tables it mentions: http://sqlmag.com/site-files/sqlmag.com/files/archive/sqlmag.com/content/content/8787/webtable_01.html http://sqlmag.com/site-files/sqlmag.com/files/archive/sqlmag.com/content/content/8787/webtable_02.html
look up aggregate functions like MAX(): select inning, max(timestamp) from table1 group by inning 
If you want yo remove the time component, SQL 2008R2 and above allows Convert(varchar(10),date_field,103). If you want to set the time to 00:00:00, try Cast(Floor(Cast(Getdate() as float)) as date time). 
 ... group by game, inning 
yeah they break things. I had them pushed on my hard in school but have all but abandoned them. What are your thoughts on this application though. It kinda makes sense to me except I would throw this task to the front end developers as it needs a, I cant remember what they are exactly, basically triggers for non database programs and are intended to be used on Transactional systems. Thoughts?
I don't have anything by game, only pitcher name, pitcher id, pitch result, at bat result (ex: walk, in play out(s), etc), play (which is basically something like "Joe Schmo grounded out to Grobler Peanut"), game date, inning, and zulu time. In addition to stuff that is irrelevant to this, like pitch velocity, spin rate, break, et cetera.
Put all the logic in the Stored Procedures.
It would be a lot easier to help you with a small sample of data, even if it was faked. You have a lot of baseball knowledge implicitly wrapped up in your question. But it's going to probably be something like `select max(time) from gamedata group by game, inning` If you can't group by game, like you say in the other thread, no problem, just fake that by grouping by the date and two teams involved. Or if that won't work, fine; just tell us some way the data can do it, and we'll show you the SQL for it. Having a sample would *really* help
Is this what you want? I made a .csv file of the relevant information, here is a picture of that. Or do you want the actual .csv? This is the data from one pitcher from one game. http://i.imgur.com/aIJvLlq.png
An extra good question... what flavor SQL are you working with?
From what you said, the latest pitch per play would be select name, play, max(time) from table group by name, play You probably need to include the game date in that.
Thats a pity because what you really want here are windowed functions, and mySQL doesn't do windowed functions :( You can make some assumptions though (like each zulu time is unique to that one pitch , ie no simultaneous games) And use them to get the last row for each play of the innings.( This is once again assuming that the play will be unique to that innings which may not be the case if its a simple description with no names) If there was a playId or something like that then you would be better off using that. Additionally if there was a PitchId then you could use that to join instead of having to join on a bunch of columns... SELECT table.* FROM table INNER JOIN ( SELECT play,inning,name,max(tfs_zulu) max_tfs_zulu FROM table GROUP BY play,inning,name ) max_table ON table.play = max_table.play AND table.inning= max_table.inning AND table.name= max_table.name AND table.tfs_zulu= max_table.max_tfs_zulu
Running stored procedures require exec permissions. Something not typically given to users. I would start there.
Yes. Yes it does. 
This peaks my interest. I watched a short intro video and it seems like its their equipment connected to the cloud Azure but also integrates hadoop? Whats the benefit of buying it vs. setting up your own? Im guessing your not finding much because its soo new. I learned about it at a PASS conference but they were more selling the Microsoft server and didn't touch on the other pieces as much.
Awesome website! Downloaded the file a few weeks ago with no issue. They also offer a lot of free webinars.
Honestly, when I read your post I wasn't even taking consideration that you wrote it but rather you inherited it. Inheriting heaps of bad code is fairly common. I would calm down, it wasn't meant to be a personal attack. I'm really sorry I said what I said and could see how you would take it that way. If you are interested, I would love to help you offline. I will pm you my email. If you email me the code, I'd love to help point you in a direction.
MS Services employee here. Been specializing in APS since I started with the company a year ago. Still a LOT to learn and the bulk of my work has been on smaller scale POC's, but I understand most of the concepts, and am actively a part of an internal APS readiness group. There may be some things I can't share, specifically about roadmap and the customers I've worked with, but fire away, and I'll do my best to help.
Awesome. I hope to hear from you!
Well my goal was to figure out how to tell games apart, so limiting it to one game is actually a problem. Can you give me something like that which has at least two or three games in it though? It would help a lot if you could tell me how I tell two games apart using the data. I don't baseball.
Hah, well, there isn't a "game" stat. What you see is what you get. There is no set way of telling games apart other than date. You can assume an umpire is a constant for a game, and it would be a good assumption 99% of the time, but the other 1% the umpire gets injured or has to leave the game due to illness. Here is a csv for all the data from july 4th http://s000.tinyupload.com/?file_id=34384310153923427419 sorry, forgot to include the column names: name player_id pitch_type pitch_result atbat_result start_speed z0 x0 pfx_x pfx_z px pz break_angle break_length spin_rate spin_dir zone balls strikes outs play game_date inning inning_topbot tfs tfs_zulu catcher umpire umpre_name stolen_base_attempt stolen_base_success batted_ball_type blank angle batted_ball_velocity direction hc_x hc_y pitch_id distance_feet to explain those columns: name (of pitcher), player id (of pitcher), pitch type, pitch result, at bat result, start speed (of pitch) are all self explanatory. The next 11 all describe where the ball started and stops and how much it moved et cetera. Balls and strikes represent how many balls and strikes there were when pitch was thrown. The play is a description oif the result of the at bat. Game date is date. Inning is the inning. Then the next one tells you if it is the top or bottom of an inning. tfs and tfs zulu are the date and time stamps. Catcher id is the catcher, the umpire name and id are the umpire. Then you have whether a guy stole a base or not on the pitch. If the ball was put in play, you have the type of batted ball, the angle it was hit on, velocity, some other stuff, and how far it went.
You could use Left(), but there would be no proper rounding..
Assuming all entries have the same number of digits after decimal (true in my case)...below solves my problem but may not solve the significant digits. Declare @Deci as int SELECT @Deci= LEN(CAST(REVERSE(SUBSTRING(STR(Val, 13, 11), CHARINDEX('.', STR(Val, 13, 11)) + 1, 20)) AS decimal)) from tbl Select round(Avg(Val),@Deci) From tbl;
Thanks. Your post gave me a idea to just count the digits next to decimal and pass to round function. See my other post. 
excellent catch.
Good point but in my case the value can't be zero...we are measuring a weight of something. However, If that's the case I would like the output of avg to be 0 if the value is below 0.5 and 1 if greater than equal to 0.5. It's just about restricting the precision of calculation to precison of measured values. 
CONVERT(DECIMAL(10,2),@Float) will round to 2 decimal places. You'll have to figure out how many decimal places to round to. One way to do this is to convert your float values to String and get the max length of string to the right of the decimal point. Then pass that to CONVERT.
I think their site is kind of neglected. There's a few things on it that are really, really outdated.
Basic logic seems correct. Are you sure it's inserting a row and not just updating? Add the update to check. Also try having it set the value in a column to ensure the trigger is firing instead of running the SP.
So your order of operations is: 1. Every 5 minutes, use API to check status 2. If API indicates success, update table 3. Trigger on table fires when status updates to Successful 4. Trigger sends email 5. Trigger updates table to indicate email sent Assuming the above is correct: Why can't your job that runs every 5 minutes send the email notification and update the flag after it successfully updates the field in step 2? Why hold up your process while waiting for the trigger to send the email (by comparison, a very slow process)? It may be possible to fire off that email asynchronously from the trigger, but that will involve extra plumbing and potential failures that you won't catch; you can do the same, in a safer way, from your main job. By doing it in the main job, you can also eliminate the issue that you noted in your code with `NOTE: This is flawed, as it currently assumes the email/sms were sent successfully` (assuming whatever you use to send the notification can respond with a success/fail) Triggers *are* very useful, but not appropriate in a lot of situations where they *seem* like a good idea.
&gt; tfs and tfs zulu are the date and time stamps 1. These (as well as a few others in your schema) are **terrible** column names. Descriptive column names don't cost extra money. Fix this. 2. Storing two timestamps for the same thing is wasteful and presents a huge potential for error. Store one timestamp, preferably UTC, and the local timezone offset from UTC. For example, in SQL Server, I'd use a `DATETIMEOFFSET` and store `2015-08-10T10:41:24-04:00' for the current time (my timezone is EST). This lets me convert easily between "zulu" time (assuming your "zulu" is UTC) and the local timezone of the event properly.
Thanks for the reply. I think my biggest issue with that was if there are more than one field getting updated. I wasn't sure how that would get handled, as opposed to using a cursor in the trigger, iterating through the INSERTED table. I would prefer to not do the sending/updating from within the trigger. 
&gt;cursor in the trigger, iterating through the INSERTED table There are few things that will raise a DBA's blood pressure quicker than this. &gt; I would prefer to not do the sending/updating from within the trigger. You are right to have this preference.
product group a 1 0 product group a 2 0 product group a 3 0 product group a 4 297.56 product Group a 5 297.56 product Group a 6 297.56 product Group a 7 297.56 product Group a 8 297.56 product Group b 1 0 product Group b 2 0 
I would want the group a 4th month to have 297 but then 0 for 5th, 6th, 7th 8th month
Sorry, MS SQL
 select * from TABLE where (PRODUCT, TYPE, MONTH) in ( select PRODUCT, TYPE, min(MONTH) from TABLE where CYCLEENDINGBALANCE &lt;&gt; 0 group by PRODUCT, TYPE )
smart. now where would i add the aggregate function of 'sum(case when ChargeOffFlag = 1 then cycleendingbalance else 0.0 end)'?
To limit a result to only one row in Oracle: WHERE ROWNUM &lt;= 1
Thanks for the advice. I will go back and look at adding additional steps to the job. As for the select, I am setting the variables that are being used in the stored procedures, with the values from current cursor.
I am the backup "DBA" ("Hey you, you are the backup DBA.") :( That is one reason why I am trying to get a little input about triggers. I've seen some say it's not good to have that logic in the trigger, and others who say it's fine, depending on the application. I appreciate the advice, and will look at other routes for implementing this. Thank you.
I am getting this data from official sources and I know what all the columns mean, so this isn't an issue. It is much more important to me that I can communicate with people using the same dataset. My problem is that I don't know how to select the last pitch in an at bat without having to export everything, run a script, and then reimport the 450,000 rows of data.
Where do you get the dates from the second result from? I thought you wanted to get an avarage per week but realizing that's not what you're after. Sorry can't help without more information
I don't know how well it will work from a performance perspective (I've never had a chance to use them in Oracle), but [`LATERAL JOIN`](https://docs.oracle.com/database/121/SQLRF/statements_10002.htm) is what you're talking about. For example: SELECT * FROM tbl1, LATERAL ( SELECT * FROM tbl2 WHERE tbl2.id = tbl1.id LIMIT 1);
I would generally take a backup of the existing database, create a new database on the new server, and restore the backup into it. If you just need a limited amount of what's in the source database, you can always do something like SELECT * INTO server2.dbname.dbo.tablename FROM server1.dbname.dbo.tablename This will create `tablename` on server2.dbname (which must exist already) and fill it with the contents of that table on server1. It doesn't preserve things like indexes or constraints, though, you only get the data.
backup restore mentioned above is going to be the cleanest/easiest way to do it if you don't have to deal with really big DBs. The "select * into" is also a pretty good option, I would just add for the OP that you probably need to have a database link defined between server1 and server2. An additional option provided by several pieces of software is to navigate your source DB, "right click" the item and select "copy to script". If it is a table, indices and contstraints can be added automagiclly. Most of these products usually allow the data to be scripted as well, e.g. TOAD, sqldeveloper, MS Management Studio. 
&gt; it should stop scanning through the right table and only join on the first match it finds? I'd say you're talking about a unique constraint on the second table's join columns. If your optimizer is tuned right, that should make Oracle "want" to do a unique lookup (just once) instead of a range scan.
I used this and it worked for what I'm getting at: [dbo].[FTable] as F (nolock) ON DATEADD(DD, 2 - DATEPART(DW, L.Date), L.Date) = F.Date Let me know if you think this is wrong in someway. Thanks! 
Since you know the AVGs are on Monday (and I assume you NEVER skip a Monday) it should be fine. Sunday will jump forward instead of backward, but all the other dates should correctly find the previous Monday. Your solution makes a lot of assumptions about the data, but there's nothing thechnically wrong with it. It's just not very safe or maintainable. What if they change the AVG calculations to Sunday or Friday? What if the system is down and a Monday gets skipped? 
this doesn't make any sense. you got new rows in the answer and no idea where you got them? it just looks like you sorted by average with a different data set. got another way of phrasing your question?
This works great and elegant solution. Really appreciate time went into researching this. Thanks a lot! 
How does that work for you? Do you have multiple users/requests doing this? I have a bunch of macro enabled spreadsheets that we're trying to get rid of. The idea of using an excel import job like this has come up a few times, but makes me cringe a bit.
 having sum(e.subcount) &gt; 0 or count(e.subcount) is null 
&gt; missing values in the joining data set will be null and any operator (=,&lt;,&gt; etc.) is going to evaluate to false Minor correction: When comparing with NULL all operators (except "IS [NOT] NULL") will evaluate to UNKNOWN. https://en.wikipedia.org/wiki/Null_%28SQL%29#Comparisons_with_NULL_and_the_three-valued_logic_.283VL.29 
Can you use windows aggregates and a CTE? I like the use of OVER and filtering later with the CTE.... Another thought is to look at SQL's execution plan to see what might be going wrong as well. Just another idea to think about. 
Oops yeah sorry!
I am not OP. However, I have a similar question but expanding based on other responses. Lets say there are 100 SQL instances with over 100TB in data. These are going to be migrated to new prod servers and documented. The existing servers would only be used to ensure the data was successfully and fully transferred. What process would you recommend for this?
I'd like to take a database and copy it so that it appears in the other Server. Or conversely, simply copy select data. I have a script that grabs data entries from various databases on a server and moves that to a different table in the same server. If I could modify that script so that it direct exactly which server its looking in.
http://stackoverflow.com/questions/12807021/sql-sort-with-default-value-always-on-top
By joining everything up and counting, you're creating a relationship between tables that don't really have one (they have a relationship to the parent, but not each other), so your counts won't be correct. I would use **union all** and pivot the data. Untested as not near a DB. But something like this.... select * from ( select D.ITEMID, E.SOURCE from ( select ITEMID, YEAR(DATE) as AYEAR, 'P' as SOURCE from P union all select ITEMID, YEAR(DATE) as AYEAR, 'R' as SOURCE from R union all select ITEMID, YEAR(DATE) as AYEAR, 'I' as SOURCE from I union all select ITEMID, YEAR(DATE) as AYEAR, 'S' as SOURCE from S union all select ITEMID, YEAR(DATE) as AYEAR, 'C' as SOURCE from C) E join D on D.ITEMID = E.ITEMID where E.AYEAR = 2015) P pivot ( count(*) for SOURCE in ('P', 'R', 'I', 'S', 'C') ) as PVT order by ITEMID
Which database platform are you using?
Rank users according to some criteria, rank chairs according to somethin too. Join on rank.
SQL 2008
This query shows a way to allocate a seat to each user in order (whilst avoiding expensive filtered cross joins) . Run it to see what I mean. Simply change into an update once you understand it. The left outer join on seats is to allow situations where a user may not have a seat if there aren't enough. select u.USER, s.SEAT from ( select USER, row_number() over (order by USER) RN from USER_TABLE) u left outer join ( select SEAT, row_number() over (order by SEAT) RN from SEAT_TABLE) s on s.RN = u.RN
Ok, that will work as long as I've got a 1 to 1 person to chair relationship. What happens when I've got 4 people and 8 chairs, and Bob can only take chair 1 or chair 2, because he's got a bad back? If I give Bob the choice of chair 1 or chair 2, how do I make it possible that Jim (who gets the next choice), can choose any chair from 3-8 and whatever chair Bob didn't pick?
Great minds think alike!
This sounds like an optimization and not a join/project problem. SQL might not be the right tool for it to begin with. Of course, if you can find a set of criteria that ranks users/objects reasonably, the solution going to hold. (say rank/sort disabled seats first, and people with disability first). 
 Let me see if you have the problem I think you have. The price table has multiple records for each PART_ID, with different prices, and the EFFECTIVE_DATE that price changed. You need to grab whatever the current price was for a given SALES record, based on the date of the sale. In particular, there is NOT a record in the PRICE table for every date, only for dates when the price changed. If this is right, we had to do this at my last job, and it's not particularly easy. Depending on your use case, there might be different ways to do it. If you're pulling a small enough record set, building a temp table that matches every date for the PRICE table might work. One problem we ran into is that the easiest ways require that the price never goes back to a previous value. Unfortunately, I can't remember exactly what answer we got. Try this out and see what you get: Select p.part_id, p.create_date, max(s.price) keep (dense_rank last order by effective_date) as price from sales s inner join price p on (p.part_id=s.part_id and p.create_date &lt;= s.effective_date) 
You nailed the issue. I am looking at your suggestion now. 
Maybe two dozen jobs like that in a company of ~3500 over 7 years. The three big caveats are * After months of saving `EmployeeBirthdays.xls` to the appropriate share, the user suddenly decides to save `EmpBdaysAugust.xls` instead; a script step in the SSIS package can enumerate the directory contents and rename the file * The schema of the user's own Excel sheet changes, which requires manual intervention and retooling * The user's machine is upgraded or reimaged and the shortcut to their network share disappears; one call to IT gets this fixed I don't really do much in Excel, so I'm not sure how you might handle the macros.
I work in Business Intelligence where I have to use SQL to prepare financial information to stock brokers every single day. Maybe I'm just having a bad day I don't know...
Really depends, can you afford to take your time copying the databases or do you need to swap from server a &gt; b at the flick of a switch?
Bigandos, Thanks for the reply. But it's actually done under SQL Server Authentication. When I do try and connect the DB in the object explorer using Windows Authentication, I actually get login failed. The login is from an untrusted domain and cannot be used with Windows authentication. (Microsoft SQL Server, Error: 18452). We are connecting to a remote database. Any suggestions for the first tip you gave me?
did you get an outline of which sections you did well in and which you did not?
Solution: Many thanks to /u/ichp for giving me the idea to rank the people and the chairs separately and go from there. I wasn't able to completely dump the cursor, but instead of cursing through the individual people I was able to cursor through the groups of people, rank the people in the group using ROW_NUMBER, and then rank the available chairs that group could sit in (also using ROW_NUMBER). Joining the two groups on the ranked ID took the processing time down from over 10 minutes to about 10 seconds for 1500 records. Edit: Code looked roughly like this: UPDATE u SET u.seat = a.Seat_Number FROM User u LEFT OUTER JOIN (SELECT ROW_NUMBER() OVER (ORDER BY Seat.Seat_Number) as Seat_Rank_ID, Seat.Seat_Type, Seat.Seat_Number FROM Seat WITH (NOLOCK) WHERE Seat.Seat_Type = 'Handicapped') a ON a.Seat_Rank_ID = u.User_Rank_ID WHERE u.Seat_Type = 'Handicapped'
I've never had the problem with native MS SQL SSIS not being able to auth with a specified user/OLEDB connection on itself, however I have seen this problem numerous times with external source (AS400/Oracle) connections. Unfortunately the only work around I've found that worked for me was to switch to DontSaveSensitive, build out a connection config file, specify user and pass, then be sure to add that config file to the SQL Agent Job config when(if) you build out a job for automation purposes. (This also gets around having to give the pass out to people who need to run it)
Thanks for your thoughts!
yup this will fix your problem, or set the security to dont save sensitive
Does this work for you? select LEVEL_1 , LEVEL_2 from da_table group by LEVEL_1 , LEVEL_2 order by LEVEL_1 , LEVEL_2 
Thank you heaps man I think that's exactly it. Ive gone home for the day but will try first thing tomorrow
There's not a lot of information here, but it sounds like you need a [join table](https://en.wikipedia.org/wiki/Junction_table). That is, assuming you already have a `users` table with PK `user_id` (plus, you know, like, username, email address, and whatever other user info), a `lists` table with PK `list_id` (plus whatever data you need on the lists), and then a `posts` table with PK `post_id`, you'd then create a fourth table `lists_posts` with at least three fields `(list_id, post_id, user_id)`, which should be the primary key there (or at least a unique key if you want to go the surrogate route) and with appropriate foreign key relationships defined. You would probably want to also store some metadata there, like a creation timestamp and such. A rough mockup might go like this: CREATE TABLE users ( user_id int PRIMARY KEY, username varchar(100) NOT NULL, email varchar(255) NOT NULL, pass_hash varchar(255) NOT NULL ); CREATE TABLE lists ( list_id int PRIMARY KEY, name varchar(100) NOT NULL, created_on timestamp NOT NULL user_id int REFERENCES users (user_id) ); CREATE TABLE posts ( post_id int PRIMARY KEY, title varchar(100) NOT NULL, created_on timestamp NOT NULL, user_id int REFERENCES users (user_id), post_body varchar(MAX) ); CREATE TABLE lists_posts ( list_id int NOT NULL REFERENCES lists (list_id), post_id int NOT NULL REFERENCES posts (post_id), user_id int NOT NULL REFERENCES users (user_id), tstamp timestamp NOT NULL, PRIMARY KEY (list_id, post_id, user_id) ); Now, if lists are going to be exclusive to a single user, you can simplify a bit, moving the `user_id` reference to the `lists` table (with the FK relationship), and dropping it from the join table. In our mockup above, you'd make the `user_id` field in `lists` be `NOT NULL`, and change the `lists_posts` table to: CREATE TABLE lists_posts ( list_id int NOT NULL REFERENCES lists (list_id), post_id int NOT NULL REFERENCES posts (post_id), tstamp timestamp NOT NULL, PRIMARY KEY (list_id, post_id) ); Hope this helps. Edit: A couple notes - 1. Those `int` PKs should probably (i.e., unless you have some other canonical source for ids) be whatever your database's unique identifier is. This would probably be `serial` in PG, `int AUTO_INCREMENT` in MySQL (gross), and `int IDENTITY` in SQL Server. 2. You should also think about how (and if) you want to handle having lists, posts, and users being deleted. Most of those FKs can probably be `ON DELETE CASCADE`. However, if you want keep lists or posts even when their creator is deleted, `lists.user_id` and/or `posts.user_id` shouldn't be `NOT NULL` and the FK should be `ON DELETE SET NULL`. If you want to keep items added to lists if the user who added then is deleted, it gets a touch more complex. You'd need something like: # CREATE TABLE lists_posts ( list_post_id int NOT NULL PRIMARY KEY, list_id int NOT NULL REFERENCES lists (list_id) ON DELETE CASCADE, post_id int NOT NULL REFERENCES posts (post_id) ON DELETE CASCADE, user_id int REFERENCES users (user_id) ON DELETE SET NULL, tstamp timestamp NOT NULL ); (Regarding the PK's type, see edit 1.) Edit 3: I realized there's some ambiguity in the first case about what to do if two users try to add the same post to the same list. In the current setup, it would just add it twice (once for each user). To only allow it once, you'd actually do: CREATE TABLE lists_posts ( list_post_id int NOT NULL PRIMARY KEY, list_id int NOT NULL REFERENCES lists (list_id) ON DELETE CASCADE, post_id int NOT NULL REFERENCES posts (post_id) ON DELETE CASCADE, user_id int REFERENCES users (user_id) ON DELETE SET NULL, tstamp timestamp NOT NULL, UNIQUE (list_id, post_id) ); Or CREATE TABLE lists_posts ( list_id int NOT NULL REFERENCES lists (list_id) ON DELETE CASCADE, post_id int NOT NULL REFERENCES posts (post_id) ON DELETE CASCADE, user_id int REFERENCES users (user_id) ON DELETE SET NULL, tstamp timestamp NOT NULL, PRIMARY KEY (list_id, post_id) ); This will also fix a problem in edit 2 whereby the lack of constraints lets the same post be added to the same list over and over (even by the same user).
That is ironic. The mysql instance on my wordpress site crashed. Its back now.
If it's similar to the MS SQL Certification, working with SQL isn't remotely close enough sadly. The exams go into so much detail that I'll be honest, it's much more likely a DBA would know rather than someone who is in business intelligence/IT Business Analyst/Systems Analyst. You know how to prepare for the exam if you choose to retake it. The better question you should ask yourself though, is it worth taking and having under your belt? You've already shown that you're more than capable of it at work, why the need for a cert?
Which DB?! (Asked count #766).
AHH I knew I forgot something--MS SQL
Wouldn't the amount of writes that SQL performs be the same regardless of RAID array setup, it would just be a matter of if the controller card has to have only 1 disk writing the data (raid 0), all disks writing data (raid 1), half the disks writing data (Raid 10), or 2 disks writing the data (raid5)... etc... Now if you are looking for recommendations for what the minimum IOPS of write/read should be for a database, it depends on the application. A log/archive/warehouse database that no one reads from unless they need to rebuild a cube, or research an incident that occurred would have very different requirements than say a database that handles sales transactions. And even within those groupings, keeping up with sales at a single Starbucks requires a different level of write performance than say the whole chain.
&gt;Generally in MySQL we send queries massaged to a point where optimizer doesn’t have to think about anything. The fact that this is necessary is one of many reasons not to use MySQL.
Hi PrezRosslin, Thank you for the solution--I will try to test what you provided but I'm pretty green in the gills in regards to SQL development, so I may have some questions. At any rate, this is actually a bug in our DB--a change was made on the part of our DBA in how our DB handles the updates fed from our customer management system's DB in regards to fee amounts without any of us developers being any the wiser. Soon, this historical data will be separated and housed in another table, so it will become a non-issue, but until that occurs, we have to have provisions for existing ad-hoc requests. I really appreciate the suggestions though, because I want to learn more about both the development side and the administrative side (going to try and do a 'fun' project of my own analyzing songs' chord choices, and figured I'd build a DB of that so I can analyze similarities), so thank you for taking the time to reply!
Just so you know the part that begins `with cte as` is a common table expression. The part that reads `row_number()over(partition by...` is called a window function. That should help you find more info if you need.
Ahh, so that makes much more sense now. It's basically creating a table (not like a temp table or table variable, though) from a results set of a query, sorting descending by lastchange, then selecting only the first entry (essentially max(lastchange)) based on region PK and product, and selecting those values for the CTE. Then you join those results to any/all other tables needed. Thank you! I'll have to try this solution shortly... 
Yeah I'll try your method because the nested select solution provided by /u/ziptime is giving me syntax errors I cannot locate--probably due to other joins, but it is pointing me to the lines with the `where in` statement and the `group by` line. I'll let you know if I am able to do it in this fashion!
It seems that SQL server doesn't support multi-column in clause, other DBs like Oracle, DB2, PostgreSQL etc. do. Here's an alternative that should work select l.*, f.PRODUCT, f.PRICE1, f.PRICE2, f.PRICE3, f.LASTCHANGE from LOCATIONS l join FEE_AMOUNTS f on f.REGIONPK = l.REGIONPK join (select REGIONPK, PRODUCT, MAX(LASTCHANGE) as LASTCHANGE group by REGIONPK, PRODUCT) f2 on f2.REGIONPK = f.REGIONPK and f2.PRODUCT = f.PRODUCT and f2.LASTCHANGE = f.LASTCHANGE
Would you rather look through all the records for the needle in the haystack or have the needle be obvious? Also if you add a count(*) column to the select, you can see how many of each combination you have.
I see what you did, grasshopper.
What is it about posting a question that gives me the ability to figure out the answer right after? You have to prefix the value with 'N' for SQL to recongnize the characters, so like: insert into [TableName] set [FieldName] = N'测试值' Thanks me! 
You might want to consider why you're receiving more than one result on any of the subqueries. Is that expected behaviour? Should there only be one data point from the float table for any given DateAndTime, Millitm and TagIndex? You could "fix" your query using the existing structure by using "select top 1 (...)" in each of your subqueries where you select more than one data point. Add an "order by" to each and you have some control over which value gets picked if there are more than one. Still, the question should be answered why there are multiple ones to begin with. ziptime's Pivot-approach might be what you need. You're still making a choice about which value to actually use in the max(value) bit though, so make sure it's the one you want. 
Sorry, should've been VAL. Have changed it in the query above, try it now.
It looks like your actual column name is `val`, not `value`. Change it and try this solution again, as it looks a lot cleaner and probably performs better. He also didn't account for values from `FloatTable` appropriately, so you may want to try just the strings first. edit: split infinitive
you want to team viewer and check it out?
 select st.DateAndTime, st.Millitm ,max(case when TagIndex = 3 then st.val) as Operator ,max(case when TagIndex = 5 then st.val) as ShipName ,max(case when TagIndex = 8 then st.val) as StatsDate ,max(case when TagIndex = 9 then st.val) as StatsTime ,max(case when TagIndex = 14 then st.val) as BoxDetail ,max(case when TagIndex = 15 then st.val) as StevadorName ,max(case when TagIndex = 16 then st.val) as CranTechName ,max(case when TagIndex = 0 then ft.val) as CraneCount ,max(case when TagIndex = 1 then ft.val) as CraneNumber ,max(case when TagIndex = 2 then ft.val) as CycleTime ,max(case when TagIndex = 4 then ft.val) as PickupOrDrop ,max(case when TagIndex = 6 then ft.val) as BoxSize ,max(case when TagIndex = 7 then ft.val) as Weight ,max(case when TagIndex = 10 then ft.val) as SpreaderID ,max(case when TagIndex = 11 then ft.val) as SpreaderX ,max(case when TagIndex = 12 then ft.val) as SpreaderY ,max(case when TagIndex = 13 then ft.val) as SpreaderZ from StringTable st inner join FloatTable ft on ft.DateAndTime = st.DateAndTime group by st.DateAndTime, st.Millitm edit: if you do have duplicate values for any dates/times, obviously this is going to eliminate any but the max. Your queries and the answers you've received thus far do not account for duplicates though.
Eh just try the other solution I submitted. Not sure about from an efficiency perspective, but it should be more straightforward.
Can you elaborate on your question/desired output? SELECT ord_hdrnumber, stp_type2, stp_type3, stp_event FROM stops WHERE ord_hdrnumber = '111277' ORDER BY stp_event desc
Maybe this? with cte as ( SELECT ord_hdrnumber, stp_type2, stp_type3, stp_event ,row_number()over(partition by ord_hdrnumber order by stp_event desc) as seq FROM stops WHERE ord_hdrnumber = '111277' ) select ord_hdrnumber, stp_type2, stp_type3, stp_event from cte where seq = 2 edit: stupidity
Another gotcha (if you're using stored procedures) is to make sure you change the parameter type to nvarchar as well as changing the table column to nvarchar.
 COUNT(*) OVER(PARTITION BY loc) AS "Count" can then throw WHERE clauses into it for anything over 2
It's just a clip you reminded me of with that statement. :( :((((
Are you a member of /r/4ch? edit: I'm not sure it's what I think it is now .... I thought it was for if you have a 4ch username ... like 3ch.
Ich bin ein Berliner. Mmm ... pastries.
I admitt that I really have a hard time to even acknowlage MySQL in the RDBMS realm. I wouldnt compare it to Access or Filemaker, but RDBMS in the same league as Oracle, MSSQL, Postgree, DB2, Informix, Pervasive and there such .... nahh, not really. The first thought coming to my mind is the horrible horrible defaults that make ACID a mockery. Then I reason to myself that one can change those settings IF one knows which ones to change, for which you'd need a MySQL DBA, and how may of the installations are really supported by one? But I honestly think the MySQL optimizer to suck tennis balls trough a garden hose when compared to postgree, with postgree being just as "for free", and being as secure and reliable as you can ask for. I really honestly, without flaming or trolling, do not get why people use MySQL when there is postgree. I do not have the first clue why mySQL is deployed to production these days.
maybe. it's a standard Macola table. oe = order entry module ordlin = order lines history table is oelinhst_sql. It does make some sort of sense once you get used to their naming scheme.
sorry for the late response. I want to be able to select row 2 AS a subquery. example: (select stp_type2 from stops where stp_event = 'LLD') as stptest However this subquery returns more then one row. All I want is row 2 of the first statement.
Even once you've tweaked the defaults, there's still no way (as far as I know; I may be out of date) to prevent a lot of terrible, terrible things from happening. Did you just try to insert a NULL into a NOT NULL? That's ok, we'll just put a 0 or '' there (which, of course, is just as correct as correct as assuming it should be 37.4 or 'banana'). Speaking type issues, try `DELETE FROM tbl WHERE 0='banana'`. Or better yet, don't do that or you'll wipe the table. `CHECK` constraints? Let's parse them, but ignore them and not raise a warning. You want foreign keys? I hope you don't plan to use a full-text index because you can't have both. It's such a joke, but to use PG, one actually needs to have some basic understanding of the technology they're using. Not a lot, but some. MySQL's motto should be "The database for people who don't understand databases."
I honetly wouldn't have a clue on how to make MySQL "work". I know of some of the "wtf"'s, but surely not even close to all of em. And well..... to handle data, I would honestly say, someone should have at least the first idea on how to handle data -&gt; use postgree. My guess is we agree pretty much on that ;) //edit : I think the where id = 'banana' can be fixed with configuration. I think it was a session setting, which means the client can overwrite the server default to go back to "no error no problem" (which makes it useless imho) but I think that one is one of the config parameters
do you mean write it without a CTE, like this? SELECT ord_hdrnumber, stp_type2, stp_type3, stp_event FROM (SELECT ord_hdrnumber, stp_type2, stp_type3, stp_event ,row_number()over(partition by ord_hdrnumber order by stp_event desc) AS seq FROM stops WHERE ord_hdrnumber = '111277' ) a WHERE seq = 2
Amen
RunAs Radio podcast gives you something to talk about at work
Open up the SSIS package and change it to Save sensitive with password, give it a password and build the package. Delete the old package you have deployed from the integration server and import the new one you just rebuilt. Then go to your job and when you setup the step to run the SSIS package it will ask you for the password. Type in the password, save the job and you should be good. All of our packages are setup this way and it works great.
I've always been more than half tempted to a similar career switch. I hope this post gets some replies.
I too hope for more replies.
Is the SSAS, SSRS, SSIS components relevant to a DBA? Or is a DBA's role purely administration? Would you expect a DBA to have BI knowledge? edit: Could you describe a typical day for a DBA in a reasonably busy work environment?
While it's not technically a book on SQL, one book that I think should be required reading for anybody who uses databases for anything beyond very simple one-table queries is Date's *An Introduction to Database Systems*. You can pick up a used copy of an older edition for [under $5 on Amazon](http://www.amazon.com/dp/8178082314/), and it's very much worth it.
You can take tests and certifications, which do look good, but there isn't much of a replacement for experience. I would say learn everything you can from your current job, and play with Sql server at home. You can download developer editions or express versions of Sql and install it on your home computer. You can then play around with your own environment, break things, fix them (or Google how to fix them, which we all do) and learn how everything works. Sql can seem simple at first, but it can get extremely granular pretty quickly, so understanding the basics of scheduling and taking backups, restoring databases, working with logins and permissions, creating jobs, importing and exporting data, etc are the building blocks that you can play with at home with no stress of breaking a million dollar environment. 
Try this: SELECT tableID AS 'id', 'int' as 'id/@type', val AS 'value' FROM myTable FOR XML PATH('items'), TYPE, ELEMENTS XSINIL I don't do anything with XML, but found [this](http://blog.sqlauthority.com/2009/02/12/sql-server-simple-example-of-creating-xml-file-using-t-sql/). edit: maybe attribute needs to come before element? Not sure.
I could've sworn I had tried that before and it didn't work, but it does work. Thank you!
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/sysadmin] [\[MS SQL 2014\] SQL AlwaysOn Availability Multi-Subnet Storage Requirements : SQL (X-Post /r/SQL)](https://np.reddit.com/r/sysadmin/comments/3gzy0f/ms_sql_2014_sql_alwayson_availability_multisubnet/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
Try something like: SELECT "Hour" = CONVERT(VARCHAR(13), DATEADD(hh, -6, archiveddate), 121) + ':00' If archiveddate = '2015-08-14 04:45:12.345' for example, the above query should return '2015-08-13 22:00'. That what you're looking for?
Yes, that's exactly it. Here's my final query. I really need to examine it further so I understand it better, so it's converting my string to a date, by hour, taking away 6, and it must understand taht's 6 hours and because it's the dateadd function? What's the 121? select "Hour" = CONVERT(VARCHAR(13), DATEADD(hh, -6, archiveddate), 121) + ':00', "Hourly Rate" = count (*), "MB (original)" = sum (originalsize)/1024/1024, "MB (compressed)" = sum (itemsize)/1024 from saveset,savesetproperty where saveset.savesetidentity = savesetproperty.savesetidentity --and archiveddate &gt; dateadd("hh", -24, getdate ()) group by CONVERT(VARCHAR(13), DATEADD(hh, -6, archiveddate), 121) + ':00' order by CONVERT(VARCHAR(13), DATEADD(hh, -6, archiveddate), 121) + ':00' desc
From inside out: - Use DATEADD to subtract 6 hours from archiveddate - Convert that from DATETIME to VARCHAR - 121 is the format, in this case yyyy-mm-dd hh:mi:ss.mmm - Since we put VARCHAR(13), it only takes the first 13 characters of that, so it will look like yyyy-mm-dd hh - Then we tack on ':00' to make it yyyy-mm-dd hh:mi, which will work either as a VARCHAR string (what it is now), or would also fit into the DATETIME data type As mentioned above, this will only be correct half the year due to DST changing. There are a number of ways to deal with this: manually change it in your script, make a function to handle time zone conversions, etc.
It's not very clear what you want. All I can assume is given the following predicates * A = ABC with a receive date of 8/1/2015 * C = CBS with a receive date of 7/22/2015 * N = NBC with a receive date of 8/2/2015 * T = TBS with no receive date * E = TEA with no receive date You want to select a person if (A and C and N) and ((not T) or E)? Is that correct? If not I've no idea what you mean.
To this I would add: find and follow as many SQL blogs as you can. Guys like Phil Factor, Aaron Bertrand, Gail Shaw, and others. Along with the advice, they sometimes include sample databases for you to download and test out the concepts they discuss. This is great for things like testing out how to recover a database that didn't shut down cleanly. I would include some examples, but I'm posting from a new laptop and don't have any bookmarks handy.
Everyone is different. There are so many factors to consider: level of experience, project deadlines, home/family life. I normally work 40-45 hours. A week ago, I probably worked double that for two weeks as my team pushed towards a major product launch. Knowing others' work hours won't help you much. You need to get a better understanding of the work load at the company at which you are interviewing, keeping in mind that you are starting with a new team that may do things differently than you're used to, resulting in a bit longer time to accomplish some tasks.
I apologize as I had difficulty myself trying to explain what I want to do. Basically I want to pull anybody from the SARCHKL table that have a code or codes and all of those codes have a received date EXCEPT for those codes designated in the script that I want to exclude while paying attention to the other constraints in the script as well. Here is a kind of example of put together and which records I would want pulled: http://i.imgur.com/pxkw1BJ.png Does that help any? My apologies for not being clear.
Highly dependent upon the week and what projects are going on. 
About 3 hours a week.
So your rule set is... Select anyone who has a codes, all of which have a date unless TEB, TBA which doesn't have to have a date? 
I also live in the UK and work similar hours, I agree completely with your point if view. I assume the above are from US developers - as I think our situation is the norm over here. 
Honestly I'd just make a C# service or something that converts it, it's just such a messy thing to do in sql
One more thing, what are the PK and FK columns for those two tables? 
Yes, that's very normal. I am contracted for 40 hours, but I regularly do 45. I can do 60 if deadlines require it.
Same. My role is a junior role so I'd never expect to work longer than that. If people aren't happy with the speed I develop at, then they need to re-evaluate their decision to hire a junior instead of a fully qualified developer. 
&lt;= 40 hours a week for me. Anything more than that is usually a special project, but is rare for me. Bootstrapping infrastructure would qualify as a special project. Once all of that is deployed, I wouldn't expect to work more than 40 hours a week on the norm. I do ETL/DW work.
I don't know the PK and FKs between the two tables, but this should give what you want. select distinct saradap_pidm from saradap p where p.saradap_term_code_entry &gt;= '201560' and p.saradap_apst_code = 'I' and p.saradap_resd_code != 'I' and p.saradap_levl_code = 'GR' and exists (select null from sarchkl l where l.sarchkl_pidm = p.saradap_pidm and l.sarchkl_term_code_entry = p.saradap_term_code_entry and l.sarchkl_appl_no = p.saradap_appl and l.sarchkl_admr_code is not null and l.sarchkl_receive_date is not null) and not exists (select null from sarchkl l where l.sarchkl_pidm = p.saradap_pidm and l.sarchkl_term_code_entry = p.saradap_term_code_entry and l.sarchkl_appl_no = p.saradap_appl and l.sarchkl_admr_code not in ('TEA,'TEB','TEC','TED') and l.sarchkl_receive_date is null)
It is worth pointing out that in the us, some companies measure as 45 hour weeks, but include 1 hour "paid" lunch daily. So after this difference in measurement we're mostly just seeing a 2.5 hour difference. 
They are pidm.
I only work half days now. You know, 8 to 8. (I love that old joke.) But seriously, I've been a SQL / BI consultant in telecom (phone, cable, satellite providers, etc) for the past 15 years. Before that it was banking, insurance, health care. I don't think i've ever worked less than 40 per week in my entire career of 30 years. With the normal flow of projects and production rollouts and general system problems, we used to have a 3 month cycle that seemed to go "1 month crazy, 2 months normal". meaning 60-80hr weeks for one month followed by 40-50 hour weeks the next 2. At one point this shifted to 2 months crazy, 1 month normal for a while. It got really bad and at that point I was considering quitting. But it eventually got back to "normal". In IT, the "work is always there". There is always something to do, so putting in a few hours after dinner or before bedtime, gets you ahead of schedule for one thing, but is gratifying and provides a constant sense of accomplishment. If you're a developer as opposed to a DBA or SysAdmin, then you are a "creator". You build stuff. I find that rewarding so whether i'm getting paid overtime or not, I'm gonna do it. EDIT. PS... I'm working (from home) right now at 10 am on Saturday. But yeah, also browsing reddit. 
I wrote a query for you [**here**](https://www.reddit.com/r/SQL/comments/3h19sd/oracle_ensure_all_items_on_a_table_meet_wanted/cu3td4w), try it and let me know.
Wow, that worked. I want to know why it worked too as I am interested in actually learning so I don't have to ask or utilize other people's help in the future. Would you care to explain how this pulls what I desire?
I don't know your level of SQL knowledge, but the key parts to this query is the **exists** subquery, and the **not exists** subquery. The **exists** part is saying *"for all people I pull from the DB, there must exist records linked to them which have a code and date."* The **not exists** part is saying *"for all people I pull from the DB, there must not exist records linked to them which have no date which aren't TEA, TEB, TEC or TED."*
I think this hits closest the reality of the situation. I work about 50 hours a week on a slow week. It's very rare I haven't signed into work from home. I was up at 7am doing work and will be doing more work tonite around 9pm and then again sunday morning at 5am. Definitely overworked and underpaid but on the underpaid the problem is there's always people out there that can easily replace you.
40, and if there needs to be more then management needs to do a better job.
Salaried employee but if I work over 40 hours I get paid OT. So usually about 40 hours a week...
My level is novice at best. So why does the exist/not exist work in this capacity but my original coding does not when it seems like it should, to me any way? You're absolutely awesome by the way! Thanks!!!
I am a contractor so I am slightly different. I work till I am done, to be fair though 9 times out of ten it's similar to the permies. Around 35-38 hours a week but near release it can be much higher.
Depends on the firm. In Big Finance I've had jobs that expected 12-14 hour days (yes really.) But for the most part I work a 40 hour week. Some roles I've had have had an "on call support" component, which is fine if all of the lines and expectations are clear. I had a Big Finance ETL role in NYC (with most of the team in Southern California) that had serious problems with clear expectations and support schedules. 
40-42 usually. I'll typically put in one 10ish-hour day between Monday and Thursday, maybe a 9 also. On Friday, if it's nice out and I feel like leaving at 3:00, I'll leave as long as I'm still at 40 for the week. (I have an hourly rate)
45-50. Sometimes much less, sometimes a bit more
Side note. Avoid using (MAX) when you set up the field, unless it's limitless commenting data. MAX sets aside .. a lot of space for potential content in the field. And it can't be indexed. If you have a pretty good idea how big the field will be, give a bit of a buffer and set a limit.
wow.. that simple, huh. Cheers!! you've made my day
I actually feel I get paid well, the issue is that it is not commensurate with how much I work and I work so much because I love my job.
My goal with the question was to find the industry norm. I've only been a developer at 1 place and so I don't have a great insight into the normal developer world. I was really trying to learn if most developers are working 50+ hrs a week.
ah, then that's good. At least you're making the money. Here's the trick i use: I also love to code and I love to work, in general. So i think, how much less money would I do this job for before the workload and conditions started to bother me. let's say i arrive at 15% less. So now I just consider that 15% as my overtime pay. I don't think anyone at google gets overtime pay. Yet they attract greatly talented staff. They do that by cool projects and lots of cool employee rewards and fun working conditions. I wouldn't say that 50hr weeks is the expectation at my job, but there is a tacit agreement that "you're gonna work hard". Remember, it's not "work" if you like what you do. 
You use the function in the order by 
You incorrectly used the alias in the order by section instead of the formula
What you said
Can you elaborate please? 
Bitrate is the alias (column header). You can't use the alias in the order by section. Instead copy the formula from the select portion of your statement before "as bitrate" and then paste that where you have the word bitrate in the order by statement. It would be easier for me to compose the code for you but I think it will make more sense if you follow my directions.
Care to explain why you're advocating using the function in the order by instead of the alias? There are no tables or anything being aliased - I don't see a problem using the alias in the ORDER BY. Have I been missing something?
I'm calling the new column name the alias. I understand tables have aliases so I'm probably using that term in a confusing way. I've worked in sql server and Oracle for a while and my experience has been that using the new column name (what I called an alias) instead of the formula in the order by section throws an error.
I'm primarily an Oracle guy. I've never encountered problems using a column alias in the ORDER BY as illustrated here... 
thank you, you have been very helpful, you have gone above and beyond! that is super cool!! im going to bookmark sqlfiddle, seems like a great site. :D 
I would make sure there are not duplicates as well. It is a best practice to fully qualify not only the fields being called but the database and schema as well. My example below assumes there is a primary key. If there is not you may want to run a distinct count on your product identifier field in order to make sure duplicate entries are not a problem. Ie - as a replacement for #1. Select (&lt;primary key field&gt;) from mysql.dbo.product_table #2 Distinct count Select distinct (&lt;product_no&gt;), count (&lt;product_no&gt;) as [count] from mysql.dbo.product_table Group by &lt;product_no&gt; Order by [count] desc In the example above replace "&lt;product_no&gt;" with the field you want to count. I have placed the alias "count" in brakets as well since count is a sql keyword. Hope this helps!
https://en.wikipedia.org/wiki/De_Morgan%27s_laws
Thank you!
change the code. Not really sure what "global fix" you are talking about. There is none. In SQL 2005 a number of things changed, and a number of features were deprecated. 
it is a project I have recently taken over/inherited. Trying to make the best of it.
Even if it is easier to do a 'global fix', the correct thing to do is to fix the queries. The SQL standard states that the only way to ensure results are ordered are by using ORDER BY. 
Shouldn't one of the ANDs be an OR? Either it's a typo or your audience fails to understand it because it is wrong.
How? Let's say it's alphabetical are you going to reorder the records each time the letter A is used?
Perhaps he's referring to clustering on the sort order? I'm not saying that will work, but that's where my brain leads me. 
Why are you on a 32-bit OS?
SELECT [Table A].ID, CASE [Table C].Gender WHEN 'M' THEN 'Male' WHEN 'F' THEN 'Female' END FROM .....
Install Java 8 JDK for Windows x86 (this is the 32 bit version). Then install SQL Developer 32 bit.
The error is correct, as your sample isn't well formed XML. You are using an XML element name as an attribute. Attributes are exactly that, an attribute of an element, so have their own name. These would be correct : &lt;macro_id id="50" /&gt; &lt;macro_id&gt;50&lt;/macro_id&gt;
Thank you
Even this is just an implementation coincidence though. SQL Server has a tendency to return rows in order of clustered index, but its not a guarantee. Indeed without an ORDER BY, the order in which rows are returned is completely undefined. 
Even if this "seems to work", it doesn't really (see /u/pease_pudding's response). Plus, if you're doing a lot of inserts, or even updating the field which you're clustering on, you're going to have a performance hit.
Moving to SQL Server 2008 is just going to leave you going through the upgrade process *again* in a year. Upgrade to the latest version possible now.
Yeah, people are forgetting that you're here asking for help. I work in a software support role with a heavy amount of SQL operations. I have to combine the "right" way of doing something with what a customer is asking for. In this case we are not quite sure what a global fix is because we don't perceive there being anything wrong about the tables. Is the data corrupt? Is it old to the point where data types are not shared? What's inherently considered "wrong" about them? Questions like these can prevent you from making costly mistakes. So please be sure to consult someone who asks these questions. People will throw answers at you and if your business can't fit that solution, forcing it may be a disaster. So ask away and don't just take the first answer regardless of whether it turns out to be the right thing. You'd rather do the homework and get what you need then immediately implement a solution that can cause major regret.
Exactly. I really appreciate all the time all of you are taking to write and read this and specifically appreciate your help too.
Thank you! that worked.
Does TableC always (via constraints) contain "M" or "F"? If so, then what /u/ziptime said is fine. If not, then the statement as constructed will (I believe) return a NULL if Gender is something other than "M" or "F". If that's OK in the context your SELECT lives in, then all is well. Otherwise, you should include an ELSE. For example: SELECT [Table A].ID, Gender = CASE [Table C].Gender WHEN 'M' THEN 'Male' WHEN 'F' THEN 'Female' ELSE 'Unknown' END FROM ... (I also added the column name "Gender" -- it can be whatever you want -- just a best practice demonstration. And I reformatted the CASE to MY particular preference for readability... YMMV) 
Neither of the patterns you're looking for (`source=` and `&amp;eng=`) exist in the string. This works, DECLARE @myvar varchar(255) SELECT @myvar = 'http://www.url.com/page.jsp&amp;notsourceid=123&amp;sourceid=456789&amp;engid=101112' SELECT SUBSTRING(@myvar , CHARINDEX('&amp;sourceid=', @myvar)+10 , CHARINDEX('&amp;engid=', @myvar) - CHARINDEX('&amp;sourceid=', @myvar)-10) AS 'SourceID'
That was a typo. I'm using the same syntax as you but am getting some invalid lengths causing an error.
I think you're thinking small. with cte as ( SELECT O.splitdata FROM ( SELECT *, cast('&lt;X&gt;'+replace(F.my_field,'&amp;','&lt;/X&gt;&lt;X&gt;')+'&lt;/X&gt;' as XML) as xmlfilter from #my_table F )F1 CROSS APPLY ( SELECT fdata.D.value('.','varchar(50)') as splitdata FROM f1.xmlfilter.nodes('X') as fdata(D)) O ) select left(splitdata, charindex('=', splitdata) - 1) as url_param ,substring(splitdata, charindex('=', splitdata) + 1, len(splitdata) - charindex('=', splitdata)) as url_value from cte where charindex('=', splitdata) &gt; 0 for your url http://www.url.com/page.jsp&amp;notsourceid=123&amp;sourceid=456789&amp;engid=101112 this produces: url_param | url_value ---------|--------- notsourceid | 123 sourceid | 456789 engid | 101112 
Well, I stand corrected. It works. I was wrong. It parses. Your original should work.
Is this what you're looking for? select unqiueUserID, location, COUNT(special) AS specialCount from ( select distinct uniqueUserID, location, special from UsageTable WHERE special = 1 AND date BETWEEN '08/01/15' AND '08/09/15' ) a
The use of temporary tables, derived tables, or inline views (simliar to a CTE in MS SQL) can help break this problem down to get the end result. Trying to compare too many variables at once otherwise might get tricky and not return the right results. 
That seems to be what I need to do. Looking at the subquery information aroung google, some people show using the ROW_NUMBER() OVER clause, is this needed or can it be excluded? It seems to return results fine either way at this point. 
I've heard bad things about temp tables due to their slower speed. The data set I am working with will only return about 3,000 rows, but just looking at what it should return in the WHERE (without group by) is over 50,000 rows.
True, even at 3,000 rows it'd be pretty sluggish. There are easier ways of solving this, but it's always an option to break down these multiple variable requirements. 
Can also be written as: SELECT [Table A.ID], CASE WHEN [Table C].Gender = 'M' THEN 'Male' WHEN [Table C].Gender = 'F' THEN 'Female' END Gender FROM ...
Since I haven't received any responses, I will share my workaround for future travelers. However, I would still love an answer if someone can help! First, in my stored procedure that calculates the date/time differences, I replaced all negative and unknown values with 0. Then, I added separate denominator columns for each item. So, in the example above I had: CASE WHEN MinutestoContacted = 0 THEN 0 ELSE 1 END as MinutestoContactedDenominator Then, in my MDX I used that in place of the complicated Filter statement. This allows me to continue to use the Contacted Count as the count of all that were contacted, but only use those with a positive value when calculating the average. For the CASE statements, I actually added them in a view instead of as physical columns on my fact table. My data source view references that view in a named query instead of the table. That makes it easier for me to add new ones as needed. However, physical columns would have worked just as well.
To aid mobile-users, I'll link small subreddits, which are named in the title, yet are not linked. /r/answers: reddit answers: a knowledgebase built on reddit --- ^I ^am ^a ^bot ^| [^Mail ^BotOwner](http://reddit.com/message/compose/?to=DarkMio&amp;subject=SmallSubBot%20Report) ^| ^v0.6 ^| ^[Changelog](https://github.com/DarkMio/Massdrop-Reddit-Bot) ^| [^Ban](https://www.reddit.com/message/compose/?to=SmallSubBot&amp;subject=MassdropBot%20Report&amp;message=ban%20/r/Subreddit) ^- [^Help](https://www.reddit.com/r/MassdropBot/wiki/index#wiki_banning_a_bot)
I'm 3 years out of uni with work experience and recently got myself a new, decent job. Personally, I believe I should be happy to work long hours because I see this paying off in the long run in terms of experience. As a junior doing a BA role, would you agree with this point of view?
Under the first SQL Quiz, questions 7 &amp; 8 have the correct answer marked. And AFAIK, #4 has no valid answer offered, and #15 also has no valid answers because the strings aren't quoted. When I submit my results, the little pop-up says "Java quiz result" but the body says SQL quiz. It says that #4's answer (how to delete data) is INSERT (um...what?), and #15's answer...well, none of them quote the strings, so they're all wrong. This "quiz" could do with a **lot** of proofreading.
Thank you for your remarks but after submitting what you see se checked is the real correct answers, those which you checked before you'll see a "!" near them. 
Do there tend to be any performance differences between distinct and a group by in this case?
absolutely. Almost everything in this biz, is "on-the-job-training". I came out of school with Cobol and Fortran. I don't use them anymore. :) So the more hours you work the smarter you will get, just by osmosis. Plus if your main tool as a Business Analyst is SQL, you will start to build up reusable sets of scripts that you can leverage for the next set of problems they throw at you. In my experience, as a junior, I always tried to be the go to guy in some specific area. It gets you recognition, opportunity and job security. "Oh you want XX data from the YY system? Go to EatDiveFly, he got if for me last time." You end up building a fan club as well as expertise. When you go for your next job, it's always important to stress in the interview how much of a hard worker you are. Talk about a project that required 60 hour weeks for a whole month, and how you delivered it on time. That's the kind of thing i look for when I interview. I can't come out and ask, are you willing to work 60hr weeks occasionally, but if you offer it up, I'm gonna notice. Find a balance that works for you (and your family). Long hours are an investment. 
That gets to the heart of it much more quickly and efficiently.
Yeah I'm always wondering because my stuff usually "just works," so I don't question it too much unless that ceases to be the case.
I think he would probably email you but he deleted your email address while trying to insert it.
I'm not really sure what you're asking but I'll try to go through both queries and hopefully others will chime in to correct me. Select sum(some_value) as Sum1 sum(some_other_value) as Sum2 From Table So we want to select the sum of two things from a basic table. Nothing to see here folks. Where Condition1 = Reference The only time we want the sums to calculate is whenever Condition1 (=), or is the exact same, as a Reference. For example, I want to sum sale items, and revenue but only for the current month. (*Where Condition1 = 'August'*) Having ( sum(some_value)&lt;&gt;0 AND sum(some_other_value)&lt;&gt;0 ) So normally you need to GROUP BY to use a HAVING statement, so, e.g., SELECT Condition1 , sum(sale_count) , sum(revenue) FROM Table WHERE Condition1 = Month(Getdate()) GROUP BY Condition1 But you want to drill in a little deeper. You know your table keeps track of returns and exchanges, such as a table that looks like this: Condition1 | Item_No | Sale_Count | Revenue | Return_Count | Exchange_Count |:------|:------|:------|:------|:------|:------ 8-1-15 | 1673x | 1 | 200.24 | 0 | 0 8-2-15 | 125vx | 1 | 291.16 | 0 | 0 8-3-15 | 1613x | 1 | 68.18 | 0 | 0 8-4-15 | 171xt | 1 | 74.77 | 0 | 0 8-5-15 | 1673x | -1 | -200.24 | 1 | 0 8-6-15 | 267at | 1 | 721.24 | 0 | 0 8-7-15 | wv21g | 1 | 11.95 | 0 | 0 8-8-15 | 171xt | -1 | -74.77 | 0 | 1 8-9-15 | av17r | 1 | 16.22 | 0 | 0 So: Having ( sum(Return_Count)&lt;&gt;0 AND sum(Exchange_Count)&lt;&gt;0 ) You only want to sum the sales &amp; revenue for the items that didn't end up being exchanged or returned. You're saying HAVING a SUM &lt;&gt; (not equal to) zero, AND then another sum that isn't equal to zero either. Having not ( sum(some_value)=0 AND sum(some_other_value)=0 ) So this is a little different. You're saying HAVING NOT a sum equal to zero, and another sum equal to zero. In this example data set there is no functional difference between the two statements, but each individual component can be slightly tweaked to yield wildly different results. EDIT: I'm pretty sure if you execute either of these queries on *any* data set that they should yield exactly the same results and failure to do so cannot be explained in terms of SQL, but rather only in terms of how your data is housed and what values are being summed, when/how they're populated, etc. Basically I'd be asking business questions and not SQL questions if I encountered an enterprise data set that gave separate answers for those example queries.
&gt; what rule would you want to apply then? If there was *only* an Assessment record (no other status), then I would also exclude the row. But, at least that's easy enough to filter out in other ways. I think your 2nd suggestion might be what I'm looking for (or at least gives me a starting point to tweak)! I'll let you know if it works out. Thank you again for your help.
You're welcome! As this is using IN performance will be better if you use a column which is indexed. 
I tried that, and get an error stating that the datefield name is invalid in the ORDER BY clause. Hence why I am hoping reddit can help. 
I think I understand your problem. The datefield is not a part of your final query. You have 2 options, you can either add a month indicator in your grouping (it will need to be in your select), or since it is only 5 months, you can do it manually. Option 1: SELECT Datename(month(datefield)), month(datefield), count(*) from whatever GROUP BY Datename(month(datefield)), month(datefield) order by month(datefield) Option 2: SELECT Datename(month(datefield)), count(*) from whatever GROUP BY Datename(month(datefield)), month(datefield) order by CASE WHEN Datename(month(datefield)) = 'Feb' THEN 2 WHEN Datename(month(datefield)) = 'Mar' THEN 3 WHEN Datename(month(datefield)) = 'Apr' THEN 4 WHEN Datename(month(datefield)) = 'May' THEN 5 WHEN Datename(month(datefield)) = 'Jun' THEN 6 END If you want to post your actual code I can change your code instead of posting pseudo code.
As described in my other comment. Option 1: SELECT companyName, DATENAME(month,dateRequestEnd) AS [Month], MONTH(dateRequestEnd), COUNT(*) AS Total FROM RequestLog (NOLOCK) -- Some SQL Stuff GROUP BY companyName, DATENAME(month,dateRequestEnd), MONTH(dateRequestEnd) ORDER BY CompanyName, MONTH(dateRequestEnd) Option 2: SELECT companyName, DATENAME(month,dateRequestEnd) AS [Month], COUNT(*) AS Total FROM RequestLog (NOLOCK) -- Some SQL Stuff GROUP BY companyName, DATENAME(month,dateRequestEnd) ORDER BY CompanyName, CASE WHEN DATENAME(month,dateRequestEnd) = 'February' THEN 2 WHEN DATENAME(month,dateRequestEnd) = 'March' THEN 3 WHEN DATENAME(month,dateRequestEnd) = 'April' THEN 4 WHEN DATENAME(month,dateRequestEnd) = 'May' THEN 5 WHEN DATENAME(month,dateRequestEnd) = 'June' THEN 6 END 
Thanks for the reply but still isnt working. I assume when I add PRIMARY KEY to id I dont need to repeat myself on the last line. 
What SQL platform are you using? In MS SQL Server you'd do this using a CTE or a temporary table: ;WITH units_to_use AS (SELECT policynum, COUNT(*) as num_records from theTable GROUP BY policynum) UPDATE tt SET units = 1.0 / utu.num_records FROM theTable tt INNER JOIN units_to_use utu ON tt.policynum = utu.policynum Let me know if you need clarification to understand any part of this. Note that if "units" is an integer field, then it's especially important that you use 1.0 instead of 1 in the division statement. 
MS SQL Server. I'll give this a shot as soon as I get a chance. Thanks!
Thanks. I was just trying to research option 2 on Technet, but wasn't exactly sure what it was saying, and how to relate it to this query. Seeing it makes it obvious.
I came up with a less verbose way to do it. We can convert DATENAME(month, dateRequestEnd) back to an integer using the MONTH() function instead. All we need to do is tack on a pretend day of month and year to turn it into a date. Like so: SELECT companyName, DATENAME(month,dateRequestEnd) AS [Month], COUNT(*) AS Total FROM RequestLog (NOLOCK) -- Some SQL Stuff GROUP BY companyName, DATENAME(month,dateRequestEnd) ORDER BY CompanyName, MONTH(DATENAME(month, dateRequestEnd) + ' 1 2015') 
Awesome! Just curious, why are you using text over char or varchar? The latter datatypes are faster and almost always work well with any type of text data. 
Sweet. I tested this out and it works. Seems like there should be an easier way, but hey, this works, so I'm cool with that.
Well for learning I guess it doesn't really matter, but in the real world, varchar is much, much faster, especially if a lot of data is being stored. 
The MONTH() function gives the month number like you're looking for - January = 1, February = 2 etc. But it doesn't the NAME of a month as an input. It takes a full date - month, day and year. So instead of just sending it the name of the month as determined by DATENAME(month, dateRequestEnd) we tack on the string ' 1 2015' so instead of calling MONTH('February') we're calling MONTH('February 1 2015') which correctly returns the value of 2. So ordering it by the output of the MONTH function puts our results in the order you want.
It helps if you post the actual code that's giving you the error. Unless I misunderstand what the problem is? 
***Edit: Whoops, didn't read your post carefully, so this may not be too helpful.*** If you're just learning it's usually: Server type: Database Engine Server name: &lt;your computer's name&gt; Authentication: Windows Authentication Connect. ... Otherwise, it's: Server type: Database Engine Server name: &lt;your computer's name&gt;**\**&lt;instance name&gt; Authentication: SQL Server Authentication User name: &lt;your username&gt; Password: &lt;your password&gt; Connect. ...
If I understand, you don't need the @prod_num. Just use prod_num in the where statement: where prod_num in (7382651,515637,938726,)
Hoping it's similar enough to MS SQL to be helpful to you :) - think this'll do it: SELECT DISTINCT user_id, taking_id, exam_id FROM development_table t1 where exists (select t2.user_id from development_table t2 where t1.user_id = t2.user_id group by t2.user_id having count(distinct t2.taking_id) &gt; 1) ORDER BY taking_id ASC; ?
From someone who is stuck on SQL Server 2008r2 thanks to legacy code: There are some gotchas out there between SQL Server 2005 and 2012 that can bite you, hard. [Here](https://msdn.microsoft.com/en-us/library/ms144262%28v=sql.110%29.aspx) is a page that shows what was removed for 2012, which is when setting compatibility back to 2005 quits being an option. /u/DrTrunks provided a link for 2014 which is great, but you're going to want to look for similar documentation on changes (deprecations, removed functionality) for 2005 to 2008, and 2008 to 2012, just to be safe. I'm stuck waiting on our development team to update a bunch of stored procedure logic that still had the old RAISERROR syntax (no parentheses) before I can move us to 2012. It's basic grunt work, almost menial, but the number of stored procedures combined with all of the customers at various build levels have caused management to push back on making it a priority. So for our internal environments we run Windows Server 2012 and SQL Server 2k8r2 Service Pack'ed as far as we can, and we recommend the same for our customers that host the product themselves. Like others have said, testing is going to be your friend. Good luck.
Are you pushing or pulling? If the source server is the link server, your pulling which is good. The table your inserting into have alot of indexes? Are you sorting while your querying? 2gbs over a link server shouldn't take very long. 
&gt;Are you pushing or pulling? The reason I use OPENQUERY is so I don't have to worry whether I'm pulling or pushing. It's executed remotely. &gt;The table your inserting into have alot of indexes? I just do INTO #TempTable. I believe it creates indexes automatically. Inserting would actually be faster without indexes, so this isn't it. &gt;Are you sorting while your querying? I don't think so. I don't think it would make a difference since the query is executed remotely. It's not that big of a table to sort. &gt;2gbs over a link server shouldn't take very long. I've done this before but for smaller data sets. The transfer speed tops out at 2mbps every time. I want to know how the other method gets around this limit.