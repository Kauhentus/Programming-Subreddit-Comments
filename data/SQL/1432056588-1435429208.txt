sorry I'm still confused.. (I'm a beginner). Just to clarify my original post, I want to capture and sum primary key ranges.. that's what I meant by rows my query is pretty basic select * from table the primary keys are the product numbers.. does rn= still play a part here 
How about some example data, and an example of what you want as an output? We have a difficult time guessing what you want based on a schema that we've never seen.
Interesting concept, it's like foreign wrappers but already fully installed. Anybody knows what version of SQL it uses?
Why are you grouping it this way?
lol, I just threw that together since I can't paste sensitive company data on a website, can't get in trouble.. i'll try that now..
I just read one of your other comments, my query won't roll it up the way you described. But it WILL create another column with the proper sum. Is there some other column on the table that would indicate that these rows need to be summed up? Usually there is a Foreign Key or indicator that items could be grouped by.
Well what I mean is aside from grouping based on the row number, what is the goal? Why are these grouped? They don't seem to be related entries based on what I see.
so the id column is unique? then how come 4 and 5 are both tree climbing? this grouping still doesn't make sense perhaps you need another table that can specify the "real groups" that you want summarized
that's just something I threw together real quick. What do you mean by the grouping doesn't make any sense? Here in case this helps understanding what I'm getting at. so 5-10 I would want grouped together since they are home activities. http://i.imgur.com/Iod4skU.png 
this is just a quick schema I made real quick. Rows 5-10 are grouped because they are part of a specific category. In my real life application of this the logic makes more sense but I can't go pasting company data on reddit. M Maybe this example will make more sense http://i.imgur.com/Iod4skU.png Hope this helps understanding what I'm getting at. so 5-10 I would want grouped together since they are home activities. 
Just copy your table headers and fill it with dummy data for us. I am trying to help, sorry if I am seeming douchey. 
I kinda figured that's something you were going for. First, you can get the values. I would do a case statement to insert into a temp table. I think you can do a group_by on case statements, but thats ugly. Create table #Table2 ( Type varchar(20), Minutes INT, (If you are going to have decimal points do DECIMAL(10,2) Calories INT, heart_rate ) Insert #Table2 select (leave your ID out for this, it will just complicate things.) case when table1.Type = 'activity_A' then 'Home_Activities' '' (go through your list of activities that should be the same in this syntax) '' else table1.Type end as Type, table1.Minutes, table1.calories, table1.heart_rate You are going to then select from table 2 and sum the values with group on table2.type You really *REALLY* need a category table or an additional column for this.
I worked in the education industry for some years and I had a curriculum started a while back but never applied it as I simply didn't have the time to teach being overworked as I was running all the student information systems for a large district with 2 staff. SQL is like learning nuclear fusion. Building a nuclear reactor to harness nuclear fusion is like PHP. You are creating an interface in which everyone can utilize SQL, users may unwittingly not know they are using SQL because they are pushing buttons on a webpage that is making SQL calls to a database and interacting with the database just like someone is unwittingly knowing they are harnessing nuclear power when they plug in their vacuum and turn it on. What is mysql? Mysql is a database engine. Just like MSSQL, Oracle, MySQL and PostgreSQL are all database engines. Access is a database engine as well. We take questions about Access here but many times we refer people to /r/msaccess as its language its similar to SQL but it uses functions and methods differently from the more enterprise SQL engines. &gt;Should i learn php to make all this stuff work or is access and using sql within access enough? It depends on what you are trying to teach your students. Databases alone can totally turn off students. Creating a table, adding records to a table and then creating queries is boring! A students view would be "how does this make the next facebook?" Heck, you seem bored with the stuff yourself :) You need to teach simultaneously how to apply the knowledge of databases while teaching them databases. Should you mix PHP and Access? I wouldn't. Access interacts with applications through ODBC and its machine specific. MSSQL, MySQL, Oracle, PostgreSQL can use ODBC to connect them but you really really want to use ADO or OLEDB when creating web applications. I'm jumping too far head though. My recommendations: Step 1. Don't teach students how to use access. It wont go much further from there. Step 2. Create your own MySQL/PHP host. Download [Xammp](https://www.apachefriends.org/index.html). The guys at apache friends basically created a whole suite of hosting tools rolled up into a simple folder. Simply install to any folder on your machine (I usually choose c:\xampp\). The program will install a [dashboard like this one](http://butlerccwebdev.net/support/testingserver/xampp-cpanel-running.png). It has http hosting (apache), mysql hosting, ftp hosting (filezilla), mercury (email hosting), and Tomcat which probably wont use. Hit start on the services you would like to use. You will be able to access a website hosted on your machine by going to http://localhost/. Users on other machines will have to navigate using your computer name http://COM-LAB-TCHR/ as example. So lets say you install it to C:\xampp\ In C:\xampp\htdocs is where your http documents go, such as php/html ect. In htdocs create a user for each of your students. C:\xampp\htdocs\jsmith C:\xampp\htdocs\tjohnson C:\xampp\htdocs\bhoward .... You can then navigate to their folders like http://COM-LAB-TCHR/jsmith/ although they wont contain anything. Using filezilla in xammp you can click on "admin" and add users. [Here is a good tutorial](https://help.webcontrolcenter.com/kb/a1069/filezilla-ftp-server-setting-upconfiguring-new-users-groups.aspx) but ignore step one as you are opening filezilla from xampp and don't have it natively installed. Give the students full rights to their own folder. For MySQL click on "admin" and it will open phpMyAdmin, it is a MySQL administration webpage (just happens to be written in php). You'll need to create a database for each student. https://www.youtube.com/watch?v=-JwXtoJeos8 You'll also need to create a user/password for each student. https://www.youtube.com/watch?v=OE5WanVr9-8 Students can log in and create tables and insert records using phpmyadmin using the url http://COM-LAB-TCHR/phpmyadmin/. I would recommend though that each student download and use http://dev.mysql.com/downloads/workbench/ which a mysql suite similar to what MSSQL DBA's and developers with SQL Server Management Studio. The workbench and SSMS are great for beginners. PHPmyadmin is really a tool when your doing light administration and running a script here or there. So... you now have http/ftp/mysql hosting for each of your students. The bare minimum of what they need is an FTP client (download FileZilla ftp client) and a text editor. I'd strongly recommend you try the mysql workbench as an option. Step 3. Students will create a table in their own databases. Step 4. Students will create a simple select query. Step 5. Students will create a mypage.php. The code will be tailored from this example http://www.w3schools.com/php/php_mysql_select.asp The tailored code will look like &lt;?php $servername = "COM-LAB-TCHR"; $username = "jsmith"; $password = "jayrulezzz"; $dbname = "jsmithDB"; // Create connection $conn = new mysqli($servername, $username, $password, $dbname); // Check connection if ($conn-&gt;connect_error) { die("Connection failed: " . $conn-&gt;connect_error); } $sql = "SELECT column1, column2, column3 FROM MyTable"; $result = $conn-&gt;query($sql); if ($result-&gt;num_rows &gt; 0) { // output data of each row while($row = $result-&gt;fetch_assoc()) { echo "id: " . $row["column1"]. " - Name: " . $row["column2"]. " " . $row["column3"]. "&lt;br&gt;"; } } else { echo "0 results"; } $conn-&gt;close(); ?&gt; Step 6. FTP upload to students folder Step 7. See the results at http://COM-LAB-TCHR/jsmith/mypage.php Once that is completed. Move on to teach PHP forms and have them create a form that will INSERT a result into tables in their database then go back and view the results in the page that selects the data and [delete data](http://www.w3schools.com/php/php_mysql_delete.asp). Students can also learn how to not just echo the data back but they can also use that data to build an html table with the results from the select query. Teaching students the database and the interface can be mind blowing and you may set students on a great track as they are very passionate at a young age and that passion will drive them through higher learning when they get a taste of their potential career at a young age. 
I see. let me play around with this. Unfortunately my hands are tied on adding tables.. maybe I can do a workaround in Excel since my excel sheet is running the sql query in the background.. I can probably combine and "if" and "and" function to determine what the group name will be then just let the pivot table sum it up.. but ultimately I wanted to do this via SQL since I need to get my hands dirty as much as possible. Excel feels like a crutch in some ways
Yeah. Or perhaps the list should instead read 1. Used Java EE 2. Used Java EE 3. Used Java EE ...
I don't think the question is a particularly good one, and the course should really explain joins before nested queries - it'll make a lot more sense when you understand how tables are interrelated and joined. 
http://sqlzoo.net/
Whoa, does Oracle actually have an IDE with things like autocomplete for apostrophes? I've only worked with SQL Server and DB2. SQL Server is okay, but I would love some real functionality like that of Visual Studio (without third-party add-ins).
I'm by no means an authoritative source but not that I am aware of. I think that is just the hackerrank editor that does that. 
Yea I would put my sql skills as barely above beginner and all of those problems were super easy. Still good for beginners though. 
yeah, the leetcode ones are better, but they only offer MySQL.
yeah I've worked through that site and it's the kind of stuff I'm looking for but there aren't a lot of problems on it that i've found other than the training ones on the side (which are awesome, just hoping for more)
Try out the following web site. You've got both course and exercises online http://www.studybyyourself.com/seminar/sql/course/?lang=eng.
Just finished. Nicely done but easy.
I have a different definition of the word "challenge" it appears
Listing the tables like that in the FROM clause will give you a Cartesian product (also known as a cross product or a cross join), where every row of tblA gets paired with every row of tblB. Your WHERE clause limits this result set to the same as an INNER JOIN. There is no way to limit the result set of a Cartesian product to be the same as a LEFT JOIN. Cartesian product != FULL OUTER JOIN
I'm overthinking everything. The first question wants you to "print all the details of the table," so I'm querying from sys.objects, which is incorrect...
TSQL used to support the old non-ansi style joins using special = operators. If you're on SQL Server 2000 or your database's Compatibility Level is set to 80, it will still allow you to use the below: Inner Join is as above = Left Outer Join is *= Right Outer Join is =* The below is equivalent to a LEFT OUTER JOIN SELECT * FROM tblA, tblB WHERE tblA.column *= tblB.column 
ANSI SQL-92 was published in (surprise!) 1992. Why are you looking to **not** use the explicit JOIN syntax we've had for 23 years? AFAIK every RDBMS currently in use supports it.
This is great. Thanks for the link!
Create some very simple sample tables and ask them how a LEFT JOIN and INNER JOIN would work. If you're familiar with GROUP BY, create a sample table with a few colors ('RED','GREEN','BLUE','BLUE','YELLOW','GREEN') and see if they can get you a count of each color. Feel free to throw in a LIKE modifier question to get counts for colors that start with G (WHERE color like 'G%'). Difference between UNION and UNION all is a good question, as well as what PRIMARY KEY and FOREIGN KEY is. Order of statements when writing: (SELECT, FROM, WHERE, GROUP BY, HAVING, ORDER BY) All fairly basic SQL.
I just got in to work today, thank you for pondering my issue further. I'll test and fire up a response right now
They're looking for a `select *`, not the DDL.
Temp tables can be indexed, and if you are hitting the same table constantly in a proc it can make sense to load the data into a temp table to limit the number of queries and locks against the original table. Also, just because something worked for you in one scenario doesn't mean it's the only way to do it and all other ways are wrong. There are many factors to consider and CTEs are a valuable tool, but not the only one and certainly shouldn't be used in all scenarios.
&gt; Temp tables can be indexed but really they shouldn't. You can't say they shouldn't as a blanket statement, but I agree they shouldn't be in general. Both of the issues you mention sound like bad error handling. Table variables are a good option but in general suffer performance-wise once they get too large, in general I try to use table variables over temp tables but there are times a temp table is the best option. Indexed views can also be a good option, but can cause more overhead when modifying the data in the source tables because the indexed view also needs to be updated. There is no one right answer, any of these options could be the correct one given a specific scenario.
Difference between IF statement and CASE statement.
&gt; Indexes are permanent objects being placed on a temporary object. I'd also like to point out that the index is created on the temp table and does not live past the life of the temp table. If it were running this code twice would result in an error because the index would already exist. create table #tmp (i int) create clustered index tmpindex on #tmp (i) drop table #tmp
There's more than 1 way to skin a cat. You can use unions instead of case. select * from test_table where id &lt; 5 union select 5, 'home activities', sum (minutes), sum (calories), sum (heart_rate) from test_table where id &gt; 4 and id &lt; 11 union select * from test_table where id &gt; 10 ; 
This one combines unions and case: select case when id &lt; 5 then to_char(id, '999') when id = 7 then '###' when id &gt; 10 then to_char (id, '999') else null end id, type, minutes, calories, heart_rate from ( select * from test_table where id &lt; 5 union select 5, null, null, null, null from test_table union select 6, null, null, null, null from test_table union select 7, 'home activities', sum (minutes), sum (calories), sum (heart_rate) from test_table where id &gt; 4 and id &lt; 11 union select 8, null, null, null, null from test_table union select 9, null, null, null, null from test_table union select * from test_table where id &gt; 10 ) ; This adds two blank rows above and below the total line item. 
 SELECT AB- Jul 2014 AS 'July 2014', CLI- Dec 2014 AS 'December 2014, DM - Sep 2014 AS 'September 2014' FROM table; PS I am making the assumption all of these columns come from the same table.
I'd take a step back. Maybe you're not the best person to interview people right now. Although it's flattering to be asked, this also means you'll be linked to their success/failures. I agree with what has been said here: although technical ability is good it's more important that the person be ABLE to learn and knows how to work _well_ with others. Isn't this the reason why you're in the group and learning SQL as opposed to someone who knows SQL? If that's not the reason or you're not sure I'd ask for clarification about what kind of person they want to hire? Someone with less skill but more willing to learn/take a pay cut or someone who should be Team Lead material, knows different types of SQL in different environments and will be able to accomplish A, B and C. Some people will come in with code they say they've written. I like to ask them to explain, why it was important at the time and why they feel it highlights their skills. Everyone had to learn at some point. Most people have to deliver output. Writing SQL is basic. Writing it out in a text editor isn't really necessary. I expect people I work with to be able to figure things out. If they can't, and they need me to explain everything each step of the way, they're useless to me. I'll do it myself. Some positions require 'managing up'. Some are straight SQL jockey positions. If you have a clear understanding of your job and your position, then this should be be a direct parallel to who you wish to hire. If you don't, then you can expect your hire to be 'disappointing' because you really didn't know what you want anyway. There's a reason people use 'jargon'. If, however all they use are buzzwords, or wrong/incorrect terms that have no relevance there's a high chance they're BSing and/or they don't know what they're talking about. I know people who think this is not a 'big deal', but to me, if you can't communicate complex requirements or processes effectively I can't work with you. Other Red Flags: All their examples/conversations revolve around one tool, one job or one place. Unless that tool and all jobs are similar to yours they're gonna require a lot of training. Possibly more than you can provide effectively while doing your own job. Ask about Cartesian Products, Outer Joins, code optimization and a time when they figured HOW to resolve a SQL related issue. If it doesn't include 'I looked it up online' then I question whether or not they're going to know how to deal with unknowns. I also believe that SQL Analysts should have some basic ideas on how to minimize impacts on the databases they're hitting. If they have NO idea how running a complex query on an OLTP production system could negatively affect it, then I know they haven't done any serious work on large, corporate production systems.
What is the difference?
Right, I knew that but haven't used it in a while. Thank you.
In the system I have at my work, I have to remove the Duplicates in Excel. :(
Paste your current query Would help if you mention which database you're using, as the date function names can differ 
I go even further: SELECT empid = e.employee_id , fname = e.first_name , lname = e.last_name , startdt = e.start_date FROM employee e JOIN department d ON e.employee_id = d.employee_id AND e.key2 = d.key2 WHERE 1=1 AND d.dept_type_id = 7 AND e.employee_type_id = 5; For some reason the "=" syntax for column aliases doesn't seem common (or is looked down on?) but it keeps things organized visually for me, and allows me to flip statements into temp tables where necessary, easily.
Easy solution, convert the effective date to a string of the form "01-01" and do the comparison against the same. In Oracle, that would be "TO_CHAR( effective date, 'MM-DD' ) &lt;&gt; '01-01'" The downside is that indexes on the date are unlikely to be used because a function is being applied (unless the index is a function based index). Another alternative is to compare the effective date to the jan 1 version of the effective date (same year). In Oracle, there are a couple options - the first one truncates the date to the beginning of the year, the second converts a string (01/01/&lt;YEAR of EFFECTIVE_DATE&gt;) to a date. * EFFECTIVE_DATE &lt;&gt; TRUNC( EFFECTIVE_DATE, 'YYYY' ) * EFFECTIVE_DATE &lt;&gt; TO_DATE( '01/01/' || TO_CHAR( EFFECTIVE_DATE, 'YYYY' ), 'MM/DD/YYYY' )
Not familiar with DB2, but if it has DAY() I imagine it has MONTH() Maybe try something like AND NOT (DAY(EFFECTIVE_DATE) == 1) AND (MONTH(EFFECTIVE_DATE) == 1)) This will probably impact the queries ability to use an index, but the alternative is to prepopulate a table with the first date for every year, which is usually a worse solution since it needs maintaining over time 
Your second edit is the solution. Using that Day function, and the month function, make sure that the day isn't 1 and the month isn't 1 at the same time. If you want more help, post more complete query.
I don't, it was on a SQL competency test.
It was on a SQL competency test. I had never seen that syntax before.
I often ask the same question. PostgreSQL surpasses MySQL on pretty much every metric, but people are strange folk. I work with Enterprise RDBMS (Oracle, SQL Server), but if I was asked to start a new project where cost was a large consideration, I would definitely pick PostgreSQL.
**To check for dups in sql:** Select [UniqueColumn] COUNT([UniqueColumn]) FROM [Table] GROUP BY [UniqueColumn] HAVING COUNT([UniqueColumn]) &gt; 1 **To Delete dups:** DELETE [Table] WHERE [UniqueColumn] in (SELECT [UniqueColumn],count([UniqueColumn]) from[table] group by [UniqueColumn] having count([UniqueColumn]) &gt; 1)
I think OP is talking about values in a column, not column names. 
Weren't the early version of MySQL not logging (no transaction log, no wall) and therefore writing much faster than all other database? Because of this, everybody writting CGI / perl and php based websites were using it and it become the defacto standard of small web apps, it became the 'M' of LAMP.
It's definitely possible, he really didn't give us a whole lot to go with.
&gt; Basically its been around forever, it is so popular... Um, Postgres is from 1985. PostgreSQL is from 1994.
It all depends on how you view music. If you feel that the genre is attached to the artist, then how they are storing the data works (to a certain extent). If you feel that each song can have its own genre, then genre should be moved from artists to the songs table. That all said, likely you wouldn't want to store the same genre's over and over again in the same table. A significant improvement to everything would be to create a table for genres and then use a foreign key reference (assuming a 1:1 relationship) to the primary key of the genres table from either Artists or Songs (depending on your viewpoint above). Additionally, name should be removed from the Songs table and the primary key of the Artists table utilized as a foreign key in the Songs table so that you would pull the name of the artist of a song via a join between Artist and Songs. I typed this all pretty quick because I have to jet out the door. I hope it kind of made sense, if you want an example of what I'm talking about feel free to PM me and I can write up a sqlfiddle or something to demonstrate. Good luck!
But sqlite is not a good choice for concurrent access. 
PostgreSQL came out in 1996 from the research I've done. 
Assuming the PK is an autonumber and you're not worried about what the new ID is it'd be something like INSERT into tablename (field2, field3...) SELECT field2, field3.... FROM tablename WHERE conditions Put all of the fields except the PK into the field list.
 SELECT SUBSTR(CHAR(CURRENT DATE),7,4)||'-01-01'
So you'll probably have to start your statement with SET identity_insert table1 ON to let you write to the PK field. I assume you've got some SELECT statement that joins table 1 and 2 to get the PK so it should just be a matter of INSERT into table1(PK, field2, field3...) SELECT (table2.PK, table1.field2, table1.field3....) FROM table1 INNER JOIN table2 on table1.something = table2.something Make sure however you're joining table1 and table2 is unique so you don't end up with duplicates (you probably want to test the select statement alone before making it into an insert) and SET identity_insert table1 OFF When you're done. 
Early foothold in the market on major webhosts + name familiarity. That being said, more VPS solutions are starting to support Postgres, which is great.
I don't know what I would do without windowing functions. Or how I got by before I got comfortable with them.
That makes perfect sense, so having a reference table gives you the ability to change one entry that would in effect modify all the other entries in another table (Example, if an artist decided to switch from country to pop') So basically similar to using a vlookup on another table in excel.. You only need to modify the lookup table It does make sense, thank you
Yeap, if you want to look into it, the term is normalization and there are varying levels of database normalization.
assuming your first table is called persons, and your second table is called users (the one with 55 rows), and the first table's initials uniquely match the second table's initials (i.e. initials and profid are strictly one-to-one) then what you want is SELECT n.name , u.initials FROM ( SELECT DISTINCT name FROM persons ) AS n CROSS JOIN users AS u LEFT OUTER JOIN persons AS p ON p.name = n.name AND p.initials = u.initials WHERE p.initials IS NULL 
It worked!!! Thank you for this. I would have never figured this out myself. SELECT pa.fullname, pr.initials,pr.professionals,pr.profname FROM (SELECT DISTINCT fullname from TmpProfAssign) as pa CROSS JOIN Professionals as pr LEFT OUTER JOIN tmpProfAssign as p ON p.fullname = pa.fullname AND p.pinitials = pr.Initials WHERE p.pinitials IS NULL AND pr.QATTY = 'Y' AND pr.IsActive = 'Y' ORDER BY pa.fullname,pr.initials
thanks for the gold, man -- truly appreciated!!
I wrote the query and it seems to be working fine except when creating the table, one field (MemoValue) is throwing this error: The text, ntext, and image data types cannot be compared or sorted, except when using IS NULL or LIKE operator. In the original table, the values for this field are NULL. Here is the query: CREATE TABLE TEMP1 ( CustFrmDataId INT NOT NULL IDENTITY(1,1), FieldId VARCHAR, StringValue VARCHAR, NumberValue VARCHAR, DateValue VARCHAR, TimeValue VARCHAR, BooleanValue VARCHAR, MemoValue VARCHAR, EntityidValue VARCHAR, CurrencyValue VARCHAR)
PM me for my email address, i'll debug it for you 
OK, I am following as the newb I am..... So, in one table I may have 30 columns with names like "Order Number"... And I want to insert data from that table into a new table with column names like "order_number" and some columns in the new table do not exist in the old table (which I guess doesn't matter). So, what you are saying is I just need to write some rather specific insert select statements for each table I am importing, right? Forgive my Explain Like I Am 5 take.....
Nooope.
you understood correctly presumably your new names are worth keeping, so instead of changing them... the INSERT INTO part of the statement will list the columns using their new names, and the SELECT FROM part of the statement will list the columns using their old names 
I refuse to look at sql that is that poorly formatted.
There is not enough alcohol in the world that could make me debug this.
Some of this is needlessly verbose. &gt;N1.SKU_ID AS SKU_ID, O.SITE_ID AS SITE_ID, O.ORDER_DT AS ORDER_DT, Come on now, that is just redundant. O.SITE_ID will display as SITE_ID anyway, totally needless.
This is definitely the reason. Many people (or me, at least) were self taught (well, self taught w/ tutorials and such) after setting up our own LAMP machine and for MANY years never even looked for alternatives. 
used http://poorsql.com/ it still hurts to look at SELECT /*+ optimizer_features_enable('9.2.0') */ N1.ORDER_ID AS ORDER_ID ,N1.N1_STATUS_ID AS N1_STATUS_ID ,N1.SKU_ID AS SKU_ID ,O.SITE_ID AS SITE_ID ,O.ORDER_DT AS ORDER_DT ,O.D_TOTAL_PRICE AS PRICE ,RS.LEGAL_NAME AS RESELLER_LEGAL_NAME ,PRS.LEGAL_NAME AS PARENT_RESELLER_LEGAL_NAME ,S.COMPANY_NAME AS SITE_COMPANY_NAME ,O.ORDER_STATE_ID AS ORDER_STATE_ID ,ITEM.COUNTRY AS ITEM_COUNTRY ,E.COUNTRY AS E_COUNTRY ,C.CUST_SERVICE_REGION AS REGION ,RS.PARTNER_LEVEL ,O.FOLLOWUP_DT ,N1.APPROVER_CONFIRM_DT ,'N' AS REISSUE_IND ,sign(nvl(sum(DISTINCT (R.ORDER_ID * (- (SIGN(NVL(R.OVERRIDE_PERSON_ID, 0)) - 1)))), 0)) AS RSRC_CTRL_VIOLATION ,max(decode(p.email_address, : 1, 'Y', 'N')) AS MANUAL_APPROVAL_IND ,max(nvl(R.RESOURCE_CTRL_TYPE_ID, 0)) AS RESOURCE_CTRL_TYPE_ID ,nvl(CFA.POLL_STATUS, 'N/A') AS POLL_STATUS ,CFA.APPROVE_METHOD FROM ORDERS_N1 N1 ,ORDERS O ,ENTERPRISE E ,ENTERPRISE RS ,ENTERPRISE PRS ,SITE S ,ORDERS_RSRC_CTRL R ,PERSON P ,COUNTRY C ,SITE_ITEM ITEM ,C ERT_FILE_APPROVE_DV_T CFA WHERE N1.ORDER_ID = O.ORDER_ID AND N1.N1_STATUS_ID &lt;&gt; 3 AND N1.N1_STATUS_ID &lt;&gt; 10 AND O.CUST_ENTERPRISE_ID = E.ENTERPRISE_ID AND O.RESELLER_ENTERPRISE_ID = RS.ENTERPRISE_ID AND RS.RESELLER_REFERRED_BY_ENT_ID = PRS.ENTERPRISE_ID AND O.SITE_ID = S.SITE_ID AND O.ORDER_ID = R.ORDER_ID(+) AND O.ORDER_COMPLETE_IND = 'N' AND O.ORDER_ID = ITEM.ORDER_ID AND ITEM.ITEM_ID = CFA.ITEM_ID(+) AND ( ITEM.COUNTRY IS NOT NULL AND C.COUNTRY_CD = ITEM.COUNTRY OR ITEM.COUNTRY IS NULL AND C.COUNTRY_CD = E.COUNTRY ) AND O.ORDER_DT &gt;= to_date(: 2, 'mmddyyyyhh24miss') AND O.ORDER_DT &lt;= to_date(: 3, 'mmddyyyyhh24miss') AND BITAND(NVL(RS.ENTERPRISE_TYPE_BITMASK, 0), 32) = 0 AND C.CUST_SERVICE_REGION = : 4 AND ( P.PERSON_ID = ( SELECT max(person_id) FROM contact_site cs WHERE cs.site_id = o.site_id AND cs.contact_type_id = decode(N1.SKU_ID, 5, 19, 10, 20, 11, 21, 15, 25, 36, 29, 38, 33, 40, 37, 42, 41, 44, 45, 46, 65, 140, 61, 179, 69, 185, 73, 191, 77, 310, 117) ) OR P.PERSON_ID = ( SELECT PERSON_ID FROM ORDER_CONTACT_T WHERE ORDER_ID = o.ORDER_ID AND CONTACT_TYPE = 'COMPANY_APPROVER' ) OR ( N1.SKU_ID = 191 AND P.PERSON_ID = ( SELECT MAX(PERSON_ID) FROM PERSON ) ) ) AND O.D_PRIMARY_SKU_ID &lt;&gt; 220 GROUP BY N1.ORDER_ID ,N1.N1_STATUS_ID ,N1.SKU_ID ,O.SITE_ID ,O.ORDER_DT ,O.D_TOTAL_PRICE ,RS.LEGAL_NAME ,PRS.LEGAL_NAME ,S.COMPANY_NAME ,O.ORDER_STATE_ID ,ITEM.COUNTRY ,E.COUNTRY ,C.CUST_SERVICE_REGION ,RS.PARTNER_LEVEL ,O.FOLLOWUP_DT ,N1.APPROVER_CONFIRM_DT ,CFA.POLL_STATUS ,CFA.APPROVE_METHOD UNION SELECT /*+ optimizer_features_enable('9.2.0') */ N1.ORDER_ID AS ORDER_ID ,N1.N1_STATUS_ID AS N1_STATUS_ID ,N1.SKU_ID AS SKU_ID ,O.SITE_ID AS SITE_ID ,O.ORDER_DT AS ORDER_DT ,O.D_TOTAL_PRICE AS PRICE ,RS.LEGAL_NAME AS RESELLER_LEGAL_NAME ,PRS.LEGAL_NAME AS PARENT_RESELLER_LEGAL_NAME ,S.COMPANY_NAME AS SITE_COMPANY_NAME ,O.REISSUE_ORDER_STATE_ID AS ORDER_STATE_ID ,ITEM.COUNTRY AS ITEM_COUNTRY ,E.COUNTRY AS E_COUNTR Y ,C.CUST_SERVICE_REGION AS REGION ,RS.PARTNER_LEVEL ,O.FOLLOWUP_DT ,N1.APPROVER_CONFIRM_DT ,'Y' AS REISSUE_IND ,sign(nvl(sum(DISTINCT (R.ORDER_ID * (- (SIGN(NVL(R.OVERRIDE_PERSON_ID, 0)) - 1)))), 0)) AS RSRC_CTRL_VIOLATION ,max(decode(p.email_address, : 5, 'Y', 'N')) AS MANUAL_APPROVAL_IND ,max(nvl(R.RESOURCE_CTRL_TYPE_ID, 0)) AS RESOURCE_CTRL_TYPE_ID ,nvl(CFA.POLL_STATUS, 'N/A') AS POLL_STATUS ,CFA.APPROVE_METHOD FROM ORDERS_N1 N1 ,ORDERS O ,ENTERPRISE E ,ENTERPRISE RS ,ENTERPRISE PRS ,SITE S ,ORDERS_RSRC_CTRL R ,PERSON P ,COUNTRY C ,SITE_ITEM ITEM ,ITEM_FILE_APPROVE_DV_T CFA WHERE N1.ORDER_ID = O.ORDER_ID AND O.CUST_ENTERPRISE_ID = E.ENTERPRISE_ID AND O.RESELLER_ENTERPRISE_ID = RS.ENTERPRISE_ID AND RS.RESELLER_REFERRED_BY_ENT_ID = PRS.ENTERPRISE_ID AND O.SITE_ID = S.SITE_ID AND O.ORDER_ID = R.ORDER_ID(+) AND O.ORDER_ID = ITEM.ORDER_ID AND ITEM.ITEM_ID = CFA.ITEM_ID(+) AND ITEM.ITEM_STATUS_ID = 5 AND ( ITEM.COUNTRY IS NOT NULL AND C.COUNTRY_CD = ITEM.COUNTRY OR C ERT.COUNTRY IS NULL AND C.COUNTRY_CD = E.COUNTRY ) AND ITEM.START_DT &gt;= to_date(: 6, 'mmddyyyyhh24miss') AND ITEM.START_DT &lt;= to_date(: 7, 'mmddyyyyhh24miss') AND BITAND(NVL(RS.ENTERPRISE_TYPE_BITMASK, 0), 32) = 0 AND C.CUST_SERVICE_REGION = : 8 AND ( P.PERSON_ID = ( SELECT max(person_id) FROM contact_site cs WHERE cs.site_id = o.site_id AND cs.contact_type_id = decode(N1.SKU_ID, 5, 19, 10, 20, 11, 21, 15, 25, 36, 29, 38, 33, 40, 37, 42, 41, 44, 45, 46, 65, 140, 61, 179, 69, 185, 73, 191, 77, 310, 117) ) OR P.PERSON_ID = ( SELECT PERSON_ID FROM ORDER_CONTACT_T WHERE ORDER_ID = o.ORDER_ID AND CONTACT_TYPE = 'COMPANY_APPROVER' ) OR ( N1.SKU_ID = 191 AND P.PERSON_ID = ( SELECT MAX(PERSON_ID) FROM PERSON ) ) ) AND O.D_PRIMARY_SKU_ID &lt;&gt; 220 GROUP BY N1.ORDER_ID ,N1.N1_STATUS_ID ,N1.SKU_ID ,O.SITE_ID ,O.ORDER_DT ,O.D_TOTAL_PRICE ,RS.LEGAL_NAME ,PRS.LEGAL_NAME ,S.COMPANY_NAME ,O.REISSUE_ORDER_STATE_ID ,ITEM.COUNTRY ,E.COUNTRY ,C.CUST_SERVICE_REGI ON ,RS.PARTNER_LEVEL ,O.FOLLOWUP_DT ,N1.APPROVER_CONFIRM_DT ,CFA.POLL_STATUS ,CFA.APPROVE_METHOD ORDER BY N1_STATUS_ID ,ORDER_DT DESC
Okay, this is what I have so far. I've made a new table (TEMP2) and have copied over the values that I wanted: SELECT * INTO TEMP2 FROM CustomFormFields WHERE CustomFormFields.CustFrmDataId = (SELECT CustFrmDataId FROM Workcustomforms WHERE WorkID= @Destination GROUP BY Workcustomforms.CustFrmDataId HAVING CustomFormFields.FieldId IN (78, 71, 72, 74, 76, 137, 162, 167, 393, 620, 852, 878, 879, 880)); Then, I am attempting to modify these values but taking the primary keys from another table: INSERT INTO TEMP2(CustFrmDataId, FieldId, StringValue, NumberValue, DateValue, TimeValue, BooleanValue, MemoValue, EntityidValue, CurrencyValue) SELECT CustomFormFields.CustFrmDataId, TEMP2.FieldId, TEMP2.StringValue, TEMP2.NumberValue, TEMP2.DateValue, TEMP2.TimeValue, TEMP2.BooleanValue, TEMP2.MemoValue, TEMP2.EntityidValue, TEMP2.CurrencyValue FROM TEMP2 INNER JOIN CustomFormFields ON TEMP2.CustFrmDataId = CustomFormFields.CustFrmDataId WHERE CustomFormFields.CustFrmDataId IN(SELECT CustFrmDataId FROM Workcustomforms WHERE WorkId IN(SELECT PrintWorkId FROM WorkPrints WHERE MasterWorkID = @Destination)) So the original is being created into two copies, each with a unique primary key, since @Destination in the INSERT statement brings up more than one CustFrmDataId. Nevertheless, I can't get this to work.
How are the 1-5 reasons stored? Is it just a single free text column, separate columns, or a whole different table?
Thank you. I will look into the above references! I really want to be fully prepared! 
I am doing a contracted 6-month role. Might this change anything? I have experience with older relational database systems. This is my first role involving SQL.
Make sure you understand JOIN. specifically the difference between an inner and left join. The WHERE part is pretty straight forward. GROUP BY is important, but I don't use it that often.
 SELECT * FROM table_name WHERE reason_field_name IN ('DTI', 'ATP');
This is sad, painful and very funny. Oh so funny. 
Here is one way to approach it: WITH cte AS ( SELECT DISTINCT account FROM MyTable WHERE reason NOT IN ('dti','atp') ) SELECT * FROM myTable t1 WHERE reason IN ('dti','atp') AND NOT EXISTS (SELECT 1 FROM cte WHERE ct1.account = t1.account );
This isn't any more clear - I think you've got field names confused in your description of the tables. I still don't understand how we know which record we're selecting from table1 to use as the PK in record 2, i.e., how do I know that FieldID 7 and 4 are related to table1's WorkID1? Why do we have duplicates in what should be a PK? Maybe try writing the problem from scratch with a full explanation. I think we've gotten confused with a bunch of labyrinthine unnecessary workarounds. I've explained why your previous query wasn't working, and it's not clear what the constraints on the table are, or if you've tried setting identity_insert off. Schemas of the tables would be useful - in SQL management studio you can right click the table and script as &gt; create table.
Assuming you do not want to see any accounts at all that have that reason code SELECT * FROM TABLE1 WHERE ACCOUNT# NOT IN(SELECT ACCOUNT# FROM TABLE1 WHERE REASON IN('DTI','ATP'))
Yikes...multiple correlated subqueries!
I never understood what some people have against explicit joins. They are simple to read, easy to understand, and ANSI compliant so they work cross-platform.
My interpretation is OP only wants results if accounts had DTI or ATP but no other. So an exclusive OR (XOR) situation, not OR (which your IN clause is). So your predicate would return incorrect results, for example where the account also had other reasons other than DTI or ATP.
1. Copy it to Notepad++ 2. Remove duplicates through regex 3. Copy back to Excel [How to](http://stackoverflow.com/questions/3958350/removing-duplicate-rows-in-notepad)
 SELECT acctno FROM daTable GROUP BY acctno HAVING COUNT(CASE WHEN reason IN ('DTI','ATP') THEN 'ok' END) = 1 AND COUNT(*) = 1 
exactly. reason in ('dti', 'atp) gives me the accounts that have those qualifications, but those accounts may have other reasons and i need the acocunts that have ONLY dti and atp. i was thinking count(reason) having &lt;2 ??
it is just a text column
Because your side accepts it as a job done. Simple as that. The alternative is gating code from off shore places through someone. I do that here. It takes up a lot of my time and I essentially train new programmers through a weird trial and error basis. But all our DB code is according to my fascist conventions and I have nobody to blame but myself if I ever have to take a look at a query like that. Works for me. Not sure if it works for you.
Make sure you go through the [2012 feature matrix](https://msdn.microsoft.com/en-us/library/cc645993\(v=sql.110\).aspx) and confirm that you aren't currently using any Enterprise features. Also, will your 2012 Standard boxes have enough RAM to support a higher density of databases/instances? Can you consolidate the databases into fewer instances? Since you don't seem to have a lot of faith in your DBA, I'd recommend reading up on Accidental DBA material and getting yourself familiar with things in general. That way you'll know what to ask him, when he's BSing you, and when you to challenge him.
You could create a table of the values you need using a Common Table Expression, and then joining to it, like so: WITH TIMES AS ( SELECT * FROM (VALUES ( '7AM - 7PM' ,DATEADD(HOUR,7,CAST(CAST(GETDATE() AS DATE) AS datetime2)) ,DATEADD(MILLISECOND,-2,DATEADD(HOUR,7+12,CAST(CAST(GETDATE() AS DATE) AS datetime))) ),( '7PM - 7AM' ,DATEADD(HOUR,7+12,CAST(CAST(GETDATE() AS DATE) AS datetime2)) ,DATEADD(MILLISECOND,-2,DATEADD(HOUR,7+24,CAST(CAST(GETDATE() AS DATE) AS datetime))) )) AS T(DESCRIP, START_TIME, END_TIME) ) SELECT T1.descrip ,SUM(T2.count) FROM count_table T2 JOIN TIMES T1 ON T2.timestamp BETWEEN T1.start_time AND T1.end_time GROUP BY T1.descrip
Mathematically it's not strictly a simple XOR situation but is analogous to it. For a two input predicate.... OR =&gt; Output is TRUE if at least one of the inputs is TRUE. AND =&gt; Output is TRUE if all of the inputs are TRUE. XOR =&gt; Output is TRUE if one and only one of the inputs is TRUE.
Looking at HasSpecialOrderOption_CTE, I have a question... What are the possible values of Parts.SpecialOrder ? It looks like either 1 or null, based on your query. If that's the case, would it be faster to use distinct instead of top 1 ? The Isnull() part isn't needed in the CTE itself, because the WHERE clause only allows 1, so there are no nulls. If you take out the where piece, you might be able to use max() and a group by instead. You'd probably need the isnull() inside of your actual query, though. 
I figured it out! Thanks for all your help! 
Thanks, fixed.
It's a better idea not to modify the field and make the range dynamic instead. If you perform a function on a field, it invalidates any indexes or statistics that might otherwise help. `ActivityDate BETWEEN CONVERT(date,GETDATE()-30) AND GETDATE()`
ActivtyDate BETWEEN (GetDate() - 30) AND GetDate()
We do review code before it goes to production, but a lot of the code is old stuff that was here before I got here. I've stopped code like this from going through and forced them to re-write a few times. Due to existing schema design and other issues, we can't always stop it though...
Extremely relevant result. You always want to have your where clause run on values that don't involve modifying the field itself. In a one million record table this is the difference between two calculations (the begin period and end period) and 1 million calculations.
apply the function to getdate, not to the column value (see /u/jc4hokies' reply)
more or less this. however. i guess the issue would be that two parts could just randomly have the same price which is very strong and common . the other unique identfier would be if the price ended in .99. that makes the random chance much less.
Hmm.. I was afraid of that. On the .99 identifier, couldn't a part also end in .99, or do only single point pricings end in .99?
One of the things I'd check in your execution plan is if your CTE's are pulling all the records, or if an hash index is being built from line items prior to querying the ctes. also GetAssemblyInstructionValue can be created as a CTE as well using max just like we did the other CTEs. This should drop the execution time dramatically. I'll pm you my email if you want to send me the execution plan.
If it's for this situation exactly, you can do: [SQLFiddle example](http://sqlfiddle.com/#!3/e9add/1/0)
PIVOT operator in SQL SERVER
hopefully you didn't commit :) 
I learned a very painful lesson about having connections to multiple servers open at at the same time in a single session of SSMS. Especially when one of those connections is to DEV and the other is PROD. It becomes very easy to forget which server you are connected to when you drop a table. Thank the gods for backups. I recall one time in the dark distant past when I was working with one of the earlier versions of Sybase. This version had a bug in the error reporting for invalid columns in a GROUP BY clause. Basically, if you forget to specify one of the aggregate columns, it doesn't throw an error. It just goes ahead and produces garbage results. I had to interrupt my manager in a finance meeting to tell her that the report I gave her an hour earlier was pretty much just random numbers. It didn't help that she currently had that report up on the projector, and had made copies for all the VIPs in attendance.
A "+" where there should have been a "-". It's been decades, and I'm still scarred from that one.
Not my mistake, but an ERP system we used (very expensive, in development since the 70's, largely cobol) would occasionally delete entire customers, usually the most important ones, if one user tried to update the customer while another user already had them open. Rather than using update statements, they had chosen to delete and reinsert records, without wrapping it in a transaction. So it would delete the customer data, hitting a locked record at the very end, and fail, leaving the rest deleted. Personally I'm too afraid to type "delete" or "update" without typing the where clause first, and my own wrapper functions abort if it's missing. I did discover that if I have text selected in SQL Server Management Studio, and click run, it'll run the selected text instead of the entire query, but I can't remember what the damage was if anything.
Started a PHP function with a number (I think it was 404_check or something), instead of a letter or underscore. Didn't test it locally. Completely crashed the QA server.
That's exactly why at every new contract I tell the admins that I refuse to work on any production systems if I have admin privileges. If they need to, they can always create another non-privileged account that I will use day to day, and only use the admin account when I really have to. I once worked with a guy who was fond of saying "don't tell me what I shouldn't do - it's your responsibility to make sure that I can't do it". I have taken that to heart.
Haha, ouch.
My two worst fuckups: Was using EF and had a method to search customer contracts in the database. No idea how it happened but I put something like "Contracts.ToList().Where(c =&gt; myQuery);" and it made it into production. That did not make for a happy database. At one point I was tasked with rewriting some billing code as the old code could occasionally double submit bills. I am paranoid so I tested the hell out of my code - Was using real customer data, fake credit card numbers, and I made absolutely sure I was using the Test webservices for the payment processor. What I didn't realize was that the test webservices still emailed the customer - so one woman got hundreds of receipts for several thousand dollar transactions on her account. She was fortunately very understanding. 
Laziness. One of my developers, an older Russian guy set in his ways, does this all over the place. I've tried to get him to change to UPDATEs, but he says it's too much work .
My first DAY working on the DB at a new job I updated the entire client database, updating the "UpdateDate" columns of all related tables to GETDATE() because I forgot the WHERE clause. If it had been during polling time for our reporting mechanism, it would have A) brought our system to its knees and B) sent reports out to all of our clients as if their reports were recently finalized or addended (going back 12 years.) Luckily (or not, depending on how you see it) I was also the sysadmin. I was able to roll back to the most recent SQL snapshot and verify that no other transactions had occurred since. Things have gotten better, and I have learned to update much, much more carefully.
I like [Squirrel SQL](http://squirrel-sql.sourceforge.net/). It's a pure Java implementation and runs fine on Linux.
I had an SSRS subscription set up to export a file with a title that included a date from a table. The next day no subscriptions ran for any reports after the scheduled time for my report. The server was tied up for hours until a DBA killed my subscription execution. Well I wrote something like Select LoadDate instead of Select Max (LoadDate), and instead of exporting a single file, my subscription was trying to export the same file over 50,000 times.
Done exactly this. Thankfully the table was massive, so I noticed it was taking too long. Was able to hit "stop" in SSMS and it rolled back. Setup a default template in SSMS since then so every new window is pre filled with begin tran Rollback So at least I've got a level of protection and reminder, i execute every query in a transaction first to check the row count.
Delete statement without a where clause - Heart sunk... Since that day, run all my updates/deletes as selects first, and then I run the Update/Delete in a transaction.... number of records sound good, commit. 
I once ran a schema migration test script against a live environment for several minutes. I ran it, walked away to get coffee, and came back to realize I had dropped every index on one of our highest throughput tables in production. That was not a good day.
I have slight dyslexia so my common mistakes are usually getting something backwards. Most recent example that I have done multiple times is using age() in postgresql - getting the arguments backwards and then freaking out that my query returns no results. Also, when comparing dates in where clauses to see if they overlap using &lt; instead of &gt; or vice versa. I always have to make concious effort to triple check. It's right in my head but sometimes gets typed wrong.
Every time I inherit something with non-ansi joins that requires modification it's a simple "this is going to take at least X hours extra, because I have to rebuild it from scratch". I liken it to working on a car where someone has jerry-rigged some part of the engine together in an abnormal fashion. If I touch it, it's getting redone "right".
to help NOT make mistakes.... SELECT query that shows current situation you want to change BEGIN TRANSACTION UPDATE/INSERT/DELETE query to make your intended changes SAME SELECT QUERY AS ABOVE so you can check changes ROLLBACK TRANSACTION you can run this all day long and your changes won't get made until you change the rollback to a COMMIT I would also strongly suggest testing on a test copy of the db first if you aren't confident. As for my biggest mistake. Doing all the above then getting cocky and only highlighting the delete query and not the where clause that went with it. Don't script when you are tired or have a migraine! 
ultraedit
http://stickerish.com/wp-content/uploads/2012/03/OhGodWhyBlackWithTextSS.png
I found [this video](https://www.youtube.com/watch?v=rp8hvyjZWHs) somewhere else today. It seems appropriate here.
Wait a minute, you mean there are places where the same login gets you access to prod AND dev? Jesus.
I was telling my dad why database programming can be tricky: *"Today, the difference between a program that ran for .003 seconds and a program that would run for 3 days was one error in one line of code."* I was looping a couple hundred thousand times, and regenerating the in-memory index of a couple hundred thousand rows I would later search every. goddamn. time. That kind of defeated the purpose of why I was creating the index :P Moved the index generation step out of the loop, back to .003 seconds.
&gt; I would also strongly suggest testing on a test copy of the db first if you aren't confident. I work on a database with records going back to 1995. Test databases are essential.
What's wrong with SQL Developer?
I would've had to come clean to at least my manager and the intern. Especially to the intern. People recognize we are human and make mistakes from time to time. I find people to be more impressed by my honesty than disappointed by my mistake.
Omg, that's mortifying. 
you've now learned the value of begin transaction update table foo = 6 verify the update then run commit transaction
Worked at a magazine company we were only two guys in IT. My boss had to leave earliy and we needed to add a new user to our CRM. I managed to add the user but with wrong name, so I thought I delete the entry and add a new one. Accidentely cleared our users database. Had to call my boss who spent the evning finding a backup and restoring instead of doing what he was leaving early for. 
Maybe Eclipse with [Toad plugin](https://marketplace.eclipse.org/content/toad-extension-eclipse)
Not with the &gt; 1 in there.
0xDBE
I just had a conversation with my boss about this. Apparently it was taught that was faster some time back.
I did the same thing the first week on the job and ended up setting a bunch of records to record deleted status, all of the sudden people are wondering why they can't open their documents. I was able to resolve it fairly quickly but that was an awful feeling. Now whenever I save a query I have it set to use the train environment as my first line .
I usually do something like this: Insert into [EmptyTable] Select Distinct * from [Table] Delete [Table] Insert into Table Select * from [Empty Table]
I didn't personally do this, but one of my coworkers deleted a bunch if records from a live system. After a day of scrambling (and one of our most senior developers leading a recovery team) the site was restored to a functional state. The dev still works here, and now it's just a funny story. Early on I made a few really inefficient queries, but our product uses Oracle, so I can usually see if an unusual number of records is updated/deleted before I commit.
Everybody does this once, then you'll always use a select.
I always keep separate copies of management studio open for production and non-production
Protip: wrap any insert/delete/upate statements in a transaction. Then you can roll it back easy if the results are unexpected or disastrous. 
Yep its what I use
&gt; "...taking various data feed and trying to extract intelligence from them" seems vague I know, but what else can I tell you? I don't know anything about this job other than what you've told me (which to be fair, isn't much). &gt; will they teach me everything I need to know about SQL for the job Well if they listed SQL as a plus, I'm guessing they don't use it heavily right now, or are willing to train you I'm assuming you didn't tell them you knew SQL inside out on your application right? What if I gave you an excel sheet containing a table of.. ClientId, Region And another sheet containing Datetime, SalesPersonId, ClientId, SalesValue Could you generate a report showing each month for the past year, detailing which regions were generating the most revenue for each sales staff, and then show it alongside the average monthly revenue for that staff member? This sort of thing is trivial in SQL, but would probably be much more time-consuming to produce in Excel/VBA 
Separate from SQL suggestions, I would also familiarize yourself with common Finance terms of equations. Things like "Variable Costs" or "Contribution Margin" depending on the company.
I left a similar position not too long ago - it's a good idea to brush up on statistics and report writing as well. I would say that learning SQL now would be beneficial and depending on their db Access would be good too. 
It would help if you could post the code / query. How are you able to see the results are correct if the query is not completed? 
I made basically the same mistake as you did, about two weeks in, to a pretty critical table. Felt quite dumb.
select * from [dbo].[EOD] WHERE (Vol=null or Vol =0) and (OI=null or OI=0) Actually, I just wanted to delete them. But first I tried to run a select and see if I got the where conditions right and even tho the query never completes the execution I get a lot of values in the datagrid below. EDIT: Actually I don't know what happened or what changes but it finally did work. The query was the same and database hadn't been changed at all. So what was stuck without executing all morning now completed in around 5 mins.
I think http://stackoverflow.com/a/10112839/476309 explains what you should be doing.
Here's a cool site with exam questions: http://blog.csdn.net/rlhua/article/category/1638553 Hope it helps! :)
It's good that people publish this kind of thing. Although, with those `PAGEIOLATCH_*` numbers, I'd be hard pressed to say this customer needs to spend money on a new SAN. And with a virtual SQL Server, you want to be setting memory reservations at the hypervisor, not in SQL Server. Setting min server memory in SQL Server without addressing it in the hypervisor is a pretty good way to end up with a SQL Server running from page file.
I could be querying all 7000 catalogue items and then summing up past order totals on each of them from the sales data for each customer. I believe the problem is coming from summing up the sales data which is why I was thinking of keeping the relevant data in a temporary table. The query would be looking at ~25K records instead of 20M this way. I may be wrong in my approach, learning as I go.
I may not be understanding this correctly, but what I would like to do is create a temporary table that would have unique sales data for each user. A staging table would be unique would it not?
Maybe look at utilizing SUM with partitioning? SUM(salesValue) OVER (PARTITION BY catalogueId ORDER BY salesId)
Why do you need to segregate each user to a temp table? Why not use grouping or partitioning with a filtering clause. 
You should be putting an index on the userId or related PK for them. 20 million rows is nothing if this is all you need to do. 
I beleive there is an index already in the sales data table for the customer code. The query takes between 1 and 2 seconds currently, but under a second if I use a temp table. I am not the original creator of this database, maybe I will look into if the indexes are all set up correctly.
If you run the query without the temp table how long does it take? You might also want to consider using a view with a persisted computed column so you don't need a temp table and you can join the view directly. 
Query takes between 1 and 2 seconds without the temp table and under a second with a temp table.
That really depends on the SQL engine and what indexes are in place. Generally the best way is to try, there is always a bit of tweaking to be done. From the sound of it, partitioning or using windowed aggregate functions is going to be the way to go. *P.S. - Don't be afraid when you see them on the surface; they are somewhat complicated, but once you've used them a few times you can see when a problem would lend it to this.*
It would be faster in the temp table because it's already done all the work in setting that up. The server is still doing the work, it's just not apparent. 
The one that is named sql server? 
As I understand it, you **must** be using an OCI client in order to handle the password-expired exception... the JDBC thin client, unfortunately, simply doesn't have this ability at present. SQLPlus should always work, as should recent versions of Oracle SQL Developer (although it must be explicitly configured to use the OCI client from a full/instant client installation). Other applications tend to be, in my experience, very hit-or-miss in this area. No reason that an app using the JDBC thick or straight OCI client can't prompt for the password change, but a great number of them aren't coded to catch and handle this condition. By the way, the reason that your original sqlplus attempt wasn't working is probably that you needed to quote everything after the "@"... otherwise it assumes that everything up until the first slash is a username. So it would need to be entered as either: myusername@"myserver:1521/myservice" or myusername/mypassword@myserver:1521/myservice The above assumes that you'e using either an 11g or 12c client... I have a vague recollection that the 10gR2 client has some issues in this area, but it's been awhile and I can't remember the specifics.
What I've found is that in vmware SQL server will give up memory pretty quickly when it doesn't need it. With a bunch of virtuals you'll end up not having it when SQL server needs it. I don't go over board, just 4-8 GB depending on the situation. I'm in constant flux over my opinion of iSCSI. It's so damned slow compared to fibre channel. But I'm doing the right thing and not recommending the new SAN (yet!). It's that fine line between charging the customer for my time (which I of course like) and telling the customer just upgrade the SAN and after I've fixed the index problem and a few other things we're all done here. 
Yes, your want to use a cte with a row number then delete from it where row number greater than one 
Exactly, you can even make this sub query a cte and say delete from cte where row num&gt;1 
,*=
Just remember you're making the data distinct on the column in the group by and then must aggregate everything else
Exactly what are you doing that 2 seconds *isn't* a perfectly reasonable wait time?
https://www.hackerrank.com
Actually, thinking about it, I remembered why this looked strange to me. This bit me before. [The SQL Server doc](https://msdn.microsoft.com/en-us/library/ms189461.aspx) says: &gt; If ROWS/RANGE is not specified but ORDER BY is specified, RANGE UNBOUNDED PRECEDING AND CURRENT ROW is used as default for window frame. You've just made it a running total, I think. 
Neat, while I'll have to keep that in mind when I use this. Always appreciate learning something new. 
Thanks for your time and reply. Tried to run the code directly in the SQL console in phpmyadmin, didn't work.
I would recommend running a select statement with the same where conditions at least once before ever trying a delete. Just to be sure your only deleting the right records. 
Ok, I got it working. 1 thing left. How to only select those WHERE **field_data_field_reservation_checked_out.field_reservation_checked_out_value = 0**. Where would I implement that? The code below brings up all entries with the extra columns from the other to tables where **field_data_field_reservation_datetime.field_reservation_datetime_value** has past 15 minutes. Thanks in advance. SELECT field_data_field_reservation_datetime.*, field_data_field_reservation_checked_out.*, studyroom_reservation.* FROM field_data_field_reservation_datetime JOIN field_data_field_reservation_checked_out ON field_data_field_reservation_checked_out.entity_id = field_data_field_reservation_datetime.entity_id JOIN studyroom_reservation ON studyroom_reservation.reservation_id = field_data_field_reservation_checked_out.entity_id WHERE DATE(field_data_field_reservation_datetime.field_reservation_datetime_value) &lt;= NOW() - INTERVAL 15 MINUTE; 
Well, at that moment my reasoning was that he *did* make a mistake he shouldn't have (and he had done several times and we had already talked about it). The fact that the re-constructed numbers weren't right was secondary. But you're right, I should.
It's loading the information into an ASP page. While 2 seconds isn't bad, it isn't great either.
Dont waste your time. A portfolio should show a website/web application. If you know SQL your resume should just say you know SQL. Go to a training site and learn how to use SQL. Learn how to extract entities, create data models and entity relationship diagrams (ERDs). Learn what it means to normalize and de-normalize. Understand the terms DDL and DML. My suggestion is you install XAMPP on your PC (or MAMP on your MAC) and then go to http://localhost/phpmyadmin which is a front-end to mysql where you can play around with SQL. Here is the basics of what you need to learn: - how to create a database (using phpmyadmin and using CREATE DATABASE statement) - how to create tables (CREATE TABLE...) - how to create indexes (CREATE INDEX...) - when referential integrity is and how to create foreign keys - how to create users and how to grant permissions to users - how to insert data using INSERT statement - how to update data using UPDATE statement - how to delete data using DELETE statement - how to SELECT data using basic SELECT statement - how to join tables and use aliases inside a SELECT statement - how to group by, using having, using union, use subqueries (selects within selects) - how to write a basic stored procedure if you can do all the above then you know enough SQL to put it on your resume. Once you have the basic knowledge of SQL, and if one of your skills is a back-end languages like PHP then learn in PHP how to use PDO to connect to your database and issue SQL statements inside your PHP code. A typical example will be: - a html form where the user enters data and hits submit - a php program that recives the data and writes it to your tables - a different php program that selects the data and displays it on the screen (maybe in a table) Note there are many PHP/PDO/Mysql tutorial sites but when you search in google limit your search to the last 12 months to find more recent and up to date information. if you create a small web site using PHP and Mysql that is what you can put on your portfolio. - 
I like O'Reilly's [SQL Cookbook](http://www.amazon.com/Cookbook-Cookbooks-OReilly-Anthony-Molinaro/dp/0596009763/ref=sr_1_1?ie=UTF8&amp;qid=1432666882&amp;sr=8-1&amp;keywords=sql+cookbook). Don't only focus on SQL, because it also really helps to know relational database fundamentals. You don't need to know whether something's in a particular normal form, but you might want a high-level understanding of normalization, for example.
I use Sublime. Find a text editor with great regex support, learn them, love them.
Some good ideas here already. Don't be afraid to reheat well worn examples like video store or student enrolment, just make sure it's your own work. A few simple queries to show you grok Nulls, INNER vs OUTER JOINS and a performance trick or UDF used the right way. My 2c is do your portfolio in MSSQL Express, PostgreSQL, SQLite, Derby or pretty much anything but MySQL. These are much closer to the SQL standards, more transferable to other platforms and more "respectable" in the field. Now where'd i put my flameproof underwear? 
Going off of that - if you create an [indexed view](https://msdn.microsoft.com/en-us/library/ms191432.aspx), SQL Server will materialize the data and maintain the view, meaning that the query will essentially be against an indexed 25k row table.
I'm kind of worthless without being able to compile it myself, but this is my idea of it With InitialTable as ( SELECT DISTINCT a.max_TEA_InicioTarefa, analista, ETS.ETS_Sigla, ATC.ATC_Id, ATC.ATC_Sigla, PAT.PAT_Sigla, a.SRV_Id, ContratoComunicado.CCM_Id, ContratoComunicado.CCM_Docto, ContratoComunicado.CCM_Emissao, ComunicadoTipo.CMT_Descr, TarefaEtapaAreaTecnica_1.TEA_Revisao, TarefaEtapaAreaTecnica_1.ETS_Id, TarefaEtapaAreaTecnica_1.TEA_FimTarefa, TarefaEtapaAreaTecnica_1.PAT_Id FROM dbo.Tarefa AS Tarefa_1 INNER JOIN ( SELECT MAX(dbo.TarefaEtapaAreaTecnica.TEA_InicioTarefa) AS max_TEA_InicioTarefa, dbo.Pessoa.PFJ_Descri as analista, dbo.AreaTecnica.ATC_Id, dbo.Tarefa.SRV_Id FROM dbo.TarefaEtapaAreaTecnica LEFT OUTER JOIN dbo.Tarefa ON dbo.TarefaEtapaAreaTecnica.TRF_Id = dbo.Tarefa.TRF_Id LEFT OUTER JOIN dbo.AreaTecnica ON dbo.TarefaEtapaAreaTecnica.ATC_Id = dbo.AreaTecnica.ATC_Id LEFT OUTER JOIN dbo.Pessoa ON dbo.Pessoa.PFJ_Id = dbo.TarefaEtapaAreaTecnica.PFJ_Id_Analista GROUP BY dbo.AreaTecnica.ATC_Id, dbo.Tarefa.SRV_Id, dbo.Pessoa.PFJ_Descri ) AS a ON Tarefa_1.SRV_Id = a.SRV_Id INNER JOIN dbo.TarefaEtapaAreaTecnica AS TarefaEtapaAreaTecnica_1 ON Tarefa_1.TRF_Id = TarefaEtapaAreaTecnica_1.TRF_Id AND a.ATC_Id = TarefaEtapaAreaTecnica_1.ATC_Id AND a.max_TEA_InicioTarefa = TarefaEtapaAreaTecnica_1.TEA_InicioTarefa LEFT JOIN AreaTecnica ATC ON TarefaEtapaAreaTecnica_1.ATC_Id = ATC.ATC_Id LEFT JOIN Etapa ETS ON TarefaEtapaAreaTecnica_1.ETS_Id = ETS.ETS_Id LEFT JOIN ParecerTipo PAT ON TarefaEtapaAreaTecnica_1.PAT_Id = PAT.PAT_Id LEFT JOIN dbo.Servico ON dbo.Servico.SRV_Id = Tarefa_1.SRV_Id LEFT JOIN dbo.Contrato ON dbo.Contrato.CNT_Id = Servico.CNT_Id LEFT JOIN dbo.ContratoComunicado ON dbo.Contrato.CNT_Id = ContratoComunicado.CNT_Id LEFT JOIN dbo.ComunicadoTipo ON dbo.ComunicadoTipo.CMT_Id = ContratoComunicado.CMT_Id ) select IT.max_TEA_InicioTarefa, IT.analista, IT.ETS_Sigla, IT.ATC_Id, IT.ATC_Sigla, IT.PAT_Sigla, IT.SRV_Id, IT.CCM_Id as 'Pré-Advertencia ID', IT.CCM_Docto as 'Pré-Advertencia Docto', IT.CCM_Emissao as 'Pré-Advertencia Emmisao', IT2.CCM_Id as '1ª Advertência ID', IT2.CCM_Docto as '1ª Advertência Docto', IT2.CCM_Emissao as '1ª Advertência Emmisao', IT.TEA_Revisao, IT.ETS_Id, IT.TEA_FimTarefa, IT.PAT_Id from InitialTable IT left join InitialTable IT2 on IT2.analista = IT.analista and IT2.max_TEA_InicioTarefa = IT.max_TEA_InicioTarefa and IT2.cmt_descr = '1ª Advertência' where (IT.cmt_descr = 'Pré-Advertencia' or IT.cmt_descr is null)
You could make a game like Zork, except you can do so much better with triggers and all that. Permissions could be given on procedures based on objects that you hold.
Thank you very much, I think I was just missing a step and this is what I needed. I then run the script on the existing (production) server to export the data, and then import it on the testing server, right? Thanks again
Run with it. Once you know the basics the Updates and inserts are simple.
Saw George at a user group recently. He developed powershell scripts to do exactly what you want to do. worked like a charm. [https://github.com/gwalkey/SQLTranscriptase/](https://github.com/gwalkey/SQLTranscriptase)
Your basic DML and DDL commands. Plus Inner, outer, left, right, natural joins. Assuming it is for sql server... you could throw in some CTEs and linked servers to really wow them. Difference between a function and stored procedure. Difference between a temp table and a table variable.
THANK YOU!
I am a power-user of Outlook, Windows, IE, and Excel (knowing vlookups, index match match, logic statements, sumproducts, etc). My only concern is what will be expected from me in SQL (they know I have not coded in sql before). Thank you for the input! I want to become a valuable member of the team I will be serving - hopefully there is advancement opportunities! 
awesome, if you learn SQL, I would say you're good on skills then. Also it's quite likely you would just be pulling data with an UI like BusinessObjects, Hyperion, or Access, so don't stress too much, but prepare just the same.
Sounds like the key indexed and now it runs quicker.
I don't trust Notepad++ anymore at all, It totally breaks with find and replace for .dat files more than ~50,000 lines. It'll just fuck up with no error and remove some lines or delimiters. I've had it consistently happen. It can only handle small shit. I love its bookmark and regex stuff, but It just doesn't safely find and replace. 
If you're making a comparison based on the results of an aggregate function, try using the HAVING clause after your GROUP BY statement. this will evaluate your results example: SELECT ID, COUNT(*) FROM tblTest GROUP BY ID HAVING COUNT(ID) &gt; 1 This would show you any duplicate IDs and how many there are in the table. 
I agree that a restore from a backup is a better option; however, I have never had a problem using the import/export wizard from MSSQL to MSSQL for even 1M+ rows. It's just SSIS on the backend.
Khan Academy just released a SQL course. That should be a good place to start. 
To add to this, [Microsoft has a pretty good SQL course on edx](https://www.edx.org/course/querying-transact-sql-microsoft-dat201x-0). I just completed it a couple weeks back and learned a ton to feel comfortable with basic queries. Microsoft provides a lite version of their software and they also provide quite complicated data to work with for a beginner compared to my SAMS in 24 hours book. You can get a certificate too if you do well on the assignments and exam.
In 15 years, I've never seen someone (intentionally) use a natural join.
Stanford has an online course as well https://lagunita.stanford.edu/courses/DB/2014/SelfPaced/about
&gt; have never had a problem using the import/export wizard from MSSQL to MSSQL for even 1M+ rows If you go this route, create the schema on the destination *first*. Otherwise, things like calculated columns will be hosed.
My specialization was in databases, and only last month did I learn natural joins existed. They make my skin crawl, kind of like using aliases to get joins.
**EDIT : I completely failed to realise that this post was in /r/sql rather than /r/excel. Ignore the post below.** If you make a separate table for possible abbreviations - then VLOOKUP is probably the answer. Here's an example table, the formula in B2 ()which is then copied down) is :- =VLOOKUP(A2,D:E,2,0) Abbreviation | VLOOKUP Formula | | **Lookup Table** | | :--- | :--- | :--- | :--- | :--- | TAMSSSS | Taylors Automotive Supplies | | possible abbreviation | owner name | TAMS | Taylors Automotive Supplies | | TAMS | Taylors Automotive Supplies | STILLTAMS | Taylors Automotive Supplies | | STILLTAMS | Taylors Automotive Supplies | STILLBLG | Brown Automotive Supplies | | TAMSSSS | Taylors Automotive Supplies | BLLLLG | Brown Automotive Supplies | | BLG | Brown Automotive Supplies | BLG | Brown Automotive Supplies | | STILLBLG | Brown Automotive Supplies | TAMSSSS | Taylors Automotive Supplies | | BLLLLG | Brown Automotive Supplies | TAMS | Taylors Automotive Supplies | | | | STILLTAMS | Taylors Automotive Supplies | | | | STILLBLG | Brown Automotive Supplies | | | | BLLLLG | Brown Automotive Supplies | | | | BLG | Brown Automotive Supplies | | | | TAMSSSS | Taylors Automotive Supplies | | | | TAMS | Taylors Automotive Supplies | | | | STILLTAMS | Taylors Automotive Supplies | | | | STILLBLG | Brown Automotive Supplies | | | | BLLLLG | Brown Automotive Supplies | | | | BLG | Brown Automotive Supplies | | | | 
Something like this. Aggregates around case statements are pretty powerful. SELECT tbl.StoreNumber, MAX(CASE WHEN tbl.OwnerCode = 'TAMS' THEN tbl.OwnerName ELSE NULL END) AS OwnerName FROM tbl GROUP BY tbl.StoreNumber There's also the trivial example, but I assume there's other parts to the query. SELECT tbl.StoreNumber, tbl.OwnerName FROM tbl WHERE tbl.OwnerCode = 'TAMS'
I already have the job (and start soon). The interviewers know I have never coded in SQL before. My real question is: What should I know about SQL before I start the job?
Sounds like you could by my new coworker .... I think you can actually download an Express version of Teradata. Let me know if you have any questions about Teradata, since that's my environment currently. Oh, and here's a secret: SQL isn't that hard. Don't let yourself get intimidated.
I will ditto what /u/steven0r said about the excel part. For the SQL part, your basic ddl and dml. Look them up. they are not as difficult as the acronyms sound. If you don't have access to a db with data, you can use stackexchange's data. [https://data.stackexchange.com/stackoverflow/query/new](https://data.stackexchange.com/stackoverflow/query/new)
I keep pimping these videos simply because they did a great job. Simple and clean. Youtube: Wise Owl Turotials SQL Server Procedures and Programming https://www.youtube.com/watch?v=fjNsRV4zLdc&amp;list=PLNIs-AWhQzcleQWADpUgriRxebMkMmi4H and SQL SSRS https://www.youtube.com/watch?v=kTPJBAtv29k&amp;list=PL7A29088C98E92D5F
I agree with the guy who said make a lookup table, but since this is /r/sql left join to it instead of using VLOOKUP .... Just make a table that has the owner code, which it sounds like is consistent, plus the version of the name that you want. You can just do select distinct owner_code, owner_name into owner_lookup_table from source_table Then removes the duplicates until you have only the versions of the names you want.
I have been only doing SQL coding in MYSQL Unicode... Is my time practicing doing me any good? Do companies mainly use IDE's or a Command line (Unicode)? I don't want to be wasting my time if I could be using that time differently... 
I am studying for the 70-461 certification, and have been using material from a lot of different sources. I started with the [T-Sql fundamentals](http://www.amazon.com/Microsoft-Server-Fundamentals-Developer-Reference/dp/0735658145) book by Ben-Gan Itzik. You can also find the pdf online. This book has a ton of exercises, that got me started on the right track. Now I am using the training manual for 70-461, and the stairways series on Sqlservercentral.com to supplement. I also just signed up for the 5 week Querying with Transact-SQL course on Edx that starts on June 2nd. I am not sure how this will go. Edit: [This](http://www.reddit.com/r/SQL/comments/37em4l/sql_for_six_months/) thread covers everything you will need to learn. Lots of different resources
I just signed up for this course. Thanks for the info! :) 
That gave me an additional column but no data. 
Why are you using FIND_IN_SET? Does Products.categoryids look like a comma separated list? If it's a single value, try CASE Products.categoryids WHEN Products.categoryids = 1 THEN 'Category1' WHEN Products.categoryids) = 2 THEN 'Category2' ELSE 'MISC'
You can try some of our classes on [Joes2Pros](http://www.joes2pros.com/joes2pros/courses). We offer beginner level courses/classes and we can get you up to speed pretty quick. Feel free to reach out for more info or if you have any questions. 
Lots of things. What operating system and database vendor are you using and I could give you an example.
I am pretty newbish when it comes to SQL, but I am using MySQL with phpmyadmin.
I think it's http://sqlzoo.net/wiki/SELECT_Quiz
Assuming it's http://sqlzoo.net/wiki/SELECT_Quiz, it's pretty basic math &amp; reasoning. Look at the sample data and talk through the query out loud.
When you're running your script, don't wait till the end to commit. Instead, build an empty table, tell your ruby script to loop through the records and commit every 10K-100K records (Based on performance). If you don't want to change your script, you can try disable rollback. Depending on the DB, it may fix your issue.
I'm not a fan of loops doing data things. I would import the csv to a new table and do a join to your zipcode table.
This is the correct quiz I'm working from. . . I'm just pretty new at SQL and don't know how they arrived at that answer
This is exactly what I was hoping for, I really appreciate you breaking it down for me. In this case, was it required to run the query for all columns to obtain the correct n/2 answer from which we could backtrack the original answer?
It's varchar(1). And yeah, I'll select * from x where y='' and get all records where y==zero. Is adding an extra condition the best solution?
I like to look and build queries starting from the bottom up. Looking at the WHERE first then moving to your SELECT. Since your WHERE is WHERE population = 64000 you know you are going to only get rows that have a population of 64,000 which is the case is country **Andorra** Now that you know you are only getting back the **Andorra** row look at your SELECT statement to get your columns for your answer. You have **name** so there is the first part of your answer. **Andorra** the comma (,) says the first column request is complete move to the next. So your next is **area * 2**. Take the **area** column in the question corresponding to Andorra row and multiply by 2. **468 * 2 = 936** So your answer would be **Andorra|936** Hope this helps I'm not by any means a teacher but I am a SQL Dev by profession!
Nope, There "WHERE" clause basically selected ONLY the Andorra row, as it is the only row with a population of 64000. With that you should have seen it was asking for the name column from that row, and the area from that row, but the *2 was the modifier that should have told you to modify the area by 2. Using '*' in a SELECT statement can be a little confusing for newbies as you sometimes use it as a wild card (* means select ALL columns), and sometimes use it as a mathematical operator (*=multiply, /=divide, +=addition, etc) 
Probably use MySQL command line. mysql &lt; my_script.sql where my_script.sql contains the code suppish provided. You can schedule with Windows task scheduler or a cron job.
Apparently there is also a MySQL Event Scheduler, so see if that is available for your OS/version.
how much different is teradata than ssrs and access? i could pretty much build all my queries visually in those environments but then alter with code on need basis.
I'm with /u/Prezrosslin You're querying for blank records and your getting the number 0 records back? Can you do SELECT CASE WHEN ISNUMERIC(record) = 1 THEN 1 ELSE 0 END ISNum, record FROM YourTable ORDER BY record See if your blank records come back as 1 in the ISNum column. edit: Names
Been doing this over a decade, I still make mistakes. Not proud of them, still feel that dread when I have to fess up, but I do. Once saw that I'd made a mistake and data wasn't updating. Wanted to fix it before my boss saw it, but updated the wrong SSIS. Broke data. Bad. Another time by dba deleted the wrong DB. overwritten data more than once. Forgot the where clause in a delete. Deleted the wrong days data ... Rollback is your friend. Bigtime!
There are definitely some tools that can do that with Teradata, notably Teradata Studio Express. At my company we all use SQL Assistant still, and it's "SQL Builder" is ~~pretty~~ worthless. tl;dr I wouldn't count on it.
&gt;I however select from table where record is null, I don't get the records ='' That's because `NULL` is different from an empty string.
Doh. Yes it is. Thank you!
so whats your opinion, can i go from basic select, from, where type of statements to like atleast intermediate level in a months prior to starting job?
This definitely helps, I appreciate it! Would you mind if I sent you a PM with another q I was struggling on? 
Thank you!
Conceptually, yes. And you've inherited more than bad standardization, you've been handed bad practices too.
Thanks for this info, iam going through KhanAcademy SQL interactive learning course, il complete that and get back to my C#, many thanks
In the end, I opted for just restoring the backups. Just fyi, though, during my investigation I found a tool called ApexSQL Restore which claims to allow you to "work with backups just like with live databases"... I guess it mounts them like you would with a network drive??? I've no idea because I didn't buy or use the product (I'm not endorsing this in any way) but I thought it was an interesting find worth mentioning here.
Your welcome! Just to add something: I would completely avoid using the Azure platform because setup is really unnecessary just to learn SQL and go with installing the standalone lite software.
What you need to do is stop having others simply do your homework for you and take a step back and think about the problem you're trying to solve and what the various SQL you are writing is actually doing. It appears you do not understand the most basic aspects of table joins and where conditions (ignoring aggregate functions entirely).
Why? There's probably a better way to accomplish what you're trying to do. 
Enlighten me. Edit: I realize this sounded rude. I apoligize.
Bak files are the best, but SqlPackage.exe provides this functionality with dacpac files as well.
Add a date column to the table which records when the row was created. You can now dynamically determine how much to add to the original value by calculating the number of ten minute intervals that have passed since the row was created using TIMESTAMPDIFF(MINUTE, created, NOW()) / 10.
Does SQL have some way to update date? Also, I have never touched the date before. In what format is it stored and do you have anything I can read on the matter?
What exactly are you trying to do? What is the current state? What is the desired outcome? 
Thanks for the advice. I'll give that a try. By the way, what format are Timestamps stored in SQL? I have never worked with them before and it would be good to have some info on the matter.
I meant date time data types. Timestamp data types are not times. The are ever increasing binary values that update whenever a record changes. They are useful if you know that the max time stamp was 0x123456 the last time you looked, you can query WHERE timestamp &gt; 0x123456 to get any new or changed records.
Perfect!
not sure what MySQL unicode is, I learned first by downloading xampp and setting up a few sample databases then practicing queries, later I just moved on to using straight SQL in my workplace's Access databases as that gives me real world data and gets my work done at the same time. If you're just putting in straight SQL and you know what you're doing and why you're doing it, you would learn the basics of SQL without too much issue. Every db implementation has a way of doing things that's a little bit different from one another, but if you know the ANSI standard, you can figure things out with some quick googleing. Now as for whether you need to learn how to write SQL queries for your position, I can't say, my previous and current employer both offer some sort of UI to pull/manage data, one worked with Access &amp; BusinessObjects, both of which have a full fledged UI but still allowed you to write queries if you wanted to. My current one uses Hyperion which uses a mix of UI &amp; coding (mostly for CASE statements), but my current company also uses SAS which allows me to write straight SQL from the get go when I connect it to the db. The answer for whether you should spend your time learning SQL for your position is that it depends, I'm sure your company offers some sort of UI, but since you want to focus on Advanced Data Analytics, I would say that learning straight SQL coding would help you in the future no matter what (it also doesn't take too long to learn the basics, and then you would build on to that by running into and solving problems). 
They're not different. OP, you're not going to get results if you're joining the same table to itself. This: AND Old.FieldName = New.FieldName where Old.FieldName IS NULL is only going to return records that are null, and since it's the same table, it's null in both tables. 
Simplest way is by creating a job to run every 10 minutes and write an update script to increment by 1.
I've been getting quite a few different solutions from MySQL command lines to Event Scheduler. I am guessing this would involve using the event scheduler?
SQL Server Management Studio is a client application that generally is used to connect to a server install of SQL Server. Here's a basic description of the tool: https://msdn.microsoft.com/en-us/hh213248.aspx You could download and install SQL Server Express to create the actual instance that you would connect to and manage using Management Studio. Here's a link for that: http://www.microsoft.com/en-us/server-cloud/products/sql-server-editions/sql-server-express.aspx
Im now thinking in terms of hack, write loop and use sleep in mysql and set it for 10 mins it will keep running. Dont kill me if I sound foolish will keep thinking on it though.
What you downloaded and installed is SSMS: Sequel Server Management Studio. It let's you manage SQL Server. You don't have SQL on that computer. You can manage a computer that has SQL, but your computer doesn't actually have it. The reason it's asking you to try and sign into a server, is because the programs only function is to manage a SQL Server. If it can't connect to a SQL server, it has nothing to manage, it can't do anything it was designed to do. You need to actually download SQL Server express, and install that. Then, when you launch the management studio, you can manage SQL Server express.
Which platform are you using? In MSSQL at least, if you're using the + operator with text data it will concatenate, with numeric data it will add. You'd need to make the result of 1+1 into a string. SELECT cast (1 + 1 as varchar) + ' dogs' For your second question, it sounds like you'd want to create a variable and manipulate it over several statements rather than doing it in one statement, e.g., declare @string as varchar(50) set @string = 'Hello' set @string = @string + ' World'; set @string = @string + ' What is up'; select @string returns Hello World What is up
Not exactly what I am looking for. In Python, If I want to add 1 to a previous value, say 10, I can write... &gt;_+1 ...and Python will spit out the value 11. Is there such a symbol (_) in SQL? 
That wouldn't make sense in the context of SQL - it's a declarative querying language, not a programming language. The results of one statement aren't carried over or held in memory into the next statement, except by explicitly doing so by putting the results into a variable. 
We don't do things one row at a time in the sql world. We operate in sets
The = is unappealing and makes a wall of text. It's also extra work to type zero chance I'll ever do it
Did you download SQL server express too? If your this new, You may have installed a trial copy of sql server on your computer without realizing it Try logging in to localhost.
Phenomenal thanks a lot! 
Oh wonderful. Thank you.
Hi, yes I should indeed detail that we use Microsoft SQL. I'm talking about permissions on the databases. Lets say the developers have the ability to write to the CMS system database or even worse, maybe 6 tables of the database in the test environment. this isn't allowed on live. So when we restore a copy of the live database (s) over the test version for further development they are losing those permissions. the situation is complicated in that developers will not have full access to all databases even in the test environment (due to sensitive data and cohosting of databases they don't use etc). I do indeed want to script the permissions, however the permissions are not something I have a detailed list of. So want I want to automate is exporting the permissions set, then restoring the live database and importing those pre-restore permissions back. The tricky bit comes with me not knowing (or wanting to know) what the permissions were pre-restore. I just want to automate putting EVERYTHING they had, back again.
I believe it is sequential. Is it producing the expected results?
I am not going to make a promise, but I am fairly certain they will all be evaluated at the same time. You could use staging tables to handle a situation like this, if I am thinking about your question correctly. If the data is small you could use CTE/Temp tables too.
Can you create a temp table on the database and connect to that?
CASE is sequential. To test this you can do this (you need to substitute your own BigTable): CREATE TABLE TrivialTable (ID int NOT NULL); INSERT INTO TrivialTable VALUES (1); GO SELECT CASE WHEN EXISTS (SELECT * FROM TrivialTable) THEN 0 ELSE (SELECT CHECKSUM_AGG(CHECKSUM(*)) FROM BigTable) END; GO TRUNCATE TABLE TrivialTable; GO SELECT CASE WHEN EXISTS (SELECT * FROM TrivialTable) THEN 0 ELSE (SELECT CHECKSUM_AGG(CHECKSUM(*)) FROM BigTable) END; GO DROP TABLE TrivialTable; The first select should return quickly, because the first condition is true, other conditions don't run at all. The second select will do a full table scan of your BigTable because the first condition was false. If they evaluated simultaneously, both selects would perform the BigTable scan.
This works perfectly, thank you. SQL quite simple and powerful. :)
When I mentioned building the case in my post, what I didn't mention was that I'd have to build about a dozen of them for different fields, and multiple queries besides, so I didn't want to start building it and potentially waste hours on something I'd have to go back and modify depending on how the function worked. But yes, after creating a temp table to test it, it appears that CASE is sequential. Or at least, if multiple conditions are true, it'll choose whichever one appears first in the condition list.
https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-DateFunctions Looks like you're looking for either `add_months(viewdate, -3)` or `date_add(viewdate, -180)` But i'm not a Hive user. Why didn't you just google "Hive date functions"?
Yep, it runs sequentially just like you mentioned, thanks for the help!
Ah, I had a very different idea of what you were looking for!
Hopefully these questions from the stack exchange network will put you on the right path (SQL Server is not my forte, so unfortunately I don't have any specific advice myself). * [Scripting SQL Server permissions](http://stackoverflow.com/questions/1987190/scripting-sql-server-permissions) * [How to export all the security-related information from a SQL Server database](http://dba.stackexchange.com/questions/58772/how-to-export-all-the-security-related-information-from-a-sql-server-database) If it was me, though, I'd want a recorded list of the permissions each developer should have and would have my script operate off of that single source of truth rather than "give everything back they had previously" (in case something was given in error). The operation should be idempotent. I'll go one step further and say that you might want to look at your entire setup from a higher level, it seems odd that you'd want all your developers pissing in the same pool, as it were. Developers should generally have isolated environments with whatever sandboxed DB they need to support that rather than all colliding in the same database.
i downloaded sql server express, but no idea what to do lol http://imgur.com/Anmdb1X
dear OP, please read the sidebar, and confirm which platform you're using my guess is MySQL
Yes, MySQL!
SELECT CONCAT_WS(' ',1+1,'dog') AS two_dogs
It's sequential, I've used it almost like a nested IF in the past, the first match (going from top down) is what gets used. Edit: I read the title and answered prematurely, you've already figured it out.
The left join suggested already is usually the easiest to remember and will give you exactly what you are after. If you want the "not exists" syntax (from curiosity if nothing else maybe hehe) it would be something like this: select * from users u where not exists ( select * from documents d where d.doc_cat = 2254 and d.owner_id = u.user_ID ) 
For server name, enter **localhost** Also, check task manager processes and make sure **sqlservr.exe** is running.
That did it. Thanks much!
Yep. As I said not my database. Got it working. Removed the quotes around the numbers in the CASE. Fresh eyes in the morning. 
This is the most compact way I can think of, but a really really really long case statement is probably technically faster. SELECT CASE WHEN x.[Count] = 1 THEN CONVERT(varchar(10),x.[Date],121) -- alternatively CONVERT(varchar(10),COALESCE(t.Date1,t.Date2,t.Date3,t.Date4,t.Date5,t.Date6,t.Date7),121) ELSE 'notidentical' END FROM [Table] t OUTER APPLY (SELECT COUNT(*) AS [Count], MAX(x.[Date]) AS [Date] FROM (SELECT t.Date1 AS [Date] UNION SELECT t.Date2 AS [Date] UNION SELECT t.Date3 AS [Date] UNION SELECT t.Date4 AS [Date] UNION SELECT t.Date5 AS [Date] UNION SELECT t.Date6 AS [Date] UNION SELECT t.Date7 AS [Date] EXCEPT SELECT NULL) x) x edit: couldn't decide between the MAX(x.[Date]) or COALESCE method, so I gave you both. edit2: I like /u/ichp 's idea instead
Personally I like INTERSECT and EXCEPT for this type of operation, especially if it scales up to excluding multiple categories or unusual combinations of categories. Something like the following.... ;WITH USERIDS_MISSING_DOCUMENTS AS ( -- COMPLETE LIST OF USERS. SELECT [USER_ID] = U.[USER_ID] FROM USERS U -- EXCLUDING... EXCEPT -- USERS WITH DOC CATEGORY 2254 SELECT [USER_ID] = D.[OWNER_ID] FROM DOCUMENTS D WHERE D.[DOC_CAT] = 2254 ) -- PULL ALL USER INFORMATION ON THOSE USERS. SELECT U.* FROM USERIDS_MISSING_DOCUMENTS UMD INNER JOIN USERS U ON UMD.[USER_ID] = U.[USER_ID] The reason I like this approach (even though it is very similar to what you will achieve with NOT EXISTS) is that you can easily extend it for more complex combinations. For instance, let's say that you want to see users who do not have documents in BOTH category 2254 AND 1234. We can leverage the fact that INTERSECT takes priority to build a set of users with both documents, then exclude that. ;WITH USERIDS_MISSING_DOCUMENTS AS ( -- COMPLETE LIST OF USERS. SELECT [USER_ID] = U.[USER_ID] FROM USERS U -- EXCLUDING... EXCEPT -- USERS WITH DOC CATEGORY 2254 SELECT [USER_ID] = D.[OWNER_ID] FROM DOCUMENTS D WHERE D.[DOC_CAT] = 2254 -- INTERSECT EVALUATES FIRST... INTERSECT -- USERS WITH DOC CATEGORY 1234 SELECT [USER_ID] = D.[OWNER_ID] FROM DOCUMENTS D WHERE D.[DOC_CAT] = 1234 ) -- PULL ALL USER INFORMATION ON THOSE USERS. SELECT U.* FROM USERIDS_MISSING_DOCUMENTS UMD INNER JOIN USERS U ON UMD.[USER_ID] = U.[USER_ID] 
My brains fuzzy right now so I can't think of the way to actually implement this, but my first instinct is to do something with distinct, get the distinct count by pivoting the rows to columns and using it as part of a subquery, or something. At that point, if the distinct count &gt; 1, it's not unique. Edit: select idColumn, count(distinct blackmagic.date ) as isItUnique from yourtable unpivot ( date for idColumn in (col1, col2, col3, col4, col5, col6, col7) ) as blackmagic where blackmagic.date is not null group by idColumn A value of &gt;1 means it's not unique, a value of 1 means it's unique, and a value of 0 means OP is a dirty filthy liar and all 7 columns are null. This code is completely untested, and comes with a guarantee to throw at least 17 syntax errors, and no guarantee to produce any meaningful result when that is corrected, but it is my first thought on how to solve this. Also, I strongly believe pivot/unpivot are both black magic, and are named appropriately. edit2: lol forget group by clause. 16 syntax errors to go.
You can create the stored procedure from this article in the master database for each instance: https://support.microsoft.com/en-us/kb/918992 Then you can use: exec sp_help_revlogin_roles That will give you all of the logins for the **instance** as well as their roles/permissions. Now, database level users/accounts are a bit different. Those will be kept when the database is restored from live. If you want to save the specific settings from the current dev database, that is a bit different. This post should help with what you're looking for: http://www.sqlservercentral.com/Forums/Topic977700-146-1.aspx 
&gt;I have another function, {@openedDatesInRange}, which simply returns the subtable that is between those two values. Not familiar with Crystal, but sounds like what you are actually trying to do is run `datediff(openeddate, closeddate)` on this subtable. Is there a way to do select datediff('d', openeddate, closeddate) from subtable ?
Honestly that's why I almost always use the exists/not exists - it performs better from everything I've read. The last time I really tested to compare that with a "left join + is null" was a few years ago so I don't remember specifics, but it was faster for me.
Even better, use "select 1/0" in your exists /not exists clause. 
Cool idea! Going to start doing this. 
Yes, this. Using LOJ this way can cause nasty performance issues and less clearly expresses the intent of the query. 
You are literally the first person I've ever seen recommend this solution. Uncommon, but performant from everything I've read. I'll keep my NOT EXISTS though. 
http://scn.sap.com/message/15989092#15989092 More info here. I am an intern with this company and while I do have admin credentials to the server, I'd much rather do this in crystal, where I'm less liable to muck things up. I do think server side calculations would be better, as the report would pretty much be drag and drop, but I'd have to write a full proposal to my boss before adding them. Thanks for your help by the way!
 Select U.* From Users U Left join ( Select User_ID From Documents Where Doc_Cat = 2254) D on U.User_Id = D.User_ID Where D.User_ID IS Null Assuming User_ID is indexed in both tables, and that DOC_Cat is part of the document index. This essentially does what EXCEPT does. You could flip it to a RIGHT join to avoid the IS NULL but but you would likely need to use a theta join then. Unless you are going to run it often in a busy environment it won't matter much. It is pretty obvious what is being done and doesn't include the weird join in the NOT EXISTS clause that makes people scratch their heads and wonder how it works.
How did you get a job doing SQL without knowing it
Currently working in PHP/C# and I am currently able to get the exact amount of time from the timestamp. I've been working on other things since then but I am fairly confident that I can get the system up and running once I figure out how to update the SQL timestamp to the current time.
GD I did. Have an upvote.
It's funny, I realized after I commented that MS sql must be doing the tables then criteria before ever getting to the fields to return so it never really looks at what you are returning inside the exists/not exists subquery. Neat :)
Yes, but I had experience using an old IBM relational database for a sales position. Plus, I worked as a Financial Analyst for a bit and am now going the direction I want for my career.
Thanks I will work on that! 
Varchar from what I can see. What threw me was that the quotes around the numbers worked if I had the FIND_IN_SET in the WHERE section during an earlier test. So it never occurred to me that within the select section would act different. 
What's the job role? Needing to brush up on joins still suggests you're very very new to SQL. They are pretty fundamental. Not trying to be negative, but joins are basic SQL. Anything complex in particular, I might be able to help :)
I use "select null" for "exists" and "not exists" situations. I think it's unambiguous and probably the most efficient (null is the default for DB values) by a few micro seconds.
&gt; fizzbuzz problem learned something new today -- got my first fulltime programming job in 1971 and have never heard of it
Use a Left Outer join... Select u.* from users As u Left Outer Join Documents As d On u.user_id = d.user_id Where d.user_id IS NULL Gives you all users without documents.
I think I will go with /u/ichp. That's more how I had imagined it when I was trying and failing. However, I really appreciate your solution, as well. I did not even know the "EXCEPT" keyword existed, so I learned something that could be very useful!
Thank you, that is much simpler (and actually works) than the case mess I was coming up with!
I've read that like 10 times and I'm sorry I still have no idea what it is you're talking about?
I have a list of 500+ card numbers. And i want the expiration date for each without having to say SELECT account number WHERE account number IN (a,b,c,d,e, etc)...
Interesting! Thanks for the feedback. I actually found this very insightful, as I've only had cause to use this solution on smaller sets of data (&lt;10,000 rows) in a production capacity, so I haven't tested larger results. That being said, I did look at the query plan between this solution and a NOT EXISTS variation in a test database. I saw that both generated an Index Scan / Index Seek (for relevant set from base tables) then a Merge Join. There was a datatype conversion in the EXCEPT query which is indeed present in my test database. The two tables use different sized fields, though I found it fascinating that NOT EXISTS did not have the conversion, so it would be slightly faster on that for sure. Other than the datatype conversion and group, the query plans appeared identical. I can see that without the group it would definitely be a slower solution overall for very large production sets, though I'm not sure it would be as significant as I thought from your comment. I will want to scale up a million record test set to try this again when I have more time. Here was the query plan from both.... NOT EXISTS Query SELECT [MPP_ID] = M.[mpp_id] FROM ManPowerProfile M WITH (NOLOCK) WHERE NOT EXISTS ( SELECT 1 FROM Expiration E WITH (NOLOCK) WHERE M.[mpp_id] = E.[exp_id] AND E.[exp_idtype] = 'DRV' AND E.[exp_code] = 'LIC' ) |--Merge Join(Left Anti Semi Join, MERGE:([M].[mpp_id])=([E].[exp_id]), RESIDUAL:([TMW_Live2].[dbo].[manpowerprofile].[mpp_id] as [M].[mpp_id]=[TMW_Live2].[dbo].[expiration].[exp_id] as [E].[exp_id])) |--Index Scan(OBJECT:([TMW_Live2].[dbo].[manpowerprofile].[ix_manpowerprofile_mpp_if_updatedby_updatedon] AS [M]), ORDERED FORWARD) |--Index Seek(OBJECT:([TMW_Live2].[dbo].[expiration].[dk_idtype] AS [E]), SEEK:([E].[exp_idtype]='DRV'), WHERE:([TMW_Live2].[dbo].[expiration].[exp_code] as [E].[exp_code]='LIC') ORDERED FORWARD) EXCEPT Query SELECT [MPP_ID] = M.[mpp_id] FROM ManPowerProfile M WITH (NOLOCK) EXCEPT SELECT [MPP_ID] = E.[exp_id] FROM Expiration E WITH (NOLOCK) WHERE E.[exp_idtype] = 'DRV' AND E.[exp_code] = 'LIC' |--Merge Join(Left Anti Semi Join, MERGE:([Expr1002])=([E].[exp_id]), RESIDUAL:([Expr1002]=[TMW_Live2].[dbo].[expiration].[exp_id] as [E].[exp_id])) |--Stream Aggregate(GROUP BY:([Expr1002])) | |--Compute Scalar(DEFINE:([Expr1002]=CONVERT_IMPLICIT(varchar(13),[TMW_Live2].[dbo].[manpowerprofile].[mpp_id] as [M].[mpp_id],0))) | |--Index Scan(OBJECT:([TMW_Live2].[dbo].[manpowerprofile].[ix_manpowerprofile_mpp_if_updatedby_updatedon] AS [M]), ORDERED FORWARD) |--Index Seek(OBJECT:([TMW_Live2].[dbo].[expiration].[dk_idtype] AS [E]), SEEK:([E].[exp_idtype]='DRV'), WHERE:([TMW_Live2].[dbo].[expiration].[exp_code] as [E].[exp_code]='LIC') ORDERED FORWARD)
excel
Ah ok. You could create the script IN excel. So, have all of the numbers in column A. Then in column B write something like this: ="select card_number, expiry_date from cards where card_number = "&amp;A1&amp; " union" Then fill down, copy and paste all 500 commands into sql and bam! 
don't forget to remove the last "union"
great idea. thank you!
You should probably use something like lag(EndDate,1,0)over(partition by id, address order by EndDate) to get the previous EndDate in your result for comparison purposes. I'm not sure that code is correct as I haven't actually used that function before. edit: [LAG documentation](https://msdn.microsoft.com/en-us/library/hh231256.aspx)
Because it's only briefly covered in SQL classes currently. The one I took for MSSQL in school literally only covered CTEs, Joins, and the such for a day or so... most of it was 'do it on your own' from the textbook as "homework" (could basically copy / paste what came w/ the CD and pass the tests). Hell I still have issues scripting SPs... 
Clustered vs. non-clustered won't have much impact on your life if all you're doing is reads.
I'm gathering that that's not a thing I can do -- multiple columns in a single CASE structure.
 INSERT INTO &lt;Table_Name&gt; (EmployeeStatus, Occupation, EmployerName, PlaceOfBusiness) SELECT EmployeeStatus , NULL as Occupation, NULL as EmployerName, NULL as PlaceOfBusiness from source_table where EmployeeStatus = 'Unemployed' INSERT INTO &lt;Table_Name&gt; (EmployeeStatus, Occupation, EmployerName, PlaceOfBusiness) SELECT EmployeeStatus , &lt;FIELD&gt; as Occupation, &lt;FIELD&gt; as EmployerName, &lt;FIELD&gt; as PlaceOfBusiness from source_table where EmployeeStatus = 'Employed' 
No, case statement "then" parts are scalar values. Also, you cannot have variable number of columns in static SQL.
I've had experience with complex queries and I usually just look through code references until I can make the query I need. I don't think I would inhibit the work at all I just can't do it off the top of my head.
I am in a datawarehouse environment using Oracle and can still answer all but three of those comfortably. The clustered/non-clustered, CTE, and SSRS/SSIS. I can't remember the clustered off the top of my head because I never have to bother with it and I've never even heard of the other three.
This is the report I'm trying to make: https://imgur.com/8RbPxBM I have a table with records DateOpened,DateResponded,DateClosed,Type, and Technician. All the info to make this report is in there, I just don't know how best to use CR to do that.
me neither as a coding test. It's more fun as a drinking game called *fuzzy duck* but perhaps not best to suggest at an interview! 
since learning CTEs I can't imagine how I ever did anything without them!
i did some googling. i use these already but i've just never referred or heard of them referred to as CTEs. i read the documentation on WITH and started using it. yeah they are useful. usually i use them when i have data getting used repeatedly in a query over a db_link.
Temp tables and table variables and just repeating subqueries; but I find myself using them constantly now. Just have to be mindful of the performance hit they sometimes give, they aren't always the best choice.
Perhaps I'm not fully understand the desired result, but I would think something like this would serve well. SELECT [ID] = T.[ID] ,[ADDRESS] = T.[ADDRESS] ,[FIRSTDT] = MIN(T.[ENDDATE]) ,[LASTDT] = MAX(T.[ENDDATE]) FROM BASE_TABLE T GROUP BY T.[ID] ,T.[ADDRESS] HAVING DATEDIFF(DAY, MIN(T.[ENDDATE]), MAX(T.[ENDDATE])) &gt;= 60 Essentially, group all the records in the base table by [ID] and [ADDRESS]. Determine the MIN() and MAX() [ENDDATE] values as the range from the matching [ID] and [ADDRESS] records. Then using HAVING (which acts as a predicate for GROUP BY values) to detect those more than 60 days apart. If you wanted to carry this forward you could extrapolate the PK from the [ID], [ADDRESS] and [ENDDATE] values on one of the records then perform some action on the duplicates (Delete, Update, etc...). I've done similar approaches when faced with redundant records in the past. 
There might need to be some explanation, is this data, a query, what's the context, should you just concatenate a , are they all numbers and therefore going to have to be cast as something different. Help us help you.
You can use a case statement for each coloumn, though. SELECT CASE EmployeeStatus WHEN 'Employed' THEN Occupation ELSE NULL END as Occupation, CASE EmployeeStatus WHEN 'Employed' THEN EmployerName ELSE NULL END as EmployerName FROM source_table
&lt;le sigh&gt; dear OP, please read sidebar, you forgot to identify your platform here, have some standard sql -- UPDATE datable SET dacolumn = ',' || dacolumn
If you need to do something like this only once, I usually use Excel or regex search and replace. With regex for example: find: `^` replace: `,` That "replaces" the beginning of the line, represented by the carat, with a comma. Since the beginning of a line isn't actually a character it really just adds commas. With excel string concatenation is stupid simple too. `= "," &amp; A1` Now if you have to do this all the time then PHP or SQL is more appropriate for sure, but for one-offs use whatever is to hand ....
Thank you very much for your answer! If I am reading the script, and my results, correctly, this is taking all IDs from the base table and returning only those which have a match on ID and address while also having the first end date of that ID being at least 60 days before the end date of the match? That is a mouthful but I hope we are on the same page lol
I think plsql challenge is a very good resource. It is Oracle-based
It worked to an extent. If an ID has more than one duplicate I would need to compare the dates to the initial "end date" tied to that ID number. Your answer has help me make progress though, thanks!
Maybe `min(EndDate)over(partition by ID)`?
Its actually a primarily C# developer role, but I was just hoping to make sure I don't bomb any of their interview questions. I planned on pointing out my weaknesses, as I've already discussed a few of them during the phone interview.
oh, oh... non-standard microsoft sql there
Edit, cleaned up formatting. I think this will give you what you are looking for. Effectively, you still use the overall aggregate statement to find [ID] and [ADDRESS] combinations that have a 60 day gap between dates. However, to flag individual intermediate records, you join the base table back to the aggregate result set, then use a case statement to see if the individual records are more than 60 days past the first date. Note that the earlier solution from /u/PrezRosslin would be similar to this with Window functions, but you can only use the LAG/LEAD functions in SQL 2012+, so this solution would work for older versions of SQL Server as well. In either case, hopefully both solutions help achieve what you are looking for. -- USE A CTE TO GET THE MINIMUM DATES FOR ALL AGGREGATE RECORDS. ;WITH CTE_MINIMUM_DATES AS ( -- PULL AGGREGATE RECORDS HAVING A MIN([ENDDATE]) 60 DAYS OR MORE BEFORE THE MAX([ENDDATE]) SELECT [ID] = T.[ID] ,[ADDRESS] = T.[ADDRESS] ,[FIRSTDT] = MIN(T.[ENDDATE]) FROM BASE_TABLE T GROUP BY T.[ID] ,T.[ADDRESS] HAVING DATEDIFF(DAY, MIN(T.[ENDDATE]), MAX(T.[ENDDATE])) &gt;= 60 ) -- SELECT CORE RECORDS FROM BASE TABLE, JOIN TO CTE TO DETERMINE INDIVIDUAL RECORDS BEING MORE THAN 60 DAYS PAST. SELECT [ID] = T.[ID] ,[ADDRESS] = T.[ADDRESS] ,[DUPLICATE] = CASE WHEN ( DATEDIFF(DAY, D.[FIRSTDT], T.[ENDDATE]) &gt; 60 ) THEN 'Y' ELSE 'N' END FROM BASE_TABLE T LEFT JOIN CTE_MINIMUM_DATES D ON T.[ID] = D.[ID] AND T.[ADDRESSS] = D.[ADDRESS]
I am off work for now but I will save this and try it Monday. It looks like it will get what I need though. Thanks :)
If you are on 2012 (or above) you could use data profile solution in ssis.
Visio works for me, but there's nothing automated or reverse-engineered.
I may have access to this, but I would also need to do some conditional logic for some fields, such as when field1 = 'Y' then case when field2 = '' then 1 else 0 end So I'll look into it, but unless it supports that or I can manage some sort of hybrid solution it may not be an option.
I could assuming it is documented what all the server names and instances are. This would probably be better for load purposes as well, I could schedule the powershell to run at different times for the different servers. I know it was mentioned that they have some documentation but were unsure if it was all documented.
Thank you for your response! I like how it is possible to streamline data in an sql database to flow into an excel sheet with macros that run when data is entered! Problem is, the hiring manager looked me straight in the eyes and said "no automating!" I was hired though.
&gt; "no automating!" We like our data like our coffee. Gathered by hand. 
Did you work on DB2 or something? God I hated DB2.
AS400
If you want to work with data, you NEED to know SQL. It's really that simple. 
As an added piece, Excel/vba doesn't scale processing larger datasets
Wow. No, we're not that far behind. Although we are just making the jump to Office 2010. I'm actually looking to get out of risk because I don't think the company takes it seriously. 
Great answer. 
I have spent every waking moment reading and learning it.
I usually paste into sublime text and then use a few simple regexes to clean it up and paste it into an in list. Dealing with excels text handling is a pain. 
For SQL Server, the things I looked for when hiring people were: 1. Joins. Inner, left, cross, full outer. 2. CTEs. Recursive SQL. 3. Join clause vs where clause. (e.g.-Give me all students who failed (grade of F) or have yet to take math.) 4. Bonus: Window functions (If you used `over` organically, I would argue for your hiring till I was blue in the face. Only one guy ever did, and we hired him, and I do not regret that one bit. Just went to his wedding, in fact--and we don't even work together any more. How's that for anecdata?) 5. If you use a cursor, so help me, you're done. 6. Translating requests to queries, given a schema. 7. Designing a schema. I did mostly BI stuff, so we'd be looking to do star-schema focused stuff. One of the common ones was to ask how to build a schema for sales analysis from a receipt. Most of the stuff we were doing (when I was actively hiring) was around analysis and ETL, so DBA-type stuff like index optimization was a nice-to-have or a learned skill if you could, not something we immediately looked for. We really just wanted to get if you could think declaratively.
Giving cursors a blanket no is incorrect. Sometimes they are in fact the best way to handle certain situations. Knowing when to use a cursor is a big plus - much more so than just saying never.
Is there a reason you want to limit yourself to being a DBA (not that it's a bad thing)? Also, try posting at /r/cscareerquestions.
How do you know that Alabama = SEC? Is that data in a table somewhere?
Gotcha, I'd be tempted to create another table with all the colleges and their conferences. The... Join to that table in your select query. 
 select app_id from [dbo].[Raw_Zoot_Decision_REASON_SUMMARY] where Decision_reason_type_cd = 106 Group by app_id having count(app_id) = 1 Alternatively you could also use a select distinct so you don't need a windowed function select distinct app_id from [dbo].[Raw_Zoot_Decision_REASON_SUMMARY] where Decision_reason_type_cd = 106 Group by app_id
Saw George at a user group recently. He developed powershell scripts to script out a whole database server. worked like a charm. [https://github.com/gwalkey/SQLTranscriptase/](https://github.com/gwalkey/SQLTranscriptase/)
You can't use an aggregate in the WHERE, look up the HAVING clause. 
Might try that out, I haven't learned join function yet so that could be fun
Which certification totally depends on what your primary SQL platform is probably. At my office we primarily use MS SQL, so an Oracle certification would be fairly useless. That said, I've been working towards an MCSA in MS SQL Server for awhile now b/c I have ADD issues and keep blowing it off. As for online vs proctored I can't imagine that the certificate indicates which you took, so I doubt it would hold any less weight. Best of luck!
&gt;I am trying to get the app_id's that have ONLY X reason for decline. I know i'm on the right track, but i have had a lot of trouble with this. I'm using microsoft SQL. Thank you! I'm not sure you're doing what you mean to do. If you put `where decision_reason_type_cd = 106` You will only get results for that code, so you won't know if they have another code as well. Am I understanding what you are trying to do correctly?
I mainly use Microsoft SQL Server management studios, so SQL server 2008 R2. Ya it looks like microsoft certifications are more structured and the way to go. Do I need to take the MTA certification first before getting a MCSA in SQL Server 2012? 
I am a Business Intelligence Analyst consultant and have been working with SQL for over 20 years. So I guess I am coming at it from a different angle. You would be correct that MANY institutions, not just financials, run their entire organizations on Excel. However, it has been my experience that when I am brought in to help customers out, they can't wrap their heads around data manipulation and data organization. They can't fathom that data can be manipulated at the level of an RDBMS. I have sat in meetings where employees will flat out say "We can't do that with that data. Even with VBA and VLOOKUPS." I'll usually mention SQL and get either blank stares or frightened looks of "oh shit, I can't write code". I have seen it over and over. I come on to a project, sprinkle even the most basic SQL on the data and they flabbergasted at the results. Something that used to take several people several days to do for close of month, now only takes a handful of queries to run and a couple of hours. So my recommendation would be to, at the very least, learn the basics of tables, JOINS, Stored Procedures and CTE's. Even if you run your own local instance of SQL on your computer, you will be the rockstar on the team and have an edge when it comes to what you can do with the data. As a bonus, if your company is invested in the MS Stack, I HIGHLY recommend looking at their Power BI line of products(Free with the latest version of Excel or O365). Specifically, Power Query and Power Pivot. Those tools are a happy medium for "frustrated Excel users" who want to do more with the data and feel handcuffed by Excel. As a consultant, I have found this to be the easiest path of introducing RDBMS concepts to them. Though I still prefer a proper Data Warehouse and organized data structure, that path can be cost prohibitive for small projects. TLDR; Knowing even just the basics of SQL will put you ahead of the game if your job involves working with ANY data.
This would benefit in the long run
 SELECT name FROM Salesperson INNER JOIN Orders ON salesperson.id=orders.salesperson_id GROUP BY name HAVING Count(Name) &gt;=2; You don't need to have something in the select clause to use it in a different clause.
 Excuse formatting, on my phone. select * from ( select r.*, row_number() over (partition by TOPIC_ID order by ACTIVITYSORT) as RNUM from TABLE r where ENDDATE is null) where RNUM = 1
&gt;tweak adding in bank holidays as time rolls on [You need a date table](http://www.brentozar.com/archive/2014/12/simply-must-date-table-video/). You should not be editing your queries just to "add special dates". Set up a date table, establish your bank holidays there, and join to that table. As holidays are added, simply add them to it (or flag existing dates as holidays). &gt;Is there a resource to minify sql at all? Do you want it to be readable? The goal of SQL should not be "make the code small", but rather "make it efficient" and "make it readable." If the best query also happens to be the smallest, terrific! But that's not always the case. &gt; I feel that it might help the load if it didn't have so many goddamn CASE clauses (the only way I can see in SQL of doing if/else commands). You'd be surprised. There are definitely cases where less SQL code translates to **more** work for the database, and your goal should be to minimize the amount of time executing the code. &gt;is there an equivalent of JS lint to help me find that one little bit that throws a wobbly for the whole damn code? SSMS has a "Parse" feature (Ctrl+F5, the blue checkmark on the toolbar near Execute &amp; Debug) but that won't catch everything. There's also "Display Estimated Execution Plan (CTRL+L), but that isn't necessarily the *real* execution plan you'll get when you run the query. The Execution Plan will help you understand what's consuming your cycles.
Any chance you've taken a look at my previous comments? More info I could provide?
Before I start, the way I'd do it is with a table that had a composite UNIQUE constraint on the source/port columns, and also one on the end/post columns. But currently, you could do a count on the columns, group by name, and check for ones that appear twice in the column?
You are on the right track. But you don't need **ALL** and your string values should be in quotes. **This solution is for MS SQL, what flavor are you using?** SELECT product_description, product_id, standard_price FROM product_t WHERE standard_price &gt; (SELECT standard_price FROM product_t WHERE product_description = 'Computer Desk') 
http://sqlfiddle.com/#!9/dcb16/1 I made this. If you don't have a primary key you can COUNT over `c2.source_device` or `c2.source_port` as well, which should be equally NULL for a source pair without a destination. EDIT: For some reason my saved SQLFiddle doesn't load again, here is the schema and query: CREATE TABLE connections ( conn_id INT NOT NULL PRIMARY KEY, source_device INT NOT NULL, source_port INT NOT NULL, destination_device INT NOT NULL, destination_port INT NOT NULL -- , -- CONSTRAINT uc_source UNIQUE (source_device, source_port), -- CONSTRAINT uc_destination UNIQUE (destination_device, destination_port) ); INSERT INTO connections (conn_id, source_device, source_port, destination_device, destination_port) VALUES (1, 33, 3331, 34, 3332), (2, 34, 3332, 33, 3331), (3, 34, 3332, 35, 3342), (4, 35, 3333, 36, 3339); --- SELECT c1.source_device, c1.source_port, COUNT(c2.conn_id) destinations FROM connections c1 LEFT JOIN connections c2 ON (c1.source_device = c2.destination_device AND c1.source_port = c2.destination_port) GROUP BY c1.source_device, c1.source_port; -- -- only show abnormalies -- HAVING destinations &lt;&gt; 1;
If you can't write to the vendor database, you can still create your own db with a date table, and do a cross-database query. Just put your custom db on the same SQL instance as the vendor's so you won't have to deal with linked servers.
I purchased some training material like the official 70-461 book, the practice exams from MeasureUp and the Pluralsight training courses. There's quite a bit of stuff that it tests on that I've never had a need to know or use like pivots, so I definitely needed study material for a bit of it. That said, it probably really depends on your experience so far. Have a look at the skills measured on Microsoft's website (https://www.microsoft.com/learning/en-us/exam-70-461.aspx) and see whether you feel comfortable with the material they're testing on. That page is a pretty good breakdown of what you'll be tested on. I found that my particular exam was very XML heavy, however I've talked to people that had a different experience.
I used http://sql-ex.ru. Click on SQL Exercises &gt; SELECT (Learning Stage). You will need to create a user, but that is so the site can remember your progress. Please note that it is also using Microsoft SQL. I think it is a great site for going through some challenges and testing your SQL skills. Unfortunately, sometimes the English isn't as accurate as you would hope for when making your queries. It makes some challenges a little confusing, but you can still (eventually) figure out what they are looking for. Also, note that some of the schemas could be made better, but I believe this is intentional, as the goal is to work on your SELECTing skills, rather than say developing a database.
Shouldn't you just install SQL Server Management Studio on the client (local) machines? They can connect to SQL1 from each workstation. There's no need to install an instance of SQL Server on each machine if you don't need them.
I don't know if combining NOT NULL check constraints with both UNIQUE constraints already fulfills all of /u/Dionysus_Eye's requirements, apart from trying to squeeze data with existing violations into a table with these. It only allows every port at most once as a source and as a destination, but * it still allows for port A to be connected with port B and port B with port C * it still allows for port M to be connected with port N and port N with port M.
TIL
This formatter has a compact option: http://www.dpriver.com/pp/sqlformat.htm I also like poorsql.com, and I think it may have the option as well. I prefer to expand a lot of statements, especially if they are nested, personally. My preferred method of formatting is mostly manual honestly, because I think a lot of times whether to expand a statement is a judgment call based on nesting levels and plain line length. Most formatters aren't going to produce something you should (in my opinion) use without tweaks. edit: read your post more thoroughly so most of ~~that~~my second paragraph is useless info. :)
To be fair, the only reason I know is that I got an alert when taking a proctored exam in the fall saying that I could use that in the future.
I thought about this a little, and you could use regex to minify it yourself. The following expressions are guaranteed to have serious omissions but worked on a small script I had open. Something like: search: `^\s*--.*\n` (kill comment lines) replace: search: `(.*)--+.*` (kill trailing comments) replace: `$1` search: `^\s*([\w\b,\(\)].*)\n` (remove line breaks) replace: `$1 `
If it were me I'd write a bash script and stick it in cron, but that just depends on your comfort level. edit: /r/linuxquestions /r/linux4noobs might both be better places to get answers if you want to go that route. Also `man ftp` and `man sftp`
I love to learn everything so if you can provide any resources on how this can be done I would highly appreciate it!
Eh I'm no guru, check in one of the subs I mentioned above and I bet you'll get some really good answers.
Also notice that he uses the expression `Count(Name)` instead of the alias `num` in the `HAVING`. Some database management systems aren't strict about it, but technically I think the only clause you're almost guaranteed to be able to use it in is `ORDER BY` (since that has to do with display rather than retrieval of data).
I find Access joins to be crazy ugly vs SQL and require parenthesis for each join. As PrezRosslin stated cross joins may not work and you'd need to re-work the join. SELECT TableA.TransDate, **TableA.SchedAmount**, TableB.ProjectName, TableB.ProjectPath, TableC.FirmID, TableC.FirmName FROM ((TableA INNER JOIN Firms ON TableA.FirmID = TableC.FirmID) CROSS JOIN Projects) WHERE (TableA.TransDate &gt; 42175) ORDER BY TableA.TransDate, TableB.ProjectPath
I was under the impression that Access used the same engine as SQL Server. Especially when used as a frontend to an SQL Server instance.
Thanks for the info. I really need to go through some tutorials, I understand the basics of SELECT, INSERT, UPDATE, etc and have used those queries in C# with MySql, but never encountered JOINS before this, was caught a little off guard. I def understand it much better now.
you can do the leetcode.com questions, but they only support MySQL, which doesn't have Windowing features for instance.
Hmm, little more details. Table is massivly not optimized. originated as excel data transferred table consists of Rack Int NOT NULL RU INT NOT NULL Model Varchar NOT NULL Device Varchar NOT NULL source_port varchar NOT NULL label varchar End_port varchar end_device varchar every device has multiple entries - one for every port on the device. Only the end ports and end devices that are used have information, otherwise NULL. 
&gt;Why are you using that garbage? You're wasting our time. You've got time to waste posting shitty comments like this, seems your time isn't all that valuable.
Not sure what version of Access you're using and it's been several versions of Access since I did this sort of thing with it, but it used to be that you could set a pass through option on an Access query. It does exactly what it sounds like and passes the SQL directly to the linked source engine rather than doing any processing on it in Access. Access has a long and confusing history with SQL Server. At one point Access came with an install called MSDE, which was basically the SQL Server Express of the day. Access MDB files have always been able to link to SQL Server via ODBC and linked tables. I generally found that the easiest way to work with SQL from Access. Somewhere in the 2000s versions the ADP file format was introduced. ADP files worked with SQL Server natively. Despite how promising that sounds, I always found ADP front-ends unhandy, but that was probably because all of the VBA code I had for working with SQL via ODBC was useless in an ADP where the database object library was ADO (I think) versus Access' original native JET.
Yes Full and Simple. An aspect of the Full recovery model are differential backups. Point in time restores etc. Basically in interviews for a DBA position I would dig into that just to see if the candidate has any knowledge. For a SQL developer position I would skip the question entirely.
there should be a management studio install on the CD you have, or you can download just SSMS from Microsoft for free.
 create table #tmp(i int) BEGIN TRANSACTION insert into #tmp values(1) GO insert into #tmp values(2) ROLLBACK TRANSACTION select * from #tmp drop table #tmp
thank you!
thank you!
thank you!
You can retrieve more than one attribute in the SELECT part of a query. Simply seperate them with commas (","). SELECT &lt;attribut1&gt;, &lt;attribut2&gt;, ... FROM ... WHERE ...;
I had checked in the job activity monitor - good call. I got it figured out, but thanks! 
What about when I have multiple WHERE clauses? Each one is different
You could always use [FOR XML](https://msdn.microsoft.com/en-us/library/ms178107.aspx). If I remember right, Crystal Reports supports it as a default data source. For example: -- Data idStuff | StuffData | MoreStuffData 1 | Data1 | Data1a 2 | Data2 | Data2a -- Query SELECT idStuff, StuffData, MoreStuffData FROM yourTable FOR XML RAW, ROOT('AllData') -- Result &lt;AllData&gt; &lt;Row idStuff="1", StuffData="Data1", MoreStuffData="Data1a"/&gt; &lt;Row idStuff="2", StuffData="Data2", MoreStuffData="Data2a"/&gt; &lt;/AllData&gt; 
Your example on codepad is ok for this purpose. Simply add the commas in the SELECT part of the outermost query.
Sorry. I should have clarified. So this query will pull results from an old testing procedure and display fields that do not exist in the new testing procedure. I am trying to get rid of the duplicate entries in the fieldname column.
Like this? Select distinct fieldname from old where not exists (select 1 from new where Old.ItemFileName = New.ItemFileName AND Old.FieldName = New.FieldName AND Old.ParentFileName = New.ParentFileName )
wonderful! I'm actually doing just that; I was under the impression that if you use a command, you have to get all your data from that command, hence the need to "Struct" stuff together. All of my individual queries (see here: http://pastebin.com/tdgQGE7x) are good, but when I try to put them together, I get syntax errors. When I try to add a command for each query, I run into problems where TrackIt doesn't like how I've linked the databases together (which is not at all, as I'm not adding any tables, only deriving values from known tables) and fails to generate the report with some error. So, that's that. I look forward to hearing from you!
Why would `where not exists` be preferable? I've been asked this as an interview question, and the answer they were looking for was the `left join`.
Ah cool. Yeah we never worried about query performance at that job, so I don't know why I'm surprised.
[This](http://stackoverflow.com/questions/6485057/sql-group-by-one-column-count-entries-in-another) Stack Overflow page deals with exactly this. All information here is taken from that page. This is called cross tabulation and is included as a feature in some spreadsheet and database applications (often called a pivot function or pivot table). MySQL doesn't support it, but you can approximate it using the sum function and case statements like so: SELECT operator_name, COUNT(*) as call_count, sum(CASE WHEN ok = 'Y' THEN 1 END) as successes, sum(CASE WHEN ok = 'N' THEN 1 END) as failures FROM calls GROUP BY operator_name Disclaimer: I don't know if this is the most efficient way of doing it or if it's efficient at all. If the sum/case statements are expensive, then you could speed it up by only selecting the total and success columns, and from there you can extrapolate how many failed calls there were.
I'm not sure how Khan Academy teaches it, but think of IN as shorthand for "in this list". NOT IN is the opposite. You could also avoid IN entirely and do this, but it's more typing: select * from products where upc between 1 and 200 and upc != 5 and upc != 10 and upc != 20;
So I would want to install this on my linux server and run it from there, correct?
This is Crystal security not giving someone the keys to the castle if they get a Crystal report. Anytime you are going to be managing SQL or database connections in Crystal, go to Database, then verify Database. This will ask you for the password again, just throw in your credentials, and you should be good to go anytime you need to connect to the DB again during that session. If that fails, it's a separate issue, maybe you don't actually have access?
I do have access, I get that error when I try to open it in TrackIt, which doesn't have the verification option (at least I don't think it does)
NOSql means "not only sql" not the literal no
Try creating an ActivitySort_Order table (temporary or permanent, depending on application) with ActivitySort in one column and SortOrder in a second column. Then just join to that table and order by SortOrder.
Why not just use a weighted CTE table and join it to your main table, then use weightValue for ordering. ;WITH weightedSort AS ( SELECT 7575 AS activitySort,1 AS weightValue UNION ALL SELECT 17199,2 UNION ALL SELECT 19780,3 UNION ALL SELECT 7577,4 UNION ALL SELECT 7578,5 UNION ALL SELECT 17200,6 UNION ALL SELECT 17201,7 UNION ALL SELECT 7580,8 UNION ALL SELECT 7581,9 UNION ALL SELECT 7579,10 UNION ALL SELECT 7586,11 UNION ALL SELECT 17204,12 UNION ALL SELECT 17205,13 UNION ALL SELECT 7589,14 UNION ALL SELECT 7590,15 UNION ALL SELECT 7591,16 ) -- Example SELECT * FROM table1 AS t INNER JOIN weightedSort AS w ON t.ActivitySort = w.ActivitySort ORDER BY w.weightValue 
just keep in mind the additional ANDs each time reduce your efficiency a bit compared to an IN(). Edit: Hey figure out what the actual truth is before downvoting... See below Mostly based on indexing vs non as far as major efficiency gains are concerned. Indexed your loss rate is very low but it's still there. http://stackoverflow.com/questions/3074713/in-vs-or-in-the-sql-where-clause
Excuse my ignorance but if you can run an SQL Query against a NOSql database that means that the data is, for the most part, relational. So why would you be using NOSql in the first place? 
Thank you so much. I am already gone in this direction.I have installed Visual Studio 2013 Community Edition. Started to study Visual Studio with the three courses of Lynda.com (what's new in Visual Studio 2013, Visual Studio 2012 Essentials and Visual Studio 2010 Essentials ). Although , the latter two , 2012 and 2010 cover in part some similar Chapters , they have some things where the focus differ. So , I am new to Visual Studio. I have not used it until now, I want to do my best to learn this IDE in order to understand what is the catch with the Report Viewer Control.
Mostly agree with these and they are all fairly common general practice guidelines, except... *** &gt; Only Use Lowercase Letters, Numbers, and Underscores I'd avoid numbers any day of the week for table names, unless absolutely required. I also avoid underscores when possible since I find they make it unnecessarily difficult to read/track; camel-casing with capital letters, even for case sensitive databases is much better if you have consistent naming. The HCI / psychology principle behind this is when you utilize an underscore people's brains tend to interpret the text as two separate objects instead of a single item grouping; on average people can only keep track of 6~7 of these groups, if aren't careful people will have a hard time analyzing queries that get more than a few lines long. *** 
I would add, when you are doing normalization. Prefix your table names to their parent table. You shouldn't have to look at foreign keys to understand the schema.
Yuck.
Clearly you have never dealt with true enterprise level big data schemas that have thousands of tables.
I have, and I still think that's a stupid convention. 
I'm intrigued by the guideline of not overly normalizing. Are there rules of thumb regarding how much to normalize based on the number of variables? I'm coming from the statistics world, where there are conventions (not rules, just ballpark estimates) saying, "You need 10 observations per variable you want to include in regression." I'm wondering if there's something similar for SQL, like, "No table should be more than ___ columns."
maybe inner join to the table again with inner join ( select custid, max(rec_alt_ts) as max_ts from TABLE group by custid ) t2 on t2.custid = TABLE.custid and t2.max_ts = TABLE.rec_alt_ts edit: I meant max
Yeah, what /u/InfiniteFinaleSFW said. You need to look for a sequence or primary indicator on those records.
We have about a hundred schemas of similar scope. The abbreviation is in a way a holdover - originally, the tables were named with ONLY abbreviations back in the 90's on the old systems. When we went through system upgrades in the late 200x's, we added the more descriptive names to the end. It makes it very handy though when making aliases in a query - you can just use the abbreviation as the alias. 
What would be a way to do that?
I have just determined there are multiple email addresses and multiple phones. 
&gt; Otherwise we end up with literally thousands of schemas with a few tables each. How is that any different that having thousands of prefixes? I'd rather have hundreds of schema and synonyms any day of the week. It's also still redundant information on the table name level that only adds needless complexity. I can understand putting a parent or group in the name, that makes some sense. Adding the short in the same name makes absolutely none. Edit - **database.customer.addressesPrimary** is still much more legible than **database.customer.addr_adpr_primary**
Right there with ya, bud. That's atrocious.
what part of "holdover from when we had shorter table names" don't you follow?
Thank you. I think this touches upon one of the methods I was playing around with in my head. It's really good to have the correct terminology available so I can explore further. It seems like linking the products table to a specific attributes table relevant to, say, coffee, and another attributes table to, say, bottled drinks, etc, using a foreign key is what you are alluding to, as opposed to the XML field in the products table to hold those differing attributes. Am I interpreting what you're suggesting correctly?
I think the main advantage was being able to include multiple product attributes within the same field, but vary them depending upon the type of product. I did read somewhere that it makes it very difficult to query that field though. If you take a look at the response I've written to the other person who was kind enough to answer also, does that option sound more suitable, even though it may require more tables to achieve? As I said though, I've found what appears to be a couple of possible methods of achieving this and substantially more differing opinions upon why one is better than the other from both parties. So I'm thankful for a more direct reason as to why one option may not be such a good idea.
Got any tips for someone looking to get into R then?
check out coursera
I think XML usually ends up in databases because it comes from an XML source, such as log files, rather than as a design decision. You can simply have one additional table with a format like key|attribute | value ---|---|---- 1 | color | blue This can allow you to do some stupid things (like specify 'blue' and 'red' as the color of the same thing simultaneously) but it's very flexible and simple.
Not really a SQL question. Why not use SQLite?
I feel your pain, I'm in the medical field and the problem I'm running into a lot is that these organizations have just gone through major implementations within the past two years. This leaves me with too short a window to do proper forecasting analysis on.
The company I work for has an entire Analytics department. http://sloanreview.mit.edu/article/a-process-of-continuous-innovation-centralizing-analytics-at-caesars/
&gt; The goal: to build a deeper understanding not only of customers, but also of operations — everything from food and beverage analytics to labor analytics fancy
yes. !! just a tip: learn some basic ML algorithms and techniques. its berry interesting
that interview fucking sucked cock. as someone whose worked in big orgs who are attempting to use ML to make things better, he sounds like a typical "Data science manager" who has a bunch of squids with PhDs who werent good enough to get quant jobs on wall st or working at google running ML algos on customer data and it not doing anything of value. he created an integrated BI silo..big fucking deal. 
Do it in batches of about 50,000, then commit. Schedule it for times when server usage is low. Repeat for each batch during a given time window.
The ones marked with an asterisk are foreign keys.
In hindsight deleting data in smaller batches using a stored procedure would have been the better option for me too. I still do a similar thing for mirroring tables from one database to another where I have to deal with some schema differences (columns in different order, extra columns in development database etc.).
Yes, I know. Any thoughts about the things I stated above? :)
You're describing the exact scenario we have to deal with here as well so this is just what I was looking for, thanks! 
looks like you've got plenty of solutions here. In case it's useful to anyone else, for mySQL i've done this before just using a limit clause and re-running the query a large number of times with a sleep in between. ie, get a script to run it for 30s , wait a minute, run it again etc. Vary your numbers based on how busy your DB is of course. another option would be to create a new table using a select insert into, and then rename tables around, but the performance hit of doing that might be too much for your workload.
Your stops table is not normalized. This makes this query much harder than it should be. Your stops table should contain 1 row for each stop + line combination. Then you could simply do the following: SELECT s.id, SUM(l.bus) FROM lines l, stops s WHERE l.id = s.line GROUP BY s.id;
Try disabling the Windows Firewall on the server you are trying to connect to. 
It shows you as trying to connect to a computer with the name of Nejc-PC, and a database instance that's called SQLEXPRESS on the default port or through the browser. Is that what you're trying to do?
didn't work
If you don't know what the server name and instance is that you're trying to connect to, I'm not sure how we can help you.
Thank you for answering. I'll look into normalizing the dataset. 
I know I'm late to the flamefest, but I just wanted to clarify: "yuck" because I've been there. I've inherited poorly architected systems, ancient legacy databases, and various other monstrosities with tables numbering sometimes in the tens of thousands. One particularly awful system I inherited was built on Microsoft Access. One table per file. Several hundred tables. I still shudder at the thought of that one. Anyway, given your constraints, it might be the most adequate solution. However, it's still "yuck".
If it's installed on your local machine as a default instance you should be able to use localhost as the server name, your service.msc screenshot looks like you haven't even installed it though. Did you install the server or just Management Studio? There are several downloads available, you need to make sure you get the one with the server included.
I think Redgate SQL Data Generator is exactly what you're looking for, but it depends on your budget, if you can get it then you'll find it pretty good: http://www.red-gate.com/products/sql-development/sql-data-generator/ 
With any editor, or Rapid PHP specifically? 0xDBE does great SQL autocomplete, PDO should be taken care of by your editor's PHP autocomplete.
If this isn't an embedded scenario where SQLite is required, I'd encourage you to investigate a more full-featured RDBMS like PostgreSQL or MySQL.
If this is a commercial project and you don't have your SQL procs scripted and stored in some sort of code repository where every change can be tracked, you are failing at being a SQL developer.
Sure. Glad it helped. :)
I get where you're coming from. I don't have a rational criticism, beyond readability and ease-of-issues, but I think it shows a weakness of most relational systems. Time for sub-schemas or something similar? Namespaces? I don't know. I do know that usually the first thing I do when running into a heap of tables and schemas, is to have my team start cranking out views. With friendlier names!
the answer definitely is no by the way the NOT IN structure might perform poorly... you might consider rewriting the query without the subquery (if i understand it, you want all packt books except those written by andrew duncan) 
You don't need to as long as you're referencing your columns properly in the subquery. If you don't do that, you might end up with ambiguity in the columns in the subquery like Author_ID. But as it is there it should work fine. From a stylistic and performance view with that query, I'd also consider writing your joins properly and reorganising the query to not use a not in. 
The ERD is flawed. There's too much shit going on in the bookings table. Bookings don't depend on guests, guests is a tall table with a fk ref to booking otherwise you'd end up with either booking(guest_id_1, guest_id_2, guest_id_n, ...), a booking table full of a ton of unnecessary rows, then you'd need a payer entity to complete the relationship between bill and booking. Guests is a bullshit table no matter how you look at it. http://i.imgur.com/hdkBWLI.png Once that's fixed then there's a missing relationship between bill and guest (payer) but that's not your fault. I think you've done the best anyone could with what you were given. The only other thing I'd change is rename ref_id to booking_id and remove hotel_id from employees.
As a database guy, it always seems like programmers spend too much time trying to ORMify data into objects, rather than just treating data as... data. So much time is spent with initial setup of ORM tools, DALs, and making weird tweaks to the database to fit a model that isn't 'natural' to the dataforms, only so they can quickly write code that makes terribly inefficient use of the database and ignores most of its' key features. It feels like if they just spent all that time learning to write SQL procedures, they'd have a whole new skillset that would match well with their existing skills. Is this just how every whiney database programmer sounds? After years of writing MS-centric code (mostly vb, vba, c# .NET stuff) I have dreamt about OO database design, but shudder at the realities of implementation every time I actually look at ORMs or similar architectures. I've done tons with ADO and it does the job just fine. It's got it's hassles just like any library or language, but in the end it's manageable and gets the job done. Being able to write my own stored procedures exactly how I want gives me tons of freedom that strict ORM usage doesn't seem to offer. For the things that ORMs do quickly and easily, I can write procedures and DAL calls even faster.
The article claims that one of the main reasons why stored procedures aren't better adopted in Java is the hassle produced by binding them via JDBC. It actually promotes using stored procedures, which are indeed often a better fit.
Absolutely. What are you going to do if you need to test an insert or an update statement without actually making any changes to the target table? A dummy table to use as a target for tests is very useful here.
&gt; ORDER BY items must appear in the select list if SELECT DISTINCT is I rewrote it: SELECT distinct (person.FirstName + ' ' + person.LastName) as FullName , Person.isClient , Person.UDF1 , [Address].City , [Address].[state] , PersonAddress.Person , PersonAddress.[Address] , Phone.PhoneNumber , Email.Email , Person.Website FROM dbo.Person LEFT JOIN dbo.PersonAddress ON Person.ID = PersonAddress.Person LEFT JOIN dbo.[Address] ON PersonAddress.[Address] = [Address].ID LEFT JOIN dbo.PersonPhone ON Person.ID = PersonPhone.Person LEFT JOIN dbo.Phone ON PersonPhone.Person = Phone.ID LEFT JOIN dbo.Email WITH (NOLOCK) ON Person.ID = Email.Person WHERE ( isclient = 'prospect' or isclient = 'client' ) and [Address] is not null and name like '%Mike%' GROUP BY (person.FirstName + ' ' + person.LastName) , Person.isClient , Person.UDF1 , [Address].City , [Address].state , PersonAddress.Person , PersonAddress.[Address] , Phone.PhoneNumber , Email.Email , Person.Website ORDER BY isClient asc; This reduced the number of duplicates but did not remove them.
You don't have a SQL problem ... you have a business logic problem. Until you figure out which addresses and phone numbers you should be pulling then you aren't going to be able to solve this.
They're useful as f*ck that's what they are. 
i guess i worded my question poorly, i don't need to combine the all the records by using UNION, (or INTERSECT) since DB1.Location and DB2.Location has different number of columns - so i can't anyway. lets say DB1.Location is structured as Col_1 Col_2 Col_3 Col_4 Col_5 and DB2.Location is structured as Col_3 Col_2 Col_1 Col_4 Col_6 Col_5 Col_7 Col_6 and Col_7 is unique for DB2.Location - but Columns 1,2,3,4 and 5 are the same for both tables from two different databases. i need to display DB2.Location records according to DB1.Location's structure, with Col_6 and Col_7 can be added manually. the main thing is being able to display records from DB2.Location table in 'DB1.Location format' so to speak.
Why do you have to make a composite key field? Wouldn't it be easier to break the composite into separate fields and store each individually. Then you can make the composite from a view.
In addition, an employee starts at 8.30 to 17.30 and can do different things every 15 minutes. So I have to have set his availability on different times as well....
Thanks, yeah I know I have to have more than 1 table, just don't really know haha. And errrr, do you maybe have the good link because I don't really want to open a bank account! (Maakt verder ook niet uit, heb je eigen bedrijfje?)
Thx for noticing, I changed the link. 
Thanks, was hoping for another answer. I'm looking for clarity, but others are looking to make things simple by not having to change connection strings for older applications. In testing we've found that if you use the same instance name as the instance you're migrating, then name the listener the same as the old FCI cluster name you can get it to work without changing connection strings. 
Just because the number of columns is different doesn't mean you can't use UNION. If I understand correctly Col_6 and Col_7 don't exist in DB1 so you can just pass a few null values to get the right amount of columns to get UNION ALL to work like this: SELECT Col_1 ,Col_2 ,Col_3 ,Col_4 ,Col_5 ,Col_6 ,Col_7 FROM DB2 UNION ALL SELECT Col_1 ,Col_2 ,Col_3 ,Col_4 ,Col_5 ,null ,null FROM DB1 If this is something you have to build a report for that's frequently used I'd just create a view in DB2 with the columns in the right order and then use that instead.
1) You can find out the current date in Oracle with "sysdate": SELECT sysdate FROM dual; 2) Use GROUP BY and the COUNT function. 3) Why should a view not have joins? Start with writing the query and add the view creation commands afterward. 4) Write the query first and add the PL/SQL stuff afterward. The COUNT function should help.
This looks like homework and us doing it for you won't help you, I'll give some pointers, but you should do it yourself. When you say old syntax I take it you mean join syntax? Why the hell are you not using modern ANSI joins?! If your course supervisor is encouraging it, then they need some remedial SQL lessons. 1) Join issues to book and student tables (using Fk) and select accordingly 2) Assuming unique book count, join issues to book and student tables and select count(distinct B.BOOK_NO) from this group by STUD_NO 3) Join issues to book and student tables selecting the columns you need and create a view on this. Views can have joins. A view (non materialized) is a query that has been given a wrapper to allow for re-use or hide complexity. 4) Join issues to book and student tables (using Fk) and select count.. and wrap it in create or replace function Student116BookCount return integer is 
&gt;2) Find the number of books taken by each student. (my guess is count something and distinct? I really dunno about this one) Just wanted to respond to your "count something and distinct" statement. There are two ways to get one row per entity (in this case, student). The first is simply to use `distinct`. This is kind of quick and dirty, and it will simply give you a list of all unique combinations of columns returned by your query. The only time you would use this in conjunction with `count` is the syntax `count( distinct book_no)`, which instead of returning a total count would return, maybe, all the different books a student has ever checked out, even if they checked out the same one multiple times. This is almost certainly not what the question is asking. It's asking for a simple aggregate, so you are correct that `count` is the way to go. I just wanted to explain the difference in a little more detail. Let me know if you have any questions.
It's the 2013 final exam of DBMS here at Caledonian College in Oman, I know it looks like homework the whole college is a big joke really. And as for the old join syntax, well, we're being taught really old material, probably because they haven't kept up with the world *(and yet they charge $5k USD a semester...)* 1) I know I'm suppose to use FK but I'm just not sure how to join three tables, and sadly the software we use at college is discontinued and I cannot find it on Oracle (they upgraded to a new version that just feels more complicated) so I can't test if my code is correct. 2) Okay thanks, again if you could tell a way to verify if my query works that'd be great. (all the online 'sandboxes' I found give errors probably due to the old syntax) 3) OK. 4) ugh.. there's so many triple joining going on I wish you would just show me an example.. Thanks Ziptime!
1) That's not the problem, I'm trying to figure out how to join 3 tables (or if there is another method) 2) select count(iss_no) group by stud_no? 3) Duh, ikr. Okay will try. 4) k Thanks
Old syntax (ANSI 1989 joins) ... select s.STUD_NAME, b.BOOK_NAME, i.* from ISSUE i, BOOK b, STUDENT s where b.BOOK_NO = i.BOOK_NO and s.STUD_NO = i.STUD_NO New syntax (ANSI 1992 and newer joins) ... select s.STUD_NAME, b.BOOK_NAME, i.* from ISSUE i join BOOK b on b.BOOK_NO = i.BOOK_NO join STUDENT s on s.STUD_NO = i.STUD_NO
I don't know of any software that does that reliably. You mean you want to see a graph-like representation of how tables relate to other tables based on primary and foreign keys? Because the best a software could do is to make guesses at relating tables. It might be close but it would never be exact. I work with a database that has 100s of tables, and my method is to maintain a few spreadsheets of key relations to at least get me started. Then I join in the more esoteric tables as needed. The first time I use them, I add them to my excel sheets and draw boxes, lines, and right/left join arrows. 
[Oracle Data Modeler](http://www.oracle.com/technetwork/developer-tools/datamodeler/downloads/datamodeler-087275.html)
&gt;if the source and destination column names are not the same, it isn't smart enough to connect it for you. How would SSIS know what you wanted in this case?
I guess since I'm only doing a 2:2, I assumed it would connect both.
Well it wouldn't matter what SQL version is used. Most of the core logic hasn't changed that much. Also I mostly use Linux, but Windows is n no problem for me. And I have no idea what I did on the 25, it's kinda if a blur from 24 to 26, sorry?
It matters immensely, which is why I asked. 
It is very inefficient. I only do tables and arrows, PK and FK(s). It's for my own reference, nothing official. It started as a quick and dirty way to remember how to link tables and nothing more. But if management said "lemme see your whole schema" I don't know what I would do. Someone else posted a link to Oracle Data Modeler [screenshot](http://www.oracle.com/technetwork/developer-tools/sql-developer/what-is-erd-117795.png) ... I might actually look into that. That's basically what I do in Excel . 
That sounds like you are creating the graphing system every time you have a project
That sounds like you are creating the graphing system every time you have a project
Ok, say we are using SQL Server 2014.
I hadn't heard of Visio. Is that Microsoft Visio?
Eventually i plan to automate it as much as possible. However I'm not sure I will ever get around to it. Unless the database schema you are trying to visualize is very simple, even with commercial products there have been a lot of things that need to be tweaked. For example, if you only rely on foreign key relationships to define the edges for the nodes, you may be missing a lot of relationships in the data. 
select class_name, count(class_name) from table group by department_name Perhaps?
tried this select count(crs), crs , unit from strike group by crs, unit with rollup and a cube 
VISIO, SSMS 2012 works well
&gt; Eventually i plan to automate it as much as possible. However I'm not sure I will ever get around to it. Yeah that seems to be where the conversation is going towards, making our own script (which wasn't out of the question when I asked). I have a lot of experience with Java, so I might just make it myself; shouldn't take very long. Hopefully we can find a better alternative without paying an arm and a leg. 
SQL is utilizes set-based logic to return result sets quickly. Cursors use cursory-logic, meaning it will iterate through all rows and perform the function you program into the cursor. If you have any experience with object-oriented programming, you could compare this to a for loop. To use a cursor, you will need some kind of result set. You can select data from a table (which is the typical method), or you could make a result set in any other way (view, CTE, etc.) Let's select data from a view and populate a table. For this example, let's say that the table (test) already exists and has a column name ID (datatype INT). Below are the steps to make this work: 1. Declare the variables 2. Define the cursor's result set 3. Loop through the cursor's values 4. Close the cursor declare @id int; declare @cursor cursor; set @cursor = cursor for select id from v_id open @cursor fetch next from @cursor into @id while @@fetch_status = 0 begin insert into test (id) values (@id) fetch next from @cursor into @id end close @cursor deallocate @cursor So as you can see, we begin by declaring the variables. We need a variable for the cursor, as well as all variables that the cursor will be using. In our case, we are using one column in the cursor (id). If we had say 3 columns, we would need 3 variables (one for each column). Next, we load the cursor with a result set. It needs a set so it can iterate through something. In our case, we selected the id column from our view (v_id). Next, we will actually iterate through the result set. We first open the cursor, grab the first line of the result set and load it into @id, and then we say we want to insert into our table (test). The while loop is what allows the iteration to actually happen. Lastly, we close and deallocate the cursor. Cursors are pretty easy to use, but please note that you could do what we did here with set-based logic: insert into test (id) select id from v_id Set-based logic is much faster, so I would always recommend using it when you can (which should be almost all the time). There are some instances where you will need to iterate over all rows. In those cases, you can use a cursor.
Don't sweat them too much. There are very few situations where a cursor is the appropriate solution. Work in sets, unless the **only** way out is a loop.
you want to fetch at the very end of the loop. It appears that after you do some name manipulation you fetch another row (on line 12). You should put that fetch at the very bottom of the while loop. Otherwise you will skip rows.
Even more of a reason to +1 what /u/coldchaos said: import to a staging table first.
One approach: add a rn = row_number() using the proper order and partitioning (Date, Status values, etc) to the original query, then enclose the original query in select * from ()o where o.rn = 1
result-wise: absolutely no difference performance-wise: possible difference, but unlikely semantic-wise: explicit JOIN syntax is far better p.s. it's not a cross join if you have a join condition ;o) some would argue that because it's in the WHERE clause it's a filter condition, but don't believe them
&gt; Can you think of a good way to remove the duplicates? you mean in a query? sure, just take the row with the lower name first WHERE ... AND Name1 &lt; Name2 you mean to delete them? different story 
If I understand what you are asking something like this could work: SELECT * FROM ( SELECT rn=ROW_NUMBER() OVER (PARTITION BY t1.code ORDER BY t1.start, t1.end, t1.code), whatever, you, need FROM yourTable AS t1 INNER JOIN yourTable AS t2 ON t1.start&lt;=t2.start AND t1.end &gt;= t2.start WHERE t1.code &lt;&gt; t2.code ) AS sq WHERE sq.rn = 1 
Even performance wise there would need to be a bunch of joins to make a difference. It all comes down to personal style (I agree, JOIN syntax is not only better looking, but way easier to keep track of). My cranky old Russian Mentor was all about implicit joins though so who knows lol.
Presumably you are doing something like: SELECT P1.Code Code1, P1.Name Name1, P1.StartDate Start1, P1.EndDate End1, 'X' Conflict, P2.Code Code2, P2.Name Name2, P2.StartDate Start2, P2.EndDate End2 FROM dbo.Promotions P1 INNER JOIN dbo.Promotions P2 ON (p1.StartDate BETWEEN p2.StartDate AND p2.EndDate OR p2.StartDate BETWEEN p1.StartDate AND p1.EndDate) AND p1.code&lt;&gt;p2.code All you really need to do is specify the arbitrary "keep" phrase in the join predicate: AND p1.code&lt;p2.code -- keep the conflict with the lowest code or: AND (p1.StartDate&lt;p2.StartDate OR (p1.StartDate=p2.StartDate AND p1.EndDate&lt;p2.EndDate)) -- keep the conflict with the first start date, or if the same, the first end date. [SQL Fiddle code and examples](http://sqlfiddle.com/#!3/822f0/2/0) 
Deleting, in theory, is just a matter of DELETE P2 FROM dbo.Promotions P1 INNER JOIN dbo.Promotions P2 ON (p1.StartDate BETWEEN p2.StartDate AND p2.EndDate OR p2.StartDate BETWEEN p1.StartDate AND p1.EndDate) AND p1.code&lt;&gt;p2.code ...*but*, there a really insidious problem. What if Promo7 conflicts with Promo8, and Promo8 conflicts with Promo9, but Promo7 doesn't conflict with Promo9? Here, just picking a loser for each conflict will get rid of too much: when you delete Promo8, you no longer have a conflict with Promo9. In this case you'd have to actually put the whole thing in a loop, getting rid of one conflict at a time until there were no more conflicts. 
&gt; Even performance wise there would need to be a bunch of joins to make a difference a bunch of joins for these two tables? what the ....
Any table represented in the where clause is considered an inner join.
You need to set up a TNS (Transparent Network Substrate) entry to your DB instance. This is network information telling Oracle clients (like SQL plus) where your server is, it's needed because you could have many instances to Oracle DBs on multiple locations and networks. [Read more here](http://www.orafaq.com/wiki/Tnsnames.ora). Some modern clients such as SQL Developer can connect to Oracle in different ways other than TNS (LDAP, TCP, SDP etc), which might be what you're experiencing.
Think of a cursor as your finger when YOU must read a written paper list then do something with each line of information. Say you were transcribing an old paper phone list into your address book on your computer........you would place your finger under the first item (to keep your place as you go) then do something with the information........You then come back to your cursor, increment it (put your finger on the next line) and again, do something with the info..... &lt;Wash, rinse, repeat until no more lines&gt; 
This tool can help you generate complex sql queries but doesn't support updating, deleting, or inserting data: veroanalytics.com. Its good for advanced reporting questions.
I just downloaded [Oracle Database Express 11g](http://www.oracle.com/technetwork/database/database-technologies/express-edition/overview/index.html) and installed the HR schema to practice on. You could use sql loader to load your CSV into a table within Oracle. Then you just make an ODBC connection in windows and have SQL developer use the ODBC connection. Easy peasy. 
Seconding this. If you want to dive further, you can always setup your own localized Apache server with something like XAMPP and then you can connect in with a bunch of different connectors, depending on your fancy. You could use MySQL through that, or use like krankie stated, an ODBC to connect in, or a JDBC, or a few other options. The easiest way is probably loading a CSV inside SQLDeveloper and doing just ad-hoc analysis that way.
Don't take the comment for anything but a joke at my expense, which it was. That said you can call a Python script from a web interface and generate your excel file. The only thing similar to what you've done that I've messed with is generating reports /w graphs and shit and spitting them out as PDFs. Good luck though.
I do have a TNS file. It’s the one that was set up by the installer. As far as I can tell, it’s set up fine. Has the proper host at least. Now, I don’t see anything in this file that seems to associate instances with particular users. So I assume that there is some sort of global nature to the file. Is there any reason that a bad TNS might cause a problem for one user name but not another? 
Can’t start oraenv. Should oraenv automatically be on my path? If not, where should I look for it? Actually using Windows 7, if the file structure is different there.
Format your god damn code for fucks sake. Edit for the downvoters: It was all on one really long line before. Just a big ol brick of sql.
The report viewer control is pretty simple to implement. For some reason though a lot of web guys don't like it because it is older technology. I am not a web guy, but I get into service apps a little bit. You pretty much just point it at the SSRS server endpoint and enter the report path and click go. Make sure you use the path of the actual soap web service and not the path of the report manager url-which is what a lot of unknowing sys admins will give by accident-a lot of them don't even know there are two other endpoints exposed in SSRS.
I will try that - I am using a MySQL database with a PHP application
I see, I am confused by the Convert part at the end, I don't understand exactly what it is doing and it produces an error on my end. I don't know enough about the function to debug it. It is just saying unexpected stuff modifier. Thanks!
You see through my scare tactics. I also use DISTINCT, but only in cases where I intend to group by every column. A more precise version of my rule is "Don't use DISTINCT to fix results." DISTINCT is almost never the better solution to remove unintended duplication. Be intentional with one to many joins where you want one result.
Agreed. Usually get people to do something else because as soon as they need that one extra column/etc it breaks everything. Half the time I show people how to use ROW_NUMBER() over a partition so it's easier to pull all the information, then work from there. 
Heres my shot at it What you really need to start looking into is ways to transpose data from row data to columnar data. This can be accomplished with [PIVOT](https://technet.microsoft.com/en-us/library/ms177410%28v=sql.105%29.aspx) or your SQL engines equivalent. If not the below code works on most systems. --Using aggregates instead of DISTINCT SELECT wt.ride --nulls will be omitted from aggregates ,AVG(wt.daywait) as daywait ,AVG(wt.timewait) as timewait ,AVG(wt.dayTimeWait) as dayTimeWait ,wt.currentWait FROM ( SELECT waits.ride --This is old way to pivot data but works across SQL platforms CASE WHEN it matches criteria NULL it doesn't to be ommitted from aggregate. ,CASE WHEN dayofweek = '" . $dow . "' THEN waittime ELSE NULL END AS daywait ,CASE WHEN currenttime &gt;= '" . $minusTime . "' AND currenttime &lt;= '" . $plusTime . "' THEN waittime ELSE NULL END AS timeWait ,CASE WHEN currenttime &gt;= '" . $minusTime . "' AND currenttime &lt;= '" . $plusTime . "' AND dayofweek = '" . $dow . "' THEN waittime ELSE NULL END AS dayTimeWait --MAX doesn't really matter we are ordering by WID over wait.ride. ,MAX(waittime) OVER (PARTITION BY ride ORDER BY WID DESC) AS currentWait FROM waits WHERE park = '" . getPark() . "' ) as wt GROUP BY wt.ride -- currentwait is already aggregated in subselect it should be same value for all wt.rides ,wt.currentWait 
I would use aggregate functions with CASE statements to count the scores you want to see, then apply logic to the employee group based on that. Something like this... SELECT [EMPLOYEE_NAME] = ST.[EMPLOYEE_NAME] ,[GROUP_NAME] = CASE -- CHECK IF WE HAVE MORE THAN 2 LOW SCORING REVIEWS. WHEN ( COUNT( CASE WHEN ST.[REVIEW_RATING] &lt;= 2 THEN ST.[ORDER_NUMBER] END ) &gt;= 2 ) THEN '2+ LOW REVIEWS' -- CHECK IF TOTAL REVIEWS EQUALS THE NUMBER OF HIGH -- REVIEWS (ALL HIGH) AND TOTAL &gt;= 10. WHEN ( COUNT( CASE WHEN ST.[REVIEW_RATING] &gt;= 4 THEN ST.[ORDER_NUMBER] END ) = COUNT(ST.[ORDER_NUMBER]) AND COUNT(ST.[ORDER_NUMBER]) &gt;= 10 ) THEN '10+ ALL HIGH REVIEWS' -- OTHERWISE UNDEFINED GROUP CURRENTLY. ELSE 'UNDEFINED' END ,[HIGH_REVIEWS] = -- NULLS NOT COUNTED, ONLY COUNTS SCORE OF 4+. COUNT(CASE WHEN ST.[REVIEW_RATING] &gt;= 4 THEN ST.[ORDER_NUMBER] END) ,[LOW_REVIEWS] = -- NULLS NOT COUNTED, ONLY COUNTS SCORE OF 2-. COUNT(CASE WHEN ST.[REVIEW_RATING] &lt;= 2 THEN ST.[ORDER_NUMBER] END) ,[TOTAL_REVIEWS] = -- TOTAL COUNT OF ALL ORDERS FOR COMPARISON. COUNT(ST.[ORDER_NUMBER]) FROM SURVEY_TABLE ST GROUP BY ST.[EMPLOYEE_NAME] This is a trick I frequently use with complex aggregate counts or custom business logic (usually when writing a SQL Statement to fuel a report of some kind). Essentially, the aggregate functions (SUM, MAX, MIN, AVG, etc...) exclude NULL values. If you use them around a CASE statement that returns a NULL for records it should not count (either explicitly in the ELSE, or just by not having an ELSE) they will be ignored and excluded from the aggregate evaluation.
 [Column1] + COALESCE(':' + [Column2], '') + COALESCE(':' + [Column3], '') ... It's not a function but I think it'll work the same way assuming Column1 is not NULL, either way it's a start.
Parenthesis can make a difference, but they're kinda redundant with LEFT OUTER JOINS. They're quite useful when the join order is forced, either with a join hint or OPTION (FORCE ORDER) SELECT Stuff FROM BigTable t LEFT OUTER HASH JOIN Lookup1 l1 ON t.Lookup1ID = l1.Lookup1ID LEFT OUTER HASH JOIN Lookup2 l2 ON t.Lookup2ID = l2.Lookup2ID LEFT OUTER HASH JOIN Lookup3 l3 ON t.Lookup3ID = l3.Lookup3ID is slower than SELECT Stuff FROM Lookup3 l3 RIGHT OUTER HASH JOIN (Lookup2 l2 RIGHT OUTER HASH JOIN (Lookup1 l1 RIGHT OUTER HASH JOIN BigTable t ON t.Lookup1ID = l1.Lookup1ID) ON t.Lookup2ID = l2.Lookup2ID) ON t.Lookup3ID = l3.Lookup3ID because hash joins are faster with the smaller (lookup) table on the right. In most scenarios the query engine orders the joins properly.
Generally speaking, most DBAs spend a good amount of time as sys admins and/or back-end developers who deal with databases a lot. Being a DBA doesn't have too much to do with the kind of work excel power users do, that is more along the lines of a data analyst or SQL developer career path. Being a DBA is highly technical work that requires a lot of domain knowledge around server, storage and OS tuning, not to mention software development. 
Its hard to answer 'how much sql'... Just learn the basics and start applying them. If you fully understand SELECT, UPDATE, INSERT, DELETE, and how to use JOIN, you can do a ton. You dont need to know any of the alter or create stuff, thats what SSMS is for. Do you know what a view is? A table? You understand what a primary key is and why you need it? Youre probably good to go. There's not much to it. I fell into a DBA role at a growing company knowing less that even that stuff and did just fine. Learn as you go, langauges are intentionally designed to be as easy to use as possible. Theres no magic. Youll make mistakes. Thats what development environments are for, it'll be fine.
Thanks, I'll check it out.
&gt; You dont need to know any of the alter or create stuff, thats what SSMS is for. Sorry, I cannot agree with this. 1. You can't "use SSMS" to create temp tables. If you need temp tables, you need to do it via T-SQL. 2. The table designer in SSMS has bugs, can screw things up, and sometimes does things in a sub-optimal way. The Import/Export Wizard has similar gotchas. 1. The table designer will let you add columns to the middle of a table, which requires dropping &amp; recreating the table entirely. Column order doesn't matter on tables, but it'll let you think that it does. This makes part or all of your database unusable for the duration of the process. An `alter table add new_column` takes essentially no time and gets it right. 3. You should always understand what a tool is doing behind the scenes when it's just an overlay for something you could be doing yourself. And that's what the table designer is doing - creating T-SQL to carry out the changes that you're making. 4. If you can't script it, you can't reliably repeat it, you can't easily version control it, and you can't easily diff it.
No doubt everything you say is important and I would look for all of that in a senior DBA, but my point is that, if you're just getting started, it's OK to not know that stuff. Surely his work will suck and need review, but that's how you learn. Assuming he's not working in live banking data or missile guidance, it will all turn out OK.
DELETE FROM A WHERE EXISTS ( SELECT * FROM B WHERE B.B1 = A.A1 AND B.B2 = A.B2 ) This deletes everything from A that exactly matches the rows in B. I'd test it first, to make sure you are getting what you described -- depending on your environment, it should be simple to make a couple of temporary table and test your syntax before running it.
I don't use oracle, but could they declare and use a variable? Not that it would help much - you'd still have to use the variable name twice, but at least they'd only have to change it in one place. 
Awesome explanation. Thanks!
/u/AlexEatsKittens has the right of it, DBA's is a fairly broad title, but generally I tend to define us as stewards of the information. DBA's have the expectation to be one of the most trusted individuals at a company, because they, by nature, have one of the most complete overviews/access for the entire company. On the day to day, you're expected to ask the important questions, for example: * What is needs to be stored? (Data types) * Where does it need to be kept? (Partitioning / Schema Design / etc) * When do important operations need to occur? (Backups / Jobs / RPO / RTO / etc) * Why does this information need to be stored / stored again? (Normalization) * Who needs access to this information? (AD User Groups / Database Roles / Least Permission / etc) * How is the database going to be run? (SAN Delegation [Optimized caching/etc], AD &amp; Server Administration / Maintenance / etc) On top of everything else, you have to have a spine and good communication skills. DBA's are often the last (*only*) sober second thought when it comes to data management and security. You may find yourself navigating/negotiating between vastly differing views to ensure integrity of the systems; not always in the job description, but having pride in your work has its detractions. 
I prefer to start learning with good habits. It's much easier than un-learning bad habits later.
If you "aren't SQL knowledgeable", you won't be able to implement either of those options without your DBA or report server admin being involved because they both require administrative-level access. Talk to them about what you need. Also, make sure your query isn't hammering the server. If you're inexperienced, it's likely that your query needs tuning.
Nothing wrong with that. Lots of different learning styles out there. I find that I get more out of just 'diving in'. Otherwise I suffer from a sort of analysis paralysis and just don't make progress towards my goals. In this case, OP is asking when it is appropriate to try for jobs in the field. I feel like jumping in and starting hands-on learning (under guidance of a senior) is more effective than independent study until you feel 'ready'. Sometimes 'ready' just never happens and you end up working help desk for another 5 years wondering if you're 'good enough' for the job you really want.
One of the more "impactful" ways that you can teach yourself this stuff is to apply to your current role in some way. You say you work in a desktop support role. Build a little database of your daily activities (ticket requests, request types, who is requesting, how long to resolution, days open, etc.). Your company may already have a ticketing system in place which is fine but you could leverage some of that data in your own db. Learn how to organize your data in a dimensional model (or tabular if appropriate) and be able to write queries for questions that you or someone who makes decisions in your department might have. Simple questions like "How many tickets did I have over the past month, week over week? On average, how long does it take me to close out a ticket? Who is the most active ticket creator?" Learn to write queries using those questions. It is a great way to learn and it applies to a real business solution. Where the REAL value comes in is when you get to a comfortable point, maybe show your manager some of the queries your have written. Maybe even build out some simple Pivot table reports in Excel using your data. A lot of great "I didn't know I needed to know that" scenarios pop up that way. You immediately make yourself more valuable to your company and it also shows that you are thinking of ways to improve the overall "Business Intelligence" for the desktop support group. It allows you to learn yet not be your "day job". Even if your db goes nowhere, it will be a great learning experience and an even better speaking point on your resume. It shows that you saw a problem (lack of insight to the group) and decided to see if you couldn't help solve it through data. Trust me, this kind of "I want to solve a problem that no one is paying attention to" goes a long way in interviews.
My bad, that isn't actually in the code, it was a typo.
Thanks for the suggestion. It looks like that will be a great way to get practice and learn more.
Do you actually need to specify which schema to use? Can you just call the tables under the set schema like SELECT xxx FROM SCHEMA1.TABLE Inner Join SCHEMA2.Table or something of the sort? Hadoop works like this as a cloud DB clustered instance.
Thank you I really appreciate it!
 SELECT COUNT(CASE WHEN Servers.[Environment] = 'Production' THEN 'humpty' END) AS Production , COUNT(CASE WHEN Servers.[Environment] &lt;&gt; 'Production' THEN 'dumpty' END) AS Other FROM Servers 
Or cheat and use a pivot table.
I've never used CASE before, but it doesn't seem to work in Access. But that led me to the IIF statement which I've never heard of. I'll play with that. Thank you!
I tend to do this kind of thing with sum() and case statements. Something like this: select sum(case when s.Environment = 'Production' then 1 else 0 end) as Production ,sum(case when s.Environment &lt;&gt; 'Production' then 1 else 0 end) as Other from Servers s Ah jeez didn't realize you were doing this in Access. That would have been helpful to know ;) Not sure what I'd do there. Could get lazy and do 2 separate count queries then a 3rd query to bring them together.
test by making the delete a select. The select shows you what will be deleted SELECT * FROM A WHERE EXISTS ( SELECT * FROM B WHERE B.B1 = A.A1 AND B.B2 = A.B2 )
Assuming you aren't trying to update an identity value (which has other restrictions) this should work in mssql, not sure about mysql or others. UPDATE a set a.id = b.id FROM b WHERE b.name = a.name 
Business Intelligence and Reporting is where it is at. Just raw data manipulation, mining, analysis and insight will be something that is always needed by almost any company. The buzzwords right now are "machine learning" and "big data" but there are still everyday needs that almost all businesses need these days. Data is king for each and every company. There are mountains of data out there just waiting for warehousing and reporting.
SQL is still the main source of data storage for most websites. I'm not sure what makes you think its less popular. Most CMSs are built using it.
Are the names unique in each table? If not (and it doesn't sound like they are) you'll have to think about what you want the DB to do where there are two different ID's that it could be. Making a subquery for the second table that ensures that name is unique would work, and would look something like: UPDATE a set a.id = b.newid From table1 a inner join (select name, max(id) as newid from table2 group by name) b on a.name = b.name
Yes, that's one way.
Okay I was a bit wrong, should be a left outer join. Here's a SQLFiddle: http://sqlfiddle.com/#!3/3c49d/19 SELECT e.empid FROM hr.Employees e LEFT JOIN sales.Orders o ON e.empid = o.empid AND o.orderdate = '2008-02-12' WHERE o.empid IS NULL
With a couple more parentheses this was perfect and much more elegant than my solution. A great example of how to use the IIF function as well. Thanks so much for your time.
since it's mysql, gotta use mysql syntax ;o) UPDATE table1 INNER JOIN table2 ON table2.name = table1.name SET table1.id = table2.id
aaargh... sorry about the parens, yeah, i rushed my answer a bit
I'm doing 70-461 next week, and I'm weakest with XML, which is apparently weighted quite heavily, this is exactly what I'm looking for thanks :)
how good were you at sql before you started? I can do pretty much the basic pulls with basic two or three table joins and subqueries. how much should i be worried
You'll learn more in your first week or two on that job than you can possibly imagine. The only way you can really get better though is to actually write SQL, which means installing some sort of server, loading it with data and playing with it. All of the SQL dialects are 95% the same, but if you can, get one that's as close to what you'll be working with as possible. PostgreSQL seems to be the most respected of the free options.
The most important thing to remember is as little as you know, you are most likely still going to be the person who knows it the most in that workplace. Otherwise they would have had someone who knows it better than you in the interview process. If they DID have that person interview you then they feel your current level is acceptable. You will be fine. If you are going to be using MS-SQL, then hit up their [Virtual Academy.](http://www.microsoftvirtualacademy.com/product-training/sql-server) More important than your current level in my opinion is your willingness to learn and ability to learn.
the following assumes you have the percent allocation stored in the table, or you could create a quick temp table and join to that instead. select job_id, job_pct * ip from jobs j cross join (select sum(inventory_price) ip from inventory) i 
Learn what a window function is and what they're for. Even if don't use it very often, you'll have an *aha* moment at some point where you think, *I bet I can use a window function to solve this.* Being able to write one on the fly is just bonus points.
Exactly, so then you are fine. I interview people all the time and we ask tons of SQL questions to gauge their level. You are most likely going to be miles ahead of anyone else there. Breathe easy and work hard and you will gain tons of valuable experience.
Seeing table names prefixed with tbl makes me cringe.
Khan academy recently launched and SQL course too.
The response I got when I mentioned it was, "But how would you know it's a table?" ...
I was in your shoes a few months ago. I learned syntax and basic stuff via [SQLZOO](http://sqlzoo.net/) and [GalaXQL](http://sourceforge.net/projects/galaxql/). SQLZOO is web based and GalaXQL is a downloadable application, so you don't have to do much prep work. Then I did [Learn SQL the Hard Way](http://sql.learncodethehardway.org/book/) which shows you how to use SQL in the command line. It's great but is a work in progress - last I checked only 15 lessons were done. Good luck!!
Yea they are different. I just changed all the names in this post.
I was able to get this one to work! Thanks!
You can experiment with SQL syntax here: [http://sqlfiddle.com/](http://sqlfiddle.com/). 
[This is a good start.](http://www.w3schools.com/sql/)
Its really hard to learn sql without having access to a good database with realistic data. No compilers, IDE etc required. You connect to a database, type out your sql query and hit execute. The database receives your query, parses it, and executes code to retrieve your data. Once its done, it will send data back to the requester. Easy peasy. I learned SQL by understanding how SQL generation tools did it as a starting point. These days there arent many that you can just download without signing up for some massive enterprise deal. The only one out there that I've seen is a tool called Vero Designer over here www.veroanalytics.com. It might be a way to get started or it may solve all your needs.
Holy Moly! I do that crap in Excel!!! If this is real (and I won't be able to tell until Monday) then you get the satisfaction of knowing you've raised a smile in the UK 
I love keyboard shortcuts and you taught me a new one. Thanks!
Sweetness!
Neat trick, but it only works if all the items in your list are the same size like the GUIDs in your example.
I had sql this year in college(14weeks,2hours/week) Here you can see all the answers to questions: http://www.docdroid.net/13app/oplossigenoefeningen.pdf.html Unfortunately there is no theory included, still could be very helpful
That's a bummer. At least it'll always work on the left side.
looks like you're missing a timestamp column how were you planning on identifying the "most recent entry" for each item?
You're a beautiful human being. thank you
This is amazing. I used to make a cab in excel, then open in notepad, and copy+paste. This will save so much more time.
Ha. I used to make a program to do it and what not. Such a pain in the ass. Blew my mind when someone showed me this.
You can use regex replace in SSMS to do the same thing, almost as fast.
You mean you *don't* have African mole snakes in your server closet?
&gt; have using SSMS since version 6.5 Uhh....SSMS didn't exist until SQL Server 2005.
You can also alt-mouse move for [box selecttions](http://imgur.com/P69f08q.png). 
Yes - you need some way of interfacing with the database (which you also need to have running somewhere). There's lots of "front end" software available - many of them free, and then, of course, you need a database to connect to. A really easy way to start practicing might be with mysql. You can install it on your own machine by opening up a terminal, then typing: apt-get install mysql-server mysql-client (add sudo if needed). Google for walk-throughs on the setup. Once that's done, you can just: mysql -u username -ppassword Then: CREATE DATABASE dbname; Then google around for examples on how to create a table and populate it with data. Once you've done that, you can just: USE mydb; and then query away: SELECT * FROM mytable; You can do all your practice right from within that same terminal, no front end or third-party software needed. Postgresql would be similar: https://wiki.debian.org/PostgreSql Once you know what you're doing there, you can look around for front ends. As I mentioned, there are lots of them: dbeaver, squirrelsql, heidisql, navicat, mysql workbench, etc. You can configure those to connect to either local or remote databases.
From almost 20 yrs in retail IT, Me thinks that your Item_id should be the key.........but if this is what you are working with, then let's move on Are you sure you have all the fields? I think there should be a date field in this table too....
There is a date/time stamp, but the latest entry will be the current total. The table records aren't updated, the table is just added onto.
The item_id will repeat in the table whenever there is a change in inventory, so it shouldn't be used as the key.
Thanks! I think this comes closest to what I'm looking for so far. From the replies here and from other reading, this is my understanding so far: SQL is neither a standalone programming language nor a self-contained database environment. Instead, it is a syntax used by a class of programs commonly called "database management systems." So if I want to use SQL, I first need to install a database management system. MySQL, heidisql (thanks /u/r3pr0b8), SQLite3, *et al.* are examples of database management systems that use SQL. Is this correct?
lol. I currently work at a shop where this is done. Every view begins with vw, Stored Procedure begins with sp, etc. 
&gt; the latest entry will be the current total yeah, but how do you know which is the current total? if you don't understand why i'm asking, show us a few rows of sample data 
no column mode is as powerful as ultraedit's. in fact, configured correctly with it's "tools" it becomes the most powerful windows-ide imo. 
Thanks man! I didn't consider searching for empids that had nulls on that orderdate. Kind of opened my eyes to a new way to search things. Your the bestest! 
And it is *awful*. I still have to use it sometimes.
well shit, I've been doing SQL for years ... and never knew about this, just hope I can remember it now. have a useless internet point
rows in a database do not have any order how do you know which one is the last one? you need a timestamp column using an auto_increment or identity or serial PK as your sequence column is a design error
That's cool and works in VS, too but I'd still prefer to do a regex replace.
It's basically the best thing ever... It also works in conjunction with ctrl and the arrow keys and pageup/down and home/end.
Don't really see a ton of content-rich answers here .... SQL-based databases are the gold standard for storing structured data, whether for websites, transactional systems (like point of sale), analytical purposes (like a data warehouse), etc. There are some very popular alternatives, generally referred to as NoSQL ("not only" SQL) data stores geared toward *semi*structured data, such as email or log files.
that's correct
It is indeed real, and other good text editors supports this action as well. I use it everyday in Visual Studio for example.
Does the txt need to be the same number of character?
I have literally written PowerShell scripts to format text like that. My life is changed and mind is blown. 
[SQL Server Management Studio is a new authoring and management tool for database administrators and developers. It combines the functionality of Enterprise Manager, Query Analyzer, and Analysis Manager, and also allows management and authoring for SQL Server 2005 Reporting Services (SSRS), SQL Server 2005 Integration Services (SSIS), SQL Server 2005 Notification Services, SQL Server Compact Edition, replication, and previous versions of Microsoft SQL Server, all through the same interface.](https://technet.microsoft.com/en-us/library/ms170909%28v=sql.90%29.aspx)
Dang, this sub needs to get its text editing skills up to par. If this is groundbreaking for y'all, there's a whole world of time saving tools and techniques out there.
tblCustomer.fldIntCustId just in case you forget it's a field in your database table, of type int. The best is when types change but column names don't get updated... Ugh 
Why wouldn't you INSERT INTO foo(...) SELECT ... FROM TBL_XYZ WHERE COL1 = 'abc'; ? 
Oh ok ... but I worked in sybase ASE and we never had a problem with the prefix.
Yeah, it's just an MS SQL server thing I think,
&gt; SQL query which is on inner part of main query is called inner query while outer part of main query is called outer query. shit, this is the type of incisive, clear, unambiguous tutorial we've been waiting for! 
ok so im doing sql in my computing a-level exams and for the coursework i used inner joins but in the exams they accept both inner joins and things like: SELECT * FROM CLASSROOM, TEACHER WHERE CLASSROOM.TeacherNumber = TEACHER.TeacherNumber so what im wondering is which is actually better inner joins or that?
If yo want a very lightweight way to get started using a SQL Database, you might look into SQLLite. https://www.sqlite.org/ SQL is a query language, that is somewhat "standardized" between a number of different database management systems such as Oracle, SQL Server, DB2, MySQL, SQLLite, Ingress, etc. I put standardized in quotes because while there are certain syntax elements that are supposed to work across all platforms (mainly the simple syntax), nearly all of the vendors have slight variations that only work with their systems. But I don't understand what you mean when you say in your EDIT that you don't care about SQL syntax. Do you mean you want to learn about databases then? SQL is a language used to select or manipulate data in databases -- though for some databases, SQL isn't the only way to do so -- but it is a very common way of doing so. 
You already have at least one working answer, but it never hurts to see more as a way of helping you learn the language. BTW, as others have said, you probably want a timestamp column to determine "latest" record, though for now I'll assume that your key is a primary key, and auto increments with new values, thus the latest value for a item_id will be that with the largest item_id. Thus: select item_id, current_inventory from table t where not exists ( select * from table t2 where t2.item_id = t1.item_id and t2.key &gt; t1.key ) I prefer this over the join to subquery with a max, because it scales better for the case where you have multiple columns you need to use when determining the row you want (imagine the case where you have 5 columns needed to determine which is the "latest" or "more desired". Of course, if using a system that lets you do a qualify row_number, use that. 
What environment will you be writing SQL in? Oracle? MySQL? SQL Server? Teradata? Something else? Use Google a lot. Test the heck out of your code. Look for ways to improve it. It is one thing to write SQL, it is another thing to write it well and in such a way that you aren't bogging down the entire system because you wrapped a DISTINCT clause around your NON UNIQUE JOIN because it was the only way you could figure out how to remove all of the CARTESIAN products you are getting. 
I do not have the answer to your question; however, the audit tables I have seen capture the entire row before it is changed. This enables the ability to roll back changes, if necessary.
Looks like you need to move the group data to an intersection table. If a product can belong to one or more groups, you should have three tables: products, groups, and product_groups.
First, understand that your trigger may not be the only trigger on the table. (Sure, in your scenario, you think it's the only one, but remember that SQL Server doesn't know that - plus, multiple other triggers may get added down the road.) Other triggers might be modifying other columns in a table. To figure out what's being updated, what you really have to do is compare the inserted table - https://msdn.microsoft.com/en-us/library/ms191300.aspx - to the existing rows that are being updated, and then compare every field that you care about. Yes, this is painful - that's why other folks are suggesting different technologies than triggers. Also, keep in mind that if additional fields are added to the table down the road, your trigger will need to handle that. That's why companies who *truly* need this functionality will often spring for an auditing tool like Idera SQL Compliance Manager, Imperva, or Guardium.
Thanks /u/devperez for the topic and thank you /u/InfiniteFinaleSFW for mentioning Notepad++. I am using various SQL servers 2000~2012 based on the client company. Now I can achieve this feature everywhere using Notepad++.
http://sql.learncodethehardway.org/book/
Question to ask: Are temp tables even needed for what it is doing? *[RDMBS Independent]* I've found the vast majority of the temp tables on the systems I've cleaned up could be replaced with either directly querying/sub-querying or building proper staging tables. They have their place, but people tend to throw them at problems when they might not necessarily be required.
Thanks for your response. I did a little digging about and have created views for the data I want. I'm still learning :) my previous roles have included pulling data from access, putting it into excel and tinkering with it. Now my role is swaying more towards development, part of which is automating the access and excel portions. 
As far as I know, there's no direct path from 2003 to 2012, so you'll probably need to go from 2003 -&gt; 2008 -&gt; 2012 or 2003 -&gt; 2005 -&gt; 2012. For each SQL Server version increment, you'll want to take a full backup of the source database(s) and restore it to the new SQL Server, using the WITH REPLACE and WITH MOVE syntax to put the files into the correct drive letter location. Don't use the copy data or copy database wizards, they are useless. [ Restore database reference.](https://msdn.microsoft.com/en-us/library/ms177429.aspx) By performing the database upgrades separately, you have a fall back if something fails.
I assume you are asking about SSRS - Sql Server Reporting Services, and not Tsql querying. Here is a nice introduction to SSRS: http://www.sqlservercentral.com/stairway/72382/
There is no MSSQL Server 2003. I would first straighten out your versioning. Are you getting a new version of SQL Server or are you just moving Windows servers? If it is only a new OS server you should be fine to only install your same version of SQL Server. After that you can [detach](https://msdn.microsoft.com/en-US/library/ms191491.aspx), copy, and [attach](https://msdn.microsoft.com/en-us/library/ms190209.aspx) the databases on the new server. If you are not on 2008R2+ it might be a good time to anaylze the upgrade of the server application as well. This is all assuming your system is running MSSQL. I'd push back on it. There are many fail points in moving servers. Your employer might want to invest in having someone experienced do the move unless they are okay with the evaluated risk of losing the data.
Clarification: It's Server 2003 running MSSQL 2008 R2. They want the server to 2012 and keep the SQL version at 2008 R2.
In SQL Server, it would be something like... SELECT ProjectKey, SUM(CASE WHEN Budget &gt; Actual THEN Budget ELSE Actual End) ProjectedTotal FROM ProjectBudget GROUP BY ProjectKey I'm not sure what the equivalent in Access would be. They might have some type of IF statement instead of a CASE statement. Edit: Forgot my group by.
Thanks, I'll take this and see if I can run with it in Access.
BTW, I just realized that I forgot my GROUP BY. I edited the original post.
learn joins
Thanks, that's exactly what I ended up using! I found it'll be easier to just break it into several subqueries rather than trying to squeeze everything into a single expression. This will work well though.
I like [w3schools.com](http://w3schools.com) because you don't have to set up anything to try it out. You can practice simple SELECT statements and get familiar with the syntax. Really all you need to get started is an install of SQL Lite (free), the AdventureWorks (AW) Database (or some other free dataset, but AW has lots of good word problems already written for it), and practice. Install a database and just start mining the data. Ask yourself questions that you are curious about, manually solve the question for a few records, then see if you can recreate those results in an excel query. In AW for example, try finding all the customers who bought an item on a given day... Now do it for a date range... Now narrow the product down... Now see if any of those customers have bought other items from AW... and just keep learning pieces of SQL. I have been doing SQL development for 3 years now and am constantly adding to new functions to my tool-belt (and even writing some custom ones myself). The skillset I had to get my first SQL job was barely more than knowledge of SELECT, JOIN types, aggregate functions (I.E. GROUP BY and HAVING). I took 1 SQL class in college (got a C+), and worked in a helpdesk where SQL queries were rarely more complex than a single record delete or a 2-table join. Don't be afraid to show up to a Junior Developer job without exact syntax knowledge or experience. Understand the concepts, be able to at least get close to the correct syntax, and show that you are excited to get better at SQL. In my experience, your resume and work experience will get you the interview, your passion, ability to learn/be teachable, and your confidence will get you the job. TL;DR: Learn by doing, practice, get excited. That's how I fell into my role and I love my job!
Please note that my background is strictly in MS SQL Server. If you are looking at a different DBMS this will not apply, but I wanted to offer what I could. &amp;nbsp; **Let me state up front that I think a Linked Server is the best possible approach to this situation.** You can override and/or inherit permissions from the user performing the logging, you can manage the logging completely within T-SQL, and you can easily direct the target auditing data differently in the future by editing the Linked Server location. However, you specifically requested options other than a Linked Server. To that end there are two that I can think of. I'm providing these only as technical alternatives to avoid a Linked Server. In both cases I think the alternative introduces more work and more risk than the Linked Server option so please consider them carefully before proceeding, especially against a production environment. &amp;nbsp; First, you can use Ad Hoc Distributed Queries and OLE Automation to generate another database connection from inside your query. This is a really neat hack that I've used to embed procedure calls inside generic select statements or views (had some really odd vendor limitations that led me to this one) by using OPENROWSET(). On a test server, you can see this in action by doing the following. sp_configure 'show advanced options', 1; RECONFIGURE; GO sp_configure 'Ad Hoc Distributed Queries', 1; RECONFIGURE; GO sp_configure 'Ole Automation Procedure', 1; GO RECONFIGURE WITH OVERRIDE GO SELECT * FROM OPENROWSET('SQLNCLI','SERVER=localhost;DATABASE=master;Trusted_Connection=yes','SET FMTONLY OFF; EXEC sp_who') WHERE [status] &lt;&gt; 'RUNNABLE' What this will do is initiate a new connection to the server (assuming localhost and master db for simplicity) and run the procedure SP_WHO. This demonstrates opening a completely new connection to a server using a specified string and running the command passed. You could in theory extend this to your own code and establish a new connection to run the auditing statement. &amp;nbsp; Second, you could use a SQL CLR function to perform the auditing through .NET. If you wrote an assembly that performed the auditing (by making an external call, or any other method of writing data) you could load this as a SQL CLR object, then call it in T-SQL code to perform the auditing. The server would use the .NET Framework to run the assembly, performing the auditing through whatever means were written (presumably a simple SQL call to the target server, though you could do more advanced auditing) and would avoid using a Linked Server to audit the data.
When I started a job based around SQL, I used sqlzoo.net and brushed up on my skills. Granted, I have a master's degree in CS so to me it was "just another language" that I had to adapt to. But traditional "coding" is not really "SQL" in the sense that most people think of, at least not from a development standpoint. SQL is this really basic thing. But you can use this basic thing as a building block. From building blocks, you can make buildings. Buildings make cities, and so on. It gets really complex in a hurry when trying to decipher a statement that is nested 3-deep and uses 20 tables. For SQL, a lot of people want to start with "select * from whatever", but the biggest topics in SQL that people ignore is that there is DDL and DML (definition language and manipulation language, respectively). I'd start there. DDL is more about constructing your containers, and DML is about filling and transforming the contents of your containers. You could say that a DBA does a lot of DDL, and an analyst does a lot of DML. I think you probably want to focus on the DML side of things because you're an analyst. You want to query data, transform data, analyze data, etc. I'm more on the DBA side of things but we have a whole team of analysts. Our method we use often is to query a bunch of data using SQL, dump the output it into Excel, and then create a pivot chart based on the data we pulled. We also analyze a lot of data directly from the queries themselves. But if summary and sub-totals and complex grouping are what we need to see, it's a lot easier to dump it into Excel and pivot what we need. As far as SQL and Excel, there's really not much to it. Excel is mostly just a giant calculator and analysis tool and SQL is just there to feed Excel some data to munch on. 
I will look into it! Thank you! $1 is really cheap to get my shoe in the door. 
Sorry that was a typo error here, but it's well typed in sql developer, I will edit it. I am really sorry for the drawback
Love the analogy between the building and SQL. That's how I feel too based on what I read so far. Quick question, since you touched on the topic of Excel. &gt; Our method we use often is to query a bunch of data using SQL, dump the output it into Excel, and then create a pivot chart based on the data we pulled. We also analyze a lot of data directly from the queries themselves. Have I been to reading this wrong? When you're working in SQL, be it DBA or DA, are you analyzing the queries it self or the data stored in the spreadsheets? If this make any sense, I picture say a big spreadsheet data set for a bunch of X customers in say Y Month and you want to find out how many of these customers bought Z Item. Would you use SQL to analyze this to pull such list of X customers from the spreadsheet with SQL and then put it into a pivot table for say Sales Manager to analyze further? I just pulled example out of thin air lol so hopefully I don't sound too novice.
We generally pull more data than we need for a formal analysis in Excel. We would rather have it than not have it. But too much data is bad so there is a balance. Maybe my "transaction" data goes back to 1995 but really I only want to look at a trend during the past year of sales. I can omit all prior years and still be in the safe zone. Maybe I've got transactions with an incomplete status. I don't want those either so I can safely omit those in my result because it seems obvious. So I would say the data is partially scrubbed when it his Excel. Kind of like buying a roast. The butcher lobs off the fur, skin, some of the fat, the organs, etc. But he leaves some fat on there. You buy the whole thing, but before you make it, you further shave off some fat. But the butcher would rather leave some fat on there depending on what you want to do with it. Maybe you intended to render most of that fat or maybe you didn't. But the butcher gave it to you in case you needed it (and probably to sell you more waste with the good stuff, but I digress!) But back to SQL, all of the important table joins and STRUCTURE of how the data is gathered, is done at that point. Excel is more of a refinement and analysis tool. It is used for forecasting and prediction, to make decisions based on business data, to recognize trends, to calculate budgets, etc. Sometimes I use multivariable regression analysis, for example. That's a lot easier to use Excel, rather than try to do something like that in SQL. 
I don't think you want to have balance_due as a subquery. Try SELECT CONCAT(getFirst(invoice_id), ' ', getLast(invoice_id)) AS 'customer_name', (invoice_total - (payment_total + credit_total)) AS 'balance_due' FROM invoices WHERE (invoice_total - (payment_total + credit_total)) &gt; 0 ORDER BY getLast(invoice_id);
 SELECT stud.name, CASE WHEN ((p1.name = 'John') and (ir1.from &gt;= to_date('01/01/2015', 'dd/mm/yyyy') and ir1.to &lt;= to_date('05/05/2015', 'dd/mm/yyyy'))) THEN to_char(ir1.idroutine) ELSE 'Not assigned' END as idroutine FROM student stud JOIN individualroutines ir1 on ir1.cistud = stud.cistud JOIN profesor p1 on p1.ciprof = ir1.ciprofassign
I'm no DBA but from my experiences as a Business Analyst, I would study the ERD (entity relationship diagram) and play around with tab_columns (all_tab_columns) to get familiar with what tables hold what fields
Excellent, you helped me a lot, thank you very much!
Khan Academy recently added an SQL course. It's an interactive console style thing with a voice recording talking about the basics. Interesting style at least. https://www.khanacademy.org/computing/computer-programming/sql
Just skip SqlLite and use MS Sql Server or Oracle, as they are mainstream products that are well known.
Or PostgreSQL - much more lightweight on the system, used more every day, and they stick to the standard better than many other systems that I've used (\*cough\* MS SQL Server \*cough\*). Plus their documentation is among the best that I've seen. Tutorials on it can be found [here](http://www.tutorialspoint.com/postgresql/) and [here](http://www.postgresqltutorial.com/). Also - learn a little Python - IPython with NumPy and Pandas goes a long way - Anaconda is a great package to get it started [2.7](http://continuum.io/downloads#27), [3.4](http://continuum.io/downloads#py34) (I prefer Python 3.4 personally). JetBrains has a great [community IDE](https://www.jetbrains.com/pycharm/) for Python as well. On that note, check out /r/python, /r/learnpython, /r/pystats, and /r/IPython 
You didn't directly mention which database servers you'll be working with, but I'm assuming MS SQL Server from your book reference and context. MS was nice enough to provide an internal tool named [sp_depends](https://msdn.microsoft.com/en-us/library/ms189487.aspx) (that they are apparently replacing in the future). If you run this on any object, you should get two resultsets that indicate what depends on that object, and what it depends on. IMO, this is more helpful than a straight ERD, especially if the org isn't good about maintaining such documentation. Not to mention, the ERD for some databases becomes so complex there is no good way to see the "big picture" anyway- at best, you're looking at components that are tied together at a macro level. I've found that just looking through the tables and schemas in the database are enough to give a basic idea of some of the organization- do you see signs of 1:1 or 1:many tables named things like "Staff", "Departments", "DepartmentStaff"? How normalized is the data? Check the stored procedures and user-defined functions, if they exist, for things that jump out as "interesting"- a stored procedure named "CalculateShippingCosts" might shed some light on how orders are stored and how the system handles things like shipping rates. Basically, nothing beats just digging in and investigating. Sometimes it'll be a struggle, but at the end of the day you'll have probably gained one more tool in your toolbelt. Oh, that reminds me- SSMS Boost and RedGate's SQL toolbelt are both pretty sweet tools. SSMS Boost is available for free with an expiring license, or you can pay for one. Redgate's toolbelt isn't cheap, but worth the cost. Redgate in particular provides an autocomplete that beats the pants off the one native to Management Studio, and I've found it very helpful when I have to play the "now what did they call that stored procedure again" game. It also provides the ability to do schema or data comparisons between databases, which can be great when you have multiple versions of things floating around out there.
Find out what is missing for spot checking: select * from Table2 where not exist (select * from Table1) You can later turn that into an insert into Table1 (not sure if there is a better way), but maybe like this: Insert into Table1 (select * from Table2 where not exist (select * from Table1) )
MSSQL.. Forgot to mention that in OP.. My bad
LEFT JOIN might be faster than EXIST SELECT a.ID FROM table1 a LEFT JOIN table2 b ON a.ID = b.ID WHERE b.ID IS NULL 
Practice these maybe. [http://i.imgur.com/TPAMJra.jpg](http://i.imgur.com/TPAMJra.jpg)
Then what to do? Im kinda new at this
Thanks, that makes more sense.
I'm not sure I've seen an answer to this part of your questions yet, so I'll chip in. The data you use will be stored on a server some where. You can install a server on your own pc and any companies you work for will Have their own sql database either stored on their own servers or hosted in 'The Cloud' some where. These Sql databases can be thought of as a large set of Excel tables if you want (they're just very strict table layouts/structures) having columns and rows. You then need a programme to query these tables. I use Microsoft SQL Server Management Studio, this program then connects to the database and you use the language of SQL to ask that database to send you the information you want in a particular format. Once you have the data, you can either use the output to feed into websites/other programmes or export it into something like Excel to analyse. The upshot being, the raw data is held on a specialised SQL database, which could be in many places.
Thank you, that worked! How ever now i get this "Fatal error: Call to a member function fetch_object() on a non-object in *WEBSITE* on line 40" And this is line 40 : "while ($row1 = $result1-&gt;fetch_object()) { echo $row1-&gt;name; }
You might have more luck asking a php subreddit since the problems are more related to your php syntax. One possibility is that there are no results and row is null, which you would have to check for. It might not be that but it seems likely. 
The term you need to search for is "database federation". Postgres' FDWs might be a cheap way to do this. 
The most efficient way to find any differences between two tables with matching column selections on any SQL database (No sort, one table scan each table, big o notation of O(n + m), it doesn't get any more efficient in general than this....). : select COL1, COL2..., count(SRC1) IN_TAB1, count(SRC2) IN_TAB2 from (select COL1, COL2, ..., 1 as SRC1, cast(null as integer) as SRC2 from TABLE1 union all select COL1, COL2, ..., cast(null as integer) as SRC1, 2 as SRC2 from TABLE2) a group by COL1, COL2, ... having count(SRC1) &lt;&gt; count(SRC2) You can change the having clause to find what you want, e.g. in TABLE2 only having count(SRC1) = 0 and count(SRC2) = 1 
At present I lack the knowledge to create stored procedures. I have no idea how to go about doing it. My experience is very much limited to using access, although I appreciate there are ways to run it server side, I don't know SQL well enough to write a single query to do the whole job, or multiple queries in SQL. This is why I am replicating what the back office system is doing. I already regret taking this on as it seems to be out of my skill set at present. It's the joins more than anything that confuse the hell out of me. Take the below for example.... I have a main table, we shall call it main for the purposes of this example, which links to 4 tables, one of these tables then joins to another. Main -&gt; table 1 -&gt; table 2 -&gt; table 3 -&gt; table 4 ---&gt; table 5 At the moment linking table 4 to 5 has me baffled. 
 SELECT '['+SCHEMA_NAME(schema_id)+'].['+name+']' AS SchemaTable FROM sys.tables Then just use alt+shift to put "insert into combined_table select * from" in front of each of the table names
This is my favorite answer, the Minus is often not used and this is a perfect scenario for it.
awesome, it's a bit sad that I've never heard of that term before.
Not for the number of rows you are using. Minus operations have quite a large overhead of full table scan / hash / sort unique of both tables. If you compare [**my solution**](http://www.reddit.com/r/SQL/comments/39y3bt/is_there_a_fast_way_to_find_missing_records_on/cs89l31) with this, mine will outperform it dramatically.
Nope, trying with drop, and with create with replacement both yield the error to the filegroup being offline.
I love it when I see questions around subjects that I've presented on:) Check out my video/slides on the FOR XML Modes: http://www.aaronbuma.com/2015/06/xml-modes-raw-auto-path/ It's part of a set I'm training coworkers on: http://www.aaronbuma.com/2015/01/free-sql-server-training/
You can't drop indexes from Read-Only or Offline filegroups: * [**Remarks**: *An index cannot be dropped if the filegroup in which it is located is offline or set to read-only.*](https://msdn.microsoft.com/en-us/library/ms176118.aspx) The only thing I can see something working would be to recreate and repopulate. P.S. - Why are there no backups that can be used for this? Edit - I'm actually curious if you are allowed to disable the index... ALTER INDEX IX_YourIndex ON schema.table DISABLE;
Because there's a break in the log chain. The backups we have could not get us back to where we are, and at that point the audit time spent verifying any manual catchup is equivalent to recreating and reloading the entire database from scratch
I think somebody might have chipped on that topic, but this goes into it more detail. It solidify the way I look into SQL, but I guess I'll have a better opinion and viewpoint of the power of SQL as I progress in this journey. Question on top of your answer, are there resources to understanding storing databases better? I know there's an infinite number of databases out there and would like to get a better handle on it. In the past I have worked at an intern to a developer company for FileMaker and worked a little with SQL in that I would run a query (already written) on FileMaker to understand how it worked. This was way back in college so it's a little bit rusty. Maybe I should touch up on the concept of databases in general before I go into SQL? Thank you! 
Didn't think so, but was worth a shot. Also, are you able to take a FULL COPY-ONLY backup of the currently running database? 
Great, let us know if you have any questions/what you think! 
&gt; SELECT 'insert into combined_table select * from [' + SCHEMA_NAME(schema_id) + '].[' + NAME + ']' AS SchemaTable FROM sys.tables 
Your data and input are bullshit. You need a proper mappings table to link all the possible names. The below has flaws, but should work assuming only the Full Name has a space character and there is always at least one Full Name record in the dataset for each user. http://sqlfiddle.com/#!6/89a7fb/8 SELECT T.respondedby, count(*) tickets FROM Tasks JOIN ( Select A.respondedby , B.respondedby alias1, C.respondedby alias2 From Tasks A Join Tasks B ON B.respondedby = left(A.respondedby,1) + Right(A.respondedby, len(A.respondedby) - charindex(' ', A.respondedby) ) Join Tasks C ON C.respondedby = left(A.respondedby, charindex(' ',A.respondedby)-1 ) + Left(Right(A.respondedby, len(A.respondedby) - charindex(' ',A.respondedby) ), 1) Where A.respondedby LIKE '% %' ) T ON Tasks.respondedby = T.respondedby OR Tasks.respondedby = T.alias1 OR Tasks.respondedby = T.alias2 GROUP BY T.respondedby
Hi, thanks for the input. Although, instead of just calling it bullshit, maybe you could educate me as to why you think that. I'm pretty new to this stuff and am self-taught, so real feedback is always appreciated.
Fancy!
Yes, we can still do partial backups and restores ignoring the offline filegroup, however it is still in the metadata and therefore still there. If you can provide a script example of what you're talking about, i'll verify, but unless i'm missing something, that sounds like what you're suggesting. And restoring the damaged filegroup doesnt' work because it still restores primary when it does a partial restore and therefore needs a log chain to catch up to current, which doesn't exist in an unbroken form
Think of a database as a collection of spreadsheets (called tables) that relate to one another based on certain criteria. SQL is just the language we use to tell the spreadsheets how I want them to relate. Data mining is a lot of abstract thinking because you rarely have a document that tells you how all the different tables relate... you kind of need to just be able to look at two tables, make an assumption, and then test it out. Also, if you want to do the data mining, you most likely will not be in charge of database installation/update management, that is a whole different world. My first SQL job was about $25/hr, but I worked with other consultants that had 10 years of experience, and they billed $75+/hr. It is a great field and in high demand if you can be good at it! Here is something from another post I wrote that's kind of ELI5: So imagine you have two excel spreadsheets. One named TEACHER, and one named CLASSROOM... TEACHER has 3 columns: TeacherNumber, Name, Grade CLASSROOM has 3 columns: ClassroomNumber, Building ,TeacherNumber If I asked you how to tell me which Classroom has which teacher, you need to figure out how to link these two spreadsheets with a common field. Upon inspection, you see that there is are columns called TeacherNumber in both spreadsheets. That is because the TeacherNumber in the CLASSROOM spreadsheet is a direct reference to the TeacherNumber in TEACHER. This is the essence of a INNER JOIN. To do that in SQL it would look like this. SELECT * FROM CLASSROOM INNER JOIN TEACHER ON CLASSROOM.TeacherNumber = TEACHER.TeacherNumber This will return all cells from both spreadsheets, each row from TEACHER will be appended to the rows in CLASSROOM where the TeacherNumbers are the same. This is a quick and dirty ELI5, I see plenty of other good resources here so I wont go into any more depth, but it helps if you think of each table as a spreadsheet, and that those spreadsheets are connected by one (or more) common columns. EDIT: Formatting 
Use PowerShell or SSIS to read the file, convert it and load and schedule an agent job to run the package or script
Well then I don't know what to say; wasn't entirely sure what you did and did not have; worked very hard with my SAN admin to make sure I never ended up in this situation lol. This is completely unexplored terrain for me. Sounds like your RPO might be back to your last complete backup. Rolling forward and performing some manual work. *[Pretty sure it won't work] Have you tried adding a filegroup? Or try restoring the latest complete back backups to build the filegroups then restoring from the COPY-ONLY?*
Sounds like it would be a good fit for your needs. Keep in mind : Requires full recovery logging mode, so be aware of the overheads that brings with it. In sync mode, data writes aren't committed until the mirror commits them. This both means a slow mirror cab/will slow your main system, so keep hardware similar, and also increases it's online importance. 
How much data is considered too much for Excel to handle (I guess it depends on the pc performing the job ofc.). 1 million cells? (50k rows x 20 columns)? More/less?
 SELECT po_headers.po_seqno, po_headers.po_number, po_award, pocl_vtyv_value, CASE WHEN po_subtype = 'BO' THEN po_nottoexceed_value ELSE pcom_value_thisrev END AS avalue, pcom_negotiated_saving, pcom_revno FROM po_classes, po_headers, project_commitments WHERE pocl_po_seqno = po_headers.po_seqno AND pcom_dataseqno = po_headers.po_seqno AND pocl_vtyv_value = 'OBE' AND po_award &gt;= Add_months (Trunc (sysdate, 'MM'), -12) AND po_award &lt; Add_months (Trunc (sysdate, 'MM'), 1) AND pcom_revno = 0 AND po_subtype &lt;&gt; 'REL' Advice : 1. Use aliases on your tables and use that for their related columns. e.g. po_classes pc 2. Learn ANSI 92 join syntax, you are using ANSI 89 join syntax and it is outdated and limited. 3. Why do you need to group, you aren't applying an aggregation in your results?
Case is better as it's standard SQL, decode is Oracle specific.
This is in no way to mean to be condescending, but you cannot reference an alias in the group by clause. You have to spell the whole thing out. Isn't that dumb? The alias is created in the select, but the group by is actually executed before the select. The select is 2nd-to-last to execute, and the group-by doesn't have any information about the alias at that point. 
Aliases are allowed in the GROUP BY clause in PostgreSQL and MySQL, but not MSSQL or (apparently) Oracle. Weird. 
True, but what if you're old and set in your ways and unwilling to type out all that new wordy ANSI 92 stuff? GET OFF MY LAWN! *Dos 6.22 best OS of all time*
DR DOS 6.0 was better
Check constraints aren't supported in MySQL (yes, seriously). The common workaround is to use a `BEFORE INSERT` or `BEFORE UPDATE` trigger, described nicely [here](http://blog.christosoft.de/2012/08/mysql-check-constraint/).
Man..why do people use this?
I would use CHARINDEX to identify the first occurrence of the '/' and CHARINDEX2 to identify the second occurrence (or Nth occurrence if you so wish). CHARINDEX2 is not a built-in function, but it can be created using the simple solution here at [Stack Overflow](http://stackoverflow.com/questions/9163645/sql-text-before-then-nth-match). You can couple these functions with SUBSTRING() or LEFT() and RIGHT() to trim the string. For example: SELECT LEFT(RIGHT([YourField], LEN([YourField]) - CHARINDEX('\', [YourField])), CHARINDEX2('\', [YourField], 2) - 1) 
Huh ok, that's nuts, but thanks for letting me know. Since my blacklist was small I ended up just throwing the items in an array and checking them against the article name before storing, but if it gets more complex in the future I'll definitely look into triggers. 
since everyone else is giving you solutions without asking you to identify your platform (see sidebar), i guess i will too for mysql -- SUBSTRING_INDEX(SUBSTRING_INDEX(daColumn,'/',2),'/',-1)
The ELSE will capture the NULL scenario. In this situation, ELSE = Everything other than a TRUE predicate = a FALSE or NULL predicate.
I found another table to use in my query that resolved it! From another comment: &gt; Ah, I knew that seemed like a funky way to do it. You actually got me thinking about my column choice. TrackIT automatically generates the RespondedBy column, but only technicians close tickets, so CLSDBY must have some consistency, right? Well, it does. Full names only, and no EmailMonitor! The reason I unincluded EmailMonitor was because it came up at the top of my technician list with ~300 tickets, and I thought that was weird. That's because trackit uses EmailMonitor as its "technician" when tickets come in via email.
Yep, consistency checking blocks it. Right now i'm diving down through the DAC and diking it out of the system tables by hand just as an academic exercise. So far anything purely in that filegroup i've been able to remove with minimal errors, the issue is the cross-contaminated ones with indexes on the group and tables elsewhere. We'll see what dbcc thinks of the mess when i'm done.
Figured as much. Best of luck to you.
I added a sample to my post, SQL 2012 is the minimum version it supports. As for suggestions, try and avoid cursors at all costs. They generally don't scale and can cause you all kinds of problems if you aren't careful. There is a few specific uses where they are actually acceptable, and most of those can be swapped out for dynamic SQL. If you want to do it without the above, you can use CROSS APPLY: SELECT t1.idInvoice, t1.idCustomer, t1.value, t1.name, t1.dateProcessed, t1.etc, runTotal.runningTotal FROM yourTable AS t1 CROSS APPLY (SELECT SUM(t2.value) AS runningTotal FROM yourTable AS t2 WHERE dateProcessed BETWEEN filterStartTime AND filterEndTime ) AS runTotal WHERE idCustomer = 5 ORDER BY t1.idInvoice -- Note: This is drastically slower than my original example.
This would improve performance, I'm guessing. Thanks for the tip! Since the view has all the columns I need, if I were to not use it, how would I create a table that has the columns I need?
Awesome. This is super helpful to know. DBA is cool!
You could use my example as the definition for the cursor then just SELECT the cursor. :) 
Fun fact: IIF is internally converted to CASE during execution. [Source with additional info on the topic.](http://www.codeproject.com/Articles/610175/SQL-Servers-built-in-scalar-Logical-Functions-IIF)
The commas are there in the code... I've tried using Distinct right after Select, but it seems to duplicate anyway. What I need is Distinct on Column1 and the rest wouldn't matter. However it duplicates the Column1 because Column2 happens to be different in both of those rows. 
just as a side comment, if you already know the value of column3 (because you're feeding it into the query as a filter condition), there isn't much point in returning it on every row via the SELECT clause 
&gt;LIKE '%\^%' ESCAPE '^' This means that the caret in %\^% escaped the second %. This would match any string actually ending with a percentage sign. To do what you want with using the caret AS the escaping character and escaping the caret character you would do %\^\^%. Most systems and languages by default use a backslash as the [escaping character](https://en.wikipedia.org/wiki/Escape_character). SQL makes you define it explicitly. 
That's process intensive. You gotta run all the cuts against all the priceprice. On big tables you'll grind a server to it's knees and someone gonna get pissy.
so do a group by column1 and max() the other values, instead of 'distinct' then? 
Well in my case, with a more heavier statement it was faster than a CASE or a Where with a cross join. Also there's only one CutoffPrice. So it might be why it was faster.
Cartesian join = 'cross join' in the join clause syntax. A full outer join is not the same as the cartesian join (it's a set of conditions over a union of a cartesian join and 2 minus operations), so you will could different result sets for a cartesian (your first query) versus full outer join.
I used a full outer so I didn't have to worry about the direction. I likely use Cartesian pretty loosely (maybe I should say "Cartesian product" instead of join?), but it's the join condition that produces the Cartesian (the 0=0 condition). I guess I'm starting to answer my own question, but producing a Cartesian using inner joins behaves the same way my first Cartesian works. select * from ( select 1 col1 from dual union select 2 from dual ) a join ( select 3 col2 from dual union select 4 from dual ) b on 'x'='x'; If one of the data sets is empty, no Cartesian is produced, which is what my first query also returned. So I'm guessing that Oracle-style joins are the same as an inner join (which should not surprise anyone) and forcing a Cartesian product will also result in zeros if one data set is empty (which is the part that I did not expect).
Badass. Given the upgrade strategy of most folks, I'll look forward to implementing it with clients in like four years!
That is correct, unless there is an AD group on that box that you could be temporarily added to that has SA or Security Admin permissions. 
I'm not an Oracle expert, but why joins at all? Why not unions??
&gt; FETCH runbal_cursor INTO @InvDate, @Trx_Ctrl_Num, @InvAmt, @RunTotal; Each time you fetch the next record, you are also resetting @RunTotal to 0. You are updating the RunTotal column for the row you are working with, but the rest of the rows are still 0. Take @RunTotal out of the declare/fetch statements in the cursor.
Yeah one already loaded with a sample data set
With multiple rows, of course. You take a dual row and union with another dual row.
Why not just generate your own data?
So it looks like this would make data audit trail functionality extremely easy to implement in any application?
Sorry, I'm still not understanding. What would I do with two unioned rows from dual? The row I want to produce still has to mimic the new table.
I think the different results of your queries are correct. And not for a techical reason, but because the queries are simply not equivalent. select * from (select 1 from dual) x, new_table_name q; The cartesian product of an empty table and another table should be empty. Because the cartesian product combines all rows of the first table with all rows of the second table. If one table has no rows, there are no possible combinations. select * from new_table_name q full outer join (select 1 from dual) x on 0=0; In contrast, a full outer join is expected to contain all rows that fulfilled the join condition + all rows that never fullfiled the join condition with added null values. Therefore, it does not matter if one table is empty. As long as one table contains at least one row, the outer join will also contain at least one row. The condition 0=0 doesn't even matter in this case, 0=1 also works.
Wait, you're skipping 2018?
I'm doing a project now where the (DBA) analyst reported that the replication subscriber server to be updated had reported that there were 5 push subs to be dealt with during the upgrade. He failed to mention that three of the publishers were at remote sites owned by other folks and we had no access to them to stop their distribution agents, so we would need to coordinate with them. Or, relevant to your note, that two of the sites are running 2005, and the subscriber upgrade is supposed to be to 2014. (You can't replicate between two servers more than 2 versions apart.) Awkward meetings with client ensued. On this note, I wonder what happens if you upgrade a server without stopping distribution agents pushing to it? I'd like to think it would work just fine when it started up, but there's not a hope in hell I'm actually trying it. 
Dude.... SQL 7, I have no words. 
I had several words, most of which show up in sports thread word clouds.
Thanks, it gives me a reason to continue with research.
Ah Ok. One Cut makes a massive performance difference. 
ANSI 89 Syntax : select * from (select 1 from dual) x, new_table_name q ANSI 92 (and above) equivalent : select * from (select 1 from dual) x cross join new_table_name q 
Wow, I've dabbled i nsql for quite a while (self-taught) and never see this mentionned on any tutorial site. That's great
Afaik cursors has worse performance than a while loop. 
you know correctly.
Ah healthcare. We inherited a FoxPro database a while ago that was responsible for keeping track of the medical benefits of about a million people. It had been running along unmaintained for about a decade at that point as that was when last person who knew anything about it retired.
Well, since SQL is a relational database, you could create a table to track values that are valid for Col2 in both tables, set it as primary key, and then change both tables to refer to the new table in a FK relationship.
Foreign Keys are not restricted to Primary Keys, they can also link to Alternate Keys (any candidate key which is not the PK). So if Table1.Col2 is unique, you could create a FK constraint from Table2.Col2 to it. Failing that, you could use triggers to perform the validation and raise an exception where appropriate.
No you dont want that. This is a view, treat it as such.
If you simply skip the INSERT statement, I think you'll get what you want. You figured it out already - going the extra mile to put it in a table was where it went haywire.
Wish they had one of these for spatial joins, but there would be a lot more to cover.
Yes you can always flip it and do a left join, there are situations where you would do a right join like if you are doing something complex and or nesting different type joins. 
Good point. I like that diagram as well. 
I am using Microsoft SQL server 2012 currently.
This is a great diagram. Good to have around for beginners. Thanks for posting!
Assuming you're using 11gR2 or above, [LISTAGG will do this](http://www.oracle-developer.net/display.php?id=515)
Disregard figured it out. Thanks!
Thanks! It might be important to consider that I made this as part of a 6-hr course of introductory SQL, and by this point I had already had the students work with several different tables that intuitively needed to be JOINed, so I guess you're right about the effectiveness depending on how comfortable people feel with tables and relations. 
this helps but could someone put the (+) notation syntax in there as well? that's what gets me
this is great, could you also include the (+) syntax?
I have this printed out and taped to my desk at work. I really need to get it laminated.
The (+) goes on the side with the optional match. For example, the following 2 queries are equivalent: Select * from A Left join B On A.id = B.id Select * from A, B Where A.id = B.id (+) You don't need the (+) when you are doing joins in the format of the first query, but they are needed for the second type to create an outer join. Inner joins don't need the (+) at all. The inner join would just look like: Select * from A, B Where A.id = B.id
SQL bugs hate him!
Good luck!
Your exists is wrong. It's not correlated. Look at the example in the post http://www.techonthenet.com/sql/exists.php Pay attention to the where clause in the subquery SELECT * FROM suppliers WHERE EXISTS (SELECT * FROM orders WHERE suppliers.supplier_id = orders.supplier_id);
 DECLARE @ServerName VARCHAR(128) = @@SERVERNAME; DECLARE @DatabaseName VARCHAR(128) = 'MyDatabase'; DECLARE @CRLF CHAR(2) = CHAR(13) + CHAR(10); DECLARE @DynSQL VARCHAR(8000) = '' + 'SET NOCOUNT ON;' + @CRLF + 'SELECT' + @CRLF + ' Col1,' + @CRLF + ' Col2,' + @CRLF + ' Col3' + @CRLF + 'FROM' + @CRLF + ' dbo.TableA AS a;' ; SET @DynSQL = REPLACE(@DynSQL, '"', '""'); SET @DynSQL = 'SQLCMD -S ' + @ServerName + ' -d ' + @DatabaseName + '-Q "' + @DynSQL + '" -s "," -o "D:\Test' + REPLACE(REPLACE(REPLACE(REPLACE(CONVERT(VARCHAR(20), GETDATE(), 126), '-', ''), 'T', ''), ':', ''), '.', '') + '.csv"' + ' -h -1 -W'; EXEC xp_cmdshell @DynSQL Here is what I use to generate query to files quickly. See if it works out. The -o command of sqlcmd specifies output to file. The script uses xp_cmdshell to connect back to the database using sqlcmd. You may have to enable xp_cmdshell for this to work from SQL Server itself. Or you can just use powershell if you are more comfortable there. They are similar solutions.
Im not familiar with Powershell - would I be able to write to 30 different files , each with their own iterative name? Seems like it.. So you mean to basically write a powershell query and it runs through SQL?
when you steal a comic from a very well known site, you must give it proper attribution when you don't attribute, you look like a lame tool
More along the lines of: $yourArray = @(5, 7, 28, 41, 75, 94) foreach ($num in $yourArray) { Invoke-Sqlcmd -Query "SELECT * FROM theTable WHERE idClient = `$(num);" -Variable $num | Out-File -filePath "C:\output\reportForClientId_" + $num + ".txt" } Databases should be queried, then what queried it writing the file unless it is directly related to the SQL Server itself (backup/job reports etc). Alternatively, you should be [utilizing SSRS](https://msdn.microsoft.com/en-us/library/ms159106.aspx). *Edit - EXEC xp_cmdshell* is antiquated and should be avoided if at all possible. SQL Server should not have permission to interact with the OS in that manner and it's a feature best left disabled. 
&gt; They are similar solutions. No they aren't; xp_cmdshell gives an allowed SQL user nearly unfettered access to the local OS. PowerShell querying only allows them the existing permissions. 
Thanks for noticing the part where they are not similar! As a security rule this would not be ideal to be left open in a production environment. They both use sqlcmd to run the queries in a scripted loop. I call that similar.
 SELECT COUNT(*) AS "# Enclosures" , SUM(slots) AS "Free slots" FROM ( SELECT encl_id , 16 - COUNT(*) AS slots FROM inventory_slots WHERE encl_id IN ( SELECT id FROM inventory_enclosures WHERE vendor = 'Vend_X' AND 10g = '1' ) GROUP BY encl_id ) AS t 
Thank you so much for your help! I managed to do it correctly :) I will bookmark that webpage you linked, it is wonderful. 
Thank you so much /u/r3pr0b8, that did it!
Ill go ahead and throw it at SQL server, I appreciate your response though. Yeah, the folder with the master model and msdb. The file permissions were all full control for both the domain group and local group it was in. I even checked the special permissions.
Oh snap, I didn't consider that. The GPO may of altered the UAC setting for the service account ::smacks self:: when I get to work Ill give it a shot. Your the bestest Thriven!
&gt; EXEC xp_cmdshell is antiquated and should be avoided if at all possible. [Counterpoint](http://www.midnightdba.com/DBARant/?p=1204)
`export-csv -notypeinformation` might produce more usable results for the OP than `out-file`. But that makes a few assumptions about the required output format.
I wasn't talking about sqlcmd / Invoke-Sqlcmd; instead about the use of xp_cmdshell. OS shell calls **should never be made from SQL Server Instance unless there is absolutely no other choice**. xp_cmdshell is a superannuated feature; it's disabled it out-of-box and should remain that way. I was suggesting utilizing PowerShell externally to query the information because it supports programming like functionality for handling dynamic outputs and various other handle functionality. *Edit - I would add that you can lock some functionality down, but it usually involves and equal or greater amount of effort than just performing it externally.* 
Depending on the sizes of your tables, selecting the entire table isn't a great idea. Limit the number of rows returned with TOP (t-sql) or ROWNUM predicate (oracle).
That just depends on what OP needs. :) Sometimes XML works great if you have a matching XSLT transform to apply to it. 
Great point, thanks for the feedback! 
Is there a reason that you're saving into 30 different files, or could you potentially run all of this into one single file, with an additional column containing that variable value?
Not sure what flavor of SQL you are using, but it sounds like you want to use Pivot/Unpivot. Your example isn't too clear so maybe I'm wrong. https://technet.microsoft.com/en-us/library/ms177410%28v=sql.105%29.aspx 
You could always do the bulk insert into a staging table and store the true/false values as VARCHAR. When transferring it into the final table just use a case statement CASE WHEN [Blah] = 'True' THEN CAST(1 AS BIT) ELSE CAST(0 AS BIT) END or you could make it a little safer like CASE WHEN [Blah] = 'True' THEN CAST(1 AS BIT) WHEN [Blah] = 'False' THEN CAST(0 AS BIT) ELSE NULL END 
basically i'm attempting to combine column 1 &amp; 2 into one column. Each value will have its own row 
I am aware of a few Oracle instances where Microsoft SSIS is being used for ETL of data into the Oracle data warehouse. Many people find SSIS to be the most robust and powerful option even in oracle centric environments. 
First of all, thank you very much!! You did way more than enough!! SELECT * FROM student s LEFT JOIN IndividualRoutine ir on s.IDstud = ir.IDstud LEFT OUTER JOIN Exerciseinroutine er on er.IDroutine = ir.IDroutine GROUP BY s.IDstud HAVING SUM(numberrepitition)&gt;=(SELECT AVG(numberrepitition) FROM Exerciseinroutine JOIN individualroutine ir2 on ir2.IDroutine = Exerciseinroutine.IDroutine); I just modified the AVG(numberrepetition) for SUM(numberrepetition) and used a join in the AVG part to just get the AVG of the individualroutines, and it is working wonderful! I just need to add the filter that the routine has to be active now a day and its over. Thank you for your time, you are a big help! I am really new at this and this is helping me alot to understand more of SQL 
15 easy steps to create your first sql server report with infographics and video tutorial. http://www.technicgang.com/sql-server-reporting-services-tutorial-15-steps-report/
Afraid it has to be 30 separate files, client is pretty fussy, will not change anything on their side nor will they allow me to see how their side works , all I know is the format they want
Are you getting any authentication failure messages in SQL server's errrorlog
got it
for some reason I keep having trouble with union alls 
Take a look at your organization's network diagram - are they all on the same subnet? Look at event viewer on both client and server. See any errors there?
Have you tried '?' instead of '%s'
When I have queries that are going to start interacting with outside processes or the file system, I throw the query into a sproc and use C# + ADO.NET. All the file handling logic, error handling (and alerting when things go awry) are easy to do in C#. Once you have an executable, throw that into SQL Server Agent (or Windows Task Scheduler) and you're good!
Execution plans thanks!
Haha, no. A different game.
First time posting here and have been drinking so ignore me if I don't give enough weight in my answer. You should look into setting up a full text index on both columns initially and go from there. 
https://msdn.microsoft.com/en-ca/library/ms187384.aspx also, try removing known words and only parse the first 3 words
have you tried soundex? https://msdn.microsoft.com/en-us/library/ms187384(v=sql.90).aspx
&gt; if I was sober I would be more assistance. up(hic)vote 
Nope, but I am reading about it now. Appreciated.
Check to make sure the connecting users sufficient rights to the database.
Thinking this over, but not trying to overthink this......for now, drawing a blank on what the SQL would even look like, try EXCLUDING the professors that have count =0 in the routine OR in StudentRoutine. Because that would get you the professors who have at least one routine assigned. Thought; if the professor doesn't have a routine, it certainly won't have one assigned. in an OR statement, MAYBE...where either -0- in routine or in routineassigned
Cool, no problem :)
https://www.reddit.com/r/SQL/comments/3ab9la/when_someone_asks_me_an_sql_question_involving/
 select name from professor p inner join routine r on p.IDprof=r.IDroutine inner join (Select idroutine, count(IDstud) from studentRoutine group by idroutine having count(IDstud)=(select count(*) from student)) IR on r.IDroutine=IR.idroutine This should work. 
There are 2 ways that I can think of to do this right now. The first one which I prefer - and I will let you figure it out in greater detail - involves getting the COUNT of the number of students assigned to each individual routine. You then compare that to the total number of students and only select those routines that match and then just join to professors. The second method involves a Cartesian join to get all of the possible combinations between students and routines and then exclusion of all routines which have students that are not assigned to them. In real situations I would be extremely careful about using Cartesian joins but for didactic reasons it occasionally works. SELECT DISTINCT p.name FROM professor p inner join routine r1 ON p.idprof = r1.idprofdesignedit WHERE NOT EXISTS (SELECT 1 FROM (SELECT r.idroutine, s.idstud FROM routine r left outer join student s ON 1 = 1) r left join studentroutine sr ON r.idroutine = sr.idroutine AND r.idstud = sr.idstud WHERE sr.idroutine IS NULL AND r1.idroutine = r.idroutine) 
Thank you, will bookmark that! :)
It works excellent, thank you very much, I did not event think about joining a subquerry with the routines, that will help me alot in other consults too! thank you!
It is a good way to do it, i like it, i will check this out slowly to understand it, thank you!!
And the connection string is....?
I wouldn't recommend using Soundex as someone has suggested, it is very rudimentary and won't be very useful for your needs. [**Here's a reply**](http://www.reddit.com/r/SQL/comments/32l1w0/ms_sql_compare_text_strings_for_similarity/cqc6l11) I made to a similar question with much better algorithms.
This was an incredibly fun one to work on. I hope this approach helps, I chewed on it for most of the weekend before coming up with the solution below. Good luck! Upon initially reading your summary my thoughts were in line with the other suggestions here. SOUNDEX or CONTAINS using a full text index seemed like the optimal method to compare strings. However when I read more closely I noted that you’re actually looking for *individual word matches* within the string. The key here is that the string is not a complete object to match patterns, it is actually a *sub-set of records* (words) that need to be individually matched. I thought about the best way to break down the full text of the description field into individual words, stage them in a temporary table, then compare the matched words based upon aggregate count of ID to obtain your requirement for 4 individual word matches. Writing a function to do the parsing was an option but I felt that XML using CROSS APPLY would probably perform better given the set of records you were working with. My XPATH and XML handling is a bit weak but after some trial and error I came up with this. -- CREATE A VARAIBLE THAT WILL BE USED FOR XML PARSING OF TABLE FIELDS. DECLARE @xml XML /******************************************************************************** STEP 1: BUILD TABLES TO SUPPORT PARSING OF DESCRIPTION. ********************************************************************************/ -- BUILD A TEMP TABLE THAT WILL CONTAIN WORDS FOUND BETWEEN TABLES. CREATE TABLE #DESCRIPTION_WORDS ( [ident] INT IDENTITY(1,1) -- IDENT FOR INSERT. ,[id] INT -- PK FROM THE ORIGINAL TABLE. ,[tbl] VARCHAR(6) -- ORIGINAL TABLE NAME. ,[word] VARCHAR(30) -- WORD FOUND IN DESCRIPTION. ) /******************************************************************************** STEP 2: PARSE THE "SCORE" TABLE TO GET WORDS WITHIN DESCRIPTION FIELD. ********************************************************************************/ -- BUILD A STATEMENT TO PARSE THE TABLE AS XML TO BREAK DOWN WORDS IN DESCR COLUMN. SELECT @xml = ( SELECT -- GET THE ID FOR TYING BACK ORIGINAL RECORD. [id] = [id] -- GET THE DESCR BY PARSING A FIELD FOR SPACES. ,[words] = CONVERT( XML ,'&lt;w&gt;' + REPLACE( REPLACE( REPLACE( CONVERT(VARCHAR(MAX) ,(SELECT [t] = [descr] FOR XML PATH('')) ) ,' ' , '&lt;/w&gt;&lt;w&gt;') ,'&lt;t&gt;' , '') ,'&lt;/t&gt;' , '') + '&lt;/w&gt;' ) FROM -- ALIAS THE TABLE FOR XML REFERENCE SCORE RECORD WHERE -- RESTRICT TO FIELDS THAT CONTAIN A DESCRIPTION. [descr] IS NOT NULL AND NOT [descr] = '' FOR -- CONVERT THE ENTIRE SET TO AN XML FIELD. XML AUTO ) -- NOW WE CAN USE CROSS APPLY TO DERIVE THE ID AND WORD LIST PER RECORD, AND INSERT INTO OUR TABLE. INSERT INTO #DESCRIPTION_WORDS ( [id] ,[tbl] ,[word] ) SELECT [id] = X.n.value('@id', 'INT') ,[tbl] = 'SCORE' ,[word] = W.t.value('.', 'VARCHAR(MAX)') FROM @xml.nodes('/RECORD') AS X(n) CROSS APPLY X.n.nodes('./words/w') AS W(t) /******************************************************************************** STEP 3: PARSE THE "VENDOR" TABLE TO GET WORDS WITHIN DESCRIPTION FIELD. ********************************************************************************/ -- BUILD A STATEMENT TO PARSE THE TABLE AS XML TO BREAK DOWN WORDS IN DESCR COLUMN. SELECT @xml = ( SELECT -- GET THE ID FOR TYING BACK ORIGINAL RECORD. [id] = [id] -- GET THE DESCR BY PARSING A FIELD FOR SPACES. ,[words] = CONVERT( XML ,'&lt;w&gt;' + REPLACE( REPLACE( REPLACE( CONVERT(VARCHAR(MAX) ,(SELECT [t] = [descr] FOR XML PATH('')) ) ,' ' , '&lt;/w&gt;&lt;w&gt;') ,'&lt;t&gt;' , '') ,'&lt;/t&gt;' , '') + '&lt;/w&gt;' ) FROM -- ALIAS THE TABLE FOR XML REFERENCE VENDOR RECORD WHERE -- RESTRICT TO FIELDS THAT CONTAIN A DESCRIPTION. [descr] IS NOT NULL AND NOT [descr] = '' FOR -- CONVERT THE ENTIRE SET TO AN XML FIELD. XML AUTO ) -- NOW WE CAN USE CROSS APPLY TO DERIVE THE ID AND WORD LIST PER RECORD, AND INSERT INTO OUR TABLE. INSERT INTO #DESCRIPTION_WORDS ( [id] ,[tbl] ,[word] ) SELECT [id] = X.n.value('@id', 'INT') ,[tbl] = 'VENDOR' ,[word] = W.t.value('.', 'VARCHAR(MAX)') FROM @xml.nodes('/RECORD') AS X(n) CROSS APPLY X.n.nodes('./words/w') AS W(t) /******************************************************************************** STEP 4: JOIN THE RECORDS FOR MATCHING WORDS, REQUIRING 4 MATCHES. ********************************************************************************/ SELECT [SCORE_ID] = SCORE.[id] ,[VENDOR_ID] = VENDOR.[id] ,[MATCHES] = COUNT(1) FROM #DESCRIPTION_WORDS SCORE INNER JOIN #DESCRIPTION_WORDS VENDOR ON SCORE.[word] = VENDOR.[word] WHERE SCORE.[tbl] = 'SCORE' AND VENDOR.[tbl] = 'VENDOR' GROUP BY SCORE.[id] ,VENDOR.[id] HAVING COUNT(1) &gt;= 4 It’s long, but fairly simple. 1. Create an XML variable and a temporary table to contain the word sets within the source tables. 2. Build an XML interpretation of the records, including nodes for the description and words found for each record 3. Use CROSS APPLY to break this back out to a set of rows which is inserted into the temporary table. 4. Group matches by word then apply a minimum count so that you see ID values matched between tables having at least that many words. You may need to review the temporary table definition (if you have words bigger than 30 characters), and/or apply additional restricting to count out special characters (a “word” for me is anything after a space, but this includes text separators like “ &amp; ” or “ / ” that sometimes show up in descriptions). I already named the tables to match your SOURCE and VENDOR descriptions, but I’m just declaring a simple “ID” for the PK, so make sure that matches your table field properly. Otherwise, it should largely be runnable as is against your current system to test. 
Thank you for all the good responses, I have some research to do!
I am thoroughly blown away. I am in the process of completely wrapping my brain around this and will hopefully give it a shot in the next day or so. I will let you know how it works out. Thank you so much. It sounds like you had fun. I really hope you did.
Alt+shift. It's a real fun command! I learned it even stays after you click somewhere else so you can keep adding commas...or whatever you want after. 
Very interesting reads in your prior reply. This info will definitely come in handy. Thank you for the links!
If you want to see it in action on a much smaller record set (which I recommend until you are fully comfortable with it), then set a TOP 1000 or so on both XML statements. Doing so will let you see it in action without the massive number of possible calculations, and check to make sure the data type sizes work well on your dataset. Something like this... -- BUILD A STATEMENT TO PARSE THE TABLE AS XML TO BREAK DOWN WORDS IN DESCR COLUMN. SELECT @xml = ( SELECT TOP 1000 -- GET THE ID FOR TYING BACK ORIGINAL RECORD. [id] = [id] -- GET THE DESCR BY PARSING A FIELD FOR SPACES. ,[words] = CONVERT( XML ,'&lt;w&gt;' + REPLACE( REPLACE( REPLACE( CONVERT(VARCHAR(MAX) ,(SELECT [t] = [descr] FOR XML PATH('')) ) ,' ' , '&lt;/w&gt;&lt;w&gt;') ,'&lt;t&gt;' , '') ,'&lt;/t&gt;' , '') + '&lt;/w&gt;' ) FROM -- ALIAS THE TABLE FOR XML REFERENCE SCORE RECORD WHERE -- RESTRICT TO FIELDS THAT CONTAIN A DESCRIPTION. [descr] IS NOT NULL AND NOT [descr] = '' FOR -- CONVERT THE ENTIRE SET TO AN XML FIELD. XML AUTO ) In place of the original assignments, will keep it smaller and lighter for testing. Thanks!
NP++ will do this as well.
DECLARE @A VARCHAR(MAX) SELECT @A = FIELD + ',' FROM TABLE PRINT @A
That would have been more prudent. The amazing thing was that it ran well before I thought it would. After I made the changes I needed to (the id field name change for the PK, and my source tables are actually called IM_ITEM and JB_ITEM) there were a couple of errors (as I expected there would be - just because I am sloppy). Turns out, that only things I had to do were change the table definition of [tbl] in the temp table to varchar(7) and the data type of the id field to varchar(30) in both the definition and the two select statements at the end of steps 2 and 3. I totally expected it to error out again, BUT IT RAN!!! I was so surprised that I just watched it thinking, I should really have chosen a smaller data set, but once I decided to do something about it, I was already at 10 minutes, and I couldn't bring myself to stop it.
Other ways of quoting, OR/ANDing, comma-ing, or whatever else you need to do to your data: 1) Notepad++ has the alt-shift capability for mass-insert but also of note is the macro recording. Record your iterative action, for example quote, end, quote, comma, down arrow, home. Then play the macro until the end of the file. 2) You can use excel to re-build your column of data by inserting a quote, the contents of your cell, a quote, a comma, etc. 3) DIY script solution that uses regex or some other method like awk or sed. I like choice number 1.
[Image](http://imgs.xkcd.com/comics/is_it_worth_the_time.png) **Title:** Is It Worth the Time? **Title-text:** Don't forget the time you spend finding the chart to look up what you save. And the time spent reading this reminder about the time spent. And the time trying to figure out if either of those actually make sense. Remember, every second counts toward your life total, including these right now. [Comic Explanation](http://www.explainxkcd.com/wiki/index.php/1205#Explanation) **Stats:** This comic has been referenced 231 times, representing 0.3347% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_csf2hio)
SSMS needs to be SSMS 2012, AFAIK. But NP++ it works and I'm using a fairly old version? http://screencast.com/t/A65g5iqcgHg
I think I misread how to do this in the earlier post. This link explains it well. http://www.mssqltips.com/sqlservertip/2786/column-and-block-text-selection-using-sql-server-management-studio/ 
Thanks. I ran it and it says success but returned 0 rows. There were 18,000 matches out of 400,000 in my original compare, so I'm sure there has to be at least a few thousand. I think something might be amiss. Any ideas?
CAST might be compatible, but may not return the results as expected. TO_* allow you to be much more specific. That's basically the quick and dirty of the differences. With a single parameter passed my guess is they are very similar / the same behind the scenes. 
Try running these variations and see if they return records. SELECT DISTINCT imp.first_nm, imp.last_nm FROM import AS imp INNER JOIN contact as con ON (imp.first_nm = con.first_nm AND imp.last_nm = con.last_nm) INNER JOIN address as adr ON con.contactID = adr.entityId SELECT DISTINCT imp.first_nm, imp.last_nm, eml.email_address FROM import AS imp INNER JOIN contact as con ON (imp.first_nm = con.first_nm AND imp.last_nm = con.last_nm) INNER JOIN email AS eml ON con.contactID = eml.contact_id If both of the above return records, then the zero records is due to none of the imported records coinciding to *both* an address *and* an email.
It's not a bad idea - it's called normalization. Think of it this way, with the suburb in a separate table you've got it normalized already, avoiding data duplication. You can easily use that table for an auto-complete form in your app without having to worry about selecting distinct values (which can be an expensive operation). It also facilitates quickly targeting all the addresses in X suburb, or if X suburb changes it name to Z then you only have to update it in one place, etc. You could also use it in relations to other tables, like maybe you've got groupings of suburbs, or some suburbs that sometimes go by aliases, etc. Some more reading if you're interested: https://en.wikipedia.org/wiki/Database_normalization
Try putting the typename is not null into a where clause instead of having it as part of the join. 
Funpopular, moving it to the WHERE would essentially turn my FULL JOIN into an INNER JOIN since the only way a row in @Table1 would be returned is if it's also in @Table2. I actually was able to get this working using a CTE, but I'm still perplexed on why this isn't working as written.
You could escape the CTE by using a nested query directly: SELECT * FROM @Table1 A FULL outer JOIN (SELECT * FROM @Table2 WHERE TypeName IS NOT NULL) as B ON A.ID = B.ID Some folks tried to explain it [here](https://social.msdn.microsoft.com/forums/sqlserver/en-US/f46cb6ee-3746-49d3-b11c-6a757a613be1/full-outer-join-with-predicate-limit-in-on-clause), but the basic idea seems to be that with full outer joins, you can't really limit the results from one of the tables in the join directly in the predicate. As proof, ignore the full outer for a second, and try this: SELECT * FROM @Table1 A LEFT JOIN @Table2 B ON A.ID = B.ID AND A.id &lt;&gt; 1234 You'll still get a row in the result, even though you tried to remove the only row in A via the ON predicate. You'll see something similar if you do a RIGHT OUTER JOIN and try your original predicate. Basically, the "base" table in an outer join cannot be limited in the ON clause, and with a FULL OUTER JOIN, both tables are considered "base".
You just have to remember that the join condition is specifying when a row from A and a row from B should be matched into one result row. By putting `TypeName IS NOT null` in the join condition you are saying "a row from A can only match a row from B when B has a TypeName". With your example if you had a record with `1236` in A you'd have 4 result rows.
I've had strange problems with regex find and replace in SSMS. It randomly ate letters when I replaced newline characters. I don't think I was doing anything wrong as I'm comfortable with regex.
That's a rather obtuse interpretation of my comment. 
I'm a nice fellow, so I'll just agree, and say that while OP has made a valiant effort, their new diagram is very confusing and I would not recommend it over the existing Venn one.
i didn't interpret, and it wasn't obtuse i disagreed -- big difference 
That's true, but I don't think this approach presents that information in a way that's easy to understand. People like us can look at it figure out what it's saying well enough because we already know the answer. I don't think this is going to be very helpful to the people that don't already know the answer though and they're the only people that need this in the first place.
These are not part of the sql standard and are basically just syntax macro magic 
indeed. I only just realised I wasn't in /r/sqlserver 😀
Nope
I'm learning SQL and this was incredibly confusing to me.
ah good to know, been trying in 2008- no wonder, good to know NP++ will too
Excellent. I have had to teach join types to students and the Venn diagrams are useless at showing exactly how the joins work. I normally ignore them all together and draw up two tables each with 4 rows and show the join types (so basically what this graphic does). [](/GNU Terry Pratchett)
I'll take a look into this later, but I'll answer your question of only selecting the top row; try select top 1 command. It'll return the top row. 
Dynamic SQL, using INFORMATION_SCHEMA for your source? You can start by specifying the db and table in variables and feeding it to the query. STUFF the columns returned into a dynamic varchar string.
You should just be able to extract it useing something like DBartisan, that tool has never messed up an extraction for me and Ive done well over 100. Otherwise you migrate the hash AND 3rd option you can unencrypt them: https://blog.netspi.com/decrypting-mssql-credential-passwords/
Will the trial version of DBartisan provide me with enough functionality?
Haven't used the trial version but, I believe so.
Right on, I'll check out Pentaho. Unfortunately I'm stuck with Java 8, so Talend cannot be utilized. 
https://support.microsoft.com/en-us/kb/918992
I think you should add the set operators as well, using the same approach. * UNION * UNION ALL * MINUS (EXCEPT on some platforms) * INTERSECT 
Disclaimer: This is just my personal opinion. Speaking with some amount of bias as a former Oracle DBA, I really dislike SQL Server (I've managed a few SQL Server databases and have never really cared for it, but then again I'm also fairly down on Windows servers in general). Again, that's just my personal opinion. PostgreSQL is a really nice database, but I haven't used it in years. MySQL was historically more of a toy than a real production database, but in my opinion modern versions (with the appropriate parameter settings) make it a very nice production-ready database. Oracle is an amazing database, but XE is pretty limited (and the cost of a real Oracle version is fairly steep). Given that, unless you're really looking to delve into Oracle specifics for some reason (which probably won't matter much to you until you've got a better basic understanding of databases and SQL in general), I think you'd be better off starting with MySQL or Postgres. As far as experience as it pertains to possible jobs, I think /u/el_chief's comment outlines the typical use cases pretty accurately. Oracle and SQL Server are going to be used in larger businesses, MySQL and Posgres will generally be used in smaller businesses (but you'll find plenty of jobs for any of these, and depending on the position you're looking for specific experience with a particular RDBMS isn't nearly as important as having an excellent grasp of SQL fundamentals, which are going to translate well across any engine).
Starting off with cross joins seems wrong to me. I imagine the idea was to start with it because you can think of it as the "default" in the sense that all the other join types produce a subset of its results. However, you still end up with a graphic that starts with -- and is dominated by -- a join type that is easily abused and novices should probably never need to use. It's like starting a tutorial on a programming language's control statements with "goto" statements. Anyways, I took the liberty of rearranging the graphic so that cross joins come last: http://i.imgur.com/qlWMUmY.png 
I disagree, because on its own, the Venn diagram approach is mathematically incorrect. **[This explains why.](http://datamonkey.pro/blog/why_venn_diagram_is_a_bad_choice/)**. I think /u/bbalkenhol has done an excellent job here, and ironically, because he has accurately represented sets (via coloured rectangles), he could include the Venn equivalent as a side note and still be accurate. My suggestion is to put the Venn equivalent to the right of the result set (separate column), and include set operators (UNION, UNION ALL, MINUS and INTERSECT), then this will be the de-facto reference and great teaching guide for new SQL adopters. 
You raise a good point and you are, of course, free to change the graphic as you see fit. :) I added your change to the top post.
From what I've gathered from the replies so far, it may well be that Venn diagrams are more immediately intuitive to most novice users. Which is perhaps not all that strange when considering they see pretty widespread use in other fields too. I'm not at all opposed to including them in this graphic if that makes it easier to grasp, so I might look into it once I find some spare time. Same goes with your suggestion about set operators. I deliberately left them out of this graphic, but it should be quite straightforward to do and it shouldn't take too much space, especially when orienting them vertically.
SQL version?? What do the tables look like
You should definitely learn Access but not for traditional DB storage reasons. I am one of two DBAs for a 2500 employee company and we use Oracle (both Oracle Apps and several instances of 11g, but Access is what we use more than anything. We use it as linked ODBC, not DB, of course, but learning Access is very valuable, from both a query standpoint and from a reporting standpoint. Access is used as a front-end rather than a database, but it's really the easiest way for most users to start getting data. Pull it with Access and dump it into Excel then pivot. We tried an enterprise version of Crystal Reports but that hasn't caught on near as well as good ol' Access. I would focus your time on two products that you can get for free at home: MS SQL server express and Oracle Express. Both have their own flavors of syntax, and both also support the ANSI-92 coding standard as well. While it is important to know how to conform to ANSI when you need to, there are several handy functions that are platform-specific. In the real world you will run into Microsoft and Oracle more than anything else. And although it is preached to code ANSI-92, you won't find much of that in the real world. You'll find Oracle syntax in Oracle and you ought to be familiar with that so you don't stumble during your first 3 months as a DBA or assistant. A lot of people say "but what about portability!!?!?!" but in all my years of doing this, rarely does a company migrate code from Oracle to Microsoft or vice versa. With larger companies they are subject to vendor lock-in because of all the esoteric customizations they have made on their platform(s) and the cost is not worth switching over. So, learn both ways! Run them side by side and code something using MS and code it again on Oracle. Honestly the best way to get into it is to find one single table of interesting data that you really like, and practice on that. Joins are important, but knowing how to filter and how to find specific things will get you further ahead than just knowing what joins do. 
I'm finding that all my licenses expired 2 years from when I procured them via Dreamspark :( Still worth it while you can do it anyway. And here I thought it was mine forever. 
In the smaller investment offices or even trading departments of banks, I see people using Access. This is usually a 500 people or less company or department and the sub dept that would be using Access for their functions is probably 10 people or less. So I can see the benefit of knowing it. Question now... If I learned SQL via MS SQL or Oracle (express versions for both), would that same syntax and logical concepts be transferable to MS Access or does that have it's own nuances? I'm familiar with MS Access design view, just not hard coding anything in SQL which is what I want to learn so I'm not stuck with Access alone but can apply database skills generally no matter where I am.
Access is primarily GUI driven but it has it's own syntax for things like if-then and string building. You can also write straight SQL in Access or use the native SQL via ODBC connection by using what Access calls a "pass-through" query. But generally there isn't a lot of SQL to be written when using Access. Access is best used as a query tool rather than storing critical DB stuff. 
Please give us more information. Maybe Triggers can be used in your case. 
You can query the sys.all_columns to check what columns are not null for your table. 
No it will do the join irresepective of whether you use any columns or not Plus it will unnecessarily slow down your query. 
Help me understand. If it is not matching any data what is it joining together? 
Don't ever join to tables unless you are accessing information from that table, as it can dramatically affect performance in an adverse way. I've seen queries on production systems that were taken minutes to run (because of an unnecessary join), which took less than a second after it was removed. Accessing information from a table can be : * Selecting a column or data (count, aggregation) * Using the table to determine existence of something (e.g. a where exists correlation) * Using the table to get access to something in a related table (sometimes you have to join three tables to get to the third's data) 
Perfect. Thank you for providing the 'order of query execution' very helpful
Basically I have been asked to learn SQL Reporting and one of my tasks is converting a column full of numbers, so 1-23 to the relevent text. So in my case I have 23 different numbers that mean (example - different products) &amp; the query will show them allong with lots more data. 1 = Potato 2 = chocolate 3 = Carrot 4 = Haribo so currently my query outputs numbers like this. 2 4 2 2 4 5 6 1 4 3 &amp; I need to display then as there relevant name rather than the number. (Fyi there isnt a description row)
&gt; Fyi there isnt a description row okay, so where are the descriptions stored? 
I'm no expert, so this'll need some further research on your part or illumination from another reader. What you want is the [CASE statement](https://msdn.microsoft.com/en-us/library/ms181765.aspx). This is for SQL Server, so it may be different depending on your specific system. My syntax is likely wrong, but something like SELECT ProductID CASE ProductID WHEN '2' THEN 'Chocolate' END FROM Table 
Where are the descriptions stored? You need to INNER/LEFT JOIN that table on the product ID and then bring in the description from the second table. If your product descriptions aren't stored in a table but are just "common knowledge", then you can use a case statement like CASE WHEN products = 2 THEN 'Chocolate' END.
I have a stored procedure that does something similar to this. It accepts a username, but that could be replaced with an EmployeeID instead, then recurses up to the CEO level instead up recursing down from the CEO level, then trying to filter. It basically returns the management structure, as well as any peers (people with the same manager as the target user). Mine pulls from AD, but could be easily modified to pull from a table instead. Check it out: [Link](http://ryandevries.com/index.php/2015/04/29/recursive-cte/)
Yeah I think the transferable fundamentals is key here and knowing relational database design and some basic best practices. I probably should have specified that my questions is coming more from the perspective of a financial analyst that would use SQL to extract data from whatever database a firm is using, rather than a DBA position. Might actually install/try out 2-3 of these and experiment whichever appears more friendly for what I'd be trying to do. A sample project might be taking a data table of performance for a population of stocks and then find some relationships or organizing them into groups based upon market cap or geographic locations, domestic, foreign, emerging markets, etc. I've used Access previously to take various data files from different vendors and sources and "normalize" them into one standard format for import into our own system. Something I'd also like to do with SQL
 The tables are below and keys I'm using would be CID and PID. I want to check if a CID and PID are already associated in the "Purchased" NN Table, and if not insert it. Most solutions I'm seeing only check for one value (like the CID). I'm going to try a few new solutions today since I've had a chance to rest. **Customer Table** CID| First Name | Last Name | Email ---|---|----|---- C1 | John | Doe | JDoe@email.com C2 | Mary | Smith | MSmith@email.com C3 | Kevin | Graham| KGraham@email.com **Purchased Table (NN bridge)** C_ProID | CID | PID ---|---|---- 1 | C1 | P1 2 | C1 | P2 3 | C2 | P3 4 | C3 | P1 5 | C3 | P2 6 | C3 | P3 **Product Table** PID | Product Name | Description ---|---|---- P1 | Word | Documents P2 | Excel | Spreadsheets P3 | Powerpoint | Presentations P4 | Sharepoint | Document Management 
SQL Server 2008 R2. And I've included tables in [this post](http://www.reddit.com/r/SQL/comments/3aw52o/ms_sql_insert_into_nn_table/csh8y2b).
I'm not sure if I understand. I mean obviously I'm going to need to check the keys of the two tables and see if they're in the NN table, but that's what I'm working on. I've included some tables [here](http://www.reddit.com/r/SQL/comments/3aw52o/ms_sql_insert_into_nn_table/csh8y2b) for better visualization. 
&gt;If you're product descriptions aren't stored in a table but are just "common knowledge" Funny thing about "common knowledge" - after a while (people leave the company, people change job roles and don't deal with that data anymore, etc.), it becomes not so common anymore. If it's not in a lookup table, it probably should be. `CASE` statements, while useful, are a maintenance nightmare for a situation like this. Add a new product? Gotta update **all** your queries.
There's still a lot I don't understand here. Do you want to fill the bridge table with ALL possible combinations, or will you be inserting values when a purchase has been completed, or? What does the bridge table represent exactly?
I finally got in touch with a guy I know who figured it out. You are right, there weren't any emails. I was looking at the wrong table that contains emails. There's apparently several different tables, all with email addresses, with no description on which one goes to which.... anyway... Thanks again for the help. Right answer, just wrong input on my part.
Have you tried deleting the linked tables, doing a compact and repair, relinking them, and then running another compact and repair?
You are my hero! I didn't think it would help, but I've closed and opened it a few times and no errors, so simple I feel stupid. Enjoy the lounge!
It is unclear if you are talking about the end user tool named "Oracle SQL Developer" or if you are talking about developers using Oracle Database. 
I have worked database related jobs for the past few years as well and the long term viability for DBA and Database Developer roles is interesting. For example, cloud managed databases taking over the DBA responsibilities and abstraction layers replacing a good bit of database modelling and development. Out of curiosity what uncertainty have you heard that exists in the industry? With that being said there is certainly nothing wrong with continuing education at any age. It's a gamble if you will see the costs outweigh the benefits. However there is more to higher education than just career advancement.
haha the bakery
You could use an excel formula, and have the info in one cell input into a string of you choice. Like ="or db.table1.pk = '"&amp;A1&amp;"'" and get a list of them. That is your question right? You're looking for a way to not write out or x.x.x = xyz over and over? And you can use like instead of the = near '"&amp;A1&amp;"'" if the extra = messes up the excel formula
I can relate to most of those. All our servers are food courts... actually, our whole network is one giant food court.
Nice. I've definitely known a few Wizards of Oz in my time.
My advice is to create a table with your filter list in (single column, PK is that column), loaded from a CSV file saved from your Excel spreadsheet. Load the CSV table [using this approach](http://blog.sqlauthority.com/2008/02/06/sql-server-import-csv-file-into-sql-server-using-bulk-insert-load-comma-delimited-file-into-sql-server/), then simply join your table to the filter table. Nuke the contents of your filter table if you load new content. Simple! 
**If** it is in sequential order like your example, maybe you could try this? WHERE db.table1.pk BETWEEN '12345' AND '19999'
Awesome; now we are much closer! Thank you! However, being a bit of a newbie, my organization has restricted my access to read-only, meaning I do not believe I can create or bulk insert into a table on the DB I'm reading from. However, I think I've seen a way you can create a temporary table within a query--essentially something you don't want permanently stored in the DB but is usable each time the query is executed; I may be wrong, though--again, pretty new. If the creation of a temp table is possible like this, do you happen to know if the bulk insert feature would work on such a table? EDIT: I should clarify--I realize nuking/dropping the table is essentially the same as creating a temp-table, but would require the write access. I've seen code like the following where I can't find a permanent table by the name listed. For example, if my permanent table name is something like: db.Provider.PK db.Provider.First_Name db.Provider.Last_Name I've seen code where the syntax is not the same, and the table does not seem to exist anywhere within the database to which I have access--something like: v_Provider_Plus_Extra_Data.field1 v_Provider_Plus_Extra_Data.field2 is this second snippet creating a temporary table? 
Yes, you can, [see this link](http://stackoverflow.com/a/15655589) as an example, just remove the insertion into existing table section.
Knowing what "Table" you're querying in your example will help you out a lot. A database is made up of many tables that store different information. The product description you're looking for might be in a different table that stores information about products specifically. I've created a very simple example database diagram to help you visualize how database normalization works. This example would be part of how a grocery store might be set up. Skip to the bottom if you think you've got it. I'll address your problem specifically there. http://i.imgur.com/HjxUd6q.png tableOrders in the top left makes a new entry (row) for every single item that is purchased from the store. Each transaction starts a new orderID which will always be unique (no duplicates). Along with this new orderID, they table will also store the CustodianID, RegisterID, and ProductID of that particular transaction. So if "Igor Shlandomir" buys a Strawberry from Lane 3, you'll see a new entry in tableOrders with the CustomerID: 64315, CashRegisterID: 3, and ProductID: 3. We don't want to store redundant information all over the database, so we put information relevant to the different objects in their own tables. This way, we don't have to waste space storing the customer's birthday or the register model number every time we make a order, but we can look up that information if we want by joining those appropriate tables to our tableOrders by their foreign key fields (ID columns). This process is called Database Normalization. It removes redundant information and enforces data integrity. Imagine if a customer changed their name and you had to update your database. If the customers are stored in their own table, you can easily just change the field corresponding to their name. Their CustomerID stays the same, so all their previous transactions still point to the correct person. **Okay, so now on to your problem**. You're listing a ProductID, but you aren't getting the ProductName. I suggest first running a query that will return all the columns from your table SELECT * FROM Table See if you can find a column that lists 'chocolate' under the line for ProductID #2. If you can't find the product name 'chocolate' anywhere, then you're probably looking in the wrong table. You need to make a join between your current table and the one that stores the product information. The JOIN between my example tables, tableOrders and tableProducts, would look like this: SELECT * FROM tableOrders JOIN tableProducts ON tableOrders.ProductID = tableProducts.ProductID Remember that you're not changing any information in your tables by doing this. "SELECT" statements are just a command to help you view information, a table join is not modifying anything, it's just helping you view information from two different places. The join I listed above will display every column from both of the tables. The only rows that will be displayed will be rows where a ProductID from tableOrders matches a ProductID from tableProducts. This is an example of an Inner Join, but I don't want to go into much detail about that now. Just know that an Inner join is defaultly implied when you use the term "JOIN". Once you know how to join your tables, you can just SELECT the information you want to see. SELECT ProductName FROM tableOrders JOIN tableProducts ON tableOrders.ProductID = tableProducts.ProductID I hope this helps you find your chocolate. If you have any questions, I'd be happy to go into more detail or show you some more info on basic SQL. Remember that once you join your tables, you can evaluate any column from them. Below is an example of what my database example would like like if i wanted to join all the tables together in one SELECT statement. SELECT * FROM tableOrders JOIN tableProducts ON tableOrders.ProductID = tableProducts.ProductID JOIN tableCustomers ON tableOrders.CustomerID = tableCustomers.CustomerID JOIN tableRegisters ON tableOrders.RegisterID = tableRegisters.RegisterID If you wanted to just look at columns from the tableOrders table with their foreign key references substituted in for IDs, then it would look like this: SELECT tableOrders.OrderID , tableProducts.ProductName , tableCustomers.FirstName , tableCustomers.LastName , tableRegisters.RegisterName , tableProducts.ProductName FROM tableOrders JOIN tableProducts ON tableOrders.ProductID = tableProducts.ProductID JOIN tableCustomers ON tableOrders.CustomerID = tableCustomers.CustomerID JOIN tableRegisters ON tableOrders.RegisterID = tableRegisters.RegisterID 
Some humor value I suppose, but no slang I've ever heard of. I think the author made them up and I spit in his general direction. 
Poe's Law: Is this real or a joke? 
If you want to work into Project Management, a PMP certification will be much more marketable than a general degree. If you *do* decide to go for an online college, stay the fuck away from For Profit places (Disclaimer: I have two degrees from For Profit colleges. Wouldn't do it again). 
@TableVar is your temporary table, once loaded you can select from it or join to the table you're trying to filter on. In that example there is an insert into dbo.Existingtable, you don't need to do that bit.
Good point. I'll edit it to make that clearer.
Does it have to be where the price is over 10? I don't understand what you're asking as there is a number in every row in the column 'Price'. SELECT productID, price, priceSourceID FROM tableName WHERE price &gt; 10
I want the data returned for each productID only when there is a price for all priceSourceIDs. In this case there are 3 priceSourceIDs (1, 2 and 3). Only productIDs 101 and 103 have all 3 prices for all priceSourceIDs
Ah ok, that makes more sense to me now! SELECT productID, price, priceSourceID FROM tableName WHERE priceSourceID IN (1,2,3) Try that mate :)
You will need a subquery to accomplish this task, like so: select * from tablename where productID in (select productID from tablename group by productID having count(*) = numberOfProducts) @dotmanwill, all you need to do is replace tablename with the name of the table, and numberOfProducts with the number of products you want to check for @vonsparks, your solution would return all rows where the priceSourceID is 1, 2, or 3. Based on the sample data provided, this would return all rows. 
So that logic wasn't quite what I wanted, but it got me to a solution that worked. I'm now feeding the data from the two tables into a temporary table and using the following to accomplish what I want: MERGE PurchasedTable AS T USING #Temp AS S ON T.CID = S.CID AND T.PID = S.PID WHEN NOT MATCHED THEN INSERT (CID, PID) VALUES (S.CID, S.PID); Thanks for the assistance. 
I did something similar where I had historical item purchasing and purchases over different dates and different prices. I don't know if its syntactically correct but this was the basic method. I wanted to know when something was last ordered and the price we paid for it. Starting with what your are selecting from, you want to partition your items over a unit price descending and assign row numbers to those partitions. You alias the entire thing you are selecting from as "r". Then your outer select selects from your inner select where r=1. It will return the first row for each partition. You ordered your prices descending within each partition, so the highest price should be row 1 for each item. I don't have access to SQL at the moment but I'm working off a script where I did exactly what you're trying to do, just in a different context. I was ordering mine by last creation date of order number whereas you can order it by price descending because prices only increase and never get lower. Also, in the inner select, if you have two prices that would end up the same, order it by more than one thing. Maybe you want unit price desc then year, then size, etc. Just depends on what row 1 should return within each partition. SELECT storenum, itemnum, size, price, year FROM (SELECT storenum, itemnum, size, price, year, row_number() over(partition by itemnum order by price desc) r FROM itemmenu) WHERE r=1 ORDER item desc; 
That won't work. You are finding the max price for size, storenum, across all years, not for the max year. 
Ah, thank you....yes, no bueno.
MySQL doesn't support windowed analytic functions like row_number.
Unfortunately this does not work but I certainly appreciate the effort. When I do your query and filter for store 205706 and item 5000 I get the follow result. SELECT t.* from ItemMenu t, (select max(price) as maxprice, max(year) as maxyear, size, storeNum FROM ItemMenu GROUP BY size, storeNum) t2 WHERE t.storeNum=t2.storeNum AND t.size=t2.size AND t.price=t2.maxprice AND t.storeNum=205706 AND t.itemNum=5000; +------+----------+---------+-----------+------------+-------+------+--------+ | pk | storeNum | itemNum | vendorNum | size | price | year | status | +------+----------+---------+-----------+------------+-------+------+--------+ | 5603 | 205706 | 5000 | 3000 | Individual | 1.79 | 2014 | Active | +------+----------+---------+-----------+------------+-------+------+--------+ A raw query of store 205706, itemNum 5000 and year 2014 returns the 4 results listed above. However, your query only returns one row and I'm not sure why.. maybe because it's the max PK? 
For a hacker it was a pretty good bash!
Something like this? SELECT m.storenum, m.itemnum, m.size, m.PK, m.price, m.year FROM itemmenu m INNER JOIN (SELECT storenum, itemnum, Max(year) year FROM itemmenu GROUP BY storenum, itemnum) mx ON mx.year = m.year AND mx.itemnum = m.itemnum AND mx.storenum = m.storenum INNER JOIN (SELECT storenum, itemnum, size, max(pk) as pk, year FROM itemmenu GROUP BY storenum, itemnum, size, year) mp on mp.year = mx.year AND mp.itemnum = mx.itemnum AND mp.storenum = mx.storenum AND mp.size = m.size AND mp.PK = m.PK WHERE m.itemnum = 5000 AND m.storenum = 205706 
All you need is the CustomerLoan table since it contains c_id. select c_id from CustomerLoan group by c_id having count(*) &gt; 1; That gets you all the c_ids that exist more than once in the CustomerLoan table. "select c_id from CustomerLoan" gets you all the c_ids, unique or not; if you "group by c_id" it gives you a list of c_ids without repeating any; if you add the condition of "having count(*) &gt; 1" it works in conjunction with the group condition to narrow the list down further to c_ids that appear in that list more than once (and thus have more than one loan). If you also want to see the number of loans each customer has, try: select c_id, count(*) as num_loans from CustomerLoan group by c_id having count(*) &gt; 1;
This looks like homework. Here is a method for finding duplicate customer IDs in loans. If the same C_ID shows up more than once, then they have more than one loan. SELECT DISTINCT c_id FROM customerloan GROUP BY c_id HAVING count(c_id) &gt; 1; You could join that result with the customer table to get the name or other details. 
&gt; select c_id, count(*) as num_loans from CustomerLoan group by c_id having count(*) &gt; 1; To expand on this to pull the name a simple join is all that is needed. SELECT c_id, c.Name, count(*) as num_loans from CustomerLoan CL JOIN Customer C on C.c_id = CL.c_id GROUP BY c_id having count(*) &gt; 1; 
You, uh, may have missed the point.
Many of them are just filler as well. Could of easily been a list of 50 things with better descriptions.
Yeah. The 101 bit is a tad overdone. But there are diamonds in the rough within. I definitely learned a few handy tips / tools from it.
Ambi-debtors
Aside from consume white space, check that your page size, interactive size, and margins all line up and allow enough space for your image. Sometimes it will mess with it and cause things to wrap. All else fails, try putting it in a rectangle. 
Move the select query which reads CODE and DESCRIPT into your else section.
Doesn't work. It throws an error: ORA-01422: exact fetch returns more than requested number of rows ORA-06512: at line 18 DECLARE PRICE PRODUCT.P_PRICE%TYPE; PCOUNTER NUMBER; CODE PRODUCT.P_CODE%TYPE; DESCRIPT PRODUCT.P_DESCRIPT%TYPE; BEGIN SELECT Max(P_PRICE) INTO PRICE FROM PRODUCT; SELECT Count(P_PRICE) INTO PCOUNTER FROM PRODUCT WHERE P_PRICE = PRICE; IF PRICE = 1 THEN Dbms_Output.PUT_LINE('The product with highest price is ' || CODE || ', which is a ' || DESCRIPT || '.'); ELSE SELECT P_CODE, P_DESCRIPT INTO CODE, DESCRIPT FROM PRODUCT; Dbms_Output.PUT_LINE('There are ' || PCOUNTER || ' items with a price of $' || PRICE); END IF; END; / 
OP said he had a bit of confusion about indexing.....which I believe was explained relatively well at that site. I read an explanation at MSDN, but it added more confusion. 
You've completely misunderstood what I've said and your own problem. This is what you are trying to do. DECLARE PRICE PRODUCT.P_PRICE%TYPE: PCOUNTER NUMBER: CODE PRODUCT.P_CODE%TYPE: DESCRIPT PRODUCT.P_DESCRIPT%TYPE: BEGIN SELECT Max(P_PRICE) INTO PRICE FROM PRODUCT: SELECT Count(P_PRICE) INTO PCOUNTER FROM PRODUCT WHERE P_PRICE = PRICE: IF PCOUNTER = 1 THEN SELECT P_CODE, P_DESCRIPT INTO CODE, DESCRIPT FROM PRODUCT WHERE P_PRICE = PRICE: Dbms_Output.PUT_LINE('The product with highest price is ' || CODE || ', which is a ' || DESCRIPT || '.'): ELSE Dbms_Output.PUT_LINE('There are ' || PCOUNTER || ' items with a price of $' || PRICE): END IF: END: / 
That's ok, did it work right? Basically, what a WHERE IN clause does is a similar thing as if you did **WHERE priceSourceID = 1 AND priceSourceID = 2 AND priceSourceID = 3** it's just neater to do a WHERE IN clause :) Hopefully that query did the job for you?
Yes that was one of the first things I found when I Googled, didn't change anything.
This seems like what I'm after, need to look at how to set up the parent groups but otherwise thank you very much! 
Works perfectly - I can't thank you enough for your help!
Awesome response. I really appreciate it. 
We all started somewhere. You figure that only 10% of folks are bright enough to work in IT and only 2% are either bright enough or crazy enough to become DBAs. That you are following LaRock already puts you well ahead of the curve for making it to the the 2%. That should be encouraged. Remember number 101, a good DBA is a "lazy" DBA, lazy in that he likes his nights and weekends spent on gaming and not putting out fires or cleaning up after avoidable disasters. Let the pros "do" the work for you, follow best practices and reserve nights and weekends for the occasional patching, upgrade or maintenance, and gaming of course. 
Thanks for the kind words, much appreciated. 
here is the code i want to use: SELECT * FROM table.codes WHERE ICD9Code REGEXP ('[14-100]__)'; There are 2 _ in the above code. 
the ON DUPLICATE KEY part takes place ~instead~ of the insert you're looking for two statements wrapped in a BEGIN ... END block
I have never heard of or used BEGIN ...END blocks before. Could you show me an example of how I would accomplish this?
thanks for the reply! I am new to SQL, can you explain what each of the parts of that statement do?
Yes, I assumed the joins would perform faster but I was wondering whether there were any actual differences between the sets being returned between either approach. Thanks.
From what I read, the caveat to indexes is that they are updated as data is updated or added. If the table is constantly added to and/or updated and is HUGE, this can become a problem. Aside from that, If you are searching on 3 fields, add the 3 indexes. Especially the comments field, the text field is where you will take the hit during an unindexed search.
Should act like any other installed app- use what it wants and disregard the rest. 
Yeah... should... 
Very impressive reference.
post the whole statement
Start with learning how to use Google and search functions on websites such as reddit. Trust me... your mind will be blown.
ah... nicely done, sir.
Interactive SQL Tutorial: http://www.sql-tutorial.ru/en/content.html
I think this is much more intuitive (for me, at least) than just venn diagrams. It removes a layer of abstraction, imo.
When you say "32gb data", I'm not clear on whether you mean it's hosting 32gb of databases, or whether the physical machine has 32gb of memory.
There are less built-in functions and it's not as strict with your "group by" needs for aggregate statements, such as sum(), etc.
i don't use mySQL enough to know the specifics, but i laugh when I read about the NUMEROUS issues around its support for ACID compliance. Ex: a trigger bug recently celebrated its 10th birthday as an open issue. check online... lots of people have written about it. Also, it is my understanding that most of the issues are predicated on the default ENGINE... if your DB uses the other engine, I think the majority of issues disappear, along with performance (again, just *my* ***very*** *limited* understanding). better quest, ask the instructor why they require mysql... php works fine with other engines like postgres (most people who complain about mysql use postgres instead, which seems to receive only compliments)
Every PHP course I've looked at uses MySQL. I think they may be avoiding having to buy licensed software and choose open source instead. Also, MySQL is more popular here than PostgreSQL for some reason. 
Build a database from sample data. Then run simple SQL queries on it! You'll start by editing what you see without knowing whats going on. But eventually you'll be able to build your own. You can use a program like navicat or pgadmin to build the database and query it. 
So the whole VM is hypothetically to be given 160Gb- 128 for the DB, and an additional 32 for the OS. My director fears somehow it won't ignore what is beyond the 128Gb threshold.
Oracle is an F1 car. Stupidly quick, and 99% of the world would crash or stall it. MySQL is a clowncar. Without the benefit of being able to fit infinite clowns in it.
You probably chose the wrong security scheme during setup. Reinstall, choose windows authentication and make sure to add your account to the list in the setup. BTW this is for MSSQL not MySQL.
Open SQL Server Configuration Manager. Make sure you didn't install as an instance i.e. NOMAROMA\INSTANCENAME If you did, just use the full instance name when connecting, see if that helps.
execute eventvwr command on windows run. Expand windows log and go to Application and then try to see the cause of sql server error . The detailed cause of error you will come to know from it.
as said above, ensure you have the correct full instance name (typically localhost\myinstance), secondly ensure that the account you're using actually has access to the SQL Server. To get around that, you need to ensure you specify the correctauthentication mode (i typically select "mixed mode" as well as username:"sa", password: "sqladmin") as well as the service admin (sa) user during the installation of SQL Server. You can then set up additional user and logins after you login for the first time as sa
[This is what I see in the config manager](http://imgur.com/7yM9zPT) I have been through all the items and nothing exists that depicts my username or any indication that I have a db set up