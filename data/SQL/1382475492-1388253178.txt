You mean like DELETE a.* FROM a JOIN b ON a.id = b.id where b is your csv file?
You can avoid a commit for every delete if you are not doing it yet. CrossWired already said everything else on this subject (that i know of)
**A)** This seems like a really bad idea **B-1)** Have it create a temp table and import the CSV then... DELETE FROM [your_table] WHERE id IN (SELECT id FROM [temp_table]) --something to that extent anyway **B-2)** Data Flow Task **C)** This still seems like a really bad idea/process
Round 2: How I would implement it **** * Have an app (website/etc) that lists the data * Add an additional button that submits the table primary key to a list/table that marks it for deletion * Second part of the app is where the supervisor can log in, list the requested deletions, and confirm them with another click of a button. **** Development time for something simple like this would be a few days with good testing, it also has the advantages of: * Easier to log requests and deletion, along with the time * Accidental deletions due to laziness would be drastically reduced. 
You can bulk insert into a temporary table and then delete, along the lines of BULK INSERT @b FROM '\\server\directory\.csv file WITH ( FIELDTERMINATOR = ',', ROWTERMINATOR = '\n' ) DELETE FROM tableA FROM tableA JOIN @b ON a.id = @b.id GO
This should get you what you want if you were using MS SQL (which I am familiar with): SELECT Title ,BranchNum ,CopyNum ,Quality ,Price AS [OldPrice] ,CASE WHEN Price &lt; 10 THEN (Price * 1.1) ELSE Price END AS [NewPrice] FROM Book, Copy WHERE Copy.BookCode=Book.BookCode AND Quality ='Excellent'; And for MS Access it should be this (I think): SELECT Title ,BranchNum ,CopyNum ,Quality ,Price AS [OldPrice] ,SWITCH (Price &lt; 10, (Price * 1.1), Price &gt;= 10, Price) AS [NewPrice] FROM Book, Copy WHERE Copy.BookCode=Book.BookCode AND Quality ='Excellent';
Use an if statement (or the access version of it) Select if(a&lt;10,a,a*1.1) as answer From... Sorry if those values don't align with your originals. I can't see the post and type on my phone. If access doesn't use if maybe its iif or case when
Try this: SELECT Title ,BranchNum ,CopyNum ,Quality ,Price AS [OldPrice] ,SWITCH (Price &lt; 10, (Price * 1.1), Price &gt;= 10, NULL) AS [IncreasedPrice] FROM Book, Copy WHERE Copy.BookCode=Book.BookCode AND Quality ='Excellent';
 SELECT Title , BranchNum , CopyNum , Quality , Price AS [OldPrice] , IIF(Price&lt;10, Price * 1.1, NULL) AS [IncreasedPrice] FROM Book JOIN Copy ON Copy.BookCode=Book.BookCode WHERE Quality ='Excellent';
What's with the WHERE clauses? JOIN...ON exists. 
Using WHERE clauses is the old ANSI-89 standard and for the most part is considered less desirable than using the newer ANSI-92 ON clause syntax. For inner joins the result should be identical in performance and data regardless which syntax is used. 
What Nepobot suggested would work fine if there was a join to the second table where you check for '%APPLE%'. You could also look at [this stored procedure](http://vyaskn.tripod.com/search_all_columns_in_all_tables.htm) that allows you to search a database for a keyword; could be modified to suite most situations.
 select * from fruit inner join fruit_bunches on fruit_bunches.name like '%' + fruit.name + '%' Edit: This will only work in MS SQL. The structure is the same for other DBMS, but the concatenation method and the wildcards could be different in your DBMS. Edit 2: Other DBMS may be case sensitive by default so in those cases you may have to convert both fields to either upper or lower.
Can you show a minimal test case? You shouldn't be having problems, so showing actual code here is important. Also, if you have all us zip codes with lat and lon, the distance from one to all the rest will take a hellish long time to compute with haversine. (required for your radius search). I suggest you look up a couple other algorithms that are less accurate but faster. If you know what your maximum radius will be, and it's fairly small, you might find it better to precalculate and store in a lookup table. The distance needs to be small though ; the full cartesian product of all us zip codes with themselves is big enough to be an issue ... I may have implemented this same thing fairly recently... 
&gt; this stored procedure ... works only in MS SQL, but OP's dbms is unknown 
&gt; '%' + fruit.name + '%' ... works only in MS SQL, but OP's dbms is unknown 
It's not that hard to adapt to others, MySQL could be adapted easily with MATCH. Oracle can be done with a stored procedure [similar](https://forums.oracle.com/message/4229846) to the MSSQL one.
That is really semantics, the operators are slightly different but the concept is the same. '%' + fruit.name + '%' CONCAT('%',fruit.name,'%') '%' || fruit.name || '%' 
What do you need more than the [sample code I already have listed](http://stackoverflow.com/questions/19509413/do-custom-functions-not-work-with-php-pdo-prepare-execute-methods) to see? I don't think I'm using haversine, am I?
agreed... but if you're gonna give a solution without knowing the OP's dbms, in a subreddit which is *specifically* meant to be platform neutral (see sidebar), then only the **standard sql** solution should be given, and this rules out using microsoft's laughable plus sign for concatenation 
it's not that hard to rewrite T-SQL to some other sql variant? hoo boy, aren't you the clever one
the solution involves a **recursive CTE** check out [this microsoft technet page ](http://technet.microsoft.com/en-us/library/ms186243(v=SQL.105\).aspx) that has the exact scenario that you're looking for ("direct reports") including an explanation of how the code works
T-SQL is based around being similar to scripting; with the various benefits and detraction that come with that. I don't really see a more unfriendly method then being forced to use a function like CONCAT though. Frankly there is rarely such a thing as a '**standard sql**' statement for difficult tasks or queries; there may be a universal solution but would be unwieldy and even less helpful. Just because a solution is in a specific DBMS doesn't mean you can't easily Google other equivalents in your platform of choice. This subreddit is more about general assistance and pointing people in the correct direction then it is for explicit solutions. &gt;Edit - You should also re-read what the sidebar says: &gt;*While naturally we should endeavour to work as platform neutrally as possible* **many questions and answers require tailoring to the feature set of a specific platform.** Edit2 - Being a technician/administrator/etc worth your salts means you can research and adapt(/create) solutions.
A complete test case that exhibits the problem, not just a snippet; I can't take your code and run it to look at it. Where are $lat and $lon coming from? Why are you treating them as strings in the SQL? For a minimal test case, you also need to leave out all the other stuff that confuses the issue; instead of implode()ing a bunch of WHERE clauses that are irrelevant to the question, can you exhibit the issue with a single: $sql = "SELECT * FROM providers WHERE distance(:mylat, :mylon, lat, lon) &lt; :dist"; $stmt = $db-&gt;prepare($sql); $stmt-&gt;execute(array(":mylat" =&gt; 10.1, ":mylon" =&gt; 100.5, ":dist" =&gt; 25)); Nowhere are you checking for returned errors or looking at errorInfo Does it manifest if you do it by joining the table to itself so as to avoid binding a parameter in the aggregate? [hint: it'll probably work, and it's neither indicative of your real problem, nor an appropriate solution] $sql = "SELECT B.* FROM providers A CROSS JOIN providers B WHERE A.lat=:mylat AND A.lon=:mylon AND distance(A.lat,A.lon,B.lat,B.lon)&lt;:dist"; You're right; that's actually the spherical law of cosines. I was assuming haversine because it's what was first mentioned in the stackoverflow question.
&gt; Being a technician/administrator/etc worth your salts means you can research and adapt(/create) solutions. so basically you're saying that anyone who asks for help on this sub is not capable that's pretty elitist 
&gt; If it isn't then someone on stack exchange/forum/etc has already come up with a solution(or close). so basically you're saying that anyone who asks for help on this sub should go look on stack exchange/forum/etc instead that's pretty elitist
Awesome, I'll give that a look. Thanks for the quick response.
I think I know what you mean. Give this a try: /*Find group count*/ WITH X AS (SELECT emnekode, grnr, COUNT(bnavn) total FROM EmneStud GROUP BY emnekode, grnr) SELECT X.emnekode, X.grnr, X.total FROM X INNER JOIN /*Find smallest group total per course*/ (SELECT emnekode, MIN(total) min_total FROM X GROUP BY emnekode) Y ON X.emnekode = Y.emnekode AND X.total = Y.min_total
As somebody certified in base SAS who then went on to regularly use SQL, SAS is less logical. In many basic CRUD statements, SQL can almost be read aloud like a sentence: "SELECT this FROM that..." SAS isn't a terribly hard language, but I find it more difficult because the syntax is somewhat less structured.
No, I'm saying that if you come looking for help you shouldn't expect solutions to be handed to you. You might have to work to get an answer; that isn't elitist, that is realistic.
I think you are confusing pride in ones work/abilities with elitism. *Edit - Forums are the opposite of elitism; decentralized communal effort.*
In summary, we need: To know the number of students in each course/group combination: SELECT emnekode,grnr,COUNT(*) AS count_students FROM emnestud GROUP BY emnekode,grnr; This aggregation will play a part twice in our query, because the count of students by course and group is not readily available. As an alternative, we could have taken the results of this query and created a temporary table - I just chose not to do this. Then, for each course, we need the minimum number of students across all participant groups: SELECT b.emnekode,MIN(b.count_students) FROM (SELECT emnekode,grnr,COUNT(*) AS count_students FROM emnestud GROUP BY emnekode,grnr ) b (emnekode,grnr,count_students) GROUP BY b.emnekode; Finally, we combine the results from the above two requirements to identify the group (or groups) within each course containing the minimum number of students: SELECT a.emnekode,a.grnr, a.count_students FROM (SELECT emnekode,grnr,COUNT(*) AS count_students FROM emnestud GROUP BY emnekode,grnr ) a (emnekode,grnr,count_students) WHERE (a.emnekode,a.count_students) IN (SELECT b.emnekode,MIN(b.count_students) FROM (SELECT emnekode,grnr,COUNT(*) AS count_students FROM emnestud GROUP BY emnekode,grnr ) b (emnekode,grnr,count_students) GROUP BY b.emnekode ); HTH Multiple edits: Formatting went wrong multiple times 
$lat and $lon are coming are coming from my own data so I don't need to worry about sanitizing them. They're irrelevant and I chose to leave them out because it just confuses the issue. I am running ini_set('display_errors',1); error_reporting(E_ALL | E_STRICT); And that seems to catch just about every error. But here... I just did a bunch more tests and narrowed the issue a lot more: WORKS (returns correct results): $sth = $db-&gt;prepare("SELECT * FROM providers WHERE distance('42.8','-73.9',lat,lon)&lt;25"); $sth-&gt;execute(); DOES NOT WORK (returns all providers): $sth = $db-&gt;prepare("SELECT * FROM providers WHERE distance('42.8','-73.9',lat,lon)&lt;:postalCodeRadius"); $sth-&gt;execute(array(":postalCodeRadius" =&gt;25)); 
I believe you could use the regular [restore query](http://technet.microsoft.com/en-us/library/ms186858.aspx) from your nightly backups to do something like this; it should work out 2000 compatibility automatically. Alternatively you could create a linked server from your 2008 box to the 2000 server using Read-only credentials and just have a job pull the data during a low-use period. The above is definitely a better option. 
You could look into log shipping http://technet.microsoft.com/en-us/library/ms187103.aspx Or just replication. Worst case you can probably script a database backup to a NAS, and restore from there. In my experience its sometimes quicker than trying to DTS the whole lot.
I do wonder how many people posting to /r/sql actually want a platform neutral solution. I think in many cases they just hit up /r/sql because its the first one they find, and they don't realise a more specific reddit exists for their RDBMS Beating up Coderhawk for posting a useful, but not 100% neutral solution, seems a bit anal to me. 
You can script a backup and restore and put it into a SQL agent job. If you need the code use the GUI to set your options and script it out. 
You could always create it in design view then look at the SQL code. It's easier that way. Of course you need to be able to do it in design view.
I am suposed to find the SQL code on my own. And no, I do not know what design view is :S
How is asking on Reddit any better? Anyway... On the left you have a view button. Use that to go into design view. You can set your query criteria in there. Then go back to view and select SQL view. It will give you the SQL code for the query. I use this method daily.
You are a god amongst men
On the primary or secondary server right click on the database and choose "Tasks" then "Mirror..." from the context menu. In the new window click "Failover" to initiate a manual failover. Without a witness server, this is the only way a failover will happen. Should the primary database encounter problems, you will have to do this manually. Ideally you will want to test this by initiating a manual failover and then check if all the applications are still able to connect to the database. Not all applications support failovers, you might have to edit the database connections for some. Our workaround is having these applications configured with an alias (example: mssql-server) for which we change the ip when a failover happens, then we only need to flush the dns on the application servers instead of reconfiguring these applications. "I want to know what to do if some disaster happens." Mirroring primarily protects against hardware failures and not against data corruption (bulk updates/deletes/inserts that go wrong) as these are mirrored to the secondary server. A good backup/restore policy is also needed (and familiarity with the procedure).
Good, just making sure :)
If you are asking if you should hold out for 16gb, please do. It's always harder to add than to have it in the first place. Ram is cheap these days. $100 can get you 8gb ecc ddr3 ram... But if you settle for 8gb, you'll hear 'well it was working fine with 8gb...' etc etc. For SQL, really, get the most ram you can afford, and ssd while you are at it. VM is usually not recommended because you take a storage performance hit.
1. get input 1. insert into staging table 1. human review 1. accept or reject records 1. job that insert into actual table. The staging table should have the same structure as the actual table plus a status field. Have a scheduled job that runs every 15 minutes or so that picks up the records from the staging table where it status is "reviewed/approved" and inserts into the actual table. You can always add more statuses and routes for the data in the job logic.
I may be missing something, but is this not a case of: select {extracted hour from chosen date},count(*) as num_txns,sum(units),sum(dollars) from yadda yadda group by {extracted hour from chosen date} where yadayadayada.txn_date = (select date of interest based on current value of HH24 [either sysdate or sysdate - 1] ) order by {extracted hour from chosen date}; The key part is the calculation of the 'date of interest' in the subquery (using the current value of HH24 from sysdate). When the date of interest has been decided upon, it gets fed up to the outer query as a discriminator. Then the outer query aggregation supplies all values for the date corresponding to 'date of interest'. Forgive the pseudo-SQL - I hope this makes sense!
Something like this should work..the which date returns the current hour if it isn't 00, or yesterday at 11pm if it is. Then the select returns all the values where start_time &lt;= curdate, and start_time is the same day as curdate WITH whichdate as ( SELECT CASE WHEN trunc(sysdate,'HH24')= trunc(sysdate) THEN trunc(sysdate-1/24,'HH24') ELSE trunc(sysdate,'HH24') END as curdate FROM dual) SELECT count(*), to_char(start_time,'hh24') FROM yourtable INNER JOIN whichdate ON start_time&lt;=curdate AND trunc(start_time)=trunc(curdate) GROUP BY to_char(start_time,'hh24') ORDER BY to_char(start_time,'hh24')
SQL consumption is dynamic, but you can't hot-swap ram, so it would require some downtime and reconfiguration to add ram. As far as what makes it harder - it's usually the admins, they are reluctant to give you more than you need, and starting with 8 then saying you need to double it later just seems like you didn't plan ahead enough, and chances are they will balk at having to go back and reconfigure the vm.
Data Entry Operator http://hiring.monster.com/hr/hr-best-practices/recruiting-hiring-advice/job-descriptions/data-entry-operator-III-job-description.aspx
Not familiar with Amazon EC2 but you might be able to add it as a inked server using SSMS. Then create a job that dumps the data. Google how to create a linked server in SSMS on google. It's easy and it might help you. Other wise you might want to get a skydriv account and add skydrive folder to each machine and have jobs created that backup to that drive then allow ample time for the folders to sync then a job to restore from the synced folder on the destination machine. It's kind of a hack way to do it I guess but all I can think of at the end of the day haha. But definitely look into linked servers first. If that works then there's a simple query I can give you that will take all table data and copy it to the other server. 
How about one of these: ETL Data Integration Engineer ETL Architect ETL Developer Take your pick. 
I did a recursive SQL for the same deal let me dig it up if you want it. Just out of curiousity what's the application? I ask because ours is our Job Scheduler. Same deal, we have a jobmst table that associates an ID with a Parent ID but you can't associate a child of a child of a child unless you link up those parent IDs going up.
You need to create a linked server if the database is in another instance. It doesn't look like you set up your linked server options correctly.
What's the error when it fails? If you can get even tomorrow would be ok. I'm guessing one of the entries is causing it to crap out may be an inconsistency in the db. I know I've experienced the same thing. See if you can put limits in your recursive query don't try to grab as much data at once, start slowly adding columns or remove WHERE clauses until it fails to try and narrow down what the issue is.
Ask HR. If you end up putting something with Engineer or Architect on your resume and they have you listed as technician or something like that, you can kiss that job goodbye.
Thanks. I'm not wanting the title to actually use as a title. I don't really care about that sort of thing too much. I'm just trying to see what I'm worth by researching comparable positions...and realized that I have no idea what I'm 'called' in the business world. I only know MSSQL (for now)...but I know it pretty freaking hard. I super know my way around multiple servers full of multiple databases. The problem is that I'm self taught and basically figured out everything I know at this point and worked myself into a position I created at my company troubleshooting and fixing data issues then moved into the integration part of it....so I don't have any sort of formal knowledge on what I would be called. 
I just Googled the error and found this...pretty sure this is the error: "The statement terminated. The maximum recursion 100 has been exhausted before statement completion." What do you mean be inconsistency in the database? I know the data I'm working with is kind of sloppy, so could that possibly be caused by an infinite loop (ex. if for whatever reason entity 12345 has a parent 67890, and that parent 67890 has a parent of 12345)? I haven't checked if there are any instances of that, but I know that infinite loops can cause these to crap out. And yea, I tried to refine the query to reduce the data it looks at, but gave up after receiving some errors and decided to call it quits for the day. If I can reduce the population to the seven specific ultimate parents we're filtering out for as well as distinct combos that should hopefully help...I'm going to give that another shot tomorrow.
Excellent point! I don't ever plan on leaving my company. I'm EXTREMELY happy. I just want to have my shit together when I ask for a raise!
This is what you need to add at the end of your query - option (maxrecursion 0) You're just going past your max :) Easy fix. Guarantee it will work tomorrow with this at the end.
&gt; The key part is the calculation of the 'date of interest' in the subquery (using the current value of HH24 from sysdate) Right, it's that key part which is the trick, passing the midnight hour. The current value from sysdate only works until you run the query at, say, five after midnight. Then it'd give you zero rows of data instead of the whole previous day. Mazerrackham is on the right track, I think. 
How do I do that?
Yes, this let me think of it in a simpler way, thank you! I was at the point when my brain wouldn't let me see things simply, know what I mean? But you showed me basically that I should have used one case statement to determine the hours _beforehand_. So I ended up with this: with ALL_HOURS(day,hour) as ( select CASE WHEN to_char(sysdate,'hh24') = '00' THEN trunc(sysdate-1) ELSE trunc(sysdate) END as day, to_char(trunc(sysdate) + (rownum-1)/24,'hh24') as hour from dual where trunc(sysdate) + rownum/24 &lt;= (select CASE WHEN to_char(sysdate,'hh24') = '00' THEN sysdate+1 ELSE sysdate END from dual) connect by level &lt;=24 ) select stuff from ALL_HOURS H, Transaction_Joins J where trunc(J.transaction_date) = H.day and to_char(J.transaction_date,'hh24') (+)= H.hour; The outer join (+) also lets me have null fields in case there are no transactions during any hour. 
Don't feed the trolls. If it's not a troll it's a pedant. Either way I wouldn't feed it.
[Use this to get started](http://technet.microsoft.com/en-us/library/ff772782.aspx) - one word of advice. You need to use a SQL-authenticated account, not a domain account for this.
You could even ask in a way that implies you already know your job title.. More or less. Like "what is the specific wording of my job title?"
Data Migration specialist?
I went through the same thing once as I was switching technologies and accounts so often for a while. I eventually just changed it to Technology Consultant and left it. That pretty much covers it all and I've never had anyone ask otherwise.
The guys at work call me the sql sensei. I do integration, reports, or whatever other database stuff is needed, excluding high level DBA stuff. Unfortunately, my official title is the lackluster Developer 1. Whee. If it is just how you refer to yourself in conversations, you have options. If you are looking to update your business cards, I agree with those who say to confirm with HR. Good luck, "The Integrator"
really? Are you saying add an after update trigger to the staging table and when status = "approved" then insert into the real table. Then have an after insert trigger on the real table that will go back to the staging table based on some unique id and do the cleanup? I like that your trigger suggestion causes things to happen more instantaneously and that is a huge plus. I am of the persuasion that it is generally not the best idea to modify a table in a trigger for said table. You can run into some odd mutations and I guess I also like the idea of having all of that logic in one place so can easily digest and modify what is happening. That might just boil down to a preference. 
We have a team of these we call Data Analysts. I am called Database Administrator but I wear so many hats it is really only a title. My boss recently told me that they were hiring someone to help with my workload and asked what job duties they should have. My response was "What job duties do you want me to have? Should I focus on more administration, integration or development." It was determined that we need a Database Programmer.
Expandyour server objects and right-click on your linked server to script out. Compare what you did wrong to tutorial. Always good to test your linked server by running a simple select.
Security wise, the linked server option should be used with care. Unless you are using Windows authentication or mapped logins, any queries run against the linked server will have the credentials of the linked server. This could potentially elevate anyone with Public permissions on the server to SA permissions on the linked server which is a very bad thing. exec LinkedServerA.Database..sp_executesql @stmt = N'DROP DATABASE [SomeImportantProductionDatabase]'
I do almost the same thing: I call myself an Office Geek (I do light IT work as well) but officially I am a Data Support Specialist. My office is an engineering firm so Data Integration Engineer would be sort of an over statement.
Ha, my situation almost exactly. I'm working on EDI purchase orders, ship notifications and invoices. Nobody knows what to call me either!
We call those database developers where I work.
I do the exact same thing at my company and my job comes with the title Database Developer. Although it is just something on my name badge.
I've worked with SQL Server for a lot of years, and I gotta say...what in the hell is the Gum field? What are you trying to do?
On the off chance you're looking for the SQL server error messages, have a look in sys messages in the master database. No gum field there though :-) 
I assume that you are working on SQL Server. SELECT foo FROM bar WHERE qux LIKE '[AC]%' http://msdn.microsoft.com/fr-fr/library/ms179859.aspx
I'm doing this on my phone, so I can't test it. But try. Select * From table Where left(variable, 1) in ('a', 'c') Go 
I'm not very familiar with oracle, but why not take current timestamp, subtract an hour, then cast to a date. In MSSQL I would do it as: where datefield = cast(dateadd(hour,-1,getdate()) as date)
That last one should've worked though? Does it give an error or anything? 
It looks like you don't have the required permission granted is this the only query it comes up with this error? if no query works you try look at [this](http://dev.mysql.com/doc/refman/5.1/en/grant.html) using your root users if you dont have access to root contact your mysql provider
Thanks 0r10z I will take a look on this feature
I have used that to get hold of the GRANT Syntax before hand and [I have changed the permissions on my hosts website](http://i.imgur.com/iJDMqEV.png), and it still doesn't work :( **EDIT** Created another username to see if that would work, that's why it is different.
I think he's talking about fields with like %gum% in his table. I totally googled the shit out of 'sql gum table'
The oracle version of this is: where datefield = sysdate - 1/24 Yes, this seems to work as well. with all_hours(day,hour) as ( select trunc(SYSDATE - 1/24), to_char(trunc(sysdate) + (rownum-1)/24,'hh24') from dual where trunc(sysdate) + rownum/24 &lt;= (select SYSDATE - 1/24 from dual) connect by level &lt;=24 )
LMGTFY! http://technet.microsoft.com/en-us/library/bb522522(v=sql.105).aspx http://stackoverflow.com/questions/18744705/format-in-tsql-of-simple-merge-into-query-using-case
Thanks! I have already read the technet page about this, and I have successfully implemented it. My question is really about idiosyncrasies of the operation. Common mistakes, situations where other operations might be more appropriate, etc. Have you used "Merge Into" yourself? What are your experiences SaintTimothy? 
I have not used MERGE into and... I likely wouldn't trust it as much as doing separate INSERT, UPDATE and DELETE statements to sync the things.
What about Bio **is** null
SELECT Students.FirstName, Students.LastName, Students.Bio FROM freemen.Students WHERE Bio is null
Yes, I have also checked in my connect file that connects to the database, that the directory, username, password and database are all selected and correct. It does look like I would have to contact my host about it.
Thank you, thank you!
"IS" Thank you so much.
Sometimes null isn't enough. It depends on your dataset, but in MSSQL I tend to do this instead - WHERE IsNull(bio,'') = '' What this gets you is anything that is NULL *AND* anything that is empty-string.
COALESCE has more support across multiple database systems than Isnull... just sayin' :)
Newbie as well, but i thought select * was a big no no, since once you change the data the statistics get all messed up and risk data integrity issues...
I've never used RazorSQL, but it's going to be on you to get used to Googling for syntax. That isn't something you're just going to be able to copy/paste and expect that it'll work.
i've always thought it was weird that they don't just let you use = null.
 select t1.id ,t1.parentid ,t2.status as parent_status ,t1.status from table t1 left join table t2 on t1.parentid = t2.id where (t1.parent_id is null and t1.status = 4) or (t1.parent_id is not null and t1.status = 6) 
(this post may contain inaccurate information) The first one is for ALTER and will implicitly provide a constraint name. The second one is for CREATE and explicitly provides a constraint name.
| [CONSTRAINT [symbol]] FOREIGN KEY CONSTRAINT and symbol are optional. If you do not provide a name one will be auto generated. http://dev.mysql.com/doc/refman/5.1/en/create-table.html
Thanks much. 
This guy speaks the truth. This should give you a head start: SELECT Id, LEVEL, status, desc FROM table1 CONNECT BY ParentId = PRIOR Id START WITH ParentId IS NULL
This is probably a good case for log shipping &amp; read only mode. Replication adds some additional headaches. 
Thanks for the link. my company will going to MySQL next year.
If you have not made the switch yet I might recommend MariaDB instead of MySQL. MariaDB is the original core MySQL team who forked the codebase after Oracle bought MySQL because they were not happy with the direction it was going. MariaDB is starting to gain traction and with the original team working on it I think it will become a better product.
Subtract an hour first (dt &gt; trunc(sysdate-1/24) and dt &lt; trunc(sysdate-1/24) + 1) or use a case / some business logic to figure out the dates dynamically. 
Yep, when I was originally querying there were some data inconsistencies which had the same IDs for the parent and child which was exploding the data. Once I filtered that out and reduced the rows a bit I was able to get the results we were looking for. Thanks for the reply.
The nice part is you can filter them in the cte and make less work for the database. 
The difference is when you apply them, ALTER Adding foreign keys can be done after tables have all been created and thus, the order when creating the tables and constraints does not matter. If you use the constraint method while creating the tables, the order in which you create the tables will matter since doing it in the wrong order will break Foreign Key reference rules (ie: you can't create a foreign key if the primary key it references does not exist) Constraint method saves time because you don't have to write multiple ALTER statements but suffers from ordering the create table correctly and cannot be done after the tables have already been made. ALTER method allows you to apply constraints in any order since the tables and their primary keys have already all been created, but suffers from additional line coding and redundancy. Of course, this assumes there are multiple foreign keys that need to be made. If not, there's barely a difference between those two methods other than writing an ALTER command. Both methods allow you to specify a FK constraint name which is why they're both commonly used compared to the third method (in line with the attribute names when CREATE-ing a table) which will create its own FK constraint name with random characters n such. 
This is what I have so far....not right though..its not specific for one country all I think that im asking in this is...I want the name, continent, population in which a continent contains pop&lt;25000000 SELECT name, continent, population FROM world WHERE continent IN (SELECT continent FROM world WHERE population &lt; 25000000)
--- This is wrong, but I still think there's worthwhile information in it so I'm leaving it up --- --- Start with getting the continents with a population under 25000000 SELECT continent FROM world y GROUP BY continent HAVING SUM(population) &lt; 25000000 Then you want to get the countries who's continent is in that list SELECT * FROM world x WHERE continent IN (SELECT continent FROM world y GROUP BY continent HAVING SUM(population) &lt; 25000000) THEN you want to notice the error they made if you look at their correct answer. if you SUM(population) for all of them they are all greater than 25000000 so you get nothing in our query looking at the correct answer it seems they want the countries in oceania. So I took the first query to get Oceania's population. Put that in the 2nd query (I added +1 so we wouldn't exclude Oceania) and it says it's correct SELECT * FROM world x WHERE continent IN (SELECT continent FROM world y GROUP BY continent HAVING SUM(population) &lt; 37061829) 
this is close, everything looks good besides the sub query. The problem is that you want the sum of the populations of countries in a continent. to get that you need to use group by if you say SELECT continent, population FROM world you're going to get the a list of each continent several times with the population of each country in that continent. So you want to do SELECT continent, SUM(population) FROM world but then it doesn't know if you want the sum of the populations of countries that start with the letter A, or sum of pops for countries with a GDP over 10000000000, you tell it which grouping you want with group by SELECT continent, SUM(population) FROM world GROUP BY continent that should get you a list of each continent and it's population Since you want to use a where clause on the query BUT the thing you're whereing is found out after you've gotten the group (sorry if this seems weird) you have to either use another nested query SELECT continent FROM (SELECT continent, SUM(population) AS sumPop FROM world GROUP BY continent )a WHERE sumPop &lt; 2500000000 (when you use a sub-query as the FROM you have to name it. That's why there's an "a" after the closing parenthesis but there's an easier way! use a having clause! SELECT continent, SUM(population) FROM world GROUP BY continent HAVING SUM(population) &lt; 25000000000
Well, hopefully you learned something from me working way too hard on this atlantaDave's answer! 
you were right! the wording got me too. I believe yours is correct!
i like #1 by far
Any reason in particular? Another idea I had was to merge it with my lat/long table. I have a separate table with zipID/Lat/Long columns. What if I use #1, remove the "Primary" column and add "Lat" and "Long", but only including the data on the primary city name rows, and those columns would be null on alternate city rows
Definitely number 1, but add a STATE column to save you some headaches later. 
Thank you so much I appreciate your help!
Thank you for the great read and information!
great answer and thank you for helping explain it! 
Yeah, I made sure my client is fully aware that the lat/long will just be a single point in a zip code which could be really big. We're just using it to search listings "in the local area" from a national database.
The most efficient way (one table scan, no joins)... select ID, PARENT_ID, STATUS, DESCR, prior STATUS as PARENT_STATUS, prior DESCR as PARENT_DESCR from MY_TABLE where (PARENT_ID is null and STATUS = 4) or (PARENT_ID is not null and STATUS = 6) start with PARENT_ID is null connect by PARENT_ID = prior ID Note : DESC is reserved word used for descending ordering, so I'd suggest using DESCR or "DESC" (double quotes makes it an acceptable literal) as a column name. If you want to do a conditional column for STATUS, something like ...case when STATUS = 6 then prior STATUS else STATUS end as STATUS_EFF
 SELECT sum(work.hoursWorked) / (SELECT count(distinct employeeID) FROM employee) FROM work
what was the exact query you ran?
Copied and pasted what you wrote earlier... 
Both returned values. 65 for the first one, 6 for the second. The Average should be 10.8, I'm just not sure why it's not working.
 Does this work? SELECT (SELECT sum(hoursWorked) FROM work) / (SELECT count(distinct employeeID) FROM employee) FROM dual
 It's a special Oracle-only pseudo-table with one column and one row. Kind of an all-purpose thing when you want to ask the database things that return a single value like the time (SELECT sysdate FROM dual) or the next number from a sequence (SELECT some_seq.nextval FROM dual). 
Yes, you are right, however, for illustration there's no harm is use select *. If you were making an application/package/can make changes to code once deployed. Your point becomes even more true since you will likely have to go through a change control rather than just alter the production version.
Have a zip table with id, a city table with id, a state table with id, then a bridge with zip ref, city ref, state ref, square miles, and population. You'll be able to derive the "primary" by either area or population (or population density) that way.
The PRIMARY KEY is the information that should uniquely identify(no duplicates) a row, in this case it would probably the the PlaneRegistrationNo Proving that they are not in those normal form is fairly easy as there is a lot of duplicated or not required info. Even thinking about it: * Why does the Pilot have anything to do with the passenger? They should be tied to a flight. * Why does the plane have anything to do with the passenger? They should be tied to a flight. * Why is there a passenger name field when there is obviously a passenger foreign key that would most likely contain it already? Answers those questions and you should be well on your way. I'll start you off, but you'll have to figured out the rest of the information. It might go a bit farther then the scope, but its always worth considering the edge cases that will save yourself a headache later. **** -- This is the table that would hold the information about a customer, if that is required elsewhere it should link to a record here Table passenger(idPassenger, firstname, lastname, DOB, etc, etc, etc) -- This is the table that holds the address of a passenger, as they can have multiple addresses assigned to one passenger Table address(idPassenger, isPrimaryAddress, city, province, postalCode, etc, etc) -- This is the table that holds the information about a physical plane Table plane(idPlane, load_capacity, make, model, etc, etc, etc) -- This is the table that holds the information about a specific pilot Table pilot(idPilot, firstname, lastname, rank, etc, etc, etc) **** -- This is the table that holds the information about a specific flight Table flight(idFlight, fill in the rest) -- This is the table that links a passenger to a flight Table flightPassenger(fill in) **** 
The second option is not a [normalized table](http://en.wikipedia.org/wiki/Database_normalization), since a single field contains two pieces of information. 
Thank you sir i'll read over this and let you know what i understand if you want :D
Untested, but something like..... with qryHours as ( select level - 1 HOUR from dual connect by level &lt;= 24) select h.HOUR, sum(TRX) TRX_COUNT, sum(UNITS) UNIT_TOTAL, SUM(DOLLARS) DOLLARS_TOTAL from MY_DATA m cross join qryHours h where to_number(to_char(TRANSACTION_DATE, 'hh24')) = h.HOUR and TRANSACTION_DATE &gt;= trunc(sysdate) + case when to_char(sysdate, 'HH24') = '00' then -1 else 0 end and TRANSACTION_DATE &lt; to_date(to_char(sysdate, 'DD/MM/YYYY HH24'), 'DD/MM/YYYY HH24') group by h.HOUR
yes, that's just management studio express, you need sql server express
Thanks everyone, I ended up getting this as well: http://www.microsoft.com/en-us/download/confirmation.aspx?id=1695 I'm a programmer who is new to SQL and I think 2012 feels far more integrated and less like a GUI wrapper for TRANSACT and sqlcmd. 2005 and 2008 are spread out over a lot of download suites and just feel clunkier to me.
But there are so many [articles](http://www.codinghorror.com/blog/2008/07/maybe-normalizing-isnt-normal.html) out there warning not to normalize when it's not needed.
In the future if you download SQL if you get the one listed as "Express with Advanced Services" it'll come with the management studio and the database which can save you some time.
How do you find materials for this? I can only find the proctored exam.
Microsoft also has downloads for "SQL Server Express with Tools"; which includes the database engine and Management studio together. 
I'm not a super user but I can't see a short cut. In your "Case When" clause test for nulls case when Col1 IS NULL and Col2 IS Null then Col3 When Col1 IS NULL and Col3 IS Null then Col2 when Col3 IS NULL and Col2 IS Null then Col1 when Col1 is null and Col2 &lt; Col3 then Col2 when Col1 is null and Col3 &lt; col2 then col3 when col2 is null and col1 &lt; col3 then col1 when col2 is null and col3 &lt; col1 then col3 when col3 is null and col1&lt;col2 then col1 when col3 is null and col2&lt; col1 then col2 when col1 &lt; col2 and col1 &lt; col3 then col1 when col2 &lt; col1 and col2 &lt; col3 then col2 else col3 
Addtionally you can test for "empty strings" by taking out "IS NULL" and replacing " = '' " (That's a pair of single quotes)
 -- patients and time in months they've been a primary provided patient select PAT_ID, datediff(month, EFF_DATE, TERM_DATE) MONTH_DIFF from PROV_TABLE where PCP_PROV_ID = @providerID and EFF_DATE between @Startdate and @EndDate and TERM_DATE between @Startdate and @EndDate -- Number of patients doc was primary provider select count(distinct PAT_ID) from PROV_TABLE where PCP_PROV_ID = @providerID and EFF_DATE between @Startdate and @EndDate and TERM_DATE between @Startdate and @EndDate 
--Notes and explanation-- After a moment of research I learned that Coalesce has two properties of note here. 1) It will return the First value which is not null in a given column. 2) If all values (both columns in this case) are null, then it will return null. NULL is always false in a logical expression, so " Col1 &lt; = coalesce(Col2, Col1) " will be false if Col1 is NULL.
-- SQL SERVER 2008 and above.. DECLARE @T AS TABLE ( pk integer PRIMARY KEY, col1 integer NULL, col2 integer NULL, col3 integer NULL ); INSERT @T VALUES (1, 4, 3, 2, 1), (2, 5, NULL, 7, 8); SELECT MIN(f.x) as minval FROM @T AS t CROSS APPLY (VALUES (col1), (col2), (col3)) AS f (x) GROUP BY t.pk
You'll need to wrap an ISNULL around TERM_DATE, or nulls will propagate through. ISNULL(TERM_DATE, GETDATE()) between @Startdate and @EndDate
There are times when performance needs beat out any and every other concern, but with understanding indexing (I've changed an 8 hour query to a 30 second query with a couple of indexes before), coupled with SSDs when performance is really needed, the datasets will need to become HUGE in order to have denormalization make sense. Secondly, with option #2, you won't get any performance benefit - picking apart a string, row by row, will always be slower (and far more CPU intensive) than a normalized table.
Yep, thanks, I misread the OP's requirements and thought he only wanted terminated primary care scenarios (in which case null TERM_DATE predicate fails and those records are excluded).
Ok i filled out all of the additional info thanks, now just got to figure out the questions
Loop or Cursor. I did the same thing at work the other day with "Milestones" for a project. I'll adapt and share tomorrow night if no one replies.
I think you need AND branch num instead of where, to include the condition in the join. Right now it is filtering the entire data set. 
Okay, I swapped Inventory and Book and still got the same result as in the OP. I tried typing the syntax exactly as you have it in your comment, and that syntax doesn't fly in Access... so I tried to copy it best I could and used this... SELECT INVENTORY.BOOK_CODE, TITLE, ON_HAND FROM INVENTORY, BOOK WHERE BOOK.BOOK_CODE = INVENTORY.BOOK_CODE AND BRANCH_NUM = 2 ORDER BY BOOK.BOOK_CODE; Running this query also yielded the exact same results as in the OP. It shows only the rows that coincide with Branch number 2. Also yes, Inventory is the larger table, but both tables have every book code listed at least once, regardless of branch number. The inventory table can have a book code listed more than once, as it might be in multiple branches.
 select employment.job, sum(case when empoyee.ID is not null then 1 else 0 end) from employment left outer join employee on employment.job = employee.job group by employment.job 
Works perfectly, thanks! I only have one more question; I tried using the same method as above for putting 'managers' into the table, so including: sum(case when MANAGER.ID is not null then 1 else 0 end) after select, and left outer join MANAGER on EMPLOYMENT.JOB = MANAGER.JOB after the first join, however it gets a little messy. It doubles the result and adds it to both tables... How would I be able to fix that?
Ooohhhh.... ok. This has to be it. The sentence that says, "Be sure each book is included, regardless of whether there are any copies..." made me believe the question was calling for all copies in the database. Okay thanks... I'm going to email the instructor in the morning and ask for clarification... but I believe you must be correct.
So for each doctor and for each month between two give months you want the number of patients that each doctor had? What happens when the eff_date or term_date are not the first/last day of the month? Could that count as two different doctors being the primary care provider for that person during the same month? Or is it rather that you would like to know for each doctor the number of distinc patients that has been under the primary care of that doctor no matter for how long?
Awesome, that worked. Thank you so much!
You can use a recursive CTE to get the number of months to span between your date filters as an incremental iterator. Add this iterator to the EFF_DATE to get all of the months between your dates, rather than join to some ridiculous "All Dates" table. Here's some example code as to what I mean. Simply replace the hard coded dates with your parameters and qryData with your actual data. with qryIterator as (select 0 INCR union all select INCR + 1 from qryIterator where INCR &lt; datediff(month, '2009-04-16', '2013-05-15')), qryData as (select 1 as PAT_ID, cast('2009-04-16' as datetime) as EFF_DATE, cast('2013-05-15' as datetime) as TERM_DATE), qryCols as (select q.*, dateadd(month, i.INCR, EFF_DATE) DATE_COL from qryData q cross join qryIterator i) select PAT_ID, datename(month, DATE_COL) as "MONTH", YEAR(DATE_COL) as "YEAR" from qryCols q | PAT_ID | MONTH | YEAR | |--------|-----------|------| | 1 | April | 2009 | | 1 | May | 2009 | | 1 | June | 2009 | | 1 | July | 2009 | | 1 | August | 2009 | [SNIP....]
this is a classic "multiple 1-to-many in the same query" problem the solution is to push the GROUP BYs into subqueries... SELECT employment.job , COALESCE(e.emps,0) AS employees , COALESCE(m.mgrs,0) AS managers FROM employment LEFT OUTER JOIN ( SELECT job , COUNT(*) AS emps FROM employee GROUP BY job) AS e ON e.job = employment.job LEFT OUTER JOIN ( SELECT job , COUNT(*) AS mgrs FROM manager GROUP BY job) AS m ON m.job = employment.job 
stupid ms access probably wants parentheses... SELECT book.book_code , book.title , inventory.on_hand FROM book LEFT OUTER JOIN inventory ON ( inventory.book_code = book.book_code AND inventory.branch_num = 2 ) ORDER BY book.book_code; 
oh, wait, stupid oracle probably will barf on those AS keywords
Brilliant! Congratulations, you win the thread. That worked. http://i.imgur.com/9jpl4Fz.jpg Stupid Access...
"AS" is fine for column aliasing in Oracle.
I'm getting this error "ORA-00936: missing expression" and it highlights the first CHAR. I'm using oracle sql too if that impacts the syntax or anything?
thanks, couldn't remember which one it was care to comment on the logic of allowing it for column aliases but barfing on it for table aliases?
Column alias = "AS" alias or just alias. Table alias = Just alias select t.COL as MY_COL, t.COL2 MY_COL2 from MY_TABLE t
You can see how the function works here: [http://www.techonthenet.com/oracle/functions/replace.php](http://www.techonthenet.com/oracle/functions/replace.php)
The key was that you swapped inventory to the left side of the join vs the right, not the location of the condition. SELECT BOOK.BOOK_CODE, TITLE, ON_HAND FROM BOOK LEFT JOIN INVENTORY ON INVENTORY.BOOK_CODE = BOOK.BOOK_CODE WHERE BRANCH_NUM = 2 ORDER BY BOOK.BOOK_CODE; Should produce the same result, additionally SELECT BOOK.BOOK_CODE, TITLE, ON_HAND FROM BOOK RIGHT JOIN INVENTORY ON BOOK.BOOK_CODE = INVENTORY.BOOK_CODE WHERE BRANCH_NUM = 2 ORDER BY BOOK.BOOK_CODE; should also work. I say should because I obviously can't test it, but the location of the BRANCH_NUM = 2 is not why the solution worked.
Try a backtick instead of the single quote: `
No luck.
If you really want to make them scary for Halloween, write "TRUNCATE" on them.
paying 15 USD to anybody who can help me with the questions, willing to pay first provided you can prove you know what you're talking about
Web?
As a noob, I'm not sure why truncate is so scary. It's say GOTO or anything involving loops would be scarier.. Please explain?
Should have used sql injection fragments. ' or 1=1;drop table dbo.customer;--
Yes, this works, the thing is that I didn't tell the whole story: The ITM_DE var is defined in an external script with all the vars that calls a script with the inserts. To make this scenario work I need 3 pairs of quotes that's why I'm looking for an alternative or a solution.
Oh, that's right, and I forgot! 
Still after help, willing to pay
Script out your sp as alter and you will see your mistake.
That shoved a GO at the end, which saves my stored procedure, but still doesn't make the query part actually execute.
The "blah balh" code will execute if it is structured properly. Add a try catch clause and trap the error to see what is wrong with your code. Most likely you skip your logic by not meeting condition you check for.
Have a bunch of things out of sorts; **GO** basically acts like a split/commit, on the *client side* **not server**; so you're defining the procedure as that and nothing else. There must be something wrong in the definition itself, probably causes: * A mismatch/etc in the **BEGIN/END** block that is causing the error. * Check to see if you can copy/paste its contents and run them outside the stored procedure * Verify that you aren't using **GO** in the stored procedure itself otherwise it will only try to define it up to that point, which will almost certainly fail.
Also worth trying, attempt dropping the procedure (after backups of course) and recreating it. 
That's my problem; it seems to entirely hinge on that one GO. 
Realistically, you should not require a **GO** after the **AS** at all. USE [L7_PerformanceTest_mfg12] GO SET ANSI_NULLS ON GO SET QUOTED_IDENTIFIER ON GO ALTER PROCEDURE [dbo].[spUpdateWarehouse] AS BEGIN --Definition here SELECT 'etc' END
The fact that you have a fundamental misunderstanding of GO and ALTER combined with the the fact that your proc is **dbo.spUpdateWarehouse** is the stuff of DBA nightmares. Pure, unadulterated terror. Not sure if this is Halloween trolling...
I'm not trolling. I'm a SQL noob trying to maintain someone else's stuff. I actually want to know how this works, and I was hoping a SQL sub of all places wouldn't turn into the condescending snarkfest of the rest of the internet.
I apologize for the snark, I could have been friendlier about it, but the humor of it was irresistible: * dbo = great power * spUpdateWarehouse = broad scope of changes * no proc parameters, so the query inside the proc *will* run if you put a GO after the AS * inadvertently running contents of a proc when you just meant to alter the proc * proximity to Halloween Laugh with it, because it *is* a perfect storm scenario. Also, there is a valuable lesson here, don't let my snark cause you to miss it: don't be "that guy" who nuked the warehouse. Edit: I tried to be helpful in a separate reply. 
This is your problem... ALTER PROCEDURE [dbo].[spUpdateWarehouse] AS GO You do not need and should not put 'GO' after the 'AS' in 'ALTER PROCEDURE'. Where in the documentation for 'alter procedure' do you see the word 'GO' ? http://technet.microsoft.com/en-us/library/ms189762.aspx Here's the definition of 'GO'. Understand it first. http://technet.microsoft.com/en-US/library/ms188037(v=sql.90).aspx 'GO' definition talks about batches. What are batches? http://technet.microsoft.com/en-US/library/ms175502(v=sql.90).aspx 
Not to mention, super quickly, as it doesn't write anything to the transaction log when it does this.
&gt;If I remove the "GO" before the "BEGIN," the procedure will ALTER (save) but the rest of it won't run. ALTER should *only* save your changes to the proc. After you alter it, you can execute it: EXEC dbo.spUpdateWarehouse (EXEC keyword optional) 
I'm disappointed I had to go this far down to see this answer. ALTER is doing just that, altering the procedure, not executing it. Dangerous stuff, there, man. You may have seen some of the other comments as snarky, but you should have paid attention to quotemycode, he gave you the reference to to learn how and what the commands do.
Then what do I need to save? The DROP / CREATE example from here? http://technet.microsoft.com/en-us/library/ms189762.aspx 
&gt; Not to mention, super quickly, as it doesn't write anything to the transaction log when it does this. I know what you are saying and it is a very fast operation, but its *not entirely* true that it doesn't log anything. http://sqlblog.com/blogs/kalen_delaney/archive/2010/10/12/tsql-tuesday-11-rolling-back-truncate-table.aspx
According to your requirements, you could do this a number of ways. But, it sounds like you just need to be able to see where Device_Type is either 'Server' **or** 'Network' SELECT Customer, COUNT(*) FROM Nodes WHERE Device_Type = 'Server' OR Device_Type = 'Network' GROUP BY Customer ORDER BY Customer a simpler way to do this would be with an IN expression: SELECT Customer, COUNT(*) FROM Nodes WHERE Device_Type IN('Server','Network') GROUP BY Customer ORDER BY Customer But, you also want to see any customers that have zero servers or network devices, you would need to RIGHT JOIN to your distinct customers query: SELECT Customer, COUNT(Device_Type) FROM Nodes n RIGHT JOIN (SELECT DISTINCT Customer FROM Nodes) c ON n.Customer=c.Customer WHERE Device_Type IN('Server','Network') GROUP BY Customer ORDER BY Customer Someone correct me if I'm wrong, but I think that COUNT(Device_Type) will return 0 if there are in fact no Device_Type records for a customer. 
I actually need 2 lists one of Servers and one for network devices*, Basically I need to show CustomerA has x servers and x network devices for all 150 customers we have.
 SELECT distinct n.customer ,(SELECT count(*) FROM nodes n2 WHERE n2.customer = n.customer and n.device_type = 'Server') as 'Servers' ,(SELECT count(*) FROM nodes n2 WHERE n2.customer = n.customer and n.device_type = 'Network) as 'Networks' FROM nodes n GROUP BY n.customer ORDER BY n.customer
For the example at that link for your info it would look something like the following. &gt; Each of the horizontal lines are break points where the above query block runs, you can think of it as copy/pasting each into the same query window and running them. | -- Set the context to use the required database USE L7_PerformanceTest_mfg12 GO **** -- Check to see if the stored procedure exists IF OBJECT_ID ( 'dbo.spUpdateWarehouse ', 'P' ) IS NOT NULL -- Drop the procedure DROP PROCEDURE dbo.spUpdateWarehouse GO **** -- Recreate the procedure CREATE PROCEDURE dbo.spUpdateWarehouse AS BEGIN -- Start of the store procedure definition -- No GO statments should be found between this BEGIN and END -- End of the stored procedure definition END GO **** The reason the example you found doesn't require a BEGIN/END the statement is considered one line. 
This seems on the right track but I'm getting the following errors: Column 'nodes.Device_Type' is invalid in the select list because it is not contained in either an aggregate function or the GROUP BY clause. ORDER BY items must appear in the select list if SELECT DISTINCT is specified.
I've oversimplified the queries to make it easier to understand what's going on but basically I think the approach you want is to use the customer field as a key to joining your lists together. Most of the below is in setting up the appropriate data set - you're mostly interested in the last eight lines. DECLARE @Nodes TABLE ( ID INT IDENTITY, Customer NVARCHAR(10), Node NVARCHAR(8), Device NVARCHAR(20) ) DECLARE @i INT DECLARE @cust NVARCHAR(10) DECLARE @node NVARCHAR(8) DECLARE @dev NVARCHAR(20) SELECT @i = 0, @dev = 'Network' WHILE @i &lt; 4000 BEGIN if @i &gt; 1999 BEGIN if @i % 3 = 0 BEGIN SELECT @dev = 'Server' END ELSE BEGIN SELECT @dev = 'Network' END END SELECT @cust = CASE WHEN @i &lt; 1000 THEN 'Initech' WHEN @i &lt; 2000 THEN 'GlobalCo' WHEN @i &lt; 3000 THEN 'ServerLtd' WHEN @i &lt; 4000 THEN 'HomesRUs' END INSERT INTO @Nodes (Customer, Node,Device) VALUES (@cust,CAST(@i AS nvarchar(4)),@dev) SELECT @i = @i + 1 END --SELECT * FROM @Nodes SELECT CUSTOMER_LIST.Customer, ISNULL(NETWORK_LIST.Networks,0) AS [Network Count], ISNULL(SERVER_LIST.Servers,0) AS [Server Count] FROM (SELECT DISTINCT Customer FROM @Nodes) CUSTOMER_LIST LEFT JOIN (SELECT Customer, COUNT(*) AS Networks FROM @Nodes WHERE Device = 'Network' GROUP BY Customer) NETWORK_LIST ON CUSTOMER_LIST.Customer = NETWORK_LIST.Customer LEFT JOIN (SELECT Customer, COUNT(*) AS Servers FROM @Nodes WHERE Device = 'Server' GROUP BY Customer) SERVER_LIST ON CUSTOMER_LIST.Customer = SERVER_LIST.Customer
PERFECT, Exactly what I needed! +/u/bitcointip ฿0.01
 DECLARE @temp TABLE (CustomerName VARCHAR(64), CountServers INT, CountNetwork INT) INSERT INTO @temp (CustomerName) SELECT DISTINCT n.Customer FROM Nodes AS n UPDATE t SET CountServers = (SELECT COUNT(*) FROM nodes AS n WHERE n.Customer = t.Customer AND n.device_type = 'Server'), CountNetwork = (SELECT COUNT(*) FROM nodes AS n WHERE n.Customer = t.Customer AND n.device_type = 'Network') FROM @temp AS t SELECT * FROM @temp 
Thank you. I'm glad that I could help. Sorry I borked it up the first time around. My brain starts to shut down after 5pm :)
I'm still getting used to SQL kinda got thrown into the fire with it. Although I'm really enjoying it thus far. Thanks again.
nice 20 dollars tip
you're right, my sources or my math lied to me, nice gesture anyways, cheers
Thanks! I'll do that. 
 Sum( Case when foo='network' then 1 else 0 end) as network, --do same for server Group by name Is a beautiful thing. You can output 3 columns. Name, networkCount, serverCount.
20$ it's up to for help now
I might be confusing his point, but most parses just validate syntax and parameters. To my knowledge a SQL injection type statement, includes the authorized code plus malicious code, will parse and execute. It doesn't know whether you meant to insert multiple statements or not.
That's not what it suggests at all. It merely describes string concatenation, which is the low-hanging fruit of SQL injection. So, the code is: SELECT Id, Name, AlcoholVolume, Description FROM Beers WHERE AlcoholVolume &gt; $volume And the input $volume is a string containing "1; DROP TABLE Beers;" In pure T-SQL on MS SQL, you do this: DECLARE @SQLString nvarchar(1000); DECLARE @SQLParamaterDefinition nvarchar(500); DECLARE @InputVolume nvarchar(255); SET @SQLString = N'SELECT Id, Name, AlcoholVolume, Description FROM Beers WHERE AlcoholVolume &gt; @volume'; SET @SQLParamaterDefinition = '@volume tinyint'; SET @InputVolume = '1; DROP TABLE Beers;'; EXECUTE sp_executesql @SQLString, @ParmDefinition, @volume = @InputVolume; The most complicated thing you'd have to do here is quote escape @InputVolume. And that's if you wanted to run it in pure SQL. And you can fix even this level of problem by replacing: DECLARE @InputVolume nvarchar(255); With: DECLARE @InputVolume numeric(4,2); Since ABV is unlikely to be anything other than a number between 0 and 96, since those are the literal limits of distilling, and highly unlikely to be above 25, which is a practical limit of an undistilled fermented beverage like beer. Any reasonably useful language has their own parameterized interfaces that do this for you. As far as I can tell all they're showing is that JSON is shitty when you want to query it because hierarchical trees are a pain in the ass. Because of this, programmers tend to be lazy about how they interact with JSON, and that creates problems! He is right on page 26. Developers *don't* treat SQL like a programming language. That's why we get shitty applications that fetch rows one by one, perform an operation application-side, and then insert the results row by row. I would even go so far as to say that *most* applications I've had to support as a systems analyst are guilty of this. It's certainly easier for a developer to write code that remains procedural rather than changing to declarative programming. I'd argue it's one of the biggest "not invented here" problem in programming. Well behind all the people willfully ignoring all the frameworks and libraries that also already exist, of course. Computer science is the only profession where people see a problem that other people have solved a decade ago and immediately think the version they wrote in 20 minutes is better than their 10 year project that's dedicated to *just that*.
&gt; Include carriers that do not have any customers in your results as well. use LEFT OUTER JOIN
Thanks, even though I did not include the ERD like I originally thought haha. It works thanks, so simple. I forgot about outer joins :)
Thanks for commenting. The author is talking about a controlled full parse that retains all details of the parse. So you can decide which parts of the language you will accept (eg a single SQL statement). And a parse, when successful, reveals all the details of what matched. (So if you allowed for arbitrary SQL, you could know that it contained, say, two statements.)
Look into SQL Server Integration Services--it comes with SQL Server Standard and Enterprise. It has a "Fuzzy Matching" algorithm that we use to help clean the data. If it's something you have to maintain over time, look into SQL Server Data Quality Services (also comes with SQL Server). SQL queries can use things like [Levenshtein distances](http://en.wikipedia.org/wiki/Levenshtein_distance), but those aren't spectacular.
You could always try soundex() to group them together. 
Be careful when using `soundex` on phrases (such as first and last names), as it does not take into account vowels (including y). It can get a little wonky when feeding it multiple words, but is definitely better than nothing.
It actually DOES save: as nothing The GO acts like a split, so your statement: ALTER PROCEDURE [dbo].[spUpdateWarehouse] AS (there is nothing here) GO (GO meaning finish the previous statement and now do this statement) So with nothing being between AS and GO the procedures is altered to '' (there is nothing there). The ALTER statement DOES NOT run the procedure. This is so you can safely modify a stored procedure. There are lots of stored procedures which you would not want to run during business hours, now image you would want to edit that procedure before it should be run at 04:00. If you want to run the procedure just type "EXEC [dbo].[spUpdateWarehouse]" and you will get the results If you want to modify the procedure just type "alter [dbo].[spUpdateWarehouse] as (statement here)" and it will say "command completed" 
Is there a reason you're not fixing your data model to normalize the data? e.g. Use **SELECT** DISTINCT ProviderName **INTO** new_provider_table then adding an FK to whatever table you're referencing them from? If you did that it'd be a relatively simple UPDATE for each different name spelling, link all the names that are the same to the same one going down the new table and get rid of duplicates.
&gt;If you want to run the procedure just type EXEC [dbo].[spUpdateWarehouse] and you will get results If you want to modify the procedure just type EXEC [dbo].[spUpdateWarehouse] and it will say "command completed" Wait, those last two are the same command?
Thanks, I finally fixed it. I was pretty confused because the EXECUTE button doesn't actually behave the same as querying EXEC &lt;procedurename&gt; xzibit.jpg
I second that motion.
&gt; I was pretty confused because the EXECUTE button doesn't actually behave the same as querying EXEC &lt;procedurename&gt; Yeah, I can get that... Execute exec &lt;procedurename&gt;
Although there are standards for this, isn't that a matter of choice? I use plural forms, because for example each row in table users contains a single user. Same with roles, groups, etc. On the other hands, some frameworks (notably CakePHP) force you to use plural forms, because of automatic inflector utilised in its ORM. While I like being able to choose it to your liking, forcing someone feels bad.
sorry, i have already beat myself up enough times this week, and it's only monday you'd have to pay me to wade through that shit besides, the "i agree" button on that page does NOTHING
table names in general should be plural developers need constant reminders that they are ~not~ accessing a table's rows one at a time (as in SELECT... FROM employee), but rather, they are accessing a **collection** of rows, and nothing says that better than the plural (as in SELECT... FROM employee**s**)
Well, the short of it is "singular", but no one's going to burn down your office as long as you don't use escaping and quoting to make completely insane table names with symbols, quotes, spaces, etc. in them. Maybe you can get to the standards starting from here: http://www.iso.org/iso/home/store/catalogue_ics/catalogue_detail_ics.htm?csnumber=1758
I worked for a place where the table name wasn't so important (I mean they did stress consistency in the naming of tables) but the field names, oh my lord the field names. For example... If there was a table called "EmployeeHeader", then the field names had to be EmployeeHeaderID EmployeeHeaderFirstName EmployeeHeaderLastName EmployeeHeaderMiddleInitial I couldn't (and still can't) fathom why it was required that we do this. It makes my head hurt thinking about it.
OMG, stabby. Really stabby at the person who wrote this. Get the subselects out of the SELECT and WHERE sections. Srsly.
RBAR is what's killing you.
I think it depends on the data. If a table stores metadata about a collection of documents or files, I think it's perfectly acceptable to call it Documents or Files. If a table stores is acting as a queue, I'd prefer ImagingQueue over DocumentsForImaging. I don't think any hard-and-fast rule is going to fit all scenarios. I think a good naming schema is any one that: * is well documented * is internally consistent * uses your organization's style and coding conventions * will allow someone of reasonable intelligence and experience to understand ~75% of the structure, the first time they look at it
 Middle_Name=ISNULL(p.mi,''), Sex=CASE WHEN p.sex is not null THEN p.sex ELSE 'U' END So he know ISNULL is a thing, but sometimes he just feels like typing nine extra words? This is my SQL pet peeve (though I'm an Oracle guy, so it would be NVL instead.) Like SaintTimothy said, though, the **huge** problem is the subqueries in the SELECT and WHERE clauses. It's incredibly inefficient. 
Please let him calculate how many mb/gb he's wasting with the ' ' 's. I think that alone is a reason to fire him.
I don't want to say this is obscene and vulgar... but there's no other way to finish that sentence.
I don't believe this is too hard of a task - but I believe you may need to restate exactly what data you want pulled. Feel free to PM me if you'd be willing to restate what you've already said. It seems like a few case statements, a few joins, and a few where statements will take care of it. 
I suggest you look into using hierarchyid field types. Build your hierarchy structure, then you can use IsDescendantOf() in a join clause to go top-down to find descendants, or in your case you can filter for top of the hierarchy trees only by using "GetLevel() = 1" in your WHERE clause.
what the fuck am i reading
Some companies frown on pasting company code in public. Be careful about doing so. I know I could be dismissed for doing the same. But to provide some constructive criticism that hopefully you can use (just saying that the code makes the internet want to stab the author won't get you very far, I suspect) People have already mentioned that the subqueries are terribly inefficient. It certainly doesn't get better when you look at what is in the subquery: WHEN (Select count( distinct eGroup_Var) [from subquery] = 1 THEN... ) Also in the subquery WHERE .... CASE WHEN (tr2.xferEffDate IS NOT NULL) THEN convert(varchar(10),tr2.xferEffDate,101) ELSE p.STARTDATE END between convert(datetime,convert(char(8),Start_Date_INT)) and convert(datetime,convert(char(8),CASE WHEN End_Date_INT = 0 THEN 21001231 ELSE End_Date_INT END)) Even if there is an index on the dates, the convert statements will make them useless. ... and eGroup_Var in ('T19','DD','SDI','DDD')) = 1 THEN '19' It looks like you are trying to build a mapping between one set of values and another. I hear that in databases, you can put that stuff in a table and join on it. Odd that a database developer hasn't heard of that before. Oh look, there it is again: Suspend_Pmt=CASE WHEN s.stageID = 1 THEN 'MP' WHEN s.stageID = 2 THEN 'ME' WHEN s.stageID = 3 THEN 'MD' .... You could just use OR p.ClosureDate IS NULL, rather than dummying up a closure date that is 80+ years in the future. ELSE p.StartDate END &lt;= CASE WHEN ... THEN dateadd(dd,-1,tr1.xferEffDate) ELSE ISNULL(p.ClosureDate,'12/31/2100') END I'm really troubled by the presence of all these codes that don't appear to be any sort of key field. I suspect the very schema of the data is a disaster. and eGroup_Var in (' ','') ... What about 1, 2, 4 or more blanks? Or a tab character? This is why we shouldn't go putting whitespace-only in string fields. We already have to live with our choices between '' and NULL. adding multiple equivalent blank expressions is pretty hidious. And yet... T_CD=' ' OK ... I'm officially tired of looking at it now. Good luck. Nuke it from orbit, it's the only way to be sure. 
It looks like she learned everything she knows in Access, then figured out a few differences to make the leap into T-SQL. It also looks like she writes a lot of code snippits, then copies and pastes together these large mosaics with no thought put towards code reuse (or anything else for that matter). I know I'm in the right field, because after reading that crap, I want to RDP into your server and start stomping out that flaming paper bag of shit immediately, staying up day and night until it is an elegant, performant piece of work I'd be proud to sign my name to.
&gt; Because by definition, the subquery can only return NULL I suspect most of this stuff was copied and pasted from other queries, then edited to fit the need, brain disengaged all the while. 
Well that was a horrible start to my morning...
Now that I got some sleep I can offer another comment. To echo a few other's suggestions here, look at the fundamentals of a relational database. You'd find that yes a lot of these Case-When-Then chunks should come out and go into another table that you'd join on. Also, the In and Not In thing is really pretty bad (not only for performance) but because if one of those lists appears more than once or if something shifts from an In list to a Not In list, you'd have to go through and make sure you updated your code for every time those values appear. I'm also wondering what the query execution plan for something like this even looks like. That's just out of curiosity though.
I'm not going to comment on that code. In fact, I won't read any more of it unless you paid me. I will say, I'm sorry for you. I feel your pain.
Thank goodness they saved you typing a "." in every query! at least it wasn't: tblEmployeeHeaderID.tblEmployeeHeaderID
actually, it would've been tblEmployeeHeader.ID
On a quick guess I'd expect that SQL Server has to do an implicit conversion from the smaller type to the larger one, as that's the only safe conversion. Otherwise it might be cutting 75 characters off the actual values. So A.account has to be converted from 25 size to 100. Your solution is based on the fact that you know you can do the reducive conversion. And I'd guess that there are less records in B than in A. Thus the explicit conversion on B.account takes much less time than the implicit conversion of A.account.
Do not post the all code unless you get a get out of jail free card from your boss. If you do post without permission and it can be traced back to you, the "architect" may have the last laugh as you are escorted out the door and handed a lawsuit. If it is publicized or known you did it, you may never work in this field again. I feel your pain. My eyes were burning while I was reading the code. How old is the code? What is the history of the code? If you play your cards right, you could get yourself a promotion out of this. If not, at the very least, you will gain respect and be a go to person. Is it your job to fix this or are you taking it on as a personal project? **WE** have all written crappy code so I have a tendency not to throw rocks. You asked "how [do] you handle the individual that was writing the code". I have come across code like this in the past and have been asked this same question more than once. I would recommend that you find a section of code or two that you can fix relatively quickly. Determine how fast your new section of code is compared to the current code. Put that time saved into money saved. Bosses like to save money. Present your findings and solutions to the boss. Don't trash the current code or call people or things names. Take the high ground. Your boss and HR don't want to deal with personal attacks. Bosses want solutions to problems. The problem here is the code is not as efficient as it could be for what ever reason. Let your efficient code speak for itself. I am sure your boss or their boss will let you rework the code to make it more efficient once you present your solution to them. Here is something to think about once you are given the green light to fix the code. In fact, your boss may ask you to do this as a condition of you fixing the code. Try to determine if the "architect" can handle a review of her code. If she can handle a review or questions about her code, ask her why she did something a certain way. Don't call her or her code names. Maybe there is history you are not aware of. Maybe she meant to go back and fix it but wasn't given the resources to do so. Maybe she knows it's crap but doesn't know how to ask for help. The review should be between you and her only. See if she is willing to work as a team with you being the leader of the team. Maybe her only input is the history of the code. Tell her you are not attacking her or her code. Tell her you want to see if we (you and her) can make the code more efficient and you need her help, even if you don't really. She maybe scared that she will be out of a job. That might very well happen to her. Be prepared for her to trash you, your ancestry and your code. Be prepared to defend your code civilly. The bosses already know there is a problem with the code or the process. But no one has given them a solution. Give them a solution. They won't forget. This is a make or break career opportunity for you.
Yuck. Linked server will do a CX_Packet wait on both sides until all has transferred. Rather than do this, it may be better, depending on how important accuracy in timing is, to throw up a SQL Job that pulls that small dataset across to your side every hour or so, and then query against the local version.
I don't find it relatively logical though. Qualifying by table names can be made very short because I could just alias each table in the query (which is the proper thing to do). I think he was just taught poor conventions and wouldn't change. He also insisted that the project I was working on be developed in VB6 and not .NET (and this was 2006). His logic? Nobody wants to download the .NET Framework.
is your database local or on a server?
Remember, code speaks for itself. Do what you can without making enemies. Use this experience to your advantage. It would look great on your resume if you can say you trimmed 1.2 million lines of code and a 10 hour process time down to 10 minutes of processing time and 1,000 lines of code or similar. I have confidence you will be able to make the old system sing. **When** the new system falls and they come to you again to save them, you will be in the driver's seat. I know it is frustrating watching incompetence be rewarded. I have seen it too many times. But believe me, it will catch up with her. When it does come crashing down around her be there, be ready to save your boss and the company. Do what you are asked to the best of your ability. Let your work speak for you. When the big fail happens again, don't tell them, "I told you so", let your work do that.
Just use a Left Outer Join and ignore the valid area matches. SELECT * FROM assignmentLog A LEFT OUTER JOIN incidentLog I ON A.officerID = I.officerID and A.week = I.week WHERE I.incidentArea IS NULL -- no work OR ( NOT A.dutyArea = I.incidentArea AND I.incidentArea IS NOT NULL ) -- out of Area work
*Skip to the last section if you don't care for explanation.* There are a couple things that make this complicated (still doable): 1. The assignmentLog doesn't appear to be one row per assignment. It's one row per assigned area per assignment. To get the entirety of an officer's assignment, you have to look at all the records for that officer with the same week value. 2. The only way to determine if an assignment has ended is that a record exists for the officer with a greater week value. In an ideal world, we'd have two tables: assigmentLog(**assignment_id**, officerid, start_date, end_date) and assignedAreaLog(**assigned_area_id**, dutyarea, *assigment_id*). However since you have no control over design, let's work with what we have. _____ To fix the 2nd problem, we need to calculate end_week using the [LEAD analytic function](http://docs.oracle.com/cd/B19306_01/server.102/b14200/functions074.htm#i83834) minus 1. However, the first problem complicates this, as the next value for an officerid could be another area instead of the next week. We'll need a few subqueries to fix that: SELECT a1.officerid, a1.start_week, a1.end_week, a2.dutyarea FROM (SELECT officerid, week start_week, LEAD(week, 1) OVER (PARTITION BY officerid ORDER BY week ASC)-1 end_week FROM --SELECT DISTINCT because of problem 1 (SELECT DISTINCT officerid, week FROM assignmentlog) ) a1 INNER JOIN assignmentlog a2 ON --join back to table to pick up dutyarea a1.officerid = a2.officerid AND a1.start_week = a2.week ; _____ After that, it's a pretty simple LEFT OUTER JOIN from incidentlog: SELECT i.* FROM incidentlog i LEFT OUTER JOIN (SELECT a1.officerid, a1.start_week, a1.end_week, a2.dutyarea FROM (SELECT officerid, week start_week, LEAD(week, 1) OVER (PARTITION BY officerid ORDER BY week ASC)-1 end_week FROM --SELECT DISTINCT because of problem 1 (SELECT DISTINCT officerid, week FROM assignmentlog) ) a1 INNER JOIN assignmentlog a2 ON --join back to table to pick up dutyarea a1.officerid = a2.officerid AND a1.start_week = a2.week) a ON i.officerid = a.officerid AND i.incidentarea = a.dutyarea AND i.week BETWEEN a.start_week AND NVL(a.end_week,i.week) --NVL because active assigments have null end_week WHERE a.officerid IS NULL ;
thank you for trying to help, but this solution does not work.
lag is your friend. It enables us to create a week range to query the incidents against. **Edit** previous post was incorrect. I was not factoring in multiple areas. listagg can aggregate the areas into a string to compare against. select i.officerId, i.week, i.IncidentArea, a.dutyAreas from incidentLog i join ( select a.officerId, a.week, listagg(a.dutyArea) within group (order by week) over (partition by officerId, week ) dutyAreas, lag(week,1,week+5) over(partition by officerid order by week desc) next_week from assignmentLog a ) a on i.officerid = a.officerid where i.week between a.week and a.next_week and instr(a.dutyAreas, i.incidentArea) = 0; 
Make sure both columns are the same character set. If one is utf-8 and one is latin1 the indexes will not work properly for joining. This was my experience from mysql. Is there a MS SQL equivalent to mysql's explain statement? This would tell you why it is taking so long.
In general nchar/nvarchar joins are much slower in sql server than their Latin1 equivalents. 
If account is supposed to be numeric make it a numeric field it will drastically improve performance... If you can't the I suggest running the join through the execution plan to see if they would suggest an index for table a or b. Both things will drastically improve it. I work in a sql field where if the query including 80 million records don't come back in less than 5 seconds there is a problem. 
I would be that the one you are doing the convert on is likely doing a table scan rather than an index, as using any type of function on a join is one of the quickest ways to eliminate the usage of a join. Alter the big one to be the same size, update your indexes and I'd bet your execution plan will look nice and concise.
Trust me I wish I could. The account number that's the incorrect length is on a table I can't change or this wouldn't be an issue.
CREATE PROC must be the first statement in your query. That is, there must be a GO right before CREATE PROC. Put the DROP TABLE and error handling inside the proc.
If we are joining EmployeeHeader, EmployeeDetail, ManagerHeader, LocationUnitDetail, our default is to use the "capital" letters from the table names: eh, ed, mh and lud. When writing complex procs using dozens of field names, brevity in the aliases helps your eye scan the field names more easily. Banning aliases would likely result in developer insurrection in our shop...depends on the types of procs typical in your shop, I guess. YMMV.
i realized a lot of my grief was a simple error: I had forgotten to copy my Create Proc line from the beginning when i pasted it into my query, and never noticed so i wrote it in the try thinking it was part of the begin/end try. im an idiot. thanks!!
Isn't that why everyone loves using int as a primary key?
Sadly, sometimes that is not an option.
I'm still processing this, trying to understand it. lead and partition are new to me, so that's very cool that I'm learning something new. But on my test data that I posted above, I expect the code to find this row: officer 2, week 23, area 'A' And it's not. Am I missing something? &gt; The assignmentLog doesn't appear to be one row per assignment. It's one row per assigned area per assignment. To get the entirety of an officer's assignment, you have to look at all the records for that officer with the same week value. Yes. It's ridiculous. This is the product of some ORM - I don't know the name of it. I'm just trying to get a report out of it.
No idea... using your test data, it returns the correct entry for me. What are you seeing returned?
Just prior to the script running they drop the clustered index / primary key that orders on 1, 3, 4. The second script then re-creates the clustered index on the table with no reorg command.
Have to ask, how long has this lady been working there? Good luck with working on fixing this stuff.
8 years? She was originally the dba. Explains why all the older systems are set up terribly. 
If you listen to a MSSQL developer or admin, are there really any other tools out there? or other rdbms for that matter? :-()
Oracle, sql server, mysql are all different syntax and keywords but are very similar. Try and find sql cook book it shows examples of everyday tasks in 4 languages. Also I think w3 schools may be a good starting resource, and sql fiddle. On my phone now but I can add more info later, I am a database developer who uses Oracle and sql server. 
MySQL and SQL server are different once you are even slightly advanced. You first need to figure out the names of the tables and what table has what data. This is very important. From here start with basic sql statements such as select this from tablename. 
Try the "Joes to Pros" series. They are simply written &amp; explicitly use Microsoft's T-SQL. Good Luck.
"I heard there was an open source rdbms called postgresql, but that can't be anything more than someone's school project. No company would ever use that" -- said every mssql DBA ever 
You'll definitely want to visit [sqlzoo](http://sqlzoo.net/wiki/Main_Page) in addition to w3 schools to learn to write queries since you aren't familiar. Once you have the basics down, I'd start by asking for access to the sql server from whoever is managing the server. You should only need select access and maybe view definition, it shouldn't hurt to ask for that. Since you stated you are running "Windows SQL" you will want to [download management studio](http://www.microsoft.com/en-us/download/details.aspx?id=29062). That will let you connect and give you an interface to view the database, tables, etc. Youtube how to use it, there are tons of tutorials. Google everything, you will probably spend a lot of time on [stackoverflow](http://stackoverflow.com/) looking for answers. If you want to learn more as you go along, go to [PASS](http://www.sqlpass.org/) and create an account and then you can get all sorts of free training in the learning center. Does the language matter? Yes, it does. The syntax is different among all the varieties of database systems. Some things are similar, and others aren't. The engines all work differently, so even if the syntax is the same, the performance of the same query may not be the same given the same data, structure, and system specs. 
My coworkers love....squirrel. I hate it. Heidi is pretty good tho
Start of with w3schools. It gives you a very basic idea of how SQL works. After that, you can get a book on SQL Server and start off from there. You might have a schema or some place to experiment where you can create tables, views and such to get an idea of how things work. Best of luck. 
100 tables or 100 databases? [This](http://www.codeproject.com/Tips/459067/SQL-Server-How-to-Get-All-Indexes-List-With-Involv) should help. Then you'll need to modify the Select so that it produces a set of CREATE INDEX and Drop INDEX statements. You can copy this output to a textfile then run it when you need it. If it's 100 databases, it would be worth writing a small program instead of doing it all manually. 
100 databases, 3 tables per database that would need to have this done. This is a great link I'll see what I can do with it
Agreed, calling it a 'dirty little secret' is rather disingenuous
Basically the same, but don't dick around with a pointless self Join. select * from Events E where E.eventdate = (select min(e2.eventdate) from events e2 where E.clientid = e2.clientid) NOTE: If your clientid can have two simultaneous events (and they are the MIN), you will get duplicates.
So I was reading up on correlated subqueries (didn't know they existed. lol) and technically wouldn't it take longer since the inner query has to be executed for each, in this example, clientid? Or do you think that initiating the join would take just as long?
&gt; notoriously inefficient lol I guess that answered my question about them being slower because of essentially 'looping' through info. 
Did some searching and supposedly this is more efficient: select E.* from (select * ,row_number() over(partition by clientid order by eventdate) as rn from Events ) as E where E.rn = 1
Any Tools to access multiple databases with one tool? oracle MSSQL Postgress MySQL ? What comes the closest to a Universal tool? 
First google [hit](http://www.sqlteam.com/article/computing-the-trimmed-mean-in-sql) for (sql trimmed mean) looks pretty good. You have to decide how you want your data trimmed then the site has a SQL sample.
Would it be easier to just connect via a program like cryatal reports? 
That's why row number is better for this situation.
I don't remember the syntax, but you should be able to create a libname reference to the specific Teradata database and then use that in the proc sql. Like so (connection syntax is probably not correct): libname teradb tdpid=dbc user="MY_ID" password="PASSWORD; proc sql; create table TR_WORK_LOG as select cols from RICK_TEAM_SA a left join teradb.V_TR_WORK_LOG_BELL b on a.col = b.col; quit;
Pass-through SQL is MUCH more efficient than using the libname engine, but you cannot join to a SAS dataset inside of a pass-through query. Depending on how small of a dataset you're pulling back, it wouldn't be noticeably slower, and would be much easier to code. If you'd like to go down the rabbit hole though, you can start playing around with passing the values that you're joining on into macro variables; the pass-through query will be able to understand those at the time of execution.
Try this: INSERT INTO aeriestable.dbo.FRE (ID, CD, UC) SELECT ID, CD, UC FROM stagingdb.dbo.districtname_NKAeries2 WHERE .....
Thank you :-) I didn't think/know it was important, but there are other fields in the Aeries table that have no data and at least one is barking about a null field, even though I don't think we're doing anything with the 'SQ' field. Output of the query you supplied (into copy of original table) is: Msg 515, Level 16, State 2, Line 1 Cannot insert the value NULL into column 'SQ', table 'StagingDB.dbo.test'; column does not allow nulls. INSERT fails. The statement has been terminated. Is there something I should add or change?
Yup. Pentaho is nice too. You still need to be able to log in and know how to write the correct SQL though. 
It sounds like there are already records in the table and you need to do an update rather than an insert. Or are there some new records in the staging table that don't exist in the target table? You don't update the ID field, but you can do something like this: MERGE INTO stagingdb.dbo.districtname_NKAeries2 a USING dbo.districtname_NKAeries2 b ON a.ID = b.ID WHEN MATCHED THEN UPDATE SET a.CD = b.cd a.UC = b.UC etc....... 
Double check but Integration Services may not be included with Express.
If you're Allowed to, you may want to explore the fastload option. You should be able to load your dataset to your Teradata user space and do all the processing on the Teradata side via explicit passthrough. I'm on my phone, but this is from the sas documentation. /* Create the empty Teradata table. */ proc sql; connect to teradata as tera(user=testuser password=testpass); execute (create table performers (userid int, salary decimal(10,2), job_desc char(50))) by tera; execute (commit) by tera; quit; /* Create the SAS data to load. */ data local; input userid 5. salary 9. job_desc $50.; datalines; 0433 35993.00 grounds keeper 4432 44339.92 code groomer 3288 59000.00 manager ; /* Append the SAS data &amp;amp; name the Teradata error tables. */ libname tera teradata user=testuser password=testpass; proc append data=local base=tera.performers (bulkload=yes bl_log=append_err); run; http://support.sas.com/documentation/cdl/en/acreldb/63647/HTML/default/viewer.htm#a001405937.htm Edit: phone typo
This is great stuff everyone. I can't wait to try this stuff tomorrow. Thanks so much for he help. If any other ideas come up ... keep em coming.
I'd long hated SSMS's lack of a keyboard shortcut to the Change Connection button. I recently saw a tip. I can't recall the step-by-step (I can find it and post if requested), but basically you modify the setting of the button in the menu bar to show the icon and text (some of you may already have this set), then change the text displayed from "Change Connection" to "Chan&amp;ge Connection". That makes Alt-G fire the Change Connection popup. Note: I chose "g" because it would not conflict with other Alt-* shortcuts. You can, of course, choose your own. Additionally, I changed my text to just "&amp;G" (because I really don't want the text displayed, I'm just doing it for the shortcut), so you could use any not-already-used letter.
That's the thing. Tables I'm querying in Teradata have 15mil rows in just 1month I wanted to extract only the rows worked by that specific team of 400some employees. The data sets are pretty big. You did answer my question. I guess joining on a pass thought is not possible.
Right! That's a good point. Just work it all in the Teradata environment and after that's done bring it to my SAS project.
CTRL + R to show/hide the results pane.
The difference is that you can provide a default value with [ISNULL](http://technet.microsoft.com/en-us/library/ms184325.aspx) method. So ISNULL is mostly useful in SELECT clauses.
ISNULL is more readable and more concise, and imo should be preferred to the 'IF' syntax. IF &lt;column&gt; IS NULL only has the advantage of being portable when using different RDBMs (ISNULL is specific to SQL Server - it'll be NVL in Oracle for example)
According to idera's SQL doctor, isnull in a where clause can cause index suppression, and thus table scans, since it's a function. Is not null should allow full use of indexes. As has been said, isnull() is best in your select, since you can effectively replace nulls there.
So, in the scenario of; SELECT &lt;value or bla if value is null&gt; FROM tbl the only difference between ISNULL(value, bla) and IF(value IS NULL, bla, value) is that the ISNULL is shorter?
That's good to know, I came accross it in a select query so no scans should be done. Thanks for the insight, though, I'll be sure to keep it in mind :)
Ah, was not aware that IFNULL is sql only, good to know! Thanks :)
&gt; isnull() is best in your select COALESCE() is better, as not every dbms will support ISNULL() 
CTRL+W selects the table/object where your cursor is in the script. Regular Expressions with the search &amp; replace are very good you can easily format unformatted code or mass edit. For example: In your SELECT, GROUP BY or ORDER BY statement replace "," with "\n," (be wary of functions though) In your WHERE or HAVING statement replace "and" with "\n and" The \n will add a newline, \t will add a tab, just \ to escape a special character. You can set parenthesis () on a newline along with a tab replace "\(" with "\n\t("
The tables get updated once a week. And the data has dates, so what I'd like is to have the user give a date range, and then populate the drop-downs with the distinct values for each column within that date range. And, thanks for dropping in to help me!
Dude! Thank you! That did it :-)
See my additional [comment](http://www.reddit.com/r/SQL/comments/1q614u/difference_between_isnull_and_if_thing_is_null/cd9qifs) about COALESCE vs. ISNULL.
A day later -&gt; Thanks! Using that I was able to write something to cycle through every single database and gather the relevant information for indexes I need to update into a temporary database, then drop those indexes. I'll have a second script written to re-create those indexes from the temp location by the time of my upgrade. Big big help
ah, you should really build domain tables or use indexed views if you are going to be doing a 'select distinct' every time.
SQLIO is a good tool. What OS are you using? Server 2008 handles offsets and such very well. Server 2003 needs assistance and tweaking. 
SSMS tools. Snippets. YAY
I think dmvs might help, but I'm not very familiar with then.
I'm just now discovered partitions because of this self imposed project to find the most efficient way to do this query. They literally just clicked today. Ha!
Profiler or perfmon should do the trick.
You also might find something useful over at [stackoverflow](http://stackoverflow.com/questions/tagged/sas)
Well, I don't get how the dates would affect the dropdowns then. &gt;So, should I cache the contents of the Customer Table, and then do a distinct selection on the main table for the IDs, and used the cached data with the query results to populate the drop down? Since you would be searching on date (i am assuming) then that would be the best bet. If it's slow, make sure your query is covered by an index. &gt;dozen separate select/distinct queries when populating the drop-downs, rather than trying to do a single query across the dozen columns for their distinct values. That's up to you, it's all the same to SQL server.
You want to run a perfmon data collector set on your DB server during production hours. Add Logical Disk - Avg. Disk Bytes/Read, Avg. Disk Bytes/Write, Avg. Disk sec/Read, Avg. Disk sec/Write, Disk Reads/sec, Disk Writes/sec, Disk Write Bytes/sec and Disk Read Bytes/sec for the drives holding your mdf, ldf, tempDB and and ndf. The most important is really the maximum values for Disk Read/Write bytes/sec. Perfmon is light weight and wont hurt performance. I would run it during production hours at least every day for a week. That will give you a disk usage profile. **Resources:** Setup a data collector set: http://technet.microsoft.com/en-us/library/cc722148.aspx Perfmon counter descriptions: http://technet.microsoft.com/en-us/library/cc768048.aspx#XSLTsection132121120120 
singular - ies/s transitions to singular are too much worry if needed elsewhere (ie in column names), ignoring data/datum-type plurals...
Personal preference but there are reasons for it. never use Distinct selects. It forces a full table scan each time. Edit: Could you give a sample of the data to show why you would need to use DISTINCT? Then maybe I could advise you on an alternative.
When running on a LAN though, I'd always get extremely strange numbers. I never did figure out how to compensate for being on a LAN,. 
Yeah, I've seen funny results running remotely. Always run locally for dependable results.
Neither are part of the SQL standard, so do whatever you want. If you want SQL that works in any database, you have to use Coalesce or Case col Is Null
oh, thanks, TIL!
Yes. It is ubiqituous and can be used with a wide range of database engines. Although many people on Reddit don't realize it major corps do not chase trends. Sure various Internet companies use the latest hot stuff but older, traditional companies use a lot more SQL, COBOL, Java, C, PERL and VB than any of the new NoSQL, Ruby, etc...
i first started to use sql in 1987 with an ibm product called Query Management Facility, a front end to their db2 engine that was 26 years ago, and i don't see sql dying off any sooner than another 26 years
If the attribute is a text string, you should have a single quote mark around it, but you say it works with one value. Replace the = sign with the word "in" and ditch the ANY. select * from table where attribute in ('value1','value2','value3')
The big push for NoSQL came from huge web sites that couldn't scale with a single SQL database. The vast majority of time that just isn't needed so for most people, NoSQL solves a problem they just don't need to solve.
I would love to know the answer to this!
A lot of things will have to go in a completely different tangent than the way they are going now for current DBMS to become irrelevant. I'd say the opposite is true, data is the currency of business; and SQL has had decades of optimizations for it. Nothing else comes close.
I was just about to mention hive and presto. The sql syntax isnt going anywhere. Even google analytics has sql, even though the underlying structure is very different from mysql. 
Your idea sounds good — in theory. However, there's nothing quite as painful as a person fresh out of school who has all theoretical and no practical knowledge. Trying to teach a person the fundamentals behind how word processors are used, for instance, is completely useless. You'd learn more by learning how to actually use one. Plus, ability to do real things is what gets kids interested in stuff.
That's not really a valid comparison. We're talking about engineering. Theory is very important here. Word processing is the digital equivalent of manual labor. Universities are not generally capable of keeping up with the reality of the software field. By the time you build a curriculum, iron out the bugs, hire experts and get students in the class, it's not new. By the time those students graduate, it's out of date knowledge. Even if the topic is still something in use, the nuances will be different. A good schools teaches a student theory and gives them a couple of languages/technologies to apply it, but those are not the focus, which is really what I'm talking about. A graduate with a solid understanding of theory will be able to translate the language(s) they know into modern and marketable ones. Just to be very clear, I'm not saying they don't/shouldn't teach languages. I'm just pointing out that the actual languages schools teach have little to nothing to do with what is modern.
Because MongoDB is webscale?
The problem is that the State field cannot be more than 1 value at the same time, but you are looking for rows where the value is State is 'qld', 'Qld' and 'NSW'. Maybe you meant to use OR instead of AND there? Or maybe you meant to use an IN clause.
Certifications are pretty much useless--but do get familiar with SQL. It's a language, first and foremost--the engines that are behind it vary pretty greatly, and you don't need any clue how to operate them. By virtue of learning the language, you'll learn some of the fun things about the underlying technology when you bump into edge cases or performance issues, but it shouldn't be your main focus. SQL is incredibly powerful, and it's how you interact with data. Period. Learn as much as you can about the language, and you'll be better off for it--but don't worry about getting certified. Also, you can use [SQL Fiddle](http://www.sqlfiddle.com/) to do some testing and whatnot. That way, if you have an issue, you can always post a link to get some help with it all, and they have MySQL, Oracle, MSSQL, Postgres, and SQLite, so you can see how the different DBs handle syntax differently. Edit: By the way, MS SQL's variant of SQL is called Transact-SQL or T-SQL. Make sure you search for that when learning. It's a bit different than PL/SQL (Oracle), MySQL, or PostgreSQL. The different SQL interpretations are akin to dialects of a language.
Kind of an offshoot, but how much of a de facto database is SQL, or even an RDMS? I get the impressing it is kind of like a Microsoft of business operating systems.
Sir, you are a legend this cleared everything up.
Thank you for the tips -- I'm taking them to heart. So companies don't look too highly upon SQL certs?
 Something you need to know about NoSQL, JSON, XML. Is they are about to come full circle and have SQL or something very like it put on top of the data. The thing that makes SQL slow is that it is doing more than the other tech's eg making the database ACID complaint. So yes SQL will exist for many many years
Honestly, in the corporate world, it's better than nothing. A hiring manager won't know any better and it will probably get your foot in the door. In the technical world, it helps, but be honest when you rank your skillset. If someone asks you how good you are, and you say "expert" then you had better back that up.
For a more fundamental approach you'd want to check out this (good) free online course: https://www.coursera.org/course/db Other than that I'd try to convince your company to offer you a basic SQL course in order to understand the SQL expert better and to take simple work of his hands, since he's clearly to busy. That should be enough to convince them. Basic courses can be found pretty cheaply.
The problem with certs is they dont really demonstrate anything, other than you managed to pass the exam (which are multiple choice anyway). I've known some people with certs who were chronically incompetent with SQL. Having said that, if you want to get some SQL responsibilities in your company, you'll probably need certs just to give the impression you know what you're doing (as opposed to hey Jim in accounting has dabbled in a bit of SQL, can he have access to the SQL server?) Its not uncommon for non-tech people to learn SQL.. Finance, marketing, business analysts etc.
Sql will still exist, but there is a big big trend to make it more hidden and let the application handle everything. In ten years you don't need to know sql to execute a query.
Awesome thank you. i will test this out. still have another project im working on at the moment but i would like to automate this into my software after i test it.
may even incorporate this into the batch im currently writing to make things easier
Thank you -- very helpful reply. I'm getting the sense that I should eventually pursue a cert just for the sake of showing my superiors, but I won't take that as any evidence that I've mastered SQL (I realize that will take many years).
No... Read up on the fundamentals of databases first. Get a good understanding of how data works and is connected. Get the basics, so that when you start using sql you can then form the picture in your mind of what you are doing. 
I figured you and OP were talking about high school/middle school (judging by use of "kids"). If we're talking about college, sure, I agree it should be mostly theoretical. For younger kids, though, it's a lot more fruitless to try to teach them theoretical stuff, especially when it'll likely be their first/only tech class and computers are so accessible.
They're called window functions, and you can do that with any aggregate in MS SQL (and other engines, although syntax varies).
If the two columns are there why do you need the new one? Seems like wasted space. 
well after i concatenate them i plan on removing the other two columns.
I don't have anything to test with right now but your statement looked good. Just saved insert statements or the data before you run any deletes and you should be fine. 
getting a column not allowed here error.
are you on 11g? Why not make a [virtual column](http://www.oracle-base.com/articles/11g/virtual-columns-11gr1.php)?
i am not. edit, i am. theres gotta be an easier way tho.
did you alter table first?
not sure im following.
Rather than do an insert, can you update the new column? Update T Set New_col = (existingCol1 || ' ' || existingCol2) From My_table T 
this works. still looking for other solutions but it works.
No probs, glad you found it helpful. I was a junior developer many years ago, and the dept was just using SQL in a very basic way (fetching and storing simple records, but still doing most of our business logic in client code, instead of SQL as it should be). We knew this was a bad idea but we didn't know how to use SQL properly at the time. I decided to focus on SQL and become the dept DBA, and after living and breathing SQL for a good few years, I became pretty expert at it (especially optimising queries and server performance which interests me enormously, you perhaps less so). In terms of writing queries, you're possibly at an advantage having little programming experience. Its quite a change of mind-set since SQL typically involves operating en-masse on mathematical sets, and thats not always natural to someone whose been a lifelong coder (where you tend to deal with arrays and row-by-row operations). Stick with it, its a very valuable skill to have, and really satisfying when you can mine complex data to deliver real insight 
As it should be is a bit of an opinionated matter? The issue comes when different applications that use the same data/database in the organization don't have common business logic. Sure, this can be done in the DB. But it can also just as easily be done outside of it. If an organization is repeatedly running into these kinds of issues, it could be signs of deeper problems such as siloed development teams or incompetent team leads with NIH syndrome. Of course DBA folks will preach doing everything in the DB, but its not the one true way its often made out to be.
Ctrl-T : results as text. Ctrl-D : results as grid. F6 : swap between query results, cycle through results if multiple result tabs. hold Atl and select with mouse to select box region in query/results, works with copy and paste. 
Assuming you don't use the UPDATE solution already given, this should work: INSERT INTO my_table SELECT existingCol1||' '||existingCol2, NULL FROM my_table; This assumes a 2 column table and that you want to insert the concatenated columns into the first column (hence the ,NULL following it) - easy enough to modify if you want to use the other existing columns in the inserted row though.
exp(x) is defined as e^x , so if you want the value of e, you could just do exp(1). http://docs.oracle.com/cd/E17952_01/refman-5.1-en/mathematical-functions.html
Thanks -- that's something for me to explore.
also in [sql server](http://technet.microsoft.com/en-us/library/ms179857.aspx) and [mysql](http://dev.mysql.com/doc/refman/5.0/en/mathematical-functions.html#function_exp)
That's what I figured, and that is why I am looking into some of these things to try and get a handle on what is happening and how we can improve our situation. Thanks.
Your database vendor needs to be jettisoned post haste. I've never heard such stupid advice. If you'd like, you can send them my way, and I'll hit them upon the nose with a rolled-up piece of newspaper. To answer your questions, indexes *do*, in fact, incur some overhead. That doesn't mean they're useless--like everything in life, you have to balance everything. This "overhead" comes in two forms. First, it takes disk space to keep an index. Depending on how many columns you include with your index, it could nearly double your table size. Space is cheap, though, so you're fine. Secondly, there is *some* performance hit for *write* operations. If you're in a write-heavy environment, lots of indexes are heavily discouraged. The reason is that when you write a row, it has to update all of the indexes affected by that write before it commits. For two indexes, this time is almost trivial. Now, statistics! The very fact that your vendor dismissed statistics says a whole lot about what they know. Statistics are internally kept stats on the data profile of your tables. These stats are then used by the optimizer to tell it what indexes to use. Just because you have an index doesn't mean that it will be used--if the column has low cardinality (i.e.-not many distinct values), the optimizer will almost always do a scan rather than an index seek. The stats update on MSSQL asynchronously by default (you *probably* don't want to do it synchronously). The more out of date they are, the worse your optimizer is at giving you the most efficient query plan. Stats happen automagically by default, but after bulk loads, I almost always call `update statistics` just to be safe.
The 'overhead' they speak of is reads vs writes. Not all recommended indexes need to exist, but some do.
I would love to send them your way! I think the problem is that they are top 10 for fortune 500 companies, and they have a huge task force to assist with database and operations, this can cause some issues because you always speak to some one new every time you have an issue, and they are quick to dismiss that it may be a problem with the database, even though our database has records for 27 some odd thousand people and has been in existence and in use for nearly 20 years! I think they may shy away from the indexes because of the write operations, the database is heavy on the write side, probably a 75/25 split. And yes space is a bit cheap for us right now, so if we are able to make things more efficient at the cost of a little disk space, then so be it. I would like to hear more about how to Update Statistics, as this sounds useful. Thanks, I appreciate the advice. EDIT: is EXEC sp_updatestats; the right way to do this? ran for 6 minutes updated 100s of statistics. 
That makes sense as our database is heavy on the write side vs read. However we do have a "report" copy of the db that is entirely read. So in that regard I wonder if it would be bad for to analyze slow queries and add indexes as suggested by the tuning advisor.
In this case, it's possible to transform that correlated subquery into an inner join, and that is what MSSQL does (2008 R2). That might not always be possible/feasible and I think just a straight inner join or window function would still be preferable anyway. But you are right, in this case, it would most likely just be executed as an inner join.
MSSQL 2008 R2 transforms it to an inner join. I'd imagine other databases would do the same when possible.
Think about it. This is really no different than an inner join, and that is what MSSQL 2008 R2 executes it as. I'm not trying to advocate this method (I would go for the window function), but it shouldn't be less efficient. If you check out the execution plans of some of your queries, you'll notice that the server will adjust them to (hopefully) be more optimal when possible.
Fair enough. I still want to hit them on the nose with a newspaper. At any rate, there are two ways to update statistics, and you found one way! `exec sp_updatestats` updates the statistics for all tables and views in the database that you're using. To update the statistics for a single table or view, use the `update statistics [table_name]` command. `update statistics` is a...gentler approach. [Here's the doc for `sp_updatestats`](http://technet.microsoft.com/en-us/library/ms173804.aspx), and [here's the doc for `update statistics`](http://technet.microsoft.com/en-us/library/ms187348.aspx). I tend to prefer `update statistics` because it narrows the scope for you and doesn't inadvertently affect things you weren't thinking of. It's important to keep in mind that stats are updated by default in MS SQL during database downtime (low usage), but depending on how y'all set it up, they may not be updating. You can get a better idea of statistics and how they work from [this TechNet article](http://technet.microsoft.com/en-us/library/ms190397.aspx).
I have a weird question that you don't have to answer. Is that "someone who does" a female?
I am not touching our production DB, but our report is restored once every 24 hours fro a backup of the production DB, so I have no real idea if / when things are getting updated. Thanks for those links, I am looking into it now to get a better grasp of what I am doing here. Thanks again!
Yes, in fact. Why do you ask?
 Indexes basically duplicate data of the columns they contain. However they also organize it better so it can be searched much faster. Simply put when the overhead of using them is less than the overhead of not using them. Just use them. The stats are for something else. Says you have a column contains M / F (Male / Female). If you have 95% male and 5% female. It will use the index for when all females are selected but probably ignore it for when Male is selected since it has to read 95% of the table anyway for the query. Note: this is a really really basic example :) Your DB vendor does not know what they are doing.
Well, it will probably be around some ETL tool--OBIEE or Informatica or something along those lines. I typically ask left join and de-duplication questions just to make sure that folks can write SQL. I also typically ask about data and dimensional modelling for a job like that, since it's important.
Start by knowing what ETL stands for! A recent interviewee with us thought it was Enquire Transport and Lock. Admittedly he wasn't a SQL dev but he was interviewing for a Sharepoint PM role. 
This is great, thank so you so much! As my data set grows larger, any advantage of one vs another for efficiency?
This is spot on
The nested subquery will probably be more efficient, if you have an index on `IBH` and `ONH`, and they have decent cardinality. Otherwise, both will be doing scans of `ES_Daily`. The `select count(date) as Total from ES_Daily` should be pretty quick to return, if there's an index on `date`. I would check the query plans, though--it's going to depend on what your data looks like and the indexes attached.
My reason for asking is none of my business, but after reading your post I picked up on a few things and had a burning curiosity at least about that one aspect of your situation.
Interesting. I don't want to come off as presumptuous, but I noticed what I interpreted to be subtle (or maybe not so subtle?) indications that your interest in this is not purely of a professional nature. Or, at the very least, that the other person is female. 
You are probably correct, And I plan on keeping this one, as it is on a copy of the Database that is read only. Since I will not be writing to this one, I wonder if it would be beneficial to add some more. I continue my research and thank you for your input.
I honestly could not tell you exactly how things are written to the production DB, still a big fuzzy mystery to me, but it seems that it is a relatively complicated DB. I appreciate the input, and from what others have said it doesn't seem that I will suffer much if any degradation by adding indexes to a read only DB. 
Let me give this a try. There are about 3 dozen columns in this table, and about a dozen of them are foreign keys to other tables that contain the data/description that's common. So the main table will look like this: Date | Location | Dept | Customer | Amount ---------|----------|------|----------|------- 1/1/13 | 2 | 4 | 3 | 10.00 1/15/13 | 2 | 3 | 9 | 5.00 1/15/13 | 1 | 4 | 7 | 6.00 2/14/13 | 3 | 4 | 9 | 10.00 2/5/13 | 7 | 1 | 7 | 9.00 And let's use this for the "Depts" table: ID Description 1 Chemistry 2 Physics 3 Biology 4 Mathematics I want to be able to have the user enter the date range they want to search, then go get the distinct values to populate the drop-downs with the data from the other table. For Example, if they just want to search January, I want to be able to populate the "Dept" drop-down with just the departments that would be found in January - in this case, that's 3 and 4. The drop-down would actually display the description from the Depts table (Biology, Mathematics), and the values would be 3 and 4. I don't want to display Chemistry or Physics because they would return no results were the user to select them for January. Does that make sense? edit: Sorry for the formatting, I tried to get the table to work, but eventually gave up.
So why the fear that I might also be an employee for your company?
BTW, I've enjoyed reading about your saga, starting over in /r/sysadmin
Dear lord this shoudl probably be a submission for /r/talesfromtechsupport because of the amount of BS I have been through in the last month! However I am learning a whole lot about SQL server, which is a bonus!
 INSERT INTO CTW ([field1], [field2], [field3], ...) SELECT [field1], [field2], [field3], ... FROM [tbl1] JOIN [tbl2] ON ... WHERE ....
1) So when creating the CTW table, will I have to Modify the table, and add fields named [FirstName], [LastName], etc? 2) Where you wrote "JOIN [tbl2]" how would I write out the JOIN I need based on this (IE: Left Outer, Right Outer, etc.): Referral and RA can have the following Join: Each RA has one or more Referral. Each Referral has one and only one RA. [Referral].[ID] = [RA].[ReferralKey] Thanks for your help.
Actually, reading what you wrote again, it sounds like what you really want is a view (basically a computed table*). If you can write a SELECT query that yields the results you want, then CREATE VIEW CTW AS SELECT [whatever] You can also do it in SSMS with that awful query designer thing they have. So it looks like you want something like SELECT [FirstName], [LastName], [Date1], [Date2], [ID] FROM Referral JOIN RA ON [Referral].[ID] = [RA].[ReferralKey] WHERE RA.[CallDispoMicroKey] = 629 You don't want an outer join unless you want to include records from one table that are unmatched in another (i.e. give me all records in TableA plus any related info in TableB if it exists would be TableA LEFT JOIN TableB) \* Lots of caveats there (especially as related to indexing, google "indexed view"), but the idea is the same
Just make it into a view. So you'll do USE Claimtrak GO CREATE VIEW dbo.CTW WITH SCHEMABINDING AS SELECT ClientFirstName, ClientLastName, DateOfReferral, DateOfBirth, ClientID FROM dbo.Referral JOIN dbo.ReferralAgency ON Referral.ID = ReferralAgency.ReferralKey WHERE dbo.ReferralAgency.CallDispoMicroKey = 629 GO Now CTW will be a view, which is basically a dynamic table. The WITH SCHEMABINDING option allows you to create indexes on it if you like, and it will also stop people from altering the underlying tables in ways that would break the view. You can treat CTW just like you would any other table for the purposes of SELECT queries.
You can always review a query plan for something you'll be doing often, add an index on key columns and then review the plan again.
Try single ' in bindParam around your :categoryName? Also, is $categoryName being assigned?
Ah, I see.
If it insists you use a table and won't work with a view (which is odd to me, unless it needs to write to the table, which can also be managed with a view with some limitations), you'll probably need to resort to either getting fancy with triggers or scheduling a job to update the table. Neither are particularly simple though.
Don't you dare follow /user/SaintTimothy 's suggestion without knowing if the LEFT joined table/derived view would cause a cross product or not. Either the left join OR the exists usage in your other reply would be sufficient. But without knowing your data, only you can fully ascertain the solution for your problem.
Thanks! That makes a lot more sense. Because I'm comparing fields in the same table, joining it didn't even cross my mind. I learned to code our ERP system before SQL, which loves doing things line by line. Shame really, it makes some things awkward because it wasn't designed with SQL in mind! **Before:** SELECT LH.[Load No_] ,[schedule date] ,[confirmed date] ,[user id] ,case when exists (SELECT TOP 1 (hdr.[Load No_]) FROM [Load Header] AS Hdr WHERE HDr.[Schedule Date] &gt;= hdr.[Confirmed Date] AND hdr.[Confirmed Date] &gt; '1/1/2013' AND hdr.[schedule date] &lt;&gt; '1/1/1753' AND Hdr.[Load No_] = LH.[Load No_]) then 1 else 0 end as OnTime ,case when exists (SELECT TOP 1 (hdr.[Load No_]) FROM [Load Header] AS Hdr WHERE HDr.[Schedule Date] &lt; hdr.[Confirmed Date] AND hdr.[Confirmed Date] &gt; '1/1/2013' AND hdr.[schedule date] &lt;&gt; '1/1/1753' AND Hdr.[Load No_] = LH.[Load No_]) then 1 else 0 end as Late FROM [load header] AS LH WHERE **After:** SELECT LH.[Load No_] ,[schedule date] ,[confirmed date] ,[user id] ,case when lh2.[Load No_] is not null then 1 else 0 end as OnTime FROM [load header] AS LH LEFT JOIN [load header] as lh2 on lh.[load no_] = lh2.[load no_] and lh2.[schedule date] &lt; lh2.[confirmed date] WHERE 
One more caveat (though it doesn't appear to be a problem here) is that the LEFT JOIN will result in multiple records if there are multiple matches. A GROUP BY clause will fix that though.
This is exactly what we have been doing, and it seems to have greatly improved our read times. However the actual production database is another story as workflows are completely different and queries against the database are going to be pretty impossible for me to find, as they are all hidden in the Vendors application.
[/user/anon197812](http://www.reddit.com/user/anon197812), I don't recommend using a GROUP BY or a DISTINCT on the entire query as a first try. Use a derived view instead. SELECT LH.[Load No_] ,[schedule date] ,[confirmed date] ,[user id] ,case when lh2.[Load No_] is not null then 1 else 0 end as OnTime FROM [load header] AS LH LEFT JOIN ( select x.[load no_] from [load header] x where x.[schedule date] &lt; x.[confirmed date] group by x.[load no_] ) as lh2 on lh.[load no_] = lh2.[load no_] WHERE ...
 SET foreign_key_checks = 0; &lt;do stuff here&gt; SET foreign_key_checks = 1; You can probably just add those lines to your .sql file.
I'll give it a twirl. Be back in an hour after my text editor opens the sql file. ;)
"t-sql querying" by itzik ben-gan, ISBN13: 9780735634763 
 SELECT posts.title , comments.content FROM posts INNER JOIN ( SELECT post_id , MAX(id) AS latest FROM comments WHERE user_id = 2 AND approved = 1 GROUP BY post_id ) AS x ON x.post_id = posts.id INNER JOIN comments ON comments.post_id = x.post_id AND comments.id = x.latest ORDER BY posts.id
3.7GB is very large? 
For people who have no idea what they are doing using tools not meant to be done, yes.
change INNER to LEFT OUTER in both joins vwalah!
That looks like it's exactly what I need! Thank you very much! I really appreciate your time
I didn't look over the schema very closely. But I might ask you about your choice of data types on various fields. Seems to be a lot of VARCHAR(45) stand ins that either could be much smaller, or need to be something else entirely, such as is the case with COURSE.Capacity and COURSE.SeatsTaken should probably be a TINYINT or something.
You can't use the :old in an insert trigger, but you can in an update trigger. You have both here. On insert or delete it will do nothing with the :old value (not sure if it errors, I haven't test this). This is actually kind of weird, I'd write 3 triggers here.
Tons of good information 
Is the beginning of the financial year always July 1?
I'm writing this on my phone, but try this. date between '2013-07-01' and Cast(left(cast(dateadd(day, - datepart (day, getdate()), getdate()) as varchar), 11)as datetime) 
So a work around I've found is to kick out the CTEs all together and use: SELECT TOP 1 @WOKey = WOKey , @SOLineKey = SOLineKey FROM #tsoWOAlloc WHERE QtyRemain &gt; 0 ORDER BY WODate ASC Which seems to work, but nonetheless it's still very interesting that the variable isn't set to null from an empty CTE. **Edit** This didn't work in the loop, so I added a work around: IF (SELECT COUNT(*) FROM #tsoWOAlloc WHERE #tsoWOAlloc.SOLineKey = @SOLineKey AND #tsoWOAlloc.QtyRemain &gt; 0) = 0 SELECT @WorkOrderKey = NULL Which breaks out of the loop.
SELECT DATEADD(s,-1,DATEADD(mm, DATEDIFF(m,0,GETDATE())+1,0)) http://blog.sqlauthority.com/2007/08/18/sql-server-find-last-day-of-any-month-current-previous-next/
Yes it is :)
Don't forget to convert to date style 101
If [DateField] has a time component, then you will have too much data for a partial day. Consider this code: declare @utc datetime, @start datetime, @end datetime select @utc = getutcdate() , @start = convert(char(4), dateadd(year, case when datepart(month, @utc) &gt;= 7 then 0 else -1 end, @utc), 112) + '0701' , @end = convert(char(8), @utc, 112) select @utc [Current Date] , @start [Start] , @end [End] /* -- to be EXCLUSIVE of the current date where DateField &gt;= @start and DateField &lt; @end -- to be INCLUSIVE of the current date where DateField between @start and @end */ 
Sorry - I meant in my context. It also varies by country. I am in Australia, and as far as I am aware 99% of companies have financial year beginning 1st July.
This worked a treat thank-you!
soulstealers appears to have worked, I will definetely use this if it doesn't work when it ticks over the month. Thanks for all of your help guys!
we're using sql by example by alice rischert in my class. i would recommend it.
Hi, I actually have access to a university ERP database, so i can "briefly" tell you how ours looks like if you would like. Added: At the very least I'll suggest that the student table only have biographical information. The year/major name/stno(fk) could be in another table. 
It appears to be creating a log of the transaction you are currently in as of the execution of the trigger. In case of a duplicate it does nothing
Sure! That would be great! thank you.
It is because when you run a SQL statment in SSMS, you have certain SET options that are default. Here is what I have [set](http://i.imgur.com/E6frWYQ.png). &gt; Quoted Identifier either on or off depending on if I am using single or double quotes. Please correct me if I am wrong This is not quite right. QUOTED_IDENTIFIER instructs SQL Server how to interpret single ( ' ) verses double ( " ) quotes. There are other [nuances](http://technet.microsoft.com/en-us/library/ms174393.aspx) most of all in your situation: &gt; SET QUOTED_IDENTIFIER must be ON when you are creating or changing indexes on computed columns or indexed views. If SET QUOTED_IDENTIFIER is OFF, CREATE, UPDATE, INSERT, and DELETE statements on tables with indexes on computed columns or indexed views will fail. Here is what I'd ask...what are you trying to do? Why are you adding indexes in a SQL Job in the first place? What is your goal here?
OK so you need the full story, of course you do! I should know this being in end user support myself. We have a database that get overwritten every night with a backup of the production DB, not the best situation in the world but it works for our needs. However when this DB gets overwritten it loses these indexes. Without these indexes the database when queried for specific information sets will go absolutely crazy, as in disk IO requests in the 4-5k per second range, well above what it can handle, and then it will write 100s of GBs to the tempd DB before filling the drive and crashing. So What I am attempting to do is add these indexes manually after every restore so that we can continue to use the DB in any manner. 
Those indexes look more like stats to me rather than Use pastebin.com to post your entire job and how you are executing it (settings in your SQL Agent job). That might be helpful. If that isn't feasible, then you know you can always do: set quoted_identifier on go create index .... go 
I think that is the easiest for me to do at this point in time, as pasting the entire job would be a bear, I have it broken up into way too many steps, So i could figure out exactly where it was failing. I appreciate the help!
What I'd do is Where datefield is between '2013-07-01' and year(current date) concat month(current date) concat 31 At work we use YYYYMMDD , so you'll need to concat the hyphens in yourself ;)
ah, ok, thanks! I'm still not sure what use it could have beyond that or what a DDL statement for the transaction_mapping table could look like.
so, to be clear, today, in the middle of november, by "**next** 3 months" you actually mean december, january, and february, right?
Correct
set criteria field to "&lt;=Date()+90"
Please see update
"between 12/1/13 and 2/28/14"
I need this to be dynamic so that it always calculates off of the present date
Day(Date()) gives the day of the month of the current date DateAdd(d,1-Day(Date()),Date()) subtracts that number, less one, from the current date, effectively giving the 1st day of the current month now all we have to do is add 1 month to the above (1st day of current month) for the bottem end of the range for the WHERE conditions, and 5 months for the non-included upper end of the range -- WHERE mydatecolumn &gt;= DateAdd(m,1,DateAdd(d,1-Day(Date()),Date())) AND mydatecolumn &lt; DateAdd(m,5,DateAdd(d,1-Day(Date()),Date()))
any chance i could actually see your query?
you put square brackets around "d" which msaccess interprets as a prompt pls removes them :)
try this -- SELECT SupplierID , ContractExpirationDate FROM Ariba_Detail WHERE ContractExpirationDate &gt; =DateAdd("m",1,DateAdd("d",1-Day(Date()),Date())) And ContractExpirationDate &lt; DateAdd("m",3,DateAdd("d",1-Day(Date()),Date()))
Still no luck
i'm sorry, i am not familiar with the "still no luck" error message edit: oh, wait, maybe it was the accidental space i had between the &gt; and the =
SQLZOO has some good tutorials for basics. Be warned that it doesn't really enforce good standards, you just succeed if you return the correct data.
You'll need to create the column first. ALTER TABLE [HLTH_REPORTING].[dbo].[BillCode_Lookup] ADD Billing_Key int IDENTITY(1,1) It'll auto populate the column with IDs, then you can add the constraint. ALTER TABLE [HLTH_REPORTING].[dbo].[BillCode_Lookup] ADD CONSTRAINT PK_BillCode_Lookup PRIMARY KEY(Billing_Key)
Yeah, I looked that userenv('commitscn') up and think someone put this in for some funtime. It's not my code, but I need to understand it. So, maybe I should rewrite that. It could be in there because the db has a lot of accesses per minute and as I understand, this gives each transaction a unique code. And that's where the DDL statement question stems from. Since it would make sense to analyze the table when it is used otherwise. As I understand, that would be possible with a DDL ANALYZE trigger.
this is a very low quality article it sounds like it went through google translate a few times
Biographical Table: Has all the student info Subject Table : Has info about the subject, per year, and per qual Qualification table : has info about the qual, per year Curriculum Table : Has more detailed breakdown of the qualification, the same subject could have different credit values for different qualifications, also has fields for for 'major', 'compulsory' (which are Y/N), per year. Pre req Table: has information about subject pre-reqs, as well as 'substitute' subjects. Qualification Registration: This has information about which qualification the student is registered for, per year Subject Registration: This has what subject the student is registered for, per year, as well as the year, exam and final mark, classgroup There are a lot more tables, example, one that keeps track of test marks, one that has the student fees, staff information, timetable information , classgroup (which has a staff number field to link with staff information).. 
Hey OP, anything you can tell us anything about the aftermath of trying to do something about this mess you and/or your employer are dealing with?
How large is large here? And why exactly do you need to do this?
I simplified the project a bit in this summary. Basically, we're talking about 2.5 million records or so and more like 5 or 6 possible indices. The reason I need to do it is that of the 2000 or so different products, the vast majority of them only have a single rate that is repeated hundreds of times, while a minority are split by several different factors and have thousands of different rates. I'd like to build tools that make use of the rates for communication and calculation purposes, but I'd prefer to avoid waiting 2 minutes any time someone wants to get a list of rates for a product. 
Thank you so much for your help. Here's gold!
Yea, I'd do a union, first, those that have varying rates... a 'select distinct * where service in (select service from ... group by having count(service) &lt; 3)...' then those with 'all'...
If this is a second database that is read only, you can create as many indexes (within reason) as you want to speed up reporting purposes. Since this is a read only copy it will not affect writes to the database, so there is no downside. If you do a full backup/restore to populate this database you will need to create the index every time or you will be back to where you started. Statistics are another interesting part of the equation. I have run into many scenarios where queries were being run that have less than optimal structure. Functions... some joins missing predicates, etc.. Awful stuff. At any rate, one scenario I ran into I had to do a full scan for the statistic updates before the execution time of the query was resolved. 
That is kind of where we went with it. We restore from a full backup and then re-add about 10 indexes, and we have seen increases in performance where a report used to take 75 minutes to run, now takes 8. Or a 20 Minute report now takes 20 seconds. All in all it has turned out well for us and given me a nice playground where I don't have to be terrified of destroying mission critical data.
[Formatted version here](http://snipt.org/Baip8).
How often do you change rates? It might be worth creating a second table that contains the condensed information that is populated at a set period (hourly/daily?) to prevent unnecessary database load. If adding/updating/removing rows doesn't happen often you could also use a trigger to execute a stored procedure to repopulate the other table along with a nightly execution for additional verification.
What is the error? Where did you TRY to put them? Why can't you just add them here: FROM (SELECT DISTINCT acap.c_po_nbr, ahdr.shpmt_nbr, **ahdr.units_rcvd, adtl.retail_price,** ... Edit: formatting
Looking at /u/BFG_9000 formatted post you have the following format in your code: SELECT fields FROM ( SELECT fields FROM table1 JOIN table2 ON conditions ) WHERE conditions UNION ALL SELECT fields FROM ( SELECT fields FROM table3 JOIN table4 ON conditions ) WHERE conditions ORDER BY 1,2,3 If you want to add those fields you will have to add them in all four places that the fields are declared. ---Some other points:--- In your join criteria, "... like concat(concat('%',substr(acap.c_po_nbr,0,7)),'%') ..." will not work in many databases, but without knowing what you are using this may not be a problem. You are LEFT JOINING to the same table multiple times, is there a better way to do this? You are select a date range based on a string comparison and inequalities. This can cause problems with different date ranges. Perhaps you can increase readability with the use of CTE, so that your selection criteria is easier to see, rather then subqueries 
It finally worked. Thank you so much for your input!
Well, it sounds like your tables are not a 1:1 direct match. You'll have to account for the differences EXISTING_TABLE has, whether that be extra columns, different data types, etc. I assume your first query is something like: select * from table1 Do you have more than one row? Including the code would be helpful.
You don't use the VALUES keyword when inserting from a subquery. INSERT INTO target_table (field1, field2, field4) /*Target columns. In this example, field3 would be left null. If the subquery returns the same number of columns as the target table, with convertable datatypes, you don't have to list the fields. */ (SELECT --field list FROM source_table WHERE --blah blah blah ); 
There is. It's CAST. But from what you describe, the data type of the target column as defined is too small to hold the size of the returned data element from the source column. You'd either have to truncate the source data or enlarge the target data type, and that means modifying your target table.
Yes - could there be whitespace in your one 3-character result? You could try TRIM. Modifying columns in Oracle is a pain because Oracle won't let you modify a populated column. You have to create a new column, copy the old column's contents into the new column, and then delete the old column. It's easier if you're able to export the results and reimport them into a new table.
You could try: select trim(att1) or select substr(att,1,1)
I got the SQL code working. There was a problem with the size. I now just had an error in my Java Code that the SQL is embedded in, but I figured that out too. Do either of you know why in Java you HAVE to call ResultSet.next() or it won't let you return anything from that ResultSet? THANKS FOR YOUR HELP (:
CASE att1.GeneName WHEN GeneNames.GeneName then **'Y, '** That's 3 characters. Not sure why you'd want to put the commas like that. If you plan to concat and output in a list, do it outside of the fields: select att1||', '||att2||', '||att3||', '||att4||', '||att5 from EXISTING_TABLE;
I'd assume it's set up that way to make looping easier. Not familiar with using Java to interface with databases, but going off the [api](http://docs.oracle.com/javase/7/docs/api/java/sql/ResultSet.html#next(\)), ResultSet.next() moves to the next row, if it exists, and returns true. Once it moves beyond the last row, it returns false. This means you can loop through each row easily using a while: while myResultSet.Next() { //do stuff to the current row }
That's certainly a consideration, but I hate adding even more data duplication.
Unfortunately, the count isn't always going to be 3 here. Each service will have a variable number of durations and locations for which it is viable. I used 3 Durations and Locations in the example, but in reality some might have 50 and others might have 35. 
The site says that your query returns the correct answer. Sanity check: are you submitting in #5?
Just got a sec for this. Your conditions looks wrong, the where clause is incomplete (Not knowing th data) SELECT name FROM world WHERE gdp &gt; ALL AND gdp IN (SELECT gdp FROM world WHERE continent = 'Europe')
Hey, thanks for responding! Yes definitely. It works when I use the MySQL engine (what I suspect you have done, as it's the default) but I need to learn MS-SQL, which just returns "name"
Have uploaded screenshots showing I've taken my meds!
went back and tried this select name from world where gdp &gt; all (select gdp from world where continent = 'Europe') it worked..not sure why it isn't working for you? http://i.imgur.com/64akw4s.png
 SELECT name FROM world WHERE gdp &gt; (SELECT Max(GDP) from world WHERE continent = 'Europe')
Thanks so much! Glad I could help.
Assume it's the database version - MYSQL (default on the site) VS MS-SQL which is what I want to learn. Either way, you gave me the answer I needed! Thanks very much for your help!
Cheers, Kingka suggested this exactly and it made perfect sense. I'm going to use this as the logical step and move on with it. Thanks again for taking the time!
I concur with you sir. 
Well, everything that is relevant to the specific start is in the columns: `id`, `reg_date`, `horse_reg_number`, `horse_name`, `horse_sex`, `horse_age`, `horse_breed`, `horse_nationality_born`, `horse_nationality_owned`, `horse_nationality_raised`, `horse_driver`, `horse_startpoints`, `horse_trainer`, `horse_hometrack`, `race_date`, `race_name`, `race_number`, `race_track`, `race_conditions`, `race_distance`, `race_startmethod`, `horse_distance`, `horse_start_number`, `horse_start_position`, `horse_earnings`, `horse_odds`, `horse_time`, `horse_foreshoes`, `horse_hindshoes`, `horse_finish`, `horse_scratched` I guess the race prefix part is a little bit redundant and could be linked with a multiple column (race_date and race_number) foreign key or something? horse_distance is different from race_distance since some horses are handicapped and start 20 or 40 meters behind in a race but sometimes it's also reported as null for a race while race_distance is always reported. All the other information could change from one start to another except the unique constraint and nationality_born and nationality_raised I guess but those are simple varchar(2)'s. id and reg_date might be superfluous if I'm running an unique constraint for the other three columns, I don't know. Is it a terrible setup? Is it worth it to put a few race_ columns in a different table?
that table screams for normalization, but on the other hand if this is coming from some external source in exactly this layout, then you need at least one table that looks like this to receive the data into after that, whether you want to transform the data into normalized tables will depend on how ugly your queries are, and i suspect they aren't going to be all that bad i would carry on with this table as is
Thank you for your help, I'm trying to understand the best way to go about it and I guess that I have to trial and error my way. INSERT INTO `mytable` (`column_1`, `column_2`...`column_n`) VALUES ('$value_1', '$value_2'...'$value_n') ON DUPLICATE KEY UPDATE `id`= LAST_INSERT_ID(id), `reg_date`= NOW(), `column_1` = '$value_1', `column_2` = '$value_2', ... `column_n` = '$value_n'; Just to make sure, does this look like the correct syntax to update the whole row with n columns while keeping the previous id (auto_incremented) but updating the reg_date (datetime)? In other words, I would always have to specify the columns twice?
Don't update the ID column--you want to leave that alone. Otherwise the syntax is correct from what I can tell on my phone. Edit: Make sure you insert with the unique columns--otherwise the insert won't fail for it to update!
I don't understand why the emailQueue has to understand anything about a person or a shift. The email queue should just hold a clob of email content and metadata for recipient/sender/send_time/cc/etc... I would be hesitant to bind your email table so closely to these other two tables. Whatever process/code that generates the email will understand the relationship. Your email queue should just be a bucket that a job will empty. *edit* Also, inserting two records into the queue is fine. Whatever process that determines when to send emails should insert the 1d and 7d record and your job should see if the send_time &lt;= current_time. 
Good point, but there is a reason. The user gets the option to edit shifts. He plans shifts some months ahead, but they are subject to change. Also, I imagine he wants to edit the body of the email in the meantime or people might swap shifts. Thus, it is possible that emails which are compiled as soon as the shifts are created are obsolete way before they are even sent. I understand your problem, though. I think I'll create a process that compiles the emails and puts them in the bucket at the right time. The mailer process (like exim or an python based SMTP-trough-GMail script) will empty the bucket. In that way, the email is adjustable up to a short time before sending, but the mailer doesn't have to understand the content of the email. So, thanks for your reply. It doesn't really answer my question, but it helps anyway :)
&gt;the emailQueue has to contain the same information again, which is also wrong, in my opinion. It's not 'wrong' any more than having to have an extra join is wrong. It depepends entirely on how you want to design the database, and whether or not the normalization is paramount.
haha, after I read about the duplication to the mailQueue I became transfixed on that sentence and lost focus on everything else. If shift_id/person_id is unique in the map table and cannot be changed, then i see no reason why you cannot use those ids. duplication at that scale level doesn't bother me cause it makes it possible to forgo the map table to get shift/person information, as long as the map table doesn't have other information you need. I brought the email queue thing up because most applications are going to have an email component and the more abstracted that processes is the easier it is to keep a handle on that process. I am a consultant and I am currently helping a major franchise transform all of their one off email solutions hidden within different apps to a central source. When you just have one app then it doesn't seem that bad, as your responsibilities grow and other people join the team and need to maintain the code, it becomes very difficult very fast if every app does it a little differently. The same thing goes with logging, that is also on the list of things :(.
If you add 100 rows a day for 10 years, that's only 365,200 rows. Even Excel can handle more data. Resource planning isn't really that important when dealing with such small sets of data.
Easily done! First write the SELECT query. It should look like this - select ClientKey, LastStatementDate, ROW_NUMBER() OVER(PARTITION BY ClientKey ORDER BY IsNull(LastStatementDate, '1/1/1900') DESC) AS RowNum FROM Tablename Then, once you validate this, the update statement could look like this UPDATE t SET LastStatementDate=f.MaxLastStatementDate FROM tablename t INNER JOIN ( select ClientKey, LastStatementDate, ROW_NUMBER() OVER(PARTITION BY ClientKey ORDER BY IsNull(LastStatementDate, '1/1/1900') DESC) AS RowNum FROM Tablename) f ON f.ClientKey=t.ClientKey AND f.RowNum=1
I reckon declare a cursor which will contain each distinct clientID. Inside the cursor SELECT TOP 1 lastStatementDate From table WHERE lastStatementDate IS NOT NULL ORDER BY lastStatementDate DESC UPDATE TABLE SET lastStatementDate = cursor_lastStatementDate WHERE clientID = cursor_clientID End cursor Something like that. I am on my phone so sorry for the formatting, hopefully that will work :-) 
Yea, whenever you encounter something like "I want to get the MAX thing FOR EACH other thing" I instantly think RowNumber. Rank is cool too, but can return multiple of the same ordinal. RowNumber just picks one.
How big is your dataset? Consider running the query and then right-clicking the top-left corner of the output Results area and then selecting Copy With Headers and Ctrl+V into Excel.
The Results to File option in the top nav (looks like a blue arrow) drops the headers.
Identities are nice to have, because composite foreign keys are often tough to work with, however, one does NOT have to make them the PK on the table.
If I understand you right, you want to select items from BOM1 if they don't exist in BOM2? Try this: SELECT a.Item , a.Qty , a.Desription FROM BOM1 a LEFT JOIN BOM2 b ON a.item = b.item WHERE b.item IS NULL
Worked like a charm, thanks!
Thanks for the help!
Yeah it needs to be used about once a week 
What if you saved the query as a stored proc that you called from an excel template that was macro-enabled to refresh the query when opened. The macro could also prompt a save as to csv after that.
Is there a way to save the results to a file from a stored procedure. I have my query now saved in a stored procedure and every time the proc is executed, can it update or overwrite that file. I'm reading some stuff about using BCP somehow, but I cannot seem to get it to work.
OR, if you have an RS service installed you could throw the dataset into an SSRS report which outputs once a week to an inbox or file location.
Well, than, depending on the size of the output you can either plunk it to a folder or email it with sp_send_dbmail. DECLARE @FileName varchar(100) SET @FileName = 'FileName.csv' DECLARE @ExecQuery varchar(4000) SET @ExecQuery = 'SET NOCOUNT ON; select * from TableName' EXEC msdb.dbo.sp_send_dbmail @profile_name = 'Default', @recipients = 'recipient@emailaddress.com', @body = 'Lorem ipsum dolor', @subject = 'Lorem ipsum dolor', @query = @ExecQuery, @execute_query_database = 'DB_Name', @attach_query_result_as_file = 1, @query_attachment_filename = @FileName, @query_result_separator=' ', @query_result_header = 1, @query_result_no_padding = 1, @exclude_query_output = 1
I use bcp once a month to create mailing lists for a large mailer. I created a BAT file to execute (had to brush up my old dos skills) by creating a notepad doc and saving as a .bat. Now I can just double clck the file and it runs. The command line looks like this: bcp "select * from table where thing1 = '8-1-2012' and thing2 = '8-31-2012' " queryout c:\folder1\subfolder2\subfolder3\filename.csv -S srv-name -T -c -t, My .bat file has about 100 of these lines with different where clauses that creates about 100 different csv's for me. I assume it could run a stored proc, but I didn't go that route. I can't remember why, but I pasted the query into the .bat file instead. It works, but it is a pain to troubleshoot. I recommend Notepad ++ for creating and editing if you go this way. Let me know if this works for you.
I just did something very similar yesterday. Create a store procedure and use sqlcmd to call the sp and save the results to the csv file. You can use something like this for the body of your sp. You may have to change some of the syntax. --if the temp table exists, then drop IF OBJECT_ID('tempdb..#x') IS NOT NULL BEGIN DROP TABLE #x END --create the temp table CREATE TABLE #x ( sequence int identity(1,1) primary key , txt varchar(max) ) --create the csv headers INSERT INTO #x ( txt ) SELECT '"d.DeviceName",' + '"m.FirstName",' + '"m.LastName",' + '"m.Building",' + '"m.City",' + '"m.Disposition",' + '"m.LastEditedBy",' + '"m.LastEditedDate",' + '"d.Laptop",' + '"d.DomainName"' as txt -- create the csv body INSERT INTO #x ( txt ) select '"' + d.DeviceName + '",' '"' + m.FirstName + '",' '"' + m.LastName + '",' '"' + m.Building + '",' '"' + m.City + '",' '"' + m.Disposition + '",' '"' + m.LastEditedBy + '",' '"' + m.LastEditedDate + '",' '"' + d.Laptop + '",' '"' + d.DomainName + '"' as txt FROM Assets m, Devices d WHERE m.Devices_Idn = d.Devices_Idn AND m.Status = 'Disposed' --create the contents of the csv SELECT txt FROM #x ORDER BY sequence asc --clean up DROP TABLE #x Here is a link to the sqlcmd help page. [http://technet.microsoft.com/en-us/library/ms162773.aspx](http://technet.microsoft.com/en-us/library/ms162773.aspx)
With regards to the search function, it would probably need to be a simple SELECT query with a wildcard search in the WHERE. The query that returns your dataset for that second grid would need something like this: WHERE [Donor Name] LIKE("*varSearch*") where varSearch is the input string from that box. So "smith" would return every donor with smith in their name. Just be aware this would return Nasmith, Smithsons, etc. You will have to forgive me as I haven't had to use Access or VBA in a very long time, but that is the general thought. I think you may need to work on the exact format of having the text variable in the SQL string. I remember strange double single quoting and such to stick a string variable into a string. **On a related note, it would REALLY be helpful to NOT have a combined field for name. If possible, have a Last Name and a First Name field. Regarding your second question, I think the best way is to use a key value. Each of your donors needs a unique ID. When someone clicks on a donor, you pass that ID into the query that generates your events results. SELECT * from tbl_events WHERE DonorID = varDonorID. I know nothing of your back-end, but maybe a donor table with each donor and their ID, then an event log type table where every event is associated with the donor (each row is the event and donorID of who did the event).
I haven't done any SQL access from VB.net, so I might not be helping much. Does [this link](http://stackoverflow.com/questions/15525955/sql-and-vb-net-select-query-using-data-in-textbox) help any? What I've done in the past is create an ODBC connect that the VB application can use, then use string operations to build the query string and executing it against the DB. THIS IS PROBABLY A BAD IDEA! If you (or someone trusted) are the only user, you might be safe, but if not, SQL injection problems are a huge issue. I only used this in two applications, one was for personal use only, and the other was a mock-up used against a sample database. It might help your search to look up "dynamic SQL" alogn with VB.net. 
I will try that Dynamic SQL Advice. Also the link you sent is more than helpful for a future purpose :). Thank you it helps us view it slightly better. 
Thank you for taking the time to respond! When creating the varSearch string is varSearch what is inserted into the LIKE function or is the string name inserted in its place?
I just arbitrarily picked that as the variable name. Probably should just use "str" as preface. Your vba function will need to accept a str variable. When the user clicks the search button, you are going to need to call your function and pass the value from the text box. Something like CALL fnSearch(txtbox.value). Your search function, fnSearch, would be created to accept a variable like --&gt; Sub fnSearch(strSearch) Use strSearch in the WHERE between the wildcard symbols. Something like SELECT * FROM tbl_donors WHERE [Donor Name] LIKE("*strSearch*"). But as I said above, you are going to need to work on the syntax for putting that variable in the middle of your SQL string. Easy to do in SQL Server, but needs some punctuation when used as part of a string of SQL to be executed within VBA. I guess I should have asked from the start, is your back-end in SQL Server or are your tables sitting in MS Access? If your tables are in Access, may as well not worry about putting the SQL into VBA. Just create and save the query and then call it by name in VBA with a simple DoCmd.OpenQuery. 
This is a sub I wrote recently that takes in EntityKey and ReportDate to use as parameters for a stored proc and sets DAmount and VAmount as the output. There may be a better way to go about getting the values from the reader, but for some reason, I use a for loop to check the field name, then fire off different code inside the select. This is a very simple version of my actual process, so it may seem weird to do it this way. Imports System.Data.SqlClient Private Sub GetDVAmounts(ByVal EntityKey As Long, ByVal ReportDate As Date, ByRef DAmount As String, ByRef VAmount As String) Dim dbServer As String = "SQLServerName" Dim dbDatabase As String = "DatabaseName" Dim dbUserID As String = "DatabaseUserName" Dim dbPassword As String = "DatabasePassword" Dim connstring As String = "Server=tcp:" &amp; dbServer &amp; "; Database=" &amp; dbDatabase &amp; "; User ID=" &amp; dbUserID &amp; "; Password=" &amp; dbPassword Dim sqlConn As New SqlConnection(connstring) Dim sqlCmd As New SqlCommand Dim reader As SqlDataReader Dim StoredProc As String = "Stored_Procedure_Name" sqlCmd.CommandText = StoredProc sqlCmd.CommandType = CommandType.StoredProcedure sqlCmd.Parameters.AddWithValue("@EntityKey", EntityKey) sqlCmd.Parameters.AddWithValue("@AsOfDate", ReportDate) sqlCmd.Connection = sqlConn sqlConn.Open() reader = sqlCmd.ExecuteReader() If reader.HasRows Then reader.Read() Dim i As Long For i = 0 To reader.FieldCount - 1 Select Case reader.GetName(i) Case "DAmount" DAmount = Format(reader.GetValue(i), "f") Case "VAmount" VAmount = Format(reader.GetValue(i), "f") End Select Next i End If sqlConn.Close() End Sub
I highly recommend consulting with a database professional as there are certain business rules and configuration that you should implement that may not be obvious to you now but will be a total pain to fix later. I have worked on databases for 9 years and though I am not a database designer, I can see some key errors in your pictures - the name field as pointed out by someone else is one, the address field, the brackets in phone numbers, etc. Seriously, these may not seem like problems now but they will be in the future and if you don't fix them, you're setting someone up for some headaches in the future.
Thank you! I will read through this apply it and let you know! Thank you so much for your time.
Yeah we weren't to worried about normalization because of the users notes field that they "needed". We plan to reference the names only through SQL commands and not IDs we are talking tens of donors and nor thousands. If this fails I plan to just buy them a good dms as my contribution to the cause.
It's tables made on vb. thank you for all your help!
I hear ya but the point still stands. You are 'now' talking about tens of records but unless the charity wants to fail, you are setting them up for a shitty cleanup job down the line... whenever that may be... and if I had to do it (and I have had to do some), I would not be your number 1 fan.
So essentially all I would be changing would be add a auto-id field and split the names? The phone number is just set to show it that way and is just a group of 8 numbers on the server.
Not necessarily. What I'm saying is that if you don't already spot those things, then you don't know what you're doing enough with databases to do this properly, so get someone who does know enough to help you out. You will probably find someone who will do it for very little or free since it's a charity. Where I work, we have lengthy debates about small bits of coding because we understand just how much it matters in the long run. You need to put a lot of educated and experienced thought into it.
Looks like you need to go UP from CourseRegistrations into Courses and then into Teachers. By the by, the standard is singular, not plural. SELECT s.Name, cr.courseID, t.Name FROM Students s INNER JOIN CourseRegistration cr ON cr.StudentID=s.ID INNER JOIN Courses c ON c.ID=cr.CourseID INNER JOIN Teachers t ON t.ID=c.Teacher If Teacher on the Courses table is a denormalized form of Name rather than the TeacherID, I would buy a hammer and beat the person who designed this DB.
&gt; By the by, the standard is singular, not plural. oh please, not this again OP, plural is fine 
Our IT lead held 3 hour classes starting at 6AM for 3 months to train all of the interns and anybody else willing for that exam. That's how I got mine for "free". 
&gt;For each Data Pump send external job and Data Pump bring in job, a main procedure is formed. The main procedure gearshift the total job, including coordinating with the customers, creating and scheming a puddle of staff processes and presenting cataloguing operation. huh? 
also I noticed 'name' is an SQL command itself. Will having 'name varchar' be a problem later because of this?
Do you have the order mixed up? The vendor table should be created before the advertisement table.
Doesn't matter in some. I know Java optimizes it out, I think JavaScript does too. In C++ i know it matters, but headers are so natural to do now that I forget sometimes. But thanks, fixing the order worked.
I don't think you are doing that correctly. I believe it should be something like the following articles with the results pumped to a file. [http://msdn.microsoft.com/en-us/library/ms179331.aspx](http://msdn.microsoft.com/en-us/library/ms179331.aspx) Edit - There is also a simpler [example here](http://blog.sqlauthority.com/2009/04/28/sql-server-introduction-to-sql-server-encryption-and-symmetric-key-encryption-tutorial-with-script/).
What do you have so far? (also 4 spaces will format as code)
 Create Procedure TransferRegion (@RegionID int, @DistributorId int) As If @DistributorID is null or @RegionID is null BEGIN raiserror ('Requires distributor &amp; region id', 16, 1) END Else Begin If exists (select Distributor.DistributorID, Region.RegionID from Distributor inner join Region on Distributor.DistributorID = Region.DistributorID where @RegionID = Region.RegionID and @DistributorId = Distributor.DistributorID) Begin declare @OldDistributorID varchar(30) declare @OldWage smallmoney Update Region Set @OldDistributorId = DistributorID, @OldWage = Wage Where DistributorID = @DistributorID This is where I'm stuck, not quite sure how to really go about this problem. 
to offset a current value you can do this: ... update set wage = wage -1 ... update set wage = wage +1 ... to get a value from a table into a variable, you can do something like this: select DistributorID into @OldDistributorID from Region where RegionID = @RegionID; To update a value in a table you can do this: update region set DistributorID = @DistributorID where RegionID = @RegionID; Also, make sure you are updating the distributors wage, not the wage column associated to the region. Hope that can get you pointed in the right direction. I am an oracle guy and do not totally understand that syntax to point out syntax errors/conventions.
It sounds like the server is properly configured, since you can connect from another machine on the same local network. There has to be trouble somewhere between the remote network and the one that SQL's running on. If your network is properly configured, there's a good chance that whatever IP SQL is running at cannot be publicly addressed (at least on the SQL ports) by the internet at large. What happens if you open a command prompt on the remote network, and run telnet xx.xx.xx.xx 1433 ..using the IP of the SQL machine? What happens if you try to ping that IP?
Where is this remote network? I'm guessing you are trying to access your database from somewhere connected to the public internet. If so direct access on port 1433 to a database is not recommended. But if you want to do it you will need a publicly available IP address and access to your networks firewall. If you have a public IP address I am assuming it is connected to your firewall. From the firewall you want to do a port forward so that all inbound connection to (public IP):1433 forward to (local IP):1433
It is not a good idea to make a direct connection to a database from the public Internet no matter the port. If this is for an application to access the data I would look into creating a REST Web service to expose the database, then forward connections to the Web service on the Internet. 
Thanks for the tips nepobot, will try them after class.
This is the part where you're supposed to have an epiphany: SQL is not a language. 
What does the "L" stand for in "SQL"?
Listerine.
:-)
I would make a table that contains the years you are interested in. When you cross join this table with doctors, you will have every year/doctor combination. 
I've tested your problem on Postgresql 8.4 Are you sure you mean NATURAL JOIN? Have you tried LEFT JOIN? This type of join returns unmatched rows which can be then coalesced. Another thing I'm unsure of, is can you wildcard coalesce. How can you apply an integer as coalesced value for varchar fields without casting? Also, coalesce takes a list of expressions and returns the first that isn't null. But it returns a single value, not a row. At least PgSQL doesn't allow that.
you are right, that select statement is way off. I can now get it to half work with left join, but it is still dropping the values from TABLE1. I think what I need is a full outer join, but when I try FULL JOIN in mysql, it returns the same thing as a natural join. edit: turns out mysql doesn't have a full outer join so I tried using [this](http://stackoverflow.com/questions/4796872/full-outer-join-in-mysql) workaround, but I'm apparently not doing it right. :p
If you have a database that supports it, you can use the pivot function. You'll also need to specify the years in the query and the data will be presented differently. http://sqlfiddle.com/#!6/3c296/6/0 Alternatively, you can use one query for each year and then left outer join on the visits table. http://sqlfiddle.com/#!7/23edb/35/0
If you're only concerned creating rows for possible date/doctor combinations for dates that already exist (but not for certain doctors), then you can do a cross join to SELECT DISTINCT. Then LEFT OUTER JOIN that to the query you already have: SELECT xref.date_of_visit as "date", grp.data, xref.name FROM (SELECT v.date_of_visit, d.id, d.name FROM (SELECT DISTINCT date_of_visit FROM visits) v CROSS JOIN DOCTOR d ) xref LEFT OUTER JOIN (SELECT visits.date_of_visit as "date",avg(visits.data) as "data", doctor.name, doctor.id FROM visits JOIN doctor ON doctor.id = visits.doctor_id GROUP BY doctor.id, date ) grp ON xref.id = grp.id AND xref.date_of_visit = grp.date ORDER BY date; If you need some rows for dates that don't have any doctors at all, you're best off creating another table like /u/ASUPREMECOURTJUSTICE suggested and cross joining.
You should explain what your input data is, and what you expect, because your statement does not tell me exactly what you want to do (also, i'm no english, so excuse my grammar). But I guess it would be something like this. With SQL Server : UPDATE Products_Joined SET ProductCategory = ProductCategory + ',5612' WHERE ProductCode = 'Example123' With Oracle/PostgreSQL : UPDATE Products_Joined SET ProductCategory = ProductCategory || ',5612' WHERE ProductCode = 'Example123'
What sort of errormessages are you getting?
That is not how coalesce works. You do COALESCE([value1], [value2], ...) And it returns the first value that is NOT NULL (or NULL if they're all null). http://www.postgresql.org/docs/9.3/static/functions-conditional.html
Another reason I'm really grateful that a friend of mine shown me PgSQL when I started dabbling with DB programming back in the day. MySQL: not even once :P No, but seriously, to each their own, but full outer joins is something I'm using more than eny other join types. Can't live without them.
Try ... + ' ,' + '5612' ...
Make sure that you use UNION ALL instead of just UNION when you're using this workaround, otherwise you may miss duplicate rows that should be there.
Also, if you could please indicate your actual RDBMS... MySQL, SQLServer, MS Access etc. They can operate slightly differently.
actually, no, you ~want~ the duplicates removed, since both the LEFT OUTER JOIN and the RIGHT OUTER JOIN will return matches
Is a book linked to more than one Section, and are there any sections linked to more than one Course? If so, when you display the Coursename and SectionNumber you will have duplicated book names. Add a sample of the data you are getting so we can see, also add it as text if possible. Add 4 spaces in front of text to format it properly as follows: SELECT Name FROM Books Edit: Also you need to include a HAVING statement from the instructions it seems. Something like: SELECT b.NAME, FROM b.Books HAVING COUNT(b.Books) &gt; 1 
 SELECT DISTINCT(C.CourseName), C.CourseID, B.Name FROM Courses C INNER JOIN courses_has_sections CHS ON C.CourseID = CHS.CourseID INNER JOIN Sections S ON S.SectionID = CHS.SectionID INNER JOIN section_has_books SHB ON SHB.SectionID = CHS.SectionID INNER JOIN books B ON B.BookID = SHB.BookID HAVING COUNT(B.Name) &gt; 1 It's 2 in the morning here, let me know f I am on the right track here, and will look at it tomorrow for you again. Also hopefully someone else will also have a look for you. 
Have you checked the MS virtual academy course for this exam ? It's only videos (I prefer books :) but it's decent http://www.microsoftvirtualacademy.com/training-courses/querying-microsoft-sql-server-2012-databases-jump-start
You may get more help if you're more specific ... which DBMS ? most of them have a command to export data and import it into another db (for MS SQL server it is bcp, for postgresql psql or pg_dump ... :)
Both of those would work ... exists is usually inefficient, in/not in is better; you can also use minus/difference
big blocks of run-on text are ugly and hard to read here ya go -- CREATE TABLE Booking ( BookingRef INT(10) PRIMARY KEY , CustCode VARCHAR(10) NOT NULL UNIQUE , BookingDate DATE NOT NULL , AgreedPrice INT(5) NOT NULL , TripCode INT (10) NOT NULL , DepDate DATE NOT NULL , NumPersons INT(3) NOT NULL , InsuranceYN VARCHAR(1) NOT NULL , FOREIGN KEY (CustCode) REFERENCES Customer (CustCode) ); CREATE TABLE PriceAvail ( TripCode INT(10) , DepDate DATE , PRIMARY KEY (TripCode, DepDate) , Price INT(5) NOT NULL , TotalPlaces INT(2) NOT NULL , FOREIGN KEY (TripCode) REFERENCES Booking (TripCode) , FOREIGN KEY (DepDate) REFERENCES Booking (DepDate) ); the reason your FKs in PriceAvail are failing is because the columns that they reference in Booking are not defined to be unique (i.e. they have to be either PK or UNIQUE)
Thank you so much. It always seems so obvious afterwards. Really appreciate it
This wasn't a failure of technology. It was being run by someone without experience in software and who wasn't given the power to make decisions: &gt; ...over the past three years five different lower-level managers held posts overseeing the development of HealthCare.gov, none of whom had the kind of authority to reach across the administration to ensure the project stayed on schedule &gt; &gt; As a result, the president’s signature initiative was effectively left under the day-to-day management of Henry Chao, a 19-year veteran of the Medicare agency with little clout and little formal background in computer science. &gt; &gt;Mr. Chao had to consult with senior department officials and the White House, and was unable to make many decisions on his own. If you can't make decisions (and say "no" to feature changes), you can't build a working system. 
Okay but the count should be on Books should it not? And not on Sections? 
MarkLogic? I've been doing this 22 years and I've never run across that system Is it obscure or am I just in the dark?
Sorry, seems like am useless on this one. I am concerned that no one else is posting on this one. Can I suggest that you post this again, and see if you can get more feedback.
I had to Google it too. It's an xml store, not an RDBMS, which is absurd since they have highly relational data with almost entirely single-record updates.
Yeah, I just looked a little deeper. Looks like an xml store that you access through an API. API's are great, don't get me wrong, but I hate using them to access data. It's always a mess (type casting anyone?) Also, they claim to have: &gt;over 500 deployed enterprise projects That's not a huge track record when you're doing your due diligence as a customer looking for a vendor.
&gt;“This is real simple,” Mr. Obama said, during a speech in Maryland on Sept. 26. “It’s a website where you can compare and purchase affordable health insurance plans side by side the same way you shop for a plane ticket on Kayak, same way you shop for a TV on Amazon." I'm an engineer at Amazon so this made me laugh. Amazon has been around for 15+ years and has 10,000 highly paid engineers to make it work. It needs that many because running a website at scale is *not* simple. I imagine it's even more terrible when your dependencies are the IRS and the Social Security office and your PMs are the Center for Medicare and Medicaid. Frankly, the gov't could not pay me enough to work on this. 
The HHS and Medicare provided the PM work. The administration didn't give an *initial* spec until less than 8 months prior to go live, and gave numerous major change requests until 30 days prior to go live. Really, it's shocking that the site works end-to-end for even one simultaneous user given the massive incompetence just on the PM side of things. 
You would have needed a lot more management firepower to have any chance of success - think about the kinds of petty bureaucratic turf-fighting and procedures that would have been involved with the IRS, State, and the Social Security Departments, all insisting that their procedures be followed to access the data. It must have been a nightmare. 
No one in HHS would have had any technical knowledge to choose technologies. The private company hired did that.
Maybe SQL is the wrong tool. Have you considered using a full text index like apache lucene?
no problem :) I use lucene to index around 500G of office documents. It is usually pretty fast.
If you enjoy games, playing the Schemaverse will definitely help get your SQL skills up quickly. https://schemaverse.com https://schemaverse.com/tutorial/tutorial.php The tutorial is written specifically to try and teach SQL and the game at the same time. Edit: This is heavily Postgres based but the comfort you gain with SQL will be mostly platform independant 
Can you expand on what you're trying to do? And the platform you're using? If you're looking to take something like this: | A1 | B1 | | :-- | :-- | | test | hide me! | | notest | show me! | | three | show me! | | test | hide me! | Then you'd have SQL approximating this: select A1, case when A1 = 'test' then null else B1 end as B1 from myTable Which will give this: | A1 | B1 | | :-- | :-- | | test | (null) | | notest | show me! | | three | show me! | | test | (null) | Is that what you're trying to do?
Then you're not looking for SQL help--you're looking for web development help. That's a display-side issue, not a data-side issue. You may be best off asking around [StackOverflow](http://stackoverflow.com) rather than /r/SQL. I know plenty about web development, but your requirements currently are wildly vague and I don't even know where to start.
Sweet! I've been a bit rusty on the SQL front myself due to using too many ORMs and not writing enough queries. I've been using [SQLZoo](http://sqlzoo.net/wiki/Main_Page), but going over stuff I already know is painful. I'm hoping the game format is useful for me as well as OP :)
I think ORMs are doing that to a lot of people, and not that there is anything wrong with that, but really knowing SQL has pretty great benefits when it comes to optimization, security and ad-hoc reporting. Hopefully the game helps :) If you have any troubles at all, free free to stop by #schemaverse on the free node irc network. 
he knows about amazon.com, so I'm thinking he's miles ahead of Adams and Hoover.
Hoover was a mining engineer who came from poverty and made himself extremely rich by applying technology to existing problems. Hoover would crush Obama in any contest of managerial or technical skills. Obama's most advanced skill is using an Iphone. 
So what you're saying is that Obama is more in the know about modern technology than any other president? I work with people of the same mindset as Obama. He doesn't know tech. He uses an iPad. He monitors his kids facebook. He checks NFL scores off his BlackBerry. He's aware of what technology is capable of. He has no idea how it works, much like I have no idea on how to deal with complex political relations with foreign countries. The idea of this website is great, but the implementation sounds like it hasn't been the best (although may be significantly better than things like the unified email system of the military which I've read was a disaster).
What exactly is going into your Description field for SecondaryTable? Is it going to be a concatenation of the UserId's details like First, Last, Gender, Email, etc? Or something else?
&gt;So what you're saying is that Obama is more in the know about modern technology than any other president? Um, no? Obama [mocked](http://www.quickmeme.com/meme/3obt12) Rutherford B. Hayes for being technologically backward, but it turns out Hayes was actually much more savvy towards new technology than Obama appears to be. In my view, Ipad and Iphone users are basically 'Microsoft BOB' users who buy into the brilliant, genius level marketing directed at them. It's a simplified interface which severely limits your ability to do anything on your own except go down the path prepared for you, but the Marketing of the Ipad/Iphone is that if you use one, you're somehow 'in the know', 'hip', 'technologically sophisticated' for using a childish interface that assumes you don't know anything about tech and don't know TCIP/IP from RAM, and can't be trusted to manipulate the tech yourself. Obama isn't 'in the know' about modern technology. That isn't so bad, but he *thinks* he knows about it, and is wrong. That's much worse. 
Unless I misremember, you can't define triggers on temp tables. Edit: typo
What makes you think that lack of technical knowledge stops any would-be IT manager? Or are you desperately trying to preserve the illusion that the administration is competent? 
I guess I don't get the point of this SecondaryTable. Why can't you do everything you need to do directly to the main Users table? If your Users table had a DateAdded datetime and you used a @now datetime variable which you set to getdate() at the top of the proc, then you could use that @now variable to select back out what you just inserted.
As a step 1, I would just learn some basic SQL. You can learn the basics in an afternoon, I used w3 schools. its really straight forward. Its a very legible language. http://w3schools.com/sql/default.asp
Hi there, Thank you again for the help the other day with my SQL query. (Found here:) reddit! I found out last week that there are more records in the source csv than are in the final destination table. As such, the merge query you supplied doesn't insert those records because they aren't in both places. My question is: how do I add to / modify the existing query so that it will insert the additional records from the source into the destination ONLY if they don't already exist? I tried adding an insert query as a step in the job and, while it did work, in duplicated all of the records that were part of the merge in the previous step. Again, thank you for your help! 
Yes. Essentially, user data is spread out across multiple tables, and I need to fill a new table with all of that data. 
Thanks very much! I think that is mostly what I needed! You had mentioned checks and triggers, what would I be using those for in this situation? EDIT: Also for some reason or another when a class requires a prerequisite from another class that also requires a prerequisite this query won't work. For example if Mathematics 304 requires Mathematics 302 and Mathematics 302 requires Mathematics 301 then it won't show up. However if Mathematics 304 requires Mathematics 302 and Mathematics requires nothing, therefore NULL it will work... Any idea why?
Aha! There was a bug in my code! That's what I get when talking with my wife while trying to write SQL. So, the issue here wasn't prereqs with more prereqs--it was classes that a student hasn't taken don't show up. The reason is because the `where` clause--that `where` clause happens after my `left join` of `Student`, which means that it pares down the rows *after* I've brought them in...which means that all those lovely `null`s didn't show up. No! No worries, an easy fix. I'm going to introduce to you a concept called a `cross join`--what this does is it joins every row of one table to every row of another. This way, we get the student regardless of the classes they've taken--and the credits will show up as `null` if they haven't taken that particular class. I put this in [a SQL Fiddle here](http://www.sqlfiddle.com/#!2/8e3f4/17/0), by the way. For posterity, here's the updated SQL: select cr.crs_title, st.stu_name, sum(e.Credits) as credits from Prerequisite p inner join Course c on p.crs_code = c.crs_code inner join Course cr on p.crs_requires = cr.crs_code inner join Section sec on cr.crs_code = sec.crs_code cross join Student st left join Enrollment e on sec.sec_id = e.sec_id and st.stu_id = e.stu_id where c.crs_title = 'Mathematics 304' and st.stu_name = 'Martha Jones' group by cr.crs_title, st.stu_name having coalesce(sum(e.Credits),0) = 0 I also did the `group by` with `stu_name`, in case you wanted to do multiple students at a time (for example--finding who doesn't qualify in a list of students). To answer your first question, though--checks and triggers. You'd use these to enforce data constraints on the database layer. That way, if somebody were to go directly against the database (or, say, write a new API), they don't have to re-code constraints--the database takes care of them for you. That said, more enterprise-y databases (such as PostgreSQL, MS SQL, or Oracle) all do this much better than MySQL...which is clunky, at best, when dealing with complex constraints and rules. Only worry about them if you really intend to get very heavy into database development and data-driven applications.
admiralwaffles is the best! Also helped me with my university stuff :)
Anyone else get that invalid cert warning from chrome when trying to go here? This game sounds like a great way to learn, but idk if I'm that trusting...
I really don't know why chrome hates my cert. You really shouldn't be using an important password there anyways though so I wouldn't worry too much about your communication with the server being compromised. 
Thank you very much! This greatly helped me!
You need a group by clause and a sum aggregate function. Select DATENAME(q,"DateTime") as "Quarter", DATENAME(yyyy,"DateTime") as "Year", SUM(In_TotalBytes) as "Total Bytes" from interfaceTraffic where DATENAME(q, "DateTime") = '3' and DATENAME(yyyy,"DateTime")='2013 Group By DATENAME(yyyy,"DateTime"), DATENAME(q,"DateTime") Order By Year, Quarter The group by command will collapse the results together and the sum command will cause it to add the In_TotalBytes together based on the defined grouping. More info on Group By here: http://technet.microsoft.com/en-us/library/ms177673(v=sql.105).aspx Good luck!
Are you looking for the total bytes that a week takes up in your table? If this is true, what are the data types you're working with in this table? If you're using VARCHAR or TEXT fields, you can figure out the bytes by summing the LENGTH(Field1), etc. If you're working with other fields, you'd need to look up how many bytes they store per. Hope this helps!
Thank you sir, This is exactly what I needed I got hung up on the Grouping. This got me what I needed: Select DATENAME(wk, DateTime) AS Week, SUM(In_TotalBytes) as "Total Bytes" from interfaceTraffic where DATENAME(q, "DateTime") = '3' and DATENAME(yyyy,"DateTime")='2013' Group By DATENAME(wk, DateTime) +/u/bitcointip flip
interfaceTraffic is a view that collects data from 3 tables. Summing is safe as its just a collection of 5 minute stats every 5 minutes. after 30 days it gets rolled up into hourly averages for every hour in another table so while its not as accurate anymore I can still just get a sum from the timeframe I'm looking for.
Previous tip did not work have some BTC: +/u/bitcointip $1
You may have slightly better performance using DATEPART (which returns int) instead of DATENAME (which returns nvarchar). Shouldn't matter unless you have a ridiculously large dataset.
Thanks! I actually figured that out when trying to to return a month number. For a similar set of data. I've since changed the rest to match. 
Depending on your dataset and your requirements for speed, you should consider storing the result of DATENAME(wk, datetime) in a persisted, computed column. By doing this you can create an index on the persisted column, so rather than having to do a table scan you can use an index to find the "base set" for your sum aggregate. You can make the computed column and index by doing the following: alter table interfaceTraffic add wk as DATENAME(wk, Datetime) PERSISTED create index idx_wk on interfaceTraffic(wk) I prefer not using functions for my where and group bys all together. I normally rewrite "where month(datetime) == x" to "where datetime &gt;= 'year-month-1 and datetime &lt;= 'year-month-(maxdate in month). Again, this enables me to use my indices.
I'm pulling data from an application's database, editing the tables may affect support from the vendor. This query is only going to be ran manually once a quarter. And currently only takes ~5 seconds to run, I may investigate using a helper table in the future. Thanks for the heads up.
I am not sure what SQL Server you are on, but this is on the microsoft website: Beginning with SQL Server 2005, the behavior of schemas changed. As a result, code that assumes that schemas are equivalent to database users may no longer return correct results. Old catalog views, including sysobjects, should not be used in a database in which any of the following DDL statements have ever been used: CREATE SCHEMA, ALTER SCHEMA, DROP SCHEMA, CREATE USER, ALTER USER, DROP USER, CREATE ROLE, ALTER ROLE, DROP ROLE, CREATE APPROLE, ALTER APPROLE, DROP APPROLE, ALTER AUTHORIZATION. In such databases you must instead use the new catalog views. The new catalog views take into account the separation of principals and schemas that was introduced in SQL Server 2005. For more information about catalog views, see Catalog Views (Transact-SQL). Perhaps you can try something like this: IF OBJECT_ID('TEMPDB.dbo.#T') IS NOT NULL DROP TABLE #T CREATE TABLE #T( [RowNum] [int] NOT NULL , [Name] [varchar](100) NOT NULL, [User_drop] [INT] NOT NULL, ) ON [PRIMARY] INSERT INTO #T([RowNum], [Name], [User_drop]) SELECT ROW_NUMBER() OVER (ORDER BY name) as RowNumber, Name, 0 FROM dbo.sysusers WHERE PATINDEX('%\%',[name])&lt;&gt;0 OR (name NOT IN('guest','INFORMATION_SCHEMA','public','sys') AND SUBSTRING(name,1,3) not in('db_','dbo')) ORDER BY name DECLARE @Counter INT=(SELECT MIN(rownum) FROM #T WHERE User_drop=0) DECLARE @MAXCOUNTER INT=(SELECT MAX(rownum) FROM #T ) DECLARE @user VARCHAR(100)=(SELECT ''''+name+'''' FROM #T WHERE RowNum=@Counter) DECLARE @sql varchar(MAX) WHILE @Counter &lt;= @MAXCOUNTER BEGIN set @sql=' Drop User ['+@user+'] ' execute (@sql) Update #T SET [User_drop]=1 WHERE RowNum=@counter SET @Counter =@Counter+1 SET @user=(SELECT name from #T WHERE RowNum=@Counter) end SELECT * FROM #T --This way you see everything you are about to drop and the name is already is a variable. Worth a shot. 
Your first table listed doesn't tell me that 3C is able to replace 1A. I would assume there could be scenarios where 3C could replace 2B but not be allowed to replace 1A. Anyways, I would create two tables. One table would be for Parts, and the other would be for PartReplacements. In this example, you would have a row in Parts for 1A. PartReplacements would have two foreign keys to Parts. In this example, there would be two rows for Part 1A in the PartReplacements table. One row would have a replacement value of 2B, and the other row would have a replacement value of 3C. This model is able to implicitly handle your "grouping" scenarios, but is also more concise and easier to maintain for scenarios where parts cannot replace parts that are more removed.
"now i need a query which gives me sum of qty_returned and group it according to the date_of_order." This is not possible to do given the information that you have provided with your description. You've got nothing in table 2 that points back to a specific combination of OrderID/DateOfOrder. 
select t1.date_of_order, sum(t2.qty_returned) from t1, t2 where t1.order_id = t2.order_id group by 1
Here's a basic outline. I haven't used any of your date filters for the sake of simplicity. select sub.date_of_order ,sum(t2.qty_returned) as qty_returned from t2 join ( select t1.order_id ,max(t1.date_of_order) as date_of_order from t1 group by t1.order_id )sub on sub.order_id = t2.order_id group by sub.date_of_order The idea is that you use a subquery to get one row per order_id from the table that has the date_of_order field. This will get rid of the inflation caused by order_id appearing multiple times in that table.
see my other comment. OP can query table 1 to get the order_id/date_of_order relation, then join that to table 2.
this will still give an inflated qty_returned. If an order has two rows in t1, that order's qty returned will get added in twice in the sum. see my reply for a solution.
Thanks, I'll have to look into this. While I was waiting, I wrote 2 SELECT statements: One that got me the every order along with the date and customer name associated with it, AND One that got me the total values of each invoiced order. Is it possible to join these on the orderno?
Also using the same temp tables i posted in the last one if you use the following, it breakdown the totals by invoice and parts SELECT A.Custno ,A.custname ,A.invno ,A.partno ,A.Part_descr ,SUM(A.TotalCost) as Invoice_Total FROM (SELECT a.Custno ,a.custname ,d.invno ,b.Orderno ,c.partno ,p.Part_descr ,c.orderprice ,e.shipquantity ,c.orderprice*e.shipquantity as TotalCost FROM #Customer a LEFT JOIN #orders b on a.Custno=b.Custno LEFT JOIN #orderprod c on b.Orderno=c.Orderno LEFT JOIN #invoice d on b.Orderno=d.orderno LEFT JOIN #invprod e on d.invno=e.invno and c.partno=e.partno LEFT JOIN #part p on c.partno=p.partno)A Group by A.Custno ,A.custname ,A.invno ,A.partno ,A.Part_descr
http://dev.mysql.com/doc/refman/5.1/en/create-table.html
He asked for MS SQL, not MySQL.
Yep, My bad sorry
Ofc, /r/SQLServer exists...
Don't use "equal" in your WHERE clause in that case. If you use "equal", your subquery is supposed to return only one value (here it will return all the last names of the editors). Use the IN clause in that case. But this still doesn't work, because if John Smith is a writer and an editor, the writer-only William Smith will be considered writer-and-editor also. Do you understand why ? There are multiple ways to solve your problem (I assume it's homework ?). The best way is simply to JOIN the tables Writers and Editors ON the last name and first name (I'll let you search a bit for which JOIN to use in this case). This way you can naturally select all Writers that also are Editors. If you want to use a subquery, you'll have to concatenate first name and last name, to be able to go around the Smith issue (see above). Finally, you can ask you one more question, which is an opening to some modelisation concepts : what if two writers/editors have the same name ?
Thanks again for teaching a man to fish! That was all I needed to get it working. Have a great one!
Actual article is 3 links in: http://opensource.com/business/13/12/infinisql
Is this TSQL/SQL Server? If so, you're likely going to find what you need with [PIVOT](http://stackoverflow.com/questions/10428993/understanding-pivot-function-in-t-sql). The example there at StackOverflow is pretty good, but you can look it up in Books Online for more technical info.
TSQL and i have looked at that example actually and it just confuses me ha
Do you know the finite number of events or is this meant to be for any number of possible events? You could hard code it with CASE statements if you know the exactly number of unique events that you're dealing with. If you don't know that or the number can fluctuate over time, then you'll have to use PIVOT or build the command string dynamically into a variable and execute it from there. I'd go with the hard coding unless you're feeling frisky.
No certs. I started out developing Powerbuilder (PB 4... that long ago) against SQL Server 6.5. We didn't have a DBA. I took it upon myself to learn SQL in more than the usual developer sense - then when the recession of 2001 came along I was able to find work as a report developer. I did that for a couple of years, and then got a job as a database developer. I worked as a database developer for about 6 years, but when you do that, you tend to find that you are pushed relentlessly toward the DBA role, and eventually I gave in. I've always been more a 'code and design' DBA than an operational one, but as I've gone along I've gotten more and more operational expertise by necessity. My time writing reports against an offshored database was probably the most valuable. The database was SO bad that I had to learn all kinds of techniques to get data out in a reasonable time.
And just generally stick to any alias you use. That subselect has 3 tables with aliases and you only use them less than half the time.
PIVOT is most likely what you are going to need here. If you can post a example of the tables and a few rows of (dummy) data I can try to give you an example query.
Yeah, what you're asking for is one of the more difficult things to do in SQL, no matter what flavor. PIVOT is probably your best bet, though like jotate mentioned, if you know exactly how many/which events you want to report, you could hard code a query that will do it. Other options would be to use dynamic SQL to build a query based on a previous query, and is also a pain to get right.
It may take some effort, so buy your DBA some of his/her favorite caffeine, but you can fix this manually on the back end. SSRS has a pair of databases fueling it, most importantly, ReportServer. Start with the Catalog table to identify the objects you're looking for - the new data source, and all of the reports that link to it. It should be pretty simple to trace out from there; if I weren't on a tablet I'd try to provide more detail. If you get stuck, sort through the stored procedures in the database and find the one used to update a data source/data set reference via Report Manager. In the future, I'd recommend getting your reports into one or more Visual Studio solutions that you can rapidly re-deploy with - I ran into a similar issue that you're seeing here many years ago, luckily our report devs were well organized, and simply needed to fire up Visual Studio, hit deploy, and sit back as everything was put back to normal within seconds. Also, come visit us at /r/SQLServer - we're completely focused on the MSSQL stack, while /r/SQL here is a little more generic :)
Try digging into the sprocs then and following along with Microsoft's logic; they're very cleanly written and easy to follow.
Yea its a requirement for a final project in a database class
Than I would sit your teacher down with a spreadsheet and have him draw up exactly what he would like the output to look like. Dyanamic pivot is one of the more difficult (and inconsistent) things to do in reporting. If he can show you, statically, what columns he wants to see, then you'll be set.
Are you talking about the permanent Table1 or the temporary #Table1 (that disappears)? It doesn't look like you're creating the permanent Table1 as a part of the process, so, without a DROP call, I don't see how it disappears. Also, that many temp tables might be a bad idea (doesn't leverage indexes, adding indexes adds overhead, TempDB pressure). Maybe CTEs might be better?
Turns out that this is a known bug in SQL2008 and MS would not help us with it. We worked on fixing it programatically but just ended up upgrading.
For your first question, if your sql is like: SELECT * FROM TblA JOIN TblB on TblB.ID = TblA.ID JOIN TblC on TblC.ID = TblB.ID There is no need to add TblC.ID = TblA.ID to the second join. Your database is smart enough to figure out that (Transitive Property of Equality). In general you should try and join as many keys per table as possible. This will allow the DB to use an index. However joining other fields that are not keys (or not in an index that covers all fields in the join) means that database has to then read from the table which would make the query take longer. As for Inner Join vs Left Join, you don't make this decision based on performance, you make this decision based on the requirements of your query. 
ok, the create statements, are they interrupted by a 'go' or begin tran end tran statement? here is the deal with temp tables, they only exist for the duration of the transaction. So here is a thought (please someone prove me wrong this is just speculation) if that query has gotten large enough it might spawn separate transactions which cannot see each individual temp table. try running it specifying the query in serial mode or MAXDOP = 1 you could also specify global temp tables but I would not recommend that what so ever. 
/r/domyhomework is -&gt; that way...
Hey, I created that sub! 
For the address filter for Sioux falls I would simply use patindex('%sioux falls%', youfieldname)&lt;&gt;0 in the where clause. Of course if your server forces case sensitivity make sure to correct for that.
Well I see we've arrived at an impasse. I would CTRL+F and find the word DROP and kill it with fire wherever found. You don't have to explicitly drop #tables unless you run the SQL ad-hoc multiple times in the same window.
That being said, having an index does not mean a fast query. Columns with few distinct values(low cardinality) will not benefit from a traditional index. depending on the size of the table it could be viable to start looking at [partitions](http://docs.oracle.com/cd/B28359_01/server.111/b32024/partition.htm) to logically fragment your data into buckets based on those few distinct values. Oracle is smart enough to look in the right bucket based on the where clause. (there are a million caveats to this and this is the best case scenario)
select * from project where projectid not in (select projectid from student) 
Don't worry about "efficiency" (which I assume you mean "fastest to execute") of inner vs. outer joins. You might as well ask "which is faster, multiplication or addition?" **It doesn't matter because the two types of joins give different results.** Worry about which one is correct for what you're trying to achieve. select u.name, d.name from users u inner join dept d on (u.deptid = d.id) gives different answers than select u.name, d.name from users u left outer join dept d on (u.deptid = d.id) 
try this: select MAX (numberSold) AS maxNumber, MIN (numberSold) AS minNumber, AVG (numberSold) AS avgNumber, year FROM cd group by year 
Is the secondary table a requirement or can you use a view or computed column?
... order by year
&gt; Also, would WHERE year &gt;= 2000 work to give all years in one query on separate rows? or do I need to create one query statement for each year? The key there is the "group by year" that he added. When grouping, you get one row for each unique value in that specified column -- in this case since you are grouping by year, you will get one row for every unique year. The "WHERE year &gt;= 2000" is what limits the results you get to only those years.
Going to guess it's with this line: &lt;%vCourseID := getFormattedCourseID(tname);%&gt; Since `getFormattedCourseID` is a custom function, you care to share that code? My guess is that it takes something other than `tname`. Or a different type than `tname` is...
Well there's your problem, mate. This opening: create or replace function getFormattedCourseID return varchar2 is vCourseID course.course_id%type; vFormattedCourseID varchar2(40); Means, "Hey, create me a function named `getFormattedCourseID` with no parameters, and it returns a `varchar2(40)`. By the way, I'ma have two variables through this: `vCourseID` and `vFormattedCourseID`." What you *actually* want is this: create or replace function getFormattedCourseID (vCourseID IN course.course_id%type) return varchar2 is vFormattedCourseID varchar2(40); Which means, "Hey, I'ma create a function called `getFormattedCourseID` and it's got a parameter called `vCourseID` of type `course.course_id%type`. Also, I'm gonna throw a variable called `vFormattedCourseID` in there. And spit me out a `varchar2(40)` while you're at it!"
Follow up question, what are the max, min, and avg number of copies sold for DEBUT releases only? [Hint:](/there's a function in SQL that does a counter, but resets itself on whatever grouping you specify) 
There is no datatype called TIME in Oracle.
select MAX (numberSold) AS maxNumber, MIN (numberSold) AS minNumber, AVG (numberSold) AS avgNumber, year FROM cd Where year &gt;= 2000 --would give you 1 row select MAX (numberSold) AS maxNumber, MIN (numberSold) AS minNumber, AVG (numberSold) AS avgNumber, year FROM cd Where year &gt;= 2000 group by year order by year --will give you individual rows.
http://docs.oracle.com/cd/B19306_01/server.102/b14225/ch4datetime.htm
I don't see how removing a temp table from a transaction or query would wack a perm table. Are you doing something in the temp table or transaction code to drop the perm table? Maybe I missed something in the discussion. You can always run profiler. It should show you when the drop command is being sent.
Do you have the ability to run a SQL Profiler trace against your test server? This would allow you to see exactly what SQL is being passed to the SQL Server.
You need to figure out why this doesn't evaluate as true for the desired record(s): CustmerID=Forms!CustomerF!CustomerID Possibly a spelling error? CustmerID vs CustomerID? I don't use MS Access or whatever this is. Odd that it doesn't throw an error on a misspelled field name. 
I'm making the database in Access, so I don't have a test server. I edited that into my post so that people now know what program I'm using. I should have specified, oops. :S
Are you sure **CustmerID** is correct? I don't see that particular spelling anywhere else, but that's what you are using in your where clause. Edit: humor me, try **CustomerID** instead. 
Try replacing Forms!CustomerF!CustomerID with a string of a valid CustomerID (e.g. "1"). If that works, figure out why Forms!CustomerF!CustomerID isn't returning that string.
I JUST FIGURED IT OUT. It was because I took the square brackets out. The guy doing the seminar always deleted his brackets to make his code look neater, and that was my problem. God dammit. What a joke. So now my code looks like: &gt;SELECT [WorkOrderQ].[WorkOrderID], [WorkOrderQ].[CustomerID], [WorkOrderQ].[WorkOrderAddress], [WorkOrderQ].[InitialContact], [WorkOrderQ].[PriorityDescription], [WorkOrderQ].[StatusDescription] FROM WorkOrderQ WHERE [WorkOrderQ].[CustomerID]=Forms!CustomerF!CustomerID ORDER BY [WorkOrderQ].[StatusDescription]; I don’t know why it would work for him and not for me ughHG Thanks for your efforts though! I really appreciate it! 
I don't use Access, so maybe it requires the brackets, but I bet the real fix was correcting CustmerID to CustomerID. Just sayin'
Sorry I swore that there weren't any spelling mistakes, and I thought that I had checked over it, but you're right too. I just didn't catch it. I guess I corrected the mistake without knowing it when I was playing around with the brackets. You are definitely correct in what was wrong.
Thanks! 
my guess would be a comma in a free text field screwing with your column order. If you use format files you can go into more detailed column definitions, which might be a good idea in your case.
I would use an inline view to build an order using row_number(). Then you can use a case to define each bucket. in this example it is in increments of 25. The WITH is just to build a dataset without defining tables. WITH data ( pk ) AS ( SELECT 1 UNION ALL SELECT 1 + pk FROM data WHERE pk &lt;= 100 ) select t.rNum, case when rNum &lt;= 25 then 1 when rNum &gt; 25 and rNum &lt;= 50 then 2 when rNum &gt; 50 and rNum &lt;= 75 then 3 when rNum &gt; 75 and rNum &lt;= 100 then 4 end from ( select row_number() over(order by pk) rNum from data ) t **EDIT** Since you only have 9 categories, i would just hardcode them into the case(e.g when rNum &lt;= 250 then &lt;category 1&gt;). If you have a need to work with hundreds of categories then you could look at a different approach.
Yes, but there must be a match in number of fields and data type. Don't know which database you are using, but it would be something like ALTER TABLE atable ADD CONSTRAINT FK_atable_btable FOREIGN KEY(a, b) REFERENCES btable(x, y) Though if you need to have relations like this, it is usually a sign of the database model not being normalized enough.
Thank you so much, that is exactly what I was looking for, I completely forgot about cases. Thank you!!
Is column A just a concatenation of columns X and Y? I don't see how one column in one table will have the SAME EXACT value as two different columns in another table.
I think you can also use the [NTILE](http://technet.microsoft.com/en-us/library/ms175126.aspx) function. ;WITH data ( pk ) AS ( SELECT 1 UNION ALL SELECT 1 + pk FROM data WHERE pk &lt; 100 ) SELECT d.pk, NTILE(4) OVER(ORDER BY d.pk) from data d
&gt; I don't see how one column in one table will have the SAME EXACT value as two different columns in another table. this here is the essence of the problem -- the attempted sql would create two FK relationships that are "ANDed" but it appears OP wants them to be "ORed" tables need redesigning
Are you sure its memory causing the issues? I would dig a little deeper into how each instance is using memory to see if its genuinely the cause of the problem before doing anything drastic. How big is each db? Also, there are a few reasons they might be in separate instances - eg. Different collations, different instance settings, etc
Well, one big advantage of setting up 2 instances for production and a datawarehouse, is the relative ease of moving the datawarehouse to its own seperate machine or VM for that matter. Also its easier to manage the resources available to both instances. In SQL 2008+ you can do that with the resource governer, see [here](http://msdn.microsoft.com/en-us/library/jj573256.aspx) and [here](http://technet.microsoft.com/en-us/library/bb934084%28v=sql.105%29.aspx) Are you sure you are running into memory problems at night ? I'd have a good look at the wait stats first, to rule out the cpu and io subsystem as the bottleneck. The next thing you might want to check is how long locks on your production DB are held. That could be the issue too. Again, you will see that in the wait stats. One thing you could do if need be, would be to run your ETL on a Snapshot of the production DB, which is quickly created, while at night the overhead shouldn't be a major issue. But then you could run your ETL on the snapshot instead, which prevent locking on your prod DB. The downside is that you need one of the pricy licences for this feature, otherwise its a very good solution for long running ETL processes. With the limits on memory, I think the locking and or CPU are more likely candidates. Anyway, thats my 2 cents without knowing the environment.
This is a function I am going to stick in my back pocket, because I love the idea of it, and have a few possible uses in my head already. I love learning of new functions, so thank you!
Separate VMs/servers = licence implications for SQL, overhead for additional software licensing and support costs for the os. My pref is for multiple instances for segregation and manageability for prod with dev test on separate servers / vms
Do you mean to display a list of divIDs in a single prodID line?
Yes. I want to echo the name of each divID that a prodID has in one row. Currently it echos a single divID on each instance of the same prodID.
which dbms please? (see sidebar, eh) if you're using mysql you have the lovely GROUP_CONCAT function available
Yes, I am using MySQL. Sorry for not including that!
THANKS! This works PERFECTLY!
Sounds like you need a constraint that is NOT a real foreign key. You can mimic a custom foreign key by creating an Insert trigger which checks whether the inserted value appears in a union of FRUIT(x) and FRUIT(y), or some similar logic. 
No problem :-)
Yea it could probably be done with a check constraint. Something like ALTER TABLE atable ADD CONSTRAINT chk_atable_fk CHECK a IN (SELECT x FROM btable UNION SELECT y FROM btable) Though the reasoning as to why someone would want this escapes me. This also does not protect the referenced table I think. So values can be deleted from btable and remain in atable. Data inconsistency waiting to happen.
&gt; For datawarehouse processing with that little memory, disk could also be a candidate true, it depends highly on the volumes. It's very hard to say anything without knowing details. I would hope thou, that the db files for those 2 instances are NOT located on the same resource. That would be..... well... bad..... ;) If they are, for heavens sake, at least move the respective temp db's to different ~~file servers~~ / luns / disks ssds etc
Definitely, with the low resources mentioned, I would bet there is no SAN involved.
&gt; y, I could see where these two could peacefully coexist in the same instance. The question is thou, what will that do to the caching on the production DB. Having both on the same instance would not be my first choice I have to say. Seperate Servers would be the fist choice, but allas, how often do we get what we want ? It's to many variables, is caching even an issue ? Fast IO would prevent that, how big is the production DB, how high is the memory stress of the ETL, how is the cube set up, where is the cube located, how high is the IO load of the production DB, how critical would a "populate the cache" period in the morning be, and a crapload of things I can't think of right now. Complicated issue those databases 
well, it could be a central SAN that serves a lot of servers, small chance, but its a chance ;) I'm not an general server admin, so well, I'm the guy that tells the admins "hey, io subsystem is fucked, fix it", not someone to optimize a network infrastructure or SAN admin. I only have a basic knowlage of those babies, but it's enough to know that I don't know a lot ;) //edit : Neither do I need to know, or want to know. DBA &amp; DB dev is enough of a responsibility
In a privacy / security oriantated environment, each customer would have his own instance. If you have to do the heavy into data security audits, you are gonna run into problems having all your customers on the same instance. Dev and Prod on the same instance wont ever happen in any decent setup ofc 
I can see the reasoning in cases where an older design gets dropped in one's lap (been there). Old rule: parking permits can only be issued to employees. Period. New rule: parking permits can only be issued to employees or to the spouses of VP's. Period. (edited for political correctness) 
I figure it like this, (with these assumptions) if the DWH is doing the majority of its hard work at night, and the OLTP is doing its work mainly during the day, you're time sharing the unused resources. With only 7GB of usable memory, I would bet plan cache is the only thing being persisted in memory, that is even if all of it is. Locking pages in memory with that amount is likely just a dream, which is why i would check disk speed as that is where the next hit would be. And you are right, too many unknowns at this point to make any solid recommendations.
Ahh but as a production DBA, you have to know about those things. If you can't prove its a problem, you can't force their hands to fix it. When we order new servers, we have to tell them how to prioritize the drives so they allocate them properly on the SAN.
I would have to read up on the subject if / when i get into that situation. Currently, I have to maintain code spanning from sql 2005 to sql 2012, fixing frontend developers mistakes, analyze customer server installations, that would look bad when compared to my laptop, and work on new development. Its a very wide field, and i'm more on the development side if things currently. At least i'm one of those, that know when to shut up about their dangerouse low level knowlage about a given topic. I do know how do build a decent DB server, don't worry, but SAN managment, thats something else i would venture to say. And proving the bottleneck is not the same as beeing the guy to fix it (the prove can be done by the SQL Servers stats and logs( trace flags are awesome) I would say). the When we order new servers, we have to tell them how to prioritize part thou, when it comes to a SAN, I should not and would not presume to do so I have to say. 
I have just recently moved to being a DBA, this time last year I was a .NET/T-SQL development manager, and accidental DBA (i was standing to close to the servers when the prior DBA quit). I took to liking the performance tuning, query rewriting stuff, so I made the transition earlier this year. In the small company I was in, you had to know all of that as the Hardware guys only knew what they were shown and didn't learn any more than necessary, the SAN was managed by a difficult MSP, so I had to prove without a doubt any change I wanted to make in order ot get anything changed. Alot of learning comes with proving 'The Expert' wrong. SQLSaturday and video training is a great start for anyone looking for more info.
Well, we might want to take this to private messages since we are getting WAY of topic, but I fully agree. You learn what you deal with. You have to, and experiance is the best teacher ever. You will always have your in depth knowlage area's and your knowing what your talking about area's, and your well, I know I dont know much area's. To me the most important thing is, to know when to listen to people having more experiance than you (not talking about sales reps here). My main area is performance tuning and DB design (those to are tightly linked). And of cource writing compex SQL as it is a well, a must have I guess. SAN or OS administration (talking Active directory and there such), I stay clear of when I can, there are people to deal with that. That beeing said, Kerberus is one of my special friends sadly... I know enough to have the conversation, but I'm not the one to design a SAN. I guess you get my point. 
All great points, I'll end it here with: I agree.
I have heavy load production DB's that get 98% cache hits on 4GB Memory, you really can't tell without in depth knowlage about the environment. 
Or better yet, look at OUTPUT clauses.
Well, the golden rule of DB development and design :: it depends. Generally speaking, the production DB and reportings DB should be seperated, for performance, scalability, priorisation, hardware design, and security. Also, seperating the DB backend for different applications within a cooperation can make a lot of sense in these terms. It can make sense to have it on the same server, and by that, on the same instance. Its a question of budgeting to be honest. You really can not give a definite answer, a good DB infrastructure is to dependant on the given environment. Even with company internal databases, you can have security concerns between competing departments that warent sperating the DB into different servers, even subnets. Even if its resonable to have it on the same instance, you can have performance concerns for the production DB that prevent you having the ETL process run on that instance. Please don't make me lay out the possible reasons, there are so many. TLDR yep sounds reasonable, while it can also be unreasonable
Wow - lots of feedback - and its all greatly appreciated. I'm going to chew on this for a week or so, while my new DBA goes through his investigation as well. A few people commented on the disk - and I can confirm that it is all directly attached (no SAN) at the moment. We are in the process of determining if a high-speed SAN would address some of our performance issues. Additionally, I can confirm that the previous DBA has placed the Warehouse on C:, and that both the Production DB, logs, and backup (!!!) are on their own disk. I'm confounded as to why he would have chosen that, as its against nearly all best-practices. My guess is that it was due to resource limitations, but I'm not comfortable with it, regardless. Again - thanks for everyones thoughts. Good to have people from a few different perspectives as well.
If you look on the right side of the subreddit you'll see the order your SQL statements should have (select, from, etc). The sidebar is missing 'ORDER' so I put it in for you. SELECT count(a.field1), a.field2, SUM(b.field4) FROM a INNER JOIN b ON a.key1 = b.key1 WHERE a.field8 = 'test' GROUP by a.field1, a.field2 HAVING SUM(b.field4) &gt; 5 ORDER by a.field.3 
http://www.amazon.com/Microsoft-Server-2012-T-SQL-Fundamentals/dp/0735658145 This guys books are great. Easy to read with depth and coverage.
Masters degree? Jealous... I want to switch jobs just because I want to work in different environments with different technologies. My employer would never consider MicroStrategy - or any 1st class technology for that matter. It seems we always say "whats the best? Oh yeah? That costs a lot....hmm, whats nearly free or free?" E.G. - We don't build OLAP cubes for reporting with expensive software, instead we just build views on our OLTP tables to report from. We use a lot of open source BI tools. Reporting? Easiest of all you have listed probably. Data warehousing probably the next step. I imagine you are assuming intermediate or better level of skills in like....performance tuning/query tuning/administering SQL boxes. ETL requires some sort of actual programming knowledge, although I don't think you have to be a good programmer to perform ETL. Data mining might be the most lucrative of them all - because at some point you kind of draw the line here and say - I am done with query tuning, administering stuff, writing reports, modifying cubes, and all the data is being pushed and pulled just fine. At some point you say - I am a statistician (data scientist....). And all you do is focus on being a mad statistician. Knowing all the other stuff will help you. But I truly think data mining is a skill that is thrown around all the fucking time by non-technical people who pull out the "obvious inferences" - and to be good at it - you need to be comfortable in intermediate/advanced level of stats. Which most people aren't. Work with what sounds fun, and at places that are fun. I am young and still out with the same questions as you really, but this is my experience so far. 
What do you consider to be intermediate stats?
Well in the technology sense of things - you should be writing in R, or using SPSS Data Mining Tools. You should be able to understand gaussian, bayesian, and other techniques. This site here has some stuff that I just don't have the....time? to tackle. I took a college class on data mining, but I retained very little because it was very abstract.
Thanks for sharing. My new life goal is to have the job title of "Mad Statistician". They are pretty flexible on assigning projects to the person most interested (or hates it the least) in doing that project. I currently have a Bachelor's but have been looking into different Master's programs. A few Universities offer a Master's in Analytics, it seems like a newish major so I don't know how relevant or in demand it is in the job market. I assume it can't hurt though. That was the reason I figured I would stay there another year or so. I wanted to get a few of the MicroStrategy certifications and experience so I could do part-time consulting work while going to school. Our Analytics department is only four people so our responsibilities are not that strict. We all do a little of this and a little of that. I work with finance projections, cost analysis, KPIs and such. I also work a lot with marketing and email/web analytics. Then slowly started developing reports in MicroStrategy to take the place of the many manual (run/write query - paste into Excel - VLOOKUP to join all the data) reports that went out daily/weekly/monthly or the random Ad Hoc reports that the department liked. Our current MicroStrategy developer sorta does our ETL work too. If it is something out of his capabilities we hire consultants to complete the tasks. Sometimes I feel like my company is a startup that has too much money. They like all the cool new technology and buy it, then want to move on to the next thing before they have realized the potential of the current product. 
SQL syntax is not all that difficult, if you complete those courses you are ready to start on what I consider intermediate level SQL topics. Thus, I would reccomend this book for the person who knows the general syntax but wants to get into more complicated query's: http://www.amazon.com/Introduction-SQL-Mastering-Relational-Database/dp/0321305965/ref=sr_1_2?ie=UTF8&amp;qid=1386589869&amp;sr=8-2&amp;keywords=intro+to+sql If you have the basics down it would behove you to find out what DBMS your new job will be using and then glance through the functions they have built in. At my job I use Oracle and MSSQL and jumping from one to the other is still annoying because the functions behave differently and are sometimes called something completely different. From there, it really is about working in a production environment. A database like Northwind is completely different from anything you will experience at your job. Knowing the data you are working with is just as important as knowing SQL syntax IMHO. If your company keeps a metadata repository go look through that so you know what columns are and how they tie into other tables. 
From personal experience I wish I'd invested more time and headed down the DBA route. I've been working in the Reporting space for a few years now and it can be really difficult to get business leaders to understand the value of accurate reporting, and that there's a huge difference between a well built report and a hashed together excel spreadsheet.
Is this a one time thing or something you will be doing regularly going forward? If it's one time, I would suggest using the Import Data Wizard in SQL Server Management Studio (SSMS)
http://www.cs.cmu.edu/~awm/ I meant to paste this page in. This guy talks about a lot
I have done something similar in a report, so this may or may not be helpful. I started by creating three generic parameters, "SecondLevelGroup", "ThirdLevelGroup", and "FourthLevelGroup". Each of these parameters had the same static available values. So in your case, "Level 5 Facility", "Level 4 Facility", etc. Within the Tablix itself, the report I created would always group by Location as the Parent group, and the user had the option to choose subsequent groups (which is why my first parameter is SecondLevelGroup). First you'll want to change the grouping. In the Row Groups dialog, group properties for the SecondLevelGroup, where it says "Group on:" put an expression: =iif(Parameters!SecondLevelGroup.Value = "Option 1",Fields!Option1.Value,IIF(Parameters!SecondLevelGroup.Value = "Option 2", Fields!Option2.Value,Fields!Default.Value)) This code I also used for the sorting expression on the same group. Next, in the body of the Tablix, I took the same code and put it as an expression for the value of the SecondLevelGroup textbox. This shows the group value based on what is selected. Finally, for the column header, I put the value [@SecondLevelGroup]. So whatever parameter they passed in for the SecondLevelGroup would appear as the group header. The code above would change how it grouped and what was displayed. Now of course there are some limitations to this approach. For one, it assumes the user will want to group on all three levels, whereas your requirements indicate a need for grouping on one or multiple levels. Also, there is no check on @SecondLevelGroup and @ThirdLevelGroup, so theoretically they could group on the same group for each level. If the above approach is not possible, you might consider creating subreports with defined grouping that show/hide based on the level of grouping that is passed to the report. 
Ya I had considered that at first, but of the 9 possible groups, the user has to be able to group on any possible combination (between 1 and 9 columns). At this point, I may just do something like this: Create 9 columns labeled ColumnGroup1, ColumnGroup2, ... ColumnGroup9. Then just make the SQL smart enough to know how to group the data based on input parameters - and have the SSRS report hide all Column Groups where the index &gt;= the number of groupings the user wants to do. Gonna be some ugly SQL though.
I'd be curious to see the output of SELECT InstructionValue FROM AssemblyInstructions WHERE InstructionValue LIKE '% %' I suspect an errant space character is actually the culprit. If no results there, can you pastebin the results from SELECT InstructionValue FROM AssemblyInstructions ORDER BY InstructionValue (assuming we aren't talking more than a few thousand records). 
I've Worked in SSRS for about 6 years now and I've never been able to get this dog to hunt. Between the expressions you'd have to write, the possible dynamic SQL, and the level of abstraction you'd potentially have to hurdle, it just isn't worth it. Better to create a Model and let the users make their own Report Builder reports because at this point what they want isn't a report but predictive modeling software. 
Well we are moving to a Tableau solution - was hoping to get this report out in the interim. Just wanted to get some experienced developer thoughts on it, thanks for the input!
Aha! Glad to hear it.
Well I work for a company where innovation and 'creating atomic bologna slicers' is highly rewarded. Unfortunately I already showed the prototype to the partner in charge of our group and he wants it, so failure is not an option :) Ugly SQL it is!
Just remember, NASA went back to pencils when the 0-G pen's R&amp;D cost skyrocketed.
Figured it out, wasnt really that bad!
 So you think you're storing the values into mysql as utf-8, but when you pull the values back into a site via php they are gibberish? I guess first you want to make sure the values are indeed stored correctly, by following the directions on [this SOF page](http://stackoverflow.com/questions/1049728/how-do-i-see-what-character-set-a-database-table-column-is-in-mysql). 
&gt; it can be really difficult to get business leaders to understand the value of accurate reporting Until you get audited. Then they come a-knockin.
When I instert new values via PHP it shows some gibberish in phpmyadmin tables but when I query them the browser reads them as polish characters. Its exacly the opposite when I use phpmyadmin to insert new values to tables. It shows me polish letters in phpmyadmin but when I query it the browser just shows ??????. Checked all the charsets and they are 100% UTF8
Ehh OK. Seems like your web page is getting back exactly what it put it, which isn't a bad thing unless some other application (like phpadmin) wants at it as well. That is, your db, tables, and columns might be utf-8, but your connection isn't (I know, it's weird). Try adding this to your db connection string: SET NAMES 'utf8' With that connection, store a value, and see if phpadmin views it correctly. 
Try adding: DROP TABLE #Results right after SELECT * FROM #Results 
Technically, I don't think you need to explicitly drop temp #tables...that happens automatically when the proc ends. Meaning, that's not going to help OP's particular question...being able to access #results after the proc finishes. That said, for code clarity and consistency, I always explicitly drop temp #tables as soon as they aren't needed anymore, even if that means the drop is the last statement of the proc (where it really doesn't matter, technically).
You are right. I guess it's just a habit for me. I prefer table variables over temp tables anyways. Not sure what the OP's problem is then. Code looks clean to me. 
Agree, @table is usually better than #table. I think OP's problem is that he expects #Results to survive after the proc completes. Ugly quick fix is ##Results as another commenter mentions (and then the explicit drop practice really begins to matter).
Ok, so I've modified the following and it works fine now: If object_ID ('tempdb..##results') is not null DROP TABLE ##Results CREATE TABLE ##RESULTS (COLUMNNAME NVARCHAR(370), COLUMNVALUE NVARCHAR(3630)) Thanks for all the help, I appreciate it!
don't create a holiday table, but rather, a general purpose calendar table flag each row with its day-of-week, and also flag the statutory holidays, and then (this is the part many people miss), also flag the company holidays (e.g. black friday isn't a statutory holiday, but you want to flag it anyway) you can pre-load the table for a few years into the future (and repeat once a year) then doing stuff like date due requires fairly simple sql to look ahead in the table from the row where calendardate = CURRENT_DATE
INNER is to LEFT. It really depends on the DB and the relationships. If you have just joined a team, ask questions and find out why they use INNER JOIN's. I can guarantee you there is a reason.
There is no difference. It is one of those technically correct situations. Technically JOIN is really an INNER JOIN so for clarity they probably choose to use INNER JOIN. I think its lame to care about that, I have worked with a few guys who just get raging hardons when they can point this stuff out to you... it can get extra annoying in this type of situation...
INNER is to LEFT? Not sure what that even means, but INNER and LEFT are not the same...
JOIN specifies the two tables should be logically combined and INNER is the default modifier. So yes, they are the same thing, but if the default ever changed all your SQL would break. Also, it is very unlikely the default would ever change, BUT IT COULD.
They're exactly the same. In SQL-92, a join without a modifier is defined as being an inner join. From the spec: &gt; If a &lt;qualified join&gt; is specified and a &lt;join type&gt; is not specified, then INNER is implicit. [http://www.contrib.andrew.cmu.edu/~shadow/sql/sql1992.txt](http://www.contrib.andrew.cmu.edu/~shadow/sql/sql1992.txt) 
Also, I would recommend against putting in the unnecessary INNER keyword when writing SQL. Obviously follow whatever style your organization is currently using, but as a general principle I'd say skipping unnecessary keywords like INNER (and OUTER in LEFT OUTER JOIN) makes for more readable SQL. 
This works on Teradata (SQL should be okay on most platforms) UPDATE RESULTS FROM (SELECT SEAT AS s, MAX(VOTES) AS mv FROM RESULTS GROUP BY SEAT ) a SET STATUS='W' WHERE SEAT=a.s AND VOTES=a.mv ; 
SQL is a generic language article should clarify that it's dealing only with "Microsoft SQL Server"
see http://www.reddit.com/r/mysql/comments/1smino/help_mysql/
the answer is in the where clause. it joins the results of the "from" query (SEAT=a.s) and only updates the rows where the number of votes is equal to maximum number of votes on that seat. to understand it better, run these two queries separately: the one from the update query, and the one that you join in your select query.
Ah nederlandse kolommen.... SELECT vakantiecode,dagen,prijs,subquery.gemiddeldeprijsvanallevakanties FROM reisinfo, (select avg(prijs) as gemiddeldeprijsvanallevakanties from reisinfo) as subquery;
Using INNER keeps a more consistent length when you're mixing inner and left joins, i.e. INNER JOIN table1 INNER JOIN table2 LEFT JOIN table3 vs. JOIN table1 JOIN table2 LEFT JOIN table3 I prefer the first one.
Yes, very much so. I think they're doing 2012. Not sure, but I guess I should plan on 2 days, quote 'em three. That with travel from Memphis to Atlanta, just the full week...
We deploy various Dataservers as a package, So I can have a new server up and running in "about an hour". I'd take a WAG and say being your first time, without someone to hold your hand you might be looking at 16-24 hours working hours. That's probably, hopefully, about 8-12 more hours than you need, but I'm assuming you'll have to work out some kinks, get things configured properly once it is running and create a schema. 
I'm liking the consistency of answers so far.
This. The blog looks like notes I write myself and forget all context to what the note was about. 
just an sql server? no cluster? 1 hour.
I sort of agree with the other answers so far. The thing about installing SQL Server is that is does several checks to make sure that all of the dependencies are in place. And just when you think you're about to click the button that finally installs it, it goes into more dialog boxes. Not to mention, you have to figure out what options you want installed.
This is really all you need. Plus, I think that this is an ANSI standard, so it works with some of the other RDBMS platforms (but not Oracle).
At least have a narrative or something to describe the case usage or the pros\cons of each method.
&gt; I think its lame to care about that I don't. When you INNER JOIN you are being explicit (even though the default is inner join anyway). It helps to self-document the query If the query just uses JOIN, in some cases its because the dev doesn't understand the different types of JOIN. 
There are two situations I can think of: 1. Person looking at the code does not understand the difference. As far as they are concerned, it is self documented. 2. Person looking at the code understands the difference. He/She smiles at the screen and thinks "yeah that is really an INNER JOIN", and it is self documented enough for them to understand it.
Yeahhh...They're already buying a data historizer and they only really need data from the past week. The true purpose of this project is to run their engineering budget out before the new year, so they'll pay for whatever time I need. Seems messed up, but as I tell myself "I'm their engineer, not their accountant." As for the days, I have zero exp. setting up sql, I'll have to get it to where an HMI can grab stuff from it, and set up permissions (again, with no exp.)
There is a subquery, referenced by the alias name "a", that calculates all the winners for all seats (you do this yourself in your query that returns each winner's name). This subquery exposes two columns - "seat" and "mv" - that are used to join to specific rows of the RESULTS table in the main body of your UPDATE. All I do is put the above join into practice, updating the "status" column in RESULTS where a match is found between "a" and RESULTS on values "a.seat = RESULTS.seat" and "a.mv = RESULTS.value". The bit to remember is that there is a JOIN going on between the results of the subquey ("a" , which contains only specific (winning) rows from the RESULTS table) and the RESULTS table itself (which is referenced as the subject of the UPDATE operation). The syntax is not particularly intuitive if you are not familiar with it (I re-learn it every time I need to use it :-) ).
In that case better go with 3-4 days. I'm sure you'll run into all kinds of fun interconnectivity issues. I did the hardware/SQL install for our HMI/PLC History server (Rockwell Automation YECH!) awhile back and they can be bitchy. Our PCL EE spent a couple weeks getting everything installed and configured but the server side was easy for me.
I think JOIN defaults to INNER JOIN but it'd better to just put INNER JOIN so that it doesn't cause any confusion.
Alias needed and make sure you use the FQDN etc. Generally using Linked Servers is a bad idea though.
 Update LocalTable Set LocalColumn = Remote.Column From LinkedServer.Database.&lt;Schema&gt;.Table Remote Where .....
Joins over linked servers are a problem. My understanding is that SQL effectively copies the entire remote table to local #temp and joins from that. You might be better off copying the applicable parts of the remote table to a #table yourself and joining to that instead of a linked table. Example, if you know the n% of rows that might get used in your join (eg this year's inventory records or whatever), you might be able to outperform the linked join's implicit table copy which may not be so well informed. 
I think this is the best answer so far.... Except the standard is to write out left outer join... I know pain in the ass but if this is my worst problem I can deal.
This is the art of Logical Data Modelling - it can be tricky. Try [this](http://stinaq.me/blog/2012/11/21/an-easy-way-to-finding-the-candidate-key-of-a-database-relation) for a very quick intro to the act of normalizing data. And perhaps [this](http://www.emunix.emich.edu/~khailany/files/Normalization.htm) showing a more example-driven version. If you understand your domain (i.e. you understand the business corresponding to the data you have to work with) then things are normally a lot simpler. If not, you have to draw inferences from the data - it's granularity, the field names etc., to try to make sense of it from a "real world" point of view. That's when the task starts to get more time-consuming (and some would argue, more interesting).
Well, that is not quite the case. When you do joins, the query optimizer is able to recognize how it needs to filter on tables or indexes. The same goes for tables accessed true a linked server. When you access data true the linked server, you will get a "remote query" in your execution plan. This remote query can contain a where clause, which will filter the data that gets returned from the linked server. It also does not dump all into the tempDB as per default. That being said, on complex queries, you can end up execution plans, that do not filter the remote query and then just (often hash) join the entire remote table to the record set. So, while linked server queries are a pain for many reasons, they are not quite that bad. 
Yeah, it makes a bit more sense to me when I see things in a physical sense. Unfortunately, my college class usually refer to these parts in letters-only. Thanks for the sources.
You have a permissions issue. You need a user account on the server you are trying to run your site on. Where you create your connection to the database you need to change the credentials from root to your user on the new server. From there assuming you have the correct permissions setup it should start working.
In a previous life I was a college prof; here are my notes on that http://okaram.spsu.edu/~curri/classes/13/summer-13/DB/StudyNotes/Ch7-Normalization/Ch7-RelationalModelAndNormalization.pdf The basic idea is simple; in a table, every field should depend only on the primary key ; this gets complicated when you have more than one candidate key. Basically, you get 1nf for free; and try to eliminate 'badness' ; there's partial dependencies, when fields depend on a piece of the primary key, eliminating them gets you in 2nf, there's transitive dependencies, where a field depends on a non-key field, eliminating them gets you 3nf. You eliminate the dependencies by taking the fields out into new tables (but keeping the PK of the new table as a reference). BCNF is used mainly to torture students, as I've never seen it in real life;) to be in 3nf but not bcnf, you need two composite candidate keys, and one field of one depends on a field of the other.
You need a group by clause; add group by vakantiecode Can check http://okaram.spsu.edu/~curri/classes/13/summer-13/DB/StudyNotes/Ch10-SQL-DML/6_SQL_Aggregates.pdf
I was just assuming that you are on a shared server so you would have your own user account. For instance 'imallleg' instead of 'root'. There is a root account on the new server but it is owned by the servers administrator so it will have a different password. That is why you're getting an access denied error. Unless you are supposed to use the root account, maybe you typed the password wrong. If it's not that then I have no idea and in that case /u/HotRodLincoln may be on to something.
A portion of the remote querying has been fixed by giving SELECT schema permissions rights to statistics instead of higher levels in MSSQL 2012. That said it is usually better practice to use stored procedures / views on the remote machine to limit the data join sets remotely before performing a linked server join.
I'm a bit careful with giving general answers when it comes to sql and performance, but as Coldchaos has said, yes. If you have to use a linked server, it is a good practice to call a remote stored procedure and write that to a temp table. It always depends on the situation thou
Right click on the remote, read only database and run the export data wizard.
I don't think replication is quite what you're going for here. It sounds like what you need to do is create a linked server object on your local instance which points to the target database. There's a decent tutorial with some screenshots [here](http://sqlserverplanet.com/dba/how-to-add-a-linked-server). Once the target server is defined as a linked server on your local instance, you should be able to run a query such as: SELECT foo, bar FROM LinkedServerName.DatabaseName.dbo.MyTable Or you can create a view on your local instance which performs that query. When querying the linked server, you'll always need to qualify the objects so that SQL knows where to look: ServerName.DatabaseName.schema.object Hope this helps.
Since you're posting this in the SQL sub I'll assume that you're talking about their PL/SQL Developer certs that Oracle offers, if it's the DBA certs then my answer won't apply. Having said that, you'll probably be fine. Having some database experience will help, but doesn't sound like it might be necessary. If you're really worried about it then you can ask the instructor if there's anything you should know going in.
Thanks for the quick answer. This is the program that I'm looking at taking. http://lonestar.edu/Oracle-Exam-Prep.htm It seems that it does specialize in the DBA program. Guess I've been studying the wrong thing....
Okay, yes. I've been able to do that much, so I guess I do understand that bit well. As for when you've finished, it's a candidate key if all keys are in the closure or something right?
Thank you, this is what I needed.
&gt; What am I doing wrong? In a word, everything. You cant insert and update in same statement. Insert statements do not return a resultset, so Im not even sure what you were trying to achieve here. If you want to insert or update based on the results of a select, do a JOIN. The syntax for this differs depending on which RDBMS you are using though.
break the problem down in English. What exactly are you trying to achieve? Looks like you have 7 tables: * **products** - lists all your different products? * **carts** - lists orders(?) of clients? * **bankaccounts** - lists payment details for clients? * **users** - lists client details? * **sales** - a report to summarise your product sales? * **invoices** - contains records, one for each invoice? * **invoicecontent** - a linkage table that links individual sales to invoices? What is the purpose of each table, what are you trying to achieve in the end? (As in, explain your high-level objective, not specifically how you want the logic to work) Not sure on your syntax but as carpii has already said, you cannot insert and update in the same statement (well you can using something like MERGE, but that is an entirely different purpose and doesn't look like it's what you're trying to do). The closer I look at it (at least the first line), it looks like you're trying to keep **invoices** and **invoicecontent** synchronised in some way. 
After some study, I think I know what you're doing, and it's better done with a stored procedure. You have tables invoices,sales, products, carts, bankaccounts, and users, in addition to invoicecontent (which looks to simply be maintaining a relationship between invoices and sales). You're trying to create a record for your invoice, filling it with data about the parties involved. You are then trying to insert into the sales table the actual transaction of products (what was bought, how many, how much they cost) that you'll associate to the invoice in the next step via your invoicecontent table. A few things * Your syntax is extremely poor. Often it's better to not use `RETURNING`, because it's (to my knowledge) specific to only Oracle, and doesn't translate well to other RDBMSs. * `UNION` is a method for joining the results from two queries together in the same result set. Unless you're selecting the same ~~rows~~ **columns** from the same tables on both sides of the `UNION`, you wind up with a ton of nulls and have not accomplished what you meant to do. * If all invoicecontent is doing is maintaining the relationship between sales and invoices, I'd drop it. Since it seems like there are many sales to each invoice, I'd store the invoiceid on the record in the sales table. If you do this, you no longer need to UPDATE invoices after you've created the records in invoices and sales, since each sale knows what invoice it belongs to. If I have this relationship backwards (which I suspect I might, multiple invoices per sale makes more sense than multiple sales per invoice), then simply store the saleid in invoices and flip your order of operations here around. Something like this would handle the invoice side of it, and another that's nearly the same would handle the sales side: INSERT INTO invoices( ibuyerid, isellerid, isellerbankid, ibuyerccid) SELECT udi, psellerid, bankid, upccid FROM products, carts, bankaccounts, users WHERE products.pid = carts.pid AND products.pquantity &gt;= carts.cquantity AND buserid =psellerid AND userid = 'value'; I don't recall if Oracle can use update JOIN syntax or not (haven't used it in many years), so this is written without it. For the record, I'd recommend handling your quantity check differently. As this is, this query will error when someone tries to buy more than what is in stock, which while that may be what you want it to do, you typically don't want your system to catch the errors, you do, so you can control how it behaves when it breaks. I also don't see a need to for a GROUP BY. Your sales query should be very similar. Again, I'd recommend eliminating the invoicecontent table altogether and store that relationship in one of your tables (whichever one is the 'one' in their 'one-to-many' relationship), and putting both queries in the same stored procedure to call them. For future reference, tell us next time what task you're trying to do, ie process a purchase, then explain how you're trying to do it.
Close ; that would be a super key; since it determines everything, you could use it as a key. To be a candidate key, it also needs to be minimal, so if you take out any one field, it's not a super key anymore. Technically you could use a super key as a key, but it could be dumb if you have estra attributes
You can pull this data into an excel sheet that they could refresh themselves and get the data. Distribution should be site based, like a Sharepoint website that they can access. Setup permissions on who can access the data. In excel you can run SQL queries that would pull the data. The employees running this could have EXECUTE permissions only on the database to run the Stored Proc that would pull the data. This secures the database to some extent. Move the work into the other peoples areas, and they will be happy if they can get the data themselves when they require it. Link the folder to a WebShare in something like sharepoint or have ot copied to a repository. The statement here is you want the data to be managed and secured for access. 
I would use a blend of powershell and SQL. Through powershell scripts you can call native SQL functionality and the powershell script can be called from SQL using xp_cmdshell or SQLCLR functionality. In fact, a SQLCLR procedure may be all you need. That will require you to know some sort of VB.Net or C#.
:)
Ask the questions. If we can help we will.
well its 17:25 here so now is a pretty good time. Just let me know what kind of stuff you need.
EDIT:Added stuff Also this is if your using MS SQL Create the Stored Proc, use variables for ordernumber and switchclli. Then you would just need to type exec storedprocname 'ordnumbervalue, 'switchcllivalue' Or toss your storedproc into SSRS to make it nice and perty. You do need to define the datatype for the Variables which IE: @ORDERNUMBER VARCHAR(10) If the datatype is a varchar and 10 is the character limit SQL that i would do as follows: SET ANSI_NULLS ON GO SET QUOTED_IDENTIFIER ON GO Create Procedure Cust_OrderInfo @ORDERNUMBER @SWITCHCLLI AS BEGIN SET NOCOUNT ON; select ordernumeber, itemnumber, issuenumber, action, recordtype, notifystate, firsttfn, lasttfn, derivesuccessfulind, deriverrordata from orderitem where ordernumber = @ORDERNUMBER and switchclli = @SWITCHCLLI END GO 
when do I lose you for certain?
Do you have specific questions?
ask the questions... if we can help we will... oh, wait...
You're not helping yourself by just not asking the question outright. Post the question. There's plenty of people here to help. That's what this sub is for. 
http://www.pcreview.co.uk/forums/access-2002-need-call-openform-twice-display-record-data-t1124988.html
Thank you so much! It was in the Data Entry property, and I had to set the property to No.
could you please answer this one Which of the following E-R diagram components does NOT get its own table in a database schema? Answer Question 36 answers strong entity sets weak entity sets many-many relationships multi-valued attributes many-one relationships 
In postgres at least, you could use a case statement: case when permission_coordinator then 'permission_coordinator' else max(permission_prestige) end as permission I've no idea if that's ANSI or which other engines support it, nor what framework you are using there to see if it has case support
&gt; But, I only want the one with the highest value in one of the columns, or a true value in the other. I would use a sub-query that selects for the max(value) in the table, grouping by whatever makes sense. Then inner join that sub-query so that only the max values are included. 
this is probably the best way to do it, but there are others -- [groupwise max](http://jan.kneschke.de/projects/mysql/groupwise-max/)
Row_number() over partition
SELECT * is more than lazy. It is a very bad practice. If you select the entire table, you are almost always generating more reads than neccesary, screw up any covering index, generate unneccesary network and disk io, and open yourself up for futur problems when the table definition changes. Use select * in one time only querys you manually write to check stuff in your DB, do not use it in production code. Write out your columns that you need, and only those. It is not merley cleaner and best practice, it has a real performance impact, which in the case of otherwise covering indexes, can actually be really substential. Btw, even on a view, if you use select * in a view definition and the tabe structure changes, the meta data of the view will be out of sync with the return of said view (MSSQL). So, do not use select * !
there is a comma missing on the sp params, and the datatypes for the params, otherwise looks spot on ;)
If it's strictly numerical values, I'd use a ranking sub query to rank them, and then only pull where rank = 1.
Typically, when I'm presented with such a challenge, I start by reviewing what I know about the problem and then trying to build sets of results incrementally that I can refine until I have the question solved. Once that is complete, I can then go back and refactor the query and tune each logical "step" until it performs as needed and is nicely concise. Not everyone does SQL this way, but I find it works best for me, so in my attempt to write something helpful for you, I followed this method...but stopped at the first iteration. That brings me to a few "disclaimers": 1. I don't know your data design, DDL, volume, etc. This makes it very difficult to really write a query that does exactly what you want. Given your description, I assumed a few items about your design, these of course will be off which means this SQL won't do exactly what you want. 2. Since your question revolves around solving the permission_prestige and permission_coordinator, I only dealt with those columns. Looking at your original query I see many more tables/columns that you want to interact with, but I can't derive their relationships, so the additional items needed is up to you... 3. This query is not tuned/optimized in any way. It's only an early beginning point of how I'd solve the problem. Don't expect it to "fly" on volumes of data, or even return back the right result. Next, let's cover with my design assumptions... * two table design 1) entries table that has multiple entry_ids to orgunit_id relationship and 2) positions table that has a 1:m relationship on orgunit_id to entries table. * there is no weight difference between permission_prestige and permission_coordinator (in fact, I don't even eliminate both rows, I leave that up to you to decide which you actually want) * I'm using PostgreSQL 9.3, so depending on your DBMS, you'll be making changes to the SQL as needed... To do some initial testing, I had to build a couple tables, so I did that with this DDL.. CREATE TABLE entries ( entry_id INT ,orgunit_id INT ) CREATE TABLE positions ( member_id INT ,orgunit_id INT ,permission_prestige INT ,permission_coordinator BOOL ); And I loaded it with random data... INSERT INTO entries(entry_id, orgunit_id) VALUES (1, 1), (1, 2), (1, 3), (2, 1), (2, 2), (3, 1), (4, 1); INSERT INTO positions(member_id, orgunit_id, permission_prestige, permission_coordinator) VALUES (100, 1, 5, false), (200, 2, 5, false), (200, 2, 10, false), (300, 3, 1, true), (300, 3, 1, false), (400, 4, 0, false); Next, I tried to obtain what your query was mostly worried about, and so I evaluated it as this... PREPARE original (int) AS SELECT entry_id, permission_prestige, permission_coordinator FROM entries e JOIN positions p ON e.orgunit_id = p.orgunit_id WHERE p.member_id = $1 AND (p.permission_prestige &gt; 0 OR p.permission_coordinator = true); In looking at that, I noticed a couple things. 1. likely a PK read on member_id, so only dealing with one member 2. permission_prestige and permission_coordinator are OR'd, so we can start by rewriting this as a UNION ALL a. I like writing UNIONs in lieu of ORs because the temporary sets that you get with either the CTEs or subqueries provide your easier filter controls and can be used subsequently in other queries 3. Based on your description, you want the highest permission_prestige, so we can use MAX here... That lead me to this... PREPARE test (int) AS WITH coor_true AS ( SELECT member_id, orgunit_id, 0 AS perm, permission_coordinator AS coor FROM positions WHERE member_id = $1 AND permission_coordinator = TRUE LIMIT 1 ), highest_perm AS ( SELECT member_id, orgunit_id, MAX(permission_prestige) AS perm, false AS coor FROM positions WHERE member_id = $1 AND permission_prestige &gt; 0 GROUP BY member_id, orgunit_id ) SELECT DISTINCT qual_perms.member_id, qual_perms.orgunit_id, perm, coor FROM ( SELECT member_id, orgunit_id, perm, coor FROM coor_true UNION ALL SELECT member_id, orgunit_id, perm, coor FROM highest_perm ) AS qual_perms JOIN entries ON qual_perms.orgunit_id = entries.orgunit_id WHERE qual_perms.member_id = $1; The idea is this, 1. coor_true will gather the first record for the member_id where the coordinator value is true 2. highest_prem will get the max perstige value 3. these two items will be union'ed together giving at most two rows 4. we then join these results against the entries table, which holds the information about the entry_ids and such that you want...I don't make use of that in this query, but I show the basic syntax. ITEMS TO NOTE! * When both a record exists where coordinator is TRUE and has a int &gt; 0, you'll have two records, you will want to weigh/rank whichever has a higher priority to you....you can do this by simply appending a single literal value to the CTEs * I use an extra DISTINCT on the final query, it's not really needed, but I didn't know your data design so I didn't want to take any chances. This follows the C.J. Date style of writing SQL anyways... :) Not certain whether or not this is any help...but I hope it can come somewhat in handy! 
See also: http://www.reddit.com/r/programming/comments/1svnfr/codds_relational_vision_has_nosql_come_full_circle/
This type of problem is called the *Greatest N Per Group*. There are different approaches. SQL Server offers DENSE_RANK() as a possible solution. Since you want to use the rank in a where statement, you'll need to do something like below. Also, if you want who has the HIGHEST sales, should you order the partition by sales descending? Select * From( SELECT employees.name as 'name', sales.month as 'Month', sum(sales.sales) as 'Sales', DENSE_RANK() OVER (PARTITION BY Month ORDER BY DESC sales) AS rank FROM sales LEFT OUTER JOIN employees ON sales.name=employees.name GROUP BY 1,2) HighestSales where HighestSales.rank = 1
Try these... Put your sales total by employee/month into a subquery. Then your RANK() function will be looking at the SUM of SALES by month, rather than individual sales within the month. Not sure if you should be ORDERing by "sales" or by "sales DESCENDING" in your RANK() statement... If it is available, you can use QUALIFY rank = 1 (placed at the end of your SQL statement) to select a specific result from your OLAP function (RANK() OVER...).
Sounds like you are looking for server side or client side tracing. have a look at the [SQL Server Profiler](http://technet.microsoft.com/de-de/library/ms181091.aspx) which should be the easier option, since it has a nice GUI, or [server side tracing](http://www.sqlservercentral.com/stairway/72363/) which works very similar, but runs on the server. Both will give you every command executed by the server, for which you can filter. 
This is a good tutorial which explains what you are attempting to do and how everything works. http://www.oracle.com/technetwork/issue-archive/2006/06-sep/o56asktom-086197.html
Looking for the highest average sales across ALL festivals (events?) - or that's how I am reading your requirement. (SUM / COUNT = AVG, correct?) **Untested:** SELECT MAX(A.AVG_verkocht) as max_across_all_festivals FROM ( SELECT kaarten.festivalnummer AS festivalnummer, AVG(kaarten.verkocht) AS AVG_verkocht FROM kaarten Group by kaarten.festivalnummer ) A ;
 SELECT festivalnummer , AVG(verkocht) AS avg_verkocht FROM kaarten GROUP BY festivalnummer ORDER BY avg_verkocht DESC LIMIT 1 note: LIMIT is a non-standard mysql extension, but since you're using mysql, it's okay 
you're welcome and i wanna meet the **numpty** that downvoted a correct solution
Function AVG(column_x) = average value of "column_x", normally in terms of some other value (the thing you decide to GROUP BY). A.AVG_verkocht = column "AVG_verkocht" in subquery "A"
okay, no problem then ;o)
I'm guessing I need to do something like so - -- Insert into Table the Statistics for the FTP Jobs CREATE TABLE #TEMP (Filename VARCHAR(128), FTPJobRunID INT, FTPScheduledStart DATETIME, FTPJobStart DATETIME, FTPJobEnd DATETIME, FTPRerunAttempts INT) INSERT INTO #TEMP SELECT RIGHT(LEFT(CAST(l3.params AS varchar(128)), CHARINDEX(',', l3.params)-1), LEN(LEFT(CAST(l3.params AS varchar(128)), CHARINDEX(',', l3.params)-1))-CHARINDEX('=', LEFT(CAST(l3.params AS varchar(128)), CHARINDEX(',', l3.params)-3))) AS Filename, l1.id AS FTPJobRunID, l3.time AS FTPScheduledStart, l1.launchtm AS FTPJobStart, l1.stachgtm AS FTPJobEnd, l1.reruns AS FTPRerunAttempts FROM [server].[database].[dbo].[tablea] AS l1 INNER JOIN [server].[database].[dbo].[tablea] AS l2 ON l2.id = l1.prntid AND l1.type = 6 INNER JOIN [server].[database].[dbo].[tablea] AS l3 ON l3.id = l2.prntid WHERE l3.id IN (SELECT id FROM [server].[database].[dbo].[tablea] INNER JOIN [server].[database].[dbo].[tableb] ON (tablea.owner = tableb.id) WHERE jobrun_proddt = @ProdDate AND owner_name = 'blah' AND jobrun_cmd IS NULL AND jobrun_params IS NOT NULL); -- Remove " from tail of odd Filenames UPDATE #TEMP SET Filename = LEFT(Filename, LEN(Filename)-1) WHERE Filename LIKE '%"' MERGE DAILY_JOBS AS TARGET USING #TEMP AS SOURCE ???? DROP #TEMP CORRECTION - Idiot that I am I do have the PK for the #TEMP and DAILY_JOBS to link the two which is the FTPJobRunID. So how do I tell it that if the ID matches only UPDATE, but if the ID isn't there INSERT?
I'd think you'd be able to left join #TEMP to DAILY_JOBS on FtpJobRunID. If the FTPJobRunID comes back null then you need to insert. If not update the file name?
When I was in college 10 years ago, we used a book by Murach. The one I used is probably a little dated, but I looked at one of their newer editions for SQL Server 2008 and it's just like the one I used. [Murach's SQL Server 2008 for Developers](http://www.amazon.com/Murachs-Server-2008-Developers-Murach/dp/1890774510/ref=sr_1_4?s=books&amp;ie=UTF8&amp;qid=1387219370&amp;sr=1-4&amp;keywords=murach+sql) (They also make a SQL Server 2012 version). Just like my previous book, the first half of the book is strictly about SQL, then the other half is about more advanced concepts.
I really like the O'Reilly SQL Cookbook, although it's not a textbook. It gives clear examples of how to do specific tasks in SQL. It is ANSI SQL, but when appropriate, other SQL implementations are given if they are more convenient.
One I like very much is Date's [*SQL and Relational Theory: How to Write Accurate SQL Code*](http://www.amazon.com/dp/1449316409). It's also not too pricey ($33). The one downside is his war on window functions and other things that aren't purely relational. On the other hand, that focus on the relational theory underlying SQL is something that really helps develop clarity.
why the temp table ? you can write mege into statements using subqueries, MERGE INTO someTable as target using ( SELECT id, foo, bar FROM someOtherTable WHERE something = true ) as source ON target.id = source.id WHEN matched AND target.foo = 5 THEN update &lt;something&gt; WHEN matched THEN update &lt;somthing else&gt; WHEN not matched THEN insert (col1, col2) values (source.foo,source.bar) ; For less "pseudo code" you can read that all that up on [msdn](http://msdn.microsoft.com/en-us/library/bb510625.aspx) 
practice vs theory.... Please don't use that book if that is the case, I can already see the fresh from collage kids telling me i'm doing stuff wrong because it was written in some book (and they can't read execution plans)
Take a look at it before you knock it. As somebody who writes several-hundred line queries on a daily basis, I can tell you that book has caused me to focus my thinking far better and produce better queries. You just need need to supplement it with a bit on window functions and the like.
Oh I didn't want to knock it, as you put it. Set based thinking is the most important thing to learn when doing anything with databases. The "his war on window functions" part thou, window functions can improce performance a lot sometimes, and any definites don't belong in a textbook on sql I'd say. Having actually read the book you can tell better than me. However I haven't seen anything where I can give a always true answer concerning SQL. Well, other than anyone using the select * from table1 t1, table2 t2 where t1.id =t2.id should be able to run very fast 
Window functions don't just improve performance, they also make things possible that would otherwise be pretty much impossible (or at least unfeasible) in SQL. My job would be almost impossible without them. That's why I pointed that out as a drawback. That said, I think it's a worthwhile trade-off to make people actually think about what they're doing in set theoretic terms so that they actually get the results they want. Too many people just pick up a crappy RDBMS like MySQL and start shooting off queries not know what they're doing and then are unable to construct complex, accurate queries.
plenty run with oracle or mssql and dont know anything about set based thinking, data integrity or performance. My alltime favourite are the with(nolock) on everyting heros, cos then no lock, so faster right ? right ??? anyway, i think we are very much on the same page here ;) Edit: remember having to hack together those top 1 subqueries and exists () clauses cause there was no row_number in sql 2000 and below ? I still have nightmares (and legacy code) ....
thnx I actually posted to stackoverflow after this and got the solution which is as so - MERGE DAILY_JOBS AS TARGET USING #TEMP AS SOURCE ON TARGET.FTPJobRunID = SOURCE.FTPJobRunID WHEN NOT MATCHED THEN INSERT (Filename, FTPJobRunID, FTPScheduledStart, FTPJobStart, FTPJobEnd, FTPRerunAttempts) values(source.Filename, source.FTPJobRunID, source.FTPScheduledStart, source.FTPJobStart, source.FTPJobEnd, source.FTPRerunAttempts) WHEN MATCHED THEN UPDATE SET Filename = source.Filename, FTPScheduledStart = source.FTPScheduledStart, FTPJobStart = source.FTPJobStart, FTPJobEnd = source.FTPJobEnd, FTPRerunAttempts = source.FTPRerunAttempts; DROP TABLE #TEMP END The reason for the #TEMP table is long winded but the data is being yanked from so many places it was how I got it to work. I plan on going back and improving the code later just wanted it all to work in the start. I actually have about 8 stored procedures linked through a master stored procedure and our job scheduler is running it every 5 mins.
To be honest, that does not sound like something you want to run every 5 minutes on a production DB. Assuming it is a production DB, otherwise.... well.... You might be better serverd with tiggers on your underlying tables executing the code needed. Service broker would be an other thing that might be useful. Wihtout knowing the details, the query's, the data, the hardware, the load, the execution time... having 8 SP's executed, all based on temp tables, every 5 minutes, raises some concerns. Something like that can really put out seriouse IO and CPU. In short, better make sure that this proc runs very efficiently and fast.
Ya I tend to agree with you and as I'm not a DBA hence my wanting to improve it. Definitely looking at the trigger approach now but will take longer to implement as the data it's scraping is very complex. All told the 8 SPs take 1 second tops. This is down from the 2+ minutes it was taking for me to do manually once a day. It's pulling from production data and placing into a readonly DB for analytics hence the request for every 5 mins. I do agree that's a bit much though. Going to push back on that with the developers until we can implement a better system getting all the data loading through triggers.
Easiest approach with triggers would be to throw out the job schedule, and just have any insert and update on the base table trigger the proc. You can fill in a "history" table when the proc was executed to prevent it beeing run to often. Somethin along the lines having the last execution of the proc, the last time of writing operation on your tables, and then have a job, only loocking at "when was the last insert / update, when was the last execution", if last execution &lt; last write, exec proc. That you then can put on a schedule, and have only a tiny and very fast query execute every 5 minutes, while the proc only runs when it needs to be run. Its one of the "get creative" kind of things. 
dayum. I hadn't thought of this. Absolute genius! One concern though: the good thing is that sometimes it may not get called in 2+ hours. the bad thing is that sometimes the table/rows may see updates of a manner of x times per second depending on the program that's updating the source table. Could that cause a problem with the stored procs? I guess what I could do is remove the master proc and have triggers on each individual stored proc which should limit the amount of opportunities for a stored proc to be called concurrently. Hope that makes sense?
**An Introduction to Database Systems : C.J. Date** ... this was my actual text book in my final year (10 years ago). This book taught me almost everything I know about relational database, relational data modeling and SQL language with in-depth explanations. Of all the text books .. this is the only book I still kept in my book case. I did all the testing using Oracle 8.0.5, SQL Server 2000 and Sybase. Additional books I read at that time were : * Inside Microsoft SQL Server 2000: Kalen Delaney * Oracle PL/SQL Programming
It's a great text for learning database theory, but I find his focus there on Tutorial D is a big distraction from actually learning SQL.
which version of sql would help. assuming MSSQL 2005... do a subselect and use the partition by over to assign each one a row number. order by the boolean then the other value, and select only rows where rownumber= 1.
It would be a very poor website if it exposed any info at all about the sql server behind it You'd need to ask your DBA. If he's a good DBA, he will probably refuse to give you adhoc access using Excel :p
Dispelthemyth, you'd need an ODBC or OLEDB connection back to the SQL server and you're not likely to get that, especially if your SQL admin realizes that you're going to be connecting with Excel. What you should be doing, if you're using this for some sort of reporting function, is to ask your DBA to setup a SSRS package that distributes that data to you in a proper manner.
Every heard of a crontab? Just create a script that makes a call to the stored procedure, then schedule that script to run as you need. For example with debian/php/mysql php code: &lt;?php $db = new mysqli('db','host','user','password'); $db-&gt;query('CALL your_stored_procedure()'); ?&gt; execute in your bash: crontab -e add a job by saying: */5 * * * * php /path/to/script that will run it every 5 minutes. Hope this helps. Edit: Didn't read the title fully, if your on microsoft I believe the only difference is that there is no cron, I believe they call it a scheduled service? Same concept should apply though...
Why even do this? It's MSSQL. Create the SPROC, create the job to run it however often is needed on whatever days are needed. No need to get cron, linux, php, etc involved. 
I just started ranting without reading. Apologies. I never play with Microsuck, so it was unwise of me to comment anyways.
Why wouldn't you use MySQL's event scheduler versus cron anyway? http://dev.mysql.com/doc/refman/5.1/en/create-event.html 
figured it out, I just needed to add an apostrophe before the existing. Changing 's to ''s 
Thanks for showing your work! However, you forgot to tell us what platform you are running. This (probably) works in sql server 2012: WITH rankedCTE AS ( SELECT kaarten.vnummer ,vvv.vmanager ,vvv.vplaats ,kaarten.festivalnummer ,kaarten.verkocht ,kaarten.retour ,ROW_NUMBER() OVER (PARTITION BY kaarten.festivalnummer ORDER BY kaarten.retour DESC) AS rankedBySales --splits the set into logical partitions for each festivalnummer, for each partition give every row a distinct value FROM kaarten INNER JOIN vvv --assuming a Many to One relationship on kaarten.vnummer = vvv.vnummer ) SELECT vnummer ,vmanager ,vplaats ,festivalnummer ,verkocht ,retour FROM rankedCTE WHERE rankedBySales &gt; 1 --only the ones who don't have the highest ranking If you want to only show the best row, instead set the rankedBySales filter to = 1. You can include the rank in the select if you want.
What is the error message (that should identify the RDBMS), and what does your prompt look like when you go to run the query?
If you are a beginner, don't use command line only for practices. Use [sqfiddle](http://sqlfiddle.com/) instead. You can pick what target platform you want and put some code in and see what the results would be. Pick MySQL if you are using that and create your table structure, then link the "project" here and we can run code against that structure and try to help you. I'll look back after my workday is over. Good luck in the meantime! :)
&gt; I'm probably being too vague.... yup have you created any tables at all yet? like, a table for the classes (without bookings)?
i hate to have to actually come out and say this, but is there any chance we can see those tables? you know, to give some context to your request or should we just make shit up?
Or window functions.
*weeks*? when is this assignment due?
**Untested...** select festivalnuummer,vnummer /* get vnummers by festival that are not in... */ from kaarten where (festivalnuummer,vnummer) not in ( select festivalnuummer,vnummer /* the vnummers whose retour are the max() by festival */ from kaarten where (festivalnuummer,retour) in ( select festivalnuummer,max(retour) from kaarten group by festivalnummer ) ) ;
This should work, but OP needs to make sure the anti-semi join uses the correct key values.
There are 3 tables i see as in play for this. a ReportingTable, UptimeTable, and RawDataTable. There are currently no keys in place for any of these tables.
Personally I'd use a load of intermediate tables. E.g. student to class. This wau the student table remains pretty normalised and you can hang all sorts of things from student class table with a consistent dataset to query. I'd need to see rough tables to help more though. 
E.g. student 1. Class 1. Can take y/n. Grade for test 1. Grade for test 2. Then link them with foreign keys.
Make shit up. &lt;randy marsh&gt; I'm sorry I thought we were on reddit. A good guess might help bit it's all kinda random without tables. At least we're not Sqlservercentral. I swear half the posts on there are peopl pointing to their sig which asks for ddl and dml, tad passive aggressive but top notch help available. 
&gt; I have tableA that has Account and I want to insert into tableB if that Account doesn't exist in tableB already. Well, just to be safe make sure you have unique restraint on any column that must be unique.. ALTER TABLE tableB ADD CONSTRAINT uc_account (Account); Then... can do Select account from tableA where not exists (select account from tableB);
Tbh, I like that about sqlservercentral. If you post a question on there, providing the scripts containing test data, you get excelent advice and solutions. And there is nothing wrong with professionals demanding that you put some effort into it, when you ask them for help. Not to mention, that on a interesting issue, you get mvp's posting AND explaining the issue in detail. So, I would not diss sqlservercentral if you ask me.
well, what you could do would be to save the last execution date(time) of the proc, and the last update/insert on any of the relevant tables to a simple 2 field table. You run a query in the job, every 5 minutes or whatever schedule you would like, and just check, if the last proc execution ran after the last insert/update. If not, you execute the proc, and update the last execution date. That way you have a very small and fast query run all 5 minutes, while also having the proc run every 5 minutes *if needed*. Where you save those 2 datetimes is completly up to you, what ever makes sense. 
Either im not following your train of thought, or what you write is not, strictly speaking, correct. someForeignKey, someInteger 1 1 1 1 1 2 2 1 If I do a select count(*) from aboveTable group by someforeignKey, i will get a count of 3 for the FK1. If I do a distinct on the integer value first (on tsql that would be count(distinct someinteger)), I will get a 2. Group by is not the same as distinct. To the op, what they are talking about is, that access does not support count(distinct something) So to do that in access, you will have to write a subquery. SELECT subquery.groupByValue, count(*) FROM ( SELECT distinct groupByValue, valueToAggregate FROM somewhere WHERE something = true )subquery GROUP BY subquery.groupByValue That is true for all aggregate functions, count is just an easy one to wrap your head arround. So to do the Avg, you would just do a avg(subquery.valueToAggregate). On the Sql in 10 minutes.... I call bullshit here. Make it 10 months, then you have a *minor* chance of knowing how to sql. Relational databases are a different way of thinking when compared to "normal" programming. Set based thinking vs procedual &amp; object orientated thinking. It takes time, so don't feel presured to understand database development over a weekend. //Edit: don't anyone tell me about functional or logical programming, i strongly dislike prolog, and i abso-fuckin-lutly HATE haskell.
upvote for you posting the solution to the problem you found yourself. thats something that pisses me off to no limit, the forum posts where people post a problem. Which you find cause you have the same problem... And the Thread ends with, "hey guys I found the solution myself, much thanks /thread" So, good guy, have and upvote.
I would approach that problem the same way.
Mailing list ? what mailing list ? get me on that mailing list ! that beeing said, honest misinterpretation on my part then. To all that dont know sqlservercentral.... get to know it! btw, the for xml path('') thingy combined with cross apply to have an aggregated string concatination, THAT one blew my mind when i first learned about that beautiful beautiful idea. Oracle people are going "wtf" right now I know.... OUR LICENCES ARE CHEAPER THAN YOURS !!!!! 
Sql is nothing like php or perl or any other scrip language. For SQL you have to get used to a different way of thinking, set based, not procedural. Look at a database table as you would an excel spread sheet, its the same structure. I'll call that recordset from now on. Any join between two of those, will create a new recordset, a derived set of data. So if you have a logic to implement like you have in your exercise, just look at what information you need. top 10 value of top 10 cars per vendor. So, first you need to ger the top 10 cars per vendor, since that recordset, you need to filter the top 10 of. In this case, that means you have to do a nested subquery, to create a list, of the top 10 cars, for each vendor. That will then be the recordset, you apply the "top 10 sum(value)" on. I don't "do" mysql, so i cant from the back of my head, give you an executable query, but tbh, i wouldn't want to either, learning by doing is a good teacher. Google is your friend on syntax ;) Start low level, get the information you need, to do the calculation, you need to apply the filter. I hope that made some sort of sense
thank you. I figure next year when I have to do this again, I'll search here, and find my own solution. 
I like doing it this way.... insert into tableB(Account) select Account from tableA except select Account from tableB 
 SELECT Din.Value,Do.Name FROM ( SELECT SUM(C.Value) AS Value, D.DID FROM Cars C, Dealerships D WHERE D.DID = C.DID ) AS Din INNER JOIN Dealerships Do ON Do.DID = Din.DID ORDER BY Din.Value DESC LIMIT 10; Pretty sure this should give you what you want.
Quick fix is to replace COUNT(*) with AVG(prod_price) like so: SELECT AVG (prod_price) AS N FROM ( SELECT DISTINCT prod_price FROM Products WHERE vend_id = 'DLL01 ') Or more generally: SELECT vend_id, AVG (prod_price) AS N FROM ( SELECT DISTINCT vend_id, prod_price FROM Products GROUP BY vend_id ) GROUP BY vend_id I don't use Access, but more briefly in MSSQL: SELECT vend_id, AVG (DISTINCT prod_price) AS N FROM Products GROUP BY vend_id 
I must have not read to carefully what the OP was trying to do. I just saw SQL in 10 minutes and thought they were trying to use Distinct to group their aggregrates.
Dammit... did I do wrong?
Sorry if I wasn't clear in the way I worded! I am just learning this, so I am not always sure how to convey what I mean. I think another way of saying it is if I had the prices: 3.49 3.49 3.49 9.50 9.50 2.25 I want to use access to get the distinct values from that (3.49, 9.50, 2.25) and get an average from them. The book said I couldn't use the following in Access: SELECT AVG (DISTINCT prod_price) AS avg_price FROM Products WHERE vend_id = 'DLL01'; I was trying to figure out how to write this in Access. I appreciate your responses. Thank you. :) 
Thank you for this! I didn't even think to replace COUNT(*) with AVG(prod_price). That seems like a pretty simple fix. 
Sounds like it's being made more complicated than it is. Without knowing the full structure of the tables, this is just a guess at best: insert into dw_log_hist select * from flows_030100.wwv_flow_activity_log1$ where time_stamp &gt; (select max(time_stamp) from dw_log_hist) union select * from flows_030100.wwv_flow_activity_log2$ where time_stamp &gt; (select max(time_stamp) from dw_log_hist); commit; That *should* only grab new data that doesn't already exist in dw_log_hist.
Its related but how do I check that section and area have the same floor anyway?
when I insert a new room I wanna check that the section and area are on the same floor
Someone told me to use triggers. I got this but I don't know what to do next. delimiter $$ create trigger validateFloor before insert on room for each row begin select section.floor from section, area where area.floor = section.floor and section.id = new.sectioId and area.id = new.areaId end $$ delimiter;
thanks, let me see if i can implement it
Joins in SQL are used to associate the data from one table with the data from another. For example, if you have a users table and a seperate blogPosts table, you can associate a user to a blogPost via one of your columns (like, a blogPost will have a userId column and the users table will have a column for userId. This is called a foreign key.) You use a join to actually make that association so you can get the data from both tables. You do something like this: SELECT username, blogPostTitle, blogPostContent FROM users u JOIN blogPosts b ON u.userId = b.userId; This will get you the usernames from the users table and the blogPostTitle and blogPostContent from the blogPosts table. This is essentially how you get SQL to give you data from more than one table where there is some way to link the data together.
You could run a quick select just to check before you insert.. select CASE WHEN section.id IS NOT NULL THEN 1 ELSE 0 END from section inner join area on (section.floor = area.floor) where section.id = 12 and area.id = 4 If query returns 1 then the combination of section and area are not only valid ids, but also on the same floor
Good shout, I'm involved. Admittedly currently in Australia and not currently the SQL master I plan to one day be, but, I'm involved.
here's a radical thought... if you have "blue section, pool area" then areas should belong to sections, rooms should have a floor and reference only the area, and areas should reference the section problem disappears
Does syntax look correct? Is there a different method I could use to ditch the IN statements? Thanks to anyone that can help me out in advance.
One of the problems with this vendor is that I can't RUN this query until they say "Okay. This looks fine." It's driving me crazy. I worked unhindered for 3 years and now they put these restrictions in play. So, idType=35. That means they receive a service from the first Program Service Area (PSA1). Now, the only way I want them to show up on my list is if they ALSO receive a service from PSA2. This is why I have two IN statements, but they don't like how I originally had this coded. This is how I originally had the code, but they said "IN is expensive on resources." I call bull. The only other way to make it less taxing on resources is to use a whole crapload of IF statements. Even CASE statements wouldn't be very efficient. The code would look like crap. This is my original query: SELECT DISTINCT person.id AS "Family Member ID#", person.idfamily AS "Family ID#", person.firstname AS "First Name", person.lastname AS "Last Name", person.birthdate AS "Birth Date" FROM person INNER JOIN capcase ON person.id = capcase.idfamilymember INNER JOIN family ON person.idfamily = family.id WHERE ( person.id IN (SELECT idfamilymember FROM capcase WHERE idtype IN( 35, 17, 66, 19, 18, 33, 29, 57, 20 ) AND ( dateapplied &gt;= '01/01/2013' AND dateapplied &lt;= '12/30/2013' )) ) AND ( person.id IN (SELECT idfamilymember FROM capcase WHERE idtype IN( 7, 68, 50, 51, 52, 54, 53, 56, 28, 30, 26, 34, 13, 23, 22, 25, 62, 79, 21, 61, 44, 2, 3, 4, 5, 6, 10, 31, 64, 42, 65, 24, 46, 12, 49, 55, 27, 15, 8, 59, 60, 11, 14, 47, 9 ) AND ( dateapplied &gt;= '01/01/2013' AND dateapplied &lt;= '12/30/2013' )) ) 
The only thing the idTypes have in common is their name. They are all prefixed with the Program they are in. In this case the bigger group of numbers are prefixed with FES and the other is SS. I'd say I could use this for the query, BUT this won't work simply because of the other names we have. (Example: FES - ROSS will show up if I'm telling it to pull values like 'SS')
Maybe I don't understand how I'd create a lookup table. Would this work any better? SELECT DISTINCT person.id AS "Family Member ID#", person.idfamily AS "Family ID#", person.firstname AS "First Name", person.lastname AS "Last Name", person.birthdate AS "Birth Date" FROM person INNER JOIN capcase AS A ON person.id = a.idfamilymember INNER JOIN family ON person.idfamily = family.id INNER JOIN capcase AS B ON person.id = b.idfamilymember WHERE ( person.id = a.idfamilymember AND a.idtype IN( 35, 17, 66, 19, 18, 33, 29, 57, 20 ) AND ( a.dateapplied &gt;= '01/01/2013' AND a.dateapplied &lt;= '12/31/2013' ) ) AND ( person.id = b.idfamilymember AND b.idtype IN ( 7, 68, 50, 51, 52, 54, 53, 56, 28, 30, 26, 34, 13, 23, 22, 25, 62, 79, 21, 61, 44, 2, 3, 4, 5, 6, 10, 31, 64, 42, 65, 24, 46, 12, 49, 55, 27, 15, 8, 59, 60, 11, 14, 47, 9 ) AND ( b.dateapplied &gt;= '01/01/2013' AND b.dateapplied &lt;= '12/31/2013' ) ) 
&gt; they said "IN is expensive on resources." I call bull. you'd be right
I'd mention to their customer service dept that if a simple query with some IN statements is unnacceptable due to their system's lack of robustness, then perhaps it is time to start looking for alternate vendors.
They are probably referring to the 'outside' ins, person.id in ... Because the naive implementation would perform that query many times. The main problem I see is that you do not need those joins, since you're only using fields from the person table. Also, it seems the query in the original description wouldn't work, since you're requiring the same idtype to be in those two lists, which would be equivalent to ask it to be in their intersection. I'd ask them what their concern is, maybe to send an idea of how long the query spends, and the Explain output for it. Now, if they're just being dumb, you can replace the idtype in ... With idtype=12 or idtype=13 .... And the outer in with a join (the DBMS probably does this by itself, but ...)
Those guys are idiots. Make them show you the execution plan and point out where the IN statements are adding any additional IO whatsoever. Then drop them and find a new vendor.
Gotta get rid of the sub select like you did in you last example, then create a table where each row is a valid combination of two numbers. Join to your transactional table (capcase) twice (a&amp;b) specifying your time condition for each and then create third join to your new combination (c) table on a.idtype = c.idtype1 and b.idtype = c.idtype2. This should work fine as long as the combination reference table only specifies a unique combination once without regard to the order of values in idtype1 or idtype2. Also, this can technically be written without a where clause at all. You can write the joins with multiple clauses ex: inner join blah a on a.x = b.y and a.z = t.w and a.q &gt; 3 etc. 
I would suggest creating a view that draws on the table data… and use a CASE statement for the flag columns. You will probably hear other opinions on this... but if a view is an option - here is my best shot... Something like: Select name, date, CASE WHEN dbo.yourtable.color = 'RED' THEN 'Y' ELSE 'N' END as wore_red_flag, CASE WHEN dbo.yourtable.color = 'BLUE' THEN 'Y' ELSE 'N' END as wore_blue_flag, CASE WHEN dbo.yourtable.color = 'YELLOW' THEN 'Y' ELSE 'N' END as wore_yellow_flag FROM dbo.yourtable Further explanation of the T-SQL CASE statement can be found at: http://technet.microsoft.com/en-us/library/ms181765.aspx Edit: sorry if my formatting is jacked up - I really tried the '4 space' formatting tip in the side column.
Your's is close but doesn't merge the date rows. Select name, date, COUNT(CASE WHEN dbo.yourtable.color = 'RED' THEN 1 ELSE null END) as wore_red_flag, COUNT(CASE WHEN dbo.yourtable.color = 'BLUE' THEN 1 ELSE null END) as wore_blue_flag, COUNT(CASE WHEN dbo.yourtable.color = 'YELLOW' THEN 1 ELSE null END) as wore_yellow_flag FROM dbo.yourtable GROUP BY name, date If Count doesn't work, switch to SUM( then 1 else 0), but I think it shouldn't count the nulls and you'll get 0 for false and &gt;=1 for true. You can go another level if you really want only two possible values. Select T.name, T.date, ,CASE WHEN T.wore_red_flag &gt;= 1 THEN 'true' ELSE 'false' END) as wore_red_flag ,CASE WHEN T.wore_blue_flag &gt;= 1 THEN 'true' ELSE 'false' END) as wore_blue_flag ,CASE WHEN T.wore_yellow_flag &gt;= 1 THEN 'true' ELSE 'false' END) as wore_yellow_flag FROM ( Select name, date, COUNT(CASE WHEN dbo.yourtable.color = 'RED' THEN 1 ELSE null END) as wore_red_flag, COUNT(CASE WHEN dbo.yourtable.color = 'BLUE' THEN 1 ELSE null END) as wore_blue_flag, COUNT(CASE WHEN dbo.yourtable.color = 'YELLOW' THEN 1 ELSE null END) as wore_yellow_flag FROM dbo.yourtable GROUP BY name, date ) T
In SQL Server I would use PIVOT and use the COUNT aggregate function. What RDMS are you using?
&gt; I need to find all albums with songs that are AT LEAST 5 minutes long. If an album contains a shorter song, I need to throw out the whole album from the query. You need to identify all albums on which *every* song is at least 5 minutes?
Yes. If it contains shorter songs, I want to throw them out.
 SELECT DISTINCT medium_id FROM medium WHERE medium_id NOT IN ( SELECT medium_id FROM song WHERE length &lt; INTERVAL '5 MINUTES' Will this work? The inner query will select the medium_id of any song under 5 minutes, and the outer query will select any medium_id it didn't choose. Untested, so let me know if there's any issue.
The only way I can think of doing it is since you do not already have the original song count, you have to get that value. then compare it to the number of songs retrieved by your query. if the song count for a medium is the same with and without the 5 minute predicate then all songs must be over 5 minutes select m_base.medium_id from( select medium_id, count(*) cnt from song s group by medium_id ) m_base inner join ( select medium_id, count(*) cnt from song s where length &gt;= INTERVAL '5 MINUTES' group by medium_id ) m_5min on( m_base.medium_id = m_5min.medium_id and m_base.cnt = m_5min.cnt ) 
Wow this is working - Thank you very much. I am currently learning SQL and this is one of the FIRST assignments. There has to be an easier way for this, because all my other solutions are way simpler than this and this one is way more complex than the rest. Im happy that I understand what you are doing there.
So, the first step I would do is write a query to identify ALL of the albums which have ONE OR MORE songs greater than 5 minutes in length. You want to exclude these. SELECT medium_id FROM SONG WHERE length &gt; 5 I don't know how the SONG.length column looks, whether it is a float, whether it's stored in minutes or seconds or what. But, that should do us. So, now we have a list of all of the albums whose songs are GREATER THAN 5 minutes in length. The list is not distinct. For this purpose we don't care. Next we need to select all Albums (not a specified name... mrr...) where they AREN'T in the list we just created. SELECT medium_id, medium_title FROM medium WHERE medium_id not in (SELECT medium_id FROM SONG WHERE length &gt; 5) AND maybe something like medium_type = 'Album'
I learned something to solve these kinds of problems yesterday, you use a correlated subquery, so the sql would look something like: select m.medium_title, (select sum(s.length) as minlength from song as s where s.medium_id = m.medium_id order by s.length asc limit 3) from medium as m where m.medium_art = 'LP' I think that should be right.
Thanks mate! :) I need to use s.length in a GROUP BY clause. I hate this clause &gt;.&lt; Im trying it myself atm but I have no problems with some help :P I think I got it. Next problem message though: Subquery delivers more than one row If I change the LIMIT to 1 it works, but then he only "sums up" one song :(
Damn thats a lot of work right there.
You're looking for window functions, another great feature of PostgreSQL. By using them, the query will perform far better than a correlated subquery as it only requires one pass through the table. Tutorial: http://www.postgresql.org/docs/9.3/static/tutorial-window.html Reference: http://www.postgresql.org/docs/9.3/static/functions-window.html Be sure you understand the difference between rank, dense_rank, and row_number when ordering by song length in case there are multiple songs with the same length.
Try using a subquery, and look up the difference between IN and EXISTS (there is also NOT IN and NOT EXISTS), and when to use each.
Fixing his query: select m.medium_title, (select sum(length) from (select s.length from song as s where s.medium_id = m.medium_id order by s.length asc limit 3) as short) as minlength from medium as m where m.medium_art = 'LP';
Wouldn't a GROUP BY and HAVING combination give you a list of albums? SELECT medium.medium_id, medium.medium_title FROM medium NATURAL JOIN song GROUP BY medium.medium_id, medium.medium_title HAVING min(song.length) &gt; INTERVAL '5 MINUTES' 
&gt; SELECT medium.medium_id, medium.medium_title FROM medium NATURAL JOIN song GROUP BY medium.medium_id, medium.medium_title HAVING min(song.length) &gt; INTERVAL '5 MINUTES' wow YES! this one is PERFECT! THANKS!
Holy shit thanks! Best Christmas ever :D
I used to be all gung ho about LI groups. And then, I discovered how much of a pain they are to maintain due to spam and people (mainly recruiters) that can't follow directions. IMO, Reddit is a much better format to hold a discussion. But, if you can keep it on topic, I'm sure that it would be a great networking tool.
What you're doing is the best way I know of in Oracle. I also like to do this to check for the how many of each degree of duplication there is: select dupe_count ,count(*) from ( select foo ,count(*) as dupe_count from bar group by foo ) group by dupe_count
This. Also, I cannot emphasize enough how important it is to get away from MySQL. SQL Server Express is free. Download it and learn it. Alternatively, there's is Postgres (which I absolutely adore). It's not as big in enterprise environments, but my impression is that it's catching on rapidly (and 9.4's support for hstore json types will be HUGE). Among the features MySQL is missing (off the top of my head): 1. An intelligent query planner that can actually use indices from tables in subqueries. 2. CTEs (and recursive CTEs) 3. Window functions 4. CHECK constraints 5. Proper handling of NOT NULL constraints (i.e. not coercing NULL to 0) 6. Insisting on proper specification of non-aggregated values in GROUP BY queries 7. Indexes on expressions 8. A query plan with real detail 9. A real procedural language These are all pretty important things to know, and you'll never experience them in MySQL. I know MySQL is easy to pick up, but once you try another (dare I say, real) RDBMS, you'll never go back. Also, read [this](http://grimoire.ca/mysql/choose-something-else).
First one.
holy crap, even i am amazed how much is missing in Mysql ;) i'll add a 10th, which always has been my #1 disqualification : proper backups. Differnetial backups and real point in time recovery are not optional in my book. Ps. Thanks a lot for the link you posted, i'm going to use this the next time I hurt some webdevelopers (php "professionals" mostly) feelings when I put MySQL in the same range as Access (its not quite THAT bad, but it comes closely after Access in "bad" terms) when it comes to the topic of a RDBMS, to me at least
in mssql, i usually do that with window functions. SELECT cnt = count(*) over(partition by someValue), * FROM table ORDER BY cnt desc the only difference is that you get the entire row back, which is usually what i want when looking for duplicates. You can do the same with the group by of course, but its a lot more typing.
Dynamic SQL. Write out your query with the parameter values into a string, pass the string to openquery(). P.s. watch out for date format differences between platforms (e.g. expected date format will be different on DB2 vs SQL.Server) 
Followup question - is there a way to loop through a list of categorical values (in SS2008)? I think I've figured out what I need, but I'm curious for the future. I want to loop through: ('-03-31','-06-30','-09-30','-12-31'). Maybe there's a way to refer to their ordinal positions? Edit: I looped one variable from 1-4 and used another to say CASE WHEN loop_var = 1 THEN '-03-31', etc.
I'm having issues editing my original post, but I got it to work. Hoping to post my code later. Thank you!
Hopefully something like this should work. Not Tested. It reads the CASE table for all case ids, then uses LEFT JOIN to NOTES table twice (once to get MAX(date_entered), once to get MAX(DATE_NOTIFIED) where these exist, for each case id). The "Left Joins" ensure you still get a row for every case id in the CASE table, even if notified or entered date is missing. The missing values, when encountered, will show up as NULL, of course. select case.id,entr.max_dt_entr,notf.max_dt_notf from case case left join (select id,max(date_entered) from notes group by id) entr (e_id,max_dt_entr) on entr.e_id = case.id left join (select id,max(date_notified) from notes group by id) notf (n_id,max_dt_notf) on entr.n_id = case.id ; 
You seem to be using MySQL which I am not that familiar with, but I will try to answer your question and write a query anyway. What you need to do is join the two sub queries on `Month of Project Ended At`. Treat them almost like they were normal tables. The correct join to use would be a `FULL OUTER JOIN` but last I checked MySQL did not support them so I would instead use `LEFT JOIN` and a list of all months in the quarter. The resulting query would look something like the below, but bear in mind that it was years since I lasted used MySQL and I have not tested the query. SELECT months.`Month of Project Ended At`, CAST(q1.`Count of unique Project Id` AS float) / CAST(q2.`Count of unique Project Id` AS float) * 100 AS result FROM ( SELECT DATE_FORMAT(CURDATE(), '%Y-') + LPAD((QUARTER(CURDATE()) - 1) * 3 + 1, 2, '0') AS `Month of Project Ended At` UNION SELECT DATE_FORMAT(CURDATE(), '%Y-') + LPAD((QUARTER(CURDATE()) - 1) * 3 + 2, 2, '0') UNION SELECT DATE_FORMAT(CURDATE(), '%Y-') + LPAD((QUARTER(CURDATE()) - 1) * 3 + 3, 2, '0') ) AS months LEFT JOIN ( SELECT DATE_FORMAT(`pi0`.`project_ended_at`, '%Y-%m') AS `Month of Project Ended At`, COUNT(DISTINCT `pi0`.`project_id`) AS `Count of unique Project Id` FROM `p` AS `pi0` WHERE (`pi0`.`dollars_converted` &gt; 1000 AND QUARTER(`pi0`.`project_launched_at`) = QUARTER(CURDATE())) GROUP BY `Month of Project Ended At` ) AS q1 ON (q1.`Month of Project Ended At` = months.`Month of Project Ended At`) LEFT JOIN ( SELECT DATE_FORMAT(`pi0`.`project_ended_at`, '%Y-%m') AS `Month of Project Ended At`, COUNT(DISTINCT `pi0`.`project_id`) AS `Count of unique Project Id` FROM `p` AS `pi0` WHERE (QUARTER(`pi0`.`project_end_at`) = QUARTER(CURDATE())) GROUP BY `Month of Project Ended At` ) AS q2 ON (q2.`Month of Project Ended At` = months.`Month of Project Ended At`); EDIT: Added casts to float before division. 
What flavor of SQL\DBMS is this for?
I don't use MySql but I use two other similar SQL RDMSs. A couple of things. usually you need a "group by" when doing a select with a sum in it. That's part of the error message you're getting. "You're asking me to sort by a field that you're not selecting", (that' sort of it). But really what i think you need here, if allowed in mySql, is a subselect. SELECT @var1 = SUM(MyList.var1), @var2 = SUM(MyList.var2), @var3 = SUM(MyList.var3), @var4 = SUM(MyList.var4), @var5 = SUM(MyList.var5), @var6 = SUM(MyList.var6) FROM (select top 10 var1, var2, var3, var4, var5, var6 from ShiftData order by DatetimeStamp DESC) MyList
great! 
This is MySQL - sorry I should have read the instructions to put the variant/platform in brackets. 
You seem to be knowledgable - what is the benefit of MySQL? I've only encountered a few small functions that would be handy in MS SQL, but could be worked around.
I'm only versed in tSQL, however, the problem you're having is SQL generic. You're attempting to select a date &amp; integer and divide it with a date &amp; integer from another select. You can't apply math over two columns like that. It would be akin to asking someone to divide Frog, 50 by Toad, 5. This will be succinct, but here's the join approach to at least get you thinking/rolling. The gotcha is where the dates returned are not equal. You can change the join to account for that, which would also require you give your math an out for 0/null division. doulehyphen is also accurately giving you a clue to first pull in all possible Date ranges needed, though you may still need to handle null division, where potentially a value appears in q2 with a date that does not exist in the q1 set. Nonetheless, this should get you thinking about the join approach. select q1.MonthProjectEndedAt , q1.CountUniqueProjectIds , q2.CountUniqueProjectIds , Result = (q1.CountUniqueProjectIds / q2.CountUniqueProjectIds) * 100 from ( SELECT DATE_FORMAT(`pi0`.`project_ended_at`, '%Y-%m') AS `MonthProjectEndedAt`, COUNT(DISTINCT `pi0`.`project_id`) AS `CountUniqueProjectIds` FROM .... ) q1 join ( SELECT DATE_FORMAT(`pi0`.`project_ended_at`, '%Y-%m') AS `MonthProjectEndedAt`, COUNT(DISTINCT `pi0`.`project_id`) AS `CountUniqueProjectIds` FROM ... ) q2 on q1.MonthProjectEndedAt = q2.MonthProjectEndedAt Good luck.
Because he's already fought it and lost. It's pretty safe to assume if someone posts here it's because they tried and failed and are frustrated with it by now. Most of the learning happens when you get intrigued and chase the rabbit of your own volition.
Good thing that you mention the division by zero. But since I cast to float before the division the result should be NaN. If that is an acceptable behavior it is handled in my query.