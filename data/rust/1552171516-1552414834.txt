If you do not mind writing a bit of C++ to interact with rust, you can use Rust Qt Binding Generator. The C++ could would use the application logic (Rust) together with QSystemTray. https://doc.qt.io/qt-5/qsystemtrayicon.html
Didn’t publish anything just yet. Plan on writing a blog post and open sourcing it. But it’s a bit limited how much time I have to spend on it. Thanks for the encouragement though!
Calling a method on a `dyn Trait` means doing an indirect function call, essentially using a function pointer. This is slightly less efficient, because the CPU will probably not be able to prefetch instructions. Also, you need a heap allocation and deallocation for the box. The advantage of using a `dyn Trait` instead of a custom enum is that you simplify the interface, you stay extensible (no interface change needed if you want to return another type) and you don't expose unnecessary information. The caller knows that they are able to call all the trait methods on whatever they get. They know nothing more and nothing less. Specifically, they don't know, and don't have to care about what type of object you are actually using.
Databases can be transactional if you use them that way, they're meant to handle massive concurrency correctly. Of course, if you do something like `SELECT`, change field, `UPDATE`, you're going to get a race condition no matter what.
/r/playrust
Could you post the whole code somewhere?
REST api should be stateless. The client should handle any state. https://restfulapi.net/statelessness/
Nope, it dereferences the Arc and calls the function. Clones will never happen unless you type `clone()` (or call a function that takes in an `Arc` and clones it)
Wrong sub buddy
At a limit all the enum code really jams up the icache, though (been a problem for servo which has a few massive enums for html/css). Also plain enums can be problematic if you have any large but uncommon cases. There's real perf advantages to boxing/virtualizing in key places.
So using Arch without a clone is unsafe and compiler allows concurrent access by this. Isnt this against the borrow checker?
No, Arc only derefs immutably, so the compiler only allows concurrent _reads_. This is safe. If you want to be able to mutate the Arc, you have to put a Mutex or RWLock inside it, which lets you mutate things even when you have an immutable reference (but these types enforce that concurrent accesses don't happen)
oh i see and if im passing the variable that has an Arc in it anyway i cannot pass it to multiple functions without cloning since the ownership of the variable being transferred, correct? 
Yeah. But cloning shares the same underlying data, to be clear You may find https://manishearth.github.io/blog/2015/05/27/wrapper-types-in-rust-choosing-your-guarantees/helpful
using rust after all these years and relearning all these things feels like watching a lifehack video on youtube and seeing whole another use of a spoon :D 
btw the link is down 
fixed
I have a function that returns a tuple with 2 elements. I want to initialize two separate struct members with these returned values. Is there a way to do this without calling the function twice? Currently I am calling the function for two temporary variables and then moving them into the struct members during initialization.
Here's a thing that shows some things. ([playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=b317f542cd3796492df8b93af8b61f83))
&gt; This is going to have the issue that when this is polled, the poll call will block. Yeah, I noticed that earlier. I'm **hoping** that, as u/rjberry mentioned below, that I can mitigate that problem by setting the `read_timeout` value very low, but I guess I'll find out when I get the rest of the code working. I could also switch to the `redis-async` crate if necessary—it has pub/sub support, though doesn't appear to be as widely used. &gt; Give it ago, would love to hear if that's worked for you. Thanks! That was very helpful and I *think* I now have the `OutputStream` working more-or-less as it should. However, I'm now running into a follow-on problem: now that I have `OutputStream::new()`, I'm not able to call it at the point I need to. Specifically, I can call it *inside* the main response loop—but that defeats the point, since it still re-connects to Redis on each loop. What I can't (yet) do is call `OutputStream::new()` before the main loop and then pass it in. I had **thought** that using an `Arc&lt;Mutex&gt;` would let me pass it in, but I still get a `cannot move out of borrowed context` error on the following code in `main`: fn main() { let stream = OutputStream::new().inspect(|s| { println!("{}", s); }); let stream = Arc::new(Mutex::new(stream)); let stream = warp::any().map(move || Arc::clone(&amp;stream)); let routes = warp::path("ticks").and(warp::sse()).and(stream).map( |sse: warp::sse::Sse, stream: Arc&lt;Mutex&lt;futures::stream::Inspect&lt;OutputStream, _&gt;&gt;&gt;| { let stream = stream.lock().unwrap(); sse.reply(warp::sse::keep(stream.map(|s| warp::sse::data(s)), None)) }, ); warp::serve(routes).run(([127, 0, 0, 1], 3030)); } I know I might be pushing my luck given how helpful you've already been (thanks again!) but I don't suppose you have any tips about what I'm missing here? I really thought I had a better handle on how the borrow checker worked, but Rust has a way of pulling new tricks when I least expect it! (The full code is in an [updated gist](https://gist.github.com/codesections/e3adcf0984ae359e3d62ff2dca7353fe)
That Bash script is basically what I started with, but once I needed to update ~15 repositories, it became apparent I needed parallelism. In fact I "officially" stripped out my Bash version from my dotfiles today: https://github.com/mattmahn/dotfiles/commit/ffbebe174e6c8aa2bb577050853760fea0b4308b
I did something like that in python, but it saves a dir on a config file, then I can do mycommand pull anywhere
Threads have overhead and can compete for resources, like writing or reading to a device. It is not uncommon for a multithreaded threaded implementation to be slower than a single threaded implementation. Threads can improve performance when the running time is cpu bound instead of, e.g., IO bound. Another scenario is if a thread has to wait for a resource, such as a disk read, but another thread can continue to do work. But if multiple threads are waiting for the same resources? Toes get stepped on. Plus the overhead of message passing. I’m not saying that’s what is happening in your case. It would be hard to know without looking at the code, perhaps with a profiler. But I wouldn’t be surprised. 
if link to a code is needed. Its example from a book. [https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=74f3f1c32eb1dbfe6011aba2589b5899](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=74f3f1c32eb1dbfe6011aba2589b5899)
Great minds think alike! 😉 I just [retired my Bash version](https://github.com/mattmahn/dotfiles/commit/ffbebe174e6c8aa2bb577050853760fea0b4308b) today; long live Rust!
I haven't looked too closely at it, but there are two main reasons that adding multiple threads can decrease performance: 1. System threads are expensive to spawn - potentially more expensive than the actual work you're doing. 2. Having too many threads actually *reduces* performance. No matter how many threads you have, the computer only has so many cores with which to do work in parallel. And both scheduling and swapping between threads have associated costs that increase with the number of threads you have. It's possible the workload here is just too small to offset the cost of creating system threads. However, I would recommend trying [`rayon`](https://docs.rs/rayon/1.0.3/rayon/) to parallelize this sort of workload anyway. It will automatically create a thread pool with an appropriate number of threads and will schedule tasks onto those threads well, so it should minimize the overhead from using multiple threads.
Trying to make the switch myself but feeling a little bit of the struggle. I think the main deterrence is I can build a web API (Koa) using dependency injection and whatnot in a matter of hours in Node. I’ve just reached that level of comfort and expertise wrangling it that I feel right at home. I like Rust and have read through the book twice over. Love the concepts and I really **really** want to use it instead for my own APIs. Actix-web looks and feels great but I just feel like I’m tripping over my own feet at times. Another aspect is my desire to use Mongo (or similar NoSQL store) for persistence. So again Node works well here with JSON objects. Understandable I suppose, I’m still a newb at Rust. I’ve literally created and recreated this project repo six times going back and forth between Rust, .NET Core API, and Node.js. Just feel like I’m torn between sticking it out with Rust and just being productive with what I know.
Sure. `let (a, b) = tuple_returning_fn(..)` and now you can use a and b.
No, nll does not affect when drops are executed
I know that this is IntelliJ, but it is good practice to state this in the title, post, or comment, for people who aren’t familiar with your tooling. In future you can try to [follow this instructions how to ask a good question](https://www.biostars.org/p/75548/). 
Step 1: start programming anything
I feel like the Rust ecosystem isn't quite there enough yet to make sense unless your needs are either very simple, or very performance sensitive. I'm hoping that the ecosystem will solidify around the new futures with async-await support this year, and Rust will start to be competetive in this area. In the meantime, I've been learning C#, which has a lot (although not all) of the niceties of Rust.
Haven't touched Rust thread handling yet but if you are splitting IO into more than one thread it usually slows it down. multi threads for cpu heavy works great for ~1-2 (system) threads per core, for IO as stated earlier 1 thread for IO and other cores for CPU splitting works nicely 
&gt; It is not uncommon for a multithreaded threaded implementation to be slower than a single threaded implementation this is what I was afraid of, at least in the first case. where I was using threads to split up work. &gt; Plus the overhead of message passing. also, this. I wasn't sure how much overhead there would be with message passing. I've experience other platforms having high overhead of message passing (namely with node where it was actually serializing the data into json before sending it over the message channel and then deserializing it). Ok, I think I just needed this spelled out to me. Since I'm still in the learning phase, I was avoiding going to tokio and its futures before I got a better handle on everything else, but that might be the way to go. Also, another user recommended another library (rayon) that I haven't heard of, so I'll check that out, too. Thanks for the info!
&gt; System threads are expensive to spawn darn. I was kinda afraid of that. I haven't done much with system threads in an environment where I'd notice that kind of problem; mostly just some ObjC stuff back in like 2003 where I used them to background a DVD burning process to allow the GUI to continue updating. &gt; Having too many threads actually reduces performance. ... Yeah, this is why I wasn't terribly surprised about the slowdown in my pet project, but with the project that reads stdout, it was unexpected considering that I only spun up 2. but, having spent a lot of time in erlang/elixir land, I'm going to have to un-learn that spinning up separate threads/processes isn't the way to go. in the back of my mind, I started thinking that maybe I should look at tokio, but was avoiding it because I'm so new to rust and didn't wanna go down that rabbit hole without fully grasping the language (like trying to use eventmachine before understanding ruby or twisted before grasping python). The comments here kinda hammered that home, though. so I'll take a stab at it. I'll also take a look at rayon. Thanks so much!
this seams to be the direction that I'm being pushed by other comments as well. this has been a real learning experience. Thanks for the info!
.next() is a required method for the Iterator trait (https://doc.rust-lang.org/std/iter/trait.Iterator.html). String::split() returns a struct that implements the Iterator trait. Since the struct implements Iterator, we know it has a .next() method, but your text editor may not know that. Check out https://doc.rust-lang.org/book/ch10-02-traits.html to learn more about traits.
i was just learning traits today and what you said made me see how traits work in real example that i can come across. thanks
Thanks very much! In combination with u/sekhat's advice upthread, I think that helps me understand what's going on a bit more/get my head around how the borrow checker has my back here. &gt; This basically means that you need to move data into the future Unfortunately, I seem to be having a bit of trouble doing so. I *thought* that (once I solved my `OutputStream::new()` issue) I'd be able to move my stream into the future. I thought all I'd need to do is put it in an `Arc&lt;Mutex&gt;`, but that doesn't seem to be working: fn main() { let stream = OutputStream::new().inspect(|s| { println!("{}", s); }); let stream = Arc::new(Mutex::new(stream)); let stream = warp::any().map(move || Arc::clone(&amp;stream)); let routes = warp::path("ticks").and(warp::sse()).and(stream).map( |sse: warp::sse::Sse, stream: Arc&lt;Mutex&lt;futures::stream::Inspect&lt;OutputStream, _&gt;&gt;&gt;| { let stream = stream.lock().unwrap(); sse.reply(warp::sse::keep(stream.map(|s| warp::sse::data(s)), None)) }, ); warp::serve(routes).run(([127, 0, 0, 1], 3030)); } I **thought** this would work, but it still gets a `cannot move out of borrowed context` error. What am I missing here? Thanks again for your help—I think I'm starting to get my head around this a bit better. Any thoughts on how I can 
&gt; Creating a variable struct wouldn’t work because different variable require different fields, and different generic type parameters, so using empty trait objects seemed like the only option. Enums can do this. Look at `Result`. It has two distinct type parameters, but each variant only uses one of them. Different variants can also have different fields.
You should not be seeing a huge decrease in performance with what you are doing, and you should be able to use threads to make it at least a little faster. While the answers here are potentially leading in the right direction, none of them actually explain what you are seeing. This is a good opportunity to figure out effective profiling in Rust.
I'm giving this a second reply instead of an edit because oh man. So I looked at rayon and this is awesome and works great and is actually faster than the old sequential implementation. Also, only took me about 90 seconds to get working without modifying my original code at all. literally changed `.iter()` to `.par_iter()` and it worked. Thank you so much! I'd upvote you twice if I could.
Nothing about that design sounds good or right. Nothing about db or cache design rwqure the asinine restrictions you're artificially imoosing based on your current design. Surely you must realize that major webapps do not run in a state where they require reslurping all data to memory. Even your text about databases, pointers, and caches make no sense.
&gt; This is a good opportunity to figure out effective profiling in Rust. that's on my short list. any resources you like that you can point me to (besides google)? I've also gotta learn how to debug, if there even is a debugger. sprinkling `println!()` around and writing tests are only getting me so far.
rest api sure should be but my program doesnt have to be stateless 
What would you suggest for calculating recursive accounting of gigabytes of userdata? if there is no state then they have to be calculated every time the function is called and this opeation is really expensive thats the reason what you are saying actually doesnt make sense. If i have calculated something why to throw it away and recalculate next time ? 
*then cache it*
My needs are definitely simple. Will be a handful of micro services to support a game server (think basic account signup, management, payment processing/web hooks via Stripe). I’m really familiar with both Node and C#, so I’m battling whether I should tough it out with Rust or stick with what I know. It’s a small team of people (and I’m the primary dev) so I’m also siding with what I (and others) know over something new for both development time and maintenance.
I like using GNU parallel for easy parallelization in the shell. Building off of /u/nofunallowed98765's: gitall() { find . -type d -name ".git" | parallel cd {//} '&amp;&amp;' pwd '&amp;&amp;' git $* } However, I found the `find` part to be the slowest part in my own tree. Do you know if your directory walker is faster than `find`?
For the most part, you should be able to profile and debug Rust with the same tools you'd use for C or C++. Those tools might be lacking minor details like pretty-printing Rust types or dealing with Rust's name mangling, but they should work perfectly fine. To get those niceties (for debugging at least), if you're running Linux or macOS, you should have gotten either `rust-lldb` or `rust-gdb`, respectively with your installation of rust. They're just small scripts that wrap `lldb`/`gdb` to add pretty-printers for Rust. It's possible to get those conveniences in some profiling applications - I believe there's a name demangler for `perf`, but I'm not too sure on that front. 
Glad I could help! :)
thats exactly what i am saying i am caching it in memory and sometimes its really hard or impossible to pass this cache around without having a global state 
The pattern with using destructors to make code be called no matter how the block is exited is available more conveniently with the crate [scopeguard](https://crates.io/crates/scopeguard).
I use the same directory walker as ripgrep, [walkdir](https://crates.io/crates/walkdir). It's "performance is comparable with `find` and glibc's `nftw` on both a warm and cold file cache". Frankly, I think the performance of walking the tree is moot, since Git is almost always going to be orders of magnitude slower. 
oh, awesome. I do have those! I didn't realize. I'll def be using those from now on. I just assumed that I couldn't use `lldb` or `gdb` with these binaries. This is the first language I've used that compiles native binaries outside of C, so yeah. Thanks so much!
cli.rs -- consider using https://crates.io/crates/strum to generate FromStr and to string implementations for your color enum. https://github.com/mattmahn/gitall.rs/blob/master/src/main.rs#L49 -- why do you use `loop` instead of `for`? You can use it to loop over any iterator, and since you break whenever the iterator returns None I think it is exactly what you want. https://github.com/mattmahn/gitall.rs/blob/master/src/output.rs#L42 -- `style.set_fg(Some(if success { Color::Green } else { Color::Red }));` I didn't look it over carefully. Nice job on everything else!
Oh yeah! You should wrap it in a mockable Singleton but at the end of the day, yes, something, somewhere will hold a Singleton reference to the top of your state. It may not even be a global though, but rather a variable local to main that is passed around as part of a state struct. Does that make sense? If not, I can make up an example with global state without a global var. Cheers.
I understand the point being made with respect to control by a single company (or lack thereof in the case of Rust) - and it's a *good* point to make - but I'm not sure "Rust is the new Bitcoin" is a selling point. If I knew little or nothing about Rust and read that statement it would be a _major_ red flag to me. I'd go running as fast as I could in the opposite direction.
I think it is a good point that the run-anywhere capability of Rust + WASM has the potential to put Rust in the same or a similar niche that JavaScript occupies. On the other hand I think that there is some unnecessary FUD in this article: &gt; Learning TypeScript is bad pain &gt; Go however is solely maintained within Google, and we all know what happens to most Google projects after a few years. I think that these criticisms are not well-supported, and are not necessary to sell Rust. You already made a number of points that show that Rust stands on its own merits.
Stick with what you know, make the thing you need, and then look into Rust if you need better performance. By the time you come back to Rust it'll probably be much nicer for writing asynchronous web things.
Yeah, "Rust is the new JS/Bitcoin" isn't exactly a compliment in many people's eyes. Rust discussion outside this sub is already susceptible enough to non-constructive jerking, if this gets posted to proggit expect mockery and memes to dominate and the actual points made to be overlooked. I'd liken it to Red's (an interesting language successor to rebol) year spent with little communication about the project other than blockchain crap, which was a pretty major turn-off to onlookers who were not already invested in the community.
On the one hand, the JS community is very large, and siphoning-away even a small slice of it would greatly expand Rust's popularity. On the other hand, crates.io would be stricken by the Framework&amp;nbsp;Of&amp;nbsp;The&amp;nbsp;Month&amp;trade; phenomenon as a result.
How about the fact that a Rust re-build takes several minutes, and a Node.js "rebuild" several hundred milliseconds? Omitting that from the article seems strange.
I’d follow a rust server tutorial straight through and go off of that. Also don’t be afraid to open GitHub issues. The rust community is really nice and responsive. As for Mongo, there’s probably some really nice Serde integrations. Rust does beautifully with JSON. 
try this - ``` let args: Vec&lt;String&gt; = std::env::args().collect(); let mut thread_handles = vec![]; thread_handles.push(thread::spawn(move|| { let app_info = AppInfo::new(args[1].clone(), args[2].clone(), "Profiler".to_string(), "/profiler".to_string(), "en".to_string(), None, None, None, Some("Welcome to profiler".to_string()) ); profiler::ui::run(app_info); })); let reddit: Reddit = Reddit::default(); thread_handles.push(thread::spawn(move|| { match reddit.fetch_posts_ids(1) { Ok(pr) =&gt; { match reddit.fetch_posts(&amp;pr) { Ok(posts) =&gt; { dbg!(posts); }, Err(e) =&gt; { dbg!(e); } } }, Err(e) =&gt; { dbg!(e); } } })); for handle in thread_handles{ handle.join(); // handle.join().unwrap(); } ```
This is what [this crate](https://crates.io/crates/spanquist) does too!
Sorry - I am a Rust beginner myself. I am extrapolating from Java and C mainly. :)
I love Rust as a language, possibly more than any other language. But I strongly disagree with some aspects of this article. &gt; However, now instead of learning JavaScript, you are learning a tool from Microsoft which adds another layer on top of all the other layers to make JavaScript run on the server. If you have a criticism of TypeScript, go right ahead and say so. Saying “it’s a tool from Microsoft” in a way that implies that it’s somehow inferior isn’t a real point. Does TS solve a real problem? Yes, it adds types where there are no types, preventing certain problems from occurring. &gt; When you learn TypeScript, you mostly cope with a tool which is there to improve a language which lacks well thought through features. If you want types, and you might need them in bigger applications, you can move to a typed language right away. Why bother with a transpiler? This is generic JS hatred I’ve been seeing for a decade or more. Does JS have issues? Of course. Do we fix those problem? A reasonable person might say yes but an Internet blogger would say “nah let’s rewrite everything in a new language”. I’ll spell out why TS is a good idea. Most folks writing JS are already using a transpiler. Adding TS to their project doesn’t substantially change their build process. Next, TS allows them to gradually improve their code base. On vast codebases, it’s prohibitively expensive to propose a complete rewrite. It’s manageable to improve parts of it piecemeal, starting with the sections that need love. After a couple of years you end up with a codebase you’re happy with while maintaining a good cadence of releases. It’s the exact same process with Kotlin or Swift. You have an existing codebase that you want to convert to a better language. You improve it piecemeal, file by file, module by module. A person who doesn’t realise why languages like TS, Kotlin and Swift prioritise interoperability with the languages they seek to replace doesn’t understand software development. 
Arc&lt;Mutex&lt;T&gt;&gt; Gives you a reference pointer that can be shared between threads and a mutex that ensures the inner data type can only be accessed by one thread at the same time. There are other solutions for safe concurrency but in every language you have to pick one of them or your program is unsafe, this is not rust specific.
Yes, that's somewhat a known gotcha. Or let's say a tradeoff for the ability to be able to cancel always without extra code. It might not be too bad if people are aware about it (and use destructors for everything that looks like a resource), but will certainly lead to one or the other surpise. &amp;#x200B; There also had been recently some discussion about it at [https://trio.discourse.group/t/structured-concurrency-in-rust/73](https://trio.discourse.group/t/structured-concurrency-in-rust/73) and at the end of [https://internals.rust-lang.org/t/explicit-future-construction-implicit-await/7344/107](https://internals.rust-lang.org/t/explicit-future-construction-implicit-await/7344/107) &amp;#x200B;
Subscribe
You want /r/playrust, unless the thumbnail is wrong and this is a programming tutorial.
Not true. The rust webserver actix, is one of the fastest webservers out there. NodeJS could never even keep up with java webservers.
A rust rebuild doesn't take that long, like a few seconds for a webserver for me. NodeJS takes longer for me.
You're on the wrong forum. /r/rust is for the Rust programming language.
The typescript statement doesn’t even make sense. If learning typescript is hard what does that make rust?!?!?
I cringed so, so, so much reading this. So much of this is red flags for "someone who knows just enough to be really dangerous". I can't even think of much that would've been appropriate to write in nodejs that makes sense to rewrite in Rust, other than if you were a hype-chaser or had a specific studied use case like npm.
/r/playrust
When I *go to definition* of the `.split()` method, I see that it is declared as pub fn split&lt;'a, P: Pattern&lt;'a&gt;&gt;(&amp;'a self, pat: P) -&gt; Split&lt;'a, P&gt; The struct `Split` is defined in the same file (src/libcore/str/mod.rs), but with the `generate_pattern_iterators!` macro. While IntelliJ is able to expand macros, type inference doesn't work that well across macros. This means that IntelliJ doesn't know that the returned value of `split()` implements the `Iterator` trait, so it doesn't know about the `next()` method.
I have no idea but my experience show that an Android service without returning to Java side will receive a quit signal. I need to run my server in its own thread.
I can't say I've seen that kind of behavior with Diesel, though it's possible you have some sort of recursive behavior going on? You could ask in the Diesel gitter channel, might be easier to get help there. [https://gitter.im/diesel-rs/diesel](https://gitter.im/diesel-rs/diesel)
Looks like "Source Code Pro" based on inspect element.
In the [repo for TRPL](https://github.com/rust-lang/book) I don't see the fonts being overridden from the [mdBook](https://github.com/rust-lang-nursery/mdBook) defaults, and [it looks the same](https://rust-lang-nursery.github.io/mdBook/format/mdbook.html) to me. I think the fonts you are looking for are defined [here](https://github.com/rust-lang-nursery/mdBook/blob/master/src/theme/css/general.css#L19), meaning you want [Source Code Pro](https://github.com/adobe-fonts/source-code-pro).
Awesome!
In safe code, undefined behavior must not occur. This means that any function that might enable someone to cause UB, must be marked as unsafe. In unsafe code, you are able (but not permitted) to cause UB. Every unsafe function must have constraints/invariants to prevent UB. These invariants should be documented. When an unsafe function is called from an unsafe block, you must make sure that their invariants are upheld.
The other answers here are likely correct for the online edition, but the dead-tree version I have lists different fonts. The font used for the code samples there is "TheSans Mono Condensed".
In my opinion, if you know JavaScript and you have coded in a statically typed language before, then TypeScript feels like how JavaScript *should* be. I never looked back after picking up TypeScript.
Definitely looked around, haven’t found all the pieces as an example to follow. I suppose I’ve just thought “surely someone’s put together a basic CRUD API example using Mongo as their DB...” but perhaps not? Case in point, I’ve seen the Actix-web examples of routing but it seems to just be for basic GET /index. How does this scale with multiple controllers and routes for a single resource? How do I extract query and route params? What about injecting the db instance to all routes efficiently? All the examples I’ve seen fall way short of explaining how a simple but complete MVC-style web app would work. I hate to come across as entitled but I just feel like this simple of an example is sort of necessary if you want devs to make the switch to new languages and frameworks.
Completely agree. I don't think it's entitled to ask for some maturity in your ecosystem. I'm coming from Rails personally and there's definitely a lot of stuff I took for granted (ActiveRecord + GraphQL Ruby is so smooth). What I've been doing is a combination of reading other people's repos, posting issues and just guessing. It's tricky work and at times very frustrating, but it's necessary for the ecosystem to evolve. And it's completely possible that my app will end up with some mistakes or antipatterns that screw it. But fortunately, Rust is pretty easy to refactor. I've already done one folder structure rearrangement and the compiler pretty much guided it. As for actix, is there any reason you want to use it in particular? I've had a pretty good time with Rocket. Documentation is decent and there's built in support for query params/route params/request guards.
I've had good luck using Asynchronous Linting Engine with RLS. Check out `w0rp/ale`, which has a solid `:ALEGoToDefinition` command for at least that facet of "code navigation". Also, join the `neovim` dark side, even if it's not strictly necessary to get RLS working
I looked at Rocket and the only thing I had against it was the need for nightly builds. But since I plan on containerizing the app via Docker that may not really be an issue syncing versions. I did like the docs on Rocket for sure though, and I see they are transitioning to being on stable. I keep telling myself that most of my fears are simply exaggerated and letting perfect get in the way of good enough. And I’m sure after doing v1 I’ll learn a lot of be ready to refactor to v2. I’ll probably check out Rocket tomorrow and see how it feels. I know I won’t be nearly as fast at first but I really am interested in learning Rust and building something with it.
&gt; surely someone’s put together a basic CRUD API example using Mongo as their DB... Most people use `diesel+postgres`, AFAIK. There are drivers for mongo, though. &gt; How does this scale with multiple controllers and routes for a single resource? How do I extract query and route params? What about injecting the db instance to all routes efficiently? The Actix docs have examples for [routing](https://actix.rs/docs/url-dispatch/) and [using database connection actors](https://actix.rs/docs/databases/). &gt; All the examples I’ve seen fall way short of explaining how a simple but complete MVC-style web app would work The [here’s an example](https://medium.com/sean3z/building-a-restful-crud-api-with-rust-1867308352d8) using rocket. As of `0.4`, rocket handles the database state for you: https://rocket.rs/v0.4/guide/state/#databases. Again, if you want to use Mongo with rust, it's just not popular in the ecosystem compared to postgres, so there aren't as many examples out there. If youre set on Mongo, substituting the diesel layer shouldn't be too hard. 
Thanks for the resources! I don’t mind going a little bit on my own with the Mongo aspect. I’ve done some research on it + r2d2 connection pooling. Even writing my own ORM abstraction layer wouldn’t be too hard considering Mongo is lenient and document oriented. I’m more curious on the “best practices” aspect when it comes to building an API in Rust. Passing stuff around (loggers, db state, etc) between request handlers. The Rocket docs definitely seem more comprehensive so I may start there instead.
No prob! For logging, actix has [built in middleware](https://actix.rs/docs/middleware/#logging) for it. For a more complete Rocket project, [this repo](https://github.com/TatriX/realworld-rust-rocket) might be of interest. I personally like `gotham`. Their repo has a [pretty extensive examples directory](https://github.com/gotham-rs/gotham/tree/master/examples) which give you the basic building blocks, but they don't have a comprehensive example yet (might be an issue to file 😉)
Oh wow I hadn’t come across Gotham! I really like that it’s Hyper-based and uses fluent syntax for constructing routes and handlers. Nice set of examples and having both downstream and upstream middleware functionality is great. Looks like I have my Sunday cut out for me...
&gt; Even writing my own ORM abstraction layer wouldn’t be too hard [this library](https://github.com/zonyitoo/bson-rs) uses the awesome `serde` crate, you might want to check that out before shaving that yak ^^
`Cow::from([5,6,7,8,9])` -&gt; `Cow::from(&amp;[5,6,7,8,9])`
I think the right way to do or logic in terms of return types is Enum. Although I have a feeling that this is not what you are after.
Can't you just return an enum, and make impls for your variants of you want to transparently access the return value ?
I agree. If anything, typescript fills the void that author describes with missing types in javascript in the first place.
Rust has incremental recompilation. TBH my medium-size rust project recompiles in about the same speed or even faster than my medium-size typescript project.
Why not both? Sometimes you want to read from an immutable buffer and write into a mutable destination. Sometimes your buffer is already mutable and of the appropriate size to do the in-place enc/decryption. Unfortunately I don't think rust will let you imply one from the other, unless NLL is really good at its job. Generally speaking the user probably has a mutable buffer available so at least, as ring does, share the buffer for both purposes. Not sure what ring does but the return type should IMHO contain the appropriate slice. I.e. decrypting should return the slice without the ciphertext expansion. Encrypting should contain the whole slice.
Please be very careful with HighwayHash's claims of security until ones will be strictly proved. In general it should not be used as a cryptographic hash function. On the other hand, HighwayHash can be safely used for HashMap. In this case t1ha (t1ha2\_atonce, t1ha2\_stream) given the same properties (quality and speed), but in portable way without AVX. &amp;#x200B;
 async { loop { yield Poll::Pending; } } Is this really so complicated? Of course awaiting it will never return. At least with channels, you could potentially notify the receiver that nothing will send, but you can't do that with this example. If a future completes unsuccessfully, then it returns an error; if it didn't complete, it won't return.
Is it possible to use Rust's built-in *Trie* structure to make a key-binding trie? IE, I want to be able to say "Execute this function once the user presses Control+N", so I'd need to to support single "Key-Press" items as keys, with functinos that take no arguments as values. However, I'd also want to support multiple-key keybinds, such as `C-x C-f`. I think a Trie would be a useful data structure since it represents the mappings pretty well and allows me to avoid a bunch of error checking in the case where the user enters an invalid shortcut. 
Global variables are discouraged, but there are cases where mutable global state is required. The best way to handle it in this case is to use something like a Mutex that can grant access to a given request while making sure nothing else touches it in the meantime. There are types other than Mutexes that would do the trick like Atomics, but Mutex is the easiest to understand and use at first. You could also offload it to another process, but then you have to make sure *that* process is threadsafe. Using a Mutex is completely "safe" in that it won't corrupt your data of put your application into an invalid state, but it's generally discouraged where unnecessary since it separates what's being accessed from where it's being accessed, potentially making it harder to debug, but again, sometimes it is necessary, which is why Rust provides it.
This works for me: use std::io::BufReader; let input_lines = tokio::io::lines(BufReader::new(tokio::io::stdin()));
There is a built-in enum, for handling either owned or borrowed data: [`std::borrow::Cow`](https://doc.rust-lang.org/std/borrow/enum.Cow.html).
This only works because the array literal is static. I'd guess that OP has simplified the example code.
Meowhash has major weakness by design. Please see my explanation at [https://github.com/cmuratori/meow\_hash/issues/38](https://github.com/cmuratori/meow_hash/issues/38). Currently t1ha0\_aes\_xxx() also is not ideal, but (both internal variants) provides little bit more quality in comparison to Meowhash. So, as I noted in referenced dissus, I working on the new design of t1ha0\_aes with two injections point of data (therefore it will provides more quality and security).
Threads are not a direct analogy to erlang processes. Actix actors would be a better way to simulate that behaviour, and the actix system does have a thread pool similar to rayon.
*Someone* has to keep the array around if you're going to return it. There are basically three valid cases: 1. You're returning something owned (Rust doesn't yet have const generics, which is a pre-requisite for stack-allocated arrays of non-specific size, so `Vec&lt;u8&gt;` is what you have to use here unless you want your function to only be able to return arrays of a fixed length known at compile time.) 2. You're returning a view of something owned elsewhere. (In which case, you have to pass it in as an argument, return a slice (`&amp;[u8]`), and, if you have more than one argument, use lifetimes annotations to indicate which argument the return value depends on. 3. You're returning a constant string, in which case, you need to return a slice. `Cow::Borrowed(&amp;[5,6,7,8,9])`, as omni-viral said. Judging by your example, #3 appears to be what you want. Your code reminds me of code of my own where I sometimes want to work with `const` default values without cloning them and sometimes with dynamically-specified values.
That's actually very useful feedback. People in the Rust project often have many jobs. I'll address this point in the retrospective. Also, the issue that our user base more strongly diverges into multiple groups (e.g. "just users", "interested enthusiasts"...) is a problem we've recently been discussing. I can totally agree with the issue and you will probably see some changes in our communication structure around that in the coming months.
Yeah. I’m aware of that. When you live in one languages environment for a while you start applying its methodologies to the tasks at hand when you switch to something else. But I’ll check out actix. It sounds like it might implement the actor model which emulates the behaviour of things like scala’s aka and erlang. 
Thanks. 
I see. Thanks for your help. 
I also have the print edition but I wanted know the font for the online edition. I appreciate your help anyway though :)
Then he wouldn't able to return `Cow&lt;'static, [T]&gt;`
A similar issue exists with blocking rust as well: any call may not return due to a panic. If you are writing complex long-running resilient applications with panic=unwind, you'll need to think about it. A rule I've internalized recently is to try to push as much cleanup as possible into *implicitly* called drops. This is a cool approach, because it means that happy path and unwind path use the same cleanup logic, so you suffer less from undertested exceptional paths problem. The specific code that made me realize this rule was surprisingly close to the example from the blog post: I spawn a helper thread to do some work, and I want to cleanly join int. The first version of code looked like this: ``` let (sender, receiver) = channel() let thread = spawn(|| { for msg in receiver { do_work(msg) } }) do_other_stuff(); drop(sender); thread.join().unwrap(); ``` The problem with this version is that, if `do_other_stuff` panics (or even returns an err via `?`), the `thread` is never joined and this is not ideal: if I restart the job, there may be two interfering background threads. So I've changed the code to use a variation of `JoinHandle` which joins in drop: ``` let (sender, receiver) = channel() let scoped_thread = scoped_spawn(|| { for msg in receiver { do_work(msg) } }) do_other_stuff(); drop(sender); drop(scoped_thread); ``` This works for the happy path, but for the not-so-happy path I could get a deadlock, because `scoped_thread` is dropped first. It seems like "deadlock or a leaked thread" is a common problem in these sorts of things. The final iteration is this: ``` let scoped_thread; let (sender, receiver) = channel() scoped_thread = scoped_spawn(|| { for msg in receiver { do_work(msg) } }) do_other_stuff(); ``` Here, because I don't try to change drop order manually, I don't have "two different code paths" problem. It's interesting that you can't apply the similar pattern to the `async` code, because you shouldn't be blocking in `drop`. So leaking a thread seems like the best solution one can achieve? This seems like a general weakness with the current async story: there's no `AsyncDrop`. That is, there's no analogue to Python's `async with`. 
Is is available to add crates (rust's package unit)? I like generic build system like meson like meson or tup or something else. As for rust's ecosystem, cargo does too much (download &amp; build packages, testing, run subcommands). It's not clear or difficult to find that cargo and other build system work coordinated way.
Yes, OP doesn't actually want a borrow here: &gt; I don't need to keep the array around, so it could (and probably should) be owned. So `Cow` doesn't look to be the right solution.
And then comes the need to actually talk to enterprise backends. How is Actix dealing with PL/SQL stored procedures, using UDT parameters in a RAC cluster?
If you build with debug, you can get a stack trace when running from android studio. 
[removed]
https://gdbgui.com/ also explicitly lists Rust as one of the supported languages if you want a GDB frontend that can visualize data structures and graph the values of variables over time.
Specifically, in this case, think about multiple concurrent requests executing in parallel... ... if all requests start by locking the global state, then you are using multiple threads but not parallelism, as they are each using the state in turn.
I use vim-lsp and it works ok. It supports both NeoVim and Vim 8 thanks to async.vim. 
I think it will be interesting to read a [short article about t1ha](https://habr.com/en/post/439156/).
I'm not the asker, but thanks for the excellent &amp; thorough response. Makes a lot of sense!
Do function-specific imports differ in performance from top-level imports?
Odersky also mentions this in one of his recent talks. Csharp has a much larger grammar than scala but is not perceived as complex. https://www.youtube.com/watch?v=uqKxB3eRKlY&amp;feature=youtu.be&amp;t=595
How about returning a [SmallVec](https://crates.io/crates/smallvec)? It works just like a Vec but with the magical property that, if there are few elements in it, it doesn't heap-allocate.
What are your favorite resources for learning how to write custom derives in 2019? I'm especially looking for articles and simple examples from crate containing up to date code. For context, I'm looking to derive some mocks in my own code for testing purposes.
(thanks for the replies, I eventually got this working.. big relief, I can go back to evangelising rust guilt free, and have another go at weaning myself off the apple ecosystem with future device choices..)
Actually t1ha0 (exactly t1ha0_aes_xyz) don't relies on avx/avx2, but uses ones only for unloading results. On the other hand a compiler uses VEX-prefixes for instructions encoding, which is handly. It is also worth noting that inside there are two variants of t1ha0_aes with using AES instructions for different generations of Intel processors. For comparison t1ha with SipHash (AFAIK the default Rust hasher): 1. t1ha (exactly t1ha2_atonce) 5-10 times faster that SipHash and provide the same quality (i.e. pass all statistical tests, including most strongest). 2. All variants of t1ha provides seeding for security against DoS attacks. 3. Security properties of t1ha (exactly the t1ha2 variants) are discussed for now. - t1ha0 is not portable and may have weakness by design (since the speed is the primary goal). - t1ha1 has avalanche weakness that discovered by modern SMHasher from Yves Orton. - t1ha2 shown excellent avalanche and non-linear properties, but nobody shown complete strict cryptoanalysis for now. - by some unpublished estimations of 128-bit version t1ha2 no less secure than SipHash. In any case, I am very interested in independent third-party evaluation of t1ha2 properties. To do this, the [compression cycle](https://github.com/leo-yuriev/t1ha/blob/bf8743c4276deb7146f0aaacdabc8b6aaec8dfdc/src/t1ha2.c#L69-L74) and the final mixing must ([part-1](https://github.com/leo-yuriev/t1ha/blob/bf8743c4276deb7146f0aaacdabc8b6aaec8dfdc/src/t1ha2.c#L190-L198), [part-2](https://github.com/leo-yuriev/t1ha/blob/bf8743c4276deb7146f0aaacdabc8b6aaec8dfdc/src/t1ha_bits.h#L1080-L1086)) be thoroughly analyzed. Anybody welcome!
I spoke about this in [another comment](https://old.reddit.com/r/rust/comments/az4xu0/production_rust_help_too_many_open_files_os_error/ei5dked/). Thank you for the response. It is on my list for Monday to triple check and maybe add the work-around for good measure. Cheers!
I am using a single Hyper Client and using the reference when making the request. I am also only using a single Redis connection to listen to the pubsub so I think I'm OK there without connection pooling. Thank you for the tip about the keep alive header. I am certainly not doing that! Cheers
This is a very nice initiative
My telepathy needs improvement.
Outside extraordinary cases, connection pooling is something that you always use no matter what. In your case it could be good enough to don't do it, but in general is the very first thing you should think about :) Cheers, 
Author here. Didn't know that the article made it into Reddit! I agree, the sentiments being made were provocative! However, my reading stats and conversations with people show me that there is a huge interest i Rust from within the NodeJs community. To TypeScript: Yes, it solves many problems! As a consultant howver I have yet to come to a project where it helps to create a better code base! What I am mostly seeing is that larger Node codebases lack types. TypeScript however is not opiniated enough (like Go or Rust), so you can end up with a Java-like or a C#-like codebase of the first developer has a different background! My point I wanted to bring across (and will publish soon): Rust is a great change to step up your game! It is a super solid language with features and crates being build at a nice speed! Big companies tried it out and were super happy! So I see it with learning Lisp: Everyone says learning Lisp will make you a better programmer, even if you don't use the language! Rust will make you a better developer as well, and you can even use it (more and more). Will NodeJS stay? Of course! But the possibilities of Rust are so great that it is (in my opinion) worth the effort and struggles in the beginning! Here is the follow up article: https://www.reddit.com/r/rust/comments/azemtn/intro_to_web_programming_in_rust_for_nodejs/ I already prepared a more nuanced article of why Rust makes sense for a personal "business" perspective. I will publish it next week! Thanks for reading!
4) Move an owned `CryptoRng` into `DoubleRatchet`'s constructor to avoid repeating it and cluttering the scope.
+1. And suggest returning an `enum` with variants containing the required types or a `Box&lt;dyn Trait&gt;` as possible solutions.
Yeah I am a C++ dev at work and never really been very into JavaScript. But I picked up typescript for the first time on a freelance gig and it’s a an awesome language. I was pleasantly surprised. 
&gt; This seems like a general weakness with the current async story: there's no &gt; AsyncDrop. That is, there's no analogue to Python's async with. This begs the question: Could we have a `AsyncDrop` and should we if we could? 
Your question is so broad that it's almost impossible to know where to start. I assume "reading the output of the source" means you have a sound file of some kind. The question is what format it is in. Uncompressed audio is typically represented as a series of sample values in a row. But even then, it could be "packaged up" in a container format (such as aiff, mpeg, mkv etc) If we assume you have plain raw sound data without any container: To play it back, you need to know how many bits make up each sample (often 16, but can be 8 or 24, depending on context). Then you need to know sample rate. How fast should the samples be played back (44.1khz is always a good guess, but could really be anything) Finally for stereo (2 channels), it's quite common to interleave samples. So you need to know how many channels there are. That's all assuming you have uncompressed raw audio. If you have compressed audio, you need to know what it is. Some mpeg variant? aac? Compressed audio probably comes in a container. Depending on what, you might need one or two libraries to "unpack" the raw sound data. Example: Matroska container (mkv) can be unpacked with libmatroska which might give you AAC which you can decode using libfdk_aac. 
I notice that `scopeguard` has three macros: - `defer` - `defer_on_success` - `defer_on_unwind` But if an `async` function is dropped at an `await!`, then that is neither `success` _nor_ an `unwind`, correct? It's some kind of third thing, and it's not obvious to me how I should handle this third thing with `scopeguard`. From the docs, I would _guess_ that `scopeguard` would treat a `drop` during an `await!` as a `defer_on_success`?
I mean technically this: if id in agg: agg[id][“topic_set”].add(topic) else: agg[id] = { “topic_set”: set(), “language_set”: set(), “primary_language”: None, “watchers_count”: 0 } agg[id][“topic_set”].add(topic) would be this: agg.setdefault(id, { “topic_set”: set(), “language_set”: set(), “primary_language”: None, “watchers_count”: 0 })['topic_set'].add(topic) though I can understand doing it the other way as the second version will generate a lot of up-front allocations (whereas empty Rust collections generally don't allocate). A better alternative would be for `agg` to be a `defaultdict(lambda: { “topic_set”: set(), “language_set”: set(), “primary_language”: None, “watchers_count”: 0 })` in which case it'll only create a new aggregation dict/struct if there is none at the `id` slot. Somewhat similarly the Rust code could just `derive(Default)` on `aggregate` and use `Entry::or_default` instead of `Entry::or_insert`. There's also lots of unnecessary double lookups in the Rust code e.g. if agg.contains_key(id) { agg.get_mut(id).unwrap().language_set.insert(language); //get_mut(id) let's us get the entry and change it } why not if let Some(agg) = agg.get_mut(id) { agg.language_set.insert(language); } ?
It looks like the work that was traditionally done in a relational database is now being done, at much greater cost, using pandas dataframes. The groupings you discuss are trivial exercises within a DB. My advice is to learn relational data modeling and SQL. Use Rust to solve new problems.
Everything you have mentioned is great. But I wanted to keep the syntax as close to python as possible. If I used Some, then without knowing enums, it wouldn't have made any sense for a typical python developer. Same goes for Derive also. It was a deliberate choice from my side. Do you think I should have introduced these syntax at beginning itself? 
This is awesome \~the Rust meetup has been totally inactive for about a year. I hope beginners are welcome? See you there OP.
I have no clue what you're asking but you might want to give more details for people to be able to answer your question. Please mention what devices they are and what kind of software/OS are they running. What kind of hardware or link connects them
Communication is challenging. Rust is another Full Stack Language. It is not the new JavaScript. It is another tool for building distributed systems, not the new Bitcoin. 
&gt; Is this really so complicated? Of course awaiting it will never return. This is actually a different case than the one I'm thinking about. In your example, the future remains live, but never makes progress. (You can also achieve the same thing by never polling the future.) If the future holds a channel `sender`, the channel will remain open indefinitely. In the case I'm thinking about, the future is dropped during an `await!`, which means that any channel senders will be shut down in an orderly fashion. This may be unexpected.
Thank you to the links for pre-existing discussions! &gt; Yes, that's somewhat a known gotcha. Or let's say a tradeoff for the ability to be able to cancel always without extra code. I think I'm mostly OK with that tradeoff. Or at least I consider easy cancellation to be a _really_ valuable feature, and not one that I'd sacrifice over this issue, if it came down to that. But at the very least, there's an educational issue here for futures-heavy code.
Pandas is actually pretty fast compared to traditional DBs for many operations. It's completely in-memory analytics tool. I have used MySql, postgres and other DBs. For this use case I mentioned, it took several minutes to just import the csv file into the postgres table. Maybe I am wrong. For exploratory data analysis like this, Pandas is generally a very good choice and a popular one in data science community. Relational DBs are used when I have to perform similar queries regularly on a well designed and stable database. Here I have to do this join only once. Even Spark proved to be slow when it's sole purpose is Analytics. That's why I gave this comparison. If you still aren't convinced, please try downloading the data dump mentioned in that article and try performing the same task and see the total time it takes for the whole process. 
Yeah!! Excited to spark some activity :). Beginners are absolutely welcome and designed towards. Our definition of "beginner" is someone who's played around with the language and knows how to make a simple program, just because the time will be too short otherwise. We'll also be around to help though! See you there!
Also TS is maintained by MS on github, not by google so that’s just flat out false lol
Google hosts ClusterFuzz for open-source projects for free. See https://github.com/google/oss-fuzz Rust stdlib would definitely be in scope. In fact, one person on oss-fuzz team has expressed interest in getting it on oss-fuzz. I've also been experimenting with automatically generating fuzzing targets for libraries with large API surfaces, such as Rust stdlib. The code works for simple cases and can be found here: https://github.com/Eh2406/auto-fuzz-test 
What resources would you suggest for learning traditional data modeling and SQL? One reason I have stayed away from it as I like being able to completely reason about the performance of the system, and databases always seem like gigantic black boxes that you have to cast cargo cult spells on in order to try to get good performance out of.
I'm going to check it out! I've been trying to learn rust too, coming from Python and Java myself. This is great
Welcome to the Rust community 😀
Go people are always trying to argue that the defer feature in their language is equivalent to RAII but it’s not. The real power of the tactic comes from having members that have members that have members that manage resources. Things like scope guard don’t help with that at all.
The best thing about learning Rust for me is how much I am learning about computer science and programming itself in general some of which never clicked for me before in college. 
You are rediscovering RAII from C++.
It depends on the goal of the snippet. Are you trying to illustrate how similar to python rust's syntax can be or are you trying to demonstrate how concise rust's syntax is? If it's the latter, then you want to write proper rust. If it is the former, then the snippet is okay, though it would be better if you were to provide a different example, where proper rust is written (it's not Haskell, so there are plenty of contexts where python code can be translated almost verbatim)
Author here: Would you mind summarising your "needs" in a DM to me? I am working on a tutorial and any questions and needs are a huge help!
&gt; Google hosts ClusterFuzz for open-source projects for free. See https://github.com/google/oss-fuzz &gt; Rust stdlib would definitely be in scope. In fact, one person on oss-fuzz team has expressed interest in getting it on oss-fuzz. That would be an excellent transition for bughunt, or some derivative of it. The project is ClusterFuzz ready as-is and I'm positive oss-fuzz has more CPU time than I can afford. &gt; I've also been experimenting with automatically generating fuzzing targets for libraries with large API surfaces, such as Rust stdlib. The code works for simple cases and can be found here: https://github.com/Eh2406/auto-fuzz-test I saw that and mentioned it briefly in the blog post. I intend to try and generate targets in the coming week, as time allows, with it. 
My intention was to show how easy it is to follow Rust code even without knowing it. In addition, I also wanted to highlight the performance of Rust using real world example. So I tried to find a balance between the two. In upcoming articles, I will introduce other concepts anyway. Thanks a lot for your suggestions though. 
Thank you!
Thank you!
Is it somehow possible to use for comprehensions and simple arithmetic in toml? I need to generate a list of items with similar properties.. 
I am trying to say a more subtle thing: changing *implicit* drop order is problematic because it leads to different code paths for happy and exceptional paths, despite the fact that both paths are implemented with RAII.
Joe Celko's books are very good. His "Smarties" book is the most popular, but the other books are also useful: [Joe Celko's SQL for Smarties: Advanced SQL Programming](https://www.amazon.com/Joe-Celkos-SQL-Smarties-Programming/dp/0128007613) [SQL Antipatterns: Avoiding the Pitfalls of Database Programming](https://www.amazon.com/SQL-Antipatterns-Programming-Pragmatic-Programmers/dp/1934356557) 
It is a tough trade-off. On the one hand, you want things to be as easy to follow as possible, to show that Rust is not too scary. The flip side is that new Rust developers are going to run into idiomatic Rust code, and then be confused. I suppose you can do both, starting with the easy-to-follow way, and then explain the more idiomatic way. But that's more work, and makes the article even longer.
Thanks! I really think it's amazing coming from C how the rest borrow and lifetimes concept is set up. Just gotta unlearn couple of bad things I've learned along the way. Your post will definitely help.
That's exactly my thought. This is just an introductory article of Rust. I am planning to write a series of articles slowly introducing concepts of Rust keeping it as simple as possible and translating some appropriate Python programs to idiomatic Rust. 
I think you could've shown 'real rust' in this case. It doesn't impact the readability too much, and someone deciding on whether or not to learn rust based on this article will better know what to expect.
Part of the whole point of C++ RAII is the destructors run in reverse order of the constructors. In C++ you can’t change it even if you wanted to. If Rust lets you I would say this is a design flaw.
If you need your bindings to be dynamic, why don't you create a HashMap to dispatch a vec of pressed keys to an enum of actions possible, like this https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=20a8836ff2384da5e79617191969abfc ? 
&gt; The main point is: &gt; &gt; - I don't want to allocate if I don't need to. You can avoid allocation if the returned borrow points into an allocation and the allocation is not freed before the borrow goes out of scope. This puts a restriction on the caller. Basically your function needs the signature: fn extract&lt;'parent_borrow&gt;(a: &amp;'parent_borrow [Datum]) -&gt; Cow&lt;'parent_borrow, [Datum]&gt; which, because this pattern is extremely common, may be abbreviated without the lifetime parameter (i.e. "lifetime elision") fn extract(a: &amp;[Datum]) -&gt; Cow&lt;'_, [Datum]&gt; There may be other approaches that are valid, but you always need to design them so that the parent allocation isn't freed too early. &gt; - I don't need to keep the array around, so it could (and probably should) be owned. This is almost certainly easier and not much slower. Don't be too scared of copying data; be scared of copying giant amounts of data far too often. `Cow` is most useful in cases where you expect the borrowed path to be much more common. `String::from_utf8_lossy` is an excellent example. The case where replacement characters are inserted is almost an error condition and is expected to be less common than the bytes passing validation as utf-8, which allows them to be borrowed as `str`.
That's even better since I mainly use my Kindle!
I understand your point. When I introduced Rust to my colleagues, they became quite disinterested after looking at its weird syntax. What worked with them is writing their code almost similarly in Rust and then explaining certain elegance associated with the Idiomatic Rust. So I decided to use the same strategy here. In the following articles, I will introduce Enums, Structs, pattern matching and proper error handling and transform this code into Idiomatic Rust. 
Are these just going to teach me SQL or will I actually understand what the database is doing under the hood?
Always cleanup in drop. It's what it's there for... 
The documentation around the failure crate is a bit unclear on how you should use .context . Should I do things like some_result.map_err(|e| MyErrors::Case.context(e)) or the other way around ? some_result.context(MyError::Case) Also I've seen people using strings as context, is this a good practice? 
You're kind-of introducing the enums anyway via `unwrap`, it's just (to me) weirder. The "proper" rust version is shorter, simpler, and somewhat similar to Python's agg_ = agg.get(id) if agg_ is not None: agg_['language_set']add(language) which Python 3.8 makes into the very similar: if (agg_ := agg.get(id)) is not None: agg_['language_set']add(language)
I am not convincing anyone implementing this as I can do it myself. My goal was to introduce my vision of it in rust, but it seems, nobody is interested. I had already told many times before about why it can't be a macro, considering reading the github page, even lore, Centril also told that as he understands what is the problem with macroses and why it can't be integrated with the code quickly and ergonomically. I am tired repeating the obvious things which you could not even read yourself, not even to think about.. So continue blaming someone in thinking what in rust we could improve, doing it like that (like you do), I don't care anymore. As a challenge to your mind, implement things from my rfc via macro. Do it, then we can talk.
I care
It's amazing. Python 3.8 snippet looks almost similar to Rust's. OK I will update it. 
Thats a pretty smart idea actually. SmallVec even has a from_buff and from_vec methods that can be used to construct it from either a [T; _] or Vec&lt;T&gt;.
To be clear, are you one of the people talking in the linked bug report? If not, why do you think it's related? Do you have a similar backtrace? Memory usage of 20gb seems absolutely bizarre.
`cargo check` should be fairly minimal. Regardless - for each version of the software, `cargo build --release` only has to run once, whereas the compiled code has to run many millions of times. So it makes perfect sense that (in release-mode) runtime should be optimized-for at the expense of compile time.
I just re-wrote an NHL playoffs odds calculator from Python to Rust to make it run faster. The Rust program is much cleaner than the Python program. Serde is an absolute joy to work with, nothing compares to its speed and ease of use. And the playoffs odds calculator runs 2-3 orders of magnitudes faster.
At least in Red's case, the cryptocurrency focus has put that project on solid financial footing. Trading some mindshare for that result strikes me as entirely the right choice, especially given the project's current obscurity. Totally agree with your first paragraph, though.
I suspect that you missed that the second line is about golang?
&gt; This one is much simpler and all I'm doing is calling brew list, collecting the package names that are output, throwing them over the mpsc channel and printing them at the end. The rust program is generally about 500ms slower (sometimes up to 1 second slower) than just running brew list. I'd be *very* surprised if these two threads were causing a 1 second slowdown. I don't have a Mac, but when I replace `brew list` with `find /` and run your code, I don't notice any slowdown at all. Spawning just a couple threads is very little overhead (comparable to spawning `brew` itself), and having those threads read two different pipes shouldn't cause any problems. My guess is that `brew list` does something different when it's talking to a terminal vs when it's talking to your pipes, and that somehow that's making it slower. Do you notice any difference between `time brew list` and something like `time (brew list 2&gt;&amp;1) | cat`? For your first example, I do think Rayon is a good call. (Though currently it looks like your data set is very small, so you'll need to measure with a giant data set to know for sure.) It's going to do two big things for you: 1) It'll manage a global thread pool, so that you're not repeatedly paying the cost of spawning threads, and 2) it'll do smart things to make sure all of its threads have work to do.
Absolutely! Once you get past that learning stage, Rust is such a pleasant language to work with. 
By the title, it sounded like you moght have thought Richard Stallman (RMS) was using Rust and Apache Arrow, which I found really unlikely. But actually you are referring to a company called RMS.
They're going to teach you "just" SQL and relational data modeling. Questions about memory vs disk utilization could be answered on mailing lists where core developers participate. 
Could you provide a link to your slides? I think that would be helpful.
I am convinced and enjoy learning new approaches and will continue to experiment with one-off situations and read articles such as yours. The problem I have encountered is the department is hesitant to embrace new tools and will resist devoting any resources to rust. It seems like they arbitrarily drew a line on what is accepted and what is not with no room for discussions even though rust has clear benefits.
cc: jebrosen443 Thank you. I am currently at this stage. I have condensed the structs Post and Comment into one struct Post that has taken an additional field of Comment that is body: Option&lt;String&gt; and made all other fields optional too, I don't really know what reponse I do get from reddit. I also wrote a wrapper Posts for Post. It has one field data that is a Vec&lt;Post&gt; #[derive(Clone, Debug, Serialize)] pub struct Post { pub author: Option&lt;String&gt;, pub created_utc: Option&lt;u64&gt;, pub id: Option&lt;String&gt;, pub selftext: Option&lt;String&gt;, // if of type Post pub body: Option&lt;String&gt;, // if of type comment pub title: Option&lt;String&gt;, pub comments: Option&lt;Posts&gt;, } #[derive(Clone, Debug, Serialize)] pub struct Posts { pub data: Vec&lt;Post&gt; } Now, it should be possible to have an indefinite number of reccurrent posts, I hope. I have also implemented a Responder for the Wrapper Posts impl&lt;'r&gt; Responder&lt;'r&gt; for Posts { fn respond_to(self, _: &amp;Request) -&gt; response::Result&lt;'r&gt; { Response::build() .sized_body(Cursor::new(format!("{:?}", self.data))) .header(ContentType::new("application", "x-Data")) .ok() } } Thank you very much. I guess it works so far. It's not yet showing up in my handlebars template, but it compiles and dbg! correctly.
I thought the same thing.
wouldn't Richard Stallman abbreviate to *RSM*?
You cannot change it in Rust either. Your parent is talking about a code organization strategy that works in both languages, and has nothing to do with changing the way the language works.
RMS stands for Richard Matthew Stallman.
I completely empathize with your words. I and almost everyone here would have faced the same situation. Industry hype is such a funny thing. Without any apparent benefits, sometimes somethings get hyped up. Like Big Data fad(not that it has its benefits, but mostly it's a hype). Sometimes good tools get brushed aside without any reason. But I think Rust has enough traction nowadays to get adopted little bit in Industry. What generally worked for me is that you have to find a performance critical component in your application and if it is modular, then try to rewrite in Rust and show the performance improvements to your Boss. You need to do the extra work at the beginning. Once they see the benefit, they are likely to listen to you. Besides this, you should find a way to introduce Rust to your colleagues in appropriate manner. Don't push it down their throat. Try to help them in their learning stage and show them how to transition to Idiomatic Rust. 
This depends entirely on the API used for moving audio data throughout the system. Commercial operating systems intentionally make it difficult to intercept an audio output from other programs. They're worried that you'll rerecord DRM content. Pulseaudio isn't too bad, but there are existing programs for redirecting audio if you have a free OS.
I had thought about this, but I was not sure if an owned `CryptoRng` would be the correct solution. Lets say a user has a few hundred ongoing conversations and the application uses `OsRng`, wouldn't that mean that they are then forced to have several hundred instances of `OsRng` open, whereas having a single shared reference to one `OsRng` would be preferable?
Just a small amendment: Scala is co-owned by Lightbend (my employer) and EPFL: https://www.scala-lang.org/license/ My comment in your thread in /r/scala might also be of interest :) https://www.reddit.com/r/scala/comments/azcs5x/congratulations_to_dwijnand_of_lightbend_for/ei7kz0b/
&gt; not `unsafe` Rust, at least with the standard library, operates under the assumption that every program and library may be multithreaded. A data race at a global variable causes undefined behavior and anything happens. `static mut` is indeed unsafe.
You have developed an intuition for Pandas and you can do the same for SQL and the db engine! For some time, Pandas memory usage was mysterious. pd.DataFrame.memory\_usage() provides an estimate SetWithCopyWarning had me confused how many dataframes were actually in the namespace. Garbage collection with Jupyter notebooks seemed non-existent. Some operations required 3-5x free memory for each gb of dataframe in memory other operations did not. My OpenVZ VPS would become unusable until rebooted when pandas ran at the memory limit. Even without understanding the inner workings of the database, I knew the state of my data when the import erred or when the power was cut! 
Is there any prior art for a templating sytem that meets these requirements? (ie. an existing implementation for another language) Either I'm *horrendously* misunderstanding this or it's so ridiculously specific to a single purpose that nobody but you is likely to ever implement it.
&gt; I don't want to allocate if I don't need to. Why? Is this in a critical loop that wastes a bunch of time?
Good point. Here they are: [https://www.slideshare.net/AndyGrove1/rust-apache-arrow-rms](https://www.slideshare.net/AndyGrove1/rust-apache-arrow-rms)
Was it this one? https://leotindall.com/tutorial/a-gentle-introduction-to-practical-types/
Additionally, since Rust doesn't have a stable ABI (or support dynamic linking) AFAIK you only need libc (probably not even that on Mac/Linux)
r/playrust
From the title, I thought this sounded like a third party reporting on Rust's plans to drop the "await!" feature, and overall remove futures from the language. (But on looking closer, it's about setting up an asynchronous "await!" and being surprised that it never returns anything or resolves its "waiting" status.) (I don't know much Rust or asynchronous programming in general, so, sorry if any of this is wrong. Just thought I'd comment this in case anyone else was concerned or confused by the title.)
You learn something new every day.
Does anybody have any good up-to-date examples of using WebRender? (I've been trying to embed Servo, but it fails to build, requiring autoconf 2.13)
When I've introduced people to rust and used enums that was a big sticking point - avoiding it seems very reasonable. Devs can have a really hard time understanding an enum - a lot of people assume it's some kind of object construction thing due to syntax.
Diesel supports Postgres, MySQL, and SQLite databases. It does this to varying degrees, with most features supported in Postgres, fewer in MySQL, and the least in SQLite (as far as I remember). I've used Diesel with both Postgres and MySQL (MariaDb), and the only differences that comes to mind wrt to commonly used functionality is that Postgres bindings support `upsert` and `DISTINCT ON` clauses, while the other two supported dbs don't (due to the underlying db not supporting it). If you write your application with MySQL in mind, and then decide to transition to Postgres, all you should have to do is replace your `MysqlConnection`s with `PgConnection`s, and your ORM code should "just work". This is assuming you use only the compatible subset of supported functionality shared between the DBs, which isn't that hard to do given that the Postgres support is nearly a superset of MySQL support.
I'm trying something that I thought would be super simple but it is giving me a lot of problems. Not sure what I'm not understanding. All I want to do is read from stdin and then use what is entered immediately after someone hits Enter. Something even as simple as I enter "Hello" and there is a println of "Echo Hello" which shows in stdout. How can I do this?
Or Root Mean Stallman, if you prefer.
Is there a way to replicate the following at compile time? &amp;#x200B; `macro_rules! str_to_u8 {` `($ins:tt) =&gt; {{` `let mut num = 0u64;` `$ins.iter().enumerate().for_each(|(shift, &amp;ch)| {` `dbg!(&amp;ch);` `num ^= ((ch as u64) &lt;&lt; shift * 8);` `});` `num` `}};` `}` `fn main() {` `let a = str_to_u8!(b"abcde");` `assert_eq!(a, 435475931745);` `}` &amp;#x200B; I can do this in c++ using templates, and I wanted to achieve the same zero cost in rust. I'm using nightly so if that helps?
Ok, ya. That makes sense. I was thinking that since the sender was dropped, the reciever would just become stuck there.
While this could work, something I'm struggling with is seeing when to stop reading user input for a keybinding. Like, If I want to, I should be able to have a keybinding with five keys. However, that doesn't mean I want to allow the user to press five keys if there is nothing in the trie with the prefix. I'm also not entirely sure why use the intermediate enum value when I could make a HashMap to a function (I'm new to rust, so I genuinely don't know if this is the case). 
Here's a straightforward way to do prepared statements with actix-web and tokio_postgres pub struct PgConnection { client: Option&lt;tokio_postgres::Client&gt;, create_st: Option&lt;tokio_postgres::Statement&gt;, read_st: Option&lt;tokio_postgres::Statement&gt;, update_st: Option&lt;tokio_postgres::Statement&gt;, delete_st: Option&lt;tokio_postgres::Statement&gt;, list_st: Option&lt;tokio_postgres::Statement&gt;, create_table_st: Option&lt;tokio_postgres::Statement&gt;, } impl actix::Actor for PgConnection { type Context = actix::Context&lt;Self&gt;; } impl PgConnection { pub fn connect(db_url: &amp;str) -&gt; actix::Addr&lt;PgConnection&gt; { let hs = tokio_postgres::connect(db_url.parse().unwrap(), tokio_postgres::TlsMode::None); PgConnection::create(move |ctx| { let act = PgConnection { client: None, create_st: None, read_st: None, update_st: None, delete_st: None, list_st: None, create_table_st: None, }; hs.map_err(|_| panic!("cannot connect to postgresql")) .into_actor(&amp;act) .and_then(|(mut cl, conn), act, ctx| { ctx.wait( cl.prepare("CREATE TABLE IF NOT EXISTS users (id SERIAL PRIMARY KEY NOT NULL, email VARCHAR(100) NOT NULL);") .map_err(|_| ()) .into_actor(act) .and_then(|st, act, _| { // ctxx.wait( // cl.execute(&amp;st, &amp;[]) // .poll().unwrap() also works, i don't like it though for some reason // .map_err(|_| ()) // .into_actor(actt) // .and_then(|_,_,_| {fut::ok(())}) // ); act.create_table_st = Some(st); fut::ok(()) }), ); ctx.wait( cl.prepare("INSERT INTO users (email) VALUES ($1) RETURNING *;") .map_err(|_| ()) .into_actor(act) .and_then(|statement, act, _ctxx| { act.create_st = Some(statement); fut::ok(()) }), ); ctx.wait( cl.prepare("SELECT * FROM users WHERE id=$1;") .map_err(|_| ()) .into_actor(act) .and_then(|statement, act, _ctx| { act.read_st = Some(statement); fut::ok(()) }), ); ctx.wait( cl.prepare("UPDATE users SET email = $2 WHERE id=$1 RETURNING *;") .map_err(|_| ()) .into_actor(act) .and_then(|statement, act, _ctx| { act.update_st = Some(statement); fut::ok(()) }), ); ctx.wait( cl.prepare("DELETE FROM users WHERE id = $1;") .map_err(|_| ()) .into_actor(act) .and_then(|statement, act, _ctx| { act.delete_st = Some(statement); fut::ok(()) }), ); ctx.wait( cl.prepare("SELECT * from users;") .map_err(|_| ()) .into_actor(act) .and_then(|statement, act, _ctx| { act.list_st = Some(statement); fut::ok(()) }), ); act.client = Some(cl); actix::Arbiter::spawn(conn.map_err(|e| panic!("{}", e))); fut::ok(()) }).wait(ctx); act }) } } I have no idea what UDT parameters in a RAC cluster is though.
If you use python type annotations then `Optional[List]` in python is pretty much `Option&lt;Vec&gt;` in rust, etc.
Well after reading it again, it actually sounds really broad. What I actually mean is that I would like to get the audio stream just as my speaker does, then forward it through network (do not know if this is possible yet) and play it on a raspberry pi. By source, I meant the computer that is creating this audio stream that is supposed to go to speakers.
Lol :) Btw, I have nothing but huuuge respect for you, both your work and as a person that I have interacted with in chat. I hope you didn't mind my bringing this up or the comment about (sbt, and sbt alone- not you) that I have since deleted. 
Databases are great for certain tasks: ones that are read-mostly, have huge amounts of data, have a well defined and structured schema, and need to interoperate with different programs. They are great for *processing* data. Many data science tasks consist of *exploring* data, and for those something lighter weight, more loosely coupled and more interactive is very nice. That said, I kinda hate pandas. :-p It gets too much R in my Python!
Is it really though? I only ask for a renamed link and a rename.
Thanks for the answer! I am kinda stranger to this sort of software development. I guess I'll keep looking.
You can always make a cleanup function in the member and call it in the defer. But the problem of defer is that you need to call it for everything, instead of it implicitly calling at the end of the scope. Manually handling resources is a thing from the past...
I am using diesel with Mysql and it works fine, but if you can switch to Postgres. Nothing to do with Diesel, but its just a saner database. Having said that, I know that Diesel authors work mostly with Postgres. &gt; all you should have to do is replace your MysqlConnections with PgConnections Switching databases is never that simple. If you don't do it now, a year from now you will be so dependant on all sorts of accidental behaviors that it won't be possible. The fact that diesel query compiles does not mean it will return the same results.
Thank you, no I don't mind, I just wanted to get ahead of the curve before someone creative makes up a story. :)
This rough snippet should work: let line = String::new(); io::stdin().read_line(&amp;mut line); println!("Hello {}", line);
I'm sorry but yes sometimes people do stuff I guess. 
Is the Rust playground broken? Whenever I try to compile, I get: Compiling [crate1] Compiling [crate2] [...] /root/entrypoint.sh: line 8: 7 Killed timeout --signal=KILL ${timeout} "$@"
I have heard about 32 columns and build times increasing but don't remember the exact issue. 32 columns are not too rare I have seen quite a few production tables with 32 columns
That's what I tried too, but it doesn't work for me. I run the binary, my terminal listens for input, I press Enter after I type stuff, but nothing else is displayed. I think what's happening is `stdin` blocks the thread and since it continues to listen it doesn't even get to execute the println?
Fair point. You can use `ThreadRng` to share `R` per thread; but then you can't move a DR between threads. Short of redesigning `thread_local!` such that `ThreadRng` could avoid its indirection (performance related IIUC) that forbids it from `Send+Sync`, which I'm honestly considering, it would appear that using a contextually decided `&amp;mut R` is probably best.
No, it works fine for me. What platform are you using? (I am using linux) Also it really should work
This works. It requires nightly (for const fns) and the string must be padded with null bytes to 8 bytes (which I could fix if control flow for const fns was merged): const fn str_to_u8(x: &amp;[u8]) -&gt; u64 { (x[0] as u64) | ((x[1] as u64) &lt;&lt; 8) | ((x[2] as u64) &lt;&lt; 16) | ((x[3] as u64) &lt;&lt; 24) | ((x[4] as u64) &lt;&lt; 32) | ((x[5] as u64) &lt;&lt; 40) | ((x[6] as u64) &lt;&lt; 48) | ((x[7] as u64) &lt;&lt; 56) }
Yes, of course, which is why I qualified my statement with an assumption about keeping to a mutually supported subset of the Diesel DSL, and the scare quotes around 'just work'. The Diesel DSL/ Rust's typesystem should do a lot to catch SQL-extension-specific discrepancies, and while Diesel makes no guarantee that the SQL generated for each supported DB is the same, it certainly is a goal of their's t make it functionally similar. To be less ambiguous, a migration between databases should at the very least be supported by a test suite to catch regressions between both implementations. A likely scenario for a small application could involve no additional code changes beyond changing the connection type, although the possibility of additional changes inherent to database migration (eg. updating many the up.sql files) is still real. 
Super-fancy! Thanks for sharing this. Seems like some of the pins would be already dedicated to JTAG at powerup: it's frustrating that I can't find a real datasheet for the BCM2835 anywhere. Apparently Broadcom ain't tellin' publicly?
The gpio matrix is in a PDF I found online and uploaded it to the doc folder of the repo, if you mean that? I learned today that you can also have a config.txt so that the videocore sets the pins for you. In case of the tutorials, we can conveniently use the chain-loading that has been established in earlier tuts though, and only set the pins when they are about to be used.
&gt; The Diesel DSL/ Rust's typesystem should do a lot to catch SQL-extension-specific discrepancies It does to a degree, but it can't catch everything. A lot of differences comes from the same syntax having different semantics, especially when it comes to corner cases. I know that I had seen (and wrote) plenty of code that relies on specifics of some RBMS that don't exist in any explicit form, either because I didn't consider database change to be a possibility, or because this assumption came from another part of code and I didn't even know that the reason why some module states something in its documentation comes from assumptions about database.
Can anyone advise on working around the \`async Trait fn\` limitations? See here for current attempt and error: [https://gist.github.com/christopherswenson/44e790d86aed4116579cc7b70cd90f4e](https://gist.github.com/christopherswenson/44e790d86aed4116579cc7b70cd90f4e)
Splitting your server across multiple crates is a common pattern. This has the benefits of improved compile time, and explicit separation of concerns. There are two axises that you can do this: Split along routing/serving, business logic, and database interactions, or split along functional units of code (users, other collections of related tables) and vertically integrate the routing, business, and database layers into each crate. I have used the former crate-splitting strategy, while you seem to want the latter. I have found that using the Diesel ORM encourages the former split strategy at the database interaction level, which encourages it at higher levels as well. The split-along-functional-units strategy has the benefits of making splitting your project into microservices easier (should you ever need that), at the cost of requiring many more small libraries to be split and used by each functional unit (authentication, error handing, shared server state). Cargo-add has a solution to your project-wide dependencies upgrade problems: https://github.com/killercup/cargo-edit#cargo-upgrade 
Somewhat ironically, hiding information by using `impl Iterator` here would actually help the compiler to figure out the completions :D
Hm, that's kind of a surprising feature for Python to add. Maybe it's a good idea (I'd need to see more of it to have a strong opinion), but it seems like the kind of thing Python has quite intentionally avoided. The PEP, for reference: https://www.python.org/dev/peps/pep-0572
Huh. To me enums were a very simple concept. Maybe because of the C background, but I instantly realized they were basically a cross between a C enum and a C union.
&gt; if there are few elements in it Thanks, but a small question. Would you happen to know on how few are few?
Absolutely will do!
Thank you! I found this very useful to know and I'll make sure to experiment with both code splitting methods. 
Ah, interesting... I found out the terminal I was using it in was the cause. I tried it in another terminal and it worked. Thank you!
Shit yep I did lol
&gt; we can have (almost) any number of threads that run at the same time, and we don't pay for it This bothers me. Threads aren't free, and returns diminish _sharply_ once you have more threads than physical processors on the system.
Unless you wrap your statements in a transaction at the proper isolation level?
I do that, but not get the stack trace.
Slightly off topic: Did you use Diesel with MariaDB on a Linux system and, if so, which distribution? I am having trouble getting the diesel command line tool to build on a CentOS 7 system with MariaDB. I get the dreaded " cannot find -lmysqlclient " error, and despite quite a bit of poking around and seeking assistance, I have not yet been able to find a solution (I discuss the issue in [this thread](https://discourse.diesel.rs/t/cant-install-diesel-cli-for-mysql-on-centos-7-with-mariadb/69) in the Diesel forum).
If you are still just getting started, I still might suggest you keep everything within one monolithic crate. There is a lot of additional complexity that you take on when you split your code across multiple crates. I wouldn't say this kind of separation is needed unless you have multiple people working on a project, or you have &gt; 10 tables to work with. As long as you have the discipline to keep these concerns separate, I would say that the best initial strategy would be to keep what I just described as crate-level architecture as module level architecture within one project (using directories and use of `mod`).
Aside from project templating systems like [Yeoman](https://yeoman.io/) which can do anything (including interactively ask you questions) because the core of the template is a script written in JavaScript (overengineered in my opinion), I've never seen a project templating system which supported generating links. (And why would they? The whole point is for the generated project to be self-contained, which means it should copy resources in the template, not link to them.) Generally, they follow a pretty simple pattern which applies the same concepts you'd find within a single-file HTML template, but to a tree of folders: 1. Ignore any files which are marked as part of the template itself such as a JSON, YAML, or TOML file containing the template metadata. (Equivalent to `{# comment #}` in Django/Jinja/Twig/Liquid-style template syntax) 2. Copy every other file in the template into the new project folder. (Equivalent to most of the stuff which an HTML template passes through literally) 3. If the copied files contain template placeholders like `{{project_name}}`, replace them as appropriate. (`{{variableName}}` being the syntax for variable substitution in Django/Jinja/Twig/Liquid syntax.)
&gt; After a few weeks fighting through the book after work, you give up and wait until someone else creates an easy-to-follow tutorial. wow @ me next time
If you have a real-world use case (and the algorithms are good), the \*ring\* maintainer may be open to adding them (and if you work on that, you'll learn a good deal in the process). :)
I'd generally recommend https://github.com/dgrunwald/rust-cpython over pyo3 for now. While pyo3 is more ergonomic, it is still in alpha and requires nightly Rust to boot.
I did use Diesel+MariaDB with NixOS, a somewhat obscure Linux variant. I vaguely recall having trouble getting pkgconfig to detect where the libmysql lib was. Although this may be highly OS specific (and NixOS is weird), I recall having to explicitly set the PKG_CONFIG_PATH to include some relevant file in the distributed mysql library (something else in the OS config replaces mysql with mariadb). It looked something like: ``` PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/nix/store/1sw2a14xp6an3hd7358z23r04f2zlrl8-mysql-5.7.23/lib/pkgconfig ``` I have no idea if CentOS delivers the maria lib in a similar directory structure, but I hope this helps.
Typescript feels like a patch over a crappy language. If you can stay within it's confines, it's ok. But if you need to break out into JavaScript, it's a mess.
&gt;Seems like some of the pins would be already dedicated to JTAG at powerup: it’s frustrating that I can’t find a real datasheet for the BCM2835 anywhere. Apparently Broadcom ain’t tellin’ publicly? Nope. To get access to their data sheets, the minimum order is in the thousands. 
What's the best citation you have for "multiple memory safety issues"? Last I heard was during the Actix hullabaloo when I was saying that seeing the same main dev had me worried about PyO3's trustworthiness and all I was getting back was "Don't tar PyO3 with the same brush without evidence". (The "requires nightly Rust to boot" is and always has been a deal-breaker, but I'm curious.)
It's configurable based on the array you decide to have back the smallvec, e.g. a `SmallVec&lt;[T; 8]&gt;` holds up to 8 elements not on the heaps.
Great, good luck for your endeavours.
Thanks for the effort on your reply, but its just the proof that a long path ahead is still required until it reaches what Java and .NET drivers deliver today.
I haven't used Symfony but I think Django's "apps" are the same "split along functional units of code" sort of thing. The lack of an ecosystem of ready-made crates divided along that axis is actually one of the things which keeps me on Django (the other being autogeneration of draft SQL schema migrations). I just got fed up with having to rewrite the same boilerplate for things like search query filtering (django-filter) or rendering a result set into an HTML table where I only have to define how to render a cell of each column type once (django-tables2) or defining "a list of TODO annotations with a parent-child relationship to a GenericForeignKey" once and then being able to plop `{% todo_button orm_object %}` into any template in any project and mount the ready-made routeset to a namespace under my top-level router.
It was... controversial, to say the least. Indeed, the controversy over the feature was pretty much the straw that broke the camel's back and led Guido to abdicate his BDFL status in 2018.
How does it work with submodules and worktrees? (Does it run the command in submodules (adopted or not) and worktrees, or ignores them)?
At the moment pyo3 [violates Rust's aliasing rules](https://github.com/PyO3/pyo3/issues/342). Even if your code doesn't need rely on aliasing rules being upheld, compiler optimizations do, so this is undefined behavior anyway. This is an issue that's rather fundamental to the design and cannot be fixed without a performance hit or a major API break, it seems. [Here's another design issue](https://github.com/PyO3/pyo3/issues/320) leading to memory unsafety. And merely a segfault is actually the best outcome for this kind of issues. Many of these are security vulnerabilities too. So while I'm thrilled to see that pyo3 is actually developed again, I'd still strongly advise against using it anywhere near production.
Aren't const fns stable as of 1.33? (still no control flow)
Agreed. Thanks for those.
&gt; returns diminish sharply once you have more threads than physical processors on the system. *more active threads.* Looking at task manager, I have 3000 threads and 200 processes. You do only pay for threads while they're active, the wording could have certainly been better. Thread/Process scheduling is a bit more expensive than Futures, but if the thread isn't active, you aren't paying for it with performance. 
Nice, thanks. I'll keep that in mind. I did in fact use plain cpython when I was just compiling so files and putting in my projects, but when it's working pyo3 is pretty great and easy to configure, which at the moment it is at least for me. 
What is an example of a situation where you need to use JavaScript instead of TypeScript?
That's fair.
I agree with you, but I personally don't think that rust will be a good fit in that space anyway. I personally think that rust will be very advantageous in a microservices architecture.
I've played around with modules quite a bit. To a point to be as comfortable as I would with js es6 imports. With with added spin of pub / private. Although I'm aware that there is a lot more to stumble if I'll try to build something more complex. What brought me to workspaces initially was the fact that I need to declare modules in the programs entry point. I do prefer to keep my files rather small, for example, one file for one struct. Which ends up in very long list of mod declarations which seemed to me a bit messy. But yeah I hear you. Currently implementing a simple script to calculate commission with few services, few data entities file reading, validation etc. But I wanted to get the broader picture where I can take my learning application from there.
If you've hundreds of instances you'll probably want some form of memory-mapped database to collate the states and the random number generator would be detached.
Answerung to myself: it runs a command in every directory having subdirectory named '.git'. So no, it does not work with adopted submodules nor workdirs.
Haven't yet come to persistence layer. However I did check out the documentation. With my limited understanding I formed an impression that it's fairly low level in terms of abstraction. The way to go it would seem is repository pattern where you would use it as a service with functions which would take the struct as an argument (or return struct if you're fetching) and the function body would contain the custom query you wrote depending on your needs. I stand to be corrected on this one though. If I'm not mistaken Django uses Active Record pattern and I'm not sure Diesel's implementation would even allow that. Although it can be extended to implement some commonly used helper functions. Personally I would be more pleased to see something like Hibernate of Java being available in Rust's ecosystem, but I'm not sure that's feasible given the current common use cases of Rust.
strum looks quite nice, but when I tried to use it, I got a bunch of PEBKAC errors from the compiler related to not being able to find the module(s)... I used `loop` original because that's what the example uses. When I switched to `for`, the borrow checker tells me "No". It says the iterator `it` is moved into the for-loop and when I call `it.skip_current_dir()` in the loop, the attempted borrow fails.
Should be a straightforward version bump from 0.4.0, with the only source-incompatible change being the switch from Rust keyword escaping to raw identifiers. In terms of new features, the headliner is edition 2018 support.
[removed]
Personally I have a homemade submodule system where parts can register handlers and other similar stuff for events. I like to keep everything in one large crate (though still with separation) 
What's the error exactly? It sounds like you have a borrowed thing and Rust can't guarantee that the lifetime is enforced across threads. This could be as simple as needing `T: 'static` but I don't want to recommend that until you understand what it means and why. (There are libraries which provide that guarantee, but without knowing what you're using it's hard to guess.)
Which os and terminal emulator? That really sounds like something weird with terminal settings.
I'm not sure what you mean with that, since you can call the destructor with the `delete` operator, or with `obj-&gt;~NameOfType();`.
Although the name, \`LanguageClient-neovim\` do support vim as well.
I think Rustfmt already does some of that stuff.
Say I have a function that can throw io::Error of different kind: file not found, invalid data, expecting a file whilst it's a directory, etc. What's the best solution so I can display a clear error message to the user for each of those errors? The built-in description is very "developer-oriented", and I would like to customize these messages with more information, and with some context if possible. Like "Is a directory", it would be nice to have the directory in question in the error message... 
&gt;OMG!!! such an innocent looking feature has this big backstory. It's strange I don't follow the happenings in Python closely even though it's my primary language. 
You could try using `fd` instead of find, it may perform better
Did you review cpython for aliasing violation? As I remember it uses same approach as pyo3.
I don’t know what go-imports exactly does (or how imports work in go in general), but, if that’s what you mean, [intellij-rust](https://intellij-rust.github.io/) (Rust plugin for JetBrains IDEs) has some support for automatic `use` imports.
He's obviously talking about automatic storage duration (i.e. implicitly managed lifetimes).
The first 1 comes from entering the number. The second and third comes from `input_temp` which is the console input. // the first {} println!("{} converted to Fahrenheit: {}", input_temp, fahrenheit); However, pressing enter adds a newline character to the end of the input which is why the formatting is strange. To remove it, use the trim method in each print statement. println!("{} converted to Fahrenheit: {}", input_temp.trim(), fahrenheit The same was done in this line. let temp_c: f32 = match input_temp.trim().parse() {...}
On line 22 and 23 you are printing input\_temp, which is a string. That string contains your original input, including the newline. Change input\_temp to temp\_c.
The error is: error[E0412]: cannot find type `T` in this scope --&gt; src/ui.rs:12:62 | 12 | fn authorize&lt;T: Authorize + Send + Sync&gt;(authorizable: State&lt;T&gt;) -&gt; Redirect { | ^ not found in this scope Links to the module [ui.rs](https://github.com/aspera-non-spernit/profiler/blob/dev/src/ui.rs) and to the trait declarations in [lib.rs](https://github.com/aspera-non-spernit/profiler/blob/dev/src/lib.rs) and the impl of the traits for the module [reddit.rs](https://github.com/aspera-non-spernit/profiler/blob/dev/src/lib.rs)
Not sure about Rust, but in most other languages, destructing coroutines is handled using the same unwinding machinery that is used for exceptions/panics so that e.g. `finally` blocks get run.
Yes, `static mut` requires unsafe. `static` does not, which is what I was referring to.
Actually no, I didn't. It would be interesting to know if this applies to rust-cpython also.
Unless most of those are reads, and an `RwLock` is used, or unless handling a request mostly does work that doesn't involved reading from or mutating the global state. Or maybe this is a good place to use RCU, if request handling mostly deals with the global state, but doesn't require the _most_ recent global state. Maybe constraints were hinted at this in the OP; I tried to respond with a short list of things to try for managing global state in spite of not having much knowledge of grpc or rocket.
This sub is for the rust programming language. Please refer to r/playrust for the game.
Should have written it in rust
I expect devs who don’t want nightly and ub should review low level libraries. 
Did you try compiling it in `--release` /s
I don’t believe so 
Why would you do that?
It seems to me like channels (mpsc) and mutexes are super similar. When would I use one over the other?
Why would you not want to do that?
Because of tooling support, I guess
Suppose tooling support is currently not great. Why is it impossible that this could change? 
As far as being "possible" goes, it certainly could be. I don't know of any restrictions preventing web assembly from running in the JavaScript of an extension. Unless someone has already attempted this, you may be writing a lot of the bindings to the necessary extension APIs yourself. There are crates out there like wasm-bindgen that hopefully make that easier. 
I'm just interested why OP wants to do this _now_
Sure, but the point is that the way you change the order of destructors in Rust is by manually dropping them with `drop`, which indeed can be done in C++ as well.
Yes, it's possible. [macro\_railroad](https://github.com/lukaslueg/macro_railroad) has a WebExtension [here](https://github.com/lukaslueg/macro_railroad_ext). The extension is available - including the wasm-blob built from Rust - on the official Chrome- and Firefox-Addon-Page, so it's end-to-end-possible :-) &amp;#x200B; Just open an issue on GH if you have any questions. &amp;#x200B;
Out of curiosity - if you're going to make your \`authorize()\` method a generic one (with the \`T: Authorize\` constraint), how should Rocket know which one \`T\` to pick?
I don't use this IDE but the screen casts of the import features look like exactly what I mentioned. Thanks for sharing!
IIRC some of the options need to be enabled but it does support them.
&gt; I've written a bytecode version of lox in rust from crafting interpreters Is it on GitHub?
What's wrong with now?
Nothing. I mean, like "what caused OP to think about writing a WebExtension in Rust with exact tooling we have now"
&gt; It's interesting that you can't apply the similar pattern to the async code, because you shouldn't be blocking in drop. This is my first time thinking about this problem, and its interesting. I wonder if we should say it's bad to block in unwinding in general. Like, even in synchronous code, it would be weird if one thread's unwinding was blocked joining another thread that was waiting on random network I/O. (For example, if panics normally trigger some kind of restart, blocking a panic could delay restarting your main loop and DOS other clients of the service.) It seems like asynchronous code might actually be *better* at dealing with this, because IO can be cancelled by dropping a future.
When you use a channel, one thread can send notifications to another thread by sending data over the channel. With mutexes, both threads can mutate the inner data whenever it's available, but one can't notify the other that it made a change unless the other thread is repeatedly polling it. You would use channels when you need one thread to handle data sent *to* it, and you would use mutexes when you just need to *synchronize* data between two threads.
Not equivalent. ``` void f() { MyObject a; MyObject b; // What can I write here to make a get destructed before b? } ``` Answer for C++: nothing. Answer for Rust: `drop(a)`.
`io::Error::kind()` returns an [`ErrorKind`](https://doc.rust-lang.org/std/io/enum.ErrorKind.html), which is an enum that you can `match` on to get the specifics of the error.
i assume the type that implements Authorize In the main function I do: let reddit: Reddit = Reddit::with_ui(app_context); which runs rocket pub fn with_ui(app_context: AppContext) -&gt; Self { let r = Reddit { app_context }; crate::ui::run(r.clone()); r the ui now has got a reddit instance as managed state pub fn run(reddit: Reddit) { // should be T too ... } Now rocket has got reddit that implements Authoriz + Send + Sync whenever the route authorize is involed: fn authorize&lt;T: Authorize + Send + Sync&gt;(authorizable: State&lt;T&gt;) -&gt; Redirect {..} it knows State&lt;T&gt; is of type Reddit Don't know if it works that way, but that was my idea at leat. 
TOML is just a data format. It doesn't support any sort of computation, so you'll need to write a separate program to do that.
There's a method `.kind()` on `io::Error` that gives an enum `io::ErrorKind`, which you can match on. This will tell you things like `NotFound`, `PermissionDenied`, etc. It sounds like you want more information though, such as telling the user exactly what file wasn't found. I *think* `io::Error` doesn't store those kinds of details. What you can do instead is have your function return a `Result` of your own custom error type, and at each site returning an `io::Error`, map it to your custom type with more information provided, like a filename.
Hi, I'd like to express my gratitude for taking the time to do this video on this subject. Rust-dsl bindings have been changing quite rapidly and its really nice to have an up-to-date example of a truly simple and portable solution to start a game.
This looks like you're using Rocket. I wonder if Rocket does not work with generic functions as endpoints. I wouldn't be surprised, since you'd be generating N functions all annotated with the same endpoint, where N is the possible types that \`T\` could be.
This looks great! Question: what is your opinion on using PROST / protocol buffers on untrusted input? 
You can't `pub use` something to reexport in a function-specific import, because functions can't have associated items. Other than that, there's no difference in behavior. The performance won't be different either, because Rust's imports are a purely compile-time notion that just make names available to the compiler.
&gt; but one can't notify the other that it made a change unless the other thread is repeatedly polling it. Or you use a [Condition Variable](https://doc.rust-lang.org/nightly/std/sync/struct.Condvar.html).
Thanks for this - I just audited my crate's dependencies &amp; then ported it over to `no_std` with the help of `cargo nono` - it's a really helpful tool!
Should be fine. \`prost\` doesn't contain any unsafe code on the decoding side, and there are fuzz tests in place which gives additional confidence.
that means I cannot use generics with rocket?
Scheme hygienic macros were likely the inspiration for `macro_rules!`. Proc macros look a bit like Lisp `nlambda`. TeX has some similar-looking stuff. There's always `m4`. Beyond that I've got nothing.
No worries! I’ll try to continue adding more stuff soon :)
Awesome good to know, I’m using it as part of a cryptographic protocol which does homomorphic re-encryption. Homomorphic computation can get *really* expensive for large inputs, so having a binary format that is both well-known and very compact is super great. Thanks for your hard work! 
Rust macros were heavily inspired by Scheme macros (some early parts of the macro system were written by people who worked on the Racket macro system). There's prior art in the Scheme and Racket literature for basically everything Rust does currently. 
How does this compare to https://myrepos.branchable.com/?
I don't really consider unions in C to be a very well known construct tbh
They should send someone to the lang team, too; Rust definitely needs to learn from the FP half of Scala.
&gt; However there were some discussions on discourse that this kind of Waker implementation comes with the drawback that it can leak the memory for the Futures/Tasks for a certain amount of time (since the Futures are still stored as Wakers inside objects that might not touch them anymore after the tasks got cancelled). So it might actually not the preferred way to implement Wakers going forward. Task Handles/ID that are stored inside Wakers that avoid that problem. Got a link to that discussion? I'm not sure I follow. My `Waker` implementation effectively uses the `ArcWake` approach, but doesn't hold onto any Future/task resources that could be leaked while something still has a handle to the `Waker`. It takes the "Task Handles/ID" approach that you refer to and only holds a reference to the executor, which is expected to be long-lived.
What if there are two types that implement all those traits? If there's ever only one, why make the function generic?
Love the name 🍺🍻
[mp4 link](https://external-preview.redd.it/mp4/ayaYZ6rBS4kcmf04vV0QV7NKx5Qwv2Kto-2iUsY6JCw-source.mp4?s=9d7167b3c8334c72e2c8b4e4d406385bd0afb574) --- This mp4 version is 9.77% smaller than the gif (2.33 MB vs 2.58 MB). --- *Beep, I'm a bot.* [FAQ](https://np.reddit.com/r/anti_gif_bot/wiki/index) | [author](https://np.reddit.com/message/compose?to=MrWasdennnoch) | [source](https://github.com/wasdennnoch/reddit-anti-gif-bot) | v1.1.2
I'm making some game-like application for debug and visualization of organism behaviour using my new crate. I have to remove these walls, it looks and feel bad and spores have no space to move around :&lt; [https://github.com/PsichiX/psyche](https://github.com/PsichiX/psyche)
You mean like [cargo-edit](https://github.com/killercup/cargo-edit)?
https://learning-rust.github.io might be helpful to check basics quickly
I was recently reveling in how potentially useful an `AsyncCell` might be. It's like `RefCell`, but borrowing returns an `impl Future&lt;Ref&lt;T&gt;&gt;`. The future would be pending until the borrow becomes valid, basically forcing the borrower to yield until the borrow becomes valid. An equally good name for it would be `GreenMutex`. In the limit, beyond this and `AsyncDrop`, you can see async as some penicillin for "organizing computation": whenever you need data/some resource/a thread to join/etc and it's not ready yet, just yield. It's a great solution, but it's not totally free. As with ordinary mutexes, a `GreenMutex` can lead to deadlock, for example, and that's a tough bug to test for and/or pin down. I suspect `AsyncDrop` may run into similar problems, with the added complication that now you have to be on the look out for implicitly asynchronously dropping things. Basically what I'm trying to say is that, while async is awesome, we should be careful how far we take it. It involves a high degree of concurrency, and while we claim to be fearless about that, I suspect that we'll find deadlocks and other scheduling mishaps become serious problems if we take it too far. That said, maybe I'm wrong. Maybe it's not as bad as I suspect. Or maybe it doesn't matter- maybe it's actually worth risking shooting ourselves in the foot now, to learn what a more fundamentally async world would look like. What could be the next target after memory safety for the compiler to help with?
Cool. I played around with changing the `loop` to `for`. While it is valid to pass a mutable reference to an iterator in there, which avoids consuming it (https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=b26e911c9ed8032f438dd24202c65e96), the problem is that you can't call anything like skip_current_dir on it because that requires a second mutable borrow.
I have been writing in go for a few years and finally had a need for minimal latency. I took a few weeks to break my bad habits. Had a hard time with borrowing, and returning references to indexmap elements from functions within threads. The whole time I was fighting the system trying to just make it work. Once it “clicked” and I moved into the semi ok phase, going back to go gave me anxiety. “Am I locking this everywhere?” Instead of fighting the system, I lean on it now.
You’re looking for r/PlayRust.
🏎🏎🏎
`rustfmt` can reorder your imports and the technology is certainly there to delete them (the compiler emits very accurate warnings about unused imports) but I believe actually removing them is yet-unimplemented, and might(?) be a welcome feature in `rustfmt`. If not, `cargo` is available as a crate, so you may be able to skip `syn`. In Rust I fear there are too many ambiguities to make adding imports do the right thing often enough. I don't want an automated tool to _ever_ take my code from "doesn't compile" to "compiles but does something I didn't intend my code to do".
The borrowed variant in a `Cow` only applies to references, of which `[{integer}; 5]` is not a reference.
It's a known issue: https://github.com/integer32llc/rust-playground/issues/469 It will likely take a few days for the fix to propagate.
okay so why is protobuf or thrift or avro or capn proto or rpc or any of them better than any other?
Given that TypeScript is a superset of JavaScript (or close enough as to not matter), I don't see how there can be. But perhaps the parent is talking about situations where you have to work without types (perhaps due to lack of typings for a library), or using very dynamic typing patterns? Personally I feel like TypeScript actually feels better than most languages. It's one of the few languages where mixing dynamic and statically typed code is actually idiomatic. And the rate of improvement is very impressive.
Exactly this. Also applies to direct members.
That's not RAII, that's manual resource management.
Unless you really know what you're doing, you shouldn't manually impl Send or Sync. The compiler should do that properly where appropriate. Instead, you should look at why your struct isn't Send/Sync, which of the types it contains isn't Send/Sync. It might be as simple as using Rc (which isn't thread safe) where you should use Atc (which is an atomic thread safe version of Rc).
For rand crate, I think it is in active development and interface is nowhere near maturity to be in std and stabilized. regex crate probably could be in std post-1.0 (it's already under rust-lang organization) but the same comment applies historically.
Thrift is a rpc framework. The rust implementation is pretty lame. Capn is has similar use cases then proto but the amount of unsafe code in the rust lib should make anyone think twice. Avro is for data storage not rpc. Too complex for rpc. Proto is well supported across languages and has multiple high quality implementations in rust. It is the correct choice for rpc serialization.
yeah, I was kinda upset when I read in the Rust book that grapheme clusters aren't in the std. But crates are kinda OK, I guess
Since the barrier to integrate crates into a project is so low, they're probably confident that they don’t have to ship the kitchen sink in the standard library. I personally need a different random number generator, because it needs to have certain properties. Just offering one in the standard library would be insufficient anyways.
Awesome work! I'm particularly excited to see extern types and 2018 Edition support finally land.
Just wanted to point out that in Romanian prost means stupid :)
Go is a pleasure to work with too. But I rewrote one of my go program to Rust to realize I had a big data race bug in it😅. Obviously Rust compiler caught it. After that I changed it in Go version. Perks of knowing Rust. Luckily, previous data race bug never caused a problem in few runs. It very well could have crashed in future. 
If I make a local in Rust, I can explicitly call drop in it to force early drop, is the point made below.
I don't think is possible. But it did lead me to this fun program that print's itself: fn main () { println!("{}", include_str!(concat!(env!("PWD"), "/", file!()))); }
[removed]
Thank You
This `fancier stuff` that you linked is unfortunately not working on latest nightly (2019-03-10). Anyway, [haxe](https://haxe.org/manual/macro.html) has macros that transform AST -&gt; AST. I think there are some similarities.
Thanks, I'll dig into this feature after that will update my blog according to it.
That's not quite the same. On untrusted inputs one would also need to take care of decompression bombs and other resource limits.
It’s a hard trade off. If you don’t block, you leave threads behind. Restart is exactly the case where this can bite you, because you can end up with duplicated “singleton” threads. Another example would be tests where, if you don’t block in cleanup, you can get test interference. Finally, there’s a problem that if you don’t join the thread and the main thread has exited, the thread would be terminated abruptly, without calling destructors. I am a big proponent of structured concurrency idea: it seems to me that making sure that threads are *always* joined, crossbeam:scope style, leads to easier to understand software. The problem with this approach is that you need to make your threads cancelable. If all you do is channels and CPU, that’s not too hard. However, as soon as you start doing blocking I/O, you ar stuk because there’s no reasonable way to cancel a read. Python with Trio works much better in this case.
You can't as far as I'm aware. You have to implement a trait for both of them and then use it as a bound for the generics. Then you can access the methods on the trait. I can't give an example right now because I'm on mobile.
You need to have a trait that handles the behavior that is different between the two shapes. The way it is now, there's no way for the compiler to tell that `T` is a `Rectangle` in the first branch and a `Circle` in the second branch. For all it knows you could be passing a string or a number there instead, which don't have a radius or a base. Instead you need to tell it that `T` can only be something that implements a certain trait, and then you can use that trait's methods and associated constants. struct Rectangle { base : u32, height: u32, } struct Circle { center : (u32, u32), radius : u32, } trait Shape { fn description(&amp;self) -&gt; String; } impl Shape for Rectangle { fn description(&amp;self) -&gt; String{ format!("the base of the rectangle is {}", self.base) } } impl Shape for Circle { fn description(&amp;self) -&gt; String{ format!("the radius of {}", self.radius) } } fn describe_shape&lt;T: Shape&gt;(geometric_object: T){ println!("{}", geometric_object.description()); } fn main() { describe_shape(Rectangle{ base: 3, height: 4}); describe_shape(Circle{ center: (3, 3), radius: 7}); }
Instead of a ShapeType enum make a Shape trait that has get_base and get_radius methods and implement it into the shapes, then add trait bounds to the genetics. Or, just use and enum then pattern match the values out of it.
Once it's in the standard library, it must stay in there for good. The API can no longer change to avoid breaking compatibility, it must be maintained by the team, it takes up space in the documentation, takes up harddrive space, etc. By keeping everything in separate crates, the lang team can focus on just developing the compiler and users can pick the exact dependencies and versions that they need as independent crates. The Rand crate, for example, went through a lot of development and the API changed several times. It turns out that generating rand numbers safely and efficiently is not as easy as it seems on the surface. This development could not have happened if it wad tied into the standard library. Also, focus might shift over time. For example, some time ago everybody thought that XML was the best thing ever. Many standard libraries included a XML parser (of even several)! Nowadays, everybody is switching over to JSON and the old XML parsers are in there for good.
I think by that definition I'll be safe :P &amp;#x200B;
By definition of being generic you cannot do that - being generic means you make no assumptions at all. And asserting some type will have some fields make things difficult (field access is usually an offset from the start of object, not knowing the object size and arrangement makes this difficult in low-level languages) and unsafe (compiler cannot statically ensure the types do have said fields). You can assert a trait bound to say they all have common accessor functions etc, but that'll require you to implement the trait manually for each type since you have a different output depending on each type. If you want a single function to handle all cases, then simplest way seems to be just put everything in an enum type (which is what you're doing right now, but change geometric_object to be of type `ShapeType`).
Well that's still not as bad as me just flat-out calling my own library "dumb" right in the name. :P
Try - ``` struct Rectangle { base: u32, height: u32, } struct Circle { center: (u32, u32), radius: u32, } enum ShapeType { Rectangle, Circle, } trait GeometricObject { fn base(&amp;self) -&gt; u32; fn radius(&amp;self) -&gt; u32; } impl GeometricObject for Rectangle { fn base(&amp;self) -&gt; u32 { self.base } fn radius(&amp;self) -&gt; u32 { 0 } } impl GeometricObject for Circle { fn base(&amp;self) -&gt; u32 { 0 } fn radius(&amp;self) -&gt; u32 { self.radius } } fn describe_shape&lt;T&gt;(geometric_object: T, object_type: ShapeType) where T: GeometricObject, { match object_type { ShapeType::Rectangle =&gt; println! {"the base of the rectangle is {}", geometric_object.base()}, ShapeType::Circle =&gt; println! {"the radius of {}", geometric_object.radius()}, } } ``` Makes sense to return `0` as radius for rectangle and vice versa, you can also wrap it in `Some(t)` or `None` to have better error handling
I suggest taking a look at the rowan library for inspiration: https://github.com/rust-analyzer/rowan/. Here's a recent video explaining what it does: https://www.youtube.com/watch?v=DGAuLWdCCAI. It does use runtime-checks to implement well-typed API. In practice, this is not a cause of correctness problems (because the API is automatically generated from the "grammar"). It does leave some performance on the table, but, if you want to work with concrete syntax trees (and I think for org-mode you might want that), this seems somewhat inevitable: comments and whitespace can appear anywhere, so you can't trivially use structs with a fixed number of fields. The idea is to have a two-layered representation. On the lower-layer, you have a tree of nodes of type `Node`. On the higher layer, you have a set of typed-wrappers around Nodes: `NodeTypeA(Node)`, `NodeTypeB(Node)`. You can group wrappers in arbitrary sets of enums.
IMO @Jonhoo is right &gt;"we can't make goals with regards to the ecosystem, just our own projects"? maybe this is the reason that there exist different libraries for same use cases. &amp;#x200B;
Better is to add a trait, ie DescribeShape with a discribe function, and your shapes can implement that trait. 
\&gt; It is the correct choice for rpc serialization You very quickly write off the Capn Proto \*protocol\* based on the implementation of it in Rust. Then you make a sweeping argument that Protobufs are the better choice. I would recommend anyone making decision about these things for their projects to investigate the tradeoffs that different solutions make, instead of relying on this guy's advice. Should be noted that Capn Proto was written by the author of Protobufs, and the author states that it fixes many issues. Whether this is relevant to you, I leave up to you to discover.
Sleeping anywhere on a game thread is a bad idea. You aren't guaranteed that it'll resume when you expect it to. It can resume any time after the time you specify and different platforms have different levels of sleep time granularity. There are also some issues that can arise if you sleep the thread your message pump is on. Also, if you sleep for 16 ms, then your game is going https://docs.microsoft.com/en-us/windows/desktop/api/synchapi/nf-synchapi-sleep Also, if you sleep for your minimum frame time in milliseconds, it becomes impossible for your game to actually achieve your maximum frame rate. For example, if you're sleeping for 16 ms (60 FPS), then the only way for your game to actually achieve 60 FPS would be for your game logic to require absolutely zero processing time per-frame.
What's the project license?
&gt; You very quickly write off the Capn Proto protocol based on the implementation of it in Rust. Well yeah because people here will need the rust implementation. It is an undeniable fact that protobufs are better supported accross languages. With the main implementation supported by people at google working on it full time. I honestly find it odd how people get so butthurt when I claim protobufs are the correct choice over others. Rpc serialization needs to be two things above all else: safe and well supported across languages.
&gt; If I'm not mistaken Django uses Active Record pattern That's correct. One of the things which I've found is a huge help to maintainability in my projects is the ability to take the "fat model" approach where I hang various getters off the model which don't correspond to the database columns but, instead, to computed properties or metadata.
I've changed a few things since I last posted the [Rust cheat sheet](https://cheats.rs) (e.g., sorted by topics instead of 'sigils', closures, lifetimes, ...). I'd love to hear some feedback if it still makes sense, and improve what doesn't.
Just added an MIT license.
Looks good. Also add `license` field in `Cargo.toml` for a quick look when people searching your crate. [package] # This is an SPDX 2.1 license expression for this package. Currently # crates.io will validate the license provided against a whitelist of # known license and exception identifiers from the SPDX license list # 2.4. Parentheses are not currently supported. # # Multiple licenses can be separated with a `/`, although that usage # is deprecated. Instead, use a license expression with AND and OR # operators to get more explicit semantics. license = "MIT"
This. I could never really grasp functional programming, After learning idiomatic Rust and writing quite a lot of it, I suddenly realized I could now ~~read~~ decipher Haskell and OCaml!
pyckitup won't be uploaded to [crate.io](https://crate.io) because its dependency RustPython isn't.
&gt; The old `macro_rules!` system was ... Are macros-by-example getting deprecated?
I didn't create it. I saw the post and shared it on r/rust.
Rust follows the philosophy that standard library is for common abstractions used between crates like Iterators, io traits, strings etc.
I'd recommend putting the whole thing in an enum. Rather than have `ShapeType` and use a C-like enum, you could have enum Shape { Rectangle(Rectangle), Circle(Circle), } This is if you want to match on them at runtime. If you want to match at compile time like your example is trying to do, then you'd need a trait to do static dispatch. It would require writing not only a separate function per type, but also a separate `impl` block, however both could be reduced through the use of macros. trait Shape { fn describe(&amp;self); } macro_rules! describe { ($this:ident : $t:ty =&gt; $e:expr) =&gt; { impl Shape for $t { fn describe(&amp;self) { // Work around macro hygiene let $this = self; $e; } } } } describe!(r : Rectangle =&gt; println!("the base of the rectangle is {}", r.base)); describe!(c : Circle =&gt; println!("the radius of the circle is {}", c.base));
Check your dependencies. Is trust-dns in there? I know reqwest switch over to use it (except on Windows), hyper does not use it. We did have a bug in trust-dns-resolver leaking UDP connections 0.10.0, fixed in 0.10.1. FYI: trust-dns-resolver is fully in process, with zero dependencies on libc for resolutions, if glibc is your issue, it would be interesting to see if trust-dns could be used to alleviate that issue.
reqwest is now using trust-dns-resolver. If glibc is buggy in this context, maybe trust-dns would be an option? Disclosure: I’m the maintainer of the project.
Ah, that's some prematurely past-tense wording on my part, sorry! The new `macro foo` system is intended to supersede the old `macro_rules!` one. 'Macros-by-example' aren't getting deprecated: it's just `macro_rules!` in particular, and probably not any time soon.
The standard library is where code goes to die. If it's in an external library, it can be updated or pinned independently of the standard. It's also easier to swap out code that's not in the standard library. Find a crate that does what one of your dependencies was doing but better? Throw out the old and replace with the new. If that's too much work, the old version is still there and can be used as long as you want. If it's in the standard library, backwards compatibility goes from a convenience to a chain that prevents it from changing too much. A better solution came along? Sorry, all the existing software will break if we implement it. That said, if you add *this* to your dependencies, you can enjoy the improvements while all the software that depended on the old version can keep working. In Rust, the standard library is meant to contain only what's necessary for the compiler to output valid code. Everything not fundamental to the language gets delegated to crates where they can be updated independently.
&gt;`rustfmt` can reorder your imports and the technology is certainly there to delete them (the compiler emits very accurate warnings about unused imports) but I believe actually removing them is yet-unimplemented, and might(?) be a welcome feature in `rustfmt`. If not, `cargo` is available as a crate, so you may be able to skip `syn`. That's actually implemented (on beta, it will reach stable in Rust 1.34), and you can remove some unused imports by running `cargo +beta fix`! Unfortunately due to a design quirk rustfix is only able to apply suggestions that change a contiguous block of text, and avoids suggestions that change multiple blocks. This means some complex nested imports are not automatically fixable until [this issue is fixed](https://github.com/rust-lang/rust/issues/53934).
Nice, I totally need this. Was using a bash script line to check the status of all my repos.
And "Cheers" in German. 
The goal is to publish a new episode of my introduction to Rust for scientific computing. On the side I am going to explore a different routine to compute quantiles in bulk in `ndarray-stats` - the next release approaches!
&gt; because Rust's imports are a purely compile-time notion that just make names available to the compiler. this is good to know. thanks!
&gt; What if there are two types that implement all those traits? That*s the idea of traits I guess. Currently I have the impl Authorize for Reddit let r = Reddit::with_ui(app_context); // starting the program ui::run(r) // and the rocket based web-ui fn ui::run(authorizable_state: T) { //... rocket stuff .manage(authorizable_state) // rocket manages r: Rocket as managed state } fn authorize&lt;T: Authorize + Send + Sync&gt;(authorizable_state: State&lt;T&gt;) { authorizable_state.authorize(); // from trait Authorize } I probably have to use a trait Context that has a fn context&lt;T: Serialize() -&gt; T; so I can make sure that I can access what is called app_context in struct Reddit from all other too. if later I decide to have Facebook in my app, I'd just pass a Facebook object to rocket. let f: Facebook::with_ui(app_context); ui::run(f); I don't understand the problem with that. 
Do you have any resources that discusses these types of attacks in protobufs? I would have naively thought that the use of schemas would largely prevent this type of thing. 
r/playrust
Your code is not valid Rust code.
i wonder who had the idea of inventing the ownership and borrowing system,
Proc macros look a lot like Common Lisp macros actually: a function that takes syntax and returns syntax. Lisp's syntax is quite different and simpler than Rust's of course, but the base concept is quite similar.
Ive been having fun doing graphics. Im learning how to use the vulkano api, so i can unwrap the mystery of whats happening bellow the game engine when it comes to rendering.
Unfortunately not, I've used it in a closed source project I'm working on, sorry!
&gt; our code is not valid Rust code. Oops I made a mistake in the function bodies. I should be fixed now!
Made a huge release for [battery](https://crates.io/crates/battery) yesterday: * Few public methods are renamed in order to look more "professional" :) * [uom](https://crates.io/crates/uom) integration * Proper errors handling and propagation for all supported platforms * Multiple batteries are now supported for FreeBSD/DragonFlyBSD * Linux implementation is now ignoring batteries from the connected devices, such as mice I want to integrate fuzzing now (because it's cool, aren't it?), but can't figure out how to get access to the crate internal routines from the fuzz binaries, I really do not want to expose them to the outer world.
Thank you for informing me, does “SDL_Delay” do the same thing? And also, is there an alternative to this?
It was already posted and discussed a while ago: https://www.reddit.com/r/rust/comments/axsci4/oxide_a_formal_semantics_for_rust/
Thank you for the suggestions. The directory containing `mariadb.pc` (`/usr/share/pkgconfig`) is in one of the directories that pkg-config checks by default, based on what I get with the command: `# pkg-config --variable pc_path pkg-config /usr/lib64/pkgconfig:/usr/share/pkgconfig` However, just to be sure, I set `PKG_CONFIG_PATH` to `/usr/share/pkgconfig` and tried again to build `diesel-cli`. No luck. I still get the dreaded `cannot find -lmysqlclient` error. I've also tried setting a couple of other environment variables such as `MYSQLCLIENT_LIB_DIR` and `MYSQLCLIENT_NO_PKG_CONFIG` to get diesel-cli to build, but either I set them wrong, or setting them doesn't address the root problem, because diesel-cli still would not build. Thanks again for your help.
I would usually go for 3 if I have a function that operates on two `u32` values. If I specifically wanted the same function to for example work on both `u32` and `u16`, and possibly other types which can be converted to `u32`, I would go for option 2, as `Into` conversions always exist when `From` conversions exist, but not the other way round. In the function of option 2, you can use the ergonomic `a.into()` because the only thing you know about `a` is that it implements `Into&lt;u32&gt;`. If the addition were not inside such a function, you couldn't for example write let a: u16; let b: u16; // ... let x: u32 = a.into() + b.into(); // error Here the compiler cannot just assume that `into()` should be converted to `u32`, as the `+` operator supports many many types. So in this case I would use `u32::from(a) + u32::from(b)`.
Nice! Can it generate flamegraphs? That would give a much better interactive overview. Here's a Rust implementation of flamegraph: https://github.com/jonhoo/inferno There are also Rust profiling tools that generate flamegraphs and apparently do not depend on Inferno: https://github.com/TyOverby/flame and https://github.com/llogiq/flamer
AFAIK the Nim programming language does the same: &amp;#x200B; [https://nim-lang.github.io/Nim/manual.html#macros](https://nim-lang.github.io/Nim/manual.html#macros)
I'm working on a revolutionary technology that will allow you to run brainf *on compile time* with a macro like so: ``` let four = bf!(++++); assert_eq!(four, 4); ```
just tested it now. just for importing 5 GB file in postgres table with zero numeric conversions: &amp;#x200B; CREATE TABLE PROJECT\_LANGUAGES( repo\_id CHAR(10), language CHAR(40), bytes Char(15), time CHAR(25) ); &amp;#x200B; importing: \\copy PROJECT\_LANGUAGES FROM '/home/raja/Downloads/project\_languages.csv' WITH DELIMITER ','; Time: 587530.115 ms (09:47.530) &amp;#x200B; Original file size: 5GB Is there any way to speed up this? I disabled indexing. No primary keys either. In article I mentioned 4 files adding up to 30GB. Rust script was able to read and process in around 4 mins in HDD and 2m40s in SDD. Do you think for one-off processing like this, is it worth going for DBs?
The Cyclone programming language has a region analysis which influenced Rust: &amp;#x200B; [https://en.wikipedia.org/wiki/Cyclone\_(programming\_language)](https://en.wikipedia.org/wiki/Cyclone_(programming_language))
This will completely change the whole industry
Ah sorry! I did a search on Reddit but I only got an internal server error…
Might want to look into GHC's Template Haskell macro engine.
New to Rust, creating a CLI for managing all the one liner bash and Docker scripts I have lying around: https://github.com/BenSchZA/pier
This reminds me of `File::open` in the standard library: impl File { pub fn open&lt;P: AsRef&lt;Path&gt;&gt;(path: P) -&gt; Result&lt;File&gt; {...} } Another thing that comes to mind is impl str { pub fn contains&lt;'a, P&gt;(&amp;'a self, pat: P) -&gt; bool {...} } So, the standard library tries to be ergonomic and flexible in that the user could provide a parameter in different types. I'm sure there are more examples. Personally, I've used `Into` and `IntoIterator` a couple of times for this reason. Basically, you write such a function once and call it potentially multiple times from multiple places. So, it kind of makes sense to make it more ergonomic to use this way. However, I wouldn't go crazy with this. I don't think your example is a good one for making its interface more generic. Ask yourself how you want the (possibly generic) function to be used and how often such conversions might be useful at the call sites. The potential downside to these generic interfaces is code bloat. If your function is "large", you might want to consider separating what it does from the conversion like this: // this does not really need to be generic fn foo_inner(p: &amp;Path) -&gt; Something {...} // ergonomics improvement fn foo&lt;P: AsRef&lt;Path&gt;&gt;(p: P) -&gt; Something { foo_inner(p.as_ref()) } This is not always a good idea, though. Sometimes the compile-time type information constrains the set of potential values you get out of such a conversion. For example, taking an `x: I` where `I: Into&lt;Cow&lt;str&gt;&gt;` gives the compiler already a good idea what `Cow` variant you'll get out of `let cow = x.into();`. If `I=String`, you'll get an `Cow::Owned`. If `I=&amp;str` you'll get a `Cow::Borrowed`. This knowledge allows the compiler to eliminate branches at a later point in time which is super cool, IMHO. 
I think the downvotes are because that isn't how initials work. Why would there be an M at the end when his last name is Stallman?
Your, project sounds interesting. I don't know that much about fuzzing or how OSs manage batteries, but would you need some help ?
I want to continue my quest to contribute to servo. Last week I looked into some issues and commented on one yesterday. However, most off the time I read the issue understand the problem, maybe understand the code and then just get overwhelmed by size of the project, or how one would implement a patch. Also servo needs 8min to recompile on my laptop.
Thank you! Definitely need conditionals though to make this practical.
What type of database are you using? StackOverflow or IRC channels on Freenode will likely have your answer about speeding up imports. Loading is only one step of many in your work. Once you know SQL, not to understate the challenges of that, you can run ad-hoc queries against the dataset very quickly. When you know what results you want to conduct further operations with, you use Rust to execute the SQL and then do as you wish with the results. Also, take a look at the cool work that @bluejekyll has been working on for [extending Postgresql with Rust](https://github.com/bluejekyll/pg-extend-rs). Not sure what datatype you would use but pg-extend-rs could theoretically link postgres to a Rust type and then allow you to use a foreign data wrapper around that dataset to execute SQL on.
I've said this before a few times, but regex moving *out* of the standard library was probably the best thing that could have happened to it. When I initially wrote regex, it was part of the pre-1.0 pre-Cargo Rust distribution. It was part of a small set of crates that you could automatically pull in with `extern crate`. With the advent of crates.io and the push toward 1.0, we fairly aggressively trimmed the standard library down to a set that we felt comfortable stabilizing. The regex crate was not part of that set, and it generally wasn't clear whether it should be either. So it got moved to a separate crate. In the time when the regex crate was part of the Rust distribution, it received very little attention, particularly from me. This was because there was a lot of overhead (more than there is now) in developing in that environment. It took a lot of effort to figure out how to use the build system and get PRs pushed through. It didn't really feel like I had the freedom to iterate on its design effectively. Once it got pushed out to a separate crate and a separate repository, it became *much* easier to iterate on. Lots of performance improvements were added and the API was refined. Does that mean it should be in std now? The answer to this question depends heavily on what benefits are to be gained by this. The costs are fairly well known: * Putting it in std, in practice, means the API can never change. While I hope and believe regex 1 will have a long life, releasing a regex 2 down the road is not only a possibility, but a fairly easy thing to do with little downside. (regex is typically not a public dependency, so the key downside here is a period of time where both regex 1 and regex 2 would be included in a crate's dependencies as the ecosystem migrates.) * Putting it in std means that development iteration is harder, primarily because the barrier to contribution is (rightfully) higher. In particular, I do have long term plans for re-architecting the regex crate's internals. I'm not sure I'd move forward with that if it were in std. The benefits are less clear: * You wouldn't need to add a dependency to your Cargo.toml in order to use regexes. However, Rust development is heavily dependent on the crate ecosystem, so this doesn't confer a ton of dependencies. Nevertheless, the regex crate is perceived as "heavyweight," so some folks do (rightfully) try to avoid it unless it really pays off to use them. * I've been hearing more and more about people working in places where every dependency must be audited. Therefore, moving more and more functionality into crates maintained by others increases the auditing burden, and potentially therefore decreases the chances of Rust adoption. I still don't completely understand how these places work, and in particular, why exactly the standard library is exempt from this auditing process. But it's probably related to the perceived "officialness" of the standard library when compared with "a random crate." Even so, one might not consider this a reason for bundling things more into std, since that's potentially only one solution to this problem. IMO, more people need to write about these types of environments and how they work, and what exactly would be acceptable. &gt; but this is rather a general question as to why we need to download and build &gt; a crate for regex A useful exercise might be to consider the inverse question: why *shouldn't* you need to download and build a crate for regex?
Is it possible to put a macro in a module namespace? All is in the snippet: pub mod prelude { #[macro_export] macro_rules! hello { () =&gt; { println!("Hello") } } } fn main() { // How to make this compile? self::prelude::hello!(); }
[I wrote about that once. :)](https://phaazon.net/blog/on-owning-borrowing-pub-interface)
I am using postgres only. You didn't get my point I guess. What I meant to say is, for one time processing situations like this, writing simple iterative code is always going to be fast. On the other hand, if I have to do many queries from a fixed data, then it's better to go for DBs. If you still aren't convinced, the data is pubic only. If you have some time, you can download it and try doing all the operations in pgsql. Let me know the time taken for whole process. In my experience, it's never going to be even remotely closer to iterative code. Because it's obvious, DBs make different kinds of tradeoffs. They are completely different from one another. 
You can try pub mod prelude { // ... Your macro with `#[macro_export]` pub use crate::your_macro; } As `#[macro_export]` will always export macro at top level. You can re-export it at prelude. This should work with 2018 edition.
I've been working on [SueQL](https://github.com/sieut/SueQL), a SQL database, for the last few weeks. I'm having a lot of fun with implementing a database and using Rust. I completed the pager last week and then added a few things so I can run the database. The next thing I'm gonna work on will be transactions and it's gonna be fun!
Thanks, it works well! I don't like this, though. The macros now "lives" in 2 places, and there is an unnecessary step between the declaration and the export. I guess that it will be solved with macro 2.0
And [fast driver](https://fr.wikipedia.org/wiki/Alain_Prost) in French.
`SDL_Delay` is analogous to `sleep`. If you want to do FPS limiting, you'll need to track how long it took to do the previous frame and sleep `16ms - prev_time`. But you aren't guaranteed to actually sleep for that little (you may sleep for more).
I'm still working on learning. I'm hoping to do a proper embedded project in the future once I've got the syntax down and I'm not fighting the borrow checker quite so much. 
Nope, still not convinced! :) There will be more than one episode with a dataset. Present a summary of your data analysis to anyone and you'll get challenges, rebuttals, questions, and consequently more information to query. Then, it's time to reload that dataset back into another dataframe. You just repeated your 4 minute load into Rust and then spent more time writing Rust and compiling , revising and compiling, ... revising and compiling.. 
Is there any from scratch user guide for `yew` or well documented example? I've got no background in frontend framework and the documentation I find is very frustrating as it tends to refer to some existing popular framework; I can work with the example I find but it is quite a hassle as they bear little explaination on how they work/why they are built thusly.
How do you use a flamegraph to represent a memory dump? I think that they are more useful for data about execution times
which is why some proposed to change mut for references to uniq and keep mut only for bindings
Obviously if I there is even a remote chance that I have to do many operations in future I am going to use other means. But lots of time, you just get thrown bunch of logs with absolutely useless information. Then you just wasted a day or worse a week (depending on resources available) in your precious time getting everything setup in proper database, while you see the client jumping on to other things because he no more time for this. There is lot of difference in the life of data engineer and data analyst. One guy knows exactly what he has to do, figures out how to do it efficiently. Data science guys have absolutely no idea what is going on where most of the time in POC time. They have to figure out what results to show to his boss while having almost useless data. They have like few days to week time to prove that we have some stuff to get the client attention who now has some "AI" stuff to get funds. I hope you understand the situation I am talking about. These situations are not at all rare, it's infact a norm nowadays in the so called Data science field. If you still are unconvinced, prove me wrong. Take a laptop. Download the GitHub dump. Import everything in to any DB of your expertise. Do all the operations I have mentioned. Let me know how much time it took. Since you are an expert, I think it won't take much longer for you to setup everything. It will hardly take 10 mins for this data. And while everything is running, keep in mind that some client can come and say, leave that, check out this data instead. 
I don't have an axe to grind here with you. Pandas and dataframes are getting kitchen sinked into way more workflows than they ought to be. Good luck.
I understand that. I didn't discuss about Pandas much in the article. It was comparison with Spark which is pretty much standard nowadays and it has its benefits too. 
Doesn't this involve solving the halting problem?
How do I copy N bytes from a Reader to a Writer (for example from socket to file, or from file to file) ? N can be very large and the stream might be binary data, so I would like to * avoid storing everything in memory (chunked copying?) * avoid using String and iterating over lines
Ein prosit!
this looks really useful, makes a lot of sense
2 is the standard approach for generic functions. I wasn't even aware it was possible to place a trait bound on a concrete type until your post. I wonder why the compiler allows it...
Okay, great! I'll check that later, I just wanted to know if it was possible and if someone already did it! Thanks!
Only if he needs the compilation to guarantee to halt. If the brainf*ck code does not halt then neither will his macro. Rustc might have some limits on how long a macro can run though.
I am just curious. I had the question in my mind, and I asked it. That's all.
Better questions is why some languages feel a need to have those "basic" modules to begin with? Would you start each new project with random dependencies you might actually need, but may not? There is nothing innately better in std library than in external dependency. It's just code written by someone else. It has aura of being 'official' therefor people assume that it will be maintained and updated... no, it actually does not mean that. And quite vice versa, what it means that it will get in the way of updating the language as now your language will be hard coupled to certain type of implementations to retain BC. It's actually terrible. It's picking the most expedient path. And yeah, obviously people due to the fact that they always pick path of least resistance without much though, will prefer fat std libraries to lean ones. However, the language itself pays a huge price for it as it means that the language won't age well.
So I am on Chrome on Android, and the clock example is blank. Does it not work on mobile?
Update: it's blank when I load the page, and the clock shows up when I switch to another app and then back to chrome. Same for the crab example.
How do i use function parameter to set array size? this `fn x(n: u32){` `let np:[bool;size]=[false,n];` `}` is incorrect. how can i use n to define the size of my array?
Array size must be known at compile-time. Maybe it will be possible one day to have unsized type in the stack (more specifically VLAs, in this case) but not for now.
So much for open-source basement tinkering. RISC V can't come soon enough...
You want a `Vec` instead, those are dynamically sized and growable. You can initialize one with the vec macro. fn x(n: usize) { let np: Vec&lt;bool&gt; = vec![false; n]; } (Also in this specific case with bools you *may* want a bitvec from a crate.)
thank you @Boiethios, i'll think of another approach.
Same question - flamegraphs are great, but I'm not sure how to make one that's meaningful for representing memory usage. If you know how, I'd be happy to try it out!
i'll try it @JayDepp, tks for the suggestion. &amp;#x200B; I'll need to lookup what a bitvec is :sweating\_smile: &amp;#x200B;
I'm currently waiting on my flight at the local airport, going to work from Spain for a week, and I'll be actually getting paid to write Rust there. It's amazing. 
&gt;keeping crucial things such as a random number generator &lt;..&gt; not built into the standard library Just look at the [versions](https://crates.io/crates/rand/versions) of `rand`. Since it was moved from `std` we had 6 minor versions (read: breaking changes were introduced 5 times), and [7th version](https://github.com/rust-random/rand/issues/715) is currently WIP. If you compare v0.7.0 and v0.1.0, you can see that crate undergone a significant evolution, which would've been impossible was it included into `std`.
Checkout the -‘lewton’ crate to decide off dukes into samples as well as the ‘portaudio’ fror the For soundcard manipulation
You can, just not with endpoint functions. It doesn't make much sense anyway -- you need a specific shape of data for endpoints to work!
I'm working on a testing library. It's just for fun right now, but if it feels nice to use, I'll polish it up and release it. Small [screenshot](https://imgur.com/a/C5vMzBz) 
`io::copy` copies all of a reader to a writer using a buffer of a good size. You can limit the amount with `Read.take`. io::copy(&amp;mut reader.by_ref().take(n), &amp;mut writer) Or if reader and writer are already mutable references: io::copy(&amp;mut reader.take(n), writer)
Hey, so I'm very new to Rust and to get my hands dirty, I tried solving a classic "Check if parenthesis match" problem. fn is_valid(s: String) -&gt; bool { let mut stack = String::new(); for c in s.chars() { match c { '(' =&gt; stack.push(')'), '{' =&gt; stack.push('}'), '[' =&gt; stack.push(']'), _ =&gt; match stack.pop() { Some(top) if c == top =&gt; (), _ =&gt; return false } } } stack.is_empty() } This is what I have so far, any comments on making it more ~~Rusty~~ ~~Rustic~~ ~~Rustomatic~~ Idiomatic?
Putting something in the standard library is useful if 1. It's so fundamental to the language that it wouldn't make sense to program without it (Vec, Option, Result, String) 2. It can't be implemented without special access to the compiler. (I'm not sure exactly which parts of the standard library fall into this category) 3. Conversely, the compiler gives special meaning to the code (Operator override traits, like Mul, Add, Eq, and Index, Iterator, Deref, etc.) 4. Having more than one competing implementation would be detrimental to the ecosystem, often because crates need to share compatible interfaces (Iterator, Result, Futures, but for counter examples where this is true, but not sufficient to warrant inclusion in std, see serde, crossbeam-channels, num, http) Regular expressions are not particularly fundamental. They're very important for certain fields of practice, but not used at all in broad areas of software engineering. I only use them once or twice a year, despite working in a primarily textual environment (the web), because there's usually a simpler tool for my tasks. They aren't affected by interoperability issues, and they don't need special compiler support, and don't enable special compiler behavior. Random numbers seem like a closer case, where there is some benefit in sharing a single interface between crates, and it does get into nitty-gritty hardware and OS interaction, but as others have pointed out, the problem space is complex, and hasn't been properly explored enough to have come up with a stable interface. Maybe in five years, it will make more sense to pull it in, but there's no hurry. Channels would be a great candidate for the standard library in some ways, because they do facilitate interoperability between crates, and in fact, rust has them in the standard library, but the implementation has been found lacking (because it's a hard problem!), and most people now recommend the crossbeam project's implementation, so in spite of meeting making it a part of the standard library so early on may actually have been a mistake.
Will this new system support scoped macros like `macro_rules!`? Is there an [pre-]RFC about the new macro-by-example system?
It does work for me on Firefox for Android. So, that eliminates a mobile phone being the problem.
Have you tried cargo's -j option? Corresponds to # of cpus. Presumably if you set it to one you'll have fewer threads running during compile, and use less memory during that phase at least. 
If the comment I responded to is indicative of a larger trend, I think they're more annoyed by you oversimplifying the profession. We're always making tradeoffs. Claiming that there are always-applicable always-best solutions to a problem domain as wide as distributed systems is just ridiculous.
Very nice
Good question! I know there are some other languages that use AST-based macros, although I am not sure about the chronology wrt Rust. Elixir has them, but it is a dynamically typed language so there is much of the Rust macro logic that does not apply. Template Haskell also comes to mind. This is definitely older than Rust, and is a statically typed AST-based macro system as well. Although it is also widely considered to be difficult to comprehend. :-) 
Proc macros look a lot like Java's annotation processors to me, I.e. they are AST -&gt; source code functions. Not exactly the same as well behaved annotation processors cannot modify or replace their input tree. 
Thanks! Here is how I used it to POST to a TcpStream, while keeping the connection open in the end for more requests: stream.write_all(b"Content-Type: application/octet-stream\r\n")?; stream.write_all(format!("Content-Length: {}\r\n", fd.size).as_bytes())?; stream.write_all(b"\r\n")?; // Create a Reader from the file of exactly the size we know. If // the file changed in the meantime, this should fail. let mut reader = fs::File::open(&amp;fd.path)?.take(fd.size); io::copy(&amp;mut reader, io::Write::by_ref(stream))?; Ok(()) I will also use inverse (like the one you wrote above) to read from the TcpStream to a file, on the receiver side.
[https://docs.microsoft.com/en-us/azure/devops/pipelines/agents/hosted?view=azure-devops&amp;tabs=yaml#capabilities-and-limitations](https://docs.microsoft.com/en-us/azure/devops/pipelines/agents/hosted?view=azure-devops&amp;tabs=yaml#capabilities-and-limitations) &amp;#x200B; Azure DevOps run on [Standard\_DS2\_v2](https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes-general#dsv2-series) which have 7GB. Not sure if that helps. &amp;#x200B; Also, it might be worth linking your project. 
Before making it more idiomatic, it would be nice to make it work ;). The algorithm will fail on a string like `"[1]"` because `String::pop()` will remove a character regardless of whether it matches or not. As for being idiomatic, first, don't take a `String` if you can get away with using `&amp;str`, second, don't use a placeholder (underscore) if you plan to work with the data. This is how I would rewrite your function fn is_valid(s: &amp;str) -&gt; bool { let mut stack = String::new(); for c in s.chars() { match c { '(' =&gt; stack.push(')'), '{' =&gt; stack.push('}'), '[' =&gt; stack.push(']'), ch =&gt; if stack.ends_with(ch) { stack.pop().unwrap(); } } } } stack.is_empty() }
While I'll admit I haven't used the new macro system much recently, Scheme's (now-nonstandard) [`syntax-case`](https://www.gnu.org/software/guile/manual/html_node/Syntax-Case.html) also allows for procedural macros operating on syntax.
Looks nice to me. The only thing I would change is the function signature: you only need to read the input, therefore a reference would be enough: fn is_valid(s: &amp;str) -&gt; bool { And also it would be nice if instead of returning a boolean there was some information about why the string is not valid: `expected ")" found "]" on line 32:4` but that's obviously too much for your example.
Still contributing to https://github.com/svenstaro/miniserve and having a lot of fun with it. I'm currently implementing the "download folder" feature, that creates a tar.gz file on the fly so user can download the folder. Also, I filed a PR to tar-rs because I stumbled on an (already known) bug. 
Thanks! I kinda assumed it'd only be populated with brackets because leetcode but yeah it'd be good practice to not make such an assumption.
There's an [issue](https://github.com/rust-lang/rust/issues/48055) for this, so maybe it's not so far in the future that we'll all grow beards before it's implemented :)
If you are working on larger changes to the macro system please note that we are in the process of revamping our language team structures towards a model of working groups. In particular, I think that any new large RFC should go through a working group and get polished there as well as making sure that there's someone who can implement things later. Please reach out to a lang team member on the [`#design` discord channel](https://discordapp.com/channels/442252698964721669/443151225160990732) for now.
This seems close to the original attempt, and works. But in the long run using describe traits is probably better. &amp;#x200B; #![allow(unused)] struct Rectangle { base: u32, height: u32, } struct Circle { center: (u32, u32), radius: u32, } enum Shape { Rectangle(Rectangle), Circle(Circle), } fn describe_shape(geometric_object: Shape) { match geometric_object { Shape::Rectangle(r) =&gt; println! {"the base of the rectangle is {}", r.base}, Shape::Circle(c) =&gt; println! {"the radius of {}", c.radius}, } } fn main() { let obj = Shape::Rectangle(Rectangle { base: 32, height: 16, }); let obj2 = Shape::Circle(Circle { center: (7, 17), radius: 45, }); describe_shape(obj); describe_shape(obj2); } &amp;#x200B;
You've got it kinda backwards. The general-purpose mechanisms (quoting, splicing/antiquoting, functions run at compile time that take and receive ASTs or pre-AST token lists) are the primitive component. The pattern-matching / auto-expanding form is the more-sophisticated "convenience" UI layered on top. &amp;#x200B; The primitive mechanisms have always been in Rust's syntax-extension system -- indeed, the macro-expander is built on top of them -- they just haven't exposed a stable interface for direct use by user libraries because there were higher priorities and it's a big commitment that nails down a bunch of permanent requirements of the compiler. &amp;#x200B; Similarly, almost every system that has decently-powerful macros exposes the primitives somewhere if you dig a bit. Sometimes it's through an unstable or under-documented interface, but it's usually there. Eg. &amp;#x200B; [https://docs.racket-lang.org/guide/proc-macros.html#%28tech.\_macro.\_transformer%29](https://docs.racket-lang.org/guide/proc-macros.html#%28tech._macro._transformer%29) &amp;#x200B; [https://camlp5.github.io/doc/htmlc/quot.html](https://camlp5.github.io/doc/htmlc/quot.html) &amp;#x200B; [https://docs.julialang.org/en/v1/manual/metaprogramming/index.html](https://docs.julialang.org/en/v1/manual/metaprogramming/index.html) &amp;#x200B;
I want to launch a thread/service that keeps running on the background even after my main program exits. Let's say you do myprogram start The program will start the thread and then exits. Then you run the program again, that can still communicates with this thread. For example: myprogram info The program can detect the thread (running or not) and communicate with it. Any idea how to go about this?
Why would it not allow it? For example, `where String: From&lt;A&gt;` is a pretty interesting way to express a bound.
You can use [`Command::spawn()`](https://doc.rust-lang.org/std/process/struct.Command.html#method.spawn). The thread will be running in the background. You can then use a TcpListener, an unix stream, or something else. If you want a higher level-API, you can look in these crates: https://crates.io/search?q=daemon
An obvious beginner should use commonly accepted solutions to standard problems. Anything other then protobufs should be dismissed and I gave short summaries why. Sorry for not writing a disertation.
Yeah, I do! My problem with VLC is that it only supports a small amount of adjustment. I have two video files for each part of a two-part episode. I searched the whole web for one srt file for each episode but the only subtitle I could find was all together in a single srt file so I needed 30+ min of time correction, which VLC refused to provide. I ended up splitting the file in two and then using this tool to offset the second part by the duration of the first episode plus the intro of the second one. Worked perfectly :)
You cannot keep a thread running, but you can fork() off a child and exit the parent process.A crate like [daemonize](https://crates.io/crates/daemonize) will probably do most of the work for you. The child can then start listening on a unix or tcp socket for incoming connections and take some action on them. Could even be a webservice.
&gt; I actually use rust-peg Oh, cool I will make sure to check `rust-peg`out. I think what I saw was `subparse` in `subtitle-rs`using the `combine` crate for parse combinators, but it was the same parent repo as `substudy`. &gt; I also have some code tucked away in substudy that can automatically correct character sets and detect languages Sweet, I'll be sure to check it out. &gt; Do you mean trying to automatically match them up? Yeah, I've been working on a thingy to figure out which subtitle files correspond to which episode/video file. Nothing fancy yet, just trying to match file-names and then renaming the subs to match the video file so that VLC will detect them by default. (Too lazy to drag the file, but not to code :) )
Is there a way to set the contents of a slice to a \`Vec\`? I know \`copy\_from\_slice\`, but can you do something like \`\`\`rust let mut slice = &amp;mut buf\[0..10\]; let vec = vec!\[1; 10\]; slice = vec; \`\`\`
Looks good! A couple things though, you can use write/writeln macros for writing, which can be cleaner and also doesn't take an intermediate step of making a string when formatting. Also, by_ref doesn't really do anything, it just borrows the reader/writer which is meant to be a convenience for chaining with reader/writer methods that take a reader/writer by value but you want to keep using it. writeln!(stream, "Content-Type: application/octet-stream")?; writeln!(stream, "Content-Length: {}", fd.size)?; writeln!(stream)?; let mut reader = fs::File::open(&amp;fd.path)?.take(fd.size); io::copy(&amp;mut reader, &amp;mut stream)?; Ok(())
Thanks, that was a good suggestion but all it ended up accomplishing was making it take longer to get to the point where it failed :-/. I've edited my post with a little more information, along with the way I've gotten it working for now. thanks for your help!
Another option is fn add(a: impl Into&lt;u32&gt;, b: impl Into&lt;u32&gt;) -&gt; u32 This only works if you have no other trait bounds, AFAIK you cannot mix regular trait bounds and impl sugar.
I switched Tokio &amp; Tower to azure pipelines. It has 10 free concurrent workers for OSS projects, which is hard to beat.
The type problem isn't something Rocket can solve - Rocket needs to know the type you want in order to pass it in, but the Reddit vs Facebook decision might happen at runtime six modules away. There is no information available at runtime that Rocket can use to "find a managed type that implements Authorize". The `Send + Sync` problem is a bit easier - you can ensure that all Authorize types meet that requirement with `trait Authorize: Send + Sync`. Any authorize types need internal state will have to use a wrapper like Mutex or RwLock to control simultaneous access. If you know you will only ever have one Authorize type at runtime, you can `manage(Box::new(Reddit) as Box&lt;dyn Authorize&gt;)` and retrieve a `State&lt;Box&lt;dyn Authorize&gt;&gt;`. If you might have two authorization types, you'll need to come up with a way to track several of them. I hope that helps and I didn't leave anything out.
&gt; RustPython Huh, that's a _really_ interesting project. How complete is it? I'm guessing you used it instead of `cpython` for WASM support, right?
No, you can't. A `Vec` owns a specific memory allocation while a slice borrows an arbitrary memory location. It doesn't make sense to try assigning one to the other like that.
thank you very much. One Authorize type at runtime is fine. II will try to do that.
How are you generating the code? 
&gt; Capn is has similar use cases then proto but the amount of unsafe code in the rust lib should make anyone think twice. I haven’t looked at the implementation, so this is based on my knowledge of the protocol: capn proto is a direct memory buffer to memory buffer serialization protocol. I would assume that any unsafe is pretty much by design as it’s going to be serializing into and out of buffers in an unsafe manner. It doesn’t surprise me that it would have a lot of unsafe to perform these actions, so I wouldn’t knock it solely because of that.
Just like last week, I'm continuing work on my compiler/interpreter for ehprog, the language I'm designing. Yesterday I started work on a macro system. In other news, I had to put my Rust gamedev stream on halt until my internet gets sorted out, because we're having some issues with our ISP. 
So I finally was able to build diesel-cli and thus get Diesel working on my CentOS system The issue that was keeping the build from finishing was that there were no `libmysqlclient.so` and `libmysqlclient_r.so` files in the directory where all the `libmysqlclient` files are located. Once I created these files as symbolic links to the most recent version of `libmysqlclient.so`, the build was able to finish with no errors. I believe this problem is more likely to occur if, as is my case, you work with the most recent version of MariaDB from MariaDB, rather than the version that you get through the basic CentOS 7 repository. When I booted up a somewhat older version of CentOS 7 that uses the older version of MariaDB that comes with the distribution, I was able to build diesel-cli without any problems (after installing mariadb-devel). When I compared the `libmysqlclient` files between the two CentOS7 installations, I found that the new installation was missing the symbolic link files I mentioned earlier.
Increasing your swap may fix the memory error, but not the compile time.
I think C++ with smart pointers and RAII pattern has many similarities with ownership from Rust.
Rust's ownership model builds on the idea of RAII, or [Resource Aquisition Is Initialization](https://en.wikipedia.org/wiki/Resource_acquisition_is_initialization). Which is basically a long-winded way of saying that things can have constructors and destructors that fire when an object goes out of scope. C++ has this, for example, with types like `std::vector` and `std::unique_ptr` that are analogous to Rust's `Vec` and `Box` respectively.
Working on an actix realworld backend impl in Rust [here](https://github.com/fairingrey/actix-realworld-example-app). Yesterday I spent the time to finish writing the routes so I wouldn't have to pay attention to them anymore, today I'll probably spend time rewriting my custom errors to use `serde_json::value` instead of Strings (as that was just a way to get my feet off the ground quickly). After that, then I'll finish writing the message handling logic in `crate::db` and I should be done 😊
With a plain writeln!, but the generation step is not a problem, the problem is to compile the generated code.
And also that you never have a mutable reference to something in multiple places. I haven't used C++ in a while, but I'm sure you can abuse its RAII model to get cross-thread data corruption.
Thanks again for the contribution!
There is also work underway on tasks and templates to make people's lives easier See https://github.com/crate-ci/resources/issues/2
\&gt; Proto is well supported across languages and has multiple high quality implementations in rust &amp;#x200B; Which implementation should people be using for streaming? Last thing I read about it was [https://medium.com/@KevinHoffman/streaming-grpc-with-rust-d978fece5ef6](https://medium.com/@KevinHoffman/streaming-grpc-with-rust-d978fece5ef6) which made things sound like they were anything but high quality production ready &amp;#x200B;
Add on question: is it ok to be/should I be using the `foo: impl Into&lt;Foo&gt;` form instead of the type parameter form? Advantages are that in my ide the type parameter form is shown in autocomplete as `filter(p: P)` but with the impl Trait version you can actually see what the trait is. Are there any downsides? 
So if you want to compile on the order of Megabytes of raw code you are going to have extremely long compile times, as this will be in addition to any code you are already importing (external dependencies, helper functions, etc.) Simply put: big optimized code gen = big compile times --- How are you generating the code? You mention `writeln!` but that isn't a _compile time macro_, unless you are procedural generating code in the `build.rs`? Or are you generating macro's in the `build.rs`? Also what code is being generated? ~1000 static variables, and ~1500 type definitions is an extremely non-trivial compile job. 
I plan to continue the progress on [skribo](https://github.com/linebender/skribo). I have a couple of documents up (requirements, a deeper dive into script matching). For this week, I'd like to get requirements into shape and a working prototype that does shaping. Other projects (pulldown-cmark, druid) are moving forward as well, but I hope to keep focus on skribo for this week.
Usually, a single constructor might not make sense at all. However, my friend made a hypothetical question. Basically, he said that if all the enum where initiated with default values, it would be error-prone to manually add those defaults every time. His intention was to keep the default initialization in a single place in the code. Since he is still learning the language, I thought I'd take the opportunity to ask the community for help, in order to learn some of the cool things that the Rust type system allows one to do.
I've created a playground-link to demonstrate my problem: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=5121f68e84a96b09a9759f24e820ca1c I've got a generic type `Tally&lt;T, C = u64&gt;`, that does not seem to be respecting the generic type-hint `C = u64`. If I use a type alias `type DefaultTally&lt;T&gt; = Tally&lt;T, u64&gt;` it works. However, I don't understand why I even need to use a type alias since I'm providing a default hint. Any help appreciated!
The person asked for the differences between different protocols, not for your recommendation on what a beginner should use.
Thanks for the code. I think its much more clear than my version. I can even see myself doing some real-life code based on this.
In addition to RAII, which others in this thread have brought up, Rust takes a lot of its ownership ideas from [affine types](https://en.m.wikipedia.org/wiki/Substructural_type_system#Affine_type_systems), which were (mostly) academic in usage before Rust pushed them into the semi-mainstream.
From what I've heard so far: &gt; I still don't completely understand how these places work, and in particular, why exactly the standard library is exempt from this auditing process. But it's probably related to the perceived "officialness" of the standard library when compared with "a random crate." Either you audit it beforehand, or you assume it is safe because it is either bundled with the compiler or with the OS, both of which are checked beforehand. &gt; IMO, more people need to write about these types of environments and how they work, and what exactly would be acceptable. While I can't point you to anyone specific, this kind of company is occasionally mentioned in subreddits like /r/talesfromtechsupport, and If you ask random software developers, you'll probably get some answers, too. I don't think you can get the affected developers to lobby for changes in the Rust community, though, because most of them might not be using Rust.
People here are quoting RAII and surely that's, well, related, but in terms of it being a type-system feature the roots go to region-based systems, late 80s, which generally stayed in academia, and substructural type systems, also an academic thing. The first one was Clean, also late 80s, though it didn't use linear types to control memory resources, but side effects (for Haskellers: In Clean, you don't have to hide [`#State RealWorld`](http://hackage.haskell.org/package/ghc-prim-0.5.3/docs/GHC-Prim.html#t:RealWorld) in a Monad to have safe IO). RAII as a spelled-out concept isn't older, either. Rust's contribution here is not on the theory side of things, but combining all the logical foo in a way that's palatable to the wider industry
The problem with From&lt;T&gt; is that it would only work when all of the enum variants had a different type. Even a single repeated type among dozens of variants would spoil the fun. The only solution to work around a couple of variants having the same type would be to create a wrapper type for each case. However, I don't know whether wrapper makes for good code. I don't have much experience with wrapped types as I usually avoid them.
The problem with this approach is that all the constructors must have the same type. However in practice that is often not the case.
Prior to rust there were several LISP's written for academic purposes that used similar systems of movement, copying, typed references for borrowing, etc. to create similar ownership semantics. Their goal was explicitly to improve GC. 
ClusterFuzz isn't quite a backend like libFuzzer or AFL. It uses targets built for these backends. I don't really know how cargo-fuzz works, but +1 to making things pluggable so that different fuzzing engines can be used for the same targets. We do this in OSS-Fuzz so that targets can be built for AFL, libFuzzer, and honggfuzz
At my company we're in the process of converting our JavaScript codebase into Typescript and it definitely helps. There are issues around some of the boundaries though. Because the JavaScript is very old there are some really weird design choices and strange patterns in there that are difficult to create typings for, plus there are situations where the JavaScript is just plain wrong about its assumptions of types. Still, Typescript itself is an amazing tool and a pure Typescript project probably won't suffer from any of those issues.
I generate code in design time with a separate generator, so no macros at compile time. The code is basically an API for some generic representation, like this: pub struct SomeStruct(ObjId); impl SomeStruct { pub fn try_from(obj_id: ObjId) -&gt; R&lt;Self&gt; { if obj(obj_id)?.cls_id()? == class::SOME_STRUCT { Ok(Self(obj_id)) } else { bail!("not an instance of SOME_STRUCT") } } pub fn some_field1(&amp;self) -&gt; R&lt;SomeFieldType&gt; { SomeFieldType::try_from(self.0.value_by_str("some_field1")?.obj_id()?) } ... } Generated code is huge because the project is enterprise-level software and uses very heavy industrial standard, I can't make it smaller. &gt; Simply put: big optimized code gen = big compile times We already invested so much effort in using Rust for our new projects, I don't want to drop Rust because of this. There should be a workaround or something.
I mean one way to look at it is that compiling the generated code is the problem. Another way to look at it is that the code you're generating is the problem. When you say you're using `writeln!`, what do you mean? Did you write a program to generate your code? Is it in `build.rs`? What is the generated code supposed to be doing, and why can't it be generalized? 
does that mean that my "client" app can communicate with the process through a unix socket? (that's what I got so far). Is there some kind of authentication or any other program can do that too?
Lots of additional details on crates.io . Very nice. I’ve had to deal with self-referential structs multiple times and this looks like it will greatly improve the experience. Also reminds me of offsetof in C.
Tried with a chromium browser and it didn't work but Firefox did. Not Firefox focus tho
I'd say if implementing default values was the motivation, you could implement the Default trait for the enum, which gives you the .default method were you can return the default state of it. If you want a default value for each variant, maybe a constructor for each would be more appropriate since it states your intentions in a more explicit manner.
&gt;We already invested so much effort in using Rust for our new projects, I don't want to drop Rust because of this. There should be a workaround or something. I don't want to be the bear of news, but if you swap `s/Rust/C++/g` or `s/Rust/C/g` then taking 10-15minutes to optimize **Megabytes** of source code is reasonable, albeit expected. I'm sorry but I'm struggling to see this is deal breaking. If your enterprise has _this much source code_ it needs to maintain compatibility with surely your unit tests, and integration tests must be on the order of Days/Hours to fully execute? --- Have you looked into build farm management tools like Bazel? They offer integrated asset caching. As you start using optimized compiled languages with non-trivial project sizes moving to a build farm starts making sense. 
I wrote a program to generate that code. The generated code is a type-safe and handy API to internal generic representation. Probably I can generate it differently, but the domain model itself is huge, so there is no easy way to make it smaller. Of course, I'm thinking of ways to generate less code.
One major point missing from the benefits is the ease of discoverability in std. The std is assumed to be a good enough solution without any evaluation overhead. See Java std: until you really need extra performance or a special use case, most of the standard libraries are pretty OK unless deprecated. Don't misunderstand me, I'm hugely in favor of how rust handles it, but imo that is a very valid reason. 
Released version 0.2.4 of the [winlog](https://crates.io/crates/winlog) crate which has a more robust conditional compilation process and now also builds on MinGW (thanks to James Cassidy).
I've forgotten quite a lot about the wire-format of protobuf... so consider this a generic "schemas would prevent decompression bombs" thing. If you have an object that takes, say, 128 bytes in memory, but can be encoded in a single byte because all fields are optional (and null fields are not encoded), then I can trivially craft a message that will be 128x smaller than the amount of memory your DOM will take. Not the Billions Laughs attack, but still pretty significant.
grpc is a different story and should not be equated with protocol buffers. There is no reason to always go with grpc when using protocol buffers. I have no idea what should be used for streaming. I'm not a fan of any of the libraries mentioned.
Thank you for the replies. I realized my idiotic mistake after taking an hours break to do something else.
&gt; then taking 10-15minutes to optimize Megabytes of source code is reasonable, albeit expected I've tried opt-level = 0, so this is not about code optimizations. So, your expectations are wrong here. &gt; I'm sorry but I'm struggling to see this is deal breaking. For me, it's a deal breaker, because it actually breaks the workflow, as I wouldn't be able to change anything in this library because I don't have so much memory or time to recompile it. 
Just to be polite, OS is Solus Linux terminal zsh via gnome-terminal. It was a mistake on my part rather than a terminal session.
TIL `const` can declare constants *after* their use oO
Of course! Most important part right now is the testing, cause I'm not able to test all supported systems properly with the VirtualBox. It would be really helpful if you can run `battery-cli` and check if everything looks okay for you. I have the [pre-built binaries here](https://github.com/svartalf/rust-battery/releases/tag/0.7.0) or you can `cargo run` it from the repo. There are few issues exist right now, mostly about CLI utility and OS support: https://github.com/svartalf/rust-battery/issues In fact, any help would be appreciated: code review, documentation audit, suggestions or anything else :)
Looks like a simple way to implement moveable self-referential structs with a safe public API, as long as you tolerate unsafe code in the implementation. I'm curious what the use-cases are – it seems useful, but I can't think of any time when I would've used this. Do you have any examples?
&gt; Rust's contribution here is not on the theory side of things, but combining all the logical foo in a way that's palatable to the wider industry For ownership, certainly, for borrowing I am less sure. Verifying the soundness of the type system has been slowed down by properly modelling borrowing relationships; and the goal of the recent [Oxide paper](https://www.reddit.com/r/rust/comments/axsci4/oxide_a_formal_semantics_for_rust/) was precisely to introduce an easier formal model of borrowing for further research. All this seems to point in the direction that borrowing is sufficiently novel that researchers lack the formalism to properly work with it.
Sounds potentially interesting, but I have no idea what Now is or why I should care. Something about deploying lambdas. &amp;#x200B;
It's a serverless platform. If you're not in the market for one of those, then yeah, maybe not a big deal to you. Other than "another company adds rust support to their offering."
&gt;trying random combinations until something stuck. that's what I do
This is awesome. I recently started using Now for some Next.js projects and it works extremely well.
😂 It's a great strategy for getting started.
Sorry, but this really looks like you're dismissing non-protubuf rpc options, then dodging the question of how to actually use protobuf for rpc.
I didn't have any use for this when I started, but now that I think about it this could be the backing for async-await without pinning. Meaning we could have async functions that borrow across await points and don't need to be pinned.
Amazing hihi
I've wanted something like this for an index into a large sequence of structs. Like a list of public transit stops bundled with precalculated lookup tables for stops by id and stops by name to help users of the list.
Nice article! &gt; There’s just a little nit: Cow::into_owned obviously patterns match on the variant, inducing a small yet present runtime overhead. I actually checked this once and again right now. The compiler is usually smart enough to optimize it. If `Into&lt;Cow&lt;str&gt;&gt;::into` and `Cow::into_owned` is inlined the compiler has enough information to eliminate one branch of `into_owned`'s `match` expression. :)
The made first contact with rust v1.10. My github repository is now about two years old. I can fix errors way more quickly and I believe I can write better code than in 2016, I am using a variety of features of Rust, but I always fail at the same things. When that happens, the compiler suggests something, ie. give it a lifetime, then I put a lifetime to it, and see myself placing &lt;'a&gt; everywhere just to figure out an hour later that I head a deadlock. I return to the previous state, put #derive(Clone) everywhere and clone things. That's just one example. There was a link to a blog post explaining exactly that. I experienced the same. I rarely touch closures, and haven't used all of rusts features yet. I feel I will always be a starter somehow.
I see I'm not the only one here watching Jai and trying to implement relative pointers. One bizarre thing about relative pointers is that on their own, they're `Pin`, but if they're contained in a struct or array and only reference other items in said struct/array, then the while structure is `Unpin`. Immovable types weren't the nicest to deal with when I tried this.
Last week on [tarpaulin](https://github.com/xd009642/tarpaulin) I added code-coverage for doc-tests, improved coveralls support for different CI-servers (tested on jenkins and travis so far) and closed a few different issues. This week I aim to tackle some more issues leading to a new release later on in the week (probably Thursday or Friday). I've also got a Clippy PR I'm working on, and some ndarray based stuff on the side
Richard StallMan
Yeah its weird, but very interesting and can be useful.
The code would be something like, let frame_delay = 1000 / 60; loop { // Do game stuff here let ticks = timer.ticks() as i32; let frame_time = timer.ticks() as i32; let frame_time = frame_time - ticks; if frame_delay &gt; frame_time { let sleeptime = (frame_delay - frame_time) as u64; std::thread::sleep(Duration::from_millis(sleeptime)); } } [You can see how I did it in my tetris app around line 287](https://github.com/camccar/RustTetris/blob/master/src/game.rs)
These things are nice for serialization. I'm not sure about this specific implementation, but in general if your pointers are relative you can just mmap a data structure from disk and start using it, no need to parse or fix up pointers at load time.
As serialization schemes, ppotobufs are more compact than (uncompressed) Cap'n Proto, but far more CPU-intensive to process, and require generating larger amounts of code. Cap'n Proto is zero-copy and random-access friendly.
&gt;Macros: WTF? Rust macros feel like a left turn compared to the rest of the language. To be fair, I haven’t been able to grok them yet, and yet they feel out of place like some strange bolt-on appendage, one which only came about after the language was designed, inspired by Perl. I will eventually take some time to understand them properly in the future, but right now I want to avoid them. Like the plague. I'm a noob so don't be offended please. But I cannot agree more with this. 
What exactly does your crate do?
I still not believe that 5MB is that much code. 500MB? sure. &amp;#x200B; A RDBMS ingest much more than that (with raw sql) and can take it. &amp;#x200B; To use so much memory, must Rust building some (pointers, types, others?)tables or caches. &amp;#x200B; I try to put things behind a trait, and implement the most common code here. Also, maybe put some indirections with Box (I remember it cut compile times). &amp;#x200B; Other option, because this look alike what I plan to do with a relational language to target database development, is to put all in a NDArray: pub trait Scalar:Debug { #[inline] fn type_id(&amp;self) -&gt; TypeId; } impl Scalar for i32 { fn type_id(&amp;self) -&gt; TypeId { TypeId::of::&lt;i32&gt;() } } impl Scalar for Rc&lt;Scalar&gt; { fn type_id(&amp;self) -&gt; TypeId { TypeId::of::&lt;Box&lt;Scalar&gt;&gt;() } } #[derive(Debug, Clone, PartialEq, PartialOrd, Eq, Ord, Hash)] struct NDArray&lt;T:Scalar&gt; { pub rows: usize, pub cols: usize, pub data: Vec&lt;T&gt;, } I see you already need dynamic field access, so you are wasting all that structs? 
I didn't use `PrimInt` because I didn't know about it. Looking at it now, it doesn't do what I need for `Delta`, which are some pointer operations. I know about `pointer::offset_from`, but it is nightly only and it would drag the entire crate into nightly which is undesirable. Once it stabilizes I will switch over to using that instead.
It gives ability to produce and process brains based on neurobiological approximation of brain activity - its main goal is to give you a brain that adapts to body and environment via evolution (mutations and offsprings). in big picture.
At this point I'm still just trying to learn Rust, but I wrote a blog post about my experience so far: https://link.medium.com/7n4fngdyYU I'd like to get more involved in the Rust community as well, so I'm looking for projects I can chip in on.
Ur lost
[https://github.com/chasinglogic/taskforge](https://github.com/chasinglogic/taskforge) &amp;#x200B; Working on my task management application. Trying to add better install methods and a Github issues backend.
What do you see when compiling with `RUSTFLAGS='-C codegen-units=1 -Z time-passes cargo build`? You will need a nightly compiler to do this. I would expect most time to be inside 'codegen' or 'LLVM passes', but perhaps you're hitting some weird edge case.
Sorry, I use nightly so often I sometimes forget there's a stable rust at all. 
Yeah, so do I for most of my projects, but I wanted to keep this stable
If you use `Tally::&lt;_&gt;::new()`, it works. Don't know why it's not able to infer the `T`, though.
&gt; One thing I did in my (unpublished) version was make `RelPtr` take a pointer type as the type argument. This was inspired by the Pin wrapper and let you treat the pointer as though it was a smart pointer of the given type. That's cool! I hadn't thought of doing it that way. Was your api mostly safe?
/r/lostredditors
Little did they know it would let them abandon the GC completely.
Cool. I've been freelancing for the past year and a half, and this would have made my life much easier. Cloud Jira. Hosted Jira. Trello. Github. Gitlab. Google Calendars. All in need of integration.
Great, I have access to hardware running macOS, Linux and Windows :)
Where is it useful? I can't think of a context.
 This sub is for the Rust programming language, you are looking for r/playrust
&gt;I've tried opt-level = 0, so this is not about code optimizations. So, your expectations are wrong here. There are still convolutions are lowering being performed, even `opt-level=0` doesn't mean _do nothing_. DWARF information has to get written, and several Gigabytes of data are being passed between `rustc` and the `llvm` backend. 
Is your programming language public?
Amazing! FYI, I just moved cli app to another [crate](https://crates.io/crates/battop) and [repo](https://github.com/svartalf/rust-battop).
"Soundness of the type system" sounds like a thing egg-headed academics worry about :) [Java and Scala are unsound](https://2016.splashcon.org/event/splash-2016-oopsla-java-and-scala-s-type-systems-are-unsound-the-existential-crisis-of-null-pointers). Ocaml definitely was unsound before the value restriction, and I guess (it's not like I keep up to date about these matters) that there's still the one or other hole in the overall soundness proof. Neither fact stopped anyone from writing things in them. Haskell might be sound, but once you start to use extensions all bets are probably off. With things like C and Pascal the question isn't even properly meaningful because they provide so many legal ways to shoot yourself in the foot that hunting down arcane loop-holes doesn't seem that productive. Coq is quite sound indeed. Maude is sound, datalog is, as far as those two can be said to have type systems in the first place (they do have awfully nice theories, though). Agda is probably sound. I wouldn't want to write a game in any of them, though. Oh: Lisp is sound :)
Is there any way to break this apart into separate crates and compile each one individually? And if possible, only re-compile the crates that change when you alter something?
We have considered relative pointers and don't believe that it is a feasible implementation strategy for async/await. https://boats.gitlab.io/blog/post/2018-01-30-async-ii-narrowing-the-scope/
Not quite. It's on gitlab publicly, but I'm not posting a link because it's just not ready yet. There's almost no documentation there
Here's something to consider: 1. It's a bad idea to add different types unless one of them can be constructed from another. So, for instance, because Kilometers can be constructed out of Meters it's okay to add the two, but because Kilometers cannot be constructed out of Seconds, it's a bad idea to allow a generic function to add kilometers and seconds. 2. If you add type A and type B, make sure the operation doesn't return an underlying type. 3. You can't implement `From&lt;Foo&gt; for u32` where `Foo` is a type you defined. Considering all this the least wrong example out of the ones you provided is probably 1, but only because rust's developers have made sure not to implement things like `From&lt;u32&gt; for u8`.
The implications of unsoundness may vary from one language to another, though. In Rust, the soundness of the type system is the lynch pin of the memory safety; it would significantly alter the value proposition of Rust if its type system was unsound in a way that allows Undefined Behavior in safe code. It would still safer than C or C++, but still.
According to Google, you seem to be looking for Extra Sensory Perception. I am not sure there's a crate for it yet, though, so I am going to close this topic until you further clarify your question. (Or maybe your were looking for r/playrust?)
Rustc is an llvm frontend which currently tends to generate a *lot* of IR from its source. It may be worse than other languages. You may have luck separating the glue logic into multiple crates and generating `pub use` statements to pull them all together. I would also consider not implementing the glue in Rust if that's possible. (I haven't taken a close enough look to know.)
Thanks, and nice hindsight! :)
Does it make sense to have a Future reading user Input ? I am using Tokio to listening for connections to the node, but I wished that I could mantain a basic cli so that I could control what to send and such. Does it make sense to spawn several Futures one of them for listening and another to mantain this basic cli ? Am I making any sense ?
Agreed, I don't understand what it is from this post, either.
Ok, that makes sense. `RelPtr` doesn't capture the full picture.
The way of the future
Using macros is extremely pleasant. Writing them is a major struggle. I at least vaguely wish all macros were procedural macros. I'd rather reason about compile time rust than compile time macro syntax. It's annoying to have to learn another language.
That's very useful feedback - I've made a note of it! We intend to also do a detailed tutorial on FreeCodeCamp pretty soon on a similar topic - we'll try to adapt that one to a style where we educate Rust users on what Now is :) A brief explanation: Now is a service that lets you focus on writing and deploying code functions (lambdas) and relieves you from having to manage servers. Lambdas only get invoked when they receive a request, so they scale better and are often cheaper :) Now's landing page has more info: https://zeit.co/now
Yeah, we might want a clippy lint against this...
Hmm can you give an ELI5 explanation to an ELI5 example that shows why macros are awesome. I don't even know what the difference between a macro and a proc-macro is. Why would you use macros instead of just actual Rust code or a Rust function?
Simple case for println: rust functions can't have a variable number of arguments passed to it. It's kinda important for println to be able to print any number of things.
Macros allow you to write functions that takes an arbitrary number of arguments, something that rust functions cannot do. Other uses include stuff like Domain Specific Languages that compile down to rust code, which is basically what `println` is doing -- it's a tiny DSL for formatting strings. Another example is taking DRY as far as possible, macros allow you to generate boilerplate trait implementations or functions for multiple types with just one invocation. I've just listed some arbitrary things you can do with macros, but generally they are awesome. Although I have to admit I suck at the macro syntax myself too...
&gt;Capn is has similar use cases then proto but the amount of unsafe code in the rust lib should make anyone think twice. That's not really true in my experience. Protobuf requires an explicit parse step in order to do anything with the data, and doesn't support arbitrary seek within the encoded blob. The format in theory makes it possible for the parse step to operate on the underlying data in zero-copy fashion (like the protozero C++ implementation does), but as far as I can tell, none of the Rust implementations support that. Either way, it's critical for many applications that use especially large packed messages to be able to get the 18,000th record in a list without walking through the first 17,999, and capnproto doesn't accommodate that need at all, which is why a lot of the newer formats that can be operated on in-place became popular in the first place. That maybe doesn't matter for RPC, which it sounds like is your need, but there are lots of use cases for these tools besides RPC. &amp;#x200B; All of that said, I also had some recent frustrations with the Rust capnproto usecase. I had exactly the need described above though: I needed to do random seek on memory-mapped encoded data without having to read all of it into memory. I ended up using flatbuffers instead, which met that need (and is also maintained by Google, just like protobuf).
I understand the DSL part, but not this part: &gt;Macros allow you to write functions that takes an arbitrary number of arguments, something that rust functions cannot do. What is so hard about `println("Hello world")` Is it because of using placedholders like `{}`?
Hmm ok. So that means all other programming languages either can accept an arbitrary number of arguments or use something similar to macros as well. Is that correct? And out of interest: How does C do it? How does C++ do it?
Why not a third way, make NonogramPrinter implement From&lt;&amp;Nonogram&gt; and copy what it needs for the display ? Like a snapshot
I'm just now realizing I did all my work on the playground and never saved it (anywhere I can find), so I'd have to reimplement it. -_-
Unix sockets exist in the filesystem, so normal file access permissions apply. If you set umask(077) before creating the unix socket, it will be created with mode 600, and only processes running with your user-id will be able to connect to it.
Thanks, I'll definitely try it.
In addition to the things other people have said here, println! is a macro for another reason as well. You can only use a statically defined string as the format string (first argument) to that macro. The string must be fully defined and known already when the program is compiled. That makes it impossible to write the (common in C programs) vulnerability where a program uses user input directly as the first argument to printf and thus accidentally lets the user read and write to arbitrary locations on the stack, or at least cause the program to crash. In Rust it's impossible to write a program that uses any dynamically defined string (for example user input) as the first argument to println!, avoiding these kinds of errors.
Exactly. For example, you can write `println("{}{}{}{}{}", 1, 2, 3, 4, 5)` and `println("hello world")`, and both invocations would be valid. In fact, `println` can handle any input as long as the amount of arguments matches the amount of format braces (`{}` and friends). You cannot write a function that would do the same thing, since Rust lacks support for variadic arguments.
Not all other, but yes most do. Rust is a bit of an exceptional case in that it does not. It comes up less often than you'd think, and it's a surprisingly tricky topic to specify and implement, but I do wish Rust had something like it outside of the macro system. 
The biggest usage is reducing boilerplate. Say you have a lot of `enum` variants which map to values, and vice versa. You want to implement tests, and traits based on this. [Macro are awesome for this](https://github.com/ctz/rustls/commit/cf293724ec55fdc69da3a5db3bde19880299ea79) as they let you remove a lot of super redundant code as you can just use repeat patterns `$( $Value =&gt; $TypeName::$Variant),*` More advanced example where a [macro](https://github.com/valarauca/elrond/blob/master/src/macros.rs) is used to generate `enum` declaration AND parser functions AND traits for `AsRef&lt;T&gt;` style inheritance. [usage](https://github.com/valarauca/elrond/blob/master/src/magic/class.rs) They're something you need to get acclimated too. I found myself hating them early on, but warmed to them with time as they let me write more expressive code without repeating myself unnecessarily. --- I do not recommend `proc-macro` for this because: 1. cargo still [doesn't support `proc-macro` fully](https://github.com/rust-lang/cargo/issues/5730) 2. `proc-macro` entails writing a seperate crate, which creates another project to manage 3. `proc-macro` imports a significant part of the `rust` parser, and dealing with all optional cases can be an extreme PITA as you add options, and configuration details. The lack of higher level libraries to bridge the "_type defination AST_" and "_output AST_" is a big problem, and `quote!{}` is often in adiquate especially when dealing with lifetimes. 
&gt; I still not believe that 5MB is that much code. 500MB? sure. Exactly, that's why I started this thread. Actually, I know a few tricks how to make code smaller, it will help, but the problem is that the code I generate right now is just a small fraction of code I want to generate, so in the end, it will be much bigger, not smaller. If rustc can't handle 5MB it would not handle 50, so why investing in development? &gt; ​P.D: You can control how much emit and see when start to get unreasonable slow? Emit 70%%, 50%, 25%, etc. Also maybe put this in a gist so you can report it to the rust team Yes, make sense, I probably could replace domain types with basic uints and ints, maps and vectors. But right now I not sure rust team is interested in it.
Are you doing any hairy type inference any where? I have seen it eat mnay GB of RAM - but only when doing really really deep recursive structures.
Not only does println take a variable number of arguments, it can access the format string, determine how many extra arguments it needs, and reject (at compile time) any invocation with the wrong number of arguments (or arguments that don't implement the right formatting traits). Note also, that the initial argument, if provided, has to be a string literal, not a variable containing a string. At runtime, rust can't tell the difference between the two, because it is a feature of the syntax tree, and not a feature of the compiled binary.
just a shot in the dark, but usually when I have `T: Deserialize` and then `T` is in the return value, I have to do: fn unfry&lt;T: for&lt;'de&gt; Deserialize&lt;'de&gt;&gt;(...) -&gt; bincode::Result&lt;T&gt; Have you tried that?
``` !dbg(help_this_variable_is_broken) !dbg(x + 4) ``` That's a pretty sweet procedural macro, for example. It annotates your debug output with the line number of the debug statement, and it prints the output as well as the statement itself. You wouldn't be able to do these things post-compilation (so this can't be implemented as a function), barring some very specialized debug symbols and support for strange post-compilation introspective functions upstream (since the compiler would have changed the line numbers and exact syntax of your statement). This is possible in interpreted languages, mind you, since by definition they need to know where they are and what the exact syntax of your code is - but I digress. While it's impossible to implement satisfiably as a function, it is an extremely *ergonomic* syntax for quick debug prints. I don't need to think about my output format, but I still get all relevant output - that I'd normally need to add manually. Macros allow writing "functions" like that that use the syntax of your code to make it more expressive, before the compiler has a chance to garble it all and hide a lot of things programmers could otherwise see and use. On a side note, procedural macros are simply macros that are defined by the execution of a function at compile time, if I'm not mistaken. That is, you declare them with a function, instead of a macro_rules statement (I'm not *sure*, I've only really tried writing procedural macros).
So much faster than travis!
You can make the function have whatever signature you want and then make the body `unimplemented!()` with an optional string, which will panic when reached at runtime. Is that what you want, so you can later come in and fill in the body?
Great read! Thankfully the rust team is working on a macro keyword and improving the syntax on it too.
You could declare them in a separate crate and rename the crate on import with "extern crate blah-test as blah". Then you can swap out test impls for real ones in one line.
This is nice as well I think!
When it comes to C and C++, it's seemingly done with compiler magic under the hood and you utilize it via some special macros exposed in `stdarg.h` like so: https://jameshfisher.com/2016/11/23/c-varargs.html But this is C we're talking about so there's no type checking at all on the variable arguments, and getting the types wrong can result in undefined behavior. Which brings up another advantage to println! being a macro instead of a function. The format arguments passed to the macro get validated at compile time, which means you can't ever get the types mismatched.
I really like the fact that the lambdas just use the `http` crate instead of being cluttered up with a ton of Now-specific code.
Traits are what you're looking for. Think of them like interfaces/abstract classes if you're familiar with java. You can have one struct implement the trait with the proper implementation and another that's a mock implementation for testing.
Does `writeln!()` guarantee that all bytes will be written? With sockets there is generally no guarantee that `write(2)` (POSIX system call) will write as many bytes as requested. I found `write_all()` which saved me from using a while loop. As for using `by_ref()` I don't think I fully understand why, but it's the only way my program compiles. The signature of the function (just a bit above the code) is the following: fn send_file(stream: &amp;mut TcpStream, fd: &amp;FileDetails) -&gt; Result&lt;(), std::io::Error&gt; Declaring stream as mut is messing everything up; in general I have given long fights with the borrow checker to be able to both read from a BufReader and to write directly to the same socket.
I was thinking more along the lines of [typescripts declare keyword](https://www.typescriptlang.org/docs/handbook/declaration-files/by-example.html#global-functions)
That's what I was originally thinking but what about associated functions that are related to the trait but doesn't really need a struct. As far as I'm aware you cannot declare an associated function and them implement it later. Oddly the following doesn't result in compile errors: ``` trait JSON { fn dumps() -&gt;String; } ``` But you also can't really implement that later
Looks like a yes, it's basically a wrapper around `Write.write_fmt`, which according to the [docs](https://doc.rust-lang.org/std/io/trait.Write.html#method.write_fmt) uses `write_all` internally. If that's your signature, then stream is already a `&amp;mut W` where `W: Write`, which is what you need for `copy`. I think you can just do `copy(&amp;mut reader, stream)`. 
(you did a typo.. `!dbg` should be `dbg!`)
to the one who deleted the comment. that did it. thank you. I think this is the related part in the book. but i barely understand what's written [https://doc.rust-lang.org/beta/book/ch19-02-advanced-lifetimes.html?highlight=advanced,lifeto#advanced-lifetimes](https://doc.rust-lang.org/beta/book/ch19-02-advanced-lifetimes.html?highlight=advanced,lifeto#advanced-lifetimes) &amp;#x200B; i only see lifetime declarations.
Yes. `println!()` as a macro allows two things you can't get with a function: 1. An arbitrary number of arguments without first bundling them up into some kind of iterable and passing *that* as a single argument. 2. Because it's a *procedural* macro, compile-time checking that the arguments are correct for the given format string.
Gah!
Thanks, it works!
I'm having the same issues on Safari 12.0.1 on mac as well, but works with Firefox. Interesting project though!
I hope I understood you correctly. `macro_b!(macro_a!())` will expand B before A. B will see an actual invocation of A.
Not sure what you're trying to say exactly but this works for me. trait JSON { fn dumps() -&gt; String; } struct Struct {} impl JSON for Struct { fn dumps() -&gt; String { unimplemented!() } } fn user&lt;T: JSON&gt;(arg: T) { } fn main() { let s = Struct{}; user(s); } 
Ah, my bad - it would be a great fit for allocations profile, but since the heap dump is a cyclic graph rather than a tree, flamegraphs are not applicable here.
I don't think /u/jcdyer3 was dismissing Now as not being useful to him. It was rather a criticism of the blogpost, not the tool.
Thanks! I'll take a look. 
That's a nice quine there!
After working with AWS Lambda and Python for a bit I just assumed the primary incentive for companies to *provide* serverless platforms will be the lock-in of customers. I'm delighted to see that I was wrong.
The generic term is [Variadic Function](https://en.wikipedia.org/wiki/Variadic_function) if you want to look them up on Wikipedia, but you'll need to do some extra research on each language to see what the underlying implementation is. (eg. Python's is syntactic sugar for putting the variable arguments into Python's equivalent of a `Vec`, a `HashMap`, or both, depending on what syntax you use.) C's approach, which people refer to as [varargs](https://jameshfisher.com/2016/11/23/c-varargs.html) does it using something that's so type-unsafe that it's probably only exceeded by assembly language. You tell the compiler that a function should have variable [arity](https://en.wikipedia.org/wiki/Arity) in the function signature, then you're on your own. You get a C macro named `va_arg` which lets you pop off another value and it's up to you to: 1. Make sure that you cast each popped-off value to the correct type. 2. Make sure you don't walk off the end of the list and start misinterpreting unrelated memory addresses. For a long time, C++ had two choices: 1. It could use just the `va_arg` approach it inherited from C. 2. It could use a class and define a method for each arity you want. (eg. `myObject.function(int)`, `myObject.function(int, int)`, etc.) Under the hood, the compiler does a transform which is equivalent to if you'd written `function_1(self, int)`, `function_2(self, int, int)`, and so on. The more recent options are variadic templates and initializer lists, which, in very simplified terms, are macros and syntactic sugar for building a `Vec` and passing it in as a single argument. See things like https://stackoverflow.com/questions/1579719/variable-number-of-parameters-in-function-in-c for more info.
If you are able to post the results, I'm sure people would be interested. I know I would be :-) And it might help find a solution.
I was originally trying to do: ``` fn main() { JSON::dumps() } ``` And have a mock implementation somewhere with a real one later. I was just trying to do it this way because it didn't really need to be associated with a cluster of data like an `impl` block implies
In the examples serde_derive is required in Cargo.toml ? I thought serde_derive is exported from serde. And can you not import Serialize and Deserialize directly from Sweden instead of serde_derive? 
&gt; The thing that worries me about this is what happens when you have a library which depends on an ancient, broken version of some other library which also has a vulnerability. [cargo-audit](https://github.com/RustSec/cargo-audit) is designed to search for vulnerabilities like that.
On my phone right now so can't really test it but maybe something like ``` struct JSONMock { fn dumps() -&gt; String { "Hello" } } fn main() { JSON::dumps = JSONMock::dumps; println!(JSON::dumps()) // prints Hello } ```
Sorry, I meant that `macro_a!` would be invoked earlier in the code than `macro_b!`, not that `macro_a!` would be used as an argument to `macro_b!`.
One more thing, I understand being cautious and all, but isize literally says that it's big enough to `to reference any location in memory`. If you somehow manage to overflow that, then you're not pointing into addressable space anymore. :P
Regardless, it’s all good :)
&gt; Yes, make sense, I probably could replace domain types with basic uints and ints, maps and vectors. But right now I not sure rust team is interested in it. Interested in what? 
Yeah it does seem kinda hacky to have to have a struct but afaik an empty struct takes no memory so there's no loss in performance over it. Looking around now, it seems an empty enum is an even better way to go seems as you can never construct an instance of it. Then you can use generics like this: trait JSON { fn dumps() -&gt; String; } enum Enum {} impl JSON for Enum { fn dumps() -&gt; String { unimplemented!() } } fn user&lt;T: JSON&gt;() { T::dumps(); } fn main() { user::&lt;Enum&gt;(); } 
Actually, `usize` address all bytes in memory, `isize` does not, it can only address half of addressable memory. from the docs &gt;The **size** of this primitive is how many bytes it takes to reference any location in memory. For example, on a 32 bit target, this is 4 bytes and on a 64 bit target, this is 8 bytes. (emphasis mine) it states that the size of the primitive (not the primitive itself) is how many bytes it takes to reference any location in memory.
I don't think you should rely on that, but I currently can't come up with an example where it matters either. For your example: Names are resolved after the whole macro expansion anyway.
I'm a contributor to it -- currently pre-alpha. It *works*, should be able to run most simple scripts, but it's missing most of the standard library and crucial stuff for applications like `async`/`await`.
You *can* mix regular trait bounds and impl sugar - but when using such functions that you cannot specify generic parameters with turbofish.
Neat! Awesome to see RustPython being used in-depth for a project!
Is there a performance penalty using a std::sync::RwLock when I know that 99.999% of the time the data only has readers, and no writer? As opposed to a situation where no writing ever occurs and I just share the data through borrows.
Something to note that I realized after the fact is the `pointer::offset_from` is that it divides the distance by the type's size and truncates, meaning it wouldn't actually work unless the structure's alignment is a multiple of its size, or you do as you're doing as just pretend they're byte pointers.
I am just going to pretend that they are byte pointers, anything else will be too restrictive. `pointer::offset` does the same thing, which is why I am using byte pointers right now.
In practice, if you want to use constructors with different input types, you would just use the enum constructors directly. To construct an `A`, just call `Z::A`, etc.
&gt; How is it with Ownership. I've been trying to dig some historical sources, although have to admit didn't do enough research. That being said, if it was never implemented maybe there are sources which proposed the model in theory years ago? The keyword you're looking for is "sub-structural type system". As /u/PthariensFlame noted, Rust's ownership system is an application of *affine types*. Which has probably been less common in pre-existing languages than its stronger cousin *linear types* (the difference between the two is that a linear instance must be used exactly once, whereas you can just "not use" an affine type instance).
IIRC it's a common issue in things like video games where you might have pretty deep structures (the gamestate / worldstate) but you want easy and fast access to some of the sub-structures, having to traverse the entire thing all the time can be way costlier than you can afford given your budget.
Eh, that stuff shouldn't be _too_ important for game dev though. I'll definitely watch that project.
I'm not familiar with that macro...
Actually, they did, at least since the early 90s, according to the [Lively Linear Lisp: “Look Ma, No Garbage!”](https://www.researchgate.net/publication/262403318_Lively_Linear_Lisp_Look_Ma_No_Garbage) paper ;) 
It depends what you mean. If you use the variable name _ in your function then yes you'll never be able to access it but you will still take ownership. If this is a trait then the implementer can always rename the parameter when they implement the function.
I recently switched from LanguageClient_neovim to using coc.nvim as an LSP client for RLS. It works a treat if you want code completion as well :)
thank. I see. In my case it's a fn of a trait, i am impl that trait but actually only needed to have to know ``T`` like in macros: ``$t:ty``. ``_:T`` But I figured that it has some drawbacks at least in the situation for my lib, because unlike in macros I have to create an instance of T, or pass an existing one, only to pass a type Type. But you are right of course that any implementor can name the formal args as desired. 
That would be good except it also needs access to the original Nonogram to print out the current state of the puzzle (i.e. as the user makes progress in solving it). So it needs both cached information *and* a live reference.
Maybe this is relevant to you: https://old.reddit.com/r/rust/comments/azy15o/is_there_a_way_to_declare_functions_in_a_module/eib0c2d/ Although disclaimer I'm not the best so don't trust everything I say :P
All you are doing is essentially incrementing a atomic number. So no. Though I'd recommend parking_lot rwlocks since they are generally better
&gt; No REPL: this may be unfair, since there’s no decent C++ REPL either, but a lot of languages come with a REPL these days. There’s an open issue on GitHub about this. A good REPL is not necessary, but would be helpful. I find I want a REPL for one of two reasons - Experimentally determining an API - This is more of an issue in dynamically typed languages. With static typing, rustdoc is pretty good. - One-off coding, whether for experiments, bug reproductions or otherwise - Playground helps, if the needed crates are in it - [cargp-script](https://github.com/DanielKeep/cargo-script) seems to be a great way to handle this. So far I've only used it once for bug reproduction. I need to get more in the habit of using this.
&gt;Reply If you want to be friendly to other applications (and you have time to burn in your main loop) you can run sleep(0) or I suspect SDL\_Delay(0). This lets the OS schedule other threads to run, without actually sleeping your thread in the scheduler. &amp;#x200B; Most engines will have a physics tick rate of \~30 fps. Some a logic tick rate that's even lower than that \~10 fps for example. If you want ultra-smooth visuals, you can double buffer the data for your rendering phase and interpolate between two ticks, but it's also pretty reasonable to just spin in a loop until enough time has elapsed to draw again.
I just read it as well! I especially like the summary at the end that explains the different semantics when using the different type of traits. Thanks!
Not much. All that's happening is incrementing an atomic usize and doing some checks. 
Its worth noting this doesn't have a safe API: dereferencing the relative pointer is unsafe, and to expose that safely from your code (as in the example) you have to guarantee both that `set` is only called with something guaranteed to stay in a certain relative position (that is, within the same struct) and that the relative pointer itself cannot be moved around out of its containing struct. I don't know any way to avoid this unsafety in a library, but this makes the offset conversion convenient, not safe.
Just to be sure if I understand correctly. The function that uses Cow needs to be inlined as well right? Let's say we have a function a calling function b and b takes an Into&lt;Cow&lt;'a, Something&gt;&gt; then the function b itself does not know whether it can move/clone the results right? After all that depends on the parameter to be passed. So am I right that it can only be optimized if function b is completely inlined?
The downside of `impl Trait` in argument positions is that it doesn't work with turbofish syntax (e.g. `my_function::&lt;SomeType&gt;(...)`). I personally prefer the type parameter form since I have a tendency to push type inference to its limits and turbofish syntax can come in really handy when the compiler needs a hint.
Acquiring and releasing read lock guards will have a performance penalty, which is dependent on the OS's implementation of rwlocks. This is likely to be relatively small. On the other hand, reading from an already aquired guard has no penalty. The standard recommendation in practice is to benchmark your expected workload and compare the two.
&gt; serverless platforms will be the lock-in of customers. I'm delighted to see that I was wrong. We currently have some types that are Lambda specific in the Rust time (e.g., a workaround on [std::intrinsics::type_name](https://doc.rust-lang.org/std/intrinsics/fn.type_name.html) being unstable; a custom context object), but we don't intend to lock you into serverless platforms whatsoever—I think we're just at a stage where the entire paradigm isn't 100% mature and there are multiple viable designs. I'd _love_ to push more and more stuff code into standard libraries and interfaces. 
&gt;I probably could replace domain types with basic uints and ints, maps and vectors. I also remember that use type alias help. &amp;#x200B; I put at the top things like: type Column&lt;T:Scalar&gt; = Vec&lt;T&gt;; type Row = Vec&lt;Rc&lt;dyn Scalar&gt;&gt;; For documentation and also for speed..
I like how relatively simple your pricing is. It would be interesting to highlight the differences with AWS Lambda, which is the elephant in the room for this type of thing. From what I can tell, you're cheaper for small, fast requests, more expensive for long-running requests. Anyway, I may actually consider using the service because of Rust support. Thanks for the info!
And I'm actually considering going serverless for a project now, so that's cool.
This is interesting. If Rust had a const marker for fun arguments, could this be avoided? What the prior art in other languages, or they just depend on run time checking of string length?
I was more thinking of having Nonogram and NonogramPrinter (but not a printer) separate [like this](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=6c8a886c3309c37544957693ff33345e) You can keep one or multiple snapshots and print them as long as you want and then throw them away, or compress them or just store them. If you want to print a new state, just make a new snapshot.
My point was that soundness of the type system, at least the complete type system, is not generally a thing that's done beforehand for programming languages. Ocaml had quite a gaping hole, and for the longest time Haskell didn't even have a formal semantics, and that's with languages which actually *do* have their roots in academia. I'm only measuring Rust by the same yardstick. While things may be difficult to nail down with absolute certainty, and that is/was certainly also the case for Haskell and Ocaml, if unsoundness is going to be found, it's going to be very interesting from the theoretical perspective, and utterly contrived from the practical perspective, as in: You're more likely to stumble over a bug in an unsafe block in the stdlib than trip over unsoundness on accident. What I'm actually objecting to, I think, is calling Rust's challenge to theory a theoretical contribution: Rust was, on purpose, made up of off-the shelf, well-understood theory... arranged in a novel way, and that's where the challenge comes from. But it wasn't designed to be challenging, it wasn't designed for theoretical purposes, but according to engineering principles.
This is promising, however, I do not think it works for my use case. The ideas I have is, that I can process an arbitrary struct, it would be too complicated to impl that trait for every struct A. If there's a solution I can derive a trait that calls the function of another trait, that would be ok.
It's sort of new. Very ergonomic idea for debugging, though i'm sure some architecture astronaut library will create a version that sends to their specific logger instead of just stdout.
Note: I'm not a heavy Rust user, so if you have any interesting usecases, please comment! Originally this tool was built for Haskell, but I'm curious whether it can be useful for Rust, too.
The 'macros 2.0' RFC [came out](https://github.com/rust-lang/rfcs/pull/1584) in 2016 and [was accepted](https://github.com/rust-lang/rfcs/pull/1584#event-942021286) in January 2017 (tracking issue [here](https://github.com/rust-lang/rust/issues/39412)). The short answer to all of your questions is "yes", thankfully.
People jump to macros too soon I find, debugging them is a bitch. If you write macros then please only use them for code which can’t fail; keep them simple. 
Yeah, lucky timing for me: as far as I can tell it's the [playground itself](https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2018&amp;gist=b2efbe9a35495613f9e61317129fb9f8) that's the issue.
You do not see much desire to program! [https://github.com/dgriffen/wearte/commits?author=dgriffen](https://github.com/dgriffen/wearte/commits?author=dgriffen) 3 commits in 15 days. Good! A class? I can explain it to you if we manage to keep it. I thought it would be more fun, but you have not even tried it. Totally regrettable. &amp;#x200B; LLLLLLLLLLLLLLLLL
Isn't this already possible with ripgrep? e.g. rg --json --byte-offset --null-data --multiline '\x00\x01\x02' fname (seems to hang with stdin but works with a file)
We're using double ratchet in [Wire](https://wire.com/en/) and our Rust implementation is open-source too: [https://github.com/wireapp/proteus](https://github.com/wireapp/proteus). You might find it useful, perhaps.
&gt; import Serialize and Deserialize directly from Sweden It's probably autocorrect speaking, but I can't pass it without giggling.
[This](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=fec47c16a9604c0c16acae6c04f116d8) says otherwise. ;) Fun fact: Did you know that on x86_64, addresses are actually 48bit *signed* integers? `0xF000_0000_0000_0000` is not a valid address, nor is `0x0FFF_FFFF_FFFF_FFFF`. The address after `0x0000_FFFF_FFFF_FFFF` is `0xFFFF_0000_0000_0000`.
Oh, cool! That doesn't change how safe my api is, because accessing random memory is UB. As is accessing freed memory. The only reason to use smaller integers than `isize` is because you want to make `RelPtr` compact.
Absolutely agree. This is Rust after all; you can't have too much safety. :D
So open source AWS Lambda?
I can only speak for the specific usecase I had, which was deploying HTTP handlers using Amazon's Python runtime. There, a standard format for passing request data 100% exists (WSGI). I realize this is becoming very off-topic for this subreddit though.
Yes, although you will want to add `(?-u)` to the beginning of your pattern to make sure hex escapes are treated as raw bytes and not Unicode codepoints. Multiline requires reading everything into memory or memory maps. So when you use it with stdin, it has to block until it consumes all of stdin. Also, if you use multiline then I don't think you need null-data for this case.
The best way to implement linked lists is don't.
Cool, no one cares until you are. Now, care to explain why you've posted this on r/Rust?
I agree with that.
`use std::collections::LinkedList` and call it a day.
I'm not sure I understand quite what you want, but I wrote some code to demo how traits can be used for what I think you want. https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=e5aa9ba348f03086cad1a50cc651d01c Specifically, look at the different ways the `zero` function can be called.
You have a Markdown error that leads to bad formatting, a `'` that should be a backtick.
Why is this piece of code unreachable? let video_base_url = url::Url::parse(&amp;master_url)? .join(&amp;elt.base_url)? .into_string(); ______________ ^ In case the format is messed up, the cursor is under the semicolon. What I want is to take a master URL as a String, parse it, join it with another String and finally convert the result to a String. What am I missing?
Finishing a webdav protocol handler library. Internally it uses futures 0.3 + async/await, the external interface is futures 0.1 and the http crate. Implements all of the webdav RFC (RFC4918) and passes the webdav-litmus tests.
No: the shape of the arguments is determined by the format string. Deeper type-level programming is required, which Rust doesn’t support. Macros are the closest you get to it. e.g. `format!("{}, {:?}}", …)` would mean that you needed exactly two more arguments, with the first implementing std::fmt::Display and the second std::fmt::Debug.
https://youtu.be/YQs6IC-vgmo
So does this guy: https://youtu.be/YQs6IC-vgmo
Interesting! I too wrote a tool in Rust to work with binary files. This one allows you to do 'sed'-like editing on binary files: [https://github.com/whh8b/bed](https://github.com/whh8b/bed) &amp;#x200B;
String length isn't the only concern, at least for a C-style printf function. What many people don't know is that C's printf actually lets you both read arbitrary stuff off the stack (by putting lots of format specifiers in the format string) and write to certain locations in memory. There is one format specifier that writes the current number of outputted bytes to a location pointed to by a pointer argument to printf, which means if you control the format string to printf, you can actually write semi-arbitrary values to locations that have pointers to them on the stack. There have actually been real-world exploits based on this kind of misuse of printf (when a program just uses user input directly as the first argument to printf).
Ah, yes, but i was thinking of using a macro inside the function, but i understand why that's not viable now. I was thinking const fn println(fmt: const &amp;str, [imagine if rust has variadics here]) then inside use the a macro to match or not the fmt to the list of arguments, but i understand why not now (println can't be const, even if fmt can; because the rest of the arguments aren't by definition const). Ie: make 
Still, there's an entire chapter in the book about linked lists :)
I can see the influence, especially with regards to hygiene, but at this point I'm more narrowly interested in how the compiler picks expansion order. In *that* regard, Rust looks quite different to what I've encountered so far. In nightly Rust, we can do invocations before definitions: foo!(); macro foo() {}; I *think* the equivalent in, say, Clojure would be: (do (foo) (defmacro foo [] ())) Which fails with the expected 'could not resolve `foo`' error. You could replicate *something like* Rust's expansion order by repeatedly calling Clojure's [`expand-all`](https://clojuredocs.org/clojure.walk/macroexpand-all). Is this indeed the inspiration for Rust's expansions? How do the other members of the Lisp/Scheme family compare?
The typically approach to this to pass a `pub trait MyTrait` which has a method (without a body) that encapsulates its contract. 
Cool! It is very welcomed to have a more platform to use Rust product in web dev. I heard a couple of month ago AWS lambda is available to use the custom runtime with almost any languages including Rust. It' good news to have a Now can support either. Thanks for your work, Zeit team.
&gt; It can't be implemented without special access to the compiler. Cell and RefCell fall in this category (although both actually rely on UnsafeCell which is a lang item).
Doesn't answer your original question, but for json in Rust you would want the [serde](http://serde.rs) crate. It's awesome.
You can, but that's a more recent development than doing it from serde_derive. I'm guessing the author just wasn't aware you can use serde directly. I didn't know until fairly recently.
Things that can slow down the build: macros and generics.
What's the rustic way of doing this and also in parallel? let responses: Vec&lt;reqwest::Response&gt; = elt .segments .iter() .map(|x| format!("{}{}", &amp;video_base_url, &amp;x.url)) .filter_map(|x| { let resp = reqwest::get(&amp;x).unwrap(); if resp.status().is_success() { Some(resp) } else { is_success.store(false, Ordering::Relaxed); None } }) .collect(); // If successful. for resp in responses { // Write to a file. } I'm trying to use the iterator version hoping that I can parallelize the downloading of the segments using rayon. Is this the right way to do this? How would one implement downloading multiple segments at the same time and eventually writing to one file?
Indeed it would be awesome! Very Erlangy but seems very complex 
Thanks! What does ‘::&lt;_&gt;::’ mean? How is it different than eliding types entirely? I’m trying to provide a nice public interface with these types, so requiring either a type alias or a ‘::_::’ seems less than ideal. Thanks for the help! 
The `_` is just a type you want the compiler to figure out for you; in the example, you could also write it as `Tally::&lt;&amp;'static str&gt;::new()`, but Rust can figure out from usage that `T = &amp;'static str`, so you can just say `_`.
Re RLS: try using IntelliJ IDEA (no debugger, but free) or CLion (with debugger, but paid) with Rust plug-in. They don't rely on RLS and are more stable.
Merging the RFC does not mean they will be stable in the next release.
Hey! You guys did next!
I think you're looking for /r/playrust
Thanks. Is there a reason why, for example, ‘HashMap&lt;K, V, S = RandomState&gt;’ works with a simple ‘HashMap::new()’ and the compiler will figure out that it should use ‘RandomState’ for S? But in my example you need to force it with a ‘::&lt;_&gt;::new()’ ? To me it seems like ‘HashMap’ and ‘Tally’ should be equivalent in this regard, but it’s obviously not the case! 
IntelliJ IDE + IntelliJ rust plugin
Ripgrep is amazing, but for binary data it almost won't do. One example: I want to identify the positions matching the \`0x00 0x00 0x01\` byte sequence in a file. Before writing bgrep, I've tried ripgrep for this job, but couldn't manage to get a combination of flags that would produce a nice output. The closest I got was the output containing the positions in decimal, plus the match payload. Usually, a pipe to \`cut -d ':' -f1\` would do, but the output is null-separated instead of line-separated due to the \`null-data\` flag. Then, I would also need a pipe to \`tr '\\0' '\\n'\`, but that was when I decided a simpler tool would be more fit.
Yup. We're cool. :D
Is `rustc` reporting that this code is unreachable? That error doesn't make sense here. Can you share a larger code snippet that gives more context?
Thanks so much for your input. This is one of those times where I realize how little I still know about programming.
Wow, thanks for this detailled explanation. This really helps. I still don't think I have a use case for macros, but at least I know more aout how they work now. Thanks again!
Thanks for clarifying! This actually makes sense. In Python land we just use `*args` for that. Just one last question: Why doesn't rust have variadic arguments? Speed? Safety? ...?
Yeah, I think it's good to understand the subtleties of programming but useless in 99.9999999% of real world cases.
The stabilization RFC is the one that just got merged, so yes this will come out in the next release
Nm - it was my fault - while copy pasting I had a return Ok(...) placed a few lines before that line by mistake. Apologies for the noise. 
&gt; Macros: WTF? Rust macros feel like a left turn compared to the rest of the language. Not at all! The thing that most makes Rust for me is how it embraces pattern matching, which is fairly unheard of in imperative programming. Macros are like the match and let expressions in this regard. TBF, the Author claimed to have not yet groked them. More exposure to functional programming in the general software engineering population would go a long way.
On the one hand, it's probably not ready to be stabilized, on the other hand, it won't be ready until it's stabilized so people start using it.
You can make arguments mutable and pass/return by value. fn foo (mut a : i32, mut b : i32) -&gt; (i32, i32) { a *= 2; b *= 3; (a, b) } fn main() { let (mut a, mut b) = (1, 2); (a, b) = foo(a, b); println!("{} {}", a, b); } 
Yep, that'll do it. No worries.
I’m confused as to how the index page gets rendered? I don’t see it in the example code...am I missing something?
You can do this in Racket but not in traditional Lisps for precisely the same reason as it works in Rust -- there's a single module that the expander can look at and find the macros before expanding them. In a system like Clojure's, where each form is expanded and evaluated before the next one is considered, this is much harder. Here's an example in Racket: #lang racket ;; start a module (+ 5 (foo)) ;; prints 22 (define-syntax-rule (foo) 17) ;; defines a macro Note that we need the `(+ 5 ...)` because `(foo)` by itself won't work -- it will get expanded too early. This is needed in Racket since macros can expand into other macro definitions and can't be recognized early -- in Rust the situation is a bit different and so your example works.
One other note is that if you're thinking about eager expansion, that isn't something that exists in any language I know of, but if you wanted to do similar things in Racket you'd use either `local-expand` (see this paper: https://www.cs.utah.edu/plt/publications/jfp12-draft2-fcdf.pdf) or the pattern described in this paper: https://arxiv.org/abs/1106.2578. I'd be happy to answer questions about how that stuff works, also.
It does matter in **procedural macros** with side effects (for example, if a proc-macro reads/writes to files). I'm currently working on a complex proc-macro, and in my experience the expansion order is the same as in the source file. Example: tokens_to_stdout!(Hello,); tokens_to_stdout!(World!); This should print: Hello , World ! But I do agree that you shouldn't rely on this, since it's not guaranteed and might change in the future. Also, I didn't check in which order macros in different files are expanded.
I believe it's a relatively recent development, and it also isnt done by default, [you need to enable the `derive` feature](https://github.com/serde-rs/serde/blob/master/serde/Cargo.toml#L33)
You know how return it?
I actually had that thought of HashMap when I saw your question. The difference is that `HashMap::new()` is explicitly for `HashMap&lt;K, V, RandomState&gt;` while creating a `HashMap&lt;K, V, S&gt;` requires supplying a BuildHasher for S. In the context of `Tally`, it'd be like having `new` only make the u64 one and then having a different method like `starting_with(start: C)` that takes something to clone that each entry starts at.
This is fantastic! I feel like this should be adopted by the infrastructure team and integrated with the rest of the Rust websites. Example integration use case: Go to a crate on crates.io and have a link to find real-world usages of that crate. The link takes you to codesearch, pre-populated with a search for `(name::|use name\w)` with an exclusion for `name`s own repo. As for feedback for aelve, all I can think of is it'd be nice if someone solved the versioning issue. I imagine codesearch is most amazing in a monorepo and it generally does well in a many-repos environment but there are times when you intentionally have an old dependency or you are patching a previous release and your search results are irrelevant because they are all for `HEAD` and some of your dependencies have gone through a major refactoring. No idea if the Rust community needs it (we seem to assume using the latest dependencies), what the indexing time, index size, or complexity of this would be but one idea to solve this is if you could index all versions but exclude all but latest version. From a user's perspective, they culd - Explicitly opt-in to an old version - Pass a `Cargo.lock` to configure your repos (search all of my dependencies, pinned to these versions) - Specify a crate and version, and search the latest version of all crates that have that in their Cargo.lock or maybe have a compatible Cargo.toml dependency?
I believe the most optimal thing would be to have an update(dt: f64) method called every time and a fixed_update() called whatever ticks we decide when starting to use physics. However I have no idea how to call update() everytime AND limit the fixed_update() calls at the same time. I was thinking multithreading but is there a better solution?
Thanks for explaining how you fixed the problem. Doesn't happen enough. 
Having played with it some more, some random thoughts: - I searched for [assert_fs](https://codesearch.aelve.com/rust/search?query=assert_fs&amp;insensitive=off&amp;space=off&amp;precise=off&amp;sources=on) and it reports 87 hits but the breakdown includes less than 50. I assume it is capped and there is a way to increase the cap but I'm not seeing how. - I'm assuming there is a way to exclude a repo but I'm not seeing how - I like that the repo name links to docs.rs. - It could be nice if the code view made an attempt to link to file in the repo. This might be imperfect though - Could the [file view](https://codesearch.aelve.com/rust/src/crates/assert_cmd/0.11.0/src/lib.rs?query=assert_fs&amp;L=77#) have the code parsed for identifiers and turn those into links to more searches? My company's internal code viewer does this and it is a nice workflow improvement.
Aha! That makes excellent sense. Thank you! 
From what I understand, RCU performs better when the access is really that intense, but it's harder to get right. I'd be looking for a crate with particularly clear code. RwLock (whether you use the os implementation in `std` or one that's almost purely user-space like parking lot or crossbeam) means that if you have multiple threads acquiring and releasing reader access very often, they'll all be writing to the same counter. And that's a potential bottleneck, especially on a strongly ordered architecture like x86. "Very often" of course means that the interval between events in different threads is, at slowest, about the same magnitude as the latency between cores. Think in the neighborhood of millions of operations per second concentrated on a single lock. If that could happen, it's time to start profiling. 
Same, with Rust specifically. This removes a lot of the "well how do I get the HTTP server/routing up and running in Rust..." Rather just write the simple endpoint code and be done with it.
Incrementing an atomic counter or any `lock` prefix is about the most expensive memory operation possible on x86. Only `mfence` is more expensive. The first is a `SeqCst` operation with a few rare exceptions related to instruction fetching. The second is a fully `SeqCst` fence and almost always required when a thread will branch into shared memory, a thread migrates across CPUs, and things like that. In both cases, all writes must be flushed from the core's pending write buffer before the locked operation can execute. If it is executed speculatively, a competing invalidation message will cause a miss-speculation and flush the pipeline. It's not *that* expensive, but you don't want to be doing it inside an 80ns loop. 
You could have NonogramPrinter take a reference to Nonogram at creation time, but not keep it as a field. Then, when you actually do the printing, you can take a reference to the Nonogram again.
I've not used the serverless features that Zeit/Now provide, but I've had good experiences with the company in the past. Plus they have a really cool (open source) website for doing a bunch of webdev/programming related transformations and conversions that I use regularly: https://transform.now.sh/ I'm happy to see them adopt Rust as a primary language for their platform and may try it out for a project sometime.
Rust plugin for IntelliJ works fine and doesn't use RLS 
What about using an associated type? 
This example is exactly where I gave up learning/using Rust. I also took that approach, but far less successfully.
To clarify: Kelsey, the entrepreneur in question, invented a currency, of which there are 1.2 trillion units. According to him, Kesley Coin is "the only currency in the world," and Kesley owns all of it, making him a trillionaire (in Kesley coin). In other news, I have invented Rustcoin and become the world's first quadrillions ire.
/r/playrust is presumably where you wanted
Absolutely agree. They're way more painful and for no apparent reason. Why must the macro syntax be obscure/weird/totally different?
P.S. The docs aren't that great for it, either :-/
Wow, I had the same experience trying to figure out what pointer types to use when I tried to learn rust. I'll definitely give that linked list book a shot before trying it again! Thanks for sharing your experience.
Thanks for the reply. Would this be the same `fn main() {` `let (mut a, mut b) = (u64, u64);` `for i in 1..10` `{` `(a, b) = foo(a, b); //do something with a,b in foo` `println!("{} {}", a, b);` `}` `}`
Yea that would work. 
Wrong subreddit.
A different kind of rust.
[removed]
Some protocols, like CBOR or MsgPack, encode enough that you can convert the binary data completely to text without a schema, which is nice for debugging. But you pay for this with increased serialization time. Others, like Cap'n'Proto or FlatBuffers, are super fast to serialize but you can't do anything without a schema. They also tend to produce larger output, but you can choose your time-space trade-off by adding a compression layer. Protobuf is in between. You pay the cost of encoding the extra typing data and you can almost decode it without a schema, but not quite. 
No, Rust doesn't have JavaScript-style "global namespace". 
It's mostly the same as `where A: Into&lt;String&gt;`.
I got distracted by some other work and finally had a chance to look at this again. Strangely, I'm not seeing the massive additional overhead of using the threads anymore and the differences in speed (both with `&amp;&gt; /dev/null` and without) amount to a rounding error. While poking around a bit, did come across a library called `duct` which I played with a fair amount and I may wind up just using that instead since, in hindsight, I'm not getting much from threading this processing. I will probably return to this, though, at some point. Thanks for the input in that last paragraph. this is all good stuff.
Unintentional QotW.
Semaphore apparently has unlimited concurrent jobs via it's autoscaling(I might not know what I'm talking about). They have a free tier that's first $20/month credit is no charge, which gets you about 1,300 minutes a month. They are working towards a different/better offer for OSS projects afaik.
Rayon is the easiest use case. Though there are probably better alternatives for IO. Rayon makes the most sense in cpu intensive tasks
Even if that could work, I don't think it could check the validity of the format string in combination with its arguments at runtime, could it?
rustup 1.17 changed something in its output that the VSCode plugin parses. https://github.com/rust-lang/rls-vscode/pull/520 fixes that. It looks like 0.5.4 was just released hours ago.
I agree, this provides convenience, not safety.
It would be awesome if rustfmt can reorder and regroup imports so that I don;t have to touch them. What I wish is rustfmt to produce exact same result for all equivalent use statements. 
Just tried out the battop utility and it seems to be working minus the temperature. This is pretty awesome otherwise!
I've been working on my text editor project [Accepted](https://github.com/hatoo/Accepted). I've made it be able to open multiple files and added Tab bar. And added support for [rmate protocol](https://macromates.com/blog/2011/mate-and-rmate/). Supporting rmate protocol was needed to use this editor in my work. Since I always edit some text files through rmate protocol because opening editor on the server is not good due to network latency. Those features are not released in crates.io but will be soon after some refactoring.
A function or method can have a type parameter `T` without `T` being otherwise mentioned in its signature. See [`Any::is`](https://doc.rust-lang.org/std/any/trait.Any.html#method.is) for example.
I really have no clue. My 'idea' is that if it was possible to 'always inline println' or some thing like that, `println("{}{}", a, b);` would be turned into lib_println_valid!("{}{}",a,b); lib_println("{}{}",a,b); and the first function is a compile time macro.
&gt; // at least static X doesn't work backwards. Phew! Are you sure? Thus runs fine: ```rust fn main() { println!("{}", X); static X: usize = 4; } ```
Demotivator meme worthy.
wait, so was AWS locking you in or using WSGI? now I'm confused
I wrote duct :-D so let me know if you have any suggestions!
if one of components of the rust distribution is not ready for release, they just release a version without it, though you shouldn't have issues on the latest nightly https://rust-lang.github.io/rustup-components-history/index.html
To be honest I'm not quite sure, these three discussion threads ([1](https://github.com/rust-lang/rfcs/issues/376), [2](https://github.com/rust-lang/rfcs/pull/2137), [3](https://github.com/rust-lang/rfcs/issues/960)) are all I could find on it.
Good hint. Didn't know that, but my fn needs to know the type, it just doesn't care which one. I cannot match Any against an unlimited number of choices.
It being redundant is why I thought rust didn't allow defining bounds on concrete types in the first place.
In your example, the function `b` *does* know because it's generic. `b` does not have to be inlined itself. `b` knows the exact type because the compiler will generate a special version of `b` with the type parameter replaced by the actual type. This is called "monomorphization". That and inlining is what makes Rust so efficient w.r.t. abstraction. There is no runtime dispatch unless you explicitly ask for it using the `dyn` keyword in combination with a trait to form an unsized type.
Thank you :) Temperature graph is probably not working just because battery driver in your system is not reporting this data, which is very common for temperature values. Guess I should hide it in that case :)
Intellij with rust plugin. 
Ahhh that makes a lot of sense! Sometimes my brain deletes some important info after learning a lit haha :)
I think what they mean is `fn method&lt;T&gt;();` (so there's no `T` argument, but you can do `method::&lt;$t&gt;`. Could help to know more of the macro, though, it's possible the solution is simpler.
Thanks! I bet using PLinq will have some impacts, probably not as big as rayon though.
I use nightly Rust. I don't like the performance of the RLS, so I don't use it.
The important thing with logic and physics is that it runs stably over time. The way I'd recommend dealing with time stepping is to calculate the delta time every time you enter your game loop, accumulate the time and execute as appropriate. There's no point in drawing faster than your target framerate, but you should always run your physics/logic for the correct number of ticks for the time that has elapsed. let previous_time = now_milliseconds(); let logic_timestep = 33; let logic_time_accumulated = 0; let rendering_time_accumulated = 0; let rendering_timestep = 16; while game_running { let current_time = now_milliseconds(); let delta_time = current_time - previous-time; previous_time = current_time; logic_time_accumulated += delta_time; rendering_time_accumulated += delta_time; while logic_time_accumulated &gt; logic_timestep { logic_time_accumulated -= logic_timestep; run_logic (); } if render_time_accumulated &gt;= rendering_timestep { render(render_time_accumulated * 0.001 f); render_time_accumulated = 0; } }
cc /u/burntsushi 
The important thing with logic and physics is that it runs stably over time. The way I'd recommend dealing with time stepping is to calculate the delta time every time you enter your game loop, accumulate the time and execute as appropriate. There's no point in drawing faster than your target framerate, but you should always run your physics/logic for the correct number of ticks for the time that has elapsed. let previous_time = now_milliseconds(); let logic_timestep = 33; let logic_time_accumulated = 0; let rendering_time_accumulated = 0; let rendering_timestep = 16; while game_running { let current_time = now_milliseconds(); let delta_time = current_time - previous-time; previous_time = current_time; logic_time_accumulated += delta_time; rendering_time_accumulated += delta_time; while logic_time_accumulated &gt; logic_timestep { logic_time_accumulated -= logic_timestep; run_logic (); } if render_time_accumulated &gt;= rendering_timestep { render(render_time_accumulated * 0.001 f); render_time_accumulated = 0; } }
AWS. :)
What I want to do: // this trait pub trait Fry { fn fry&lt;T: Serialize&gt;(source: T, key: u128) -&gt; Bacon; } pub trait Unfry { fn unfry&lt;T: for&lt;'de&gt; Deserialize&lt;'de&gt;&gt;(bacon: Bacon, _: T, key: u128) -&gt; bincode::Result&lt;T&gt;; } // is currenty impl once similar for Unfry impl Fry for Bacon { fn fry&lt;T: Serialize&gt;(source: T, key: u128) -&gt; Bacon { fry!(source, key) } } I want to avoid that the caller has to implement the trait. So struct P should not impl Fry, because there could be also struct Q, R, S, T... The program currently looks like this: [derive(Clone, Debug, Deserialize, Serialize)] struct Person { name: String, age: u8, gender: Gender, // enum address: String, description: String } let vip = Person { name: "Ernst Stavro Blofeld".to_string(), age: 77, gender: Gender::Male, address: "Inside a Vulcano, Japan".to_string(), description: "CEO of SPECKTRE aka Bacon Industries".to_string() }; let bacon: Bacon = Bacon::fry(vip, key_128); The problem starts here: let mut person = Person::new(); person = Bacon::unfry(bacon, person, key_128).unwrap(); //cast from bacon to person happens in unfry Bacon::unfry needs to know into which type it should cast the unfried bacon. Therefore I am currently passing an empty Person, just to pass the type. It doesn't mutate _: P. A better solution would be, if I would not pass the type to the macro but the actual mut object and infer the type.: fn unfry&lt;T: for&lt;'de&gt; Deserialize&lt;'de&gt;&gt;(t: &amp;mut T, bacon: Bacon, key: u128) -&gt; bincode::Result&lt;T&gt;; unfry!(bacon, t, key); // macro called in fn unfry // instead of fn unfry&lt;T: for&lt;'de&gt; Deserialize&lt;'de&gt;&gt;(bacon: Bacon, _: T, key: u128) -&gt; bincode::Result&lt;T&gt;; unfry!(bacon, T, key); but I don't know how to infer the concrete type of t. The macro returns and the fn just propagates the result: let decr: bincode::Result&lt;$target&gt; = bincode::deserialize(&amp;decr_bytes); // target is T from _: T decr https://github.com/aspera-non-spernit/bacon/blob/dev/src/lib.rs https://github.com/aspera-non-spernit/bacon/blob/dev/examples/bacon_trait.rs
"code written using the futures API today should continue to work unchanged into the *future*." I see what you did there
XY Problem. Do you *really* want forward declarations to substitute later, or do you really just want a read and write a JSON file? If it's the latter, use [`serde`](https://crates.io/crates/serde) and don't reinvent the wheel.
If `Into` weren't available (and there's similar situations where this is the case), there would be no other way to express this.
Thank you!
This is my second toy project with Rust. I've been learning Rust about 1 month now. I really getting into enjoying to build something or port the existing project with Rust. This binary is the parser of the `front matter` which is containing metadata on top of markdown content. I started this project for my blog app which is built with Next.js. I usually post my content in markdown file and put it in sub-directory. Everytime I newly add a md file in that post directory, I have to update something like `postList.js` to be used inside my blog app such as title, createdAt, tags, category, etc. This parser makes this process automatically for me. I'm very new to Rust. Any comment and code review or PR is highly welcomed.
aaaahhh... You are the Tuesday hero. :) That works too. cool thank you. I can do now: let vip: Person {..}; let bacon = Bacon::fry(vip, key_128); let p = Bacon::unfry::&lt;Person&gt;(bacon, key_128).unwrap(); So cool. Thank you. Is that limited to one type, or is there a way to pass two that way? I want to bacon:Fry to support different frying techniques. Would this be possible: // unfry bacon, into type Person using Speck Bacon::unfry::&lt;Person, bacon::Speck&gt;(bacon, key_128).unwrap(); 
Yupp, they're comma-separate on both definition and use. And if you have some part of that that you want to let the compiler infer, you can write `_` instead of an explicit type. E.g. `(0..10).collect::&lt;Vec&lt;_&gt;&gt;()` will make a `Vec&lt;i32&gt;`. You can read more in https://doc.rust-lang.org/book/ch10-00-generics.html
&gt; Huge news for all of us eagerly _awaiting_ futures They polling them out of their sleeves!
I don't understand what @now/rust is from the announcement
You can use gdb for android (https://source.android.com/devices/tech/debug/gdb) to debug programs. To see logs you should use log crate plus https://crates.io/crates/android_logger and you can see logs inside Android Studio, or via adb
Awesome!
Is there a way to run a local server for dev?
I like macros in rust because of the flexibility they give you, but I hate them because people tend to invent a completely new syntax with them. For example the [graphql\_object](https://docs.rs/juniper/latest/juniper/macro.graphql_object.html) macro from the juniper library. Sure, you can just learn it, but such macros make it a way harder to read the code.
If you're asking for a procedural macro - no, no order is guaranteed, and you can't even use objects across invocations (i.e. if you try to store, say, a \`proc\_macro::Ident\` in a \`thread\_local!\` variable, using it in any way from another invocation will panic).
It seems you want module a and b under the top level. Since you wrote "mod b;" in main.rs and "mod a;" in b.rs, you have module a under module b under the top level. Write "mod a; mod b;" in main.rs to get what you want.
The design arguments of variadic arguments, named arguments, and a few other language-complexity increasing niceties have mostly spun in circles as we want to know that it all still fits together coherently and seamlessly with the rest of the language.
If possible I will even say CLion + IntelliJ rust plugin. &amp;#x200B; If you are a student or you want to use it for an Open Source project you can have it for free. Check the different discounts [https://www.jetbrains.com/clion/buy/#edition=commercial](https://www.jetbrains.com/clion/buy/#edition=commercial)
And should I change anything in [b.rs](https://b.rs) . Bcz if i don't I am still getting the error. 
I find it very worrying that many say that feature should be stabilized first for people to use it. Isn't it why we have Nightly, so community via early adopters could thoroughly test a feature before it gets stabilized, so wider public will get "ready" feature and not a half-baked one?
 impl Future for Reddit { type Output = Pun; } 
https://github.com/rust-analyzer/rust-analyzer is a good alternative.
Side note: indent your entire code block with 4 spaces to make it appear as a code block and thus preserve whitespace.
I totally agree. yesterday i decided to update my rust toolchain, and everything broke. neither vim nor vscode could launch RLS. i had to pick the last working version from [here](https://rust-lang.github.io/rustup-components-history/x86_64-apple-darwin.html), then change the plugins settings in vscode/coc.nvim. point being, i wasted hours fiddling with this stuff when i could have been writing code.
Also, a pure Rust port is here: https://github.com/SunDoge/simdjson-rs
Cell and RefCell fall more into the first category with UnsafeCell being the thing that falls in the second category.
Hi, I got it to compile is [this](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=74a5fa5cbe51d5d813452c6e465fbd00) what you want? For the why, I think the lifetime of Serializer was too long, by adding for&lt;'b&gt; you can accept shorter lifetimes and make it work.
If I understand this correctly: I constrained the bound to only 'a, but \`&amp;mut \*serializer\` is "generating" a shorter lifetime, and by changing the bound to any lifetime 'b it works. Great ! &amp;#x200B; Is there a documentation about \`&amp;mut \*T\` implications? I haven't found anything relevant to his problem in the doc yet.
thank you. This raised the idea to have a struct Fryable #[derive(Serialize)] pub struct Fryable&lt;T&gt; { data: Vec&lt;T&gt; } for use cases in which the type is not known at compile time. Not sure if it works. I thought I extend the functionality of the program to accept messages from the command line. Which are usually parsed as String, but other implementors could parse it into something else. let args: Vec&lt;String&gt; = std::env::args().collect(); let f = Bacon::unfry::&lt;Speck, Fryable&lt;T&gt;&gt;(bacon, key_128).unwrap(); let fryable = Fryable::from(args); let bacon = Bacon::fry(fryable, key_128); Which causes T not defined for Fryable&lt;T&gt;. Actually, I think I could reuse the struct Bacon that looks similar but has data: Vec&lt;u128&gt; and move &lt;T&gt; from the fn ..&lt;T&gt; to the struct. Will try that :)
&gt; I have been struggling with the use of mod to import rust files Don't think of \`mod\` as a keyword for *importing*. \`mod\` is for **creating** modules, not importing them. Furthermore, modules are *not* synonymous with files. So, for now, let's just forget about files completely, and just focus on modules. Well, the obvious exception to this is the *crate root*, which needs to be in a file, otherwise, how are you going to run `rustc` on it? So let's say, you have a crate called `mycrate`. You put the contents of the crate root in `mycrate.rs`: pub struct RootStruct; mod child { mod grandchild1 { pub struct Grand1Struct; mod grandgrandchild { } } mod grandchild2 { pub struct Grand2Struct; } pub struct ChildStruct; } This creates a module structure like this: [mycrate (root)] / \ RootStruct [=======child========] / | \ ChildStruct [grandchild1] [grandchild2] | | [grandgrandchild] Grand2Struct Note that so far we haven't imported anything, and we haven't created any additional files. The only file is `mycrate.rs`. What we created is *modules*. That's what the `mod` keyword does. Now, putting all the modules into a single file would create a gigantic unnavigable blob, so we want to split them out into different files. How do we do that? Simply by omitting the body of the module from your `mod` declaration. Seeing that, the compiler will read the module body from a file. What file, you ask? Well, this is where the `mod` keyword actually interacts with the filesystem. By default, for `mod foo;`, the compiler will look for the module body in [`foo.rs`](https://foo.rs) or `foo/mod.rs` relative to the file the `mod foo;` declaration is contained in. That's it. That's all the interaction modules have with the filesystem. Therefore, it's best not to think of your crate as a collection of *files*, but a collection of ***modules***, and use the above simple rule to locate what file your module body is contained in. &amp;#x200B; But, how do you actually *import* items? Turns out, there is an entirely different keyword for that, called `use`. Let's say we're inside the module `mycrate::child::grandchild1::grandgrandchild`, and we want to import the various structs we made in the other modules. We can simply ... use ... the `use` keyword to do that. Like this: use crate::RootStruct; use crate::child::ChildStruct; use crate::child::grandchild1::Grand1Struct; use crate::child::grandchild2::Grand2Struct; As you can see, you simply specify the full path of your desired item. The `crate` keyword refers to the *crate root*.
Well I would have used the json output for that rg --json --null-data --multiline '\x00\x01' abc | jq 'select(.type=="match") | .data.absolute_offset' But I agree that there should be a line-number/byte-offset only output mode. I've needed that before.
`--null-data` is needed in this case because the input data is binary and without it or `-a` ripgrep silently ignores the input file.
Yes, you would need -a since --null-data implies it. But you don't need to change the line delimiter. I guess it doesn't hurt in multiline mode.
By the way, is there a flag to only output the matching line numbers but not the match itself? I've needed that before to search for magic numbers in larger binary files (grep afaik doesn't have a flag for it either). It would be useful for both text but especially binary data because that can mangle the output. [example here](https://aur.archlinux.org/cgit/aur.git/tree/PKGBUILD?h=svp#n39)
I think when you call value.serialize(tmp) you ask for a &amp;mut Serializer, but you defined &amp;mut S to be a &amp;mut Serializer only if the &amp;mut Serializer lifetime is 'a so it will borrow it for 'a. \ When you add for&lt;'b&gt; you define &amp;mut S to be a &amp;mut Serializer for any lifetime. For &amp;mut *T it can do a few things, I don't know a specific part of the doc about it.
Thanks for those paper links! The first one in particular looks like it'll be very useful once I've grokked it. There are some tantalising hints there on how opt-in recursive eager expansion *should* work, but translating them to Rust (and making them ergonomic) might take a bit of effort.
No, i think your other comment that uses the JSON output is probably the right way.
&gt;&gt; How do you regression and integration test this code in other languages without it also taking a massive amount of time? &gt; As the coverage on this much code be it Ruby/Python must take hours. That's not a problem at all, no need to cover all generated methods or something, a simple read/write/compare test will find almost all bugs. The same for regressions, the library I'm working on it just a part of a huge project which covers everything we need basically. So Rust compilation + a few unit tests + integration tests on CI will catch 99,99% bugs. For Ruby and Python it's the same w/o compilation step.
I'm trying to generate as simple code as possible and also provide all the types explicitly everywhere, but there are still place for type inference tight now, for instance, in closures. Maybe I'll try to provide types for closures as well.
The output is full of line like this: time: 0.000; rss: 3234MB solve_nll_region_constraints(DefId(0/1:18676 ~ my_crate[8eda]::generated[0]::{{impl}}[1415]::method_name[0]::{{closure}}[0]))
I'm not trying to reinvent the wheel I'm just looking for a layer of abstraction so that I could more easily swap out different libraries if needed. It's a classic adapter pattern. I'm not looking to learn some other library right now, I'm looking to make the signatures the same as they would be in python so I can focus on the core logic of the neo4j driver.
My personal policy is to use stable for libraries I intend to publish on [crates.io](https://crates.io), and nightly for everything else. That way, I get to play around with all the latest features that interest me, and also help test them. &amp;#x200B; Obviously, if you are developing some critical piece of software that you intend to put into production, you should prefer stable. But I find it that sometimes stable is pushed way too hard on people, and nightly is presented as something scary. We should try to present nightly as something exciting instead, where you can try out all the latest cool features.
or use vec, almost always
That's a general misconception about how default type parameters currently work. They are in fact not "hints" as you called it. They only apply if you explicitly pass fewer than the maximum amount of arguments. `Tally` has an arity of 2, hence `Tally&lt;&amp;str, u8&gt;` will be left as-is but `Tally&lt;u16&gt;` will be expanded to `Tally&lt;u16, u64&gt;`. The crucial point is that `Tally&lt;u16, _&gt;` does not mean `Tally&lt;u16&gt;`! struct O&lt;T = ()&gt;(Option&lt;T&gt;); //let x = O(None); // error //let x: O&lt;_&gt; = O(None); // same, error let x: O&lt;&gt; = O(None); // ok let x: O = O(None); // same, ok This explains the error: //Tally::new().add(""); // error //Tally::&lt;_, _&gt;::new().add(""); // same (2 args), error //&lt;Tally&lt;_, _&gt;&gt;::new().add(""); // same, error Tally::&lt;_&gt;::new().add(""); // rule applies (1 arg), ok &lt;Tally&lt;_&gt;&gt;::new().add(""); // same, ok This seems to be a rather weak feature of Rust, not interacting with type inference. I haven't completely understood yet whether and if so how the language will change in that regard. I link [this GitHub comment](https://github.com/rust-lang/rust/issues/36887#issuecomment-342752753) which should provide a nice starting point linking to other issues.
here is an example of how I leveraged them. In Simdeez, I have a library that lets you write a function, and then be able call it using different instructions - SSE2, SSE41, or AVX2. So the one function via traits actually maps to 3 versions at compile time. But to *use* this to pick the best one at runtime is a pain. You write your generic function with inline always attribute, then you have to write another function for each instruction set you want to support and tag those with the proper target_feature attribute, then ANOTHER function to detect at runtime which feature sets are available, and then call the appropriate function from there. So its a lot of ceremony and arcane knowledge to use it properly. This macro, does all of that for you, so you just write your function: https://github.com/jackmott/simdeez/blob/master/src/lib.rs#L455 The macro then generates all of those functions, with the proper attributes on them. 
Macros seem crazy at first because of the decision to make them hygenic, which adds a lot of complexity, and that complexity is not documented well. I struggled for a long time to get it, now that I kinda do I'm tempted to write a better doc for them. 
I agree, Jon. Tide is a project based on noble intentions yet is disruptive to the ecosystem. It represents mission drift from one of tool building (language, low-level libraries) to ecosystem building. Realistically, though, Tide will die on the vine without real help. It moves at a glacier pace while the rest of the ecosystem rapidly evolves. If anyone disagrees with Tide's politics, don't contribute to it.
This crate is yet another attempt to find the ideal Rusty way of dealing with templates. There are other great crates for this out there, and they are far more advanced: \- the 100% Rust way is \[Maud\]([https://github.com/lfairy/maud](https://github.com/lfairy/maud)), which I was using successfully until recently. However, I wanted something else for multiple reasons: (1) Maud doesn't compile on stable, and I started spending more time on finding the correct version of Nightly that would compile all my dependencies, than on adding new features, (2) it mimics Rust syntax only partially, which is inconvenient sometimes and (3) separate templates bring more clarity to the code, at least they forced me to think more clearly about the structs from which pages are generated. \- another cool template crate is \[Askama\]([https://github.com/djc/askama](https://github.com/djc/askama)), which I tried using, but (1) I wanted to have real Rust in my templates, and use the usual cool tricks, in particular with iterators and (2) I was looking for a more functional way of doing things, without clearly defined scopes, limited scope sharing, etc. \- other systems use dynamic templates. This is cool for iterating fast on a design, and I might add a way to do this in Cuach at some point, i.e. templates compiled in debug mode would be able to reload partial contents from the files, provided the logic hasn't changed.
It's called a reborrow, and is essentially just crates a reference and then immediately dereferences, which can be very useful, especially for types that override the dereferences operator, like String. let s: String = "string".to_string(); let s2: &amp;str = &amp;*s;
I just installed it, and... ``` rust-analyzer failed to load workspace: cargo metadata failed: invalid type: null, expected a string at line 1 column 15823 ``` Doesn't work either. 
Could you put a link to the article you're referring to ?
As was proposed [here](https://www.reddit.com/r/rust/comments/awa3t1/baby_steps_blog_asyncawait_status_report/ehnket6/) I believe we need some kind of semi-stabilisation. In other words state of a feature in which team is fairly sure that no breaking changes will be introduced to it, but they reserve the right to do it as an escape hatch. This way probability of breaking things will be really small, so early adopters will be able to invest into using the feature more heavily. Additionally I think we should make `#![feature(stabilized_feature)]` a warning instead of hard error. This way we will be able to publish crates which today require nightly, but in future will work on stable without any changes.
&gt; This PR doesn't include async / await macro support. Thats still in the pipeline. Do we have a stable await syntax yet? Do we know if await is gonna be a macro or a keyword?
I haven't seen that before. Can you file a bug?
Finally made a stable release of [hdf5](https://github.com/aldanor/hdf5-rust) crate on crates.io, the first one under this name (previously hdf5-rs) and the first release after a very long while. // the name hdf5 and hdf5-sys were already taken, so I had to convince their owner to pass them on to us since he had no interest in continuing the development of those crates; fortunately, the author kindly agreed with the proposition, but the whole thing took a while
The usual way is to put the native object onto the heap and keep a pointer to it in a `jlong`, and then write jni wrappers for its methods that use said pointer. Jni-rs has [this](https://docs.rs/jni/0.11.0/jni/struct.JNIEnv.html#method.get_rust_field) helper method along with the corresponding setter. It boxes a rust thing and stores/retrieves it from a Java `long` field. You can also use the regular jni field setters/getters and box your rust thing manually if you'd prefer.
I am wondering what happened to this discussion (and PR) on the LocalWaker API: https://boats.gitlab.io/blog/post/wakers-ii/
That is a lot of code for something that should be trivial
If I find it, I will.
Another proc-macro question: I am expecting a string [Literal](https://doc.rust-lang.org/proc_macro/struct.Literal.html), how do I get the actual `String` representation of the string literal? More specifically, I can do `to_string()` but it returns a debug formatted version of the string literal in the Rust code with escape sequences. This requires me to go parse escape sequences. Why do I need to do that, why can't the Literal type just give me the actual string Rust parsed?
FWIW, someone is currently working on an implementation for native C-style variadic arguments for Rust right now.
Wrong RUST...lol
you want /r/playrust
Looking at the versions: this is probably outdated by 2 years. 
Unsafe and FFI purpose only
Negative, still a lot of debate around postfix `.await`, `.await()`, `.await!()` or prefix `await`, `await?`, whatever. Everybody's still disgruntled with all of them. Sign of a good compromise being available *somewhere*. You can track the syntax discussion [here](https://github.com/rust-lang/rust/issues/57640), which hasn't been updated in a week, but I'd imagine the WG is still hard at work.
I doubt it has any advantage to The Book.
You should remove `mod a;` from b.rs
/r/playrustservers
but can it compile rust ?
As an aside, are you using anything to generate that text diagram or did you hand indent it?
I am slowly building the base parts for something like wireshark/tshark in rust. Started with a parser for the old pcap format and improved from there. Existing parsers, in order of implementation: * PPI (only IEEE802.11 radio, GPS) * IEEE802.11 (only beacons, probe requests) * informations elements * supports extracting some data about devices * ethernet (including IEEE802.1Q) * IPv4 (no options) * IPv6 (no extensions) * UDP * TCP (no options) IPv4 and TCP over IPv{4|6} support checksum calculation. The next big part would be TCP flow reassembly in order to reorder and de-duplicate packets, check for completeness and then extract the L4 payload for further analysis. This started mostly as a teaching project because I worked with packet traces often and sometimes wished there was a more effective alternative to tshark. It worked in that regard as I learned about low-level protocols and designing/structuring data processing software. Much of the older code needs a re-write with hindsight to aid in API design and implementation.
I wrote it by hand.
Ah fair enough. I was hoping someone had a tool for it so I could leave some diagrams in comments etc
I figured. I hope it manages to make the next release. Would be nice to have it with futures...
This isn't a bad idea for a tool. Probably someone already wrote something like this, but I might whip something up.
I fix the SIGABRT, after nothing how in iOS all work fine. &amp;#x200B; I try to put logging for android but nothing show up so I was lot, but then hit me I need to enable permissions: &lt;uses-permission android:name="android.permission.INTERNET" /&gt; &amp;#x200B;
&gt; Hmm can you give an ELI5 explanation to an ELI5 example that shows why macros are awesome. Lemme try. Macros (or macros by example) allow you to avoid boilerplate, like for example what if you need to implement an empty Trait for (`i8`, `i16`, `i32`, `i64`, `i128`, `u8`, `u16`, `u32`, `u64`, `u128`, etc.). You could write that code by hand, or you could define a small `implTrait!` macro and then write: implTrait!(i8) implTrait!(i16) .... If the code transformation is simple you can probably do it in plain macros (aka macros by example). If not then see the following part. &gt; I don't even know what the difference between a macro and a proc-macro is Proc-macros are more powerful than regular macros because they allow custom transformations of the code and compile-time type checking and error reporting. In practice, if you want to have statically checked SQL queries that are guaranteed to be correct on your DB? Want to generate HTML templates that are checked at compile time for errors? How about formatting printing arguments that are verified for correctness at compile time, instead of run time? Well, that's what proc-macros are for. &gt; Why would you use macros instead of just actual Rust code or a Rust function? Why is something as basic as a `println!` a macro and why isn't it just a normal `println()` function? Here is a question for you. In your example `println()` is run like regular code, i.e. after you compiled your code. Now tell me how can `prinln()` which is run post compilation check that there aren't bugs in code before compilation? This is a trick question - You can't. You need meta-programming faculties. In other words, you need some things that run before the code is compiled, that will check and verify you don't have any bugs. Now, do we need to mark such things with `!` (or `#[pragma]` or `#![stuff]`)? No. But it helps to notify us, this thing is gonna do some funky stuff at compile time. Did Rust macros need to be done like `proc-macros`? Of course not. It's just the most typesafe and sane way that Rust developers settled on.
Im tryna get this hoe popular asf ya feel lmao
Note that the first paper is really a low level model (like the Oxide paper that was posted here, but for macros), and isn't really representative of how Racket programmers work. The documentation of `local-expand` is here: https://docs.racket-lang.org/reference/stxtrans.html?q=local-expand#%28def._%28%28quote._~23~25kernel%29._local-expand%29%29 which has some examples.
Hey! Awesome! Once i use it more I’ll let you know. Thanks for the awesome lib!
Thanks. I was writing from my phone, which makes it harder to look up supporting facts.
Ohhh a very nice explanation. I actually understood that. Thanks so much! Just one last question: The thing that still confuses me is: How do other programming languages do this? Do C or C++ have macros? How do they solve the problem in your example? Or is this problem so Rust specific due to the safety features of Rust?
Found something http://asciiflow.com
Ok, isn't it awkward that I can't find anything related to reborrow in the book/api docs/nomicon?
&gt; Even so, one might not consider this a reason for bundling things more into std, since that's potentially only one solution to this problem. It's not *really* a rational reason. The more stuff you push into std, the more need there is to audit it. And the more likely it is to have a security issue, and that then becomes a burden for the whole Rust ecosystem to bear, rather than the occasional mal-formed crate.
My only concern with these pipelines is I have a difficult time visualizing pricing differences between this and traditional hosting. They seem neat, but a cheapy $15/m box is a lot more predictable to cost and output. Doubly so if all it does is get slower if I start exceeding traffic by not obscene amounts. I definitely like the idea of Now, I'm just hesitant
/r/playrust
1. `Box&lt;[u32]&gt;`, or `Vec`. If you don't have `alloc`, write some abstraction on top of `NonNull&lt;[u32]&gt;` which you define as having ownership, which is basically what `Box` would be at the end of the day if it weren't magic for historical reasons.
So what? We should confront reality and deal with that instead of arguing over whether it's "rational" or not. Like, obviously your stated downside is a thing, but it neglects the likely reality that some folks may treat the standard library as "official" and therefore unnecessary to audit, where as third party crate dependencies need to be audited. Regardless of what _you_ think about the matter, this may simply be established policy at certain companies. I note that I'm identifying a _problem_ here. Not a solution. One solution might be to lobby these companies to change their policy. Another solution might be to add more things to std. Yet another solution might be a "blessed" or standard distribution of crates. Or maybe something else or some combination of the above.
`horrorshow` fixes some of the downsides of `maud`.
&gt;it reports 87 hits but the breakdown includes less than 50 That's definitely a bug. We cap at 1000 matches but we tell you ("more than 1000") when it's capped. I created an issue: [https://github.com/aelve/codesearch/issues/242](https://github.com/aelve/codesearch/issues/242). &gt;I'm assuming there is a way to exclude a repo but I'm not seeing how Not yet, unfortunately. You can include a package or several packages by filtering by path, but you can't exclude them. That would surely be nice to have. &gt;It could be nice if the code view made an attempt to link to file in the repo We don't index repos, we index actual tarballs from [Crates.io](https://Crates.io). Some package repositories (e.g. Hackage) provide [hyperlinked, colored source views](http://hackage.haskell.org/package/lens-4.17/docs/src/Control.Lens.Lens.html) for all files, but Crates doesn't. I guess that ideally Crates should do that and we would link to Crates – this way other tools can also benefit. &gt;My company's internal code viewer does this and it is a nice workflow improvement. Is it open-source? We could do this but just parsing out identifiers might not be very useful because then searches don't take scope into account (you search for `assert` and there are a hundred different `assert`s defined in various libs). Doing it right is possible – [Sourcegraph](https://sourcegraph.com/) does it, I think – but requires lots of effort for every new language we add. &gt;I might decide I am done looking at one and I want to skip past it to the next. Makes sense, yep. Subscribe to [https://github.com/aelve/codesearch/issues/243](https://github.com/aelve/codesearch/issues/243) if you want to know when that gets done.
This is definitely useful!
 type Pun = !;
Thanks for the feature request! It always helps to get feature requests because then we actually know whether anyone needs a particular feature we want to build, or not. Tracking issue: [https://github.com/aelve/codesearch/issues/232](https://github.com/aelve/codesearch/issues/232).
&gt; hoping that I can parallelize the downloading of the segments using rayon `rayon` is suitable for CPU-intensive jobs - no I/O, no waiting - that can be divided into parts. It's the wrong tool here, although it could be abused to do what you're asking it to do. There are two problems. `rayon` spawns the correct number of workers for the locally detected CPU. This is most likely different from the number of concurrent HTTP connections you should have. Second, if you put an I/O-bound job into `rayon`, it will block that worker thread and prevent you from dispatching CPU-bound jobs at the same time. By convention, the `rayon` pool is for CPU work. Is multiple concurrent HTTP connections to the same server the right solution anyway? [From what I understand it's usually *not* the best choice with HTTP/2](https://developers.google.com/web/fundamentals/performance/http2/#one_connection_per_origin), so you only benefit from this with older (but still very common) 1.1 servers, and then primarily if you are downloading many small files. [The HTTP 1.1 RFC says you shouldn't use more than 2 connections, most modern browsers ignore that rule and issue 6 at a time.](https://docs.pushtechnology.com/cloud/latest/manual/html/designguide/solution/support/connection_limitations.html) (I have a modern mid-range CPU which `rayon` spawns 12 workers on. That's a good number for computation, way too many for HTTP and most I/O.) Probably the easiest method is to use a thread pool rather than an async reactor. (`tokio`, etc.) This is less CPU-efficient than async techniques, but it's much easier to let the OS handle context switching than to make Rust responsible for it. I/O-bound threads will spend most of their time sleeping and not interfere with anything you're doing with `rayon`. I like the look of the `scoped-pool` crate. Use a MPSC channel instead of the `Vec` to collect the requests. Probably don't bother with the atomic flag for failure; `scoped-pool` propagates panic and non-panicking errors can be detected by examining the `reqwest::Response`. 
&gt; Not yet, unfortunately. You can include a package or several packages by filtering by path, but you can't exclude them. That would surely be nice to have. So the "path" field accepts either a package or a path? This didn't seem obvious to me with how packages and paths are shown in the UI. Is there way to specify both? I've tried several a couple combinations and didn't get them to work. &gt; &gt; It could be nice if the code view made an attempt to link to file in the repo &gt; We don't index repos, we index actual tarballs from Crates.io. Some package repositories (e.g. Hackage) provide hyperlinked, colored source views for all files, but Crates doesn't. I guess that ideally Crates should do that and we would link to Crates – this way other tools can also benefit. First, to be clear, I might have used the term "repo" in other contexts (like exclusion). I was referring to what I thought was codesearch's term generic term for what you are using as package but I fully admit I know very little about codesearch. The best idea I had here was to grab the repository link in the Cargo.toml and do a best effort of transforming that to a link to the repo's viewer. This would be very imperfect, probably require knowledge of each supported hosting service, etc, so unsure if its worth it. &gt; Is it open-source? We could do this but just parsing out identifiers might not be very useful because then searches don't take scope into account (you search for assert and there are a hundred different asserts defined in various libs). Doing it right is possible – Sourcegraph does it, I think – but requires lots of effort for every new language we add. It is not and is for perforce rather than git. I think we do a naive `\w` split and filter out everything that doesn't match identifier rules. I suspect this would be "good enough". Yes, not all links will be the most helpful (`std`) but it helps the user loop back around.
Yeah, it's really hard to know where that point is. I currently do all of my testing on a $5/month VPS and make sure the experience is good there for a handful of users, then I scale it up when I need to go to production. With a host like Digital Ocean or Vultr, that's as simple as clicking a button. Yes, it needs to go down, but it's usually back up within a minute, which is completely acceptable for an instance that cheap (I'm not going to run my core business on a single instance, I'll have a cluster). Many services have "scheduled maintenance", likely for this reason. However, at some point it's just not worth dealing with that complexity and perhaps it ends up being _cheaper_ to go serverless. However, it's _really_ hard to come up with those numbers on a new project because you just don't have the statistics to say how many requests you get, how much bandwidth you use, and how much CPU/RAM each request uses. I'd _really_ like it if there were more details comparing the two types of services, or even competitors within those types of services. Even an AWS Lambda vs Now would be an awesome addition so I could compare apples to apples.
It is absolutely ready to stabilize.
https://crates.io/crates/linked-list
Since you’re generating the code, can you use nested crates? This would allow you to only expose the public items of the toplevel crates to the outside world. The subcrates’ public interfaces would then represent internal interfaces that wouldn’t have to be exposed. I think this would allow for better parallelization. My other thought is to spin up a VM with AWS workspaces or just straight EC2 and do the compilation there. That doesn’t let you compile it on a workstation but it might give you a compiled binary.
I don't think you can do mutable globals in Rust (there are stuff like `lateinit` or `Once` that let you do this but it seems needlessly complicated to me) why not just go with the traits option and do something like: fn main() { real_main::&lt;MockJson&gt;(); // substitute for &lt;RealJson&gt;() later... } fn real_main&lt;T: Json&gt;() { T::dumps(); }
Are you sure that the input JSON is valid? If so, maybe `serde-json` has some issue with that specific file. You should try to reduce it and file an issue.
I think at that point you've just made compile time functions into macros anyways. The nice property functions have is that all you need to do to ensure a call is valid is to compare it to the function's signature. One of the nice things about macros is that that isn't the case.
Type punning in Rust, who would have thought...
&gt;I feel like this should be adopted by the infrastructure team and integrated with the rest of the Rust websites. What would be the first step towards that? Should I open an issue on [crates.io](https://crates.io) and ask if the team is interested, or is there a better way?
Which is why people build alternative tools to simplify things (like the OP), or why you can bundle /u/tehdog's command into an alias or another script. But no, this should definitely not be trivially supported by ripgrep.
I don't work with `unsafe` all that often, but if I did I could *definitely* see this being super useful
That's right. Maud will also probably work on stable at some point, now that procedural macros are getting better and better.
Yeah, reborrowing is rather under-documented. The nomicon used to partially cover it [here](https://doc.rust-lang.org/1.12.0/nomicon/references.html#liveness) but that's been removed from more recent versions. I briefly discussed it in [Rust: A unique perspective](https://limpet.net/mbrubeck/2019/02/07/rust-a-unique-perspective.html). Ralf Jung's [Stacked Borrows](https://www.ralfj.de/blog/2018/08/07/stacked-borrows.html) formalism also specifies reborrowing rules.
This sounds a lot like "Design By Contract" ([https://en.wikipedia.org/wiki/Design\_by\_contract](https://en.wikipedia.org/wiki/Design_by_contract)) with a bit of a limitation that you only have preconditions, with an implied post-condition that "safe Rust is still safe". Sounds sensible/useful to me! Some DBC frameworks (I think Eiffel does this) can in some cases convert the code-form of the contract into documentation (e.g. converting \`#safety\[lt(arg1, arg2)#\` to "\`arg1\` must be less than \`arg2\`") without requiring that the coder repeat themselves for the documentation.
If anyone happens to be running the recent [windows support for dtrace](https://techcommunity.microsoft.com/t5/Windows-Kernel-Internals/DTrace-on-Windows/ba-p/362902), I'd be quite curious to hear if cargo-flamegraph just works for you, as it will default to dtrace for all non-linux systems! If not I'm hoping it won't be too much work to get it working. This was made possible by really fast collaboration with Jon, whose [Inferno](https://github.com/jonhoo/inferno) crate this heavily relies on for pure-rust flamegraph generation, and Heinz N. Gies who immediately launched into adding dtrace stack collapse functionality into Inferno as soon as the first version of cargo-flamegraph was posted to twitter. No more perl scripts for generating flamegraphs :D
Welp, it looks like you were right about the compression. Now I have to figure out why our other program is incorrectly compressing a random file. Thank you!
No idea what the best route is but two I thought of: - Bring this up on [internals](https://internals.rust-lang.org/) - Another options is to reach out to and get it integrated [`crates.rs`](https://users.rust-lang.org/t/crates-rs-a-new-faster-crate-index-website/17876/62) first as kind of proving ground for the feature.
Converting `#[safety[lt(arg1, arg2)]` to "`arg1` must be less than `arg2`" is a great idea. Maybe if no description is given? Regarding the design by contract: This was kind of the idea behind this attribute. However, IMO another attribute like `#[precondition]` and `#[postcondition]` should be used here with a regular `assert!` instead of `debug_assert!`.
If you do track it down, consider making a prefferably minimal, reproducible example, and maybe filing an issue against `snap` or whatever else is causing it. Trying to do that would have helped you realize that `serde-json` wasn't causing this.
I didn't mean ripgrep should support it FYI. I just meant that it shouldn't be so hard to do in general.
It is why we have nightly, but most people would rather use tokio or actix rather than futures directly, and they are waiting for stabilization. Tokio is the single largest project that uses futures and they're stubbornly refusing to try futures 0.3 until it's stabilized. I personally have been avoiding tokio for precisely this reason, but it's very hard to find futures libraries that don't use it.
In that case, trait.
[RPL](https://vgy.me/LdvMWW.jpg) Received last night, was waiting for me when I got home. Can't wait to get started. Thank you, very much u/enby-girl
Man. Hoe would you even do that? Writing code without alloc seems really hard.
I think the issue is that you can acess unitialized memory after calling that constructor, so it can’t be safe
&gt; No more perl scripts for generating flamegraphs :D At least! I hope it lives up to this dream... I'll test as soon as possible
What's so bad about the macro? It expands to a new AST like a macro and wraps the expression with parentheses so order of operations is unambiguous. The only real thing I'd prefer is a generalized monad syntax that replaces both async await and `?`, but that's not going to happen for quite a while. I see so much discussion happening about one of the smallest problems with the while system.
This sounds awesome. If you do release it you have my pre-emptive thanks!
No pipes necessary either ;) &amp;#x200B; Please open an issue right away if anything doesn't match your expectations! As a pretty heavy-duty flamegraph user for database work this hits my needs, but the user sample size is still low and I'm sure there are things we could do to further reduce surprise!
In short, it would be undefined with the value of length other than trivial `0`. The documentation of `set_len` in "Safety" mentions the following. &gt; - new_len must be less than or equal to capacity(). &gt; - The elements at old_len..new_len must be initialized. There is no way for `0..new_len` to be initialized with exception for trivial case of `new_len == 0`. Therefore, such function would always cause undefined behaviour. If you want to preallocate vector with some known value, the following function may work. fn vec_with_value_len_and_capacity&lt;T&gt;(mut value: impl FnMut() -&gt; T, len: usize, cap: usize) { let mut vec = Vec::with_capacity(cap); vec.extend((0..len).map(|_| value())); vec } It's sorta unusual to require both some length and different capacity however. If the capacity is allowed to be the same, then `vec![f(); len]` should be just fine.
Check out [Any](https://doc.rust-lang.org/std/any/index.html) which allows casting from &amp;Any back to &amp;YourType. However, you need to be absolutely sure that an enum is not the best solution.
That wouldn't prevent such a function from existing. That would just make it an `unsafe` function. So the real issue is if it's worth having a combined set-len-and-capacity function when there are already perfectly usable functions to do each part separately.
There is a stackoverflow question about it [right here](https://stackoverflow.com/questions/33687447/how-to-get-a-reference-to-a-concrete-type-from-a-trait-object).
Sorry to have deleted the previous message, I thought nobody had had time to read it and I wanted to have time to see why the graph I had had no function name.
How is it "uninitialized" though? Here's an [extended version](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=ffce4930b22a6d75ffefb0bd0affa771) of my playground link. I don't really see what's unsafe *after* `len` is set.
I really like this idea. This is basically what the O'Reilly book goes over in the unsafe section too, the two different kinds of unsafe, and the contract by convention. We should try to force people to give a detailed description of the convention in the source.
The values that are inside the new vec are comprised of whatever data happened to already be in the allocation; if you push a bunch of strings onto the vec, pop them off, and then set the len to be larger, the strings inside the new length will be "dead". This is called a ou will use-after-free, and is textbook undefined behavior.
I agree. I don't see why people think the macro syntax is bad.
My guess is that it's because Diesel doesn't support async-io just yet?
I'd say that the current macro is as good as any version that includes "await". The main alternative would be to replace the macro with a shorter operator like what happened with `try!()` and `?`. Fundamentally, we're just coming up with a third do-notation, after try and generators. I give try a pass since it's not too obtuse and is so common is nearly every domain, and yield because generators are too complex to be implemented as a single macro, but unless await turns into a short and *obvious* operator, I would probably stick to `await!()` anyways. One of the huge draw of rust is that stuff like this don't really need language support. It's completely possible to use futures, even futures 0.3, completely without async await. Anything that makes it seem like a fundamental part of the language I'd see as a mistake.
I feel I should publish this. Anyone has a good name? "Safety" is used by a crate, which isn't used. "Contract" was reserved by some spammy user. Something like that is quite annoying and alleviates the motivation to release things at all on [crates.io](https://crates.io) at all...
I'm not sure I see how that's particularly related to what I'm talking about. I find it difficult to believe that the fact [this code](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=f6864ac553f9e182c045d7e8cbb3f9a5) works properly is unintentional.
I recommend including a link to an example output in your project description.
\&gt; //Nothing was explicitly set, but of course they're all just empty... This is very, very much not \*of course\*. This will not always be true, and even if it was practically always true, it's still undefined behavior.
first tip: go to /r/playrust 
Wouldn't it make sense to move this let begin = Vec3::new_ab((a - 79) as f32, (b - 79) as f32) * Vec3::from(0.5); let e = Vec3::new_ab((c - 79) as f32, (d - 79) as f32) * Vec3::from(0.5) + begin * Vec3::from(-1.0); into `LETTER_BLOCKS`, i.e. make it a `Vec&lt;(Vec3, Vec3)&gt;`? Also, maybe pass `LETTER_BLOCKS` as an argument, as `lazy_static!` might have a tiny performance overhead.
that's a good tip, thanks
I've extended your example to cause a segfault, which is clearly unsafe, and is a result of using a MyStruct that was never initialized (reserving memory for it is not the same as filling that memory with a valid value). My example isn't gauranteed to segfault every time, but there's a very high chance that it does. 
There's already one in [inferno](https://github.com/jonhoo/inferno), so maybe we could copy some of that over. Linking to http://www.brendangregg.com/flamegraphs.html may also be a good idea.
Flamey, as expected =)
Even if it works now, it's not guaranteed to keep working, which is the issue. A compiler update or even change in what code comes before, or a different optimization kicking in can all break that code
What I'm saying is that I don't believe that there is not actually some kind of *intentional* "zeroing" going on there.
Some allocators may do that kind of thing, but they're not required to. And regardless, in the end, it's still UB.
For it to ever *not* work like that is illogical though, which I'm assuming is the reason it *does* work like that and likely always will.