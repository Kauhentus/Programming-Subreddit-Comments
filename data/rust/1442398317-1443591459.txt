The rustty api looks really nice and I want to use it for a project, but the key event polling is linux specific, and the branch that supports OSX ate up my cpu for some reason. But the code looks great, the maintainer seems involved, and I wanna contribute when I have time. Its a project I'm pretty excited about honestly.
Would be cool to have a cargo plugin that provided Souper support.
`b' '` directly gives you a byte, statically checking that it's ASCII only. To go the other way, you probably only want to check that the byte is &lt; 128 first, otherwise it's not ASCII.
We could make a lint to catch redundant patterns. `&amp; ref x`, `&amp;mut ref mut x`... are there others?
Thanks, but trying to use the scoped threadpool crate is not proving much easier. I currently have fn compare_charsets&lt;'a&gt;(set1: &amp;Charset, set2: &amp;Charset) -&gt; CharsetComparisson&lt;'a&gt; { let mut pool = Pool::new(4); let (tx, rx) = channel(); pool.scoped(|scope| { for word1 in set1.iter() { for word2 in set2.iter() { let tx = tx.clone(); unsafe { scope.execute(move|| { if word1.chars.is_disjoint(&amp;word2.chars) { tx.send(((*word1).clone(), (*word2).clone())).unwrap(); } }); } } println!("{}", word1.original); } }); let mut matches = vec![]; for word_pair in rx.iter() { println!("{} =&gt; {}", &amp;word_pair.0.original, &amp;word_pair.1.original); matches.push(word_pair); } matches.into_boxed_slice() } but the compiler is complaining just as much at me. I'm seeing Compiling state-dict v0.1.0 (file:///home/nate/git/state-dict) main.rs:35:27: 35:33 error: cannot infer an appropriate lifetime due to conflicting requirements main.rs:35 for word1 in set1.iter() { ^~~~~~ note: in expansion of for loop expansion main.rs:35:9: 47:10 note: expansion site note: in expansion of closure expansion main.rs:34:17: 48:6 note: expansion site main.rs:30:83: 57:2 note: first, the lifetime cannot outlive the anonymous lifetime #2 defined on the block at 30:82... main.rs:30 fn compare_charsets&lt;'a&gt;(set1: &amp;Charset, set2: &amp;Charset) -&gt; CharsetComparisson&lt;'a&gt; { main.rs:31 let mut pool = Pool::new(4); main.rs:32 main.rs:33 let (tx, rx) = channel(); main.rs:34 pool.scoped(|scope| { main.rs:35 for word1 in set1.iter() { ... main.rs:35:27: 35:33 note: ...so that types are compatible (expected `&amp;[CharsetEntry&lt;'_&gt;]`, found `&amp;[CharsetEntry&lt;'_&gt;]`) main.rs:35 for word1 in set1.iter() { ^~~~~~ note: in expansion of for loop expansion main.rs:35:9: 47:10 note: expansion site note: in expansion of closure expansion main.rs:34:17: 48:6 note: expansion site main.rs:34:25: 48:6 note: but, the lifetime must be valid for the anonymous lifetime #1 defined on the block at 34:24... main.rs:34 pool.scoped(|scope| { main.rs:35 for word1 in set1.iter() { main.rs:36 for word2 in set2.iter() { main.rs:37 let tx = tx.clone(); main.rs:38 unsafe { main.rs:39 scope.execute(move|| { ... main.rs:39:21: 39:26 note: ...so that the reference type `&amp;scoped_threadpool::Scope&lt;'_, '_&gt;` does not outlive the data it points at main.rs:39 scope.execute(move|| { ^~~~~ note: in expansion of for loop expansion main.rs:36:13: 45:14 note: expansion site note: in expansion of for loop expansion main.rs:35:9: 47:10 note: expansion site note: in expansion of closure expansion main.rs:34:17: 48:6 note: expansion site error: aborting due to previous error Could not compile `state-dict`. To learn more, run the command again with --verbose. When I try to compile and I feel like I just don't know enough about lifetimes to understand how I can fix this.
seems all benchmarks run on single thread, according to the repo
https://github.com/arcnmx/nue 
An important distinction is between abstraction for papering over unfinished competition (which is sometimes necessary, but here probably we should converge instead), and abstraction for computational reasons. Mixing different matrix representations in algorithms is important in my work, for example. These are dense, those are sparse, the other one is a view over some columns of the dense one. Are you saying I should man up and copy? ;) So /u/vbarrielle: seems to me like we see eye to eye here, and I think you know a lot about writing rusty code, so please do lead the linalg design conversation, and I'll try to contribute as well. Due to installed base, interoperating with NumPy and R is much more important than with nalgebra and scirust (I say this despite having contributed a bit to scirust). 
It is correct syntax but there is no reason to write it unless you are testing the keyboard.
...or unless you want to check that people actually read the docs ;-)
Yeah, sorry, I overlooked that the argument to transmute was a reference, not a `Box`.
the argument is against methods being put inside the description of the type. Methods inside types actually damages modularity/re-useability, by drawing dependancies into one place. Free functions help you decouple. Its' covered by meyers. bjarne and sutter agree and proposed UFCS to fix the problem. Thanks to OOP culture (and in particular the model popularised by Java) we have this horrible misconception in peoples minds that functions inside a type are special, even essential google 'free functions increase encapsulation'
Having an OSX developer on board would certainly help!
member functions for virtual dispatch are fair enough , IMO. the case i'm interested in is compile-time dispatch however. The UFCS approach seems perfect to me.
180K requests per second on an I7 is not bad at all. Hyper manages 280K on a similar machine, but then it uses all 8 logical cores. It would be interesting to see how it would perform with an optimized http parsing implementation. The parsing library used in this POC and in Hyper doesn't use any SIMD instructions AFAIK which leaves a lot of performance on the table.
Yes, haven't thought the design through either, but that sounds in the right direction. I've shot my foot with NumPy implicit broadcasting. But if we have to live with explicit but cheap broadcasting as for example in mat.broadcast_like(other_mat) + other_mat (which creates a strided view), that would not be a bad world either. 
raw SIMD bindings/ compiler intrinsic aren't fully stabilized yet. I think 1.3 is deprecating core::SIMD (which was never stable anyways) While at the same time /u/dbaupp has an external crate for SIMD support on x86_64 and ARM: https://huonw.github.io/simd/simd/ Edit 1: Hi!
There is also https://github.com/shepmaster/jetscii which accelerates some operations common in hand-written http parsers. It uses inline asm, so it doesn't depend on any intrinsics.
At least on linux, there is no way of making a filesystem call that is guaranteed not to block. You can use a thread pool, but that kind of defeats the point and you might just as well execute your request handlers in a thread pool and not deal with the state machines. Or you can use AIO calls on the underlying block device (which is guaranteed not to block) but then you have to implement your own filesystem and caching layer.
&gt; It's not tight to any particular IDE or text editor, it's a self-contained library with its own pipeline of parsing, binding, type checking, code completion and so on. Cool
&gt; I think you know a lot about writing rusty code, so please do lead the linalg design conversation, and I'll try to contribute as well. I don't know that much about rusty code actually, I still feel a lot influenced by my C++ style. But that would be great if we and some other people interested in linear algebra could cooperate to eventually produce a one true linear algebra framework. Right now though I feel I need to get sprs more feature complete to really see the requirements for good linalg;
Are you interested in collaborators? I checked the repo out and was considering redoing the tokenizer/parser.
More on the failed dream of async IO: http://neugierig.org/software/blog/2011/12/nonblocking-disk-io.html As described there, I think lightweight threads are a better approach. Encoding a state machine representing the various points of an HTTP connection is isomorphic to the instruction pointer of a lightweight thread, and it has the extra benefit that it actually works, unlike async IO.
Oh, that's handy! Much less clunky than `' ' as u8`. Thanks.
You should use iterators more. Not only is it idiomatic, it can also sometimes elide bounds checks for faster code. Look at the `iter` and `iter_mut` methods. Apart from that, if you don't already, try to compile with current nightly, this may have some optimizations not yet available in stable.
&gt; and that would give bitflags extra downloads. Why is that a problem?
I sped the C++ one up by 20% or so using `openmp`, so maybe look for parallelisation opportunities.
What do you mean by lightweight threads? AFAIK, the only thing that can manage a page fault or disk IO without blocking the event loop is a regular thread.
Why? Atomic values give me the willies because I never know what an appropriate ordering argument is.
https://crates.io/crates/com and https://crates.io/crates/com-rs might be worth looking into. I found them by searching crates.io for "com".
The winapi-rs repository also has some COM stuff going on, but no idea what they support. https://github.com/retep998/winapi-rs/
You're right. I meant like goroutines, which as far as I understand are lightweight threads that get multiplexed onto real threads when they make syscalls. Semantically lightweight threads, implemented using threads occasionally.
Then just use SeqCst, which is the most strict.
I've been meaning to add sub-command support to [pirate](https://github.com/zcdziura/pirate), which is a command-line arguments parser, similar to `getopts`. Can always take a look though that!
If you want to inspect the buffer without removing data from it, you can call `BufRead::fill_buf()`. Notice that it *can* perform an I/O operation, but in practice it will only read more into the buffer when it is empty. If the returned buffer is empty then the underlying source has run out of bytes (reached end-of-file [EOF]). You can then call `BufRead::consume()` to remove `n` bytes of data from the lower end of the buffer. You might also want to keep in mind that `str::from_utf8()` is a *linear* check, as it has to iterate the byte-string to ensure it is valid UTF-8. Calling it frequently on a long buffer could get expensive. There's an alternative, but it requires `unsafe`.
Because the cost of locking and unlocking a mutex is going to be huge compared to the cost of reading an atomic boolean variable. And it should be syntactically much simpler as well.
Remindme! 2 months to look at this again.
Do you mean the [mpsc](https://doc.rust-lang.org/stable/std/sync/mpsc/) module?
Macros aren't very composable or easy to encapsulate. A lot of crates' macros break if you try to import just specific macros (because helper macros need to be available too).
`winapi` supports COM just fine. It provides types with methods impl'd on them such that you can call COM methods as Rust methods (for example`something.AddRef()` or if you only have a `*mut T` then `(*something).AddRef()`). It also takes advantage of `Deref` so you can call methods from parent interfaces on pointers to derived interfaces, as well as pass derived interfaces to functions expecting parent interfaces. It does not however provide any sort of smart pointer wrapper to automatically call `AddRef` and `Release` for you (although one can easily be written). Also constructing an instance of a COM interface means calling `CoCreateInstance` yourself. All the data structures and interfaces are provided by `winapi` and functions by the hundreds of `-sys` crates that fall within `winapi`'s umbrella. If something you need is missing, just leave me a message.
But inline assembly is also unstable. It's actually more likely that the new SIMD stuff will stabilize before assembly does. In that case, I'll move on over to the SIMD primitives. :-) 
Hey, winapi-rs seems like a really big project. The part that I need is defined in UrlHist.h (not really a file often needed...) and seems to be missing. Would be really nice if you could add support for that. 
&gt; idk reddit name Hi!
Even so, the whole Rust argument is that correct safe code can't have memory errors. One hack around this is making the constructor `unsafe`, which allows its methods to also do crazy things without themselves being marked `unsafe`. If you really want to ignore Rust's philosophy for maximum speed, you're not going to want to return a `&amp;[T]` anyway - they're checked.
Author of com-rs here. At the moment it just has the bare minimum functionality I need to work on my directx crate, but I will be adding more functionality eventually. Could you explain what currently makes it unusable?
I'm looking to use this library but i can't figure out how make the different state machines talk to each other. Basically I want to have data received by one state machine trigger actions on another, and updating its state in the process. The only way I have figured out is to move the state to the context, but that kind of breaks the model. This sort of global coordination is the reason i want to use state machines in the first place.
Adding new stuff to winapi-rs is actually pretty easy, even the COM stuff is pretty quick because it's done through macros. If you have the time I'm sure other people would appreciate it if you could make a PR adding the missing definitions :)
So basically immediately after I wrote what I was working on this week, I started a totally different project ;) I've been feeling like I just need to type certain things over and over so that I don't have to think about them as much and can think about other things. So I threw some little exercises up on twitter, people seemed to like them, I put them in a repo, and now I'm trying to add about one a day :) This is very much do-the-simplest-thing-that-could-possibly-work, it'd be nice to get this into an interface with a bit less friction, but it's already better than it started out thanks to [btbytes' prlink](https://github.com/btbytes/prlink) that makes it easy to turn rust files into playground links :) I'd love to have suggestions or contributions!!!
That doesn't make sense to me—what requires you to use threads? nginx doesn't use a thread, goroutine-style or otherwise, per connection. And if nginx actually had a stack per connection (as it would be if it were written in goroutine style) I think it wouldn't be much if any faster than it would be if it just spawned an OS thread per connection. To me, async/await is really interesting because it doesn't use an OS thread per connection, so it compiles to nginx-style code under the hood, but it still maintains synchronous code. You let the compiler generate the complex state machine.
I saw on the HN thread that you're worrying about performance while I'm worrying more about programmer sanity and correctness. (I think of all these apps written with supposedly async APIs and how they actually block all over the place. Maybe I'm just bitter because, as I wrote above, even Chrome tried to be careful about mixing asnc APIs with a thread pool when necessary and only discovered we got it wrong by instrumentation, which is to say it was hanging on end user machines.) I agree with the other poster on the HN thread that the performance of large systems washes out. But I am no expert in this area. FWIW it looks like nginx eventually added support to use thread pools for read() and sendfile() (because both block): https://www.nginx.com/blog/thread-pools-boost-performance-9x/
One thing that wasn't mentioned was the "race to sleep" goal—basically spending more energy to get to a lower CPU sleep state earlier. I love the idea of dynamically tuning the system in real time, and hopefully that will take this into account. Is the author here or does anyone else know if race-to-sleep was taken into account?
Nice i add it to https://github.com/ctjhoa/rust-learning
In sprs almost all matrix functionality is defined for both views and owning matrices. But that's not done with a trait, it's because the `CsMat`type has its undelrying storage generic with a `Deref&lt;Target=[N]&gt;` bound, which enables a single implementation for owning matrices and views. In particular there is column iteration (for CSC matrices) on views.
You communicate between state machines by messages. This is done this way for two reasons: 1. Rust borrow checker. We borrow a state machine from Slab, so can't borrow anything from the same slab 2. To avoid deadlocks. If we allow to freely communicate between state machines (e.g. by putting refcell's into slab, instead of state machine directly), you could have a deadlock (or actually a panic), when touching other machine. Note messaging in mio is done with fast queue which does no memory allocations. And yes, It's not implemented yet. But shouldn't be hard to implement. 
Oh, I'm in total agreement that you want to *write* code that looks as synchronous as you can, from an ergonomic point of view. As I mentioned on the HN thread, I'm not a fan of callback soup.
There is more to async file I/O than asynchronous writes/reads, the file system is just another attack surface for applications. Quoting from [Boost.AFIO introduction](https://boostgsoc13.github.io/boost.afio/doc/html/afio/introduction.html): &gt; Your code has to interact with a regularly changing filesystem and not get weird race errors e.g. you try to create a new file in path /X/Y/Z, but some other program has just renamed directory /X/Y to /A/B in the time between you deciding on /X/Y/Z and getting round to it. &gt; Your code keeps file handles open a long time in a place where others might delete or rename them, including any part of the directory hierarchy preceding the file.
Thank you. I will read up more about mapping.
As always, you optimize what you measure. Optimizing energy profile hasn't been done much in current browsers (I believe there was a Chrome "optimization" once that lead to greatly reduced battery life on linux. Rolling this back could well have been the first browser optimization for energy consumption), and I don't think there is a browser engine better suited to this than servo (with all that auto-generated instrumentation stuff).
That is actually a great example, now I got parts of it working. Unfortunately now I get a "Wrong parameter." hresult back, which first of all means that the function was called, but that my definition of IEnumSTATURL must be wrong... I would appreciate your help: https://gist.github.com/anonymous/2426ac7557122a785651
CPython does not unwind its C-level call stack like C++ exceptions do, but at the Python level it’s effectively like unwinding in every way: the Python call stack frames get removed one by one from the stack without necessary cooperation from each individual frame. And I’m not that interested in CPython internals. I care about Python the language, whose implementation sometimes happens to be CPython, sometimes PyPy, and possibly something else. As to “X isn’t what panics were designed for”… says who? My understanding of panics is that they’re appropriate in situations that Really Should Not happen (and thus indicate a bug when they do). That seems to be the case when a Python callback raises an exception that it wasn’t supposed to. And, regardless, what else can you do?
Since a web engine should render pages as fast as possible, it's no longer acceptable to wait for all data before showing something. But this introduces a problem: Some pages load a lot of stuff that changes how everything else is formatted. This is called *reflow*, and pages that have a lot of it take considerably more energy than pages that render straight away. Now depending on some factors (like for example "how likely is it that I have this CSS fresh in the cache?"), Servo could back off rendering if it encounters a site that is likely to have a lot of reflow. This would be slower to show the site, but take a lot less energy. If this became a "low energy profile" in Servo, I'd definitely use it on my cellphone.
Do you have any source for caches playing into this. My assumption was always that cache invalidation is hard enough as is, and this only pertains to out-of-order execution.
Your Map owns its data. It doesn't let you take it out directly (unless you remove it), so to `get` it, you have to *borrow* it. Now in Rust, there are two kinds of borrows: a single mutable borrow or possibly multiple immutable borrows. Note that in Rust you can only have one or the others. Your Map method however can only have one return type – you cannot return something that is one thing or another, as in mutable in one context, immutable in another. Rust doesn't operate that way. So we need two function to return an immutable and a mutable borrowed value. In fact there is a third method: `.entry(_)`, which returns a mutable Entry that wraps the (mutable) value. The nice thing about this is that the method names make pretty clear what you are doing and as immutability is the default, you usually get away with the shorter method name.
Well, if it didn't provide an API, then you wouldn't get to mutate it. So it does. What if none of the methods existed? It would be quite useless. That's why they exist. If the question is "why isn't get() mutable by default" the answer is because rust prefers immutability, and when you want mutability you must be explicit. It would also be a huge pain to lend out mutable references by default, as this would mean that you'd potentially have many mutable references to the same item - this is dangerous, because now you have multiple entry points for mutation. What happens if two different areas of code try to mutate it? What if they do it simultaneously? Not good. So just hand out immutable references, which are safe, by default.
Awesome, thank you!!
&gt; It would also be a huge pain to lend out mutable references by default, as this would mean that you'd potentially have many mutable references to the same item The huge pain would really be that the compiler would forbid that, so you could only `get` a single value at a time from a map.
Don't be afraid to post things like this to SO; the Rust community there may be smaller, but SO questions are far more likely to be useful to future people with the same question than Reddit threads. I'd also request that you practice the same level of professionalism that you'd expect on SO, such that a title like "RustRetard" is not encouraged. :P
Yeah, judging by the -6 I think that was a mistake. I'm pretty new to Reddit so I'm not sure how professional people act and what the desired formality is, et cetera.
I know next to nothing about this, but I do know that http://doc.rust-lang.org/stable/std/thread/struct.Builder.html#method.stack_size exists, so I would _imagine_ that you can get a thread with no stack without compiler support. Of course, all of these kinds of features get _nicer_ to use with compiler support... &gt; Why is setting up a stack expensive in the first place? Memory per stack * number of threads == maximum number of threads you can have before you run out of RAM. memory == 0 means infinite threads, from this perspective anyway.
Further to what /u/kibwen said, you can just pass C-compatible data structures back and forth across the FFI boundary using ctypes – working with them in rust is straightforward, and it allows you to integrate with Numpy. I wrote https://github.com/urschrei/convertbng (and the actual rust library, https://github.com/urschrei/rust_bng) (with a lot of help from /u/shepmaster) as a way of trying it out. 
Just checked on Chrome, and hello world is compiling fine for me... Do you have a variable "ace" that you try to use that you haven't defined with a let ace = someValue; ? or maybe it's a typo, and meant to be a different variable. (ie. "pace")
If you want to print out just the capacity use `a.capacity`.
I think in this case the `ace` being referred to is a JavaScript variable related to the Ace Editor which is used by the playpen. **Edit:** For the record, working fine on safari/ios.
My intent with this one was to guide toward adding `#[derive(Debug)]` to the `struct`, but I like the direction you've taken it! Do you like the open-endedness of the exercises or do they need more direction? Should there be hints available, maybe a link to relevant sections of the book/rustbyexample?
Yes, exactly. Your code would be getting multiple mutable references and therefor would not compile.
Cool! QtCreator was my favorite IDE after MSVC++6 ... until I found Rust. Kind of a shame it's so biased in terms of build systems though. So now I will be able to use both?
There are several termbox (like ncurses, but nicer IMO) bindings. I'm working on [an OpenGL based one](https://github.com/Daggerbot/glterm), but it's not quite ready for public use yet.
It is critically important that when defining COM interfaces you do not miss a single method, and that you have them in the correct order. Your definition for `IUrlHistoryStg` is missing quite a few methods, so you're actually calling an entirely different method resulting in the error. Here's a somewhat simplified version of your code that works with the latest git version of winapi (I'll publish a new version to crates.io later today probably): https://gist.github.com/retep998/477bb297edf57c9f8bca EDIT: Published a new version of winapi with the urlhist stuff.
There is a socket option on modern Linux kernels that allows listening on the same port on multiple threads and equally distributes the requests using flow-consistent hashing. You can just turn it on and immediately get multicore scaling with no in-language synchronization required (obviously, you still have to synchronize shared data, but there are really fast multithreaded data structures). On a 6-core virtual machine (on Linode) I got to about 300k req / s serving a static HTTP response on a real mio-based server, and that was using a poor load testing strategy (i.e., the client and server were on the same computer).
Not the author, but the mentor for the work :-) Their group has done a bunch of per-platform research before on pace-to-idle vs. race-to-idle strategies, and it turns out that pace-to-idle is almost always superior! Check out their preprint at http://people.cs.uchicago.edu/~hankhoffmann/kim-cpsna2015.pdf (cimes, who did this work, is on vacation this week before returning to his Ph.D. candidacy)
One thing I noticed right away is that your formatting is a little un-idiomatic. Would you like me to send a PR? Those functions do have some nicer replacements, I'll send you some for them.
I think you just coined the term for rust koans. ;)
I updated, it seems you're about 20% slower, but I'm not really sure what's causing it.
Does anyone else thinks more effort should be put into integrating Rust with other technologies then on developing language?
What specifically do you mean by this? (and some work _is_ being done in this area)
Ideally you'd have a `&amp;[WCHAR]` or `[WCHAR; SIZE]` of some sort so you can call `OsString::from_wide` on it. If you need to determine the length with a null terminator, then you can calculate that via `let len = wide.iter().position(|&amp;c| c == 0)`, then you can `OsString::from_wide(&amp;wide[..len])`. If all you have is a raw pointer, then things get a bit painful since there's no convenient Rust abstraction for that in libstd yet. You'd have to first figure out the length either by looping and doing raw pointer offsets, or by calling `wstrlen` via FFI. Then you'd call `slice::from_raw_parts` to get a `&amp;[WCHAR]`, and from there you can use `OsString::from_wide` as usual. Once you have an `OsString` there are several methods you can use to turn it into a `String` if you want.
You can either admonish people, or you can help them. In the Real World (tm), not everyone can get away from XP. So we can help them out, or we can tell them that they're bad, even though there might be nothing they can do about it. We choose 'help'.
Not necessarily. I'm a Linux user and I keep XP around for nostalgia gaming machines which I might want to code some helpers for. (I like to dual- or triple-boot some combination of WinXP, Win98, MS-DOS6.22+Win3.11 for Workgroups, and FreeDOS) (Though, to be fair, only because I have some older hand-me-down PCs with pre-activated XP OEM and that let me bend my rules a bit. If it weren't for that, my strict "No online-activation DRM. If I'd have to pirate it, I'll shun it instead" policy would limit me to the legit Win98 and Win98SE licenses I happen to own.)
Can't wait to play with it tonight! Does anyone know when box_patterns will be stabilized? 
Cool, thanks for expanding :) &gt; I think it needs time to become stable Is there something specific about the current stability setup that makes it not stable for your purposes? I certainly agree that more and better bindings would be wonderful, but many of the things that you're talking about also need language developments for excellent bindings: QT, for example, is in C++, and so we need better ways of mapping advanced features over a C like interface, or growing C++ interfaces directly, before said bindings can be made. &gt; Support for other platforms, a lot of people would love to give a shot on Android developing in Rust (including me). We already test every single pull request on Android, it should be well supported.
It would be fun sounding to pair this piece of news with Athlon XP support :) At least now I understand why you've been so busy lately... EDIT: Isn't support for older processors implied in Windows XP being supported though?
:D thank you! It's the first project I've put a lot of work into and I'm really glad people are getting some use out of it. To be perfectly honest I haven't given it as much attention as of late, school / college applications and all have been eating up my time. That said, this weekend should afford me some time to nail down a platform agnostic event polling system (signals are hell). The osx branch is a very rough implementation at the moment, thank you for trying it out! I'll be refining it in the days/weeks to come.
Yes I was referring to the ace editor. It seems to not be loaded for some reason.
&gt; Is there something specific about the current stability setup that makes it not stable for your purposes? Sorry, part about stability wasn't my opinion, I wanted to write a simple 2D game engine in Rust+OpenGL but I was discouraged by one more advanced Rust developer that bindings to OGL can change anytime and are not stable. The talk had place like a month ago.
I like the inevitable progress of Rust implementation. I'm on nightly by necessity, but current stable has a higher version now than the nightly I started with... Cheers to the Rust team and keep up the good work!
That's why I started this thread with "more effort should be put into integrating Rust with other technologies" ;)
I don't see how fixing bugs for people is making them less secure.
A large majority of hospitals are on xp for their terminals (computers in the hall and patient rooms), which is scary in my opinion.
Any help in making their current, less secure platform palatable is contributing to them staying with that platform.
The bindings generated by `gl_generator` are very unlikely to change. They haven't changed in the past year and a half or so, except when we upgraded to OpenGL 4.5. 
/r/botsrights
Do you have the RFC for that? It's something I've been trying to follow, but wasn't sure where
This might be of interest. http://people.cs.vt.edu/~asandu/Public/Qual2005/Q2005_skjellum.pdf
https://github.com/rust-lang/rfcs/pull/1228 is the currently open one. https://github.com/rust-lang/rfcs/pull/809 is the recently-merged one.
This probably isn't the place to ask, but for the proposed syntax/API example of: `vecdeque.back_place() &lt;- 42`, what's the return type of `back_place()`?
I updated it too. Now I get 1,06x of the C++ version by replacing the Vec with Boxed arrays (unsafe version) and 1,25x using the safe version. I am using Rust 1.3, I updated before thinking about it. How does it compare with yours? What I have to investigate tomorrow is why I was having overflow in my original version with boxed arrays. I understood the problem with arrays, but I still don't understand why I was getting overflow with Box. Especially given that now it works. Maybe it is because of Rust 1.3. Which version are you using?
&gt; I think that is where languages like Go fail, they don't have great developer experiences. My experience is the exact opposite. Go's tools have been some of the best/enjoyable I've ever used. I haven't used anything comparable to Cargo, but that isn't the only tool that matters. :-)
Since you asked... I would love better integration with profiling tools. Have to admit part of the problem is the very high expectations that the rust eco-system sets up. TL;DR: when rustc/llvm inline a function, could the "inlining path" be preserved somewhere, so that e.g. the flamegraph representation of the profile doesn't become completely flat and useless? For example, when using perf, I need to remember to to use -g in rustc (or debug=true in cargo), and then also -g and -call-graph dwarf in perf record, instead of just doing cargo profile perf &lt;optional benchmark name&gt; To have the benchmarks run and profiled. After I know to run everything correctly, what perf shows is some asm and rust source which are pretty weakly correlated (blocks of rust, blocks of asm, connections not often obvious). I understand that both rustc and then llvm transform the code, and the latter is not under rust's control, but rust could better show the relation between rust code and the generated llvm-ir. rustc emit=llvm-ir does not include any code annotations, the best I find is the function names in the calls. On the other side, it seems like it should be feasible to change the DWARF information so that perf thinks that the source that should be shown next to the asm is the (itself nicely annotated :) ) llvm-ir file. Then the discrepancy between the asm and llvm-ir would be only due to LLVM and maybe easier to follow. I'm not entirely sure this two stage process would be better overall, but maybe. Of course, if there is a more user friendly profiling solution, that would be great. I've tried valgrind/kcachegrind, but that usually does not find my source files, has no commandline to specify them (that I've found) and even after being given the source directories gives somewhat erratic output. As someone that rust enticed from "higher level" languages, this has been one of the few pain points. Compared to this the borrow checker is a cakewalk.
Thanks! I'd love to have your help contributing any exercises or any bling that you'd be interested in ;)
I refuse to believe that incentives to move away from XP has zero effect. Companies can upgrade. Individuals can switch to Linux.
Hnnnnngghhh. Super interesting! I admit I'm still wrapping my head around the timely dataflow stuff, but it seems very cool. I skimmed the Naiad paper, watched your channel9 video and watched one of your colleague's Ricon talk. Which helped a bit :) Random questions / musing-out-loud in no particular order: - Does Timely deal with fault tolerance? I saw references in Naiad about checkpointing, but it wasn't clear to me what this mean or how it works. - Is it possible for an entire computation to stall if, say, the input machine dies before the epoch is finished and downstream operators are waiting for epoch completion? - If your "working set" was larger than memory, I suppose each operator could just keep state serialized locally and page it on/off disk as required to perform the work? - Is there a way (or plans to) see the state of the frontier? E.g. as a diagnostic to see if certain computations have stalled, overall progress, etc. I see a lot of Timely-specific logging in the docs, so perhaps just monitoring the logs? - Is there a way to explicitly kill a running computation before it reaches completion? - If I start a new computation at runtime, will it reach out and tell the other nodes that a new computation has started? Or does it need to be setup first on all the nodes before starting?
I think foremost this is a cool project even in its current format. I hope you add more exercises. If you do add explanations I would prefer solutions to hints. If there is an obvious alternative solution, then a short comment in explaining why this approach was taken over the other would be helpful. Pie in the sky thought: your comments about repetition make me think of spaced repetition (like Anki), but as an interactive programming environment instead of flash cards. 
You can just transmute the pointer back to Box.
Then it'll just be de-allocated when it passes out of scoped? edit1: yup, thank you :D edit2: Really odd pattern for this, but it works. let _ :Box&lt; T &gt; = mem::transmute( *mut T );
It's all good! I share your disdain for supporting legacy platforms. I was just tweeting about Ruby 1.8 the other day. Unfortunately, it's just a fact of life.
Since `open` is generic, the trait is not "Object-safe". Generic methods cannot be encoded in a vtable, because there are theoretically an infinite number of possible implementations. What you can do is have `open` take &amp;Path instead of being generic over `AsRef&lt;Path&gt;` (at the slightly lost of erganomics)
I suppose there are other significant priorities but I'd imagine the general work for it would largely be handled by another team.
It's not about language syntax, its about memory management. If you wanted to be like Go, wrap everything in a Rc (or Gc if you're using rust-gc). That is kind of what you're doing with &amp;, you're using a pointer, just as Go does.
Just FYI Bullet Physics is a C++, not C library. It has a minor C API but it is highly highly incomplete.. You'll end up binding most of whatever you want in C first.
Just so you know you're not alone, I had this exact same problem in the exact same context, a couple of months back. I vaguely recall an issue being opened about it, but I don't think anything progressed. I was told that `type_id` is *supposed* to be stable across builds, but as you said, it currently *isn't*. One thing you *might* be able to do to try and make the string name more robust is use `$crate` to get at the name of the crate the code is being called in, and return that *as well*. At the point you have to serialise these two-part names, you can canonicalise them by picking a specific root crate and adjusting the name as necessary.
I'd like to think aturon and dherman are just really great managers (they are) that magically create the illusion that the Rust team never has to interact with The Greater Mozilla, but in reality I get the impression that they basically operate completely independently. Like, they talk, but there's little Mozilla gives the Rust team other than BLOOD SOAKED PILES OF UNMARKED CASH. Occasionally there's some one-off graphics/legal consultations. A lot of our infra, for instance, is basically random computers under aturon's and brson's desk...
I've been using this subreddit to learn rust and it's been a valuable resource.
In general, executables compiled for one processor/os combination do *not* work elsewhere. You'd need to cross-compile to Windows. This is, generally, a huge pain in the backside. If you want to attempt it, you'll need at least the following: 1. The Windows runtime libraries so Rust can link against them (make sure to pick the right ones; there are four). 2. A cross toolchain that targets the same platform as those runtime libraries (I believe you need `gcc`, `ld`, and *maybe* `ar`?). 3. You may or may not need to modify your Cargo config to point it at these cross tools; I've never used Cargo with cross compilation before, so I don't know what you need to do here. At that point, building for Windows *should* "just work". Good luck.
[removed]
I don't have an issue with references itself, I have used C++ quite a bit. It's just syntactically odd that AsRef/Borrow can't be used in trait objects (although it does make sense from a technical point due to the generics+dynamic dispatch). It seems like a bit of of an increase to the syntax burden to the caller. Granted you only have to add &amp;, so not the end of the world but by understanding in the Borrow/AsRef eliminates the need for that. And rust's println! macro similarly deals with eliminating the need for syntax (ie no need to dereference pointers). With something like [Borrow&lt;T&gt; or AsRef&lt;T&gt;](https://doc.rust-lang.org/book/borrow-and-asref.html) generics, my understanding it should work for either an owned or borrowed type so no need to add any syntax. Which works great when your not using a trait-object. But it seems to fallover when it is a trait object because the dynamic dispatch stuff doesn't work with generics (which kind of makes sense since you would be mixing runtime and compile time). It seems like it could technically be done though. Either through some kind of reverse generic, where instead of the called function being rewritten, the calling code could be rewritten to unwrap whatever type you have (owned, &amp;ref, Rc, Box, Gc, etc...) automagically. Might be a bit too odd to make it worth while. Could be restricted to the very specific case, it trait-objects only and AsRef/Borrow only. Maybe 'macro' would be a better description for it. Actually a kind of member macro might do it where you could call fs.open!(path) and it would insert the code to unwrap the type which would include calling the actual open(&amp;path). A thorough implementation could take the full range of Boxes, &amp;refs, Rc's, Gc's and so on and unwrap them to the correct type. Although the error handling for the smart pointer types might be an issue. I guess a simpler implementation would be to just use open(fs, path) with generics instead of fs.open(path).
I've just scrobbled through it randomly for a few minutes (this thing is almost 2 hours long!), and it looks more like "How to Rust" than "Why Rust" to me. That said, from the few snippets I saw it looks like a good introduction.
*Should*? No. But there's also the [forum](https://users.rust-lang.org/) if you want alternatives.
The timely code is not fault-tolerant. It's a non-goal at the moment. It is meant for cluster compute on streams that have been persisted in other ways (or that you don't care to persist). Naiad had more stuff, but mostly because the research community is all hot and bothered about fault-tolerance, because they want to pretend they are Google-scale (they aren't). If a machine dies, the computation gets torn down, because the socket dies. If a machine stalls silently, the computation will stall. If you go to swap, you go to swap. Generally the performance is "fine", because the operator implementations are designed to be cache-friendly/oblivious. If you end up needing to do random access to state larger than your RAM, no magic there. But, you don't need to manually serialize anything (you can, of course!). You can, already! The `probe` operator captures the state of the frontier at any point in the dataflow graph. At the moment it is mostly used to query `lt` and `le`progress, but if you'd like to look at the state it exports, it is just a matter of making the fields public. If you mean more like a dashboard, the logging signals are what lead to that, but it is (currently) up to you to watch them. We're (some folks at ETHZ) in the process of writing some tools for this, though. You currently kill a computation with `^c`, and start a new one by running processes. Timely isn't a cluster management software, it's just a compute runtime. It is possible to do this (you can back timely dataflow graphs with other processes), but none of it has been spec'd out or tried in anything other than cheap prototypes. Good questions, though!
Take a look at https://github.com/ctjhoa/rust-learning if you have any questions before posting
I have a couple of separate nostalgia PCs for the same reason I have half a dozen different genuine console controllers hooked into my main PC... sometimes I want a certain minimum degree of authenticity of experience. (I've got three PCs in here and the one I haven't mentioned yet is a 133MHz Pentium with an under-monitor power center, dinky little box speakers, an external modem for its looks, the genuine SoundBlaster 16 with real OPL3 chip that I always wanted as a kid, and a genuine Gravis PC Gamepad I bought back when I was a kid. Unfortunately, Rust will never compile for DOS, so any helpers I write for that one will have to be in C using either DJGPP (protected mode) or the freeware release of Pacific C from the FreeDOS website (real mode)) (Or I suppose I could try learning something new that I've always associated with a bygone era. FreePascal has a DOS port.)
Thoughts while watching: ♢ "[Cargo] is a really nice build system." *(Steve is unable to build his code because he doesn't have internet access)* Frankly, I'm *astonished* it didn't try to update the index first. ♢ "Is this red going to be a thing?" At 240p, yes. It's an unreadable smudge in the midst of surprisingly legible code. High frequency red: every video codec's worst nightmare. ♢ `// don't forget the \n!` what if it's an `\r\n`? **#WindowsUsersMatter** ♢ (being able to grep for function names) Oh my **yes**. You mention Ruby, but C++/D are worse to me. *Urgh*. ♢ The discussion on mutability was a bit confusing. You say that `let mut x` lets you re-bind `x`, but you can't modify it "in place". But that's the opposite of what happens, AFAIK: you *do* modify the memory that the label `x` is bound to (*i.e.* the memory on the stack). Also, both `mut`s are saying the same thing: the storage this thing is referring to is mutable (in the case of `&amp;mut`, it's by virtue of having a unique pointer to it). But that could just be the way I see things. Wait... you're mutating through an immutable pointer? Wat‽ ♢ Isn't the compiler support for `Box` to allow you to move *out* of it? ♢ "The forbidden comment syntax!" **#BlockCommentsMatter** ♢ "Uhh.. uhh..." "Sync!" "Ah, yes." :D ♢ "Friends don't let friends do things that are wrong." Suddenly, I want an overlay in my editor that shows Ferris happy and smiling if everything compiles, concerned if there are warnings, and covering her eyes if there are errors. ♢ "Can you unlock manually?" Don't forget you can `drop` the guard, since you aren't persisting the borrows to the interior of the guard. ♢ You talk about macros being able to use arbitrary syntax and didn't mention `Hodor!`? :P Fun talk. I think it's a good one to keep a link to for showing people used to lower-level programming how Rust works and what it does.
`cargo profile`... now here's a subcommand I'd like to see. For now it could just build with `--release -g` and start perf, oprofile or valgrind (depending on configuration).
Yeah, your setup is perfectly fine and often the more viable option. I was just commenting on the "games run like shit on VMs" thing which is not universally true and is becoming less of an issue all the time with advancements in virtualization tech and hardware.
I think the *best* place to ask questions is at Stack Overflow. You'll get answers to basic questions pretty quick, with minimal noise. Another good venue is to hang out in the IRC channel (#rust on irc.mozilla.org)
&gt; most of the time you want a constant reference &gt; to a mutable object, you're probably better off &gt; just putting the object on the stack, &gt; aren't you (unless of course, the object is really big)? Having the object on the stack is fine but suppose you want to return this object from the function. How do you do it? A very common way is to allocate on the heap and return a pointer to it. The best way to do it in Rust is to return a Box&lt;T&gt;. I remember doing it in C. You malloc memory for a string, populate it and return a pointer. The caller is then responsible to invoke free() once it is finished using it.
There are many Linux distros that don't require users to be wealthy nor computer savvy.
&gt; I dunno if it's the one that the Servo team is planning to use No, we use Spidermonkey's GC https://blog.mozilla.org/research/2014/08/26/javascript-servos-only-garbage-collector/ &gt; they've been clamouring for a good GC for Rust for quite some time now What? Noooooooooooooooooooooooooo &gt;.&lt; We want better tools for integrating GCs, but we'd not like to go down the two-competing-GCs-in-one-application thankyouverymuch :P (Apparently Gecko has a DOM CC and a JS GC and the two hate each other.) Tools that make the spidermonkey GC integration safer/more efficient would be nice (e.g. LLVM's stack roots, etc). Things like rust-gc will not be used in Servo, even if it ships with Rust itself (not likely, IIRC the rust team is also more interested in a framework for GCs without an actual stdlib GC).
You need to be computer savvy enough to know they exist. And not everyone has the energy for the transition to linux, either. For many, it's a matter of familiarity. It takes time to get acclimatized to a new OS. Same issue with poorer schools. One thing that gets implicitly taught is familiarity with a computer. This really means "familiarity with an OS" and "familiarity with the Internet". The former is completely shot out of the water if they teach a non-Windows OS -- most jobs require _Windows_ knowledge. I know plenty of middle-aged+ folks who use their computers for surfing the Internet, email, and occasional document-making. Many are on XP/2000. They update the software (and browsers these days keep themselves up to date, which is sort of necessary because the Web changes _very_ rapidly), but they work the OS on muscle memory; they don't know how the computer works. A different OS would be rather daunting for them to learn, and would take time. Time they'd rather spend doing something else. I also know plenty of low-income families who have old machines. Switching to Linux is a non-starter because everyone _else_ uses Windows, and like it or not, LibreOffice/OpenOffice aren't quite compatible with Office. Their kids won't be able to use the home computer for working on their projects. No XP support for Rust means that these people can't get browser updates for Firefox. They can't get new software that was written in Rust. Don't drag them into your ideological warfare. 
Well, [you can use it now](https://stackoverflow.com/questions/30022084/how-do-i-set-connect-timeout-on-tcpstream/32650047#32650047) :)
Thanks a lot! I tried this and was able to get it working, see my [comment above](https://www.reddit.com/r/rust/comments/3kzff7/how_are_rust_threads_well_behaved/cv5pvif).
I had the same problem so I implemented the `select()` solution: https://stackoverflow.com/questions/30022084/how-do-i-set-connect-timeout-on-tcpstream/32650047#32650047
Yup. It's really about uniqueness. Mutating the pointee of `a` can only be done if there is only *one* way of accessing it at a time. I think you'll make peace with how it works in Rust. I did, too. :)
My "ideological warfare" is primarily anti-XP **in relation to which platforms rust should spend energy on supporting**. Schools and individuals with old XP machines and a lack of computer savvy should just continue to use whatever they wish.
Yeah well, I was replying to a comment of someone running WinXP, Win98, MS-DOS 6.22, Win 3.11 and FreeDOS. WinXP, sure, but the rest :)
Sorry I missed that. 
@ steveklabnik1 could you link the slides? :)
That's interesting, I had assumed that having the `TypeId` change when the crate changes was desired behavior, it would great if it stayed stable :D Ideally I'd like to avoid doing too much manipulation on the type-names to get this to work. Part of the reason I went with the solution I did was that it's relatively cheap to just take a substring of a static string, and it also keeps the resulting string short so it doesn't take long to hash. Even though it's a debug feature it still needs to be fast.
Its called engineering debt. Hospital systems, for example, are ridiculously costly to design, certify, and deploy because of all the laws surrounding patient privacy, etc. It would be asinine to invest the time and money to rebuild that system from the ground up for windows 7/8/10/Linux without a very good reason. And since they don't seem to view EOL support as a good reason they're not going to; they have way too much invested in their current platform. Is it unsafe? Potentially, if you're not careful. Is it the right thing to do? Potentially, if the cost to red engineer still outweighs the benefits.
True, the "standard" allows the compiler to rearrange struct members as it sees fit, but in practice I haven't observed it doing such a thing and as far as I know that feature hasn't been implemented yet. I also don't mind that it's unsafe because it's a debug-only feature. Once I start running into more issues I'll switch to a safe, stable solution.
Can you write if(memberAComesFirst) ? If so without unsafe you can make your API depend on it, which sounds bad to me.
&gt; As an Erlang programmer What types of things do you think you need GC for? I'm not sure how experienced in rust you are, but the lack of GC doesn't mean you have to do manual memory management like in C.
Hey, that RustConfig is for telling the heroku buildpack what rust version to use: https://github.com/emk/heroku-buildpack-rust
/u/pnkfelix is hard at work on *GC integration*, e.g. being able to talk to engines like V8 (for node.js integration) or Spidermonkey, both of which need some mutual understanding about their GC. His plan is to do this mostly in the library space, by providing a very general "trace hook" mechanism in Rust that is capable of walking over the structure of types at runtime. This is also closely connected to the Rust allocator design. While integration with existing GCs is the initial goal, the same infrastructure would hopefully support building a custom GC for Rust, again as a library. But that's more speculative.
Say you wanted to implement a GC'd language in Rust.
Actually `print!(..)` *does* return a `Result&lt;(), Error&gt;`. Where do you get the idea that it panics from? Edit: revisiting this, it does indeed panic if stdout cannot be written to.
Can't you just use `std::io::Write` if you want a result? That's ultimately what `print!` does anyway, as I understand it. Maybe you want `print!` to return a result, but it threads through multiple different systems; it could return IO errors or formatting errors. You'd have to wrap all that up in a conflated type that most people don't need... On top of that, your basic Hello World program would have to discard a result. That's not what you want such a basic program to require. It's not good practice, and it's a magic incantation.
print! appears to be defined in terms of "_print" defined here (and it panics): https://github.com/rust-lang/rust/blob/master/src/libstd/io/stdio.rs Am I looking at the wrong code? 
Wouldn't something like #[ignorable_result] which I proposed on the function which print! expands to work? To make hello worlds look clean (without lots of unwraps). I'm just concerned that this seems like such an unnecessary panic, and I'm concerned that panics like this will continue to creep in, causing unwind-less rust to be unusable, without going to great length finding out what may or may not panic.
I was more asking what OP in particular was looking to use a GC for, since he said "as an erlang programmer". I know there are plenty of legit usecases for GC.
That's the thing: putting #[allow(unused_result)] before every print! invocation is not going to make your code look clean. You'll be suppressing compiler warnings all over the place, and you'll develop a habit and comfort for ignoring results. On the other hand, anything may panic. What happens if you run out of memory? Should every function call always return an `PossiblyOutOfStackMemory` result? There are tradeoffs you have to make, and printing to stdout is reliable enough that if it doesn't work, you've probably got bigger problems than a possible panic in your code.
I think you should guide the user to the answers. In this exercise, people who don't know about the `#[derive(Debug)]` annotation may go for manually implementing the `fmt::Debug` trait. While it works, it's not really a good idea since adding a field to the `struct` requires them to also remember to add it to the trait implementation. And as you can see from /u/supBot's answer, people can make bigger changes to the code than you were expecting, which may not really help them learn anything new.
Alright, that makes it a little cleaner. You'd still have to wrap multiple error types into a single type for `print!` to return, which might make handling the errors more complicated than just using `Write` in the first place.
Why is `print!` panicking for you, anyway? Is stdout being closed?
You can get addresses of stack variables, static constants or heap allocations in safe code. You can determine whether ASLR is enabled in safe code. You can make your code depend on that. Rust is a systems language after all. I'm not sure what your point is.
I agree with the sentiment. Rust used to panic *a whole lot*, and before 1.0 there was an effort to squeeze them out. `print!` and `println!` panic to avoid the surprise of silently throwing away errors. These were considered special case conveniences, basically for prototype, 'hello world' style code, and that it's better for the errors to surface. Notice that [`write!`](http://doc.rust-lang.org/std/macro.write!.html) returns the result. Edit: /u/acrichto will know the exact rationale
I don't really have any other examples other than "print!/println!", and I may have gotten stuck on them, imagining that they are a symptom of a larger problem. If these are the only exceptions to an otherwise sparing use of panics, then it's not really a problem. I guess the expectation is that a normal unwrap-less program should expect never to unwind, unless oom or serious logic errors occur?
I suppose another option is to create another print macro that returns any IO errors.
While your suggestion makes sense *in theory*, I don't think it will work *in practice*. I believe we programmers are all lazy. If there's a chance to ignore the result, *we will do so*. `print!` returning `#[ignorable_result]` will effectively be the same as the one returning `()`. The errors will be silently ignored. See, `printf` in C does return a `Result`-ish numeric value that can be used for error handling. But nobody actually does that. And I don't believe Rust programmers are fundamentally more diligent than C programmers. :P
I understand that most people would never check the result if we modify print! to return an ignorable result. And the reason it panics now is that few would ever _want_ to check. But to have a critical program fail because it could not print stdout debug messages is probably not what people want either. As it is it seems that we have print!/println! to make small, one-off programs look nicer, and important long running programs will take care never to print!, and instead use write! and check the results. And I guess that's fine, actually... :)
I'm fine with `println!()` panicking. I don't want to be handling errors for something that should not fail in the first place. And if I really had to, I would use `write`. Any other examples?
Depending on the context, yes; and even in this case there will likely be a **main** macro in which is in scope for the remaining. Like `log`'s macros all referencing `log` internally. Although `log` is not a helper; it could contain any such helpers.
Then we are making assumptions about which macros are in scope. It's fine, document it, but it's a limitation.
Indeed. But this is better than filling the namespace with helper functions in which would quickly conflict cross-crates. And in either case, documentation is still needed. A helper macro that isn't documented to be `#[macro_use]`'d will definitely cause issues when a consumer selects specific macros.
DNSSEC may be important. However, DNSCrypt is yet more so. DNSSEC only verifies the integrity while DNSCrypt actually encrypts the queries, and so they cannot be sniffed to leak the domain name interests of the endpoints.
FWIW, I don't think you need GC integration to get to a very useful level of Node integration. I've been experimenting with building some Node bindings lately, and I'm getting close to being able to demonstrate a pretty useful spike (creating JS objects from Rust and passing them back to JS, and decent automation for the build process). TL;DR as long as you don't need to embed Rust data inside JS objects, I believe lifetimes are sufficient for defining a safe Node bindings API.
I think that their goal was to be `.unwrap()`-ish, in order to have easy "debug" output for beginners to toy with.
I think the explanation just got a bit off track because you started explaining mutability using `Box`, which put the focus on bindings, but then you were in the middle of changing to references when someone asked for more clarification about mutable bindings. This put the code you were referring to out of sync with what you were explaining, and it can be a bit hard to reconcile the two from that angle. Part of the issue is that the behaviour of `Box` is actually fairly nuanced. A `Box` only lets you dereference it mutably if you can borrow it mutably. There are two ways I can see of explaining this, one in terms of the borrow checker (it doesn't understand smart pointers directly, but instead the compiler takes out a borrow of the pointer before dereferencing it, which the borrow checker can then verify), and the other in terms of the rewrite rule for dereferencing smart pointers. Each of these requires a fair bit of context for an introductory talk, though. At least the first one introduces an important language concept, so that might be the way to go. The only other option I can see is to sidestep the issue somewhat and pretend that you always need a mutable binding to get mutable access to the contents (unless you use interior mutability), and then you can just say mutable references are the exception that proves the rule.
By the way, do you have a link to the issue about changing the behavior of `type_id()`? I can't find it myself.
I would much appreciate it if someone could explain a bit (or point at something) on why its hard, what rust is trying to do about it, and hence what we should expect? because I don't know if what I'm seeing is a configuration/use problem on my part, just the current state of the art, or a bug I should report.
You should post your questions to [Stackoverflow](http://stackoverflow.com/questions/tagged/rust) under `rust` tag. We're always happy to help you there, and also asking questions there will help lots of others who have the same or similar question.
There is generally not _One True Way&amp;trade;_ and you will have to tailor your API to the needs of the user. Is it all about filtering and data flow? Look at how iterators work. Are there a lot of knobs to tune? Look at builder patterns, or make a big struct, with public fields, and make use of the `Default` trait. It's hard to list all the possible solutions ;) Looking at the standard libraries for inspiration, as /u/killercup mentioned, is a good idea. It's the standard, after all. You can also take a look at popular or trending projects at crates.io or GitHub. They may sometimes provide some more innovative ways of solving problems. Whatever you do, don't sweat it. No one is an expert from the start, so begin with what you know and build it from there. Simply build, test, repeat. And ask questions. I use to think that it's better to ask, than to assume something that turns out to be wrong. This became a bit cheesy, but what I'm trying to say is that you should look at the needs before you look at the patterns. Nothing wrong with asking ahead, of course, but it's always easier to find solutions when you know the problems. That's all. Have fun! :)
`Clone`, `Debug` and `PartialEq` are the most important IMO.
Ah, yes, you'll need to either: 1. use `Cow` in your `struct` 2. use two different `struct`s with the two different types Seems like #2 is the way to go, there's no convention for doing this, exactly.
pixels -&gt; screen? I'd use Glutin + Glium. No external non-rust dependencies, and it's about as fast as you can get with OpenGl.
Working this out for myself: "Clone works only for going from &amp;T to T. The ToOwned trait generalizes Clone." to_owned() is used to convert `&amp;str` to `String`. It sounds like Clone would match the signature of owned_copy() above. But that doesn't work if the struct contains a slice directly (instead of using a pointer type). In that case it's a dynamically sized type. You can't pass or return dynamically sized types by value. You need to return a pointer, like `Box&lt;MyStruct&gt;` instead. Going from `&amp;MyStruct` to `Box&lt;MyStruct&gt;` would match ToOwned.
Best pure rust way to do it would be glutin + glium and build a texture from your pixel buffer, then draw a quad with the texture. You don't have to mess with dynamic library issues at all.
Even better, you can blit the texture to the screen without shaders or vertex buffers.
One useful heuristic is to start with writing some sample API consumer code: you can determine how would you like to use the API. This is a Top-Down approach, which tends to be rather abstracted from the implementation details.
A dynamic (PyQt-style) interface would probably be easier than a static one, given Qt's custom language. Maybe rust-cpython will get nice enough to actually use PyQt.
How do you suggest using `Cow`? I'm using it in structs which reference contiguous data already but I don't see how I can leverage that.
Oh, ok. Thanks.
No. A reference to a slice never owns that memory. Note that if it did (e.g. the reference included a magic flag), the slice still wouldn't know how to dealloc correctly. Rust programs don't allocate using C malloc / free (by default). They happen to use the jemalloc library instead. You could create your own "smart pointer" type though.
No, it's only needed for emscripten support.
I have no idea, but I would assume not.
Glad to be of help :-)
There is a wayland backend in the works for glutin, but it's not yet ready (if you want to help: https://github.com/tomaka/glutin/issues/577 ). However, if you want to do some pure-wayland pixel rendering, you can use directly my [wayland bindings](https://crates.io/crates/wayland-client) (they are still very work in progress as well though, and help is very welcome as well).
`partial impl` is somewhat orthogonal to impl specialization, especially in its interaction with the rest of the trait system (partial impls are essentially syntactic sugar to new impls, while specializing impls can affect the operational behaviour, if not the well-typing, of existing code).
We can *totally* include a typeid in the vtable - in fact, we already do so. To allow for downcasting to *traits* is somewhat more complicated, given that we need to check whether the type-id implements the trait. A boolean associated constant would be enough up to unsafe boilerplate. Ideally, we would encode the constant as a bit-flag. That would require some impl-work.
C-style inheritance is a useful feature, that is needed at least for FFI (e.g. to create a `PyObject` that contains a Rust enum), which is very useful in dynamically-typed contexts. I feel that associated fields (with an enforced layout) are the right way to handle it. Encoding it in traits is nice, although I think `#[repr(thin)]` is a tad too magical - I would prefer that the struct acknowledge the existence of the trait (a `: virtual Trait` would do).
I don't like mixing concrete types like structs with traits that should just specify parts of the public interface and characteristics of a type. So I prefer the *traits with fields* approach because it is quite simple and elegantly circumvents the need for a usable base class, something I rarely need in OOP code. I typically either have an abstract base or none at all. I still like the idea of using enums for it, although the proposal is too complex. Once I needed hierarchical structures in my own code most of the time I didn't want the overhead of another allocation per node whereas too large elements for an enum either don't occur or can be trivially circumvented with another box. So I put all the Types in a single enum and implemented dynamic dispatch myself by implementing the trait for the whole enum and then performing a match in every function. struct AImpl { ... } impl Foo for AImpl { fn foo(&amp;self) { ... } } ... enum SomeEnum { A(AImpl), B(BImpl), } impl Foo for SomeEnum { fn foo(&amp;self) { match self { A(x) =&gt; x.foo(), B(x) =&gt; x.foo() } } It worked nicely because you could treat the inner types as instances of the trait and work with them without having the performance penalties of dynamic dispatch and could also use the whole enum in the same generic functions. Unfortunately it had horrible boilerplate and did not benefit from fixed offset fields. Still I like the pattern and extending it with *traits with fields* and some already existing syntax would be quite simple: #[derive(Foo)] enum SomeEnum { A(...), B(...), } impl Foo for SomeEnum::A { ... } 
The greatest benefit is the cheap tag, which can be mixed with embedded vtables for a catch-all "other types" variant, if you really needed that. It's also almost the exact layout used by LLVM classes and v8 objects: this pattern is not uncommon in C++, and it's much faster than RTTI (IIRC that can reach two orders of magnitude), but it's not as foolproof or extensible.
With `Unsize` you might be able to do something in a library.
I guess it's a difficult balance to strike. The current balance seems like a sane middle ground for many purposes, but the problem is that basically any middle ground will completely change the way you have to think about programs. At the extreme you could pretty much panic only on OOM, index out of bounds, bad unwrap, divide by 0 and elswhere where it could be seen as an `assert` failure in C. Then in my code, I can just forget about exception safety, and all the complexities it brings about (I can even disable unwinding). Now that's probably still an option, but not as reliable as it could be because we have what amounts to `assert` in unreasonable places like `thread::spawn` and `print!` and *possibly any function I haven't checked*. I know there are alternatives to those, but my point is that I pretty much have to know every panicking function, and its alternatives in order to write a reliable program this way. I see this as buying some convenience at great cost. If I want to write a long running program in Rust now, it seems like I'm pretty much forced to catch panics at some level, and try to continue execution, because panics will happen at some point, if only because I was careless and used a panicking function, instead of its panicless alternative. But Rust seems to encourage me to catch panics at the outermost level only, and that makes meaningful recovery very difficult. At that point I could just as well build my program as multiple independent processes. And for what Rust gives me at that point, I have to make sure all my code is panic safe (even though they should be rare). About the `SIGPIPE` thing, if the C behavior is what we want, then we could just abort in the signal handler. That's not what I would want though. Usages of `print!` are almost always non-essential, so I would be fine with it returning an ignorable `Result`, rather than doing the "correct" thing and killing my program because of an errant debug print at the wrong time. But I can live with the current behavior, even though I don't agree with it, since it is easy to avoid using `print!` if I need to. By the way I am open to the possibility that I am exagerating a bit here, and that there are only a coule of functions to watch if I want to avoid non-self-inflicted panics. In that case I hope it stays that way, but I'm afraid that panic use will increase (especially when catch_panic stabilizes).
Story goes like this: I've realized I need to use OpenCV for automating silicon microphotography, and I already wrote bindings for my microscope, [rust-touptek](https://github.com/whitequark/rust-touptek). However, I found Rust bindings for OpenCV incomplete and/or extremely intimidating, which scared me away from further development for several weeks. Here's what I eventually realized: * You really should use the C++ API. The C API is abandoned, deprecated, contrived, comparatively less documented, and most example code and documentation refers to the C++ API anyway. * The only ~complete binding to the C++ API is [opencv-rust](https://github.com/kali/opencv-rust), not to be confused with [rust-opencv](https://github.com/woxtu/rust-opencv) and [rust-opencv-sys](https://github.com/renato-zannon/rust-opencv-sys), which bind the C API. * opencv-rust is pretty well usable once you get past its idiosyncrasies, complete lack of documentation in the generated Rust code and the rather baroque approach it takes when generating C++ and Rust code. (Read this paragraph as: it solved my problems.) Anyway, I hope this example helps someone pick up OpenCV &amp; entertains them with its complex behavior &amp; inspires to implement a useful thing.
&gt; in fact, we already do so We don't: the vtable is `(destructor, size, align, method, method, ...)`, e.g. trait IsZero { fn is_zero(&amp;self) -&gt; bool; } impl IsZero for i32 { fn is_zero(&amp;self) -&gt; bool { *self == 0 } } fn main() { let _x = &amp;0_i32 as &amp;IsZero; } Includes this line of IR: @vtable852 = internal unnamed_addr constant { void (i8*)*, i64, i64, i1 (i32*)* } { void (i8*)* @_ZN2i88drop.85017hee05ea675e7d5b26E, i64 4, i64 4, i1 (i32*)* @_ZN10i32.IsZero7is_zero20h9aef33f721faeccenaaE }
I don't think I'm as well-versed in some of the finer details as other commenters, but I like this a lot! It's fortuitous that trait specialization, which has its own use cases, completes all but two of the goals of inheritance in Rust. I agree that struct inheritance is the best of the three proposed methods for field access. It's clean and it seems to me to be the most intuitive. The only thing I dislike is that it makes it unclear whether an unfamiliar bound refers to trait implementation or struct inheritance, which is a big difference. I think it would be beneficial if struct inheritance included a keyword or something, as in: fn take_node_descendant&lt;T: extends NodeFields&gt;(node: &amp;T) -&gt; Arc&lt;Node&gt; `extends` is the most obvious keyword, but may not be ideal given that `NodeFields extends NodeFields` could be an unintuitive truth based on experience with inheritance-based languages (and the normal English use of "extend"). With struct inheritance, would traits implemented for a parent struct be silently implemented for child structs (presumably overridable by specialization)? What about inherent impls? The first seems enticing and very wrong (e.g. accidental PartialEq impls that ignore child fields), but the second seems more compelling. What about some sort of explicit `super` analog for accessing the impls of the parent struct? Overall, I especially like this better than the enum-based proposal mainly because it seems easier to understand for less experienced users.
There aren't any "feature complete" IDEs for rust yet. I personally use Atom with the rust syntax plugin and the racer auto-complete plugin. I don't know how compatible these are with Windows 10 though. 
You may want to look at [`std::sync::atomic::AtomicBool`](http://doc.rust-lang.org/1.1.0/std/sync/atomic/struct.AtomicBool.html). It may work a little bit better for you. Edit: Additionally, if you go that route, you can move the `AtomicBool` out of the `Mutex`. struct Searcher { pub should_search: AtomicBool, other_fields: Mutex&lt;/*..stuff..*/&gt;, } You will have to drill down to get the Mutex, but now the bool won't lock.
I also like the enum solution because it lets you focus on memory layout in a closed hierarchy, which is nice for a lot of systems programming-y things. And, it precludes language-feature-based open hierarchies, encouraging composition for cross-crate stuff. But this post (and Niko's) raise some good points that make it a little less appealing than it would be otherwise.
&gt; My first implementation was a lot of read_u8 and read_u32 on file object wrapped in a Cursor, which was very slow. `File` already implements `Seek`. Do you also need `Cursor` on top of it? Have you tried wrapping a `File` in a `BufReader`? As far as I understand, every `File::read()` call is one system call, which can be expensive if you read one byte at a time. `BufReader` adds some buffering on top to reduce the number of system calls. I don’t know how this compares to `Mmap`. Speaking of reading one byte at a time, your for loop would probably perform better if replaced with: let buffer = vec![0; header_length]; r.read_exact(&amp;mut buffer).unwrap(); (`Read::read_exact` is unstable, so if you’re on stable Rust I suggest copying [its implementation](https://github.com/rust-lang/rust/blob/2915f891673f9c8c8be3cc06aeb3bebf7df66115/src/libstd/io/mod.rs#L596-L611) into your code.)
Zarathustra30's solution is correct, here's why: The issue is that `Cell&lt;T&gt;` is not a `Sync` type, meaning that you cannot have simultaneous access to that data across threads. So you cannot share an `Arc&lt;Cell&lt;T&gt;&gt;` between threads, or an `Arc&lt;Searcher&gt;` where `Searcher` has a `Cell&lt;T&gt;` field. The reason `Cell&lt;T&gt;` is not `Sync` is that it doesn't specificy an ordering policy, unlike the Atomic types. Of course, if the `Cell&lt;T&gt;` is wrapped in a mutex as `Mutex&lt;Cell&lt;bool&gt;&gt;`, `Mutex&lt;T&gt;` does specify an ordering policy (mutual exclusion, of course), which is why `Mutex&lt;T&gt;` implements `Sync`. But if you want something looser, the atomic types are `Sync` themselves, and so don't have to be inside of a `Mutex`. But if your struct has other fields that cannot be implemented with the types in `std::atomic`, or that you don't want to for whatever reason, you need to hide those inside of a `Mutex`. And if your `should_search` field is inside that `Mutex`, it necessarily is subject to that `Mutex`'s ordering policy, which requires a lock to access. But if the mutable fields are inside a sub-struct, which is wrapped in a `Mutex` field carried by `Searcher`, you can have shared access to the `Searcher` struct as long as all of the fields outside of the mutex _are_ `Sync`.
The trait based approach looks nice, but it would be great to have sugar for switching back and forth between memory representations. Perhaps an enum-based memory layout could end up looking like this: struct Node { position: Rectangle, ...} // The `Upcast` trait could work like `Coerce`, and require a specialized memory layout. #[derive(Upcast(Node))] struct TextElement { node: Node, ... } // Require each variant to implement `Upcast(Node)`, allowing for fast upcasts #[derive_from_variants(Upcast(Node))] enum Element { Text(TextElement), Paragraph(ParagraphElement), } Similarly rust could have special tags allowing for specialized memory layout of enums of enums and of structs containing enums.
I think the real problem is that the `print!` macro is underdocumented. The docs should state if and when a macro invocation may panic. Edit: Issue at https://github.com/rust-lang/rust/issues/28510 Edit^2: PR at https://github.com/rust-lang/rust/pull/28511
Which blocker are you using?
&gt; It fits on a floppy? Make it `Copy`! I think it's not appropriate to not offer a `Copy` implementation just because your type is too large. LLVM should optimize unnecessary copies (I think). The only reason to not implement `Copy` is because the type is not POD (that is, it implements `Drop`, or contains a pointer that can't be aliased, etc).
Thanks, I was wondering whether the opencv-rust is usable yet. I will however wait for documentation..
Well, ATS does this, but it's a research language that probably won't gain a following. I hope that something like Rust 2.0 (or a future competitor in this space) adopt some form of dependent types and/or some integration with theorem proving.
Cool! I'll look into that, even though getting it to run under XWayland would have been good enough.
In short, we already have `Rc`, a reference-counted pointer that's appropriate whenever your data doesn't contain cycles. But in any case, in Rust you need to think about who "owns" a given data structure; it would be a poor fit to make everything garbage-collected. So I suggest you consider what the Rust borrow checker offers in terms of memory management before using a GC. For example: A function should always take references (`&amp;T`), unless it's not possible. If you need to change its contents, see if can receive a mutable reference ( `&amp;mut T`). You can generally receive such references unless you want to store the parameter in a data structure or you want to consume it (deallocate when your function returns). Note that even if you use a GC, you can pass a reference to a function (but not a mutable reference). So if you have a function `fn something(&amp;Data)`, you can pass to it both values that are garbage-collected and values that are not garbage-collected (either because they are on the stack, or because they are boxed without a GC). That's great, because you can then use libraries that do not know about your affinity towards GCs. A recursive data structure (say, a tree) should generally put the recursive member in a `Box`. That allocates it on the heap but doesn't manage it with a GC; the whole tree will be deallocated when the root is deallocated. Whenever a given value has more than one owner (and should be deallocated when all users are done with it), put it behind a `Rc` (or `Arc` if it's shared between threads). You can make a tree share subtrees with other trees by having them include their recursive member as `Rc` instead of `Box`. If the value inside `Rc` needs to be mutated, you can use `Rc&lt;Cell&lt;T&gt;&gt;` if `T` is plain old data (has no destructors and generally can be copied by copying its bytes), and `Rc&lt;RefCell&lt;T&gt;&gt;` otherwise. The only trouble is that if a `Rc` value contains a cyclic reference (that end up pointing to itself), that's a memory leak (that's most common with trees with nodes that have pointers to its parents) You can fix with with a cycle collector, or by using `Weak` for back pointers. The stuff I told above is all present in the standard library, and [is described in more detail](https://doc.rust-lang.org/book/choosing-your-guarantees.html) in the book. If you dive into Rust, you will be surprised how seldom `Rc` is actually needed. The design for a `GC` that can handle cycles is more complex (and probably won't be in the stdlib anytime soon), but I can point to [this](https://www.reddit.com/r/rust/comments/3j4bx2/designing_a_gc_in_rust/) that was posted some weeks ago.
&gt; after the first virtual call, additional calls on self are statically-dispatched I'm familiar with Java and Python, where if the virtual method impl is inherited (aka a default method), it might _want_ to make a virtual call to another method. It provides a hook, where subclasses can override behavior. Rust code can still make the virtual call if it wants (cast to trait object). llogiq suggested these [hooks lead to unreadable code](https://www.reddit.com/r/rust/comments/3d6ivg/rfc_impl_specialization/ct2muo1), so it's not necessarily a bad thing. It's just that it's different, and might need careful explanations for people with C++ assumptions. I don't think I've got my head round it yet.
Whew, that was a long (but worthwile) read. @aturon: As always, impeccable writing! You manage to make the complex topics around the possible implementations of inheritance hierarchies at least somewhat understandable to folks without prior implementation experience. Thank you for writing this. Now, while I happen to quite like the [specialization RFC](https://github.com/rust-lang/rfcs/pull/1210), I'm of mixed mind when it comes to data inheritance. There are two use cases we want to enable here: code reuse by (partial) delegation, and memory savings by dynamically sized enums. Let's concentrate on the former first: Let's say I have a `X { a: T }` and an `Y : X { b: U }` for some types `T` and `U`. With your struct inheritance suggestion, the latter would be roughly equal to `Y { inherited_x: X, b: U }` or `Y { a: T, b: U }` with an additional guarantee that all fields of `X` that `Y` inherits keep their order and padding. I think the former should be possible even today. Combined with a `Deref&lt;Target=X&gt;` for `Y` that just returns `&amp;self.inherited_x`, this could make delegation quite painless, right? Also, I think this should work for multiple "inherited" types. Of course, this would have to work with specialization, so we'd have to account for that. Am I missing something here? Now for the latter: A possible solution would be to allow `unsized enum`s that take only their variant's memory. This means that the resulting values are immutable with respect to their variant, and are unsized (and thus must be boxed) – I guess that also means they need to implement `Drop`, right?. They also may not be `Copy`, lest they be put into a `Cell` which cannot `set` them to something different. They also cannot be part of an array or Vec for obvious reasons. Then to get rid of the indirection, we could introduce an `UnsizedSuffixCell` (name subject to bikeshedding) type that may only be included as a last argument in a struct or enum and makes the containing type unsized as well. This `UnsizedSuffixCell` could then hold arbitrary unsized data with the same restrictions as the above `unsized enum`s. It could also contain dynamically sized arrays of sized types. This emulates the C trick of putting a dynamically sized type at the end of a struct. Note that both proposed solutions don't interfere with each other. Containing structs can still decide to implement methods themselves (which would trump deref coercion). UnsizedSuffixCell would move boxing higher up the hierarchy and we could even have one dynamically sized value on the stack, as long as it doesn't change size while alive.
Have you looked at submitting these completions to the fish-shell project directly? They're always happy for people to add in support for tools!
Keep your eyes on https://github.com/PistonDevelopers/VisualRust. The only thing it's missing from becoming a proper Rust IDE is a lack of Cargo integration. You can also get VS code which comes out of the box with rudimentary Rust support, just gotta mess with the config to let the build command invoke cargo (https://mobiarch.wordpress.com/2015/06/16/rust-using-visual-studio-code/).
/u/tsion_ (scott on IRC) kept pointing out how some of these rules are trying to force C++ into being more like Rust, so here they are. Feel free to pick any point in there and give your view on how Rust improves on the C++ situation (or makes it worse, it can't really be 100% perfect). The ones mentioned on IRC so far: * [I.12: Declare a pointer that must not be null as not_null](https://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md#i12-declare-a-pointer-that-must-not-be-null-as-not_null) * Rust makes all safe pointers non-nullable, and uses `Option` to opt into a type&amp;memory safe nullptr optimization (for the `None` case) * For raw pointers, you can wrap them into `NonZero` to get the same optimization (this can be considered similar to the C++ `non_null`, but it's currently unstable) * [C.61: A copy operation should copy](https://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md#c61-a-copy-operation-should-copy) * In Rust, implementing or deriving `Copy` only opts into shallow copy semantics, which cannot be overriden * All custom implementations have to be of `Clone`, which is explicitly called (`x.clone()`) instead of being automatic * [ES.28: Use lambdas for complex initialization, especially of const variables](https://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md#es28-use-lambdas-for-complex-initialization-especially-of-const-variables) * This is usually done in Rust with blocks and value flow: [](http://hack) let x = { // Note the "mut". let mut val = Widget::new(); // 2...N would be ideal, but this is just an example. for i in 2..(N+1) { val.insert(some_obj.do_something_with(i)); } val }; * ^^^reddit ^^^formatting ^^^is ^^^annoying * That specific case can also be done with "freezing" e.g. `let mut x = ...; ...; let x = x;` * [R.32: Take smart pointers as parameters only to explicitly express lifetime semantics](https://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md#r32-take-smart-pointers-as-parameters-only-to-explicitly-express-lifetime-semantics) * In Rust `&amp;T` and `&amp;mut T` are preferred where they suffice, specifically because they are more general (they work with smart pointers, stack variables, etc.) * References also track lifetime semantics, so they can't be misused, and some safe usage patterns are more efficient than passing `shared_ptr` around * [R.33: Take a unique_ptr&lt;widget&gt; parameter to express that a function assumes ownership of a widget](https://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md#r33-take-a-unique_ptrwidget-parameter-to-express-that-a-function-assumes-ownership-of-a-widget) * Passing `widget` by value in Rust would suffice
# C++ sucks lol * CP.1: Assume that your code will run as part of a multi-threaded program * Don't *have* to in Rust. ♥ `Send`/`Sync` * CP.2: Avoid data races. **Reason:** Unless you do, nothing is guaranteed to work and subtle errors will persist. * Good luck with that. * T.40: Use function objects to pass operations to algorithms. * "Function objects can carry more information through an interface than a "plain" pointer to function." :D * T.44: Use function templates to deduce class template argument types (where feasible). * Not a problem, and it is *glorious*. * T.61: Do not over-parameterize members (SCARY). * This appears to be solved by Rust's (sometimes erroneous) refusal to even *allow* unused parameters. * T.69: Inside a template, don't make an unqualified nonmember function call unless you intend it to be a customization point. * "If you intend to call your own helper function `helper(t)` with a value `t` that depends on a template type parameter, put it in a `::detail` namespace and qualify the call as `detail::helper(t);`. Otherwise the call becomes a customization point where any function helper in the namespace of `t`'s type can be invoked instead -- falling into the second option below, and resulting in problems like unintentionally invoking unconstrained function templates of that name that happen to be in the same namespace as `t`'s type." * Once again: *boy* am I glad Rust doesn't allow unconstrained overloading. # Rust isn't universally better? *Nooooo!* * T.4: Use templates to express syntax tree manipulation. * I **wish** I could do this in Rust. The annoying thing is that *even if* we get stable syntax extensions, we *still* won't be able to get type information. *le sigh* * T.12: Prefer concept names over auto for local variables * I'm not familiar with this, but it *looks* like they can use a concept as the type of a variable. It'd be akin to `let x: Fn() = || ();`. Have to admit, that would be kinda nifty... * T.20: Avoid "concepts" without meaningful semantics * We actually violate this advice in that `Add` is a standalone trait. They actually use `string + string` as an example of bad code. See, I *told* you we shouldn't allow that! Now the C++ers are gonna laugh at us! * T.42: Use template aliases to simplify notation and hide implementation details. * This is something we can't do *inside* a type (impl or definition). For example, we can't have a private `type Something = Long&lt;T&gt;::Complicated&lt;Type, Thing&gt;;` inside an impl; we have to keep repeating it over and over again. Ick. * T.var: Variadic template rules. * *sniffle* * T.123: Use constexpr functions to compute values at compile time. * *sniffle* # Potato, potato * T.10: Specify concepts for all template arguments. * I'm already *very familiar* with that particular bit of self-flagellation. * T.11: Whenever possible use standard concepts. * Except we keep shoving them *out* of the standard library... * T.22: Specify axioms for concepts. * "Specifying semantics is a powerful design tool. … Currently, C++ does not support axioms…". Wat. This basically boils down to "comment stuff". 
Nice ! I've been waiting for something like that. :D
As last week, we have few nominations, but no clear winner has emerged as of yet. If you folks don't chime in, I'll have to select a crate to write about without democratic oversight ;-)
&gt; ...and get cheap downcasting. Or do I misunderstand something? It's a bit more complicated than that. There are two issues: - you may not want to down-cast to a *leaf* - in general in Rust, with an open-ended hierarchy, the number of traits a given struct implement is unknown I tried very hard to an efficient representation allowing down-casting for open-ended hierarchies in the [rust-poly](https://github.com/matthieu-m/rust-poly) prototype and finally settled on a dual-encoding: - each `struct` knows all the traits it implements in the crate it is defined in - each `trait` knows all the structs implementing it in the crate it is defined in Given that you can only `impl` a `trait` for a `struct` if one of them is defined in the crate, then you are ensured that at least one of them knows the other, and thus querying each one (in turn) allows to down-cast appropriately.
&gt; with an additional guarantee that all fields of X that Y inherits keep their order and padding Maybe not completely. In C++, when you have `Derived: Base` it's fair play for the first few fields of `Derived` to be stashed in the tail-padding of `Base`. It helps saving a couple bits...
&gt; and it's much faster than RAII You meant RTTI right?
`trait Monad&lt;M&lt;T&gt;&gt;` is where some of your problems stem from. That's incorrect. You have `T` in there even though `M` is supposed to be generic. And where's `Self`? It looks like it's `M&lt;T&gt;` in your case, again, incorrect, there is no trait-level `T`. A working definition could be: trait Monad for Self&lt;type&gt; { fn bind&lt;T, U, F&gt;(self: Self&lt;T&gt;, F) -&gt; Self&lt;U&gt; where F: FnOnce(T) -&gt; Self&lt;U&gt;; fn unit&lt;T&gt;(T) -&gt; Self&lt;T&gt;; } impl Monad for Option { fn bind&lt;T, U, F&gt;(self: Option&lt;T&gt;, F) -&gt; Option&lt;U&gt; where F: FnOnce(T) -&gt; Option&lt;U&gt;; match m { Some(t) =&gt; f(t), None =&gt; None, } } fn unit&lt;T&gt;(t: T) -&gt; Option&lt;T&gt; { Some(t) } } // Hypothetical syntax. Could be for&lt;T&gt; Result&lt;T, E&gt;. impl&lt;E&gt; Monad for Result&lt;_, E&gt; { fn bind&lt;T, U, F&gt;(self: Result&lt;T, E&gt;, F) -&gt; Result&lt;U, E&gt; where F: FnOnce(T) -&gt; Result&lt;U, E&gt; { match m { Ok(t) =&gt; f(t), Err(e) =&gt; Err(e), } } fn unit&lt;T&gt;(t: T) -&gt; Result&lt;T, E&gt; { Ok(t) } }
Still reading, but: // Create new iterator wrapping self and F yielding items from // the iterator f(self.next()). Essentially flat_map but // without the need for allocations. Isn't this exactly `Iterator::flat_map`?
How does this deal with lifetimes, iterators or the problem with always taking `self` by-value?
*Whoops*. That's what we get for having so many acronyms.
Yes, `flat_map` does not allocate - in Rust at least, maybe they were talking about `flat_map` in another language?
While implementing this has barely been started, MIR exists at the point where basically *everything* is known, and trans will be a deterministic transform from it to LLVM IR. If you don't have something, like the complete set of monomorphizations, you will be able to compute it by just doing the same call site visiting trans does. Accessing a type's fields and impls is actually almost trivial in *today*'s rustc, but you can only use it for lints. The MIR will enable you to transform code (and generate new functions) based on all the information the compiler will ever have.
I started to write the code before I started to think the types through, going to write another post where I correct this, hopefully taking a more formal approach to it compared to this experimental approach. I will probably have to go through this article and make corrections, using your trait definition instead of my initial one.
Given that its documentation, when added, will just consist of stuff copied from OpenCV's doxygen, I am not sure a little convenience is worth the wait.
Concepts (traits) are the one thing that we (well, I at least) favour filling std up with.
Well, `Cow` represents a value that may be borrowed or owned. So like use std::borrow::Cow; #[derive(Clone)] struct Foo&lt;'a&gt; { data: Cow&lt;'a, [u8]&gt;, more_data: i32, } fn main() { let data = &amp;[5, 4, 6][..]; let x = Foo { data: Cow::Borrowed(data), more_data: 5 }; } Now you can call `into_owned()` on `data` to take it from `&amp;[u8]` to `Vec&lt;u8&gt;`
&gt; even if we get stable syntax extensions, we still won't be able to get type information Could you expand on this one a bit?
&gt; Except we keep shoving them out of the standard library... "common traits for interoperability" are very often cited as one of the things we want the stdlib to be full of...
Almost the only usecases for this I can think of are closed hierarchies. The sorts of situations where you need *that* much control over memory layout are relatively rare and you nearly always need total control.
&gt; Accessing a type's fields and impls is actually almost trivial in today's rustc, but you can only use it for lints. &gt; The MIR will enable you to transform code (and generate new functions) based on all the information the compiler will ever have. I assume that lints cannot generate new functions, right? I'm interested in the ability [to implement delegation](https://internals.rust-lang.org/t/syntactic-sugar-for-delegation-of-implementation/2633/15?u=shepmaster) of some kind. Knowing about other types and then generating new code based on it seems required for what I want.
The cost of open hierarchies don't justify using their solutions for closed hierarchies. I gave two more cases of closed hierarchies that are production-quality and both use type tagging. `rustc` itself could be more efficient given improvements to closed ADT hierarchies: right now it has manual indirection to avoid causing small variants to be bloated by large ones, which is suboptimal, since they're always placed in arena *anyway*. Open hierarchies are useful, but not the subject of this particular performance hunt: I am not *against* making open hierarchies more powerful, but context here is the DOM. I should actually apologize for not understanding this last year: it wasn't clear at all (to me) the the DOM is a closed hierarchy and that type tags could "just work".
&gt; Is there any way to avoid the panic on OOM? Don't use `libstd`, use `libcore` instead. For example, here's /u/dbaupp implementing a `Box` that returns `Result` instead of `panic!`ing: https://news.ycombinator.com/item?id=10237126 
Who on earth is afraid of that syntax? It would be amazingly useful.
One option might be to add a new type that is not `Copy`, that included the old type as a member. Well, that's not always possible, and has usability problems, but one could try. (if Rust ever gain virtual struct stuff, perhaps the new non-`Copy` type inheriting its fields from the old `Copy` type would at least make the syntax better)
Indeed, rustdoc would produce this. In the current state, I actually inject the default parameters for methods and functions in rustdoc comments. I did not push rustdoc results, as I do not feel these bindings were at that point yet. As was said before, it's still quite... baroque.
Type lamdbas could introduce infinite loops at compile time making type level termination checkers desirable perhaps? They also cloud the equality of types since mere syntactic equality won't take you as far as you'd want. It's ultimately just a big feature that is not usually taken up by languages which don't already have hyper powerful type systems (e.g. full dependent ones). Scalia has them and Haskell has a trick which lets you talk about partially applied type constructors as non-functions. They're getting defined type functions nowish but there have already been a lot of subtle bugs.
Yeah! I was looking into that earlier this morning. I should have started there because I found out about some functions that I could have used. My plan is to make a few changes and then submit a pull request.
As someone just getting started, what does cargo do better than other package managers like pip or maven?
 I think that's one thing I've been a bit confused about - taking `n` bytes from a file. I see that I can create a vector of length `n` and then do `read(&amp;mut vec)` Is that what you mean by a single slice operation?
Library authors in general, rather than having them be special for things in the language. It's a good thing.
I'm a fan of this syntax because it's makes sense with the current syntax: trait Foo: Bar { fn foo(&amp;self); } looks like: trait Foo for Self: Bar { fn foo(&amp;self); } with the "for Self" omitted
What I mean is that traits in libstd allow library authors to integrate their library within the wider ecosystem, more than would be possible with OOP. This is both very usable *and* powerful both for library authors and - users. And that's a good thing.
It was a mistake to implement `Add` on `String`, + should be commutative but `a + "b"` may work while `"b" + a` doesn't. The right thing would be to have a concatenation operator.
Yeah! I think the fact that `println!` can panic should be more publicized. I'm sure that many production software will still print with this macro without the programmer knowing about this panic source.
Why can't struct inherritance work simply by embedding it directly - like this ( similar to go): struct NodeField { event_target: EventTarget, //... } // Instead of trait inherritance notation struct T : N // Just have T { N } instead, with cascading inherritance. struct ElemFields { NodeField, // &lt;- Just include type local_name: Atom, namespace: Namespace, } 
Right, this is for integrating what rust-gdb does into user configuration for gdb.
Actually it doesn't. Struct inherritance described/proposed is based on a semicolon model: struct Parent {} struct Child : Parent {} This leads to long chaining: struct Child&lt;T&gt; : Parent&lt;T&gt; : Parent2&lt;T&gt; {...} This has describing bracket based polymorphic inherritance (similar to go): struct Child { Parent, // &lt;- Parent Declared as type. prop: type, }
&gt; This document is a very early draft. It is inkorrekt, incompleat, and pÂµÃ¸oorly formatted. Hmmm.
Are you sure? As far as I'm aware the significance of these commands is identical to what rust-gdb does: it adds Rust's directory to the search path for gdb and gdb's python. This amounts to a couple stat/lstat calls on gdb startup, but nothing big processing-wise; it doesn't read the contents of the python files Rust installs. gdb decides to actually *load* a given python file for pretty-printing based on the contents of the .debug_gdb_scripts section of executables compiled with `rustc -g`, which holds the bytes "\x01gdb_load_rust_pretty_printers.py". I actually did all this digging because I wasn't getting pretty-printing when debugging the executables produced by `rustdoc --test`... it turns out those lack the section and need the user to run import gdb_rust_pretty_printing gdb_rust_pretty_printing.register_printers(gdb.objfiles()[0]) in gdb's python shell.
Do you have the source for those lists? I'm using uBlock too.
Thanks for trying but that's not helpful. /u/heinrich5991's [assessment](https://www.reddit.com/r/rust/comments/3lfl5p/trait_method_that_returns_owned_copy/cv69vhj) is that what I'd like to do isn't possible in Rust at this time. Do you disagree?
Looks like they just went ahead and misspelled all three words, instead of just going for the usual "inkorrekt, incomplet and p^(o**or**l)y fo*r^(m)at*^(ed)."
Never heard of fish. Thanks :)
Question, can GJ distribute work among N threads? (with a work stealing queue or something like this). Is there any library that does this?
I am going to make the minor argument that HKT traits should be very close to traits over kind *. In this case, introducing the for seems unwieldy, and looks vastly different from normal trait definitions. That would be the 3rd overloading of for. Do we want that? I think an extension of where clauses is probably more appropriate.
Can you elaborate on the costs you see for `match` versus vtables? It'd be great to see some measurements or other substantiation. I know /u/nikomatsakis was leaning toward vtables even for the closed case, due to expressiveness benefits (allowing for extra type parameters in variants, essentially).
Hell yes, you rule.
They are called [Adblock Warning Removal List‎](https://forums.lanik.us/) and [Anti-Adblock Killer | Reek](https://github.com/reek/anti-adblock-killer). They are listed in the third party filters of uBlock Origin (idk about uBlock).
Wonderful! And I'll clarify that extending "syntactic equality" to "normalize and then compare" is roughly the extension I meant. You can either talk about that as nbd, or as quite a large extension!
A GJ event loop is confined to run on a single thread. This has some important advantages --- it lets you share data between tasks by using `Rc&lt;RefCell&lt;T&gt;&gt;` rather than `Arc&lt;Mutex&lt;T&gt;&gt;`, and it makes it easier to avoid race conditions. The downside, however, is that GJ cannot automagically distribute work among threads. GJ does allow you to start multiple threads with an event loop on each, hooking them together with unix sockets. My plan is to implement Cap'n Proto RPC on top of GJ, and for that use case having a unix socket between threads is often all that you need. I also hope to implement something like `::std::sync::mpsc::channel()` that integrates with the GJ event loop.
"Restricting" T? But those definitions are incompatible with the Haskell or Scala ones, so I can only say they're wrong. `Monad` is **not** a generic trait, it's a trait implement for a generic type (or a type constructor, if you will).
&gt; My plan is to implement Cap'n Proto RPC on top of GJ Why not offer two sets of data types, one that can be shared between threads and another that can't? Would Unix sockets here be faster than sharing objects in the same address space?
The less example is pretty persuasive. Changing it to silently ignore the error would probably not break real code. Are there other cases where common Rust patterns don't work with less?
You may want to add a license, to control who can use and contribute.
I have run clippy actually, it was awesome, it found quite a few little things. Haskell and Clojure have given me an aversion to mutating things, but I've been meaning to just move the code to more and more idiotmatic rust, I'm going to try doing it mutably now. 392 is a great point, got rid of two pointless intermediate variables that way. Good point on gen_pass, I moved converting them into owned strings just before I join them so I only do it for 4 of them instead of all of them. Was there some way you meant I could avoid owning any of them? I feel like I need to to convert them into a single String at some point. A huge thanks to you, steveklabnik, and Brian Campbell ("lambda" on github) for all the help and pull requests, this community is great. 
While they look similar, Rust has the "pattern" position and the "type" position. The former is used to declare names out of the value and the latter is used to clarify the (static) type of that value. `&amp;mut` is used in both: * In the type position `&amp;mut T` is a (exclusively) mutable reference type pointing to another type `T`. * In the pattern position `&amp;mut x` is a pattern matching that type, and another pattern `x` binds to the pointee. Note that if `x` were a simple name the pointee will move into `x`, which is only possible when `x` is `Copy` [1]. An easy way to understand this relation is to memorize this valid statement: `let &amp;mut x: &amp;mut i32 = &amp;mut 42;`. The type of `&amp;mut 42` is `&amp;mut i32` and matches with a pattern `&amp;mut x`; consequently `x` will be assigned `42`. [1] Otherwise `&amp;mut T` will have no value and the following read will be clearly unsafe. You can only reborrow in such pointee patterns; `&amp;mut ref mut x` is, while almost useless, a valid pattern.
So your first example should be let mut a: Vec&lt;i32&gt;; This means a is a mutable vector of i32s. This means that you can do whatever you want with it. let a: &amp;mut Vec&lt;i32&gt;; is different. It means that a is a reference to a mutable vector of i32s (basically, a pointer that you can mutate through). One of them means that the Vector is on the current function's stack, the other means that it's somewhere else, either another function's stack or the heap.
&gt; Scalia has them Ha! Not sure if you're American or not, but the idea of Supreme Court Justice Antonin Scalia having a sophisticated type system amused me. Makes sense, though. He'd never vote with those wild dynamic judges like Ginsburg.
Changing bindings and changing data are different. let mut a = 2 means that I have assigned a binding "a" and can mutate it all I like. let b = &amp;mut other_val; means I have taken a mutable reference to "other_val". I can call mutating methods on it, and I can reach in and change values if I really wanted and those fields were public. What I can't write is b = &amp;mut different_val; because b was an immutable binding! If you need to change the binding every so often, you'd have to say let mut focus = &amp;mut different_val; I often use something like that for traversals. If I don't need to reach in and touch things, I could have also written let focus = &amp;mut different_val; Anyway, there are actually no tricky rules to remember here! mut applies to whatever you're writing it next to. Writing mut a means that the memory labled "a" by your compiler can be written over multiple times. Writing &amp;mut b means you can follow the pointer to some other piece of memory and write over (portions of) that. So all you need to think about is what memory you'd like to write over on occasion: the binding, the thing being pointed to, or both?
I don't see how that's relevant. However powerful `const fn` gets in the future has no bearing on what you can do *right now*, and it's really only *right now* that matters. And *right now*, you can't even use `const fn` in stable. Basing comparisons on hypothetical future developments is a terrible idea. I wasted over half a decade waiting for D's toolchain problems to get fixed before just giving up on the language. Talking up unusable or outright non-existent features just breeds disappointment and distrust.
Ah, maybe that's how that works. I was assuming it loads the entire thing on startup. Haven't used actual gdb since lldb and that from my understanding loads the script on startup no matter what.
Another question here: What is `mut`(without `&amp;`) then? let mut x = 3; I'd say it's part of the let binding. So neither part of the pattern nor the type, right? And it is always directly in front of a name/identifier? And what if I want a mutable binding to a mutable reference with `ref`? I guess it's not possible.. let (mut x, mut y) = (1,2); match x { ref mut a =&gt; { *a += 3; // ok a = &amp;mut y; // error :( } } `mut ref mut` doesn't work. I agree, that it's not super useful, but still.
Aha! I get it, thanks.
&gt; Passing `widget` by value in Rust would suffice Surely the Rust analogue would be `Box&lt;Widget&gt;`.
:)
Can't this be fixed somehow? Is there a list of things to fix in a 2.0 release somewhere?
I would guess this only happens because i32 has the Copy trait; without it a move would happen (causing its own trouble).
Hey, thanks! I think the "!" between the "do we have..." and the pointer equality comparison is confusing the issue, btw (though didactically sometimes a good surprise helps hammer in the point! :) )
The Vec's array data always lives on the heap, however the Vec itself can live on the stack or the heap, since it's just a struct.
cargo-count was written by /u/kbknapp -- and I was wrong: According to the Readme, it's only "originally based off and inspired by the project rusty-cloc by Aaronepower".
Eventual (https://github.com/carllerche/eventual) is a pretty complete async future &amp; stream library that is designed for concurrent situations.
Haha, don't worry! We all overlook the trivial things from time to time.
Maybe something like this? https://play.rust-lang.org/?gist=15fc6e042e1181a4d583&amp;version=stable
&gt; Yes, but in Rust boxing is unnecessary to get move semantics The same is true for C++ (using C++'s definition of a move). 
Speaking of mio, has anyone been able to use it with rust-openssl?
I updated to Rust 1.3, so our versions can be compared on the same version of the compiler. I see your version is faster because it is multi-threaded, while my version is faster on a single core.
:) It was a typo due to my phone, but I'm just going to keep it.
One thing that I noticed reading your code more carefully is that you don't handle the case that `send_to` doesn't write your data because it would block (or the case that it sends only part of your buffer). This would be signalled by `send_to` returning `Ok(None)`, which you don't check for. That being said, I'm not actually sure this is a problem in your particular case, because I don't know under which circumstances `UdpSocket::send_to` would `EWOULDBLOCK`. So it might be fine, I'm not sure. It wouldn't hurt to be more explicit, though, and maybe have an explicit `panic!()` if you think this shouldn't happen. Similarly, `recv_from` isn't guaranteed to actually be ready because `mio` might emit spurious events, so strictly speaking you might want to handle the `Ok(None)` return value. Shouldn't you always `clear` the buffer at the end of `ready` to empty it for the next packet? I don't understand why it's only cleared in the `MEASURE_PPS` branch. &gt; Is there a way to send echo reply without allocating new buffer (rbuf)? I can't use flip on MutByteBuf because it requires to be able to move value which I can't do as it owned by handler. But you're using the same buffer for every event? What you do looks fine to me (modulo me not understanding why the buffer isn't cleared). If you want to to use `ByteBuf::flip`, you could temporarily replace the buffer by `ByteBuf::none()` (which doesn't allocate a buffer) using `std::mem::replace`, but I actually think your current way is nicer / less complicated. &gt; Also for some reason in the crate documentation there is only descriptions for traits not buffers themselves. As I said, bug in rustdoc. If you want to build the docs locally, clone the repo and replace `mod buf` with `pub mod buf` in `src/lib.rs`, that should give you the docs for the various buffer types. &gt; 5) If I'll send through mio's channel some message with token id and in the event loop using it I'll get socket which will be used to send UDP packet using send_to, will it work or there is other way to do it? Yeah, that works. Tokens are `Copy` and `Send`, so that's fine. I do that in one of my projects and I haven't had any issues. 
`let (a, b, c) = (0.5, 556, "hello");`
You can use tuple destructuring and do something like this: `let (mut n, mut m) = (1i32, 2i32);` Functions can also return multiple values using tuples which you can destructure afterwards. fn foo() -&gt; (i32, i32) { (1i32, 2i32) } let (n, m) = foo();
I assumed `unique_ptr` was used mostly because there's little need to move (as opposed to copy) unless you have a heap allocation, and `unique_ptr` is the simplest heap allocator.
well, the mistake is using something that will ever run out of support without having planned from day 1 on to upgrade in time. maybe it wouldn’t be that bad if the choice was open source, where you can at least backport security fixes, but still: it should be illegal to risk patient privacy by being too short-sighted to maintain a secure platform.
Syntactic sugar can be nice sometimes. As as counter to the argument about commutativity, no one is particularly confused or surprised to learn that string concatenation is implemented with "+", and the practical downsides of a non-commutative operation implemented with "+" are not inarguable or obvious.
I was never on 1.2, I'm usually on nightly, so I did not :) Since I updated to use Cargo three days ago the link in my previous post no longer works, but the updated version is [here](https://github.com/Thiez/Comparison-Programming-Languages-Economics/tree/master/rust). I haven't seen the multithreaded c++ version.
I've never used your library before, but I've used a Python implementation of the XDG Base Directory Specification, and it's made the tools I write vastly more usable and flexible. Have you considered taking the app-name as a constructor parameter, so that users can just say "settings.cfg" instead of "app-name/settings.cfg" every time they read or write something? In particular, this would help for libraries that want to use configuration, since they don't necessarily know what app they're running inside (and its configuration shouldn't necessarily be shared among all apps linked with that library). It would be handy if there were some kind of iterator that would return all the paths that `find_config_file` checks, but in reverse order—so an app can read defaults from a system-wide config file and then overrides from a per-user config file, etc.
Hm. What about list_files functions? These can encounter an unreadable directory. Currently it would be just ignored.
[Wish granted](https://github.com/whitequark/rust-xdg/commit/1b4ce577602152283c61e036b48b94391ec23ea3).
Argubly the weird characters are believably the result of some crazy formatting/encoding issue.
Indentation in main.rs (haven't checked the other files) is a mess; the mix of tabs and spaces results in a weird layout (e.g. https://github.com/Aaronepower/tokei/blob/master/src/main.rs#L21, https://github.com/Aaronepower/tokei/blob/master/src/main.rs#L77). Common Rust style dictates indentation with 4 spaces.
Caution about this other meaning is why the impl for Vec was removed. But you can't impl Add for Vec anyway, only std can do that. If you want to implement element-wise addition, you need a new type. From a practical perspective, its conceivable that someone might be confused about what vec + slice does, but its such a common syntax and it's just something you might one time have to learn. The objections about commutatitivity seem totemic to me, rather than based on actual usability issues. I know it wouldn't fly, but I'd be glad to see string multiplication added as well ("foo" * 3 == "foofoofoo")
Not built in. Yet. There's an RFC for swappable test runners. There is https://github.com/reem/stainless which works with macros, which is the basic solution you have to take right now.
1. [Wish granted](https://github.com/whitequark/rust-xdg/commit/98a63d4a1bc7de98881e5b704e8363f3d7ae93d7) 2. I disagree. I imagine an application will either store the `xdg::BaseDirectories` instance somewhere, or use it once during startup, or query a few paths and store them. Further, I think that being completely independent of the process environment after creation is a good feature to have. In any case, I don't think the overhead of allocations is large enough to complicate the interface.
I have [updated](https://gist.github.com/anonymous/2480dfc4c7a5b3e8c7b0) server code using your advices. I have one additional question about `MutSliceBuf`. How it's should be used? Because I couldn't find a way to extract info written into it using public methods, nor do it has `bytes()` analog, nor way to get `pos` value. So I could read slice its wrapped, but I can't determine how many bytes was written into it.
Oh, I get what you mean. The sentence is expected to be explanatory, and then we check the result of the test. That works but you do have to connect that result to what is tested, which is in the source. Whereas I intended the following value to be a confirmation of what we are demonstrating—in other words, it’s intended to be an assertion. Of course then you have to see in the source *how* we demonstrated our point. In fact If I were to do it again then [why not use actual assertions](http://is.gd/Icsw2j).
Working on spare-time prototype integrating racer with rustc to get 'perfect' completions. Also to help drive out changes to rustc to support wider ide functionality 
`val` from `Err(val)` is a borrowed reference from `rest`, so for iter in rest.iter_mut() { if iter.peek().unwrap() == val { iter.next(); } } evidently can't work as it would invalidate `val`. ~~In this case it would be safe, but Rust doesn't know that.~~ EDIT: Actually, it's not safe. Not sure why I thought it was. Rust to the rescue! Sadly, Rust is also erroring out on the `Ok(())` branch, as the first loop `for iter in rest.iter_mut()` is extended by the borrow regardless. Not sure quite how to fix that.
I'm going to start learning this language! I'm really excited. :-)
Hi sorry for the newbie question: impl Monad for Option { fn bind&lt;T, U, F&gt;(self: Option&lt;T&gt;, F) -&gt; Option&lt;U&gt; where F: FnOnce(T) -&gt; Option&lt;U&gt;; match m { Some(t) =&gt; f(t), None =&gt; None, } } Where did **m** on line 4 came from?! 
I think it's reasonable to talk about the benefits of unstable features. The two largest Rust projects in existence are on nightly.
Would using a closure help? Like: if iter.peek().map_or(false, |x| x == val) { iter.next(); }
Probably a typo and should've been `self`.
It's well and good to talk about unstable features in a "here's what's being worked on" context, but when you're talking about what you can *actually do*, I don't think they have any place except in the footnotes.
In this case, I don't think so since the error happens at the `for`. Also, see edit.
The whole point of the guideline is that somebody writes an algorithm that requires Add and somebody else passes it a string and gets a run-time error because the algorithm assumed commutativity. It is basically saying, "don't use concepts/traits for syntactic constraints only" (use it for semantic constraints). When I read the definition of `Add` in the docs, there is no mention about any semantics at all.
&gt; I have used C++ quite a bit. It's just syntactically odd that AsRef/Borrow can't be used in trait objects If you are familiar with C++: What you want is similar to having function templates as virtual functions which is *also* not possible in C++. It won't work because a generic function is not a single one but a family of functions and we can't create an entry for each member of that family in the V-table nor delay code generation until runtime like more dynamic languages such as Julia (where compile-time *is* run-time).
Oh wow, good point! Thank you!
Toying with an idea for a plugin which lints your rust source file and generates a C header file for the things which would be callable from C. 
You can even do let Foo { bar, baz, .. } = foo; And have `bar` and `baz` variables.
&gt; A Box only lets you dereference it mutably if you can borrow it mutably. Yes. But in case of an "unique *owning*" type such as Box, this makes perfect sense. I prefer to think of `Box` as a fixed-size-one-vector that supports an unsized element type -- not as a pointer. C++'s `unique_ptr` is quite different in this respect. If you have read-only access to a `unique_ptr&lt;int&gt;`, you can still mutate the int. But even if you wanted a library-defined type such as `Box&lt;T&gt;`, `sync::MutexGuard&lt;...&gt;` or `cell::RefMut&lt;...&gt;` to feel more like a `&amp;mut T` in the sense that an immutable binding of such a smart pointer still lets you modify its pointee we currently *can't* do it because we would need "unique borrowing" for safety and the only kind of "unique borrowing" we can express in terms of function signatures is "mutable borrowing" (which, of course, we can't use on an immutable binding). So, to make this work for smart pointers as well, we would need a [new kind of borrowed reference type](https://internals.rust-lang.org/t/unique-borrowing/2642). A reference that is about unique borrowing (only allowing one usable borrow at a time) but does not require the borrowed variable itself to be mutable. Rust already deals with this kind of borrowing internally (magic!) because that's what happens if you dereference a mutable reference. You can think of mutable references implementing this *hypothetical* trait: trait DerefUniq { type Output; fn deref_uniq(&amp;uniq self) -&gt; &amp;mut Output } Hi /u/steveklabnik1 ! Thanks for spreading the word about Rust. I just wanted to remind you that we talked about the usage of `Box` in presentations and IIRC that resulted in you agreeing with me that using examples with `Vec` instead would be better and closer to real world cases in which nobody would box a single int. I fear that such `Box` examples put too much emphasis on the `Box` type possibly making people think that it's quite normal to box things while in real world code `Box&lt;T&gt;` is only used if T is unsized (trait object or slice), if one wants to avoid the infinite-struct-containing-itself-recursion or if `size_of::&lt;T&gt;()` is really really large. At least that are the only three reasons I can come up with. I'm saying this because even the C++ guys need to be reminded once in a while that something like void foo() { unique_ptr&lt;Widget&gt; p (new Widget); // stuff } is totally pointless and could be simplified to void foo() { Widget w; // stuff } And even if a function returns something, a C++ programmer might think of this first unique_ptr&lt;Widget&gt; make_widget() {...} But really, if `Widget` is move-optimized -- in Rust every data type is move optimized which is sooooo cool! -- it might be better to still avoid the heap: Widget make_widget() {...} 
More work on https://github.com/Marwes/embed_lang. Should finish up a blog post explaining the goals and progress of it at the end of the week.
Getting started with rust + ios, which led to the just now published [`cargo-lipo`](https://github.com/TimNN/cargo-lipo) utility to automatically create universal libraries for iOS from a cargo project.
For console input, there's a create called [whiteread](https://crates.io/crates/whiteread).
Awesome, thanks for the answers! Sorry for the delay, was out of town for the weekend. &gt; But, you don't need to manually serialize anything (you can, of course!). What did you mean by this? I assumed that any data structures used inside of each closure would need to be manually persisted/paged if it was potentially larger than memory. Were you talking about something else? &gt; If you mean more like a dashboard, the logging signals are what lead to that, but it is (currently) up to you to watch them. We're (some folks at ETHZ) in the process of writing some tools for this, though. Yeah, I was thinking more like a dashboard...a way to monitor the whole system to see where the various moving parts are located. &gt; Timely isn't a cluster management software, it's just a compute runtime. Gotcha! Makes sense given the answers to the rest of the questions. Thanks again!
Writing a general parser generator. Soon, I'll add inline actions and bootstrap it.
&lt;3 for slaving over Turbine's terrible codebase :)
My pleasure. I'm using Disruptor in Java quite often, so I figure I might be qualified to do this. :-) I need to push my latest changes, btw. I usually work on this while on the train. Edit: Pushed.
Neat! /u/frankmcsherry you should post these yourself :P I'm curious about the notifications. It seems a little un-ergonomic that you have to request notify_at() for each input, and then manually collect the notifications after the input is done processing. I'm not sure how Timely is backing those notifications (hashmap?), but could it become expensive to request the same notification repeatedly if you have millions of inputs at the same `time`? I can see the use-case for it, because you may want to request notification on certain data but not other. But I wonder if there is a more ergonomic shortcut if you know that you are requesting notification for an entire epoch regardless of data? Sorta like a pre-input notify_at() that gets called once? Can you "de-register" a notification request if you decide half-way through processing data that you no longer want to be notified? Very cool stuff, looking forward to part three!
That's reasonable but I have a hard time imagining it being a problem in practice.
&gt; Higher kinded types is something which has been discussed a lot related to Rust in the past year, both as a feature people want, but also as a feature people do not really know what to do with. And, not that I don't appreciate this post, but it does nothing to help me understand what to do with all this. I'm pretty sure I want HKTs, monads and all this, but I really struggle understanding what they're about. Every theoretical explanation I find on Google seems understandable, but the (way too rare) occasions where it's associated to actual examples of what this allows, everything I thought I understood falls apart. I really need a precise explanation illustrated by straightforward examples detailing what we can't do without HKTs, and how we can do it with them. If anybody has one, or a link to one, I'd appreciate it greatly. I have found some that might address all this, but they all have some kind of experience with Haskell as a prerequisite, which I have none of.
Actually, the noncommutativity in types makes this a nonissue. It will be a compile time error.
&gt;Given that std exists, I'm sure there must be a better way than what I'm doing... There's a better way... buy a modern Core i7! The whole bootstrap including LLVM build and running the tests will take around half an hour. I tried various build configurations to reduce the build times in different scenarios, but the sad truth is that only hardware upgrade truly helped.
They are hygienic. Relevant: https://doc.rust-lang.org/stable/book/macros.html#hygiene
This looks like a specialization of more generalized vector/array/list operations. Why only bytes?
I'm not sure what else I can say, if I'm failing to convey my position on this. The Rust compiler and Servo aren't really "normal" Rust projects. They're significant outliers in that they predate 1.0 and, yes, rely on a lot of unstable features. But then, prior to 1.0, *every* feature was unstable. But part of the language growing up and becoming broadly usable is distinguishing between what you can rely on from what *might* be. I used to maintain a *really* useful library that I see (indirect) requests for on a regular basis, but I gave it up because: A. it was a compiler plugin and I just couldn't keep up with the breakage, and B. I don't think it's fair to promote the unstable side of the language. Heck, I have one project that uses a tiny smattering of reasonably uncontentious unstable features that *of course* will be stabilised in more or less the same form sometime soon. One of them got unceremoniously shot in the head and I had to replace it. I've said this numerous times, but it's really the lynchpin of my position on this: I see in the promotion of unstable features shades of what I experienced with D. And what I experienced was a long burn of unfulfilled and empty promises that left me bitterly disappointed. It helped construct an unrealisable totem of the language in both my and others' heads, that soured a lot of my future interactions. I've also said this before: I don't think Rust needs to have its capabilities misrepresented or inflated by talking up things you can't really use yet. Even things like `const fn` *do not count* because until they are marked stable, there is zero actual guarantee that they won't get unceremoniously shot in the head. I'm OK with Rust doing this because of what I perceived (hopefully not erroneously) strong current of "well, that's the risk of unstable: don't use it expecting even short-term stability". If Rust starts promoting unstable features, then it rapidly becomes less *socially* acceptable. I strongly believe that it's better in the long run to be honest about what's *stable*, what people can use without potential nasty surprises down the road. Rust *does not* have a feature comparable to CTFE... *yet*. I probably will soonish, but that has enough asterisks attached to it that unless you go to the effort of spelling those footnotes out, it's probably more honest to just not talk about it. I can't really think of what else to say about this; maybe it's purely a function of my experience and everyone disagrees with me and I should just shut up.
Is there a way to get the binary representation of an f64 value in stable?
&gt; What did you mean by this? I assumed that any data structures used inside of each closure would need to be manually persisted/paged if it was potentially larger than memory. Were you talking about something else? Just, that the OS will swap for you. It isn't like the JVM where you get killed if you use more than 64GB. If you have a cache friendly algorithm that uses a truckload of memory, fair enough, it runs (I do this); you might be able to go faster with manual staging of data, but as a first step you can just see what happens. :) 
You can always `transmute` it to a `u64`.
Thanks for the in-depth reply! I agree that having `repr` choices affect what you can and can't do is not ideal, although in some cases representation and functionality are really entwined. &gt; Wouldn't it be possible, either through compiler built-in or with a clever library, to only get a thin-pointer when you want one? Quite possibly, although you'd want to account for things like upcasting as well, which may be tricky. To be honest, I suggested `repr(thin)` largely as the simplest solution I could imagine, but as you'd expect that also means it's less flexible than it might be. I suspect there's a point of diminishing returns here, where the additional complexity needed to support this nicely in the library space is addressing only extremely niche cases. But it's definitely worth thinking about.
Even a half-hour compile-edit-test cycle is pretty annoying, though. Even more so when I know that it isn't really necessary -- clearly, I shouldn't need to bootstrap the compiler just to test a change in std.
Note that this was only one of several ways we might deal with fields that the post discusses. I personally find it a bit too magical, and prefer straightforward struct inheritance.
I understand your point, but you are mistaken about what is implemented. Add&lt;&amp;str&gt; is impld for String. "ab" + "bc" will not compile because both are &amp;str (test your code before you claim it compiles :p) You cannot write generic code that uses add commutatively and accepts String. It just isn't possible with the existing impl because it is noncommutative at the type level. A generic function for concat should not be written using the Add trait. You're correct that being generic over the ops overloading traits is a bad idea in general.
This is now like 3 levels off topic, but how do you mean ints are concatable? Or do you just mean vectors of ints?
The build system is currently a bit messy, I agree. There has been some investigations to find better alternatives ([this](https://github.com/rust-lang/rust/issues/20235) being one), but the general consensus was "if it works, don't break it until a new method is really really needed", I believe. Rewriting the massive makefiles, and proving it to work correctly as before would be extremely time consuming. But it would be great if some brave guy could tackle on this! Anyway, what I do currently is: don't run tests while developing, only run them just before creating a pull request. There is the `make rustc-stage1` target that skips the tests and stops after building the binaries. On my machine (MacBook Air 2015), it takes about 10 minutes to finish. If what I changed doesn't affect `libsyntax`, it becomes even shorter. This still isn't ideal and far from "instantaneous", but the pain is greatly reduced, at least.
I'll have to look at this later, it sounds really cool! You may be interested in my framework for audio processing. It might help streamline some of the mundane stuff. Search for oxcable on github or crates.io.
&gt; [...] how do you mean ints are concatable? As in integer concatenation: `concatenate(1234, 7653) == 12347653` . 
&gt; Add&lt;&amp;str&gt; is impld for String. "ab" + "bc" will not compile because both are &amp;str (test your code before you claim it compiles :p) I was using `".."` to denote `String`s, not `&amp;str`, sorry for the confusion. &gt; You cannot write generic code that uses add commutatively and accepts String. It just isn't possible with the existing impl because it is noncommutative at the type level. `Add` is implemented for `String` and `&amp;str`, so any generic function taking 2 `Adds` that get's passed 2 `Strings` can use `String + String` (the second `String` `Deref`s to `&amp;str`) and expect a result that is commutative. Or what exactly am I missing? EDIT: I think I see what you mean now! The second string does not deref to `&amp;str` automatically, so the error just cannot happen. TIL! Thanks!
Ok, that helps. I was wondering if the values were being displayed differently or if they were different numbers. Here I'm getting for stable: f64: 1.1234899999999999 3FF1F9D0A67620EE and for beta f64: 1.12349 3FF1F9D0A67620EF So they do differ in that last bit.
 fn main() { println!("{}", "ab" + "cd" == "cd" + "ab"); } [Doesn't compile.](http://is.gd/oVfrck) fn main() { let ab = "ab".to_owned(); let cd = "cd".to_owned(); println!("{}", ab + cd == cd + ab); } [Doesn't compile](http://is.gd/VOlNYF)
Right, but as I mentioned in the original post, `make check-stage1-std` isn't working for me, because it doesn't rebuild `libstd` when I change `src/libcore/str/pattern.rs` Edit: sorry, I meant that `make check-stage1-std NO_REBUILD=1` isn't working for me. Unfortunately, a new computer isn't in the cards for me right now, and re-bootstrapping on my laptop takes much longer than 8 minutes.
Getting frustrated with the behavior of `dllimport` in Rust and pushing to get it fixed. Finding [bugs](https://github.com/rust-lang/rust/issues/27438#issuecomment-142087038) and [ICEs](https://github.com/rust-lang/rust/issues/28575) while investigating how to get `dllimport` working. Also standard working on `winapi` and procrastinating. Helping people get their DX stuff working.
This is great! I am super excited to see Rust expand more in regards to Emscripten support! :) The more language choices we have for doing both frontend and backend work (in the same language) the better! It will be very nice once it is supported without any hassle, e.g. once https://github.com/rust-lang/rfcs/issues/604 nears completion
THAT would be very surprising. The outcome of concat should not be based on what base the int is interpreted to be in.
Deref coercion also wouldn't make &amp;str impl Add&lt;String&gt;, which it would need for a String to be passed to a generic function that uses Add commutatively.
Off topic, but I saw some code like `fn bind(self + 'a, f: F + 'b) -&gt; I&lt;U&gt; + 'a ` for constraining lifetimes. Is there documentation somewhere that describes this kind of lifetime usage? I went through most of the docs a few months ago but don't remember seeing anything like that.
Well, `make tips` says * `NO_REBUILD=1` - Don't rebootstrap when testing std (and possibly other crates) so I was hoping `make check-stage1-std NO_REBUILD=1` would rebuild the parts of stage1 that needed to be re-built without rebootstrapping. If `NO_REBUILD=1` doesn't rebuild anything at all, then what's the point of having it in the first place?
Okay, I was wondering if that was the case. Seems like I'll have to rethink my type system...
It is easy to make it work for numbers on any base [0], but IMO in the context of Rust `i32` and `i64` are "by default" interpreted as base 10 numbers (e.g. in `std` for conversions from and to `string`). For dealing with numbers in other bases one is better off defining other integer types, which allows implementing concatenation appropriately for that base (or doing this in a Trait, like `Binary` or `Octal` do in `std`). To me it would actually be surprising to use `i32` in a base different than 10 without wrapping it in a safe abstraction for that base. [0] https://en.wikipedia.org/wiki/Concatenation_(mathematics)#Calculation
This won't work either in this case, since `Eq` is not object safe.
Can you elaborate on what you mean by object safety?
Perhaps you could make a more complicated version of the function that returns an iterator with element type `Result&lt;PathBuf, Error&gt;` where `Error` contains a `PathBuf` and an `io::Error`.
&gt; GitHub Issues looking near-perfect in Servo (only pixel snapping + vertical-align left) Was really confused about vertical-align: left for a minute
Evaluating Rust for a new game company's first project (still). At the moment, trying to refactor some code samples into multiple files. The type and module systems fade away when everything is crammed into main{}, but simple stuff like "breaking main{} into multiple functions" has me buckling under the weight of my ignorance. For example: how do I separate [this example](https://github.com/PistonDevelopers/piston-examples/blob/master/src/cube.rs) into two functions (i.e. setup() and render(), with a struct for the shared data)?
http://doc.rust-lang.org/nightly/book/trait-objects.html#object-safety
This is amazing :D The javascript is a much more reasonable size than I was expecting too: less than 300kb on the wire.
This has been posted before and I'm posting it again without regret, it's just a very clear, nice talk about why Rust brings something new to the field. I hope more people get to see it. - Highlight: [Shared and mutable borrowing @ 22 minutes](https://youtu.be/O5vzLKg7y-k?t=1309) - Highlight: [Aliasing vs mutability diagram @ 33 minutes](https://youtu.be/O5vzLKg7y-k?t=1988)
That doesn't allow you to store trait objects in the hash map.
Just found this and am liking what I see. Can a FileProducer handle a gzip?
Typo should be fixed now, along with the mockup HKT syntax.
Of course, you're right, that still requires you specify a concrete type, which is the whole reason OP wants trait objects in the first place! If [this](https://www.reddit.com/r/rust/comments/3lujh1/storing_boxed_trait_objects_in_hashmaps/cv9gxh1) is right, it might not be possible to do it currently...?
I have now changed the syntax (aka. imitated Scala) and removed the `T` from `Monad` where possible (`T` is still needed for the variant where the `Monad` trait is generic over the function-type passed to bind, since the restriction on the function type requires `T`). Here is a [diff](https://github.com/m4rw3r/m4rw3r.github.io/commit/68bceb3f427b9f24ba0ec60f1b749a569a4a8981). From what I can tell this does not actually change anything, and the following issues still remain after I changed the HKT-notation and moved `T`: * Type of the function-trait of the function passed to bind (`Fn`, `FnMut` or `FnOnce`). * Type of the receiver (`self`, `&amp;mut self` and `&amp;self`). * Lifetimes of parameters to `bind`, since the callback passed to it might not be invoked immediately. * Partial application of type-constructors and types with associated types. * Monad implementation on a trait (though this seems to be solvable through other means).
You need to decide what sort of equality you want between trait objects, if you want to use them as a key: it sounds like you are assuming reference equality? That's certainly possible in rust, if unusual. However, you will need to use `Rc` or `Arc` to store the trait objects (without sharing, no two references will ever be the same!) You'll need to wrap the Rc or Arc in a newtype, and implement Eq and Hash such that they perform reference equality. This is significantly more difficult than it should be, since rust doesn't yet have a Hash implementation of fat pointers, so you'll have to transmute to a `raw::TraitObject`: you'll then be able to hash the data pointer. Hopefully the situation will be improved soon, and eventually there will probably be a ReferenceEquality wrapper built into the standard library, so you wouldn't have to do this manually.
&gt; For example: how do I separate this example into two functions (i.e. setup() and render(), with a struct for the shared data)? No idea, but it might be worth making a post about it either here or on StackOverflow. Most of the team members are more than happy to help out with people who are new to the language.
gotcha. Yes, I understand that :D
Rust's conversion from decimal strings to floating point numbers (and vice versa) used to be horribly inaccurate. Both directions have been now fixed, but while the formatting is [fixed](https://github.com/rust-lang/rust/pull/24612) slightly before 1.0.0 the parsing was [fixed](https://github.com/rust-lang/rust/pull/27307) only in the 1.4.0 release cycle. Stable is currently at 1.3.0 and you will have to wait for the accurate result. :)
In the first case, you're putting a `LinkBot` directly in the `Mutex`. In the latter case you're putting an `&amp;LinkBot` in the `Mutex`. `LinkBot` is `Send`, which is why the first example works, but it isn't `Sync`, which means that `&amp;LinkBot` isn't `Send`, so your second example doesn't work. The error about `*mut` comes about because the compiler figures out that `Mutex` is `Send` if `&amp;LinkBot` is `Send` and that `&amp;LinkBot` is `Send` if `LinkBot` is `Sync`. Since there's no `unsafe impl Sync for LinkBot`, the compiler falls back on the default implementation, which checks whether each field is `Sync`. This encounters the `*mut field`, resulting in the error. Edit: beat by /u/burntsushi's much more detailed answer.
I've converted D3D12 bindings into winapi-rs format. Also, I did some testing of my safe(ish) wrappers around them. http://i.imgur.com/h7ehtps.png The code can be found here: https://github.com/red75prime/dxgen/tree/master/src/dxgen/scaffolding You will need to install [Windows 10 Standalone SDK](https://dev.windows.com/en-us/downloads) or Visual Studio Community 2015 to build it, and you'll need Windows 10 to run it. set LIBRARY_PATH=C:\Program Files (x86)\Windows Kits\10\Lib\10.0.10240.0\um\x86 for building with Rust-mingw i686. I was unable to get working executable with Rust-mingw x86_64. Use Rust-msvc x86_64 to build 64-bit executable.
Not at the moment. It's vaguely on the roadmap for the future, but there are still some wrinkles to work out.
Is there a way to optimize out the runtime overhead of dynamic dispatch so that I don't have to copy paste the same code into every implementation of the trait object? (Not that I would do such a horrid thing unless speed were absolutely critical, of course.)
First, carve out your submodules with `mod { .. }`, adding `use` statements to make the required dependencies available (the compiler is already pretty good at telling you which). Next, move the contents of the module blocks into their own files (sans squiggly brackets) *with the same name as the module*, leaving only empty `mod render;` etc. statements. Happy rusting!
I think the name "deprecated" is perhaps a bit suboptimal. The crates therein aren't deprecated in any sense of the word. Perhaps "rust-waiting-booth" would be more fitting, as those projects wait for new maintainers...
[Here](https://gist.github.com/anonymous/e64e4fd33aa3f5c8a183)! The shader program is compiled offline to triangle_vs/triangle_ps.
Orphaned?
For comparison mioco version: https://github.com/dpc/mioco/blob/master/examples/echo_udp.rs Some features missing, but should be trivial to add. Multithreading comes for free.
It should be doable to write a producer that takes a file and decompresses it before sending it to the consumer
I was kinda supprised that num was with the deprecated ones. Then I realised all the cruft that it contains. I have only ever used it for the traits it offers.
Ah, took me only 16 hours to understand the joke...
It's already done. Try it: https://github.com/rust-lang/getopts Github takes care of it automatically. :)
Show off :P
*The Doorstep. It's where crates go to die...*
You're a hero.
Nice slides, good introduction.
The original post says this: &gt; cheap downcasting; &gt; Sort of: see the as_activatable pattern. But not as fast as a type tag check. I was thinking that a `TypeId` would be a type tag (which is universal, but that's just a bonus). Is that not the case - is the type tag more of a bit mask of implemented traits or something? &gt; I tried very hard to an efficient representation allowing down-casting for open-ended hierarchies in the rust-poly prototype and finally settled on a dual-encoding Cool stuff :-)
In case anyone is wondering what this project is - you can see the end result running here: http://tcr.github.io/rust-todomvc/ &lt;quote&gt;TodoMVC implemented on the client entirely in Rust cross-compiled to JavaScript.&lt;/quote&gt; The Rust project that handles the transformation of Rust code into javascript (and provides the javascript browser integration libs) is: https://github.com/tcr/rust-webplatform It seems similar to the emscripten project that converts C++ -&gt; JavaScript: http://kripken.github.io/emscripten-site/ Actually, from looking at the rust-webplatform project it seems it's using emscripten under the hood. A page full of related insightful comments: https://github.com/rust-lang/rfcs/issues/604 
Jokes aside, it's obvious to me that a `rust-lang-orphanage` would fit right alongside `rust-lang-nursery`, and send a clearer message to boot about the ultimate destination of the crates in question. So I'll second that proposal.
I must have meant this then: https://github.com/japaric/std-with-cargo Why not cross-compile and just copy the result over? That would be cheaper and faster with all those arm Soc's around.
Thanks! What's kind of funny is, the explanation why is more "low-level" than just listing the restrictions, which is why I just kept it small. I'll see what I can do...
Fair enough.
curl -L https://raw.githubusercontent.com/juanibiapina/zas/master/scripts/installrootkit.sh | sh Na, thanks.
OP's title even says emscripten ;)
It conflates a lot of things that should be separated into different crates. The traits, the bigints and the complex numbers should be separated. The bigint implementation is dreadfully slow (by a factor 1000 compared to GMP). *Edit:* Also see https://github.com/rust-lang-deprecated/num/issues/102.
[AtomicHashMap](https://github.com/valarauca/StringInterning/blob/master/src/lib.rs) was completed last week :D but its not *that* useful unless you use unsafe code to pass its references to each thread. Now I'm attempting to build a key value storage on top of it. [Write up if anyone is interested](https://github.com/valarauca/KeyCache/blob/master/src/datastructures.txt). Learning about boxed closures was really interesting. Now I'm trying to avoid using Tuples for everything, but the ability to selectively ignore, and deference pointers within a function declaration is stupidly nice... Or just make my whole function body a single match statement of the Tuple. 
&gt; I was thinking that a TypeId would be a type tag (which is universal, but that's just a bonus). Is that not the case - is the type tag more of a bit mask of implemented traits or something? It's a type tag, however it is not present directly in the type; you need to dereference the v-pointer to access it. This has two implications: - a v-pointer is a pointer: 8 bytes and 8-bytes aligned on x64 (when most of the times you don't have more than 256 variants...) - indirect access to the v-table, so potentially some cache miss It's not too bad, but it's a bit "worse" in terms of memory size/access speed.
&gt; I suspect there's a point of diminishing returns here, where the additional complexity needed to support this nicely in the library space is addressing only extremely niche cases. But it's definitely worth thinking about. I agree. Complexity is a tax, for this feature and any further feature that will come to interact with it; it may however be an argument to actually move that outside the compiler as much as possible :)
It's good, this way we'll get num in shape. It's in need of remodeling.
There has been some talk about [Non-lexical borrows](https://github.com/rust-lang/rfcs/issues/811) and it's generally agreed that the current borrow checker could be improved in this regard, but getting all the details right and sound is not easy. As for your actual problem, I *think* the problem is the following (and I'll count on people correcting me if I'm wrong :-) ) In `parent.get().borrow().field`, there are two objects constructed and dropped, one `Rc&lt;RefCell&lt;Child&gt;&gt;` and one `Ref&lt;Child&gt;`. The `Ref&lt;Child&gt;` borrows the `RefCell` for the lifetime of the `Ref&lt;Child&gt;`, so `Ref&lt;Child&gt;` must be dropped *before* the `RefCell` - otherwise, there would be no `RefCell` to un-borrow when `Ref&lt;Child&gt;` is dropped. Let statements are guaranteed to be dropped in reverse order when the scope ends, so this would work: let a = parent.get(); let b = a.borrow(); But in the case of a single statement, this is not guaranteed. Therefore this does not work: parent.get().borrow(); As for *why* things are not guaranteed to be dropped in the reverse order for statements, I'm not sure. Maybe someone more qualified can fill in on the details.
I often see these projects defined as "X in SomeLang" where X is something I've never heard of and completely non-descriptive. I stopped following / googling to find out. Now I look into only those projects where a description actually describes what it does. What is Pow? I guess it doesn't matter.
&gt; this is going away in six weeks What? Rust 2.0 will be released in 6 weeks? :O (Deprecated stuff in `std` that was previously stable is not going away anytime soon.)
Only if it was unstable at the time of deprecation.
/u/dbaupp has a series of 4 blog posts with a lot of detail on the subject with http://huonw.github.io/blog/2015/01/object-safety/ being the most relevant.
It's a tradeoff between describing the whole thing or comparing to something that already exists. Choosing a target audience.
That I don't know. /u/nikomatsakis, do you?
 // And use the now uninitialized pointer! Wee! Party like it's C/C++! Can we have `rustc` emit different code/wrapping features like UBSan, ASan for the sake of `unsafe` blocks?
This sounds like something lintable, really.
Yep, I think that also would have done this. This code though dates way back to 2011 (boy I'm old), so I can't quite recall if this was written before we had lifetime elision.
There are many different uses of `unsafe`. The problem with this advice is that you'll have to know what to test for (unless you use afl and throw a lot of CPU time at the problem). For example, I have used `unsafe` exactly once now (in my [optional](https://github.com/llogiq/optional) crate), and I'm pretty sure it's correct in all cases (there's only a struct with a valud that I want to return a zero-or-one-length slice over). It uses an elided lifetime, which I'm also pretty sure of. I *could* add a compiletest where I try to get a static lifetime out of this, but barring bugs in rustc it would be quite pointless. There are other uses of `unsafe` in [Turbine](https://github.com/polyfractal/Turbine), which I'm trying to get to work after it went stale for about 6 Rust versions. Maybe I'll try this technique on its tests.
But I have java installed, and on my path.
According to [`configure`](https://github.com/rust-lang/rust/blob/655b2ef/configure#L774-L776), you need *both* `javac` and `antlr4` in the command line to use the lexer test. The message seems a bit misleading, it should read "javac/antlr4 not available" instead (Hint: a good nitpick to make a PR!), but it is expected otherwise.
I think it is in the book isn't it? 
As with all things, there's always room for nuance :) You're right, your `optional` crate looks safe as far as I can tell. But as /u/gankro said in the Rustonomicon: &gt; This is the fundamental problem of safety: it's non-local. The soundness of our unsafe operations necessarily depends on the state established by otherwise "safe" operations. A project like yours could go through some refactor years from that accidentally breaks your safety guarantees by slipping an errant `'a`, (or some other violation) in the wrong place. In your specific case, you'll probably be safe since your usage is currently well constrained, but it still can be useful for documentation and auditing purposes. A large project like Servo is eventually going to have to sweep through all their dependencies to make sure they aren't slipping in security holes. It'll be a lot easier to check that a library using `unsafe` is in fact safe is if there are tests that guarantee that certain operations are impossible.
As a macro writer, are you interested in the same feature applied to tuple structs, i.e. `struct TS(); // Empty`? [RFC 218](https://github.com/rust-lang/rfcs/blob/master/text/0218-empty-struct-with-braces.md) considers empty tuple structs useless and therefore the are disallowed.
I posted this because of the incremental safety profiles and lifetime stuff
That's a good idea. Even if Rust *should* make such errors impossible (unless you use unsafe) it cannot hurt to be able to check. Also this should improve results of afl runs.
Stackless coroutines are effectively state machines. This is async/await concept present in multiple languages. Async function body is transformed in such a way that each 'await' there becomes kind of 'yield' entry point. So when async function is constructed, you receive not a plain function, but a fn-like object. Then, when execution reaches await, function is stopped, storing its stop-point somewhere in fn's context. And then, I suppose, fn is added as finish callback for await'ed future. When it finishes, our async function is restored from the place it started. Something like this. Next, on stack. I personally dunno why setting up a _stack_ by itself is expensive. But setting up a thread definitely is. Because it implies creating kernel object for thread, setting up its context, etc. etc. The stack itself is just a piece of memory being utilized as stack. Lastly, on single-thread coroutines. We cannot assume anything safely because of modern OS schedulers. But. Let's assume we're recreating coroutines via OS threads. Then for each coroutine we need to create new thread object, then run it, then kill it when coroutine finishes. And creation/shutdown is what really expensive. Then, let's review stacked coroutines construction. I'll base my explanation on boost.context. To construct coroutine there, you need to: a) allocate chunk of memory for the stack b) optionally, add guard page at stack top (just in case someone will try to overflow coroutine's stack) c) place dummy context store data on our new fake stack (registers, stack base/top, instruction pointer etc.) d) set instruction pointer so that our new coroutine will start executing function we specified Next, to switch coroutines you need: a) store current registers and some other thread data onto current stack b) write current stack's top into memory cell provided by caller - so current coroutine is stored properly c) use destination context pointer as other stack's top and read other context's data into registers etc. in reverse order d) continue as if nothing happened - you're already on a different coroutine. Then on how long it takes. Coroutine switching takes around 40-50 asm instructions. Which I consider to be cheap. I dunno anything about thread switching on HW interrupt. But I think it should be cheap because modern OSes usually flip threads relatively often. But I suggest that constructing coroutine is much, much cheaper than a thread. Allocated stacks can be pooled for reuse. So you'll pay only about the price of context switch to create new coroutine.
Why are these slides vertical? Not only video can now be vertical but also slides.
Umm, how stack management is more expensive than syscall? I mean, most coroutine implementations use assembly tricks to store current registers/stack/IP and then restore them from other place. So stack here is just a memory chunk utilized as stack. And managing it shouldn't be more expensive than of a native thread stack. And if we pool those pseudo-stacks, we amortize their construction costs.
&gt; Bullet 2. is a good reason. Would it work to add the cleanup handling as a destructor on Notification's iter struct? It's probably all possible, it just gets a bit painful to pick only one pattern. For example, maybe a new bullet 3 would be "you want to be able to request new notifications in the context of an existing one", so holding a borrow can be painful. It probably makes the `for_each` pattern bad as well, but the `while let Some(time, ...)` pattern ends up being ok. 
The static analysis tool they demoed (to be given a preview release by Microsoft next month, according to slide 67) will probably help correct a ton of bugs in real code. Some of the lifetime examples were very impressive and Rust-like. Here's a somewhat complex example of what it can do right now, from slide 50 in the 'function parameters' section: 1 void f(int*); 2 void g(shared_ptr&lt;int&gt;&amp;, int*); 3 4 shared_ptr&lt;int&gt; gsp = make_shared&lt;int&gt;(); 5 6 int main() { 7 // ERROR, arg points to gsp’, and gsp is modifiable by f 8 f(gsp.get()); 9 10 auto sp = gsp; 11 // ok, arg points to sp’, and sp is not modifiable by f 12 f(sp.get()); 13 14 // ERROR, arg2 points to sp’, and sp is modifiable by f 15 g(sp, sp.get()); 16 17 // ok, arg2 points to sp’, and sp is not modifiable by f 18 g(gsp, sp.get()); 19 } Notes on the example: * Pointer-like return values of member functions of `gsp` are assumed to be owned by `gsp` (if not, the member function needs a lifetime annotation, or there will be an error when *it* is compiled). So, `gsp'` (the raw pointer returned by `gsp.get()`) is owned by `gsp'`, and the pointee is also owned by `gsp` * The local `shared_ptr`, `sp`, is deduced to alias the inside of `gsp` (I'm not sure how) * The function calls succeed or fail depending on whether there is a dangerous form of aliasing involved. (Hopefully line 18 would error if we made all the `int`s into `vector&lt;int&gt;`s). Miscellaneous thoughts: * The static analysis is 'local', which I understood as meaning it can look at one function at a time (plus any visible class definitions and signatures of called functions, but not their bodies) * If I understood correctly, the tool is not given special awareness of any user-defined types like `shared_ptr` * ~~It seems that this doesn't relate to the `owner&lt;T&gt;` and `not_null&lt;T&gt;` annotations from the guidelines/gsl project~~ * Concurrency hasn't been addressed * Could this work for a sufficiently large subset of C++ to be useful? Or is it doomed? Edit: [paper pdf](https://github.com/isocpp/CppCoreGuidelines/blob/master/docs/Lifetimes%20I%20and%20II%20-%20v0.9.1.pdf)
Theoretically, will it be possible for C++ to have some kind of `safe {}` block, inside which everything behaves like in Rust? :-)
Thanks! :) In the end, I couldn't even attempt the live code demo because there were so many questions about the borrow checker. Too bad, I really wanted to demo how cool cargo is. Also, I think I focused too much on the "systems programming" aspect. The audience was basically half C++, half Java, half Scripting (err... there's probably some overlap) so I hope I didn't lose half of them. Still, I got some pretty positive feedback.
It gives me a warm fuzzy feeling that a /r/rust regular learned something from my presentation :) Thanks for the link /u/Baby_Food! I added it to the presentation.
Learning more every day :P
&gt; I needed something like this but couldn't find it anywhere `python2 -m SimpleHTTPServer` or `python3 -m http.server`.
See https://internals.rust-lang.org/t/lint-against-unbound-lifetimes/2257
I guess it's optimized for printing. You don't want to print one slide per page.
Haha. I thought that was intentional! I loved it! :-)
Other way around, I think. Rust is heavily influenced by C++ best practices.
For now, if someone want to use this and use Wahoo here is package for that https://github.com/hauleth/pkg-rust (after installation you need to `cd $WAHOO_PATH/pkg/pkg-rust; and git submodule update --init; cd -` for now, till [#100](https://github.com/wa/wahoo/issues/100) is resolved).
In general, yes. The function would probably just be called `child` for a `Ref` and `child_mut` for a `RefMut`. Unless you can avoid the `RefCell` altogether somehow - if you could, that would be even better.
``` python -M SimpleHTTPServer ```
Was this the prime motivation for the change? Couldn't think of any reason myself. And what's up 👆 with the funky pattern matching rules?
Memory safety avoids only a small class of bugs.
`practically_rust {}`
Has Herb given this presentation already, and if so is recorded anywhere?
The questions I have right now is: - Is there a way to determine the positions of the channels with a PCM object or with the hwparams? (for surround sound configurations). - I enumerate the list of supported formats with the `hwparams`. When I query the `hw` device I get only the formats supported by the hardware, but when I query `default` I get all the possible formats that exist because ALSA does the conversions for you. Is there a way to filter out the formats that would require a conversion to be performed by the software? - When I enumerate the list of devices, I open each device one by one in playback mode to check if opening it returns an error. Is there a way to query whether the device is capture or playback without opening it? I saw in the source code of an utility that devices have an `IOID` characteristic that can be `Input` or `Output`. However most devices simply return null when I query their `IOID`. I had a lot more questions one or two days ago, but after a lot of experiments I'm slowly getting a better grasp at how it works. 
Right.. seems like the perfect opportunity for *you* to contribute to [The Book](https://github.com/rust-lang/rust/tree/master/src/doc/trpl) :)
Fine but this requires a good insight. And automatic coersions are so fundamental that I desire no less than a complete understanding.
He has, and it is, but the video isn't out yet. Given Bjarne's talk went up two days after he spoke, I would expect Herb's to be up tomorrow.
https://github.com/rust-lang/rust/pull/28603
Very true, I've hit that wall myself. Or to be more precise, I'm still hitting that wall from time to time. I'm by no means proficient in Rust. Did you read the chapter about [Deref coercions](https://doc.rust-lang.org/stable/book/deref-coercions.html) in the book? Maybe that clarifies things a bit.
One thing I've wondered about Rust -&gt; emscripten: presumably emscripten needs to emit code for any rust libraries you depend on, even very basic things like maps. But ES6 already has a map type. Would it make any sense for Rust to have a Map type that is designed to implement exactly the same semantics as ES6 map, so that the output code can use ES6 maps directly? Is it possible? Would it provide a notable benefit (code size and/or performance)?
This is monothreaded though. It can only serve a single file to a single client at once. 
SIMD here we come!
What do you mean with "they can't access memory"? A lambda in C++ can take a closure over its enviroment by &amp;, const&amp;, copy and move.
Progress! struct B; //let b : B = &amp;B; - does not work let _b2 : &amp;B = &amp;&amp;B; // works Why? It *is* in [8.3.2](https://doc.rust-lang.org/reference.html#coercion-types): &amp;T to &amp;U if T implements Deref&lt;Target = U&gt; and low and behold: there [is](https://doc.rust-lang.org/src/core/ops.rs.html#1102-1106) actually a Deref impl converting &amp;&amp;T -&gt; &amp;T :) Now to understand how Deref trait and '*' operator interact..
I've got rendering issues with github viewer... ... but still manage to spot a tip up to Rust on slide 4 :)
Thank you!
I thought the reason was that it introduced an ambiguity between the empty tuple constructor function and the empty struct's single value.
I needed this for my own purposes so I figured I'd wrap it up nice in documentation/examples in case it's useful to anyone else. If you have any suggestions please do file an issue and/or make a pull request. I tried quite hard to create providers for local and in-memory filesystems, but I ran into object safety issues which halted my progress. I needed a FilesystemProvider trait to have associated types (i.e. for the File type, the directory iterator type, etc), but this involves referencing Self which violates object safety and as a consequence makes trait objects for FilesytemProvider impossible. If anyone can think of a way to get around that I'd be super happy to hear it.
I have continued coding my OpenGL wrapper, and last night got to the point where I'm able to render [a triangle](http://i.imgur.com/T4tFPec.png).
Thx a lot, I suspected my terminology was wrong :) &gt; Method calls, field access, indexing and calls ... get autoderef. Just to be 100% clear - is there a small typo here? "method calls ... and calls"?
At least the array -&gt; slice is because of Deref.
Hi Steve, are you saying these coersions &amp;[T;N] -&gt; &amp;[T] &amp;mut [T;N] -&gt; &amp;mut [T] *const [T;N] -&gt; *const [T] *mut [T;N] -&gt; *mut [T] work via a Deref and so may be removed from [8.3.2](https://doc.rust-lang.org/reference.html#coercion-types)?
The coercion docs are at https://raw.githubusercontent.com/rust-lang/rfcs/master/text/0401-coercions.md
calls refers to overloaded calls - `let f = compute(); f(foo)`
Cool! FYI, /u/frankmcsherry, you got a typo in the "Breadth first search" section. I think you meant to have `if node.is_none() {`, not `if done.is_none() {`?
I _think_ that this is true, but haven't checked in the code to confirm. This isn't my area of expertise. DST took a while, and I'm not sure how much of it is special.
C# also has an `unsafe` keyword with similar purpose (e.g. use raw pointers).
Woops. Probably `done[node].is_none()`. Stupid pseudo-code. &gt;&lt; Thanks!
You can use `*mut ()`, but I would strongly recommend using `Unique&lt;()&gt;` if you can.
There is `c_void` in the `libc` crate.
In the talk, Herb Sutter repeated the point about excessive annotations in Rust, but then went on to acknowledge that it had similar elision rules to this system.
Oh no, I totally confused that with something else, didn't I? For some reason I thought that the "introduction to" articles were a trilogy and added the "of 3" to make it more exciting (because it something like the finale of the "introduction" subplot). And to be honest, I posted that here before reading this part after lunch. Well, damn. Next time, I'll try to drink more coffee. I'm looking forward to reading part 4, then!
Method calls are `receiver.method(...)` whereas overloaded calls (via `Fn`, `FnMut` or `FnOnce` impls) are `receiver(...)`.
That RFC doesn't specify the unsizing coercions as they are today (which is why it took so long for them to be implemented, the RFC was super-complicated about them).
Great, thank you!
Paraphrased audience questions referencing rust directly: 1:08:09 - "Can we have the safety of rust by making a library?" 1:17:50 - "Should we put rust-like tools in the compiler? Or make separate tools?"
Thanks, my bad.
I don't think preventing mutable aliases is even on the table right now. There would be a *huge* problem with noise in existing codebases. Right now a well-written C++11/14 codebase will be "mostly clean" according to the restrictions they've laid down here. Going beyond that to tracking mutable aliases or any of the messy concurrency issues requires a lot more changes to source code. This is the same reason their tool errors on *using an invalidated pointer*, rather than Rust's *error when you invalidate a live pointer*. It's so common to invalidate and then ignore a pointer in C++ code that erroring on it would make the tool useless when examining legacy code.
Yeah. I wasn't sure whether it was a contradiction or something I was missing.
On dealing with `Option`: You should almost never use `unwrap()` on `Option`. I really only use it in doc tests. There are several ways of handling `Option`, depending on what you are trying to do. If you are making a decision on the interior value: match option_thing { Some(val) =&gt; val, None =&gt; // do something else. } I try to avoid this, because it is tedious. However, if you want to compare multiple options, or options wrapping errors or whatnot, you can make more complicated matches, and these are awesome: match (option_1, option_2) { (Some(a), Some(b)) =&gt; //... a &amp; b have values (None, Some(b)) =&gt; //... b has a value (Some(a), None) =&gt; //... a has a value _ =&gt; //... Both are None. } These are useful if you don't want to do anything when the value is `None`: if let Some(val) = option_thing { // Use val in here. You own it. } if let Some(ref val) = option_thing { // Use val in here. You've borrowed it. } The idea is, if you don't put `ref` in there, you're trying to destroy your `Option`. Another thing, if you have an option wrapping a result, you tend to do this: if let Some(Ok(val)) = option_result_thing { // Do something with val } If you just want to extract the interior value and panic (like `unwrap`): let val = option_thing.expect("error message") --- On `Result`: The `try!` macro will unwrap your result if it is `Ok`, or return the error if it is `Err`. Thus it only works in functions that return `Result`. If you want to ignore an error value, you convert it to an option with `result.ok()`, which will become `None` if it holds an error. You can then `.expect()` on it to get the value (or anything I've listed above.) I'd keep a link to [Result](https://doc.rust-lang.org/std/result/enum.Result.html) and [Option](https://doc.rust-lang.org/std/option/enum.Option.html) nearby, because some of those functions can really save some time and some needless nesting of brackets.
I'm also pretty new to rust, but if I'm not mistaken, raw strings can be used to improve that json literal in the example you cite: &gt; let data = Json::from_str("{\"foo\": 13, \"bar\": \"baz\"}").unwrap(); Could become: &gt; let data = Json::from_str(r#"{"foo": 13, "bar": "baz"}"#).unwrap(); Might help with writing test cases or something. Just figured I'd mention it.
That made me laugh. [Probability density functions](https://en.wikipedia.org/wiki/Probability_density_function)? ha
Ah neat, so r#"..."# is the syntax for that? Sounds useful for regexs, thanks.
That's a [closure](https://doc.rust-lang.org/book/closures.html). It's a way of creating an anonymous function so you can pass an action to another function. What `map` does is apply the function to the interior value(s) of an object. &gt;Do you use a lot of combinators in error handling (or in general) like in this article[1] ? I haven't had a need to, but it looks like a good idea.
Don't quote me on that. I'm pretty new to rust. That's the syntax used in part of the lalrpop tutorial, but I've also seen just `r"..."`. And I think the actual syntax lets you put whatever character you want in place of `#`. Hopefully someone who knows the syntax better will comment.
Is there a way to "overload" a function with two options? One function will have a borrowed parameter and another one with moves? I'm guessing you can't overload so what is the best practice? 
wow - a language that can replace C with anonymous functions... That's pretty damn cool. Thanks
It appears just by existing Rust makes the IT landscape move in the direction of safety, as if it was a gravitational vortex. Everyone who bashes C++ because of unsafety, take note: We don't even need to replace C++ to improve general safety, it suffices to be there for them (well, to exist at all, really) so they'll improve themselves.
This needs a more structured process than a forum post. Perhaps it could be integrated into crates.io or made into its own site: weekly.crates.io maybe?
One way to get the full path to a type is to add a function to the type's impl that does what you do now (take the last bit of type_name) and prepend it with module_path!() e.g. impl Foo { // in mycrate::foo pub fn typename() -&gt; String { use std::intrinsics::type_name; let str; // module_path!() returns "mycrate::foo" // type_name::&lt;Self&gt;() returns "foo::Foo" unsafe { str = format!("{}::{}", module_path!(), type_name::&lt;Self&gt;().split("::").last().unwrap()) } str // mycrate::foo::Foo } } You could turn this into a macro you invoke in all your type impls.
when i do that it fails at the end of next method. onyl if i clone the rule compilation succeeds. and cloning introduces constant increase of memory usage. as i said before, everything works with &amp;'a str but not &amp;'a Rule. edit: if i create new Rule outside of this file (e.g. main.rs) and pass the reference to World::new everything works as expected. I think i understand the problem but i could not think a neat solution. i wish to pass a string identifier as a rule and create the internal Rule struct inside World implementation.
Nice! Looks like a fun game, are you going to write a web client?
Sorry, I'll have to defer to someone who a) knows more than I do about web programming and b) has the time and access to crates.io to actually pull this off. Otherwise, great idea!
Why not? It should omnicomplete, it works like a charm here.
Would storing an `Rc&lt;Rule&gt;` in `World` fix the issue, along with the memory usage? (I'm not familiar with Rust or the implementation of the Game of Life, so my apologies if this question/suggestion is out of place!)
It usually is the time when the cpu has been turned on - precise_time_s returns the value of internal cpu counter which is increased with every tick (in case of TSC) or a counter that's increased at constant frequency (in case of HPET), scaled by the frequency. 
Try https://github.com/contain-rs/linked-list maybe?
What happens is that the Internet has finally shown how bad idea it was to bet on C and keep its semantics in Objective-C and C++, ditching the safety features from Algol, Mesa, Ada et al. Meanwhile, developers that knew those languages were forced to adapt, while newer generations got used to the idea that C was the ever first systems programming language. Now we need languages like Rust to bring the safety back, while improving other ideas in the process, like the substructural type system. Also curious to see how Rust will fare against Swift in the Apple world.
You might need to set up racer itself along with the vim racer plugin
That'd be more work for me. Also, I'm sure we'll come to the smaller crates in time. I'll have a lot of weeks to introduce crates in.
I had the same problem (I also use Windows), but openssl is a dependency of Iron. Do you know of a similar library/framework that doesn't depend on openssl?
You could add an optional feature "rebuild-css" to your `Cargo.toml` and check in your build script if it's set (cf. [env vars in build scripts](http://doc.crates.io/environment-variables.html#environment-variables-cargo-sets-for-build-scripts)). Though that means you'll have to remember to use `cargo run --feature "rebuild-css"`, but it also allows you to define optional dependencies for that feature. You could also do some hack-y stuff like checking whether the Cargo.toml of the parent of the current output directory is that of mdBook or just anther crate that uses mdBook as a dependency.
I'm with you on this. I feel a bit ashamed to show people that, for example, even simple REST service requires nightly. I store configuration in INI and for some reason INI parser *needs unstable features*. An INI parser! I wish there would be "Stable Rust" badges on repos more than I need "Safe Rust". Even more useful would be a list of unstable features the crate uses so you could know the potential impact of unstable features getting removed and at least some timeframe to expect stabilization in. On Android, some developers explain to end users why they need particular permissions for the app. We need something similar I think. And now even hyper, being the default HTTP implementation, doesn't mention it needs nightly *at all*.
Are Hash* in the std lib good for use? I remember a post a few months back disputing that using the std versions was a poor idea. Sorry if I'm out of touch. 
They're good for use, bu they're a touch slower than you might expect due to using a cryptographic hash (SipHash) by default. This protects you from hash collision vulnerabilities, but is typically somewhat slower than using a non-crypto hash.
Is omni different than ctrl x + ctrl o ? Cause I know that works
If you stick a FNV hasher in it it's great (or xxhash if your keys are real long)
Seems like a better safe than sorry strategy which I can understand. It's useful to know where there is a performance gain if necessary down the road. Seems more than adequate for general use though. 
Thanks for the info, ~~I am however not able to use the env variables in my build scripts. ``` let regenerate_css = env!("CARGO_FEATURE_REGENERATE_CSS"); ``` gives me the following error: ``` error: environment variable `CARGO_FEATURE_REGENERATE_CSS` not defined ``` Do you know what I am doing wrong?~~ **Edit:** I was able to get it working by using: ``` if let Ok(_) = env::var("CARGO_FEATURE_REGENERATE_CSS") ``` Thanks !
Thanks for the suggestion, tho I'm kind of reluctant after seeing a comment stating "100% legit (no it's not)" inside an unsafe block :/ Maybe /u/Gankro can tell more about the state of it?
Is there a way to set up vim-racer's autocomplete so that the suggestions appear as you type, instead of having to press c-x c-o manually? (I.e. Like how YCM behaves)
Both std and this one do basically the exact same thing (iirc, there were some patches to the std impl recently to reduce this a bit, maybe it's totally "clean" now?). Writing `unsafe` everywhere is a pain in the neck in LinkedList, so we both write a simple wrapper around a raw pointer that hides that from us. There's little benefit in writing it all over the place (I maintain unsafe is a module-level taint, so the existence of any unsafe block is sufficient). You're writing a doubly linked list without GC. It's obviously unsafe. I'm unaware of any decent abstraction that would be worth introducing to make this even vaguely safer. It's flipping around a bunch of pointers. I actually do try a bit to limit the API; references are tied to their handle so you aren't always dealing with unbounded lifetimes. iirc it was mostly just annoying, though. 
* Fantastic, didn't know about clippy * allow(dead_code) was around main to mute the warning that main was unused when compiling the tests, but as with your last suggestion, I could move the test to the top level * Specialized errors here sound better * I will look into adding doc tests Thanks for the feedback!
serde works on stable
You'd want something like shoggoth-rs' binary representation of type-level numbers. Peano numerals are about the worst possible representation when it comes to performance / recursion depth.
thanks for the tips. at least i understand why the code did not compiled. and the borns and stays hashmaps, yes i used them for just efficient key access and the contains function. i thought with a regular vector it may be inefficient to check if some elements exist in the vector or not. 
Yeah, but it's at compile time, so who cares?
YCM. I meant YCM, not omni-complete. Ugh. I use omni-complete. 
It doesn't work with YCM yet, AFAIK. 
🙊 I've been identified
Right: https://play.rust-lang.org/?gist=8752e38feaed9e215d6c&amp;version=nightly :) 
There is such a flag! rustc devs think of everything. I still don't consider that very viable. We need code that is generic in all array sizes, N = 10^9 too, and it's not really viable to raise the recursion limit to that level.
&gt; What type is obj, and how would I figure that out? trick from IRC folks: compile this and read the error message let x = ...; let () = x; 
Yeah, the author of dimensioned mentioned 63 or 64 as the recursion limit somewhere, I think. Anyway, I started this with a use case in mind (type-safe tensors), but now I realized it can actually be done a bit easier ;) Still, even writing ~5 impls manually looks ugly, so if it can be done in a nicer way, it's still something, right? ;)
You might want to use a `HashSet` instead.
Hey, I wrote dimensioned. I'd be happy to split peano into a separate crate. I just didn't think it would see much standalone use when I wrote it.
i think that it can still be packaged by distros. But it is hardcoded to call `launchctl` - perhaps it works only in Mac OS X? (a worse issue is that it calls `sudo` in install.sh. A script should either be meant to be run as root or not...)
&gt;sorty.rs:1:19: 1:31 error: unresolved import `rustc::lint::EarlyContext`. There is no `EarlyContext` in `rustc::lint` [E0432] ?
I have been tempted to see if switching dimensioned to using a binary representation would be worthwhile as well. Arrays tend to need much larger numbers than dimensions do, though, so it makes more sense for you. In any case, the last time I looked at shoggoth it wouldn't compile so I put that on hold. It seems that type-level numerics will be likely be coming to Rust eventually, so any implementation will be temporary (even if counted in years). I'm glad you've appreciated dimensioned. It still needs a couple things for me to consider it truly useful, but if it helps give insight to Rust's type system, then that's great too.
Rustc will fail to compile some peano constructs and be *very* slow on others.
I've forked [cargo-clippy](https://github.com/arcnmx/cargo-clippy) and created [cargo-sorty](https://github.com/jbradaric/cargo-sorty). I hope it's useful to someone other than me.
Ah, I see. I was curious because /u/Manishearth keeps a custom branch of [clippy](https://github.com/Manishearth/rust-clippy) to run against Servo, fixing stuff it finds.
This is the video of Herb Sutter's presentation at CppCon from Tuesday. It includes some explanation of lifetimes in C++, and a demo of Microsoft's forthcoming lifetime static analyzer (which will see a preview release in October). Related links: * [Slides from this talk [pdf]](https://github.com/isocpp/CppCoreGuidelines/raw/master/talks/Sutter%20-%20CppCon%202015%20day%202%20plenary%20.pdf). * [r/rust](https://www.reddit.com/r/rust/comments/3m1h6j/herb_sutters_presentation_writing_good_c14_by/); [r/cpp](https://www.reddit.com/r/cpp/comments/3m0d41/writing_good_c14_by_default_herb_sutter/); [Hacker news](https://news.ycombinator.com/item?id=10263129) * [Bjarne Stroustrup: "Writing Good C++14" [video]](https://www.youtube.com/watch?v=1OEu9C51K2A). * [r/rust](https://www.reddit.com/r/rust/comments/3m1ifg/cppcon_2015_bjarne_stroustrup_writing_good_c14/); [r/cpp](https://www.reddit.com/r/cpp/comments/3m1hdp/video_cppcon_2015_bjarne_stroustrup_writing_good/); [r/programming](https://www.reddit.com/r/programming/comments/3m6j2c/cppcon_2015_bjarne_stroustrup_writing_good_c14/) * [Lifetime Safety: Preventing Leaks and Dangling [pdf]](https://github.com/isocpp/CppCoreGuidelines/raw/master/docs/Lifetimes%20I%20and%20II%20-%20v0.9.1.pdf). * [r/rust](https://www.reddit.com/r/rust/comments/3m324d/c_lifetime_safety_preventing_leaks_and_dangling/); [r/programming](https://www.reddit.com/r/programming/comments/3m33gd/c_borrowing_builds_on_the_shoulders_of_giants/) 
Yeah, I wonder what version of Rust /u/hpsutter looked into. Maybe he'll be willing to tell us :)
It's pretty awesome that Rust is causing C++ to be made safer by its mere existence.^1 :) Collateral benefits. I wonder how they propose to deal with all the other not directly memory-related oodles of undefined behavior from C, like signed overflow. ^1 Technically, we only have correlation, but the causation *feels* pretty obvious to me.
I had given epsilonZ a PR that made it compile on a then-1.2-nightly, but he never merged it. I may be tempted to try my hand at such a type-level numeral crate...
I'm an owner on 23 crates and almost all of them are dual licensed under Unlicense/MIT, so that counts for a considerable fraction of the Unlicense crates you found. :-) I'm sure the Unlicense's current relative popularity won't be as noticeable in a few months time!
Perhaps defensiveness is the sincerest form of flattery :) Edit: I sincerely hope that the relationship between the c++ and rust worlds stays friendly, and technical arguments stay mostly fact-based. It'll be interesting to see some side-by-side examples of rust and c++ lifetimes for both syntax and semantics, once enough information is out.
It's really a tradeoff between effort and reward. BLAS and LAPACK have decades of engineering behind them, and it's going to take a long time to build out a competitive library. But it's still fun to build out libraries. You got to start somewhere, right? You can see a couple math libraries on [crates.io](https://crates.io/keywords/math), and if you search the archives here, you'll find other discussions on matrix libraries.
He mentions the similarity between c++ and rust lifetime elision later in the talk, not sure about the timestamp.
It doesn't work for me with just normal version numbers. I'm thinking it might be screwed up because I'm using multi rust. I ran Racer with log level debug, and it seems to find the right paths, but no completions for libraries 
Thanks, didn't know about this! I was recently doing experiments where this would have come in handy.
Where did you see that?
Any time. Me too, which is how I found it.
I'm a fan of the BSL (http://opensource.org/licenses/BSL-1.0). Like Expat but without the attribution clause, which is obnoxious for downstream proprietary users.
For anyone who might be wondering why MIT and Apache are so dominant here: dual MIT/Apache is the license that Rust itself uses. I personally license all my crates in this manner so code may freely flow between Rust and my crates without worry (also a lot of my crates are code that was ripped out of std ;)).
So I guess you're using the [regex](https://crates.io/crates/regex) crate? It looks like a bunch of stuff is re-exported from the private internal module `regex::re`. Here are the lines in question: https://github.com/rust-lang-nursery/regex/blob/f6199c24b09969b7ebb3df806564d3cf2356599a/src/lib.rs#L412-L417 Try using `regex::Error` instead of `regex::re::Error`. Seems like the compiler error could be improved here.
Try this instead (it compiles for me): use regex; #[derive(Debug)] pub enum PathParseError { RegexError(regex::Error), } // implemented From here impl From&lt;regex::Error&gt; for PathParseError { fn from(err: regex::Error) -&gt; PathParseError { PathParseError::RegexError(err) } } I got to this by thinking "If `regex:re` is private, it must be used internally by the regex crate, but if they want errors to be used outside, then they would re-export them under a different namespace." Thus, it is probably exported as `regex::Error`.
[Image](http://imgs.xkcd.com/comics/authorization.png) **Title:** Authorization **Title-text:** Before you say anything, no, I know not to leave my computer sitting out logged in to all my accounts. I have it set up so after a few minutes of inactivity it automatically switches to my brother's. [Comic Explanation](http://www.explainxkcd.com/wiki/index.php/1200#Explanation) **Stats:** This comic has been referenced 56 times, representing 0.0674% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_cvdepuy)
&gt; fn&lt;'a, 'b, 'c: 'a + 'b&gt;(&amp;'a T, &amp;'b T) -&gt; &amp;'c T I haven't watched the talk but this doesn't seem like a good elision: it allows returning `&amp;'static T` when passing in two `&amp;'anything T`s. Do you mean fn&lt;'a&gt;(&amp;'a T, &amp;'a T) -&gt; &amp;'a T or are they just forced by back-compat to have unsafe defaults?
I don't get it, that link is for a license that uses a clause that does not appear in the unlicense. As a recent convert to the unlicense do you have any real cases against it? This quote 2 links deep from burntsushi's post sums up why I dont think its an issue quite nicely: &gt; But one has to remember that contracts are not parsed by computers, &gt; but humans. If a particular contract is invalid in a jurisdiction, a &gt; *judge* will generally attempt to determine intent. The unlicense is &gt; designed to stand quite well as a statement of intent in jurisdictions &gt; that do not respect public domain assignments.
I'm wondering how this will affect C++ compile speed in the future. Heard Stroustrup saying yesterday he doesn't want a slow compiler like some other languages he didn't want to mention.
What language compiles slower than c++?
What does `'c: 'a + 'b` mean? It seems impossible to find in the documentation. As far as I understand, Sutter's approach gives the result the lifetime of both arguments (the result's "points-to" set is the "points-to" set of both arguments), so the shortest lifetime "wins". That is, given `'static` and `'a`, the result would be bound by `'a`.
This is really cool 
I use BSD-3-Clause because I am porting code from non-rust project and it uses that. I wanted to avoid all possible issues.
The intent can be manipulated to some degrees by the interpretation of the license, and while it seems not very plausible that the Unlicense will *actually* cause a problem, I would prefer to be [more explicit](http://programmers.stackexchange.com/a/147120). That's why my [Encoding](https://crates.io/crates/encoding) subcrates [1] and [Rust-strconv](https://github.com/lifthrasiir/rust-strconv) bears the [CC0 license](https://creativecommons.org/publicdomain/zero/1.0/) instead. I don't like an arbitrary bashing to Unlicense users anyway, but still there is a point. [1] I believe a significant portion of `CC0-1.0`-licensed crates is mine.
Since this is Python it could have been written (arguably more clearly) as `200 &lt;= req.status_code &lt; 300` or `req.status_code in range(200, 300)`. And that code is Python 3 (print function in the second snippet, hashbang in the github repo) so it actually only checks for the `200` code as Python 3 uses real division, so it should be `req.status_code == 200` or even better `req.status_code == requests.codes.ok`.
[Related issue](https://github.com/rust-lang/rust/issues/26454) about the compiler suggesting to use a private trait when there is a public re-export.
&gt; On is that lifetimes require too much anotation although I don't think he knows about the lifetime elision rules obviously he knows as he actually said that Rust has a very good default when it comes to lifetime.
I think it's worth leaving the GPL in - the reason being that MIT, Apache, BSD and MPL are all broadly similar in ideology. GPL is a more truly different option.
Don't you mean longest common subsequences? :-P
I first thought maybe you could use `no_default_features` to disable the ssl. But I got no luck with that, as the staticfile crate also depends on Iron but has no way of turning of ssl.
How does dual licensing work? Do I need to abide by both licences, or can I pick one?
Let's build it together. Do you want to set up a project or should I do it?
Packages are not complete? What do you mean by this? 
For importing things from external crates, you should always consult the public API documentation for the path. One of the nice things about `rustdoc` is that it presents a view of the world constructed by public imports inside the crate. So if things are re-arranged, there's no fear, just consult the docs and you'll get the public import path. :-) (This is not meant to discourage you from reading code! Do that! Definitely! I just mean, in the future, for finding import paths, it's easiest to consult docs.)
&gt; If it doesn't match that regex without the "a" (just (\d+)), that regex just gives me None so I can't see any parseint error. Indeed not. Regexes have no awareness of "integers" or any other such construct. All they know is, "Does the pattern you gave match? And if so, here are the offsets of the match and any offsets of capture groups you may have had." It is up to *you*, the programmer, to assign meaning to "the pattern does not match." In this case, you might add a new variant to your `PathParseError` enum like, "InvalidPathComponent" or something, possibly with some meta data about what went wrong. You'd construct this error if your regex didn't match and return it.
Brakmic on rust-users has written up a [tutorial](https://users.rust-lang.org/t/using-atom-to-work-with-rust-under-win32/2280) to get atom to work under windows. Perhaps this can help you sort out your problems?
It was already getting outdated last year when I started learning it (I didn't continue though). I believe the official documentation is getting updated, so if you need help, I suggest you start there. Edit: Yes, I forgot to mention that everything after 1.0 will be ok. Edit2: There are a lot of old tutorials whose writers hopped onto the hype train and then jumped off of it.
This is, indeed, important and something that can cause some real confusion. It has been brought up before and you may be interested in [this post on users.rust-lang.org](https://users.rust-lang.org/t/clean-rust-off-campaign/1393?u=ogeon) and [its Reddit discussion](https://www.reddit.com/r/rust/comments/366ey7/clean_rust_off_campaign_the_rust_programming/). It's about an attempt to collectively and systematically do something about the problem, but it seems like the discussion died off. /u/steveklabnik1 has also [created a version badge](https://users.rust-lang.org/t/version-badges-for-rust-blog-posts/2592?u=ogeon) that can be added to blogs and tutorials to show which Rust version they are targeting. Some blogs has a comment field or a link to some other place where feedback can be given. I'm not suggesting any mail bombing (please, don't!), but a gentle reminder that their tutorial/post will show up in searches and that an update or a disclaimer would be appreciated. Respectful and constructive critique is often well received. People forget about old posts, so this may require an effort from both sides.
&gt; ...look it up...look it up... Languages with type inference provide throwback of inferred types in the IDE. 
Dear $deity, not this discussion again, I had quite a lot of fights with /u/burntsushi about this and I really don't want that again, because he's a formidable human being and you don't try to get into too many fights with them. tl;dr: there are (quite large, like for example most of Europe) jurisdictions with no concept of public domain and the unlicense (while I understand and even support the intent) is just plain unusable there. There's a reason why CC0 has a fallback license should a public domain not exist. I find burntsushis solution quite charming: it keeps the intent, but allows to choose a permissive OSI-approved license instead. Other discussions with varying degrees of hostility can be found here: https://lists.opensource.org/pipermail/license-review/2012-January/000052.html http://programmers.stackexchange.com/questions/147111/what-is-wrong-with-the-unlicense
Thanks, that makes sense.
If you want to avoid legal minutiae, why not choose a license approved and recommended by the OSI? They did the work for you. Also, it doesn't have to be a judge misinterpreting it, there's quite a few layers before that - company lawyers, package maintainers, potential users.
yea, I can see that. It is just kind of a habit for me, so I didn't think anything of it at the time. /u/masklinn sent in a PR that I merged, so everything is right with the world now :)
I know [SolidOak](https://github.com/oakes/SolidOak).
Didn't take a look actually...
OpenSourceBash is an event to bring new contributors into FOSS projects: they present themselves, give a clear why and try to convince them to help. If you are in Berlin: It would be great to see you there. If you are not in Berlin: if you have good beginners tasks, ping me and I will try to put them on my slides.
I switched the generic arrays to the binary implementation now, but I tried modifying earlier version of the code to use your crate and it worked fine :)
The problem is that the first argument to `println!` has to be a string literal. This works: println!("{:?}", println!("Moo!")) Which outputs: Moo! () This is because `println!` returns `()` which is basically Rust's version of "I don't *have* a return value". Also, what the glossary says is broadly true; *most* things you can put in the body of a function are expressions: they evaluate to *something*. That said, numerous constructs don't *have* a meaningful value, so they evaluate to `()` (like loops, assignment, things like `println!`, …). The simplest *useful* example is this: let x = if use_zero { 0 } else { 1 }; In conventional imperative languages, `if` is a control statement that doesn't evaluate to anything. In Rust, `if` is an expression that evaluates to *either* the `then` expression or the `else` expression. **Aside 1**: note that you can turn pretty much anything *into* an expression by wrapping it in braces. For example, `let` isn't an expression, but this *is*: let x = { let y = 42; }; This assigns `()` to `x`. **Aside 2**: `;` trips some people up. In many cases, you can think of it as an operator that means "evaluate the left hand side, throw away its value, then evaluate the right hand side". When there *is* no right-hand side, it just evaluates to `()`.
In Rust, basically anything with braces is a block. A block can have a return value. For example, here is an if/else statement that returns a value: let foo = if 1 == 0 { "bar" } else { "baz" }; The "Why Rust" book explains this as well on page 7: http://www.oreilly.com/programming/free/why-rust.csp 
I love racer but idk why there is still no library interface to it. Running a new racer process for every keystroke is a bit silly. That is the reason YCM doesn't have a completion engine for rust, they can't implement racer.
It used to: https://github.com/rust-lang/cargo/pull/1165/files
&gt; Macros / syntax extensions may in I think you accidentally a paragraph. :)
You need to have a full mingw-w64 installation in your PATH, so that the `gcc` crate can use it to compile some things.
Is that extra elision rule actually going to result in meaningful decreased annotation burden? Off the top of my head I can't think of a function that this would actually reduce annotation burden for—maybe some kind of offshoot of `max` that returned references, but I've never actually used such a thing. We did a lot of analysis on the effect of the elision rules and I think we're at a pretty optimal location.
People has most likely been aware of the problem and different ways of avoiding it for a long time. I suspect that the biggest obstacle that prevented it from happen way sooner is that is has to be sufficiently fast, sufficiently ergonomic and sufficiently expressive. Heavy analytical algorithms are slow, sprinkling the code with annotations is tedious and not so ergonomic, while limiting the language to a manageable subset affects expressiveness. It's fascinating how far we have come and seeing what we are able to do today, compared to just one generation ago. Compiling Rust on a computer that housed one of the first C compilers would more likely result in a flaming computer than an executable. I remember how I used to power on the family computer ~13 years ago and grab something to eat while it booted up. The same process takes only a few seconds today. Anyway, thanks for sharing :)
&gt; What if tomorrow Germany votes to disallow waiving of copyright rights? Suddenly cc0 becomes meaningless. Granted so would the unlicense but the point is neither of them solve the problem. It doesn't need a vote for that. Germany already has no concept of waiving authorship. It only allows you to give everyone all rights of use. &gt;I plan to use the Unlicense for the same reason we don't compile packages for 386 anymore: at some point you need to draw a line a say "anyone on that side of the line needs to change or live without. We're not letting you hold us back anymore." Any jurisdiction that would twist or allow to be twisted the clear terms of the Unlicense are now deprecated in my eyes. This is an incredible argument, basically putting a complete law system above another. Just to put it that way: the British copyright system and the mainland system evolved at the same time, they are neither more or less advanced, just different in their approach. Finally: the terms of the Unlicense are not very clear.
The example was exactly C++'s std::max, which has this signature and is consequently a huge source of bugs because e.g. `std::max(y, z + 1)` is a bug (z + 1 is temporary on input but is returned from the fn by-reference).
Slower than Scala? That's hard to believe :P Couldn't help myself. I remember compiling rustc for like 8hrs. My Rust VM needs more than 4GB of memory :( I heard Rust improved on this front? I haven't compiled a binary in quite some time.
Do you have a demo of your presentation available anywhere? EDIT: Actually, I suppose it would be in German, wouldn't it. I'm of no help then. :P
Not a guy, so I guess I don't have to update my stuff ;)
Sorry, couldn't resist: You probably shouldn't be using `isize` (and `usize` in this case), use `i32` or `i64` depending on what you need.
Aliasing rules are my white whale: https://doc.rust-lang.org/nightly/nomicon/references.html Unforunately, we're not really sure what we want here. It sounds nice and great to say that oh such and such can't be aliased, but there's so many stupid corner cases. For instance, `Box&lt;T&gt;` definitely is the unique owner of `T`, but you can take `&amp;`s and `&amp;mut`s into it temporarily. So it can be aliased. You might then say "well, it can only be aliased by pointers derived from it" (this is the notion LLVM uses). That is, someone indexing into an array on the stack can't randomly find themselves in the Box's memory. But unsafe code can do whatever it pleases with raw pointers. It can do all sorts of stupid things *and we have no idea which of those stupid things is ok*. `Rc&lt;T&gt;` produces `&amp;mut T` when the refcount is one. BTree holds raw pointers throughout itself for "coming back later", but most of the time we have `&amp;`s to some places aliased by those raw pointers. We don't particularly do anything to obey any aliasing rules. We just do whatever makes sense. As far as I can tell, the only reason we're sound is that llvm has a really conservative aliasing model (its "derives" notion doesn't chase embedded pointers). You might think "oh well raw pointers don't have aliasing rules". Box is a raw pointer. Vec is a raw pointer. String is a raw pointer. Rc is a raw pointer. You would throw away any of the interesting alias stuff Rust can do. However with the exception of Rc, they don't contain a *bare* raw pointer, but rather a `Unique&lt;T&gt;`. I would like to believe we can do interesting things with Unique wrt to alias analysis. For similar reasons, I propose a [Shared](https://internals.rust-lang.org/t/pre-rfc-raw-pointer-cleanup/2544) type for Rc. Anyway the fundamental problem is there's a tradeoff: really strong aliasing rules open up avenues for novel compiler optimizations *and* for programmers to make assumptions. However they also open up avenues for programmers to fuck up and hit Undefined Behaviour (alias rules are basically just declaring a huge swath of Things With Pointers UB). To my knowledge, Fortran is supposed to be really fast because it has strong aliasing guarantees. Similarly, noalias is supposed to be unusable because it was too strong. I tried to trick Niko and Dan Gohman into hashing this out, but not much came of it. My current hopes are pinned to the Rustbelt team that is building a formal model of Rust including some unsafe constructs. Apparently they need to handle aliasing rules. *sigh* computers
Sounds good. I actually started it yesterday, but I haven't uploaded it yet because I can't think of a good name. I'm calling it `cint` so far, for compile-time integers, but I don't really like it. You have any better ideas?
ha. That's the spirit.
Do you know if there's a license that protects the authors from liability claims but doesn't have any condition, essentially MIT but without its clause? Most of my projects are very small and don't need complex terms or a clause like that.
Yep. Assume anything prior to the 1.0 release is broken. However, moving forward things should have better compatibility.
Rules of thumb, by date: * earlier than 2015: lol * earlier than March: probably not * earlier than May: possibly * post May 15: 🎉🎊🎉🎊
[BSL](http://opensource.org/licenses/BSL-1.0) (the boost license), that cmr mentioned. It is just like MIT but explicitly allows binary redistribution. I've heard from license-pedants that the phrase 'machine-executable object code' may be problematic (but IMO every legal document has seams for pedants to poke at and it's not a practical problem). [zlib](http://opensource.org/licenses/Zlib), though it has some additional, and unconventional clauses, that most don't care about. Edit: None of these are *quite* what I think you are asking, which is for no redistribution attribution requirements at all. I haven't heard of such a license. Maybe this is [CC0](https://creativecommons.org/choose/zero/) that is also mentioned in the thread, but I haven't any experience with it. It doesn't seem to have a disclaimer of warranty that MIT-likes do. [The CC0 FAQ says it does disclaim warranties](https://wiki.creativecommons.org/wiki/CC0_FAQ#Does_using_CC0_affect_my_ability_to_disclaim_warranties.3F). This may be what you want if you don't want to require anybody to mention your name, the name of your software, or the license; you just want to safely set it free. IANAL, see other discussion in this thread.
But why it should panic in first case? If you cast `-1` to unsigned type you will get `usize::max_value()`. If you want to make safe cast between two numeric types you should use [`num::cast`](http://doc.rust-lang.org/num/num/traits/fn.cast.html) which return `None` if there is no possible conversion. But really I don't see the point. Probably you should write us what is your whole problem, not just one part of it.
I think rust has been getting leaner and faster for some time now. Even with evil compile-time shenanigans, I rarely see builds taking more than a minute, apart from really big projects like servo or rustc itself.
How about typenum?
But you are doing that (assume that `usize == u8` for simplicity): 0usize + (-1isize as usize) == 0b00000000usize + (0b11111111isize as usize) This is because x86 use [U2](https://en.wikipedia.org/wiki/Two%27s_complement). So what you get is: 0b00000000usize + 0b11111111usize == 0 + 255 == 255 This is how computer works and you should learn that in college/university/whatever. `as` is simple binary cast, it doesn't know anything about sign of number that it is casting. **EDIT:** In the case with subtraction: 0usize - 1usize = 0b00000000 - 0b00000001usize And that is impossible as will overflow. If we use `0usize.wrapping_sub(1usize)` then the result will be the same as above.
 (if signed &lt; 0 { unsigned - (-signed) as usize } else { unsigned + signed as usize }) is an expression. It can be made into a macro if you aren't fond of the size.
You've been downvoted but maybe you're onto something deep? Or is it about green threads again?
The main catch that comes to mind is if an app writer is relying on webkit/Chrome specific features. That could make plugging an app built for Chromium's runner into into a Servo's app runner a problem. Keep in mind that Servo's support of the web stack is fairly incomplete currently.
You might be interested in perusing http://doc.rust-lang.org/stable/book/academic-research.html . Lots of stuff that influenced Rust is really old, Cyclone being one of the largest, from 2001!
I am not a laywer. As authorship of some work, like some code, you hold the copyright. By default, rights are "all rights reserved," meaning nobody can use it. But since you hold the copyright, you can also provide the work under the terms of any other licenses, as well. Including multiple. &gt; Do I need to abide by both licences, or can I pick one? You can pick any license for which you're authorized to use. In the case we're talking about here, you can choose either, or both. But there are more interesting cases, as well. For example, some projects offer software for free under the GPL, but also sell MIT licenses. The basic situation is that the more restrictive (sort of, I actually like the GPL, ugh this is hard) license is free, but you get a less restrictive one in exchange for money. So in this case, it's dual licensed. But unless you've given the authors money, you can only choose one of them. I am not a lawyer.
Hehehe fair :)
I have a girl's name, is that good enough?
Agreed. 
`-isize::min_value()` will panic, unfortunately. So this isn't a solution and neither is the third one I wrote above.
I love "stream of consciousness" style programming blog posts, especially Rust ones. Very cool.
It looks like the recommended method of installing rust.
OK, again IANAL, and relicensing is complicated and murky, but I have some experience with it. I believe the answer to your direct question is yes, you need contributors' approval to add zlib to your existing MIT codebase. (If the two licenses were believed to be clause-for-clause equivalent I *think* it would be fine). You don't have the right to apply new restrictions to somebody else's code, even if you continue to provide the MIT option. Rust has done several emergency relicensings (sigh...) and the process generally goes like this: * Give people public warning that this is going to happen and opportunity to discuss and accept it * Find a list of all contributors who's names are *still represented* in your code base. Copyrightable material that is no longer represented in the present codebase, but just the history will continue to only exist under the old license, but nobody cares about this code. Note that making this determination is tricky since trivial code changes can make it look like somebody's contributions have fallen out of the tree when they still exist, just in a slightly tweaked form. * Contact them privately and ask them to *publicly* state their past contributions are licensed under the new license (perhaps on a GitHub issue or mailing list). We've done this both privately and publicly and I prefer the later for transparency. * After some time, assess which confirmations you've gotten. What you do next kinda depends. * If everybody said yes, just go ahead and do it. * With big projects that won't happen :) You have to examine all the exceptions: * If there are significant chunks of code without permission, leave those files un-relicensed and do the others, to get things started. You can continue to think about the others later. * Some minor contributions are not copyrightable, because they represent no creative expression (or whatever else implies copyrightability IANAL); think typo corrections. I've used this reasoning for individual cases, stating so publicly as documentation. * If it's code that isn't critical, you may be able to just throw it out wholesale, or e.g. replace one crate with another that has a better license * Other cases are harder. The legal issues around rewriting others' code to change the license are complicated, and we haven't done it. * Save relicensing statements somewhere (maybe sign them) This is basically the process that /u/graydon2 and I made up and evolved based on informed opinion of lawyers, but I provide no warranty that it is legally sound. Don't sweat making minor mistakes - nobody is going to get in serious trouble for accidentally failing to relicense a small contribution. 
I'm not quite sure what you mean. I care only about whether the result overflows, not whether the intermediate computation overflows. Eg in `1usize + -1isize` the intermediate computation overflows, but the resulting value has not overflown: it is still mathematically correct. But in `0usize + -1isize` the intermediate computation and the result has overflown, so we should panic in debug.
Thanks /u/rorlrkfirke. Glad you enjoyed the post.
I hope so, as I based my decision to license stuff under LGPL on that assumption. o.O Anyway, worst thing that can happen, is that some closed-source project won't be able to exploit my labour for profit.
They is mega PC.
I'm just wondering if we could do without the C code to remove the extra dependencies. From what I've seen there's mainly ASM magic happening in aesni_helpers.c and util_helpers.c. Reading through the docs I found there's the [asm!](https://doc.rust-lang.org/book/inline-assembly.html) macro for that purpose in rust. Can this be put to use? Don't get me wrong I'm not particular experienced in ASM nor C, just aksing.
You are still thinking in mathematical way. In CS we operate on bits, so `0usize - 1usize` is completely different operation than `0usize + -1isize`. I will be more explicit: In U2 (two's complementary) binary representation of signed integers we compute negative numbers as: -a == !a + 1 // where ! is binary negation So `-1i8` is `!0b00000001 + 1 == 0b11111110 + 1 == 0b11111111`. As `0b111111111` is also perfectly valid unsigned number (`255`) it **can** be **binary casted**, but it will completely different number. On the other hand it **cannot** be **mathematically casted** as `-1` has no representation in unsigned world. Rust `as` operator is **binary cast**, so it returns perfectly valid number `255`. No overflows, no panics. About your example `1usize + (-1isize as usize)` it will overflow as `1 + 255 == 256` which exceeds range of `u8`. So you need to understand that **binary cast** isn't **mathematical cast**. And `usize - usize` isn't the same as `usize + -isize`.
What do you mean by this?
No, I understand two's complement just fine. I'm in CS myself. I don't understand how your explanation provides a solution to my problem. I'm asking if there is an easy way to do addition between unsigned and signed integers that carry the same overflow semantics as the usual Rust addition operators (ie. they panic if they can't represent the value). Let's say we have such a function `f`. We could call `f(0, -1)` and it would panic, due to the mathematically correct result -1 not being representable as unsigned. Likewise, `f(1, -1)` would not panic *regardless of the intermediate computations*, as 0 *is* representable in unsigned.
It's intentional in order to avoid accidentally typing stuff like (C code ahead): if (input = expected) {...} When you actually wanted to type: if (input == expected) {...}
Thanks for reporting that. When you follow the compiler error message to the letter and it leads you back and forth it can be super frustrating for a newbie.
This is explicitly chosen. One problem with having assignment return a value is that most values are stored directly, not by reference. It would be incredibly unclear what to do with a non-Copy value. Would it be implicitly referenced? Would assignment just return `()` if the type isn't Copy? This syntax works fine when the value being assigned is a plain data type, but gets very ambiguous for other kinds of data.
I really strongly disagree with you and don't think this is a good venue for this discussion. I would encourage you not to raise the issue in contexts like this one in the future, because I don't think it furthers anyones' interests.
The other answers here about `if (foo = bar)` are most of the story, but not quite all of it. In most cases, even if the assignment evaluated to what is assigned, in `if EXPR { }` EXPR must be of type `bool`. So only if `bar` is a bool would `if foo = bar { }` compile. But both of these rules were chosen in the belief that it makes code clearer, not only in conditionals but in all cases. I almost never need to assign multiple variables to the same value in the same scope, but when I do I write this, which I think is clearer: let (bar, baz) = (foo, foo);
Rust uses this controversial technique, and this is a good dispassionate explanation of why we think it's ok.
Yes, returning the result of an assignment would make a complete mess of move semantics.
So did you give her yours in return, or is she still out there somewhere, unnamed and confused? :-P
Maybe there could be a method to assign copies where they're needed (I'm assuming there'd have to be a matching referencer tool as well): let (a, b, c, d, e, f) = 3.copier(); // Pass out copies to all variables let (g, h, i, j, k, l) = 3.referencer(); // Pass out references to all variables
Boring fact: I only found out that "a guy" meant "a man" when I found reddit, before that I'd only ever heard the plural version. 
You could pattern match on tuples on stable. Let (a, b) = (false, false);
It's all good. One thing about licenses is that organizations sometimes only accept code for specific license. Some places may only use MIT software, and not use Apache. In Ruby world, for example, MIT is used almost exclusively (or BSD), and nobody uses Apache. learning a new license is complex, and fraught with details. Not everyone may agree that the extra patent stuff is worth the complexity of a larger license.
Still not a lawyer. Generally, to re-license, you need to get permission from everyone who has the copyright. This is why many projects have a CLA, that assign copyright.
Don't forget that for the assign-and-test case, we can do that explicitly. if { a = predicate(); a } {
First, Rust's memory safety doesn't guarantee an absence of leaks. Rust's use of RAII patterns makes leaks less common, but its not a guarantee. See the notes on [mem::forget](http://doc.rust-lang.org/stable/std/mem/fn.forget.html) for some more info about this. Can't identify the leak in your code without knowing the definition of `read_file()` and `process_data()`.
Though if the intention is to avoid repeating a copy variable a bunch of times, tuple destructuring wont help any.
Yes I'll definitely consider the Apache license for bigger projects. At the moment though, none my projects are remotely close to being patent material. :p
Right, I should've added that I know I can still cause a leak by explicitly holding memory somewhere and forgetting about it. Anyway, added the source. It's a little more complex than the original pseudo. It actually waits for a file to exist, reads it, then copies a file to a location.
Are you sure you're actually leaking memory? On [line 42](https://gist.github.com/anonymous/4c6d5ebbd7d9879e64e7#file-playground-rs-L42), you're reading the entire file into memory. This should get freed after `read_bind_from_file` exits, but if you're reading bigger and bigger files, then your process's memory may appear to grow without bound. (Freeing memory doesn't necessarily given it back to the OS immediately.) Could you describe what you're observing in more detail? And possibly relate it to the number of files it's reading?
If your distribution of choice for some reason doesn't package rust itself anyway. Though really if you are doing more than a quick check, getting familiar with how your compiler is installed is probably not a bad thing as eventually you may want to install it in a non standard way and the source package is quite friendly at this point.
"political correct" (in the sense you're using it) was coined in the 90s. Singular "they" has been around for hundreds of years.
Even with the numbers, people still don't read them at times. Someone on HN was citing one of my posts from the 0.8 days, very frustrating.
Can I have a bigger picture of the servo runtime, in order to help making it possible? Where should I start or what kind of documents can I refer to? Thanks :)
Rust also uses it for more or less the same reason, we don't have packages for everyone. On Arch, I'd install using the community package (or previously the AUR package), Debian Sid has a package now, but not all distros have packages so there needs to be an easy way install Rust in the meantime. 
True. It seems like too many people happens to be very good at not reading important or informative notices, sometimes... I think the best we can do, in the end, is to push the old stuff into oblivion with new up-to-date stuff.
While I'd also like something like this, how would you propose to handle ambiguous results? type Farm = Vec&lt;Animal&gt;; type Pasture = Vec&lt;Animal&gt;; In the meantime, I tend to just use newtypes and write the boilerplate to delegate methods, as needed: struct Farm(Vec&lt;Animal&gt;); struct Pasture(Vec&lt;Animal&gt;); Someday we might have a nicer [delegation story](https://internals.rust-lang.org/t/syntactic-sugar-for-delegation-of-implementation/2633?u=shepmaster) though.
&gt; This is how computer works and you should learn that in college/university/whatever. Remember that not every programmer follows this particular path, and not every programmer actually needs to know this type of information. Let people learn, don't chide them for asking \^_\^.
Yay! You are using `unsafe` code (in this case, an entire C library, it looks like). Chances are, it will be [this function](https://github.com/jeremyletang/rust-sfml/blob/708c46580b26a943e770d9e98dd8cd2a5db8bf89/src/network/ip_address.rs#L103-L109): pub fn to_string(&amp;self) -&gt; String { unsafe { let string: *mut u8 = ptr::null_mut(); ffi::sfIpAddress_toString(self.ip, string); str::from_utf8(CStr::from_ptr(string as *const i8).to_bytes_with_nul()).unwrap().into() } } I don't know anything about the library, but it certainly seems suspicious. I'm not sure what ends up owning the string from FFI, so it's likely to be either a memory leak or a use-after-free, which would explain your crash. Time to fire up LLDB / GDB!
It's true, but I really hate this concept from a purity point of view. In my mind, `Deref` should be used for pure wrappers around another types. Note that most of the implementations in the standard library are for owned types reusing the borrowed type methods (`String` -&gt; `&amp;str`, `Vec&lt;T&gt;` -&gt; `&amp;[T]`, `Box&lt;T&gt;` -&gt; `&amp;T`, etc.). The pattern just doesn't flow the same for me. Then there's the problem about having composing together multiple parts — `Deref` only allows one.
Well, you can always wrap it and unwrap it: pub struct LibraryType; struct MyUseA(LibraryType); struct MyUseB(LibraryType); mod library { use super::LibraryType; pub fn make_thing() -&gt; LibraryType { LibraryType } pub fn do_cool_stuff(_: LibraryType) {} } fn start_thing_1() -&gt; MyUseA { MyUseA(library::make_thing()) } fn my_thing_1(a: MyUseA) { library::do_cool_stuff(a.0) } fn start_thing_2() -&gt; MyUseB { MyUseB(library::make_thing()) } fn my_thing_2(b: MyUseB) { library::do_cool_stuff(b.0) } fn main() { let a = start_thing_1(); let b = start_thing_2(); my_thing_1(a); // OK // my_thing_1(b); // Nope my_thing_2(b); } 
&gt; *RustyCage, FTFY ;) Thanks, typo fixed. ;) &gt; How about Ironborn instead? :) Looks pretty cool. I'm not the greatest fan of "rustacean", to be honest. If it was up to me, I'd make it "rustian" probably. English is not my mother tongue, though. ;)
Probably because it's been planned for a while to use `a: b` for type ascription (*i.e.* inline type hints). That's not a problem for struct literals because there's no confusion due to context, but that wouldn't be the case for function arguments. To answer the more *general* question of "why doen't Rust have named arguments"... it's not *that* important, and I don't believe there's any consensus on what it should even look like.
What version of SFML are you using? rust-sfml only supports SFML 2.1 right now, and is known to crash on later versions. But it's also possible that it's another kind of bug in the library. You can open an issue on https://github.com/jeremyletang/rust-sfml/issues/
heh... whatever brew install sfml installed I guess. $ otool -L iptest iptest: /usr/local/lib/libcsfml-system.2.2.dylib (compatibility version 2.2.0, current version 2.2.0) /usr/local/lib/libcsfml-window.2.2.dylib (compatibility version 2.2.0, current version 2.2.0) /usr/local/lib/libcsfml-audio.2.2.dylib (compatibility version 2.2.0, current version 2.2.0) /usr/local/lib/libcsfml-graphics.2.2.dylib (compatibility version 2.2.0, current version 2.2.0) /usr/local/lib/libcsfml-network.2.2.dylib (compatibility version 2.2.0, current version 2.2.0) /usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1197.1.1) There you go. Thanks. From opening it up with lldb I think the issue is its trying to call strlen on a null or invalid pointer maybe. Or it's something weird about the simd instruction that it wants to execute, and a null pointer.
The problem was that `sfIpAddress_toString` expected to write into an existing buffer, but it was passed a null pointer by rust-sfml. Since the implementation of `sfIpAddress_toString` is trivial, I added a pure rust implementation instead that works correctly in https://github.com/jeremyletang/rust-sfml/commit/a0af92fe58fac52f9e96cb33e6a25075ebb3ef96
Oh, awesome, thanks! Didn't realize you were a primary contributor. Yeah, I ended up doing it this way, treating it like C code: pub fn ip2str(ipu32: u32) -&gt; String { let ip3: u8 = (ipu32 &amp; 0xff) as u8; let ip2: u8 = ((ipu32 &gt;&gt; 8) &amp; 0xff) as u8; let ip1: u8 = ((ipu32 &gt;&gt; 16) &amp; 0xff) as u8; let ip0: u8 = ((ipu32 &gt;&gt; 24) &amp; 0xff) as u8; let ipstr0: String = format!("{}.{}.{}.{}", ip0, ip1, ip2, ip3); ipstr0 } Any idea which way is more efficient? I'm not sure how much overhead there might be to a `format!`macro, or my shifts versus the conversions you were doing.
I'm not sure which one is more efficient. The internal representation of an sfIpAddress seems to be the ip address string itself. My implementation just converts that into a UTF-8 string. However, if you are not using sfIpAddress, and just want to convert an integer to an ip address string, your version *might* be more efficient, and you are probably better off not using a heavy weight dependency like rust-sfml just for doing that.
Yeah, I'm certainly better off without importing sfml into the project at this point for that function alone - but I did want to test SFML out because the ipaddress and http modules might be very helpful in the future. And I have to admit, coming from Python, I'm very used to just importing a whole library for small reasons. I have to break that habit. Anyway, in regards to contributing, I'll be sure to use Rust-SFML more in the future, and if I run into any bugs again like that, I'll see if I can patch it, or at the very least I'll come up with detailed debug notes, how to reproduce and post an issue. Thanks for your help!
I’m not familiar with this myself, sorry. I’d recommend reading https://github.com/servo/servo/issues/7379 (and other issues linked from there) and commenting on it.
Has anyone considered a `foo.bar { arg: val }` call style?
To be fair, it's not very interesting. :P pub trait Invariant&lt;T&gt; { type Context; type Err; fn check(&amp;self, value: &amp;T, context: &amp;Self::Context) -&gt; Result&lt;(), Self::Err&gt;; } impl&lt;F, T&gt; Invariant&lt;T&gt; for F where F: Fn(&amp;T) -&gt; bool { type Context = (); type Err = (); fn check(&amp;self, value: &amp;T, _: &amp;()) -&gt; Result&lt;(), ()&gt; { if (*self)(value) { Ok(()) } else { Err(()) } } } #[derive(Copy, Clone, Eq, PartialEq, Ord, PartialOrd, Hash)] pub struct Unsafe&lt;T, I&gt;(T, I); impl&lt;T, I&gt; Unsafe&lt;T, I&gt; where I: Invariant&lt;T, Context=()&gt;, I::Err: ::std::fmt::Debug, { pub fn new(value: T, invar: I) -&gt; Self { invar.check(&amp;value, &amp;()).unwrap(); Unsafe(value, invar) } pub fn set(&amp;mut self, value: T) { self.1.check(&amp;value, &amp;()).unwrap(); self.0 = value; } } Also, I fibbed *slightly* in that it's using `&amp;()` rather than `()`, *but still*.
Oh, yeah, it's expected that if you've got generics/overloading via traits then some functions may take `()`. What would be surprising to me is the second half of my comment: actual code creating that `()` via an assignment, e.g. `self.1.check(&amp;value, &amp;(foo = 1))`, or, more dangeriously for the proposed syntax, `foo.bar(baz = 2)`.
That's not as clean though, and it doesn't unify struct-like constructors and function calls.
For me Rustacean sounds similar to something edible from the waters of earth (also too long, same as "ferrous oxides"), Rustian similar to a word describing something from the country. English not mother language either. :)
Can you tell us what doesn't work? Does it compile? If not, please show us what errors you are getting. If it does compile, can you describe what output you get and what the expected output is? 
That `asm!` macro is what `rust-crypto` used to use, but they decided to move away from that because `asm!` is an unstable feature so you can't use it in stable Rust.
One thing that may be of interest: you can attach `#[doc(inline)]` to a public re-use of a symbol. This causes the documentation to pretend that the *canonical* definition of that symbol is the re-use itself. So there's that. As for the rest... yeah, that's a weird one. I'm not sure what the convention should be. *Personally*, I'm inclined to have the `-sys` crate just be the C FFI bindings, with higher-level ones (*i.e.* those that reconstruct the corresponding "high-level" C++ bindings) in the non-`sys` crate. But that's just me.
did same here..except 'bash not recognized
&gt; I should try to find what pointer it's trying to pass to strlen. Maybe it's zero here and that's what's going on. You already have that, actually: thread #1: tid = 0x318a8, 0x00007fff92360732 libsystem_c.dylib`strlen + 18, queue = 'com.apple.main-thread', stop reason = EXC_BAD_ACCESS (code=1, address=0x0) So yes, you are definitely running `strlen` on a NULL pointer (`address=0x0`), as [CrumblingStatue also found](https://www.reddit.com/r/rust/comments/3mey50/weird_runtime_error_illegal_instruction_4/cvem93i) 
...and swapping could maybe look like this: `a = (b = a);`
I guess we'd have to see if we'd break anything by making `struct Foo {...}` have a presence in the value namespace, but it should otherwise work, i.e. nothing assumes much about expressions like that before type-checking inspects them.
i argued for “*don’t* wait until 1.0”, because this way the stdlib could make use of it instead of being built around its absence oh well, that ship has sailed --- another thing is that this is basically a more efficient and pretty alternative to the builder pattern
Also, if you have a large type (like one that has `[u8; 1024]` embedded in it), you're at the optimizer's mercy whether or not all of the `memcpy`s are yanked out.
Happens allllll the time. 
For a more traditional alternative check out [gtk](https://crates.io/crates/gtk) ;)
I mean, the advantage of multirust is that you can have the stable Rust by default, but for some projects use the nightly - cargo will build with the right compiler. I think this is great because some important packages may not build on stable, I'm all for easing the transition to nightly (and back) when necessary.
Tip: don't expose the implementation details of your structs (like [here](http://www.johz.me/figtree/figtree/types/struct.Document.html)). I mean, the advantage is that the client can call `HashMap` methods on the `node` field.. the disadvantage is that they can call those methods (and will break if you change for something else)
That's not apparent to me. Some people complain about the way it *looks*, particularly on OS X.
`.wrapping_neg()` seems to exist, too
Yeah, I'm not sure how to avoid the boilerplate. Perhaps someone will have a better suggestion.
It's all over the submission form and the sidebar. Does no one ever read or are Rust players all only on mobile?
That amount of ram should definitely be enough to compile even the largest Rust project (unless you build in parallel in which case you might be getting it close sometimes). Your CPU should be good enough to get reasonable build times. As for your GPU, Rust does not currently use GPU acceleration to speed up compilation or optimizations, perhaps you'd be interested in contributing to that area?
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/rust_gamedev] [Piston question: how can I change the color of an image?](https://np.reddit.com/r/rust_gamedev/comments/3mhgwb/piston_question_how_can_i_change_the_color_of_an/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
They are fast at uploading the videos :) The slides are available [here](http://dev.unhandledexpression.com/slides/strangeloop-2015/). Please ping me if you have any questions about that talk. I wanted to give a feel of how hard data encoding and parsing can be before getting into how nom eats your data.
Rust has macros, which are approximately as good an idea as inheritance for code reuse; why introduce another way?
No problem here on FF with NoScript+uBlock after allowing JS. Thought that the controls were broken at first because they weren't visible, but arrow keys work. Transitions are very choppy though.
The problem is that inheritance conflates code reuse with subtyping, when they're really pretty different things.
I don't have a link on hand, but I remember the controversy, as it were. The purists believe `Deref` isn't meant for general wrapper types, but specifically for pointer types. A `Vec` isn't a pointer to a slice, and a `String` isn't a pointer to a borrowed string.
Again, I can only agree, yet notice that without actual cases of code our discussion is purely academical.
Fair enough. Anecdotally, what I've seen people really want inheritance for in Rust is not code reuse, it's the cheap upcasting (a la netty) which probably does require a form of structural subtyping.
Not exactly directly related to the talk, but what situations do you think Nom is *not* well suited for?
No, not yet. I'm a late presentation finisher. The presentation will be in English.
Dual license means that the person who is using it (copying it, distributing it, whatever act may be protected by copyright that needs a license) can choose between the licenses. The Unlicense is designed to be as unrestrictive as possible, essentially dedicating the work into the public domain. The problem is, the way it is worded is believed not to work well in certain jurisdictions, and thus leave a legal cloud of uncertainty over whether the work can be used, distributed, and built upon in those jurisdictions. So, the MIT license is another choice that you have for using the work. If for whatever reason the Unlicense doesn't work for you, you can instead use it under the terms of the MIT license. The MIT license is widely known, widely accepted, and most people feel that it meets the requirements for licensing in most jurisdictions. The intent, then, is to license the code as liberally as possible, allowing people to do what they will with it, and dual licensing it under the most liberal of common licenses, or the Unlicense which goes even further but may not be accepted everywhere, achieves that goal.
I don't see any Travis CI testing, do you need a hand setting it up? Does it have any system dependencies? Does it runs in parallel when possible? Cheers and good work
In the presence of the Unlicense, assuming a jurisdiction where it functions,, copyright is disclaimed, so what powers the MIT license? I'm not very good at law. :(
Won't I just get the same image with a background of the color of the rectangle?
cpp? Do you mean the C preprocessor?
Why are people voting the parent down? This is actually a good question. The short answer is that those that live in jurisdictions where the Unlicense doesn't work wouldn't be able to use your changesets, as you haven't given a license for them to be used.
&gt; that the author doesn't have the copyright to the work anymore since it's now in the public domain? In the jurisdictions where unlicense can't work, that phrase doesn't make sense. That's literal: in most of continental Europe, an author *can not* lose their moral rights (they're perpetual and imprescriptible) and *can not* put their work in the public domain (that *only* happens when the economic rights to the work expire), so your fork through the unlicense is literally unlicensed, for all intents and purposes "all rights reserved" and possibly illegal, and the original work can only be used through MIT.
You can use Deref::deref_mut() and Deref::deref() to control the mutability of references to the inner information at deref-time. This is actually used by the compiler when you use the * operator, but sometimes the the compiler can't work out the mutability of the reference you want (I've only run into this when chaining methods but it might happen at other times) so it's super useful to know about it.
Still somehow baroque, but it now has a crate, a version 0.2.0, a documentation page (with some text extracted from doxygen comments).
He can't. Dual licensing is a terrible idea and requires you collecting colyright reassignment papers for contributions. You cannot claim implied license agreements that way. Nobody should be doing this.
A bit late in the week, but I've been working on adding support for rigid body animation to my [ray tracer](https://github.com/Twinklebear/tray_rust). So you can do things like move objects and lights around over time and make animations, here's two short samples: [moving block](https://www.youtube.com/watch?v=H3b0coVvl6s) and [moving light](https://www.youtube.com/watch?v=CBgrco4gHf8) with relatively low samples per pixel (why they're noisy).
Andrei Alexandrescu [gave a talk a few years ago](https://channel9.msdn.com/Shows/Going+Deep/C-and-Beyond-2012-Andrei-Alexandrescu-Systematic-Error-Handling-in-C) about error handling in C++ where he showed his Expected&lt;T&gt; class, which is a lot like Result. A [variant of it](https://www.reddit.com/r/cpp/comments/2686fn/n4015_a_proposal_to_add_a_utility_class_to/) was proposed for standardization a while ago, with a reference implemention [to be found here](https://github.com/ptal/expected). I seem to recall Boost having it as well, but I can't find it.
&gt; PS: I recommend you ask this in Stack Overflow, having "rust" and "opengl" tags. It's typically more helpful. Even if I ask about Piston in particular? It's not too specific?
I hadn't read that before. Is there a source you could post for this? Why can't I just stipulate that contributors must use the same licensing terms?
Because monads are an abstraction, they exist even if you can't directly express them. This is where a language without higher minded types, like Rust, sits: we can't express the general concept of monads, but we can have certain _instances_ of Monad.
This makes me ecstatic, and opens up so many possibilities in kernel space.
How small are your frequencies? You might consider a sanity check. Instead of summing probabilities, just sum frequencies. Check that their sum equals the total number of words.
I'm happy to see this being worked on! I recently had a project where I needed this, and was running into the issues this fixed.
Can you give us some examples of the magnitudes of numbers involved?
That's great! Having your install script independent of port redirection would be cool too (in Linux, you can bind to port 80 without root by setting [CAP_NET_BIND_SERVICE](https://stackoverflow.com/questions/413807/is-there-a-way-for-non-root-processes-to-bind-to-privileged-ports-1024-on-l/414258#414258), so iptables isn't strictly necessary either)
Something I don't quite understand is whether each launched program is one-shot (like CGI), or is long-running, serving many requests once it is started by Zas.
This is really cool, especially suitable for Rust. How do filesystems interact with this? (Mounting, tmp, etc.)
[As I understand it](https://importantshock.wordpress.com/2009/01/18/jquery-is-a-monad/), any wrapper around another type with a wrapping constructor and support for method chaining (methods which return another monad) counts as an instance of the monad concept, regardless of whether the language has full-blown "support for monads". `Result` and `Option` easily satisfy those requirements.
This is ridiculously cool. 
I'm pretty sure that `as` conversion on integers was decided to never panic (I don't remember where it's documented though). I recall I've tested this code on `usize::max_value()` and it worked on debug mode.
Most of the time, if you need more than double precision, you are doing something wrong... typically using unstable algorithms. The others suggested it already (but didn't say it that way) : change your algorithm, for probabilities, f16 are usually enough (not always and I'm not advising to actually use f16 for anything at all, I'm just saying that for most probability stuff like you describe, f16 should be enough so f64 even more).
Yeah, the basic idea is that you mount block devices. For example, you can pass `-b somefiles.iso,/data` to the rumprun launcher. This will tell the hypervisor (i.e. qemu or xen) to expose `somefiles.iso` as a block device, which is automatically mounted by the guest into `/data`. I believe it should also be possible to mount a network share using smbfs, although it would require some manual work. The default configuration just has a minimal vfs containing `/dev` (for `/dev/{null,zero,random,urandom}` and device nodes) and an empty tmpfs in /tmp. 
`min_depth`, `max_depth` and `max_open` should probably take architecture-independent integers (like `u32`) instead of `usize`.
https://ticki.github.io/rebar/ doesn't work for me in Firefox, I don't see any crates and title is always switching between "Rebar" and "Connection...". Works in Chrome though, maybe I just have too many extensions installed in my Firefox. Also, storing IP address hash for IPv4 (which are the majority of client addresses currently) doesn't increase any privacy as it is trivial to reverse.
I briefly considered that when I initially wrote down the types, but all three of them are used in `Vec` indexing/length operations.
Uhhh unless I am misunderstanding, this is very ill advised. Logs aren't homomorphisms, not even by approximation. Edit: I should clarify that I meant under summation. The log from the multiplicative reals to additive reals is in fact homomorphic.
I am trying this now. I do remember hearing this talk earlier in my computer science career, so I will go change my adding algorithm and see what happens. 
Not a bad idea - I'll do that as well. Currently the smallest is 1/741, but it will go to much smaller. It is the amount of times a word is used divided by the total number of words, so in a book with 100,000 words, if a word is only used once, then it would be 1/100,000. 
Thanks, I'll look into this as well.
I haven't, but that's a good idea. I will do that.
One thing to watch out for: boost::optional&lt;T&gt; implicitly converts to bool, which can lead to subtle bugs if you were trying to implicitly convert T to bool. Example: http://ideone.com/q3K4lt
Rust is about zero-cost abstractions, it isn't a tool primarily targeting numerical analysis so the std shouldn't pick and choose which random algorithms from that field it should include. 
I was more suggesting you could poach the code than that you should use this directly. edit: to answer your edit, you're right, its not a Kahan sum, its a more accurate algo stolen from Python's [fsum](https://docs.python.org/3/library/math.html)
Does anyone know of a good blog post covering the porting process for the `std` crate ? I would like to port it on y bare metal kernel but I find it hard to find information online.
This example is not complete. You don't include in this example how you calculate the `times` value for each `word`, nor `total`, nor even whether `accuracy` is actually initialized to 0.0, all of which would be crucial to checking your work. I'm also a little puzzled, since you talk about counting words, but you have a variable `unique_chars` which appears to contain pairs of characters and counts. Are you sure you're actually counting words here? Are you sure that `times` and `total` are both being incremented exactly one for each `word` that you are counting, and you are not including anything extra? For instance, I notice that you are storing `\n` in your `unique_chars` array, but are ignoring that when outputting to `file`, is it possible that `times` for those entries has a non-zero value but that is not being included in `total`? I would recommend you read that link I provided about writing a [short, self contained, correct example](http://sscce.org/); it discusses how to produces a good piece of example code to ask your question, that includes everything necessary to compile, run the code, and see the problem. If you don't know what the problem is, and you don't include all of the code that is necessary to reproduce the problem, then it's possible that your problem is in the code outside of that which you included. Here's an example of summing up floating point numbers in the range that you give, that shows that while you do wind up with a little bit of error, it is a tiny error compared to what you are seeing ([playpen link](http://is.gd/Usy1FQ)): fn main() { let mut sum: f64 = 0.0; for _ in 0..741 { sum += 1.0 / 741.0; } println!("{}, {}", sum, 1.0 - sum); } This gives you: 1.000000000000008, -0.000000000000007993605777301127 So, ideally you can distill your problem down to something that you can run in the playpen; but even if not (for instance, it needs input and you can't find a good way to reproduce the problem without an actual input file), it should be something that we could compile and run on an input file to see for ourselves, rather than just a snippet.
Hmm.. yeah, but the `sum` is more like a convenience method in any case.
Ah, I just copied the loop out of my code. And I'm currently analyzing chinese characters, so a character is a word. Thank you for taking the time to explain my errors in this, I'll go work on replicating it in the playpen, and read those links.
Ok, I made a playpen link with exactly the same logic that I am using: http://is.gd/VKH4Lq I am getting zero accuracy "drift" (I don't know if there is a proper term, I sort of made that up). I thought the error must be in my population of the list, then. So I went to check it using the constant total variable and comparing it to my unique_chars total, and sure enough, they weren't matching up. This whole exercise of recreating it in the playpen helped me find the true underlying issue, so thank you! I do feel that this thread is a bit of overkill, though, now that I realize what the error is. Sorry about that. I'll now know how to approach asking a question in the future! 
I love all the examples in the readme! After about 30 seconds of reading, I know enough about the API to use this crate effectively.
You mention that this is designed address issues with the stdlib's `walk_dir` implementation, do you have any plans to submit this to the stdlib?
`libtest` is hidden behind `#![feature(test)]`, and is considered internal, so it's hidden from the main docs.
Although you've had other answers and found your issue, Another option is to use a rational type that is made of a numerator and a denominator. As long as your denominators are all the same (not counting simplification), you've got a data type you can sum up without any issues whatsoever.
A small pedantic point: monads have kind `* -&gt; *`. Option has the same kind, but Result has kind `* -&gt; * -&gt; *`. In practice, one fixes the error type which is a partial application of sorts. Then you can define a monad over the ok type. I believe this is the reason why Either in Haskell uses "left" for the error type, since it works well for partial application.
&gt; This whole exercise of recreating it in the playpen helped me find the true underlying issue, so thank you! I do feel that this thread is a bit of overkill, though, now that I realize what the error is. Sorry about that. Great! I'm glad that this exercise helped, and you probably learned a bit more about floating point in the process, even if that wasn't the actual issue. But even more importantly, you learned an important debugging tool, simplifying the problem down to a small, independent subset, that is also really useful for asking questions online if you still can't figure it out yourself. So while some of the thread may be overkill, if you've learned both about floating point and a new way to debug problems, I'd say that's some time and effort well spent.
Wow this is pretty neat, thanks! your contribution was not missed
yeah not a bad idea, might do that in the future
Your name is stepping on the toes of Erlang's ecosystem: https://github.com/rebar/rebar/wiki - I'd suggest finding a new name. Perhaps "UpCrate"?
parsing programming languages with parser combinators can be a bit hard sometimes, moreover when indentation is meaningful. But I plan to add more parsing backends for that.
&gt; Also, storing IP address hash for IPv4 (which are the majority of client addresses currently) doesn't increase any privacy as it is trivial to reverse. That's true, but it doesn't have to be extermely secure. This does at least make me unable to (without a bit of work) see the IPs. Also note that it's modulo 15,000.
Well, the problem with this mantra is how "cost" is defined. Most of the time, it's "speed", but in this case, loss of precision is certainly another type of cost that requires developer interaction. It's a good goal, but it doesn't always work.
As always, comment here or on rust-users.
The most idiomatic way would be: let (i1, i2, i3) = (10u32, 20u32, 30u32); let mut output = String::new(); write!(output, "{} {} {}", i1, i2, i3); http://is.gd/diqSOZ
Ah. Alright. Looks like my problem must be more deeply rooted. I'm trying to get a handle on threads but trying to compile results in a "mismatched types" error. Here's my code: use std::thread; use std::fmt; fn main() { let mut children = vec![]; children.push(thread::spawn(move || {10*2})); children.push(thread::spawn(move || {20*2})); children.push(thread::spawn(move || {30+40})); let adder = thread::spawn(move || { let mut sum = 0; let mut result = ""; for child in children { let value = child.join().unwrap(); result = format!("{}", value); sum += value; } return result; }); let sum = adder.join().unwrap(); println!("{}", sum); } edit: This is work in progress. The ultimate goal is to get the adder thread to output a string containing "{} + {} + {} = {}"
After [graphics](https://github.com/tomaka/glium) and [sound](https://github.com/tomaka/rodio) (last week), I'm tackling [ui](https://github.com/tomaka/eui). Instead of designing the library alone, I'm writing my game UI while developing the library, so that I don't miss something important in the design. 
Your type problem is that `""` is a `&amp;str`, so you're defining `result` as a `&amp;str`, but then you're putting a `String` inside it (because that's what `format!` returns). Either initialise `result` with an owned string or make `result` a [Cow](http://doc.rust-lang.org/std/borrow/enum.Cow.html).
"" is of type &amp;str and format! returns value of type String; that's why you get mismatched types error, so to make your version to compile, you will have to replace: let mut result = ""; with let mut result = String::new(); To concatenate the results you will have to use something like: result = result + &amp;format!("{} + ", value); Here's version of your code that achieves your ultimate goal by using iterators: use std::thread; fn main() { let mut children = vec![]; children.push(thread::spawn(move || {10*2})); children.push(thread::spawn(move || {20*2})); children.push(thread::spawn(move || {30+40})); let adder = thread::spawn(move || { let mut sum = 0; let result = children.into_iter().map(|child| { let value = child.join().unwrap(); sum += value; format!("{}", value) }).collect::&lt;Vec&lt;_&gt;&gt;().join(" + "); return (result,sum); }); let (result,sum) = adder.join().unwrap(); println!("{} = {}", result, sum); }
In your case, result is first an `&amp;'static str` (a string slice), but format returns a `String` (dynamic string). If you replace the `= "";` by `= String::new();`, it should at least compile.
Very nice! Thank you so much.
Interesting viewpoint; the author doesn't talk much about Rust, instead offering his views on a few higher-level functional languages. Perhaps this is also why he misses the great thing about Rust: While GC takes care of the memory safety story, it moves things like iterator invalidation to runtime. Granted, this is less of a problem in pure functional programming (where your data is usually immutable), but in the name of performance, it's often necessary to compromise the pure immutable-data style. Also the pure style is often [more complex to write](http://prog21.dadgum.com/54.html). Rust offers a position between purity and practicality that gives us guarantees which are simply not possible in those other languages (barring rewriting borrowck to work in those other languages, as we are currently seeing by efforts of the C++ community). In programming (as opposed to politics), safety=freedom. If you know that something is safe, you'll feel free to do it. So the safety Rust gives us is freeing us from the fears that keep us from doing certain things, which IMHO is the unique thing about Rust that makes it so much fun. When I write Rust code, I feel like a great programmer, even though I've only recently graduated from being a complete newbie to someone who mostly knows his stuff.
Finally getting http://areweideyet.com/ a table version (the information is by now comparable enough to put it into a table). Also playing around with CardDAV... until now I just can list all addressbooks for a user, maybe there's more.
Can you help me think through the pros and cons? If I use `u32`, then values &gt; `2^32 - 1` on 64 bit systems cannot be used even though they are technically legal. If I use `u64`, then the worst case is that callers can express limitations that are never reached on 32 bit systems. What happens if the pointer size is 16 bits? I'm not sure if Rust supports any such platforms, but is it feasible that it could? (I seem to recall someone trying to do this not too long ago.) A rebuttal to the above is that if I use `u32`, then having a directory depth or a fd limit of more than `2^32 - 1` is exceedingly unlikely, and probably impossible. Which means `u32` would work just fine in practice. But even with all that said, what exactly are the downsides of using a `usize`? (Other than the fact that it would be unidiomatic in C.) I can't really think of any. Am I missing something? If I've got everything straight above, then in the absence of a compelling argument either way, it seems prudent to use types dictated by the underlying data structure.
The lifetime parameters in `bar` requires that the mutable borrow of `f` and its content has to be valid at the same time. This has the effect of borrowing `f` until it drops out of its scope. Removing `'a` from `&amp;'a mut`, to make `bar` look like this allows a shorter borrow: fn bar&lt;'a&gt;(f: &amp;mut Foo&lt;'a&gt;, z: &amp;'a u32) { f.x = z; }
Better might be Niko's more recent project (I think?) [lalrpop](https://github.com/nikomatsakis/lalrpop). I don't know if lalrpop means rusty-peg is deprecated or not.
I think that the most reasonable thing to do would be to leave `sum` as is, and add a trait somewhere that provides an `fsum` for doing floating point summation, like [Python does](https://docs.python.org/2/library/math.html#math.fsum). That way you could have naive but fast summation using `sum` as you expect, which works fine if all of the numbers are reasonably close in magnitude and you don't mind a little loss of precision, as well as explicitly opting in to a more stable summation when you need it.
Some people on HN were complaining, do you maybe have a browser extension or something that might be interfering?
Quote away, though as vks_ implies the human beings line is paraphrased from http://scattered-thoughts.net/blog/2015/06/04/three-months-of-rust/
It's because I use HTTPS Everywhere. HTTPS Everywhere detects that this domain offers an HTTPS version, so redirects me. But the scripts that render this are served via plain HTTP, and Firefox and Chrome both block unsecure content loaded from a secure domain by default. If I disable HTTPS Everywhere, or if I choose to allow insecure content, the JS loads and the page renders.
Last week I implemented a crude parser and repl for a simplified prolog (I got a start from [miniprolog](http://andrej.com/plzoo/html/miniprolog.html)). This week is all about improving the parser, the repl, and possibly adding new features. Beyond the standard prolog features, it would be nice to take advantage of rust's strengths in any of these directions: * eliminate copies during unification and increase sharing * play around with efficient encodings of the warren abstract machine * explore different search strategies * play with some of the prolog type systems from the literature
I look forward to trying out whatever libraries you come up with.
The main issue in regards to performance is how much of religion it is. Don't get me wrong it is surely important, but I never wrote a single program (since the mid-80's) where bounds checking would matter to the application use case. However thanks to the C culture, many equate performance with micro-benchmarking each code line they write. So, the biggest hurdle we have while trying to convince others to use safe systems programming languages is proving that they are fast enough for production code. When then when the implementation has problems, the language gets blamed, instead of understanding the amount of work each compiler backend has gotten through the years. I find very positive what the C++ community is now trying to do, but I wonder if it isn't a attempt to fight against Swift, Rust, Java getting AOT and value types, .NET Native, Julia and Chappel in HPC and so on.
&gt; A language without garbage collection, in 2015? Garbage collection is a kludge (in my opinion). My question is: *so* many languages *with* garbage collection, in 2015?!?
Nice to know. Yes, personally I see substructural types as the only alternative to GC. RC is still better than manual, but the amount of tricks to make it perform well are basically a mini-GC.
Well. The B5500 had an OS fully implemented in Algol 60 in 1961. Algol W had GC support. Xerox had a workstations done in GC enabled systems programming languages in the early 80's (Mesa/Cedar). ETHZ had workstations done in GC enabled systems programming languages in the mid-90's (Oberon/Native Oberon). So my question is, OSes still implemented in manual memory languages in 2015!? Please note I don't consider substructural typing manual memory management, rather compiler assisted management. 
I think I've never met a situation where having bounds checking off by default made sense. Even in a very performance intensive application, 99%+ of the code isn't going to be significantly affected by bounds checks.
Isn't any pointer essentially Option? It's eithera value, like Some, or null, like None. 
I doubt it, specially if the compiler does ellision.
I'm amazed at how closely aligned my opinions are to those of the author...but I don't have nearly as pessimistic of a view of manual memory management. In my view, both are necessary, they are just varying levels of explicitness about semantics. And in that regard, I much prefer the ideas behind lifetimes compared to the cognitive burden and abstraction leakage of inheritance based destructors. But yes, I want garbage collection too. Why can't both forms of memory management exist in the same language?
Some relevant doc links: [`channel()`](https://doc.rust-lang.org/std/sync/mpsc/fn.channel.html) which returns a ([`Sender`](https://doc.rust-lang.org/std/sync/mpsc/struct.Sender.html), [`Receiver`](https://doc.rust-lang.org/std/sync/mpsc/struct.Receiver.html)) pair.
The most efficient forms of garbage collection involve moving things around in memory, which is difficult to combine with pointers to memory locations.
Not to assign homework, but I'd be very interested to read a blog post describing some of the highlights of the benchmark changes!
I'm taking a crack at doing this on one of my toy rust programs. I'll report back if I get it to work. (It might take a while. I'm using an aws micro instance.) Edit: I didn't see the 1.5gb of ram requirement. I'll try anyway but I'm not expecting success. 
Oh great. Even if you don't have the time to explain them, maybe you could post the important pull-requests here?
&gt; Garbage collection is a kludge (in my opinion). Which is illustrated by the amount of time spent trying (desperately) to: 1. on the dev side, tune the garbage collectors (with heuristics) to try and get them to perform well in so many different work-loads 2. on the ops side, tune the options passed to the garbage collectors to try and get them to perform well for a given application 3. on the dev side, regularly implementing *manually managed* pools of objects to try and get them to perform well 
&gt; When I write Rust code, I feel like a great programmer, even though I've only recently graduated from being a complete newbie to someone who mostly knows his stuff. OMG so awesome.
I've been plugging away at my [regex-dfa](https://github.com/jneem/regex-dfa) library for the last few weeks, and it's almost at the stage when I can make an apples-to-apples comparison of its performance against the [regex](https://github.com/rust-lang-nursery/regex) crate. Hopefully I'll finish that this week, and then I'll know whether it's worthwhile to continue polishing up regex-dfa.
You'd need some way of disallowing pointers/references into the GC'd section by any code that isn't part of the GC system, in order to make it safe. If you can manage that, then there's nothing stopping you from having a generational garbage collector running alongside a traditional allocator, if you don't mind the wasted space.
I used to do C++ for a living so I've got quite an affinity for RAII. Still not sure what's so great about GC sometimes!
Sadly UNIX killed them all given the free availability of UNIX System V source code and cheaper hardware costs, given that graphical workstations were way more expensive than PDP-11 like hardware. I used Native Oberon for a while back in the 90's and it was quite nice. Its GUI was the inspiration for Plan9's ACME editor. The best we have now, is IBM, Apple, Google and Microsoft going full automatic memory management for userspace applications in their OSes. 
&gt; I often have the impression that for many people there is just garbage collection and the primitive and error prone manual memory management. Maybe there should be some sort of ... PSA that informs people that there's more than just the above.
Wow. Not sure what this is actually representative of (for instance, Rust users might be more inclined to use Github, and out of everyone on Github Rust users might be more inclined to star things) but it seems reasonable to assume that Rust is rising, fast. Awesome!
EVERYONE in the Ruby community uses Github though. But they apparently are less inclined to star the project itself for some reason.
&gt; Yes, personally I see substructural types as the only alternative to GC. Would you mind expanding a bit on this? Edit: Interesting! https://en.wikipedia.org/wiki/Substructural_type_system
The Ruby repository on Github is just a mirror of the real one.
&gt; Shouldn't the point of a language that sells zero cost abstractions ... while still being expressive and high level? Not sure what you are asking. Expressiveness isn't the primary goal, features that facilitate it are only considered when they don't add abstention overhead. Rust wants to be C++ that is safe, it will convenience its users only when there isn't any cost associated. For sum both implementations are valid for some use cases, so if at all sum should be present in std then it should be the one that aligns more with goals of the language. 
Yes, that is what I was referring to. Rust's approach is just one variant of several possibilities, all part of what is known as substructural types.
I completed the initial work on [gfx_mesh](https://github.com/csherratt/gfx_mesh) and [hairball](https://github.com/whiske-rs/hairball). I am now looking at building an importer so I can convert from `obj` files into a hairball and then integrating `hairball` into [whiske-rs](https://github.com/whiske-rs/whiske).
My guess is because Rust is actively developed *on* Github, though pull requests, issues, RFCs, etc. Ruby's repo, in contrast, is just a mirror of the real repo, and collaboration on development is done through a combination of mailing lists and an external bug tracker.
It's been decades of non-GC kernels winning against GC-ones. It's not a bad luck of the industry, that's for sure. GC saves developer time at the cost of predictability, performance and control. The lower down the stack you go, the less you can tolerate these drawback, and more time you can spend to get manual memory management right. IMO. Manual m.m. is necessary in big chunks of software ecosystem and Rust tremendously helps reasoning about manual m.m. and avoiding bugs associated with it. And it does is it so well, that it can shift the line of where it does not pay off any more to do manual m.m. BTW. AFAIK one for the reasons why iOS delivered much smoother UX, than Android was GC-lags. And GC was removed from OSX and never existsed on iOS. So Apple does not belong to this list, IMO.
[removed]
The 128 and 256 bit registers can only be used for simd operations for floats (though some of the instructions operate on only one element of the vector register). There are integer instructions that operate on the entire register, notably 'and', 'andnot' ((~a) &amp; b), 'or', logical shifts, 'xor'.
I fundamentally don't agree with his premise: &gt; A language without garbage collection, in 2015? The idea seems to be that very few applications need predictable performance, which seems wrong to me. To be sure, there are a lot of applications where 10s or 100s of ms of random delay (or huge overhead for concurrent GC) is acceptable, but at the same time there are more performance sensitive apps than *ever before* because: 1. We have plenty of client-side apps that use fancy animations. Customers demand that these run at 60Hz without stutters. (or at least 30Hz, but even that is pretty much a no-no on anything but gaming consoles - e.g. phones are 60Hz or bust) 1. Many of these apps run on underpowered hardware (phones, tablets, TVs). 1. Many of these apps also run on battery powered hardware, so occasionally trawling through all your memory is a big deal for power consumption. 1. The electronic entertainment industry is bigger than ever (console/pc/mobile gaming). 1. Heaps are growing bigger, and thus pause times are as well. 1. Pause times are not getting that much faster as CPUs are getting faster (since it's mostly about memory latency, which improves very slowly). Any time you have a deadline of 16ms to produce an updated frame, a GC is if not unacceptable then a huge hindrance. This is exactly the kind of things where a modern, high level, language with no or limited need for a GC is what you need (optional GC for some things seems fine, especially if it's isolated to a separate thread and doesn't stop the world).
The "servo on windows" image is a broken link :/
Whoa, since when has Servo worked on Windows? That's news to me!
Since like last week. Not yet merged to master though.
I completely disagree with that. An unboxed lambda should be a completely distinct entity in memory, but a generic lambda would have to have many copies in the code segment of the binary. One per each instantiation of T. let l = &lt;T: Display&gt; |T| { /* code */ }; Then l is an opaque type and &amp;l is meaningless. Perhaps I am missing something here? I don't know what c++ does in this regard but I assume it is something close to magic. To anyone wanting a generic lambda, I would say use a trait object or write a generic fn (which can be written at every scope). Variadic functions are divisive, that's all I'll say about that. :)
I was asked to write a blog post about our recent improvements, care to join in?
It's a "regular" UI library, not immediate mode. It's a bit inspired by React. You create your own custom widget types, and then there are two hierarchies of widgets: the "public" hierarchy that contains the high-level state of each widget, and the internal hierarchy that contains the positions, rotations, scale, etc. of each element. The `Widget` trait defined by the library has a `build_layout` method that must return the list of children (through `Arc`s) and how they should be arranged. Then the library handles the rest. Since it's changing a lot, I'm not updating the examples for the moment. But it is very simple to add/remove elements (the other elements will automatically adjust) and to add animations to the widgets. 
[Something to do with lifetimes.](https://github.com/rust-lang/rust/issues/20871)
/r/playrust
This is awesome. I'd love to listen to Herb's thoughts on Rust's lifetime management, and how it relates to C++.
WOW i added it to the wrong section ? lame lol thanks kinghajj
I know, right? I'm still not sure if posting here was the right bat-signal for the language design team. I hope they will read Sutter's comment.
An `f64` can measure the distance to the moon to the nearest 10 **nano**meters. Perhaps that puts into perspective when it makes sense to start asking for more precision!
This is 100% what makes me interested in Rust. There is simply no viable alternative to C or C++ yet.
&gt; Is this idiomatic to have results within results? Not particularly. It would be better to combine the set of possible errors into one error type. e.g., enum Error { APIError(...), AsyncError(...), } You can use `From` impls to make this ergonomic with `try!`. See the [Error Handling](https://doc.rust-lang.org/nightly/book/error-handling.html) guide.
FYI, I sent mail to Herb this afternoon; we'd love to take him up on this!
&gt; And yes, Rust clearly shows that high performance code doesn't need to be unsafe. The entire `regex` crate doesn't use any unsafe code at all. :-) (I'm not being a total weasel either. It's true that it uses `memchr`, which requires `unsafe` for ffi. But none of the core matching code uses `unsafe`.)
Yes, and we will reach out. Thanks!
I also doubt it. I wrote `regex` (which tops the benchmark game as of now), and I've tried to sprinkle `unsafe` code that elides bounds checking in several hot loops. I've never been able to observe a noticeable performance improvement. Of course, this doesn't prove you wrong or anything (and I could be doing something wrong!), but I am skeptical until there's some solid evidence otherwise.
&gt; There are use cases where you need that, and a replacement for C++ in its remaining niches is a worthy goal Perhaps I'm misunderstanding the point of the article, but for what it's worth, this "worthy goal" is, in fact, the goal. Put something memory safe (including when dealing with threads) in the C++ performance niche.
Yay quickcheck! I so wish I could find the cycles to write a blog post on it. It has taken the [Rust testing ecosystem by storm!](https://crates.io/crates/quickcheck/reverse_dependencies) I think it could do even better with some well written guides. I've found quickcheck [easy to use](https://github.com/BurntSushi/suffix/blob/master/tests/tests.rs#L197-L204) when testing [data structures](https://github.com/BurntSushi/rust-memchr/blob/master/src/lib.rs#L306-L311) with [simple invariants](https://github.com/BurntSushi/rust-cbor/blob/master/tests/tests.rs#L66-L76). But it [can](https://github.com/rust-lang-nursery/regex/blob/master/regex-syntax/src/properties.rs) quickly [get](https://github.com/BurntSushi/walkdir/blob/master/src/tests.rs#L149-L218) hairy with [more complex](https://github.com/BurntSushi/xsv/blob/master/tests/tests.rs#L147-L167) invariants. There's a pretty big chasm there, and to use quickcheck effectively as broadly as possible, it really does require figuring out how to write good shrinkers. Writing a guide is tricky because what makes a "good shrinker" is usually dependent on your data structure. (In my experience, this is also true in Haskell's QuickCheck.) In any case, I'm really pleased that so many folks are using it. It's an invaluable tool for testing and has caught so many bugs in my code.
/u/glennw has been working on a completely new rasterizer (to replace skia), that is tailored specifically to web content, and runs *on the GPU* (existing browsers do their drawing on the CPU).
All of my favorite languages have problems: Java: JVM (non-native), overly verbose C: Unsafe, inconsistent C++: Unsafe, Hard to debug, so much baggage Python: Slow :'( Ruby: gets puts chomp slurp. Slow. Basically Python but ugly. Haskell: Slow unless you're writing "haskell $ for! $ \`seq\` pros" Groovy: JVM, not Python, but wishes it was Scala: JVM, and so very counter-intuitive ... Rust: Native, fast, intuitive, fun. Unfortunately, also not actually Python. :(
You can build Python packages with Rust, so you can get the best of both worlds.
The actual problem is that the borrow corresponding to `f2` is capable of escaping into itself in the body of `bar`, and the borrow checker cannot verify that this doesn't happen across function boundaries. The reason is that the type of the value passed as `f` to `bar` is capable of containing references with its own lifetime. This means that you could borrow one part of `f` and store that borrow somewhere else in `f`. Since `bar` takes a mutable borrow, it is fully capable of doing this, so the borrow checker has to assume it *has* happened in order to ensure correctness, and this means extending the borrow for the lifetime of the value. [Here]'s an example showing a slight modification to the code which demonstrates that `bar` can legally make the borrow escape into itself. The solution is just to not use the same lifetime for a mutable reference and its contents. PS – I like to call mutable references like this *ouroboros* references because they eat themselves. [Here]: https://play.rust-lang.org/?gist=86377d942e65a73261eb&amp;amp;version=stable
Since this is replacing Skia, would it be possible to eventually replace Skia under Firefox OS with this? Or is it not targetting the same use-case. Also, does the GPU rendering require any extensions that limit what version and how old the GPU drivers can be? I'd like to still be able to run Servo on my laptop, for example, even though its drivers are old as the sun &gt;.&gt; Along with use-cases like old OpenGLES on phones, especially budget ones like FirefoxOS would be running on.
The +/- feature had me completely lost. Either way I was gonna start working on the avx2 decimal functions into a usable crate.
I guess links to the code + discussion f techniques used. I'll see if I can whip up a draft.
Yeah, I can do that. Kind'a makes me wish I version-controlled it all, so I could talk more about what *didn't* work.
I should make clear that the syntaxes given here are, of course, independent from the semantic choices (though I think `@` works a bit better for the binary use case). The main question I'm interested in is the overall path to take here, and I feel confident we can find an appealing syntax.
Would the elision proposal that you favor here require inspecting function bodies?
Due to references and lifetimes, closures are open relations between types. The core reason is the output couldn't depend on the arguments without them being parameters. Parameters to traits are sometimes called "input types" and associated types are sometimes called "output types", which hints that they should be used for the corresponding positions in functions (arguments are "inputs", the return is an "output"). Basically, without doing this, one couldn't have a closure that took an arbitrary reference, and so couldn't create functions that manipulated them, e.g. consider: fn foo&lt;F: FnOnce(&amp;[u8])&gt;(f: F) { let bytes = &amp;[1, 2, 3]; f(bytes) } foo(|x| println!("{}", x.len())); The key points here is I can pass a reference that the closure definition knows nothing about: the lifetime of that reference can be shorter than the lifetime of the closure itself. This desugars to `F: for&lt;'a&gt; Fn&lt;(&amp;'a [u8],), Output = ()&gt;`, and any closure passed in will have a compiler-generated definition like: struct Closure(...); impl&lt;'a&gt; FnOnce&lt;(&amp;'a [u8],)&gt; for Closure { type Output = (); ... } If the trait had both inputs and outputs being associated, then it would have to look like: struct Closure&lt;'a&gt;(...); impl&lt;'a&gt; FnOnce for Closure&lt;'a&gt; { type Input = (&amp;'a [u8],); type Output = (); ... } This forces the argument to the closure to *outlive* the closure object itself and also forces the lifetime to be decided at the definition site of the closure, meaning the code above doesn't work, only something like: let bytes = &amp;[1, 2, 3]; let f = |x| println!("{}", x.len()); f(bytes); This is essentially exactly the same problem that you're meeting with full types, and the Rust solution (being higher-ranked over lifetimes) can be applied to types too, ala C++ generic closures. --- Also, it's totally possible to make a closure that implements the traits generically, e.g. struct Duplicate; impl&lt;T: Clone&gt; FnOnce&lt;(T,)&gt; for Duplicate { type Output = (T, T); fn call_once(self, (t,): (T,)) -&gt; (T, T) { (t.clone(), t) } } (One can think of generic function items acting like this, e.g. `fn duplicate&lt;T: Clone&gt;(t: T) -&gt; (T, T) { (t.clone(), t) }` make writing `duplicate` by itself fairly equivalent to the `Duplicate` struct.)
Yeah, but what did work is also good stuff. ;-) Draft is [here](https://github.com/llogiq/llogiq.github.io/blob/master/_drafts/fast.md).
Perhaps an alternative syntax could look something like: fn foo&lt;I&gt;(i: I)&lt;O&gt; -&gt; O where I: Iterator, O: Iterator, { ... } Where generic parameters declared on the left hand side of the function arguments are input generics given by the caller of the function, and those on the right hand side are anonymised and inferred by the function body? Just trying to think of alternatives to `impl Trait` as it seems to be a little controversial (though I don't particularly mind it). edit: I should mention I'm ridiculously excited about this feature and really appreciate the blog post!
I should take the time to read why exactly I want quick check. It seems like the httparse crate is a perfect instance to use it? 
No, replacing Skia is explicitly out of scope (except for rendering the results of CSS layout, of course). In fact, the whole idea behind the project is to operate on a retained mode, CSS-specific scene graph rather than the immediate mode API that SVG and canvas (particularly canvas) demand. We don't have any plans to replace Skia for canvas; Google has done some nice work with Skia-GL and Microsoft has done a fantastic job with Direct2D to make that sort of API fast.
Cross post from the RFC: I'd prefer the following over the arrow syntax: trait IterAdapter: Iterator where Self: Clone if Self::Inner: Clone, Self: DoubleEndedIterator if Self::Inner: DoubleEndedIterator { type Inner: Iterator; } To keep APIs sane, I wouldn't even allow the inline version. Also, this alone probably deserves its own RFC (it seems like it would be useful by itself). -- edit: unfortunately, this doesn't allow more complex constraints.
Holy shit, this is real! Servo devs continue to completely destroy at browser dev. inb4 Firefox is deprecated
Veedrac has offered to help me out, and I already started writing (at 3AM, no less!) so there *will* be a blog post. Meanwhile you can look at the recent PRs in teXitoi's [benchmarksgame-rs](https://github.com/teXitoi/benchmarksgame-rs/pulls) repository.
One way to do this is via environment variables - you access them with `env!("DATA_DIR")` syntax. It's resolved to a string at compile time. [Docs](http://static.rust-lang.org/doc/master/std/macro.env!.html)
&gt; Since this is replacing Skia, would it be possible to eventually replace Skia under Firefox OS with this? Or is it not targetting the same use-case. It can't fully replace Skia, as I mentioned in the reply to the sibling comment. However, it could be a great complement to Skia for accelerating CSS painting. &gt; Also, does the GPU rendering require any extensions that limit what version and how old the GPU drivers can be? It uses OpenGL ES 3.0, but that isn't essential and we could easily add an OpenGL ES 2.1 fallback.
But what about spmc channels? In my opinion it's a pretty common situation when one thread creates tasks and multiple workers do the work.
Finally managed to make [`Widget`s efficiently composable from other `Widget`s without consuming the public `WidgetId` space](https://github.com/PistonDevelopers/conrod/pull/565) in conrod (which has been quite the feat considering the immediate mode nature of the lib)! Next will be: - vastly simplify the existing conrod widgets by building them from other widgets (using the changes in the above PR). - work on a timeline widget (in a separate crate) for control over playback of various kinds of media. Hopefully you'll be able to instantiate the timeline with whatever tracks you need (ie. video, audio, midi, automation). Planned on writing more, but realistically that's easily enough to last me the week :)
I didn't quickly find one on crates.io, but there are several that offer mpmc, which can be used to accomplish your task. (spmc would just have better performance on the sending side). 
Jesus, don't even joke about that.
Is there a link to the talk or slides or some thing?
You could make it more composable, but that would also require a lot of extra traits trait CloneAdapter: T where Self: Clone if T: Clone {} trait DoubleEndedIteratorAdapter&lt;T&gt; where Self: DoubleEndedIterator if T: DoubleEndedIterator {} trait IterAdapter&lt;T,I&gt;: Iterator&lt;Item=T&gt;+CloneAdapter&lt;I&gt;+DoubleEndedIteratorAdapter&lt;I&gt; {} EDIT: Still trying to fix the syntax
Unintuitive, but consistent and logical. There is a very comfy plateau at the top of the learning curve.
How is Scala counter-intuitive? Also, what's wrong with the JVM?
* Java: I see the JVM more as a strength than a liability. Then again I'm writing Java code for a living for more than a decade. The engineering that went into the JVM is beyond insane * C/C++ Full ack * Python can be fast if you use one of the faster JIT implementations * Ruby IMHO took the worst features from Perl and Python and still managed to become a palatable language. Chomp slurp. * Haskell can be fast, but only if you show you're a PhD :-P * Groovy, Scala: Nice niche languages in their own right * Rust: Fortunately not actually Python. :-)
Apart from the error messages, lifetimes are actually quite easy. I have yet to read up on De Brujn typestates, but I still write Rust code that works.
(That sentence is just throwing together random jargon in a meaningless way.)
Specifically: https://www.reddit.com/r/rust/comments/3mrq32/preliminary_results_for_servos_new_gpubased/cvhnk2h
Last I checked, the only way to call Rust from Python was to compile the Rust code into a `.dll` using Rust's C Foreign Function Interface, and then call the funtions in the `.dll` with Python's `ctypes` library. The ball would be in Python's court to make a `rusttypes` library. However, this was a while ago and I only looked into it when I was relatively new here. The situation may have completely changed.
&gt; I'm not sure you're actually looking for coroutines, and instead looking for synchronization through communication, which the above channel can do. My first multi-threaded program was with Go so I have no idea outside of go routines + channels / workers how all that works. It was so easy in Go that I hoped something similar would exist in Rust. I would like to recreate my multi threaded web crawler with something like the channel/worker combination of Go if this exists in Rust, having 30-300 workers (variable size as parameter) that consume a link from a channel, calls a function with it to get all the links + test the links for illegal characters, same host etc and then put it into another channel for a single worker to do stuff with it. I'll look into the Rust book if there is something explained but if you know the answer to something I wrote I would be very happy to read more from you.
Just chiming in to say that I hate sigils in programming languages with a burning passion driven by nothing in particular.
Go routines are faster to create. Each new thread need to make syscall since green threads doesn't. It is only that. You can think about green threads as a user space threads.
Thank you for all these replies, I started to play with `env!`, it does fit my needs. I'll probably have to wrap cargo in a Makefile to set the environment for distribution builds, and use a sane default for development.
I’ve been thinking about this on and off since your reply, I thought I'd have a go at answering some of the high-level design Qs you raised in your post! &gt; Can it appear anywhere a type can? I imagine it could appear anywhere a solid type can, perhaps accessed as though it's a type associated with the function or method. i.e. say we had the following function fn foo&lt;Input&gt;(i: Input)&lt;Output&gt; -&gt; Output where Input: Trait, Output: Trait, { ... } `foo::Output` would act as a type alias to the (often lengthy and sometimes un-writable) solid type `Output` that is inferred via `foo`'s body. &gt; struct definitions? A struct definition with a field of `foo`'s `Output` type might look like this: struct Bar { foo_output: foo::Output, } I imagine it would also be possible to store a `foo::Output` in a struct’s generic field (as long as it satisfies the bounds): struct Bar&lt;T: Trait&gt; { t: T, } let bar = Bar { t: foo(i) }; … fn baz(bar: Bar&lt;foo::Output&gt;) { … } baz(bar); &gt; type aliases? Again, something like: type MaybeFooOutput = Option&lt;foo::Output&gt;; &gt; arguments? fn bar(foo_output: foo::Output) { … } However, I think we’d have to restrict a function from using it’s own “anonymised output type” anywhere in its own arguments (or argument types) to avoid inference getting stuck in a cyclical graph - i.e. this doesn’t make any sense: fn foo(output: foo::Output)&lt;Output&gt; -&gt; Output { output } &gt; must be possible to return an unboxed closure and store it in a struct &gt; must be possible to return a compound iterator without giving the type explicitly &gt; must cope with multiple such types appearing as components of a return type (e.g., returning a pair of different unboxed closures) &gt; must be able to assert that at least some traits are satisfied &gt; minimal signature verbosity &gt; simple semantics/explanation of the feature, especially if it looks like a type &gt; type abstraction &gt; more ergonomic newtypes &gt; applicable to struct definitions, not just function signatures I *think* these would be satisfied? &gt; must be able to deal with conditional trait implementations I’m not sure I understand this fully, but seeing as we’d be able to use `Output` in a `where` clause, we should be able to describe the bounds more expressively? Here’s /u/dbaupp’s example from another comment rewritten: fn foo&lt;I&gt;(it: I)&lt;O&gt; -&gt; O where I: Iterator&lt;Item = u8&gt;, O: Iterator&lt;Item = u8&gt;, O: DoubleEndedIterator if I: DoubleEndedIterator, { it.map(|x| x + 1) } &gt; compatible with adding new OIBITs Haven’t quite wrapped my head around this issue from the blog post yet, so I’m not sure if this is satisfied or not (maybe I just need to see an example). I’m probably hugely overlooking some major issues here in my naivety! Just thought I’d have a crack as I’m interested in seeing the `fn foo&lt;I&gt;(i: I)&lt;O&gt; -&gt; O` design discussed a little further and I thought I might learn something along the way :) edit: newbie reddit formatting
You can use the "cffi" library from the pypy guys if you want something to automatically wrap up rust's C interface.
- strong type enumerations - bounds checking (can be disabled if you are really really sure) - modules - reference parameters instead of pointers that can point to anywhere - no implicit numeric casts - casting between disparate types and pointer tricks requires the SYSTEM module - no decay of arrays into pointers - real strings - exceptions Yes you still need to manage memory manually, but there is a whole class of errors that would be gone.
Whilst awesome, I'm always reminded of the quote that one of the 2 hardest things to get right in computer science is cache invalidation.
When do normal threads offer a significant disadvantage over go routines? Like when I need to spawn 10.000 threads?
I suggested something similar to /u/aturon, I believe, I like `if` for this.
Even without C++ it's not too bad. I had solid experience with C, Haskell and Java and that got me 95% of the way there.
I guess most of the browser slowness is due to the huge amount of features implemented – I guess Firefox would also perform faster if it wouldn't have to implement all of the HTML spec.
&gt; I think Go has more users overall, which is developed on Github as well Go is not developed on github. It used to be developed on google code, the canonical repository [is on googlesource](https://go.googlesource.com/go) and contributions [use gerrit](https://go-review.googlesource.com). Only bug reporting is on github.
Good to know, but the original point stands. You have to use a C interface to talk to Python.
* you can use ctypes, that's generally not recommended * you can use cffi, that's more recommended and safer than ctypes * you can build native modules for cpython using the C API
I guess you weren't in Apple forums back then. Or had the pleasure to use third party libraries whose developers didn't take care of all the possible issues that Apple had on its documentation, which did indeed lead to random crashes. Specially if they weren't compiled in GC aware mode. Apple's solution made sense given Objective-C semantics, as they basically built the Cocoa retain/release patterns into the compiler. Just like Microsoft did with Visual C++/VB support for COM. If the solution is so technically great, why haven't they implemented ARC besides Objective-C classes that follow such pattern? The answer is quite simple, C semantics, hence Swift for having a language with proper automatic memory management where one can apply ARC to all heap allocated data. Ah, but ARC is great otherwise why didn't Swift use GC instead? Again simple answer, if Swift used a GC and Objective-C ARC then Swift wouldn't be able to inter-operate seemingly with the Objective-C runtime. And a solution similar to the Runtime Callable Wrapper in .NET would be needed, thus adding an indirection layer and slowing done Swift &lt;-&gt; Objective-C calls. So if Objective-C GC had worked properly, Apple would never had needed to resort to ARC instead.
Not from the Boston meetup but he did say that it was his slides from his talk at Rust camp which are up online believe. He just expanded upon it at the talk.
Hmm, you can definitely downcast to trait objects though. I do it in quickcheck in a half-hearted attempt to show some useful output when a test panics: https://github.com/BurntSushi/quickcheck/blob/master/src/tester.rs#L417-L420 [EDIT] Looks like the above is mostly wrong or misleading at best. I should have known better, because I see "UNABLE TO SHOW RESULT OF PANIC" a lot. :-(
Can you elaborate on the value proposition? I'm assuming this proposal would allow someone calling `foo() -&gt; ~Iterator` to talk about `foo::Output`.
I don't think the Servo devs have had any indication that their speed is due to lack of functionality. They add functionality all the time, and I don't think they've been experiencing many notable regressions. Firefox is a codebase that dates back to before multi-core processors were really a thing to take seriously, and is written in a language that's even older. Servo is a tabula-rasa project that gets to architect itself to optimize for modern hardware using a language designed for that task. 
I swear I tried that a couple dozen times... However, [my test case](https://play.rust-lang.org/?gist=bc25750d87ad44a81446&amp;version=stable) doesn't seem to work...
I'm really starting to like mitchmindtree's [proposal](https://www.reddit.com/r/rust/comments/3mrsvx/resurrecting_impl_trait_aaron_turon/cvhvz6n) because it works by leveraging most of the existing semantic around generics: - The difference between input and output type parameters is reused: `fn foo&lt;A&gt;()&lt;B&gt; -&gt; B {}` is analogue to `trait Foo&lt;A&gt; { type B; fn foo() -&gt; Self::B; }` - The existing generic declaration syntax is reused. - Specifying the bounds as `where` clauses is reused, with the addition of the `if` modifiers - The ability to talk about the return type as a simple associated type alias allows normal composition with arbitrary type positions. `let x: foo::B = foo();` - It offers the same possibilities for future extensions as input type parameters to a function, eg HKT: `fn foo&lt;T&gt;(t: T)&lt;M&lt;*&gt;&gt; -&gt; M&lt;T&gt; where M: Monad {}` Regarding OIBITs, this should work like existent generics: They are not there without bounds declaring their existence. To tackle the resulting verbosity and trait-object-like pain of missing bounds, we'd need to combine this with the ability to succinctly define conditional trait aliases, like outlines in aturons's second proposal: trait IteratorAdaptor&lt;I&gt; where Self: Iterator if I: Iterator Self: Send if I: Send Self: Sync if I: Sync trait DoubleEndedIteratorAdaptor&lt;I&gt;: IteratorAdaptor&lt;I&gt; where Self: DoubleEndedIterator if I: DoubleEndedIterator fn map(self)&lt;J&gt; -&gt; J where J: DoubleEndedIteratorAdaptor&lt;Self&gt; { ... } This would give a number of benefits: - Succinct type signatures because there would be common trait aliases in the std lib for iterators, functions, etc. - Named trait aliases can be documented. - Named trait aliases can be reused. - Defining conditional bounds naturally enables type checking and enforcing them. Example for the last point: struct SingleEnded&lt;I&gt;(I); impl&lt;I&gt; Iterator for SingleEnded&lt;I&gt; { ... } // No DoubleEndedIteratorImpl fn foo&lt;I&gt;(i: I)&lt;J&gt; -&gt; J where J: DoubleEndedIteratorAdaptor&lt;I&gt; { SingleEnded(i) // ERROR: SingleEnded&lt;I&gt; does not implement DoubleEndedIterator if I does. } If this seems like a good general mechanism, but too verbose for common usecases, there could also be additional optional short forms (all meaning exactly the same thing): fn foo()&lt;T&gt; -&gt; T where T: Bar { ... } // full "where" form =&gt; fn foo()&lt;T: Bar&gt; -&gt; T { ... } // shorter "classic" form =&gt; fn foo() -&gt; type T: Bar { ... } // short form for just one output type.
I'm not sure if that actually does anything. [This reduced example](https://play.rust-lang.org/?gist=519e3b0787fd806e78c0&amp;version=stable) *should* print "5" (u32 impls `Debug`), but instead prints "UNABLE TO SHOW RESULT OF PANIC."
*And* Servo gets to have a layout engine designed for modern CSS patterns.
I especially like the proposed `@Trait` solution as this seems cleaner than just `_` or `impl Trait`.
I should make another stab at using it Ramp. When I first tried it just span there but I think I had some memory corruption issues at that point. 
`[]` could implicitly unwrap the result of `.index` in the same way it already implicitly derefs the `&amp;T` returned by `index`. The user can call `.index` directly if they want the complete semantic information, or `[]` if they just want the value and assume it exists.
I wish there was an overview of all OS developments in Rust, including comparisons, screenshots, and statistics. Perhaps, "areweosyet.com"? ;) I'd like to understand where we are, without actually digging up the code and learning the field just yet.
Okay, so, I haven't watched this yet, but I've talked a bunch with /u/wycats, and this is a sorta-kinda-TLDR: Rails has a library, `ActiveSupport`, which adds methods to Ruby core classes. One of those methods is `String#blank?`, which returns a boolean (sometimes I miss this convention in Rust, the `?`) if the whole string is whitespace or not. It looks like this: https://github.com/rails/rails/blob/b3eac823006eb6a346f88793aabef28a6d4f928c/activesupport/lib/active_support/core_ext/object/blank.rb#L99-L117 It's pretty slow. So Discourse (which you may know from {users,internals}.rust-lang.org) uses the [`fast_blank`](https://rubygems.org/gems/fast_blank) gem, which provides this method via a C implementation instead. It looks like this: https://github.com/SamSaffron/fast_blank/blob/master/ext/fast_blank/fast_blank.c For fun, Yehuda tried to re-write `fast_blank` in Rust. Which looks like this: extern crate libc; mod buf; // a small buffer struct + impl, not shown use buf::Buf; #[no_mangle] pub extern "C" fn tr_str_is_blank(b: Buf) -&gt; bool { let s = b.as_slice().unwrap(); s.chars().all(|c| c.is_whitespace()) } Turns out, this implementation ends up being faster than that C one, while also being significantly more straightforward. This video is a two-hour dive into why that is.
Thanks, that summary is spot on
What accounts for that 10%?
[This blog post](http://aturon.github.io/blog/2015/09/18/reuse/) might be interesting, it's about taking specialization to the next level in future versions of Rust. I'm not totally sure what it means for dynamic `Any`-style typing, but it certainly concerns up- and downcasting.
 - Ensuring that the OpenGL context is current, with `wglGetCurrentContext`/`wglMakeCurrent`. This is purely safety-related to avoid problems if you use multiple OpenGL wrappers simultaneously, and can be disabled at runtime with an `unsafe` function call. - Avoiding redundant state changes by comparing the required state with the current state. Glium even caches the uniform values of each program to avoid calling `glUniform` when not necessary. This is the second highest performance culprit of the library. However these checks should pay for themselves. Removing them would probably lead to more OpenGL function calls and worse performances. - Checking the OpenGL version and extensions to dispatch between various paths. This is required to ensure compatibility between all versions, and to use more optimized versions of some functions when available. These checks should have 0% branch misprediction though. - Loading uniforms locations and attribute locations from hashmaps instead of hard-coding them. This could be removed with plugins. - Checking the types of the attributes and uniforms. Especially uniform buffers/SSBOs are costly because it checks the whole struct. Again this could be removed with plugins. - Storing the VAOs, FBOs and samplers in hashmaps. This is mostly usability-related (to make glium easier to use), and a bit safety-related (because of contexts rebuilds, if you want to change the characteristics of your context). Loading the VAO from the hashmap is currently the highest performance eater (probably around half of the 10 percents). I have opened an issue dedicated to this one. This is definitely one of the points that should be improved. 
No, the hardest 2 things is naming things, cache invalidation and off-by-one errors.
I meant that I don't think Servo will ever switch to glium or any other OpenGL wrapper.
Oh and there's also the problem of the textures and buffers bind points, which is another overhead in addition to these 10%. 90% of the time there's a clean winner when you have to choose between two different ways of doing the same thing. For example `glInvalidateBuffer` is better than `glBufferData(..., null)`. However the best way to bind textures, uniform buffers and SSBOs to the various bind points depends on what you are going to do next. Glium can't predict the future, so it may do something suboptimal compared to manual OpenGL code. This is also something that should be improved by giving more control to the programmer. 
Or couldn't we use fn foo&lt;I; O&gt;(i: I) -&gt; O where ... This way, all generics arguments are at the same place, their role being separated by a semicolon. This syntax might feel strange whenever you simply want to return an unboxed closure: fn make_adder&lt;; F&gt;(x: i32) -&gt; F where F: Fn(i32) -&gt; i32 { |y| x + y } However, it seems to me that a semicolon is much less clutter than a list of types on the right side. I feel like having to search for generics in two locations might be unwieldy. Not sure if it changes much in practice, though.
I kind of agree that introducing generics on the other side of the parens might be a little drastic. I don't mind both your suggestion and /u/diwic's. At the same time I also like (in the original) that the generics that are inferred by the function body are more clearly separated from those given by the caller, and that the inferred "output" generics are on the "output" side of the function's args.
Well great you've sold me into watching this, after Strangeloop I really didn't need *more* hours-long videos to watch.
I know, right? When am I gonna get any _work_ done? (don't read this pls /u/nikomatsakis /u/aturon 😅😅😅)
Depends on your interpretation of sisyphus' ship.
I think the use case described can, in many cases, be better captured with other patterns, such as having `StableThing` trait with an `as_animal(&amp;mut self) -&gt; Option&lt;&amp;mut Animal&gt;` function: http://is.gd/FXcpng There are cases where this won't be good enough and it'd be nice if they can be addressed in the future, but there are ways to use what's currently available in an ergonomic way to handle situations where fully dynamic typing is used in other languages.
Preliminary question: how are you running this? If you're not using `cargo run --release` (or at least building with optimizations), it's going to be very slow indeed.
why do you need to separate them with ; at all? why can't we use? fn foo&lt;I, O&gt;(i: I) -&gt; O where ...
/u/tomaka17 Is there a sort of comparison for glium and gfx-rs? In terms of openGL and what glium sets out to do. I've used glium in the past and liked it, but am working in a project now that was already set up with gfx-rs and wonder if it's worthwhile to switch it over or not. Also, I don't understand why servo doesn't use glium, I actually thought it did! Perhaps that was glutin I was thinking of
Why is the syntax: fn produce_iter_static() -&gt; Iterator&lt;u8&gt; Not valid? That seems to be part of the original RFC explanation but the proposal adds an "impl"?? Is that because a Trait is not considered a type and therefore you don't want to use the name directly?
&gt; Is it really so unacceptable to have to fall back to explicit structs and impls when you need these kind of conditional bounds? The problem is that, in general, you can't even write the types down (for unboxed closures), or you can but really don't want to (massive iterator types). So there has to be some way to talk about them abstractly in a signature. Now, I don't propose cramming all of the bounds directly into the signature; I think we'd need trait aliases (desired for other reasons) or something like that to keep things concise, as I mention in passing in the post.
Why not? What with glium does not fit the use case of Servo? 
Also: in the Python version, you're iterating over the lines of the file and processing each one. This allows you to use a "small" buffer Python provides and get to work immediately. In the Rust version, you're reading the entire file into an empty String. This requires that String to resize its buffer internally several times to get to the point where it's large enough to hold 4MB. Each time it resizes, it has to move the data from the old buffer to the new, larger one. Once all of that's complete, it can finally get to work. You should use something like [`BufRead::lines`](https://doc.rust-lang.org/std/io/trait.BufRead.html#method.lines) instead.
Its confusing to read because you aren't getting an `Iterator` back you are getting a type that implements `Iterator` back. It may be have other traits implemented for it as well.
For a start, you're using a regular expression to do the splitting, that's going to be very slow, especially without optimizations enabled. Next, you're not taking advantage of rust's features: you're allocating a new string for every single word and then copying across. You can actually do everything you need to with zero copying: 1) Measure the file size and preallocate a buffer large enough to hold the entire string (saves having to resize it while reading) 2) Read the file into the buffer 3) Convert in-place to lowercase, using `make_ascii_lowercase()` 4) Split the string, by iterating, and calling `split_at` where necessary 5) Collect the results in a `Vec&lt;&amp;str&gt;` 6) Sort the vec Note that the original `String` still owns the data, and the data is still laid out contiguously in memory: it hasn't been moved since it was read in from the file. The entire splitting and sorting process just involved manipulating offsets into the original string. 
Does "fast" mean high throughput (server, simulations), low-latency (UI), real-time (audio, control) or what? My beef with Java's performance is that it can have high latencies (especially on start-up), and that writing fast code often means writing ugly, complex code.
Ah, you're right. I remember that NPOT textures didn't work on iOS while the same textures once resized worked, and concluded that NPOT textures were not supported. I should have checked my facts. (textures really aren't robust in glium yet because of the limited knowledge I have about all these details)
Waaaat. There was a Boston meetup? Dagnabbit. I had no idea!
You could still potentially get hiding: struct HiddenIter&lt;T: Iterator&gt;(T); impl&lt;T: Iterator&gt; Iterator for HiddenIter&lt;T&gt; { type Item = T::Item; fn next(&amp;mut self) -&gt; Option&lt;T::Item&gt; { self.0.next() } } fn some_iter_returning_fn() -&gt; HiddenIter&lt;~Iterator&lt;Item=u8&gt;&gt; { ... } With some magic to support generalized newtype deriving which would allow `#[derive(Iterator)]` this would only get better. **Edit:** Changed code to reflect a potentially more sensible way of doing the same thing. Old code: struct HiddenIter&lt;T&gt;(~Iterator&lt;Item=T&gt;); impl&lt;T&gt; Iterator for HiddenIter&lt;T&gt; { type Item = T; fn next(&amp;mut self) -&gt; Option&lt;T&gt; { self.0.next() } } fn some_iter_returning_fn() -&gt; HiddenIter&lt;u8&gt; { ... }
Well, FWIW, a decent amount of what I'm writing was influenced by your last round of comments, so *something's* working here :) Re: elision: in hindsight, it was a mistake, and I've updated the post to remove that terminology. For some reason it seemed like it was a more clear way of putting things, but now I see that I was wrong about that.
Fast for the respective use case. Applications where shaving off ms counts are plain niche in the world of software. Startup problems in Java are easily fixed with an AOT compiler. There are other vendors besides Oracle. My point was more about any language that offers value types and aot compilers. I do like C++, but I am convinced that if instead of Java the IT world kept using aot compilation with languages that had value types, C++ might not gain the widespread it did. We had Oberon, Component Pascal, Delphi, Ada. But they faded, for multiple reasons, so C++ became the only alternative to C for those looking for languages with aot toolchains. 
Following up on this: if we allow OIBITs to leak, it's possible that we'd never need conditional bounds on closures, in which case falling back to explicit newtypes, as you suggest, could be plausible. Also, for compound iterators, you usually end up with a relatively *concrete* iterator, which just *is* double-ended (or whatever). So I'm coming around to the idea that in the majority of use-cases, a simple list of traits (usually one) suffices, and in the harder cases we can use newtypes (to begin with). That makes the type abstraction approach more palatable.
Another happy customer ;-)
hmm,sorry,i dont know it ,but thank u
We will forever fight about this. :) here's hoping specialization makes things moot.
There's nothing really wrong with it, it's just that sometimes you want to run your program without having a runtime dependency.
BTW I changed the code to a solution which doesn't require `~` notation for struct fields. I'm not sure I understand what you mean by other direction.
You're using a Regex in the Rust version, but not in the Python version. It seems you are trying to split based on whitespace alone (correct me if I'm wrong!), so I guess you should use [`std::split_whitespace`](http://doc.rust-lang.org/nightly/std/primitive.str.html#method.split_whitespace) instead of a regex. Running a regex on 4MB of text is going to be very slow, I think. Also, nitpick: let diff = start - time::now(); You're substracting the start time from the current time; you probably want to do the opposite. Shouldn't matter much, though.
Personally I don't see much value in the `@` syntax, if we've got the `~` one. You can hide the implementation behind a newtype: struct HiddenIter&lt;T: Iterator&gt;(T); impl&lt;T: Iterator&gt; Iterator for HiddenIter&lt;T&gt; { type Item = T::Item; fn next(&amp;mut self) -&gt; Option&lt;T::Item&gt; { self.0.next() } } fn foo() -&gt; HiddenIter&lt;~Iterator&lt;Item=String&gt;&gt; { Some(3).into_iter().map(|x| x.to_string()) } This leverages the existing mechanism of privacy for hiding, and with generalized newtype deriving, the Iterator impl could be replaced by `#[derive(Iterator)]`. (Bonus: also handles all conditional things, by conditionally implementing traits on `HiddenIter`).
I bumped into the lack of this recently when dabbling with rust. Do you know when/where this is available? I tried searching already, but my Google-fu fell short. Thanks!
Yeah I haven't had the extra time to hack on it more :/
That would require IDE integration. Actually, with if you use inotify, you would need to re-parse the code and match the old code with the new.
Very not stable, though nrc is working on it.
Pretty much, though the details might not be identical to the current implementation.
https://github.com/TeXitoi/benchmarksgame-rs/pull/24 &gt; $ time bin/chameneos_redux 6000000 &gt; [output omitted] &gt; real 2m0.756s &gt; user 1m25.856s &gt; sys 1m49.006s &gt; $ time bin/chameneos_redux_veedrac 6000000 &gt; [output omitted] &gt; real 0m2.254s &gt; user 0m8.837s &gt; sys 0m0.009s Wow, a 60-fold improvement in real time, no less! And with no `unsafe`, which I is really neat to help drive the point home that Rust is fast *by default*. Regarding https://github.com/TeXitoi/benchmarksgame-rs/pull/17, I am afraid that when binding to multiple cores the atomics get shared between cores and requiring exclusivity of an atomic (CPU-level, necessary for a write) requires in x86 to send a message to all other cores having a copy (to invalidate it). I would not be surprised if a significant slow-down was observed between the "cores within a single CPU" case and the "cores across multiple CPUs" case.
Done.
And scheduling/descheduling them takes similar time to scheduling native threads?
&gt; The engineering that went into the JVM is beyond insane Indeed. I don't like Java much, but I am in awe of its VM.
I've always wondered about this: Wouldn't the kind of optimizations you describe glium doing by caching / diffing be the first thing an OpenGL driver implements while being optimized for performance? Why couldn't glUniform do this check just as cheaply as you?
&gt; https://github.com/rust-lang/rust/pull/28531 Debug performance does matter! Did you know that most C++ Standard Libraries had extra validity checks that could be activated via a compile-time flag? No? You're not alone, those extra checks cause things to slow down so much that most people have long abandoned their use and thus nobody talks about it :x
I think the consensus is that the application developer is supposed to avoid state changes, so checking twice would be inefficient.
/flee
actually a /r/playrust server
I don't think anyone has told them to put it up there. Probably should just add to it myself when I get the chance.
Yes, there certainly *can* be pathological behavior, but we haven't measured it in the benchmark case.
A comment (by whom I can't recall, and I'm too lazy to look it up now) in the original RFC struck a chord with me -- it essentially laid out the principle that the best (or at least default) way to do something should also be the easiest way. Currently, using generic function arguments is a lot less ergonomic than using trait objects, which breaks with this philosophy. I think remedying that situation is worth bending over backwards for. Edit: though I do think it's still more important to enable abstract return types than abstract argument types. I'd like both, though.
Sweet! I'm working on a few things: * [exefmt](https://github.com/monocasa/exefmt) A library for parsing executable formats similar to libbfd. Support for ELF and raw binaries currently. * [opcode](https://github.com/monocasa/opcode) A library for manipulating machine code representations. Similar to capstone and GNU libopcode. Support for chip8 and powerpc at the moment. (although I don't think that I've pushed powerpc yet). * [exeutils](https://github.com/monocasa/exeutils) A reimplementation of GNU binutils mainly as a test suite for the two previous libraries. Please don't depend on this anyone. I'm using these to learn Rust better, and am constantly changing the interfaces as I get a better idea of what is idiomatic.
(author here) I agree, it doesn't do any of the complicated stuff (not, that it was designed to). It's about starting **integrating** some basic piece of rust in an haskell project for haskellers that might want to use something else than C/C++ for their lowlevel stuff. I didn't find any documentation for the basics haskell integration, so I thought I would write some; It's primarily for my own benefits, but some people can (and did) find values in this "very low value" post. Looking forward to your high value article on rust bindings ...
Nice. As I said, I didn't find one "quickly", basically I just search for 'spmc', which also searches keywords from crates. 
&gt; Each time it resizes, it has to move the data from the old buffer to the new, larger one. Once all of that's complete, it can finally get to work. You should use something like BufRead::lines instead. I'm not really convinced of this advice. The growth of the buffer will be exponential, so there will only be a couple of reallocations and the data will only be copied a couple of time. On the other hand, `BufRead::lines` wil perform an allocation for *every* line.
Or perhaps here after the `-&gt;`, like this: fn foo&lt;U&gt;() -&gt; &lt;T: Iterator&lt;Item=U&gt;&gt;T The `&gt; &lt;` might look a bit "South park" though. :-)
About all that OpenGL 1.x compatibility - additionally it might be a good idea with some Arm SBC's in mind which only have GLES but thanks to a translation library called glshim can run native OpenGL apps without a problem. 
Holy Hell! I am Jackpot51, the writer of Redox. I can confirm with a commit and push to master if necessary. I have been working on this for a few months, am currently working on write access in the filesystem. I need some way to communicate with this new community (300 stars and going!). Lots of documentation to do to. I made sure it would work in Windows and OS X so that you guys could see it when I published, looks like that happened early! 
If anyone wants to IRC: irc.mozilla.org #redox
You can have a lot of it be automatic, with a couple small changes. pub trait StableThing { fn as_animal(&amp;mut self) -&gt; Option&lt;&amp;mut Animal&gt; { None } } impl &lt;T: Animal&gt; StableThing for T { fn as_animal(&amp;mut self) -&gt; Option&lt;&amp;mut Animal&gt; { Some(self) } } impl StableThing for Motorcycle { } Like this every Animal is automatically a StableThing, and things that aren't animals only need an empty impl to become one.
Moving over to moz
I don't have very much programming time at home, but when I do I am working on a Rust kernel for the Jupyter notebook project (formerly IPython). https://github.com/pwoolcoc/jupyter-rs
My major criticism of Rust is that some of the unsafe things simply don't work. * Any optimization at all can cause memory operations to be reordered in a way that breaks drivers. It is very verbose to fix this ( volatile_load and volatile_store ) * I had to twiddle things very specifically to get my context switching function to work (it needs to push/pop a predictable amount of times) * LLVM issue probably, but once rustc compiled aligned SSE operations on the stack pointer! * Some useful features, like #[deprecated] are not useable in unstaged packages * A lot of things I do require the nightly build of rust (I think this is ok, just saying) * panic_impl /can't actually be implemented!/ so I had to patch libcore and recompile it to get panics to work * The standard library relies too much on libc, when a significant piece of it could directly call the kernel. You could have a libstd_system that calls the int 0x80 interface for things like read/write/open/close/seek/exec etc... * Alloc could implement malloc using brk in rust alone, instead of relying on jemalloc Things are getting better though, and they are good enough to make a kernel with as is!
I am talking only about a "keep my code built/error log current" daemon. Keep the daemon running but passive (so no inotify) until you "cargo build", and you have the current model back, except with less (de)serialization. What of all of that would require IDE integration? The fact you can implement IDE integration on top of it is a bonus. BTW, note that the RFC specifically states that parsing is not yet incremental, so it happens anyway. I guess (it was not stated) that parsing is fast enough compared to other stages to just redo. Needing to reparse and using inotify are orthogonal decisions.
Optimizations are not all disabled, it is compiled in O2. SSE is disabled in the kernel code, and would benefit throughput for display flipping. Sometimes, however, I have to look at the assembler output to double check that the compiler did things right, which is why I sometimes have to disable optimizations. User applications can make full use of optimization and SSE.
You're just using slightly off types; http://is.gd/reOmn1 works as intended.
It is Unix-like, with file based interfaces. There is a small caveat, URL's are used to provide obvious division of different interfaces: https://github.com/jackpot51/redox/wiki/URL Some thoughts I have had can be found here: http://www.redox-os.org/
As dbaupp [said](https://www.reddit.com/r/rust/comments/3mtim0/rusts_flawed_dynamic_type/cvhyn5d), this only works if the erased type was `Box&lt;Debug&gt;` in the first place. It won't work if using multiple traits or if you don't have control of the constructing code.
I wanted to make an abstraction over `Automaton` and `std::str::pattern::Pattern`. So I tried sometime like use std::str::pattern::Pattern; trait MyPattern { fn matches(self, haystack: &amp;str) -&gt; bool; } impl&lt;P: Pattern&gt; MyPattern for P { fn matches(self, haystack: &amp;str) -&gt; bool { self.is_contained_in(haystack) } } But then I got stuck at implementing `MyPattern` for everything that implements `Automaton&lt;P&gt;`. So trait inheritance won't work for me here, because I want `MyPattern` to be implemented for all `Automaton`s, but also other things. Anyway, thanks for the workaround. I also found [another one](https://gist.github.com/27cda09d138eb3e5b0e7), using a wrapper struct and `PhantomData`, which has the advantage of not requiring me to modify the trait `Foo`.
I don't see any automated testing in this project, any reason why it seems to be avoided? It could save a lot of time and improve maintenance in such a huge project like an operating system.
I notice you still have f32/64 functions in libcore. How are you dealing with the [SSE issue](https://github.com/rust-lang/rust/issues/26449)?
&gt; Any optimization at all can cause memory operations to be reordered You need to use `UnsafeCell` if you want to break the aliasing rules. `&amp;T`/`&amp;mut T`s noalias semantics are provided to LLVM, which makes optimizations based on that. 
Oh, I know that, I'm saying that some of these optimizations may fall in the range of noalias UB if done wrong.
I stumpled upon this and posted it to reddit, because this is so awesome! **If you got to redesign rust, how would it look?**
You very often "know" what states you need to set due to app specific logic, so you can set exactly what you need to. That's much cheaper than setting stuff redundantly and doing a last minute check.