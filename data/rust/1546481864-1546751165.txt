I seriously don't get what's so good about this, especially coming from a C# background where there is a vast landscape of official APIs that propriety code can use to interoperate consistently. The way I see it, if I want to be able to read XML, or do network I/O, or whatever, there are only two possibilities: - There is one library that is vastly more popular than all others due to strong interdependencies. For example the [uuid](https://crates.io/search?q=uuid&amp;sort=downloads) crate is consumed by many other libraries that use its public types. There is no "competition" and there likely never will be. This makes it the only choice, *effectively* making it a part of the std library, but without the rigour that entails. Umm... yay? - There are multiple competing libraries of roughly equal popularity. No matter what, before I even being coding, I have to sit down and spend hours or days researching which library is still maintained, and by *whom*. Some of these may be popular now, but the work of some kid in Russia. Who knows!? the crates.io site requires no authentication of identity of any type. You can upload crates effectively anonymously. It gets hideous when I may also want to use other crates later that depend on third-party lower level crates. I could be painting myself into a corner due to incompatibilities or differing design approaches. This complicates the "decision space" to a point that is merely annoying now, but could become absurdly difficult if Rust ever becomes popular and the "choice" increases. An example that pops into my mind is standardised treatment of XML. For example, C# had one official library (System.Xml) that pretty much everybody uses. Older languages like C++ and Java had multiple competing third-party libraries, so for example if you wanted to interop between XML from some SOAP API with some other library such as a message queue, you had better hope they both use the same XML library otherwise you're in for a week of fiddly coding to translate between the two. Three XML APIs to deal with? Triple the number of shims! Yay! Some background reading of why more choice is not always better: https://en.wikipedia.org/wiki/The_Paradox_of_Choice 
That's a good point. Go also does a great job at this. I think it's also worth saying that Go and Rust, while sharing a similar space, differ in that Go has essentially unlimited resources by comparison to what the team Rust has available to them. This isn't strictly a negative, since the slower pace of development allows people more time to experiment and find optimal patterns and expose pain-points. I'm not particularly worried either way about it, but it would be convenient to have more ready-made solutions available.
You can make a top-level module with your custom prelude add-ons, and then essentially include it into every other file with a single line. This seems like a best-of-both worlds. Example: Make a file src/local_prelude.rs. Put "pub use rand::prelude::*" and whatever other stuff you want in your custom prelude there. In src/main.rs or src/lib.rs, put mod local_prelude.rs. In every file, put "use crate::local_prelude::*".
You may be right on that. Also, not sure why the `.eq()` is necessary.
Hey check this - I could tie a unique, incrementing id to the listeners...
I'm going to chime in about the warning against using a `Mutex` or `RwLock` in your Application state, since that might not be obvious. State in Actix is shared across every Actor in that Application. Imagine two Actors sharing a `State { important_date: Arc&lt;Mutex&lt;Data&gt;&gt; }`. If the first Actor locks that mutex, the second Actor will need to wait for the first to unlock it before it can do anything. This is almost universally a "bad idea" in the world of Actors, and it instead makes more sense to create a new Actor type that owns that `Data` and accepts messages to Read or Modify it such as `enum Msg { Read, Write(new_data: Data) }` where the response of `Msg::Read` will be a copy of the original `Data`, and `Msg::Write` will tell the Actor to update that `Data`. This doesn't take into consideration how important it is for `Data` to be updated once a `Msg::Write` is sent, but should help with the case where you inadvertently throttle your Actors because too many are trying to lock an owning `Mutex` at once.
There's a saying in Python, "the standard library is where good code goes to die." The idea is that good code is written and well-loved so it is adopted by the standard library, but then rots as a result of the stability requirements. If we have a third-party module that's de facto standard, it is permitted to evolve separately from the language and standard library, and most importantly users can choose to use a new rustc version (which they should always be able to) and to _not_ adopt any change in a library. Maybe there was a performance regression in a library version but you want a new language feature, or maybe there was a breaking change to a library that's too hard to adapt to, but you really want the compilation speed improvement that was just shipped.
I don't think you have to name it `local_prelude`. I just go with `prelude`. If you need to have a public prelude, you can share the module, and use `pub` vs `pub(crate)`, I hope. I haven't actually tried yet.
Continuing to implement my mini-SCM, or the SCM part specifically. I'm thinking a good P2P architecture would be good, but how would people get lists of peers? Trackers, which said peers connect and ping to?
Neat article! It's nice to see more people discover that procedural macros don't need to be scary -- this one is just 35 lines and quite readable. Custom derives eliminate a lot of opportunity for bugs in copy-and-paste trait implementations, particularly if the trait is intended to be a safe abstraction around unsafe code ([`#[derive(RefCast)]`](https://github.com/dtolnay/ref-cast) is an example like that). One correction: everything here was stabilized back in Rust 1.15, around two years ago, and is not tied to the 2018 edition. Other than the import syntax being different on one line where it says `crate::`, literally the exact code that you wrote compiles exactly the same on rustc 1.15 (thanks to modern versions of syn and quote having really broad compiler version support). The 2018 macro features that Alex wrote about are for procedural macros *other than* derives. Also some of the implementation details have changed, but if you are using syn and quote that mostly doesn't affect you -- as you saw, the same procedural macro compiles on every rustc between 1.15 and 1.31.
Correct, there is no way. Macros (procedural and macro_rules) should be conceptualized as operating on Rust tokens, not strings of source code. What would you need the string for?
I disagree personally. What’s the need? Anything in where spacing matters that’s not in quotes. I want to put a macro for html for instance: html!{ &lt;div&gt;Salt &amp;amp; Time&lt;/div&gt; }
At first it seems nice but in the long run it tends to break down. Example is Java ships with tons of XML, GUI, database, RPC and myriad other stuff hardly anybody uses anymore, but can’t be removed now because it’s standard lib. It’s taken their engineers a decade to unravel their standard library dependency hell to break up to smaller modules. Python is a bit similar situation with including many libs that nobody uses but are stuck with.
There is a "call for participation" section in weekly TWiR https://this-week-in-rust.org/blog/2019/01/01/this-week-in-rust-267/#call-for-participation Some of them also provide mentoring. That would be a good place to start. Although they are not always related to web projects, but I think you will get more comfortable with the language.
Macros are not intended to be a way to embed arbitrary non-Rust code into a \*.rs file. They are intended as a way to transform Rust tokens into other Rust tokens. Whether or not whitespace is made available, you aren't going to have a good time with mixing in syntax from other languages. For example consider a hypothetical macro to embed a fragment of C++ code: cpp! { std::cout &lt;&lt; R"("})" &lt;&lt; "\n"; } In C++ the `R"("})"` is a [raw string literal](https://en.cppreference.com/w/cpp/language/string_literal) token whose value is `"\"}"`. But interpreted as Rust tokens the macro would see it as: cpp! { std::cout &lt;&lt; R "(" } // trailing garbage: ) " &lt;&lt; " \ n "; }�
Hm, good point, thanks!
I maintain a list of people who are willing to mentor others with Rust! Check it out: https://github.com/not-yet-awesome-rust/mentors Also, feel free to PM me -- more than happy to help. :)
Je pense que tu devrais commencer par demandé au bon endroit... Ici on parle (en anglais) de programmation avec le language "Rust"...
If you want to protect yourself only from exceptions and panics but not segfaults and memory overwrites, you could try just writing something like let e = catch_unwind(|| { call_rust_plugin(); }); // recover from the error And for C++ plugins you would write a small piece of C++ code that does a try/catch and link that statically with the host program. Now, there is no guarantee that is actually going to work, because the ABI isn't stable. But there's a good chance. So if the performance cost of process switching is too high, then replacing a fence with a curtain might be the least bad option. :-) 
One thing that might be helpful is to look into the [bindgen](https://crates.io/crates/bindgen) crate. I know you probably want to actually write the binding yourself but the output of the bindgen cli tool might be a good point of reference on how to do that in an ergonomic way. 
Ok, that sounds more like [compiler_fence](https://doc.rust-lang.org/std/sync/atomic/fn.compiler_fence.html) maybe? But yes, I agree that some raw pointer ergonomics could be improved, to make it less tempting to turn a `*mut` into a `&amp;mut` (which one should not do unless the reference really is unique).
Question about your motivation: was there a reason you couldn't do something like: impl&lt;T&gt; WritableTemplate for T where T : Template
Everything else aside, the tooling is superior.
Awesome, TIL! I didn't know how long it has been available. Also I'm not sure how I missed that, I'll re-read Alex's post.
Doh! I knew I missed something about the type system here. I asked around and I must have missed this feature, though I remember it now. I'm still a bit clunky around the type system and specifically traits.
One small downside of my idea is that you'd get `WritableTemplate` implemented for every single `Template`, whereas your approach lets you pick and choose. But I think something like this would solve that problem: trait WritableMarker {} pub struct MyTemplate {} impl WritableMarker for MyTemplate {} impl&lt;T&gt; WritableTemplate for T where T : WritableMarker
That's an awesome resource, thanks!
This is great! Thank you so much. And I'll be sure to reach out soon
Ce n'est pas le subreddit pour le jeu Rust. Essayez /r/playrust/ 
That's interesting stuff! I guess it's similar to how C does it! Given the following string, ``` char* s = "hello world"; ``` We can do `s[2]` to get the third character, or equivalently `*(s + 2)`. It looks just like the slicing sugar! In this case, I guess `str::index` can be thought of as a smarter way to do pointer arithmetic. Oddly enough, even though `std::ops::Index&lt;Range&lt;{integer}&gt;&gt;` is implemented on `&amp;str`, `std::ops::Index&lt;{integer}&gt;` isn't, so we can't easily do the following to get a single character: ``` let s = "hello world"; let c = *s.index(2); // doesn't work let r = s.index(2..5); // does work ``` However, if we think of the string as a character array, both kinds of indexing works: ``` let s = ['h', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd']; let c = *s.index(2); // does work let r = s.index(2..5); // does work ``` I guess a followup question would be why we can't index a `&amp;str` by integers, and only by a range of integers? I found the following from https://doc.rust-lang.org/std/string/struct.String.html#utf-8: &gt; Indexing is intended to be a constant-time operation, but UTF-8 encoding does not allow us to do this. Furthermore, it's not clear what sort of thing the index should return: a byte, a codepoint, or a grapheme cluster. The bytes and chars methods return iterators over the first two, respectively. If I understood that right, it was left unimplemented because the behavior for indexing is ambiguous? I guess that's a fair reason. I also found this Reddit comment https://www.reddit.com/r/rust/comments/5zrzkf/announcing_rust_116/df0v1h1/ linking to a blog explaining the quirks of UTF-8 encoded strings. I think once I digest that I'll be happy! :-) Thank you for the help!
And I would counter that the unravelling has been a disaster. I can no longer compile ANTLR on newer versions of Java, because Oracle removed something from the standard library during its componentisation, thinking that "nobody uses it". Well, I was using it. Terrance Parr was using it. Thousands of users of ANTLR were using it. I have more experience with the equivalent process in the C# world, where I have a sinking feeling that the new Microsoft staff working on it have come from a background of only NPM experience and think that it's a *good thing*. Since the introduction of NuGet and the standard lib being spun off into modules the quality has taken a massive nosedive. I was absolutely shocked at the glaring mistakes and gaps I saw in fundamentally libraries, the kind that simply would have never happened in the past. I actually jumped through the hoops to submit a pull request and trouble ticker for a particularly glaring one that made a new File I/O API virtually useless for any real-world application and was told "sorry, we'll get around to it later, functioning correctly is not a high priority at the moment because this library isn't held to the same standards as System.IO". I had a similar experience in Rust, I found massive bugs in popular libraries, and the feedback was similar. "Not our problem, contact the library author"... Okay sure, I did... his response: .. "This is just a personal project, not the standard lib, I don't care." Awesome. This just isn't a selling point for me. It sounds like a shifting of responsibilities, like the repeated mantra in politics that the "free market will solve it" when there is no evidence of any such thing ever occurring in practice in the past...
The TWiR issue tracker and source code are on GitHub - https://github.com/cmr/this-week-in-rust
[removed]
You probably want to post in r/playrust.
That's pretty good! You even got the accents and cédille right. _Ceci n'est pas une macro_
A much better way to do what you want would be with a tool written specifically for the job (i.e. working on arbitrary strings instead of Rust tokens) invoked in build.rs.
I see the point, but I can counter this in a couple of ways: First, there seems to be a conflation in some people's minds with the version of Rust the compiler and the version of of Rust's "std" library. While they're linked *presently*, this is not a requirement by any means. For example, Visual Studio allows newer C# language features to be used when targeting older frameworks, as long as the language feature is backwards-compatible. Second, many people are under the impression that just if something is implemented in Rust's standard library then it's impossible to use an alternative implementation. This is obviously a false assumption, because Traits allow a completely native integration of a new implementation, down to even the inlining of code, which is not possible with Object Oriented virtual interfaces, for example. Third, some people seem to be under the impression that pull requests to the Rust std library are ignored. My impression is that the Rust core language maintainers are quite open to such things, in stark contrast to the C++ committee or the Oracle Java process, neither of which is very "open" in comparison. Fourth, this again is just a shift in responsibility, onto my shoulders instead of the core language team's. If I need a downlevel library because of some terrible regression that cannot be lived with, then surely this is a *failure* of the system, not a *feature*. It certainly feels this scenario would be a lot of work for *me*, the end-user! I'd have to go through a massive effort of checking compatibilities, making sure there are no conflicts, making sure that newer versions of library aren't pulled in transitively by other libraries, etc... Not to mention that ever worsening compatibility risks as Rust the language forges ahead and my library gets ever further out of date. Ugh.
That's interesting stuff! I guess it's similar to how C does it! Given the following string, ``` char* s = "hello world"; ``` We can do `s[2]` to get the third character, or equivalently `*(s + 2)`. It looks just like the slicing sugar! In this case, I guess `str::index` can be thought of as a smarter way to do pointer arithmetic. Oddly enough, even though `std::ops::Index&lt;Range&lt;{integer}&gt;&gt;` is implemented on `&amp;str`, `std::ops::Index&lt;{integer}&gt;` isn't, so we can't easily do the following to get a single character: ``` let s = "hello world"; let c = *s.index(2); // doesn't work let r = s.index(2..5); // does work ``` However, if we think of the string as a character array, both kinds of indexing works: ``` let s = ['h', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd']; let c = *s.index(2); // does work let r = s.index(2..5); // does work ``` I guess a followup question would be why we can't index a `&amp;str` by integers, and only by a range of integers? I found the following from https://doc.rust-lang.org/std/string/struct.String.html#utf-8: &gt; Indexing is intended to be a constant-time operation, but UTF-8 encoding does not allow us to do this. Furthermore, it's not clear what sort of thing the index should return: a byte, a codepoint, or a grapheme cluster. The bytes and chars methods return iterators over the first two, respectively. If I understood that right, it was left unimplemented because the behavior for indexing by integers is ambiguous? That's a fair reason, but I'm not sure I'm really convinced yet. We can just use a range of `2..=2` to get the 3rd character: ``` let s = "hello world"; let r = s.index(2..=2); ``` Works fine. Although it breaks on unicode strings: ``` let s = "┬─┬ノ( º _ ºノ)"; let r = s.index(2..=2); // panics because byte index 2 is not a char boundary and inside bytes 0..3 ``` From this it seems pretty unambiguous what an integer index should return. It should return a byte, since indexing by ranges already does so by bytes! Maybe I'm missing something. I found this Reddit comment https://www.reddit.com/r/rust/comments/5zrzkf/announcing_rust_116/df0sydn/ discussing the byte-oriented slicing with a link to a blog explaining the quirks of UTF-8 encoded strings. Maybe once I digest that it'll make sense. Thank you for the help!
Small correction: I believe `extern crate` is only required for `proc_macro` in Rust 2018.
Thanks! I’ve never heard of this, I will check it out
&gt; There is no "competition" and there likely never will be. That might be true, but putting something in the std lib makes competition nearly impossible. uuid might not be the best example, but imagine if 10 years from now, a superior, more modern implementation comes along. If uuid is in the std lib, it is extremely difficult to rip it out and replace it (assuming APIs differ significantly) given backward compatibility. And in the first place, it would be very difficult for the newer, better lib to gain any traction with the functionality already existing in the std lib. Mostly, the problem is setting an API/paradigm in stone. You need to be sure that a lib has THE api for whatever it does isn't likely to ever change and works well for the vast majority of use cases, which admittedly is probably true for uuid. XML is a good example of something that has any number of good ways to be implemented that have various pros and cons. I'd argue it's not the std lib's job to decided which one everyone should use. As far as choice being a problem, in this case it is greatly mitigated by the fact that you rarely have many lib choices as long as you stick to widely used and well regarded ones. And when you do have many choices, it isn't the std lib's job to pick (unless they were literally all slightly different impls of the exact same thing, I suppose). Not that I'm against adding things to the std lib. uuid might actually be a good candidate. I just think langs like C# and Python go too far. But C doesn't go far enough (and C++ to a small extent, e.g. still no networking as of C++17).
This worked along with a "rustup update"
That's an interesting idea, but not practical in some cases. If those plugins are developed by third parties and no longer maintained then it wouldn't work. 
Decisions about what goes in the standard library should be based on current and practical, not theoretical capabilities. Of course we could also have third-party implementations of things. Note how Python ended up with `urllib` and `urllib2` in the standard library, which are just beginner traps. Plenty of beginners have to be told to use a third-party library. The strength is that you can hold back a version but bump your compiler version while you open an issue or patch a regression. I have done this, and it's nothing like the straw man you're creating. The Rust team is not "avoiding effort" by not incorporating things into the standard library; they maintain a substantial fraction of the heavily-used packages on crates.io.
I personally try to avoid copying data when dealing with types that are views of other types, but I find myself falling back on clones more often than I'd care to admit.
Sorry, it wasn’t clear from my post but I already used bindgen to create a sys crate of the library. Next step is using those bindings to create safe Rust bindings which I’m trying to do now!
Why reference count the raw pointer instead of wrapping the pointer and reference counting the wrapper? 
We have some good first issues in Quinn: https://github.com/djc/quinn We (Benjamin and I) are always happy to mentor people working on those, and as an implementation of QUIC it hardly gets more low-level web than this. 
I took some French in university and I configured my `Control_L` as `Compose`. It's the grammar, vocabulary, and avoidance of anglicisms that I'm shaky on because I don't actively use it and I've been too busy to pursue fluency.
Since you share almost identical dependencies with Amethyst, would you care to elaborate why you didn’t use that engine? In theory it should make it easier to write and maintain specs-based games.
Yes, Maud does escape by default. According to the [documentation](https://docs.rs/horrorshow/0.6.4/horrorshow/#escaping), I believe Horrorshow does as well?
You might want to look at https://github.com/bodil/typed-html - it implements a html macro like this, but requires all literal strings to be quoted so that whitespace is preserved.
Not related to the author or anything, thank you for your efforts.
From what you describe, you can have a look at [http://www.craftinginterpreters.com/](http://www.craftinginterpreters.com/). The author covers all the steps for compilation/interpretation in an easy to read style. The first part is in Java but you can write it in Rust instead. It is quite fun to convert from an OO style to more idiomatic Rust (i.e., Pattern matching instead of the visitor pattern). &amp;#x200B;
Thanks for the interest /u/Eh2406, /u/steveklabnik1 and sorry for the delay. I checked the issue and it looks in fact false positive, `cargo update` was given an it worked as expected. Nevertheless, the problem was this: a new crate was added as dependency. Later on the crate was removed. The dependencies of the removed crate were left to Cargo.lock. This was resolved with `cargo update`. What seems to be missing is some kind of command that would clean up the Cargo.lock without changing existing versions of other dependencies on the workspace. Maybe there is one? I did not find it mentioned anywhere in the docs, though. What is tried to be accomplished here is to have controlled way of introducing `cargo update` and additional development should not mess up workspace Cargo.lock-file. One can of course argue that version control saves the day here, but it would still be nice to be able to resolve the crate removal in the Cargo.lock of workspace without going into restoring older versions.
Pretty much, yeah. Makes it less granular to try and match "every kind of error this crate can throw", but most such structures have a wildcard "other" variant as well anyways. Having just one `the_crate::Error` structure just makes it easier to manage many kinds of errors without having an error struct per public function and all the conversions between them that would be necessary.
I'm using a trait `Repository` implemented by both the database stuff and a mock.
Boxing is a generic term in programming languages: https://en.wikipedia.org/wiki/Object_type_(object-oriented_programming)#Boxing
Thanks, Sorry
Hm, I like these documents, but most of the "Community improvements" section is actually more around project management and communication suggestions. Which has an overlap with community, but is IMHO different.
Are you literally new to reddit, too? Because you're looking for /r/playrust ;)
You can always just learn Rust instead, it's much more friendly.
Wrong sub my chum. 
You mention the trade-off between useability and performance while choosing web frameworks in Rust. Are there any benchmarks on this?
Post hog ergo Procter hog
Ever spent an entire evening randomly adding and removing mut, &amp; or * trying to get the compiler to shut it's fucking mouth? Wanting to KOS is just part of the Rust experience. 
Can you describe an example project that I should implement with my favorite GCed runtime to experience all the performance issues?
OCaml's tooling has been improving. Intelligent editing support is very good, across at least Emacs, Vim and VS Code. There is a single preferred packaging and library distribution system (opam) and a single preferred build system is emerging (dune). On the minus side, a lot more could be done about onboarding, tutorials and discoverable online learning materials, and the best book (Real World OCaml) uses a particular standard library that is not universally shared and therefore teaches not OCaml but Jane-Street-flavored OCaml.
Newb here - should I be storing &amp;[T] in structs too, say for json serialisation where I just want some list of something? 
If you are willing to sink a lot of effort into it, you could probably achieve a similar effect using [binary translation](https://en.m.wikipedia.org/wiki/Binary_translation).
Desktop link: https://en.wikipedia.org/wiki/Binary_translation *** ^^/r/HelperBot_ ^^Downvote ^^to ^^remove. ^^Counter: ^^229329
&gt; and only those 4/8 bytes that make up that reference get copied 8/16 actually, `str` is unsized so `&amp;str` is a fat pointer.
By the way: hashsets/maps do not have a deterministic iteration order, and two hashsets with same values might yield them in a different order. The correct way to compare hashsets would be [the way `std` does it](https://doc.rust-lang.org/src/std/collections/hash/set.rs.html#768-779) (and then you don't even need to do it manually, because `std` already implements this).
A couple more things: 1. `join_photos_vertically` loads both images and then saves the result to a file. Then `write_text` loads that back into memory just to write text and write the result back. Instead you could have `join_photos_vertically` return the image, and `write_text` take the image as argument. You could also separate loading and processing into two distinct functions, so that you could for example load each image only once, instead of once for every iteration of the loop. 2. I'm pretty sure that you shouldn't ever need `.filter_map(|x| Some(x))`, as it does not do anything. 3. I think it would be more idiomatic to do `list.par_iter().for_each(|item| { ... })` instead of `list.par_iter().map(|item| { ... }).collect()`.
It does not force you to have only one error type in the whole crate - you can still use `std::result::Result&lt;Foo, SomeOtherError&gt;` where you need it. However, when most of the functions have the same error type having a type alias is more concise (also, subjectively, for me stuff like `io::Result&lt;Foo&gt;` simply looks prettier than `Result&lt;Foo, io::Error&gt;`.
Hey, this looks interesting! Does it support #[no_std] applications? 
Thank you, ill try doing these. just as a note, i used those hardcoded numbers because for my specific use case, i didnt plan to share this at first. thanks for the detailed comment!
A possible solution is to tack an empty capture at the end of the regex. The alternation operator is defined as "`x|y`: [match] x or y, prefer x". So for `r"x|()"` is guaranteed to succeed. You can know that `x` matched by checking if the last capture is `None`. That wouldn't work with RegexSet though.
Thank you very much, i'll do these fixes.
Current GCs are mostly-concurrent but if you run for long enough you get heap fragmentation, and that's hard to solve without stopping the world since you have to move objects around. (Even C isn't immune to that problem, though it gives you more control over when it happens since `malloc`/`free` calls are explicit).
I haven't marked as no_std, but I don't see any reason why it shouldn't be.
This deserves gold. 
Spending time optimizing performance beyond what's needed would be a dereliction of duty, like an architect who designs a taller building than needed because they like tall buildings. IMO developers already spend too much time optimizing performance (because it's fun) when we should be focusing more on correctness, maintainability, and speed of implementation. An example: Java, Python and other languages adopted a complicated sorting algorithm as their default, trumpeting how it was 10% faster on typical data. But this algorithm is so complicated that for years the default sorting in those languages was literally broken - you could pass it sequences that it would sort into the wrong order. You could say that's just one bug, but the decisions that lead to it are a prime example of the screwed-up priorities that drive decisionmaking in this industry.
&gt; Rust is closer to what I'd hope to see out of a language like Scala native than Scala native actually is. I care more about the features that make programs correct and maintainable than compatibility with Scala-JVM code... Could you be more specific? I care more about correctness and maintainability than compatibility with anything in particular, but that's exactly why I'd generally pick Scala (with HKT and something approximating a record system) over Rust.
Where do your DAOs get their connections from? Are you using, e.g., R2D2 connection pools or working with them some other way? 
I'm very interested in the progress of async support, but I found it hard to track the progress of all the separate features which are going through the process individually. So I made this site for tracking it. Feedback welcome! If Networking Working Group or other official Rust team wants to utilize this domain, I'm happy to donate it as well.
To add onto what others have said, a common approach is to use an Enum for that one error type, so you can still have varying errors the calling code can easily match against.
Since it's not obviously mentioned anywhere in the linked repo's readme or documentation: &gt;[Wikipedia](https://en.m.wikipedia.org/wiki/Wayland_(display_server_protocol\)) &gt;Wayland is a computer protocol that specifies the communication between a display server and its clients, as well as a reference implementation of the protocol in the C programming language. A display server using the Wayland protocol is called a Wayland compositor.
&amp;\[T\] doesn't anchor the allocation like Vec does, it just temporarily refers to something held elsewhere, and only so long as the borrow checker can see what you're doing.
As is, it isn't compatible with `no_std` applications, because you import `std::ops::{Div, Rem}` at the top. However, those also exist as `core::ops::{Div, Rem}`, and that import should work fine for both `std` and `no_std` applications.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/rust_gamedev] [Published my first game written in Rust.](https://www.reddit.com/r/rust_gamedev/comments/ac4v6h/published_my_first_game_written_in_rust/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Great to see this work progressing. I look forward to running a Wayland and Rust powered desktop in the hopefully not too distant future. 
&gt; And I would counter that the unravelling has been a disaster. I can no longer compile ANTLR on newer versions of Java, because Oracle removed something from the standard library during its componentisation, thinking that "nobody uses it". Well, I was using it. Terrance Parr was using it. Thousands of users of ANTLR were using it. Sure, that's the problem when you *already have* a stdlib full of batteries and then try to pare it down. Which can't happen if you never have one to begin with.
I kind of want a new full-featured Wayland-only Desktop Environment that is also comparatively lightweight. Would be awesome to see rust there, but I guess the gui problem should be solved first.
Thank you. I would provide more context, especially for people not following this closely. This could be misunderstood to mean that async IO is not currently possible in rust. What you seem to be tracking (which is good!) is a standardization of async building blocks within the std lib and features which simplify code using futures.
True, and it's made a bit worse by the fact that "the stack" is mostly "a stack" when you view it from the outside, but not really when you look at it as a whole.
I love the term Box! It creates the magical illusion that I can fit something as big as I want into this pointer sized thing called a “Box”.... It's like Mary Poppins magical carpet bag of the call stack From this view it's as if Rust only has a call stack and no “heap” to mush up your mental model
I would love this too. 
I really like this project and write-up. For those who are unfamiliar (I was), MPD refers to \[Music Player Daemon\]([https://en.wikipedia.org/wiki/Music\_Player\_Daemon](https://en.wikipedia.org/wiki/Music_Player_Daemon)) and FUSE refers to \[Filesystem in Userspace\]([https://en.wikipedia.org/wiki/Filesystem\_in\_Userspace](https://en.wikipedia.org/wiki/Filesystem_in_Userspace)). &amp;#x200B; So the ELI5 of this is, OP uses an MP3 player which expects MP3s to exist on some folder of his computer. He wants that MP3 player to be able to index files from SoundCloud, but he doesn't want to scrape SoundCloud and pre-download all of those files because that would be wasteful. So he instead uses FUSE to pretend to have all of the MP3 files pre-downloaded, while really streaming them in realtime when they are being played. Please correct me if there are any errors here, as I said I was unfamiliar with both MPD and FUSE before reading this post. &amp;#x200B; One question for OP, did you have to implement pre-fetching of data, or is it enough to download the range from SoundCloud after MPD has already done the read for that chunk of audio? 
What would you consider to be actual community things? In my head, communication and project management is about enabling the wider Rust community to be involved in the project and it's development.
Well done and thanks! Purchased for the Linux build. :D
This is great! Async support is very much needed. 
In a Rust world where templating engines are faster than each other in **nanoseconds**, one ought not make a costly adoption decision based largely on this measure. What are the non-performance measures one ought to use to evaluate a templating engine? One is compiler dependency-- can it compile using stable features that *guarantee* backwards compatibility? Another measure is usability-- is the templating language intuitive (look at open and closed github issues to help answer this question if you can't evaluate the engine on your own). Another measure is project mission-- is the project actively maintained and improved by the author? Yet another measure is adoption-- is it being used for non-trivial projects? * Compiler dependency * Usability * Mission * Adoption 
Yes, although I think the trick might be to provide quality guarantees, but *not* stability guarantees...
This work adds a "subset of rust" frontend (prusti. I don't think it's available yet, or at least I couldn't find it) to ETH Zurich's [Viper](http://www.pm.inf.ethz.ch/research/viper.html) verification framework 
&gt; And I would counter that the unravelling has been a disaster. I can no longer compile ANTLR on newer versions of Java, because Oracle removed something from the standard library during its componentisation, thinking that "nobody uses it". Well, I was using it. Terrance Parr was using it. Thousands of users of ANTLR were using it. This is the idea behind shipping libraries with Rust (and thus officially endorsing them, potentially including maintenance), but also making you depend on them explicitly. If in 5 years time they are less popular, then they can be removed from this blessed collection, but your code will still work because the crate will still exist as an independent entity on crates.io. 
Thank you, i really needed this. It would be awesome to also have an up-to-date status on the "async readiness"/"async stability" of core ecosystem libraries/framworks (tokio etc.), as this space is still evolving rapidly.
Testing under `no_std` can be tricky though as the `test` crate doesn't exist in that situation and `cargo test` fights back.
If you would be OK adding it I would love to see a feature to enable `no_std` mode.
A versioned standard library would be an interesting alternative to my proposal. I think there are technical difficulties with this in Rust though, because types from different versions of the same crate are considered different types (even if their definition and name are identical). Thus a library depending on (a hypothetical) stdlib v1.0 which exposed the `Vec` type as part of it's external API would be incompatible with an application or another library which depended on stdlib v2.0. And without such versioning, then the stdlib can't break backwards compatibility *at all*. Which is a disadvantage for times when a better way of doing things is subsequently discovered. &gt; If I need a downlevel library because of some terrible regression that cannot be lived with It's more that you'd want to stick with the older version of the library you have because you don't want to change your code at all. Whereas other people might want to use improvements to the API. And you might want those improvements too, at a convenient time when you have time to upgrade.
https://www.techempower.com/benchmarks/#section=data-r17&amp;hw=ph&amp;test=fortune Search for "actix" and then "rocket". Rocket is still using the synchronous 0.10.x version of `hyper`, which is significantly slower /less scalable than the latest version.
&gt; but without the rigour that entails. Umm... yay? And without the overhead that adding it to the std library would add to Rust development.
You seem to be saying it shouldn't be in std because it can be less rigorous that way. Why exactly is this a good thing, for me, as a Rust user, considering that UUIDs and GUIDs are a commonly used type underpinning thousands of far-reaching APIs and systems? Are you saying that because its namespace isn't prefixed with 'std::' then it is absolutely fine to introduce breaking changes to it, even though it has diesel, actix, postgres, rocket, object, and mysql in its first-level dependencies? Would you be happy, as a Rust user, to have to paper over incompatibilities between different uuid/guid types used by various libraries?
This is a great opportunity to use \[proptest\]([https://github.com/AltSysrq/proptest](https://github.com/AltSysrq/proptest)) for correctness!
Very nice! Having a subsection on Tokio and their futures 0.3 support would also be very useful, I think!
GNOME is in the process to rewrite some softwares in Rust.
Yeah but killing people with no loot? I come from playing Dayz and it's a low thing for a fully looted person to go hunting for fresh spawns.. 
Holy shit so sorry guys I just noticed this is the wrong sub. My bad lmao
I feel retarded atm
This is also a bit of a confusion point too. The actor terminology is new to me, where as I can at least draw parallels from other languages for Futures, async/await, etc. It makes me ponder if I should even be using Actix.rs, because I'm unsure if the Actor model is what I need/want. More research is definitely needed. 
Awesome! Source? 
You could do the DevOps method. Run N copies of the rust program and load balancer then and then kill/respawn any unhealthy copies of the program. Depending on how much state you need, architecturing this can be anywhere from "it just works" to "rewrite to keep all the state in a highly available database" A bit of a cheating answer, but there's a reason a lot of people are using this method :)
Not fully rust related question, but any idea which software/project is powering this forum? https://users.rust-lang.org/
Allowing breaking changes is what allows an API to evolve, which is in the interest of API users. If you have it in std you can never fix your errors, so you have to get it right the first time. Getting something right the first time is possible, but depends on a significant amount of luck.
Does the compiler not do this already?
For example: https://www.phoronix.com/forums/forum/software/desktop-linux/922303-gnome-s-svg-rendering-library-migrating-to-rust But generaly speaking, there is a clear dynamic toward Rust in the GNOME team. They published gtk-rs crate to support officially the language.
What about GATs? Isn’t this required so we can have async trait methods?
Just because box allocated on a heap, doesn't mean it is semantically required to. A smart compiler could allocate it somewhere else, so long as it acts well enough like a `Box` to suit all the purposes it is used for.
&gt; You seem to be saying it shouldn't be in std because it can be less rigorous that way. No, I am saying that it shouldn't be in `std` because the cost of putting it in `std` is high, and the value that paying this cost delivers is either zero (not enough people use it), or negative (freezing the API of `uuid` forever is not "added value" but another extra cost). 
Aren't arbitrary self types needed?
"Blockers" Heh
When I started looking at amethyst, i was considering it for a tile based game. But the renderer at the time didn't allow for much customization. I ended up getting pretty stuck on rendering tile maps, so just did it in a dumb way using quads, which is rather inefficient. Anyways, this led me to having a simple renderer setup + specs that i was able to use for Ludum Dare. Amesthyst since then has been under a lot of active development, so it's hard to know what pieces are ready and what's fully documented. That today is a reason why im not sure if it would be usable. It might be :)
As to the critique of Rust editions, I strongly disagree. As a non-rustacean beforehand (but a very long time lurker and fan), the Rust 2018 announcement generally marks a point in my mind where I finally found Rust to be usable enough for me to try it. A lot of "smaller" stuff contributes to it, many of which were finalized somewhat earlier (mostly by virtue of the Ergonomics Initiative I believe, e.g. NLL or the new use/module syntax, or the various elisions), but I strongly feel that the Edition scramble really helped motivate the community to complete numerous ungrateful but super important "last mile" polishing efforts. Such as landing clippy in stable. For me personally, Rust 2018 is the real Rust 1.0.
/u/upsuper How about: "Yes, but important ergonomic language features are not yet stabilized, and the larger ecosystem has yet to stabilize &amp; standardize on modern APIs". 
Not necessarily. Afaik only Pin as method receiver needs to be allowed, which, according to the page, was stabilized.
Just a little more info on the compiler error: &amp;#x200B; error[E0495]: cannot infer an appropriate lifetime for lifetime parameter in function call due to conflicting requirements --&gt; protec-core\src\data\client.rs:70:40 | 70 | let result = get_root_as_data(&amp;self.buffer[..]); | ^^^^^^^^^^^^^^^ | note: first, the lifetime cannot outlive the anonymous lifetime #1 defined on the method body at 67:5... --&gt; protec-core\src\data\client.rs:67:5 | 67 | / fn load(&amp;mut self, filename: &amp;str) -&gt; bool { 68 | | // We need to store the buffer, flatbuffers is a view into the raw data 69 | | self.buffer = load(&amp;filename.to_string()); 70 | | let result = get_root_as_data(&amp;self.buffer[..]); 71 | | self.data = Some(result); 72 | | true 73 | | } | |_____^ note: ...so that reference does not outlive borrowed content --&gt; protec-core\src\data\client.rs:70:40 | 70 | let result = get_root_as_data(&amp;self.buffer[..]); | ^^^^^^^^^^^ note: but, the lifetime must be valid for the lifetime 'a as defined on the impl at 66:6... --&gt; protec-core\src\data\client.rs:66:6 | 66 | impl&lt;'a&gt; Load for ProtecData&lt;'a&gt; { | ^^ = note: ...so that the expression is assignable: expected std::option::Option&lt;data::protecbin_generated::protec::fat::Data&lt;'a&gt;&gt; found std::option::Option&lt;data::protecbin_generated::protec::fat::Data&lt;'_&gt;&gt; error: aborting due to previous error &amp;#x200B;
It's very cool that we're getting pure Rust Wayland implementations to contrast the freedesktop C code base. This should highlight issures rooted in monoculture at the very least. I've tried Sway, but I need master pane like in dwm or xmonad, and haven't been able to feel comfortable with the i3 tiling model in sway. I know it's a miniscule difference, but when it comes to ux and ergonomics, trivial things matter a lot. I'm writing this because I'd rather not start building a Wayland compositor that behaves like xmonad or dwm.
Thank you for putting this together. Async iterators clearly is dependent on basic async/await. I hope they're on the radar for 2019, though.
Oh compiler_fence sounds exactly like what I need, thanks!
&gt; You seem to be saying it shouldn't be in std because it can be less rigorous that way. Why exactly is this a good thing, for me, as a Rust user, considering that UUIDs and GUIDs are a commonly used type underpinning thousands of far-reaching APIs and systems? Your implication here isn't strictly wrong, but your framing of this problem is a bit out of whack. This is an exercise in trade offs. Different strategies to this problem have different trade offs. A lot of us have observed how other ecosystems---Python is a fan favorite example---have developed behemoth standard libraries that contain modules that serve little purpose today other than as beginner traps and impediments to using the language effectively. The philosophy of a small standard library is to specifically acknowledge that many APIs will undergo evolution, and that the standard library cannot feasibly support every kind of evolution that might be required. Moreover, a small standard library more effectively distributes the work for building core libraries. The standard library, by necessity, has a very high bar set on what to include. Small API additions can generally find their way into std via a PR and a subsequent stabilization consensus, but larger additions require the full weight of the RFC process. Ask pretty much anyone who has written an RFC and gotten it accepted (including yours truly), and they will tell you how much effort, time and patience is required. It is no easy feat. It is a heavy weight process, and for some things, that's exactly what you want. From a strictly end user's perspective, this might seem less than ideal to you, because *of course* you want every library you depend on to go through the same process. After all, you, like anyone, want to only use dependencies that are of a certain quality. The problem is that producing such high quality is code is a hard, time consuming and thankless task. It doesn't just happen for free, and if too much code requires going through that process, then the ecosystem might not mature and evolve as quickly as we all would like to hope it will. After all, new people are adopting Rust every day, and with these new people come new use cases, new requirements and the requisite API evolution. Some evolution is predictable, but some is not. But API evolution isn't the only concern here. Since the barrier for contributing to std is much higher than any old crate, the implementation work itself may also be neglected. For example, back before Rust 1.0 was released, even before Cargo was a thing (the current Cargo, not the old Cargo), I wrote Rust's regex library. It was submitted as a PR to the standard library itself and was shipped with the Rust distribution to everyone that used Rust. For the most part, the API of the regex crate has remained pretty much the same, with a few small tweaks. However, the implementation has changed dramatically. After I submitted a PR for the regex library to Rust itself, I pretty much stopped working on it. The iteration time on building the Rust repo (or, rather, understanding the build tools enough to only build what I needed) and then getting a PR accepted was just way too high for me to handle. So I mostly forgot about it. But in the road up to Rust 1.0, Cargo got released and the standard library was split apart into a bunch of ecosystem crates. The regex crate was now just another library, like anything else. I could then take ownership of it, and IMO, it's the best thing that ever happened to the library because it allowed for rapid iteration on improving the quality of its implementation. I think it's safe to say this was a good thing! Performance for most common cases increased by about an order of magnitude, to a first approximation. If regex had stayed in the standard library, it would have kept its API---which wouldn't have been the end of the world---but it's unlikely I would have continued to work on it inside of std. I might have just forked it and built out my own crate. At the end of the day, we've wound up with a cohort of fairly dedicated maintainers that look after the quality of core libraries. I maintain a few of them myself, and if I had to go through the RFC process or the standard library to build and maintain them, I can say with 100% certainty that I would have done none of it. Now I'm just one person---the project would live on without me---but I'm obviously generalizing here. Your supply of labor is going to decrease, and you *don't actually wind up with the thing you want*: high quality libraries. Or at least, if you do, you might get them, but over a longer span of time (assuming the lack of them didn't inhibit the project's adoption in the first place). As you've pointed out, this isn't a pure win. There are real, significant challenges to be faced with a small standard library and a very distributed ecosystem. It takes coordination and social capital to affect real change. Sometimes, I really hate to see the growing list of dependencies when I build my project. Sometimes, work needs to be done in order to move forward. I've been involved in the Rust ecosystem since the beginning, and I've certainly run into similar problems as you. And fixing them isn't always easy, but it's always within your power. You just need to work your dependency chain. It sucks when you find an unmaintained transitive dependency that's blocking forward progress, but that just means you need to go talk to someone until you find someone willing to help you fix it. More to the point, this problem doesn't go away if you have a beefier standard library. Maybe its incidence decreases, but it's always going to be an issue in a decentralized ecosystem. It is a cost we pay in exchange for the distribution of work. As the ecosystem matures, we steadily march toward a collection of higher quality crates to pick from. It's easy to see this from my perspective. Just take a look at improvements made to log, rand, crossbeam, serde and more. We haven't tackled everything yet, but those are just a few examples of high quality crates evolving outside of std.
I can respect the decision to try to remain close to Scala proper code, but I think Scala Native would've been much more interesting if they looked at any tradeoffs made for the sake of JVM/Java compatibility in Scala's design, for example nulls &amp; exceptions, and made the other decision. Completely divorce it from Scala-proper, its reliance on Java/a runtime, and its JVM roots and make it a heavily Scala-inspired language. If a Scala-Native existed like that, Rust wouldn't even compete in my mind :) I do tend to pick Scala over Rust for most things, as I do believe well-written Scala creates very maintainable and correct code, but I'll always pick Rust over Scala Native. It just isn't what I'm looking for.
If you don't then I (eventually) will :P
The problem is that you are trying to create a self-referential struct, where `data` field tries to borrow `buffer` field of the same struct. The problem with that is basically what the compiler says: you claimed that the result will be valid for lifetime `'a` (which the user of the function can pick arbitrarily long), but it cannot outlive the reference to `self` that you get - once the function returns you have no control over how long it will be valid - the user could decide to immediately drop or move it. So the end result is plainly "you cannot do this". Basically you won't be able to have the `load` function both read the file and the parse it (well, unless you are fine with leaking memory every time you call it). Instead you should try to rethink the architecture to have the bytes loaded from the file passed to the struct as a parameter, so that they could have some other owner.
&gt; 1. Functions contain too many lines of code (button.on_click at line 37 Some 20 lines of code is definitely not too many for a function. &gt; 2. The parser suggested I use _guess instead of a mut variable at line 39, I don't understand why Because you're not using that variable anywhere. The later declaration on line `45` creates a new variable unrelated to the former one. &gt; 3. I don't understand what move is for and why it's sometimes ok to not have an argument between the pipes (lines 31, 43, 63) In the context of closures, the `move |..|` syntax creates a closure that moves the variables it references into itself, as opposed to accessing them by a reference. Basically a `move` closure ends up "owning" the variables it references. This is very much related to Rust's ownership system. &gt; 4. Have I used clone correctly? I'd say yes, but I'm not familiar with libiui, so I'm not sure. &gt; 5. I'm not sure why associated functions of libraries are called with '::' and other methods are just use '.' The functions called with `::` are either free functions or static functions (of some struct or trait), they don't have a `self` argument. The `.` syntax is used to call member functions only (those that do have a `self` argumnet), however this is under the hood the same as calling a free/static function - the `foobar.some_method(arg1, arg2)` syntax is in fact just syntax sugar for `Foobar::some_method(foobar, arg1, arg2)` where `Foobar` is the type of `foobar`. 
Does `release_max_level_off` Cargo feature of `log` remove the formatting code?
A somewhat follow up question, since I've been curious and confused with self referential structs. Would certainly appreciate any answers. So why the following case works out fine? ``` struct Test&lt;'a&gt; { buf : Vec&lt;u8&gt;, point : Option&lt;&amp;'a [u8]&gt;, } fn main () { let mut t = Test { buf : vec![0, 1, 2, 3], point : None }; t.point = Some(&amp;t.buf[..]); } ``` But not this ``` struct Test&lt;'a&gt; { buf : Vec&lt;u8&gt;, point : Option&lt;&amp;'a [u8]&gt;, } impl&lt;'a&gt; Test&lt;'a&gt; { fn test(&amp;mut self) { self.point = Some(&amp;self.buf[..]); } } ``` [link to playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=fb46f4d7c161fc2b2c86dd90a26f776c) Thanks!
I think this will work. See this [example](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=9c9bac93eecf3bf9452147be6de3738e).
I have had a similar search when looking to convert Markdown to PDF using templates. Sadly I haven't found an obvious solution.
This is pretty much the origin story of [WeasyPrint](https://weasyprint.org/) (which I worked on for a few years). It was even preceded with a similar SVG solution (with [CairoSVG](https://cairosvg.org/).
I believe people have been talking about taking shortcuts to make this work sooner. Like how we can stabilize async/await without needing to stabilize generators. Afaik it's still under consideration if a similar approach would be the right one to take. Mostly depending on if it's the right fit for the compiler right now.
Perhaps having a more general section on ecosystem would be nice. Tokio is not the only executor in existence. For example https://github.com/withoutboats/romio/ already supports Futures 0.3 and would probably be useful to share for people looking to try an async/await compatible executor.
Here is some feedback: 1. Track not only stabilization, but also upcoming things that are in earlier states of WIP. AFAIK, GATs will help for async (allowing async trait methods) and are planned. You could add an entry for that with a status of "not yet implemented in nightly". 2. Someone else already said this, but the heading: "Are we async yet? No ..." could be taken to imply that Rust is not capable of any kind of Async I/O at all, which is just wrong. futures 0.1 and tokio are used by a lot of software, including in production. Perhaps you should change that line to something like "Are we async yet? Yes, basic preliminary support is available and widely used, but important ergonomic language features are not yet stabilized, and the larger ecosystem has yet to stabilize &amp; standardize on modern APIs".
In your case after assigning `t.point` you cannot do anything with `t` except taking immutable references to it - but you cannot move it (because `t.buf` is borrowed), and you cannot take mutable references (because `t.buf` is borrowed, and thus that reference would not be unique). In OP's case there's already a mutable reference to `self` that was passed in by the caller of the function. After returning that reference has to be valid (because the result does not borrow it, so the called can select a lifetime of exactly this function call), so when he tries to borrow `self.buffer` and then return that borrow, the compiler complains that the borrow cannot actually extend past the end of the function (`the lifetime cannot outlive the anonymous lifetime #1`).
&gt; I do tend to pick Scala over Rust for most things, as I do believe well-written Scala creates very maintainable and correct code, but I'll always pick Rust over Scala Native. It just isn't what I'm looking for. I'm still not following why it's a different choice when it comes to native? Like, if Rust advantages (lack of nulls/exceptions and level of polish/community/ecosystem) outweigh Scala's advantages (HKT, record-like operations, typeclass derivation for custom typeclasses without having to write macros) for you then fair enough, but in that case wouldn't it make sense to use Rust for all your projects? Conversely if you think the advantages of Scala are worthwhile, then why don't they apply when it comes to writing for native?
&gt; Personally, I think there's another really big gap that isn't fully solved yet: free parallelism and concurrency. Maybe something like Erlang? I think the actor model would be a good fit for this idea. Actors could be automatically managed by the language and spun-up as necessary.
One more question : I don't understand why the following case works, which seems to be same as the original failing case ``` struct Test&lt;'a&gt; { buf : Vec&lt;u8&gt;, point : Option&lt;&amp;'a [u8]&gt;, } fn test2&lt;'a&gt; (t : &amp;'a mut Test&lt;'a&gt;) { t.point = Some(&amp;t.buf[..]); } ``` [link to playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=fb46f4d7c161fc2b2c86dd90a26f776c)
&gt;https://users.rust-lang.org/ It's \[[https://www.discourse.org/](https://www.discourse.org/)\](Discourse)
Oh I meant in the context of this kind of aliasing [cpp.sh](http://cpp.sh/9jdxz): #include &lt;cassert&gt; struct Nested { int foo; }; struct Wrapper { Nested nested; }; int main() { struct Wrapper wrapper; struct Nested nested; nested.foo = 42; wrapper.nested = nested; int* foo_ptr = &amp;wrapper.nested.foo; nested.foo = 13; wrapper.nested = nested; assert(*foo_ptr == 13); return 0; } Or consider this C# example: (dotnetfiddle.net)[https://dotnetfiddle.net/D5QlcT] using System; public struct Foo { public int foo; } public class Program { public static void Main() { var foo = new Foo { foo = 42 }; int result = AliasedWrite(ref foo, ref foo.foo); Console.WriteLine("Result: {0}", result); } public static int AliasedWrite(ref Foo foo, ref int i) { foo.foo = 13; return i; } } It would be nice if Rust could support this kind of aliasing with Cells. 
Any benchmarks? I just don't believe llvm doesn't do it already. It's a very basic optimisation every student learn about
Thanks for the reply that's really helpful. &amp;#x200B; I usually stick to about 5 lines of code per function not including variables, so I guess it's not that terrible really as not much here is repeated. Regarding point 2, I didn't realise I had defined 'guess' twice, so I removed the first definition.
It does, when the divisor is statically known. This crate could be useful when you have runtime generated divisors that are used often enough to amortize the setup cost. This claim from the docs is highly suspect though: &gt; Benchmarking suggests that for u8, u16, and u32, on a x64 windows PC, using StrengthReducedU## is always faster than naive division or modulo, even when not used inside a loop. Just throwing the example code in the README on github into some benches immediately gives me the (expected) answer, that that's not the case. I've taken a look at the crate's benches which look like this: fn $standard_bench_name(b: &amp;mut test::Bencher) { let width: $primitive_type = 200; let height: $primitive_type = 90; b.iter(|| { let inner_height = test::black_box(height); let mut sum = 0; for y in 0..height { for x in 0..width { sum += (x + y) / inner_height; } } test::black_box(sum); }); } the black_box would be blocking the compiler optimizations, but there's something else going on that's a bit wonky. Just removing the black_box doesn't make it fast, I also had to move the `width` and `height` into the closure. I don't know why that changes anything. However, with those changes the standard division is faster than the manually reduced one.
&gt; Almost any RFC in major consideration is not to add a graphics library or printer support to Rust but to add control structures and syntax you cannot implement in a crate to support paradigms yet missing. I might be wrong but I think what would be a big plus for Rust, especially for new people picking up the language like me, would be to have a practical standard library similar to Python2's standard library. Something which allows to do relatively complex common tasks with relatively little code. 
Thanks!!
Yes for the general ecosystem section. `romio` sadly is not an alternative for `tokio`. It's more like a way to experiment with `async` + IO. I do think Tokio itself deserves a prominent place in the ecosystem section, if not a section for itself. Albeit just a section that says "nothing yet, for experimenting, see `romio`".
If you remove the black box in sure the compiler will optimize away the divisions.. I'm on mobile but you can check it on godbolt.
Thanks. That makes some sense. But shouldn't the buffer be dropped if I replaced the Vec when "loading" new data? But since this struct has short life (sensitive data), reworking so the owner of the \[u8\] is other shouldn't be too much trouble, it should always be dropped at the end of the function accessing the ProtecData struct.
Not op, but the author of the article. :) Do you mean pre-fetching of the MP3 stream before serving it over FUSE? No, I don't do that. The players that I have used to test playback already implemented such a buffering scheme, so reinventing that would not have had any benefit.
Why did you choose IntelliJ over an open source solution, such as Emacs or Eclipse (which already had some work done, which could potentially be reused)?
Romio was just an example. Other examples include the futures crate which comes with an executor included. There's also standalone thread schedulers available. Dedicated solutions for WASM and Embedded are likely to pop up soon too, following in Fuschia's steps. These projects are relevant to different people, and don't necessarily have anything to do with Tokio. I think it's too early for the async ecosystem to pick favorites. I'd prefer this status page to share the range of options available, and let people pick the ones that work best for them.
Where does actix fit in to the broad overhead view, in relation to futures and tokio?
With this signature you basically say I need a mutable reference that is valid for as long as the thing it points to, which essentially is the whole lifetime of both the reference and the string. So if you call this function you cannot ever do anything with that struct, which is perfectly safe.
FWIW, `tokio` also [supports `futures` 0.3 ](https://tokio.rs/blog/2018-08-async-await/), through a Cargo feature available on nightly. The `tokio` team also [has a plan](https://tokio.rs/blog/2018-12-recap-2018/) for the ecosystem migration from `futures` 0.1 to 0.2. 
&gt;4. Have I used clone correctly? Looking at the code of the `UI` struct in `iui`, it holds a single `Rc&lt;UIToken&gt;` object. So, when you call clone, you are essentially incrementing the ref count of the `ui` instance (well technically it's member, but it has a single member). If you wouldn't clone the `ui` object and `move` it into the closure, your code would probably wouldn't compile because the `ui` object outside the closure "wouldn't exist anymore". The closure would be the owner.
Solved partially using this. But the API readability is a bit worst, it needs to first load the \[u8\] and then create the struct, and that was what I was trying to avoid, I was trying to make it cleaner and hide part of the \[u8\] buffer. pub fn new(buf: Option&lt;&amp;'a [u8]&gt;) -&gt; Self { match buf { Some(x) =&gt; ProtecData { data: Some(get_root_as_data(x)) }, None =&gt; ProtecData { data: None } } } &amp;#x200B;
Actors are a powerful abstraction, for sure. Definitely read about them under the "Actor Model". It's probably also a good idea to listen to some of Joe Armstrong's talks, who is a main creator of Erlang - probably the earliest production grade actor system. And Futures and Actors do compliment each other well, but it might be overkill for your use case if all you need is a learning experience around Async in Rust. You can always check out Actix later, since that project is pretty boss. Best of luck to you, we're here if you have questions:)
Yea, I use a R2D2 pool for my db connections as this seems to be the de facto way.
What relatively complex common tasks don't have crate support? Theres an adage in programming languages that the standard library is where code goes to die. Once adopted a signature in any standard library is practically immutable. Rust might have edition boundaries to possibly change these things but there would be an immense amount of hesitation to ever revise a signature in std. Some great examples are configuration variables, command line argument parsing, and logging. Rust has dotenv, clap, and log respectively to handle those use cases. They aren't in standard so they can still see revision and iteration over time but if you search for any of those terms (dotenv is the hardest to find) on Google these crates are in the top five search results. Javascript is a great example of the *worst* of both worlds. There is the massive nodejs package manager that has infinite packages for every simple task but the language itself isn't based on node, its baked into web browsers. So ECMAScript will keep iterating and expanding its standard library and syntax (quite exponentially in recent years at that) leading to swathes of the node package repo being made redundant while the design of these APIs is functionally set in stone permanently in the language specification. To get into Node programming you need to basically know all of the baseline Javascript which is nowadays about as complex as C++ (if not as semantically mind destroying) to know what you actually need from your node packages. Then you also need to know if those node packages use standard functionality or depend on other packages implementing standard functionality in a different way. 
&gt;They published the gtk-rs crate to support officially the language. What do you mean? I can't find any mention of gnome on [https://gtk-rs.org/](https://gtk-rs.org/) .. 
I am not sure about the argument. I agree that the Javascript solution isn't acceptable. You could however use the same argument to defend the C++ standard library compared to Python but one has to admit that Python's library is more supportive when solving common tasks. Having many different implementations does not necessarily improve the quality of C++ libraries. I am not against versioning a standard library, or having different levels. It is true that it is not necessary to have library functions as a fixed package, one can divide them into crates. Python's library is evolving, too. However one important question is how to make different library functions cohesive and working well together. 
&gt; I usually stick to about 5 lines of code per function not including variables I don't think functions in web-centric JS code and functions in application (or even system) Rust code are really comparable. It's a different kind of thing. In Rust a 5 line limitation would be pretty extreme. You will find a lot of Rust code that has much longer functions but is still idiomatic and well-written. In programming languages like Rust et al. functions will typically be somewhere in the range of a few lines to few dozen lines, usually within 100. But even several hundreds of lines per function might not be entirely off the table in some cases, especially in algorithmic code. 
&gt;impl&lt;'a&gt; Test&lt;'a&gt; { fn test(&amp;mut self) { self.point = Some(&amp;self.buf\[..\]); } } A working load version, with proper lifetimes. The missing lifetime on trait definition was the problem. Also using always the same buffer (only clearing the contents, to avoid any leaks). impl&lt;'a&gt; Load&lt;'a&gt; for ProtecData&lt;'a&gt; { fn load(&amp;'a mut self, filename: &amp;str) -&gt; bool { // We need to store the buffer, flatbuffers is a view into the raw data load(&amp;filename.to_string(), &amp;mut self.buffer); let buf: &amp;'a [u8] = self.buffer.as_slice(); let result = get_root_as_data(buf); self.data = Some(result); true } } &amp;#x200B;
crossbeam-channel: https://github.com/crossbeam-rs/crossbeam/tree/master/crossbeam-channel
Have a look at the crossbeam-channel crate!
The macro in std is deprecated because it is now covered by [crossbeam](https://github.com/crossbeam-rs/crossbeam/tree/master/crossbeam-channel) which I believe is superior in most ways? Have you looked into crossbeam at all?
Thanks very much for your replies! I am still wrapping my head around the ideas. I might message you via reddit some time later to confirm my understanding if that's okay.
Sure I got one, if you don't mind. I ask, because perhaps I'm going about this backwards - coming in with assumptions. I'd rather design my program, but it's difficult to design when parts of Rust are black holes to me. So; my question is about rust application design; ensuring that I don't back myself into bad patterns or non-Futures friendly code. Let me start off with some pseudo-Go looking code, to give some framing: ``` server.bigCache = map[string]BigData{} func (s Server) handler(req Request) Response { if s.bigCache[req.Key] == nil { loadData(s.bigCache, req.Key) } // do stuff with data ... } ``` Odd code, but it's just for context. With each request I'll be taking a `bigCache` and checking if the requested data is loaded. If it is not, we load the data before acting on it. This means that the handler is going to be performing async, mutable actions inside of `loadData`, and caching them into `bigCache`. This is well and good, but it means that concurrent requests need to have some form of locking - right? So lets step back from Actix.rs for a second; Since I'm aiming towards supporting futures, I assume classic Mutexes are still bad, right? Do futures provide locking? Is https://crates.io/crates/qutex a bad solution here? It's odd, Rust makes so much sense when dealing with single threaded, but once I go multi threaded, I have no idea how to write anything. Locks (and variations of Locks) have always been a straight forward answer, and now I'm left without them.. I'm quite confused lol. Anyway, I appreciate your time, and hopefully my locking question made sense - I just need some direction. This `bigCache` is effectively an in-memory DB with large read-only datasets, and I'd hate to write it in such a way that I have to throw away a bunch of code down the road.
Interesting. Although the way Python and GTK via FFI is used sort of precludes the self-contained goal :( It seems this would be a similar situation to the Latex approach. 
Inspired by Advent of Code (and because I needed an idea for my first procedural macro), I spent the last few days writing a macro, that derives a \`FromStr\` implementation based on a regex provided by macro attribute: [https://github.com/df5602/adhoc\_derive](https://github.com/df5602/adhoc_derive) Example: struct InnerRectangle { x: usize, y: usize, width: usize, height: usize, } impl InnerRectangle { fn new(x: usize, y: usize, width: usize, height: usize) -&gt; Self { Self { x, y, width, height, } } } #[derive(FromStr)] #[adhoc(regex = r"^#(?P&lt;id&gt;\d+) @ (?P&lt;x&gt;\d+),(?P&lt;y&gt;\d+): (?P&lt;width&gt;\d+)x(?P&lt;height&gt;\d+)$")] struct OuterRectangle { id: usize, #[adhoc(construct_with = "InnerRectangle::new(x, y, width, height)")] rect: InnerRectangle, } let rect: OuterRectangle = "#123 @ 3,2: 5x4".parse().unwrap(); assert_eq!(123, rect.id); assert_eq!(3, rect.rect.x); assert_eq!(2, rect.rect.y); assert_eq!(5, rect.rect.width); assert_eq!(4, rect.rect.height);
@killercup I have had a look at this and it seems very neat. One problem (which I failed to mention) is one of these channels described above is a broadcast channel: [https://github.com/jonhoo/bus](https://github.com/jonhoo/bus) From what I can see, crossbeam does not support broadcast channels? And hence crossbeams' select macros won't work :(?
Minor observation: &gt; For example, the length of "🇨🇦" is 8 in Rust, and 2 in Python, but you really want it to be 1. The crate unicode_segmentation is pretty helpful with that. You can do this pretty easily without an external crate -- the length in terms of number of characters is `whatever.chars().count()` and an individual character is `whatever.chars().nth(n)`, though as those are both `O(n)` it might be useful to collect them into a vector or something first if you're going to be doing it multiple times. The `char_indices()` method on strings is also helpful if you're going to end up, say, splitting the string into pieces, and want to figure out where the boundaries are without actually allocating the vector of all the characters.
The thing is it's quite nice having the stuff the "almost nobody uses anymore" stick around and be maintained. Backwards compatibility is generally a nice thing because "almost nobody" generally reveals itself to be "most people occasionally" when you break it. And if having old code around just makes you feel icky you could always have some kind of versioning system where libraries are actually removed if you say you support a new version but merely deprecated if you done. We could call it "editions" or something.
This looks neat! Any chance for support of integer based log\* ? 
Why is this fine (with fn fetch_word(&amp;mut self) -&gt; u16 being defined elsewhere): fn ld_nn_sp(&amp;mut self) -&gt; u8 { let n = self.fetch_word(); self.mmu.write_word(n, self.regs.sp); 20 } but with this: fn ld_nn_sp(&amp;mut self) -&gt; u8 { self.mmu.write_word(self.fetch_word(), self.regs.sp); 20 } I get "cannot borrow *self as mutable more than once at a time"
You could implement a broadcast channel on top of crossbeam: create a thread which reads messages from a channel of the form: enum BroadcastAction&lt;T: Clone&gt; { Send(T), Subscribe(Sender&lt;T&gt;) } When you want a new broadcast receiver, you create a channel, and send the "Subscribe" message with the sender end. The background thread receives messages from the channel: if it is a "Subscribe" message it adds the channel to a list. If it is a "Send" message it iterates the list sending the message to all channels. If it tries to send a message to a closed channel, it removes the channel from the list. When the main broadcast channel is closed, the background thread can stop.
I thought they were going to change the name of Unpin due to it being confusing?
Sounds interesting! There's already a ChordPro implementation in Rust called [chord3](https://github.com/kaj/chord3) but it only outputs PDF. I'm wondering what's the use case and audience of the software? I would assume that most users simply want a pretty PDF or HTML file. I think that your solution with templates and LaTeX is too complicated. Do the templates provide much value to regular users or only you, the developer? I would just write some ugly code to generate the PDF and HTML files. For more customization options, I could also output things LaTeX but keep it optional.
It’s on top of everything 
Yeah, no argument there. I was just mentioning this as the simplest solution I know of for small data. It comes up surprisingly often and this served me well so far.
This won't work - after calling `load` you won't be able to do anything with `ProtecData`.
Yes, which is my point: you'd need a separate marker trait to distinguish those types, and possibly a Type/Field trait (not a thing in Rust) to identify the sub-list of fields which can safely be "rented" out.
That looks like a really great project, I'll take a look at some of the pull requests and ask for guidance for the ones I find most approachable soon. Thanks!
Proposal: don't add a feature to enable no_std, just be permanently no_std.
They aren’t required to just write async code. But it seems like they are if we want to build zero cost abstraction around async functions and types. Eg I stated a discussion here about some traits on top of futures which are currently not possible: https://github.com/rust-lang-nursery/futures-rs/issues/1365 
This is the subreddit for the Rust programming language. You’ll want to check out /r/playrust instead.
my bad thanks
I followed that tutorial as well, so [the end result is pretty similar](https://github.com/DamianX/memevm). It's very rough around the edges, as I didn't originally intend to publish this but since you posted I figured, what the hell, I'll do it. I didn't use unsafe because out of laziness I just accepted having to type ENTER after entering a character :)
Crossbeam-channel is multi producer *and* multi consumer fyi
Very true, but, from what I experienced when tinkering with crossbeam is that: if one of the consumers grabs the message with their recv() that message will disappear for the others (whereas I want each consumer to have an individual inbox - broadcast). 
I see, thank you. It'll take me a while to work out the exact implementation details. I was hesitant about crossbeam but looking at the benchmarks it seems strictly better than mspc (and bus?) in nearly every scenario (apart from the size of the code base and the contrary to the philosophy of "no garbage collectors"?)
If you are owning the list, then you want Vec. Slices just point to data somewhere else. Which is why every reference or slice has a lifetime parameter, which tells the compiler how long the pointed-to data lives. Each reference (or slice) cannot be used after the lifetime of the pointed-to data ends.
Rust can't and won't drop the buffer if the Vec is "replaced". You can't replace the Vec. The buffer holds a reference to it. Think of Rust references as incrementing a ref count. As long as the ref count &gt; 0, then the original data being borrowed will not be dropped. Borrowing != ownership, btw. The Vec *owns* the data the buffer is borrowing. Make sure you work through the Rust book and its examples before trying to do more code. I know, seems tedious, but it will make the ownership/borrowing paradigm more sensible. 
A few months ago, [Chalk implemented GATs](https://github.com/rust-lang-nursery/chalk/issues/116). Last week, a minimally working implementation of the new-style trait solver (Chalk) was [merged into rustc](https://github.com/rust-lang/rust/pull/56384). Currently, it can be used with the command `-Z chalk`, but keep in mind, the integration is a work in progress. (afaik) although it exists, the GATs feature gate is not supposed to be used _yet_. - [Tracking issue for GATs](https://github.com/rust-lang/rust/issues/44265) - [Issues tagged with chalk-integration](https://github.com/rust-lang/rust/issues?utf8=%E2%9C%93&amp;q=label%3Achalk-integration+)
The difference is that C++ doesn't have a functional module or package system. Rusts Cargo is one of its greatest strengths and the ease of use of crate dependencies in Rust vs other languages makes comparisons less one to one when discussing what should be "batteries included" in std. The Rust team has also done a fairly good job "adopting" fundamental block crates into the rust-lang-nursery. These are crates that see widespread community adoption and are as close to "official" as you can get without freezing the API in std. There is an even closer level of proximity to std in official crates. Right now proc_macro is the only major one shipping but its basically a crate adjacent std that the Rust team ships but is still its own crate.
This looks all kind of awesome! Sneak preview of a simple invariant: #[pure] #[requires="0 &lt;= n &amp;&amp; n &lt; length(r)"] fn get_nth_x(r: &amp;Route, n: i32) -&gt; i32 { if n == 0 { r.current.x } else { match r.rest { Some(box ref r) =&gt; get_nth_x(r, n-1), None =&gt; unreachable!(), } } } Sneak preview of a complicated invariant: #[requires="0 &lt;= n &amp;&amp; n &lt; length(r)"] #[ensures="result.x == old(get_nth_x(r, n))"] #[ensures="afterExpiry&lt;result&gt;( length(r) == old(length(r)) &amp;&amp; get_nth_x(r, n) == before_expiry&lt;result&gt;(result.x) &amp;&amp; forall i: i32 :: (0&lt;=i &amp;&amp; i&lt;length(r) &amp;&amp; i != n) ==&gt; get_nth_x(r, i) == old(get_nth_x(r, i))) "] fn borrow_nth(r: &amp;mut Route, n: i32) -&gt; &amp;mut Point { // ... } Beyond `requires` and `ensures`, you also have `pure` and `trusted`. Also, beyond formal verification, the paper mentions the possibility of eliding overflow checks, for example, maybe not realizing that this can be taken a LOT further. The absence of overflow means the ability to use `nuw` and `nsw` (no unsigned/signed wrap), which imply a guarantee to the optimizer that the operation does not overflow, and therefore places guarantees on the range of the *output*. In the same vein, if a function is proven never to `panic!`, then all branches leading to panic can be eliminated, and all checks to take those branches can be eliminated. This both improves performance (less check) and reduces code size (no panic code, no entry in unwinding tables). This would be of particular interest for: - Eliminating bounds checks. - Eliminating `unwrap`/`ok` checks. So, not only do you get a safer software, you also get a faster and lighter one. I am really looking forward to the ongoing work here! Ada/SPARK better starts watching its back!
If a crate has something listed under its `[features]` section that should be possible to enable by specifying some `new_crate = { version = "=$list_version", default-features = false, features = ["$WANTED_FEATURE"] }` right? The problem is I'm attempting to build the `reqwest` crate with the string `reqwest = { version = "=0.9.5", default-features = false, features = ["rustls-tls"] }` and `cargo` is telling me that `rustls-tls` isn't a feature, but the [`Cargo.toml`](https://github.com/seanmonstar/reqwest/blob/master/Cargo.toml#L50) seems to disagree?
/r/playrust?
Chars gives you code points though, which will give you 2 for the flag.
LLVM does a lot of these kinds of tedious strength reduction optimizations many of which are more complicated than these (e.g. detecting a bitwise rotate idiom and replacing it with a rot instruction)
I'd say even in rust, 120+ line functions can/should usually be split up. But I agree that 5 lines is way too short a bound- the function header/type declaration itself can be 5+ lines for some of the more complicated generic functions... A shorter bound like 40 lines _might_ be ok too, depending on what the code's doing.
After further thought, this scheme seems to be 1:N broadcast channel becomes a N separate 1:1 channels? I'm not familiar with the internals but it seems this could scale quite badly?
sounds great, thank you
Broadcast channels are inherently not the most scalable things. You'll have to try it for yourself, but the 1:1 channels themselves don't have very much overhead.
Purely functional programming languages like Lisp or Haskell use (persistent data structures)[https://en.wikipedia.org/wiki/Persistent_data_structure] wherever possible. A singly linked list is persistent (that means, you can prepend an element in O(1) without mutating the original list, so it is effectively immutable). An array/Vec is not persistent, unless you clone it every time you mutate it. This is against functional paradigms.
Near the end, that first post says &gt; First, the tokio-async-await crate only provides compatibility for async / await syntax. It does not provide support for the futures 0.3 crate. It is expected that users continue using futures 0.1 to remain compatible with Tokio.
Man, as a new (wish) Rust user, this shit is endlessly confusing lol. I've been gone about a year, and work has me coming back into Rust, and the unstable nature of things has me feeling just like I did when I left. It's so young, and I don't know where the landmines sit or what I should or shouldn't touch. Really tough as an outsider.
This can be encoded into a generic adapter type implementing the Write trait and internally buffering like BufWrite does.
Ooooh. Well, my bad, I stand corrected! I think it got mixed up in my mind when reading that part of the plan: &gt;This adds simultaneous support for both async/await and futures 0.1. Then, async/await becomes the primary API and futures 0.1 can be used via a compatibility layer. I assumed that support for `async`/`await` means support for futures 0.3, because, if I understood correctly, an `async` function returns an opaque type (backed by a generator), which implements the nightly `Future` trait from the `std`. But, again if I understood correctly, futures 0.3 is just the nice combinators and API around the core primitives from the `std`. So, I don't get how one can support `async` functions without supporting the more general case. I might need to dig a bit more in all this!
Thanks for your responses. I'll get tinkering.
Thanks, your repo looks super useful.
&gt; behaves like xmonad Not in Rust, but there's https://github.com/waymonad/waymonad
MPMC is different from broadcast. In the broadcast case, each message is dispatched to *all* consumers, not only to the *first*. In a sense, broadcast is more akin to a Kafka queue, where each client remembers that it's ready up until item "5", and all will therefore read item "6" when (finally) available.
Short functions can also be harder to read, because you need to jump around to the function definition to check what it really does instead of going with the flow and reading the whole function from top to bottom. At the end of the day it's a tradeoff.
I've rendered directly to PostScript before now (a long time ago), and enjoyed working with it. Since PDF encapsulates a subset of PS, I wonder how hard it would be to output to PDF directly, perhaps an uncompressed and very simplistic PDF, but standards-compliant nevertheless. Or perhaps look over some PDF generation code from other projects to see if you rip a small subset of that code out. Maybe you don't need layers and layers of libraries.
I vote for the answer starting with "No". There is still too much confusion and I believe only a few advanced developers are using async rust with the full understanding of how it is working. It is absolutely OK to wait a couple of weeks or months and answer "yes" when async rust is clear and fully supported.
This can be used in cases where you don't know of the value at compile time. For example if you're making a hash map and need to map the hashes into an array, one would often compute the index modulo the array length. In this case the array length will vary depending on the number of items in the map, and isn't known at compile time, but since resizing is rare, this crate could be used to speed to up the modulo.
Then you don't want what exact shifts you should do. In this case conditional branching will kill all the profit
I will reread the book regarding Borrowing, it was about 2 years the first time, and since then I worked little with Rust. This was the first big project I decided to port to Rust, and more will follow, so learning by experience is the best way ;).
Haha sorry :D
I think the first one works because the borrow &amp;t.buf[..] is 'static, while the borrow of &amp;self.buf depends on the lifetime of &amp;mut self. From my experience, every time I start throwing explicit lifetime declarations everywhere it's almost always a sign that I should rethink how I'm modeling the problem. Eg.: In your test, you could store the bounds of the slice instead of the actual slice, and just create the slice on demand where you need it. ``` struct Test&lt;'a&gt; { buf : Vec&lt;u8&gt;, start: usize, end: usize } impl&lt;'a&gt; Test&lt;'a&gt; { fn test(&amp;mut self) { self.start = 0; self.end = self.buf.len(); } fn use_slice(&amp;self) { let slice = self.slice[self.start..self.end] } } ``` You can also replace the references with reference counters, like Rc. Or you can just cheat and use *mut pointers instead. Just don't tell anyone I told you that.
Oh good call.
Rust standard hash map prevents DOS attacks at some performance cost. Since you said that security is no concern for you, I'd try some other community created hash maps instead of creating an own hash map implementation. [hashbrown](https://github.com/Amanieu/hashbrown/) shows some promising numbers, a [crates.io](https://crrates.io) search also yields some more hash map crates. &gt;... Rustacean's preferred way to solve this problem. I guess the preferred way would be to create a benchmark and compare the standard map with whichever alternative you lay your eyes upon. Also, if the hash map is used in many places, a type alias like `type MyHashMap = std::collections::HashMap&lt;usize, String&gt;;` may be useful. It allows for changing the type quickly for experiments. Small note: One of rust / cargo's major strengths is that other crates can be integrated without much work. In contrast to languages like C(pp) I wouldn't be too hesitant to depend on other libraries. &amp;#x200B;
Ah, thanks for elaborating. Agreed.
I would assume that statement is meant to mean to only apply to divisions with runtime divisors, that is, ones that the compiler cannot optimise, and is thus comparing the CPUs division instruction to the crate's version.
This crate seems to demonstrate that's not true. It calculated the shifts (etc) needed at runtime.
You don't actually need to branch in order to divide using this crate. Remember this crate has a setup cost that would be paid at every hashmap resize, so the appropriate shift is known when you need to do the modulo. In fact you can see the code performing the division [here][1]. [1]: https://github.com/ejmahler/strength_reduce/blob/master/src/lib.rs#L92-L101
That's true in the context of async. Thankfully the rest of the language and ecosystem is pretty stable and you can create large projects in rust not even touching async :)
This is an interesting idea! In fact, it seems PostScript just might be enough, since it has the [stringwidth](http://www.math.ubc.ca/~cass/courses/ps.html) command which is the key piece missing from SVG needed to align chords with lyrics properly. And also there seems to be a rust [ghostscript binding](https://github.com/albel727/rust-ghostscript). I could generate PostScript and then use ghostscript to convert it to PDF. Thanks for mentioning this, I'll look into PS. 
Okay, I buy it. &amp;#x200B; But if it's generic optimization, isn't it implemented in LLVM? Or if it isn't, then why? I do believe that such an optimization isn't implemented because it has different tradeoffs and you lose performance in some most scenarios while winning in some others.
I believe your problem is that your program ends. When the main thread finishes, operating systems tend to kill everything. There isnt enough time for them to run before it dies. You should add a way to join the queue, waiting for it to finish up at the end of `main` As a quick hack, you can add `std::thread::sleep_ms(1000);` to the end of main and see that everything appears to runs correctly now.
&gt; Sounds interesting! There's already a ChordPro implementation in Rust called chord3 but it only outputs PDF. I think I've seen this before, but I wanted to reinvent the wheel myself :) Also, my implementation is a bit more project-oriented, ie. it doesn't just process individual files on the command line, but it first reads a TOML file (á la cargo, Zola, and others) which specifies the sources and configuration. The thing about existing single-file-oriented ChordPro implementations is that you end up having to use a Makefile or something to supplement this. &gt; I'm wondering what's the use case and audience of the software? Me primarily, but and also potentially friends &amp; family... &gt; I think that your solution with templates and LaTeX is too complicated. Do the templates provide much value to regular users or only you, the developer? I understand what you mean and yes, integrating with LaTeX would make it complicated. Templates I think are fine, the HTML template for eaxmple is pretty simple and easily customizable. Especially since the template engine is integrated and doesn't require anything external. My motivation is basically the experience with ChordPro. You start with some simple dirty PDF rendering and then you figure you want various fonts to be customizable and then someones asks for the positioning or alignment or whatever to be customizable and so on and so on until the configuration grows into huge proportions like with ChordPro, where it basically _bacame_ a sort of a template, but just sort of. 
In `Drop` impl the lock on flag lasts until the end of `drop` - you need to wrap `let mut w = flag.write().unwrap(); *w = false;` in a block. Otherwise when you try to join a thread then it cannot actually read that flag to know that it is supposed to exit. But even with this fix, I think you can still deadlock: 1. You create a new `DispatchQueue` and initialize it. 2. You immediately drop it. `DispatchQueue` notifies all threads blocked threads (which still haven't started yet). 3. Worker thread finally starts, locks job queue, waits on the lock, but never gets notified because it started too late. 4. `DispatchQueue` tries to join worker threads, and deadlocks as the threads are still waiting to be notified. Also, for this you don't even need to drop `DispatchQueue` before worker threads start - just give it some work to do. Like this: 1. You create a new `DispatchQueue` and initialize it. 2. Enqueue a function to run, workers get notified. 3. One worker pops that function from work queue, starts executing it. 4. `DispatchQueue` gets dropped - sets the flag, notifies blocked threads, and joins them. 5. Worker finishes executing the function, locks queue and waits for notification. 6. Thread is stuck waiting for notification, main thread is stuck on waiting to join it - deadlock. Similarly, if there's still work to do when queue gets dropped, available workers will pop those and run them, and after that will get indefinitely stuck waiting to be notified again.
`drop` will most definitely get called when `q` goes out of scope. An on the playground you can see that his program does get killed because of a timeout.
Actually, LLVM already uses numeric range analysis in its optimizer, and can e.g. elide bounds checks on array access. I have once used `assert!()` as an optimization because of that :) It would be interesting to compare the LLVM range analysis against the one in Prusti, by the way. It is entirely possible that they are pretty much equivalent and LLVM already performs all the optimizations. If not, Prusti could be used to eliminate the runtime assert and pass hints to LLVM directly.
Oh, i guess you're right, i didnt notice, and everything seemed to have print correctly.
I think the key is that in the benchmarks, the divisor isn't known at compile-time, whereas in the readme example, the divisor *is* known at compile time. I can add a clarification that this is intended for cases where the divisor isn't known at compile time.
How to implement work stealing threading with a bounded queue? I have several slow consumers and a single fast producer which I want to slow down to decrease memory consumption. But it should be fast enough to keep consumers always busy with tasks. In python I do this with `queue.Queue(maxsize)` and when the queue is full `queue.put(msg)` is blocked until some messages are taken from the queue. In rust I see several libraries for concurrency and I'm confused about which one is good for my task.
The `Vec` is still dropped at the end of the scope, because nothing took over its ownership. You need to `std::mem::forget(it)`.
Before nll you have to use blocks to unborrow
Try [crossbeam-channel](https://docs.rs/crossbeam-channel). Its `bounded` function might be what you want.
I use my own CVec/CVecView types to pass Vector out/in C++ respectively. See [https://github.com/nebgnahz/cv-rs/blob/master/src/lib.rs#L104](https://github.com/nebgnahz/cv-rs/blob/master/src/lib.rs#L104) and [https://github.com/nebgnahz/cv-rs/blob/master/native/utils.h#L15](https://github.com/nebgnahz/cv-rs/blob/master/native/utils.h#L15)
Not op, but another question: why not request for a certain read, just the frame(s) being read instead of requesting everything? Also, why not consider caching using, say, a file-backed store?
\&gt; Rust makes so much sense when dealing with single threaded, but once I go multi threaded, I have no idea how to write anything. &amp;#x200B; **Async** in Rust is still a mess right now, stay away from it. But **multi threading** is straightforward and safer than most languages. Have you read this: &amp;#x200B; [https://doc.rust-lang.org/book/ch16-00-concurrency.html](https://doc.rust-lang.org/book/ch16-00-concurrency.html) &amp;#x200B;
I just made it clearer that the "no" is for async syntax, and clarified that async IO is possible. Hope it's better. The main motivation for me was really just to track the async syntax :)
I'd be very interested in that as well, but it's not clear to me how to track that (i.e. where to find the information, and how to make it clear). Suggestions welcome.
async trait methods are currently not listed, since there isn't even a section in the RFC for it at the moment... But yeah, I guess I probably need a separate section for that given its own dependencies.
Brilliant! Thanks a lot! 
How does it compare with [wlroots-rs](https://github.com/swaywm/wlroots-rs) (beside being in pure Rust)?
Have you looked at ReportLab (Python)?
No, it does not. It disables logging, but the formatting code appears to stay.
Actix could be considered loosely a BEAM for Rust.
The problem is that you are trying to create a self-referential value: `self.turn.player` contains a reference to `self.players`. Rust currently doesn't support this, because in theory, moving a self-referential value would invalidate its references to itself. Of course, this doesn't happen in your code because moving a `Vec&lt;T&gt;` doesn't invalidate references into its elements, but the borrow checker isn't aware of that. You can fix it by having `self.turn.player` be a `usize` index rather than a reference. If you want slightly more type safety, you can wrap it in a newtype (e.g. `struct PlayerIdx(usize)`).
It targets more or less the same feature space, though currently wlroots is much more mature than smithay. As such, most of the abstractions provided in smithay are more bare-bones at the moment. The overall structure is a little different too. Right now smithay presents itself as a set of very-loosely-coupled low-level modules. We plan as part of the next steps to introduce higher-level abstractions on top of these low-level modules. Wlroots provides a more unified API, and already contains higher-level abstractions (notably regarding the actual drawing to the monitors).
We are advancing at our own pace, given for now Smithay is mostly /u/Drakulix and me. Any help would be very welcome!
It seems to me that easy fix is to notify in loop inside drop until all threads exit.
That will leak the allocated buffer, though.
Version 0.2 is out, and it's unconditionally marked `#!no_std]`
Version 0.2 is out, and it's unconditionally marked `#!no_std]`
You're an angel
Thanks for the advice! Version 0.2 is out, and it's unconditionally marked `#![no_std]`, with the changes you suggested.
That's great to hear! Glad the error messages are helpful to you!
This is definitely the source of the deadlock. A similar race could happen later after startup. The worker checks `flag`, see's that it's still true, and then immediately after the check (before the worker gets back around in the loop to `wait`), the main threads sets `flag` and calls `notify_all`. Then (too late) the worker gets to `wait` and sleeps. I think the underlying issue here is that this isn't quite how `CondVar` is supposed to be used. Whatever flags or state you need to share between threads should really be inside the one `Mutex` that's associated with the `CondVar`. Having a separate flag like we do here makes it more likely to have bugs like this where things interleave unexpectedly or get checked in the wrong order. Random side thought: I don't think there's much reason to use a `Vec&lt;Option&lt;...&gt;&gt;` that gets initialized with a bunch of `None` values. Why not just use a `Vec&lt;...&gt;` of thread join handles and initialize it to empty?
&gt; Thankfully the rest of the language and ecosystem is pretty stable and you can create large projects in rust not even touching async I'd actually like this, but I don't know what that is, or what that looks like. Mind elaborating? Eg, I'm architecting *(trying to)* our Go server(s) in Rust. This means I need to write what is effectively a custom in memory DB. Which means I need to write a struct that spans multiple http calls, and thusly I'm dealing with async junk. Now, my plan so far is to make a movable (Send or Sync) struct, with Mutexes on it for the shared mem stuff. If that works, there are downsides in that a Mutex will lock the full thread - so in the future I'll change the mutex to use Qutex _(or something like it)_. Note that all this Mutex talk is because it's locking GBs of in-memory data. Luckily that data is sharable _(readonly)_, but I still need to mutate some in-memory storage when it's first requested. In the end, I'm having to deal with Async-related world. Does any of this sound like I'm way off base? Or was your comment mainly targeted at other types of applications, such as CLI/GUI/etc apps?
Is there any point avoiding `impl trait`?
Yea the concurrency side of Rust is pretty understandable so far - this Async stuff has been the more confusing aspect. I appreciate your comment. I need an HTTP server though and I'm toying with Hyper.rs. It seems though that even Hyper uses Async - should I stay away from Hyper too? In your "stay away from async" concept, I mean. My immediate plan was to use Hyper and largely ignore Async _(even though it uses futures)_. I was going to treat Futures as a thread, and mutex the thread at the cost of performance/etc. In the near future I could then try switching my locks to Async locks (Qutex/etc). Does this sound off base?
As you said, "... it's largely a synchronous process. Nothing fancy is needed at this time" then actix-web is the wrong tool for your project. I recommend Rocket instead. Don't kill yourself with futures if you don't have to. Soon, async/await will abstract away their complexity for common asyncio control flow. 
You can name the return type. Which is nice, I think.
Oh, but that second one is also really interesting! "expected bool, found usize" -- does that mean rustc went ahead and continued compiling *as if you wrote len()* and it found that you can't use its return value is not a boolean? It does it leak the type of the private field? [It looks like it's the latter!](https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2018&amp;gist=5bca9cfb20ef8a1a4d637932a742510b) (Is this an issue?)
I feel like if things are split out when it makes sense it just naturally ends up being reasonable. I have had some data structure code that did end up being pretty interconnected and couldn't be split out at around 100 lines, but otherwise I feel like long functions just aren't an issue generally unless someone is purposefully not separating things out. Then again, maybe I'm a little too aggressive in separating things out that seem conceptually modular, but I think that principle is generally (though not always) good. It also gives opportunities for documenting things, even if they are internal. Internal docs do exist, though few use them.
Did you run clippy?
Appreciate the advice! Do you think Rocket (no Futures) will be easier than something that uses Futures like Hyper.rs? Disclaimer: I know Hypers is not a friendly web framework - I'm using it as an RPC base, so I'm not at all concerned about it being framework-y. However, Hypers (at least, the version I'm using) is using Futures. Should use a non-Futures version of Hypers? Or can I simply not use more Futures, and thus I basically don't use Async. In other words, because the latest versions of Hyper.rs uses Futures, should I stay away from that version? Will it .. affect my code in a negative way, even if I don't use Futures in my own code? Thoughts?
Hmmm don't think that's an issue. I am new to Rust but it seems that compiler (or is it checker first?) works like tests without bail strategy if you used it. So it won't stop after it gets the first error.
That it continues to compile instead of exiting at the first error is great! The issue is that you can observe the type of a private field :)
LLVM only performs this optimization when the right hand of the divide/modulo operation is known at compile time. As for why it isn't automatically added for hash maps, it boils down to two things: 1. This optimization only helps if the right hand side of the modulo operation changes rarely. 2. Adding this optimization to a hash map would add three fields to the struct, since the result of the setup must be stored somewhere. Both of these stop LLVM from performing the optimization. It has essentially no way of figuring out how often the modulo would be used compared to the resize. In fact if you called [shrink\_to\_fit](https://doc.rust-lang.org/std/collections/struct.HashMap.html#method.shrink_to_fit) after every insert, the reallocations would be very common, and you'd have to pay the setup cost very often, making it a bad optimization. It's just that with hash maps we're lucky, since reallocations are very rare. A reallocation on a hash map typically doubles the number of elements it can contain, so with 40 reallocations we've already used a terabyte of memory. On the other hand, a modulo is required for every `get`, `insert` and `delete`, so clearly those will be way more common. Figuring out these things requires some mathematical analysis on the doubling behavior of the size along with the knowledge of `get` and `insert` being a lot more common than `shrink_to_fit`. There's also the trouble with adding fields to the struct. The compiler will never change the layout of the struct in such a significant way. At most they reorder your fields. In conclusion, LLVM doesn't perform the optimization because it's a really big and complicated one, with tradeoffs on the setup cost that need to be considered. Note that when it's done at compile time, the setup cost is paid at compile time, and thus will always be an improvement when the right hand is known to the compiler. &amp;#x200B; Side remark: In some hash map implementations they actually have a hardcoded list of possible sizes (maybe they have 40 sizes or so), and the modulo is performed as a large switch statement looking something like this: match (len) { 3 =&gt; i % 3, 9 =&gt; i % 9, 26 =&gt; i % 26, 41 =&gt; i % 41, ... (I picked these numbers at random) } With this method the compiler can perform the optimization to each branch at compile time, and this often turns out to be faster than a simple `i % len` despite the branch. Of course this means you are limited to a small number of possible values of len, which wouldn't be a problem with this crate. A different approach is to always use sizes that are powers of two, in which case the modulo can be replaced by the bitwise and operation, which would be much faster than any other method you can come up with. Unfortunately powers of two often result in large numbers of hash collisions with many hash functions, which are far more expensive than the modulo, and stuff like Fibonacci numbers etc. are used instead.
Anyone here can explain how the compiler knew about that? How can one implement that message when someone tries to do something like this with a library when the user tries using something private? 
Please remember that async only matters for a subset of people who actually use rust.
Because the property and the method shared the same name. If they had written `.size`, there would not have been a helpful message.
Doesn't seem like too much of an issue to me. Privacy in Rust doesn't mean secret; you can go to the stdlib source and find out what goes on inside a `Vec` if you like. It just means that the variable is a key part of upholding `Vec`'s invariants and so it doesn't want you to mess with it directly. ...Even though it does provide an unsafe `set_len` method in the event you really *do* need to mess with it directly, but that's beside the point =P
It looks like you have a bunch of unneeded `Box`es in your types. For example take a look [here](https://docs.rs/msoffice_pptx/0.1.1/msoffice_pptx/document/struct.PPTXDocument.html). As far as I can see, no `Box` mentioned on this page is necessary, and can be removed without changing much.
Agreed, it's great that it's not exiting after first error! Honestly, not sure if that's an issue, but it's somewhat confusing when you get to see/use private props and methods.
Yeah, Futures/async is quite the rabbit hole though... What I will recommend is this stream from Jon Gjengset: https://youtu.be/9_3krAQtD2k It's long, but does give a good understanding on how all the pieces fit together and explains how the OS does async itself, how Mio handles the lowest level interfacing I/O operations with the OS, how Futures is a structure with the necessary pieces to enable running async code in Rust but does nothing themselves, and how a executor (like Tokio) is needed to actually schedule execution and drive the whole process to completion. P.S. Disclaimer I'm still learning myself so if there's any incorrectness in the above section, please leave a comment. My guess regarding the warning in Actix is that blocking constructs is bad in general for an async web server, and not related to Futures per se. A blocking `std::sync::Mutex` will not return a Future and and allow "parking" the request, if it did it would allow the process to execute (or at least accept) other requests while waiting for the mutex to unlock but instead it will potentially block the whole server and not accept new requests at all. I guess a sort of async mutex that returns a future and had a mechanism to notify when it's unblocked could work, however they suggest the better way is to follow the Actor model instead that seeks to avoid this (and many other potential issues). 
Thanks for taking the time to write such a detailed reply! I think we're sort-of in agreement, as exemplified by this statement of yours: &gt; The problem is that producing such high quality is code is a hard, time consuming and thankless task. It doesn't just happen for free, and if too much code requires going through that process, then the ecosystem might not mature and evolve as quickly as we all would like to hope it will. I agree, and I think this is the crux of the matter. I think I'm a little bit spoiled, having had the opportunity to use languages like Java and C# that have had literally billions of dollars of effort thrown at their standard libraries. My comparison of these huge-corporation-owned languages to community-driven languages like Rust and Python is a little unfair, as obviously they don't have the same level of resources available to make the standard library all-inclusive and yet good enough to not need constant iteration as crates or modules. So maybe you're right, that the module-based approach is the only one that *can* work for languages like Rust, given the absence of bottomless pits of money. On the other hand, it still leaves Rust in a state where to end-users like me, it feels like "batteries are not included". In terms of pure productivity, this is definitely measurable. I'm vastly more productive in C# than I am in Rust, and it's not the *language* holding me back in the latter case. I have had a personal pet theory for a while that investment in language design, standard libraries, etc... has an incredible return per dollar spent. For every improvement in a common language component, thousands or even millions of developers benefit. For every developer that benefits, thousands or even millions of end-users benefit in turn. This is huge. API issues like not locking standard I/O streams by default, tripping up just about every new developer have real costs, measured in multiples of millions of dollars in aggregate. Even really tiny things like removing a single unnecessary variable in the right place might work out to petabytes of memory saved in total across all users! That's no joke. It irks me to no end that Vec by default is resizable, even though this is rarely needed for applications that only need a *dynamically sized* array, not a *growable* array. This extra 'usize' adds up to an ungodly amount of wasted cache, CPU cycles, etc... So what's the solution that would make me happy? Maybe governments could fund open-source language design, putting up billions of dollars for such projects in the sure knowledge that the investment has an enormous return? I can dream, but I suspect this is a fantasy... Alternatively, improvements to cargo and crates.io could possibly have a similar outcome. Things like having "vetted" crates, marking crates that have undergone code reviews (like scientific peer reviews), making the number of open issues most visible, etc... It'll be interesting to see what the community will come up with, and whether it'll be sufficient to make Rust popular enough!
 fastdivide does that too.
/r/playrust
This is the wrong subreddit for posts about the game Rust. This subreddit is for a programming language called Rust. Try /r/playrust instead.
&gt; I know data structures in Rust can be difficult that's an interesting point. isn't the purpose of a general programming language be to make managing data structures simple?
Correct. OP needs to decide how to own the Vec.
As others have pointed out, your `sample_mh_randomwalk` function is wrong. It owns a `Vec&lt;f64&gt;` that it receives from a call to `mhe.sample(...)`. It then askes the `Vec` for a pointer to its contents, and returns that pointer to its caller, and then *it frees the `Vec`*. This is exactly analogous to the following (broken) C++ code: double* get_foo() { std::vector&lt;double&gt; v; v.push_back(...); v.push_back(...); return v.data(); } In both cases, you have a container that "owns" some memory, and you've gotten a pointer to that memory, *and then you've freed the memory*, and then you've used the pointer. You need to decide what the lifetime / ownership is, for that `Vec`. One solution would be that the C code allocates the buffer for the output data and passes that buffer across the FFI to Rust, and then Rust code writes into that buffer. That would look something like this: #[no_mangle] pub extern fn sample_mh_randomwalk(samples_buf: *mut f64, n_samples: u32) { let q = RandomWalk::new(); let p = |x: f64| { (-x.abs()).exp() }; // Laplace distribution let x0= 0.5; let mhe = MHSampler::new(p, q); let ref mut rng = rand::thread_rng(); let mut samples = mhe.sample(rng, n_samples as usize, x0); println!("Rust says: number of samples is {}", samples.len()); println!("{:?}", samples); // Copy data into samples_buf. std::slice::from_raw_parts_mut(samples_buf, n_samples) .copy_from_slice(&amp;samples); } In this case, the C code allocates a buffer, and passes a temporary reference (basically, a slice) to the Rust function `sample_mh_randomwalk`. That code does its job, then writes the results to the buffer given. There are other solutions, where you allocate a buffer in Rust using `Vec&lt;f64&gt;` and then transfer ownership to C code by using various methods on `Vec` + `mem::forget()` + `Vec::from_raw_parts()`. But that's a more complicated approach, and should not be the first thing you try. 
In the case of Vec of course you don't really want to see its private internals. But in the case of types you're defining yourself, there's a good chance that you actually forgot the `pub` keyword. In that case, continuing with the compilation can be more helpful.
Also try the rental crate.
Thanks everybody for the input and the corrections, I have updated the post accordingly!
What happens on overflows? Does it wrap?
&gt; Is wrapping a raw pointer like this safe (as in not using from_raw)? I think you're getting a bit confused on how and when to use `Rc`. First, don't use `Rc::from_raw`, because it doesn't do what you think it does. Use `Rc::new` and other related methods. In order for your code to be "safe", it needs to adhere to the safety rules of Rust. From what I understand, you are getting there. By definition, since you're working on FFI code, you *must* have some unsafe code involved. There's nothing wrong with that -- `unsafe` does **not** mean "incorrect" or "risky" or "unreliable". `unsafe` in Rust means "the Rust compiler cannot verify any correctness properties of this code". It means that you, the developer, are responsible for understanding the rules and adhering to them. Which is exactly how 100% of the code in C/C++ works -- the entire language is in an `unsafe` block. Your goal here is a good one. You're defining Rust types that represent your C/C++ types, and which will expose only safe semantics for your C/C++ types and code. It looks like you have a lifetime constraint: a given Tree must not outlive its parent Earth object. Is that correct? If so, then using `Rc` may be a good way to meet that constraint, but you'll have to use it correctly. I would try something like the following: struct InnerEarth { earth_object_ptr: C_Earth_Ptr } impl Drop for InnerEarth { fn drop(&amp;mut self) { unsafe { C_Earth_delete_earth(self.earth_object_ptr); } } fn new() -&gt; InnerEarth { unsafe { InnerEarth { earth_object_ptr: C_Earth_create() } } } } /// This is the user-visible, "safe" facade for Earth. pub struct Earth { rc: Rc&lt;InnerEarth&gt; } impl Earth { pub fn new() -&gt; Self { Earth { rc: Rc::new(InnerEarth::new()) } } pub fn create_tree(&amp;self) -&gt; Tree { // Create the new Tree object by calling into C code. // Also clone the Rc&lt;InnerEarth&gt; so that the Tree will always have a // valid (indirect) reference to Earth. unsafe { Tree { tree_object_ptr: C_Tree_alloc(self.rc.earth_object_ptr), earth_rc: self.rc.clone() } } } // More methods pub fn do_earth_stuff(&amp;self, ...) { C_Earth_do_earth_stuff(self.rc.earth_object_ptr, ...); } } /// Tree is an object that can be created by Earth pub struct Tree { earth: Rc&lt;InnerEarth&gt;, tree: C_Tree_Ptr, } impl Drop for Tree { fn drop(&amp;mut self){ unsafe { C_Earth_delete_tree(self.earth.earth_object_ptr, self.tree); } } } impl Tree { // There is no need to have a public Tree::new() method, if the Earth // is responsible for constructing the tree. // Now methods that work on Tree. pub fn do_something(&amp;self) { C_Tree_do_something(self.tree); } // ... } The difference here is that the `InnerEarth` type is the one that has the `Drop` impl that calls into C code. This way, when `Rc&lt;InnerEarth&gt;` is dropped, it will correctly call the `Drop` impl for your `InnerEarth` type. And because `Tree` contains an `Rc&lt;InnerEarth&gt;` (but not an `Earth`!), your destruction ordering constraint should be satisfied. 
Division should always do the exact same thing "standard" division does.
Really? After having learnt about async from Rust and probably more from Go, I use it in std::async as core feature in c++ and occasionally python in my day job. It enables better use of resources (CPU or io) so helps my coffee be more performant
Generally, although Rocket and Diesel are currently atuck on incompatible versions of uuid at the moment (rocket upgraded, but diesel won't because it doesn't want to make breaking changes due to already releasing 1.0)
Is this intern position primarily for Machine Learning research? This line: "As a Machine Learning Intern at Mozilla, potential projects you will work on include:" seems to indicate that this is the case?
Private types are somewhat 'leaky' because of auto traits (eg changing len to be a Cell&lt;usize&gt; would be a breaking change)
I would recommend against it if you're just starting to understand rust, but you might want to take a look at `std::pin` (unstable) for self-referential structures.
The only safe way not to leak the memory is to deallocate it as a vector identical to the original one.
`assert!` isn't ideal tho, I usually make a if and put an `unreachable_unchecked` in the else. In the future `assume` will stabilize so it will be easier.
Two mostly blocked threads are much cheaper than a polling, non-sleeping thread. If the aggregators don't need to interact, using two threads is a perfectly reasonable way to do it.
I've always like that Haskell uses the word "namely" in its errors.
That’s very helpful feedback, thanks! Once the new rendering engine renew lands in Amethyst in a few weeks time I’d say it should be close to the level of maturity you were in need of when you started this project.
Since many Programming-Language specific topics are mentioned in the text, I assume that they just forgot to s/Machine Learning/Language Design/g
It'd be spectacular if methods that return a member field could be coupled to that field, so field access was automatically suggested to the accessor regardless of edit distance
My point is that in C you usually have something like arena allocations and it is uncommon to have spaghetti objects like in garbage collected languages (in this case it includes modern C++ which, like Rust, is an outlier on this spectrum), if a GC language were to use the same data structures C uses to reduce calls to free/malloc (e.g. an ECS or an arena) then GC pauses could be ameliorated (I am not suggesting this is a good idea, this reply was more of a mental experiment)
I realised I didn't post this when I made it, but this is a small crate that pretty much builds the `lines` implementation from the standard library in `BufRead` but operates purely on bytes. It operates much more quickly than the implementation in stdlib because it doesn't have to do any `String` validations. I've needed it several times so figured I'd turn it into a crate in case anyone else needs it! It offers a way to collect into a `Vec&lt;u8&gt;` or an `unsafe` implementation which uses `&amp;[u8]` (and thus avoids allocations), so you can choose whichever is more appropriate for your use case. The latter is obviously faster, and is safe enough if you read through the constraints in the documentation - but if you're going to allocate anyway, you might as well use the former. 
It's not "pure" rust but you could write a Rust wrapper for [gnuplot](http://www.gnuplot.info/). It's also available as a library. 
That's a neat idea that I hadn't thought of, but that should actually be doable! Could you file a ticket for it? I think that annotating methods with the locals that they return for suggestions would could yield some neat suggestions. That being said, I worry that they might go the way of the "same return type" suggestions, where we suggested things like `count_zeros` and other methods that were rarely relevant...
Cool! I was poking around the repo a little bit today, mainly at the examples, it seems like there is a fair bit there. Might give it a shot for some experimentation :)
Looking into it right now, thanks for the suggestion :)
I mean, obviously?
Unless you're careful to make sure that they're both using the same allocator, you need to make sure that the same language handles both allocation and destruction. For calling Rust code from C++, it's easier to do both from the C++ side: void main() { uint32_t n_samples = 40; vector&lt;double&gt; numbers(n_samples, 0.0); sample_mh_randomwalk(n_samples, &amp;numbers[0]); // use numbers } ... fn sample_mh_randomwalk(n_samples: u32, ptr: *mut f64) { let buffer = unsafe { slice::from_raw_parts_mut(ptr, n_samples as usize) }; // use buffer, which is a &amp;mut [f64] } Direct allocation in Rust is possible, and if you're giving up ownership entirely I think it's better to directly use `std::alloc` (analogous to `new` and `delete`) instead of creating, disassembling, reassembling, and dropping a `Vec`. void main() { uint32_t n_samples = 40; double* numbers = sample_mh_randomwalk(n_samples); // use numbers // note: unlike delete[], Rust's deallocation needs to be passed the size deallocate(numbers, n_samples); } ... use std::alloc::{alloc_zeroed, dealloc, Layout}; use std::mem::align_of; use std::slice; fn sample_mh_randomwalk(n_samples: u32) -&gt; *mut f64 { // Won't panic, since align_of::&lt;T&gt; is guaranteed to return a valid alignment // The unstable Layout::array::&lt;f64&gt;(n_samples) function could make this cleaner let layout = Layout::from_size_align(n_samples as usize, align_of::&lt;f64&gt;()).unwrap(); let buffer = unsafe { // I think this could actually panic if allocation fails, but some OSs // will pretend that it succeeded and then segfault if you use too much let raw_buffer = alloc_zeroed(layout); slice::from_raw_parts_mut(raw_buffer as *mut f64, n_samples as usize) }; // fill buffer, which is a &amp;mut [f64] buffer.as_mut_ptr() } unsafe fn deallocate(ptr: *mut f64, len: u32) { // must be identical to the one passed to alloc above let layout = Layout::from_size_align(len as usize, align_of::&lt;f64&gt;()).unwrap(); dealloc(ptr as *mut u8, layout); }
Unless you're careful to make sure that they're both using the same allocator, you need to make sure that the same language handles both allocation and destruction. For calling Rust code from C++, it's easier to do both from the C++ side: void main() { uint32_t n_samples = 40; vector&lt;double&gt; numbers(n_samples, 0.0); sample_mh_randomwalk(n_samples, &amp;numbers[0]); // use numbers } ... fn sample_mh_randomwalk(n_samples: u32, ptr: *mut f64) { let buffer = unsafe { slice::from_raw_parts_mut(ptr, n_samples as usize) }; // use buffer, which is a &amp;mut [f64] } Direct allocation in Rust is possible, and if you're giving up ownership entirely I think it's better to directly use `std::alloc` (analogous to `new` and `delete`) instead of creating, disassembling, reassembling, and dropping a `Vec`. void main() { uint32_t n_samples = 40; double* numbers = sample_mh_randomwalk(n_samples); // use numbers // note: unlike delete[], Rust's deallocation needs to be passed the size deallocate(numbers, n_samples); } ... use std::alloc::{alloc_zeroed, dealloc, Layout}; use std::mem::{align_of, size_of}; use std::slice; fn sample_mh_randomwalk(n_samples: u32) -&gt; *mut f64 { let n_samples = n_samples as usize; // Won't panic, since align_of::&lt;T&gt; is guaranteed to return a valid alignment // Layout::array::&lt;f64&gt;(n_samples) is a cleaner way to do this, but is currently an unstable function let layout = Layout::from_size_align(n_samples * size_of::&lt;f64&gt;(), align_of::&lt;f64&gt;()).unwrap(); let buffer = unsafe { // I think this could actually panic if allocation fails, but some OSs // will pretend that it succeeded and then segfault if you use it let raw_buffer = alloc_zeroed(layout); slice::from_raw_parts_mut(raw_buffer as *mut f64, n_samples) }; // fill buffer, which is a &amp;mut [f64] buffer.as_mut_ptr() } unsafe fn deallocate(ptr: *mut f64, len: u32) { let len = len as usize; // must be identical to the one passed to alloc above let layout = Layout::from_size_align(len * size_of::&lt;f64&gt;(), align_of::&lt;f64&gt;()).unwrap(); dealloc(ptr as *mut u8, layout); }
I am totally okay with this. In a runtime like Java where "private" can be ignored via reflection I think it's more of an issue, but when there's no escape hatch I think it's alright. 
Is it thoroughly tested? I am surprised the multiplication does not overflow.
AFAIK gcc does this for c++, it'll suggest the function that returns a member variable even if they're differently named
Possibly a useful message were the field in your code base and you're considering making it public as the fix instead of accessing it via a method.
This is because [Deserialize trait](https://docs.rs/serde/1.0.84/serde/trait.Deserialize.html) requires that the type implements `Sized`. Trait objects (e.g. `dyn LocationTrait`) do not implement `Sized`, so you will have to deserialize to a type implementing `LocationTrait`, then turn it into a trait object: let country: Box&lt;LocationTrait&gt; = bson::from_bson::&lt;Country&gt;(bson::Bson::Document(item)).unwrap().into()
I just opened a PR that changes the wording of this message... whoops!
Why? 
As a side note, if you do want strings, this is quite a bit faster than `lines`: ``` let mut line = String::with_capacity(1024); while buf.read_line(&amp;mut line).unwrap() &gt; 0 { // Do something with &amp;line; line.clear(); } ``` 
`Vec&lt;T&gt;` today implements both `Send` and `Sync` if `T` does, but if `Vec` grew a `Cell&lt;usize&gt;`, then it would no longer implement `Sync`.
Inspired me to finally push my [linereader](https://crates.io/crates/linereader) to crates.io. No unsafe code and about 30% faster than BufRead's `read_until()` while being a bit more convenient to use.
Ah, I see what you mean. It stores the intermediate multiplication results in an integer one size higher. So for u64, it stores intermediates in a u128. For a u8, it stores intermediates in a u16. That's why there is no `StrengthReducedu128`: there is no larger primitive to store intermediates.
Awesome!
Really nice work!. I hope it finds its way in to a lite ultra-efficient text editor somewhere.
This crate started as basically a yak shave that ended with this: $ mkdir /tmp/regexdfa $ ucd-generate dfa --name GRAPHEME_BREAK_FWD --sparse --minimize --anchored --state-size 2 /tmp/regexdfa '(?:\p{gcb=CR}\p{gcb=LF}|[\p{gcb=Control}\p{gcb=CR}\p{gcb=LF}]|\p{gcb=Prepend}*(?:(?:(?:\p{gcb=L}*(?:\p{gcb=V}+|\p{gcb=LV}\p{gcb=V}*|\p{gcb=LVT})\p{gcb=T}*)|\p{gcb=L}+|\p{gcb=T}+)|\p{gcb=RI}\p{gcb=RI}|\p{Extended_Pictographic}(?:\p{gcb=Extend}*\p{gcb=ZWJ}\p{Extended_Pictographic})*|[^\p{gcb=Control}\p{gcb=CR}\p{gcb=LF}])[\p{gcb=Extend}\p{gcb=ZWJ}\p{gcb=SpacingMark}]*)' You can then essentially write an iterator over graphemes like this: use blah::GRAPHEME_BREAK_FWD; #[derive(Clone, Debug)] pub struct Graphemes&lt;'a&gt; { bs: &amp;'a BStr, } impl&lt;'a&gt; Iterator for Graphemes&lt;'a&gt; { type Item = &amp;'a str; #[inline] fn next(&amp;mut self) -&gt; Option&lt;&amp;'a str&gt; { let end = match GRAPHEME_BREAK_FWD.find(self.bs.as_bytes()) { None =&gt; return None, Some(end) =&gt; end, }; let grapheme = self.bs[..end].as_bytes(); self.bs = &amp;self.bs[end..]; Some(::std::str::from_utf8(grapheme).unwrap()) } } This is just proof of concept stuff at this point, but the idea here is that this can be made to work on arbitrary bytes. On top of that, it uses a very small amount of space (only 10KB for everything required to do forward grapheme breaking): $ ls -l /tmp/regexdfa/ total 28 -rw-r--r-- 1 andrew users 10223 Jan 3 22:10 grapheme_break_fwd.bigendian.dfa -rw-r--r-- 1 andrew users 10223 Jan 3 22:10 grapheme_break_fwd.littleendian.dfa -rw-r--r-- 1 andrew users 1143 Jan 3 22:10 grapheme_break_fwd.rs
Nice work!
&gt; an unsafe implementation which uses &amp;[u8] (and thus avoids allocations) I'd *strongly* suggest getting rid of this. It's an `unsafe` function masquerading as a safe one - it would be much more straight-forward just to drop the pretence of being an `Iterator` which *can't be safely used as one*, and just return a slice which Rust knows is owned by your structure and will enforce the safe use of. This is what I did in linereader - yes, it's unfortunate that you can't use a `for` loop with it, but it's much less unfortunate than having an API that can blow your foot off.
The ability to have fields and methods share names is such an underappreciated feature of Rust imo. No more of this silly `self._field` or that sort of thing.
It's late and I'm headed to bed, but you can't do this: https://github.com/whitfin/bytelines/blob/e80ed08efe14496c014827996e0b3d84fe194eaf/src/lib.rs#L169-L179 --- It's UB because you're mutably aliasing a `&amp;[u8]`. In general, using `unsafe` to "extend" lifetimes like this can't work. Basically, if you hand me a `&amp;[u8]` and I have to somehow be "careful" with that `&amp;[u8]` in order to avoid memory unsafety, then you've violated the contract of `unsafe`.
The hierarchy is basically, from slowest to fastest: * `lines()` &amp;mdash; allocates a `String` for each iteration, copies into it with UTF-8 validation. * `split()` &amp;mdash; allocates a `Vec&lt;u8&gt;` for each iteration, copes into it. * `read_line()` &amp;mdash; copes into a provided `String` with UTF-8 validation. * `read_until()` &amp;mdash; copies into a provided `Vec&lt;u8&gt;`. I wrote [linereader](https://crates.io/crates/linereader) because there was no method that avoided the copy. Also because I got tired of forgetting to type `buf.clear()`.
Incoherent edits are the only kind of edits I make.
Is there any reason rustc couldn't suggest \`.size()\` -&gt; \`.len()\`? I feel like with even a rather small list of synonyms rustc could cover most of the common cases
This is the first time I've used unsafe, so I don't really understand - I thought the point of `unsafe` was specifically to point out exactly that; that you have to be "careful" in addition to what I give you? That's what I understood from the stdlib unsafe functions, at least. 
So would you recommend moving to the `while let Some(x) =` syntax rather than the iterator? I had originally thought about this, but I guess I misunderstood the point of `unsafe`. I'd read that as long as the contract is well communicated, it has a place (and the documentation points out exactly what that contract is).
I'm curious. I know ropes are often used for text buffers, but I've also heard they are sub-optimal. Is there a more optimal data-structure? Are ropes more friendly for most types of text insertions? What's the short scoop on all this?
Unsafe is supposed to be interior, AFAIK -- you can use unsafe inside your own functions provided that they actually expose a safe interface. If the function isn't safe, it should itself be marked as unsafe.
That's what I understood too; and why the function referenced above is indeed marked as `unsafe` with an explanation as to why it's marked as such. I guess I misunderstood.
You have it right in principle, yes. The problem is that you've committed UB *regardless* of what the caller does because your program supports mutably aliasing a &amp;[u8]. As soon as that's possible, you've lost. Remember, unsafe does not disable anything, it just gives you access to more stuff. The aliasing invariants around &amp;[u8] and &amp;mut [u8] must still be maintained at all times.
Twain is among several people that this quote is commonly attributed to, but it made its way into English usage via a translation of a passage Pascal wrote in French.
If unsafe behaviour leaks from the function, it should itself be marked `unsafe fn ...` so callers have to wrap it in their own `unsafe { .. }` block. Even on the face of it that's not worth it - who would want to write `unsafe { for ... { } }`?
Great starter project: thanks for sharing it! I was impressed with the small amount of `stdweb` needed to make this go as a webapp. I'll leave the fun of improving it to others, but I'm filing a few issues.
I liked this idea so much [I implemented it](https://github.com/ejmahler/strength_reduce/pull/1). As it turns out, some previously-untested inputs do indeed cause panics. 
You probably want the `quote` crate if I were to guess.
I have "asyncing" feeling that we aren't yet.
About `Vec&lt;Option&lt;...&gt;&gt;`, I was having trouble calling `join` on the handles because it consumes the handle. 
Awesome! Great work! I have been in need of a way to chunk utf8 files. One question I have about that: The docs for `Chunks` mentions: &gt;There are no guarantees about the size of yielded chunks, and except for CRLF pairs there are no guarantees about where the chunks are split. For example, they may be zero-sized, they don't necessarily align with line breaks, etc. Does this mean that it could split in the middle of a grapheme cluster? Like what if you have the sequence: Shrug + zero width joiner + male sign? All three together form a single grapheme cluster (🤷‍♂️), but is composed of 3 individual code points (`char`s). Would it potentially split after the shrug code point?
That's actually one I'm already reading through. A couple of years ago I found his book on design patterns, and I have loosely been keeping up with him since then. Thanks for the suggestion though!
It's not short, but those answers are probably in [rope science](http://abishov.com/xi-editor/docs/rope_science_00.html) somewhere (use the menu button to access posts). I've been meaning to read it for a while...
Indeed, you'll have to remove them from the Vec first, to avoid leaving an invalid element in there after `join` consumes it. The easiest way is to just iterate over the Vec directly (rather than over an `&amp;mut` of it), which will consume the Vec and yield each element by value. However, getting the Vec itself by value is a little tricky, because it's a struct field and `drop` doesn't take the struct by value. You'd need to create an empty Vec and then use `mem::replace` to get the non-empty Vec out, which is a bit confusing. Another approach would be to call `.pop()` in a loop, which might be a little simpler.
&gt; `None` stored in queue Wouldn't the queue return `None` when its empty? So we are essentially using the empty queue as a signal to finish, are we not?
The following seems to work: ### ` src ` ### ` src/bin ` ### ` src/bin/bin1.rs ` fn main() { use cratetest::bill; use cratetest::jane; use cratetest::jane::susan; println!("bin1: val1 is {}", cratetest::val1()); println!("bin1: val2 is {}", bill::val2()); println!("bin1: val3 is {}", jane::val3()); println!("bin1: val4 is {}", susan::val4()); } ### ` src/bin/bin2.rs ` fn main() { use cratetest::bill; use cratetest::jane; use cratetest::jane::susan; println!("bin2: val1 is {}", cratetest::val1()); println!("bin2: val2 is {}", bill::val2()); println!("bin2: val3 is {}", jane::val3()); println!("bin2: val4 is {}", susan::val4()); } ### ` src/jane ` ### ` src/jane/mod.rs ` pub mod susan; pub fn val3() -&gt; String { "jane".to_string() } ### ` src/jane/susan.rs ` pub fn val4() -&gt; String { "jane/susan".to_string() } ### ` src/bill.rs ` pub fn val2() -&gt; String { "bill".to_string() } ### ` src/lib.rs ` pub mod bill; pub mod jane; pub fn val1() -&gt; String { "fred".to_string() } ### ` src/main.rs ` fn main() { use cratetest::val1; println!("main: val1 is {}", val1()); println!("main: val2 is {}", cratetest::bill::val2()); println!("main: val3 is {}", cratetest::jane::val3()); println!("main: val4 is {}", cratetest::jane::susan::val4()); } 
The Wikipedia link looks like it was broken by the formatter, it should be: https://en.m.wikipedia.org/wiki/Rope_(data_structure)#Comparison_with_monolithic_arrays
For the mortals in the audience such as myself : Differences with the regex crate The main goal of the regex crate is to serve as a general purpose regular expression engine. It aims to automatically balance low compile times, fast search times and low memory usage, while also providing a convenient API for users. In contrast, this crate provides a lower level regular expression interface that is a bit less convenient while providing more explicit control over memory usage and search times. 
Any idea why the below runs fine on my Linux machines but crashes with "invalid argument" on MacOS (Mojave)? I've spent way too much time today and made no progress. use std::net::UdpSocket; fn main() { let socket = UdpSocket::bind("[::]:0").expect("couldn't bind socket"); socket .connect("239.255.255.250:1900") .expect("couldn't connect"); } Traceback: $ RUST_BACKTRACE=1 cargo run Finished dev [unoptimized + debuginfo] target(s) in 0.03s Running `target/debug/udp` thread 'main' panicked at 'couldn't connect: Os { code: 22, kind: InvalidInput, message: "Invalid argument" }', libcore/result.rs:1009:5 stack backtrace: 0: std::sys::unix::backtrace::tracing::imp::unwind_backtrace at libstd/sys/unix/backtrace/tracing/gcc_s.rs:49 1: std::sys_common::backtrace::print at libstd/sys_common/backtrace.rs:71 at libstd/sys_common/backtrace.rs:59 2: std::panicking::default_hook::{{closure}} at libstd/panicking.rs:211 3: std::panicking::default_hook at libstd/panicking.rs:227 4: &lt;std::panicking::begin_panic::PanicPayload&lt;A&gt; as core::panic::BoxMeUp&gt;::get at libstd/panicking.rs:476 5: std::panicking::continue_panic_fmt at libstd/panicking.rs:390 6: std::panicking::try::do_call at libstd/panicking.rs:325 7: core::ptr::drop_in_place at libcore/panicking.rs:77 8: core::result::unwrap_failed at libcore/macros.rs:26 9: &lt;core::result::Result&lt;T, E&gt;&gt;::expect at libcore/result.rs:835 10: udp::main at src/main.rs:5 11: std::rt::lang_start::{{closure}} at libstd/rt.rs:74 12: std::panicking::try::do_call at libstd/rt.rs:59 at libstd/panicking.rs:310 13: panic_unwind::imp::find_eh_action::{{closure}} at libpanic_unwind/lib.rs:102 14: std::alloc::default_alloc_error_hook at libstd/panicking.rs:289 at libstd/panic.rs:392 at libstd/rt.rs:58 15: std::rt::lang_start at libstd/rt.rs:74 16: udp::main
Yeah, I never thought about `.pop`. That would have been a simpler solution, indeed.
For other people who stumble across this, as best I can see, \`TokenStream::from\_str("some tokens go here")\` is the best option
Very cool! Reminds me of [xi-editor ropes](https://xi-editor.io/xi-editor/docs/rope_science_00.html) which was my first exposure to rope data structures. Looks like they have the [xi-rope crate](https://crates.io/crates/xi-rope) published. [xi-editor GitHub](https://github.com/xi-editor/xi-editor/blob/master/rust/rope/Cargo.toml). Gives something to benchmark against ;)
Yes, that's correct: grapheme clusters can be (and often are) split across chunks. In earlier versions, Ropey guaranteed that graphemes weren't split, but in the end I decided to remove that guarantee. The main reason being that graphemes aren't 100% strictly defined, and different use-cases may define them differently. However, Ropey is still very much designed with correct and efficient grapheme (or other segmentation) handling in mind. See the grapheme [examples](https://github.com/cessen/ropey/tree/master/examples) for some possibilities. If you need to pass grapheme clusters as contiguous text to other APIs, using cow (copy on write) strings is likely the best approach. You can create a cow string from any RopeSlice with a simple `.into()` call, and it will reference the Rope's text data if it's already contiguous or copy it if it's not. In practice this ends up being very efficient for something the size of typical grapheme clusters. &gt; I have been in need of a way to chunk utf8 files. Maybe I'm misunderstanding what you mean, but the purpose of Ropey isn't to chunk things. It _happens to_ chunk things as part of how it works, but it's not (nor is it intended to be) a text chunking/segmenting library.
cc: /u/raphlinus 
Was about to mention xi-rope as well. How does this crate differ from xi-rope?
I had a nasty stack overflow because of some large data types, so my quick fix was to `Box` the types that store these large data types. Since then I have looked at every `struct` and `Box`ed everything that i found too large, so it's probably unnecessary to `Box` the containers, but I wanted to make sure i won't run into a stack overflow again. Thanks for reminding me though.
I mean `Some(None)`
Ah yes, so you mean the signature should `VecDeque&lt;Option&lt;fn()&gt;&gt;`. That seems like the better solution. I actually took yours and parent commenter's advice and implemented it [here](https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2018&amp;gist=274e9f6f034c7d3576f6c82d36c7527e) but it still uses a flag variable and I think I am still using Condvar in a confusing way. Storing an option seems like the better solution.
I don't know of any data structure for text buffers that can strictly be called "optimal". They all have trade-offs. I think ropes have a lot of good properties: 1. Very good edit performance even in worst-case. 2. The rope data structure naturally lends itself to tracking things like `char` counts and line endings, without having to maintain an external data structure. This allows fast indexing into the rope's contents by `char`/line index. 3. You can make "free" copies of a rope, sharing data between the rope and the copy. As edits are made, they slowly diverge, but still share data where they can. This is really useful for e.g. saving large documents without blocking the user from editing during the save: first make a copy in O(1) time, then send the copy to another thread to be saved to disk. To contrast with other popular text buffer data structures: - Gap buffers have much better best-case edit performance, but much worse worst-case (e.g. multiple cursor editing). They also require a separate external data structure to track things like line endings, and making copies are O(N). - Piece tables are fantastic for huge documents. In fact, depending on how they're implemented, they can even handle documents much larger than available memory, which is a big benefit over ropes. They also have the same "free" copy feature as ropes (although the book keeping can be trickier in complex cases). But they require external data structures to track things like line endings, etc., so the implementation becomes more complex for equivalent functionality. I certainly don't think that a rope is objectively the best choice, but I do think it's a _good_ choice for many use-cases. Hope that helps!
Well, multiple things. I wanted to do something in Rust other than "hello world" and i looked at the list at [https://github.com/not-yet-awesome-rust/not-yet-awesome-rust](https://github.com/not-yet-awesome-rust/not-yet-awesome-rust#microsoft-office). Saw that Rust lacks in Office document parsing and I have some experience with the format, so I thought why not? I have written most of this stuff in C++ already for my work (our app can import pptx files), so it's mostly just porting the code. It turned out better than the original code (better error handling, cleaner code), so it's also a good opportunity to demonstrate Rust at my workplace and maybe convince the higher-ups that we should port some of our components to Rust. This crate also made me think about creating a simple pptx file viewer app, which led me to developing my own GUI framework. Mostly for educational purposes, but maybe in the future I will open source it as well. Also, maybe in the future i will write a CLI tool to convert pptx files from/into odp.
This looks great but I am waiting for crates/cargo plugin details. No link, schedule. Even google does not provide further support than this paper 
Nice! Thanks for doing this: it looks like it was a ton of hard work. Some very off-the-top-of-my-head questions: * Isn't supporting '^' and '$' as anchors as simple as adding special states at the beginning and end of the DFA and (maybe notional) sentinels to the target? Or am I being the dumbs again? (Suspect the latter.) * There are some tricks that I've used with RE reversal to provide lookahead; it might also allow "meet-in-the-middle" to take care of examples like the one you've posted (although it's easy to construct harder examples). As a start in this direction, any chance of providing classic regular language operations (intersection, union, reversal, complement) in the Builder or somewhere? Or is this a bad idea?
This is awesome! One suggestion: is it possible to declare the test ignore predicate somewhere? Picking a function with a matching signature seems like it could easily go wrong with helper functions…
The function is not marked as unsafe; that would look like `unsafe fn next(...)`. It's just a function that contains an unsafe block.
You can start by removing the boxes from the HashMaps. A HashMap already puts its data on the heap, so stack overflows are already dealt with. 
Minor thing in the readme: at the end, it says "A paper by Yang and Prasanna (2011) actually seems to provide a way to character state blow up such that it is detectable.". Isn't the correct word "characterize" here?
&gt; I was impressed with the small amount of stdweb needed to make this go as a webapp. Indeed. If someone wants to create a new branch that implements a GUI with wasm-bindgen instead (which, imo, is a great first project to learn wasm-bindgen), I'd be happy to see that as well. In fact, that's how I intended to do it before I realized how simple the implementation with stdweb is. &gt; I'll leave the fun of improving it to others, but I'm filing a few issues. Much appreciated!
Ah, that makes sense!
In Rust analyzer, we have two things: * a hook itself as a rust binary (why use unix-specific bash when you have Rust) * a rust binary to install the hook So, new contributors are advised to run roughly “cargo run —bin install-hook” after cloning the repo. We also check formatting on CI. Installing rustfmt seems not to take that much time? Here’s the code that achieves all this: https://github.com/rust-analyzer/rust-analyzer/pull/271 See https://matklad.github.io/2018/01/03/make-your-own-make.html for details about the aliasing maneuver :) 
Not sure I understand your question. What do you mean by "picking a function with a matching signature"? For file-driven tests, you specify function name in the "if" predicate. For data-drive tests, ignoring individual items is not supported (for no reason except that we didn't need it at the time).
[https://wiki.gnome.org/Hackfests/Rust2018](https://wiki.gnome.org/Hackfests/Rust2018) &gt;Agenda, goals &gt; &gt;\- Improving gtk-rs and bindings for other GNOME libraries &gt; &gt;\- Continuing the work of gnome-class implementation &gt; &gt;\- Continue to improve librsvg Rust's implementation &gt; &gt;\- Discussing further on gtk-rs release process? &amp;#x200B;
You're welcome. I inspire from the [onion architecture](https://www.infoq.com/news/2014/10/ddd-onion-architecture) for information. I don't like to put all the code in the route methods, it feels non organized.
Sorry, I commented right after waking up: that might have been a mistake (-: I was referring to [https://docs.rs/datatest/0.2.1/datatest/#ignoring-individual-tests](https://docs.rs/datatest/0.2.1/datatest/#ignoring-individual-tests), missing that there's an \`if\` on the \`input\` clause already. Ready to declare this crate unqualifiedly great (:
RemindMe! 1 Day
I don't care what it is, I'm just pleased someone in the community stuck to a project long enough to give it a 1.0 version Thank you! This evokes confidence 
I will be messaging you on [**2019-01-05 09:27:53 UTC**](http://www.wolframalpha.com/input/?i=2019-01-05 09:27:53 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/rust/comments/acgei6/yet_another_mutability_question_need_help_real_bad/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/rust/comments/acgei6/yet_another_mutability_question_need_help_real_bad/]%0A%0ARemindMe! 1 Day) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
Graphs in Rust are kind of weird like that. You can just wrap everything into a reference-counted pointer to have it be safe at the cost of some overhead, or you can use a crate like [petgraph](https://docs.rs/petgraph/).
[rustplot](https://crates.io/crates/rustplot) is a plotting crate that uses gtk and cairo. Might be a good place to start. From the documentation: &gt; rustplot is a simple GUI based plotting library for Rust. &gt; &gt; rustplot provides a data parsing module to for extracting data from CSV files and uses gtk+ for creation of graphical user interfaces for displaying charts.
Cool! Do you know about [Laminar](https://github.com/amethyst/laminar)? Would be great to chat with you on the Amethyst chat/forum.
Cool stuff and as u say a great way to learn. Others might also be inspired by your work and write other office format parsers. Nice work
IIRC, gtk-rs was originally an independent project that as since been integrated into GNOME. This may explain the ambiguous state.
your rust programming buddies? maybe not.. i think u are in the wrong sub.. 
Oh my god now i understand why i didnt understand half of these posts 😂😂😂😂 sorry guys 
How did you install Rust on your Mac? Linux and MacOS don’t have the same kernel interface for system calls, so the version of the STL that you have on your Mac might not be the right one.
I was wondering, have you considered using an RRB Vector like one provided by [bodil's im.rs](https://github.com/bodil/im-rs). In theory, it should provide excellent concatenation speeds at least for large inputs.
As a Rust beginner, I highly appreciate this!
Just use a power of 2 and bitshift. If your hash function is so broken it causes collisions then fix your hash.
I did some benchmarks of ropey vs [xi-ropes](https://crates.io/crates/xi-rope) and [an-rope](https://crates.io/crates/an-rope) a couple months ago, comparing it to my own [C rope library](https://github.com/josephg/librope) thats based on skip lists and my own [rust port of that](https://github.com/josephg/rustrope). The benchmark results [are here](https://josephg.com/c3/report/), although the stats haven't been pulled out into a single comprehensive report and I don't have descriptions of what all the benchmarks actually are. [Benchmark code here](https://github.com/josephg/rustrope/blob/master/bench/src/main.rs#L164-L295). This is using ropey 0.9.2. Performance may have improved since then. My conclusion is that xi-rope and ropey both performed great. xi-rope is a bit faster when linearly inserting, but they're both orders of magnitude faster than they need to be to build a performant editor. They're fast enough that I decided not to publish a crate for my own rope library to avoid ecosystem fragmentation. I recommend against using anrope because its missing tree rebalancing or something; it performs *much* worse than the other algorithms.
Nice work. Any chance of a local export (PDF or standalone HTML)?
There was an analysis done (I can look this up, its somewhere bookmarked) where OCaml basically has the best GC when it comes to latency (yes its faster than Go), considering its also ultra optimized for single cores (C++ HFT programs do a similar thing, they pin the binaries to only run on a single physical core) As mentioned here [https://www.reddit.com/r/rust/comments/abm6hy/why\_rust\_is\_successful\_compared\_with/ed3ao0r/](https://www.reddit.com/r/rust/comments/abm6hy/why_rust_is_successful_compared_with/ed3ao0r/) if you program in a specific style of OCaml you can achieve similar results compared to C for latency. Historically OCaml is one of the few "functional" programming languages which has had decades of optimization for low latency/performance since this is also where the language has traditionally been used
The problem is that in many cases the returns are reaped by those who did not invest in the language. If you need a dynamically sized, unresizable array, you can use `Box&lt;[T]&gt;`, this does what you want and is already possible today.
Rocket would be easier for you when you get started. However, if you want to complicate your work a bit and use futures, although you don't need them for your work, what you are trying to accomplish probably doesn't require a PhD in futures and you could just adopt patterns from examples. Whatever you're working on could probably be done in actix-web without requiring much understanding of asyncio internals. Actix-web has dozens of examples you can learn from. It's a really great project. 
This isn't entirely true, Haskell's biggest issue with memory resource management is not the fact that its a managed language, its that Haskell is a lazy by default language, i.e. see [https://www.reddit.com/r/haskell/comments/1e8k3k/three\_examples\_of\_problems\_with\_lazy\_io/c9xyxxy/](https://www.reddit.com/r/haskell/comments/1e8k3k/three_examples_of_problems_with_lazy_io/c9xyxxy/). Laziness by default is honestly a gimmick and is one of the biggest things that holds Haskell down;. Languages like Scala can verify at compile time that resources are freed properly (even in the cases of exception), you can check out the \`Bracket\` typeclass [https://monix.io/blog/2018/11/07/tutorial-bracket.html](https://monix.io/blog/2018/11/07/tutorial-bracket.html) or ZIO [https://github.com/scalaz/scalaz-zio](https://github.com/scalaz/scalaz-zio)
&gt;It's no longer a trade off between dev time and performance. You get both. For free. There is definitely a huge mental burden in Rust when learning how the borrow checker/linear type system work. Yes Rust in almost all cases verifies that memory is managed correctly at compile time, but you still have to do memory management. Coming from primarily Scala dev, this cost in "dev" time is certainly not free. And thats not even touching areas points such as cyclic references (i.e. graphs), where in fact arguably using a GC is the most efficient way to solve such issues (i.e. GCC now uses an optional GC in their compiler because of how difficult it is to deal with graph like structures in memory managed languages)
I think this may be because you're binding to an IPv6 address, but connecting to an IPv4 address. There is an option called IPV6_V6ONLY, but it's not standard and not implemented on every platform. It's turned off by default on many Linux systems, but turned on by default on Mac systems. Because it's not cross platform, you would somehow need to use setsockopt from libc to set the option. Alternately, use two separate sockets for each ip version.
Also: lifetime invariance.
/r/rustjerk might appreciate it more
Your problem is similar to this binary tree implementation: https://gist.github.com/aidanhs/5ac9088ca0f6bdd4a370 Rust can't know the size of a struct that has itself as one of the fields. One possible solution is to box the field and store it on heap.
You can export as png only.... I have not found a way to convert canvas or image to a pdf (that has scalable fonts). Sorry.
oh, sry.
Yes. It's just a typo.
Heroku worked just fine for me when I did an android back-end with Rocket for some university course. There should be a Rust buildpack (or w/e its called on Heroku) available.
:-) * The lazy DFA in the regex crate supports them. It's not like I don't know how to do it, it's just that I'd like to approach it from a more principled direction. I don't want to hack it together. In practice, you can achieve the behavior of `^` by setting `anchored(true)`. Achieving the behavior of `$` is similarly possible by reversing an anchored DFA. It's just not convenient. I'd like a more principled approach so that I can take what I've learned and apply it to more complex cases, such as Unicode aware `\b`, CRLF aware `$` and capturing groups. * I'm not sure if they specifically belong in the builder, but sure, it seems like they'd be in scope. I don't think I know what you mean about RE reversal to provide look-ahead. To give you an idea of where my head is at with respect to look-around, see this paper: http://re2c.org/2017_trofimovich_tagged_deterministic_finite_automata_with_lookahead.pdf
In the end it was stupid of me to think `Rc::from_raw` would work in my case but learnt in the hard way. Anyway, this is exactly what I wanted to achieve, thanks a lot! &amp;#x200B;
&gt;I'm not sure you're getting any benefits by wrapping the pointer with a Rc. I wanted to wrap the pointer with Rc to ensure Tree does not outlive the Earth object! &amp;#x200B; &gt;I'm also not sure if you get anything from dividing your unsafe block into 2 parts, as you did in Tree::new. Ah this was something I was wondering while programming in Rust. In all of my code so far I'm dividing the unsafe blocks into several parts but I'm going to revisit this. Thanks!
To elaborate a little, here's your code reworked a bit to use `Rc&lt;RefCell&lt;HumanInfo&gt;&gt;`: [playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=386102bdcffa7852d1557b4287bc0f44). You could implement some traits to clean up some of the implementation, but this is sort of the raw form that you might write. Note that screwing up will now lead to runtime errors instead of compile-time failures.
Yes, you can do a lot of tricks in PS, e.g. letting PS calculate things dynamically based on bounding boxes etc, since it is a full programming language. I also miss being able to do stuff like this in SVG.
This is really great! I am going through this and this is very useful – even for non beginners! I find myself often consulting the docs and this is really handy. Just one thing i found after glancing at it, your HashMap example with the Vikings does not work, a Viking holds a `String` and you giving it a string slice. Its obvious and no big of a deal but a beginner could be confused. 
Ohh you understand the other half. That's great
Awesome! Couldn't find the macros section though.
I'm a bit confused about HashMap vs Struct. Both are data types which store key value pairs, so why would I choose one over the other? Same goes for tuples vs arrays / vectors.
Every explanation I've read about closures is that they are functions which can access other variables in their scope, but I've been accessing variables in the scope of non-closure functions all the time. What am I missing here?
What's the difference between `&amp;str` and `&amp;String`?
Yes. In Rust the `self`argument can be explitly provide when invoking member functions, and you need to use [Universal Function Call Syntax](https://doc.rust-lang.org/book/ufcs.html) when explitly providing. Sometimes it's so interesting, such as `2.0_f64.powf(3.5)` vs `f64::powf(2.0, 3.5)`
I find the package structure pretty confusing. But that's just organizational stuff. Peaking through some of your code one thing that jumps out to me is that you're being very cavalier with resizing vectors. A quick optimization is to call `Vec::reserve` to allocate ahead of time so calling `push` won't have to free then allocate. Something that also jumped out to me was that `Chunk` has a member of `Vec&lt;Vec&lt;Vec&lt;f32&gt;&gt;&gt;`. You can implement multidimensional arrays with a single dimension vector and index into it with a little math. That removes two pointer indirections and guarantees the data is all located contiguously, which may improve cache performance. 
If you peek previous commits I was using it but for some reason I changed it, but now that the crate works will try to measure the difference between multidimensional vectors and single dimension vectors. Need to check it out what’s Vec::reserve about. Thanks for your tips.
Perfect time to use `cargo bench` 
It's a little involved if you are not familiar with more complex hosting environments, but both AWS and Google Cloud give you a monthly budget for free that would be more than enough to host a Rust based server.
Thank you very much for your help!
Thanks! Gonna look into it!
I like that approach a lot, I'll try it out!
Basically, your problem is that your data forms graphs. Graphs are hard in Rust. Not impossible by any means, but they require some thought. You have people, and people have friends. At the moment, you store the information "is-a-friend-of" as each person having a vec of other people. If you think about it, that doesn't really work: John is a friend of both Michael and Ron, but if you can't split the one and only John into two Johns: "John-as-a-friend-of-Michael" and "John-as-a-friend-of-Ron". Plus, John has Michael as a friend. That forms a loop! Basically what you trying to do there is to have references to people. Michael and Ron don't "own" John, (John doesn't exclusively belong to them), but there is John, and both Michael and Ron consider that John as their friend, so they both hold a "reference" to John. The normal references in Rust support only tree-shaped data structures, so your "network of friends" is impossible to represent with them. One solution is to use reference counted pointers \`Arc\` and \`Rc', but that might also be ill-suited for this situation. You need to think about what happens when you have to delete a person: is that person going to "dangle" within the friend lists of other people or should that person also be deleted from there too? In the latter case it's best to have a canonical list of persons that exist, as a \`HashMap\` with some IDs. (Maybe integers? Maybe UUIDs? Depends on your requirements.) Then the people keep lists of their friend's IDs. That way the hashmap of the person functions as a canonical list of people that exist, and the IDs function as weak references. Problem solved!
&gt;I wanted to wrap the pointer with Rc to ensure Tree does not outlive the Earth object Whoever clonned the Rc would still have access to your pointer, plus you already clean it on drop anyway. &gt; I'm dividing the unsafe blocks into several parts but I'm going to revisit this. It can be useful. You should try to turn "unsafe-mode" on only to do the unsafe operations, so the compiler keeps checking things properly between the unsafe things. I just pointed it out because you have 2 consecutive unsafe operations, so might as well write a bit less.
Amazing!
Hahaha, no offence, but I'm not sure what's worse. Not even looking at a subreddit and still posting anyway. Or actually looking at it, reading posts that show that something must be off, yet STILL post anyway. 
Yes. Thanks!
Thanks for the answer! 
Thanks, need to read about bench.
Two ideas: A) Are you sure that you set up actix to listen on port 8088? (I would make this configurable with an env var a la SERVER\_PORT or some such.) &amp;#x200B; B) I think I had some trouble with traefik once with the port not being picked up correctly. There is some way to manually specify the port with in the frontend rule (\`traefik.frontend.rule=Host:questionnaire.local\`), but I'm not sure how anymore, you'd need to check the documentation. 
I mean in principle you're right. The problem is just that many languages don't really give the developer implementing hashmap that much control over the hash function. For a good example of this take `hashCode` from Java. The user has to implement it manually for every type. I recommend reading [this](https://probablydance.com/2018/06/16/fibonacci-hashing-the-optimization-that-the-world-forgot-or-a-better-alternative-to-integer-modulo/) article.
Is there a way to add a dependency without knowing it's version? Kind of like you can with node where you can just \`npm install &lt;package&gt;\` and it automatically selects the latest version and adds it as a dependency?
Via`unsafe` you can access it (more or less).
Yeah, you would only be able of passing a str slice to get-like methods.
`foo = "*"`, but I think you might not get the latest version if another dependency uses an older version. You also cannot publish to crates.io with \* dependencies. 
`&amp;String` is a reference to a heap-allocated string buffer; `&amp;str` is a reference to any contiguous slice of bytes that can be treated as UTF-8 characters.
Some people really were born yesterday, and you can't blame them for it.
A `HashMap`'s key-value pairs are determined at run-time, and the values must all have the same type, whereas a `struct` always has a specific set of "keys" &amp; values and they can each have their own type. Most of the time you want to use a `struct`, unless you need to add &amp; remove pairs at runtime. A Tuple is just a struct where the "key" names are automatically generated (0, 1, etc.) You'd use one of these whenever you want to use a struct but don't really want to define one (often in cases when a function has to return 2 things.) Arrays and vectors are collections of elements of the same type. You'd want to use them when dealing with sets of items.
A closure can access variables in their defining scope, while functions can't. So you can't do something like: ``` fn foo() { let mut x = 1; fn bar() { x += 1; } bar(); } ``` but you can do: ``` fn foo() { let mut x = 1; let mut bar = || x += 1; bar(); } ```
SVG might work?
I wouldn't mind helping out, feel free to PM me. Been working with rust professionally for over two years though I didn't do a lot of webdev (I am working on a PR for [tide](https://github.com/rust-net-web/tide) on my spare time but work has been demanding lately).
So I'm coming at this from the perspective of someone who works primarily in Node/frontend JS and C++. Rather than discussing Python, I'll discuss JS because for these purposes I think they're nearly interchangeable. In JS, half the time, I'm still doing memory management, at least in my mind. For cheap scripts like what we're talking about, everything is just primitives, strings, and similar simple collections, never or rarely leaving scope. Easy in both Rust and JS. Never touch an `Arc`, never touch a lifetime. Just copy or clone everything. Even if it's asynchronous, and even without `async`/`await`, Rust can handle this without too much ado in very much the same way that JS does (from the consumer perspective). For expensive operations, in JS I do have to consider "Does this closure allocate too much memory? Will it ever release that memory?". I really rarely create objects that are shared in memory across different parts of the program, and when I do (maybe once every two weeks?) I do a little mental check like "hey, if I modify this here, will it get modified there?" and I actually have to follow it through because there are no types. It's a little quicker to type, a little harder to read. You don't need to think about multi-threading because it doesn't exist in JS, but you barely need to think about it in practice in Rust because the borrow checker handles it for you. For larger applications, there's definitely a difference and a tradeoff. In larger applications, I think I prefer Rust for tons of reasons, but I'd understand why people would prefer Node for some things. For smaller applications (one-off scripts), I definitely still prefer Node, but I think that's going to become less and less true over time, which is what I was saying in the beginning.
Well, from code readability perspective, a custom enum would be better: `enum QueueItem { Work(fn()), Exit }`. Also, I think you need either the worker to push `QueueItem::Exit` back into the queue when it pops it, or have `DispatchQueue` push `n` of those when cleaning up (where `n` is worker thread count) - otherwise only one thread will find the message that says that it has to exit.
this is a amazing talk ! 
Without looking at the code, you are unlikely to beat GNU coreutils. But you can try. &gt; RUSTFLAGS="-Ctarget-cpu=native" For more manual control of specialized instructions you could look at [faster](https://github.com/AdamNiederer/faster) or go even more low level. P.S. If you wish to publish, you should probably change the name. [hashbrown](https://github.com/Amanieu/hashbrown) is the name of a very fast HashMap implementation. 
It's these kinds of questions and all of the helpful answers that explain the situation that are helping me learn Rust. The book is great, but all of the examples are simple use-cases (IMO), and don't prepare for "real-world-Rust". Thanks everybody for the thorough explanations!
Another beautiful gift from the master! Some quick questions about how this fits with your older work: - How is this similar to (`fst-regex`)[https://crates.io/crates/fst-regex]? They both provide a way to custom drive a regular expression matcher. `regex-automata` also allows saving to disk. Dose `fst-regex` have advantages? - Both this and (`regex_macros`)[https://crates.io/crates/regex_macros] provide a way to compile a regular expression ahead of time. Could `regex_macros` be brought back on top of this?
Coming from Python and Java, most definitely find this useful! Thanks good sir
`fst-regex` is and was a prototype as a way of building a machine that could be made to intersect with an `fst`. It's quite likely that `fst-regex` should be made to use `regex-automata` directly. I'm prertty sure a `regex-automata` DFA can implement the [`fst::Automata`](https://docs.rs/fst/0.3.3/fst/trait.Automaton.html) trait. Otherwise, I don't think there is anything in particular worth salvaging in the `fst-regex` crate. As you might have noticed, it's still using `regex-syntax`, which is ancient at this point. It is hard to answer your question about `regex_macros` concisely, because there are different dimensions to it. I'm _pretty_ sure it would be nearly trivial to write a procedural macro these days that uses `regex-automata` to build a regex at compile time and embed the resulting DFA into the binary. (I imagine the hard part is just figuring out how to expose all of the options for building the DFAs in a nice way.) You would still have a runtime dependency on `regex-automata` in order to read the DFA's representation itself, but you can drop the [dependencies on `regex-syntax` and `utf8-ranges`](https://github.com/BurntSushi/regex-automata/blob/17dd7e9afbe46e7b4bda959dc89333ce106958fb/Cargo.toml#L35-L36), since they are only needed at compile time. Heck, based on what I've heard about `const fn`, it might even be possible for that to Just Work some day with `regex-automata`, although in that instance, I don't know how to distinguish between compile time and runtime dependencies. (Depending on `regex-syntax` isn't the end of the world, but it does come with a whole crapload of Unicode tables.) Now, the above conception is less "build regexes at compile time" and more "build low level strictly DFA based regexes at compile time." The latter really isn't going to fit into the mold of a general purpose regex library. For one, it lacks submatch extraction, although at least in theory, that might be added one day. Same thing for look-around. But the other problem is that fully compiling a DFA ahead of time can use a *lot* of space. If you're not careful, a regex like `\w` can be almost a megabyte big! (Because Unicode.) There are a variety of ways to shrink that which are exposed by `regex-automata`, but it can only do so much when building the complete DFA ahead of time. The regex crate avoids this for the most part by setting a fairly small limit on how big a DFA can be, lazily building it, and falling back to a (much) slower engine if the DFA keeps thrashing. What that means is that if a particular part of a DFA is never seen, then it's never built. e.g., The DFA for `\w` in the regex crate on strictly ASCII input (the common case) isn't going to be any bigger than the full DFA for `(?-u)\w`, which is the ASCII version of `\w`. In any case, this is a *much more* complex runtime model. So if you want to support *general purpose* regexes at compile time, then `regex-automata` isn't going to cut it, although it might indeed cover lots of use cases. Supporting full general purpose regexes at compile time is harder precisely because the runtime model currently demands the ability to defer parts of compilation to search time. So the whole compile time and runtime systems are intertwined.
This is insanely useful. Thank you. 
Your second compiler explorer link is broken for me!
Disclaimer: I have only implemented SHA512 myself, as part of [orion](https://github.com/brycx/orion), and have therefore only looked through your `sha512.rs` file. As I see it, there are several potential ways to speed up your SHA512 implementation (and likely also the others): 1. By using SIMD instructions you could gain a significant performance increase. For reference, see the [RustCrypto](https://github.com/RustCrypto/hashes) implementation, which does exactly that. 2. By using `#[inline(always)]` to force compiler optimisations on relevant code. Here I see functions `perform_round` and `prepare_schedule` as good candidates for this. 3. When implementing SHA512 directly from the spec, you end up with assignments as you have in the `sha512.rs` file L135-142. Some of these mutable assignments can be "elided". This optimisation I found in the [mbed TLS](https://tls.mbed.org/sha-512-source-code) implementation, which I have [adopted](https://github.com/brycx/orion/blob/master/src/hazardous/hash/sha512.rs) into my own project as well. 4. Instead of returning a new working state `hash` each time you call `perform_round` and re-assigning it in `hash_with_initial`, you could let `perform_round` take in a mutable working state and add the state together with the working variables in `perform_round`. For more optimisations, take a look at this fine presentation: [Improving SHA-2 Hardware Implementations](https://www.iacr.org/workshops/ches/ches2006/presentations/Ricardo%20Chaves.pdf).
I gotcha; I was hung up on the fact the function to create the iterator was marked unsafe but not the actual iterator methods. I’ve changed it up to remove unsafe in what I feel is a much better pattern!
In my system there are already quite a few threads hanging around, am I correct in saying that the number of threads should \~ number of cores? What are the consequences if #threads &gt; #cores?
This makes sense; I was mistaken as I didn’t realise the Iterator could be *used* without unsafe; it was only creation that was locked behind it - so yes, I have removed this! Now there’s a safe API for interaction with Iterator; it requires an allocation for ownership but avoids unsafe so that seems like a good trade :p
Thanks for doing the benchmarking! We need more of that kind of thing, often it's difficult to choose among crates offering similar functionality.
Sure!
You can install [cargo-edit](https://crates.io/crates/cargo-edit) which adds a `cargo add` command which will add the latest dependency.
I'll point out that xi-rope has [grapheme segmentation](https://github.com/xi-editor/xi-editor/blob/eeefad4fd6fb357f0e99c6c0cd78cb73e1a17617/rust/rope/src/rope.rs#L759) methods, and has a similar philosophy as Ropey here - segmentation is a layer on top of the rope storage, not built into the rope itself. Basically, Unicode grapheme segmentation rules are too complex to form an efficient monoid homomorphism, which is the mathematical underpinning of xi-rope.
Ah! Sorry I was driving into the office... should be fixed now. The reason I have the vector declared in the C++ but not the Rust code is because rustc unrolls a _lot_ more in the vector instantiation in Rust (+1 for Rust) but Clang unrolls a _lot_ more with a vector argument (is that a bonus? I don't know). The assembly is relatively equivalent for both this way, so you can see the major difference in the inner loop. 
I've been using this and I have nothing but great things to say about it.
Oh wow, thanks for putting in the work to benchmark these! A few notes: - xi-rope provides a single `edit` method that can do deletion and insertion at the same time. That may give it a performance edge over Ropey in certain cases, and I don't believe that's represented in the way the benchmarks are done. - Indeed, Ropey's performance has not changed since 0.9.2. This 1.0.0 release is almost identical to 0.9.2, just with cleaned up documentation etc.
Why on earth would you import size_of but not the traits.
Glad I could help. :)
This seems to be because [`HashSet` has additional bounds on its `Default` impl](https://doc.rust-lang.org/std/collections/struct.HashSet.html#impl-Default). It's a long-standing, hard to fix bug: https://github.com/rust-lang/rust/issues/26925
Just some more closing thoughts and links: I don't use Rust in my day-to-day work, so if I'm missing some things or these features I'm talking about are available, please let me know. I'm aware that there is [cargo-pgo](https://github.com/vadimcn/cargo-pgo) for PGO, but it doesn't look like it's compatible with current nightly and isn't maintained. It's also possible to do [PGO by hand](https://github.com/Geal/pgo-rust) today. The [Custom Test Framework eRFC](https://github.com/rust-lang/rfcs/blob/master/text/2318-custom-test-frameworks.md) has been merged. What work needs to be done to get it on stable, or to write a complete RFC? It's unlikely that [cargo-bench](https://github.com/rust-lang/rfcs/pull/2287#issuecomment-362431922) will ever land on stable, with the Custom Test Framework preferred as the means going forward. The [bencher](https://crates.io/crates/bencher) and [criterion](https://crates.io/crates/criterion) crates are excellent starting points. There has been some really good work to stablize [black_box](https://github.com/rust-lang/rfcs/pull/2360). [There's still a lot of work to be done with `const fn`](https://github.com/rust-lang/rust/issues/24111), and the major PR for [const generics](https://github.com/rust-lang/rust/pull/53645) looks like it's close to being merged. At the end of the day, there is a lot of great work being done and things in nightly or landing there shortly, and looking forward into 2019 it will be excellent to have these things land on stable. 
Because I hacked that example together at like 6 this morning before writing this post. 
Ohhh I've been thinking of accessing modules / structs in main.rs with different functions. I assume that's different and only applies to variables?
[duplicate?](https://www.reddit.com/r/rust/comments/aavcd3/video_35c3_how_to_write_pcie_drivers_in_rust_go_c/)
For Rust to beat C++, tooling is also very relevant. From the context of a JVM/Android and .NET developer, that occasionally makes use of C++ when needed, beating C++ means 1:1 feature parity with mixed language debugging on the respective IDEs, COM/UWP on Windows, GPGPU, and binary libraries support.
Here's current slack-app version: [https://github.com/cztomsik/slack-app](https://github.com/cztomsik/slack-app) ![slack-app](https://github.com/cztomsik/node-webrender/raw/master/docs/slack-app.gif)
For #1, you may find my crate [`numeric-array`](https://crates.io/crates/numeric-array) to be of interest, which wraps `generic-array` and implements num traits for the entire sequence in such a way that it can usually be optimized to SIMD instructions via autovectorization. It’s honestly staggering how well it worked out. 
I'm not sure what you mean by "mutex the thread". If there's state you need to share between threads/futures, a Mutex might be a good way to go if it needs to mutate. I think it really would depend though. How many readers are there, how often will it mutate, can you break a it up so that each piece of your state is under a Mutex so that a piece can be written to while the rest is unaffected (or even immutable)? I'd also look into updating threads shared state via a Channel if updates are handled by a single worker, sending an `Arc&lt;State&gt;` to each thread that they can reference. If it feels like a lot, it's because it is, so just be aware of all your options and vet them along with your assumptions among other developers and you'll probably land on a good solution. As far as Async goes, the last I had checked, the `async`/`await` syntax was stable, but it isn't backed by a real implementation outside of [Romio](https://github.com/withoutboats/romio) which is explicitly an experiment, so not something you'd want to use in production for work unless you're feeling confident. I don't really have any good advice for you on that one, since my Async experience has been with Actix, Rayon and just spawning `std::thread`'s and sending messages via a Channel.
IntelliJ is also open source: https://github.com/JetBrains/intellij-community/ (license is Apache 2.0), as well as the Rust plugin itself: https://github.com/intellij-rust/intellij-rust. The reason why I started working at JetBrains on IntelliJ Rust is mostly serendipity: I happened to discover Rust while interning at JetBrains, and I vividly remember seeing [this slide](https://youtu.be/jDQaIl_1Nfk?t=865) from Rust Camp 2015 keynote, which shows Turbo Pascal (JetBrains has some specific ties to Borland) with a snippet of code with comments (and even variable names!) in Russian. So I thought "can I build this?" and my internship pivoted from some ClojureScript experiments to writing a Rust IDE. The reason why I was able to continue to dedicate significant amount of my time to IntelliJ Rust is that I was employed by JetBrains. Also I am pretty sure that IntelliJ is currently the most advanced platform for building code analyzers ([Post IntelliJ IDEs](https://martinfowler.com/bliki/PostIntelliJ.html)). elisp is just not a very good language/runtime for building compilers. I am not too familiar with the Eclipse situation, it *might* have similar infrastructure to that of IntelliJ. However, the existing RustDT project didn't even try to implement it's own Rust compiler, so there had been nothing to reuse.
Well I don't know if it's a coincidence or what, but I happen to be using the same architecture. I'm learning about it and Rust. The only difference is that I want to do it with actix-web instead of Rocket, but I'll cross that bridge when I get there. Thanks, again. 
I know that's not the point of your post, but with some trying around I came up with a \[slightly more optimal version\]([https://godbolt.org/z/wN75PQ](https://godbolt.org/z/wN75PQ)) of your Rust code without any unsafe code. It unrolls the iteration to work on 8 values at once, instead of only 4. &amp;#x200B; It seems like all the information is there for LLVM, it maybe only needs a different optimizer pass over the LLVM IR or in a different order.
Any host that isn't shared?
Why did they choose not to include block comments in Rust?
It seems that if you actually compare the same code, Rust is actually more vectorized: https://godbolt.org/z/oU0hGG
First of all, I want to give a shout-out to the Xi project in general, which is amazing! I think the biggest difference is goals: xi-rope seems to be first-and-foremost a component of Xi and only secondarily a library (its last crate release was 1.5 years ago, and the current xi-rope in git master has changed quite a bit since then), whereas Ropey is first-and-foremost a library, aiming for API stability and solid documentation. Technologically, they have a lot in common: both are b-tree ropes, both have excellent performance, etc. I think the biggest differences from an API perspective are: - Ropey operates primarily in terms of `char` indices, whereas xi-rope operates in terms of byte indices. IMO this makes Ropey a bit easier to use, but you also pay a small performance cost for it. (Note: you _can_ operate in terms of byte indices with Ropey, but it goes a little against the grain.) - Ropey recognizes all eight Unicode-specified line breaks in what it considers to be a line, whereas last I checked xi-rope only recognizes LF and CRLF. In practice this probably isn't a big deal, since the vast majority of text files are either LF or CRLF anyway. But it may matter for some use-cases. - In general, Ropey provides a richer set of APIs. Ropey is designed to be easy to get into, but also to grow with you as your needs become more advanced. From a performance perspective my recollection (which seems to be corroborated by sephg's benchmarks) is: - Both are very fast, performing most operations in under a microsecond for typical documents. - xi-rope is faster than Ropey in the fastest case, but slower in the slowest. - Ropey's performance scales better to huge documents, where xi-rope starts to fall behind. Additionally Ropey's memory consumption is more consistent (which is something I put a fair bit of effort into). IIRC, xi-rope tends to explode in its memory consumption with certain editing patterns, whereas Ropey tops-out at around twice the document size even in the worst cases and is typically far less than that. But it's been a while since I did those tests, so I may be misremembering some of that. In any case, it probably doesn't matter for real usage, since the problematic editing patterns aren't likely to occur in practice.
Oh sorry, didn't see this one.
Also [on the Rust playground](https://play.rust-lang.org/?version=stable&amp;mode=release&amp;edition=2018&amp;gist=485b65144cd22430f6761fd43f79261d) this seems to use the packed SSE instructions.
1) Here are two code pieces for point one. Have I misunderstood it, are they not equivalent? First it's implemented in raw pointers and it vectorizes, secondly it's in a safe Rust iterator formulation, and both seem to be just as well or better than the C++ version. https://godbolt.org/z/iv6_xK Also, I'm using the compile option -Copt-level=3 so that we get all default "release mode" options in Rustc.
For the type traits, something like what you suggest exists already in the [num-traits](https://crates.io/crates/num-traits) crate. Not in `std` but a very central crate maintained by members of the Rust team. Unfortunately, however, at least in my tests with this in the past you end up with rather complicated trait bounds nonetheless if you want to do a very generic function and you still need to add all trait bounds for the operations you want to cover. See e.g. [here](https://play.rust-lang.org/?version=stable&amp;mode=release&amp;edition=2018&amp;gist=d8b29873f5d4b9398f8bc6c95f2f0390).
Two pointers to make your code vectorize: * Instead of using `&amp;mut Vec&lt;T&gt;` use `&amp;mut [T]`. Otherwise LLVM will not be able to LICM a load, because the necessary noalias metadata is currently [disabled](https://github.com/rust-lang/rust/issues/54878) due to an LLVM bug. * Use `-C opt-level=3` instead of `-O`, which corresponds to `-C opt-level=2`. In order to vectorize your unrolled loop the SLP vectorizer rather than the loop vectorizer is needed, and we only enable it at O3. Of course, for your particular reduced example the easiest way to vectorize it is to stop trying -- LLVM will vectorize a straightline loop without blocking just fine. Of course this may fail for more complicated code.
Another option is to make it installable/available offline with a service worker https://developers.google.com/web/fundamentals/codelabs/offline/ https://developers.google.com/web/fundamentals/app-install-banners/
How do you deal with something like `withFile "somefile" \x -&gt; return x` In rust, you would likely only get a reference to the file and this would give a lifetime error.
Nice! I knew there was a nicer way to do it with iterators. I'm no LLVM wizard so here's the actual emitted LLVM from both compilers for the inner loop clang++: ;&lt;label&gt;:33: 1 %34 = phi i64 [ 0, %19 ], [ %51, %33 ]. 2 %35 = phi i64 [ %20, %19 ], [ %52, %33 ] 3 %36 = getelementptr inbounds float, float* %8, i64 %34 4 %37 = load float, float* %36, align 4, !tbaa !10 5 %38 = fmul float %37, 2.000000e+00 6 store float %38, float* %36, align 4, !tbaa !10 ; ... lines 3-6 copied 3x in unrolled loop rustc: ; long mangled name 1 %iter.sroa.5.046 = phi i64 [ %39, %"_ZN101_$LT$core..slice..ChunksExactMut$LT$$u27$a$C$$u20$T$GT$$u20$as$u20$core..iter..iterator..Iterator$GT$4next17he48a3d2c2c80b344E.exit" ], [ %iter.sroa.5.046.unr, %"_ZN101_$LT$core..slice..ChunksExactMut$LT$$u27$a$C$$u20$T$GT$$u20$as$u20$core..iter..iterator..Iterator$GT$4next17he48a3d2c2c80b344E.exit.prol.loopexit" ] 2 %iter.sroa.0.045 = phi i64 [ %52, %"_ZN101_$LT$core..slice..ChunksExactMut$LT$$u27$a$C$$u20$T$GT$$u20$as$u20$core..iter..iterator..Iterator$GT$4next17he48a3d2c2c80b344E.exit" ], [ %iter.sroa.0.045.unr, %"_ZN101_$LT$core..slice..ChunksExactMut$LT$$u27$a$C$$u20$T$GT$$u20$as$u20$core..iter..iterator..Iterator$GT$4next17he48a3d2c2c80b344E.exit.prol.loopexit" ] 3 %25 = inttoptr i64 %iter.sroa.0.045 to [0 x float]* 4 %26 = getelementptr inbounds [0 x float], [0 x float]* %25, i64 0, i64 0 5 %27 = load float, float* %26, align 4 6 %28 = fmul float %27, 2.000000e+00 7 store float %28, float* %26, align 4 ; lines 4-7 copied 7x in loop unroll It looks like the only difference is that C++ isn't using `inttoptr` before loading the float. 
That's puzzling. The C++ one also changes if you take the vector by reference. Does anyone know why that happens? Possibly it's because the compiler knows that the memory is zeroed out already?
Good reminder that we should do more crate releases from xi. Partly it's laziness. Having dependencies would help! You're right about plain CR (and the other strange Unicode line endings, which I believe nobody uses). This was a deliberate tradeoff. A couple of other things to note: xi-rope now computes offsets in both UTF-16 and UTF-8, which is useful for interop with many GUI toolkits. Lastly, xi-rope doesn't use any `unsafe` for core rope operations, though the diff/compare logic does use unsafe for SIMD.
? Rust has block comments, with the exact same `/* syntax */` like other C-like languages.
My understanding of RRB Vectors is that they're actually very similar to ropes: both represent a flat array of data as a tree structure. So, I don't know enough details about RRB Vectors to be sure, but my suspicion is that Ropey's implementation is pretty similar to an RRB Vector anyway, it's just using a different kind of tree (with any trade-offs that implies). It would be really interesting to see someone implement an RRB Vector-based rope!
&gt; I'm not sure what you mean by "mutex the thread". If there's state you need to share between threads/futures, a Mutex might be a good way to go if it needs to mutate. I mean, standard Mutex will lock an entire thread right? Which is terrible if you're trying to use a task scheduler. Which, using a plain mutex is what I plan to do for the short term. Long term, I think an Async Mutex will be what I use - ala Qutex *(or similar)*. Still heavily planning though, learning how to lay the data structures out. 
Note that (at least on my system and maybe on yours) sha1sum links against OpenSSL's highly optimized implementation of SHA1. You'll need at least SIMD intrinsics or possibly hand-written assembly to match it. Getting within 50% with portable code is very well done.
And the clever part with the nested `k` loop is not even needed for all this to work: you can simply run over `arr.iter_mut()`. It seems like godbolt and the playground do something different.
More specifically I was asking for a service that hosts a website and runs a server _freely_ for a free domain. The first one I used to use, Hostinger, for instance, supported only PHP. There're various others (UOL too), but all of them only support PHP for pre-processing. For Rust's case I'd need a Linux shell, I think.
&gt;Instead of using &amp;mut Vec&lt;T&gt; use &amp;mut [T] What inspired this example was a computing linear convolutions where I was doing just that, and it compiles roughly the same. &gt;Use -C opt-level=3 I had no idea that this was a compiler option, thanks for the heads up! (and for future readers, put `opt=3` in your `Cargo.toml` under `#[profile.release]`). That said Clang with [-O2](https://godbolt.org/z/kKRRiG) will perform the autovectoriziation. &gt;This may fail for more complicated code, which is I assume the reason why you're trying to use this more explicit pattern Pretty much. I was just trying to come up with something trivial where I could generate inferior assembly from `rustc` than from `Clang`, but I guess I was naive about this particular example. More experimenting should probably be done to identify hiccups in code generation. 
I added the bit to C++ in order to make it more legible, since it unrolls a lot more of the code. The inner loop body is what matters in the comparison, rustc is actually pretty cool where it unrolls the vector instantiation entirely. 
&gt; That's puzzling. The C++ one also changes if you take the vector by reference. Does anyone know why that happens? Possibly it's because the compiler knows that the memory is zeroed out already? I think i was dumb and shouldn't have used `0.0` as the vector value. 
But why would I choose a tuple (keys of integers) vs an array or a vector (also use integers to get values)?
&gt;You're right about plain CR (and the other strange Unicode line endings, which I believe nobody uses). This was a deliberate tradeoff. Absolutely, and I think it's a solid choice! Honestly, the line ending support in Ropey has more to do with my sense of completionism than anything else. &gt;Lastly, xi-rope doesn't use any unsafe for core rope operations Oh yes, that's an important point: Ropey uses unsafe code. It's not arbitrary or haphazard, but it is in there, so that's an important thing to consider if your application may face adversarial conditions. Also, /u/raphlinus: just want to say what a huge fan I am of all of your rust work. Big fan of Xi, and I'm stupidly excited about your audio work!
Yeah, that makes sense - also I wasn't aware of Qutex. If a Qutex is not lockable inside a future, is it able to notify the task scheduler with `Async::NotReady` so that the scheduler itself doesn't lock up? I would also look into the actor model if you haven't looked at it before. Not saying you should pick it up, but it's really good at abstracting shared state access into message passing. It might be overkill for you, but I've found it helpful to reach for in the past.
I appreciate this writeup, but is performance the reason that new projects are still choosing C++ instead of Rust? tbh, as a newcomer I would say that some of the biggest stumbling blocks to Rust are the module system. If you don't understand it (and I still don't!), you literally can't do *anything* with the language. Full stop. `#include "name-of-file.h"` might be primitive but it's also extremely straightforward to understand. 
The release profile in Cargo.toml is already opt-level 3
Please can someone convert this to pdf :|
&gt; If a Qutex is not lockable inside a future, is it able to notify the task scheduler with Async::NotReady so that the scheduler itself doesn't lock up? No idea, I've not used it - but their pitch is async-safe locking. Which makes me assume that they're using a small atomic to acquire lock and "blocking" with a non-blocking `NotReady` when the lock isn't ready. &gt; I would also look into the actor model if you haven't looked at it before. Not saying you should pick it up, but it's really good at abstracting shared state access into message passing. It might be overkill for you, but I've found it helpful to reach for in the past. Out of curiosity, does the Actor model solve anything over Async? It sounds like it's an abstraction but I'm unclear if it's trying to solve problems similarly solved by Async.
You won't be able to use rust for that. Shared hosting sucks, I get that you want things free but there are VPSs for 2.5 dollars or w/e and Heroku can be free if you use it up to a point.
You choose a vector if you need a list of the same type of element but don't know how long it's going to be (it's resizable at run time.) You choose an array if you need a list of the same type of element and you know how long it is. You choose a tuple if you have a single "thing" with multiple properties that you want to quickly bundle together. It's not a list of things, it's an association of things.
Yes. In Rust, that outer stuff (modules &amp; definitions) aren't actually a scope like they are in other languages (like JavaScript or Python), it's just a big sea of definitions. When they say a closure can access variables, they specifically mean it can access runtime values.
&gt;I appreciate this writeup, but is performance the reason that new projects are still choosing C++ instead of Rust? It's not the only reason (the biggest reasons would probably be talent pool and technical debt), but it's a reason nonetheless. &gt;\#include "name-of-file.h" might be primitive but it's also extremely straightforward to understand. `#include` isn't really a module system, and it's possibly the worst design decision in C and C++ (especially with templates). You should compare the module system of rust to that particular mechanic. I'll agree the module semantics are a little wack, but it makes sense once you go through it a bit. 
It's definitely a reason, but I'm not sure that it's The Thing To Focus On in 2019 is all I mean. As to `#include`, I definitely agree that it's Not Good, but it is _Approachable_. If people can't use it, the Goodness doesn't do you any Good! Developer tools / languages win when they are Approachable (see: PHP, React, etc etc). Rust's module system needs to be both Good _and_ Approachable_
Why was \`impl\` created so that adding functions to structs is separate? I'm coming from JS, so I'm not sure if this is a normal thing for other languages, but I would have thought if I'm adding fields to a struct definition that I would be able to add a function to it as well.
Thanks for the heads up! This definitely covers a large surface area of what `&lt;type_traits&gt;` does, at least for numeric types. There are some things though that you can do in C++ that are trickier. Some examples are `is_struct`, `is_union`, `is_constructible` or `is_trivially_constructible`, `is_member_function`, etc. 
&lt;10us? How many instructions could that be?
Actors are a different paradigm completely. An Actor can receive a set of messages, send messages and return responses to messages and have their own state. State is only shared between Actors as an immutable message. So, saying I had some shared state (maybe a `counter`) that six other threads wanted to increment, I could have a `Counter { count: usize }` Actor that receives a few messages, one to increment the counter and one to read the counter's value. Other Actors (or just threads) could then interact with that state in a non-blocking fashion via message passing. I'm not sure how much help it would be with your situation, but it's a handy abstraction to be aware of in case you find yourself needing to coordinate complex tasks in an asynchronous way. Here's a site with a quick intro if you have a few minutes to kill: https://www.brianstorti.com/the-actor-model/ . Also, I don't think there's a solution for Distributed Actors in Rust quite yet, which the site mentions.
That's fair criticism. I guess a cooler take here is that most people have been saying that 2019 should focus on stabilizing what they've said should be stabilized (const fn, const generics, async/await, etc), and improve quality of life through tooling and compile times. If that's the decision the community makes, then priorities still have to be taken. I think focusing on performance is a way to go about that, both internally in the compiler, and focusing on stabilizing things that can improve performance (or be used to improve performance, like benchmarking/profiling). 
Oh, thanks. I was reading [the book](https://doc.rust-lang.org/book/ch03-04-comments.html) and it says you need to put `//` on each line. I was just curious if there was a reason behind that, but I guess that section is just incorrect. Thanks!
Oh nice, `unreachable_unchecked()` is a thing so we can turn proofs from Prusti into optimizations already! Its use over `assert!()` still has to be justified by benchmarks though. For one, `assert!()` could catch unsafe code going haywire while Prusti assumes that all unsafe code is correct.
The current situation with Rust is that it's pretty good at codegen if you tweak a few knobs, but is not that great on default settings. More info: https://github.com/rust-lang/rust/issues/47745
The "gross" C++ looks more readable and less verbose than the rust version.
Thanks. I didn't know about `IPV6_V6ONLY`, but that's along the lines of [what I was thinking](https://github.com/kevincox/rustymedia/issues/9#issuecomment-451210229). Apparently UPnP multicast in IPv6 should be `FF02::C:1900`, and if I `bind` and `connect` to that address as per below it runs. However, if I then try to `send` anything, it crashes with `Error: Os { code: 102, kind: Other, message: "Operation not supported on socket" }`. use std::net::UdpSocket; fn main() -&gt; std::io::Result&lt;()&gt; { let socket = UdpSocket::bind("[FF02::C]:0").expect("couldn't bind socket"); socket.connect("[FF02::C]:1900").expect("couldn't connect"); socket.send(&amp;String::from("foo").into_bytes())?; Ok(()) } 
rustup on both
Actually, could you elaborate on why did you use unsafe code? I'm participating in the [Rust Secure Code WG](https://github.com/rust-secure-code/wg) and it's very valuable to understand why people resort to `unsafe` so we can identify missing safe abstractions, etc.
That's because I took away the truly equivalent version that has similar compile time constraints . Originally i had typed up template&lt;class T&gt; typename std::enable_if &lt;std::is_floating_point&lt;T&gt;, void&gt;::type void foo (std::vector&lt;T&gt;&amp; v) { // etc... } But I eliminated that for brevity 
I think my major takeaway from this post is that we need some community folks to put together some documents on optimizing in Rust, especially when it comes to tuning cargo and rustc. 
Fantastic work. Thank you for your effort!
One of the reasons is that `impl` blocks allow you to specify trait requirements for generic type parameters. For example: pub struct Foo&lt;A&gt; { ... } impl&lt;A: Eq&gt; Foo&lt;A&gt; { pub fn do_stuff(&amp;self) { ... do stuff that requires A: Eq ... } } You *could* add these constraints to lots of individual methods, like so: impl Foo&lt;A&gt; { pub fn do_stuff(&amp;self) where A: Eq { ... } } But when you have to re-state the same trait constraints for N different methods, it gets repetitive and frustrating. Being able to specify all of them on the `impl` itself is super helpful. Also, you can add `impl` methods on way more than just a single type. You can add `impl` methods on generic type instantiations. For example: pub struct Foo&lt;A&gt; { a: A ... other fields ... } impl Foo&lt;String&gt; { pub fn do_thing(&amp;self) { ... } } impl Foo&lt;i32&gt; { pub fn do_thing(&amp;self) { ... totally different behavior ... } } fn example(x: &amp;Foo&lt;String&gt;, y: &amp;Foo&lt;i32&gt;, z: &amp;Foo&lt;usize&gt;) { x.do_thing(); // something happens y.do_thing(); // something different happens z.do_thing(); // compiler error: no do_thing() method defined } This is way more flexible and powerful than how most languages deal with declaring and resolving methods. Also, remember that `impl` is used both for adding "ordinary" methods to a type, as well as implementing traits. It provides a really nice symmetry between the two cases. And remember -- you don't have to implement a trait on a specific type that you define. You can define it for *any* type. For example, let's say you defined some trait `Foo`. Your crate (that defines `Foo`) could `impl Foo` for lots of different types, such as: pub trait Foo { ... } impl Foo for (i32, i32) { ... } impl&lt;T&gt; Foo for Vec&lt;T&gt; { ... } impl&lt;'a, T&gt; Foo for &amp;'a [T] { ... } 
Another thing in this space... this blog post came out of the recent C++ firestorm on gamedev twitter: http://lucasmeijer.com/posts/cpp_unity/ One thing he mentions here is "performance as correctness", which is something that really only Unity's C# work is doing right now, as far as I know. Something Rust could provide to compete in this space and improve on C++ is a way to specify that a loop *must* be optimized to some specific set of conditions in an optimized build (vectorized, inlined, unrolled, etc), and a compile error happens if some change is made to the code that causes any of those conditions to not be met.
&gt; Actually, LLVM already uses numeric range analysis in its optimizer, and can e.g. elide bounds checks on array access. Yes, however in Release mode Rust uses wrapping-arithmetic (by default) since the cost of overflow checks is too high with the current implementation. Those, in turn, means that the statement `i = i + 5` can actually *reduce* the value of `i`. On the other hand, if Prusti proved there is no overflow, then the operation could be marked as NOT wrapping (`nuw nsw`), and then the LLVM range analysis could deduce that `i = i + 5` can only increase `i`.
That would be pretty neat. I can think of a few synonyms that I commonly mix up when switching between languages such as get - fetch, save - store, remove - delete, append - push, create - init, replace - substitute, etc. Although it could also lead to more confusion when the compiler starts suggesting unrelated functions.
This is very helpful, thanks for sharing it! It will make a fine addition to my collection of tabs that are constantly open on my second monitor, right next to the Book and Standard Library documentation.
That is, [until they aren't](https://www.reddit.com/r/rustjerk/comments/abljmo/rust_has_better_error_messages/)
Not much. In C++, the state-of-the-art wire-to-wire is ~2.5 us. This includes over 1us spent in the hardware/network stack, leaving about ~1.2us for actual computation. At 5 GHz (overclocked CPU), that's 6,000 cycles; at 4 GHz, it's ~5,000 cycles. Note that an integer multiplication is a single instruction, but 3 cycles. In general, those hot-paths have very little logic, as you can imagine: - bit of decoding, - yay or nay, - table look-up, - maybe some extra validation/scaling, - bit of encoding.
Listing 2-3 of https://doc.rust-lang.org/book/ch02-00-guessing-game-tutorial.html is: use std::io; use rand::Rng; fn main() { println!("Guess the number!"); let secret_number = rand::thread_rng().gen_range(1, 101); println!("The secret number is: {}", secret_number); println!("Please input your guess."); let mut guess = String::new(); io::stdin().read_line(&amp;mut guess) .expect("Failed to read line"); println!("You guessed: {}", guess); } The doc states: &gt; First, we add a line that lets Rust know we’ll be using the rand crate as an external dependency. This also does the equivalent of calling use rand, so now we can call anything in the rand crate by placing rand:: before it. What is the line that this is referring to? * I thought it was the `use rand::Rng;` but the wording suggests not. The next paragraph also refers to entering this line. * I thought maybe it was the `Cargo.toml`update but the previous sentence explicitly refers to editing `src/main.rs` 
This is horrible to read on mobile (Firefox)
&gt; The normal references in Rust support only tree-shaped data structures, so your "network of friends" is impossible to represent with them. This is technically a little too strong of a statement- references can form arbitrary graphs as long as the objects have the same lifetime. For example: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=4d8f6e6ac4fe8429af8fd64991669eb9 I've given them the same lifetime just by declaring them in the same `let` statement here, but /u/kuroljov could achieve the same result by allocating them in an arena, as in this post: https://exyr.org/2018/rust-arenas-vs-dropck/ Of course you're right about the "delete a person" scenario. An arena doesn't make that very straightforward, so often `Rc` or a map of IDs is good enough. But if you're implementing a data structure with a more specific shape that you understand (perhaps you just want parent pointers in a tree?) you could encode that knowledge with a bit of encapsulated `unsafe` that deletes a node only after cleaning up any references to it.
Everything in rust is trivially constructable, moves are defined to be `memcopy`. `Copy` allows you to reuse the value. There are no constructors in rust so `is_constructable` that’s not particularly relevant. All member functions are statically dispatched in rust unless you take an object by `dyn Trait`, so I don’t think `is_member_function` is particularly useful either. I’d love something like `is_struct` and `is_union` and a few other things to put `repr` blocks into the type system which would be really nice for safe, highly optimized io using `mmap` and friends. 
I'd like a way to check at compile time if a struct implements a function by its name, such as `new` with zero arguments. So not quite the same as "trivially constructible" but "can I construct this struct with private fields using `new()`. 
If you have C++14 at hand, use `enable_if_t&lt;...&gt;` instead of `typename enable_if&lt;...&gt;::type`, e.g. template &lt;typename T&gt; std::enable_if_t&lt;std::is_floating_point&lt;T&gt;::value&gt; foo (...)
It’s a bug in the text, can you check the nightly book and let me know if it’s fixed there?
They’re not considered idiomatic, so we don’t cover them in the book.
Wish we had that upstreamed yesterday. It’s so close!
Check out chapter four of the book for a real in-depth answer to this.
I'm working on something for it right now. I'd like to make it happen soonish.
The ‘Default‘ trait provides that
Amazing, thank you so much!
Oh I forgot about the requirement for same types. Thank you!
Writing new bugs alongside new features. Job security is an art form.
This looks like it could really be handy...if there was some reasonable way to view it. * There doesn't seem to be any way to export it * It's not flowable or formatted to page size, so I can't really print it * I can't use the referenced web page offline * Using a massive image as a reference doesn't seem practical How do others see themselves using this?
Unfortunately I don't have time to go into your code at the moment, but I just want to say that I found [this article](https://gist.github.com/jFransham/369a86eff00e5f280ed25121454acec1) extremely helpful when I was learning how to optimise my Rust code.
Not really? That requires authors to implement `Default` for their structs, but you don't see that in many crates which have a function named `new` that takes no arguments. Is that on the crate devs? Sure, but it's also a reality. 
Guaranteed performance is *really* hard, though. Essentially, you need the *optimizer* to have a *optimize or fail* mode, and LLVM doesn't have one at the moment so rustc would need to perform the optimizations itself. This is not that outlandish, as MIR is a good target for high-level optimizations. However, at this point, it may very well be simpler to offer *in-code* facilities to directly code the optimizations. For example, by having safe SIMD crates.
I agree the Rust community is better than the go community . There are a few bad apples on stack overflow specifically making it hard to ask questions about rust without it getting flagged incorrectly and deleted. 
Well, howdy! Thanks for being here and wanting to maintain the culture of kindness! :) 
Why can't I export `proc-macro` definitions? I've found myself writing _a lot_ of boilerplate when working with `proc-macro` (no `quote!` is insufficient for my usages), and I wanted to export a helper library. But apparently it is a compiler error to write a `pub fn` that accepts a type `::proc_macro::TokenStream`? Whats up with this? Will it change?
That is the trait for it, if it’s missing and there’s an equivalent `new` I’d consider it a bug, then file a PR or use a wrapper new type that does implement it. This is more a philosophical issue than a feature issue in my opinion. Rust traits just don’t work that way and likely never will. It’s like private fields. Just because you want access doesn’t mean you should `#define private public`. The rust system does have real benefits: for error messages, there is less chance of accidentally calling an unrelated method, and it is faster for compilation times. It also has its downsides in that it’s more difficult to cajole some else’s code into doing something that it wasn’t designed for, even in trivial and annoying cases like this. On a more practical note I think it was a mistake to provide `new` methods with no arguments in the standard lib. I think it makes people forget about default, even though it’s derivable!, and pushes a convention over a handy type system integrated tool. 
Absolutely, I'd be happy to! As a starting point, please read Ropey's [design overview document](https://github.com/cessen/ropey/blob/master/design/design.md). In particular, the section about memory layout. Ropey uses unsafe code in precisely 4 places: * `str_utils.rs`, which contains functions to do things like count chars and line endings in a string. * `tree/node_text.rs`, which implements a stack-allocated fixed-capacity string with accompanying edit operations. * `tree/node_children.rs`, which implements the container for a node's children. * A single method, `Rope::from_reader()`, in `rope.rs`. This is a one-off. `str_utils.rs` contains the overwhelming majority of the unsafe code in Ropey, and it uses it for optimizations. All of the functions can be implemented with only safe code (and were at first), but are much slower that way. They're all very hot functions, and optimizing them had a significant positive impact on Ropey's performance. I don't think there's much that can be done to remove this unsafe code, since a lot of it is optimizations for pretty niche things: even if there were a crate for it... I would probably be the only consumer/maintainer, so it wouldn't practically be any different. `tree/node_text.rs` uses the unsafe code for a couple of things: * Implementing the `Array` trait from the `smallvec` crate. This is only necessary because of the API gymnastics that `smallvec` has to do because of missing const generics. Once Rust gets const generics, I expect this unsafe code to disappear. * Implementing edit operations on the fixed-capacity string type (i.e. insert, remove, split\_off). Looking at things from a higher level, if there were a crate that implemented a fixed-capacity string type with `insert_str()` and `remove_range()` methods, that would probably be enough. Although to avoid the same impl issue above with `smallvec` we'd need const generics. `tree/node_children.rs` contains what is to me the trickiest/most worrying unsafe code, because it involves manually handling drop semantics. The data structure here is conceptually a lot like an entity component system, except with fixed-capacity/stack-allocated arrays that share their `size`. The reason for this is simply to squeeze more bytes into a smaller space. Having said that, this may not be worth its weight anymore, and I've already been considering removing it for an all safe data structure. Finally, the `Rope::from_reader()` method uses unsafe in precisely two places: 1. To skip a single redundant utf8 validity check where the utf8 has already been validated. 2. To shift bytes around in an array. This *can* be done safely, but is much slower. This unsafe could be easily removed if the `copy_within()` method on `&amp;mut` slices were stabilized, since that's literally identical to what I'm doing. Hope that helps!
ლ(ಠ益ಠლ)
This is really impressive, and the app code looks just as simple as one done in DOM. Have you figured out text overflow and scrolling up to fetch history?
You may realise this, but `push` rarely resizes (only `O(log final_size)`. It may be harder to get an accurate value to pass to `reserve` than to just `push` repeatedly. That said, if there is an accurate estimate of the final size, `reserve` can definitely help if reallocations are high on a profile.
How to use rust on a secondary drive? is it just creating a new cargo on that drive or what?
Rust 2018 module system is really simple, they eliminated the worst pitfalls in the new edition
The first step for optimising is running it under a profiler, like `perf` on Linux or Instruments.app on macOS. This will help highlight what pieces of code are taking the most time, and thus focus your energy where it matters. Also, just to double check: are you compiling the Rust code with optimisations on (`--release`)?
What makes the DFA for \w so big? Is it because the character category of a particular bit pattern is more or less incompressible data?
Because this is a tree and every value is owned by something without forming a loop, I think you can replace the `Option&lt;Rc&lt;RefCell&lt;TreeNode&gt;&gt;&gt;` with `Option&lt;Box&lt;TreeNode&gt;&gt;`. Please note that `Box` is there so the compiler can figure out the size of the `TreeNode` struct, if it contains another version of itself which contains another version of itself it essentially needs an infinite amount of memory to ensure this works. By using `Box` we just point (with ownership, it's one of the smart pointers) to the other `TreeNode` which solves this problem as a pointer does have a fixed size. I realise that you can't change the struct declaration but thought I would show something I believe is more idiomatic. I'd like to give some advice on your code but leetcode gives a 404 error. Could you find another way to share it?
I'm on windows, so it's harder to find tools, seems that rust has more tools for linux than for windows. Wow, I'm not so worried now, with release it takes almost no time, less than 1 second to process 131 chunks of 4096 elements (while debug takes 20 seconds). So maybe later will install some linux to do the optimization and profiling, when the library get a release candidate state. Thanks for the advice.
But how much time is a L1 cache miss? Or L2 cache miss? Do they warm up the caches and try to keep everything within L1+L2?
800KB is how big it is when you don't minimize it, don't shrink its alphabet into equivalence classes and represent state identifiers as `u64`. So it's pretty much the worst case. e.g., `ucd-generate dfa . '\w'`. But if you minimize it, it shrinks to 280KB, e.g., `ucd-generate dfa . '\w' --minimize`. If you further shrink the alphabet and use `u16` to represent state identifiers, e.g., `ucd-generate dfa . '\w' --state-size 2 --classes --minimize`, then it shrinks to 64KB. If you then use a sparse representation, e.g., `ucd-generate dfa . '\w' --state-size 2 --classes --minimize --sparse`, it goes all the way down to 8KB. That's still not that small from what you might naively expect, and it can grow pretty quickly, e.g., `\w{10}` as a sparse DFA is 120KB *and* it took ~12 seconds to generate. If you actually look at the DFA itself, e.g., `regex-automata-debug /dev/null '\w' --debug &gt; words` ([output](https://gist.github.com/8ea9283af853fd53d8a20f7564a36139) (~400 states, shown mostly as a sparse representation, since a dense one is not really readable)), then there is actually quite a bit of redundant information if you wanted to compress it using classical methods. But of course, you trade search time. I did experiment a bit with compressing my sparse DFA representation a bit more, but it hurt search times, made the code more complex and didn't seem to help too much, although I didn't spend a ton of time on it. But in terms of *recognizing* the language, then automata theory says that a minimal DFA is the best that can be done from the perspective of a finite state machine. Conjecturally, `\w` is one of the worst cases because it contains both a large number of codepoints (128,640) and is spread out quite a bit over the entire set of codepoints. e.g., `regex-debug utf8-ranges '\w' &gt; words-ranges` ([output](https://gist.github.com/c4f5deb61b093f9fa72dba32886f382b)). So you wind up needing a lot of states to disambiguate across a selective range of a significant fraction of all codepoints. It's unfortunate that such a common case like `\w` also happens to be one of the worst offenders.
Structs don't store key value pairs. That's how interpreted languages like Python and JavaScript do it (in theory) but not compiled languages like C and Rust. From a code generation perspective, a field is a compile-time name for a fixed offset into a sequence of bytes. At runtime this means access is implemented with a single `add` instruction (or, indeed, without an additional instruction at all on x86/amd64) instead of a dictionary lookup which is at minimum **thousands** of times more expensive. The only reason to choose a `HashMap` is if you don't know ahead of time what keys exist or when they'll be accessed; and even then, you need to make sure you're not implementing an algorithm in an unnecessarily complex way due to the prevalence of interpreted languages.
Most people who are making a serious objective consideration between which language to use for which projects are not going to be hung up on the syntax and conventions for modules. You'll learn those things before you ever start a project for which the language choice matters. If the language choice *does* matter, it is probably because you care about performance, maintainability, portability, available libraries, etc. Getting code to compile for beginners were relevant, I don't think C++ would even be an option.
Thank you! Here's the [playground link](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=796175da0ba0713769dbb6e78cec2122). I think this should work Yes, unfortunately, the data structure is fixed. But, I'm still not sure why `Rc&lt;RefCell&lt;T&gt;&gt;` was used in this problem though. &amp;#x200B;
If you've been watching the game development "C++" lamentations threads the biggest complaints about C++ have been: * Compile times * Debug build performance * Complexity/Cognitive load If Rust was able to compile faster than C++ I think that would be quite compelling. Rust's compile times aren't great but they have been steadily improving. I think the other thing that would help for adoption in this area is a distributed and cached build solution for larger projects to reduce compile times with more hardware. I think Rust probably has similar issues with debug build performance in that inlining is really required for a lot of constructs to be performance, not sure what can be done here. Rust is a complex language, maybe not as complex as C++ but the borrow checker is singing people are going to struggle with coming from other languages. What Rust has over C++ fit me in this area is coherence. C++ has so many little inconsistencies that you need to keep in your head as a developer, so far Rust feels very coherent to me. I don't think there's necessarily anything for rust to do here other that keeping C++ in mind as a cautionary tale.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/cpp] [Rust 2019: Beat C++](https://www.reddit.com/r/cpp/comments/acn05b/rust_2019_beat_c/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Agreed. The tooling is nowhere *near* the level of support something like Visual Studio offers. 
&gt; C++ has so many little inconsistencies that you need to keep in your head as a developer The recent "modern C++" discussion has really underscored that for me. How many competing STL variations are there? Did you choose to use exceptions or not? What about RTTI? How many different naming convention permutations can we come up with? What do you mean remove_if doesn't actually remove the elements?
If number of *active* (i.e. running) threads exceed number of cores, they won't be able to run full time, and you can get excessive context switching, poor use of caches. If you have a parallel task, then yes, having number of threads roughly equal to cores is generally a pretty good rule of thumb, though less can sometimes be better due to lock contention and cache use. Blocked threads aren't running, so only use up a bit of memory.
It still unnecessarily makes a distinction between modules and the filesystem, making a fair bit more complex than python, javascript, etc.
If anyone could export to a manageable format and upload it, that would be awesome! I tried but didn't have much luck doing it.
Thanks for the detailed answer! This is unfortunate indeed. It sounds like the issue is basically unsolvable :( Assuming they're distributed randomly, the ideal encoding would give log_2 (137374 choose 128640) =~ 5.9KB, so 8KB is very good, considering. Of course they're far from distributed randomly, but still. Is the sparse DFA encoded in the way it's displayed? I used the following method. For a state you have an array C of characters encoding the transition ranges, and an array I of state IDs, such that characters in the range C[i]..C[i+1] transition to state I[i+1]. The C[-1] is implicitly 0 and C[last_i + 1] is implicitly the highest character + 1. The nice thing about this is that you can do a branchless binary or linear search that also handles the edge cases in a branchless way to find the ID of the next state. Basically something like: l = -1, h = len while(h - l &gt; 1){ m = (l+h)/2 if(C[m] &lt; currentChar) l = m else h = m } next_state = I[h] (the if should compile to a pair of CMOV's, but there are faster but less readable ways to do this search) However, you can sometimes get a lot of transitions to the failed match state, which your representation seems to avoid? OTOH, in the \w DFA the next range almost always starts where the previous one left off.
Wow, this was very informative! Thanks! I confess that `str_utils.rs` looks rather scary to me. That's a lot of pointers and casts, I feel like I'm reading C all over again! Perhaps functions such as `char_to_byte_idx_inner()` could be simplified by using [slice.align_to()](https://doc.rust-lang.org/std/primitive.slice.html#method.align_to)? This will give you byte slices that can be operated on as normal, without raw pointers, and you can use iterators on them to bypass bounds checks. Something like [faster](https://github.com/AdamNiederer/faster) might also be able to handle this in theory, although I don't think this particular crate mature enough for your use case.
One major benefit is to visually separate the data that every instance of the type has from the functions that operate on it. If we have a: ``` struct Foo { a: String, b: u32, c: bool, } impl Foo { fn foo(&amp;self) { ... } } ``` We can immediately see what data we're going to be throwing around whenever we talk about a Foo: exactly those three things and nothing more (except for some padding, depending on alignment.) We can also just look at the impl block for its operations. The language could have been designed so that you would put the functions in the struct definition (like something like Swift or Java), but it would potentially look a lot messier.
First of all, here is what I came up with: use std::rc::Rc; use std::cell::RefCell; impl Solution { pub fn is_unival_tree(root: Option&lt;Rc&lt;RefCell&lt;TreeNode&gt;&gt;&gt;) -&gt; bool { match root { None =&gt; true, Some(n) =&gt; { let val = n.borrow().val; is_correct(val, &amp;Some(n)) } } } } fn is_correct(value: i32, node: &amp;Option&lt;Rc&lt;RefCell&lt;TreeNode&gt;&gt;&gt;) -&gt; bool { match node { None =&gt; true, Some(n) =&gt; { let n = n.borrow(); if n.val != value { false } else { is_correct(value, &amp;n.left) &amp;&amp; is_correct(value, &amp;n.right) } } } } I do agree `Rc&lt;RefCell&lt;TreeNode&gt;&gt;&gt;` is unnecessary, has more overhead and makes for ugly code. I couldn't get it better than this. Solution::is_unival_tree(root_rc.borrow().left.clone()) &amp;&amp; Solution::is_unival_tree(root_rc.borrow().right.clone()) I think `root_node` should be used here instead of `root_rc`, maybe you even want to do `let node_rc = node_rc.borrow();` to stop you from calling `borrow()` again. The `clone()` here is really ugly but necessary if you want `Option&lt;Rc&lt;RefCell&lt;TreeNode&gt;&gt;&gt;`, you could however easily use `&amp;Option&lt;Rc&lt;RefCell&lt;TreeNode&gt;&gt;&gt;` which makes passing a reference enough. It would become: Solution::is_unival_tree(&amp;root_node.left) &amp;&amp; Solution::is_unival_tree(&amp;root_node.right) Also something I'd personally do (although more subjective): let node_r = node_r.borrow(); if node_r.val != root_node.val { return false; } To: if node_r.borrow().val != root_node.val { return false; } The `if let Some` is the best you can do with your function architecture (I think, could be wrong). &amp;#x200B; All in all, it was indeed the `Rc&lt;RefCell&lt;T&gt;&gt;` which caused your code to be messy, but with some knowledge you can avoid too much `clone()`. Minimising the usage of `borrow()` is a lot easier. Know when what you are doing is a hack/workaround to get it to compile or what you actually mean to do. This means reading the [documentation](https://doc.rust-lang.org/std/rc/struct.Rc.html#impl-Clone), learning what a function actually does and what a datatype actually represents. Particularly important when working with smart pointers.
You have two options: # 1) Use Rust from your home directory Provided you already installed Rustup in your home directory, you should be able to compile and run projects on your secondary drive. # 2) Download &amp; unzip an archive of the latest Rust If you go [here](https://forge.rust-lang.org/other-installation-methods.html), you can pick an appropriate archive for your OS. Then you can unzip it to your secondary drive, and add the `bin` directory of the unzipped archive to your PATH. This will work equivalently to method 1, but you will have to add the directory to your PATH each time you open a new session and have to manually update the archive.
You probably want r/playrust. This is a subreddit about the Rust programming language, not the game.
Oh, sorry then xD
Beating C++ will not be matter of language improvements, the language is already better due to intense redesign. The tools and libraries are what will determine if rust ends up being used as more than just a novelty.
It was interesting to look over, unfortunately it doesn't even bookmark properly, so ... Why do web developers hate the web so much these days?
Try clang trunk :) it fully unrolled your C++ code :D 
The enable_if/SFINAE is only needed if there are multiple versions of foo. 
This looks very interesting. Combined with druid this looks like a wonderful yet lightweight approach to doing GUIs. Looking forward to contribute a widget or two someday :)
&gt; I’d love something like is_struct and is_union and a few other things to put repr blocks into the type system which would be really nice for safe, highly optimized io using mmap and friends. Wouldn't it be as simple as struct and union each implementing a marker trait?
Thank you. That was very helpful
&gt; The space is incredibly fragmented, out-of-date editor plugins sometimes create more friction than solutions, and no single package provides for the full use case. I find that C++ has identical problems, if you consider that the ecosystem is split across three major compilers each with their own surrounding set of tooling.
Definitely could be. There are some details about whether the trait should apply recursively that are fairly important but tricky, eg should the `Packed` trait apply if and only if all of its members are packed or just if the top level is. 
Seems like the documentation is broken now
Are these positions paid?
It appears to be fixed there - thanks!
Awesome. Thank you and sorry!
All positions are volunteer, as of the time being. [We've just formalized as a non-profit](https://www.amethyst-engine.org/blog/non-profit-announce/), so we will be seeking grants to help fund our development.
Right, but while it’s fragmented each of your compiler options are much more robust and feature-rich, especially when it comes to debugging and (to a lesser extent) profiling.
Yep, gave up and hosted it on way-cooler.org
I came up with something very similar, only using the additional neat trick of iterating over the value of an option. (node.iter().all(|n| ...) ``` pub fn is_unival_tree(root: Option&lt;Rc&lt;RefCell&lt;TreeNode&gt;&gt;&gt;) -&gt; bool { fn has_value(node: &amp;Option&lt;Rc&lt;RefCell&lt;TreeNode&gt;&gt;&gt;, value: i32) -&gt; bool { node.iter().all(|n| { let n = n.borrow(); n.val == value &amp;&amp; has_value(&amp;n.left, value) &amp;&amp; has_value(&amp;n.right, value) }) } root.iter().all(|r| { has_value(&amp;root, r.borrow().val) }) } ```
Couldn't you work around this with a weak RC or something?
You're in the wrong subreddit. This subreddit is for the rust programming language. You want /r/playrust
Oh crap haha thanks 
It's cleaner to just `.drain()` the Vec.
Great to have you here! If you haven't already, you might want to see if there's a Rust meetup in your area (see https://www.meetup.com/topics/rust/, https://community.rs/meetup-map/, and https://calendar.google.com/calendar/embed?showTitle=0&amp;showPrint=0&amp;showTabs=0&amp;showCalendars=0&amp;mode=AGENDA&amp;height=400&amp;wkst=1&amp;bgcolor=%23FFFFFF&amp;src=apd9vmbc22egenmtu5l6c5jbfc%40group.calendar.google.com&amp;color=%23691426)
I love these kind of posts and the replies. I learn more about the languages from them than just reading alone. Thank-you all. 
Lol no apologies needed. If I have a problem I'll demand a refund for my $0 :P Thank you for your work on the Rust Book. It is one of the more impressive pieces of technical documentation I have read* and does an excellent job of conveying concepts both in and outside the context of Rust that is helping me mature as a general programmer. (* haven't read all of it. Got about half the way through some months ago and am restarting it now) 
No, this isn't. If you only need one or two tokens or you're *specifically* getting a string as input, then that's fine, but what you're *probably* looking for is the [`quote`](https://docs.rs/quote). It's a macro that you put a bunch of tokens into, and it returns a type that you can call `.into()` on to get a TokenStream. It also allows type-safe token interpolation by prefixing an ident with `#`, and your editor will syntax-highlight the tokens as well :).
Yes, Rust is actually a fantastic for this. I'm actually doing so right now ;) Look at how easy it is: https://anderspitman.net/blog/rust-docker-barebones/ Note that the blog post assumes you are building locally and using a Docker image for the last stage. The only difference with multistage is simply having a first stage ;) It gets more complicated when you are linking to C libraries as those need to be compiled statically too. So using OpenSSL is painful. See for example https://stackoverflow.com/questions/41723327/how-to-build-an-executable-that-depends-on-curl-for-x86-64-unknown-linux-musl The first time I tried this, I was surprised how tiny (couple of MB) a full blown web server could beat Python implementation is ease of use and performance.
That does seem a lot nicer. Thanks!
That depends on what you define as unnecessary. I stick multiple modules in single files to work around the module being the boundary at which the private/public distinction takes effect. Without that, I'd be pushed in the direction of Java's "one public class per file" decision which forces a forest of tiny files and an IDE to navigate them.
Thanks 
I'm new to Rust too, and I'm interested in learning it as a system and close-to-hardware language. Could that be a scope where unsafe blocks are justified? Also, sorry for off-topic, but a lot of my colleagues say that if I really want to learn to program "bare-metal", I should ditch all that new and polished "hipster" languages like Rust and learn ol' good C -- with all its "beauties" as constantly shooting myself in the leg and learning how to manage memory manually all the time. Are they right? Would learning C first make me easier to understand low-level concepts and learn Rust later? (I'm not considering C++ as an option because a lot of people whose opinion matters for me say that going really low-level and stuff in C++ results in C code with .cxx extension -- I won't need nothing of STL or Boost goodness). Sorry for off-topic and bad English, that's just quite an important question for me 
Ugh, can't even print it to PDF to get an offline / non-mouse copy.
Also, take a look at this image: https://hub.docker.com/r/ekidd/rust-musl-builder Looks well maintained and includes some static libs.
I don't know any C or C++, and I used to be a script-kiddie (Python, Shell), and I learned Rust and still use it today. Along the way I learned Java at an AP (basically college-level but in high school) level, and I'm taking it right now in college. Even if you don't know C, you can still appreciate a lot of Rust's features.
I'd honestly recommend learning C either first or concurrently with Rust. C really does give you a much better idea of how everything is being done in the computer, even if it does have some weird ideas (I'm looking straight at you, typedef struct {} x). Rust may show you some lower level things (especially in unsafe{}), but C is practically a small shell over assembly. Additionally, learning C will give you a greater appreciation for slightly higher level languages and their abstractions. 
thank you! there are already some features that I really appreciate in Rust, and they resemble me a couple of things from Ruby, which could be a perfect language, if, alas... but the main reason I'm having this concern is that... lemme try to explain, Rust is great and strict, and disciplined language, but that's not *my* inner discipline. Like, "anarchy brings order" – going through hell with C and learning how to write a safe, supportable and extensible code in a language that would allow me to BSOD my system to hell, will make me appreciate Rust more. In other words, I'm worried that not having that kind of paranoia and not 'having seen a certain s&amp;@t' will make me a sub-par programmer who makes decisions in code that were way to go in some higher-level languages, but in a low-level and mostly bare metal environment would result in fatal doom's day scenario. also, I want to wonder, would C really help me to understand how all the magic works on the lowest level (like how the memory allocation and freeing works at hardware level and stuff) – or that doesn't depend on a language, and some CS course with detailed hardware learning would be way to go? (Although I'm pretty sure that they'd use C in the course) Disclaimer: I don't have a degree and I never studied CS in school or university (which I ditched a lot of years ago). All my programming and CS knowledge is kinda "self-made". 
thanks, an almighty telepathist! you've answered all of my questions from my comment above before even posted it :D
What are your goals for learning low-level programming? It makes a huge difference. If you want to do something "practical" *right now*, C is going to be a shorter path. There's just tons and tons of low-level C code, tutorials and teaching materials, compilers and frameworks out there. I don't think you need to know C to learn Rust, but it surely won't hurt. If you want to learn low-level just for the sake of learning, starting in Rust might not be the worst plan. The going will be harder in most ways: you'll need to get a copy of *Programming Rust* and pay special attention to the memory layouts and stuff, and it would *really really* help if you had someone you could ask questions of. The upsides of this plan are that you are learning a Language Of The Future (probably) and the compiler really does try to stop you before you run your code when something looks wrong. Once you're reasonably familiar with the basics of the language, grab a cheap embedded board and follow the Embedded Rust tutorial to make it do a thing. This will make you familiar with what the hardware is doing, and will be fun in its own right. At least that's my take. I think there's a lot of room for disagreement here.
&gt; I guess a cooler take here is that most people have been saying that 2019 should focus on stabilizing what they've said should be stabilized (const fn, const generics, async/await, etc), and improve quality of life through tooling and compile times. Note that we are already doing this. There are people dedicated on working on these areas. For example, a lot of my time is devoted to stabilizing more `const fn` stuff. It's just a lot of work.
thank you! yes, my goals are actually to learn low-level for the sake of learning and improving me as a programmer (and ofc making something "practical" in future due to my imposter syndrome "muh web developers ain't no real programmers, real men do real shit bare METAL" and stuff). I think I'll follow Darrion's advice and learn them concurrently -- because tbh I'm really interested in both of languages, and considering Rust allows C bindings, it's never bad to speak hardware'ish, in my humble opinion 
Rust provides the same amount of low-level access and foot-shooting capability as C. The difference is that in Rust offers powerful tools to let the compiler enforce safety for you at compile time. &gt;Are they right? You'll have to define bare-metal first. In my experience that often refers to running code without an operating system which could certainly be an adventure. I think your colleagues are biased though and know little of Rust; if you really want to learn a lot about the language go `#[no_std]`, and try implementing safe abstractions on your own, like printing. --- &gt;My programming experience includes writing ThatLanguageWhichCannotBeNamed (JS) I don't know who hurt you, but we don't do that here. Please don't be ashamed of your background.
also, I want to ask -- in this case is K&amp;R's The White Book still a way to go in 2019? I'm not talking about the language standard, but about it as an actual handbook that could teach me something?
Why not both? Reimplement part of coreutils in both languages simultaneously. That should also give you an appreciation for their differences. Then go program something with drastic performance constraints (by today’s standards) like an esp8266.
Agree. &amp;#x200B; Learning Rust and C concurrently makes sense. I learned C about 20 years ago and I'm now learning Rust, but if I didn't know C and I was coming straight from python/interpreted languages, I think I'd be throwing myself in the deep end and introducing gaps in my knowledge if I didn't learn C. &amp;#x200B; Rust fixes a lot of the problems which C/C++ has but if you don't know what those problems are in the first place you won't appreciate Rust as much. It will seem like no more than a fast python.
Rust is a lot more like C++ than it is like C. C is closer to assembly, in that it has fewer abstractions. If you're working at a level where even C++ is needlessly abstract, I'm not sure Rust is what you're looking for. If you're looking from an interest point of view, though, I personally find Rust a lot more interesting &amp; fun than C. From a career perspective, though, C++ and C are still much more widely-used. If you're gunning for a position in, say, firmware design, look into C.
The ansi C edition of that book is possibly still the best language reference I've ever encountered. Clear, concise, and still friendly enough to not intimidate the reader. 
You may also be interested in [OrbRender](https://gitlab.redox-os.org/FloVanGH/orbrender)
Sooooooo you want /r/playrust
&gt; Perhaps functions such as char_to_byte_idx_inner() could be simplified by using slice.align_to()? Oh goodness, I didn't even know that existed! Looks like it's new in Rust 1.30? I'll definitely see if I can use that to clean some of the unsafe up. Thanks for the pointer! Unfortunately I expect that won't help much for the line-ending related functions, because those are doing something more fiddly than just dealing with unaligned bytes at the start and end. But maybe... &gt; Something like faster might also be able to handle this in theory I don't think that would actually help much in this case, unless I'm wildly misunderstanding what `faster` does? In other words, I don't think the SIMD wrappers are the scary part of my code.
As an aside, if you really want to experience low-level just to understand better how your computer works, you could consider learning a bit of assembly as well. If you do, I would recommend getting either a raspberry pi, or perhaps an Arduino, then learn assembly on either of those. X86 is a pretty horrible experience as far as machine code is concerned; ARM or AVR are both far more pleasant. With a raspberry pi you have a real computer with a real os. You can experience how to use it from the lowest level. With an Arduino you have almost nothing, and just blinking and LED will feel like a major accomplishment.
Rust does a awesome job for someone looking into low-level programming, so you can learn Rust first. But is also good to know C since there a bunch of code written in that language
thank you! well, the most 'desired' definition of 'bare metal' for me is maybe demo scene and 64k – something not very "practical" but sure very fun. also, I actually tried to start Rust a year ago, and even looked up into \`#\[no\_std\]\` trying to write clones of some \*nix programs like \`echo\` or \`cat\`. the reasons behind it were: * actual \`echo\` weighing 8 KB vs Rust + std 500\~ KB * a desire to understand how 'Rust w/o batteries' works but I failed and dropped it at some point. anyway, that's not the way of Dai Gurren Dan and now I'm well determined to finish what I've started \*\*\* \&gt; I don't know who hurt you Projects did :"D Two months of writing a new markup and front-end logic in form of AB-tests (generally it was a \`&lt;script&gt;\` tag with jQuery script loaded into the Google Tag Manager, that would be appended to the page only with a certain URL-parameters and then just \`hide()\` the old layout and show the new one... sorry, I've started having flashbacks. Let's just say I don't like JS very much (no offense to JS fans, that's just personal)) &amp;#x200B;
Thank you! Assembly is definitely on my list for future, I was thinking about x64 NASM, but ARM assembly sure is much wiser to learn, since I don't think I'd want to program the CPU on my laptop :D
I managed to make a screenshot in a resolution where most of the text is readable: [https://screenshots.firefox.com/k9dclHLInIH08JV4/www.breakdown-notes.com#](https://screenshots.firefox.com/k9dclHLInIH08JV4/www.breakdown-notes.com#) The layout is really horrible. It would be great if this was a website with a table of contents to jump to a specific section.
I used it for a demo Rust deploy recently. It was easier to deal with Rust than most other languages (e.g. Python).
Version 0.2.1 is out, which fixes the problems you found. Thanks again for helping uncover those problems.
X86 basically has 40 years of cruft baked into it; there's ten different ways to do every single thing, and you need to understand all that history to know why you should use any one way and not some other. ARM is far cleaner and more pleasant, especially if you are a beginner. Spend more time learning the ideas of machine code instead of the idiosyncrasies of a particular cpu architecture. You can also run a Linux ARM machine in an emulator on your laptop if you like. Don't even need physical hardware to learn.
Why you have choosen so illegible syntax?
I will take a look, thanks for the reference!
You can delete this post.
This is great!
This is cool. I'm still in Ch. 1 and imagining doing something like this. Thanks for sharing. Their syntax is a little obscure. I have to assume it will make sense when I get there. Cheers
On the map the one in Warsaw is but a ghost. How can I request its removal?
I think it's a huge stretch to call `#include` "Approachable" since you need to understand header files, include guards, forward declarations, object files, linkers, etc., to be able to use them. Rust modules, especially 2018 edition, are way more approachable than that.
After reading this I finally went back and watched Raph’s talk on Druid. It’s great. https://www.youtube.com/watch?v=4YTfxresvS8
You're looking for /r/playrust.
Sender implements the private trait UnsafeFlavor which gives it the inner() and inner_mut() methods. Look at the source [here](https://doc.rust-lang.org/src/std/sync/mpsc/mod.rs.html#661).
Why is this hidden from docs?
Oh it's part of sync
When your code doesn't compile because of HRTBs but they are elided so you have idea what the error message means... Those are real pain points. I still don't understand why HKTs are needed to write code one way, but not another way. You're complaining about something that takes two minutes to set up. You don't even need to understand it, just copy the file layout of another project with your own names instead.
Yes, as others have said, Azul can do this, but it's a bit of a pain right now. Azul has currently no real "focusing" for text fields, what I do is doing the focusing manually via onclick event handlers, storing the text field index and the node index to get the current focused node + current focused text field. Scrolling is also not finished, I currently simply shift margin + padding around when the node graph is dragged - it's fast enough in release mode, but it sucks in debug mode. And there is a problem with clipping the nodes to the node area, so the nodes currently can float "out" of the node graph area, because `overflow:hidden` is tricky to implement with regards to absolute-positioned elements. Other than that, it wasn't that hard to implement (besides implementing validation for the fields so that you can't set a field to 0 and get divide-by-zero errors). The whole "drawing a connection" thing is simply a SVG component using position:absolute to draw the lines behind the nodes. So it's possible to do this in azul (code is roughly 800 lines of UI rendering, 800 lines of callbacks / UI logic and 400 lines of CSS). I haven't open-sourced the node graph yet (mostly because there are many problems with the framework in general), but it shouldn't be that hard to do, just start by positioning some absolute-positioned rectangles, move them around using onclick event handlers, and work from there.
Try putup (pyscaffold) sometime. Once credentials are in pyrc, only 2-3 shell lines: putup pkgname; pkgname/setup.py release; 
Hack is a small assembly language created for the "From Nand to Tetris" book and course. See https://www.coursera.org/learn/build-a-computer. It's an enjoyable class that walks you through computer architecture from basic electronics (nand gates) to memory chips to CPU design to assembly language to writing a higher level compiler for your assembly language so you can build a very simple "OS".
I have a pretty solid use case for Word serialisation if you’re interested. I’m writing https://github.com/cormacrelf/citeproc-rs which will eventually have to generate footnotes in a word plugin. The Word JavaScript API doesn’t support footnote insertion, so it’s XML only, and if you mess it up Word will crash. It would be cool if you could mark which elements are common and which are PowerPoint-specific so that Word could be added later. (I think existing citeproc/endnote etc things are built on macros or other non-JS plugin APIs, but they will never run in Office Online or on an iPad, etc.)
Indeed. Cross-platform compiled extensions are a pain. 
Thanks, this atleast works better than the existing one.!
To report back on using `slice.align_to()`: I was able to get rid of the unsafe code on `char_to_byte_idx_inner()` and `count_chars_internal()` with it. Unfortunately, I can't figure out any reasonable way to use it for the line break functions (and I did try). `count_chars_internal()` remains pretty much just as fast, but `char_to_byte_idx_inner()` takes a significant performance hit since it now has to manually keep track of something that was just a pointer subtraction before (essentially: this turns one calculation in the function from O(1) to O(N)). I have vague thoughts on this topic that I need to clarify and write down at some point, but I think this case is a decent illustration: some optimizations really are just that low-level, and higher-level abstractions just won't get it for you. I think there is always going to be some level of tension in the Rust community because of that--unsafe code has this "don't ever use it" reputation, but for applications where performance is more important than strict memory safety we're going to see unsafe code anyway because the cost-benefit analysis is different. I do think it's important, however, for people to be _aware_ of what crates do and don't use unsafe code (including transitively) so they can make informed decisions about what crates to use based on the priorities of their own projects. That's why I have the note about unsafe code in Ropey's readme. In any case, despite the performance regression, I've committed these changes anyway. In this particular case it doesn't seem to have that big an impact on Ropey's performance over-all, even though it does for very specific APIs (but which were crazy fast already--now they're just somewhat less crazy fast, but still very fast).
I think you should learn both C and Rust at the same time. Make a small library in C for doing something simple. Then a program in C which uses it. Now rewrite the library in rust and export the same interface as the c one. When I started porting C libraries to rust I learned a lot about memory, how to think about pointers, lifetimes and some goodies about linking. And it is fun to see the program still work without having to recompile it! 
Setting RUSTFLAGS shows a minor improvement. I don't necessarily want to be as fast as coreutils, I just wasn't sure if there was something horribly wrong with the code that made it significantly slower. It seems like I'll have to look into SIMD to get some more performance out of it. &amp;#x200B; Thank you for pointing me towards faster!
Thank you for the extensive reply. I will look into SIMD and the inlining optimisations. Having someone else's code (RustCrypto) to look at is also pretty neat. &amp;#x200B;
Good to know that this sort of code is expected to be that much slower without serious optimization. I'll see what I can do in regards to SIMD.
Thank you for pointing me towards that article, I wasn't really sure how to approach the optimization Rust code in general!
thank you! that sounds very useful! the only thing is that I'm terribly, terribly bad at finding ideas for such things like libraries. I'm struggling almost 1.5 years with an attempt to come up with an idea for JS library, for you to understand :D would a tiny toy programming languages do the trick?
I think something more librariesque is easier. Just do something simple, such as a library for talking with your favorite API. Or if you are more into raw performance do some matrix calculations.
sounds like a way to go, thanks!
You are most welcome :)
Since you are a python programmers check out pyo3 or rust-cpython as well. It might actually be useful for you at work. Something I've been dying to try is to iteratively rewrite a python service in rust. First by embedding the python interpreter in rust and simply calling the service. Then move bit by bit into rust until there is hardly anything left. 
Is there a write up of the high level design anywhere? Would love to learn more
&gt; Also, I want to wonder, would C really help me to understand how all the magic works on the lowest level (like how the memory allocation and freeing works at hardware level and stuff) – or that doesn't depend on a language, and some CS course with detailed hardware learning would be way to go? (Although I'm pretty sure that they'd use C in the course) No, not really. C is *not* the lowest level. C does *not* map directly to the machine; it maps to the “C abstract machine” which is, as it says, a model that abstracts what computers were like some 30 years ago (which is why C allows such oddities as 10-bit-bytes, or 1s-complement integer arithmetic, even though no CPU in the last few decades has supported such things). The C abstract machine has no concept of the cache hierarchy, and had only *just* grown the ability to describe concurrent memory access by multiple threads of execution. C won't teach you about how memory is allocated, about how functions are called, or about how the stack works. Although I don't know of any relevant research, I'd be surprised if C was a better *teaching* language than, say, Rust (or Pascal, or a host of other old compiled languages). If you want to know how to work the machine, assembly is what you want. It's also reasonably pragmatic - there are some things that you may run into that *require* assembly, either because they're doing low level stuff that higher level languages don't bother to expose or because they're a part of the*implementation* of higher level languages, like the function call ABI, or the implementation of thread-local storage. It can also be useful to read the compiled output of your code, when profiling or in the rare occurrence of a compiler code generation bug. Now, there *are* good reasons to learn C - there is a *stupendous* quantity of code written in C. Some platforms - notably POSIX - are specified in terms of C functions, too. I just don't think it's a particularly good way to learn how things work. &gt; …going through hell with C… I don't think “learn to juggle chainsaws so you can appreciate how nice juggling pins are” is particularly valuable 😀. Notably, there are a bunch of problems that *C* hides, and you *never* hear how people should learn to allocate registers and remember what values are in them themselves to appreciate how C does that for you, or how everyone should learn the correct registers to save and restore across a function call, or…
This is very true, if you are using Visual Studio then you are probably on MSVC which means you are missing out on all the sanitizers that ship with clang.
I was playing around with docker images like alpine that are small and use the musl c library, but ended up switching to Ubuntu at the end. Too much problems troubleshooting applications that didn't work properly with musl. 
Maybe there should be a lint for this?
But you can't use reserved keywords like \`type\` as field names... oh, hello again, `self._type`!
I opened this thread merely to see if someone had said this. K&amp;R is the best programming book I have ever read. It's been over a decade and I still recall the larger exercises.
This is not a subreddit for the game Rust, but for the programming language Rust ([https://www.rust-lang.org/](https://www.rust-lang.org/)) r/playrust is the subreddit you want.
Thank you!
I hope me cross posting this question here is allowed, if it's not I'll go ahead and delete it! I had a question regarding learning good programming through learning data structures and algorithms. Was wondering if Rust was the language to learn these things in after hearing that Rust teaches good programming.
-O2 actually is not that aggressive as -O3 in producing and looking for the loops that could be vectorized, it is both in gcc and clang. https://gcc.gnu.org/onlinedocs/gnat_ugn/Optimization-Levels.html So you simply did not even turn this on for rust but turned it on for c++.
I would like to add that I don't completely agree with this. I have roughly seven years of C++ experience and I've been actively learning Rust for maybe a year now. I do agree that knowing C will definitely give you useful insights into how things work. But learning a programming language takes time and effort and during that time you could instead progress further in learning Rust.
There is a [clippy](https://rust-lang.github.io/rust-clippy/master/index.html#new_without_default) lint for this and it's enabled by default :)
Wish more pages were presented like this one: https://motherfuckingwebsite.com/
How is integrating with Fortran? Is it C ABI compatible?
I do more algorithms work in Python than Rust, honestly. Many interesting algorithms are small, and you have to be careful with them anyhow: a strong type system often won't save you. Python is a mixed bag for data structures: it is literally missing structures as well as enums/symbols and any reasonable kind of multidimensional arrays, but it has sets and maps, which are really handy. At the end of the day, though, it really doesn't matter :-) . I've implemented fancy algorithms and data structures in probably a dozen different languages and they all have their pros and cons. Learn to pseudocode. The main reason I called out Python is that it looks a lot like pseudocode. Rust not so much :-) . But Rust's high performance is great for evaluating performance in practice, and the code you build is more likely to be generally usable because of a decent module system.
Learning C also gives appreciation for C itself. It is actually quite genius in its simplicity.
Also it's gross because its gross code and not because it's C++.
Also you can use `is_floating_point_v` instead of `::value`. Besides concepts in C++20 Will help making this code more readable and get rid of the SFINAE.
Hi folks I'm messing around with threads and I run into ThreadPool which is amazing. Althought I do have a question: In [this](https://docs.rs/threadpool/1.7.1/threadpool/#synchronized-with-a-barrier) example with barriers I'm missing the point of calling `barrier.wait()` inside `pool.execute`. I know that we have to wait for all spawned threads before we go on and thats what the second `barrier.wait()` is in the code. But why is there the first one I mentioned above?
This might be an unpopular opinion here, but I recommend against using rust to learn data structures. The reason is that the borrow checker can only reason about lifetimes that are known at compile time. But lots of interesting data structures are interesting specifically because of how they dynamically allocate, partition, reference and use memory. Because the rust compiler can't prove the correctness of any non-trivial data structure code, in practice you'll waste lots of time fighting (or avoiding) the borrow checker. Writing a linked list is trivial in just about every language, but the complexity of implementing it in rust [can fill a whole book](https://cglab.ca/~abeinges/blah/too-many-lists/book/). Lots of data structures will have this sort of problem in rust - skip lists, invasive collections, doubly linked lists, trees with parent pointers, etc. You can do all this stuff in rust using pointers and lots of unsafe blocks, but its a distraction from learning the data structures themselves. I'd pick something (almost anything) else if you're just getting started. Python, C# or C/C++ would all be good choices.
&gt;actual echo weighing 8 KB vs Rust + std 500~ KB Here is a good read (though a bit outdated) on why Rust binary sizes and how to make them smaller: https://lifthrasiir.github.io/rustlog/why-is-a-rust-executable-large.html
&gt; Also you can use is_floating_point_v instead of ::value. That's the last line in my comment ;).
Go 1.0 came out with all the batteries included for creating web services: the standard library not only contains a HTTP server, but JSON, XML support, a standard SQL database interface, a HTML templating engine, etc. And, of course, a great concurrency model out-of-the-box. Rust intentionally chose to have a much leaner standard library, to the point where even random number generation is something you have to get from another crate. Many of the crates you need, like Futures/Tokio or Hyper are still in a state of flux and versioned as 0.x. I think this is what is currently holding Rust back on the web space: the "de facto" leaders in the required tooling have not yet emerged. Once we [are async](https://areweasyncyet.rs/) and [are web](http://www.arewewebyet.org/), I think Rust will definitely be on par with Go for creating web services.
&gt; Rust probably has similar issues with debug build performance in that inlining is really required for a lot of constructs to be performant, not sure what can be done here. Cranelift backend for debug builds could be awesome
My reason for sometimes choosing C++ is that "native" to modern Unix. - it's ubiquitous — the compiler is already there - it "likes" shared libraries (unlike cargo's preference for static `rlib`) - it's a mostly-superset of C, so C interop is pretty much perfect. Rust's bindgens are pretty good, but still, can't copy paste from one language to another, have to especially rewrite C magic like `containerof` type macros, etc
I can't use any of my keyboard shortcuts on Firefox, e.g. Alt+1, Ctrl+L, how come?
Looks like it uses cairo and very primitive. At least the text rendering.
Oh, didn't seed that; my bad.
You could interface with JavaScript through wasm in order to use one of the many js interactive data rendering library.
[`piet`](https://github.com/linebender/piet) looks like a simplified [`resvg`](https://github.com/RazrFalcon/resvg). Only `resvg` doesn't try to separate backends that much. And I'm not sure if this even possible. The problem is that at least cairo and Qt have a pretty different API logic. In cairo, painting and stroking are separated, and in Qt it's a one command. Also, cairo has a global path state and Qt don't. In cairo, clip path must be set before the object path. So you have to set a clip path, apply clip, set an actual path, and then stroke/fill it. In Qt, it's just two command: clipPath + drawPath. So if we forget to apply the clip path in cairo, we may end up with two paths, instead of one. Which is impossible in Qt.
The latest version of \`crossbeam-channel\` doesn't use \`crossbeam-epoch\` anymore, so there is no garbage collection involved.
Some keys are 'reassigned' in breakdown notes. Alt 1-4 are shortcuts to change the font size. But if you open a map in view mode, most of the key events are supressed, because in view mode there are no edits possible. As a result nothing happens, not even the expected browser default behavior. 
It starts to makes sense when you read the chapter about the machine language. However, I still feel weird when I see any hack code.
I have been working on [img\_diff](https://crates.io/crates/img_diff) a cli tool to diff folders of images. Since this is my first project I have been using it to explore rust, currently over in [this branch](https://github.com/Mike-Neto/img_diff/blob/drop-dssim-dep/src/lib.rs) I'm trying to come up with a good enough trait/type parameter to avoid having branching code for different image types as I intend to add jpeg support latter. &amp;#x200B; While this is not a question per say, I'm asking for a code review on that branch as here seems like the appropriate place on this sub to ask for it.
Coming from JS, I'm not used to dealing with references / pointers so much. I am having a ton of trouble understanding dereferencing. When do I need to use it? What is it actually for? I can display the value of a &amp;variable without any issues, and I can make it &amp;mut variable to change it, so I'm not sure what dereferencing does.
Why isn't it considered idiomatic?
Yes, in fact I will disappointed if Rust doesn't far surpass Go for this usecase, because it has much better support for high-level abstractions than Go. In a few years Rust might even be competing with python-django/ruby-rails/php-laravel if it plays it's cards right. The productivity of those frameworks with the performance of Rust code: now that would be one hell of a combination. Currently the biggest blockers are that the async I/O ecosystem isn't mature yet. Specifically: - futures aren't stable (not in their final form, or at least we can't be sure of that yet). - Tokio is only supporting futures 0.3 in an experimental fashion. - There is no async db driver that supports the latest tokio 0.1 (tokio-postgres is still using the old 'tokio-core') - Diesel (ORM) isn't async at all, or easy to use from async code in an out-of-the-box way. - Rocket (http://rocket.rs) isn't async or performant yet, and none of the other web frameworks come close to it's completeness or lack of boilerplate. Also, Async/await syntax isn't stable. But I actually think that is less important. Once these issues are sorted (and I have high hopes for 2019), I think the biggest throttle will be compile times. Go compiles almost instantly, which gives a really fast edit cycle which you just don't have in Rust at the moment (where you might easily be waiting 10 seconds to compile even for a small app)
we did it for sozu: https://github.com/sozu-proxy/sozu/blob/master/Dockerfile
Indeed you need both code and data to stay in the L1 cache if possible, L2 cache at worse. They generally stay warm by themselves, by the virtue of evaluation being triggered dozens/hundreds of times per second. If the frequency of evaluation is not sufficient, then "fake" signals are triggered every so often (like every ms or 10 ms), and their output discarded once computed; this can be useful for "rarer" instruments, whose data could be bounced out of the cache. It's also very useful to do as little else on the core running this workload (and use core pinning) both from a latency perspective and a cache perspective, so computing the look-up table (which involves an order magnitude more data) or handling the replies to the orders sent are both done on other cores. It's a very specialized workload, with its own requirements and idioms.
When you put `&amp;` before an expression, you get a reference - for example if the expression had a type `Foo`, the result has type `&amp;Foo`. Dereference (`*` operator) is the opposite of that - if an expression had a type `&amp;Foo`, then adding `*` before it changes its type to `Foo`. In some cases you will need to add `&amp;` and `*` manually to makes the types match up, and in some cases compiler inserts them automatically to increase ergonomics. Let's look at some examples: --- fn foo(x: u32) { ... } fn bar(x: &amp;u32) { foo(*x); } Here you need to add the dereference when calling `foo`, because `foo` needs `u32`, but you have a reference `&amp;u32`. Compiler won't try to automatically insert a dereference here, and will simply give a regular type error (`expected u32, found &amp;u32`). --- fn foo(x: &amp;u32) { println!("value: {}", x); } Here it seems that we have a very similar case - we have a reference, but it prints as a value as if we tried to print `*x`. However, the reason for that is that [there's a `Display` impl for references](https://doc.rust-lang.org/src/core/fmt/mod.rs.html#1909-1924) that just forwards the formatting to the referenced value. So actually the reason why this works is simply that the references are displayed like that. And that means that even if you have a `&amp;&amp;&amp;u32` it will also be printed as a number. --- struct Foo { ... } impl Foo { fn foo(&amp;self) { ... } } fn bar(x: Foo) { x.foo(); } Here `Foo:foo` takes a reference, but we can call it on a non-reference. Here's one of the places where the compiler will automatically insert `*` and `&amp;` as needed to make the types match up - because writing `(&amp;x).foo()` would be very cumbersome and would not really increase readability very much. --- #[derive(Copy, Clone)] struct Foo { ... } impl Foo { fn foo(self) { ... } } fn bar(x: &amp;Foo) { x.foo(); } A similar case - here the compiler will automatically insert a `*` (as if we called `(*x).foo()`), and it all works out because `Foo` is `Copy`. If it wasn't `Copy` the compiler would still insert a dereference, but then you would get `cannot move out of borrowed content` error. --- fn foo(x: &amp;mut u32) { *x = 3; } Types on both sides of `=` must match up. If we tried to write `x = 3`, then on the left side we would have `&amp;mut u32`, but on the right side there's an `u32` - so we would get a type error. So we write `*x` to change the type from `&amp;mut u32` to `u32`. You could also write `x = &amp;mut u32` - it also fixes the type error, but then it means that you are reassigning the reference (changing what the reference points to), instead of modifying the value itself that it points to (also you would get a borrow checker error of "value does not live long enough"). In JS this would similar to this case: function foo(x) { // similar to `x = &amp;mut 3` - caller cannot see if we changed anything x = { 'field': 3 }; // similar to `*x = 3` - called can see changes x.field = 3; }
I don't like the Java approach (6 line files are just silly), but I think Rust code often goes too far the other way, with 600+ line code files being commonplace. It's these that I find I need an IDE for, because I can't tell what's actually in the file. If it's public and private fields that you're talking about, then I can't say I find that feature very important. I come from JavaScript which doesn't really have notion of private items, and while I appreciate a lot of Rust's safety stuff (e.g. enums, ownership, etc). But private/public fields? I'm pretty much always going to be looking at the docs for a type that I'm using anyway, so if a field's marked private, then I won't be using it! Each to their own I guess.
Cautionary word that C mostly shows how a PDP-11 computer works; it can be argued it's not so different from a x86 computer, however the devil lies in the details. To truly understand how a computer works, you need to look at the hardware and learn assembly; many assembly instructions do not have an equivalent in higher-level languages like C. It's not a bad approach, and it's certainly one of the lowest-level portable programming languages, but it still is a high-level language compared to assembly/micro-code.
Use C++ (C with STL) or Java. Python as a third option. These languages are, by far, the most commonly used in Competitive Programming sites (codeforces, hackerrank, hackerearth, timus, atcoder, csacademy etc. where you presumably would want to practise), and most resources will use C++ as the language for examples. 
Why isn't the Debug trait implemented/derived out of the box? I have to do it manually every single time. It can't be speed reasons, because it's only called when actually used by something like `{:?}¸ so the only disadvantage I can think of is compile time maybe? But how long does it take to compile a simple little trait like Debug? So what is the reasoning behind it? Is it just a random choice? What am I not seeing/understanding?
I'd tend to agree. For learning data-structures, I'd pick a language: - Which features a GC, no need to worry about object lifetimes. - Which uses mutable values. Those are the "bare" minimum requirements. I'd prefer using a statically typed language still, rather than Python, and recommend a language where getting unit-tests up and running is relatively easily, as it's only while exploring corner cases that'll you really understand the subtleties of the data-structure. Similarly, a language with good debugging support will make it easier to interactively step through the program and inspect the state of the data-structure while the operation is in fly. I'd guess... Java or C#, due their maturity, would probably be my languages of choice. Boring choice, really, but superior debugging support is really helpful.
Ultimately? Maude. Less educational and academic and more practical: Whatever language the author of the best library you can find chose to write it in. That failing, the language you're currently writing in. That being too slow or awkward, either Rust or C.
Quick ELI5: A pointer is just an address to a location in memory. There's no actual data. That data is stored in memory. So when you want to access the actual data in memory you have to say "Hey I don't want to change the pointer, I want the actual data im memory!!", that's what dereferencing is! 
I can provide you the `perf` output, but I can't quite figure how to compile your example code (in `gaiku-3d`, I assume).
I'm trying to use the \`serde\_json\` crate to deserialize a JSON API response into a struct I created. However, the JSON response has a key value pair of \`@timestamp: thetimestamp\`. I can't figure out how to create a struct with the \`@\` character in it since it's a special character.
yes
Wait, why do you need to count bytes as you go at all? I am pretty sure there has to be a decent way to query your offset into a slice once. I think you only need that info in the `if` condition that goes into loop break, so no need to count it as you go! I think`.iter().enumerate()` should do the trick. It it doesn't, try using a for loop in a way that would make LLVM elide bounds checks.
thank you!
You can put `#[serde(rename = "@timestamp")` attribute on the field.
It does make a huge difference having the game on the Web. For instance, I just played about 15 mins of the game on my phone - there would have been 0 chance of me actually installing an executable on a device to play it. That combined with the hopeful high performance future of Web assembly (hurry up Chrome!) leaves me very excited. 
I find it weird to use Rust (or anything else with Emscripten) for making game *primarily* targeted to Javascript. What's even weirder is that it probably works and might be even easier than using raw JS or something that's closer to JS execution and memory model (i.e. bucklescript, clojurescript). 
You need to clone the repo and run the cargo.toml under gaiku-3d/example
BTW do you know if there's a way to make the default browser behavior override the website's bindings?
Rust can have issues with Tree-like data structures when you want to write recursively. It's not too fond of things like doubly or circular linked lists either. 
A pointer is just a variable that is stored on the stack and is just an address to a location on the heap. Where the actual data is stored. It's much faster to just copy the pointer address around than the actual data. But when you actually want to access the data you have to say "OK now I don't need the adress, but the actual data". That is what dereferencing does. Saying that you want the actual data from the heap and not just the memory address (pointer) which you used because its a lot faster to copy around. Understood? If not let me know where exactly the confusion lies.
I have a lot of problems with this comment, but I think we are interpreting his question differently. It seems your interpreting it as purely learning C outside the context of the assembly it’s compiled to and outside its integration with the system. I have never seen C taught that way, so I’m reading the question as asking whether he should take an operating systems or systems programming course that involves C, to which I would answer: Yes, you should learn those concepts (virtual memory, TLBs, MMIO, cache coherency, privilege levels, etc) with C if you plan to work at that level, just because the current ecosystem is still highly favorable to C. You’re right that C is not the lowest level, but if you plan to work on micro controllers and access specific registers, C makes that more clear with pointers, whereas you’d need to journey into unsafe rust otherwise, and that’s for systems that rust supports. Even for things like GPGPU acceleration, rust is still building support there. I would agree that it’s not necessary to learn C, if you plan to just be focusing on more general purpose applications. 
On the last point about appreciation, I always point out that C was first created over 40 years ago. We’ve learned a lot since then. 
C was built on the shoulders of giants. Imagine being alive when structured programming with braces was first introduced. All you had before then were just gotos in straight line code. Was indenting even a thing? 
I do think Rust has more friction to use it. I've been building a small thing on actix-web, which I like, but I find it tricky to know what methods return what types, and what functions/api you need to use to get things done. Right now i would still choose Node over Rust for web dev for sure.
I got that part, but I'm still confused. If I want to access the data I can use the variable name like `heap_var` to get the value. If I don't want it to be owned by whatever I'm passing it into, I can use `&amp;heap_var`. If I want to mutate it I can use `&amp;mut heap_var`. So it seems like I can do everything to `heap_var` I can think of, so I'm not sure what the point of dereferencing is. I can still get the value from it without dereferencing, right?
tokio-postgres is currently being refactored; I do believe that the [current master branch is supporting tokio 0.1](https://github.com/sfackler/rust-postgres/blob/master/tokio-postgres/Cargo.toml). It also supports query pipelining which should be a performance boost for typical web applications (no need to "wait" for a free connection).
This is awesome! Thank you so much!
Well not all types are also stored via pointers. A integer for example is stored directly on the stack. While a String is stored on the heap. Have you covered that yet?
 ~ $ cd gaiku/gaiku-3d/example ~/g/g/example $ cargo run error: failed to parse manifest at `~/gaiku/Cargo.toml` Caused by: no targets specified in the manifest either src/lib.rs, src/main.rs, a [lib] section, or [[bin]] section must be present That's because your top-level `Cargo.toml` looks like a workspace manifest, but isn't one. And the whole crate structure is a bit weird. I think there was a `cargo` flag to ignore the parent manifest, but I don't remember it. I just removed the manifest. I also had to create the `output` directory. Onwards. &lt;&lt;small_tree&gt;&gt; Chunks: 5 Reader: 0 Baker: 0 secs Export: 0 secs &lt;&lt;terrain&gt;&gt; Chunks: 131 Reader: 0 Baker: 0 secs Export: 0 secs &lt;&lt;planet&gt;&gt; Chunks: 56 Reader: 0 Baker: 0 secs Export: 0 secs target/release/gaiku-3d-example 1.36s user 0.06s system 91% cpu 1.561 total $ strace -f target/release/gaiku-3d-example [snip] openat(AT_FDCWD, "example/assets/small_tree.gox", O_RDONLY|O_CLOEXEC) = 3 fcntl(3, F_GETFD) = 0x1 (flags FD_CLOEXEC) read(3, "G", 1) = 1 read(3, "O", 1) = 1 read(3, "X", 1) = 1 read(3, " ", 1) = 1 read(3, "\2\0\0\0", 4) = 4 read(3, "I", 1) = 1 read(3, "M", 1) = 1 read(3, "G", 1) = 1 read(3, " ", 1) = 1 read(3, "K\0\0\0", 4) = 4 getrandom("\xa9\x53\xa8\xff\x72\xd1\xeb\xe6\x87\xf8\x77\x4c\xbe\x67\x44\xf9", 16, GRND_NONBLOCK) = 16 read(3, "\3\0\0\0", 4) = 4 read(3, "b", 1) = 1 read(3, "o", 1) = 1 read(3, "x", 1) = 1 read(3, "@\0\0\0", 4) = 4 read(3, "\0", 1) = 1 read(3, "\0", 1) = 1 read(3, "\200", 1) = 1 read(3, "B", 1) = 1 read(3, "\0", 1) = 1 read(3, "\0", 1) = 1 read(3, "\0", 1) = 1 read(3, "\0", 1) = 1 read(3, "\0", 1) = 1 read(3, "\0", 1) = 1 read(3, "\0", 1) = 1 read(3, "\0", 1) = 1 read(3, "\0", 1) = 1 read(3, "\0", 1) = 1 read(3, "\0", 1) = 1 read(3, "\0", 1) = 1 read(3, "\0", 1) = 1 read(3, "\0", 1) = 1 read(3, "\0", 1) = 1 read(3, "\0", 1) = 1 read(3, "\0", 1) = 1 read(3, "\0", 1) = 1 read(3, "\200", 1) = 1 read(3, "B", 1) = 1 read(3, "\0", 1) = 1 read(3, "\0", 1) = 1 read(3, "\0", 1) = 1 read(3, "\0", 1) = 1 read(3, "\0", 1) = 1 read(3, "\0", 1) = 1 read(3, "\0", 1) = 1 read(3, "\0", 1) = 1 read(3, "\0", 1) = 1 read(3, "\0", 1) = 1 [snip] You're probably reading directly from a `File`, one byte at a time. You need a `BufReader`. You should probably take a generic `R: Read` instead of a `File` in `GoxReader`. The `Gox` type takes a `&amp;mut dyn Read`, which is better, although not ideal. So I'll just hack it in `GoxReader`: use std::io::BufReader; let gox = Gox::new(&amp;mut BufReader::new(stream), vec![Only::Layers, Only::Blocks]); Well, this isn't working: ~/g/g/example $ time target/release/gaiku-3d-example &lt;&lt;small_tree&gt;&gt; Chunks: 5 Reader: 0 Baker: 0 secs Export: 0 secs &lt;&lt;terrain&gt;&gt; Chunks: 131 Reader: 0 Baker: 0 secs Export: 0 secs thread 'main' panicked at 'index out of bounds: the len is 56 but the index is 65536', /rustc/96d1334e567237b1507cd277938e7ae2de75ff51/src/libcore/slice/mod.rs:2463:10 note: Run with `RUST_BACKTRACE=1` environment variable to display a backtrace. target/release/gaiku-3d-example 2.86s user 4.01s system 98% cpu 6.971 total I'm not sure what, but the buffering seems to break some assumption in your code. I looked at `gox::read` and it's mildly inefficient, but I don't see anything that should make it crash. The error is here: let block = block_data[data.block_index]; So presumably the data is read, but comes out corrupted. I'm not sure what's wrong at this point, unfortunately. Reverting my change and running perf: 83.49% gaiku-3d-exampl gaiku-3d-example [.] gaiku_3d::bakers::voxel::VoxelBaker::index 3.96% gaiku-3d-exampl gaiku-3d-example [.] core::num::flt2dec::strategy::grisu::format_exact_opt 1.49% gaiku-3d-exampl gaiku-3d-example [.] std::io::Write::write_all 1.02% gaiku-3d-exampl gaiku-3d-example [.] core::fmt::write 1.02% gaiku-3d-exampl gaiku-3d-example [.] &lt;std::io::Write::write_fmt::Adaptor&lt;'a, T&gt; as core::fmt::Write&gt;::write_str 0.94% gaiku-3d-exampl gaiku-3d-example [.] std::io::Write::write_fmt So it's spending most of its time in the vertex lookup function. Maybe some sort of BVH or even a hash table would help there?
Urg, thank you, I totally forgot about drain.
See also [horrorshow](https://github.com/Stebalien/horrorshow-rs).
RRB Vectors are like RRB Tries (I think it's tries not trees), that can be used as a vector. But what I'm suggesting is that perhaps you could use RRB Vector as a backing store for Ropey. I think they have much better append/prepend rate, but you'd have to talk to bodil (I'm unsure she is on Reddit) and/or benchmark it. 
I think the confusion I have was because of the automatic dereferencing. Thank you!
So this might be the wrong place to ask, but can you explain why the ArrayLength type needs the type that is stored in the Array in its type signature? that makes it a bit harder to do vector-matrix multiplication stuff. Also in general a bit surprising. Another thing is serde, you might wanna mention the One Weird Trick you need to make things generic over ArrayLengths (De)-serialize derivable. Other then that: good stuff, enabled me to use generic fixed size arrays in rust, thank you for your work!
I feel like it would be harder to get people to pay money to play games in a web browser, vs like steam.
&gt; Is the sparse DFA encoded in the way it's displayed? Hmm, you might have missed my edit? The end of my previous comment describes more precisely how sparse DFAs are encoded. Your suggested encoding sounds like how it works now? And yes, I did do it that way with the intent of using binary search, but: https://github.com/BurntSushi/regex-automata/blob/17dd7e9afbe46e7b4bda959dc89333ce106958fb/src/sparse.rs#L1106-L1122 &gt; However, you can sometimes get a lot of transitions to the failed match state, which your representation seems to avoid? Yes, I think that's effectively the defining quality of a sparse representation in this case. For some DFAs, dropping the explicit representation of dead states is by far the biggest contributor to decreased. But not always. I started with just dropping dead states in sparse DFAs without compressing adjacent and contiguous transitions, and it wasn't always a huge win. I don't have any numbers though.
I think they have two very different objectives even if they seem similar. I see Rust competing with C and that kind of low level languages where you need to go bare metal and want the less overhead possible. Go, on the other hand, i think, tries to compete with server languages like Java. You can do mostly the same things with those 4 (and more) languages nowadays but they try to target different things. It seems to me that Go and Rust overlap very little.
I am using actix-web and rust-postgres for my web dev. Much of my compilation is done using ``cargo check``. Compile times are pretty good. By the time async-await lands in Rust stable, hopefully it has some functionality left. They're tearing functionality out in order to release *something* and hopefully that *something* is sufficient to address typical workflow.
Why?
So not sure if this is an easy question exactly, but: I have multiple threads that do some work and then write the results back into the main thread via a channel which then writes them into a buffer. This is slow, queueing and dequeuing just takes too much time. And i don't actually care about data races in this case. The buffer is write-only and its perfectly fine if it gets overwritten all the time. I assume that the answer will be some combination with UnsafeCell. How do i do this? do i wrap the whole buffer into an UnsafeCell, pass copies of the *mut to the threads? Do i invoke UB doing that? Do i need to wrap every single member of the buffer into an UnsafeCell?
People just expect web content to be free or ad supported. People are used to logging into steam and paying 5$. Or Xbox, ps4 , iTunes etc. I don’t know anyone that pays money to play a game in a web browser 
Good video, looking more into this DB 🤔
Awesome! Thank you so much!
Go has a certain level of usability that Rust code lacks. Coming to Go from Python or C feels like "coming home" in a sense. The Concurrency works, automatically. All the batteries are included, good HTTP(s) clients with well thought out and usable API's ( inspired by (python)Requests and (c)Curl), templating, and a fairly simple error handling that while verbose, is readable. Then come Rust, where you can't even do a HTTP request without spending half an hour on the net to google which is the hype one today, where you can't load a TLS certificate from a PEM file without figuring out which bloody crypto library works with that random HTTP library you just found. And don't get me started on the ergonomics of command line parsing in rust. Here's a Curl, Python and C equivalent of code I just couldn't figure out how to work in Rust: Shell: `$ curl --cacert /tmp/ca.crt --key /tmp/client.key --cert /tmp/client.crt https://example.com/secret` Python: import requests s = requests.Session() s.cert = ('/tmp/client.crt','/tmp/client.key') s.verify = '/tmp/ca.crt' s.get('https://example.com/secret') C: #include &lt;curl/curl.h&gt; int main(int argc, char *argv[]){ CURLcode ret; CURL *hnd; hnd = curl_easy_init(); curl_easy_setopt(hnd, CURLOPT_BUFFERSIZE, 102400L); curl_easy_setopt(hnd, CURLOPT_URL, "https://example.com/secret"); curl_easy_setopt(hnd, CURLOPT_NOPROGRESS, 1L); curl_easy_setopt(hnd, CURLOPT_USERAGENT, "curl-tls-example"); curl_easy_setopt(hnd, CURLOPT_MAXREDIRS, 50L); curl_easy_setopt(hnd, CURLOPT_HTTP_VERSION, (long)CURL_HTTP_VERSION_2TLS); curl_easy_setopt(hnd, CURLOPT_SSH_PRIVATE_KEYFILE, "/tmp/client.key"); curl_easy_setopt(hnd, CURLOPT_CAINFO, "/tmp/ca.crt"); curl_easy_setopt(hnd, CURLOPT_SSLCERT, "/tmp/client.crt"); curl_easy_setopt(hnd, CURLOPT_SSLKEY, "/tmp/client.key"); curl_easy_setopt(hnd, CURLOPT_TCP_KEEPALIVE, 1L); ret = curl_easy_perform(hnd); curl_easy_cleanup(hnd); hnd = NULL; return (int)ret; } Rust? "tokio tls" to google sends you to a deprecated project, going to the proper ones doesn't even have functional examples for loading certificates and keys. So, no. Rust certainly doesn't have the usability necessary. Rust documentation is a jumble of structs and functions in an assorted mass, without crate documentation on _how_ those exposed Structs and Functions are supposed to be used together. Rust's built-in verbosity for "handling all cases" makes it hard to make small, simple examples that are digestible for documentation usage. 
Thanks! You can find the slides at https://jon.thesquareplanet.com/slides/rust-twosigma/, the prototype and link to the paper at https://pdos.csail.mit.edu/noria, and the conference publication and presentation at https://www.usenix.org/conference/osdi18/presentation/gjengset. I also tweeted about it here: https://twitter.com/Jonhoo/status/1081550591237730306.
I should take a closer look at resvg internals. It would be awesome for resvg to have a piet backend. If that's not practical, I would like to know what the major sticking points are. Global path state doesn't sound like it should be an issue, piet internally does not rely on global path state, and the Direct2D back-end doesn't have it. The other API issues sounds like they're abstracted by piet - the Cairo back-end owns the Cairo Context, so you could never "forget" to apply a clip path. I haven't done clipping yet (it's next), but think I have a pretty good idea.
Obviously an unpopular opinion here, but not a chance. Compile times, general complexity and especially async code complexity are all too much of a crutch. 
I hate that there are several standard libraries in OCaml. On the one included lacks some functions(can't remember which right now).
https://doc.rust-lang.org/nightly/core/hash/trait.Hasher.html you can implement this yourself, panic on anything that is not write_u32, return the u32 upcasted to u64 in finish. then you can use the hasher to construct a HashMap like this: https://doc.rust-lang.org/nightly/std/hash/struct.BuildHasherDefault.html
Honestly from the few times I've tried Rust (coming from a Python background), Go is way easier to get into. Rust has so much syntax, Go was way easier for me to get going with. Honestly the module and dependency story with Go is atrocious, but most other features absolve Go of that. A Go with cargo like packages would be great imho.
Well, since you like curl: [https://docs.rs/curl/0.4.19/curl/](https://docs.rs/curl/0.4.19/curl/) Otherwise try: [https://docs.rs/reqwest/0.9.5/reqwest/](https://docs.rs/reqwest/0.9.5/reqwest/)
Rust is likely one major success story away from opening the flood gates to major web development adoption. Social contagion is strong among programmers. 
&gt; A pointer is just a variable that is stored on the stack and is just an address to a location on the heap. This is not correct. You can have pointers to other stack-allocated data.
Do you have any examples of Rust questions being unfairly deleted on SO?
While I do agree, we have chose Rust as a server language and love it. We don’t feel like we’re writing c++ but that we’re writing a fast maintainable web server. And we are a web dev team that mostly had python and php experience. (Some Haskell) We actually started a Go server but ultimately switched it for Rust because we liked it more.
I see rust competing more with c++, c doesn't have basic data structures in its STD. C should be the choice when you really need some weird hand rolled data structures. Of course, disclaimer : human emitting opinion while doing other things.
to elaborate: the rust compiler code actually does not contain any info on what byte \n is, it just knows because the compiler it was compiled with knew, leading to funny code like r"\n" =&gt; "\n" without ever defining what \n actually is.
It's not a zero-sum game. Rust conceives itself as a systems language first, but that doesn't mean it's unsuitable for other use cases. In fact, it is suited for a wide range of applications because it supports abstraction so well. This is in comparison to C, in which the highest level of abstraction is the `struct`, and it is not unheard-of to implement vtables by hand; and Go, which is inappropriate for systems programming (and also inexpressive). To me, writing web code in Rust is pretty similar to writing in Go. Sure, I can't (reasonably) *just* use the standard library, but adding hyper or a web framework as a dependency is a complete non-issue.
its not only free to, it actually does, or im going crazy. at least i think i lately had two separately defined tuples with the same fields, but they had different layouts.
Wouldn't a IDE help with that kind of explorative programming ? I use vscode with the rust plugin, what's the state of the rust plugin for the IntelliJ platform ?
Was this a public event? There wasn't a RustNYC meetup in December so this would have been perfect to attend.
not yet, it's barely working :-) BTW: if anyone wants to join (to either of these projects), just let me know
It definitely does. You can see it in this example, where the compiler saves space by putting the two `u8` fields next to each other: use std::mem::size_of; struct Ordinary { small1: u8, big: u64, small2: u8, } #[repr(C)] struct ReprC { small1: u8, big: u64, small2: u8, } type Tuple = (u8, u64, u8); fn main() { println!("size of ordinary: {}", size_of::&lt;Ordinary&gt;()); println!("size of repr(C): {}", size_of::&lt;ReprC&gt;()); println!("size of tuple: {}", size_of::&lt;Tuple&gt;()); } Prints: size of ordinary: 16 size of repr(C): 24 size of tuple: 16 That said, I'd be surprised if the compiler gave a different answer for two effectively identical tuples.
Unfair by the Rust communities standards or unfair by SO's?
I *really* like the idea of maintaining the materialized views live. I still remember a *very* simple task that I just couldn't manage to get to work with acceptable performance on Oracle (v11?): a simple, frequently updated, table `folders` contains essentially 4 columns: folderId, actedOn, messageId, publicationTimestamp. How fast is `select count(*) from folders where folderId = ? and actedOn = 'N';`, with an index on `folderId`/`actedOn`? It's O(N) in the number of rows matching. Maintaining a `folderId` &lt;-&gt; `count` was impractical: it caused high contention on the most popular folders. I never got to attempt the idea of a `folderId` &lt;-&gt; `count` with 10 or 20 rows per folder, using randomized distribution of the increment/decrement to reduce contention by taking advantage of row-level locking; in the end we just axed the idea of an accurate count for folders with more than 1,000 messages and would display 1,000+ instead. So... how do you think Noria would manage on this benchmark: 1. A constant stream of updates inserting/removing messages in the `folders` table with folderId X and actedOn 'N'. 2. A constant stream of updates flipping actedOn from `N` to `Y` (never in the other direction). 3. How fast can you get `select count(*) from folders where folderId = ? and actedOn = 'N';` to be for folderId X? *(Assuming MVCC, if the count returns 3 and we select the messages, we want 3 of them, not 2, not 4)*
I have to agree. Go also has monumental amounts of support and momentum from big players and the parent company. It also supports [wasm](https://github.com/golang/go/wiki/WebAssembly). Rust is definitely an awesome language and there will be a lot of really cool things coming out of it, I can definitely see it supplanting C++ for a lot of purposes. I don't think it will be adopted on the same scale as Go for the types of things Go is used for though.
I have yet to fully watch the video, but this has greatly piqued my interest. I've been theorizing on building a kind of synchronization manager in Rust. Our company uses several kinds of them from various vendors, and ultimately they are way too damn slow in performance. Sadly the vendor issued software don't do async very well, if at all, which slows things down. The other thing that slows things down is the database. It is just too damn slow to lookup several million rows, compare it to current data, decide what columns to update, if any, and then write update to the database. I've heard stories of some customers requiring SEVERAL DAYS to do a full synchronization, even if there really aren't all that many writes necessary. Write performance is really needed only when adding a new source of data, or when great deal of data has changed since last sync, or when doing the first sync. So from that standpoint, this seems like it just might be the perfect fit for this project I might pursue. So if compared to MySQL, this gives 5x read performance improvement. What is write performance like? Yes, I know both greatly depend on various variables, but a general idea would suffice. Also, is this just an exercise or will this actually be production ready sometime? What about reliability and ACID? &amp;#x200B;
We also need a async threadpool, r2d2 is sync so you have to get a new connection every time you want to do async io on the db.
You need to get used to the d Api docs, it's great and takes almost no time to check the return type.
Hey Jon! I spoke about Rust and Postgres at this exact venue just a few months ago, organized through the [RustNYC community](https://www.meetup.com/Rust-NYC/) though. It bums me out that I wasn't aware of your talk about Noria because I would have definitely gone (not sure whether this was a public event?). You're generally not in the NYC area, right? Is anyone working on a postgres adapter for Noria yet?
So, Noria doesn't currently have MVCC, and so can't quite give the snapshot isolation you want for your query. However, it will reply to the query in constant time (which, for Noria, is a bit under the time it takes to make a memcached GET) for popular folders whose count is likely to be materialized. For eventually consistent systems (as most materialized view systems are), it often makes more sense to talk about the latency until a change is visible, as opposed to how long it takes to do a read (since most reads are constant time). In Noria, a query like that should be _very_ efficient to maintain, and I'd hazard a guess that it'd be able to handle ~200k updates/s, depending a little on the degree of batching you'd be able to achieve for those updates.
I think Go is a great language but it's specialized for web. Rust should focus on competing against c and C++ in systems programming. For web the niche that Rust can clearly win against Go in is WASM, web assembly. 
Go compiles to wasm as well though, the only problem is the binary size since Go has to ship its entire runtime as well. But wasm is also very young, and I expect some solutions to this problem in the future. Since Go is maintained by Google, which also has the largest browser by user-base, it's not unreasonable to see Google bundling Go's runtime with Chrome.
Ah, no, sadly this was an internal talk held at the Two Sigma offices. No, I'm normally at MIT in Boston, but NYC isn't _that_ far away, so I'm likely to be there again! There is no active work on a Postgres adapter for Noria, but there's been some discussion [here](https://github.com/mit-pdos/noria/issues/111#issuecomment-451653725). In theory the biggest chunk of work would be to implement the server side of the Postgres binary protocol, and then it's just a matter of matching that up to the Noria API.
The reason is control. The cost of deriving `Debug` is small, more so if you already derive (`Partial`)`Ord`/`Eq`.
Oh hey, it's you! Really pleasant talk, thanks! I wrote a small program using `fantoccini` just yesterday, so thanks a lot for that library too. The docs helped a ton.
Noria's write performance is also _generally_ very good, and in some cases better than traditional databases, though as you say, it depends on the exact workload. It is eventually consistent, though we have plans in the works for how we might add atomicity guarantees and stronger transactions. As for production use, it's hard to say at this point. I think it's relatively unlikely we'll commercialize Noria, though I will continue to work on it for a few years as I continue my PhD :)
I'm glad to hear that! Always fun to hear that what I build gets used!
I do it was Shepmaster on several of my questions. After complaining to stackoverflow help he ended up rewriting my question. It had one line similar to another question and he marked it as duplicate. The line was part of the error message. It was not a duplicate at all. If I ask a question about any other language it's fine but if I ask a question about rust that guy really makes it impossible to get a question answered.
Is there any plan to implement MVCC, or is eventual consistency a design limitation? I don't work on this any longer, or anywhere close to a database really, however I do recall that MVCC was *very* much a requirement back then and anything less would not have flown. ACID+MVCC just simplifies the life of developers so much!
Sure, I’ll try to write up some design notes today and add that to the `generic-array` repo. Will comment again when that’s ready. 
Maybe Rust doesn’t explicitely market itself as a Go competitor. But when someone is going to choose a serverside language in maybe 5-10 years (maybe a bit of a negative prediction) then Rust and Go will be two valid options to consider. So they’ll be put against each other directly in many situations. Rust is competing as a web language and it makes sense. The language promises safety on multiple levels, where most languages don’t. 
So, `reqwest` is a typical example of this. Documentation is a hodge-podge of Structs with no over-achieving documentation for how to use it. A server certificate is passed in via the `add_root_certificate` function on the ClientBuilder, but that _adds_ to it, removing _all_other_ is undocumented at best. Then figuring out how to pass in _client certificates_ and it's once more down into random structs, in this case the `Identity` struct, which does in fact, NOT work with pem formatted certificates or keys. Because reasons. So, API consistency is nigh-nil, the various attempts at authentication (client cert, login/pass, etc) aren't abstracted in the same way, and don't even accept the same file-formats, because why would you ever? Sure, there's a curl crate, which links to a hard-coded commit that'll give you a version of curl from somewhere in September, with what looks like three CVE's fixed since then, including use-after-free. ( But hey, who's counting. Just bump to the latest version and... wait.) So no, Rust still can't replace those four lines of Python code.
Pretty good imo. I suggest trying both IntelliJ and VS Code and see for yourself what works better. They use different engines to power semantic features, so the difference in features is pretty substantial, and neither is strictly better than the other.
Yes, I missed it. The encoding is not the same, because I reused the end of the previous range as the start of the next one. So the array [5,76,145] encodes the ranges 0..4, 5..75, 76..145, 145..255. Then you have a next state array of size 4. The nice thing about this encoding is that it is very fast to find the next state. If you want linear search you can do that as well. Basically, given a character c, the index into the next state array is the number of elements in the range array &lt;= c. For example if c=78 then 2 elements are &lt;= c (namely 5 and 76) so the index into the next state array is 2. So the start and end don't need any special cases. I found binary search to be faster than linear search, if you make sure it gets compiled without a branch. The early-exit test will kill the performance of binary search. Paul Khuong has done awesome research about this, if this interests you. E.g. https://www.pvk.ca/Blog/2012/07/03/binary-search-star-eliminates-star-branch-mispredictions/
Great Talk. Just wanted to say that you have found a great way to structure it. Its not easy to "squish" that much information in such a short amount of time. It was really helpful and i am eager to read the paper now. Turns out the `evmap` was also very interesting to me because i don't think there is much choice of concurrent hashmaps in rust currently – at least i think that is the case. May i take the opportunity to ask if there is a way in `evmap` to have an `insert` followed by a `refresh` in such i way i can't forget to call both like in a `insert_refresh` method? I see many benchmarks use this pattern and i think you talked about it in your talk as you compared noria to other databases that this was the pattern you used for comparison. I think it would just be more convenient. I couldn't find it in docs. Another thing that could be quite useful with the way you structured `evmap` is something like transactions for maps. Having something like a `discard` method that would discard every inserts(and updates etc.) to the point of last refresh (so basically a copy of the current read map) sounds like a useful thing to have given the way `evmap` works. Anyway Thanks for this talk, your streams (especially the async/tokio one) and the crates and overall work you've done! 
Thank you for your reply. I have still yet to fully watch the video, but in the video the DB is described as being eventually consistent. Now, how would I ensure the DB is consistent after synchronizing from data source A, but before starting sync from data source B? Because unless it is possible to somehow wait until consistency has been reached, it is possible that synchronization rules do not work as intended when they operate on partially outdated data.
&gt;It would be awesome for resvg to have a piet backend. I think in the future, resvg should use `piet` directly, as a main "backend". But since it's very primitive at the moment - the are nothing to implement. &gt;If that's not practical, I would like to know what the major sticking points are. Here is a general list of [required features](https://github.com/RazrFalcon/resvg/blob/master/docs/backend_requirements.md). Also, I forgot about paths rendering quirks. Even cairo and Qt are pretty different. Qt has a strange handling of ClosePath ([my workaround](https://github.com/RazrFalcon/resvg/blob/d5f97660ec818e6cc1a9ebecdb1c0c813ec568cd/src/backend_qt/path.rs#L55)). Cairo doesn't support/fails to stroke paths with a square cap ([my workaround](https://github.com/RazrFalcon/resvg/blob/d5f97660ec818e6cc1a9ebecdb1c0c813ec568cd/src/backend_cairo/path.rs#L101)). And there are at least three more problems with paths rendering that are not solved yet. In both backends I use. By the way, I'm very interested in the `piet` development. I've planed to do the same by myself for `resvg`, but I thought that I should implement backends separately at first, to see what actually can be done generically. So at the moment, `resvg` backends are partially generic and partially separated. Text rendering is the main pain point at the moment, since it's implemented very differently in cairo (pango actually) and Qt. Also, I will be glad to help with the `piet`. Like adding a Qt backend or any other features.
I would imagine a Rust wrapper around SUNDIALS to be a bit easier to implement since the code base is more modern, plus in my experience SUNDIALS &gt; ODEPACK. With that said, we definitely have a chicken/egg problem here - I've often thought about doing exactly this (along with ARPACK and expokit), but didn't think there would be enough users to make it worthwhile. Any reason you aren't hopping on the Julia bandwagon? Their DifferentialEquations.jl library is light years ahead of everything else. 
Ah I see now, yeah, that's neat. I don't know whether that would use less space necessarily, since storing the extra transitions might offset the space saved by using one byte per range instead of two. But yeah, maybe I'll experiment with it some day!
I'm using `clap` for CLI argument parsing for things like `--verbose` or `--dry-run` but in my code the logic I have been adding everywhere is "if verbose then println" and "if dry run, don't do this thing". This seems pretty unclean and I feel like there must be a better way. Any suggestions on how to add this without if statements everywhere?
&gt;Do i invoke UB doing that? Allowing data races is [categorically UB](https://doc.rust-lang.org/beta/reference/behavior-considered-undefined.html), so everything you're trying to do here will invoke it. If you are indeed OK with your main thread reading potentially garbage data, I'd probably go with the workers just writing to *mut slices.
Cool notes about `evmap`[1]! Excited to give it a try! [1]: https://docs.rs/evmap/4.0.1/evmap/
Actixweb is pretty nice... The async stuff does need to get fixed though 
It's really easy to connect Fortran and C. I believe it was Fortran 2003 that added 1st class support to link C and Fortran libraries together.
Yeah, this totally makes sense. My use case is a personal project [uwc](https://github.com/dead10ck/uwc) that is like `wc`, but is Unicode-aware. I want a way to count all the things in a file without soaking the whole file up into memory, which means I need to operate on pieces of the file at a time. Currently, I just naively split on LF, since that will always be correct, but I was thinking about ways to make it more consistent. To count grapheme clusters correctly, I need to make sure the chunks don't split in the middle of grapheme clusters, since it would get double counted. Incidentally, after writing this, I realized I'd have the same problem with splitting in the middle of words, so what I actually need is to guarantee that it doesn't split in the middle of a word. I had planned on implementing this chunking myself anyway, but was wondering if there was a lib for this already. In any case, ropey looks like it is easier to work with than a `BufReader` directly, so I think it could still be useful.
Has anyone linked Julia libraries with Rust? If it's not too bad that might be a really good option to go down assuming you don't see major performance issues from making a ton of calls to a Julia library.
I saw this great list of modern books on C a few years ago. For each book on this list, I've either read it in its entirety, or scanned through it, reading the sections that I needed. To be fair, I did read through some version of K&amp;R many years ago when I first started doing C. However, I believe the following list would give you a very thorough grounding in modern usage of C from scratch. &amp;#x200B; &gt;Personal opinion - (in order from beginner) &gt; &gt;\* C Programming Absolute Beginner's Guide by Perry and Miller &gt; &gt;\* C Programming A Modern Approach by King &gt; &gt;\* Programming in C by Kochan &gt; &gt;\* C Primer Plus by Prata &gt; &gt;\* Head First C by Griffiths &amp; Griffiths &gt; &gt;\* 21st Century C by Klemens &gt; &gt;\* Understanding and Using C Pointers by Reese &gt; &gt;\* Expert C Programming - Deep C Secrets by van der Linden &amp;#x200B;
Ah, I see. Linear search would probably be better in that case, since you can't beat a single correctly predicted branch. A DFA compiled to actual code is probably the fastest, since you can arrange the tests to do linear search or binary search or something else on a state by state basis, and the branch predictor could learn the frequencies of each transition separately. Maybe that's also the only reasonable way to do capture groups, because you then need to update additional state on some but not all transitions.
Is it apple’s fault or a bug why the games don’t work on IOS? I’m getting an error when loading the audio.
Ohh...TIL :-) What's the point of this though? Why would you ever want to to do this?
What exactly does control mean? You mean a sort of "explicit is better than implicit" ?
I don't know that I agree, but I'll admit I don't know much about this space. From my armchair Netflix and Hulu and Spotify and other such services seem to do okay - I think a game store could thrive with proven value. People will totally pay money for web-based services. I'd pay for games on a steam-like equivalent that serves up wasm for my browser instead of installing something.
I come from Python, which has the same lack of enforced member privacy, and I do also use JavaScript. One of the biggest reasons I consider it important is that writing a safe wrapper around `unsafe` often involves maintaining invariants and controlling access to private members is key to that. That's why people will sometimes claim that "`unsafe` contaminates the entire module scope". You have to audit the entire module if you're running into a bug caused by breaking an invariant that's supposed to be upheld by member privacy. In Python or JavaScript, bugs can manifest in frustratingly obtuse ways at times, but you still have a runtime that aims to guarantee that bugs cannot cause stack corruption and the resulting broken tracebacks.
it's not eventually consistent. The point was that one mechanism of doing it could make it eventually consistent. And even in that case, it sounded like that could be a tunable parameter. However, the actual implementation makes it consistent. At least that's what I understood from the vid.
&gt; A DFA compiled to actual code is probably the fastest, since you can arrange the tests to do linear search or binary search or something else on a state by state basis, and the branch predictor could learn the frequencies of each transition separately. Dunno. Maybe. There's a lot of "ifs" hidden in there! For Rust in particular, which doesn't have `goto`, you'll wind up leaning pretty heavily on the compiler to optimize your code. The obvious way to implement a DFA in pure Rust code is to use an `enum State { ... }` where your core loop is a `match` over those states. For that to be as efficient as the table base approach, I suspect you need to rely on the compiler to compile that code as if it were using gotos. I don't know if that works today. &gt; Maybe that's also the only reasonable way to do capture groups, because you then need to update additional state on some but not all transitions. It could even be smaller in size, since you no longer need the reversed DFA. Unfortunately the literature on this topic is hard to read. In my README, I link to a paper by the [re2c](http://re2c.org/) maintainer that does a more thorough treatment on tagged DFAs: http://re2c.org/2017_trofimovich_tagged_deterministic_finite_automata_with_lookahead.pdf --- I've only read it once so far, but I can indeed confirm that the literature on this topic is hard to read. :-) But I have hope I can fully grok that paper with a few more read throughs. The author is however skeptical of its utility at runtime: https://groups.google.com/d/msg/comp.compilers/vlyUoZopxgY/CAEADtlsDAAJ
I find the docs pretty stellar and clear. Its instantly clear that PEM doesn't work since it states der only. So either convert it or find a crate that will for you (its not hard!). That particular crate is not all batteries included like curl (which as someone who doesnt use curl often LOL at figuring out how to do what you did w/o google). Given the .der thing though it replaces those four lines of python pretty well. You come across a bit overly negative. These docs have a few examples of how to use it and every struct has great documentation on what it is and how it works. Any further questions and you can be brought right to the source. Not sure what else you want other than a complete hand hold. 
The difficulty level of rust really jumps up on data structures, however, i tend to agree unsafe rust is 'not worse' than C or C++ to implement a data structure on, modulo the number of gotchas that C family languages pretend aren't there and that rust users want handled.
Maybe one day. More likely it’s just repackaged as in electron and sold in steam.
How should I figure out if the Julia bandwagon is worth jumping on? I personally like the appeal of using a single C/C++ like language to do all my work in. I don't really know if I like run-time compilation anymore :(
Whatever a reactive framework is...
We've had mixed results with opt-out threads so far, so the initial approach was to use them sparingly. Besides, it's easier to spot something that is there than something that isn't, and some types shouldn't implement `Debug` (or need a special manual implementation).
The easiest way I found was `[&amp;'static [TokenType]; n]` like so: struct TokenType; const prec_1: [TokenType; 1] = [TokenType]; const prec_2: [TokenType; 2] = [TokenType, TokenType]; const prec_3: [TokenType; 3] = [TokenType, TokenType, TokenType]; const foo1: [&amp;'static [TokenType]; 3] = [&amp;prec_1, &amp;prec_2, &amp;prec_3];
Each struct has a semblance of documentation, not "great" at all. There's no topic search-able way of going from "I want to do this" to the api in question, only random dicking around with each struct to try and figure out if it's somehow connected to what you want to achieve. In this case, the certificate and keys are specified, unless the code itself does the re-transformation in-line, it's not "just" to reformat them and re-do. The files are in a standard format. So, given this, how much harder do you think it'd have to get in order to make a simple HTTPS request? How many lines of code, hours of googling, and seemingly random rust crates do you _need_ to replace a five line Python script in rust? As for the curl comment about how Curl's API works? take the example code above, add `--libcurl example.c` and it auto-generates the code for you. This ergonomy problem is the part of rust in general, it's fostered a community that thinks that it's _okay_ to not have an easy-to-start documentation. To simply dump you with an index of `structs` named somewhat-akin to what they do, and place examples for each piece. ( Compare the `clap` documentation with the `reqwest` one. Or the python `requests` documentation.) The book `rust by example` is a decent start, but even that starts throwing in seemingly random crates for various usage, because the language itself is too painful to use, and doesn't go through _how_ it decides on these crates, or how they are to be maintained etc. ( See fex. the section on command line parsing, where they punt a proper example to the `rust-lang-nursery+clap` because parsing command-lines with rust is too painful to give an example of. So yes, there's a few ergonomy issues with the language that need to be seen to, and maybe even it's community needs to get bitch-slapped a few times to get out of their stockholm-syndrome'd "of course we're aweome, yayaya!" American patriotism style. The language has promise, the community has come to a good start, but what people accept as "this is fine" wants me to just toss them a plastic bag of gasoline and leave. 
Say you have a function `func1` that creates an array `foo` in its body, and wants to use some function `func2` to modify it somehow. When you call `func2`, passing the array by value, you have to copy the entire array. When you call `func2` and pass it a pointer to the array, you’re only copying 8 bytes (on a 64-bit machine), or the address of the array. It uses less memory, it’s faster, etc. Your question is really asking what pointers are good for, so I would just google that if you want to wrap your head around it.
How do you expect it to work with threads writing over each other? If you mean atomic writes, then you could make the buffer consist of atomic types and write to that - no locking needed, and can be done completely with safe code.
If a function takes a reference, you don't need to box the data to be able to pass it to that function - just have the value on the stack, and you can pass a reference to that.
You are awesome man, I thank you very much that you take your time and teach me some things that I didn't know. &amp;#x200B; This is my first time reading binary files, so don't know if theres another way more efficient to do this, but the spec of the [gox format](https://github.com/guillaumechereau/goxel/blob/master/src/formats/gox.c#L26) doesn't has a specific order for the layer, img, blocks, etc, so I need to read some parts (but the author is[thinking on change](https://github.com/guillaumechereau/goxel/issues/131) the binary format), if there's any other way to read kinda dynamic or unordered arrays from binary files please let me know. &amp;#x200B; Will research about the BufReader and try to see how it will improve the reader itself. &amp;#x200B; I think cargo has a feature for examples so I don't need to declare another toml file for the examples but need to read more about it. &amp;#x200B; Will try this week your proposed change about the BufReader and try to check why it throws the out of bounds error. &amp;#x200B; Oh! you are rigth, a Hashmap could improve the performance because I don't need to iterate over it, you are a genius. &amp;#x200B; Regards
intellij blows vscode out of the water if only because of the inline type annotations that display next to variables. But it loses the ability to infer types once code becomes nested with combinators and such. If I had one big ask for rust 2019 it would be to make IDE tooling much more robust. Doing async programming without IDE guidance is painful. I pretty much think that all languages should look at Idris for ideas.
&gt; Not sure what else you want other than a complete hand hold. Nah I agree with him, documentation for reqwest is awful, I use the client that ships with actix-web even if i'm not writing an API.
What a great talk! The epoch counters and the even/odd trick are pretty cool.
&gt; What Rust has over C++ for me in this area is coherence. C++ has so many little inconsistencies that you need to keep in your head as a developer, so far Rust feels very coherent to me. I don't think there's necessarily anything for Rust to do here other that keeping C++ in mind as a cautionary tale. Indeed. I've thought a lot about this, and I fear that there are two issues plaguing C++: - Backward compatibility with (most of) C has saddled C++ with a lot of semantics, and leads to inconsistencies when people nonetheless introduce goodies for the C++-specific parts of the code. This leads, for example, to the Empty Base Optimization: a data-member is not allowed to be 0-sized, for compatibility with C, but a base class can be because C has no base classes. - Piecemeal design. Adding features to C++ is, understandably, a heavyweight process. However, instead of promoting quality, this has led most proposals to be as minimal as possible (and even then, it's still hard work). This, in turns, leads to a language which feels "tacked on" with a lack of coherence, and uniformity. All in all, I sometimes think that C++ lacks a vision statement. Everyone pushes for their own pet feature, with no real goal in sight, and this pulls the language hither and tither.
For me it just comes down to what language I like more and it is Rust for various reasons. I think in the end that will be the determining factor. As long as there are equivalent libraries available for doing web programming. Performance is also a thing that people want and rust is shining in that respect. It's hard to predict but I think both will be big and the followers will have different reasons for their choice.
Indeed, header files are just much harder than proper modules! I've seen so many beginners completely stumped by an atrocious message which resulted from copy/pasting a header to get started, and forgetting to change the header guard name, or by not copy/pasting one and forgetting the header guard name (or `#pragma once`). In either case, the compiler is totally unhelpful, either complaining about missing types (what, but I so am including the header!) or about redundant types (why do you mean it's already defined in the same header, of course it's defined there!). I am SO looking forward to modules in C++, no matter which solution is adopted with macros.
Holy shit. That double map / deferred write is pretty cool and easy to understand. I'm not really a computer scientist or programmer but i'd like to ask if this idea is the same as read-copy-update exclusion i've heard about in the linux kernel?
Hum... I seem to remember that there were issue with regards to the `?Sized` bound; because of its special spelling (starting with ?), it was not originally accepted everywhere a bound was accepted. I wonder if the code of `std::io::copy` was written in a time where `?Sized` had to be next to the function name forcing the author's hand, and the author still decided to place other bounds in the `where` clause to avoid "surcharging" the declaration line.
&gt; especially async code complexity Man, ain't this the truth. Doing shit like chaining futures with combinators is fucking awful, alternatively I have to implement Future which is a shitshow in and of itself if what you're doing doesn't map cleanly to `try_ready!`. Documentation is generally poor, very few examples, compiler errors are impenetrable, etc etc. Async/await is _not_ going to help in this case, or if it does it'll be a superficial benefit. The second you start needing to do something even slightly more complicated beyond an echo server or a stupid fucking chatroom example you get inundated with `Expected Unhelpful::Error but got () instead`or trait missing. The thing is, I don't think there's anything a priori bad about the design of the libraries, they're just not documented very well. The async programming book hasn't had any updates since march of last year, and probably will never see completion, tokio guide saw a few extra pages and topics but there's a lot of repeated information and very little curation. It's basically an amalgamation of stuff from the doc blitz effort and just not very useful. I get it though, writing documentation is boring and thankless, but the hard to swallow pill for the various rust core team members involved in these efforts is that adoption is never going to happen without it. Rust is substantially harder to use than other programming languages, hands need to be held more, information needs to be spoonfed more, etc.
You’re looking for r/PlayRust.
It can be just authors personal style. For instance I was used to place lifetime bounds at generic parameter introduction and trait bounds in where clause.
Investigating further, I don't think it's actually the byte counting that's the issue. I tried a variety of alternative approaches: - Using `.iter().enumerate()` (I actually tried this before you suggested it, and unfortunately it doesn't help). - Doing a `for i in 0..len` with manual indexing (both checked and unsafe unchecked), using `i` itself to accumulate byte count at the end. - Doing a `while i &lt; len` with manual incrementing of `i` and manual indexing (both checked and unsafe unchecked), using `i` itself to accumulate byte count at the end. The latter two explicitly avoid counting bytes beyond the indexing itself. And they all result in the `char_to_byte` benchmark becoming about 2x slower than the pointer-based implementation. So I think something else is going on here. Maybe the pointer-based version is more obvious to LLVM, and it's able to do more clever optimizations after inlining it in certain places. In any case, I can imagine the optimizer getting good enough to handle this particular case. But I think the line-ending functions are a lot trickier. Would love suggestions there, because I definitely want to minimize unsafe code, but I'm skeptical if they can be safe-ified without losing a good deal of performance.
Sure. Note however that as long as you need only one connection, the current tokio-postgres master branch should work fine: The idea is basically that you have one "client" struct is cheaply clonable (soon?) that puts your request in a channel and you spawn off the "connection" struct that reads off the channel and resolves your futures when the response from postgres comes in. Also note that there is already an r2d2-but-async library, aptly named [bb8](https://github.com/khuey/bb8). But I'm not sure how mature it is, it's not on [crates.io](https://crates.io). There's an [postgres example in the bb8 repo](https://github.com/khuey/bb8/blob/master/postgres/examples/static_select.rs), but I think this does not work with current tokio-postgres master (yet).
You can save yourself a lot of unnecessary pain by learning how Rust structures stuff in memory. You still deal with pointers. You still are dealing with malloc. You still need to understand the difference between fixed and variable length types. You are working with structs and arrays. Basically, Rust is C with certain best practices baked in, and with a lot more modern conveniences. It doesn't really let you do anything that you couldn't do in C - it mostly just prevents you from doing stupid stuff that you could do in C but probably shouldn't. Buy the O'Reilly book. They do a superb job of explaining the data structures actually being used for various things in Rust. 
I don't understand why, with move semantics, Rust copies the vars y and z in this example code. Why doesn't it re-use the storage? The original struct is only dropped once at the end: const ASIZE: usize = 65536 - 1; struct Big{ s: u64, s2: [u64; ASIZE] } impl Drop for Big { fn drop(&amp;mut self) { println!("Drop: {}", self.s); } } fn main() { let y = Big{s: 0, s2: [0; ASIZE]}; println!("y: {:p} ", &amp;y as *const _); let y = y; println!("y: {:p} ", &amp;y as *const _); let z = y; println!("z: {:p} ", &amp;z as *const _); let z = add2(z); println!("z: {:p} ", &amp;z as *const _); } fn add2(mut x: Big) -&gt; Big { x.s += 2; x } See in Playground here, the pointer addresses keep changing by the size of the struct: [https://play.rust-lang.org/?version=stable&amp;mode=release&amp;edition=2015&amp;gist=a3aded564a13180b190ad6ac18af160d](https://play.rust-lang.org/?version=stable&amp;mode=release&amp;edition=2015&amp;gist=a3aded564a13180b190ad6ac18af160d)
To be blunt: if your use-case is read-only, then Ropey is almost certainly a terrible choice of library for your problem. Ropey is optimized for mutation of the text. It also _has_ to load the entire file into memory to operate on it, so I especially don't think it helps in your case. I suspect that what you'll want is some kind of streaming approach, where your algorithms for counting items (graphemes, words, lines, etc.) simply don't depending on where the breaks happen. In other words, design your algorithms so they can resume after arbitrary breaks and still get the right answer. It's definitely possible, and would make your program robust against even pathological files, which would be awesome. :-) But Ropey is 100% not the right library for your problem, I promise.
I used your `IterGen` type with my `BigUint` type to iterate over its digits (and eventually sum them), and the resulting code was nearly twice as slow as when I summed the digits "directly" (so by adding them to an accumulator instead of yielding them). Is this kind of overhead to be expected, or might I be doing something inefficiently? It's a bit hard to post the code for this because there's a lot of code involved, but I may be able to come up with a smaller example.
I'm pretty sure a trie is simply a kind of tree. Different kinds of trees have different trade-offs, of course. So maybe an RRB Vector would have some advantages over the tree I've implemented--I'm not sure. But the point I was trying to get across is that it's not just a "backing store". I can't just swap it out: the chosen tree structure _is_ Ropey. Ropey _is_ an implementation of a particular tree data structure. So a rope based on an RRB Vector would be a completely different library--a complete re-implementation. I would love to see someone implement a rope that way, but that's just not what Ropey is.
Honestly, you probably don't even really need an async threadpool with query pipelining in place. The throughput of a single connection is pretty substantial and will probably not be a bottleneck.
Ah, I see. Thanks for your feedback and advice!
I'm not trying to say reqwest is the poster child for docs. For a small crate though it has enough documentation to get you going without too much trouble. It's not the go-to in rust like curl or request are in their respective domains so comparing documentation is silly. Look at Rocket if you want to compare best in class docs because it blows most projects out of the water. For the most part crates in rust try not to do too much and have a baseline decent documentation s.t. usability is pretty good for newcomers. Very popular crates do even better for the most part. Picking the most popular packages of a decades old language that's been iterated on over decades is a bit unfair for a comparison to crates literally developed in the last year or two. 
I had the same idea about a streaming approach, but the unicode-segmentation lib only operates on `&amp;str`s, so without implementing the Unicode algorithms myself, I think I'd need to pick segments somehow no matter what.
RuneScape would like to have a word with you
What version of Ammonia are you using? In versions before 2.0.0 add_tags() takes a &amp;mut Iterator whereas in 2.0.0 it takes an IntoIterator. Based on your example I'm guessing you've got your code based on 2.0.0's docs but perhaps you're including 1.something instead of 2 ?
If you're on Google Cloud Platform, here's a multistage build using Cloud Build steps: - name: 'rust' id: 'cargo-build' args: ['cargo', 'build', '--release'] - name: 'gcr.io/cloud-builders/docker' waitFor: ['cargo-build'] args: ['build', '-t', 'gcr.io/${PROJECT_ID}/monitoring/actix-test-client:latest', '.'] options: machineType: 'N1_HIGHCPU_32' images: ['gcr.io/${PROJECT_ID}/monitoring/actix-test-client:latest'] and the Dockerfile FROM debian:jessie-slim RUN mkdir -p /app WORKDIR /app COPY ./target/release/actix_test_client . EXPOSE 8080 EXPOSE 4567 CMD ["./actix_test_client"] 
Browser support has been discontinued. No Java support in new browsers. Like I said harder. Exceptions happen. But if you are trying to make money selling through a market place will give you a high chance of success.
For an example of the abyss that is C++ see https://mikelui.io/2019/01/03/seriously-bonkers.html. I agree that backwards compatibility is one of the reasons C++is in this state, but not just with C, backwards compatibility means most of the problems introduced in C++ itself will never be removed. Backwards compatibility is of course a useful feature but at a huge cost. As far as vision statement goes I believe they've finally created a kind of steering group to try reign in the madness, I feel it's far too late but good luck to them.
&gt; Rust has so much syntax I wonder if you can elaborate on that? On the scale of minimalist grammar (Lisp) to grammar for the sake of grammar (Perl), I see Rust as mostly equivalent to Go. The one area where Rust is more complex is type theory (Go doesn't need a syntax for a higher rank trait bound ( `for&lt;'lifetime&gt;` ) or abstract return type `-&gt; impl Iterator` or so on because the type system is much less powerful). But you're not going to run smack into that as a newbie unless you [already love that stuff](https://image.slidesharecdn.com/qconny-12-120618131645-phpapp02/95/domain-modeling-in-a-functional-world-31-728.jpg?cb=1340300096), so I'm confused. I'm even more confused, because Python has more syntactic bells and whistles than either. 
Oh yes, for sure, you'd want to split on codepoint boundaries at least, so you can pass valid utf8 strings to your code. But that should be relatively straightforward, I think. The max length of a utf8 codepoint is 4 bytes, so you're guaranteed to be able to do that in bounded memory.
You can make a macro to print only in verbose mode. It's harder to address the dry run issue without seeing your code. It might be possible to first construct a plan represented by some data structure then execute it if dry run isn't set. If isn't always bad though.
I'm not sure exactly, but have you ever seen doc comments? There's a lot of content there, like markdown, code blocks, etc. and (especially with code blocks) it's nice to be able to see by the starting characters of that line that it's a comment, and not something else. After doing rust for a while, I can also appreciate no "bare" comment lines where there's just text on a line with nothing marking that it's a comment.
Ahh it might be. Then I’m also looking at the wrong docs. 
If you are locked to 1.x then try adding a .iter() to the array
The good news is that the usual use case doesn't typically require adding a self-signed certificate to make HTTP requests. The reqwest docs walk through how to do the most common things with very little code. If you'd like to see a certain workflow improved, I'd recommend opening and issue to discuss it, whether it's just improved documentation, or API adjustments. And contributions in those areas are highly appreciated!
It's been a while since I've tried Rust so it might well have changed, but when I tried it last time my problem was actually the type system: it can do so much and it's so powerful that I often got confused on how things should work. Go's types are less powerful but that also makes it way easier to understand how things work. Python has a lot of syntax as well but is very easy to get in to, as you don't have to use too much of it to make something useful. When I was trying out Rust I felt like I had to know 110% of the language to make a simple CSV parsing program. Still, the last time I've tried Rust was several years ago, it might actually have been before 1.0. I might try it again soon just to see if I still have the same problems.
Check out https://doc.rust-lang.org/reference/influences.html for a start!
Python doesn't really have macros in the same way Rust does, it has other ways to enable metaprogramming. Macros are mostly a Lisp thing, and are also extensively used in C/C++. Also, the language reference [lists some of its influences](https://doc.rust-lang.org/reference/influences.html).
Rule 1 when using condvars - use locked resource before waiting.
Not sure why you think the macro concept comes from Python, which is a language that has no concept of macros to begin with. Rust is mainly an ML family language with a lot of history particularly based in OCaml. I do see a lot of python influence in terms of usability and ergonomics however I also feel both rust and swift have very similar languages (for many reasons) and can see both languages (maybe coincidentally) riffing off of each other.
Yeah, but my problem is that simply ensuring segments are valid utf8 isn't enough. As an example, if we want to count words, and we have the string `foo bar baz`, if we end up splitting this into `foo b` and `ar baz`, counting the words in each string and then summing them up would yield incorrect results—it would count `b` as a word, and then count `ar` as a word. We could have an analogous situation with certain grapheme clusters. So depending on what you're counting, the choice of boundary needs to be more granular than `char`s for correctness. I think you're correct in that this could be done with a pure streaming approach; say for example, you could implement the unicode grapheme cluster or word segmentation algorithm on an iterator of continuous `char`s, or even `str`s, but alas, no such lib already exists, and implementing the unicode algorithms is not a trivial undertaking. So I think the only option is reading chunks of the file at a time, buffering results, and iterating over segments whose boundaries are the largest unit of text that you're counting.
&gt; maybe even it's community needs to get bitch-slapped a few times to get out of their stockholm-syndrome'd "of course we're aweome, yayaya!" American patriotism style. Even better than "bitch-slapping" is contributing. You know the famous "be the change you wish to see" idea. I would never claim that reqwest's documentation is amazing. I do think the main page is pretty clear to make common requests. What often happens is more advanced features are added, but as I don't typically need them, I don't feel the pain of them having less fleshed out docs. So, as someone who found parts lacking, let's make it better! 
Working on learning c and c++ in depth, I am SO glad I started in rust. Dealing with segfaults is hell. And while there is obviously a ton to be said for c and c++, it feels like a train wreck of legacy. People tout the safety of rust, which I now definitely appreciate and miss, but for me the deciding factor was the package management for rust. I’ve grown to love the language for many other reasons, but that’s neither nor there. But now working in c/c++, It feels like a variation of concepts I already know. Btw, just as an aside, you can write high level code that that turns into very optimized code. For instance in rust, there are iterators (I think they’re called ranges in c++ but don’t quote me) that do an operation on a list of items. The traditional way would be use a for loop, but if you use an iterative, the compiler can tell that it should unroll the loop which is faster. So, just saying take the “if you truly care about performance the only way to do it is how c does it” with a grain of salt. Not that there’s no truth to it, but it really depends on your situation. 
It was one of the big points for 2018, but it sort of got neglected. Making it a true focus in 2019 would be nice.
Hi, I really like your work jon and I believe I saw it in the past on reddit. Firstly, the adaptive element of this is amazing and the cache principle is very good compared to memcache/cache-stampede/cache-invalidation of the past. However, as a DBA/data-performance-engineer, you cannot really compare to an actual materialised view that someone took the time to data model well. If I setup a manual materialised view on MySQL (with a refresh per hour/5min or trigger based), it might actually beat those 14million reads. Maybe if I throw in ProxySQL in there with a TTL cache, it could. Not only that, but you're half way to setting up a data mart/data warehouse as well. I am regretful that MySQL and to a large degree Postgres, does not really have materialised views like Oracle and MS SQL. I believe that had it done so, we would have not seen 2/3 of all the middleware/caching we are dependant on today in the open-source world. Hopefully, one day someone comes up with a Rust-based database that has fast materialised views. And they will probably use your evmaps.
https://www.reactivemanifesto.org
`std` uses `cfg` to base code off of the platform, so for major platforms that wouldn't be a problem.
https://github.com/rust-dev-tools/fmt-rfcs/issues/17 is the canonical discussion on the issue.
This series is great 
I agree, but a Connection Pool should also deal with the issue of re-connecting which can be a pain to deal with...
&gt; not unheard-of to implement vtables by hand It is exceedingly common in larger C code bases.
I use RxJs and Spring Reactive Web, however at first glance I don't quite get the api. Assume you know RxJs, how would I filter a Subject, map the value and push the value to all subscribers? 
He still has a point. Rust is harder to understand than Go. He is negative because he wants to use the language but gets stuck, and it doesn't happen with Go. The answer to this issue is improved documentation where you need to know less things before you can be effective in the language. 
You want to spam [r/playrust](r/playrust)... this subreddit is about the programming language.
[http://mekelatosgaming.com/](http://mekelatosgaming.com/)
These really are overloaded terms that have lost some of their meaning due to over use. In more concrete terms, I refer to the way the Flux pattern promotes the notion of keeping a single mutable state for an application and having the rest of it (often, but not necessarily a GUI) be composed of stateless _reactors_ that are triggered upon changes in that state. See the [documentation](https://docs.rs/reducer/1.0.0/reducer/#overview) for a brief overview of the Flux pattern and an example usage of Reactor.
The devil's in the dets, but Python's great for writing direct logic that's easy to refactor. Eg no fighting a borrow checker or matching brackets while experimenting, and a nice REPL. (Ipython, or Jupyter)
C# for async/await should be added ‘?’ I think originally came from C# as well, in an option context, where Swift borrowed it which I think influenced Rust?
I don't think I've had a positive experience yet looking at the documentation of Go packages. I'm not a frequent Go developer, and haven't looked at too many, but the ones I did look at are super popular, and the experience was horrible. We probably judge the experience based on how well we already understand the language and common patterns. Comparatively, since I know Rust so well, I can usually be fine with crates that just list their types and methods.
RxJs is actually [quite different from Redux](https://stackoverflow.com/questions/34497343/redux-rxjs-any-similarities), which is the corresponding counterpart of Reducer in the JS world. Both promote reactive styles of programming, but while RxJs provides basic building blocks (kind of like Rust's [message passing](https://doc.rust-lang.org/std/sync/mpsc/) on steroids), Reducer, much like Redux, rather aims to shape the overarching architecture of your whole app. Put it simply, you can think of `reducer::Store` as a single global observable that everything else in your app observes. Ideally the store should be the only conceptually mutable object in your entire app, from which everything else derives. Does that help?
&gt; Sure, there's a curl crate, which links to a hard-coded commit that'll give you a version of curl from somewhere in September Which has this to say: &gt; This crate links to the curl-sys crate which is in turn responsible for acquiring and linking to the libcurl library. Currently this crate will build libcurl from source if one is not already detected on the system. Which leads to https://github.com/alexcrichton/curl-rust/blob/master/curl-sys/build.rs which hinges on the `static-curl` feature. A quick glance at [Cargo.toml](https://github.com/alexcrichton/curl-rust/blob/master/curl-sys/Cargo.toml) shows that `static-curl` is not set by default. Therefore, starting at line 24 of build.rs, `curl-sys` will check a special case for OSX then vcpkg on Windows, or pkgconfig on others. If the OS has a version of libcurl installed, then Rust will use that. If not, *then* there's the fallback to the included and possibly out of date C sources for libcurl. Python `request` encourages you to copy the source into your project, [see, right here](http://docs.python-requests.org/en/latest/user/install/) to wit: &gt; Once you have a copy of the source, you can embed it in your own Python package, or install it into your site-packages easily: And if I'm reading that correctly, they're saying to redistribute it! I'm not sure how - or if - they plan to fix all downstream projects affected by [last September's security issue](https://nvd.nist.gov/vuln/detail/CVE-2018-18074). To be fair, I run Debian for my amateurish Rust hackings, and unless I specifically think to install `libcurl3-dev` then I would be using Alex's snapshot. Debian at the moment has version 7.52.1+5deb9u8. 7.52.1 is 2 years old, but if I just type `apt-get changelog libcurl3` I can see which of the CVEs have been fixed and logged. The last three vulnerabilities are: - CVE 2018-16842 warning message out-of-buffer read - fixed in 5+deb6u8, 30 Oct - CVE 2018-16840 use-after-free in handle close - not yet fixed - CVE 2018-16839 SAS password overflow via integer overflow - fixed in 5+deb6u8, 30 Oct So let me sum up how exposed I am. If I just put `curl` in my `Cargo.toml` and use the snapshot, I'm exposed to all three. If I have `libcurl3-dev`installed, then the two more severe issues were fixed 30 Oct. The UAF ([which is in fact only a *potential* UAF](https://curl.haxx.se/docs/CVE-2018-16840.html)) isn't fixed in either. (Actually, it's interesting to think about the full details of the issue. The problem in libcurl is that the C abstract machine should be instructed to write null to a pointer in a structure, in the hopes that a null pointer dereference will crash the program instead of allowing use-after-free. The problem is that the C standard doesn't require a null pointer dereference have *any* specific behavior; it's undefined. I've spent a little too much time on this post, but I wonder if the fixed version uses a volatile write of `size_t` or something similarly pedantic. Doesn't an optimizing compiler have some wiggle room to be lazy and not actually null a pointer? This seems like the kind of thing where compilers will emit the more secure literal interpretation, but *technically* the compiler can notice that you never check for null and conclude that, within defined behavior, you'll never store null.) I don't think Rust is looking so bad on this issue. Again, how does Python's `requests` compare? Writing in Python means much less opportunity for mishaps, but it's not immune to security issues. And it looks to me like the way `requests` is distributed makes it significantly harder to push security updates to everyone. The project overview boasts 11 million downloads per month, but what about downstream projects who follow the advice and copy source into their repositories? 
Postgres had materialized views for a while....
`?` in Rust is *very* different from `?` in C#. Perhaps the latter once served as inspiration for the former, but I suspect it will be less confusing all around if we pretend there is no connection between the two :-p
C# only has the `?.` operator, which is similar to `Option.and_then` and thus quite different from Rust's `?`/`try!`.
Still does, but its not incremental. You have to refresh the whole thing.
The cairo behaviour around round vs square caps is intentional: https://cgit.freedesktop.org/cairo/tree/src/cairo-path-stroke.c#n836 This behaviour was chosen to match what Adobe Acrobat does for PDFs.
https://en.wikipedia.org/wiki/Rust_(programming_language) - Wikipedia has "Influenced by" for every programming language
How correct is [http://cglab.ca/\~abeinges/blah/rust-reuse-and-recycle/](http://cglab.ca/~abeinges/blah/rust-reuse-and-recycle/) as of this date?
I feel Swift is pretty much a Vala clone, not connected to Rust all that much though. There is also a lot of Erlang influence in Rust; there was even more early on, but it's still felt today.
You can use Actix or Rocket today and have a great out of the box experience and make any web application you want. Both are being actively developed and both will keep getting better for the forseeable future.
I suspect that Rust &amp; Swift will end up converging here, with Swift innovating on C# by using ‘?’ as a quasi-monad, Rust borrowing the syntax for the Result monad, then cross applying for Option but Swift is stealing Result from Rust, which got it from Haskell &amp; Erlang
If i had to guess, it has to do with array semantics. It looks like using a vec works fine: [https://play.rust-lang.org/?version=stable&amp;mode=release&amp;edition=2015&amp;gist=d7ea3b8967193355ec2792a39fec47bb](https://play.rust-lang.org/?version=stable&amp;mode=release&amp;edition=2015&amp;gist=d7ea3b8967193355ec2792a39fec47bb)
Let me elaborate a bit on my use-case where I evaluate Rust, as a `systems language` and find it lacking. The goal here is to replace a *shell script* on an embedded system. The script today uses (mostly) busybox built-in functionality, openssl, and curl. The first thing it does is generate a key and CSR, (openssl) then pass it off to a server, and gets a signed certificate back. ( There's nothing "self-signed" around here, it's all standard x509 PKI.) Then it downloads a set of sha256-sums, identified with the client certificates above, and downloads any missing files, or fixes up any files not matching against the checksums. Pretty standard \`system level\` programming in my world, a bit of auth, a bit of checksumming, some file management and some bog-standard https. 30 lines of shell-script, approx. Writing it out in C wouldn't be hard, and could easily link against the system headers, not too much of a problem there. I guess in theory one could use \`unsafe\` + \`libc\` crates in Rust to do the same. Or just do \`system\` calls to binaries and call it *solved.* Now here I am, beginner at Rust, and this barely works. There's nothing \`built-in\` for any of these common cases, community cannot even suggest which external project to use \_this week\_. When pointing out "hey, this is what I'm trying to do, but in rust" we get threads like this, without actual examples, with suggestions to "use this thing here that doesn't do what you want". \`Reqwest\` has this blurb as the first example of it's abilities: * Uses system-native TLS * Plain bodies, JSON, urlencoded, multipart * Customizable redirect policy * Proxies * Cookies (only rudimentary support, full support is TODO) Note how _none_ of those in the `docs.rs` section are actually _links_ to how to do it. Or even example documented. Of the total amount of thinks mentioned, `forms` and `json` are documented. This project doesn't need more bug reports, it's documentation as it's already a big TODO item. 
If I understand correctly, the writes are blocking (and slower due to re-materializing views) but reads are therefore very fast since they read materialized data. I have some questions: 1. It's simpler to effectively materialize a query many users make, if it's in fact the same query. But how do you handle a multi-tenant system where there are lots of queries which are all different, and thus impossible to effectively reuse the same cache? E.g. lots of users all querying their own personal feed. How do you handle when cache sizes get out of hand? Do you have any "intelligent" logic which tries to figure out what part of common user queries have in fact overlapping data (to reuse the same cache) and what don't? 2. Did you ever consider writing an "eventually correct" model where both writes and reads are fast, due to write data being asynchronously materialized, at the expense of queries not always getting exact up to date data? It's not as useful for transactional systems, but can be very useful for reporting systems where slight delays are OK. Perhaps even allow a read query to have a metadata flag called "live", where if true, the read would block until all writes are complete (as in the same of traditional systems), but if false, it'd read the last materialized version, so one could use different flags for transactional uses vs. reporting uses.
I think it"s just a different style. Rust feels at home coming from javascript.
Swift's history is very connectes to Rust, with more than one early core rust dev going on to work on swift.
rocket is indeed quite good, same level as Pyramid and Django, which is pretty darn good. However, right now I'm comparing the usability of simple https clients, and their documentation, and what people have come to expect as being documentation. In this case, the documentation, as it's formatted and shaped by the `docs.rs` site, has a usability problem in that it has taught the Rust users who've been in this relationship for a while that it's okay to have a front-page where random Structs have names that don't mean anything, and to get examples of what you want to do, you go elsewhere. The Python community has ended up with this split between `pypi` and `readthedocs`, where the site layout of Read The Docs set up an expectation of what kind of documentation there _should_ be for a project. This is part of the ergonomics of a language, and where Rust hasn't just failed, but has pulled down it's community's standards into thinking that the current state of failure is somehow exceptional. 
That's actually a good idea! Didn't think to more the array of references (so size doesn't matter).
The violence was a bad call from me, and I apologize. The current documentation of `reqwest` feels like a TODO-list of what to document, not a documentation itself. It even starts with a formatted bullet-point list of what _should_ be documented how to do, but then doesn't actually document any but two of them. The structs have random-related names, where `Identity` doesn't represent an signed-in `Identity` but some kind of x509 abstraction layer. Caching and timestamps are nowhere to be found in the documentation, neither is documentation of how to follow a redirect or not. On a per-function level, grabbing by example: `CLient pub fn new() -&gt; Client` "Construct a new Client". Sure, I can read the line above as well. Copy the paragraph above, tell the user why they want to use a function. `Client pub fn builder() -&gt; ClientBuilder` Why does this function exist? Why should I want a `ClientBuilder` and what is it good for? How does calling `Client::builder()` compare to calling `ClientBuilder::new()`? Why would you use one or the other? Further on, it just gets worse, more and more of those hopeless documentation lines that just repeat what the function call says. `foo = bar // Set foo to bar. ` The few places that have examples have oddly phrased English. `ClientBuilder::add_root_certificate` First line, `add` or replace? how do you prevent/zero the existing root certificates? Second phrasing: `this can be used` so, what other things _can_ it be used for? If it's only used for this one thing, it should be `this _is_ used` 
Have you seen [esp8266-hal](https://github.com/emosenkis/esp8266-hal) and [esp-rs](https://github.com/emosenkis/esp-rs)? Using mrustc, you can target ESP8266! Funding the continued development of mrustc (which compiles to C that can be compiled with GCC) might be cheaper than adding a new LLVM backend.
Interesting idea - that seems like a fairly simple (and useful) pull request that could be made against evmap, assuming nothing else is required beyond an insert and a refresh. 
Yes, good point. I forgot about that possibility.
I think if you are worried about getting people to just to bother (and risk!) installing your game, getting them to pay for it is out of the question.
It should be very similar, with the exception (I assume) that evmap also has a multi-value hashmap on top of it (and likely more optimized for it), whereas the kernel RCU is just a plain value (pointer?) that gets updated over time. 
Thanks for the reply, it's very helpful, especially the list of required features. That seems like a reasonable roadmap to work from. I plan to read through the resvg source more carefully so I understand what's going on. In general, I see a lot of potential benefit to joining forces. Maybe it would be higher bandwidth to have a video chat? Then I can explain the motivations for some of my strange choices, you can talk me out of them, etc.
Awesome response, thank you! `requests` itself vendors several of it's upstream packages, but generally the python story about packaging is a bloody mess. Most projects I've seen don't vendor it, and either use system python package, or build it and install it once in a venv and then never update it. The Python tooling for updating packages is a bloody mess as well. Then again, Python code has far less exploitable security vulnerabilities due to the lack of foot-guns in the language compared to Curl ( and curl in general does better than most C software, let's not get into that either) The Rust vendoring story is the age-old one where right now the pendulum goes towards `build everything statically and keep all minor pieces updated` rather than the previous centralized solution where nobody installed updates anyhow. Rust isn't making it better, or worse. But it's making it a fuck-ton harder to cross compile. 
What
That was an amazing talk! And as always with your Rust content, I found myself learning stuff once again -- you're great at explaining complicated things.
In practice this data seems small and only gets initialised once anyway so won't be a noticeable performance loss to use \`lazy\_static\` and then \`Vec\` as normal. Similarly to using \`RC\` when I can't figure out a borrowing situation IMO it's not worth suffering from perfection paralysis when confronted with stuff like this.
Overall, I haven't had too much trouble with reading Go code. Using it and getting started isn't too bad. Packaging and vendoring of it? Yeah, annoying. But not worse than C (none) or Python (messy). However, _reading_ Go code is a lot more pleasant than reading rust code. The comparative code to the Python and C examples above in Go would be [this gist](https://gist.github.com/michaljemala/d6f4e01c4834bf47a9c4) Except that it also prints the response and parses the commandline. 
Thanks, and yes, I'm pretty salty about this. It's a project that I've _wanted_ to get working quite a few times, but each time end up just throwing it all out and given up in disgust. Don't get me wrong, I've written (and shipped) Rust code, but this is an area where Rust is so far from usable as it can be. 
Noria looks very interesting. One thing that I am curious (though not entirely related to it but that I thought of immediately at the beginning of the talk when the architecture diagram came up) is a way to effectively disable the query planner and plan manually on the client. For many web applications the queries are hand written anyways and many of us have been bitten more than once where the query planner planned something we did not want. I fully understand why one wants a quert planned for olap and similar data processing tasks but it’s not all that useful when issuing the same reads over and over again. Noria’s design make make that not as useful any more I assume since most reads would be cache hits so i wonder if that makes any sense in that design. The second question is if this design can be used to efficiently debounce certain writes. For instance we debounce a lot of increments via a redis buffer which flushes over the increments every few seconds to reduce the cost of locking in postgres.
Procedural macros remind me a lot of python's decorators. At least the usage is very similar, but I suspect it's more limited.
Thanks. This link [https://docs.rust-embedded.org/faq.html#when-will-rust-support-the-xtensa-architecture](https://docs.rust-embedded.org/faq.html#when-will-rust-support-the-xtensa-architecture) probably needs to be updated to mention mrustc. I'm happy with any tool chain support for esp32 and esp8266. I agree maybe keeping mrustc might be the right way to go as well. mrustc, might be something I can more easily contribute to myself as well. It would a good way to learn the rust semantics, and I can see it a more general approach. 
&gt;I also feel both rust and swift have very similar languages (for many reasons) and can see both languages (maybe coincidentally) riffing off of each other. I write Swift code for a living and I agree wholeheartedly, Rust has many of the aspects that drew me to Swift. The main difference between their goals I see is that Swift seems to favor elegance / beginner friendliness and is willing to give up some correctness for it (e.g. automatic optional promotion, a `T` will automatically be promoted to a `Optional&lt;T&gt;` if the context asks for it).
&gt;ty
Everyone knows documentation matters, but it’s not being invested in till the designs are done. The async book has had a total re-write since then, you’re probably looking at the wrong repo. It still has a long way to go.
They wouldn't be hosting 'rocket.rs', they would be hosting arbitrary binaries. As such, there is no reasonable way to secure the tenants of the host from each other. That is why you are not finding any traditional shared hosts.
Not sure how you would find modern Rust. It is still more complex than Go, but it's a very different beast to pre-1.0 Rust (and I believe it is a fair bit simpler). https://github.com/BurntSushi/rust-csv is the defacto library for CSV parsing, and it has very good documentation, if you're interested in what that might look like in modern Rust.
This is rust the programming language. You want /r/playrust/ &amp;#x200B;
&gt; current master branch is supporting tokio 0.1 Indeed, I expect a lot of this stuff to land fairly soon. But I'm not sure I want to count it as "ready" until it's actually part of a release!
&gt;And i don't actually care about data races in this case. Can you elaborate on this?
Isn't this more an issue with the library ecosystem than the language itself though? Some crates (for example https://github.com/BurntSushi/rust-csv) have *excellent* documentation.
Swift's early announcements explicitly mentioned Rust as an influence. There's also been some syntactic features in each inspired by the other (e.g. `if let` was suggested because of Swift, and Swift's raw strings were heavily influenced by Rust's), plus more major things like some devs working on both (but not at the same time) and Swift's massive "memory ownership" feature.
Haha sorry new to reddit ty tho
First, i don't think blindly trusting the file extension is a good idea (File format usually have some header at the beginning that you can check for correctness) &amp;#x200B; Second, I think it's best if you work with the same \*decoded\* image data internally. So your Image would be a struct, not a trait. And you would have a Decoder trait or enum (A trait would let people using the lib use their own decoders, you can also do that with enums if you add an enum value which uses a generic trait given in its constructor but it's a bit more confusing imo) that transforms the file stream into your Image struct or simply an array/vec of bytes of decoded image data.
Ah, I was making in incorrect assumption: the `unicode-segmentation` crate definitely can handle split graphemes correctly (see the `GraphemeCursor` type), so I assumed it also could handle words that way. But apparently not. That's a shame. You could probably hack it, though: 1. Count words in chunk 1. 2. Count words in chunk 2. 3. Create a small string out of the last grapheme of chunk 1 and the first grapheme of chunk 2 and see if there's a word boundary at the seam. Subtract one word from the count if so. 4. Continue with chunk 3 (count it individually, then check the seam between it and chunk 2). And so on. A little inconvenient, but should work, I think...? There are probably some corner cases to handle, of course.
Because of the value given to the barrier's constructor, it's going to wait for n+1 threads (Plus main thread). According to [https://doc.rust-lang.org/std/sync/struct.Barrier.html](https://doc.rust-lang.org/std/sync/struct.Barrier.html) : "A barrier will block n-1 threads which call wait and then wake up all threads at once when the nth thread calls wait." &amp;#x200B; &amp;#x200B; If you only place the second wait, only the main thread will call barrier.wait, meaning it will never wake up. Because of the way it was constructed, it needs to be called by all the threads (pool.execute) and from the main thread (the second call).
To me, swift is to rust what c# is to c/c++ Obviously not a 1:1 comparison, but switching between those four languages daily, it's how they generally feel to me
[https://github.com/rust-lang/rust/issues/40090](https://github.com/rust-lang/rust/issues/40090) [https://github.com/rust-lang-nursery/failure/issues/71](https://github.com/rust-lang-nursery/failure/issues/71) &amp;#x200B; These might provide some context, even if not an actual answer. &amp;#x200B; On the second point, I would imagine the answer is probably that TokenStream is the internal representation already used by the compiler (I vaguely remember reading this somewhere), and cleaning it into something more usable would add overhead which may not always be desireable. Iirc syn is even recommended by the docs on procedural macros in the book.
Many of these comments are saying the same thing: async isn't stable yet and so all the frameworks that use it are following nightly (e.g. "unstable") or are split between futures 0.1 and 0.3. This, unfortunately, seems to have the impact of incompatibility. Why should I write an asynchronous database driver when I'm targeting a moving standard? It's an opinion, but I think that as soon as asynchronous/pin/futures hit stable, there will be a huge storm in the community to take existing libraries and add that support. Since Go basically started with the premise of Go routines, there wasn't a huge divide between synchronous and asynchronous code. Go hides all that complexity behind the scheduler and the incredibly simple `go` keyword. I think it's going to be challenging for Rust to adopt and embrace async, but Google's Fuschia operating system is doing some absolutely _gnarly_ stuff with futures being a core part of the operating system. The question remains though: how will existing libraries evolve to support both synchronous and asynchronous implementations? Cargo feature flags might be the answer, but I suspect that things are going to feel weird until the community comes up with a good design pattern around it. Another good point made here is that Rust's goal with that standard library is to provide a lightweight set of tools. Once you bake something into the standard library, it's very hard to change it. Go went the opposite direction, providing a ton of utilities in the standard library, making it really easy to build web services with few external dependencies. Rust, in a sense, depends on the community to provide frameworks for this. This has resulted in a lot of really novel frameworks that each have a different feel and objective. As an aside, I find Go very boring. I think of it like being Java. It's verbose, but the power is there to build massive and complex applications. I spent a long time as a senior Java engineer for JEE applications. I picked up Python and Ruby along the way and loved the ability to one-line things that were super verbose in Java. However, Python and Ruby (CPython and C Ruby) have a GIL and a number of things (infinite width arithmetic in Python) which seriously limit performance on modern hardware. IIRC Ruby has had lots of memory leaks that were subsequently fixed. I knew JavaScript as well, and NodeJS is a single-threaded event loop; it performs better than synchronous code, but is still fundamentally single-threaded. I learned Scala a couple years back and though the learning curve was hard, it exposed the performance of the JVM while avoiding the verbosity of Java. C and C++ were always sort of out of reach, and the hassle of memory management made it really hard to get buy-in at companies to write web services in them. The advent of Go really changed things, no more JVM! However, it's pretty verbose and boils down to for loops and if statements. Rust for me was the best of all worlds: expressiveness, performance, safety, minimal memory usage, no runtime, almost automatic memory management, portability, and the ability to customize just about everything. The learning curve was difficult, but after you get through the hurdles of borrowing and understand the borrow checker and type system, the rewards are incredible. In short, Rust has a ways to go, but even in the last two years it has come an awful long way. The next year is going to be very challenging, but I have no doubt that the community will rise to the challenge. In 2019, I expect to write web services, other network services, command line utilities, some embedded experiments with RTFM, and even ETL systems in Rust and honestly I can't wait to get started. I'll still have to fall back into Go at times, but Rust is still a serious contender right now for a do-everything do-anything language.
Its literally a few days old and the cairo backend was considered just recently because it now compiles on redox (the primary target for [OrbTk](https://gitlab.redox-os.org/redox-os/orbtk)) so the goal is to also use cairo besides WebRender, OrbClient, SDL?(i am not sure) and std-web ... its not decided yet. 
Apparently [Espressif are working on an LLVM backend](http://llvm.1065342.n5.nabble.com/llvm-dev-ESP32-Tensilica-Xtensa-LX6-backend-Interest-Prior-attempts-tp120780p120853.html). Assuming that is released then adding Rust support would be more likely. 
I would also add the type safety that struct's give you (The compiler can't ensure that a hash map has a key, but it can ensure a struct has a given field, making them safer) and how they don't require you to deal with string-indexing (Or hashable-indexing), which can get ugly very fast.
&gt; Rust is mainly an ML family language with a lot of history particularly based in OCaml. To be honest I think the OCaml influence is overstated and in particular not as marked today as it might have been at one time. Yes, OCaml has ADTs, pattern matching, type inference, but so does Haskell. This feels like a general ML heritage (in the sense that you can legitimately call Haskell an ML since Haskell was influenced by Miranda which in turn was inspired by ML). However, traits are a key aspect of Rust and without it the language wouldn't be recognizable or feel anything like Rust does today. The naming of things in the standard library also has substantial Haskell influence. Deriving is another thing straight from Haskell (with our own variation). So in the interest of fairness, as much as you mention OCaml you should mention Haskell.
I see what you are saying. But people also tend to equate paying with quality. It’s like if you price something to low on Craig’s list people may think it’s junk and not bother enquiring about it
Not quite. It's an issue with the _language_ because the _language_ (Well, developer community, etc.) decided to _not_ have batteries included. By consciously making that choice in 201x, and by not leading with examples _themselves_ of how to do it _"right"_, the system ends up with this semi-chaotic half-undocumented state, because it's acceptable. And, you're right about the `csv`crate, that documentation is _good_. ( From the same author as RipGrep by the looks of it?) 
C# also has the \`??\` operator, which is a quasi-monad for \`Nullable&lt;T&gt;\` (the \`Option\` of C#). The story I'm trying to tell (somewhat uncertainly) is one where C# invents a \`?\` and \`??\` syntax for simplifying optional types, Swift picks it up and simplifies it further, and Rust borrows it for the \`Result\` quasi-monad rather than the \`Option\` (and then extends it back to \`Option\`. The alternative story is one where a \`?\` quasi-monad is independently invented by C#/Swift and Rust.
&gt; But it's making it a fuck-ton harder to cross compile. Isn't that a different point of discussion? It's not true, anyway. Rust doesn't currently have ~~good~~ much of any support for dynamic linking between Rust artifacts. That's it. It can and does link to C ABI dynamic libraries just like pretty much every other compiled language. I haven't tried cross-compilation proper (yet, maybe if I keep getting annoyed at my phone) but [from what I hear it's no worse of a mess than usual.](https://github.com/japaric/rust-cross).
You guys might find pf-extend-ra interesting. I posted about it the other day. We’re about to land foreign data wrapper, attribute macro, as well: https://github.com/bluejekyll/pg-extend-rs
* SML, OCaml: algebraic data types, pattern matching, type inference, semicolon statement separation * C++: references, RAII, smart pointers, move semantics, monomorphization, memory model * ML Kit, Cyclone: region based memory management * Haskell (GHC): typeclasses, type families * Newsqueak, Alef, Limbo: channels, concurrency * Erlang: message passing, thread failure, linked thread failure, lightweight concurrency * Swift: optional bindings * Scheme: hygienic macros * C#: attributes * Ruby: block syntax * NIL, Hermes: typestate * Unicode Annex #31: identifier and pattern syntax Source: https://doc.rust-lang.org/reference/influences.html
You would have to ask Niko himself, but it's probably just a matter of him being too busy right now to continue the series. Maybe a better way to get this started would be to ask of the community in general to make more videos/series like this rather than putting things on Niko. I know there are lots of people in the Rust community who could help with something like that. :)
That's fair. I'll edit to reflect that
The website [Try It Online](https://tio.run/#rust) has Rust and standard input support. Unfortunately, it doesn’t have the top 100 crates cached, but it has been requested and might be there soon.
Hello! I don't know if this is an easy question or not, but I'm interested in some of the implementation details of how Rust handles copies and moves. How does making a type implement `Copy` avoid the "double-free error" mentioned in the ownership chapter of the Rust book? https://doc.rust-lang.org/book/ch04-01-what-is-ownership.html From what I understand, if we have the following where `b` is not `Copy`able, ``` let a = b; ``` then what happens is a new variable `a` is created on the stack that points to the same data that `b` points to on the heap, and then `b` is invalidated. I think this idea of "moving" variables is a pretty cool one for avoiding double free errors. However, from this thread (https://www.reddit.com/r/rust/comments/7smcbc/move_vs_copy_optimized_performance/dt5tej8), moving apparently doesn't zero out the original binding. If that's the case, then I'm guessing as a program runs, Rust keeps track of which variables are valid or invalid, and as they go out scope, Rust will only free the valid ones. Is my guess correct? Further, since the only difference between a "move" and a "copy" is whether or not the original binding remains valid, then how does Rust figure out which variables to free as they go out of scope?
Yep i use vscode, and it does help. But it can also be tricky to know what to need, or where to find things. Or how to glue them together.
Cross compiling with a C library was a bit iffy when I tried to get it running. That might have changed, it was a while ago. And yeah, cross compiling in general is a PITA, Needing different headers and libraries for different versions/OS/architectures as well isnt the most ergonomic usecase for C. Admittedly, C -can- do it, unlike some other languages, but still isn't fun or pretty. 
yeah that's what i've been doing. That and the docs on the website. For example the other day i was getting a compiler error. error[E0621]: explicit lifetime required in the type of `req` --&gt; src/routes/games/create.rs:45:10 | 32 | pub fn create(req: &amp;HttpRequest&lt;AppState&gt;) -&gt; FutureResponse&lt;HttpResponse&gt; { | ---------------------- help: add explicit lifetime `'static` to the type of `req`: `&amp;'static actix_web::HttpRequest&lt;app::AppState&gt;` ... 45 | .responder() | ^^^^^^^^^ lifetime `'static` required https://github.com/agmcleod/sc-predictions-server/blob/master/src/routes/games/create.rs#L45 the code for that. Because im using a HttpRequest with app state, and i want to parse the request body, im not sure if i just need to specify the lifetime, or if im not returning something from the db handler call properly.
Wait - I think I'm confusing myself. That same chapter says &gt; Rust won’t let us annotate a type with the Copy trait if the type, or any of its parts, has implemented the Drop trait. If that's the case, does that mean `Copy`able types can never be stored on the heap? In other words, only types with a known size can be `Copy`able? 
I actually think good short video tutorials for some topics are sorely lacking in the rust ecosystem! I'm hoping that starting next week I can work on some streams and videos for some rust topics like Futures/Tokio, Error handling, smaller things.
This is a bit bizarre to me, I guess I just don't understand, but `struct Big` does not implement `Copy`, yet it seems to be copied every time it is "moved"!
That's a lifetime errors it has "nothing" to do with types. You need to box it.
Struggling with `serde` again, and I can't find an answer matching my question. I want to do something like the following: trait T: Sized where Self: Deserialize + Serialize { fn read(s: &amp;str) -&gt; Result&lt;Self, Error&gt; { toml::from_slice(&amp;fs::read(s)?).map_err(...) } } #[derive(Debug, Deserialize, Serialize)] struct A&lt;T: Serialize&gt;(Vec&lt;T&gt;) #[derive(Debug, Deserialize, Serialize)] struct B&lt;T: Serialize&gt;(HashMap&lt;T&gt;) My idea here is to have a number of backing data structures for a trait-provided set of functions (e.g., `read`, `write`, `insert`). The first two require (de)serializing. This is all fine except for the bound on `Self`, because I'm requiring `Deserialize` without a lifetime. Adding the `'de` lifetime to the trait results in a error that the bytes for the deserialization don't last for the whole lifetime of `T`. I guess I have two questions then: - why don't `A` and `B` require binding `T` by `Serialize` to be able to derive it? - how can I make my trait-based approach work, rather than `impl`ing every backing struct I might create?
This is really exciting. Thanks for sharing the slides, I’ll need to watch the video later. I want to play around with this now! As an aside, you say async network programming isn’t ready on one of the slides. I’m guilty of suggesting this too, but I think it’s important to clarify that it takes a lot of boiler plate now, but is perfectly stable and ready for use (through Futures and Tokio). You do need to learn the patterns. I know you know this based on other videos of yours, but I think it’s important for others too understand that things are stable and ready, if not yet boilerplate free. Anyway, super exciting! Evmap looks great too.
Where is Rust making these copies? 
&lt;3
Not that rust can't do that but is there any chance the language you heard of is 'R'?
Are Rust copies actually shallow copies? Take for example the following Rust code: ``` #[derive(Copy, Clone)] struct A { x: u32 } fn main() { let a = A { x: 2 }; let mut b = a; b.x = 3; println!("{}", a.x); println!("{}", b.x); } ``` We create a struct `A`, assign it to `a`, and then copy it over to `b`. My understanding of a shallow copy is that if we use `b` to change some properties, the properties for `a` would change too, since both variables reference the same underlying data. However, the above Rust code prints `2` and `3`. This is unlike what happens in Java: ``` class A { int x; A(int x) { this.x = x; } } public class Test { public static void main(String[] args) { A a = new A(2); A b = a; b.x = 3; System.out.println(a.x); System.out.println(b.x); } } ``` Here changing `x` through `b` also changes it for `a`, and the above Java code prints `3` and `3`.
*Excellent talk.* What was that cargo option that forces your crate to document all the things before building? My google-foo is failing me on finding that feature to turn on.
The difference isn't really one of shallow vs. deep. `Copy` indicates that a value can be copied simply by copying its bits directly (and is implicit), whereas `Clone` may have extra operations it has to perform to successfully clone a value. In your example, when `A` is copied, its bits (the value of `x`) are copied verbatim, which is why modifying it doesn't modify the original - they are separate entities. This is in line with what a shallow copy is in other languages; it copies the top-level values but doesn't do any recursive copying. The Java example is misleading, because Java doesn't have semantics similar to Rust; saying `A b = a` is similar to Rust's `let b = &amp;a` - there's no copy of anything except maybe a pointer to the object itself.
Copy is intended to be used for things which can be duplicated "cheaply", while clone is intended to be used for things which are more expensive to duplicate. Rust cannot perform a shallow copy like you are describing, because that would break the ownership model and the borrow checker. It is up to the implementer of a given struct to decide if it should be clone, or copy, or neither. But both perform a full copy of the object.
All implementing `Copy` does is allow you to use the original binding as well as the new one. Both moving and copying copy bits in the same manner, but after moving you're prevented from accessing the old version. Not entirely sure why those moves aren't being optimized away. Maybe has something to do with the fact that the pointers are being used in the print statements?
I might be wrong on this, but I'm reasonably confident: The short answer is that `Copy` and `Clone` both do the same thing on the surface -- they provide a data type "copy semantics" instead of "move semantics". They both provide an entire new set of data for the second variable to work with. Typically how they differ in usage is that `Copy` is used for fixed-size, stack-located data that we know we can stack allocate -- `str`, number types, and so on -- where `Clone` is used for data structures that we don't always know the size of, or more importantly, may have to reallocate memory for at runtime. By comparison, Java's assignment only copies by reference -- it doesn't have to worry about how big the data for `a` is, because `b` just points back to it instead of duplicating it. The equivalent Rust would actually be let a = A { x: 2 }; let b = &amp;a; // or rather &amp;mut a, since Java is mutable by default Rust doesn't really have the same idea of shallow and deep copying as OO languages due to the memory management model. In Rust each variable is assumed that allocated value has exactly one variable owning it, and when that variable goes out of scope, the memory can be cleaned up with no issues. If Rust employed OO-style shallow copying, you could have two variables referring to the same memory, and one variable now pointing to unallocated memory when the other goes out of scope. This is really what move semantics guard against. Shallow copying works under the theory of having a garbage collector, since each variable is now no longer responsible for its own memory. Variables get dropped, the memory lives on, and when the GC notices that no one is referring to that memory any more, it's free to clean it up.
&gt;R It was! I posted here thinking R was Rust. Sorry about that!