Doesn't `while let` do option 2? Though I still agree that this probably isn't a needed feature.
It does. Are you implying that a language cannot possibly have more than one way of doing the same thing, because if so, I have an `if let` to sell you... :P
Correct. I intentionally left out those extra bits for clarity. ;-) 
Check out https://rustjobs.rs/ currently there don't seem to be openings, but that may change.
&gt; Support zero-sized types in argument lists; this is impossible in C, but possible in Rust" How did this work in Rust before if LLVM didn't support it until now? Is it because Rust uses its own LLVM fork?
I definitely agree. I expect the semantics of the `else` keyword to mean "run this if there is nothing to iterator over" / "default if nothing ran". But this is not what it means at all. `finally` would have made more sense for how it actually works (and is already a reserved keyword with a similar function)
There is not much to expand I think... I could give you an example: With today's iPhones you can either install Apple approved software from the store, or you're SOL. If you disagree with this policy and want to install software on *your* phone on your own terms, you have to jailbreak it to circumvent the OS' lockdown mechanisms. These jailbreaks mostly rely on your usual cadre of memory bugs. Buffer overflows, arbitrary code execution, etc. Now, imagine a few years from now Apple releases the iPhone 27 with a completely new iOS. Written from scratch in Rust, based on redox, whatever. There's a chance that the better memory safety will prevent people from creating further jailbreaks and thus using the hardware they bought in the manner they want to use it.
Is it confirmed to be fixed in 6.0?
&gt; For the type of application mentioned here (solving dense and sparse linear solvers, eigensolvers etc.) you don't need integer generics, because the size of your linear systems is most likely unknown at compile time. In Fortran, C, C++ or Rust you'd use dynamically allocated matrices and vectors for this task. Solving large linear systems is ~~often~~ sometimes neither the most computationally expensive part of an application, nor the part where 99% of the application logic goes to in most cases. Unless one is in the business of implementing linear solvers, solving a linear system is typically closer to a one liner: `solve(A, b, x);` (+- choosing linear solver, ordering, preconditioners, etc. but still pretty small). 99% of the logic of numeric-heavy applications goes into assembling and updating these linear systems. And while you are correct that the size of big linear systems is rarely known at compile-time, the sizes of the building blocks (matrices, vectors, etc.) from which they are assembled are almost always known at compile-time: interpolation (stencils, methods, order, ...), numerical discretizations (numerical method, spatial dimensions, stencil, order, ...), image processing (filter method, window size, ...). Eigen3 is not king among the C++ linear algebra libraries because it has data types for storing large dense and sparse dynamic matrices, vectors, and tensors, many linear solvers, preconditioners, factorization algorithms, etc. We have had this for 50 years in Fortran already, and if the only thing your program does is load some systems of equations from a file, solve it, and write out the solution, then you don't really need Eigen3. Eigen3 is king because, beyond all the features mentioned above, it recognizes that in most applications 99% of the logic goes into assembling and updating these linear systems and has excellent support for assembling and updating these large linear systems from smaller matrices. It allows view some parts of a large linear system (e.g. a strided sub-matrix) as if it were a real small matrix and because all the bounds on these views are known at compile-time it all compiles down to SIMD vector instructions. It also has excellent support for small matrix/vector/tensor/quaternion arithmetic, so you can compute the building blocks on the stack, and then assign them to a view to modifying a large matrix. And it allows the user to do all of this by writing code that looks like pen-and-paper linear algebra. And when I say efficient I mean more than mapping down what the user wrote do assembly instructions. Eigen3 works by creating a compile-time graph of a computation, and before emitting assembly, it uses **type-level integers**, **specialization**, and **meta-programming** to choose suitable SIMD types, add parallelization using OpenMP, offload the computation to an accelerator (e.g. cuda), recognize sub-expressions and emit external library calls (e.g. to the MKL), fuse or split kernels to minimize temporaries and maximize spatial and temporal memory locality, figure out what the user actually wants to do with the result and compute only what's necessary, etc. Or in other words, Eigen3 is a DSL for linear algebra, that allows you to work on arbitrary memory layouts, that comes with a built in optimization engine and a cost model for linear algebra computations and its own "C++ code generation backend", that comes with lots of linear algebra algorithms and utility classes built in (big and small, dynamic and fixed, matrices, vectors, quaternions, tensors... OpenGL, geometry, interpolation, optimization...), and that allows programmers to write code that reads like pen-and-paper linear algebra, which allows non-C++-expers (aka scientist) to write very efficient code. Comparing the current idiomatic libraries Rust libraries (ndarray, nalgebra, ...) against Eigen is not really useful nor fair because Rust does not allow them to do what's necessary to become more like Eigen. One would need to write a compiler-plugin that can access type information for this. So every time I read that "Rust enables high-level language developers to write low-level code" I actually think "Well, C++ eDSL's allow non-programmers to write high-level code that performs better than low-level code".
No, I'm just basing this on the comment I linked to. It could be that we have to wait even longer. I hope not. EDIT: To clarify, NewGVN fixes the codegen errors, but it's not 100% confirmed to be enabled in LLVM 6.0.
A file_name_ might be a string, but a Path is just what it says, a path, on most filesystems a path through a tree (well really a graph once you include symlinks). It happens to have a convenient string representation, and the value at each node is pointed to by a string identifier, but a path is definitely more structured than a string.
A few relevant posts from [Niko Matsakis'](https://github.com/nikomatsakis) excellent blog, [Baby Steps](http://smallcultfollowing.com/babysteps/), for those curious to know more: [Associated type constructors, Part 1](http://smallcultfollowing.com/babysteps/blog/2016/11/02/associated-type-constructors-part-1-basic-concepts-and-introduction/) | [2](http://smallcultfollowing.com/babysteps/blog/2016/11/03/associated-type-constructors-part-2-family-traits/) | [3](http://smallcultfollowing.com/babysteps/blog/2016/11/04/associated-type-constructors-part-3-what-higher-kinded-types-might-look-like/) | [4](http://smallcultfollowing.com/babysteps/blog/2016/11/09/associated-type-constructors-part-4-unifying-atc-and-hkt/)
I can't wait!
Not an author of this book, but co-author of TRPL. We are going to be updating it every so often. That said, this kind of thing is true of many, many languages, and people deal.
Well, Rust-the-language has one: `str`. Rust the stdlib has those other ones.
Piston is best for 2d. I would go with gfx probably. Gfx is looking to be the defacto 3d graphics engine in rust
Very excited for this to come out!
Sure, but the context of this thread is considering how we teach Rust to new users, and implementation details are a far-off concern to someone who's just starting out. Strings in Rust *are* hard, because we have two of them, which is twice what most languages have, and so a new user must learn when each one is properly used (and in the process, realize why it's a good idea for each to exist). But for the other types you mention, this same teaching problem doesn't exist: the answer to "when would I use `OsString`" is "when interacting directly with OS APIs; convert it to a `String` as soon as possible". The answer to "when would I use `CString`" is "when interacting with C APIs; convert it to a `String` as soon as possible." The answer to "should I use a string or a path for working with the filesystem" is "Rust doesn't even let you use strings to interact with the filesystem; you are forced to use paths (unless you want to reimplement the filesystem APIs from the ground-up, anyway), so there's no worry about which to use". The answer to "when would I use `String` or `Vec&lt;u8&gt;`" is "the question is ill-defined, what exactly are you trying to do? (If you've got a buffer of bytes, what encoding is it? If it's supposed to be a string from some other environment, are you sure it isn't a Vec&lt;u16&gt; or Vec&lt;u32&gt; or Vec&lt;char&gt;, or better yet would you be better off using CString or OsString, or a type from some third-party library? If you're receiving data from the network, are you sure that you're handling endianness properly (again, consider using a library!) and are you sure that this data is semantically a string (which Rust has very specific semantics for, specifically that it's UTF-8)? If you're just backing up a custom data structure, are you sure a Vec is the proper collection rather than something like a trie or a rope, and to reiterate, are you certain that this is semantically a string?)". TL;DR: Teaching strings in Rust is hard enough without having to overcome the meme that Rust has a dozen string types (especially considering that that perception ignores why we have types in the first place; would it be better if everything were just [stringly-typed](http://wiki.c2.com/?StringlyTyped)?)
Strictly speaking, the variation for a problem in P can be larger than one that is NP-complete, since the former can be (sub)polynomial to exponential to anything larger (e.g. merge sort vs bogosort) while an NP-complete problem can only be superpolynomial/exponential to anything larger. :P (But yes, I guess NP-complete problems often require more "tricks" to get faster.)
I really don't want stdx, personally. It doesn't solve any direct problem I have, but I can predict it creating problems for me, much like boost. Crates.io and it's new search features feels right to me, and also allows new libraries to gain traction. A stdx would make it harder to replace individual libraries, when necessary. It will also make it harder to quickly incorporate src based libraries to patch any issues with libraries. None of these are blockers, but they would make things a little more annoying. My two-cents... Edit: there is one area I would like an idea of common extended standards, and that would be much like the http lib for common request response types, but the impl libraries wouldn't be included. But all of those are still domain specific and don't necessarily need to be all bundled up into a stdx.
You could take a look at https://github.com/PistonDevelopers/piston-examples/blob/master/src/cube.rs and see how to use Gfx with Piston to render a cube.
The goals you are describing sound more aligned with aturon's idea from about a year ago of distributing some blessed crates -- I don't remember the name of it but iirc there was a blog post. The proposal didn't pan out but he still thought there were important ends which could be served by something along those lines. I see stdx as more of an "awesome list".
So, I wasn't speaking about ergonomics here, but about performance. You are right that a lot of the logic goes into the matrix assembly part in a numeric application, but I think that in many cases, most of the run-time is still spent in the solver/preconditioner part, that's why I was focusing on that. I also like Eigen and I'm not suggesting that you can't make good use of type-level integers, expression templates etc. for the assembly part, but there has to be a way to write something with an ergonomic interface, which is also fast enough, without all these language-level features. There are people using Python + NumPy, SciPy for scientific computing, right? &gt; perform the computation (e.g. if you solve a linear system but only read some elements from the result you might not need to solve the whole system), etc. Does Eigen automatically analyze your matrix to understand that it can be reordered to a block-diagonal form? Does it do that at compile time? That's neat! Anyway, I may agree with you that maybe Rust isn't the best choice for scientific computing (right now?). I think that all the features related to memory safety and safe concurrency are a bit less important in a scientific computing setting, where most of the complexity lies in the algorithms, not in the program flow. For instance you have more cases of simple fork-join parallelism, than cases of multiple communicating threads doing completely different things at the same time. My reaction was due to the fact that in many discussions a specific missing language feature is cited early on, and I wonder if this doesn't discourage many from even trying to make something.
Are you interested in small jobs, such as updating libraries, improving docs, implementing some features and testing performance in the Piston project? I am thinking about hiring somebody get all these small things done to improve the quality of the codebase.
The cookbook seems like the new form of stdx, yes.
Until now associated types were decided only by the implementing type. That is, something like pub trait Iterator { type Item; could be seen as a function over types: fn Item(instance: Type) -&gt; Type This rfc means that the type functions can take additional arguments: fn Item(instance: Type, itemLifetime: Lifetime) -&gt; Type Or with real syntax: trait StreamingIterator { type Item&lt;'a&gt;; Full higher kinded types would also allow traits for generic types, for instance: trait Mappable { fn map&lt;A, B, F&gt;(self&lt;A&gt;, f: F) -&gt; Self&lt;B&gt; where F: FnOnce(A) -&gt; B, which could be implemented for future, result, option and so on.
I googled that message and found that it happens when `read_exact` gets an "EOF" before it can fill the entire buffer that it's passed. It may just be that you're getting disconnected mid-conversation. Are you the one calling read_exact? Are you sure that it's the appropriate method to call? and can you verify that your connections aren't randomly getting disconnected?
Maybe I'm being dense, but what do you mean by "HRTB"?
I'm sorry, but could you explain this a bit more for me? Where is an assignment happening? I've been going through The Book and so far everything has been perfectly simple and straightforward, but I find this "if let" syntax really strange and confusing. I understand how to use it, but I don't like the way the syntax looks and don't see how there's any assignment. let some_u8_value = Some(0u8); if let Some(3) = some_u8_value { println!("three"); } The first line has an assignment: `let new_variable = value`. But the second one is: `if let value = existing_variable`. Firstly it seems backwards for an assignment, but secondly the only variable mentioned is an existing one.
Could someone clear up this `if let` syntax for me, and explain what's going on and why it's written the way it is? Where is an assignment happening? I've been going through The Book and so far everything has been perfectly simple and straightforward, but I find this "if let" syntax really strange and confusing. I understand how to use it, but I don't like the way the syntax looks and don't see how there's any assignment. let some_u8_value = Some(0u8); if let Some(3) = some_u8_value { println!("three"); } The first line has an assignment: `let new_variable = value`. But the second one is: `if let value = existing_variable`. Firstly it seems backwards for an assignment, but secondly the only variable mentioned is an existing one. I think this would be a lot clearer and more readable if it was something like `if match Some(3) in some_u8_value` instead...
Ah, I see. Thought it did, but might have been a while back.
“Higher Rank Trait Bound”; it’s a standard name in Rust for the `for&lt;...&gt;` syntax he was using.
haha &lt;3
It's more like ATC subsume HKT. [Nico](http://smallcultfollowing.com/babysteps/blog/2016/11/02/associated-type-constructors-part-1-basic-concepts-and-introduction/) had a wonderful set of blog posts about all these if you want to know more about all this.
I find that the if let syntax makes a lot more sense when you consider that in fact, `let` performs pattern matching too. For example: let (fst, snd) = (19, 84); This may look like some kind of multiple-assignment, but it is, in fact, pattern matching! The bindings `fst` and `snd` are being matched to the 19 and 84 inside the two-element tuple on the right of the =. One limitation of this pattern matching is that the pattern must be irrefutable, in other words, the thing on the right-hand-side of the = must always fit the pattern on the left. So this is not allowed: let Some(number) = an_option_of_u8; Because `an_option_of_u8` might be a `None`, `Some(number)` is a refutable pattern. This is where if let comes in, unlike let, if let will allow refutable patterns, and in the case that the pattern doesn't match, simply provide an else branch to deal with that case. if let Some(number) = an_option_of_u8 { "Yeah!" } else { "Bummer!" }
I also just don't like linking in libraries I won't use. I think initiatives like the [Rust Cookbook](https://github.com/rust-lang-nursery/rust-cookbook) are a better thing than stdx when figuring out generally accepted community crates when new.
Did you reply to the wrong comment? I know how GAT works. Most of it is already possible (ContainerFamily, PointerFamily, LockFamily, ...), but at the moment it's quite inconvenient to use. However burntsushi showed that regarding lifetimes there actually are cases that truly aren't possible today. Your Mappable trait is already possible today btw.
stdx isn't that. It covers a bunch of crates used for different things that the stdlib doesn't cover, like http requests. Even if there was another library for clis diversity in crates is not a bad thing! :D Look at all the different web framework and parsing crates that exist. You get to choose what fits your needs best.
I think that the [Rust Cookbook](https://rust-lang-nursery.github.io/rust-cookbook/) and [Libz Blitz](https://blog.rust-lang.org/2017/05/05/libz-blitz.html) mostly replaced what `stdx` was trying to do. The point of `stdx` was to pick out a set of high-quality, frequently used libraries that could make up a core platform that is broader than that provided by `std`. But just picking those libraries and making a package that depended on all of them didn't really help that much; it means that if you pull that in as a dependency you're pulling in way more than what you need, and it also means that `stdx` has to choose when to update libraries and potentially introduce breaking changes. Really, it's better for developers to just depend on the specific packages they need. `stdx` also documented some basic usage examples; but that's also exactly what the Cookbook does, and it's being maintained via a community process. The [Libz Blitz](https://blog.rust-lang.org/2017/05/05/libz-blitz.html) has picked out a number of libraries that have potential for forming a sort of standard, base platform, and working to evaluate them against some consistent guidelines, and improve them to follow best practices, so there's a sense of consistency and coherence. I think it's been doing a really good job of that. As part of that effort, these libraries are also being documented in the [Cookbook](https://rust-lang-nursery.github.io/rust-cookbook/), which makes it a lot easier for people to find what they need and learn about how to use it. The [Rust Playground](https://play.rust-lang.org/) has [recently enabled use of crates from the Libz Blitz and Cookbook](https://github.com/integer32llc/rust-playground/pull/198), in addition to the top 100 crates by download that it initially supported. So these have become that list of "smoothly interacting and well tested crates" that tools are rallying around. Basically, I think that `stdx` was a good idea, but doing it as a crate with dependencies didn't really work out, and that the Libz Blitz and Cookbook are pretty much doing exactly what it intended to do. The Cookbook will obviously continue; not sure if the Libz Blitz will turn into a Libz Slog after it ends, or be continued in some way. I think that the format has the potential to be a standard part of the community process, though perhaps with a slightly more relaxed schedule once the initial Blitz is over.
http://aturon.github.io/blog/2016/07/27/rust-platform/
Yeah, wondering if someone can get in touch with /u/brson to update `stdx` to maybe refer to the Cookbook as it is currently maintained while `stdx` is not (unless he hands over maintenance to someone else).
No, that's not true. The primary point of epochs is to give coherent ways to group together changes. Epochs would still be useful without the ability to break things. In fact a lot of the inspiration is from what C++ does, except it's designing it before we need it so we don't paint ourselves into a corner. Similarly, would-be breaking changes would still happen very similarly without epochs. The difference would be that with epochs you have a step after deprecation where you remove some stuff in the next epoch, whereas without epochs you have to stick to deprecation. But this affects learning materials equally, since learning materials don't want to teach deprecated stuff. Epochs do help group functionality for learning materials. Rust hasn't broken anything since 1.0 but it's already a pretty different language.
By default, Rust is built using the [rust-lang/llvm](https://github.com/rust-lang/llvm/) fork, but it supports "vanilla" LLVM too with some reduced functionality.
I'd also like to point out that the lib blitz specifically focuses on at least 12 of those on the stdx list. 
I don't know, I'm skeptical. I see no reason why a CFG could have any impact whatsoever on the rest of the process, or why it would help with any dependency information. You can produce the same ASTs either way, can't you?
Maybe I'm dumb but I don't see it.
Yep, there's a pretty good amount of agreement among the community on many of the "best crates to solve a particular task". So the real trick is to find a way to make it easy to find that list, figure out how to use them, and make sure they are all consistent and work well together. The Libz Blitz helps with the last point, and the Cookbook the first two.
This sounds like you're essentially using the Strategy pattern for the algorithms you're using. My question is, why not just use functions? Why does your client code need structs, as opposed to knowing the name of the function they want?
[Check the release mode assembly](https://imgur.com/LiT36Gv)
Switch to release mode, click the LLVM IR button and there's this line of code: store i64 3628800, i64* %_10, align 8 
Is there an accompanying implementation branch for the compiler yet? It would be an early Christmas for me if it at least landed in Nightly.
I suppose so, as long as you figure out all the features (gcc has a flag to do that).
Thank you very much for explaining Eigen far more elegantly than I did in my post. Basically, writing Eigen code feels almost like writing MATLAB code, but in C++. And often the result runs very fast right out of the box. I am a mathematician and do mainly scientific computing, so this is very convenient. MATLAB and numpy are widely used in the community, but they aren't performant enough for some applications, so C++ is used there. &gt; Solving large linear systems is often neither the most computationally expensive part of an application, nor the part where 99% of the application logic goes to. Unless one is in the business of implementing linear solvers, solving a linear system is typically closer to a one liner: solve(A, b, x);. This might be an oversimplification. I often need more fine-grained control over the sparse solver, such as doing the symbolic decompositions, the actual factorization, and the back-substitution separately (see, for example, the methods on [this Eigen solver](https://eigen.tuxfamily.org/dox-devel/classEigen_1_1SimplicialLDLT.html) ). You are correct that 99% of my application logic is not the sparse solver, but the construction of a linear system and the processing of its output (which is used to construct new linear systems, etc.). This is not true for computation time: in my case it is definitely the most computationally expensive part. Choosing the correct solver for the job often means applications much faster than they did before.
It's honestly insane how far LLVM and Rust can go with some compile-time evaluations. In fact, for some optimization tests, I've had to purposefully use a "black box" function to force it NOT to evaluate expressions at compile time.
Thank you for your reply. Are there good ways to interface with C++ libraries from Rust that don't just go via a C API? Basically, to use C++ features of these libraries? For example, to use libraries that are object-oriented (like [Ipopt](https://projects.coin-or.org/Ipopt) ), ore use libraries that make heavy use of templates like Eigen. More generally speaking, what kind of scientific computing infrastructure is still missing in Rust? That might be a good place for me to start coding.
Hmm... But in `let (fst, snd) = (19, 84);`, "fst" and "snd" are labels for variables, and "19" and "84" are values that can be assigned to those variables. In the case of `if let Some(3) = some_u8_value`, "Some(3)" is just a value and "some_u8_value" is a variable containing another value. So it's different... I suppose it makes sense in your example, where you have `if let Some(number)` - the difference there being the "number" part that I suppose is a variable you can use within the following scope? But without that "number" part, when it's just `if let Some(3)` as in The Book's example, the syntax seems very strange to me.
I realised that one machine was running OSX 10.12.5 and the other 10.12.6, but after updating I still get the same error https://i.imgur.com/5GRiAxI.png. I'm going to put together a minimal script to see what's happening :s
&gt; Similarly, would-be breaking changes would still happen very similarly without epochs They wouldn't be happening without epochs. There have been very very few breaking changes in Rust before the epochs RFC. Most were using the "bugfix" rule of the stability promise, and even then they were phased in over several releases. The only "breaking change" I can think of was the RFC to deprecate unnamed arguments in function declarations in traits, but even here it only added an allow by default lint to the compiler, so nothing much changed for rustc. Now compare the pretty low number of breaking changes before (1: RFC 1685) to the large number of breaking changes after (2: RFC 2113, RFC 2126) the proposal of the epochs RFC. &gt; But this affects learning materials equally, since learning materials don't want to teach deprecated stuff. That's true for learning materials that want to be "perfect", where the author updates every single code example to use the newest and shiniest way to solve the issue, but it does not hold for learning materials that are not perfect, which is I think most materials out there, which would be updated only very very rarely normally, but where some user reporting "this doesn't compile wtf" would trigger a change.
That's good feedback! I'm mostly thinking of teams with massive codebases that need to know: - the licenses of their dependencies - that all the dependencies play nicely together - that there are not security vulnerabilities - that there are proven solutions for shipping on multiple architectures A meta-crate could achieve a large number of these goals. But I see your point -- outside of this use case this might be less important than I thought
Lost track of this thread. I'm using Rust because it's the language I'm most productive in.
This is exactly the kind of feedback I wanted, thanks! I think this makes a lot of sense and is a better path for rust to take, especially at this stage in it's development. Eventually rust may want to ship some kind of pre-compiled and verified rock-solid meta-crate. But that day is probably not today -- the lib blitz solves more of the current issues. Thanks!
Is any one of the two you mentioned, nalgebra and rulinalg, considered standard? I.e., if I write a scientific computing application that is based on a standard linear algebra interface, which one will users of my application be more happy to have?
If you're talking about [#364](https://github.com/SergioBenitez/Rocket/pull/364), that has been closed "in hopes of a better solution". So there still seems to be a need for help.
&gt; It's honestly insane how far LLVM and Rust can go with some compile-time evaluations. It's actually a fairly easy optimization to perform once you can evaluate the purity of an expression (which, depending on the language, can be quite difficult).
Not that I am aware of, though eddyb is working on some of the stuff that is needed to implement it: https://github.com/rust-lang/rust/pull/44275
That's true, but I still would like the more specific loops to support return values as well.
The i686 target actually supports SSE2 as well as SSE (or at least, it does for my installation of rustc).
&gt; They wouldn't be happening without epochs. No. For example, look at the latest modules RFC. It proposes major changes and deprecations, however there is only one final breaking-change step (turning deprecations into errors), which i think even the RFC notes that we don't have to go through with (but if we did, we'd use epochs for it). Even if the epochs RFC is rejected, this RFC can still land. The epochs RFC makes this RFC _better_, but is not _necessary_. &gt; Now compare the pretty low number of breaking changes before (1: RFC 1685) to the large number of breaking changes after (2: RFC 2113, RFC 2126) the proposal of the epochs RFC. More accurate to say that the desire of the team to make these changes helped spur on the need for epochs (I was around during the initial discussions of epochs, both a path reform and dyn trait were cited as examples of things the team wanted to do that may need epochs. But epochs were also being done because we didn't want to make the same mistakes that C++ made with their model and wanted to design something early). So you've got the chronology backwards there. &gt; That's true for learning materials that want to be "perfect", Learning materials need to teach what is ergonomic, too, otherwise it just leads to confused mental models as folks learn completely different models from the materials and in practice code out there. "not compiling" is a major problem, but so is confusing people. 
I don't think any crate is established enough to be called out as standard. Nalgebra is probably more popular. There are also other crates specialized to low dimensions (4 and less) that I did not mention. Note that such a standard does not really exist in C++ either.
If you consider a for loop to be structured like this (simplified): loop { if let Some(pattern) = iterator.next() { // loop body } else { break } } then it's (IMO) not a long way to this: loop { if let Some(pattern) = iterator.next() { // loop body } else { break {/* else block */} } } This has the advantage that you can not only use the return value of the loop (this doesn't apply in Python obviously), but you can also write code that runs if the loop *didn't* end with a `break`, which is extremely useful in some circumstances and would require you to do the full desugaring (as above) to encode without an `else` clause.
If you use gfx stick to 0.16 for now. They're redoing their API as we speak. The nice thing about gfx is that it's easy to switch between low level libraries (gfx supports metal, vulkan, and opengl with the same high level API). glium is not a bad choice if you need opengl and don't want to use gfx (but, the author of glium has stepped away from the project to focus on other things). If you want to understand how the code you write in gfx or glium works, I recommend these sites: * for opengl (such as glium/gfx): https://learnopengl.com/ * for vulkan (such as with Ash/Vulkano/gfx): https://vulkan-tutorial.com/ In either case you'll need to interact with your operating system to get a handle to the underlying device and a window to render in. I think both the links above recommend using GLFW. I don't know if there is a rust binding for that, but I would use glutin for opengl and something like winit for vulkan (you can also use SDL but more setup is required). You can find examples of how to do this by looking at the example code in whatever Rust library you choose for rendering. I hope that helps.
&gt; without all these language-level features. There are people using Python + NumPy, SciPy for scientific computing, right? Python meta-programming is way more powerful than C++'s and Rust's. Python doesn't have these features (specialization, type level integers, or even traits) because it doesn't need them, the problems they solve don't exist in Python. &gt; Does Eigen automatically analyze your matrix to understand that it can be reordered to a block-diagonal form? Does it do that at compile time? Eigen does not mutate matrices for you if you don't tell it to. If you attempt e.g. a linear solve it will test the matrix properties before to choose an appropriate linear solver and an appropriate pre-conditioner, but if it decides to reorder the matrix it will need to re-order it back afterwards (I don't think it does this). Choosing the linear solver, preconditioner, matrix order, ... (or re-ordering algorithm after assembly) are typically done by the user.
I've used the [bindgen](https://crates.io/crates/bindgen) crate to automatically generate bindings for a C library. I haven't tried it for a C++ library, but I think the Mozilla folks are using this crate to bridge the C++ and Rust parts of their codebase. If I remember correctly, the painless generation of C++ bindings is a high-enough priority for them. Of course, automatically binding all of C++ is very hard. On the documentation page they [list the currently unsupported features of C++](https://rust-lang-nursery.github.io/rust-bindgen/cpp.html). I'm not doing any scientific computing right now, so maybe other people in the community can be more precise. Based on what I was using when I was doing scientific computing, I think Suitesparse (UMFPACK) or Pardiso bindings would be nice, also bindings for a subset of Eigen would be really neat. I see there are no crates for these things. On the distributed memory parallelism side, I see that there is a crate for MPI (I haven't tried it!). A nice thing to have would be a Rust interface to Trilinos or PETSc, then one could start running Rust applications on clusters and supercomputers. As a side note, Rust on the supercomputer is easier to sell to admins, since you don't need a specific runtime library on the nodes...
The associated type constructors are part of a larger feature known as higher kinded types. At least, that's my understanding.
I'm pretty sure it's the same as `let` (except you don't directly write the expression it should be matched on, because it will be generated by the iterator).
Why can't str be implemented in the standard library?
But if it's in the LLVM IR, that means it's Rust that's computing the factorial at compile time - not LLVM.
https://godbolt.org/g/9zbdp6 There are even more impressive examples. For specific loop patterns, LLVM is able to get rid of the loop itself. Its capability seems pretty limited though, as this optimization is based on a sort of pattern matching rather than full algebraic understanding (for example, it is not aware of optimization opportunities across function boundaries yet).
Is the IR displayed before or after LLVM optimizations? LLVM IR exists at both times.
Thanks for your reply. Based on that, I think I will go with Nalgebra. &gt; Note that such a standard does not really exist in C++ either. While that might be true, I don't think anybody ever complains for offering an interface in Eigen for scientific computing libraries. Many people use Eigen, and if they don't, it is possible to create Eigen types using the standard C-style dense and sparse matrix representations with almost no overhead.
&gt; Eigen does not mutate matrices for you if you don't tell it to. If you attempt e.g. a linear solve it will test the matrix properties before to choose an appropriate linear solver and an appropriate pre-conditioner, but if it decides to reorder the matrix it will need to re-order it back afterwards (I don't think it does this). Choosing the linear solver, preconditioner, matrix order, ... (or re-ordering algorithm after assembly) are typically done by the user. Ok, it's neat to have this builtin login and the option to change it. What I was trying to understand, because you mentioned lazy computations, was how the linear system can be partially solved if you only need a subset of the solution.
It's worth noting that let doesn't necessarily do an assignment to a variable. For example, this is valid: let () = (); This works because let accepts irrefutable patterns, and `()` is an irrefutable pattern for a value of type `()`. In the case of if let, `Some(3)` is just another pattern. Yes, there is no assignment, but even let doesn't have to have an assignment, as noted above. Although usually, you _do_ assign values in if let expressions, otherwise the code can just be written as `if some_u8_value == Some(3) { ... }` You are correct that with `if let Some(number)`, the value is bound to the `number` variable and can be used within the block.
Ok. While I am probably not proficient enough to create a Rust interface for clusters and supercomputers, bindings for sparse solvers seem like a good place to start.
I was thinking of it from a different angle. Imagine you want to port a simple Python script. You will have have to decide how to translate your Python strings to the appropriate data types in Rust. It is quite possible that you will have to deal with all the types I mentioned, because they are all used where you would just use a string in Python.
Are you certain that the LLVM IR displayed is from *before* the optimization passes, rather than after? From what I remember, release mode doesn't do much before the translation to LLVM IR aside from omitting things like the overflow/underflow checks. That's why "rustc throws horribly un-optimal IR at LLVM and trusts the optimizers to clean it up" is a well-known aspect of Rust's wall-clock compile time issues and debug-mode performance problems. (And one of the things MIR is designed to enable solutions for.) (And, personally, I think it would be more useful to show the LLVM IR after optimization as a way to investigate optimizer behaviour on your program without having to learn full-blown assembly language for one or more arches.)
Will GAT unblock your streaming iterator designs or is there more to do as well?
I know, but sometimes it requires special build flags -- it would be nice if this was pre-packaged in some way.
&gt; This is not true for computation time: in my case it is definitely the most computationally expensive part. Choosing the correct solver for the job often means applications much faster than they did before. Yes you are right, whether this is the case heavily depends on the application, it is easy to construct examples of both. 
Definitely. I fully support people taking breaks from open source. Thing is, it's very nice to leave maintainers if you can!
Getting the word out as it can be useful to some people
It could have been, but we decided it wasn't worth it: https://github.com/rust-lang/rust/pull/19612
&gt; What I was trying to understand, because you mentioned lazy computations, was how the linear system can be partially solved if you only need a subset of the solution. So in general you cannot do this, but Eigen has many types of matrices, and some of them are special (diagonal, upper/lower triangular, etc.). So if you write `float x_3 = solve(A, b)[3];` and `A` is special, then depending on how special it is Eigen might not need to solve the whole system (e.g. if `A` is diagonal the computation is trivial).
And then there are some examples of very poor code generation... https://play.rust-lang.org/?gist=51f82a7db7a25f36230820693d41ac65&amp;version=stable
After. Rust can't print out LLVM, it just calls LLVM-FFI (or the Rust-dev curated LLVM-FFI). Then asks the LLVM to print its IR after compile. This is how `rustc --emit=llvm-ir` is implemented. Rust can pass gigabytes of data to the LLVM before optimizations. 
Ok, that makes sense. Thanks for the clarification!
Thanks yeah it works and runs the binary code. The only issue is that the code i had was written for the cdecl calling convention which rust apparently doesn't use so Its returning a value of 21940 instead of 25. Do you know what caling convention rust uses? i can probably figure out how to make it work with LLDB if not. I'm actually not sure if I'm even going to need the calling convention specifics once I start running emulated mips code. but it could be useful for loading register values. 
I would definitely give the bindgen crate a try. I was apprehensive before using it, but it's quite nice. The documentation has a guide to get you started: https://rust-lang-nursery.github.io/rust-bindgen/ Of course, this crate automates the process of making low-level bindings to C or C++ libraries. Making the higher level, Rust-idiomatic interface is something you'll still need to do.
Gfx is not a 3D engine. Yes, you can use it to render 3D but you can also use it for 2D or something else entirely. It doesn't have have any features specially for 3D rendering.
&gt; For example, look at the latest modules RFC. It proposes major changes and deprecations, however there is only one final breaking-change step Its still being discussed, there might be more epoch involvement like e.g. adding a `local` prefix for crate-local, which would obviously be a breaking change (as you could have a local module in the root) But yeah the current form of the RFC is very minimal on actual hard breakages. &gt; The epochs RFC makes this RFC better, but is not necessary. A breaking change is *definitely* neccessary if you want to attain the actual goal of the RFC which is simplification. If you just add some new model and let the old model in place, you force people to learn both models as they might encounter both. This is what C++ is doing and the reason why its getting fatter and fatter. &gt; So you've got the chronology backwards there. I wasn't around so I couldn't tell. Interesting that the motivation is the other way round. Still, the actual RFC proposals came in after the epochs RFC. Also what you say proves my point: if dyn trait and a path reform are being used as reason for epochs, then the team really didn't want to do them *without* epochs or a mechanism like that, so epochs were needed.
To get it using cdecl, you probably just need to treat it as an extern function. So, instead of `fn(u64) -&gt; u64` use `extern fn(u64) -&gt; u64`. Then Rust will use the cdecl, although you might have to use `unsafe` to call the function pointer then since it is no longer considered a Rust function. Rust's calling convention hasn't yet been defined, so if you need a calling convention that won't change, you have to use the C calling convention, from what I understand.
No, simplification is conceptual simplification; deprecating stuff is sufficient. In the same vein, the modules thing is something they wanted to do anyway via deprecation, but epochs would make it a much smoother experience.
Is there anyway to control this? (Are there ways in compilers for other languages?) I find myself thinking all the time about a hypothetical knob that you could use to adjust how much of a (deterministic) function should be evaluated at compile time and at run time, respectively.
I get what you mean, that's a strange use of `if let` IMO. If you're not assigning anything I'd use a regular if: `if some_u8_value == Some(3)`.
&gt; In the Apache web server, a number of modules can be used to limit the damage caused by the Slowloris attack; the Apache modules mod_limitipconn, mod_qos, mod_evasive, mod security, mod_noloris, and mod_antiloris have all been suggested as means of reducing the likelihood of a successful Slowloris attack.[1][5] Since Apache 2.2.15, Apache ships the module mod_reqtimeout as the official solution supported by the developers.[6] Async hyper, like nginx, will hopefully be inherently pretty resilient to this attack because it doesn't block any threads. But if you're using the currently very common rocket framework, you're using blocking hyper, which I suspect is extremely susceptible to HTTP slowdown attacks like slowloris, and AFAIK has not done much to mitigate it.
`write!` doesn't add escapes for characters, it only does string formatting with `{}`. You can avoid that also by using [`write_all`](https://doc.rust-lang.org/stable/std/io/trait.Write.html#method.write_all). So `toml`s serialization is escaping the string, which is probably what you'd want most of the time. I'd open an issue on the toml repo about this.
FWIW we switched from rdkafka to https://github.com/spicavigo/kafka-rust
I wrote my own implementation of `printf` in Rust a while ago as a way to teach myself parts of the language. LLVM compiled almost every constant format string I tested directly into a sequence of constant single-character writes to stdout, even for very complicated formats. The amount and complexity of code it had to evaluate at compile time to pull that off was huge.
It's rough, but good enough if you just want a look at the generated code for something in a simplified test case - in C, I'll often put up "boundaries" to this sort of optimization by declaring variables `volatile`, forcing the compiler to generate code that actually loads them from memory. [`std::pointer::read_volatile`](https://doc.rust-lang.org/std/ptr/fn.read_volatile.html) should give the same effect in Rust, though I don't think I've tested. `__attribute__((noinline))` (in Rust, `#[inline(never)]` IIRC) has a similar effect on entire functions.
Yeah my bad, you're right. Basically I just meant to say that if you want to do 3d then gfx is the best way to go.
No, rustc's `--emit llvm-ir` mode is post optimizations. Rust doesn't do many optimizations on its own so for all intents and purposes the debug LLVM IR output is what LLVM is fed.
Not really, [man rustc](https://www.mankier.com/1/rustc#opt-level) --opt-level allows you to set optimization levels and llvm passes but that's about it. Nothing comparable to dozens of switches that [gcc](https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html) has. IIRC there are some code level defines such as ``#[inline(never)]`` that allow to set this (for a list see this [bug](https://github.com/rust-lang/rust/issues/6444) or [#attributes documentation](https://static.rust-lang.org/doc/master/reference.html#attributes)).
Isn't that just the price of porting to a language with types, though? Anyone porting a Python script to Rust is also going to have to decide e.g. which of our eight integral types will suffice for their numbers, but here there's a simple guideline: "to represent a number from your Python program, use `i32` until you need something specific" (yes I know Python numbers are transparent bigints; the vast majority of the time, `i32` will suffice as a first pass :P ). The guideline for strings is a bit less straightforward because there are some places where we force you to use `&amp;str`, but it really does boil down to "to represent a string from your Python program, use `String` until you need something specific". Porting path-handling code isn't comparable because [Python is fucked when it comes to paths](http://lucumr.pocoo.org/2014/5/12/everything-about-unicode/); because Python treats files as strings, you cannot open files whose names aren't valid Unicode, even when those are valid identifiers within the filesystem (and one quickly realizes that edge cases like this is exactly what `OsString` in Rust is designed to be used for :P ). Overall I agree that it's a lot to take in for someone just looking to port a Python script, though that's not because of `Path` and `OsString`, it's because of static types in general combined with a lack of high-level veneers like bigints and implicitly copyable strings. If the argument is that we can teach it better then sure, I'm on board, but I don't see how we can reduce what we have while remaining a strongly-typed systems-level language.
The answer to the "largest" exercise is this: fn largest&lt;T: PartialOrd&gt;(list: &amp;[T]) -&gt; &amp;T { let mut largest = &amp;list[0]; for item in list.iter() { if item &gt; largest { largest = item; } } largest } The important thing here is that the type of the local variable "largest" has changed from T to &amp;T, notice the `let mut largest = &amp;list[0];` line at the start of the function body. As for the "cacher" struct, what sort of errors are you getting?
~~Why is `parking_lot::deadlock::check_deadlock` unsafe? What are the invariants that the caller must uphold?~~ ~~Is it possible to make a safe wrapper for it?~~ edit: [it's fixed](https://github.com/Amanieu/parking_lot/pull/41)!
Although it's in early development, I'd recommend giving [`three`](https://github.com/three-rs/three) a try.
I would prefer to have stdx as a curated list of crates with great community traction and minimal overlap. Inclusion or removal from stdx would be an useful signal to help evaluate similar crates. To directly depend on a `stdx` crate on Cargo.toml is something that... doesn't really makes sense for me.
thanks, yeah i see what your saying now
I would say the "rust way" to do something like this is with traits. Abstract out the common interface of the algorithm into a trait. Then implement the trait for some unit structs (or maybe they contain some data too). Then when you instantiate the `uniprocessor` you do so with a type argument. Everything then gets monomorphised at compile time - zero runtime cost. I put an example on the playground of the kind of thing I mean: https://play.rust-lang.org/?gist=31a6aee6afd721ab7d02175a438a96e6&amp;version=stable
You're right, there doesn't *need* to be an assignment. But having a value on the left side is only a minority of cases where `if let` is used. Where it really shines is for assigning new variables to inner parts like: if let Some(x) = some_u8_value { } Here, `x` is assigned the "inner" part of the Some, if `some_u8_value` is not None. Basically, if you know how `match` works you also know how `if let` works: if let pat = expr { ... } else { ... } // is the same as match expr { pat =&gt; { ... }, _ =&gt; { ... } }
Tbh I thought this was gonna be like the constexpr abuse
I couldn't find an issue for this, but I'm pretty sure somebody knows about it. If I enable all of those the text looks terrible because of the subpixel rendering. And after a couple of seconds it seems to stop: WEBRENDER: unavailable by runtime: GPU Process is disabled ADVANCED_LAYERS: unavailable by runtime: Advanced-layers requires a D3D11 device My graphics card should support DirectX 12. I can file an issue if you feel that it's worth it.
I used to think this was a good idea but I've changed my mind, although for a different reason than other commenters here (based on a quick skim so I could've missed something). Basically in every position in the language, it should be clear whether a refutable or irrefutable pattern is expected (or, as in the case of `match`, a collection of patterns which are *together* irrefutable), and providing the other kind should result in a warning or error. Doing otherwise opens the possibility that you can write something thinking it's one kind, but it's actually the other, and it compiles fine, but at runtime does something different from what you intended. Even worse is if e.g. you write a `for` loop with an irrefutable pattern, but one of the types in the pattern gets extended with a new variant or some similar change, and instead of the typechecker yelling at you about it, your `for` loop silently changes behavior.
how does it trip up on *this*?
&gt; Although usually, you _do_ assign values in if let expressions, otherwise the code can just be written as `if some_u8_value == Some(3) { ... }` This only works because integers implement `PartialEq`. `if let Some(Value) = x` works for all types whose internals are visible, without making use of `PartialEq`. In fact, I've implemented `PartialEq` using `if let` statements once or twice. So in that sense, it's a more "primitive" construct than the `==` operator. 
Text being terrible is well-known. Lots of platform-specific stuff to dig through to get great text. Please do file a bug against the `Graphics: WebRender` component for the GPU process getting disabled: https://bugzilla.mozilla.org/enter_bug.cgi?product=Core
It's a bit fragile, but some simple vec allocations can be removed (it's really using `box &lt;fixed size array&gt;` behind the scenes here). So it can constant evaluate an expression like `vec![x, x, x, x, x].iter().sum()` for constant `x`. [playground link](https://play.rust-lang.org/?gist=d3785f4d5e703cb727ff95f8b77ee4c0&amp;version=nightly)
What's the most idiomatic way to turn `Option&lt;T&gt;` into `T&amp;`? If I use `Option&lt;T&gt;.unwrap()` it moves the value out and I don't really want to do that. I'm writing tests so have already verified that that the option is `Some`.
I'm generally wary of filing issues for "obvious" bugs. WebRender is clearly under heavy development and I don't want to waste the time of the person triaging issues. I've managed to report quite a few duplicates even after using the search (:.
This bothers me too. I have a very hard time parsing `if let`.
The `for` loop desugaring is actually very involved. First, the `range` struct has an impl of `Iterator` [here](https://doc.rust-lang.org/src/core/iter/range.rs.html#211-245). fn next(&amp;mut self) -&gt; Option&lt;A&gt; { if self.start &lt; self.end { let mut n = self.start.add_one(); mem::swap(&amp;mut n, &amp;mut self.start); Some(n) } else { None } } You can see though that this in turn calls `mem::swap` whose (inner) implementation is [here](https://doc.rust-lang.org/src/core/ptr.rs.html#149-154). #[inline] #[unstable(feature = "swap_nonoverlapping", issue = "42818")] pub unsafe fn swap_nonoverlapping&lt;T&gt;(x: *mut T, y: *mut T, count: usize) { let x = x as *mut u8; let y = y as *mut u8; let len = mem::size_of::&lt;T&gt;() * count; swap_nonoverlapping_bytes(x, y, len) } #[inline] unsafe fn swap_nonoverlapping_bytes(x: *mut u8, y: *mut u8, len: usize) { // The approach here is to utilize simd to swap x &amp; y efficiently. Testing reveals // that swapping either 32 bytes or 64 bytes at a time is most efficient for intel // Haswell E processors. LLVM is more able to optimize if we give a struct a // #[repr(simd)], even if we don't actually use this struct directly. // // FIXME repr(simd) broken on emscripten and redox // It's also broken on big-endian powerpc64 and s390x. #42778 #[cfg_attr(not(any(target_os = "emscripten", target_os = "redox", target_endian = "big")), repr(simd))] struct Block(u64, u64, u64, u64); struct UnalignedBlock(u64, u64, u64, u64); let block_size = mem::size_of::&lt;Block&gt;(); // Loop through x &amp; y, copying them `Block` at a time // The optimizer should unroll the loop fully for most types // N.B. We can't use a for loop as the `range` impl calls `mem::swap` recursively let mut i = 0; while i + block_size &lt;= len { // Create some uninitialized memory as scratch space // Declaring `t` here avoids aligning the stack when this loop is unused let mut t: Block = mem::uninitialized(); let t = &amp;mut t as *mut _ as *mut u8; let x = x.offset(i as isize); let y = y.offset(i as isize); // Swap a block of bytes of x &amp; y, using t as a temporary buffer // This should be optimized into efficient SIMD operations where available copy_nonoverlapping(x, t, block_size); copy_nonoverlapping(y, x, block_size); copy_nonoverlapping(t, y, block_size); i += block_size; } if i &lt; len { // Swap any remaining bytes let mut t: UnalignedBlock = mem::uninitialized(); let rem = len - i; let t = &amp;mut t as *mut _ as *mut u8; let x = x.offset(i as isize); let y = y.offset(i as isize); copy_nonoverlapping(x, t, rem); copy_nonoverlapping(y, x, rem); copy_nonoverlapping(t, y, rem); } } Then, the `for` loop desugaring turns this iterator impl into a loop which calls `next` repeatedly; the MIR for this looks like this: fn main() -&gt; () { let mut _0: (); // return pointer scope 1 { let _1: (); // "_result" in scope 1 at src/main.rs:2:5: 3:6 } scope 2 { let mut _4: std::ops::Range&lt;i32&gt;; // "iter" in scope 2 at src/main.rs:2:5: 3:6 scope 3 { let mut _6: i32; // "__next" in scope 3 at src/main.rs:2:5: 3:6 scope 4 { let _10: i32; // "val" in scope 4 at src/main.rs:2:5: 3:6 } } } let mut _2: std::ops::Range&lt;i32&gt;; let mut _3: std::ops::Range&lt;i32&gt;; let mut _5: (); let mut _7: std::option::Option&lt;i32&gt;; let mut _8: &amp;mut std::ops::Range&lt;i32&gt;; let mut _9: &amp;mut std::ops::Range&lt;i32&gt;; let mut _11: isize; let mut _12: i32; let mut _13: (); let mut _14: (); bb0: { StorageLive(_1); // scope 0 at src/main.rs:2:5: 3:6 StorageLive(_2); // scope 0 at src/main.rs:2:5: 3:6 StorageLive(_3); // scope 0 at src/main.rs:2:14: 2:21 _3 = std::ops::Range&lt;i32&gt; { start: const 0i32, end: const 1000i32 }; // scope 0 at src/main.rs:2:14: 2:21 _2 = const std::iter::IntoIterator::into_iter(_3) -&gt; bb1; // scope 0 at src/main.rs:2:5: 3:6 } bb1: { StorageDead(_3); // scope 0 at src/main.rs:3:6: 3:6 StorageLive(_4); // scope 0 at src/main.rs:2:5: 3:6 _4 = _2; // scope 0 at src/main.rs:2:5: 3:6 goto -&gt; bb2; // scope 0 at src/main.rs:2:5: 3:6 } bb2: { StorageLive(_6); // scope 2 at src/main.rs:2:5: 3:6 StorageLive(_7); // scope 3 at src/main.rs:2:5: 3:6 StorageLive(_8); // scope 3 at src/main.rs:2:5: 3:6 StorageLive(_9); // scope 3 at src/main.rs:2:5: 3:6 _9 = &amp;mut _4; // scope 3 at src/main.rs:2:5: 3:6 _8 = _9; // scope 3 at src/main.rs:2:5: 3:6 _7 = const std::iter::Iterator::next(_8) -&gt; bb3; // scope 3 at src/main.rs:2:5: 3:6 } bb3: { StorageDead(_8); // scope 3 at src/main.rs:3:6: 3:6 _11 = discriminant(_7); // scope 3 at src/main.rs:2:5: 3:6 switchInt(_11) -&gt; [0isize: bb4, otherwise: bb5]; // scope 3 at src/main.rs:2:5: 3:6 } bb4: { _1 = (); // scope 3 at src/main.rs:2:5: 3:6 StorageDead(_10); // scope 3 at src/main.rs:3:6: 3:6 StorageDead(_7); // scope 3 at src/main.rs:3:6: 3:6 StorageDead(_9); // scope 3 at src/main.rs:3:6: 3:6 StorageDead(_6); // scope 2 at src/main.rs:3:6: 3:6 StorageDead(_4); // scope 0 at src/main.rs:3:6: 3:6 StorageDead(_2); // scope 0 at src/main.rs:3:6: 3:6 StorageLive(_14); // scope 1 at src/main.rs:2:5: 3:6 _14 = _1; // scope 1 at src/main.rs:2:5: 3:6 _0 = _14; // scope 1 at src/main.rs:2:5: 3:6 StorageDead(_14); // scope 1 at src/main.rs:3:6: 3:6 StorageDead(_1); // scope 0 at src/main.rs:3:6: 3:6 return; // scope 0 at src/main.rs:4:2: 4:2 } bb5: { StorageLive(_10); // scope 3 at src/main.rs:2:5: 3:6 _10 = ((_7 as Some).0: i32); // scope 3 at src/main.rs:2:5: 3:6 StorageLive(_12); // scope 4 at src/main.rs:2:5: 3:6 _12 = _10; // scope 4 at src/main.rs:2:5: 3:6 _6 = _12; // scope 4 at src/main.rs:2:5: 3:6 StorageDead(_12); // scope 4 at src/main.rs:3:6: 3:6 StorageDead(_10); // scope 3 at src/main.rs:3:6: 3:6 StorageDead(_7); // scope 3 at src/main.rs:3:6: 3:6 StorageDead(_9); // scope 3 at src/main.rs:3:6: 3:6 _13 = (); // scope 3 at src/main.rs:2:22: 3:6 _5 = (); // scope 2 at src/main.rs:2:5: 3:6 StorageDead(_6); // scope 2 at src/main.rs:3:6: 3:6 goto -&gt; bb2; // scope 2 at src/main.rs:2:5: 3:6 } } You can see the calls to `next` there which calls that mess I showed earlier. Ultimately, though, LLVM is still able to optimize this whole thing down to a simple loop. Unfortunately, the following C code doesn't optimize away in LLVM! It has something to do with the way the loop ends up, with an exit flag variable that means the loop variable doesn't get incremented on the last iteration, breaking a bunch of optimizations. void do_nothing() { int i=200, q=0; while(!q) if (!--i) i=q=1; } void do_nothing_2() { for(int i=0,q;q=i&lt;100,i+=q,q;); } Both are examples of the same issue. Both LLVM and GCC really struggle with a lot of simple examples like this. There is an issue tracking this: https://github.com/rust-lang/rust/issues/41097
We'd much rather close a duplicate than lose out on info about a bug that could help us fix it. :)
`option.as_ref().unwrap()`
[rust-clippy](https://github.com/rust flange nursery/rust-clippy) (sorry, nightly only for now) will give you an error, as well as other helpful errors and warnings.
Interesting point! I've never thought about pattern matching in this way before, I guess a match is made if the two items are structurally identical, which is something more fundamental than PartialEq.
Aha, setting up largest right was where I went wrong. Great tip. The error messages can lead you in a circle when you're a noob because its not actually a problem until the comparison with item or the return. Probably that means I'm doing something similarly wrong in the cacher. My current version is https://github.com/medwards/rustbook2/tree/master/cacher but its worth noting I've tried a few different things now and it might be a mess. Broadly I want to support both str and String as input values and but I guess I'm screwing up the references again. My experience so far in the transitfeed crate is "screw it, everyone has to give me a String and they can clone it if they can't move it" which is very straightforward right now but I want to learn to be better with refs :)
This is so much better than any answer I had hoped for, thank you for taking the time.
Any difference in performance?
You've had to do that for something that mattered in real code? Or just for debugging purposes? I think I've heard of this in gcc as well lately. My understanding was always that compilers never did unsafe optimisations by default, so you should never have to disable optimisations (except for compile speed and debugging clarity), but could enable more judiciously.
I cannot figure out where to put "RUSTFLAGS = "-C target-cpu=native"" or even what kind of "environment variable" people are talking about. Is there a way to put it in my cargo.toml file?
&gt; implementation details are a far-off concern to someone who's just starting out I don't think Rust is targeted towards the Computer Science 101 market. Surely Rust appeals more to experienced C programmers who would very much appreciate implementation details? This is why I'd expect any book on "programming Rust" to carefully explain strings over several chapters. Just as "The C Programming Language" by K&amp;R explain C strings succinctly, along with pointers, I expect Rust to do the same for its language. But of course Rust is considerably more complicated with memory management and so there must be more written out of necessity on the topic.
I predict that a common question will be "why can't I send an HTTP request with this?" While that is answered in the [second paragraph of the docs](https://docs.rs/http/0.1.0/http/), many people simply skip any discussion like that and just try to dive in. I can imagine a lot of people missing it. Can someone add a line to the very beginning of the README that says something like "This does not actually perform any HTTP requests, it just contains types describing them. If you would like to perform a request, try [reqwest](https://crates.io/crates/reqwest)."
&gt; You can produce the same ASTs either way, can't you? it's the fact that you can't produce an AST for C++ without considering almost every header file , in some cases... you change one thing, you have to go and check everything else to verify some #define hasn't produced some weird effect whereas with Rust, every file is already parseable individually; so you should be able to tell which files got affected much earlier, i.e. keep more information cached without dirtying it even within each file, it's much easier to step through and generate the AST (the fact that in C++ you need the symbol table to be set up sequentially to even parse it, whereas the shape of rust is much more stable), I would conjecture this caching process might be easier to do at a sub-file level.. e.g. down to individual items / methods
My plan is to run a Rust web app using the Rocket framework with NGINX acting as a reverse proxy for static assets. I think it's much better to think of Rust, with or without a framework, as an application server requiring a reverse proxy in front of the process/workers. Similar to how Rails typically is run within Passenger with NGINX server a lot of its static assets.
Do yourself a favor and don't start with a low level API like Vulkan. Start with OpenGL and learn the concepts https://learnopengl.com/. After that have a look at Vulkan or the new gfx(ll branch) to restore your sanity.
Even thought there are no current openings it might be a good idea to send an open application to the companies listed on rustjobs.rs. It might land you a job or it might lead to nothing but as long as your e-mail is thought out and professional there is no disadvantage to giving it a shot.
&gt; I don't think Rust is targeted towards the Computer Science 101 market. Nor do I (though I don't think it's necessarily any worse than targeting C or C++ at that market). I'm talking about teaching Rust to experienced programmers who have never successfully used a systems language in anger before. Think of how many Java devs there are out there who have gone their whole careers without needing to think about pointers, or how many PHP/Ruby/Python/Javascript developers there are out there who have gone their whole careers without needing to consider how many dynamic niceties their interpreters provide them without them needing to think about it (*especially* around strings). If these people ever want to branch out into a low-level space, I think they in particular benefit from having a strict compiler to teach them the ropes, which is why I recommend Rust as a language for first-time systems programmers. Of course, I'm not trying to imply that Rust is only supposed to appeal to fans of high-level languages! We go to great lengths to appeal to C and C++ crowd as well. Rust is neat in that it can serve as a common ground between high-level programmers looking for more speed without sacrificing abstractions, and low-level programmers looking for better abstractions without sacrificing speed. But in this specific context, I'm not concerned about C or C++ users being confused about why a language might have more than one string type, or about asking them to understand why you might want to have a distinction between a type that owns a chunk of memory and a type that merely references a chunk of memory, even if the two types otherwise have many things in common.
Is there an overview available of the specific functionality that will be reduced with vanilla LLVM?
Then again, the more people that boldly run web server crates in production the faster we might be able to discover issues and weed out bugs, no?
Environment variables are like implicit globals that you set on your command line or wherever else you run `cargo build`. The way to set them is different for each command interpreter, but typically you run a certain command once to set the variable's value for all subsequent commands, and the value remains for as long as you're using the same instance. If you want this flag to remain permanently, you'll need to consult the documentation for your operating system on how to permanently set environment variables. ### Windows CMD/Command Prompt, run this command once before using `cargo build`: (don't use quotes, it doesn't like it) set RUSTFLAGS=-C target-cpu=native Powershell, run this line once before `cargo build`: $env:RUSTFLAGS = "-C target-cpu=native" ### Linux or OS X `sh` or `bash`: export RUSTFLAGS="-C target-cpu=native" or prepend it to your `cargo build` command, which sets it for that command invocation only: RUSTFLAGS="-C target-cpu=native" cargo build ### But I use my IDE to compile Rust projects? Then depending on your Rust plugin, it should have somewhere in the build/run configuration to either set environment variables, or supply flags for `rustc`; consult the plugin documentation, or you can try the following instead. ### Cargo config You can also set Rust flags in a config file, but it's not your Cargo.toml. Instead, you need to create a new directory, `.cargo` (with the leading dot), and a new text file inside it, `config` (without any extension; this is difficult to do in Windows Explorer so look up how to do it from the command line instead). This `.cargo` directory can be in the root of your current Rust project, or [in any directory up the heirarchy](http://doc.crates.io/config.html#hierarchical-structure); thus, if you keep all your Rust projects in the same folder, you can create `.cargo/config` there to set this flag for all of them. Then all you have to do is open `.cargo/config` and add these lines (you don't actually need to figure out your target-triple): [target.`cfg(any(windows, unix))`] rustflags = ["-C target-cpu=native"] If you put `.cargo/config` in your project, I would recommend **against** checking it into version control, because if someone clones your project, it might cause issues if they want to compile it on one computer and run it on another, and it wouldn't be immediately obvious what the issue is.
Apparently what rust calls i686 is a feature set a little later than the Pentium Pro, probably the Pentium 4 from 2000 or something similar. Both this and the 64 bit level is ~15 years old so almost no one should be left out.
If your programm will not grow to a full grown game or similar, I would go with kiss3d. I used it a few days ago to render a generated mesh and it was quite simple. (Except that there were two kinds of Meshes, but take a look at the examples in the github account and the pages http://ncollide.org/mesh_generation/ and the docs)
&gt; I'm just saying that Rust can become a mature solution for running web applications. All anyone's saying is that its not there yet. Resistence to known DoS attacks is implementable without someone actually demonstrating that they're possible against hyper, though. We don't need peoples' servers getting pwnd to "get there."
HKT is really a Haskell term of art. This isn't first-class higher order type operators, which is what HKT usually refers to. But its equally expressive, with some worse syntax for some patterns that are common in Haskell (but the syntax is more than sufficient for the Rust-specific patterns we know we need, like `Iterable`).
Yes, because it doesn't define HRTB of type arguments this can't be used to introduce additional bounds on the GAT in where clauses. But that's a limitation that will eventually be lifted. This is a deep semantic change to the language. Indefinitely pushing the type arguments outward, as your second example suggests, is not equivalent (in your code example, I can create a container family that *only* supports `i32`, whereas in the first example, a contract of a `ContainerFamily` is that its container support anything).
&gt; The semantics of the else clause on for loops in Python is actually pretty simple to describe: it is run when the loop ends without a break. And yet in a survey conducted at PyCon 2011, [only a quarter of respondents gave that correct answer](https://blog.glyphobet.net/blurb/2187/). It may be simple to explain, but we have empirical evidence its hard to learn. We've decided not to make `for` and `while` loops evaluate to anything but `()` for the time being.
This has probably been asked before, but what prevents deadlock checking at compile-time? Does it reduce to the halting problem?
Is there anywhere that these sort of optimization tips are aggregated? Especially the part about using an environment variable to tweak the compilation of the dependent crates - I understand the difference between what it's doing and cargo rustc, but it seems really counter-intuitive. I also generally dislike the use of environment variables like this because it's effectively making the compilation dependent on global state.
Yeah, at the moment you need to put a ton of trait bounds for all the different types you are going to be using it for, which is absolutely painful, but at least it's some workaround for when you really need trait bounds on the GAT until the HRTB extension eventually lands. Great job on the PR btw. I'm really excited for GAT :)
ATC is exactly equivalent in power to native HKT, but only in the presence of HRTB. ATC by themselves are a strict subfeature of HKT
No, my purpose was to test the assembly output for an algorithm using mock data, and I had to hide the mock data behind the black box because it kept evaluating the entire algorithm at compile time and just placing a number there, instead of indicating how it would run in a real-world scenario.
Try the new pretty serializer in the toml crate (`to_string_pretty`, see [docs](https://docs.rs/toml/0.4.5/toml/ser/struct.Serializer.html#method.pretty)).
Can someone show a concrete example of where E0491 "reference has a longer lifetime than the data it references" is actually necessary for memory safety? In other words, a code snippet that the compiler would accept if it were not for E0491, and which would directly break memory safety or trigger UB at runtime?
Look at how the ”bytes“ crate implements every function for Little- and BigEndian, for example: fn put_u16&lt;T: ByteOrder&gt;(&amp;mut self, n: u16) { let mut buf = [0; 2]; T::write_u16(&amp;mut buf, n); self.put_slice(&amp;buf) } and you use it like that: let mut buf = BytesMut::with_capacity(1024); buf.put_u16::&lt;BigEndian&gt;(1234); This way you don't need a variable, just a type parameter.
The libz blitz has produced a bunch of quality stuff, eg https://github.com/rust-lang-nursery/api-guidelines
If you need it to be dynamic, then your solution using trait objects (”Box&lt;Algorithm&gt;“) is already the best. That's what trait objects are for. You could use macros/annotate your implementations with custom attributes if you want to reduce redundant code and the effort is worth it.
I also love when it knows math. In this example, it detects the arithmetic series and use the closed-formula to optimize the loop away https://godbolt.org/g/cV9xTX
That's perfect. Likely didn't see it because I was using an old version.
*whoa*
FWIW, the author of nalgebra posted here not too long ago with some benchmarks and attempts to add functionality. I got the sense they were pretty serious about trying to get nalgebra toward something like what you'd see in C/C++. Ndarray is another thing to look at. In those benchmarks, ndarray was often (but not always) slower than nalgebra, but in reading stuff about ndarray and by that author, I get the sense that it's well set-up for the long-term. Having said that, others are right that there's nothing remotely like Eigen for Rust at the moment. A lot of people ask about it, but C++ has been around for so long that it will take a little while for Rust to catch up. 
thanks :)
I'm not aware of this. Can you show me something you could write in Haskell using higher kinded polymorphism without any higher ranked constraints that you can't write using generic associated types in Rust?
&gt; No, simplification is conceptual simplification; deprecating stuff is sufficient. I disagree. You can't make a system more simple by making it more complicated.
I guess it's similar to preventing memory corruption. To correctly decide if some program causes memory corruption in *every case*, you'd have to solve the halting problem. However, you can avoid that problem by being stricter than necessary. Rust can guarantee memory safety for some useful set of programs, and it rejects everything else (including a lot of safe programs). I'd assume that deadlock prevention works in a similar manner.
I believe so, but I am a crappy type theorist. I won't know for sure until I can play with it.
It's not always possible to know how long a slice is, or what the index is, at compile time. Either one of those could come from external inputs. The compiler does give warnings when it can evaluate those things at compile time and believes that a panic will occur. For example this code produces a warning: let x = [1, 2, 3, 4]; let y = &amp;x[4]; warning: this expression will panic at run-time | 10 | let y = &amp;x[4]; | ^^^^ index out of bounds: the len is 4 but the index is 4 
 Hi! This is just a friendly reminder letting you know that you should type the shrug emote with three backslashes to format it correctly: Enter this - ¯\\\\\\\_(ツ)_/¯ And it appears like this - ¯\\\_(ツ)_/¯ --- *^If ^the ^formatting ^is ^broke, ^or ^you ^think ^OP ^got ^the ^shrug ^correct, ^please ^see [^this ^thread](https://www.reddit.com/r/john_yukis_bots/comments/6tr5vq/u_you_dropped_this_a_shrug_fixing_bot/)^.* ***^Commands:*** *^!ignoreme, ^!explain* 
however, [this](https://play.rust-lang.org/?gist=cc5269f5d59ea5050e57e7e43f877085&amp;version=stable) optimizes just fine.
Small guide: https://gist.github.com/kvark/840c8cadf755b0d822b331222b0c3095
Even though it's original creator no longer works on it, I have yet to find a library that feels as good to use as `glium`. It's well documented, has a good (enough) tutorial to help you get started, and feels fairly intuitive to use. That being said, I agree with a lot of the advice in this thread in that you should try and also learn the OpenGL API on it's own to better understand a low level graphics API.
The output of `rustc --explain E0491` gives a starting point: // struct containing a reference requires a lifetime parameter, // because the data the reference points to must outlive the struct (see E0106) struct Struct&lt;'a&gt; { ref_i32: &amp;'a i32, } // However, a nested struct like this, the signature itself does not tell // whether 'a outlives 'b or the other way around. // So it could be possible that 'b of reference outlives 'a of the data. struct Nested&lt;'a, 'b&gt; { ref_struct: &amp;'b Struct&lt;'a&gt;, // compile error E0491 } I expanded this example to show how it leads to UB: https://play.rust-lang.org/?gist=38d1efb8686488aaf3a47e79ab23b24d&amp;version=nightly When I run this in the playground it prints: abcdef ghijkl ...despite supposedly printing the same string twice. Of course, in order to make a demo, I had to "get around" E0491 using `mem::transmute`, so... I *think* this is a valid demonstration but I'm not 100% sure.
Huh. I went back to Niko's blog posts, where I thought I had gotten the idea from, but didn't find it on reread. I think it was reinforced by my troubles in trying to implement the following data type in "final" encoding (the only way Rust can accept it) with ATCs but without HRTBs: data Eq a b where Refl :: Eq a a Obviously, GADTs go a bit beyond the norm. XD EDIT: I guess the “family pattern” can be seen as a type level version of [defunctionalization](https://en.wikipedia.org/wiki/Defunctionalization), so it should be exactly as expressive.
You don't have to stop adding features to stablize your API; you just have to draw a line in the sand and pick a subset of your API and go: **this** is stable. Like I said, if you feel that you are on such shifting sands that there is *no place* in the code base that you can do that for, this framework is not ready for prime time. It's not ready for general use. It's just an experiment. &gt; I don't think we should ditch it or freeze the API at this point. Sure, that's a fair call. ...but you can't *also* say: &gt; Rust is not ready for production yet, but it has a lot of potential since there are few nice frameworks already, it is fast and has great stability. It's one... or the other. Pick one. &gt; You can use cargo for freezing versions so you have a stable api. I'm completely bewildered to hear you say this, because I really respect your contributions to the community. Have you ever had to maintain a large software project? How can you possibly assert that the entire package management architecture used in every language in every facet of software development is irrelevant, because you can simply pin the exact versions of your dependencies? I mean, I understand the sentiment, but if you look at the approach that golang has taken over the last 5 years, which has been, effectively 'vendor the exact versions you want', you'll see that it has been a massive failure at scale. That's why there is now an official effort to produce a cargo like package manager ([dep](https://github.com/golang/dep)) I'm not saying 'stop experimenting' or 'rust web frameworks are bad' or 'stop adding features': I'm saying, **stability** is *more important* than features. In fact, it's so important, that if you can't commit to stability, *having additional features is meaningless*. This a akin to the pre-rust 1.0 debate about if/when rust should '1.0.0' itself. Was it the correct decision? The answer **categorically** yes. A stable release train is exactly what people need to have the confidence to pick your platform (be it framework or language) to use. An experiment is just that; an experiment. Don't be surprised if people play with it, like it, then claim that it's not production ready. It's not because they don't like it: it's because *the maintainers* have failed to demonstrate it *is* production ready.
Since this crate is mostly intended to be used first by other crates providing HTTP related features (such as reqwest), I think it should have a less obvious name (e.g. http_common). I agree that with that name, it seems likely that there will be a bunch of confusion generated for first time users.
&gt; EDIT: I guess the “family pattern” can be seen as a type level version of defunctionalization, so it should be exactly as expressive. That's exactly right.
No, I think the name is great. It is the glue that holds many pieces of the HTTP ecosystem together. It means your types can be things like `http::Request`, and have that shared between libraries using different implementations. But I do think it needs a more prominent warning, since the confusion is inevitable.
Why does this compile? fn till_ten&lt;'a, I&gt;(iter: &amp;mut I) -&gt; Vec&lt;u32&gt; where I: Iterator&lt;Item=u32&gt; { iter.take_while(|&amp;x| x &lt; 10).collect() } Specifically in regards to `take_while`. According to the Rust docs, Iterator's `take_while` takes `self` as a parameter, not `&amp;mut self`. For example, *this* code doesn't compile, which makes sense to me. let mut iter = 0..20; let q: Vec&lt;_&gt; = iter.take_while(|&amp;x| x &lt; 10).collect(); let w: Vec&lt;_&gt; = iter.collect(); &lt;-- use of moved value And the code below works compiles just fine, and even does what I want it too (`q` is 1 through 9, `w` is 11 through 19) let mut iter = 0..20; let q = till_ten(&amp;mut iter); let w: Vec&lt;_&gt; = iter.collect(); 
Don't worry, I don't even count how many stupid mistakes I did. :) Glad to help.
&gt; it's the fact that you can't produce an AST for C++ without considering almost every header file , in some cases... you change one thing, you have to go and check everything else to verify some #define hasn't produced some weird effect But what does this have to do with the nature of the grammar? It seems to me that this is an issue with the module system (or absence thereof). As far as I know, there aren't a lot of elements to the C++ grammar that make it context sensitive, and it only makes the parser complicated. A slightly different syntax would solve it definitely, but would change nothing of what you're talking about.
I guess you can think of it this way: while it's true that the conceptual value of `%rip` is larger in the second instruction compared to the first, the conceptual value of the label itself (`foo` in your example) is that same amount smaller in the second instruction, so the differences cancel out.
A mutable reference to an iterator is itself an iterator. That is, there's an [`impl&lt;'a, I: Iterator + ?Sized&gt; Iterator for &amp;'a mut I`](https://doc.rust-lang.org/src/core/iter/iterator.rs.html#2280). So when you call `take_while` in `till_ten`, the type of `self` is `&amp;mut I`, not `I`. In your failing example, the type of `self` is `Range`, which is not borrowed and gets moved. It will work if you have `&amp;mut Range`. Try adding ` let mut iter = &amp;mut iter;` to reborrow and you'll see it run.
It's a bit tricky. The checking function will unpark the deadlocked thread in order to collect its backtrace, it may trigger some bad behavior (unreachable panic) if the deadlock is not for real. That shouldn't be possible unless the user leaks lock guards (which is safe). 
Thinking a bit more about this, if someone leaks a guard there's no chance of unparking the waiting threads with safe code, so checking deadlocks can be made safe. 
I.. don't get it. If leaking guards is safe, how can it lead to unsafety here? Or actually.. this "bad behavior" - `unreachable!()` panic - is actually perfectly safe, right? The user can call it directly from safe code... Is `unsafe` here just a lint to warn that this function is tricky to use? If yes, that's a poor use of `unsafe` -- you can add other forms of "speed bumps", like [`UnwindSafe`](https://doc.rust-lang.org/std/panic/trait.UnwindSafe.html) and `AssertUnwindSafe` in the stdlib &gt; Note, however, that this is not an unsafe trait, so there is not a succinct contract that this trait is providing. Instead it is intended as more of a "speed bump" to alert users of catch_unwind that broken invariants may be witnessed and may need to be accounted for.
Oh, I commented below before reading this. This sounds good!
http-interop or http-types might have been a better name? Nah, I think http is the right name for this. 
Haskellers tend to view this as a mistake, in my experience. List comprehensions and partial pattern matches in `do` are not generally used in production code.
https://github.com/Minizinger/planet-rust/tree/master/src I was working on this project a while ago. It might be slightly outdated, but I believe you can take vertex.rs, triangle.rs and renderable.rs and apply them to your case, there is no project specific code in these parts, just rendering. It's MIT licenced, so I'd be just happy if it helps
Yes, this is a problem and the solution right now is that if the project is on GitHub, fork it, update the dependencies, link to it with `discord = {git = "yourrepo_url"}`. Then make a pull request and tell the owner to make a new version. I've not found a solution for this, sadly, but this is my workaround.
Can you use an older version of reqwest that uses a compatible version of openssl?
*ONE* This sentence is a bit confusing to me: &gt;It's intended that this crate is the "standard library" for HTTP clients and servers without dictating any particular implementation. Maybe it's just me, but when I heard "standard library", I expect something batteries included. I think I understand the sentiment, and understand/support the idea of the `http` crate, but maybe a differently worded sentence would better level set user expectations? *TWO* &gt;`use http::header::{self, HeaderName};` &gt;`let name: HeaderName = header::ACCEPT;` &gt;`assert_eq!(name.as_str(), "accept");` Shouldn't that be `Accept`? Or is HTTP not case sensitive? Maybe just a docs typo? *THREE* I know nothing about http2. Will these types be usable by http2 libraries?
If you have the control flow written in a Turing-complete language, you would decide whether the lock is released by checking whether a certain part of the code always executes (probably a destructor), which is the same as deciding whether whatever precedes it always executes to the end (without returning or entering an unbounded loop)... which is the halting problem. One way to solve this is to introduce total functions, that are functions guaranteed to return -- for those functions the halting problem is trivial. It's complicated to make them usable to do real stuff; for most languages I know with them you also have dependent types, which makes the language very complicated. But, there's another problem: a thread can be killed by (potentially) any part of the program, and when they are killed all resources they locked is gone. So to really be sure there's no deadlock, you need to check at compile time that no part of the code will kill a thread that holds a lock (easier if no code kills a thread, period). Anyway, here's a more practical way to prevent deadlocks: don't use locks, and use lock-free data structures instead (those are implemented with atomics). They have constraints and tradeoffs though, and aren't necessarily better than locking data structures. Anyway see [this excellent post](https://ticki.github.io/blog/an-atomic-hash-table/) written by /u/ticki_ that describes an atomic hash table. An excerpt &gt; There is not really a single answer to that question, but there's a few things to keep in mind. First of all, we probably want to avoid reallocation. That thing is messy, and often impossible, to do atomically; you will almost always end up introducing some kind of lock, either explicitly or implicitly, so we must necessarily use a kind of linearly growing structure. That's quite a constraint.
Yeah, it might be possible to find a version alignment here. Also, don't forget you can just add that version of OpenSSL which is required and use it directly, unless reqwest interacts with discord, in which case do this and try and find compatible versions.
PR: https://github.com/Amanieu/parking_lot/pull/41
I like `http` too, but if I were suggesting things, maybe `http-base` ?
Thank you :)
So all labels are relative? Or that specific label is relative for some reason? I thought all labels are absolute.
You only need to do this if/when you want to publish your source code. You can make the changes locally and just use { path = " ../discord" } for example.
I don't see this as a big advantage as you could `extern crate http_types as http`.
This is so cool!
http1 is not case sensitive. http2 is, and mandates all lower case. this library is intended to be useable for http2 (and is being used in an implementation of http2 already)
Hey, no problem - I spent a while hitting my head against this one before :)
Great! Thank you for the details!
Hmm, you are in the right direction, there are a few steps that you need to take to get it working. I won't give a full solution, just a couple of hints. First, the definition of the struct will need to look like this: struct Cacher&lt;T, U, V&gt; where T: Fn(U) -&gt; V, U: Copy + Eq + Hash, V: Copy, { calculation: T, values: HashMap&lt;U, V&gt;, } Making U and V Copy makes things so much easier. Why those 'a lifetimes wasn't working is another story for another day. Then, the values method's signature can be changed to this: fn value(&amp;mut self, arg: U) -&gt; V Everything else remains pretty much the same, there might be a few tests you need to change to appease the borrow checker, but after that, you should be good to go!
That would just encourage other people to compete with `http-foundation`, `http-prelude`, `http-basic`, `http-fundamental`, and so on... Naming it `http` is a pretty strong flag in the ground that says "this is the actual package that defines HTTP."
Agreed
Also this video from RustConf: https://www.youtube.com/watch?v=t4CyEKb-ywA Not everything discussed in it is applicable for small or medium sized projects in my opinion, but otherwise it's a good checklist for crate authors.
If discord is compatible with the newer version of hyper and/or openssl, you can override dependencies: http://doc.crates.io/specifying-dependencies.html#overriding-dependencies Or you can always submit a PR that updates them to the newer version.
&gt; A breaking change is definitely neccessary if you want to attain the actual goal of the RFC which is simplification. Not really, users can turn deprecation warnings into errors very easily. Cargo could even automatically add the code for this when initializing new projects without breaking the language or old projects. This enables you to opt-in to the benefits with very little cost. At some point you end up with lots of code to remove lots of deprecations, so we could group all those deprecations into "a deprecation group". This still gives you all the benefits of epochs without any breaking changes for very little cost. So as you see, the modules RFC could still simplify the language significantly without epochs. Turning very old "deprecation groups" into hard errors by default, and allowing users to opt-into deprecated behavior, is just the last touch. But a lot can be done towards simplification without that. &gt; If you just add some new model and let the old model in place, you force people to learn both models as they might encounter both. People will need to learn both models sooner or later anyway. Old crates using old epochs will still be usable from newer crates using new epochs. Some of these old crates might eventually be finished, and not need any updates. Users that want to read their source code will need to learn the deprecated functionality to understand it. That is, either the system maintains crate-level compatibility with older epochs, and you end up with this problem, or the system requires all crates to be compiled with the same epoch, in which case there are many crates that you might not be able to use and you break the ecosystem. Not breaking the ecosystem is the correct choice, and the one the epoch RFC makes. &gt; I wasn't around so I couldn't tell. The dyn trait RFC is not new. We had it before, two times (once before 1.0 and once after), both times proposing to deprecate the "bare trait" syntax. There wasn't any time before 1.0 to look at it, and once it was analyzed after 1.0 it turns out everybody agreed Rust made a mistake with the bare trait syntax. This mistake became more obvious with `impl Trait`, because people would prefer to use the bare trait syntax for that. The same can be said about the module system, and macros by example. It was well known that they were not perfect, and the hope was that one could improve them significantly without breaking backwards compatibility after 1.0. It turns out that the module system can be improved significantly without breaking backwards compatibility, but macros could not, so we now have two "macros-by-example" systems in Rust. Anyhow, every single time each of these 3 topics/RFCs over the last 3 years was discussed, the discussion of "we could do way better if we could break the language" popped up. It was always delayed because 1) we had no time to have it, and 2) we hadn't "good" RFCs for the features, nor an implementation, so we did not know if pursuing that was worth it, but the plan always was "we'll have the discussion about breaking the language once we need it". Now we have good RFCs for these features, and implementations there of, we know what can be done without breaking the language, and we know how we could break the language to make it better, and whether that's worth it or not. So the time to have the discussion about "epochs" has come, and even here the plan is that an epoch does not need to imply breaking changes at all. They can still be a win without breaking changes.
Oh dear, we eventually accepted that complex swap? IMO this should *at most* be something the compiler generates when lowering to LLVM IR, out of the shape of `mem::swap`'s MIR or whatever. Or maybe it should be done entirely in LLVM. Having it on the Rust side will hurt us a lot, especially if we try to do MIR optimizations again...
&gt; But what does this have to do with the nature of the grammar? the fact that you can't even parse C++ without knowing the whole symbol table, as defined by a specific sequential order depending on the header files and of course he order of items in one file. So even figuring out the dependancy graph is complex. In Rust, you can establish the shape of a source file (or even subset of a source file) in isolation, *then* get on with figuring out the dependancy graph (and hence which parts you'd need to recompile). So C++ (as defined now) will always have a longer up-front phase (although there are attempts to workaround with PCH) e.g. what do these mean? a&lt;b&gt;c() a&lt;b,c&gt;d() those are all over the place, angle brackets are incredibly common. And there's worse possibilities than that ..
Oh, right, I get it. Yeah it does sound like a big issue.
I believe the Pony language guarantees absence of deadlocks as well, by simply not having any locks (actors instead), but I'm not familiar with the details.
I think that you have a misunderstanding about how the `label(%rip)` syntax works. The label is not the value that is added to `%rip`, it is the final address. So the actual value that gets encoded into the instruction is `label - %rip`. So basically this means that if you have 2 instructions like this: movq label(%rip), %rax movq label(%rip), %rbx Then `%rax` and `%rbx` will end up with the same value: the address of `label`.
I didn't know about `.cargo/config`. Awesome! I'll add this to my rust perf pitfalls blog post.
i had wondered if you could do something like make an assumption ('most of the time, anglebracks and commas mean...), parse it one way, then get on with actually calculating things, but then backtrack and change if you later discover the assumption was wrong 
nobody would actually do this, though
Consider breaking it up into two separate programs? Use a message queue to request web scrapes, and send the results back via the queue to be processed for chat?
Have you tried a patch-crates.io entry in your Cargo.toml?
It's merged/released now. Maybe you could add an edit to the toplevel comment?
Indeed, thanks for explaining! &gt; The label is not the value that is added to `%rip`, it is the final address. Is this specific to `%rip` or does it work for all registers?
There is this [awesome website](https://godbolt.org/g/Gb2NQJ) that shows you the assembly code as you type. It has an option to enable intel syntax and it may be easier to read than AT&amp;T syntax.
&gt; This mistake became more obvious with impl Trait, because people would prefer to use the bare trait syntax for that. Honestly I wouldn't use bare trait syntax for anything. You can't distinguish traits from structs otherwise. &gt; It was well known that they were not perfect The module system is *very* close to perfect. The only major issue I see is confusion between extern crates and modules in the root. &gt; It turns out that the module system can be improved significantly without breaking backwards compatibility *all* proposals for changes of the module system by lang team members involved one breakage or another. Like in the current module system RFC, `extern crate` and `use foo;` where foo is from the local crate would both be deprecated. Or take other proposals that deprecate mod.
It's specific to `%rip`.
Auto-~~dereferencing~~referencing is something that is being discussed, and could end up being added: https://github.com/rust-lang/rfcs/pull/2111 EDIT: Indeed coder543
Thanks I got that to work with some more mangling of Cacher.value (it didn't like inserting after I've gotten from values). I get the sense that a lot of the time I should focus on getting things to work with Copy/Clone first and later if I want better memory behaviour then I start working with references more.
How about using an enum instead? That way it's dynamic basically for free, no heap allocation needed.
I think all those types in Rust are very well justified, and I believe it's better to teach about those different types and the subtle reasons for their existence rather than pretending there are only two string types. (The only thing I dislike is that they are so inconsistently named.)
I am actually the author of that swap code. I found it provided significant speedups, especially on ARM, and for small types it reduces to the old code. I do agree, though, that `mem::swap` should eventually be turned into an intrinsic. At the MIR level, we should be able to say things like let ref mut a; let ref mut b; swap (a, b); swap (a, b); // no-op swap (a, b); f (b, a); swap (a, b); // f (a, b) I think the second rule alone if properly generalized could be exceptionally powerful for optimizations. Does LLVM offer something in this vein?
it's sometimes said [IoT](https://en.wikipedia.org/wiki/Internet_of_things) is an opportunity for Rust (a field needing it's overlap of unique features), does that translate into any commercial opportunties? (startups in the new space, new products =new sourcebases). I imagine established organisations would find it hard to justify throwing out their existing C++ sourcebases.
You are right, that would work, too. Personally, though, I would prefer the trait objects variant, because it separates the different algorithms more nicely.
Nice! I [opened an issue](https://github.com/bluss/rust-itertools/issues/185) against itertools a while ago for this very feature
Wouldn't it be easier to just say that it's basically algebra? Variables are just containers that represent a modifiable value? Functions take inputs, apply expressions, and return an output?
The crates using it (reqwest, hyper, etc) could do it though.
I recently started working on a little project that makes use of a lot of crates and macros. Release build with LTO already takes ~~10~~ 44 seconds and it is not yet even 1k lines of code. This is assuming all dependencies are already compiled. Full build from zero is a good few minutes. EDIT: it is actually much, much slower than I remembered. I was confusing the release build time with the debug build time.
I ran into the same problems with discord-rs and when I tried to pin dependencies it just punted the issue one layer further with other crates now balking on the newer hyper... In the end I switched to [serenity](https://github.com/zeyla/serenity), and it's been working well so far.
I believe this is exactly what I'm looking for: not having to deal with all the corner cases of OpenGL but able to tweak with the rendering. I think I will actually start with your code to further improve my understanding of 3D rendering and go on from there! Thank you.
A relatively easy type system to prevent deadlocks, is to require mutexes to be ordered and then only allow locking mutexes ordered lower than those that are already held. I think something like that could be implemented in pure Rust, but I'm not sure how many useful programs can be expressed with those constraints.
Thanks! This is a very nice guide.
I will definitely check the learnopengl doc. Thanks
That's exactly what I will do: I will look at glium and project that use it, and then try to understand the inners of OpenGL. I'll use /u/Gurchi's planet-rust as my starting point. Thanks.
What's the LLVM bug for the C examples ? Clang trunk optimizes the two examples you mention: https://godbolt.org/g/AnFcWS
So I do think it should be either a MIR statement or an intrinsic, but only if we can't match `t = a; a = b; b = t;` in rustc or LLVM. LLVM would be great for this but I don't think it vectorizes or even recognizes swaps (other than to generate target-specific instructions, e.g. `XCHG`, which don't even start as `memcpy` calls, but rather plain loads and stores, so they're easier to recognize).
I don't know whether this was mentioned before: A decent high-level icalendar and vcard library which is both able to read and write these formats. There are some low-level ones and one or two high-level ones, but these only feature either reading or writing. I guess, from an API standpoint, [`icalendar`](https://crates.io/crates/icalendar) is the nicest so far, but it does not feature reading, yet (I contacted the maintainer, though. Lets see what he answers).
How do you split rust code in to multilpe files. 
&lt;3
You can declare modules with `mod` and then have a file with that name, e.g. `mod too;` and the file `foo.rs`. Edit: you can also create a `foo` directory and add `foo/mod.rs|. This allows you to have further submodules, e.g. `foo::bar`.
Have a look at the [RTFM](http://blog.japaric.io/fearless-concurrency/) framework by /u/japaric does. It's designed for microcontrollers and ensures deadlock-freedom at compile-time by using hardware interrupt priority levels.
Rust is using LLVM4 at the moment, and it doesn't optimize this code.
Yep. It's usually not a big deal for statically typed languages, like Rust. But it's even possible for a dynamic languages as well. As an example of the very same optimization, I may suggest to [have a look here](https://github.com/0x7CFE/llst/pull/92) where I provide experimental type harvesting and inference solution for a completely dynamic language Smalltalk. Even in case, where operators may be overloaded and basically each method invocation may yield a value of a different type, it's still possible to infer types across several methods and tie things together. This makes me stay optimistic about Rust's future and believe, that its type system would eventually unlock such optimizations, that we've never dreamed of before.
[A little more info here.](http://blog.system76.com/post/165129943088/popos-weekly-update-installer-sprint-theming) "Pop!_OS" is still a leading candidate for the dubious honor of being my least favorite OS name ever, and I'm not sure why we need Yet Another Distro, but if they're going to start writing components in Rust, I'm significantly more excited than I was previously. I had also forgotten /u/jackpot51 worked for System76!
That's not a very compelling counterargument.
System76 has a lot of software written in Rust, as you could imagine when the Redox OS BDFL is working there. - https://github.com/system76/buildchain - Reproducible, distributed builds using LXD - https://github.com/system76/certification - Hardware certification - hosting server is Rust using Rocket - https://github.com/system76/distinst - Ubuntu installer - https://github.com/system76/ecflash - Library for querying System76 laptop embedded controllers - https://github.com/system76/firmware-update - Firmware update frontend - https://github.com/system76/libparted - Bindings to libparted - https://github.com/system76/libparted-sys - Automatic C header conversion for libparted - https://github.com/system76/lxd-rs - LXD library for Rust - https://github.com/redox-os/uefi - UEFI library in Rust And there is more to come...
Just call it Pop. When you see Ubuntu 17.10 compared with Pop 17.10, I think you will understand why we need Yet Another Distro :)
I eagerly await that day!
Perfect explanation. Thanks!
And they'd re-export the types, surely?
Just wanted to add to the other replies: Sadly discord-rs is currently in a difficult position. Updating stuff requires a bunch of other stuff to be updated, which breaks the entire thing. I've tried updating parts myself, and another person is rewriting the connection logic. But it's going slowly :(
done
&gt; But that's a limitation that will eventually be lifted. The current RFC also mentions this, is there a time-frame for this HRTB RFC? What are the challenges?
How do you deal with non-termination in general?
Are they on the friends page yet?
Let me guess, it's because you needed to change the colorscheme? ;)
I don't know the heuristic for inlining, but I would guess: 1. Assign a cost to functions. 2. Only inline those that have a low cost 3. If you you assigned costs correctly, the when you get to the constant folding stage, you won't accidentally be folding an expression that takes infinitely long to evaluate.
&gt;This makes me stay optimistic about Rust's future and believe, that its type system would eventually unlock such optimizations, that we've never dreamed of before. You can do both constant folding and inlining in Go as well, and Go doesn't have a good type system.
I was hoping the thread title meant "As a part of its normal compilation process, LLVM at one point computes 10 factorial"
&gt;So... although non-exhaustive pattern matching would make absolutely sense here without any ambiguity I could think of, it will be rejected. &gt;I propably wouldn't use it, but I feel like this would make Rust simpler in a "less special cases" way. Actually, that sounds like it would be a special case of dependent typing. I can't really think of any way to introduce that behavior into Rust without it making things more complicated.
&gt; 1. Assign a cost to functions. &gt; 2. Only inline those that have a low cost The problem of making sure you terminate while inlining recursive functions (or unrolling loops) is pretty much unrelated to those heuristics that measure the (local) cost of inlining... unless you have some sort of holistic reasoning phase that can somehow _certify_ (in which case it's really not a heuristic anymore) when the interplay between inlining and constant folding will terminate. In any case, not "actually a fairly easy optimization".
Wait, no, RIP-relative addressing is a thing, right?
&gt; unless you have some sort of holistic reasoning phase that can somehow certify (in which case it's really not a heuristic anymore) when the interplay between inlining and constant folding will terminate. You just need to add a "count" that will stop after a time. It doesn't hobble regular optimizations because functions whose count is large are going to be expensive anyway.
No, you can't. This works normally, however for native dependencies Rust refuses to install multiple versions of the crate because you can't just link multiple versions of the same C library together.
It's just an assembler thing for labels. The assembler will calculate the distance between the current instruction and the label for you. You also have the option of specifying an explicit offset like this: `0x80(%rip)`.
I'm not sure I understand what you are talking about, but you don't need to downvote me when you disagree.
oh wait I misunderstood your original comment
[removed]
OP isn't talking about constant folding or inlining, but "optimizations that we've never dreamed of before."
"We're all coders now."
It's not difficult to come up with a bunch of examples that don't optimize. The technology is just not there yet. It depends a lot on pass ordering and such. void do_nothing() { for(int i=0;i+=i&lt;1000,i&lt;1000;); } But it does appear that this particular issue has been fixed in LLVM since 4.0 - which is great news! Maybe this will bring great benefits to rust optimization.
[removed]
That is actually the feature I am most looking forward to, now that impl Trait is usable (out of all the other big ones in progress at the moment: HKT, async/await, NLLs, macros 2.0, etc; even though all of them are awesome and I'm really looking forward to 2018). It would also allow implementing things like a fixed-point arithmetic type generic on the precision/shift bits.
What about `http-types` considering that's what it actually defines, not HTTP? 
I know more about the ghc inlining logic which presumably differs a bit because there are less cases to consider in a pure lazy language and inlining is even more important. In ghc there are two huge considerations: Can inlining duplicate code and can inlining duplicate work. It then checks how many possible inline sites there are (one means we won't duplicate code) and how often these can be executed (inlining into a closure/loop might duplicate work and side effects). Inlining a function or data constructor is always work safe because that duplicates a bounded amount of work and no side effects but it is only worthwhile if we match on it or apply arguments. If there is more than one call site it checks whether there are possible gains from inlining (discounts for each argument we know something about and for matching on the result) and compares that to the expression size. Ghc won't inline recursive functions but it will try to split them into a worker and wrapper where the wrapper does the pattern matching/result construction. The wrapper is then inlined which effectively results in a custom calling convention. If the function is a join point (only saturated tail calls) then it can be inlined and transformed like a non-recursive function. Loop unrolling is done later by a backend, like llvm.
If I write this in my .cargo/config on the top level of my workspace, will it be used for all targets, all dependency crates and workspace crates? [build] rustflags = "-C target-cpu=native" Btw, does cargo merge all the configs from all .cargo/config files of all parent folders? Or does it only use the first file it finds?
I believe that key is only for the last crate. And iirc it does some merging.
Yes, that was the first and perhaps most important change :P
No, they are not.
Can we have a specialized version of an Iterator (Range) for a simple integers? We throw all this unnecessary code at LLVM, and then complain about slow compilation times.
Is there talk, and or definitive path forward to moving these types or Traits into nursery and eventually `std::http`? 
&gt; the Redox OS BDFL is working there. I couldn't find who's the BDFL in the Redox site, then I realized it's probably you... right? :)
Yes, that is so
[..THE book](https://doc.rust-lang.org/book/), the first thing on the Rust documentation page.
I'm sorry I probably should have checked the website first :(
Setting aside implementation challenges, there are questions of syntax &amp; what we would actually allow. For example, what about this? where T: (for&lt;U&gt; Foo&lt;U&gt; where U: (for&lt;V&gt; Bar&lt;V&gt; where V: (for&lt;W&gt; ...)))
Note that there are two different books there. I've only been through the first edition and liked it. The second edition, though, was written to overcome the first edition's perceived shortcomings and has been well received here so I think you should go with the second edition. Don't hesitate to jump into the api docs as well once you get going. 
Maybe the nursery at some point, but I don't think moving it into `std` would be great. In general, std should stay small in favor of crates.
Thank you. I'm starting to read it now.
Oh, I didn't know that. Will keep that in mind.
Also check out rust by example.
&gt; it is run when the loop ends without a break. does it give an error if you have a loop with no break, but an else?
The second edition is incredible. Highly recommend it. I've read both and the second is far superior (though the first was good enough too!).
 struct Node{ value: i32, next: Option&lt;Box&lt;Node&gt;&gt;, } struct List{ size: u32, head: Option&lt;Box&lt;Node&gt;&gt;, } fn push(value: i32, list: &amp;mut List) { let mut current = &amp;list.head; while *current.next.is_none(){ current = &amp;current.next; } } fn main() { let mut list = Box::new(List {size: 0, head: Option::None}); } error: no field `next` on type `&amp;std::option::Option&lt;std::boxed::Box&lt;Node&gt;&gt;` --&gt; hello.rs:15:17 | 15 | while *current.next.is_none(){ | ^^^^ error: no field `next` on type `&amp;std::option::Option&lt;std::boxed::Box&lt;Node&gt;&gt;` --&gt; hello.rs:16:22 | 16 | current = &amp;current.next; | ^^^^ error: aborting due to 2 previous errors why doesn't this work? and how do I make it work with the fewest number of keystrokes? thanks
we could use lambdas to do something similar in c++ but like this it's a bit messy with the extra nesting level auto result= []{ for(){... return}; return..}(); personally I don't think this is any clearer than having 'for...else..' and having to remember what the 'else' semantics are I suppose in Rust at least we could make a macro with annotations, but the nesting levels in macros always bug me a bit aswell.
it might get cryptic, but reducing nesting levels is nice IMO. perhaps it could be called 'for match...' or 'for let...' to clear it up. (allows googling "what does ```for match``` do" if it's unclear to anyone). **actually as a compromise, how about this..** for x in iteration match { pattern1 =&gt; ...., _ =&gt; .... } r.e. the option 'should it run for all matches, or until it doesn't' - i'd expect *all* matches, and use 'while ...' for 'until it doesn't I'd say the same thing with '```for ... else```' which I quite very keen on. I implemented that independently in my pet language, without knowing it existed elsewhere. As such, I think Python's behaviour is reasonable: it matched my own intuition
The zeroth problem using this approach was confusion because in maths "variables" are immutable. :D The second was that she didn't like Algebra in school, so I prefer not to use math more than necessary.
OK, thank you very much!
Agreed, calling it pop would be a massive improvement over that esoteric weirdness, I still wouldn't go with that as a name, but its better and shorter.
Shame it was received that poorly.
cf [Function multi-versioning in GCC](https://lwn.net/Articles/691932/).
Don't forget about the rust docs that are always with your rustc install: rustup doc
At first I was quite puzzled by the announcement, but I guess it makes some sort of sense if they already have lots of Rust-proficient developers. I *definitely* understand wanting to use a statically typed language with reasonably good "if-it-compiles" guarantees. (I think most of the current installers are based on really bad languages like "sh" or similar?) (Rust doesn't strike me as quite the right level of abstraction for an "installer" which is only invoked once, but that may just be me. I would have thought that static type checking would actually be the important thing here.)
Currently GC is used to allow high abstraction in the language. Maybe we can improve this in the future. I think you can disable GC [0] and use malloc/calloc in Crystal as in C, also things like sruct/StaticArray/etc using just stack. I know would be totally unsafe but is possible :) [0] - https://github.com/crystal-lang/crystal/blob/master/src/main.cr#L11
Also some time ago, Crystal didn't have a GC, see: https://crystal-lang.org/2013/12/05/garbage-collector.html
This is probably not a popular opinion on this sub, but have you tried Python as a teaching language? For absolute beginners, especially those with no math background, sometimes a simple dynamic language is the easiest to learn. 
Ah yes of course, that's how I used to do it in C++ but instead making the for loop in its own explicit function (not a lambda).
Any chance of that happening? :) Also, really glad to see all of this, I knew you were doing some but this is SO MUCH, that's really awesome.
Yes, she didn't like it at that time.
Why not Serde for this use-case?
Off topic, but can I get a show of hands: Do any of you actually desire a dedicated numpad and/or a heavily-offset trackpad on a modern laptop? Their machines are cool, but the offset trackpad is really a dealbreaker
In my job, I work on very complex software written in C. There's lot of global mutable state, weird legacy constructs and memory bugs. Debugging such complex system is nightmare. One of the things we had to do is code an installer plugin (macOS). There were many bugs. One developer spent 4 hours searching for trivial memory bug. I believe it'd be much less error-prone to write it in Rust or something similar.
Serde is more complicated and dynamic. It doesn't have so simple binary format. Also, I'm not sure I could rely on Serde keeping order of fields. This is also mostly intended to be used for existing protocols. For new protocols I'd certainly use serde or protobuf, or something similar.
Glad to see so heavy use of Rust! Especially UEFI and firmware related things, where bugs are fatal as we almost hear every day in the news! But also this build chainthing sounds interesting! Any chance system 76 is hiring a student currently doing his masters thesis who is willing to move all across Europe? :-)
&gt; though I don't think it's necessarily any worse than targeting C or C++ at that market Yes, I agree that Rust might be a good thing for developers from high-level languages, because the compiler and the language is good at showing them around systems programming concepts. However, I disagree that it is good for "Computer Science 101" or students/beginners. Rust is incredibly bad for implementing various data structures -- something you want to do as you are learning data structures and algorithms. Implementing linked lists, graphs, and many other data structures is incredibly painful (if not outright impossible) to do in Rust without `unsafe`, and gets a lot uglier than it needs to be as you need to work around the safeties of Rust semantics. Both C and Java are much better suited for this job.
&gt; Serde is more complicated Sure. Or more configurable, one might say. &gt; [Serde is more] dynamic Hmm, I don't think so. The same thing implemented in Serde would do exactly what your derive is doing -- serialize by going through each field and writing bytes, deserialize by grabbing bytes and interpreting them as each successive field. &gt; It doesn't have so simple binary format [Bincode](https://github.com/TyOverby/bincode) is an example of a Serde format that is practically identical to yours. The only difference is that it also supports serializing maps and vectors and enums and strings. You could certainly write a Serde format like Bincode but that only does structs and primitive integers. For little/big endian you would provide `#[serde(with = "le/be")]` in place of `#[le/be]`. &gt; I'm not sure I could rely on Serde keeping order of fields You can rely on Serde ordering fields the way you wrote. Bincode and lots of other formats rely on this.
Typing this on a System76 laptop. Centered trackpad as it's a few years old. I always use a mouse on left side anyways. Have been using numpad while exploring roguelikes. Reinstalled Windows onto it. Downside for me is that one of the clips on the bottom are snapped &amp; that causes the fan to bow away &amp; be noisy at times. Keys are suitably clicky Currently exploring getting a desktop, considering System76, but want an AMD gpu over Nvidia for eth mining purposes My other machine is a 2009 low power desktop with an ION gpu from System76. Running a stripped down Archlinux
How to make it apply to all crates?
This looks interesting - but what exactly does it do? are there any screenshots of the GUI? I've seen various node/patch-based editors for creating demoscene productions...curious if this is at all similar?
The types could be named as `HttpRequest` etc. then people maybe don't care so much about its crate name, then we have opportunity to not name it `http`. I don't think `http::Request` has many more goodness over `http-anything::HttpRequest`. (Note that `http-anything::` will not appear directly in the rustdocs.)
I haven't found much documentation about what the [patch] thing does. Could you describe how it works, or point me toward documentation? Thank you.
It can lead to action-at-a-distance (when the passed type has a *Cell in it), and can also lead to confusion if the type is !Copy (because a non-Copy type would be moved, but if it auto-referenced then it wouldn't be moved after this apparent use-by-value). The above RFC proposes to handle that by only allowing Copy types without UnsafeCell in them.
Yes. If you use 3D software, like Blender, it is a must.
I use my laptop's dedicated numpad a _lot_, not having one is a dealbreaker for me for a work laptop. Don't care about an offset trackpad, I adjust to it wherever it happens to be.
Note: RFC is only for `Copy` types. Which are types that are already implicitly copied without a fuss.
In Rust you know how the values are being passed, without knowing the signature of the called function. That greatly improves code clarity when reading it. The time spent on typing the code, is a fraction of time spent on reading it, in any project that is not just "one-man, write quickly and throw away".
How does this differ from [Friends of Rust](https://www.rust-lang.org/en-US/friends.html)?
It's traditionally difficult to get big name companies to post to friends pages, which is why most of these aren't on there. 
ION gpu?
Why not C then? It's easier to understand the rules Rust imposes when you understand the consequences of not following them. I suppose this depends on exactly why she didn't like Python.
Form a Japanese viewpoint, there is additionally a few big ones: Line, the most popular IM app in Japan, and Nico Douga, a popular Japanese YouTube alternative. Additionally, I'm doing most of the programming I do in Rust where I work at, the Recruit group. (Which may be familiar to Americans as the owner of Indeed, Inc.) Maybe some day in production ;)
Cool! Do you have links to the companies (or employees) talking about using Rust?
what's the compile size look like? ferris (logicoma) was mentioning that in order to make the output small enough for 64k they had to avoid using generics and libstd.
You'll want r/playrust.
Why are there no jobs at any of these companies on rustjobs.rs?
Ah, now I see that my post might have look like I'm trying to teach her Rust. Although, I'd love to eventually, it's not the case. She's learning webdev, so I was teaching her JavaScript this time. (We tried Python before she got interested in webdev.)
The second edition of the book is very good. I read it and it's been great for me starting my first Rust project.
I'm on mobile, so I'll try to help by manual review: can you explain the `while current.next.is_none() { current = current.next`? Also you try `.next` on an `Option&lt;_&gt;`. You want to unwrap that option first, preferably by pattern matching or combinator function.
Hmm, bincode's ability to work with more complicated types may have confused me, maybe also lack of endianess attributes in example on the front page. (Why they aren't explicit anyway?) How well does it work with `no_std`? I don't have to write `no_std` code now, but I might want to port it later to `no_std` architecture, so I'm trying to write as much `no_std` code as is reasonable. If I remember correctly, Serde's errors are behind boxes so they can be moved faster, or was it some other crate? Could serde calculate the lengths for me too, as my crate does? It is absolutely necessary for my use case to know the size of the struct before it's parsed. Since I need to identify my structs, I added the `Identifier` trait and derive, so everything can be configured in one place. I don't think serde could do that too. Finally, serde's error handling would mean I have to handle also errors that can't possibly happen with my crate. The only error is insufficient length, which must be checked beforehand. I plan to update my crate to require arrays instead of slices once const generics are available, so it all will be statically checked and known for sure not to panic. (Another constraint of my project is that it has to be very reliable.)
Just from a glance I don't think this is designed for size, but rather just making cool demos
I was in the same situation as you. I am not an expert but in https://medium.com/learning-rust I blogged what I learned so far, might be it helpful for you **to catch Rust basics quickly**. 🎓 **You can go through,** ▸ Rust Documentation [First edition](https://doc.rust-lang.org/stable/book/first-edition/) &amp; [Second edition](https://doc.rust-lang.org/stable/book/second-edition/) ▸ [The Rust Reference](https://doc.rust-lang.org/reference/) &amp; [Rust Syntax Index](https://doc.rust-lang.org/book/first-edition/syntax-index.html) ▸ [The Unstable Book](https://doc.rust-lang.org/nightly/unstable-book/the-unstable-book.html) - The Book about Unstable Features of Nightly Rust &amp; [The Book about Rust Macros](https://danielkeep.github.io/tlborm/book/) ▸ [Rust by Example](http://rustbyexample.com/) &amp; [Rust Cookbook](https://rust-lang-nursery.github.io/rust-cookbook/intro.html) (Developing...) ▸ [Rust Programming Course of University of Pennsylvania](https://cis198-2016f.github.io/schedule/) 🎖 👥 **To get a help,** ▸ [The Rust Community](https://www.rust-lang.org/community.html) &amp; [#rust-beginners IRC](https://client00.chat.mibbit.com/?server=irc.mozilla.org&amp;channel=%23rust-beginners) 🎖 ▸ Sub-Reddit, /r/rust (HERE) 📚 **Books** ▸ [Programming Rust](https://www.safaribooksonline.com/library/view/programming-rust/9781491927274/) 🎖 
It doesn't work for the exact reasons the compiler gives you :p [Here](https://play.rust-lang.org/?gist=4790896db77e0d964b3ee9f7cad7b1cd) is a working version. Try reading [this](http://cglab.ca/~abeinges/blah/too-many-lists/book/) guide if you want to experiment with linked lists.
Could you please give a short example i just cannot wrap my head around it...
https://www.reddit.com/r/rust/comments/637wiu/popular_japanese_streaming_site_nicovideo/
Is google really writing any actual components of Fuchsia in Rust? The linked repo is only a set of bindings to the kernel. And the other repos I could find were SDKs/bindings as well.
Which includes both editions of the Book.
Paging /u/raphlinus who is involved in Rust at Google.
Why wouldn't you want to allow that? I agree that it would be challenging to implement, but if one limits it, can't it be worked around anyway? For example: where T: for&lt;U&gt; Foo&lt;U&gt; where U: TailFoo&lt;U&gt;; type&lt;U&gt; TailFoo = U: for&lt;V&gt; Bar&lt;V&gt; where V: TailBar&lt;V&gt;; type&lt;V&gt; TailBar = ...; I don't think that banning all the possible ways in which this can be worked around is easy. We might just hope that those writing such code won't complain when it compiles slowly.
You could also put `Connection` behind `Mutex`. I'm not sqlite expert, so I can't comment on what's best. You could get BUSY error also if something else is accessing the database outside of your program. I did encounter BUSY error when I had the database opened using `sqlite3` command-line tool. But maybe I'm remembering C implementation in the days before I knew Rust.
&gt; I assume that the proper way is for each thread to open its own Connection on the same file. Yes. You can also add some connection pooling with `r2d2`, but I'm not sure it's worth it. &gt; The sqlite3 documentation says that if you do this, writing threads may get a SQLITE_BUSY error sometimes. I did, and I don't know why, because there shouldn't have been multiple writers at the same time. My workaround was to set the database to WAL mode, which is supposed to make these errors happen less often. I still have to look into why it happens.
You can use [r2d2_sqlite](https://github.com/ivanceras/r2d2-sqlite) connection pool which will handle details for you and will allow to conveniently scale up number of threads with the access to database.
Line is already on the Friends of Rust page. Check /u/est31's link for stuff about NicoNico Douga, it's basically a presentation of their employee that mentions that they are using Rust code in their mainly Erlang -based codebase.
wrong subreddit. /r/playrust
There is also the [Shared-Cache Mode](http://sqlite.org/sharedcache.html) but currently rusqlite doesn't support [Unlock Notification](http://sqlite.org/unlock_notify.html).
That contest isn't realy a "highload" competition - it's an "answering lots of queries fast" competition. Go and C/C++ have more mature async I/O support, and some solutions were probably better optimized. I know a guy who worked on organizing the contest, I've asked him and I'll see what I can find out.
Welcome to Reddit. Before posting to a subreddit/forum/community, you should check to see what that subreddit is for. This includes reading the sidebar and the rules. You should also pay attention to warnings that you're posting to the wrong subreddit. Check /r/playrust.
cheers
That's OK, it's just a personal preference. Maybe I'll have another look later then.
Exactly. As 'http'.
Because the positions are already filled? Just because a company uses Rust does not mean that it recruits Rust developers; it may prefer to recruit developers that work with other widely used languages in the company, so they can work in any team, not just the one team using Rust.
Oh, quite possibly, sorry. I am not that familiar with rust so I have this bad habit of thinking in terms of haskell where the family pattern would fail the ambiguity check. Anyway, I think streaming iterators run into problems because associated types can't express bijective relationships. Haskell allows explicit dependencies because of this but it is super ugly. Without hkt: class Iter c (l::Lifetime) i | c l -&gt; i, i -&gt; l where or class Iter c (l::Lifetime) where type Item c l = i | i -&gt; l The relationship between that and associated types seems a tad complicated, though. The paper that introduced associated types in haskell mentioned &gt; An immediate question is whether associated types can generally be encoded as functional dependencies and vise versa. This question does not have a simple answer. Duck et al. [6, Example 4] use the following class head: ```class Zip a b c | a c → b, b c → a``` &gt;It is not immediately clear how to translate this to associated types. However, as Oleg Kiselyov [17] demonstrates, the simpler class header class Zip a b c | c → a b &gt; is sufficient for this and similar applications. This second declaration can also be readily captured as an associated type synonym. In fact, it appears from inspecting publicly available code as if functional dependencies are normally used in a way that can be directly translated into associated types. In particular, even sophisticated uses of functional dependencies, such as HList [18] and a type safe evaluator, can be mirrored with associated types. So I think with generic associated types most useful things should be expressible.
Author here. spectra is designed to build demos, not intros (size-limited). It’s not an editor nor an engine. It’s a somewhat demoscene framework that gives me: - a shader EDSL, named SPSL, that turns GLSL shader into composable, module-driven piece of code (you don’t write shader stages anymore but functions and everything gets composed by spectra for you) - hot reloading of pretty much anything (a lot of things is a JSON file, so it’s fun to edit a demo in live!) - assets/resources caching - splines for animation (interpolation) - edit stuff (timeline, clip, etc.) - scene models, objects (with .obj loaders) - a very raw and basic GUI that provides, for now, a poor GUI experience :D - an audio system based on `alto` and `vorbis` to play tracks. I guess it’s designed for animation, and at a certain extend, video games (though there’s nothing about collision, network, AI, …). About 64k, I plan to cope with it, but not with spectra. Cutting off a binary to less than 100 ko requires to get rid of the std lib, jemalloc, the default main, etc. It’s something I have experience with in C/C++ but not (yet) in Rust. About screenshot, I made two demos with it. You can see them [here](https://www.youtube.com/watch?v=pYqZS1C_7PU) and [here](https://www.youtube.com/watch?v=sLT--TtkzSE). They’re currently “proof of concept” that spectra works – they’re not really impressive.
Another high profile Rust user is Parity which is very popular Ethereum implementation. As you can see, e.g. [here](https://ethstats.net/), around half of the nodes are using Parity and the other half Geth.
https://en.wikipedia.org/wiki/Nvidia_Ion
**Nvidia Ion** Nvidia Ion is a product line of Nvidia corporation intended for motherboards of low-cost portable computers. It uses graphics processing units and chipsets intended for small products. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.27
I think link to [Web Framework Benchmarks](https://www.techempower.com/benchmarks/#section=data-r14&amp;hw=ph&amp;test=plaintext) will be relevant here. In it Rust got top 4 only in "plaintext" nomination and performed much worse in others. It was discussed previously in this [thread](https://www.reddit.com/r/rust/comments/651rzz/horrendous_performance_on_techempower_benchmark/).
Why wouldn't this work? let names = vec!["Bob","Jessica","Mary","John","Mel"]; let births = vec![968, 155, 77, 578, 973]; let data: Vec&lt;(String, i32)&gt; = names.iter().zip(births.iter()).collect();
Pretty much every binary format out there has variable length packets, with the length as part of the packet. I don't think a solution for fixed length packets only is very useful except for the most trivial of cases. On the positive side, I do think that there is space for a packet deserialization library that is not bound by the serde restrictions, if you manage to figure out some way of cleanly handling variable length packets.
RemindMe! 7 days "More info on tests"
OK, `mod foo` tells rust a module called `foo` exists (you can prepend `pub` to make the module part of the public interface or add `pub use foo::*;` to import all of `foo`s items into the global namespace). Either you put the module's contents in `{ .. }` brackets directly after the `mod foo` declaration or you simply end it with a semicolon and add a file `foo.rs` whose contents will be put under the `foo` namespace. So, e.g. // in lib.rs or main.rs pub mod foo; mod bar; pub use bar::{baz, blorp}; // in foo.rs /// this is now foo::bleep() pub fn bleep() { .. } // in bar.rs // those are directly imported into the crate namespace pub fn baz() { .. } pub fn blorp() -&gt; bool { true } Now time for a short quiz: * How would you call `bleep()` from your crate? How `blorp()`? * If this was a library introduced with `extern crate ..;`, what `use` declaration would bring `baz` into the global namespace? What about `foo`?
How can I have a 2D array where I do not know the size of the array and each dimension may have a different type? For instance, these two are supported: [["hello", "world"], [1,2]] [["hello", "world"], [1,2], [3.3,4.4]] Is there out there any crate which already implements something like this? 
Seriously this format for a 'championship' simply sucks. It's basicly a contest who reinvents the wheel more. In the end handwritten http parser for this contest may do weird things to reach good performance but are no longer compatible with the original http protocol. Rust currently is not perfect for async I/O so It will struggle abit in this benchmark. Also C++ can use SIMD much better. Is there a way to see the Rust entries? In theory you could also reinvent the wheel in rust and write your own http parser which only works for this benchmark.
To get the `String`s, you need to consume the original `Vec` with `.into_iter()` or call `.cloned()` on the iterator. Otherwise you'll get an iterator of `(&amp;str, i32)` tuples. 
Rust within Fuchsia is still early-stage. There are no system-critical components written in Rust yet (ie it's certainly possible to build and boot with no Rust). I have been porting xi editor and am pleased with how that's progressing. Hope this helps.
Sorry, bad news. They didn't look into the solutions themselves, all the testing was black box, and he didn't want to share the source code with me for obvious reasons. 
Wow that really sucks...
I may have tracked down the developer though. He's @uppi on GitHub, and has a fork of tokio-minhttp on his profile. 
Seems like Python would be a better choice
You cannot have an array that has different types of items. That means: 1. You can not have an array that contains both [&amp;str; _] and [i32; _] 2. You can not have an array that contains items of different lengths. (Unless it's an array of slices)
I think they had tests with incorrect requests and weird URL encodings, so you had to follow some standards at least.
So, let's try to meet your criteria one by one: * arrays in Rust have a size known at compile time. If you want variable-length, `Vec` is your friend. * All of the members of an array or `Vec` must have the same size (otherwise it'd be impossible to index). Furthermore, multiple types are not useful in Rust, because types only exist at compile time. So depending on what you want to achieve with the items of your array, you could use a trait and implement it, which would require you to store your values as references (or with another way of indirection via `Box`). Or you could use an `enum` type with variants for the contained value types (which is not extensible, however). There is also [`Any`](https://doc.rust-lang.org/std/any/trait.Any.html), which allows you to do dynamic type-based dispatch. It very much depends on what you want to do.
It's not only simple queries, some requests can modify the data, so you had to write good concurrent data structures as well.
Yes but think about the following. A http request a format like this: * Request Line(Method|Url|Version) * Headers * Line Break * Content(Opt) Imagine all requests are send from the same client library It's very likely to assume the Header block will always have the same size. So you could just predict how many bytes you can simply ignore if there's no Content-Length header. And even if there's a Content-Length Header for Put/Post/etc requests It will be a very small change in size. If you use Hyper It will properly parse all this header and ensure everything is correct. However in theory you can probably just search for double Line break(End of header + begin of content) in something like a 12 bytes window. Does test also cover that a request can be split up? That's another huge thing. Some librarys just assume they call read once and get the whole http request so they can parse It in a sequential style. That's a dream for every optimizer but not really a real world scenario. Also a proper library will parse the url completly. If you know some static urls you can just skip the parsing and compare the string directly against It and only parse for edge cases. Unless they really covered all edge cases I think proper solutions will be always slower. You'll end up with a hacky solution. They could also just offer a protocol which is designed to perform with minimal overhead. For example Cap'n'proto Rpc. So the focus is more solving the problem Itself.
http://doc.crates.io/specifying-dependencies.html#overriding-dependencies
Shameless plug: I'm working on a llvm-sys safewrapper called Inkwell: https://github.com/thedan64/inkwell Current thoughts on Inkwell's progress: Pros: * Pretty safe overall, with ~40% codecoverage of safety test cases * Covers a lot of llvm already, though there are still missing features for sure (PRs welcome!) * Regular updates, I try and commit at least once a day if possible Cons: * Not yet released, so you have to build off master (but I keep master always compilable) * API still taking shape and subject to change (though the general foundation is there) * Only 3.7 is supported at the moment (but 3.6 and newer version support is planned) * Documentation is still scarce at the moment
I'm same as you, want to learn. I just picked up Mastering Rust and really like it so far. It is targeted to programmers of other languages so it doesn't waste time telling you what a variable is or anything, but it does do a good job of explaining everything about Rust one concept at a time, and there are exercises at the end of each chapter which are fun and challenging and drill in the important concepts of that chapter. Also, the overall project you work on throughout the whole book is a simple strategy game, so it makes it fun to work on it. I'm only half way through but loving it and would highly recommend it. Make sure you do all the exercises! https://www.packtpub.com/application-development/mastering-rust
Twitter has paid Rust development in Pants Build System. Contact stuhood for details.
I've been learning Rust, and I have to say I really like the second book. Occasionally there are some missing details, but you can usually find them in the respective section of the first book.
Hi there! Just to note that the earlier referred sync message is now implemented, it finalized to be pretty cool feature where peer servers can update the state of the root server in case root server is restarted. It is described in more detail in the latest blog entry: http://mles.io/blog 
Thanks, I will take a look! It's for a personal project, so I don't mind building off master.
About the simd thing that's just a rust fault. Don't hand wave it.
Looking at your code logic. I would drop the first check if !dir.is_dir() { return; } and just make sure your calls are correct ( or perhaps turn it into assert!(dir.is_dir()) at first ) The first ' match entry ' should probably terminate your whole program. AFAIK this can only fail if you don't have permission, the target is unmounted, or a similar program is removing directories at the same time. Either way , i would prefer if it just stopped. The two 'continue' are not needed either. You are already saying ' if { ... } else if {...} '. 
Yeah, I did drop that first check (see my macro'd version elsewhere in the comments). I disagree about the first match though, as it triggers on subdirectories as well (since the function is recursive), and the entire idea of the function is to delete as much as possible, even if there are failures. Good point about the continue statements, too.
&gt; Seriously this format for a 'championship' simply sucks. It's basicly a contest who reinvents the wheel more. In the end handwritten http parser for this contest may do weird things to reach good performance but are no longer compatible with the original http protocol. "Reinventing the wheel" is exactly what Rust is doing. It's making the argument that this wheel is safer than but as fast as the existing wheels. If Rust can't even match the performance of Go or Java, which are garbage collected, that's a bad sign. &gt; Rust currently is not perfect for async I/O so It will struggle abit in this benchmark. Also C++ can use SIMD much better. Is there a way to see the Rust entries? In theory you could also reinvent the wheel in rust and write your own http parser which only works for this benchmark. I believe that's exactly what tokio-minihttp does. If the language is lacking in certain functionality and can't optimize code as well, then that's a legit criticism of the language, not a failure of the test.
Check out the #serde channel on irc.mozilla.org. I believe dtolnay frequents there and you can get direct answers to these sorts of questions.
Which one?
I knew pants was using Rust but didn't know they were paying someone!
I think tests like this are great. There is so much speed to eek out of modern machines and with the semantics of Rust, it is better positioned to take advantage of crazy optimizations. The test might be ridiculous, but lets still kick ass.
Lots of middleware doesn't need to handle all corner cases, the clients and the servers are controlled. What they do need to handle is hundreds or millions of RPS. I wouldn't considered the test nor the hacky solutions contrived.
Thank you!
Print the value of `split`. One of the values in there is not a valid `u64`.
I used a for-loop to print the values contained by split, and double-checked with a counter. I have two integers in my original string ("2\n3"), and it printed them both.
What's the output of `println!("{:?}", split)`?
Mayhaps you're on windows and created your file via notepad or somesuch? Windows uses "\r\n" for line-endings, so splitting on "\n" will yield "2\r" and "3", the former is not parseable but since CR is a control character it generally won't print. The "debug" representation will escape (and show) non-printables: https://play.rust-lang.org/?gist=99df1d106b3f9b2e909529ec90708d7f&amp;version=stable Either way you may want to use `String::lines` which exists specifically for that purpose (and will correctly handle both line separators) rather than hand-roll a lines iterator.
When I said array, I should have said "a data structure" I want, in order to learn the language better, write a dataframe implementation. A dataframe is a table where each column may be of a different type. Then you can do things like filtering and grouping of rows. You can think it as of a small DB engine, maybe.
&gt; println!("{:?}", split) Split(SplitInternal { start: 0, end: 4, matcher: StrSearcher { haystack: "2\r\n3", needle: "\n", searcher: TwoWay(TwoWaySearcher { crit_pos: 0, crit_pos_back: 1, period: 1, byteset: 1024, position: 0, end: 4, memory: 0, memory_back: 1 }) }, allow_trailing_empty: true, finished: false }) If I understood it correctly, the issue is caused by Windows using \r\n for newlines internally instead of just \n. Is there a platform-independent way of handling this problem in Rust?
Thanks! 
Thank you, I actually just came to the same conclusion after trying what someone else suggested.
You should probably use [`BufRead::lines`](https://doc.rust-lang.org/std/io/trait.BufRead.html#method.lines).
No need to even go through BufRead, [there's an `str::lines`](https://doc.rust-lang.org/std/primitive.str.html#method.lines)
Thank you everyone, this stage is working now! Guess it's time I start working on the algorithm itself.
`BufRead` will work better if you're reading large files though.
True.
I ended up doing this and seems to work: struct Column { data: Vec&lt;Box&lt;Any&gt;&gt;, } struct Table { headers: Vec&lt;String&gt;, index: Vec&lt;i32&gt;, columns: Vec&lt;Column&gt;, }
It's okay for us to not do well in *every* benchmark. That doesn't make Rust less important or beneficial, it just tells a more complete picture. If there are things we can learn and places we can improve, that's good, no?
I'd expect some companies to fall off the list as they used Rust less, or refactored and focused on other technologies. Since the list is opt-in, it's also opt-out. We do an outreach to commercial Rust users yearly, and that's another place we could reach out and find that Rust isn't being used any more. At that point, we'd remove them from the list.
Welcome! If you have any suggestions for improvement, feel free to file an issue on github. I'd love to get some feedback!
&gt; The project is hosted at KDE 🎺🎺🎉🎉
Soooo... you guys got any graduate/junior jobs going [in the UK]? *Wink-wink etc.*
You seem to fail to understand how tokio-minihttp works. This crate depends on the httparse crate(https://github.com/seanmonstar/httparse). It basicly allows you to write a full rest api on top off It without much drawbacks. You basicly just implement the service and you can do almost everything. This crate is a push parser which does ensure the request is a valid http request. Having a valid parser and having a very performant one is a tradeoff. That's not rust fault. Http is just a very complex protocol to parse properly. See one suggestion i made earlier in the comment below. In terms of web development It's often important that every client can connect to your server. For example a line in the http request may end on CR LF or just LF. I expect a parser to properly handle both cases. Can you also tell me how Rust reinvents the wheel in that sense? I don't see alot of people building crates which are basicly only for benchmarks and wont work in the real world. Rust performs very well in the benchmark game. From my testings with Cap'n'proto rpc rust completly dismantles Go. The Techempower benchmark showed the same. Rust is very close to provide very good pattern for Async I/O with futures and await so soon It will be nice and fast(assuming It's still in nightly). Edit: Regarding the Pull parser, as soon new data arrives It will constantly retry parsing as soon new data arrives. It can return errors and actually informs the user If the request is complete. If you can actually predict when you have a full request in your buffer you can very likely increase performance alot incase the real request is splitted up. Also simply skipping blocks gives you a good performance increase.
Well unless we see the code we wont know. For Go there's fasthttp for example. By default Go copies all strings from the request buffer. fasthttp keeps references to them. However thanks to Go providing no generics working with those references easily turns out awful. Most methods provided by Go working on strings don't work on the []byte type so you also loose funcionality. Also If you don't handle the corner cases why then even use http. I always see http as a protocol where you plan to support alot clients. If you need the raw performance use something like thrift/cap'n'proto or even grpc(http2 based).
In which rank does rust fall. http://prntscr.com/gjkk7g
&gt; If Rust can't even match the performance of Go or Java, which are garbage collected, that's a bad sign. There is SIMD support, IIRC on Nightly. Obviously, Rust devs are human. They can't achieve everything. Not to mention, C++/Java are really mature, optimized languages. Hell, even Go has top notch devs, that made more languages than most of Rust core dev combined. But Rust is making strides. It's just that things these benchmarks test, aren't the things Rust core devs focused on. I mean, sure you support SIMD and Async, but then who is improving usability of language and working on making it compiles faster, or any number of things you could optimize for.
&gt; **These tweets were posted by Steve Klabnik, who is apparently on the Rust Core Team?** Steve Klabnik's *private* political opinions are irrelevant to his work on the Rust team. Like any code of conflict, the 'Rust Code of Conduct' properly applies to conduct *within* the Rust project. We're of course deeply sorry that you may feel unwelcome due to these issues, but any other choice would raise far more severe problems - it would basically be a huge distraction from the relevant technical goals we are pursuing.
In that you post more frequently on /r/t_d then here, I'm going to guess that you don't care about rust, you're just here to spread FUD. 1/10 made me reply.
Anywhere you see C or C++ plus Internet servers. There seems a fair bit of interest in scientific computing as well, but it'll take some time for that interest to translate to mature libraries. 
I think this is the wrong place to raise this topic. This subreddit is not meant for political discussion and i don't like rust or anything like it being politicized. Nethertheless, if you feel like this is wrongdoing, offer your complaint to rust-mods@rust-lang.org, as they are responsible for handling CoC violations. This looks more like a smear campaign. And now, i'd say nitpick, but i feel that this is more important: Just calling somebody out for supporting antifa is imo wrong. There are many different movements in the left, like anarchists, communists and many more (more fine grained would be lenists, marxists, whatever). About Antifa, it is not a us phenomenom, these folks exist everywhere. Some people try to fly under the banner "Antifa" for their own purposes, but in general Antifa just means Anti-Fascists. And in that regard, everyone should be an Anti-Fascist, because racism, pure hatred is not an opinion.
I'm new to Rust. Of course I haven't posted much here, because I'm new to it. What do you expect? And what does /r/t_d have to do with anything? Could you explain?
The conduct probably just applies to the Rust channels (github, irc, here on reddit, etc). It will become a bit insane otherwise and would incourage witch-hunts I imagine.
Thanks... I suppose that does make sense. Behaviour elsewhere could be considered separate from within Rust communities. Could you explain why this post is getting so downvoted though? Is it wrong for me to express concerns about this? Is the code of conduct not there for a reason? You gave me a kind response and have made me feel a bit more welcome, but another poster in this very thread seems to be attacking me.
&gt;For those unaware of Antifa, they are a US group of domestic terrorists [...] This political propaganda doesn't seem appropriate for /r/rust. In fact, this entire paragraph seems engineered to elicit an emotional response, and I can imagine exactly how that would be used. Sigh. The Rust community wouldn't be the welcome, inviting, safe-to-question, safe-to-learn awesome place that it is without /u/steveklabnik1. Or maybe it would, but it's clear to me the very positive role he has played in making the Rust community what it is today.
Thanks for the email address. Is it not appropriate to raise issues of code of conduct violations on this subreddit then?
I wasn't aware that he was here on Reddit. Why do you say "at least have the decency" as if I'm supposed to be psychic and know his Reddit username? Also, you say "the Rust developers do not condone violence", but isn't Steve Klabnik a Rust developer? And didn't I show in my OP how he has condoned violence? He clearly said, word for word, "100% okay with that" in response to a tweet about violence.
Personal opinion: I follow steve on twitter, mainly because his awesome work on the rust team. Yes, i don't agree with him in a lot of ways, yet i like to see what other parties are up to. So is politics. Does or should this affect rust? I don't think so, because by now, he has been able to differntiate between his personal views and his work on rust. It is called beeing professional. And as long as he does that, i'm against efforts for removing him.
&gt; It's getting downvoted because you're taking a single Twitter thread I'm presenting an example of a Rust developer advocating violence. Is "a single Twitter thread" not enough? Are you saying that it's perfectly acceptable for someone to advocate violence, as long as it's just every now and then? &gt; and making it seem as if the entire Rust community is defined by it. Well, he was accepted into the Rust Core Team, so wouldn't that suggest that his behaviour is accepted and tolerated, or even encouraged? Plus, if you look at all the responses I've gotten in this thread, it DOES make it seem like this is the entire Rust community. I was hoping to have my mind put at ease, to find out that the Rust community is in fact friendly and welcoming, but the replies I'm getting here are the total opposite. I'm seeing hostility and hatred, people going through my post history and attacking me because I've posted on /r/t_d in the past. Why is that acceptable? &gt; Then you spent over 75% of your post talking about antifa and politics, which no one here cares about or wants to talk about. I was explaining Antifa for people who may not know anything about them. Not everyone is in the US or follows news. And 75%? It was one paragraph and then a list of links. It was 50% at most, but probably less.
My opinion: Whether OP is right or not, the comments here are horribly toxic and I don't want to be a part of this anymore.
&gt; About Antifa, it is not a us phenomenom, these folks exist everywhere. Some people try to fly under the banner "Antifa" for their own purposes, but in general Antifa just means Anti-Fascists. OT but actually, this is not quite right. Antifa is not really a self-contained political movement, it's more like a recognizable *tactical* stance that involves a, um, 'confrontational' reaction to fascism, and more subtly, to whatever the individual Antifa activist might regard as 'fascist'. This is where it gets really confusing and where "calling somebody out for supporting antifa becomes wrong", because while of course everyone is an Anti-Fascist in some sense (plus of course, violence that's committed purely *in self defense of oneself or one's community* is often regarded as morally acceptable, and not just on the left - consider the widespread rhetoric surrounding the 2nd Amendment as a defense against political oppression), different people have very different ideas of what counts - Steve Klabnik may have very little in common with the violent anti-Trump protestors OP is taking issue with!
What exactly are you referring to when you say I'm throwing around accusations? I posted solid evidence of what I was saying. A screenshot of him saying in his own words that he 100% supports violence. How is that an accusation? How can you say I didn't do any research? I presented evidence of what he said. I stated that I was aware that he was a part of the Rust Core Team. I presented video evidence of Antifa's actions. I linked and quoted the Rust Code of Conduct. Is that not research? Honestly the responses here are just terrible and are only serving to deepen my concerns about the Rust community. Why do you people have to be so angry and hostile and full of hatred?
[removed]
Why are you just dismissing my post as a "trolling" post? Why are you just dismissing Steve Klabnik as "another left-leaning person" and ignoring the whole "100% support" of violence thing?
See [this reply](https://www.reddit.com/r/rust/comments/6zbl57/do_the_rust_developers_support_and_advocate/dmu1w5t/) by /u/coder543 &gt; Then you spent over 75% of your post talking about antifa and politics, which no one here cares about or wants to talk about. This is a place for technical discussions. &gt; The only politics you'll find here are the ones you bring with you. If you focus on technical discussion, that's all we're here for too, and you'll never have to worry. As i've said, complain to the mods team. They are responsible for handling Coc issues, the or more like this community is not. Rust should feel welcome to everyone and this is the reason why we don't bring policits on here, as politics brings (often unnecessarily) bad blood. If there is something which unites us, it is the interest in science and tech. Therefore we focuse on that, rather than trying to wear down each other.
The test isn't a commentary on any individual's worthiness as a human. It's a benchmark of how specific implementations in various languages do against each other. C++, Java, and Go have huge incumbent advantages in this space. Most recruiters and developers I've spoken to haven't even heard of Rust. If Rust intends to be a serious competitor in that space then it has to live up to its promises, namely that it's safer than any of them but as performant as C/++. Complaining that the benchmark is unfair because Rust doesn't have as good async IO or SIMD is missing the point. If Rust's lack of mature async IO or SIMD support has a huge negative impact on server-side performance then the takeaway should be that those need to be addressed for it to be a logical preference for production server applications. However keep in mind that Java and Go have performance handicaps in the form of garbage collection. Java's garbage collection is so bad that at the last Rust meetup I went to, people were swapping horror stories about major companies switching it off during peak hours or sending clients to other servers to keep servers responsive in production settings. If Rust still can't deliver comparable performance in spite of that handicap then it's a bad sign, because both languages are easier to learn and have more widespread adoption than Rust. ...we're sure they compiled with --release for these benchmarks?
&gt; Rust should feel welcome to everyone Well, that was exactly why I was posting this thread though... Because I *don't* feel welcome and had concerns... And the replies that I've gotten here have just made me feel even more unwelcome. I'm seeing people say that the Rust community is so nice and friendly and wonderful, and yet I'm just seeing hatred and hostility and personal attacks against me here.
Well i wouldn't attribute it as an own political movement, more like a general stance, like you said. I'm sorry if i didn't made that clear enough :) &gt; because while of course everyone is an Anti-Fascist in some sense, different people have very different ideas of what counts That's what i've meant with &gt; &gt; Some people try to fly under the banner "Antifa" for their own purposes But you might have a point there / expressed it better than i did. Also there might be regional nuances, which might affect perception.
But you're simply wrong when you say that. You might think you're right, but you're not. I suggest you actually go there and see what the community at /r/t_d is like instead of just going by what the alt left tells you. It's actually a very friendly and welcoming community, that engages in serious (but not always) discussions considering different views and such. They don't support violence and aren't racist. Those are just lies that the alt left spread. It's simply a subreddit that supports the elected president - what's wrong with that? Edit: You say "what [I] already participate in". Are you not making a wild accusation against me there? I invite you to go through my post history and find an example of me advocating violence or being racist or anything like that.
Now I'm just curious why the mods let this thread live but deleted the 'kill all men'-tweet thread. :-S
Sigh. Typical. So you're just going to make accusations against me and /r/t_d, and brand us as evil, without actually engaging in critical thinking or providing any evidence.
Sorry, I removed that bit. I decided I didn't want to associate this thread with the other one that I was thinking of, and didn't think it was fair to you to imply they were related. That having been said, I do still think this post reads like one big concern troll, largely for reasons that have been already been mentioned in this thread. I'm not going to repeat them here again. I'm probably not going to participate in this thread further and leave it to calmer heads and better representatives of the community.
As i've said, politics brings bad blood. Yet you raised a political topic, so expect to make people upset and get upset by their reactions. Posts/comments in technical regards are treated like everyone elses, from a technical point of view. Contributions are welcomed, doesn't matter from whom. In fact, the rust community is very diverse, as in political views. If you're able to differntiate between your views and your technical interests, you'll be welcomed!
If "someone supports antifa" makes you feel unwelcome, you might wanna check if you're a fascist or supporting fascists.
&gt;I suggest you actually go there and see what the community at /r/t_d is like instead of just going by what the alt left tells you. Time to bring in the mods and get this nonsense thread locked. The "alt left" is not a thing that exists. Period. Full stop. End of discussion. "*/r/The_Donald is so reasonable*" -&gt; Top post is currently "Can you imagine a world without Islam?". Can we please end this joke of a thread? Honestly, it was sort of convincing until you acted like t_d is some bastion of thought. I scrolled through the home page, I'd love to know what you find so thoughtful and full of critical thinking. 
[removed]
&gt; This crate is a push parser which does ensure the request is a valid http request. Having a valid parser and having a very performant one is a tradeoff. That's not rust fault. Http is just a very complex protocol to parse properly. See one suggestion i made earlier in the comment below. In terms of web development It's often important that every client can connect to your server. For example a line in the http request may end on CR LF or just LF. I expect a parser to properly handle both cases. [The Techempower benchmarks](https://www.techempower.com/benchmarks/#section=data-r14&amp;hw=ph&amp;test=plaintext) show disappointing performance for Rust frameworks as well. That leads me to believe that this isn't as simple as this benchmark not exercising the libraries well enough. The benchmark I've linked to is where Rust did the best, but that was tokio-minihttp which basically warns you that it is a limited implementation that's only suitable for benchmarks. So at least in that case, it sounds like the Rust implementation is actually the one that's sacrificing completeness for performance. &gt; Can you also tell me how Rust reinvents the wheel in that sense? I don't see alot of people building crates which are basicly only for benchmarks and wont work in the real world. Rust performs very well in the benchmark game. From my testings with Cap'n'proto rpc rust completly dismantles Go. The Techempower benchmark showed the same. Rust is very close to provide very good pattern for Async I/O with futures and await so soon It will be nice and fast(assuming It's still in nightly). Yes, Rust actually does better than C on some of the benchmarks game. But the benchmarks game is even less of an accurate representation of a language's ability to parse HTTP than either of the two tests linked to above. Rust reinvents the wheel in that it's another programming language. As I said in another comment, C++, Java, and Go are all reasonable well-established languages in production. Most people don't seem to have even heard of Rust. Asking a company to adopt Rust is generally an uphill battle (as I personally found out). There needs to be a clear advantage to doing so or it simply won't happen. &gt; Edit: Regarding the Pull parser, as soon new data arrives It will constantly retry parsing as soon new data arrives. It can return errors and actually informs the user If the request is complete. If you can actually predict when you have a full request in your buffer you can very likely increase performance alot incase the real request is splitted up. Also simply skipping blocks gives you a good performance increase. If there's a use case where the Rust implementation performs better than other implementations, then the right way to showcase that would be to make a benchmark against a few of the top contenders to demonstrate that.
[removed]
What's the advantage of this compared to [rust-protobuf](https://github.com/stepancheg/rust-protobuf)?
How so? Direct feedback is probably more constructive than vague dismissals, if you're interested in being constructive.
Here's a fresh post with my thoughts on this thread: 1. I came here expressing a concern, providing evidence, quotes, links, etc. to support what I was saying. 2. I was immediately down-voted to oblivion, had personal attacks against me, had people going through my post history, got accused of being a troll, and even called a fascist. 3. My posts in this thread have all been calm and reasonably polite. I have not attacked any of the posters here. Even when faced with attacks against me, I have responded calmly, only to be met with more hostility and toxicity. 4. Some people post saying that "there's no such thing as an innocent Trump supporter" and that they SHOULD have violence against them. Wow. 5. The moderators removed my post from the subreddit. This experience has only served to show me that this is a horrible community filled with hatred, hostility, toxicity, and bigotry. The Rust Code of Conduct, and the rules on the right sidebar of this subreddit, are both a joke as nobody here adheres to them.
[removed]
[removed]
[removed]
Indeed JP employee here. I didn't know it was used at recruit. That's awesome. We have one big project getting ported to rust too.
Politics isn't a great topic and brings the worst out of fine people. I feel like you. I'm glad that the post got removed.
[removed]
It's good to see some work being done on QML since the [qmlrs](https://crates.io/crates/qmlrs) and [qml](https://crates.io/crates/qml) crates don't seem to be maintained anymore, which is disappointing considering how intuitive the names are. My main feedback is that it would be nice if there were a "Quick Start" example. 10 pages is a lot to ask someone to read through. If you already understand QML and Qt, all you really need to know is how the syntax changes to link things into the QML. It'd be nice if cargo was the only build tool required here (perhaps build.rs could be used in some capacity?) but I don't know how much effort it would be to remove the cmake dependency.
[removed]
[removed]
[removed]
Like I said, "serious (but not always)". There are a lot of joke/meme posts too, sure. Try looking at any of the threads that are discussing an actual event, like Trump's election, various things relating to Hillary Clinton or Wikileaks, threads related to policy or what Trump is actually doing, etc. There is absolutely a lot of serious discussion.
[removed]
&gt; Try looking at any of the threads that are discussing an actual event, like Trump's election The ones where they lie to themselves about the crowd size? I can't believe you actually listed this one as an example. &gt;various things relating to Hillary Clinton or Wikileaks, The ones where they lie and cause people to show up and take a pizza join hostage with an assault rifle? Or did you mean the ones where they loudly, proudly support collusion with Russia via Julian Assange? &gt;threads related to policy or what Trump is actually doing Feel free to link one and I'll line-by-line the top 3 comments and tell you why they're fabrications or stretches such that they obscure reality. &gt;There is absolutely a lot of serious discussion. There's a lot of delusional circle jerking and lying to each other to convince one another. I *really*, sincerely hope you're smart enough to see that and are just trolling us here.
[removed]
[removed]
[removed]
Hey just letting you know your first link is pointing to the wrong crate right now :)
What do you mean? Just look at the way everyone is acting here, ganging up on the OP and attacking him while frothing at the mouth. This is disgusting.
[removed]
Eh, you know better or you would've actually cited examples. The conversation is remarkably tame with specific answers to the OPs claims and a rather clear discussion of what the CoC does and does not apply to. I don't see ganging up, I see an unpopular opinion. Also, the guy believes that aerial photos of the inauguration were faked. I'm not losing sleep.
[removed]
[removed]
[removed]
None of those "lies" are actually lies. Wikileaks has a long history of providing truthful and accurate information. It really does amaze me to see that people can be so brainwashed like this...
Yeah man, the aerial photography was totally faked, I'm so brainwashed. I'm ignoring you, you literally aren't worth anyone's time. (Disabling inbox replies to this, this isn't going anywhere and I have other things to do)
&gt; Most methods provided by Go working on strings don't work on the []byte type so you also loose funcionality. The bytes package was at some point updated to have an API that roughly mirrors the strings package. The only loss of functionality I can think of (other than immutability) is the for loop syntax over runes.
&gt; Politics are not welcome here. Well, it doesn't say anything like that under "Rules" on the sidebar. This post is related to the Rust community.
[removed]
&gt; Eh, you know better or you would've actually cited examples. Basically almost every post in this thread. I'm unsubscribing from this subreddit.
So, still not interested in an actual conversation. Okay? Sorry I tried to understand. Good luck.
Okay, but perhaps it should be made clearer then if "no politics" is a rule.
&gt; C++, Java, and Go have huge incumbent advantages in this space. Most recruiters and developers I've spoken to haven't even heard of Rust. If Rust intends to be a serious competitor in that space then it has to live up to its promises, namely that it's safer than any of them but as performant as C/++. Note that this plays into the competition too: a single Rust sample doesn't say much about the "true" performance of the language, especially because there's a pile of both C++ and Go solutions that are slower, meaning there's not really enough information to establish any sort of order between those languages. With no information about any of the source code, it's hard to draw many conclusions at all from such a tiny sample size, and the observed difference could be due to random external factors things like the Rust person not devoting as much time as the people above them. &gt; Java's garbage collection is so bad that at the last Rust meetup I went to, people were swapping horror stories about major companies switching it off during peak hours or sending clients to other servers to keep servers responsive in production settings Focusing on just those stories is somewhat misleading: the JVM GC is a very high quality GC, that generally gets high throughput (at the cost of latency), which is likely to be "optimal" for what this test is measuring: (something correlated to) the number of requests per second, not much about the distribution of time taken to return them. There's lots of cases where one does need to control latency or has extra knowledge that let's one optimise even more (by moving things off heap), but the JVM GC usually does a pretty good job. (And, who's to say that the faster solutions aren't doing the GC-avoidance tricks you allude to?)
Probably the simplest thing to do is a free function that takes a Player and x/y coordinates and returns the direction to look in. That's pretty close to your Option 3, except you don't need the CanSeePlayer trait and you return the value instead of setting it through a reference. You might wrap the coordinates in a Position struct, or just a tuple, to cut down on the proliferation of arguments, but at least in this example it's not too bad.
Seriously. This guy needs to stop playing the victim here. Violations of codes of conduct discussions are not meant for an open forum. It gets worse when politics are mixed in.
[removed]
There's more ways to organize game entities. You should look into Entity-Component-Systems. A player isn't a type in the type sense, it's just an entity that has all the components required by a player (transform, sprite/model, physics body, and maybe a player flag component). `specs` is a great example of a library that implements a high-quality parallel ECS. Give it a look!
[removed]
Well, I am sorry if I broke any rules (again, that wasn't clear to me). It would be nice if the Rust community was as you say it is, but how can I feel welcome here after this? And you say "No one here has anything against you". That may or may not be true for most of the posters, but at least one person does seem to be quite convinced that I'm a fascist, neo-nazi, racist, and whatever other labels, and that I deserve violence against me, so I don't think that's completely true...
[removed]
&gt; Complaining that the benchmark is unfair because Rust doesn't have as good async IO or SIMD is missing the point. I agree with you but I'm not sure -Y0- was complaining, but rather pointing out that these things are still being worked on and are going to take time. I think most people in the Rust community recognise that these are areas that need work. I, for one, am happy that they're not taking priority over the language ergonomics work that's being done. &gt; However keep in mind that Java and Go have performance handicaps in the form of garbage collection. Java's garbage collection is so bad that at the last Rust meetup I went to... Java actually has an excellent garbage collector, and in benchmarks like these the garbage collector isn't going to have a large negative effect. You just need to look at the TechEmpower benchmarks you posted earlier to see that. The problem with Garbage collectors (especially if they're stop-the-world) is that they're unpredictable and can cause high variance in the latency which may be unacceptable in some systems. This is where systems languages without garbage collectors (Rust, C and C++) shine.
Forgive me if this sounds silly, but it seems like you're trying very hard to fit a square peg in a round hole here. Composition in Rust is pretty simple: struct Thing { other: Other, more_stuff_here } Thing composed of Other and whatever else, it could be composed of many things. It could be composed of generics bounded by traits like CanSee, SeeFrom, Shoot, Position, MakeLook. A lot of the code up there isn't valid Rust either so that doesn't help for me as a reader. I feel like you're trying to shoehorn this to get some kind of OOP-like syntax. i.e. Object.function(). It doesn't have to be that way, it's perfectly fine to make modules that accept things like `fn move&lt;M: Move&gt;(movethis: &amp;mut M, x: i32, y: i32)` Why not a Trait that can see things, taking generic arguments, or a generic type bounded by some set of parameters. trait Look&lt;B: Position + Move + Direction&gt; { fn look_at(&amp;mut self, b: &amp;B) { /* uses dir, x, y */ } } trait Position { fn x(&amp;self) -&gt; i32; fn y(&amp;self) -&gt; i32; } trait Move { fn x(&amp;mut self, x: i32); fn y(&amp;mut self, x: i32); } trait Direction { fn dir(&amp;self) -&gt; i32; } Does something like that help a bit? I think generics and ad hoc polymorphism is your friend here. Just a suggestion.
Here's an example based on Unity's structures ([playground link](https://play.rust-lang.org/?gist=e82fb9486514b2aedc3e166967cc3a1f&amp;version=stable)) EDIT: Note that struct_base is optional in this example, but I expect it might be useful later on. However I think you can remove struct_base! entirely and just replace the calls to it with as_ref!. macro_rules! struct_base { ($type:ty, $field:ident, $target_type:ty) =&gt; { impl std::ops::Deref for $type { type Target = $target_type; fn deref(&amp;self) -&gt; &amp;Self::Target { &amp;self.$field } } impl std::ops::DerefMut for $type { fn deref_mut(&amp;mut self) -&gt; &amp;mut $target_type { &amp;mut self.$field } } as_ref!($type, $field, $target_type); } } macro_rules! as_ref { ($type:ty, $field:ident, $target_type:ty) =&gt; { impl std::convert::AsRef&lt;$target_type&gt; for $type { fn as_ref(&amp;self) -&gt; &amp;$target_type { &amp;self.$field } } impl std::convert::AsMut&lt;$target_type&gt; for $type { fn as_mut(&amp;mut self) -&gt; &amp;mut $target_type { &amp;mut self.$field } } } } #[derive(Debug, Default)] struct Position { x: i64, y: i64 } #[derive(Debug, Default)] struct Rotation { direction: i64, } #[derive(Debug, Default)] struct Transform { position: Position, rotation: Rotation } trait Transformable where Self: Sized { fn look_at&lt;T&gt;(&amp;mut self, o: T) where T: AsRef&lt;Transform&gt;; } impl Transformable for Transform { fn look_at&lt;T&gt;(&amp;mut self, o: T) where T: AsRef&lt;Transform&gt;, { println!("Changing {:?} to face {:?}", self.rotation, o.as_ref().position); } } #[derive(Default)] struct GameObject { transform: Transform, } as_ref!(GameObject, transform, Transform); #[derive(Default)] struct Player { object: GameObject, } struct_base!(Player, object, GameObject); trait CanSeePlayer: std::convert::AsMut&lt;GameObject&gt; { fn look_at_player(&amp;mut self, p: &amp;Player) { self.as_mut().as_mut().look_at(p.as_ref()); } } #[derive(Default)] struct Chaser { object: GameObject, } struct_base!(Chaser, object, GameObject); impl CanSeePlayer for Chaser{} #[derive(Default)] struct Turret { object: GameObject, } struct_base!(Turret, object, GameObject); impl CanSeePlayer for Turret{} fn main() { let mut c = Chaser::default(); let p = Player::default(); c.look_at_player(&amp;p); }
&gt; Note that this plays into the competition too: a single Rust sample doesn't say much about the "true" performance of the language, especially because there's a pile of both C++ and Go solutions that are slower, meaning there's not really enough information to establish any sort of order between those languages. &gt; &gt; With no information about any of the source code, it's hard to draw many conclusions at all from such a tiny sample size, and the observed difference could be due to random external factors things like the Rust person not devoting as much time as the people above them. A small number of middling results isn't reassuring to someone who's going to invest millions of dollars into a technology. Lack of evidence may not be evidence of a lack, but they are going to look at the top results and see that as a reassurance that it is proven to get that kind of performance, even if it requires hiring a guru. If the server is open source, then you can crack it open and figure out how they got that performance. &gt; Focusing on just those stories is somewhat misleading: the JVM GC is a very high quality GC, that generally gets high throughput (at the cost of latency), which is likely to be "optimal" for what this test is measuring: (something correlated to) the number of requests per second, not much about the distribution of time taken to return them. There's lots of cases where one does need to control latency or has extra knowledge that let's one optimise even more (by moving things off heap), but the JVM GC usually does a pretty good job. &gt; (And, who's to say that the faster solutions aren't doing the GC-avoidance tricks you allude to?) In that case I'd expect that an endurance benchmark would show constantly increasing memory usage and the JVM would eventually fall over (assuming that the server *never* switches it back on) or you'd start seeing inconsistent response times (which is mainly what people were talking about). In those cases you have a clear benchmark you can point to where you have a steady line for Rust (and presumably C/++) and not for Java / Go. And that would be a legitimate real world scenario, since there are people writing VoIP services where realtime latency is a major concern.
If you don't get better source, you can probably use https://twitter.com/stuhood/status/847922182038605824 instead. (I think that tweet is pretty clear.)
I think saying that the benchmark "sucks" counts as complaining, but I'm OK to agree to disagree on this one. :p I didn't realize that the TechEmpower benchmarks tracked the latencies, so I went back through them. The Rust frameworks fare a bit better in the first case, but for the other cases they're still further down than quite a few other frameworks in various languages. I started paying attention to the max latency as well, but this still wasn't consistently better for the Rust frameworks.
Fixed, thanks
&gt; The way you can feel welcome is by realizing ... and then moving on. If you can't do that, then I guess you will feel unwelcome I really hope you realize what a close-minded jackass you sound like, regardless of the subject matter.
If I recall correctly, I think rust-protobuf has an optimization for decoding varint that checks if there is more than 10 bytes remaining on the payload or not and works off a &amp;[u8] instead of a byte iterator if that's the case. Have you tried that?
That code of conduct everyone's talking about applies _here_, not on Twitter; and you're being an abrasive asshole _here_. Are you a representative of /r/rust? Are you a moderator here? To what level do you need this broken down? Chill out.
Ah. Fantastic!
&gt; A small number of middling results isn't reassuring to someone who's going to invest millions of dollars into a technology. Lack of evidence may not be evidence of a lack, but they are going to look at the top results and see that as a reassurance that it is proven to get that kind of performance, even if it requires hiring a guru. If the server is open source, then you can crack it open and figure out how they got that performance. Yes, I wasn't trying to justify the benchmark as support for Rust, just that there's so little information we can't really say much nor draw many conclusions. The other benchmarks people have linked to, where there's source code (etc), are more useful because they demonstrate not-so-good performance and have enough information to actually diagnose/do something with. &gt; In that case I'd expect that an endurance benchmark would show constantly increasing memory usage and the JVM would eventually fall over (assuming that the server never switches it back on) or you'd start seeing inconsistent response times (which is mainly what people were talking about). Right, but this benchmark doesn't try to observe any of this, so let alone the other things about the information being so limited, the one dimensional "Rust is worse than GC!?!" judgement doesn't seem that useful either. &gt; you'd start seeing inconsistent response times (which is mainly what people were talking about). Yes, this is the latency part of the throughput/latency tradeoff I alluded to. Many GCs, including the default in the JVM, decide that most applications (especially server ones) can take being stopped for a little while occasionally if it means the long term average (like this benchmark is measuring) is higher.
(Might want to reply to /u/sepease so they see your comment. You posted a top-level comment.)
&gt; I'm not a moderator. I thought I made that clear. Amazing... Yes, I _know_ you're not a moderator – it was a rhetorical question. And yet, despite not being a moderator, you're telling people they're categorically unwelcome in this community... &gt; I don't appreciate your attitude Yeah, the person who misses the joke usually finds it the least funny. Go figure.
&gt; I think saying that the benchmark "sucks" counts as complaining, but I'm OK to agree to disagree on this one. :p I think you must be talking about BrubLub because -Y0- never said the benchmarks sucked. &gt; I didn't realize that the TechEmpower benchmarks tracked the latencies, so I went back through them. The Rust frameworks fare a bit better in the first case, but for the other cases they're still further down than quite a few other frameworks in various languages. I started paying attention to the max latency as well, but this still wasn't consistently better for the Rust frameworks. Well, like I said, the garbage collector in the JVM (as well as Go etc.) is excellent. Most of the time you're not going to notice it. The problem is, when you do run into problems, it's much harder to do anything about it. You wind up having to tune the garbage collector or do some serious refactoring. In Rust, you're going to be controlling memory usage from the start.
This is not a reply to sepease question in any way. This is more a question to @danburkert that was sparked by reading the release note.
Oh okay, sorry then, I misunderstood!
I don't know the details, here but it could even come down to the fact there were more go/c++ contestants, putting more total time in? (rather than any language limitation)
The first section of the [readme](https://github.com/danburkert/prost#prost) gives a fairly complete list of the differences between `prost` and other protobuf implementations in Rust, but the most visible difference is that it generates extremely clean code by taking advantage of derive macros (examples further down in the readme).
I don't recall exactly how rust-protobuf's implementation works, but one of the optimizations that landed as part of 0.2.0 is a [fully unrolled varint decode from a slice](https://github.com/danburkert/prost/blob/v0.2.0/src/encoding.rs#L81-L121) based on the C++ protobuf implementation.
I would think of the turrets and enemies as like additional players. ("Mobs" being a traditional name.) They're more expensive to think about than other game objects, so they shouldn't be thrown together in the same container as doors and walls and whatnot. A modern PC CPU looks at memory in 64-byte chunks (cache lines). If the AI state is stored next to position and direction, that forces your rendering code to look at that extraneous data. Because of this a single "object" in the game design sense will be split up into different data structures, each specialized for feeding a different part of the game engine. Afaik, "object component" describes a game engine that tries to have a foot in both camps. I would prefer to not bother with the illusion of unity. Instead each task of the game engine provides information to the next. Think of the flow, inputs and outputs. AI observes game state and expresses intent. That's what the player is doing too. Then there's a referee enforcing the basic "physics" of the world (realistic or not) and some way of resolving collisions / conflicts (probably the hardest problem and biggest source of exploitable glitches). Then you have animation, then the low-level details of rendering pixels and audio samples. Alright, so with that in mind, let's focus on the AI step alone. You'll have a collection of mobs or agents or whatever you're calling them. They don't all have the same behaviour. There are really only two choices: - split up the collection so that each collection is homogeneous - decide individually what code to run on each one. This could be done with a match or statement or with some kind of function pointer ( TraitObject / closure / generator ). The main difference is how things look and feel. The match statement is likely faster, but perhaps not by a huge amount, unless the number of active mobs is high compared to the other things the CPU needs to do. I would only factor out common behaviour that is especially common. Knowing which mobs have LOS to the player seems like a pretty good thing to track (and could be it's own step in the game loop) But I wouldn't try to make "tracks the player" a common behaviour which the compiler knows and cares about. Compilers aren't smart enough to take a list of desired "things this thing should do" and write an implementation for you - because it's not a precise definition of the problem in the first place! What you can do is a visibility check, which triggers AI activity, which needs physics resolution, and which drives visible and audible output. 
The API guidelines recommended by Rust's library folks and the nursery say that Deref should be reserved for smart pointers and *not* used as a way to delegate method calls to a pseudo super class like that. It works, but it's arguably not in the spirit of Rust.
It would be good to solve any potential performance problems sooner. I think being critical of this is good so we can improve.
It's friggin' Sunday.
Hello, I really have no idea about OSs, if someone can explain me how does this work?, I think they say they have their own kernel, is it in rust? how drivers work? can it run in any machine? they coded the dm? is everything in rust? if I want to create an app for it do I need something special? I just want to know the general idea so yea, thanks.
If you really think, reaching 92 percent performance of the top is disapointing, you will be disapointed often. Libraries and frameworks always trade away a little speed for safety, usability, maintainability, correctness, generality,... If you ignore some of those aspects, you can go faster, but if the difference is only 8% it will rarely be worth it.
That's 100% a fair criticism and the deref stuff was what I was most unsure of. The problem I kept running into was that it really did make intuitive sense for me that there is a "is-a" relationship rather than a "has-a" relationship between a particular object type and an object. If I were to go full on Unity-style and model data structures as separate components that attach to the object it might get easier, but that would've added extra overhead and such that seemed inappropriate for this simple context. I didn't want to go with the practice of just adding the individual data structures on every object because that would get tedious really quickly for the full set of data structures you would need for each object in a game of any significant complexity. It would lead to a lot of copy-pasting of code and major pain if you needed to change one of the types for a field on a data structure.
Keep up the great work Dan!
This might be more in the spirit of Rust, but it looks like it could turn into a real pain for any game of significant complexity, with audio, models, behaviors, and so forth associated with each object. Duplicating every one of the fields for every object type would lead to a lot of duplicate code and would be a major pain if refactoring were ever needed. Particularly if multiple people were working on the project and went ahead and made direct access to any of a struct's fields without being conscientious of which were 'owned' by a particular trait. I'd at least want to try and group things into hierarchical data structures so that you're only having to add one or two per object, and the general behavior / code for a generic object is encapsulated within its own struct. Still, like I said, this may be overengineering if these really are the only three types that will ever be present in this game and it's only being built by a single person.
What would make it "highload" in your opinion?
Redox has a kernel written in rust. It runs on an x86_64 system. It has programs, also written on rust, that runs on the OS using redox syscalls, and normal rust code.
&gt; which is likely to be "optimal" for what this test is measuring: (something correlated to) the number of requests per second, not much about the distribution of time taken to return them Actually it measures exactly sum of latencies (meaning time to get an answer from your server). Incorrect results give you 2 seconds, so do timeouts.
&gt; Java's garbage collection is so bad that at the last Rust meetup I went to, people were swapping horror stories about major companies switching it off during peak hours or sending clients to other servers to keep servers responsive in production settings. Note that the [Shenandoah GC in the latest version of the JVM](https://wiki.openjdk.java.net/display/shenandoah/Main) *significantly* reduces GC latency, at a slight expense of throughput. See also: https://www.youtube.com/watch?v=N0JTvyCxiv8
Yes, tokio-minihttp did score 92% on the plaintext, but it isn't intended as a framework that people can use. Hyper, iron, and nickel are intended to be frameworks that people can use for writing servers. They scored at 10.1% to 1% on that same test - they scored better on some of the other tests, but nowhere near the top (for performance). If I were to write a production server for Rust today, those would be the practical choices for me to use, and I would therefore expect the upper bound for my performance to be what I see on the benchmarks. Even after using Rust for most of a year, I still don't feel anywhere near as proficient as alexchricton or seanmonstar (although I think they are probably just way better programmers than I am). tokio-minihttp is just the start of the solution, it isn't the complete solution.
I'm not sure how to test that, but to me "high load" was always more about being able to scale and less about hyper-optimizing your front end. 
&gt; It runs on any x86_64 system Mainly virtual machines. Before they implement Linux driver translators there probably won't be much luck running on actual hardware.
I feel like this is supporting my overall point, although disagreeing with a specific one. ;) If garbage collection issues can be resolved to the point that they are irrelevant to all but the most time-critical work, then the onus is on the Rust team and/or community to show that Rust shows enough of a safety advantage that gives real productivity or security demands to justify its use in a web server context instead of Go or Java. This is probably one of the biggest challenges that I see to Rust in this space today. Last I saw, Go claimed to have gotten stop-the-world times down to ~2-3 ms, which is negligible when network latency can easily be 100-300 ms or higher for international connections. There is a lot of thinking that garbage collection saves, and there also seems to be a lot of advocacy in the Rust community for keeping some of the explicitness that tends to trip people up who aren't familiar with a particular situation in the language. Last but not least, Rust also takes the position that expressiveness is better than minimalism - the complete opposite of Go. There's something to be said for being able to fearlessly refactor, but it also means that the language is inherently larger to learn, and longer to compile. Being able to show rapid progress is a big deal. Lots of management types are not necessarily good programmers if they are programmers at all, so it's easier for them to tell when "the work is being done fast" than "the work is being done right" and a ton of technical debt is accumulating that is going to bite someone later. Bar graphs likewise are easier to understand than a lot of technical arguments for why the bar graphs might not be accurate. Commercial adoption may not be the end all be all success of a language, but it's a lot easier to make time to work on something when you can make a living off of it. :)
Just as you can run Rust programs on C OS, you can run C programs on Rust OS too.
I'd actually expect NetBSD drivers to run first, because rumpkernel. Getting hold of some linux drivers is probably a must in the long run, but to get a proper collection of network card and whatnot drivers it'd be much too much work as you'd have to port the drivers individually, they're not in a form where they could be easily run without essentially running the whole of Linux beside them. Such work would only make sense for e.g. GPU drivers. Linux ABI compatibility, too, to run application blobs. I guess with proper squinting it's even possible to design an API that lets you set syscall handlers for child processes without having system/root rights, that'd be neat: You wouldn't get to run any of your own code in kernel mode, just a basic "syscall number -&gt; function call" translation layer that then gets compiled for you based upon an abstract (and very non turing complete, regular should probably suffice) description of it. (This was yet another instalment of /u/barsoap ranting his head off about the general topic "Redox as-is is insufficiently sel4")
Oh I'm absolutely nitpicking. I don't disagree with anything else you said or your larger point. I just want to make sure people understand that a GC isn't some insurmountable boogey man. "We can't use language X because it's garbage collected" is an argument that should probably be limited to just game development and embedded systems with hard realtime requirements. Horror stories about Java's garbage collection in the context of a network service probably only come from people who don't understand how to tune Java's garbage collectors.
I recall seeing some developer talking about linux driver translation layer being on the todo list down the road.
A Quick Start example should fit in a comment. So let's try. Keep in mind that the solution that this project offers does not mandate a build tool (except cargo for the rust part) but leaves that step open. If you want to contribute more convenient integration into cargo which generates the bindings from code annotations, that would be very welcome. There's only so much time in the day. Assuming you have a Qt and CMake project already and you have programmed with Rust and Cargo before. 0. [clone](git://anongit.kde.org/rust-qt-binding-generator) and build rust_qt_binding_generator and place it in your path, 1. create a bindings.json file or [copy one](https://cgit.kde.org/rust-qt-binding-generator.git/tree/templates/qt_quick/bindings.json) from the templates 2. add [a cargo project](https://cgit.kde.org/rust-qt-binding-generator.git/tree/templates/qt_quick/rust) where implementation.rs and interface.rs will be built into a static library, 3. add [CMake code](https://cgit.kde.org/rust-qt-binding-generator.git/tree/templates/qt_quick/CMakeLists.txt#n42) to run rust_qt_binding_generator in your project and include the generated .cpp in your project 4. write `implementation.rs` 5. use the generated QObject in your code Still not so short. Getting this set up is indeed a chore. Once you're done, commit it in your project and enjoy it. There is no crate to depend on. Until this generator is widespread and mature, I advise to add the generated code to the project. 
Ideally what I'd want to see is something like: **Cargo.toml** `# Some simple cargo file with dependenc(ies) here.` **build.rs** `// code to generate bindings here` **src/main.rs** `// code to create a window, load the QML, run it, and export a variable here` **res/main.qml** `// Quick example of using the variable that was set here` Execute `cargo run` from the project directory root to build and run your application. Or something like that. I haven't looked at this implementation in-depth enough, but that's easy enough to bootstrap things for someone who is brand-new to Rust but experienced with QML to expect that they might give it a shot just for the hell of it. EDIT: Many formatting fixes
I'd love to see a GRPC implementation with full tokio integration based on this.
Someone forgot the --release flag ;)
Wanted to comment, but it's so large I made a post: https://www.reddit.com/r/rust/comments/6zdvza/my_experience_participating_in_highload_cup_re/
True, but I believe this is pretty good for a contest. There's a lot of infrastructure and a lot of issues with measurements.
Anyone know why they chose u16 to represent the status code instead of the strongly typed enum that [Hyper uses](https://docs.rs/hyper/0.11.2/hyper/enum.StatusCode.html)?
Here is an alternative solution. The idea is that you define a set of building blocks, and assemble each monster at runtime. The beauty of this approach is that you can store all your monster descriptions in a json file, so you can change the monsters without having to recompile. struct Position {x: f32, y: f32 } struct Direction{ angle: f32 } struct Entity { position: Position, direction: Option&lt;Direction&gt; } If you end up with several versions of Direction, then you can replace Direction with an enum. (This covers Janus, the Greek god with an eye in the back of his head), If you insist on defining your mobs at compile time then you can replace Direction with a trait trait Direction { fn can_see(&amp;self, own_position: &amp;Position, other_position: &amp;Position) -&gt; bool; } struct Entity&lt;D: Direction&gt; { position: Position, direction: D } impl &lt;D: Direction&gt; Entity&lt;D&gt; { fn can_see(&amp;self, other: &amp;Position) -&gt; bool { self.direction.can_see(&amp;self.position, &amp;other) } }
I do not think Rust is the best at any of these categories. But it is coming fast. Investing on it is a bet. It could go well or not. I guess people on this forum would agree that it will go well.
Http infrastructure is well understood. It isn't the best but many internal stacks are based around it as the main RPC mechanism. In these scenarios, having a client lib not work against the server stack is the responsibility of the client to fix it, not the server. I believe Rust can beat Go and C++ even if they are using Shenanigans. I'd love it if we had some widely deployed transport and encoding (capn or grpc) but HTTP1 still rules and these tests have merit no matter how dumb they may seem. A naive org might look and see Go beating Rust and say the Go won. But raw RPS is only dimension out of many, I don't disagree. But we still need to show up and show our code, because as you say, the competition's code is probably pretty embarrassing. 
Unfortunatelly, we had to use our own C++ libraries, so Python was out of question (we do use scripting langs where appropriate). Our other stuff is performance-critical, so no way to switch from C++ to Python. I tried to pursue Rust, but unfortunately, working with our existing libraries showed to be more difficult than I expected. :(
Have you considered using an R* tree for calculating average? You can probably optimize it even more if you make each node of the R tree store the average across all its children. That way you'll have to make less operations for large range requests. I heard about the contest only 3 days before the end, so I didn't have time to implement a solution.
The project I'm working on does have constant size of packet per struct. The protocol is "command" in the first byte which determines the struct I need (hence the `Identifier` trait) followed by data for that specific struct. Also as I mentioned somewhere, I work only on one side of the protocol, so I can't change that to something like protobuf. Maybe I'm the only on who will ever use it, but in case I'm not, I decided to share the code in form of crate.
Sorry for the delay in getting back, I was very sick over the weekend. I've found the issue, and it was indeed my stream getting disconnected. (and yes, read_exact was being called, but by code in a crate and not my own). I've fixed the disconnecting issue, so things are working as expected. I would still like to know if using a stream over multiple threads is safe to do, or if a mutex should rather be used?
Nice! Does it support JSON serialization?
Thanks for the story! It's great to hear your thoughts about every part of the project. A minor thing in the list is on me (arraystring) so I'll keep it in mind. :-)
When you say Rust is not best in any categorise you mean Rust isn't the best language to be used in any of those categories. Or the prefered category to use Rust isn't in any of those categories.
No, for the most part just using caches (all visits of particular location in a Vec) is enough. Maximum number of visits is about 1000 and averaging 1000 numbers is almost instantaneous on modern CPU.
Would be great!
~~No, you hadn't (if you wanted to win). Updates and creates always come in distinct, 2nd, phase. I'm sure top-10 don't use any synchronization, as my colleague in top-20 doesn't.~~ I had incorrect perception he didn't use synchronization because he said that about initial version that was single-threaded, and because I myself thought data races here are acceptable since all updates come in 1 phase. I stand corrected.
Having used both for some little services at work, I prefer `prost` over `rust-protobuf` for two reasons (a) simple types that interop better (like `Vec` rather than `RepeatedField`) and (b) tiny generated files without the embedded binary descriptor. This is not a criticism of `rust-protobuf`, both those things are conscious decisions with different trade-offs: (a) `RepeatedField` and co reuse allocations, making repeated parsing more efficient and (b) allows for runtime reflection.
That actually seems like it could be optimized further. Isn't every one of those slice accesses going to trigger a bounds check?
Or perhaps even build one component as a statically linked native library, then link to it. That should get past Cargo, but i'm not 100% sure you won't get some kind of conflict from the same symbols being present twice. 
Then I didn't understand the reqirements well enough, sorry.
Submitting PR is still good thing to do.
How did you know there is a limit on a number of visits? Did you analyze the data provided for testing?
Well, I didn't. Most people I know - from chat and colleague in top-20 - just used these simple caches, say there're no "really big" data and give that estimate I gave above. The full data is here: https://github.com/sat2707/hlcupdocs/tree/master/data/FULL/data And this entire repo is about 300 megabytes.
So sadly... I so wish to any experienced rust developer would be on top5. My experience is so positive for me: * my place was 112 in rating with 791.69732 score (Евгений Татаркин) * where are I am backend developer on Python (in past PHP developer), and I don't use any compiled language about 8 years * used my own fork of `tokio-minihttp` for http layer (parse request body) * for memory db - `Arc&lt;RwLock&lt;BTreeMap&gt;&gt;` (it was my mistake - vectors would be enough for that) * alpine docker container with static musl executable * no timeouts at all !!! * I have't time to optimize and rethink data structures * my solution have less 1% of mistakes responses, as I say - have't time to solve all mistakes * I have only 2 success rating launches, and about 4-6 launches killed by oom killer - when I use `HashMap` and `CHashMap`. As I say vectors would be enough If you interested I can share my solution for you. My resolution - Rust + my poor experience with it give me a high result! Rust is new breath for me ) Sorry about my poor English. 
Some things to consider to manage by the build system: * qrc files * running moc on the generated code (it has QObjects) * having the qml editor (Qt Creator) understand your setup so the QML editing is convenient edit: formatting
&gt; Contest is bicycle-building This might be unclear for non-Russian speakers. To invent the bicycle in Russian has the same meaning as reinvent the wheel in English. When programmers are saying that there are a lot of bicycles in code that means that it contains reimplementations of freely available libraries instead of using them.
[removed]
Just my 5 cents on results 1. Best C++ was 127 seconds 2. Best Golang was 186 seconds 3. Best Java was 196 seconds 4. The only Rust is 220 seconds I personally don't consider being less than 100% slower here as 'quite poor' - especially taking in account that #1 had 261 seconds worst. I think it's more about concrete implementation - which weren't discussed in detail. But there's definitely room for improvement and optimization on Rust end, and IMO no sane person would deny that. Like SIMD and more mature async.
Google also has a point point cloud [viewer](https://github.com/googlecartographer/point_cloud_viewer) developed for [Cartographer](https://github.com/googlecartographer/cartographer). But I guess it's debatable if we can count it as "Google using Rust".
/r/playrust
&gt; The test isn't a commentary on any individual's worthiness as a human. It's a benchmark of how specific implementations in various languages do against each other. The benchmark is a commentary on the maturity of Rust's web framework ecosystem. Nothing more, nothing less. &gt; If Rust can't even match the performance of Go or Java, which are garbage collected, that's a bad sign. &gt; If Rust still can't deliver comparable performance in spite of that handicap then it's a bad sign, because both languages are easier to learn and have more widespread adoption than Rust. In lieu of your comment, look at C language. It's older than most languages listed there. It performs very well on few tasks, but for most tasks it is relatively low placed. Does this means C language can't match performance of Go/Java/C++? No. It means no one wrote a well performing C web framework. Same is for Rust. If no one programs in Rust, then no one can write a well performing Rust web framework, and if people are dissuaded from writing web framework, because Rust doesn't have good performance, it becomes a self fulfilling prophecy. 
Good job! I missed such example that when I first read about qml generator
Right, after much faffing, I thiiink what's happening is that Git is corrupting the binary by replacing line endings. I've updated my `.gitattributes` file to have bin/main binary bin/main eol=crlf and this seems to have stopped the error, at least for now.
The problem here is that there's currently only a single high-quality public R* crate ([Spade](https://crates.io/crates/spade)), and it's not optimised for insertion speed. You could roll your own if you're feeling up for a challenge, or link `libspatialindex` (but you'd have to write your own bindings), but it'd burn quite a bit of time either way.
imo , its really important to not do this. A different approach could be to have your accepting function be generic over Into&lt;MutDirection&gt; and implement MutDirection instead of AsMut
To much! I need to track down a problem in https://github.com/laumann/compiletest-rs which unfortunately are not fixed by a version bump as were expected, find a new reproduction for https://github.com/nikomatsakis/lalrpop/issues/240, finish up the improvements I made for gluon's language server https://github.com/gluon-lang/gluon_language-server/pull/10 (EDIT: And https://github.com/gluon-lang/gluon_language-server/pull/11 !), get the first stab at no_std https://github.com/Marwes/combine/pull/122 past all the tests and then finally I will be able to get back to adding universal quantification to gluon which was what I were doing before I got side tracked https://github.com/gluon-lang/gluon/pull/320 ! Unlikely to get all that done in a week though so one step at a time!
(I wrote several hundred words on this, but in the end decided to throw it away in favour of just the basic links. Then it grew again.) Some background on the design space: https://chrismorgan.info/blog/teepee-design-status-line-take-two.html. Teepee and thus Hyper (which essentially forked Teepee when it stalled because I couldn’t make up my mind on parsing behaviour) got #2. Then objections were raised to #2: https://github.com/hyperium/hyper/issues/177. I stand by my objection to the change from #2 to #1, though by this time due to enum variant scoping the backwards compatibility aspect of #2 was broken. (Associated constants are a thing now, so #2 is back to being a good solution if you don’t care about status codes outside 100–599, which we shouldn’t.) The representation of status code in the `http` crate is [the very first issue](https://github.com/hyperium/http/issues/1). And then in https://github.com/hyperium/http/pull/121 it got upgraded to associated constants, because we have them now. Summary: \#2 (C-style enum) is a great solution (though unorthodox). The value in memory is the number. But it suffers from the feature that you can’t represent stupid illegal HTTP status codes in it, and some people seem to dislike this feature. \#3 (u16 newtype) is a good solution (though I still prefer #2 in most ways). The value in memory is the number, and you *can* now even pattern match on it (I don’t think you could back in mid-2014). And associated types make it as pleasant as an enum to use. \#1 (enum with Unregistered(u16) variant) is a bad solution, because it wastes memory and computation cycles. It always was bad. It’s just that #3 was unergonomic at the time when Hyper decided it had to move away from #2. Basically, #1 (Hyper’s enum) still has all its problems while #3 has improved with the language. It looks like Hyper was already going to be changing to using a struct with associated type constants (https://github.com/hyperium/hyper/issues/1195), now I expect it’ll just ditch its status code type altogether in favour of `http::StatusCode`.
&gt; ear for non-Russian speakers. To invent the bicycle in Russian has the same meaning as reinvent the wheel in English. When programmers are saying that there are a lot of bicycles in code that means that it contains reimplementations of freely available libraries instead of using them Thanks for this Russian Idiom explanation I've been confused by it before.
&gt; When programmers are saying that there are a lot of bicycles in code that means that it contains reimplementations of freely available libraries instead of using them Presumably the metric for this would be bicyclomatic complexity?
Thank you for the thorough reply.
I added the support for the abstract namespace for unix domain socket in my [fg-uds crate](https://github.com/antoyo/fg-uds-rs) in order to use them in [titanium](https://github.com/antoyo/titanium), a keyboard-driven web browser. This replaced normal unix domain socket (file-based) and also replaced the crate I was using to check if the browser is already running: this resulted in a startup speed improvement because the crate I was using was slow. I also fix a long-standing bug in titanium: now it can open JavaScript links (like the ones on Github) in a new window. I also fixed other long-standing issues in [mg](https://github.com/antoyo/mg), a minimal GUI library (à la girara) based on [relm](https://github.com/antoyo/relm) an idiomatic GUI library: * Fix shortcuts not working for special command (i.e. was unable to use `C-w` to delete the previous word in the search prompt). * Disable the completer in commands that don't need it (like special commands and dialogs without completers). * Avoid showing the characters pressed in insert mode in the status bar (the current command should only be shown when not in insert mode). * Update the completions after using a shortcut like `C-w`. * Fix commands like `C-w` to use `String::chars().count()` because `String::len()` returns the number of bytes. Since `titanium` uses `mg`, this obviously fixed a lot of issues in the web browser. I plan to continue to fix other issues in these crates.
It's also ridiculously stupid in debug mode. Simple fn add(a: i32, b: i32) -&gt; i32 copies the arguments from register to memory and back 6 times.
What is arraystring? This is the first time i've heard of it, and a quick look at the Rust and Serde docs doesn't find anything.
Thank you for your work on this, it could end up being a pretty big deal to the ecosystem!
Why is this restriction there? Shouldn't the binding be treated like any other crate and allow multiple versions?
Does `Box&lt;Error&gt;` not implement `Error` ([playground](https://play.rust-lang.org/?gist=e339b93bc7edaa409868dacbecc2e996&amp;version=stable))? Is there a reason why `impl Error for Box&lt;Error&gt;` has not been implemented? 
After spending a good bit of time on [`uom`](https://github.com/iliekturtles/uom) (type-safe zero-cost dimensional analysis) last week I think I'm getting close to being able to support all types that implement the `Num` trait as an underlying storage type.
Looks like some parts of you release machinery need tweaking, there's a dead link to https://docs.rs/prost-build/0.2.0/prost_build/ from https://github.com/danburkert/prost/tree/master/prost-build Thanks for PROST though, it looks like something I might want to use :)
I am new to Rust and maybe I'm asking it too much. I'd like to make a very ergonomic interface to work with 2D vectors of arbitrary types (each column is a different type but the column must be of one single type). Ideally I pass a vector of vectors and it figures out how to convert. I am very very close to it, this is what I got ([playground](https://play.rust-lang.org/?gist=6d490aa58028b82ad309625ae1c71696&amp;version=stable )) I still need to use .into() on the last call to make it work, I'd wish I have not to. Also, is this overblown? Seems quite a bit of boilerplate. Thanks!
It would be great if you could share your result on github or similar. Also if you could mention any problems you had during development that would be invaluable. Did you scratch your head for too long trying to solve something where better docs or a better API could have in retrospect had save you a lot of time?
I'm sorry I don't quite know if you are agreeing or not haha
I think it's fine to use a stream from multiple threads - if it wasn't, it would not be `Send` or `Sync`, so as long as you're not using any `unsafe` code you should be fine.
A cursory search for "table" turned up [this implementation](https://github.com/PistonDevelopers/table/blob/master/src/lib.rs), which uses a hash map instead of a vector of vectors. Same kind of boilerplate to what you have though, so I'd say there really is no better way of going about it.
[A fixed-size, stack-allocated string](https://docs.rs/arrayvec/0.4.0/arrayvec/struct.ArrayString.html).
The thing is, we're engineers -- probably from all corners of the spectrum of religious and political belief. We know we'd probably disagree with one another if we got onto one of those topics. So we stick to our shared interests, where we can be productive. This is the basic rule of "getting along" in a diverse culture, which is the nature of any international effort such as Rust, with participants from all over the globe.
New-ish Rust user here. Yesterday I released my first crate, [bimap](https://crates.io/crates/bimap). It's a two-way bijective map backed by HashMaps. I've been toying around with Rust for a while and this is the first project where I've actually been thorough about testing and documentation, so any feedback is welcome!
There are definately advocates for doing so, but also people who don't want to. I'm slowly becoming an advocate, personally.
This "stutter" is directly contradictory to Rust style guides for names.
[It sure is!](https://doc.rust-lang.org/src/std/error.rs.html#305) The problem is that type `Error` is a trait object, which is not `Sized`. So what you really want is impl&lt;T: Error + ?Sized&gt; Error for Box&lt;T&gt; and *this* has not been implemented by the standard library. I can't see a reason why it's not implemented though, so it could be an error. ^(*pun most definitely intended*) It probably should be. You can always hand in a pull request. They love pull requests. 
It seems here they also need to surface the internal objects. I wish I can do DataFrame::new(vec![vec![1]]) and all convertions would kick off and work that way. Right now, I need DataFrame::new(vec![vec![1]].into()) to make it work
Is there a way to use the FromStr trait to deserialize a field with serde? I have the following struct: use std::collections::HashMap; use serde::Deserialize; use ipnet::Ipv4Net; #[derive(Deserialize)] struct NetSegments { segments: HashMap&lt;String, Vec&lt;Ipv4Net&gt;&gt;, } And Ipv4Net implements FromStr, but is located in an external crate so I cant implement Deserialize for it. Is there an elegant way to use the FromStr trait to desarialize the struct (of course I could wrap the Ipv4Net struct and implement Deserialize for the wrapper, but that would be quite ugly)?
[Here's a thread with a glossary](https://www.reddit.com/r/rust/comments/48ttky/need_for_an_index_of_termsacronyms_used_by/), with terms like HRTB, OIBIT, UFCS, ZST, and the most important of all, INHTPAMA ([article about it](https://manishearth.github.io/blog/2017/03/18/inhtpinhtpamaa/)).
&gt; The Techempower benchmarks show disappointing performance for Rust frameworks as well. https://www.reddit.com/r/rust/comments/651rzz/horrendous_performance_on_techempower_benchmark/?st=j7g91r8u&amp;sh=7aaad323 There's something very fishy going on with the benchmark. It's really hard to diagnose whats up.
Thank you very much for your assistance
Haven't been posting here much, but there's no reason why not. I'm one month in on a three month retreat at the [Recurse Center](https://www.recurse.com/) and have been focusing on [Ghilbert](http://ghilbert.org/), a project to make formal proofs more accessible. Last week I also did some work on optimizing syntax highlighting in xi editor, currently in the [rs11 branch](https://github.com/google/xi-editor/tree/rs11). This week I'll fill out more interactivity for the proof presentation. At some point soon (this week if lucky) there will be a "join point" of the two big projects, as I'll start working on a plugin for xi. I'll also attend the [irc meeting](https://internals.rust-lang.org/t/interested-in-editor-support-for-the-rls/5873) today about RLS support in editors.
That works for a start, the problem is you can't really do anything with a `Box&lt;Any&gt;` until you downcast it to the right type... which isn't really taking advantage of the Rust type system to track such things at compile time. So you may find yourself wanting to use a trait or an enum in the future to narrow down the possible types stored in the data vector.
&gt; (a) RepeatedField and co reuse allocations, making repeated parsing more efficient and The new `Message::clear` method in `prost` 0.2 will keep the allocated capacity of repeated fields in the message, so subsequent `Message::merge` calls will reuse the allocation.
It does not, nor do I have plans to implement that. I figure it's easy enough to add SerDe annotations to the structs and do it that way.
Thanks for the heads up! That's something I noticed yesterday as well, tracking it at https://github.com/danburkert/prost/issues/28.
Thanks! But is `impl&lt;T: Error + ?Sized&gt; Error for Box&lt;T&gt;` equivalent to `impl Error for Box&lt;Error&gt;` for my use case? I thought the `T` version only worked on types and not the trait object.
Who? Where? Due to the extreme difference between the Linux and Redox driver API, this is unlikely to happen. We will likely rewrite the drivers in Rust, using Linux source as one of several references
I am playing with the [public data](http://opendata.cern.ch/collection/ALICE-Reconstructed-Data) of the CERN based [ALICE](http://aliceinfo.cern.ch/Public/Welcome.html) experiment. The idea is to recreate some published results in Rust. The data is published in the ROOT binary format, so its not quite possible (yet) to make this a pure rust endeavor. However, so far, I have managed to drop the dependence on all ~5M lines of ALICE specific code, which is very rectifying by itself. The IO side used rayon and futures::mpsc to send a stream of individual "events" (data collected for one particular particle collision) to the analysis thread. This is my first stab at Rust, and the api needs some cleanup before I can publish it in the next week(s).
Hello, The closure you pass to [for_each]( https://docs.rs/futures/0.1.14/futures/stream/trait.Stream.html#method.for_each) must returns a type implementing [IntoFuture](https://docs.rs/futures/0.1.14/futures/future/trait.IntoFuture.html) ... try instead of returning Ok(() , return futures::ok(()) ? 
The closure passed to `for_each` [expects a `Result` to be returned](https://docs.rs/futures/0.1.14/futures/stream/trait.Stream.html#method.for_each) - you need an `Ok(())` after your `println`.
It's because `for_each` expects a function that returns a `Future` (or `IntoFuture`), whose `Item` is `()`, but instead you're giving it a function that returns `()` directly. If you do want to keep the `for_each` (because this is placeholder code that'll eventually do complicated things), then `Ok(println!("{}", line)) // Note the lack of semicolon` is a valid return value, I think, because `Result` implements `IntoFuture`. Alternatively `wait` iterates the stream in order in blocking fashion, which can be useful when debugging/writing testing code. &gt; This started happening once I added BufReader::new, but if I didn't have that, io::lines would complain about conn missing the BufRead trait. That's somewhat misleading, the `BufRead` trait error was probably just hiding the other error (because the compiler couldn't get any further.
They're not equivalent, no. This: impl&lt;T: Error + ?Sized&gt; Error for Box&lt;T&gt; implements `Error` for *every type* of the form `Box&lt;T&gt;`, so long as the `T` in question also implements `Error`. This: impl Error for Box&lt;Error&gt; only implements `Error` for one type: `Box&lt;Error&gt;`, which is the boxed trait object. &gt; I thought the `T` version only worked on types and not the trait object. The trait object type `Error` is also a type. It's a bit special, but it is a type all the same. 
&gt; A fixed-size, stack-allocated string Doesn't arrayvec support serde?
But not deeply. The point of `RepeatedField` is that it actually keeps around nested allocations. Its implementation is the equivalent of `length: usize, elements: Vec&lt;T&gt;`. On `clear`, `length` is set to zero, but if for instance `T == String`, it will also keep the `String`'s allocations, to be reused on future `merge` calls. Makes sense? See [#84](https://github.com/stepancheg/rust-protobuf/issues/84) for more details. It mirrors the implementation from C++ for what it's worth.
You can write the functionality in the trait and only require a method that tells you how to access the struct you are composing.
Is the quote of the week still a thing?
I find myself working around this a lot. I wonder if we should implement `IntoFuture` for `()`?
IIRC it was just a side comment on some of these progress reports of redox, might have been the one about becoming self hosting? Anyways, i might have just misunderstood.
Do you think drivers written for Redox could potentially be portable to/from sel4 with minimal changes?
Makes sense, thank you!
Thank you :D
Thank you :D
:)
I'm not sure. `decode_varint_slice` is only called from one place, and it checks that none of the slice accesses will be out of bounds: https://github.com/danburkert/prost/blob/v0.2.0/src/encoding.rs#L68 , so the answer to your question is: 'depends how smart the optimizer is'. I haven't checked the assembly or benchmarked it to find out, yet. I'm reluctant to use unsafe slicing there since I want to keep unsafe to a minimum (it's currently only used in [one mostly unavoidable place](https://github.com/danburkert/prost/blob/v0.2.0/src/encoding.rs#L22-L51) in `prost`).
ah, so this is what we're spending our compile time on
It does! But it was added recently, and I also forgot about that. I spent the weekend with serde in petgraph.. my head is spinning from serialisation apparently.
&gt; But not deeply... Yep, that's true. Haven't thought about it deeply, but perhaps `prost` could add an all-in-one clear+merge operation that reuses nested allocations (call it `decode_into`?). The performance difference here isn't something I've found to be significant in my applications yet, so I haven't thought a lot about it. I think the perf improvements from inline struct nesting, static dispatch in decode/encode, etc. more than make up for this difference (plus it's great for ergonomics). Edit: I should clarify: the main usecase which motivated `prost` _can't_ reuse messages except in very trivial situations, so even if this optimization were significant, I wouldn't have observed it. I've [created an issue](https://github.com/danburkert/prost/issues/29) to track this. Thanks!
Me too! My use-case for `prost` is not for `grpc`, so I won't be working on it, but it would be a great project. `prost` _is_ designed to be easy to use from Tokio/async contexts (mostly by virtue of using the `bytes` crate), so it should be possible.
Docs still need some proofreading after 0.4 api changes I see.
for match in expr { } could be a way to write it that actually looks kind of understandable?
Yep, I submitted it.
How is the state of TFS? Is ticki on holiday?
This was just added to nightly: https://doc.rust-lang.org/nightly/std/vec/struct.Vec.html#method.drain_filter
Link to documentation on crates.io doesn't work. https://docs.rs/prost-build/0.2.0/prost_build/
I plan on working on this shortly after the [HTTP/2](https://github.com/carllerche/h2) library stabilizes.
[Images of new login screen and editor menu](http://imgur.com/a/4E7sz)
Holy cow
Cows can hear lower and higher frequencies better than humans.
[Last week](https://www.reddit.com/r/rust/comments/6xyqdw/whats_everyone_working_on_this_week_362017/dmk8a7b/) I talked about starting a Rust game engine and my reasons for doing so. At the start of the week, I already had working submodules for `Renderer`, `Camera`, and `Scene`. I had two major accomplishments for the week. Update to scene handling: - Switch scene representation from a Map to a Tree (using `id_tree` crate for this currently, so far I'm really happy with it! - Scene traversal during rendering now takes into account all ancestor transforms. So a node is transformed relative to its parent node, etc. all the way up. - Cleaned up the entire `Scene` API to be easy to use and hide implementation details. More on API cleanup... API refinement and cleanup: - Switch from a `pub use` style of exposing submodules to a explicit re-export of necessary things. This marks the transition from a submodule of my game project to a crate that's focused on being easy to consume. - Due to the fact that window creation is handled by `gfx` and the graphics context is tightly coupled to window creation, previously window creation and initialization steps were a static method on `Renderer`. However, because window creation requires creation of an events loop (in the case of `Glutin`) and so on, it makes more sense for this to not be a concern of the `Renderer`, so I moved it to a separate builder struct. The resulting API is slick and easy to use. Goals for this week are - Allow scene to handle Text nodes and Renderer to change rendering based on node type (3d or text). - Get the project on GitHub and start getting Readme, Roadmap, Issues, and more up.
If you don't want to rely on nightly at the moment, the `odds` crate has a utility for this https://docs.rs/odds/0.2.25/odds/vec/trait.VecExt.html#tymethod.retain_mut
Oh, so I did overlook it! See it now. I guess what I expected to find is guide-level "types in this library support serde and here's an example". Might be a fail on my part, too - not paying too much attention to the docs.
hmm that's a lot to digest, thanks
Isn't there an Rfc to coerce () into Ok(()) as well?
Thanks! I thought it might be similar to bikeshedding at first.
Doing that is blocked by [this issue](https://github.com/rust-lang/rust/issues/36262) at the moment.
Working on my post regarding generators. This is turning into a long one but I'm hoping it'll be a good resource for those wanting to use them
There was a [PR that](https://github.com/rust-lang/rust/pull/43351) that added a test that avoided it for small types, though it got closed, maybe it could be revived. That might help LLVM get rid of the complex code much more easily for cases with small values instead of having to optimize the complex swap. Though, it would even better if rustc could eliminate the dead branch before it even gets to llvm.
Whether or not it's safe to share a connection between threads is determined by a mix of compilation options and runtime options. https://sqlite.org/threadsafe.html It CAN be safe to share the connection between threads. That being said, if you're doing two separate things, I would recommend just acquiring different connections. The thing about sqlite is that it's not very concurrent. For the most part, if one connection is in the middle of a long running write-transaction, and has overflowed it's in-memory buffer and started writing to the actual file on disk, no other transaction can be writing to the file. WAL mode (or shared cache mode if you KNOW you're the only process accessing it) can help by allowing reading at the same time as writing, but you still won't get 2 write transactions at the same time. The way you handle concurrent writes is: 1. Set a sqlite busy timeout for something like 5 seconds. This will randomly sleep and retry many times before finally giving up and returning SQLITE_BUSY. If you're only running normal short write transactions, this will pretty much eliminate all problems with SQLITE_BUSY. (PRO TIP: If you don't trust the compilation of sqlite, or you compiled it yourself, make sure it was compiled with usleep support, or else it only waits full seconds at a time! This causes sqlite to only try 5 times during a 5 second timeout, which will increase your SQLITE_BUSY errors. This is actually very easy to accidentally do.) 2. Consider using BEGIN IMMEDIATE for write transactions and you'll be sure that your transaction will fail early, and won't fail with SQLITE_BUSY half-way through, or at the COMMIT. Personally, I never understood the point of connection pooling when it's just a local file. I've never found connection setup to be a performance factor in sqlite. I've also had bad experiences of people accidentally executing connection-specific PRAGMA's or leaving transactions open on the connection pool.
You are right though -- serde is supposed to be mentioned in the feature list here and it is not: https://docs.rs/arrayvec/0.4.0/arrayvec/ Edit: Fixed in 0.4.1 :-)
Ooo I both want this really badly and worry about it being too magical :)
Same and same.
I am very interested in your work. Please post it when you get your Github page up.
Is it on a hot path? I was thinking an iterator might be more useful instead - I read somewhere that this would avoid the bounds check. However I could be completely off the mark since I haven't run code through a profiler.
Hmm. New benchmarks are up that show much better performance, but it still seems lower than it could be. https://www.techempower.com/benchmarks/previews/round15/#section=data-r15&amp;hw=ph&amp;test=json - Steven
I'll post here for sure. I aim to have it up tomorrow or the next day.
There's actually C entries into the Techempower benchmark that perform quite well, mainly [h2o](https://github.com/h2o/h2o) - Steven
Yes, ticki is on holiday. TFS is still in development, and is not ready for use.
Like I said in another reply, the library should not really contain what amounts to an optimizer's output, written by hand.
Wait, shouldn't that sample code not increment i in the case that you remove an object? (I would file a bug but I'm on mobile and that would be painful) Edit: filed anyway :P https://github.com/rust-lang/rust/issues/44499
Serde actually provides some attributes that give you some control over codegen, though you have to do a little bit of the legwork yourself. The attributes are listed [in the documentation](https://serde.rs/attributes.html) but the one we're interested in is `#[serde(deserialize_with = "path")]` which lets us write our own custom deserialize function: // add this import use serde::Deserializer; #[derive(Deserialize)] struct NetSegments { #[serde(deserialize_with = "deserialize_ip_map")] segments: HashMap&lt;String, Vec&lt;Ipv4Net&gt;&gt;, } fn deserialize_ip_map&lt;D: Deserializer&gt;(de: D) -&gt; Result&lt;HashMap&lt;String, Vec&lt;Ipv4Net&gt;&gt;, D::Error&gt; { use serde::de; // A little helper function to parse `Ipv4Net` or convert the error with helpful information fn parse_ip&lt;E: de::Error&gt;(string: String) -&gt; Result&lt;Ipv4Net, E&gt; { Ipv4Net::from_str(&amp;string).map_err(|e| de::Error::custom(format!("{}: {:?}", e, string))) } let map = HashMap&lt;String, Vec&lt;String&gt;&gt;::deserialize(de)?; // a lot of type hinting is necessary because of the `collect()` calls map.into_iter().map(|(k, v)| Ok((k, v.into_iter().map(parse_ip).collect::&lt;Result&lt;Vec&lt;_&gt;, D::Error&gt;&gt;()?))) .collect::&lt;Result&lt;HashMap&lt;_, _&gt;, D::Error&gt;() } Of course, this isn't the most performant implementation possible, but it should work. It might need some fixing up in some of the type declarations though, and there might be an unpaired parenthesis or angle bracket somewhere.
I have this concept that the OS kernel do syscalls and stuffs to the hardware a certain way, this forcing different compilers (e.g. windows, Linux), so is there a compiler for this Rust OS kernel? or other compiler works?
Just as Linux kernel is compiled by gcc, Redox kernel is compiled by rustc. There is no special separate compiler.
I think it would be possible for a sel4 based system to provide Redox driver syscall emulation, yes. It is unlikely though.
To expand on the other answer, you're reading the whole primes.txt file into memory before processing it. That's not a problem if the file is small, but it's something one should always avoid when ultimately processing the data in a stream. You should use BufRead::lines and parse the lines as they come instead of storing them.
Won't using the Linux source as a reference make it difficult to maintain a non-gpl kernel?
Is there a lock-free construct for immutable data in single-writer, multi-reader scenario where the readers should not be blocked by the writer (and writer not be indefinitely blocked by the readers)? I am looking at implementing a construct that can allow multiple reader threads to read an immutable struct from an atomic pointer and do some short-lived processing based on the parameters in the struct, and where a single writer thread can swap out the struct for a newer version and wait/spin until all readers are done with the old version and then free it. However, I suspect someone has already written something like this, tips? (I expect the writing of the struct to be a rare event, so it's OK for readers to retry getting the latest struct if they happen to get it just as the writer writes the new version)
https://github.com/sevagh/wireshark-dissector-rs/tree/master/epan-rs I'm trying to improve my Rust-in-Wireshark FFI method by using bindgen with the Wireshark EPAN library, so that I can write everything in Rust (and not just call Rust from the C packet-*.c file like the first method).
Someone should open a bug on travis-ci: ###My library isn't broken, it just takes a really long time to build and ask for a feature to detect whether or not a build will halt. Using this library as the example.
It's not difficult with an indexed loop. Look at the standard library implementation of `retain` - it doesn't even require `unsafe`.
The drivers using the Linux source as a significant resource may have to be GPLv2. The kernel, and other drivers, have no such restriction. Each driver is its own licensing domain.
Just one good advice: Use rust nightly and the futures-await crate. Makes life MUCH easier!
Virtualize Debian, pick *any* C api, and play around with it. With the standard library and bindgen the entire world is your oyster. Last night I was curious about X11, so I grabbed a couple old tutorials and started hacking from there. 
Just to learn the ropes, try a command line based game of hangman. It'll let you go over a lot of the rust concepts in a practical way but also has a small project scope so that you can start and finish and move on to the next thing.
Thanks for the advice, but I tend to dislike nightly as it requires constant updating and features might be unstable or breaking.
I'd even encourage just copying the code for that.
&gt; and ask for a feature to detect whether or not a build will halt. This is probably an instance of the [halting problem](https://en.wikipedia.org/wiki/Halting_problem) brson recommended the use of `#![recursion_limit = "1024"]` for [error_chain](https://users.rust-lang.org/t/announcing-error-chain-a-library-for-consistent-and-reliable-rust-error-handling/6133) which might have some relevance for this kind of issue. Also note that you can always spin up an AWS box with dozens of vCPUs to do compilation. This seems to really help for the parts that can be parallelized, but there seems to be a heavy dependency on single-threaded compilation for the last stages of compilation (even codegen-units did not help speed it up). In this particular case, it sounds like this is very memory hungry, so that may be another reason to use a cloud service with powerhouse machines.
To be honest, if I had to choose, I'd pick hand-editing QML over having to interact with CMake and C++. (A legacy of the many years I spent using PyGTK. Glade's ergonomics for custom widgetry in Python apps are *much* worse than Qt Designer's.) At present, I generally write my Qt applications using either PyQt exclusively (if I can't find a neat way to split them into a frontend and a backend) or PyQt+[rust-cpython](https://github.com/dgrunwald/rust-cpython)+Rust with [setuptools-rust](https://github.com/fafhrd91/setuptools-rust) and [just](https://github.com/casey/just) to provide unified build and test commands. (Though, admittedly, I've also been putting off the projects which would be best served by QML and using Python as sort of a "QML for the QWidget API" in my Python+Rust projects. I take "must feel native on a KDE desktop" much more seriously than Qt devs do.)
Great starting point for a beginner 
Is `docs.rs` behaving oddly for anyone else?
I've merged the support. I'll aim to make a new release soon.
&gt; You should look into Entity-Component-Systems This is genius! Thanks for that man, this is really what I was looking for.
Seems to be working fine for me.
Lifetimes are currently lexical. If you borrow something, it stays borrowed until the end of the scope, even if it doesn't strictly need to be. You can hack around it by manually introducting a new scope like [this](https://play.rust-lang.org/?gist=adee18da5f9a87a0be25098232478953&amp;version=stable), and there's ongoing work to make non-lexical lifetimes part of the language.
I want to pass to a function a Vec&lt;tuple&gt; where every tuple in the vector would contain the same types and be of the same lenght. But different calls to the functions would be done with vectors which contain tuples of different types and lengths. I do not know this beforehand (reading info from a CSV file), is this possible in rust or I'm trying something too dynamic?
&gt;&gt; and ask for a feature to detect whether or not a build will halt. &gt; This is probably an instance of the halting problem I suspect that this was the joke. :)
You're right, this sort of thing isn't as simple in statically typed languages as it is in, say, Python. You can't put different types of tuples directly into a single vector. However, you could get close by using an extra layer of Vec instead of tuples. So for example, maybe you could parse the CSV file into a `Vec&lt;Vec&lt;String&gt;&gt;`, with the outer vec containing rows and the inner vecs being the columns of a single row. If you want the values of each column to be different types and not just `String`, you might need to define an `enum` with a variant for each type, so then you'd end up with some kind of `Vec&lt;Vec&lt;MyEnum&gt;&gt;`.
Wow - this is a wonderfully simple example of the limitations of lexical lifetimes, and the benefits of the upcoming non-lexical lifetimes feature. I've never really understood the problem, but this helped a lot.
Do students/interns/coops get to do rust at google? I've been looking for opportunities to work on rust outside of my personal projects.
You can use [Any](https://doc.rust-lang.org/stable/std/any/index.html) to emulate dynamic typing, though it can get verbose.
Sources https://github.com/estin/travels-task I don't want public it, because code so stupid and ugly. So many things I do`t understand yet. I spent a lot of time for: * enable `tokio-minihttp` to parse request body (yep, I know request body not interested thing for `tokio-minihttp`) * `serde_json` and `null` fields where `field_name: Option&lt;String&gt;` must be invalid if passed in JSON like `{"field_name": null}` * find a way to close connection immediately after write response to the socket, without luck... I hope to decrease usage of OS resources to handle many opened and not used connections, client must close this connections by `Keep-alive` header, but I want to close connection by server first * parse url query string by url crate by path + query string without any url base, something like `Url::parse("/some/path?some=query")?` And other part of time spent for trying: * remove unused parts of `tokio-minihttp` * `HashMap` and `CHashMap` to store packed data (already serialized to JSON strings), for response by raw data and to not serialize it on each requets - **Out of memory** * `HashMap` and `CHashMap` to store unpacked data (structs represented related JSON entries) to decrease memory usage - **Out of memory** * [cask](https://github.com/andrebeat/cask) to decrease memory usage - **Data process time limit exceeded** * and then I replace `HashMap` to `BTreeMap` and now I wait my T-shirt as prize because my score less then 1000 ) And my thoughts for optimization are: * change `Arc&lt;RwLock&lt;BTreeMap&gt;&gt;` to `Arc&lt;RwLock&lt;Vec&gt;&gt;` or `Arc&lt;Vec&lt;RwLock&lt;_&gt;&gt;&gt;` * store already packed data as `(data_len, data_json_string)` for skip serialize to json and calculating result string size * `target-cpu=native` ? 
The easiest example to show off NLL is just: let mut a = Vec::new(); a.push(a.len()); // this breaks let mut a = Vec::new(); let b = a.len(); a.push(b); // this works 
Could you please elaborate on the "Virtualize Debian" suggestion? Also, what part of X11 are you playing with? It sounds interesting and I'm itching for a nice project.
I started with writing a simple sudoku puzzle solver. It was a good introduction to the borrow system, arrays/vectors/slices, basic loops and debug messages. The only thing it didn't really cover was lifetimes.
Write a CHIP8 emulator. It is easy, there are tons of resources and examples and extremely funny way to learn/improve rust skills.
A few - Amit Levy and Tristan Hume have been interns working on Rust recently - but it's not something I would count on.
Thanks, this pretty much confirms what I had gleaned from Googling. I now have one connection per thread, plus WAL mode, and it's working nicely. rusqlite already sets the busy timeout to 5 secs, so no problems there. I didn't know about BEGIN IMMEDIATE, but it looks useful. Will try it.
I feel like I'm compiling PyPy.
The things that I know of are: - `MCSubtargetInfo::getCPUTable()`, used for `rustc --print target-cpus` - `MCSubtargetInfo::getFeatureTable()`, used for `rustc --print target-features` and the unstable `#[target_feature]` attribute. - Support for the JavaScript targets (emscripten). - ... and a bunch of backported fixes from LLVM trunk. Also, since you can currently build Rust with any LLVM &gt;= 3.7, choosing an older LLVM will of course miss out on any general enhancements that were made since then.
Awesome.
Working on more [tarpaulin](https://github.com/xd009642/tarpaulin), was very busy with other stuff last week but finally read enough docs and example code to start actually working on my new syntex_syntax based line filtering. Still hopeful on finishing the feature by the end of this week but we'll see.
(I've done exactly that)[https://play.rust-lang.org/?gist=6d490aa58028b82ad309625ae1c71696&amp;version=stable] And works well to add data by column. But for ergonomics, I'd like to add a way to add data by rows. As that's a very natural way of adding data. For instance reading from a file would happen per row, I do not want the user to do work of converting rows to columns. 
That's not the easiest 'cause it doesn't *quite* explain as well what's going on.
That's not actually solved by NLL- it's a special case that's piggybacking it's way into the language with NLL.
Probably means running Debian in a virtual machine of some kind. Perhaps VirtualBox or libvirt. Debian can be installed to be incredibly bare bones, and you have the widest array of software available from a few commands.
&gt; `trait foo { type bob : brian; } impl ...` &gt; &gt;is VASTLY more efficient than. &gt; &gt; `trait foo { type bob; } impl ... where ....::bob : brian { ...}` why? &gt; I was pretty desperate before I found that, I was running out of RAM on my 32G machine (which I bought after running out of 8G...) even before adding the mandelbrot iteration. Did you buy 32Gb of RAM for this? Respect.
Propose and implement [rust cookbook recipe](https://github.com/rust-lang-nursery/rust-cookbook) or contribute to any [libz blitz](https://internals.rust-lang.org/t/rust-libz-blitz/5184) project for fame and glory ;) ! Quality mentoring guaranteed.
That explicit reference is so important. At my work we have some C++ products and in them we forbid using references to pass arguments to functions that mutate the argument since you can't tell it would happen from the caller side--to mutate you must pass a pointer with the address operator. The explicit reference in rust provides a similar clarity so you can know caller side not only whether the argument will be mutated but also whether it will be moved.
Is there any other type of cows?
You might want to try out [cargo-expand](https://github.com/dtolnay/cargo-expand). It'll print out the expanded macro code. You can also just run `cargo rustc -- --pretty=expanded` to see what gets output by the macro code.
The macros only generate some glue to make futures and generators work together, the state machine is not built by the macros but rather by a bunch of nested calls to poll() getting inlined. So no, you won't be able to see it, but you can take a look at the methods on Future and imagine what they look like when everything is inlined.
This has `*** draft ***` at the top; is the author intending for it to be circulated widely yet?
Right... I realized this about 20 minutes after I shared it, but Dave told me it's no problem.
That is insane. I like it!
unfortunately, unless you're very good at reading MIR there is no way to see what's being generated. Neither `async!` nor `await!` are responsible for generating state machines, Generators are. You can read more about generators and state machines here: https://doc.rust-lang.org/nightly/unstable-book/language-features/generators.html `await!` is a very simple macro, that converts yields into Future's NotReady: https://github.com/alexcrichton/futures-await/blob/master/futures-await-macro/src/lib.rs `async!` is a little bit more complicated, but basically, converts this #[async] fn do_work(self) -&gt; io::Result&lt;u32&gt; { ... } into something similar to this fn do_work(self) -&gt; impl Future&lt;Item=u32, Error=io::Error&gt; { move || { ... } } 
Maybe if we understand what you are using it for we can help out better? "A tuple with some number of fields containing something" is not a very useful type... not much you can do with that in any language except dynamically match on type or something. The type should be based on what you want to do with it, so that you know that the function contents are valid for all possible values.
The _state-machine_ is generally only generated after the LLVM has done a full optimization pass. That being said, from my experience this is more a marketing term you'll actually see. `cargo rustc -- --emit=asm -Cop-level=3` Will dump the assembly in human readable form.
Pretty sure we don't allow jokes here :trollface:
I dislike nightly too, but if you want to use it without constant updating, pin the nightly version by creating a file `rust-toolchain` in the root of your project with a value like `nightly-2017-09-11`, and make a commit including it. Then run `rustup install nightly-2017-09-11`. Whenever you do `rustup update`, it won't update this toolchain. If you need to update the Rust version, change the `rust-toolchain` file and do `rustup install new-toolchain` (optionally removing the old one). One benefit is that everyone will be in the same toolchain so debugging problems might be easier. When the need arise, one can update the toolchain manually. It looks like [Servo uses this model](https://github.com/servo/servo/blob/master/rust-toolchain) for example.
If you end up playing with a libc-based API, we'd welcome a Rusty wrapper around it into [`nix`](https://github.com/nix-rust/nix). If you aren't certain exactly what it should look like or if it would be accepted as a PR, we'd love you to raise an issue as well to discuss it before you do too much coding.
I wonder: does this amplify performance bottlenecks with the Rust compiler in a way that would be helpful for identifying and testing fixes?
Thank you all - it's much appreciated. I'll take a look at all the options presented.
If you have an interest in systems programming or allocators, there are [plenty of issues](https://github.com/ezrosent/allocators-rs/issues) here to tackle relating to pure-Rust allocators.
The project I'm working on is a retro game engine / framework / what-have-you. I started on an IBM PS/2 (as old as I am) running DOS 6, QBasic, a shareware assembler, and the occasional game console emulator. So while I'm not working on an emulator of any specific system, I *am* looking to provide abstractions similar to what existed then. Tile-map backgrounds, sprites, phase modulation audio, palette rotation, that kind of stuff. Go, Rust, and assembly are good modern equivalents to Basic, C, and assembly, so those are the languages I'm targeting. The current problem I'm trying to solve is vertical refresh sync on X11. X11 wasn't really designed with the idea of frames (redrawing the screen tens of times per second must have sounded insane back then) but I suspect that DBE can be hacked to do what I want. Sync extension doesn't have a frame counter unfortunately (there's an idea for an Xorg PR). As for the first question, I'm running Debian Jessie on Virtualbox. Situation requires me to run Windows, but I like Linux so much more for initial development.
There's a github repository called spmc-buffer which appears to be just that. Your key words are lock free single producer multiple consumers.
+1 on this! If you really wanna get crazy, write a gui for it. Insane? Write custom chip8 software. I've done that lol
After reading the first few words, I was like "Ropes? I wouldn't recommend doing advanced datastructures for the first thing in Rust..." until it came to me :P
In my experience, the best way to learn a new language is to implement something you've already implemented before in the new language. It means you already have a good idea how to solve the problem in general, and it lets you focus on the new language.
Unless I'm misunderstanding the question, retain() itself should work: let mut v = vec!["a","adj","aiodfj", "aji", "jjj", "blah"]; let mut parsed = Vec::new(); v.retain(|&amp;e| { if let Some(k) = parse(e) { parsed.push(k); true } else { false } }); 
[removed]
You want r/playrust bruh. This is a programming language sub.
Memes? No. Jokes? As long as they are not a post or have more to them than just the joke, eh, why not? At least, that's how I see it.
You sound like someone who likes pineapple on pizza
Either that or I want to see a bug closed with "mathematically impossible." For anyone who didn't get the joke, or who wouldn't mind wasting eight minutes on a [quality educational animation,](https://youtu.be/92WHN-pAFCs) the fact that Rust's type system is "Turing complete" guarantees that it is impossible to place any time or memory limit on builds (no matter how large) without excluding some correct programs which would *eventually* compile. This is moot point, of course. If something takes too long to build, you can't actually *use* it. Real memory is also finite. 
Op needs to modify the retained items.
as long as it's put there at compile time 
Can't you just compile Rust into Web assembly via emscripten?
Welcome to Reddit. Before posting to a subreddit/forum/community, you should check to see what that subreddit is for. This includes reading the sidebar and the rules. You should also pay attention to warnings that you're posting to the wrong subreddit. Check /r/playrust.
i can see a case for either, i guess '```for match..```' makes sense for the same reason as '```if let ..```' (i suppose you could also try '```for let...```') r.e. the trailing 'match', there is an appetite for a single-function match sugar, something like ```fn foo(...)-&gt;.. match { ..... }``` .. they agree they want it but no one has yet figured out a syntax
Phew! Thanks for checking.
Is there a problem with that?
You might be interested in [neon](https://github.com/neon-bindings/neon) which is for using Rust with Node. There's also [domafic](https://github.com/cramertj/domafic-rs) for webassembly stuff and interacting with the DOM. If you're talking about compiling Rust to JS then there's nothing I know about currently that fits that description.
DBE? I've done some work with X11 and I haven't encountered issues with redrawing the screen at 60 fps. Interesting.
I want to implement a Dataframe (playground)[ https://play.rust-lang.org/?gist=6d490aa58028b82ad309625ae1c71696&amp;version=stable] Adding columns works well (one column is an vector where each element is of the same type) but adding rows will be needed also (for instance when sorting by column, I'll need to read and write rows) I though a vector or tuples would be good but I'm up to any solution.
This is an interesting contest! Even though it's over now, the sandbox is still up so I might have a try at it with Rust. Regarding &gt; but then some spurious errors about buffers’ sizes not being enough popped up I wonder if it has anything to do with the socket send/receive buffers. Increasing those might increase performance by quite a lot. [https://github.com/rust-lang-nursery/net2-rs/pull/39/files](https://github.com/rust-lang-nursery/net2-rs/pull/39/files) 
And then an LLVM compiler backend for CHIP8, in Rust obviously, so that you can write that custom software in Rust. =D Just kidding. Very good recommendations.
This is amazing.
The video is here: https://air.mozilla.org/bay-area-rust-meetup-august-2017/
Understood. Thank you very much. 
Great work! A heartfelt thanks to everyone involved! I hope we can make more improvements like this.
Can you elaborate a bit more on what you're asking about?
TWiR, some clippy work and I just landed a PR to allow [pikkr](https://github.com/pikkr/pikkr) to build without AVX2. Also perhaps I'll find a way to restore soundness in [optional](https://github.com/llogiq/optional). If there's any time left, I'll do some preliminary research on how to add a keyword to Rust (to implement variable-length arrays).
Well, I ran into an obscure (for me anyway) lifetime issue yesterday night, so I'll switch to this and see if it can enlighten me!
Yep, with retain, the arg to the inner closure can't be mutable. (which I need). Otherwise I guess I'm too functional programming oriented, because this solution does the trick (provided than parse(e) takes a non mut ref). I like the drain_filter() better (again looks more FP oriented to me ... ). 
Nice work, great docs. Your code was very easy to read. I'll definitely keep your crate in mind.
Is it? Scala-JS just transcompiles Scala into JS doesn't it? That's what emscripten does as well. 
Wouldn't a new rust user not be aware implicit lifetimes, therefore be confused by "declared with different lifetimes"? Bikeshedding: I could think of something like "... have their own lifetimes respectively, Hint: You enforce the same lifetime by anotating them with 'a like &amp;'a" But from the perspective of somebody knowing this mechanism, this is incredible! Awesome work! 
BTW. There is an old RFC for permitting traits to acces fields. This would which save you from the trouble of writing traits for getters and setters, but AFAICS the current status is "punted" struct Turret { position:Position, direction: Direction } trait LookAtPLayer { // Require the implementor to have the following fields position: Position; direction: Direction; fn look_at_player(&amp;mut self, p: &amp;Player) { // Default implementation is allowed to read self.position and self.direction } } impl LookAtPLayer for Turret { type position = self.position, type direction = self.direction, // Uses default implementation for look_at_player } * https://github.com/nikomatsakis/fields-in-traits-rfc/blob/master/0000-fields-in-traits.md * See also https://github.com/rust-lang/rfcs/pull/250
Hi! nice work, Are you using this in production? If you are serious on this project, I may have to create pull requests createtime varchar NOT NULL, updatetime varchar NOT NULL The datatypes for time should be `timestamp with time zone` this way you can do queries with date arithmetics such as `createtime &gt; interval '2 days'`. 
reminds me that someone did [Conways GameOfLife in C++ template meta programming](https://github.com/sirgal/compile-time-game-of-life). I like to joke that *"C++ can do GoL with compile time meta programming, but it can't find definitions out of order"*
Yep, from the linked Twitter thread: &gt;&gt; Yay for the improvement! Now to suggest how to fix it ;) &gt; Yeah, that is indeed planned as the next step. Still a bit more work to do on diagnosing though.
CMake is what currently is used in KDE, so to use Rust there it must be possible to use it. I chose to use it because it would serve as an example. That does not mean that there cannot be a crate that hides the work that cmake does. With serde you could even put the binding JSON inline in the rust code and have it type checked. 
hi thanks, it can work now. but somthing will do like this, i have many things to learn, PR always welcome!
Wow, nerdness galore! I love it :) I used to do similar stuff with the Scala type system (which is also Turing complete), but I never got to fixed point numbers and mandelbrots. Certainly had the same issues with insanely long compile times and high memory usage...
A good question on 'why?' - but on a previous version before I wired the full mandelbrot code in it went down from 1h20m to 3m run time by clearing those out! rustc seems to get stuck in 'wf checking' for a long long time. &gt; Did you buy 32Gb of RAM for this? Respect Mostly :-) I needed to buy my Dad a new machine, it's just he got a bottom end machine but 32GB of RAM of which he'll use about 4 :-)
It's ok, we all are :)
Shout-out to the PingCap team for the talk and their open source projects, they're really pushing the boundaries.
https://habrahabr.ru/post/337710/ - the story of 13th place Unfortunately, it's russian too. In short: 1. Zero synchronization, due to known testing pattern 2. Extensive buffer reuse, also due to known request/response patterns 3. Multiple tweaks to reach as much throughput as possible, including customized parsing of HTTP header and response header being a stub. 4. Magic pill BUSY WAIT on epoll, which gives ~ -20% time, if used properly (1 listener + 3 workers) My conclusion: it has very little to do with language. Very, very little. So saying Rust performed quite poorly isn't correct in this context IMO.
Yep it's OK, I finished the write up late on Sunday night and hadn't checked much and know the code I uploaded spits out a few other errors as well as the actual output and I should clean those up.
at a glance there are two things to remember. Don't use f64 if its not necessary just use u32. Are you compiling with the --release flag ? Edit: I don't know what your coworker did and if he is going through the same motions that you are. Obviously you can optimize this problem to a +1 if rng() &lt; 0.25 or even simpler just println!("Average Card Won: {}", iterations/4); :P But seriously: thread_rng() might be overkill. using a XorRng will probably give you a big speedup. You could create the deck once , and then you can copy it before you shuffle. i do not understand your highest_spades function. This is how i would write it. But it will always return 13. fn highest_spade(mut hands: Chunks&lt;u32&gt;) -&gt; u32 { let mut highest_spade = 0; for hand in hands { let lowest_spade = hand.into_iter().fold(0, |max, i| if *i &lt;= 13 &amp;&amp; *i &gt; max { *i } else { max }); // highest number lower then 13 highest_spade = cmp::max(highest_spade, lowest_spade); } highest_spade } 
This is a pretty nifty little program. I never would have thought to use coloured cell backgrounds to print the pixels of an image. I like how you've done proper error handling everywhere and leveraged existing crates to do all the hard work. I'd probably want to break out that main function though. Anything longer than a screenful is too much. I usually try to separate the argument parsing and setup code from the core functionality (in this case a function which takes some file path and prints it to the screen). Then my `main()` just looks like: ```rust fn main() { let args = parse_args(); if let Err(e) = print_image(&amp;args.file_path) { println!("An error occured, {}", e); } } ```
Thank you! It means a lot to us!
Also, try moving the assignment of `deck` out of the loop, so that you don't allocate a new vector on every iteration.
Someone give a million pounds to those responsible for this change! Can't wait until it reaches stable.
And my axe!!
It's a highly atypical test case; bottlenecks in this sort of code might not be bottlenecks in "normal" projects.
cool ;) I would suggest use some light CSS framework to make the website less ugly :P. I also suggested that Rocket will have some examples in Readme, maybe you could send a PR there and add link to your project :)
I'm reading the second edition of the rust book and up until now I've been able to reason about why the compiler can/can't make guarantees about memory. I don't understand why this doesn't compile: use std::sync::Mutex; use std::thread; fn main() { let counter = Mutex::new(0); let mut handles = vec![]; for _ in 0..10 { let handle = thread::spawn(|| { let mut num = counter.lock().unwrap(); *num += 1; }); handles.push(handle); } for handle in handles { handle.join().unwrap(); } println!("Result: {}", *counter.lock().unwrap()); } error[E0373]: closure may outlive the current function, but it borrows `counter`, which is owned by the current function --&gt; | 9 | let handle = thread::spawn(|| { | ^^ may outlive borrowed value `counter` 10 | let mut num = counter.lock().unwrap(); | ------- `counter` is borrowed here | help: to force the closure to take ownership of `counter` (and any other referenced variables), use the `move` keyword, as shown: | let handle = thread::spawn(move || { The closure is guaranteed to be dropped at the end of `main` which is the same place that `counter` will be dropped. Why can't the compiler allow this?
you are right, when i begin the project. i want anybody not to know much front end framework and can Familiar with the project. maybe it will have one at sometime
https://github.com/gilbertchen/benchmarking mentions rdedup but shows no results. Are time and storage space benchmarks available for rdedup? I'm currently using borg which is popular and smooth but uses a lot of cpu. 
Someone dare to fill me in on why/how each line has only two opening parentheses on the left side, but seemingly a bazillion closing ones on the right?
i understand. it is important to use simple html if you are not focusing on front end. you can try bulma. its very light and simple and great for prototypes. 
See the 'output' section; the raw output is more like '((N2, (N2, (N2, (N2, (N3, (N3, (N4, (N9, FalseT))))))))' but I run it as: rustc play.rs 2&gt;&amp;1 | sed -e 's/((/X((/g' -e's/, (N//g' |tr 'X' '\012' which loses the ', (N' between each pixel and inserts a newline on the ((
Thanks! That could work, maybe not the license but at least I know there's something like this out there, guess some epoch-based lock-free structure can fill the need as well. Impressive search!
Sorry but the new wording is less clear to me. The previous version had numerous advantages that the new one lacks: * It is talking about _anonymous_ lifetimes and calling them as such. The new one only says the lifetimes are different, and my -- and probably many beginners' -- reaction would be "But I'm not declaring any lifetimes here at all!" * It explicitly numbers them rather than just vaguely saying they are "different". It also describes the actual vs. expected state of the "outlives" relation between those lifetimes. * It is consistent with similar messages you would encounter when working with explicit lifetimes (`&amp;'a`), which helps the user to reason about their mistake and the possible fixes. * The new version uses non-standard terms. What does it mean for the data to "flow into" a binding? What is even the "data" here? Rust has an established terminology for borrows &amp; moves at this point, and compiler should _especially_ be following it very strictly. One thing that the new version does better is pointing out where exactly is each anonymous lifetime defined. However, this could've been fixed without rephrasing the entire message in a detrimental way.
[removed]
/u/aturon /u/alexcrichton
Actually, I am a bit more optimistic :-) It is true that one can construct an example that shows pathological behavior. However, with the kind of code hygiene being promoted by the C++ Modules TS, I have found through internal tests in various practical configurations -- including examples like of giannormous sets of specializations from [C++/WinRT](https://github.com/Microsoft/cppwinrt) -- that integral factors (larger than 1) of speed up are commonly possible. The general point of semantics processing taking a significant portion of the entire compilation process stands. What the C++ Modules TS is doing is to provide a context for a more efficient management of that time, leveraging code hygiene.
thanks your suggestion.
I came here to make similar-ish comment. I believe the new error message is a step in the right direction but: * Should state explicitly that there are elided lifetimes here and what are those lifetimes such that the subsequent error on lifetimes make sense in reference to that. This code could have been written by anyone without even the lifetime concept yet. * I agree on the terminology suggestion. Data "flowing into" did surprised me at the beginning. The terms used should match those of the book and the other doc material? 
&gt; It is true that one can construct an example that shows pathological behavior True, I am on Linux/MacOSX so I am only using clang modules. My tests are compiling range-v3's and libc++'s test suites. It would be nice to see the process of modularizing range-v3 (it already works with clang-modules) for the Modules TS, and the speed ups it achieves on MSVC. There is a fork by CaseyCarter that works on MSVC. Clang achieves a reduction in compile-times of 35% when compiling the whole range-v3 test suite with modules over clang without modules. &gt; with the kind of code hygiene being promoted by the C++ Modules TS, Clang modules is not hygienic at all. I actually love this, because it has allowed me to modularize large code-bases with minimal efforts, but now that I am there, I'd like something that further improves compile-times and am willing to sacrifice hygiene somwhat (I'd need to export macros though). &gt; What the C++ Modules TS is doing is to provide a context for a more efficient management of that time, leveraging code hygiene. I wish you'd had the time to write blog posts about his. I'd like to know more about how Modules TS is actually implemented in a compiler. Just by reading the TS it is hard to know what speedups can be expected in practice.
See https://twitter.com/nikomatsakis/status/907343628846161921 The error message is not finished, it is missing the "note:" and suggestions fields. The note would probably say that the lifetimes are implicit, what the compiler inserts for you, and how to manually annotate a life-time. 
I *really* appreciate having the full transcript of the talk instead of only a video and the slides. Thank you! Also, the talk itself was full of useful information and very clear.
On a separate note...I didn't realize that the futures crate provided channels as well? Why was it not possible to use the std:: channels?
Well if you count asm.js as JavaScript, then you can also target asm.js. asm.js currently has a few advantages compared to WebAssembly as it's actual JavaScript and therefore integrates nicely with all the tools in the JS ecosystem, while WebAssembly isn't quite there yet.
&gt; I wish you'd had the time to write blog posts about his. I'd like to know more about how Modules TS is actually implemented in a compiler. Just by reading the TS it is hard to know what speedups can be expected in practice. That is a popular request; I might get to it eventually :-)
Great, but what would be terrific is a message that would explain what the options are in order to fix this.
There is support for hot reloading of functions that allows for fast iteration times in the browser. Once set up it is a fun way to develop. Although I never used it for more than simple practice code.
[I put together a list of projects](https://github.com/whostolemyhat/learning-projects) which I've been working through to learn Rust.
Ambitious project! Nice work :-)
Thanks, was looking for something like this recently.
What `a.push(a.len());` does in reality is this: Vec::push(&amp;mut a, Vec::len(&amp;a)); Now it should be obvious why NLL doesn't help and what is the problem: we would have to define order of evaluation of arguments. No matter how you define the order, you may get uncompilable code. Evaluating order dynamically based on situation might help: first evaluate all expressions which return value with `'static` lifetime, then do the rest as is currently done.
&gt; Preferably without Trait objects because that would require inserting type parameters all over my application. How so? Sounds like you can just have a `Box&lt;BackendOp&gt;`.
This will work without type parameters, right? But still I'd need a way to give `Object` a `BackendOp` constructor.
It will. Why would you need an `Object` to construct new `BackendOp`s?
Because they might open files or network connections. `Object` also implements `Clone` which means I will have to duplicate those. That was my initial reasoning. Do you find this cleaner?
Shouldn't the connection be created inside `BackendOp` then? Can you explain the API a bit more here?
`spawn` isn't able to guarantee that the parent thread will respect lifetime limits. Also as far as the Rust compiler knows, `main` is a normal function. So you're correct, but the compiler wants a stronger proof. The trait bound that must be satisfied is `'static`, which essentially means "nothing borrowed." You need shared ownership, which is provided by `Arc`, in addition to the shared mutability provided by `Mutex`. So, your counter will be `Arc&lt;Mutex&lt;i32&gt;&gt;`. Then you should clone it, use the `move` keyword before `||`, each thread will own one clone and it'll work. 
There might be a better way to do this, but you can wrap `counter` in a `std::sync::Arc`, create a clone on each iteration, and then have the closure take ownership of that clone. [Here's](https://gist.github.com/anonymous/ae5551e257734966893fd97d116fa549) a modified example. 
I did this. Also writing a Chip8 assembler with nom
This sounds amazing and I look forward to reading more about it!
The old one looks like a compiler bug because it says "outlives" but points to exactly the same end of lifetime for both. "x outlives y" has the *very* technical meaning "y cannot be proven live everywhere that x may be live." That's a surprising use of a plain English word, and not something to be preserved. An improvement on both would be to talk about the "different lifetimes" of "different formal parameters." I agree with the rest.
&gt; "x outlives y" has the very technical meaning "y cannot be proven live everywhere that x may be live." That's a surprising use of a plain English word, and not something to be preserved. Since I'm not a native speaker, I'm probably missing some nuance in the meaning of "outlives". In the context of lifetimes, it always implied to me that it's both a "spatial" relationship (how the definitions overlap in code) and a temporal one (when are the values dropped relative to each other). At least for the latter sense, "outlives" sounds like an acceptable choice for referring to the `'a: 'b` lifetime relation. Do we have any better alternative that should be used instead? Saying merely that the lifetimes are "different" doesn't seem precise enough, as `'a: 'b` rarely (if ever) means `'a == 'b`.
The funniest part is that modern compilers tell you exactly which declarations it can't find, where it's located and where it must be to work.
So this was why I was a bit careful when I filed https://github.com/rust-lang/rust/issues/42596 which I guess I should update. I mean I expected my crazyness to cause the compiler some indigestion - but how much? I think the 20G/40mins is reasonable for the run - the way it blows upto 100G+/4hours+ if I make a small mistake in a where clause seems more like a bug.
I want to avoid generating more frames than will be displayed.
You could rewrite `retain` as `retain_mut` or an in-place `filter_map`.
加油！
The error is " 'x: 'y is not satisfied." Or " 'x might not outlive 'y." But the compiler said that 'y outlives 'x.
Taking things to failure is a common engineering exercise. Many of the limits removed in GCC are there to support generated code which can easily become pathological. 
Yes, that is too dynamic for standard library types. A dynamic language would probably interpret that as a hash map of hash maps or something else equally indirect. That would be why dynamic languages are so slow. You actually could go the "stringly typed" route and keep a `Vec&lt;String&gt;` of CSV lines. That would be the GNU approach, would work well enough. I suspect that's how BurntSushi does the CSV crate. Haven't read the source yet though. (Could also be some very clever state automaton. Because BurntSushi.)
Sorry for the late notice, we're just setting this up in the Berlin Mozilla Office, so think of this as a Beta test. If it works out, we may be able to stream future Rust meetups as well.
It definitely looks like it's reconnecting/handshaking. You might want to first run `tcpdump` where you're running the client to see which side is disconnecting the session first. Something like : `tcpdump -n dst host &lt;ip of backblaze host&gt; ` should give you basic information to see which side ends the connection with an RST or FIN packet. 
The best introduction to Rust Futures ever!
&gt; I understand Rust, being systems language, can be adapted to be very fast. This is true, but it's usually referring to CPU-bound, not IO-bound tasks. For this kind of thing, you're looking for async IO, primarily. "Tokio" is the big library people are using to build stuff here. &gt; If so, which framework? Rocket? Rocket does not yet use async IO. &gt; But I am asking if your team were to build something with Elixir and Phoenix, would you suggest Rust instead? It entirely depends; the web ecosystem is still early. Lots of stuff is in very active development, and so you may need to build stuff. Basically, all of the important bits are *there*, but aren't packaged up super nicely, and are still generally in flux. That's either exciting or a deal-breaker, depending on who you are, what you're doing, your comfort level, etc.
Yes. Rust can go very fast, and it can build servers that can scale out to 10's of thousands of connections while using far less memory than Elixir. If you just want speed and don't care about handling lots of connections, Rocket is a good choice. Its current threading model is one that would be better suited to having a bunch of processes being load-balanced by nginx or haproxy. (which is a great architectural model for reliability) If you want to support many connections with a single process, you might want to look at Gotham (https://github.com/gotham-rs/gotham) Either way, you should probably also take a look at : http://www.arewewebyet.org/ for additional options and information. 
Hmmm, I know, but I thought with recent experimental generators feature Rust is trying to be Go level fast in async I/O too?
Yup! That's what I said: it exists, it's just early.
How do I serialize and deserialize `{ "field": 123 }`, `{ "field": "foobar" }` and `{}` respectively to `Enum::Int(123)`, `Enum::String("foobar")` and `Enum::None` using serde json? Deserializing to `Option&lt;Enum&gt;` where `Enum` only has variants `Int` and `String` would also be fine. Edit: Ok, actually this is not difficult to figure out on my own :) #[derive(Serialize, Deserialize)] #[serde(untagged)] enum Enum { String(String), Int(i64) } #[derive(Serialize, Deserialize)] struct Data { field: Option&lt;Enum&gt; } serde_json::from_str::&lt;Data&gt;(r#"{ "field": 123 }"#) 
Nitpicky but would it make more sense to say "data from y becomes owned by x" instead of "data from y flows into x"? EDIT: *borrowed, not owned
Oh okay. Thanks. :D
If you just want to learn, I've found that /r/dailyprogrammer is really helpful. I've done a couple in rust. 
So I am still kinda noob on this topic, but will there be any effect on performance if I use a multiprocess architecture for a messaging service? Thanks for saying about Gotham, seems nice. :D
The author states definitively that async IO is faster than sync IO without showing any performance numbers or evidence. The last performance test I saw showed sync IO only marginally slower only under high load.
I have added `rdedup` support to `linux-*-test.sh`, and to bench `rdedup` all you have to do is to specify `$RDEDUP_PATH` and run the test. https://github.com/gilbertchen/benchmarking/blob/master/linux-backup-test.sh#L110 The official results were never redone afterward. If you happen to rerun them or compare against `borg` in other way, please let me know how it went. I know that `linux-*-tests.sh` are rather inconvenient for `rdedup` because they are all about small files changing fast, and `rdedup` does not have a smarter in-built file crawler yet.
I fully understand CMake's role in KDE... but I'm primarily a Python developer who's mainly come to Rust for the same reason JavaScript developers take up TypeScript. I have very low tolerance for allowing it to complicate maintainership, distribution, or installation... and using pure Rust with cargo (eg. CLI tools), pure Python with PyQt only, or PyQt with rust-cpython and setuptools-rust makes for projects that: 1. I trust myself to maintain 2. I trust to be easy for others to build from source (The key word being "trust". I have yet to feel confident in this approach's ability to satisfy either of those requirements for people not invested in the C++ and CMake ecosystems.) ...plus, I write portable software. While I understand that building for other platforms will never be as simple as pointing a tool like py2exe at pre-built PyQt packages, I'd prefer something with instructions as simple and comprehensive as possible.
Cool writeup. Typo in this line in unary example ``` response_sink,send(response) ```
It really depends on what you are trying to implement. I love rust but elixir is build on top of erlang OTP and thus coveres almost all you can think of in networking. 
Thank you very much. I thought about using deserialize_with, but didn't know how to deserialize the HashMap.
In pretty much all cases, I'd recommend an architecture where your application servers (Rocket, Phoenix, Rails, whatever) sit behind a Load balancer of some sort that was meant to face the open internet. Nginx and HAproxy are the most likely candidates for this. Or if you're in AWS the ELB is good as well. So no matter what back-end you choose, your requests will be taking a hop through the LB. Let's say you want to handle 10,000 concurrent connections (which is quite a lot in the realm of normal web products) If you chose Gotham, you could, theoretically, have fewer Gotham processes serving requests. Maybe 1 LB pointing to 3 Gotham back-ends. If you chose Rocket, you would probably want 1 LB pointing to 40 Rocket back-ends. If you're not storing much state in your applications (e.g. massive, persistent hash tables, etc) . then 40 processes really isn't much in the way of RAM. If they're very busy, then you will incur a slight penalty in the OS scheduler. Note that if you're in the cloud, it would be easy to spread these 40 processes over a few servers. I'd say the performance difference would be negligible between the two approaches. I should note that there is a big upside to having many processes on many servers: If you lose a process or a server, it's no big deal. You get a lot of resiliency with many processes. 
If you block on a channel you can't block the thread, only the task. So you have to return a future. 
[A million pounds of rust](https://thumbs.dreamstime.com/z/pile-rusty-metal-lot-scrap-closeup-63341261.jpg)? I'm not sure that would be welcome ;) This change would have helped me learn about lifetimes *so much better* while learning Rust, so I'm truly grateful for it :)
Wrong subreddit. Try /r/playrust
[This is amazing](https://www.myinstants.com/instant/this-is-amazing-lord-shaxx/)
I like this idea. I'll give it a try.
The "state machine" in the context of async/await does not depend on LLVM at all- it happens much earlier, at the MIR stage.
The docs for [`owning_ref::StableAddress`](https://kimundi.github.io/owning-ref-rs/owning_ref/trait.StableAddress.html) include this example: #[derive(Clone)] struct Foo(Rc&lt;u8&gt;); impl Deref for Foo { type Target = u8; fn deref(&amp;self) -&gt; &amp;Self::Target { &amp;*self.0 } } impl DerefMut for Foo { fn deref_mut(&amp;mut self) -&gt; &amp;mut Self::Target { Rc::make_mut(&amp;mut self.0) } } &gt; This is a simple implementation of copy-on-write: Foo's deref_mut will copy the underlying int if it is not uniquely owned, ensuring unique access at the point where deref_mut() returns. However, Foo cannot implement StableDeref because calling deref_mut(), followed by clone().deref() will result in mutable and immutable references to the same location. I'm having trouble understanding that last sentence. How can we call `clone` on a `Foo` while the `&amp;mut u8` returned by `deref_mut` is still alive?
Is this library on crates.io yet? Is it related to [rust-qt](https://github.com/rust-qt), that contains a C++ binding generator? &gt; If you prefer to use only cargo, you'll have to tell it to perform steps 1, 3 and 4 in a build.js file. I think you meant build.rs
If you want to debug HTTP, an essential tool is [Wireshark](https://www.wireshark.org/). If you are having trouble because the server is using TLS, you have a couple of choices; if you can run your own server, you can just load the server key into Wireshark and it will be able to decrypt the traffic. If you can only test against their server, you can use [mitmproxy](https://mitmproxy.org/) to proxy their server with a certificate you control, and then use Wireshark for that connection. A few things that I can think of that might be causing your problems, if it is actually opening a new connection for each; I don't know the backblaze API or reqwest/hyper very well, so these are just a few wild guesses. Are all of the requests for the same hostname/port? If the hostname/port varies between requests, then the client may only keep one or a few connections alive, and you may have more hostname/port pairs than it supports, so you wouldn't actually reuse them. You also mention that it creates multiple concurrent requests, and alternates between them to read. Concurrent requests couldn't share a connection in HTTP; HTTP can't interleave requests, it does one after another. So if you're opening up multiple concurrent requests, and alternating which ones you're reading from, then they would have to open up separate connections, though once one of them finished and another request was queued that could reuse an open connection. It is relatively common to take advantage of a small amount of parallelism in HTTP by opening up 2 to 4 concurrent requests to the same host, but no more. That gives a good balance between the extra overhead for opening a new connection, with allowing for some parallelism which helps head-of-line blocking issues in HTTP. 
Once I receive a Response from the server, the Read object doesn't appear to have any trouble been concurrent to others. It's the request not considered over when I'm just reading the body?
So am I right in saying that the compiler could say: * If `fn x` is the entry point. * If thread `y` could never be disowned * Then all references in scope `x` must be valid for the lifetime of scope `y` 
Technically the language is named after a [parasitic fungus](https://en.wikipedia.org/wiki/Rust_\(fungus\)), which is probably even worse.
No, what I'm saying is that for a reqwest `Client` object to support multiple concurrent requests, it would have to open up multiple connections to the server. Here is an example of an HTTP request and response: * request: GET /hello HTTP/1.1 Host: www.example.com * response: HTTP/1.1 200 OK Content-Type: text/plain; charset=UTF-8 Content-Length: 12 hello world If you did two requests concurrently over the same connection, how would the client tell which part of each response went with which request? HTTP/1.1 200 OK Content-Type: text/plain; charset=UTF-8 HTTP/1.1 200 OK Content-Type: text/plain; charset=UTF-8 Content-Length: 12 Content-Length: 12 hello hello world world So over a given connection, an HTTP client can only do one request at a time. It is possible to do [pipelining](https://en.wikipedia.org/wiki/HTTP_pipelining) to queue up multiple requests, and get one response at a time, though it's not well supported so frequently disabled. But from what you describe, it sounds like you're trying to read very large bodies, and do so simultaneously. That would require multiple concurrent connections to the server, and so presumably if you start a new request when a previous one is not finished, reqwest would have to open up a new connection in order to service it.
**HTTP pipelining** HTTP pipelining is a technique in which multiple HTTP requests are sent on a single TCP connection without waiting for the corresponding responses. The pipelining of requests results in a dramatic improvement in the loading times of HTML pages, especially over high latency connections such as satellite Internet connections. The speedup is less apparent on broadband connections, as the limitation of HTTP 1.1 still applies: the server must send its responses in the same order that the requests were received — so the entire connection remains first-in-first-out and HOL blocking can occur. The asynchronous operation of the upcoming HTTP/2 or SPDY could be a solution for this. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.27
I might not understand the question. &gt; How can we call clone on a Foo while the &amp;mut u8 returned by deref_mut is still alive? You can't. It says that. &gt; However, Foo ___cannot___ implement StableDeref because calling deref_mut(), followed by clone().deref() will result in mutable and immutable references to the same location.
It is and it isn't, there's no single reason for the name.
But data from `y` is not becoming *owned* by `x` – only the borrow to the data is forwarded to `x`. ‘Owning’ data is very clearly defined in Rust world and different. But I agree that ‘flows’ is not the best word here…
This is nice but sadly doesn't solve a pet peeve that I have. Often I would start out with a struct, write a few impls and then I realize that I need to add a reference, which means I need to add an explicit lifetime. And then I have to update all the impls manually. I guess now I only have to write `'_`. It would be nice if I could remove the lifetime completely if I never need to name it. impl Iterator for MyIter { ... } vs impl Iterator for MyIter&lt;'_&gt; { ... } I guess the refactoring problem can be solved with tooling, so I guess it is not that bad. 
Clearer wording on "flows into" and I'm sold. The general concept is communicated much more clearly in this than in the previous error message.
I mean, this isn't exactly a controversial fact. In many cases, w/ large number of concurrent connections, using blocking I/O isn't tenable. Odds are the performance test you saw didn't really represent real world usage patterns.
By that logic literally every program you write in Rust is a statemachine. 
Yeah, but I can't find a million tons of that fungus very easily.
Yes, you can build a base in the `main ()`. On the other hand, I believe you are looking for /r/playrust. /r/rust is for a programming language of the same name. :p 
I'm assuming that StableDeref is a typo for StableAddress everywhere in that doc. Anyway, you're right: that sentence doesn't make any sense, because you could never call the three functions in that order. It sounds like it's warning of UB by mutable/immutable aliasing, but that shouldn't be possible because there's no unsafe code yet. If you reorder it to borrow check, like this: let mut foo = Foo(Rc::new(42)); let fooc = foo.clone(); let immref = fooc.deref(); let mutref = foo.deref_mut(); Then the references do not point to the same location. So there's no UB. But that's the real reason you can't implement StableAddress: because the address isn't stable! cc /u/Kimundi
Yep, every program *is* a state machine, particularly at the assembly stage. Your suggestion to look at the generated assembly is unhelpful because it just jumps straight there. What the OP and I am referring to is the particular lowering conversion from code with `await!`s in it to code without. This transformation replaces any control flow that spans an `await!` with an explicit, reified set of states that it uses to keep track of what it was waiting for each time it is polled. This is distinct from any transformations LLVM does.
I find it weird that it isn't controversial considering I've seen no performance tests that have shown async IO substantially faster than sync IO. I see this conclusion repeated in every blog, that "async is high performance web scale" as if it were some cargo cult opinion we just accept as fact.
Overall this looks really excellent. But `'_` is hard for me to stomach. I'm going full bikeshed here. fn blah(&amp;self) -&gt; Struct&lt;'_&gt; { ... } I'm not really sure what it's saying---"the lifetime of this struct will equal the elided lifetime of &amp;self in the function"? This is different from just requiring explicit lifetimes, and a bit confusing as it introduces a gross new syntax. Why not just deprecate the elision without introducing a new syntax: fn blah(&amp;'a self) -&gt; Struct&lt;'a&gt; { ... } Pretty straightforward. Or something less ugly: fn blah(&amp;self) -&gt; Struct&lt;'&gt; { ... } fn blah(&amp;self) -&gt; Struct&lt;_&gt; { ... }
 error[E0623]: lifetime mismatch --&gt; src/main.rs:2:10 | 1 | fn foo(x: &amp;mut Vec&lt;&amp;u32&gt;, y: &amp;u32) { | ---- ---- these two types are declared with different lifetimes... 2 | x.push(y); | ^ ...but `x` will become invalid when `y` ceases to exist Replace "invalid" with a better term if you can come up with one.
Yes, I'm pretty sure that OP has made a mistake somewhere. If you shuffle a deck of cards, divide them between four hands, and then find the highest card between the four hands, I'm pretty sure you're going to find... the highest card available in the deck, every time! OP, are you sure you've written the program correctly?
I'm sad to see that backreferences (lifetimes implicitly named after their argument) was removed. - I felt it was revolutionary for simplifying explaining lifetimes. - Encourages meaningful lifetime names by starting people off with something more than "a" and "b" (making me think of some bad functional language tutorials).
Yeah, so this always returns 13, which is not right. The idea is that four people draw their lowest spade, the highest of those four wins and is counted towards an average. Your advice was helpful however! After moving the allocation of the deck out of the iteration, I got some speed increase. HOWEVER, moving the allocation of the Random Number Generator out of the iteration saved me the most time, dropping my runtime from 1.6ish seconds to 0.17. Crazy fast, this beats my coworker's Crystal implementation by a factor of 10.
Interesting. I just switched to an XorShiftRng, and somehow it's faster to create a new one inside the loop (but incorrect to so do). EDIT: Also any time you're matching on an option in a loop like that, there's probably a better way fn highest_spade(hands: Chunks&lt;u32&gt;) -&gt; u32 { let default = 0; *hands .map(|hand| { hand.iter() .filter(|card| **card &lt;= 13 as u32) .min() .unwrap_or(&amp;default) }) .max() .unwrap_or(&amp;default) } (this is also a touch faster on my machine)
Just one small correction, [Bücher](https://crates.io/crates/bencher) runs perfectly fine on stable and beta. I use it to compare the various Rust versions with [bytecount](https://github.com/llogiq/bytecount). Click the Travis badge for benchmark results (but take them with a good helping of salt; benchmarks on CI are unreliable in general).
You could toss `Iterator::for_each` or `Iterator::fold` in there too. It depends on the iterator, but on some they will be better -- I imagine for example on any `a.chain(b)` or `VecDeque` and in some other cases (arrays from ndarray).
It seems `vec_closure` doesn't do the same work as the 2 other functions, and instead allocates a new vector: fn vec_iter(test_vec: &amp;mut Vec&lt;u8&gt;) -&gt; () { for i in test_vec.iter_mut() { *i += 1; } } fn vec_par(test_vec: &amp;mut Vec&lt;u8&gt;) { test_vec.par_iter_mut().for_each(|i| *i +=1); } fn vec_closure(test_vec: &amp;mut Vec&lt;u8&gt;) { test_vec.iter_mut().map(|i| *i+1).collect::&lt;Vec&lt;_&gt;&gt;(); } Something like this would be fairer: fn vec_closure_fixed(test_vec: &amp;mut Vec&lt;u8&gt;) { test_vec.iter_mut().map(|i| *i += 1).collect::&lt;Vec&lt;_&gt;&gt;(); }
Yeah, me too! This syntactic choice has been discussed a *lot* in the earlier [internals thread](https://internals.rust-lang.org/t/lang-team-minutes-elision-2-0/5182), and the short story is that these other options all have significant downsides. But I've added an unresolved question to continue thinking about whether we can do better here.
I'm a bit confused about how the "drawbacks" section failed to identify "typos" as a new class of logic error. ``` fn doodad&lt;T&gt;(&amp;'data T, &amp;'daat T) -&gt; &amp;'data T; ``` would have generated an error if you were obliged to name the generic lifetime parameters (and your intent was to have just one), but (as I understand it) now it will not. Similarly, I guess, we won't have shadowing errors anymore, just unification of the two lifetimes. Erm. I can't tell if this represents progress. **Edit**: I should disclose a belief that ergonomics for me is about minimizing the time to correct program, not minimizing the number of characters I type. Lots of the ergonomics PRs seem to be about the latter at the possible expense of the former.
That line of reasoning is the "scoped fork-join" or "scoped thread" pattern. Once upon a time the standard library even supported it. The problem with the library implementation was that it depended on a guard being dropped before `fn x` exited: `drop` called `join`. The lifetime system can guarantee that a value will be dead before a particular scope exits but it can't guarantee that all dead values will be dropped. A different approach is safe: - `z: fn` calls `scope` with a closure `x: FnOnce` - parent thread enters `x` - `x` calls `Scope::spawn` with a closure `y: FnOnce` - the local variables of `x` stay alive until both `x` and `y` exit - after both `x` and `y` complete, `scope` returns The `crossbeam` and `rayon` crates implement this - difference is that crossbeam spawns a real OS thread, rayon deals in lightweight non-blocking tasks.
I understand you, I was very much in favor of it but some very good arguments have been made against it during the discussion.
All of the points you mention are addressed in the RFC. The typo error you mention is explicitly addressed in the RFC via a lint for lifetimes names that appear exactly once. The shadowing piece is addressed via a convention and lint around `impl` blocks. More broadly, the lang team [also agrees](https://blog.rust-lang.org/2017/03/02/lang-ergonomics.html) that ergonomics is not about raw character count. As I wrote: &gt; A corollary is that ergonomics is rarely about raw character count. When you have good momentum, the difference between typing pos and position isn’t so significant; what matters much more is how easy it is to remember the right one to type. I'd appreciate if, rather than talking about "lots of the ergonomics PRs" in broad terms, you instead focus on specific cases where you think the balance is incorrect, and why. It's also worth remembering that the roadmap this year is focused on learning curve and productivity in general, and some RFCs involve typing *more* characters (like `crate::` for crate-local paths) in exchange for greater clarity. This RFC is aimed both at teaching and at ergonomics. The RFC makes a case for the teaching aspect, and in particular sketches out a guide-level introduction to using explicit lifetimes in this new world. Many people struggle with the notion that lifetimes are *parameters*, and find it easier to grasp them as relationships. (But I'm just relitigating the RFC here). In terms of raw ergonomics: when refactoring code in a way that introduces a need for an explicit lifetime, today you need to update not just the relevant argument/return, but also go back to (or introduce) the generics bindings. I consider this a minor papercut. The main motivation here is learnability.
There's decades of stuff on this--long before Rust. And the picture shifts slightly over time as kernels get better or worse at one or the other. (epoll/kqueue vs. select, better thread schedulers, multicore, etc). There are definitely some circumstances where threads do better than async. In general, low concurrency, and/or high computational expense per task. In particular, tail latencies with threads are better IME, up to a few hundred concurrent jobs. (probably b/c of preemption) Under high concurrency, many-cheap-jobs loads--like proxies, caches, in-memory databases--the thread counts get too high and (in general) the linux kernel struggles to schedule them well. Overall throughput suffers as more time is spent context switching, cache misses, vm management. As of a about 6 years ago, the kernel really broke down when it had 10-20k threads to schedule. I haven't really pushed threads since then, so I don't know what the number is now. Here's one of my favorite talks about what's expensive about context switches, and some tricks folks have done to eliminate those costs under many-threads, single process type loads: https://www.youtube.com/watch?v=KXuZi9aeGTw
Thanks, that helps heaps.
Video linked by /u/jamwt: Title|Channel|Published|Duration|Likes|Total Views :----------:|:----------:|:----------:|:----------:|:----------:|:----------: [User-level threads....... with threads. - Paul Turner - Google](https://youtube.com/watch?v=KXuZi9aeGTw)|Linux Plumbers Conference 2013|2013-10-15|0:34:00|131+ (97%)|14,765 &gt; "Multi-threaded programming is hard. Synchronous... --- [^Info](https://np.reddit.com/r/youtubot/wiki/index) ^| [^/u/jamwt ^can ^delete](https://np.reddit.com/message/compose/?to=_youtubot_&amp;subject=delete\%20comment&amp;message=dmx9rt5\%0A\%0AReason\%3A\%20\%2A\%2Aplease+help+us+improve\%2A\%2A) ^| ^v2.0.0
I've attempted to use `rust-protobuf` for a side project at work. I've had 2 times the library, or its dependencies have broken underneath me in just the last 4 weeks. --- I've submitted PRs/Bug fixes. Stephen is quick to fix things their pretty awesome.. But for stability stake I've been forced to maintain a full fork. --- Also for the `RepeatedField` weirdness I suggest you put a bunch of macros in your root `lib.rs` file, then on the generated `.rs` files annotate the generated code with select macros. You can automate this with `awk`. 
&gt;This is nice but sadly doesn't solve a pet peeve that I have. Often I would start out with a struct, write a few impls and then I realize that I need to add a reference, A structure without a lifetime is state or data, a structure with a lifetime is a smart pointer. They're very different things from a user's perspective, so refactoring one to the other is not trivial.
&gt; The shadowing piece is addressed via a convention and lint around impl blocks. Is it possible that this is missing? I've literally just read each of the 12 occurrences of `lint` in the RFC and while there are mentions of "changes to the style lint" they .. don't seem to be specified? Or do you mean the "make sure people use multiple characters" lint? &gt; I'd appreciate if, rather than talking about "lots of the ergonomics PRs" in broad terms, you instead focus on specific cases where you think the balance is incorrect, and why. I did, in at least two other FCP threads. You know this, because you just merged at least one of them after I did so. ;) I didn't see this one in the list of FCPs or I would have commented there. Things that would have been errors are now becoming lints; does this mean that I'll need to be using clippy (and nightly) to detect them? Serious question; I don't know whether this is meant to be treated as an error, or just "bad style". I'd love to vote for "make programs either correct or incorrect" rather than presenting possible behavioral defects as lints. Shadowing generally has ended up with bugs that only get caught due to "unused binding", which weirds me out to no end. As I mentioned in the FCP post (on implied bounds for lifetimes), almost all of the "head scratching, time taking" errors I have (and largely "have had") with Rust have been about lifetimes, and the magical things Rust does behind the scenes. Adding more magic sounds fun; I just haven't seen the resulting debugging struggle enter into the calculations at any point. Maybe the discussions happen and they get trimmed from the RFCs before public presentation, but from my position outside, a lot of the ergo work seems to want to make Rust "appear" simpler, with unspecified (and possibly small) technical debt to be paid off by other users.
With the *capnp* crate, I have the following code: let stream = ::util_capnp::byte_stream::ToClient::new( ByteStreamImpl::new() ).from_server::&lt;::capnp_rpc::Server&gt;(); let mut req = self.cas.get_request(); req.get().set_key(key); req.get().set_stream(stream); let _result = self.core.run(req.send().promise).unwrap(); Now how do I examine my ByteStreamImpl object; how can I keep a reference and modify it if I wanted?
Fair enough, I had always heard that it was the specifically the fungus, but that was just internet lore and I 100% believe you if you tell me that it's a mix of things. Has Graydon Hoare ever done a post on the history of the Rust language? I'd love to read it if so. P.S. My post was intended to say that a million pounds of fungus was worse than a million pounds of oxidized iron, not that naming the language after a fungus was worse than naming the language after oxidized iron. I see now that it's easy to parse the comment the other way 😳
You just mentioned "elided lifetimes" and that had me think how if we could explicitly declare anonymous/throwaway lifetimes like we do with pattern-matching and generics (`fn foo(&amp;'_ bar, &amp;'_ baz)`), we could simply have the error message desugar the code like that, to highlight how even though the user may not have even heard of lifetimes yet, they're still there (and allow tutorials to gently introduce the concept or self-taught programmers to understand that there might be something missing there). error[E0623]: lifetime mismatch --&gt; src/main.rs:2:10 | 1 | fn foo(x: &amp;'_ mut Vec&lt;&amp;u32&gt;, y : &amp;'_ u32) { | ---- ---- these two types are declared with different lifetimes... 2 | x.push(y); | ^ ...but data from `y` flows into `x` here
&gt; does this mean that I'll need to be using clippy To clarify, these are lints which are in the compiler. They are basically compiler warnings, the fact they are lints means you can turn them into hard errors or ignore them. You don't need Clippy or nightly.
How do I use casting in a match arm? I have created a simple win32 app and now I want to access the menu items from the enum I created before. enum FileEntries { New, Open, Quit, } ... WM_COMMAND =&gt; { match wparam { FileEntries::New as WPARAM =&gt; { unimplemented!() }, FileEntries::Open as WPARAM =&gt; { unimplemented!() }, FileEntries::Quit as WPARAM =&gt; { PostQuitMessage(0); return 0 } } }, ... However, I get this error. error: expected one of `::`, `=&gt;`, `if`, or `|`, found `as` --&gt; src\main.rs:206:34 | 206 | FileEntries::New as WPARAM =&gt; { | -^^ unexpected token | | | expected one of `::`, `=&gt;`, `if`, or `|` here error: aborting due to previous error
Categorical assertions about "X is faster than Y" are almost never a good idea. I've done a lot of work with async I/O, and it is definitely *not* always faster. The real answer is *always* "It depends what you're doing -- did you measure it?" Ops/sec, CPU affinity, the blocking mechanism used, and a zillion other things will affect whether async I/O or sync I/O is the fastest mechanism. Or the most efficient by whatever metric is the most important to you -- such as cycles / core, not necessarily ops / wall time.
Fun fact... It's interesting how these two tests take approximately the same time: * `ginormous_vec_iter_bench` * `ginormous_vec_par_bench` Why isn't Rayon's parallelism helping here? Shouldn't the parallel version be around 4 times faster, assuming we have 4 cores? What's up with that? Let's try figuring this out... :) In these tests, we're iterating over the ginormous array and incrementing every number, that's it. Pretty simple. Both tests process 200000000 bytes of data (length of the `Vec&lt;u8&gt;`) and take around 16000000 nanoseconds (from `cargo bench` results). Let's enter this into Google: &gt; 200000000 bytes / 16000000 nanoseconds The answer is: &gt; 12.5 GBps Okay, so those two tests are processing around 12.5 gigabytes of data per second. The article mentioned the machine on which the benchmarks were run: &gt; Here's the results on my Macbook Pro (3.1 GHz Intel Core i5) Let's google that as well: &gt; Macbook Pro (3.1 GHz Intel Core i5) The first link gets us to Apple's [specs page](https://www.apple.com/shop/product/G0T21LL/A/Refurbished-133-inch-Macbook-Pro-31GHz-Dual-core-Intel-Core-i5-with-Retina-Display-Silver). Here we see that the RAM inside the machine is: &gt; 16GB of 2133MHz LPDDR3 onboard memory We're getting close. Let's see what's the throughput between CPU and RAM. Some further googling [reveals](https://en.wikipedia.org/wiki/Mobile_DDR) that this chip is typically connected over a 64-bit wide bus (i.e. 8-bytes wide bus). That means that the total transfer rate is: &gt; 2133MHz * 8 bytes = 17.06400 GBps Interesting! So there is no way to transfer more than 17 GB per second between RAM and CPU. This number is very close to our benchmark's throughput of 12.5 GB per second. And there's the answer... Parallelizing work on CPU won't help because it isn't the bottleneck here. Communication with RAM is. :)
**Mobile DDR** Mobile DDR (also known as mDDR, Low Power DDR, or LPDDR) is a type of double data rate synchronous DRAM for mobile computers. Just as with standard SDRAM, each generation of LPDDR has doubled the internal fetch size and external transfer speed. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.27
More specifically, Graydon said he used to make up answers when people asked, but the fungus one was one he really liked :) Graydon hasn't really, but I did do a talk once https://www.youtube.com/watch?v=79PSagCD_AY
It has a definite appeal: fn do_thing(&amp;self, x: &amp;i32, y: &amp;i32) -&gt; &amp;'x i32 { ... } Gets a little messier, though: fn do_thing(&amp;self, x: &amp;'self i32, y: &amp;'x i32) -&gt; &amp;'self i32 { ... } But by introducing fewer "names" for things it simplifies things conceptually.
vec_closure collect to a Vec, and I think it explain the poor performances in ginormous_vec_closure_bench. using Iterator::last shouldn't be preferred ?
I want to mention this. But a big motivator for preferring async patterns for me is that it gracefully avoids tricky deadlocks that can arise from improperly sized thread pools when writing synchronous concurrent code. Consider if you have n threads. And your application handles n requests, each of which perform their own threaded operations on the same pool that they block on. What will happen? All threads are busy blocking, and your service can't make progress on any request. A solution is to always partition each blocking kind of operation on its own thread pool. And always perform nested operations in the "correct order". Another one is to always spawn a new thread for each operation. This entire class if concurrency related issues just goes away with async programming.
Thanks, the virtualize debian part threw me off. Thanks for sharing your project. 
Yes, you were right. I was wondering if the suggestion was drifting towards, writing your own debian container or something along that path. 
I'll work on a site for Gutenberg and tera 0.11 if I have time too.
Did I say "X is faster than Y"?
I know Josh from his work with the xcb project back in the day. Brilliant guy.
Also really nice and super fun to talk with. Got to meet him at Rust Conf this year and had dinner with him and few others and learned quite a bit. Stoked he's on the team now.
&gt; Duplicating every one of the fields for every object type would lead to a lot of duplicate code and would be a major pain if refactoring were ever needed. Sorry this wasn't my intention; just a lazy example. You're free to make a supertrait and have subtraits like trait Player: Position {} etc.. the same way trait Ord: PartialEq + PartialOrd&lt;Self&gt; {} This keeps things hierachical also
congrats to Josh!
Ops, sorry! Thanks for pointing out. I am fixing it right now.
Is there any place that lists which versions of the nightly compiler work with which versions of clippy? I just updated to `1.22.0-nightly (dd08c3070 2017-09-12)` and the latest `clippy_lints` (v0.0.159) fails to build, so I'd like to roll back to a previous working version until it works again.
I would love feedback on these new interfaces. Also, there is some logic in the way connections are being picked in the NameServerPool that I need to adjust. This was discovered in this discussion today: https://github.com/bluejekyll/trust-dns/issues/185 If anyone else has similar feedback about the performance or way in which the resolver is working, please let me know by either filing an issue, sending me email, or on any of these discussion boards. I think this is getting close to the API I plan to ship for 1.0. Thanks everyone for the support and contributions!
 ... so `y` may be dropped before `x` Help: you can specify that y "outlives" x using 'x: 'y See REF-123 for more information
Awesome, really glad to have someone with a lot of Debian packaging experience on the Cargo team.
Try them! You’ll quickly see that the Rust compiler will only accept one particular usage.
It might not be the best answer, but [your approach can be made to compile](https://play.rust-lang.org/?gist=c726e76a13eb5f4492a623d3b88cab76&amp;version=stable) with some slight tweaks. It might be a bit simpler to have Obj:new be generic over BackendOp rather than a closure type that returns a BackendOp, but without seeing the rest of your program, it's impossible to say whether that solution is workable/too simple.
I am interested in backblaze support for `rdedup` - with similar requirements. Are you planing to release any of this code?
Good job! *Pats back.*
Your `r1` borrows `s` uniquely, so it is no longer available for modification through that binding. Thus `s.push_str(", world")` should fail to compile so long as `r1` is still in scope.
It was alluded to in the RFC.
Nice spam bot you got there, is it written in Rust?
Lol 
We are working on getting clippy into rustup so you can sync it with the compiler. This will also let you run clippy on stable, btw.
You are in the wrong subreddit. Try /r/playrust
Sorry man I'm not spamming I got the ads link here is the actual one: https://youtu.be/hXy2zVeb8zU
You might want to take a look at a [series of posts](http://smallcultfollowing.com/babysteps/blog/2016/11/02/associated-type-constructors-part-1-basic-concepts-and-introduction/) Nico did regarding ATC and HKT that was based off the recently merged RFC. HKT through ATC was the plan from what I understand.
I see where you're going with this but people talk about lifetimes in c++ aswell. I think you can only go so far making this friendly to beginners. You need to establish domain specific jargon to avoid discussions getting bogged down Analogy to what happens in other languages: the GC people know of 'objects being live' , and escape-analysis. I think 'object lifetime' alludes to parallels with these fields aswell. Perhaps the language can progress by increasing the scenarios where they are elideable. (one vague idea I have: could lifetimes constraints be deduced if you write a trait and an example implementation in one place... 'other implementations of the trait can only exhibit similar lifetime behaviour shown here' training by example.) I suppose you could pick a pair of the words you list to help a little (e.g to disambiguate the concept of a statically reasoned lifetime from dynamic lifetime mangement e.g. Box, Arc.) .. saying that it's a 'lifetime parameter' (e.g. as opposed to a smart pointer type telling us 'how the dynamic lifetime managed') might be enough
One could rename them in such cases.
The way to do this is to manually convert your `wparam` to a `FileEntries` and then match against that casted object. You can't cast an integer directly to an enum, thats a bad idea anyway! let file_entry = /* convert wparam to FileEntries */; match file_entry { FileEntries::New =&gt; ... } I'm guessing `wparam` is of the type i32 or similar, so I would write a constructor for the `FileEntries` like this: impl FileEntries { fn from_wparam(wparam: i32) -&gt; Option&lt;Self&gt; { match wparam { 1 =&gt; Some(FileEntries::New), 2 =&gt; Some(FileEntries::Open), 3 =&gt; Some(FileEntries::Quit), _ =&gt; None, } } } And then use it like this: WM_COMMAND =&gt; { match FileEntries::from_wparam(wparam) { FileEntries::New =&gt; unimplemented!(), FileEntries::Open =&gt; unimplemented!(), FileEntries::Quit =&gt; { PostQuitMessage(0); return 0; } } }, I hope this helps!