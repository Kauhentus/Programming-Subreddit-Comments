Woa! I Thought that it would be optimized by passing a pointer to array + length. Didn't know it did a bitwise copy @o@.
Incremental compilation is great but I'm still curious what's really slowing things down. I'd care less about incremental compilation if small projects were fast enough, though it's obviously worth having. Anecdotally generics seems to be highly problematic. I have a small ~800 LOC project and almost a quarter of that is tests. It compiles in ~10 seconds. Contrast that with [rustendo64](https://github.com/yupferris/rustendo64) that u/yupferris was writing, which is &gt;1k LOC with no tests and compiles in ~2 seconds. The main difference I see is that I'm heavily using generics while that is using none. I've also been switching between stable and nightly, which currently means I hit a fair number of full rebuilds anyway, so any incremental benefit would disappear. It'd be nice if artifacts were separated by compiler version so I could make a change and then have `rustup run stable cargo build` followed by `rustup run nightly cargo build` both only recompile what had changed.
Incremental compilation was not waiting on MIR (because we have MIR working, we decided to make it MIR-only, but it was never *blocked* on MIR).
The keyboard shortcuts are totally weird here (Firefox, Chrome, IE on Windows x64): * CTRL-C copies the selection to both the system and editor's clipboard, hides it and leaves the cursor where it is * CTRL-X, CTRL-X selects from the current cursor position to where the selection started when I last pressed CTRL-C * SHIFT-DEL pastes from the editor's clipboard I've never really used Emacs, but the mark left in place by CTRL-C reminds me about it.
When the set is quite sparse, I am sure it will be faster. Only with a certain fill factor a matrix version would be faster. 
Well that's embarrassing. Guess I need to re-read the book.
It take a lot of work and and open mind to incorporate all the feedback from the original idea. This followup post and proposal really solidify my trust in the people behind rust, and the future of the platform.
And I think that's the missing piece: work on crates.io should be prioritized. It's the main way a Rust user interacts with the ecosystem. I think a better short-term discussion given the way the initial response to this proposal went would be to change the question from "How do we improve the Rust ecosystem?" to the more-manageable subproblem of "How do we improve crates.io?". Tackling smaller problems is generally easier, but since nobody's done it yet, maybe it needs official support from someone.
The while let is going to hold onto the iter for the duration of {} block after it. It'd be faster to just do it single-threaded and not put stuff in a Arc&lt;Mutex&lt;T&gt;&gt; if you're doing it this way. If you break up the `pixel_iter.lock().unwrap().next()` and the actual matching of the result, it won't do this though. e.g., instead of doing `while let` do loop { let step = pixel_iter.lock().unwrap().next(); match step { Some((x, y, pixel)) =&gt; *pixel = image::Luma([applications_until(interpolate(x, y), function, threshold, Some(255)) as u8]), None =&gt; break } } If you want some example code to see, https://gist.github.com/Esption/efa2a4ecfe5aa20cedb9f50f9f2b789d
You're right! It's new since I originally did this work, and I forgot about it. It should definitely be in there.
A slight amount of more detail is here: https://lists.swift.org/pipermail/swift-evolution/Week-of-Mon-20160725/025701.html If you have questions about this topic, please feel free send an email to the swift-evolution mailing list. That way, everyone interested can see and participate in the discussion. -Chris
Looks very interesting. You might want to look at the work that Adam Megacz is doing on [Generalized Arrows](http://www.megacz.com/berkeley/garrows/) ?
&gt; Anecdotally generics seems to be highly problematic. That makes sense. Generics are monomorphized, which is a tradeoff: faster at runtime, takes longer to compile and takes up larger binary size. Historically (and I don't really work on the compiler, so I'm not sure how true this is today, please take with a large grain of salt) we've produced a ton of LLVM-IR and asked it to deal with it, which works well but is slower. If we add certain optimizations into MIR, we could generate less LLVM-IR in the first place, which could help as well. In my understanding.
Swift has been built from the ground up to interoperate with C-like langauges (Obj-C being the most notable). Its compiler is deeply integrated into clang and LLVM (literally its parser/AST are built on clang's primitives, and SIL is built on LLVM's). It also already has move/copy ctors baked in at the low level. The only way a language could be more well-poised for this kind of thing would be for it to actually be built as a compile-to-C++ language. Rust is not nearly as well-poised. Nor is it clear that it would truly be desirable for the warping affects it would have to the language.
Thanks for dropping by! I'm really excited to see what you all come up with.
&gt; work on crates.io should be prioritized. I hear you, and I feel this way too at times. It's really tough. Everyone has their own features that they want more than others. We had someone swing by IRC today who said that Rust shouldn't have been called 1.0 before debugging support was done, for example. I've been doing a bit of crates.io hacking, but not a ton. If anyone is interested in getting involved, I'd be happy to help get you going.
Good question; I believe this is impossible without some cooperation from the threads (or the OS, but that'd be wildly unportable).
&gt;That makes sense. Generics are monomorphized, which is a tradeoff: faster at runtime, takes longer to compile and takes up larger binary size. Then how does D do it to be so fast?
Unfortunately not really right now. At Stanford, we're stamping out a less modular but otherwise the same components that will be much much cheaper to produce and we'll distribute it to researchers and hobbyists. The repo for it is at https://github.com/helena-project/imix. We're going through initial phases now and should have a version ready for distribution by the fall. The SAM4L is a pretty incredible MCU, so this is a great platform in that respect. Meanwhile, porting to the SAM4L development kit should be fairly straightforward (if you do it, let me know and I'll help). And we're starting ports to a couple other widely available platforms.
It doesn't! D defers semantic analysis of templates until instantiation time - meaning that if you don't instantiate them it never does type checking or code generation. This means things like the standard library (which has relatively few instantiations compared to definitions) can compile incredibly quickly, despite its size. Rust, on the other hand, eagerly checks the bounds on all generics, and performs all semantic analysis regardless of instantiation. This is a lot of extra work compared to D's model, but also means you get guarantees in advance about what your code can/can't do (there are merits to both ways of doing things - that's a different debate though). They both, obviously, pay the code for code generation of instantiated templates/generics. Most of the remaining time difference comes down to the backend - if you compare LDC/rustc you should expect roughly similar times for code generation. Obviously dmd will be significantly faster here due to its far more lightweight backend.
Agreed on the last point (different backends are super important here), but your discussion of the differences in strategy is odd. Pre-checking generics means you don't have to type-check the expansions. So if you use all the generics in your code (reasonable if you have any kind of code-coverage), post-checking should be strictly more expensive; you need to re-check a function for every instantiation! However Rust's system can come with significant expense in type-checking the *bounds* on the function -- this process is turing complete, and so can take unbounded time.
&gt; (And I still don't understand why unoptimized Rust code is so much slow) A lot of our abstractions are designed to be easily compile-able away. But without optimizations, you end up with tons of extra layers of this stuff, so it ends up orders of magnitude slower.
Do you know anything about the status of compiler plugin stabilization? I looked around, and I didn't find much.
No, trans is almost entirely just spewing LLVM IR and waiting for LLVM to sort it out. Generic checking happens in (to steal a phrase from /u/carols10cents) the "middle end" of the compiler. Often somewhere in the transition from AST to MIR (may have changed since my time).
https://github.com/rust-lang/rfcs/pull/1681
Thank you for the answer.
It's actually quite a fun little project to do that, but a sensible person would just wrap libffi's closure interface which already handles some of the cross-platform fun you would otherwise need to deal with.
Also note that /r/servo exists.
&gt; my xkcd sword fighting skills have improved ten fold [These](https://xkcd.com/303/) xkcd sword fighting skills?
[Image](http://imgs.xkcd.com/comics/compiling.png) [Mobile](https://m.xkcd.com/303/) **Title:** Compiling **Title-text:** 'Are you stealing those LCDs?' 'Yeah, but I'm doing it while my code compiles\.' [Comic Explanation](https://www.explainxkcd.com/wiki/index.php/303#Explanation) **Stats:** This comic has been referenced 744 times, representing 0.6191% of referenced xkcds. --- ^[xkcd.com](https://www.xkcd.com) ^| ^[xkcd sub](https://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](https://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_d5y00he)
That's pretty incredible!
And someday sorta kinda soonish http://rust-lang.github.io/book/ (v2 of TRPL)
I'm also planning on releasing the encrypted UDT wrapper code as a separate library once I refactor a bit.
The book we need is something like Goetz's Java Concurrency in Practice, but for Rust.
Any idea how to activate incremental compilation? I tried RUSTFLAGS="-Zincremental=target/INCREMENTAL -Zorbit" and I have a lot of modules in one file but it didn't give me any speedup so far.
But... But I've implemented this a long time ago! https://github.com/myfreeweb/secstr
It's not a bad idea but the problem I see with this are: * It's baked into cargo * Not very discoverable * Who is gonna maintain those? I think this would be better solved if crates.io is upgraded with a community driven meta-package system and better searchability.
Chis Lattner wrote "We have a few folks who are experts at Rust that are helping contribute ideas and experience to this though.". Alexis seems to be interested in Swift, so it would not surprise me.
Rust programs can be built to a dynamic library that is loaded by the server: rustc -C prefer-dynamic hello.rs -o hello.rs.so Here is the C API and wrapper using the FFI: https://github.com/ers35/modserver/tree/master/src/api/rust Here are example programs: https://github.com/ers35/modserver/tree/master/src/example/rust Feedback on the Rust code or the project in general is appreciated.
&gt;Documentation. In today's Rust, reading the documentation for the Iterator trait is needlessly difficult. Many of the methods return new iterators, but currently each one returns a different type (Chain, Zip, Map,Filter, etc), and it requires drilling down into each of these types to determine what kind of iterator they produce. Oh my god! Yes please! As a beginner I found that so confusing!
Well, how did you install Rust in the first place?
Any reviews? I've heard there is a book out that's "not the best" to say the least. But i can't recall which one.
I think that's the intention: meta-packages hosted on crates.io that auto-populate Cargo.toml. I'd prefer this "population" step that explicitly adds these package entries to Cargo.toml versus a single Cargo.toml line that magically imports an unknown number of packages.
https://doc.rust-lang.org/std/ptr/fn.write_volatile.html &gt; Volatile operations are intended to act on I/O memory, and are guaranteed to not be elided or reordered by the compiler across other volatile operations. It seems to be sufficient.
Depends on whether you want to learn the current APIs, or the concepts of how rust works (and how you can implement such APIs yourself) – it would surprise me if the latter changed drastically in the next 1-3 years.
The only "real" payed book that is currently out is "Rust Essentials". It seems you are better off using the official documentation than this one. [Amazon reviews](https://www.amazon.com/product-reviews/B00YEVZ70Y/ref=acr_dpx_see_all?ie=UTF8&amp;showViewpoints=1)
[removed]
Great :)
What would be the advantages over using an embedded server (like hyper/iron/etc)?
Hey, awesome. I've working an ffi for libdrm and did the bindings manually. How did you go about generating the bindings? And what are your plans to make it work with glium? Cheers!
yep
It's actually pretty cool to see this in action: bool lotsOfWork(int x) { for (int i = 0; i &lt; x; i++) { } return true; } class Foo(T) if (lotsOfWork(10_000_000)) {} // Uncomment this line to add ~10 seconds to the compile time //Foo!int foo; This is obviously a contrived example, but it's interesting to think about.
I actually am not sure. We got `impl Trait` sooner than I thought. We might get HKTs, dependent types, variadics, coroutines,... soon as well, who knows. What is clear is that those features will significantly change how to write APIs.
I'm with you on this. I hardly see why I'd want to use a server written in another language that dynamically loads Rust. It defeats the whole purpose of using Rust which is about safety and speed. I use Rust to get away from C, not to use more C.
Maybe a tuple? It'll require that you have the information for both keys each time though.
What is done in many cases where dynamic types are necessary, e.g. for serialization libraries, is to create an Enum which creates a variant for each acceptable type, e.g.: enum HandleType { Handle(ConnectionHandle), Socket(ConnectedSocketHandle), } Then you should be able to key the `HashMap` on `HandleType`.
I can think of a couple of strategies: * 1a) Two hashmaps, one ConnectionHandle -&gt; Rc&lt;Connection&gt; and one ConnectedSocketHandle -&gt; Rc&lt;Connection&gt; * 1b) Two hashmaps, one ConnectionHandle -&gt; Rc&lt;RefCell&lt;Connection&gt;&gt; and one ConnectedSocketHandle -&gt; Rc&lt;RefCell&lt;Connection&gt;&gt; ...depending on whether or not you need a `&amp;mut Connection`. * 2a) Two hashmaps, one ConnectionHandle -&gt; ConnectedSocketHandle and one ConnectedSocketHandle -&gt; Connection * 2b) Two hashmaps, one ConnectedSocketHandle -&gt; ConnectionHandle and one ConnectionHandle -&gt; Connection ...depending on which key you'll make use of more often. &gt; I did wonder if I might be to actually store my Connection objects in a Vector, and then store the vector index in two different HashMaps. Is that optimal? Could work. If it's optimal is hard to tell, but it seems that the vector might become fragmented if connections come and go? 
But if I store with Handle(foo), I want to be able to lookup on Socket(bar) or Handle(foo).
Could you not use deref coercions? Something like [this](https://is.gd/TGbWPC)?
Then write the hashing and equality yourself so that it doesn't matter.
In a three layer system, this is the only layer that gets to know both types. The layer above doesn't care about sockets, doesn't use socket; and doesn't/shouldn't know what a ConnectedSocketHandle is.
I believe your plan requires that either both types hash to the very same in all cases (probably really hard to guarantee) or you put everything twice in the hashmap which just equals to two hashmaps + extra pain. It is not worth the trouble. 
I think quantifying the quality of a crate by measuring code coverage with testing and documentation as well as taking into account user usage/stars would be very worthwhile, compared to devs who can be quite opinionated at times when in a position of power. e: and using this data to highlight crates of interest
So I actually didn't implement bindings to libdrm itself. I decided to directly call the ioctls and manipulate the data that way. This meant that I only had to generate the ioctl numbers and link directly to the structures that `libdrm/drm.h` provides. It's a bit hacky, but it works for now. I'm not really sure how to get glium to work with it. It appears I can create a headless context using glutin, but I need to figure out how to get a handle to the gbm buffer that it uses.
It is production ready everywhere except MSVC.
https://play.rust-lang.org/?gist=fa4f35073c036c46519c9b87fd8cd04b&amp;version=stable&amp;backtrace=0 Well, it works. But ugh. The primary hashmap has to store (K2, V) otherwise you can't delete from the secondary HashMap given only K1.
Thanks for the help everyone. I think I have enough info to get things going now.
My understanding is that Hyper uses mio for async I/O. This is great if the use case already fits that model. modserver uses processes for concurrency. Existing synchronous libraries with blocking I/O can be used without modification. One may also prefer having one server with multiple pluggable application libraries vs. one executable containing everything. The server restarts the application automatically if it crashes. See: http://seanmonstar.com/post/141495445652/async-hyper https://github.com/hyperium/hyper/pull/778
&gt; Can I store data in some sort of HashMap such that I can retrieve it using either of two different keys, of differering types? Yes, the [`get` method](https://doc.rust-lang.org/std/collections/hash_map/struct.HashMap.html#method.get) allows this if the actual key can be borrowed as those other types and the `Hash` and `Eq` implementations are consistent for all types. [Example](https://is.gd/V1sxHt).
The two handles are likely to both be integers, but they will be different integers. The two handles are independently generated and will not match.
They don't have to match completely - there just needs to be some consistent state between them that you can use to hash, or intelligent implementations of `Hash` and `PartialEq` that net the same effect. Presumably at some point you have access to both at once. At that point can you make sure the state that is hashed is consistent? Maybe by storing a second integer in `ConnectionHandle` that matches the `ConnectionSocketHandle`, or by storing a copy of the `ConnectionSocketHandle` in `ConnectionHandle` and delegating the hashing to that? [Example](https://is.gd/dDchR1). As long as you can appropriately maintain the `Hash` and `Eq` consistency as far as the map is concerned I think it'll work out. (I have made no real attempt at that in the examples). Otherwise I think you'll need two maps as others suggested.
Given an iterator that spits out strings, is there some kind of join operator to concatenate these strings, with some separating string in between, into a single one? Something like this: fn main() { fn make_string(number: u32) -&gt; String { "(".to_string() + &amp;number.to_string() + ")" } // I would like the following to print "(0)-(1)-(2)-(3)-(4)". // println!("{}", (0..5).map(make_string).join("-")); } https://is.gd/o4q05i
Then you have to learn scala to use rust and deal with the build tool taking a second to start up and using 200-500MB of ram. 
I had been working on something like this (ported from another language), but I've been distracted for a while by other pieces of the world that I need to write ... https://github.com/endoli/commands.rs I'd be more than happy to talk more with you about this! 
Wow… Is there anything preventing there from being one for an iterator of `String`s?
gotta get those sweet youtube subs I guess....
[removed]
Drive-by comment: multi-map typically refers to something different, namely a map with multiple values per key: https://en.wikipedia.org/wiki/Multimap
The itertools crate has `join` for iterators, so no. It's just not in the standard library.
Thanks!!
I don't know if you read this three part series or not, but it might be inspirational: http://notes.willcrichton.net/the-coming-age-of-the-polyglot-programmer/
Thanks. I'm going to try and see if I can get [kmscube](https://github.com/robclark/kmscube) to work with it. I'm thinking about making a new crate for GPU buffers. libgbm was not designed with rust's ownership system in mind, and I think rust's compile-time checks would work very well in buffer management. You mentioned you were working on FFI bindings for libdrm. What was it for? I'm definitely wondering about what sort of applications would use it.
It's a personal project of mine. I'm currently working on a libretro implementation with a specific focus on making it drive old CRTs in native resolution, 15 kHz horizontal sync. Modesetting is crucial to accomplish that. I already have an example working with dummy buffers, but I haven't gotten around to continue working and make it function with EGL.
Hi All, I'm posting here to get some feedback on an idea I've been working on. [Github here](https://github.com/AtheMathmo/learning-machines). It's very much a work in progress so I was a little worried about posting it - but I think some early feedback would be great. I wanted to play with web development in Rust while building something with [rusty-machine](https://github.com/AtheMathmo/rusty-machine). Learning Machines is written in Rust using [Iron](https://github.com/iron/iron). In the future I'd like it to contain some tutorials and information about machine learning - while showing off what Rust is capable of. It is in a **very** early stage right now but I'd love to hear what you think. Feedback on both the idea at a high level and the technical details would be greatly appreciated. If anyone is interested in helping out I'm pretty desparate to make the thing look a lot less ugly. And the server side code could use some work too. Thanks!
Very cool! I think that you should have a breadcrumb bar or something like that on each page in order to allow people to go back and forward more easily
Thanks for the reply. Sorry, that i was not clear enough about my motives. Ill try to elaborate: This interface is not to be used directly by users. Rather they can decide how data is stored (in-memory for small and on-disk for large applications). So the trait definition itself can change and is allowed to be ugly (even if I would rather try to avoid that). I need iterators to the collection because of the way they are processed. This piece of software is going to be an information retrieval library and Vec&lt;Posting&gt; stores the document ids and term positions. I am using the Iterator&lt;Item=&amp;Posting&gt; abstraction to build neat little nested queries. At the very bottom of these queries are the iterators that just iterate over the postings of a single term. Higher up can come several operators (and, or, next to). Using iterators instead of collections in the process of query execution proofs to be elegant and extremely fast. I would really like to stick with that idea. So, to come back to the code you posted: I can return a Rc&lt;Vec&lt;Posting&gt;&gt; and I would really much like to do that. But I have no idea how to turn Rc&lt;Vec&lt;Posting&gt;&gt; into an iterator that owns that Rc. Ill have a look at the LRU cache, though. 
Good to know! Thanks.
I think you meant to post this to /r/playrust :) /r/rust is about the Rust programming language, /r/playrust about the game
You want [the offset method of pointers](https://doc.rust-lang.org/std/primitive.pointer.html#method.offset). I haven't worked much with raw pointers and pointer arithmetics yet, but I think it would look something like this: // uint16_t *lens = (uint16_t *)((char *)m_pevt + sizeof(struct ppm_evt_hdr)); let lens = pevt.offset(1) as *const u16; // char *valptr = (char *)lens + m_info-&gt;nparams * sizeof(uint16_t); let mut valptr = lens.offset(info.nparams as isize) as *const u8; // for(j = 0; j &lt; params.length; j++) { for len in from_raw_parts(lens, params.length) { // valptr += lens[j]; valptr = valptr.offset(len as isize); } * `(*const T).offset(n)` implicitly takes `size_of::&lt;T&gt;()` in account * this assumes `pevt: *const PpmEvtHdr` * I have no idea what `params.length` is or where it comes from so I translated it literally
Aw, was hoping this was some conspiracy theory about the core team.
Just FYI, if you indent text by four spaces in your post it will become a [code block](https://daringfireball.net/projects/markdown/syntax#precode) which is slightly easier to read than paragraphs of inline code.
This is very cool. I think sklearn does a great job of documentation for ML so I'm going to suggest following their path. One thing about sklearn that's so impressive is that not only is the API of an algorithm documented, but there are examples (with visuals) as well as high level explanations of how the algorithm works and what it's good for. I realize this is written with rusty-machine in the backend, but I think an ideal expansion would be to show some of the rust code. For example: http://scikit-learn.org/stable/modules/tree.html#tree Here we have the User Guide for decision trees in sklearn. It starts off with a visual (not as cool as yours!) and then goes into the high level pros and cons. We then get code showing how to use it. The code is chunked up and documented. I realize that rusty-machine may not be stable API-wise so this would incur some serious documentation overhead if you plan on making breaking changes - perhaps instead you could dynamically grab the examples from testcases in your source code or some such thing and embed it in markdown? Anyways, great job and I'm excited to see progress on rusty-machine.
I'm happy to discuss. I've written bindings to D-Bus. For me, dispatching a callback goes through several steps. First I call `dbus_connection_read_write_dispatch` which is the "wait for either event or timeout" function. The D-Bus library will, during that callback, call some other callbacks which I've registered. I've made these callbacks deliberately simple; they just add information about the callback to a queue. I have an iterator which returns the next item in that queue. I then have functionality to build up trees which can dispatch such items. The trees contain boxed closures which my API user has supplied, one for each D-Bus method that can be called. To add to all this, I've made the tree generic over either Send+Sync+'static, Fn+'a or FnMut+'a so the API user can determine how they want to deal with the tree. This adds a lot of mess, but I think it's worth it. Also, D-Bus supports Async I/O by waiting for file descriptors, which I support too by having an API for handing out those file descriptors, so the user can integrate it with mio.
Why are you randomly formatting words as inline code? IMHO it makes reading your post really annoying.
Thanks! Now I get your problem more clearly. So the main issue with your design is `Iterator&lt;Item=&amp;Posting&gt;`. This borrow needs to originate from some local variable. So now you borrow from a `Ref` (the `RefCell`s handle). But as you say, this *freezes your cache* -- you can't change it at all while an iterator is active. So I'm not sure what benefits do you get from the `RefCell`. If you want to keep using `Iterator&lt;Item=&amp;Posting&gt;`, I don't get why you can't do something like that: let collection: Rc&lt;Vec&lt;Posting&gt;&gt; = cache.get(id); consume_iterator(collection.iter()); This way you can do anything with your cache while the iterator is active. And if you use `Arc`, you can even do it in separate thread. ~~One other solution is to change `Item=&amp;Posting` to `Item=Posting`. This way you can create a wrapper iterator:~~ That's totally wrong (you can't get an owned `Posting` from an `Rc`, stupid me). You can get away with it by, requiring `Item=T where T: Borrow&lt;Posting&gt;` (or `Deref`) (this `T` in your case would look like exactly as the `RcIter`, except of `next_index` would be `this_index`. Then you impl `Deref` for it and it should work. It requires refcount bumps on every item, though). struct RcIter { data: Rc&lt;Vec&lt;Posting&gt;&gt;, next_index: usize, } If you implement iterator for this struct, you can move it freely, modify the cache simultaneously, and the collection will be dropped when the iterator will be dropped, unless it's still in cache. I hope I've helped at least a little! PS: Anyway, you can consider dropping iterators at all and use something called *streams* or *streaming iterators*. It differs from `Iterator` in the `next`'s signature, namely: fn next(&amp;'a mut self) -&gt; Option&lt;&amp;'a T&gt; I'm pretty sure there's a crate for that on crates.io with all the combinators (maps, filters, etc.), but could't find anything. :(
the `Send+Sync+'static` closures were an attempt to handle the case where a callback was registered in a thread, possible holding references to that thread stack, and then somehow gets dispatched in an other thread. Or similar funny stuff like that. Though to be honest it was a first attempt to make things safe, without deep thought, but I think trying to release that bound would end up with quite a lifetime puzzle... I'll give a look to what you've done with dbus, it'll help me get a beeter view of this sort of issues. Thanks for your input :)
I've been playing with a [project](https://github.com/stensonowen/page-mon) which periodically checks on web pages, `diff`s the page with a cached version, and alerts you with any updates. It's still very much in the brainstorming phase, but I keep making things needlessly complicated (read: interesting), like by using a cron-esque config file parsed by a LR(1) grammar (shoutout [lalrpop](https://github.com/nikomatsakis/lalrpop)). I don't expect the project to really go anywhere, but it's a good excuse to play around with some interesting crates.
&gt; `m_pevt` is of type `ppm_evt_hdr`. You mean the type is *pointer to* `struct ppm_evt_hdr` or an *array* of `struct ppm_evt_hdr`, right? In that case, the C code looks unnecessarily complex. uint16_t *lens = (uint16_t *)((char *)m_pevt + sizeof(struct ppm_evt_hdr)); char *valptr = (char *)lens + nparams * sizeof(uint16_t); should be uint16_t *lens = (uint16_t *)(m_pevt + 1); char *valptr = (char*)(lens + nparams); instead. But this all seems very dirty and problematic w.r.t. alignment and padding. Anyhow, (T*)(some_ptr + ofs) in C is equivalent to Rust's some_ptr.offset(ofs) as *mut T Also: Consider whether `*const T` is more appropriate. 
Trying to write a sane wrapper to `ptrace` to make interfacing with it not make you want to cut your eyes out. 
I've finally made my first open-source piece of software! [zrtstr](https://github.com/indiscipline/zrtstr) is a small utility for all the music-makers and audio-engineers. It checks if stereo wav files have identical channels (faux-stereo) and converts them to mono. Build using [Hound](https://github.com/ruuda/hound), [Clap](https://github.com/kbknapp/clap-rs) and [pbr](https://github.com/a8m/pb). It's a problem that persists with some of the older but still widely used DAWs, such as Cubase 5. You can't export both mono and stereo tracks at the same time, so frequently you get *lots* of faux-stereo files in your stem renders. Those, who ever bumped into this behaviour know how annoying it is. Please, feel free to critique and point to my errors. Any feedback would be much appreciated!
Not sure about lifetimes, but at least callbacks with just `'static` lifetime can own `Rc` references instead of `Arc&lt;Mutex&lt;_&gt;&gt;`
I note that the [Rust project](https://github.com/rust-lang/rust) itself isn't built with Cargo. Might it be eventually, or is there some reason for why that wouldn't work out?
Looks like you might want something like https://github.com/ludat/hado-rs or https://github.com/TeXitoi/rust-mdo
It's precisely the fact that I believe this is making my code less clear and readable that I brought this up. More and more indents makes the code look a bit silly, and a little straining to deal with (as you can sometimes find yourself with 4 or 5 indents, and end up being confused about which '}' belongs to which '{'. You also might find that you have some default actions you want to take if any one of the Options turns up to be None. What do you do there? Do you insert default_action() calls in the 'else's to each one? What if you forget/miss one? (By the way, it looks like this /u/Quxxy and /u/nayadelray 's suggestions might be helpful in this situation, but its still not quite as clean IMHO as the way I was first thinking, though this may just be personal preference.) I agree that Rust doesn't have to add all the features under the sun... You only have to look at C++ to realize why that's not a good idea. I'm not even that mad that Rust doesn't have this sort of thing. It's just that having this feature would have made certain situations I've come across just a little easier/pleasant to deal with, in my humble opinion.
This is a good idea - though right now things are a little too flat for it to be of much value I think? I'll add an issue though, thanks!
The more I think about it, the more I am coming around to /u/Quxxy and /u/nayadelray's suggestion for the specific situation I put up in the OP. Thanks to them for their suggestions. Still doesn't help when you need short circuit evaluation, but oh well. :) Thanks for the discussion and suggestions all!
I’m writing a demoscene production (my third one) using my [luminance](https://crates.io/crates/luminance) graphics framework on a *demoscene framework* of my own. I’ll release that in Germany in two weeks! I’ll blog about it, of course.
I struggle really hard implementing a git backend with git2 for my crate [imag](https://github.com/matthiasbeyer/imag). If someone can help that would be awesome. The branch "libimagstorestdhook/git" is what I'm working on.
&gt; DMD D compiler is not as optimizing as LLVM, but it performs several optimizations There's no comparison. LLVM has SSA with a huge list of algebraic simplifications in instcombine, GVN, LICM, SCCP, SROA, a full suite of IPOs, autovectorization, a pattern matching instruction selector, a highly tuned register allocation, instruction scheduling, a machine-level optimizer with things like LICM redone at the machine level, and that's just the tip of the iceberg. &gt; like allocating a struct or a stack array inside the caller function. I think currently Rustc sometimes misses this optimization I don't know what this means exactly, but if you see missing optimizations, please file bugs. &gt; Still DMD compiles D code with plenty of templates with optimizations much faster than Rustc compiles Rust code without optimizations. Because templates aren't anywhere near as complex as a full region system with a unification-based typechecker that features subtyping. &gt; And unoptimized Rust code is usually 5-10 times slower than unoptimized D code. Because Rust has more abstraction. D implements a lot of stuff in the compiler as built-ins. &gt; I can't help but think that on a modern PC a Delphi compiler could be five or six orders of magnitude faster than Rustc. Sure, if that Delphi compiler didn't do modern optimizations. But so what? Rust isn't Delphi!
Can anyone recommend inexpensive hotels near the venue?
Ah, I didn't realize that this was wholly separate from rusty-machine, I see. In that case what I said is a bit less relevant. I saw this as the SKlearn guide of rusty-machine essentially. Still, I think the code would help. In terms of dynamically grabbing examples I'm not sure of the best way to go, but you can probably just grab doc commented markdown? idk
[I proposed a `let ... else` expression](https://github.com/rust-lang/rfcs/pull/1303) to simplify code like this, but my RFC was postponed after failing to reach any sort of consensus. I have hope that some similar proposal might be accepted someday. In the near future, `catch` and `?` and `try!` might be extended to work on Option as well as Result. For now you could use or write a macro, or you can use Option combinators to make the code a tiny bit more concise (but no less nested): pub fn collides(&amp;self, id_one: usize, id_two: usize) -&gt; bool { self.get_collision_rect(id_one).and_then(|rect1| self.get_collision_rect(id_two).map(|rect2| collision(rect1, rect2))).unwrap_or(false) } 
I'm creating a ffi binding and a wrapper for xcb-image https://github.com/jeandudey/xcb-image-rs.
Google can sometimes make things more complicated than they need to be. So essentially this is like running a continuous query but it is exposed as an event system?
Cool stuff. It's been decades since I last worked on demoscene stuff. Ah, the memories.
While burkadurka is correct that you cannot (easily) hold references in a struct to data that struct owns, in your example the data that you reference is external to both `Bar` and `Foo`, so you can use multiple lifetimes and `PhantomData` to express that the references `Foo` yields are tied to data that outlives the struct itself. You likewise need to alter `new` to express that the `&amp;str` from which you create `Bar` outlives the instance of `Bar`. When you do that, you don't need to store `Foo` (or have it mutable according to the API you shared). [Example](https://is.gd/WchD7F). 
Erm, more like an Eventual Consistent data store, where data write is fast.
The interesting thing is that potentially it could be used as a queue too, it supports live stream of new events appended to the log. It does not work very differently from how kafka works, it has the same concept of offsets on the query side, with the only (major) differences that logs are not partitioned, it currently does not support clustering, and there's no concept of storing offsets, that would need to be handled in the application level code on the client side.
Can you elaborate on that? I understand that i32 would use Copy semantics, so this would work just fine: fn value_plus_square_ref(x: i32) -&gt; i32 { x * x + x } fn main() { let mut x = 32; println!("x = {}", x); x = value_plus_square_ref(x); println!("x = {}", x); } But in terms of actual "waste", would compiler do things differently or would it optimize the difference away between the two versions of the code (with and without a reference)?
So using https://rust.godbolt.org/ with 1.9.0 nightly: code: pub fn foo(x: i32) -&gt; i32 { x } pub fn foo_ref(x: &amp;i32) -&gt; i32 { *x } gives this asm: pushq %rbp movq %rsp, %rbp movl %edi, -4(%rbp) movl -4(%rbp), %eax popq %rbp retq pushq %rbp movq %rsp, %rbp movq %rdi, -8(%rbp) movq -8(%rbp), %rdi movl (%rdi), %eax popq %rbp retq Big differences: that final `movl`, `movl` vs `movq` and the `-8` vs `-16`. What gives? First of all, on x86_64, a pointer is going to be 64 bits, not 32: so we end up having to use twice as much space. This is the 8 vs 16 issue: `8x4 = 32`, `8*8 = 64`. So we're wasting space there. This is also why the second is `movq`; we're also moving 64 bits of data, rather than the 32 with the `movl`. Finally, there's the `movl (%rdi), %eax`; this is the pointer deference. Since we're passing a pointer, not a value, we need to grab the value from that pointer, rather than just using it. So, in summary: * we're using more memory, which is wasted space * we're moving more data * we have to dereference the pointer. Does that make sense? And of course, on such a trivial function like this, it probably doesn't matter too too much...
I am a Web Developer and just learning Rust. In here I tried to summarize what I learned. Mostly I studied and wrote this at late nights, so this can have many mistakes. Please correct me if you found any kind of mistake, even in grammar, even at the first post :) Gitbook version can be found on https://www.gitbook.com/book/dumindu/learning-rust/details Thanks
&gt; Oh well, I guess I'll pretend I didn't see this and keep going. Multiple implementations are good! Software is often about tradeoffs, and different versions can make those tradeoffs differently.
Is there some kind of solution for defining constraints? Does anything like that have to happen in application logic in interpreting the events?
Thanks for your reply. I finally got a working solution: Building my own OwningIterators :D They hide the mutation of pos (the only thing mutating while iterating over a collection) and thus allow both ownership and handing out references. Unfortunatly I had to turn all my iterator implementations upside down. But the user gets an abstracted iterator over the results (which are only u64, thus copyable). And it is not even slower than my old solution. So all in all I am quite happy. Nevertheless this exercise leaves me thinking about the lifetime of mutable objects. If I only care about fields a b and c of a struct why does a change of d invalidate the structs lifetime. Using RefCell helps, but does not feel quite right. Here is my final Code: pub trait OwningIterator&lt;'a&gt;{ type Item; fn next(&amp;'a self) -&gt; Option&lt;Self::Item&gt;; fn peek(&amp;'a self) -&gt; Option&lt;Self::Item&gt;; fn len(&amp;self) -&gt; usize; } pub struct ArcIter&lt;T&gt;{ data: Arc&lt;Vec&lt;T&gt;&gt;, pos: RefCell&lt;usize&gt; } impl&lt;'a, T: 'a&gt; OwningIterator&lt;'a&gt; for ArcIter&lt;T&gt;{ type Item = &amp;'a T; fn next(&amp;'a self) -&gt; Option&lt;Self::Item&gt; { let mut pos = self.pos.borrow_mut(); if *pos &lt; self.data.len() { *pos += 1; return Some(&amp;self.data[*pos - 1]); } None } fn len(&amp;self) -&gt; usize { self.data.len() } fn peek(&amp;'a self) -&gt; Option&lt;Self::Item&gt; { let pos = self.pos.borrow(); if *pos &gt;= self.len() { None } else { Some(&amp;self.data[*pos]) } } }
I'm happy that you have a solution. Small nit though: for simple copyable types like `usize`, just use `Cell`, not `RefCell`, as it doesn't have any additional runtime cost. Overall, it seems that you've made something really similar to that "streaming" iterator :) (Btw, I wrote `-&gt; &amp;'a T`, I meant `-&gt; Option&lt;&amp;'a T&gt;` obviously). I'm only curious, why `&amp;self`, not `&amp;mut self`?
Hey krdln. I found your streaming iterators by the way: https://github.com/emk/rust-streaming Cell is a good idea, Ill do that. Thanks. next(&amp;mut self) allows for only one Item to be valid at a time. In some cases I need to use the result of peek and the result of next at the same time. So that would not be possible.
Hey yinz, I was originally supposed to give this talk only one time, as a keynote at FOSDEM, right before Rust 1.0. Then FOSDEM lost the video. So I gave it again for the ACM, and added in a bunch of stuff that happened in our first year. Slides are here: https://github.com/steveklabnik/history-of-rust
Is it similar to Kafka then?
Can structs be parameterised by other things than types? For example, you could have a `Matrix&lt;T&gt;`, which has elements of type `T`, but can you also have something like a `Matrix&lt;T, n_rows, n_cols&gt;`, which specifies its size, so that matrices of different sizes can be seen as different types at compile time? The only thing I've found is [this question](http://osdir.com/ml/rust-mozilla-developemnt/2013-08/msg00249.html) from three years ago. Apparently, it wasn't available then.
[Another post](https://www.reddit.com/r/rust/comments/4vmnkj/multiple_whileif_lets/d5zlk51) already covers how to do it if you want both expressions to be ran (which might not be easy if you're calling functions and have side-effects). Here's how I'd propose doing it with short circuiting. if let Some((rect1, rect2)) = self.get_collision_rect(id_one).and_then(|rect1| self.get_collision_rect(id_two).and_then(|rect2| Some(rect1, rect2))) { It does add a bit of extra, but I feel this is fine because it makes it very explicit that ordering is happening on how things are calculated. Honestly I'd rather that the default boolean operations didn't short-circuit because this might complicate things in tight-loops were a branch can break the pipeline's rythm. So that way you'd have the standard operators which eagerly run both sides and the delayed operator method (maybe boolean methods `or_then(self, f: Fn()-&gt;Bool)` and `and_then(self, f: Fn()-&gt;Bool)` when developers want to explicitly opt out.
It has some similarities and a few differences: 1. it has the concept of collections rather then topics but they are both file based 2. a consumer can consume from only one collection at a time 3. it does not have a configurable retention period, events are persisted and never deleted by default 4. it hasn't been designed for massive scale (yet?), but for events order guarantee 5. the queries are quite different, and they are inspired by Akka Persistence Query (http://doc.akka.io/docs/akka/snapshot/scala/persistence-query.html#Predefined_queries) It should allow to write events quite fast, on my machine it hits around 350k small events/s when writing, and around 500-600k when reading. Performance, of course, deteriorates depending on the number of collections and active subscriptions. Haven't looked at performance optimization yet though, it might come later, but in general it should become almost as fast as disk I/O allows (for writes). The use case is different than Kafka's, most real-world application don't really need an infinitely scalable queue/buffer, but they might need a quite fast event store on the write side, and the ability to do event sourcing from it from a not-so-massive number of consumers and build projections from them with a guaranteed order of events. I've ran a few tests with collections of ~10GB/200 million events, and writing/reading from it is still quite fast, it might need to split up the logs into different files at some point, but I didn't feel the need yet.
This is the sort of smartness I wanted to see if someone could inform me of. I'm still not used to thinking in a functional mindset, so I find it hard to remember all the stuff you can do with options and iterators. Maybe some time soon I need to investigate a pure functional language to get a better idea. Thanks.
Yeah, but "abi only" changes lead to errors or pass through cleanly with automatic conversion. Manual bindings just leads to horrible silent corruption in this case. Avoiding that is a really big deal, imho.
This is an interesting question to me. How transitive should trait bounds be and when can you rely on them? The degree of transitivity is related to backwards compatibility. For instance, `Vector: MulAssign` *now*, but might not be in the future. If the `MulAssign` bound were completely transitive and `foo` didn't need to specify it, then `foo` would suddenly stop compiling if the `MulAssign` bound were removed from `Vector`. If `MulAssign` is not transitive, then `foo` needs to re-state its requirements and thus when the bound is removed the function would continue to compile for anything that worked while `Vector: MulAssign`. Are there any rules on this? I could see relying on transitive bounds to be okay in default methods of the trait but beyond that I'm not sure. I can also see that not allowing transitivity may result in extra verbosity when it's not strictly needed. Perhaps it's enough to say you can rely on transitive bounds within a crate but not across them? I tried a brief search to find these answers but came up dry.
No, there's no way to do this yet. Eventually `const fn` will be stable and usable in array sizes, but even then `size_of` won't be const due to various implementation things, and I don't know what the plan is for solving those.
*shameless plug* Until Cargo learns to lazily cross compile core for you, you can use [xargo](https://crates.io/crates/xargo). `xargo build --target cortex-m0` will lazily cross compile `core` (and some other crates like `alloc` and `collections`) and manage a sysroot for you so you don't have to manually fetch the rust-lang/rust repo, build `core`and copy it to a sysroot.
*shameless plug #2* &gt; Ditching the device/hardware abstraction layer If your ultimate goal is to go 100% Rust and 0% C, you might be interested in checking out my WIP [Copper book](http://japaric.github.io/copper/). Right now it walks you through setting a Cargo project to build Cortex-M "executables" that you can directly flash in your microcontroller and how to use OpenOCD+gdb to (step by step) debug your Rust programs. Though I wonder how much of it it's applicable in your case because it seems you are using a bootloader to flash your programs ("Copying a .hex file into it flashes the program onto the micro-controller") instead of a JTAG interface, which is what OpenOCD uses. I think you might not be able to debug but still be able to use Cargo to directly build flashable executables if you use a carefully crafted linker script that doesn't overwrite the bootloader.
Like I said, I'd like it if the language had buckled down and gone the way of "operators and function arguments are *always* simultaneous (no guarantee of order) while operations in optional order are always through method chaining". That way you'd solve the problem with similar patterns everywhere.
What are you using for webdev with rust? I want to start developing web applications (nothing complicated), and i'd like to use rust, but i don't really know where to start. 
One might even call it a heinous misappropriation of the English language. :)
Without the commit hash of the compiler for that slide, I can't say for sure :) but `obj` seems to have been the syntax for structs, not functions, so it does seem like a typo here.
Jesus Christ. So yeah, heinous.
This was a really great talk. Loved listening to it. Thank you for doing it!
For those of me who live under a rock and haven't ever really looked into what MIR is, [link](https://blog.rust-lang.org/2016/04/19/MIR.html) with an explanation.
On a side note, blanking values is irrelevant in a memory safe program anyway.
*Shhh!* That's going away soon, isn't it?
It's moving to the stack, not going away entirely.
Ah yes, spit those Hegelian dialectics. Nice talk Steve :)
Would that CRC code be worth breaking out into a separate crate?
Is there any requirement to use a specific type of compression? Or to reduce the size by a certain amount?
Incredibly exciting :)
I am not entirely sure; I collected these samples from diving into old builds of the compiler, so maybe I made an error. Sharp eyes!
Looks promising! &gt; Another definite possibility is to adapt the ideas into existing renderers Have you considered contributing your ideas to the rusttype crate? I use it in a few projects and although I've not run into any performance issues with it, I can't help but be curious about whether or not this could improve it even further :) /u/ZRM2 I'd be curious to hear your thoughts on the post.
 &gt; Possibly font-rs becomes production-ready. This would depend on community interest. I can already tell you that #servo seems to be interested in this. :) And I bet the Rust gamedev community as well.
Good first contribution to Servo: port it off freetype onto font-rs. If you're a masochist who likes working on difficult, time-consuming things for a first contribution.
That depends on the use case. To replicate everything in freetype would be a massive undertaking. I'm personally not that interested in hinting or rendering modes other than grayscale, because those are primarily of interest on low-res displays. I am interested in CFF, but it's nontrivial. And it's research-quality code in many places, lots of `unwrap` rather than robust error handling. Making it more robust and adding CFF would possibly bring it to the point where it could be used, say, in a text stack for a mobile operating system. A large part of the point of publishing this is to gauge how much interest there is, both in using such a library, and potentially collaborating on it.
I'm happy for the ideas to be adapted into rusttype, but pretty limited on time to work on it myself.
Fair enough :) 
In Rust packing is a structure attribute so `pack(push)` does not exist *but* you'll have to apply the relevant packing to all structures (rather than pragma once at the top of the file and have it applied to all structs). That aside, to byte-pack a structure you [`#[repr(C, packed)]`](https://doc.rust-lang.org/nomicon/other-reprs.html) it. `repr(C)` means rustc must not reorder struct members (which it can do with the default repr) and `repr(packed)` means no padding (I don't think that's configurable with a width). #[repr(C, packed)] struct ppm_evt_hdr { ts: u64, // timestamp, in nanoseconds from epoch tid: u64, // the tid of the thread that generated this event len: u32, // the event len, including the header type_: u16, // the event type } should give you what you want. Note that the last element had to be renamed as `type` is a keyword in rust, and as previously indicated, *you'll need to individually pack each struct* between this one and an eventual `#pragma(pop)` or the end of the file.
No. Minimum LLVM version is currently 3.7 and will stay the same for the moment.
That doesn't seem right. Aren't the glyphs cached once they're rendered and only re-rendered when their size changes?
Try to repeat your thought process for CJK scripts where you can actually have thousands of different glyphs in a single screen :)
&gt; `repr(packed)` should be equals to `pack(1)`? Yup, with the difference that `#pragma pack(1)` will set alignment boundaries to 1 for all structures until the end of the file, whereas `repr(packed)` will only do so for the specific struct to which it is attached. &gt; Can you elaborate a bit on "repr(packed) means no padding"? Just that, `repr(packed)` is an on/off toggle to ignore alignment (and thus not add padding bytes between struct elements to align them according to the platform's guidelines).
I cannot recommend any that are very close, but it's easily reachable by subway, so travel times from anywhere in Berlin shouldn't exceed 30 minutes, even at nighttime. I generally enjoy the Circus Hostel/Hotel (they are different, one over the street of the other), which is well connected, it takes you 1 tram and a bit of footpath to reach the venue. 
Can someone give me an ELi5? Does it really require much horsepower to render simple text on screen when say for example, the first ever video games were text based? 
Is the LLVM community aware of the problems you are having with compiler-rt? What do they think about it? Don't other frontends have these problems? (e.g. LDC, GHC's LLVM backend, Go...) To me it kind of seem that it would be better to improve compiler-rt for everybody instead of hacking around its build system or rewriting it. I kind of wish rust would be able to always track LLVM master someday.
I'm making a [Neko](http://nekovm.org) VM in Rust using [jit.rs](https://github.com/tombebb/jit.rs) and [rust-gc](https://github.com/Manishearth/rust-gc). So far I've almost got a simple hello world working. I've also been working on [jit.rs](https://github.com/tombebb/jit.rs) itself by adding new structs for more specific types and updating old assertions so they actually work.
You can conditionally move. For example let x = String::new(); if rand() { drop(x); } println!("x might still be alive"); // drop x if drop flag not set
&gt; Because that's for a single glyph, and a web page can have thousands of glyphs visible at any given moment. Yeah, this is true. You can actually get big slowdowns on modern browsers when rendering pages that have a lot of complicated glyphs, such as Chinese characters or z̧͈̹̲̜̠̥̹̩̜̣̠͓̝̬͓̜͝à̢̕͏҉̳̼̫̱̜̬̥͉̻͇̥̫l̨̳͎̞̘̦̣͚͓̞ģ̶̴̩̜̭̬͙̟̯̰̖͎̖̕ͅo͠҉̨͉͕̭͔̪̳̜͈͔̣̣͟ ̸̴̡̤̩̼̖̘̦̖͓́͞t̴̷̙̭̦̼̮͎̭͔͚̺̗͖̥̞̣͓̺̖͝e̸̜͉̹͜͞͞x̷̛̫̜̝̱̕͡t̸̨͔̩͖͇̭̩̲͘. They're pretty efficient these days though but they get re-rendered quite often (e.g. when a page's layout changes as a result of some JS driven DOM manipulation on the page.) So it's definitely still relevant to make our font renderers more efficient, even today.
Nitpick: [This](http://i.imgur.com/AU2bdqN.png) confused me for a little while. The zero looks like the letter 'o', so I read it as "46 ons". A space before the unit would remove any ambiguity.
I think we need a story around using simd on stable Rust. How do we accomplish that without stabilizing a bunch of intrinsics? Maybe crc is a special case, I don't know. The regex crate uses the simd crate, so I'd like to see a path to stabilization there. (I might try to work on that, but my knowledge of the domain is fairly limited.) There is still a problem with actually getting target features to work though, yes? I think I had to set it as a special compiler flag to rustc using RUSTFLAGS, or am I missing something? And then there is cpuid, which would let us choose the right implementation at runtime (Go does this for crc in ASM). Rust already has a few nice crates for it too, but they all require inline assembly last time I checked.
But that means the stable users wouldn't be able to enjoy SIMD whereas with /u/raphlinus solution everyone can enjoy it, right?
There's a world of difference between bitmapped and vectorised fonts. A bitmapped font is just a series of pixels that are overlaid at the location you want. But you can't scale it up or down without it looking ugly. A vectorised font is a set of instructions outlining a character with straight lines and curves. The character can be easily scaled up or down and still look perfect. But the graphics library needs to a) calculate where the lines should go, b) determine which pixels are inside those lines, and c) draw those lines and filled in pixels. It's a lot more computational power for the convenience of infinitely scalable perfect looking characters.
Right. The issue is, if nobody uses the SIMD crate, it will never get stabilized, so we will always need C for this. Maybe the only issue with the SIMD crate is that it only works on nightly, or maybe he tried to use it but it had other problems. Any feedback would be really appreciated here since intrinsics like SIMD are a pain point for some people.
Strange, there are still blockers listed on the "Launch MIR" milestone. What happened to the idea of testing for performance regressions across crates.io first?
`pacman -S mingw-w64-x86_64-freetype` is working fine for me. If you have a specific problem, can you open a bug report here: https://github.com/PistonDevelopers/freetype-sys/issues ? Fixing those should be easier then completely rewrite FreeType in Rust.
True, but using a signed distance field can be used to render the glyph via shaders which is a lot faster then rasterizing it every frame on the CPU. &gt; And I'm pretty sure I've read that imgui is heavily bottlenecked on text rendering, in general. I don't know imgui, but I bet they are already caching the rasterized glyphs. So I don't think that FreeType has anything to do with that bottleneck.
I believe that's still happening before they decide to leave it on for good. In the meantime, they want broader exposure to the nightly ecosystem to tease out any regressions people might have that aren't related to crates.io. There's still plenty of time to roll back MIR before the next beta, should it come to that.
Rock-dweller here, thanks. I was wondering what the hell Ubuntu's display server had to do with Rust.
Congrats to everyone involved! (Btw we're in space now folks.) EDIT: You saw nothing.
While being in space is pretty cool, a lot of text being basically invisible not so much.
That's the problem with space. It's mainly void.
[Mir](https://www.youtube.com/watch?v=iVLiFviKKzM)
Felix von Leitner better known as fefe is a popular german blogger (he mainly blogs political topics but also about software and security sometimes). He owns an IT-security company and is well known for his preference of C over virtually every interpreted language and his rants about bloatware and antivirus products. **Translation:** Did you hear about Rust? This is [a programming language](https://www.rust-lang.org/) which was proposed by the the Mozilla-people. Purly idiomatically it is sort of C++ with blatant self-restrictions and the new feature in Rust is that the compiler enforces these self-restrictions! Imagine for example that you may not modify a container (this means also: transfer no non-const references!), as long as there still exists an iterator to this container. Rust has some really great ideas at this point and the promise of Rust is to erase memory corruption as an error class. Sadly Rust is from Mozilla better known for fucking stuff up than that in finite time something working is produced. But Rust seems like an exception in the Mozilla portfolio, where it is really like that. I observe this in any case fascinated from afar and wish all the best. The main problem for something like that is that in practice the real spooky code bases which would really need something like Rust, are things like Firefox. Far too big as that anyone could realistically do something about it. This is the usual objection. And then the Mozilla-people write a [HTML5-Layout-Engine in Rust](https://de.wikipedia.org/wiki/Servo_(Software%29). I am cautiously optimistic. But well, a layout engine is one thing, but what about all the codecs? And the font rendering? Personally, I do not trust even the whole font render engines. Personally, I do not trust even the whole font render engines not so far as I can take a small car. And it turns out: [in work](https://github.com/mozilla/mp4parse-rust) (so far only the mp4 container, not the actual codecs; but hey, yes you have to start somewhere), and [font rendering is also in the works](https://medium.com/@raphlinus/inside-the-fastest-font-renderer-in-the-world-75ae5270c445)! And then suddenly it turns out that it is possible to do font rendering a nearly a magnitude faster than before (comparison with Truetype, default engine on Linux and Android)! Why am I mentioning Android? Because the fellow who is doing it is from the Android-UI-corner! That is on the one hand, we have to again thank Google here that spends its money for things that will help us all, and secondly that Android may jump on the Rust-train. And of course all is [open-sourced and on Github](https://github.com/google/font-rs). Messages like this that cause me a restrained sense of hope. Maybe we'll get all that shit still regulated. Update: [There are also people writing an OS in Rust.](https://www.redox-os.org/) 
For me only working on nightly would be the deal breaker. But since I only dabble occasionally on Rust during weekends, plane travels and so on, don't value my comment too much.
I think you've got your programming languages mixed up. Rust uses `()` where C uses `void`, and `Void` in Rust can never exist.
fefe is also known for writing [dietlibc](http://www.fefe.de/dietlibc/).
Indeed. It's a more subtle argument than just measuring raw speed, but a real justification for this work is that the sweet spot for cache size will very likely be considerably _smaller_ than if your cache-miss case is unacceptably slow.
Simple `&lt;` comparison should be cheap, so maybe you don't actually need short circuiting. In that case, this should work: while let (Some(x), true) = (something(), x &lt; some_value) { ... } Edit: x won't be in scope yet, oops
Good stuff!
Very cool! I'd be very interested to see how well the simple act of using Rust mitigates against some of the font-parsing/rendering attacks that have been featured on the Google Project Zero blog.
`enum Void {}` innit?
&gt; There's still plenty of time to roll back MIR before the next beta, should it come to that. Not just that, but turning it off is a boolean, so the patch is very, very easy.
This is very interesting. Is there a quality comparison between freetype and font-rs though? It'd want to see if there are any font-rendering improvements or regressions. It might be a good replacement for GIMP (where I get cutoff text in bold fonts sometimes).
Thanks for the questions! &gt; Is it only for large stuff? It will also work in the more general purpose cases - and maybe it will even perform pretty well. While developing I focused on using methods that would scale well to large amounts of data - occasionally in ways that would make it work less well (read _less efficiently_) for smaller data. &gt; Are there any cases where a different library might be better? If you're working with low dimensional matrices like 2x2, 3x3 there are often closed form solutions to a lot of these linear algebra problems. In that case a library that takes advantage of those will likely be a better choice. In this case [nalgebra](https://github.com/sebcrozet/nalgebra/) may be a better choice (among others). &gt; Is there any way to distinguish type-wise between matrices of different sizes Sadly no. I believe this would require integer generics - and even then maybe something more would be needed. That said - if you're working in low dimensional spaces (3x3 and 5x4 for example) then nalgebra supports different types for these (I believe). I'm hoping to do some benchmarking soon to figure out more accurate answers to the above. 
Naw man, [MIR be all like](https://spaceshipaloha.bandcamp.com/album/universe-mahalo-volume-1).
Oh, I see now, that is a bit higher level than what I was thinking. If you compare the benchmarks from your `rust-snappy` fork with the benchmarks from my library (or the C++ implementation), you'll see that they aren't that close. :-) The tricks I'm thinking about are things like "don't call `memcpy` for small copies" or "copy 8 bytes at a time instead of 1", etc etc. In that context, we already have a buffer in memory, so the small/large reads/writes from IO are a bit further up the chain. For my implementations of `Read`/`Write`, I'm using a custom buffer (not `io::BufReader`) and I don't think I used any `unsafe` there.
&gt; I thought the bitmap was used after that to blit the characters to the screen. That's true, but you end up rendering new bitmaps pretty often. You need one for each unique Unicode code point × font size (and possibly for each subpixel offset and set of combining characters?).
Yeah I hadn't even gotten to try to optimize vs the C++ implementation. This was just purely against the unsafe rust implementation I was trying to improve :)
&gt; But since I only dabble occasionally on Rust during weekends, plane travels and so on, don't value my comment too much. If Rust wants to grow the size of its community, it's very important to value input from those who aren't yet using it frequently (or at all).
Yup, understood now. Can't wait to see what happens with stateful. Super exciting stuff!
We got generators working pretty [well now](https://github.com/erickt/stateful/blob/master/tests/test_generator.rs)! I'm currently refactoring things to get [async/await](https://github.com/erickt/stateful/tree/async), which really doesn't appear to be that much work to add.
Congratulations, that is really impressive. Kudos also to the great work done on `crater`. I have the impression it really helped stabilizing `rustc` and thus I do assume the number of new bugs introduced by carving out the guts of `rustc`is very little (although they may be serious).
We have a few qualms with LLVM and do intend to get in closer communication with them. I also sit right next to an LLVM committer and give him a piece of my mind more often than he would like. But with respect to compiler-rt we have independent motivations for moving to our own implementation. The primary one is that we are trying to get to a place where cargo can rebuild all the code for any target, so compiler-rt must become just another Rust crate to make that happen.
&gt; I'm personally not that interested in hinting or rendering modes other than grayscale What do you mean by grayscale? Rendering w/o subpixel rendering? Shouldn’t it be fairly easy to add that, maybe even as some kind of prost-processing by downsampling?
To answer your side question: a font contains glyphs, which are a layer or two removed from codepoints. A font cache is keyed on glyph+size (and perhaps other stuff). If the font designer drew out how a particular combination of characters should render, the font's ligature table can then map a pair of combining characters to that single glyph. But at the layer a font renderer works, it's unaware of these things -- it is just handed a glyph and asked to render it. http://unicode.org/faq/char_combmark.html talks about this a bit.
Thanks for keeping the updates flowing. I love seeing the progress. And I really like your minimal website style.
How often is llvm able to optimize away the copy though? When you move values into a function, is it typical for that to be optimized away?
Which nightly should this end up in? The next one?
I'M SORRY, WHAT WAS THAT?
The Rust game subreddit is over here: /r/playrust This is the subreddit for the Rust programming language by Mozilla, but we do appreciate some nice fungi, if you have any.
I ran out of hard drive space on my macbook and found that I had something like 30GB of compiled rust output! So I hacked this together; hopefully someone finds it useful!
Thanks! It was quickly hacked, in the last few weeks, ... I'm not much of a webdesigner... I do not like the font yet, but I like everything else and I won't change it.
Small suggestions: you can use `dirname` instead of `sed`. And you may want to quote that `%s`.
You can check my thoughts a few inches below, someone already pointed this out.
Okay, next is Linus... might need some effort still \^^
For what it's worth, you can do this all with a `find` one-liner. $ find . -name Cargo.toml -type f -execdir cargo clean \; This finds only files with the name `Cargo.toml` and runs `cargo clean` only in the directories which contain that file. Edit: Note - On Debian with `find 4.6.0`, I keep getting a "no such file or directory" message for the `target` directory when it actually executes. Everything is cleaned, just the message gets printed. It's from `find` trying to recurse into the `target` directory, but I haven't found a way to prevent that yet given that `find` seems to build up its list of next directories as soon as it enters one. This also seems to be a regression. I don't get the same results with `find 4.4.2`. BSD `find` on Mac also works fine.
On [Swift](https://blog.fefe.de/?ts=ad6dcda7): He references a page with microbenchmarks saying Swift is often more than magnitude slower than Objective-C because of reference counting. He expects real programs to also run slow on Swift. I did not find any mentions of Go or D. He sometimes rants about interpretet languages and especially their module systems like npm, ruby gems and others. He would not like some mini-crates in Rust. ;)
All of Linus's [complaints](http://harmful.cat-v.org/software/c++/linus) about C++ aren't true in Rust. * No Exceptions * No Allocations without programmer knowledge I doubt the LinuxKernel will ever be ported to Rust. But maybe one day he'll have another Git level panic attack and code a while user land program in Rust. 
Is it valid to use "x" after the "if" block ends? If yes, is there a runtime check to determine if "x" was not dropped yet? If no, why doesn't there exist an implicit "else" block which drops "x" unconditionally?
that's a knightly accolade!
The order is specified in the language itself, not by the user. We could specify it to work the other way.
HE CANNOT HEAR YOU BECAUSE WE'RE IN SPACE! (so this message is useless as well, damn...)
There was a significant amount of discussion about how things should be dropped when the details were being nailed down (including discussion about so-called "eager" drop semantics), e.g.: - https://github.com/rust-lang/rfcs/pull/239 - https://github.com/rust-lang/rfcs/pull/210 - https://github.com/rust-lang/rfcs/pull/320
That's awesome! This example emphasise really well what MIR has unleashed (and will). This is kind of a hello world for static drop, am I correct?
Indeed. The `#[unsafe_no_drop_flag]` attribute has no operational effect under MIR trans, static/stack-dynamic drop takes over.
Jokes on you guys, I just posted here in order to find better ways of doing it. MUAHAHAA
The line in the `configure` script for that option is "get MIR where it belongs - everywhere; most importantly, in orbit".
sorry, by packages I mean libraries. So much stuff but so unorganized and verbose :| For gui I would like something like Electron, but right now libui is the best that we can use. The only reasons the we are waiting until next year are compiling time and better tools (Intellij IDEA support). Rust has a great future and we will walk together!
Presumably because of [Mir](https://en.wikipedia.org/wiki/Mir), a space station.
Type layout doesn't change with `-Zorbit`, though. It will take a couple releases before we can get rid of the flag completely.
Ausgezeichnet!
If there is any intrinsic that you need but it is not exposed please fill a small issue. Adding missing intrinsics to rustc is really easy (it is a good way to start hacking on rustc). The one you mention, `_mm_shuffle_epi8` is [`shuffle_bytes`](http://huonw.github.io/simd/simd/struct.i8x16.html#method.shuffle_bytes), it is implemented [here](http://huonw.github.io/simd/src/simd/src/x86/ssse3.rs.html#164) in the SIMD crate. It basically calls an `extern-intrinsic` function that the rust compiler defines/implements [here (in `librustc_platform_intrinsics`)](https://github.com/rust-lang/rust/blob/master/src/librustc_platform_intrinsics/x86.rs#L1320) to call the LLVM IR `llvm.x86.ssse3.pshuf.b.128` intrinsic. The intrinsic jump tables in those files are automatically generated from [this platform intrinsic generator python script](https://github.com/rust-lang/rust/blob/master/src/etc/platform-intrinsics/generator.py) which basically defines a domain specific language for adding new intrinsics (no, i'm not kidding you, somebody actually created a DSL for adding new intrinsics). Basically it reads some [json files](https://github.com/rust-lang/rust/tree/master/src/etc/platform-intrinsics/x86) describing the intrinsics and generates both the jump tables for mapping them in rustc to llvm IR intrinsics as well as blocks of extern functions that you can directly bundle in your rust code (e.g. to wrap them in a library like the SIMD crate does). The DSL might be a bit over-engineered but it is actually pretty awesome. For example, adding AVX intrinsics that work on _a lot_ of vector types is a breeze: just specify the pattern, the range of vector types, and done, [all in ~5 lines of json](https://github.com/rust-lang/rust/blob/master/src/etc/platform-intrinsics/x86/avx.json#L27). So adding a new intrinsics is basically adding `~5` lines of json to specify it, regenerating all the jump tables, and done.
Alright, my mistake.
Have you tried that? I don't think the x variable exists yet in that statement (It gets introduced in the 'let' part of the statement). Something() and x &lt; some_value get evaluated, put into a tuple, and then taken out of the tuple. X is initiated when the values are taken out of the tuple. Your solution relies on short circuit evaluation, and I don't think it should work.
Awesome! Thanks so much to you and eddyb!
I've never been rooting for a firefox release to go smoothly as much as I did this time :)
Question, have you done any programming in C/C++ (Not judging either way, btw)? I'm asking this, because it's useful to think of references as values themselves. In C, if I create a pointer: int *a; This is itself a value: the location of the value *we are pointing to* in memory. This could be on the stack, ie: int value = 3; int *a = &amp;value; or on the heap: int *a = (int *)malloc(sizeof(int)); *a = 3; In both cases the pointer is on the stack, and it's pointing to another location in memory, either on the stack or the heap. Rust references are basically glorified pointers (the difference being that Rust references CANNOT be null). These are denoted as: let a: &amp;i32 = &amp;value; // the ': &amp;i32' is optional, I just put it here to show what type 'a' is. In this example, 'a' is a reference to 'value', and 'a' is on the stack. 'value' may be on the stack or the heap, 'a' doesn't care. The memory is owned by something else. Thus it's okay to copy that reference as much as you like. (Rust has rules regarding keeping multiple '&amp;mut a''s, and having constant references and mutable references at the same time, but that's another matter.) Box&lt;T&gt; and Vec&lt;T&gt; are pointers that own the memory. Any memory that is owned, needs to be on the heap, as stack allocated memory gets deallocated when the stack frame it belongs to goes out of scope (ie. the function it's in ends). Behind the scenes Vec&lt;T&gt; just contains a pointer to the owned memory, and the length of the vector. Doing a shallow copy of a Vec&lt;T&gt; would result in two "owning references" to that memory, which could result in the memory being deallocated twice, or accessed after that memory has been deallocated, resulting in undefined behavior (This is the cause of some pretty bad security breaches in C and C++). This is bad, hence why Rust has the "move" semantics for Vec&lt;T&gt;'s instead of straight up copy like in primitives like 'i32'. Hope this helped in some way.
That's a spacy meme.
I've never used C or C++, I've heard a *little* about pointers but never actually used them before learning rust. Helped alot, thanks.
Rust drivers would be good...
Nice! It would be better if the text crate was included. Lack of formatted input (scanf / scan!) is annoying...
http://www.portal2sounds.com/198 (may or may not be spoilers for Portal 2)
I'm working on a static site generator. I called it "Aluminum" because it seemed right, I'm not that creative, and no one else had taken the name yet (that I know of...). Anyway, all the work is going on in the open in a public Git repo and I'm doing all my project planning in a public Pivotal Tracker project. I've made some good progress after the past few weeks and using a tool like Tracker helps me stay motivated and on track. I'm getting into some more difficult stuff, though, so the progress will likely slow down. Anyway, if you're interested in it, here's a link to it: https://github.com/ELD/Aluminum-rs
Nice!!!
Yeah, when I think about it that wasn't the best example. I just thought a pit-fall of coming from Java to Rust would be not realizing you need to be explicit about the references in function calls, whereas Java hides that. Unfortunately, as you said, Rust is technically passing a slice, instead of purely a reference to the vector, but in practice we can often think about it as being a reference to the vector.
In case anyone wants to read more (in German): http://mspr0.de/?p=4272
The air around fefe that he is an instance in technology of that kind is really harmful. I've been doing community work for technologies that were often a target of fefe and most of the time, issues were far from as big as he made them or even different. When informed about this, his answer is always the usual one: he wants to make sure his readers read stuff properly and don't rely on him as a source. That's a nice way out of things.
What kind of static site generator are you planning on building? I can't help but link to [my article from a year ago](https://pascalhertleif.de/artikel/silicon-zucchini/) -- feel free to implement any of those ideas! I wish I had the time to do it myself.
They are type aliases. Numbers are encoded as type-level binary lists. e.g. 12 =&gt; 0b1100: http://paholg.com/typenum/typenum/consts/type.P12.html
You're right, the x won't be in scope yet, I didn't notice that it was the same x. In a match statement, you'd be able to use a guard like this: match something() { Some(x) if x &lt; some_value =&gt; { ... } _ =&gt; { ... } } But that doesn't work with the while let syntax, unfortunately.
Err... n-no crashes, Sir, this is controlled lithobraking! With fancy explosions for show-off.
`!` *(Hopefully quite soon. Best RFC by now in terms of excellent language design.)*
Yes the glyphs are pre-rasterized. Completely off-topic with the original post, but in imgui it is only bottlenecked in the sense that items that are clipped/out of view often still need to calculate their width/height based on text contents. So if you submit one million items and rely on automatic clipping (*), the bulk of CPU time used will be spent calculating text size, which itself is rather optimal for what it does because the glyphs are already rasterized. I'm pulling glyph widths from a hot-array that is rather cache friendly. So this explanation has absolutely nothing to do with the topic which pertain to the rasterization of glyph/text. It is true that the majority of games uses a glyph cache so performance of e.g. FreeType aren't a big bottleneck, but it doesn't mean that there isn't room for other and better font renderers. And non-games usually have more variable fonts requirements than games. If people can get something that's 4 times faster they'll use it. (*)which is a bit stupid btw, if you have one million items than you should coarse clip them to only process those into view
&gt; If I create a variable (i32), memory is allocated to the stack, and then the variable gets a reference to that memory. In Rust the variable is not an extra reference: the variable holds the i32* directly. *i32, or struct, or Vec, or reference, or whatever.
How is anything you linked "bad"? Native German speaker here. Fefe has nothing against inclusion, equal rights etc. He has something against "SJW" – Feminists how seek advantage (Anita Sarkeesian ..) by imposing a victim role ... "militant" Vegans etc. and anybody else who is just seeking attention or want to turn around things like Feminists who want more power than male in revenge.
So according to that stacktrace this line is causing the problem: ptr::drop_in_place(&amp;mut self.buf[0..self.size % len]); and the message is "panicked at 'attempted to calculate the remainder with a divisor of zero'". Sounds to me like `len == 0`, so you can't execute `something % 0`. It's normal for a `Vec` to contain an invalid pointer if it's freshly created, as I recall it only allocates on the first call to `Vec::push`.
I can kinda see how his straight out ranting would interfere with an inclusive community - however he is probably no linus tarvalds yet :D
What are the | | | for? let args: Args = Docopt::new(USAGE) .and_then(|d| d.decode()) .unwrap_or_else(|e| e.exit()); Some sort of conditional? operator? 
Would be glad if someone can came up with arguments and not just plain downvoting ..
Thanks!
Small typo in http://doc.crates.io/source-replacement.html : "to express strategies along the lines of mirrors or **vendoering** dependencies".
Rust also makes allocations behind your back in the STL w/ BTreeMap, Vec, HashMap, etc. I think the idea is more over the exceptions also automatically attempt to de-allocate/close objects in C++ as the stack unwinds. 
This is your chance to become a cargo contributor! https://github.com/rust-lang/cargo/blob/master/src/doc/source-replacement.md
It get replaced [here](https://github.com/arthurprs/mpsc/blob/73b4c159cdb17983ae3c1130e13bdd11c5cfae28/src/sync.rs#L336), so it can actually have a zero cap.
Oh wow, well spotted! Seems weird how the buf's private member is mutated. With unsafe you really want to move Buffer to a separate module.
google recommended I read http://codeforces.com/help
This is great! I love the idea of mentored issues, hopefully more major projects will set them up. It looks like the set of fixes linked by /u/kibwen have all been taken, so if anyone is looking for something else to work on you can view all the mentored issues here: https://github.com/rust-lang/rust/issues?page=2&amp;q=is%3Aopen+is%3Aissue+label%3AE-mentor 
Mentored issues? Seems like a pretty great idea for lowering the barrier to entry.
I'm just doing a Jekyll-like static site generator. I'll give your article a read and see what ideas you present. I'm mostly doing this to see if I can, but if it can turn into something useful, that would be nice too! Progress on it is slow and it's not super functional right now (no templating yet), but work is progressing and I'm keeping myself on track.
It is useful to include rustc_driver::commit_date_str() To show what nightly playpen is using, current version is 2016-08-01.
My guess is that the chunking into four/eight lines (a transpose operation) will end up taking more time. But it might be worth experimenting with. One other thing I didn't explore is AVX. My experience has been that AVX is much harder to work with than SSE*, but maybe it's worthwhile.
Would it be possible to avoid the transpose? The newer AVX instructions support strided loads/stores. Maybe you could even avoid the transpose without stride support, by using a vertical difference rather than a horizontal difference? Then you compute the cumulative sum vertically, and that way you avoid loading non-adjacent values.
Well, that's certainly an interesting idea. I'd be surprised if it were a net speedup though, you have twice the read bandwidth of the accumulation buffer, and they have to be unaligned reads because the width of the line doesn't have to align on a nice power of 2. By all means feel free to play with it!
&gt;Instead of call divide_by_zero_handler, we use a placeholder again. The reason is Rust’s name mangling, which changes the name of the divide_by_zero_handler function. To circumvent this, we pass a function pointer as input parameter (after the second colon). The "i" tells the compiler that it is an immediate value, which can be directly inserted for the placeholder. Is it possible to use the #[no_mangle] attribute instead of this? Or is that doing something else? 
Minor note after doing some reading in other comment sections, I don't think it's in Servo *right now*, I think it will be landing in the nighties tomorrow.
there's not much detail in that PR. for those of us who haven't been closely following servo's development, what's exciting and new here?
[Cunningham's Law](https://meta.wikimedia.org/wiki/Cunningham%27s_Law)
It was in /u/bytewise comment in the parent tree which according to your comment you read...[link](https://blog.fefe.de/?ts=adc2cd52) &gt;während ich mich für Transgender-Einhörner eingesetzt habe. *Transgender Unicorn* is derogatory in English. I don't know if the German connotation is also, or if this is something lost in translation. My German isn't the best but the context of that paragraph it doesn't feel positive in the slightest. 
If you compute difference vertically, you have to have a 1 scanline buffer for the sum, and do a read from the source, a read from the buffer, the sum, the write to the buffer, and the conversion to 8 bit and write to the output. Doing it horizontally, the accumulation state fits in a register. So that sounds like a lose to me. You could add padding, but it increases the total memory bandwidth, and the padding could be a nontrivial fraction of the total at small and medium sizes. But as with all such things, the only way to know the performance impact for sure is to measure.
I was thinking of doing the first four vertical lines from top to bottom, then the next four vertical lines, etc. The loop would be a simple {load &amp; add to accumulator; store 8-bit converted to output}. If the last lines aren't exactly four, then you could do a masked store to store only 1 or 2 or 3 elements in the output. That would remove the need for padding. As far as I remember unaligned loads are not a problem with recent CPUs (more recent than Core2). Unfortunately I don't have access machine with SSE right now, but I may investigate later, though it may be rather useless to optimise this operation if it is negligible already.
https://github.com/servo/servo/wiki/Webrender-Overview
And I believe it is not the default renderer.
Yes, `no_mangle` would work as well. However, it would unnecessarily pollute the global linker namespace. For example, we would get a linker error if we defined another unmangled `rust_main` function.
&gt; And referring to this he is saying: Stop seeking the (Star Trek)|(Soccer)|(Transgender) Unicorn. Yeah he's staying to not attempt to be Trans. That is what I'm saying is derogatory. 
And be aware that jntrnr has now published a blog post to help guide folks in working on these issues: http://www.jonathanturner.org/2016/08/helping-out-with-rust-errors.html
Welcome to the Rust community! Documentation itself is not stored on crates.io, but typically it will have a link to the project's documentation. For example, if you go to the crates.io page for the `regex` crate: https://crates.io/crates/regex and scroll down to "Links", you will see the link to the docs. Specifying the location of the docs is the responsibility of the crate developer, so if a crate doesn't have a "Documentation" link on crates.io, it might warrant opening an issue with the crate in question.
Looks super, super cool! Definitely going to use this the next time I want to try out something network-related with rust :)
I haven't done any performance optimisation work on RustType yet. From my uses it seems to perform adequately enough that it hasn't been a high priority. I'd definitely be interested in incorporating any speed improvements /u/raphlinus's approach can bring at some point though, as long as they don't infringe on the current feature set. The SIMD stuff in particular looks like an easy win.
It's the default in the nightlies.
Ohh yeah, thanks, didn't see it, awesome. Thanks, keep up the great community.
&gt; Yeah he's staying to not attempt to be Trans. That is what I'm saying is derogatory. This is not was he is saying. How could he? Being Trans. is no decision you make, its how you are. What he is saying is to not let "this" be the central topic of your life – by missing everything else.
I've been playing around with an mio-based [async branch](https://github.com/tikue/tarpc/tree/async) of tarpc for a while now. It hasn't been merged upstream because getting it right has proven quite tricky. Early results indicate that using tokio instead of raw mio provides a **huge** ergonomics win, as well as increased confidence in correctness. .../tarpc$ $ git diff --stat async tokio ...&lt;snip&gt; 22 files changed, 635 insertions(+), 2618 deletions(-) Needless to say, I'm super excited about using tokio going forward!
So when I started the async port of hyper, I created my own abstraction over Mio, called 'tick'. It's still there, rotting. It didn't have a goal of composing machines, just to ease registering a socket with Mio. When rotor was announced, I noticed that it'd be better if I let someone else work on such an abstraction, while I work on HTTP. So I substituted out the tick code and put rotor in underneath. It was a fairly simple change, since they worked similarly and I never exposed those innards in the public API of hyper. I nearly released 0.10 of hyper when Carl approached me about tokio. I was nervous about using Futures internally, as they come at a performance cost. Instead, I can implement the internal HTTP state machine as a tokio `Task`, and it's still just as fast. No Futures needed internally. And the gains include exposing an improved API, and integration with other async network protocols. Since interest is registered implicitly, I have a proposed new Handler API: https://github.com/hyperium/hyper/issues/881 For those who still want just an HTTP server, it remains fast, and is more convenient. For those who want to connect multiple protocols together, the Service trait will let you easily do so, at the expected cost of Futures. 
Ah okay that clears a lot of things up! Thanks!
🎉
/u/pcwalton mentioned on HackerNews that him or Glen Watson (don't know his username) might do a write up about it sometime.
!
/r/playrust
Damned, it seems like only yesterday that I was watching the Webrender demo, and already the second incarnation is integrated. That's freakingly fast!
In the Rust Platform post, it seemed there were more out there that wanted to help with HTTP in Rust. So, I've written a proposal of how to adjust the current async Handler into a (hopefully) much improved one. I'd greatly appreciated comments on the original issue, if you have any!
sorta-kinda yeah.
What kind of performance cost Future incur? Any article/benchmark on that?
Yes, part of the API is banking on `impl T` landing
Yeah, I probably should :)
It's funny because it's so true.
The `Future` trait is a lot like `Iterator` in that chaining transformations is zero cost but actually writing the exact type down is painful enough that a lot of people will just prefer to return `Box&lt;Future&lt;Item=Foo, Error=Bar&gt;&gt;`. This will probably be a bit more painful than it is with `Iterator` because futures are going to end up in APIs a lot more frequently, I think. The ongoing work on anonymous `impl Trait` types will alleviate this greatly.
&gt; I don't know if the German connotation is also, or if this is something lost in translation. My German isn't the best but the context of that paragraph it doesn't feel positive in the slightest. A wise man once said: &gt; If one truly does disagree with their opinions then you should attempt to bridge the gulf of understanding, rather then dismiss their statement off handily. Perhaps you shouldn't judge Fefe before you've perfected your german! :p
Once I've decided to depend on a crate I use `cargo doc --open` as hoodinie mentioned, so that I can get the docs for all my dependencies together under one tab and search bar and know they match the version I depend on. Of course that doesn't help in the early stage of vetting crates, so it's much appreciated when crates host their docs. A couple gotchyas: a few crates place a significant portion of their documentation in a readme, which doesn't get included in the generated docs. So it's worthwhile to check github repos for that. I think you still need a public item documented in your crate for `cargo doc --open` to work, so put `//! Placeholder` at the top of your main file.
Not sure if that's what you want, but: struct S&lt;'a&gt; { t: &amp;'a T, } trait T { fn t(&amp;self) -&gt; &amp;T where Self: Sized { self } fn s(&amp;self) -&gt; S where Self: Sized { S { t: self, } } } fn main() {}
At least for Firefox, it’s the build servers more than the developers that need to work without Internet access. It’s to avoid cases like [every commit fails CI because zombo.com is down](https://bugzilla.mozilla.org/show_bug.cgi?id=616085).
Thanks :)
Is there a new switch to use it, or is it the same as WebRender1 ?
/u/carllerche, Thank you for continuing to build projects like this. The work you're doing is the basis for Rust's Async I/O story, and Mio is already a core library for many projects (including my own). I'm really excited to see what the community can do with Tokio. Rust and the Rust community are lucky to have you. Thank you thank you thank you! Can't wait to try this out later!
If you're only boxing the result of that expression, I don't see a reason why it shouldn't be well-optimized (the entire thing would be a virtualized version of a completely-monomorphized thing which could const-fold and inline all it wants). Are you expecting find_user to also return a box here? The pain of boxing iterators is that you have a hot loop that's trying to go through the virtualized interface, and you want to optimize across multiple calls to `next` but you can't. But Futures are strictly one-shot, right? You set 'em up, and then at some point you say "ok now I want the stuff in there" and block (or yield or whatever), which is where the virtualization kicks in. Hmm, I suppose I could see a situation where a deep call stack keeps delegating futures and everyone boxes at every level...
`lerp_iter` doesn't look like it belongs in the `Lerp` trait. Your declaration, `fn lerp_iter[..] where Self: Lerp&lt;f64&gt; + Sized` requires *two* impls of `Lerp` for `self`: one for `Lerp&lt;F&gt;` (i.e. the current trait impl that this fn is part of) and one for `Lerp&lt;f64&gt;` (brought in by the where clause). It would probably make more sense to have a separate trait with a blanket impl, like `impl&lt;T&gt; LerpIter for T where T: Lerp&lt;f64&gt;`.
Token + IO = tok-io
&gt; But Futures are strictly one-shot, right? You set 'em up, and then at some point you say "ok now I want the stuff in there" and block (or yield or whatever), which is where the virtualization kicks in. Not exactly; usually, you shouldn't be blocking on a future. The futures lib in question is based on [polling](http://alexcrichton.com/futures-rs/futures/trait.Future.html#tymethod.poll): &gt; For example this method will be called repeatedly as the internal state machine makes its various transitions.
This is really awesome. In past, I have used Finagle framework to build services. I would love to try this out this weekend. Thanks for all your hard work in producing this framework.
Thanks for your hard work! :)
&gt; I'll be hanging around if anyone has any questions or feedback! Would love to share code between redis-rs and your redis thing. If you want me to move something into a common utility lib let me know :)
The readme for the futures lib repeatedly touts it as being zero-allocation, is this incorrect?
I'm basing that off of the `futures::Promise` type, which definitely allocates. However, it does seem like it's possible to make futures that don't, since `Future` is a trait, and you could implement it on anything.
I'm having problems with `typenum`. I'd like to make a tree with a power-of-two, compile-time fanout, e.g. extern crate typenum; use typenum::{U1, Unsigned, Shleft}; use std::marker::PhantomData; pub trait Log2Sized { type Log2Size : Unsigned; type Size : Unsigned; } pub struct InternalNode&lt;N&gt; { _phantom: PhantomData&lt;N&gt;, } impl&lt;N: Unsigned&gt; Log2Sized for InternalNode&lt;N&gt; { type Log2Size = N; type Size = Shleft&lt;U1, N&gt;; } but this self-destructs with recursion in the compiler. error: overflow evaluating the requirement `&lt;typenum::UInt&lt;_, _&gt; as std::ops::Sub&lt;typenum::B1&gt;&gt;::Output` [E0275] note: consider adding a `#![recursion_limit="16"]` attribute to your crate note: required because of the requirements on the impl of `std::ops::Shl&lt;typenum::UInt&lt;_, _&gt;&gt;` for `typenum::UInt&lt;typenum::UInt&lt;typenum::UInt&lt;typenum::UInt&lt;typenum::UInt&lt;typenum::UInt&lt;typenum::UInt&lt;typenum::UTerm, typenum::B1&gt;, typenum::B0&gt;, typenum::B0&gt;, typenum::B0&gt;, typenum::B0&gt;, typenum::B0&gt;, typenum::B0&gt;` note: required because of the requirements on the impl of `std::ops::Shl&lt;typenum::UInt&lt;_, _&gt;&gt;` for `typenum::UInt&lt;typenum::UInt&lt;typenum::UInt&lt;typenum::UInt&lt;typenum::UInt&lt;typenum::UInt&lt;typenum::UTerm, typenum::B1&gt;, typenum::B0&gt;, typenum::B0&gt;, typenum::B0&gt;, typenum::B0&gt;, typenum::B0&gt;` ... I [asked](https://github.com/paholg/typenum/issues/61) in `typenum`'s repo, but the author didn't see anything obvious wrong – any compiler / type system wizards have suggestions?
&gt; This book is very interesting! Why, thank you! &gt; As you can see in my README, I started from a working example and removed as many pieces as I could without breaking it. You take the top-down approach (like a true hacker/maker would!) whereas my book takes the bottom-up approach. I think it's great that the community can read about both approaches!
Same reason Mozilla is adding it to Firefox. A memory safe low level language with high level constructs, package management, and c like performance.
No need for crate level documentation to generate them. Also OP, if you want the documentation of a certain package in your crate, you can always use `cargo doc -p &lt;package-name&gt;`
Not a ton; https://gmjosack.github.io/posts/dissecting-cratesio-minimum-mirror/ is a decent guide.
Definitely need to research this more when I get time. My senior project idea is network based and I haven't found anything I particularly like yet, but this looks solid. 
Hmm, is this sticky worthy?
It's Ctrl-A, Ctrl-C, and then Ctrl-V into pastebin. I'll try retyping it by hand. Edit: Actually the problem was that I skimmed over the error message. One of my dependencies, url 0.5.9 had the incorrect toml. All is fixed. 
What is the purpose of the Service trait? To me it just looks like a duplication of the Fn trait. Why not use that so you are able to define services inline with a closure? You could still have a Service type alias to improve readability of type sigs.
I'm not sure specifically, but you don't need to use the `Fn` trait to have it work with closures -- you just need to impl `Service` for `Fn`. Tokio isn't doing that now, not sure why, but it is doing something similar with [`simple_service`](https://github.com/tokio-rs/tokio/blob/master/src/service.rs#L173-L176). My personal opinion is that just because two types/traits are isomorphic doesn't necessarily make one of them redundant.
Thank you! 
Something that's missing IMO, is why you should use `?Sized` markers if you work with slices and `AsRef`. For instance if you want a function that works both of `&amp;[u8]` and `&amp;str`: fn print_first_byte&lt;S: AsRef&lt;[u8]&gt; + ?Sized&gt;(string: &amp;S) { let byte_ref = string.as_ref(); if byte_ref.is_empty() { println!("no first byte"); } else { println!("first byte is {:x}", byte_ref[0]); } } If you leave off the `?Sized` you can't do `print_first_byte("test");` and I don't really know why (it works with `let test = "test"; print_first_byte(&amp;test);` even without the `?Sized` marker.
It looks interesting, but… Would somebody be so kind, to explain in simple words, what this crate does and why it is so exciting? I don't seem to quiet get it. What would be a typically use case? Thank you :)
Any update on how that is going?
My setup: - Mac Os X 10.10 - iTerm 2 - terminal emacs - racer-mode - rust-mode - company - flycheck-rust [Screenshot](http://imgur.com/a/TeE7P) Autocompletion is a bit hit-and-miss, anyone got that working reliably?
In the `Service` trait, shouldn't `Req` be a type parameter instead of an associated type? I mean:`Service&lt;Req&gt;`. I remember hearing that type parameters are for inputs and associated types are for outputs. IIRC, /u/aturon said that on https://air.mozilla.org/bay-area-rust-meetup-august-2014/ .
Trying to improve my Rust skills by creating a small library: https://github.com/lunemec/rust-num-digitize and trying to replace macros with Traits for better extensibility (with some success).
Small tweak, but it needs python2 for the commands if you have python3 as default python. Also needs cmake installed. Looking forward to my first contribution!
AFAICS Tokio allows you to create reusable libraries for stuff like logging, user authentication, service discovery and load balancing. These libraries can be written to support many protocols. Here is a video about Finagle. It looks nice. https://www.youtube.com/watch?v=30v5WVFvlno&amp;t=17m8s
This, please. A lot of his work supports the kind of toxic nerd culture that Rust is actively trying to keep out of their community and I'd be sad to see those positive efforts go to waste. And there's many people defending him already...
I added a link to this to my [growing collection](https://scribbles.pascalhertleif.de/elegant-apis-in-rust.html#liberal-usage-of-intot-asreft-fromstr--and-similar) of (opinionated) criteria for elegant APIs :)
Not OP Like `.and_then(function(d) { return d.decode(); })` in JS?
I admitted I could be wrong, and I was engaging in a dialog concerning my shoty german it's not like I was ignoring other's input...
This looks fantastic, and definitely inline with what seastar/finagle seem to operate on as a primative (even OS kernels really!), some form of Task like thing. Fantastic, I'd love to see a pg, cass, and redis client now written with this in mind ;-)
I wrote a little [tool](https://github.com/srdja/na) for sharing files on LANs and to learn Rust :). I find it comes in handy when you're on the same wifi with someone and just want to quickly share some files without having to login to some external service.
&gt; Seems odd to me as most of the time you'll want mutable variables. This is extremely basic but $ cd ~/src/cargo $ git grep "let" | wc -l 3105 $ git grep "let mut" | wc -l 519 so, that's 16% mutable.
https://crates.io/crates/strfmt
[ZA WARUDO! TOKIO TOMARE!](https://www.youtube.com/watch?v=kSkeqYkd-ng)
No, that doesn't work with `format!`, it's a syntax extension so it can't resolve constants.
I'm working on a spec'd network protocol called [RTPS](http://www.omg.org/spec/DDSI-RTPS/) (real-time publish-subscribe) as I dig my way up to [DDS](http://portals.omg.org/dds/) (data distribution system). Generic names, I know. Cool tech used in industrial communication. These are under the umbrella organization OMG (object management group) known for such hits as Corba, XML, and UML. I am porting over all the UML entities to Rust. Surprisingly satisfying once I got down the patterns for exporting from modules. This could probably be done with some XML-foo but I'm learning by doing. This is all made easy by the organization of the protocol doc, they break it down by purpose, properties, behavior state machine. So I'm just digging my way up! I also implemented a CDR (common data representation) serde serialization interface. I still need to do the deserialization bit. Hopefully I have it sending UDP packets this week!
It would be cool to have WebAsm as a Rust target! Specially if we could access the DOM API and reuse javascript libraries...
There's also [this thread](https://internals.rust-lang.org/t/need-help-with-emscripten-port/3154/51) following the LLVM approach. I'm also going to shortly post a suggestion that somebody sprint ahead before this all works in-tree and start implementing the web API bindings so they will be ready to coincide with the wasm launch. It's all quite tough work though.
Ouch.
What are the pros/cons of one vs the other? Is this one of those things where we will know when we have implementations doing both ways?
Hi all! I wrote a little program to calculate the entropy of a file, it's my first rust project so I would like a quick review. [Code on github](https://github.com/Badel2/rust-shannon/blob/master/src/main.rs). Any comments will be appreciated. My biggest problem was to store the filename. At first, the entropy calculation function was defined as: fn calculate_entropy&lt;P: AsRef&lt;Path&gt;&gt;(path : P) -&gt; Result&lt;f64,std::io::Error&gt; { And then I called it passing an &amp;OsString, which was converted into a &amp;Path, but I couldn't convert it back to an OsString for some reason, `path.as_os_str()` resulted in the following error: error: no method named `as_os_str` found for type `P` in the current scope Which I don't understand because even if P is a generic type, it gets converted to Path, right? I guess not. Anyway, I solved it changing `P` into `&amp;OsString` and it seems to work pretty well. And the most important thing: this program is a rewrite of a C program, which I though would be orders of magnitude faster, but it's only 1.5-3x faster than rust. I consider this a success, so I will probably expand the rust version with more features, since programming in rust feels way better than in C.
Because `path` isn't a `Path`, it's something that implements the `AsRef&lt;Path&gt;` trait. You need to call `.as_ref()` on it, then you'll get a `&amp;Path` and you'll be able to call `.as_os_str()`.
I saw this argument somewhere else, but it might also apply here. Making this implement the `Error` trait would rule out using `()` as the `Error` type, wouldn't it?
With mir to wasm would it be easier to do source maps (since it is closer to the rust compiler)? Also, does wasm support source maps?
I hadn't thought about that issue, but yes I believe it would be easier. I don't know if wasm supports source maps.
`&amp;` a reference to some data `'` with a named lifetime `static` The name of The life time for things that got compiled into your program. `str` A set of utf8 data. So it says a string that is a constant in the code you wrote.
Nice, it works! Thank you for the help.
Well that works, but as long as the ldap_initialize function only writes into the pointer anyway, you might as well just keep the memory uninitialized and let the function take care of it.
Oh yeah, just thought about it again. Yeah it's wrong.
This isn't a suggestion, but a question - how does the async API client make multiple outgoing requests without using multiple threads a la node js? Will each struct instance of the client maintain its own event loop of current requests? Anyway, I have used the current released version of the hyper client quite a bit and it has been a smooth experience, so keep up the good work!
Must [build scripts](http://doc.crates.io/build-script.html) in Cargo be written in Rust? How do I get it to run something else?
That is unfortunate. Has this been brought up in an RFC?
This blog series is what got me interested in Rust. Good job man!
I believe Cargo will only execute a Rust build script, but there's nothing stopping you from using that to spawn another process to do work. There's an example of that for [gcc](http://doc.crates.io/build-script.html#case-study-building-some-native-code) further down the page.
&gt; Mutating through a *const is undefined behavior, IIRC, or at least when it is coerced from an immutable &amp;-reference like /u/thiez's example. That seem reasonable, but it does raise one question: how does one get two `*mut` pointers to an existing thing without coercing from `&amp;mut`? Edit: or does the coercion of `&amp;mut` consume it? More edit: it seems coercing `&amp;mut` does not consume it, the following compiles: let mut n = 5; let n_mut_ref = &amp;mut n; let n_mut_ptr: *mut _ = n_mut_ref; let n_mut_ptr: *mut _ = n_mut_ref; I think the rules around references and unsafe are a bit vague at the moment :(
It would be interesting to see what effect (if any) throwing the MIR switch has had (I assume this isn't included in this graph?). Disappointing that the compiler team's focus on compile times hasn't (yet) yielded any major wins on full compilation. Most performance gains in one area seem to just get lost in the other. Maybe we will see a more consistent downward trend in the next 6 months post-MIR.
It seems like `&amp;mut` only enforces that the *reference* is unique, not that that nothing else points to the same memory location.
I see. I managed to do it that way now. Thanks.
No, this isn't MIR. MIR bootstrapped in March([#32080](https://github.com/rust-lang/rust/pull/32080)), so it doesn't cover the whole range. It should be possible to include MIR comparison in the graph. I will try to do so.
No, but `&amp;mut` does assume that it is the *only* reference through which the referenced thing will get modified. So when I create a `*mut` pointer from a `&amp;mut`, can i modify the pointer-to thing through the `*mut` pointer? What if I explicitly `drop` the `&amp;mut` reference first?
The graph is up to the end of June so MIR is not enabled by default for the whole range.
Right. He wasn't indicating it should be on the graph. He's just wondering what effect MIR might have on the next set of metrics. 
I haven't been following Web Assembly that closely, but I've believed that Web Assembly could eventually be a platform independent machine language. I'm not sure if that's realistic, though.
`parent: &amp;'a Parent` will never be mutable, references in structs don't change mutability. it's hard to tell what's going on, `c = c.next().unwrap()` looks suspicious. If `c.next().unwrap()` borrows c I don't think you can write to it.
Interior mutability is achieved with Cell/RefCell
We don't know what `next` nor `first` are. I guess you have restricted mutability/ownership in these functions. 
Mutating through a const raw is fine iirc. Mutating a non-unsafecell pointer obtained from an &amp;T is not fine.
A good point. I will try discussing with him.
&gt; they may be correct 90% of the time Unfortunately from my experience they are correct more like 25% of the time. When I try using clippy on my projects, I get flooded by hundreds of warnings but few are actually legitimate :-/
Please file bugs :) Lints which are wrong 75% of the time have false positives. Lints which are wrong &lt;10% of the time are okay. Note that when I say 90% of the time, I mean 90% of the time over different projects -- I gave the example of the float comparison lint which is wrong 100% of the time in Servo, but it's usually correct overall. You should be disabling lints which don't make sense because of the nature of your project. Edit: But yes, clippy can be quite buggy. We're working on it :) My current priority for clippy is getting that defaults rfc up first. (Help appreciated! I'm willing to mentor anyone who wants to work on clippy bugs, and we have marked many bugs as easy with general instructions in contributing.md)
Thanks for detailed reply
You should hunt your deers over in /r/playrust
The new error messages are beautiful. :-)
I went through the warnings and reported some bugs to clippy for cases I consider false positives or questionable suggestions. I'm curious, which are the 1/6 that circumvent bugs in rustc? The only obvious ones for me were the 2-3 cases where you had to do `let x = expr; return expr` instead of `return expr`.
IIRC: - As you said, the unnecessary temporary variable. - The fact that you need to do `for i in 0 .. a.len() { let elem = a[i]; ... }` instead of `for elem in &amp;a { ... }` is to bypass a lifetime related problem, although I don't recall what exactly. - Using `#[inline(always)]` because rustc wouldn't inline my function. It's an expensive function that I designed in a way so that it can be trivially discarded if it gets inlined. - Although that's a bit vague, I once remember noticing a big difference in the generated assembly in favor of using a function with lots of parameters compared to a function that takes a struct as parameter. When clippy complains that a function has too many parameters, it's maybe to avoid using structs. - The fact that I've been using `while let Some(access) = access.next() {` instead of `for access in access {` or `access.by_ref()` is not an accident, because I never use `while` unless it's necessary. I don't remember why I put this code, but it's probably also because of a lifetime error. - The struct `struct Subpass&lt;'a, L&gt; { render_pass: &amp;'a Arc&lt;L&gt; }` can't derive Copy/Clone, because if I do rustc will add `where L: Copy` and `where L: Clone` to the trait impls, and I don't want that. Clippy complains because of this. I put that in the "Rust problems" category, but you may consider this a clippy false positive.
Currently `&amp;mut` pointers are *not* marked `noalias` because of some LLVM [issues](https://github.com/rust-lang/rust/issues/31681). But once they are, I would expect the following to change when compiled with optimizations: #[inline(never)] fn uh_oh(n_mut_ref: &amp;mut u32, n_mut_ptr: *mut u32) { unsafe { let n_mut_ptr_val = *n_mut_ptr; *n_mut_ref /= 2; *n_mut_ptr = n_mut_ptr_val; } } let mut n = 100; { let n_mut_ref = &amp;mut n; let n_mut_ptr: *mut _ = n_mut_ref; uh_oh(n_mut_ref, n_mut_ptr); } println!("{}", n) When `n_mut_ref` is marked `noalias`, LLVM is able to eliminate the load/store from/to `n_mut_ptr` in `uh_oh`, which means `n` will print `50` instead of `100`. Edit: you can see the effect of `noalias` [here](https://godbolt.org/g/oWqtGk).
I'm not sure.. is this a _note_ taking application or a _todo_ application?
Alas I can't code anymore due to health issues :&lt; I'm keen to see an example of chains of iterators if you have some (short) examples.
Interesting. Thanks :)
I think you should have used a loop. The "typo" was only possible because of the duplication anyway.
I'll have to look for where I saw it. In the RFC I was reading it was indicated that once `!` is made an official type, it could implement `Error` and `Result&lt;T, !&gt;` would be favored over `Result&lt;T, ()&gt;`. EDIT: still looking for the comment, but in the meantime I found this from 2015: https://github.com/rust-lang/rust/issues/25023 EDIT 2: Here is the comment/thread I was looking for: https://github.com/rust-lang/rust/issues/33417#issuecomment-235745046
&gt; Not sure if MIR itself made things faster (probably a bit?) [Or may be a lot?](http://perf.rust-lang.org/) "-54%" is a bit too hard to believe, though!
You can't directly, but you could have the build script check if it's installed, then error out if it's not.
Yep. There are a lot of possibilities. wasm is basically the simplest possible definition of a sandboxed virtual machine that will translate down to (relatively) efficient machine code on most modern architectures that matter. Compile a Rust program to a single wast file and run it anywhere. There will be lots of wasm interpreters, including the web browser you already have on your machine. [Here's another standalone one](https://github.com/AndrewScheidecker/WAVM). You might be able to think of wasm as Rust's JVM. So many possibilities. We could ship a Rust-specific wasm VM built on Servo. Wrap those web APIs up in something a little more pleasant and you've got a best-in-class, self-contained, super-sandboxed application platform. And it's also compatible with lesser implementations of the web API like chrome ;) 
[A little more insight][1] [1]: http://perf.rust-lang.org/graphs.html?start=Mon%20May%2030%202016%2023%3A34%3A34%20GMT%2B0000%20(UTC)&amp;end=Fri%20Aug%2005%202016%2000%3A52%3A24%20GMT%2B0000%20(UTC)&amp;kind=benchmarks&amp;crates=helloworld%2Cregex.0.1.30&amp;phases=total&amp;group_by=crate
Aha!! I wonder what happened on July 1st...
&gt; I'm a complete beginner to rust and i have no idea what the RUST_SRC_PATH should be besides the path to the source folder which refuses to work. So, just to be sure, how did you download the Rust source code?
From the source button in this link: https://www.rust-lang.org/en-US/downloads.html
Hmm I've tried C:\rustc-1.10.0\src\ but that doesn't seem to work, I also have zero clue about how export works
Which version of Windows are you using?
Actually you were right, I just set RUST_SRC_PATH to C:\rustc-1.10.0\src\ and it worked, thanks alot!
Great! Glad to help :)
Thanks, that's pretty much what I came up with (though I called it `_self_ref_hack` :) ). `as_trait` sounds nicer.
No description and tries to load Flash in Chrome. Not what I expected on a Mozilla site. Transcript? Anyone know what this is about?
It's a talk from Scott Carr, who was / is interning at Mozilla and working on MIR optimizations. He briefly explains what MIR is and how it allows for better optimizations. He mentions a couple of optimizations he implemented: - [Efficient switch arm generation](https://github.com/rust-lang/rust/pull/33999) - [Move up propagation](https://internals.rust-lang.org/t/mir-move-up-propagation/3610) He also mentions a testing framework to [test MIR optimizations](https://internals.rust-lang.org/t/testing-mir-optimizations/3630/2)
Excellent, thanks for the solid answer.
If this were a catastrophic bug I'd sticky it, but it looks like the only effect will be that certain crates will fail to compile. Given that rustup makes it easy to roll back to previous versions of the nightly, I'm inclined to let upvotes alone handle the visibility of this issue, since we're limited to two stickies and I'd rather not unstick any of the existing ones.
Once you have created a refence to one of the elements of the vector, then the checker regards the entire vector as borrowed. Therefore it complains when you try to create a second mutable reference to an element in the same vector. Sometimes you can avoid this problem using borrow splitting, or you you can use cell as mentioned below. https://doc.rust-lang.org/nomicon/borrow-splitting.html In general you seldomly need several mutable references to elements of the vector. So perhaps the best advice is that you try to rearrange your code so that the long-lived references become unneccesary. I wonder if this question should be in the faq "How can I create mutable references to several elements of the same vector ?"
Considering Mozilla and Google are about to begin blocking flash in their respective browsers, I find it quite funny that this Mozilla property is hosting flash videos.
If you're okay with using Powershell, open an instance (as administrator) and run [Environment]::SetEnvironmentVariable("RUST_SRC_PATH", "C:\\rustc-1.10.0\\src\\", "Machine") [Microsoft TechNet link](https://technet.microsoft.com/en-us/library/ff730964.aspx)
It's why I think partial compilation probably shouldn't be benchmarked like this, because the benchmark code for compilation isn't changing. (And thus either it's a first build and there's no time difference, or it's a subsequent build, and it's pretty much instant.) That being said you could benchmark the overhead of partial compilation...
I have a file template in a separate file that is included during compiled time using the `include_str!` macro. I don't know why, but the `format!` macro requires the format string to be an actual literal, and not an `str` reference. I would have used some an external crate for templating, but it seems a bit of an overkill for a single use.
It would be great to see the source for this, whatever state it's in.
I would like to as well. Want to tackle one of these clients? :)
I see. What is the alternative for runtime formatting?
 println!(include_str!("file"), "value"); might be enough for that use case. 
Doesn't seem to solve my problem. I don't want to print the string, and I need to store the original string as a constant for usage in multiple places in the code.
https://github.com/rust-lang/rfcs/issues/885 https://github.com/Manishearth/rust-clippy/issues/829
I've never had to solve this problem but does `format!(include_str!("..."), ...)` not work?
It might, still not suitable.
Direct link to video here: https://vid.ly/k2d1n4?content=video&amp;format=hd_webm 
Something that could help visibility is judicious use of post flairs, set by the moderators. For example, a flair `Nightly regression` or similar could be appropriate. It would work even better if such flairs were color-coded (for example, red is a good color for any breakage flair)
Does reddit impose a limit on the number of stickies? 
Why is this not suitable? Macro expansion is an AST transformation, it doesn't have access to constants because it occurs at an earlier stage of compilation. If you want to be able to change the name of the file in only one location, you'll need to create a new macro rather than a constant.
&gt; Nothing else happens. Nothing else can happen Well, the compiler *can* optimize away the move, and not do any copy. Indeed, the reason it's practical to design Rust this way is because in many cases, moves will be optimized by LLVM. But here is my question: for current stable Rust, in what cases a move won't be optimized away, when compiling in "release" mode?
Is there ever a reason to write to a const in Rust though? If not, why not make this a warning, error or clippy lint? Nvm, looks like there's issues and RFCs for that.
&gt; Given that rustup makes it easy to roll back to previous versions of the nightly... Does it do that without having to re-download the old nightly, and without me having to try and work out *which* nightly I was using before?
Two, yes.
So I can see these optimizations helping for non-release build performance, but is there anything MIR can do yet that LLVM can't for release builds? I was hoping to see some discussion about that in the presentation.
I think a large part of MIR optimisation isn't so much faster runtime (even though there are some cases where it may help), but faster compile times. The less IR LLVM has to process and optimise the faster compile times will get.
That almost looks like D's string mixins, just with tokens.
There's no need to allocate in the case where no argument is supplied: use a `Cow` as the result of the conditional outside the loop, and then borrow it before entering the loop to get a `&amp;str` with non-static lifetime. To optimize further, rewrite `BufWriter` to take control of the buffer: instead of clearing it after the contents have been written, simply reuse the same buffer over and over. To go even further, have the `BufWriter` use a stack-allocated large buffer, and fill this buffer with multiple sequential copies of the text to write out (making sure to use `memcpy`). Finally, to improve responsiveness, instead of immediately writing many copies to the buffer, write one copy of the text at the start of the buffer to begin, and flush that immediately, then double the amount of text written into the buffer on each iteration (by copying with `memcpy` again) until the buffer is full, at which point maximum possible throughput has been reached.
This is awesome!
Compile times are one of the biggest problems I (and I know others), have with Rust, so this is exciting!
Sorry for the delay. The Client utilizes an event loop to progress on a request as much as it can, then work on others in the queue while the electricity shoots across the ocean and back. The current Client on master will create a whole new loop and worker thread for each new() call. The Client returned is actually just a handle that can be cloned and shipped to any other thread. 
I don't know if it's just me but I don't get any failed tests when I run the unit tests (`python src/bootstrap/bootstrap.py --step check-cfail --stage 1`) after I've updated the error format. So, I'm sort of confused on what I should be updating. Help?
I'm making a text based version of the game Pentago to both get back into programming and also to learn this cool new Rust language. Most of my prior experience is in Java so it's not very intuitive to me.
A lot of it is that every invocation of println has to acquire the lock on stdio, do its stuff, then release the lock. In OP's code they acquire the lock manually and hold it for the whole time. Going through the formatting architecture certainly doesn't help.
Do other languages typically protect stdout with a lock?
Afaict it's very common, although broadly underspecified? e.g. standard C/C++/Java impls seem to do this, in spite of not being required to (possibly because too many applications rely on it). Seems like a Quality of Service thing -- very common to mindlessly log stuff to stdout.
I don't know why you got a downvote. Not only D metaprogramming is amazing, but if you're wrong I'd like to know why. I would be very happy if Rust could do macros just like D does. (I would be also happy if it compiled as fast as D :D ).
iostream uses a sentry, but in GCC the sentry will only hold for a single invocation of `ostream &amp; operator&lt;&lt;(ostream &amp;, …)`, so concurrent calls of cout &lt;&lt; "Hello " &lt;&lt; "World"; will lead to interleaved output. (TL;DR: iostreams are not thread safe, and concurrent access will lead to a data race and undefined behavior)
Rust also has a global stdout buffer, which is the main reason the lock is required.
Looks correct 1.531*1.093*1.005*(1 - 0.958)*(1 - 0.004)*(1 - 0.001)*1.004*1.859*1.022*1.001*(1-0.005)*(1-0.283) = 0.096 0.096 is -90.4% which is close to -90.5%. Error is probably because of approximations. EDIT: corrected the numbers. 
Ah this is awesome :) I've been slowly working through an Elasticsearch client in `rotor` and will have to look into a `tokio` implementation too.
"more pleasant"? I'm not aware of a way to access any real objects (including the DOM) from within asm.js/wasm without JS interface code. If there is, I'd love to know!
&gt; The fact that you need to do `for i in 0 .. a.len() { let elem = a[i]; ... }` instead of `for elem in &amp;a { ... }` is to bypass a lifetime related problem, although I don't recall what exactly. That's because you are calling `entries.insert`. Because you are doing a `return` immediately afterwards, this *would* work with non-lexical lifetimes (NLL are magical like that), but in general is caused by https://github.com/Manishearth/rust-clippy/issues/1105. &gt; `while let Some(access) = access.next() {` A `for`-loop should work here &gt; `while let Some(r) = ranges.next()` And here too
Could a known bad nightly be pulled from distribution, or otherwise be flagged for rustup as "don't update to this one"?
Not if you care about forward secrecy. If an attacker compromises your server they shouldn't get access keys that are no longer in use. It'll be a long time until *all* programs on a server are memory safe and even then you need to consider an attacker with physical access to the system.
So I'm trying to get cargo workspaces to work, so all crates in my project use the same workspace and do not recompile the world for every subcrate (because travis takes up to 50 minutes already). But I guess I'm doing something wrong here, am I?
I'm pretty sure workspaces are just for providing crate *source* so it doesn't need to be downloaded, rather than build artefacts. It's useful on really big build servers (e.g. Servo, Debian packages). On Travis, you can just enable the [https://docs.travis-ci.com/user/caching/#Rust-Cargo-cache](cargo cache).
You should check out the [Implicit relations](https://github.com/rust-lang/rfcs/blob/master/text/1525-cargo-workspace.md#implicit-relations) section of the rfc, you shouldn't need the `package.workspace = ".."` in all your crates. Although, the RFC doesn't actually document what's supposed to happen with "virtual crates" which appears to be what you're using. I would imagine that it would work as you're using it, but the implicit relations only talks about path dependencies. You could try adding a `members` field to your root Cargo.toml. edit: yeah, the section "Trees without any crate root" in [workspaces in practice](https://github.com/rust-lang/rfcs/blob/master/text/1525-cargo-workspace.md#workspaces-in-practice) says that you need the members key.
I have cache enabled.
https://www.reddit.com/r/rust/comments/3mcdf9/c_noalias/cvdyp5s But that's what we're doing, because there is no defined behaviour. *Nobody* knows what the rules are.
An attacker with physical access wins unless you exclusively use hardware backed crypto, and probably even then. As for one process attacking another, that's what sandboxing and permissions are for.
https://github.com/rust-lang/rust/issues/35424
Yeah, it's new commits on the linked PR
Well, take this sort of nose bleed optimisations with a grain of salt, it's not necessarily good style. Sorry I took over your repo for a silly joke. But there is this optimisation-for-the-lulz-subcommunity going on which is educational and fun. I'm an amateur, but there's a lot to learn from the /u/Veedrac s and the /u/eddyb s.
Fair enough, and no apology necessary. I feel humbled with all the discussion happening around a repo that I threw together when first playing with rust and subsequently forgot about. I've always wanted to get active in this community; this might be the push to encourage me to do so.
I'm impressed you managed to read far enough to get to the doubling part of my breakdown, but (understandably) you've misunderstood my aim there. I only intended to replace a `repeat` with a logarithmic sequence of `memcpy`s. Early writes to `stdout` with one or two copies of the input are likely to take as much time as just `memcpy`ing to 8k of data (because it's so damn fast), so by the time you've gotten to, say, the 10th line of output, I expect early writes will become pessimisms. Anyway, the startup is negligible regardless. I consider the inability to reliably do ./target/release/yes "$(head -c 1 /dev/urandom)" a much bigger issue ;).
I doubt that it worked as I intended: https://travis-ci.org/matthiasbeyer/imag/jobs/150272478
Wow!
What are the best IDEs to use for Rust development? Coming from a Java background, I've been using the IntelliJ-Rust plugin which works pretty well but I'm curious to see what else is out there.
If you have the desired element type, you can use `Borrow` which is implemented for `T` and `&amp;T` and then explicitly dereference: https://is.gd/13MNIV Making it fully generic causes trouble as the compiler is unable to infer the element type: https://is.gd/EgMXrE However, with a little hint on the type of `U`, it can be made to work: https://is.gd/QOM2lG
What are you actually trying to do? This looks like a case of the [XY problem](http://xyproblem.info/) to me (and I have a feeling you may not even need to have your question answered).
Update: Ugh. I get it to run now by doing `LD_LIBRARY_PATH=~/local/rust/lib cargo clippy`. It's unfortunate that this extra step has to be taken when you have the installation in a nonstandard place, but perhaps it's the right thing to do. In any case, it's not Clippy's fault.
&gt;I've looked all over the web to see if there were any questions relating to sorting strings but there was none. I don't believe you. First link in google for "rust sort" points to: https://doc.rust-lang.org/std/vec/struct.Vec.html#method.sort 
Thanks, I'll give that a shot as well.
Just pick any one of them that you're familiar with -- the questions are about your experience with a particular ecosystem. There's a couple optional questions near the end where you can compare to other ecosystems if you have any insights about that.
It served me an HTML5 player. In any case, the video host is external.
How do I fix [this code](https://is.gd/eJhrJ7)? I don't understand why the borrow outlives the scope. (It's a simplified version of a problem I am having, hence the unnecessary mut etc.)
I'm guessing you need something like this: https://play.rust-lang.org/?gist=2c44dade6879c1e4084b339b5d946808&amp;version=stable&amp;backtrace=0 Just convert the string into a Vec of chars, sort it, and convert it back to a string.
Is your goal here to simplify the API of a library you are writing? If yes, AsRef is probably what you want.
To answer your question, I'm trying to parse binary data. This answer doesn't help you, because you most probably wanted to know, why do I ask such question. :) So I'll provide context. I'm writing PoC program (not a library) but I was thinking that if I found the function useful and wanted to move it to a library, I'd like to make it as useful for users as I can. One of obvious improvements is to allow them provide any source of data (not just slice/vec). This lead me to finding out that it doesn't matter whether the byte is passed by value or reference. When thinking about how to express this, I thought of using `From` and that made me realize that allowing conversion from anything would probably made it even more useful. To my surprise I didn't find the impl for `Clone`, not even `Copy`, so I was wondering whether this is on purpose and what is the reason behind it. TL;DR I want to learn new stuff. :) Is that clear now?
Meant non-deterministic output, sorry, but you will get UB if you set `std::cout.sync_with_stdio(false)`. By default it returns `true`, but if, for any reason, you set it to false, you _will_ get UB. &gt; ### 27.2.3 Thread safety [iostreams.threadsafety] &gt; Concurrent access to a stream object (27.8, 27.9), stream buffer object (27.6), or C Library stream (27.9.2) by multiple threads may result in a data race (1.10) unless otherwise specified (27.4). [Note: Data races result in undefined behavior (1.10). —end note 
“&amp;” means “is a reference to...”, so &amp;str means “is a reference to a str”. str is the fundamental string type; a sequence of unicode codepoints. “'static” is a *lifetime annotation*. It modifies the “&amp;” to mean “is a reference to ... that is valid at least as long as *static*” *static* is the whole-program lifetime. So, in total we have “name is a reference to a str which is valid at least as long as the program is running”.
What does the ' mean specifically? I have seen str and static used before in other programming languages but not ' in that context.
Also why isn't it used str rather than a reference to a str?
' means “the following is the name of a lifetime”. You won't have seen this before because Rust is, as far as I know, the first non-academic programming language with region-based static reference validity typing. You'll see functions, traits, and structures parametrised by lifetimes - you may have noticed a bunch of *'a* in the documentation. That's a named lifetime, telling the compiler that two references share the same validity. *static* is, to my knowledge, the only specially named lifetime.
For consistency, if I remember correctly. You can't have a value is type *str* (just like you can't have a value of slice-type) - you can have a value of type String, as that owns the memory, but if Rust used &amp;String for the basic string reference type it wouldn't be as easy for other containers to provide &amp;str borrows. Likewise, slices - you can't have a [u8] , only an &amp;[u8]. &amp;Vec&lt;u8&gt; coerces to &amp;[u8], and so does &amp;[u8 ; 5] (an array of five elements), among others.
Hah! And somehow I missed that this was already answered!
Here are absolute values for what is 100% in the graph: * docopt 204MB * handlebars 184MB * jpeg-decoder 157MB * postgres 205MB * url 153MB
&gt; Do you mean several repositories? I could, but I do not want that because it makes development unreasonably hard, IMO. No—what /u/badboy_ said :) &gt; With this I start not two build jobs (for stable and beta) but for example 20, one for each crate with rustc stable and one for each crate with rustc beta. &gt; &gt; Is this correct? &gt; &gt; As of now, most crates depend on eachother […] Yes, this is what I meant. If you crates depend on each other AND most of that time is spend compiling (i.e., _running_ the tests is much faster than compiling them) that won't help you much. Sorry.
Spurred me to create a crate: [stringsort.](https://github.com/serprex/stringsort) Lack of random access to characters without creating a Vec&lt;char&gt; makes it so that even insertsort can't use binary search. However with ascii I was able to optimize with countsort. I include a countsort that uses a HashMap for cases where more nonascii characters are expected, based on the assumption that all texts will in general use a much smaller set of characters than their length. This incurs a cost of O(n * k * log k) where n is the length &amp; k is the number of unique characters Thinking over this I don't see a meaningful use for sorting a string's characters. One would be better off building a Vec&lt;(char, usize)&gt; or HashMap&lt;char, usize&gt; or BTreeMap&lt;char, usize&gt; for character frequencies
If you have a 'master' crate which includes all other crates then perhaps you can build and test only from that crate. In gluon I have the `gluon_c-api` crate which happens to use all other crates so from that crate I can run all tests with [this script](https://github.com/Marwes/gluon/blob/master/test.sh). It is a bit annoying that all build artifacts end up in the c-api/target but otherwise nothing is rebuilt unnecessarily.
Unfortunately I have not one top-level crate but several (all which start with `imag-` plus the one in `bin`).
Yeah, it only works for libraries. I didn't consider binaries. I believed workspaces would be the proper way to avoid rebuilding crates but as far as I can tell its not meant for building and running tests :/.
Why not add some more ram? Surely that would be cheaper than having to spend a lot of time working with this limitation? 
Why are there error bars? Are they feeding these programs different inputs each run?
Rustc &amp; GHC are the only things which cause me to run into memory pressure (Besides that I could get into my opinons that good software shouldn't require more memory, &amp; that I'd rather not have not-good software on my system, but I'd prefer to avoid getting into that kind of discussion)
I'm not sure how to write this in ELI5 style (I wrote [this](https://scribbles.pascalhertleif.de/rust-ownership-and-borrowing-in-150-words.html) a few days ago), but I maybe try to tell te reader what makes rust different (ownership/borrowing)?
They are great for primitive type constant, that you actually want to inline, such as math or physics constants(Pi, e, g, ...), protocol errors numbers, hexadecimal color codes, etc.
Msgpack is just an alternative serialization of JSON with a few extra features (such as actual integers instead of just floats.) It isn't as an efficient format as protobuf because field names have to be encoded (there is no schema), but the advantage is that it's JSON compatible.
Thanks - I will update the post to clarify these points :)
Heh, given the assume no prior knowledge aspect, I think I will avoid that particular rabbit hole
That is a great idea, thanks :-)
Is there any fix on the horizon for this variety of borrow-checker bug? It's been irritating me for years. fn main() { let mut vec = vec![1usize, 2, 3]; vec.push(vec[1]); } Fails with ``cannot borrow `vec` as immutable because it is also borrowed as mutable``. edit: This bug, too: fn main() { let mut vec = vec![1usize, 2, 3]; let reference = &amp;vec[1]; drop(reference); vec[1] = 100; }
A pity you don't track `-C codegen-units=4` memory usage. On my 1GB ARM computer I used to be able to shave off `50 minutes` of bootstrap time thanks to the above setting @`-j4`. Nowadays, probably for at least 4 months now, it always OOMs (merely using about half of the available 1.8 GB ZRAM swap, maybe MIR killed memory compressibility)
This is why we need type ascription, rustc is not 100% smart with type inference (maybe on purpose, but still)
In the pursuit of efficiency, maximization of resource utilization should be a stepping stone. If you're only using 768MB of RAM, and you have idle processor cores, logically you should want to get the most out of your resources. Using zram artificially increases the amount of RAM you have, and slightly increases the processor load, meaning you are wasting nothing. It would also alleviate his current situation without spending more money, by using amazing software to counterbalance not-so-good software.
I would say that Rust is a systems programming language because you could write an OS in it. In fact, from what I've seen, it's pretty good at this task. If it was just good at writing large systems, it would belong in the same class as Go. But I think the usages of Rust are much wider than those of Go.
it's worth noting that preventing predictability is an important security measure for hash tables. if a hash table with predictable hashing behavior was used to map user-provided keys, an attacker could supply a ton of keys that collide, leading to that O(*n*) worst-case hash table behavior and causing lookups to become a bottleneck
The hasher is initialized with a random number, so the hashes are not predictable. Even in the absence of other sources of randomness, this behavior could change compile times because of the cache.
I highly recommend Visual Studio Code. The RustyCode extension has good integration with [cargo](http://imgur.com/0hxwK8l), [git](http://imgur.com/Zf2ndnj) and [gdb](http://imgur.com/cCfX0pZ) and the [racer autocomplete](http://imgur.com/SD1Ohwa) is usually very good. I can't take a screenshot because the dialog box dissapears but hovering a function also shows the documentation. The editor is pretty comparable to what you can do in sublime with basic features like search and replace and renaming all symbols as well as multi cursour mode.
i guess you use a text based browser?
As mentioned by /u/nwydo, rustc uses FNV for hashing, the more likely reason is that ASLR changes the behavior of memory allocators.
Thanks. The problem is that in the real version of this code, I can't call next_sibling twice, because it changes the Entries struct to point to the next sibling each time it is called. I can add a method to Entries that returns the current entry without moving it. Can you explain why the borrow lives that long though? Is it because of the return? It doesn't seem intuitive to me that the borrow is kept on the non-return path too.
I run Windows on a laptop beside the Linux box to browse &amp; game. If I want to download things I use curl. I worked as a remote C# developer for a few years, so being able to run Visual Studio on an SSD was pretty important
Is your string ASCII only? If it's got Unicode things get more difficult (not impossible, but require external dependencies). See @Gyscos comment above about the [unicode-segmentation](https://unicode-rs.github.io/unicode-segmentation/unicode_segmentation/index.html) crate which will allow you to split in a string in unicode fragments (and then sort the fragments and re-assemble).
You're completely right. I latched onto the hardware side of things because it felt like the easiest for someone who has say never programmed to understand, but I think in doing this I am glossing over important distinctions.
Honestly it is because I don't want to throw too many terms at a reader – but I think I've not done enough in this iteration. And yeah, it very much is a WIP right now.
My system generally stays around ~100Mb usage. I disabled swap because the system locks up if memory gets that high that I wish the OOM killer would step in sooner
Just completed it. Does have a few hard-to-answer questions where a N/A would have been nice. Ones around breaking changes especially, if you have never had a breaking change, then questions around dealing with them don't really have answers.
The real work should be done by Cargo, dynamically noticing when memory is low &amp; then deciding not to run multiple copies of Rust As for the memory usage argument, I will look into zram the next time I'm running into memory issues, but while your argument is sound, &amp; it's the argument used to defend Firefox's memory usage, in reality it doesn't play out so cleanly. Programs that allocate lots of memory generally have poor cache performance. CPUs are many times faster than memory, so recalculation with in-cache objects is often faster than loading out of cache objects. [Suckless](http://suckless.org) works to demonstrate that the tradeoff you propose of memory vs time isn't something where one can easily gain time by throwing memory at a problem. I don't have a phone, but I think 6GB has a negative impact on the phone app space. It offers a crutch to developers, not users Indeed, [Rust time complexity](https://www.reddit.com/r/rust/comments/4w7e83/six_months_of_rustc_performance_201601_201606/?st=irl73vml&amp;sh=71675e87) hasn't fared particularly well in counter to this memory usage increase I appreciate your argument, &amp; for informing me of zram which is something I haven't looked into but should
In the article, 'Insted' should be 'Instead', 'totaly' should be 'totally' Nice writeup. Most JPEG explanations I've read try to get into explaining FFT which is a bit too ambitious since then it's an FFT explanation more than a JPEG explanation
Thanks! This should be fixed now. 
Most likely the translation items stuff. We keep a bit more stuff in memory than we did before (the list of translation items) and also push a bit more code to LLVM than we did previously (which also uses more memory). The feature is basically necessary for incremental recompilation though, which should reduce the memory footprint in day-to-day usage anyway.
I'm on vacation this week, so I thought I'd take some time and duplicate some classic linux debugging tools in Rust. Not because it needs to be done, necessarily, but as a learning exercise. I'm thinking `lsof`, `netstat` or `ss`, and `strace`. I'll hopefully expose these tools as libraries with Ruby bindings too. That way you can quickly whip up a custom tool in Ruby, with the heavy lifting done in Rust.
Wrong sub. This is for rust the systems programming language, not Rust the videogame. 
I've just started porting my play-by-email board games service to Rust from Go (~100k LOC in Go.) Have finished porting the first game which is [Lost Cities](https://boardgamegeek.com/boardgame/50/lost-cities). I started the project in 2012 as a way to become proficient Go, I'm doing the port as a way to get proficient in Rust and also because some of the more complex games were tricky to do in Go. My PBE service is fairly unique in that it's pure PBE; the server emails a coloured text rendering of the game state to the player, and the player replies to the email with the commands they wish to execute (an example from Lost Cities is "play r6" to play the red 6 card.) Most other PBE services just notify by email and you have to use a web interface to interact with the game. Probably the most pleasant part of the port is being able to use powerful parsers for parsing command input instead of manually doing it myself. I've been using `nom` and while it's not terribly easy to use, I'm super happy with the end result.
 #[cfg(test)] #[macro_use] extern crate foo; How do I make this work? Either directive works by itself, but not both together. 
Interesting that you bring up suckless, as it serves quite well as an example for me. The issue that there are more factors than just time and memory. Code complexity is an important factor to keep in mind. Suckless is a good example of what you can do when you strip out all but the essential features, but there's a reason not every Linux user is using dwm/dmenu. I used to use that combination and while it's not bad, I wanted a little more functionality. You're right in that "just" throwing memory at a problem won't speed things up. Sometimes storing a list of pointers is more efficient despite the pointer chasing, but other times storing a list of values might be faster. Often, reducing memory usage will also reduce time taken. The problem is that few programs are simple enough that maintainability is not a concern. One of the best things for maintainability is encapsulation. Different parts of the system should only know how to talk to each other, not how they work. The problem is, however, that most of the time manipulating the internal state of another subsystem is going to be more efficient, but kills encapsulation. It also makes it much harder to improve performance in a subsystem, as you have to make sure you don't break anything that it relying on some internal detail. It's a constant balancing act between making sure your APIs allow for performant code, and not locking yourself into an overly-specific design. There's a reason most (if not all) of the suckless tools are relatively simple. `surf` seems like the exception, but the meat of it, the actual renderer, is WebKit. They aren't writing their own `gcc` or an X server replacement. Don't get me wrong, suckless is great as a demonstration of how simple something can be when you just have the bare essentials, but much like life, that level of simplicity isn't for everyone. One last factor is that for complex tasks, it's more efficient to have a few programs serve everybody's needs, than have thousands of independent projects catering to a precise niche. There are 3/4 web renderers in widespread use (Blink and WebKit being very similar), 3 C/C++ compilers (`gcc`, `clang`, `icc` and even `icc` is much less common than the other two), 4 RDBMS' (SQL Server, MySQL, Oracle, and PostgreSQL, other one are not really in the same space as those 4). Hell, X only recently started getting competition as "display server for Linux", and it's not really competition, most people think X should be replaced! The reason for this is that often the "essential features" are actually quite complex, and it's more efficient to have the "superfluous" ones be ignored. I don't use most of the features of PostgreSQL, but it's not worth carving out the subset I use because chances are somebody else uses a different subset, and somebody else another subset. The argument that more memory and more performance is a crutch is true, but not useful. Good programmers should want to write code that uses as little memory and is as fast as possible. Poor programmers will always write bad software. Resource constraints ultimately just limit what you can do. Many of the design decisions in C were influenced by the lack of memory on most machines. In contrast, Rust is only practical because doing significant compiler optimisation is practical. As for `rustc`'s performance graphs? The simple explanation is that it's not that memory usage has increased time taken, it's that both have a common cause.
With them both in at the same time the `macro_use` seems to not be applied during test: the imported macro is unavailable. 
Explain the lack of I/O. I don't get the general design philosophy.
Not the author, but it makes sense. Focusing on just the logic needed to process the bytes in accordance with the protocol means a lot of nice things happen. The author points out you can then use the parser with many libraries, but you also could write your own way of figuring out IO bytes or write tests with recorded input. Though maybe this just seems good to me coming from a Haskell world. 
This belongs in /r/playrust.
This seems to be already commonplace in Rust - to implement the protocol in terms of reading `&amp;[u8]` and writing into `&amp;mut [u8]`, with possibly extra convenience methods for `io::Read` and `io::Write`.
Working on my turn-based strategy game [Zone of Control](https://github.com/ozkriff/zoc), as always :) . ### This Week in ZoC Last week I [fixed AI hangup](https://github.com/ozkriff/zoc/issues/196) and [cleaned up some code](https://github.com/ozkriff/zoc/compare/62c5...3cdf). Still working on [sectors/victory points/game results screen](https://github.com/ozkriff/zoc/issues/124): - https://youtu.be/hI6YmZeuZ3s - https://youtu.be/_0_U-h1KCAE - http://i.imgur.com/gWbWorT.png This week I [hope to](https://github.com/ozkriff/zoc/labels/s-active): - [Finish sectors/VP thing](https://github.com/ozkriff/zoc/issues/124) - [Fix reaction fire logic for armored units](https://github.com/ozkriff/zoc/issues/191) - [Add water tiles](https://github.com/ozkriff/zoc/issues/204) - [Add smoke screens](https://github.com/ozkriff/zoc/issues/160) By the way, here's ZoC on android photo: http://i.imgur.com/MzFwvI7.jpg :) [@ozkriff](https://twitter.com/ozkriff)
Never mind. The problem is more confusing than I realized: I got it all to work for me now too. I'll post more when I understand all that is going on.
For educational purposes, of course! :)
If you want to write in an asynchronous style you need to write like that. Though I'm not sure it's universally the best as it's can be more complex and less performing.
Great job on the documentation!
&gt; what does y?:xevaluate to, y?y:x? y?0:x? Its a GNU C extension. It return the result of the conditional in the omitted value. &gt; I checked the source code and there are no comments, the main file seems to be optimized to use the fewer spaces/lines/characters possible I did too, but I disagree. This code is obviously optimized to maximize the number of [WTFs per character](http://i.imgur.com/J1svNp7.jpg). How can anybody know what this thing actually does? &gt; You probably know that the size of the compiled program doesn't increase if you add comments or spaces to the source code Some people that do this like to include the original source file within the binary for whatever reason, maybe the author wanted to do the same? /u/__s 
Yes. Seems like too much overhead to me.
Scripts are here: https://github.com/sanxiyn/sandbox/tree/master/rust
Yes this seems an non-lexical lifetime problem. As a workaround, you can take value from `self.grand_iter` temporarily. fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; { let (ret, new) = match self.grand_iter.take() { None =&gt; { if let Some(kid) = self.child_iter.next() { let it = walk_tree_iter(self.depth + 1, &amp;kid); (Some((self.depth, kid)), Some(it)) } else { (None, None) } } Some(mut grand) =&gt; { if let Some((grand_depth, grandkid)) = grand.next() { (Some((grand_depth, grandkid)), Some(grand)) } else { (self.next(), Some(grand)) } } }; self.grand_iter = new; ret }
OHH. So, basically you could do something like use the I/O-less API and tell it to build you a GET request for a specific URL, and it'd feed you back the bytes that you need to send on the wire? So essentially you could mock out the server completely and feed back a response from a text file as a byte array, and it'd feed you back some data structure that represents the headers and payload, etc? If so, that makes absolute sense.
It will make it so much easier to write unit tests that I'll never write anyway!
I'm trying to figure out the best way to use enums and structs. Perhaps you can offer some advice. Say I want to represent points in a plane. The point in itself should not be associated with any particular coordinate system, but the user should be able to create a point by providing coordinates in a few different coordinate systems (say, Cartesian and polar) and also be able to read out the coordinates in any of those systems (as a way of converting from one coordinate system to another, for example). I'm thinking of representing a point with a `struct Point` and having an enum of coordinate systems: enum Coordinates { Cartesian {x: f64, y: f64}, Polar {radius: f64, angle: f64} } Creating a new point would look like `let p = Point::new(Coordinates::Cartesian{x: 1f64, y: 2f64})`. Am I at all on the right track here? Is there a better way? For reading out coordinates, I would have liked to use the same enum, so as not to redundantly define the list of coordinate systems again: p.getCoords(Coordinates::Polar); The problem now is that I'm using `Coordinates` in a new way, not providing `Polar` with the fields `radius` and `angle`, which gives an error. Help! What is the correct way?
I get how this works for small fixed size structs, but when the data is delimited by e.g. a null byte or a newline you effectively have to read in everything in a buffer before you can work on the data. Isn't that kind of inefficient?
This is strange behaviour but the strange thing happens during indexing. The compiler will also happily accept `[1, 2, 3, 4, 5][2] = 6`
Yes, that is a option, but I'd rather have multiple threads listening (one could be busy recving data from slow connection...)
I've been looking into this recently (I've wanted to write a log store thing for a while). Someone with more experience may come along and correct me, but what I've learned thus far is that in simple cases, any append to a file *below* a limit which is OS and filesystem-specific *will* be atomic. I wrote a little program that just spawned a crapload of processes that all hit the same file simultaneously, and I think I got writes up into the megabytes on Windows+NTFS and Linux+ext4 without any sign of corruption. So, if you can guarantee all appends are below some reasonable limit, you should be OK in simple cases. If you *can't*, you'll need some kind of framing (actually working on that right now :P). However, there's big problem area: NFS doesn't support atomic appends. Apparently, neither do most other distributed filesystems. On the other hand, I vaguely recall reading some really horrifying things about file locking on those same kinds of filesystems, so it might be a wash. Actually, I've read some horrifying things about file locking *in general*; that's actually why I'm currently trying to avoid it completely. :P The other thing you could do is log to a pipe or socket, and have a separate process that aggregates all the results. I don't know how that would perform relative to simple file appends, though.
I often compromise between these - put in no-ops for consistency, but comment them out. Both reduction in token density AND consistency matter, and I feel both should be addressed - a comment allows me to maintain the same consistent layout, but disable it from actually happening. 
Thank you. From what I read on stackoverflow the limit is 4096B on ext3 and 512B on NTFS, that is way too low. Framing (or Queue) it is ...
/r/Micro_Bit/
Try this script: http://www.notthewizard.com/2014/06/17/are-files-appends-really-atomic/ It worked for me on Mac, 256B was OK but 257B threw errors.
nobox is certainly not the kind of code I write for my employer. One has to view it more as a minimalist piece of art which also acts as a basic window manager. It also demonstrates that the function call abstraction isn't always the right tool. The goto structure works as a funnel that weaves through the main loop I did run into the issue that I would show nobox in interviews as a bit of a joke, &amp; because my best examples of clean code are unfortunately in proprietary software. The best example of demonstrating that I'm able to work well with others &amp; follow code guidelines is in my [wordcode patch to Python](https://bugs.python.org/issue26647) &amp; [openEtG](https://github.com/serprex/openEtG) That said I think comments can be overused. If I want to make my code readable, I'll make the code readable, not tack on a comment. Where I'm currently working there's a bunch of stale comments everywhere. Most of the comments lie or reformat the following line into English syntax Other 'artworks' include a [connect the dots game](https://github.com/serprex/Crumb1/blob/master/C.py) where as the user moves their cursor between dots a line connects those dots, but the user loses the game if they cross their line. A dot appearing will create craters in the line, allowing pathways to evolve. The code is a bit larger than necessary, because it's [optimized to be gzipped](https://github.com/serprex/Crumb/blob/master/Crumb.py) Sometimes my style has been able to find good results. I maintain the [fastest pure python AES implementation I know](https://github.com/serprex/pythonaes) &amp; the [fastest befunge93 implemetations I know](https://github.com/serprex/befunge) (nb funge.py was updated to only work on Python 3.6 as it JITs befunge code into specially crafted Python bytecode) I showed nobox on the Arch forums [6 years ago,](https://bbs.archlinux.org/viewtopic.php?id=103499) where then I'd explain the code with less apology &amp; more words tl;dr I get paid to write good code; on my free time I like to let loose &amp; see what cool code I can come up with. Where cool is some combination of minimalism, over optimization, &amp; fun
Looks like a good place to start. 
I guess you can use it to check if a word is an anagram of another word. Might be useful when making an anagram solver :)
Exactly, and you feed the library a stream of bytes that makes up an HTTP request and it does the parsing, etc of them. This way you could use synchronous I/O, async I/O, or read directly from a network card using DMA if you wanted to. 
This isn't a language question, but what happened to the unofficial Rust ppa (ppa:hansjorg/rust)?
Hooray! This was a really long-term project that took a ton of work. Lots of kudos to Alex and the Servo team, as well as several people on the Firefox side.
OHhhhhhhhhhh wow I _love_ Lost Cities. Awesome.
That all makes sense. Thanks!
This is great!!!
Haha, nice.
This is a sweet intro! I made a spluttering, mildly successful attempt at using `nom` for a binary parsing task, and feel like what's missing is an intermediate guide (or more detailed inline documentation). I'm not yet comfy enough with it to write that guide, though.
thanks!
The PPA was unmaintained for a while though, I think.
Could you elaborate on what you want to do? If you want to read/write to arbitrary memory in the current process, just use raw pointers. If you want to do it for another process, this depends on the OS. [The answers for C and C++ apply here too](http://stackoverflow.com/questions/960036/how-to-change-a-value-in-memory-space-of-another-process), since Rust can call C functions.
Could you elaborate on what you want to do? If you want to read/write to arbitrary memory in the current process, just use raw pointers. If you want to do it for another process, this depends on the OS. [The answers for C and C++ apply here too](http://stackoverflow.com/questions/960036/how-to-change-a-value-in-memory-space-of-another-process), since Rust can call C functions.
Awesome, I did not know that.
I am pretty new to nom myself. Maybe I will give it a shot in the future. I wanted to get more into error handling as well.
Edit some value at address 0x845idk in a remote process or read from that address. I am planing to do this on Windows 10 or on Ubuntu 14.04, possibly using offsets. 
Won't that example break if the name includes non-ASCII Unicode? Is there a parser generator targeting nom?
I will probably be working on [euclider](https://github.com/Limeth/euclider). It's a non-euclidean ray tracer written in Rust. I need to make it look better, add blending layers to entity surfaces, fix some bugs and write some documentation for it.
&gt; Am I at all on the right track here? Is there a better way? This seems reasonable, yeah. But... &gt; For reading out coordinates, I would have liked to use the same enum, so as not to redundantly define the list of coordinate systems again: So, rather than a `getCoords` method (which should be `coords`, btw...), I would instead write a method that swaps variants. So like enum Coordinates { Cartesian {x: f64, y: f64}, Polar {radius: f64, angle: f64} } impl Coordinates { fn as_cartesian(self) -&gt; Coordinates { match self { Coordinates::Cartesian =&gt; self, Coordinates::Polar =&gt; Coordinates::Cartesian { x: self.radius, y: self.angle }, // of course, you'd do the actual conversion here } } fn as_polar(self) -&gt; Coordinates { match self { Coordinates::Polar =&gt; self, Coordinates::Cartesian =&gt; Coordinates::Polar { radius: self.x, angle: self.y }, // of course, you'd do the actual conversion here } } } I'm not 100% sure this is the best way to go, but it feels better to me.
Thanks for the suggestions! I agree that the crypto needs a bit of work. I've started the process of figuring out how to improve it in a way that makes in easy to migrate existing encrypted profiles to a new scheme.
Awesome, reminds me of the net before there was the web.
Please do a library version of `iftop` and `iostat`. I'd really like to measure io bandwidth and latency for the storage layer using pure Rust!
Yeah, the tricky party about this is that to work asynchronously, protocol-parsing code can't ever *pull* a certain number of bytes off the stream (because they may not be available), it has to work on a byte buffer that is *pushed* to it. And then it has to save its state completely if it reaches the end of that buffer and hasn't parsed a full value yet, so it can resume on the next chunk. State machines are generally a good way to deal with this (hence their use in rotor), but they're more complicated for newbies than simple "read X bytes, split on delimiters, convert to datatypes" approaches that many simple protocols can be parsed with. Interestingly, this problem pops up in different forms all over computing: * For a UDP protocol, you have an additional level of uncertainty in that packets may be dropped or arrive out of order. As a result, your "parser" can't even be sure the next bit of data will arrive, let alone that it's available. * For a filesystem, you're trying to read disk blocks that may be all over the platter, and combine them into a stream that you can present to userspace as a file. * For an IDE, you have an editor window where the user may be making code edits that may not even be syntactically valid, and then your parser's trying to make sense of this and present derived data to the developer in real-time. * For MapReduce, the "shuffle" stage ensures that all keys outputted by the map are collected and delivered in sorted order to the reducer, so that the reducer can assume that its entire input will be present while it operates. In each case, the solution is some combination of buffering and blocking. TCP buffers incoming IP packets for delivery to userspace; read() blocks if the next sequence number isn't available. Filesystems buffer incoming disk blocks into the filesystem cache; read() blocks if the next chunk isn't already in cache. IDEs typically run their parsers in another thread or process, which only updates the UI if you pause typing long enough for a parse to run to completion. MapReduce stores incoming map keys on disk until the master indicates that all mappers have completed, and only then starts the reduce phase. And in each case, you often pay a severe latency penalty for this buffering, which in performance-sensitive applications makes devs bypass the abstraction. Video streaming and games usually use UDP, with custom packet-dropping algorithms to avoid latency. Databases often wish they could talk to individual disk blocks instead of going through the filesystem. There's been a fair bit of research on incremental parsing algorithms. MapReduce is often supplemented or replaced by stream-processing systems like Apache Storm. It makes me wonder whether there's some sort of generalized abstraction that could automatically convert a naive, "assumes the input is all there" parser into one that can operate on random-access chunks. Blocking is nothing more than capturing a continuation and invoking it when more input is available (hence the effectiveness of state machines, which make the continuation explicit in language data structures). Is there some generalization lurking that'd give you the ease-of-use of traditional, ad-hoc parsing methods, while still letting you effectively suspend them for async IO?
You can use something slightly different for UTF-8: named!(name_parser&lt;&amp;str&gt;, chain!( tag!("hello") ~ space? ~ name: map_res!( not_line_ending, std::str::from_utf8 ) , || name ) ); I am not sure about the parser generator.
Ah hah, thank you!
The random IV will not cause a breaking change and requires no changes to the decryption algorithm.
Enums in Rust are tagged unions so there's only one type (`Coordinates`) and several runtime variants. `Coordinates::Cartesian` is not a different type from `Coordinates::Polar`, just a different representation. So with it defined as an enum you'll need to do runtime checks and conversions. Something like fn polar_coords(&amp;self) -&gt; Coordinates { match self.coords { Coordinates::Cartesian(x, y) =&gt; Coordinates::Polar(..), Coordinates::Polar(r, a) =&gt; Coordinates::Polar(r, a) } } fn cartesian_coords(&amp;self) -&gt; Coordinates { match self.coords { Coordinates::Cartesian(x, y) =&gt; Coordinates::Cartesian(x, y), Coordinates::Polar(r, a) =&gt; Coordinates::Cartesian(..) } } Or something what steveklabnik1 suggested. The type system won't help you much in this setup though; notice how nothing prevents you from accidentally returning a `Polar` representation from the cartesian method. Maybe that's fine for what you need but there is another option. You can instead have structs for your coordinate types, which allows you to do things a little more safely at compile time, like make methods that only operate on `Polar` or whatever coordinate system they need. You can also provide easy and type safe conversions with the `From` trait. [Here is an example](https://is.gd/U1eA6C). In that example I added a `Coordinate` marker trait. You could call that `Point` if you wanted. If you decided you needed a `Point` wrapper struct, you can make it generic and still maintain the safety and conversions. [Like this](https://is.gd/kdF9cP). There are tradeoffs with all the approaches. I'd probably start with the distinct structs for `Polar` and `Cartesian` and use the marker trait, which you could add methods to in the future.
Yes. See `man ptrace`.
Working on [dux](https://github.com/meh/dux), trying to improve the performance of the luminance based adaptive brightness in common cases that produce a gorillion huge area damages (like scrolling in a browser). Probably have to resort to adding some kind of dimension/burst threshold that tries to gather damages and collapse them into an enclosing bounding box after a certain amount of time. 
Coroutines
This would be very useful to me - I have an old orchestration software that could benefit from LetsEncrypt functionality.
Did you get the "TypeError: Path must be a string. Received undefined" error?
Seconded. I've had an idea for a performance ADS in rust for a while but was turned off by the lack of these tools.
&gt; this cannot possibly be the best way. Part of the issue here is that the APIs aren't quite designed in the best way for returning both the key _and_ the value. So you can't escape cloning the key, at least. Also, `remove` already returns the value, I would write let key = m.keys().cloned().next().unwrap(); let value = m.remove(&amp;key).unwrap(); Some((key, value)) instead, which is a bit cleaner, and it removes the need for the value to be clonable. I tried using `entry`: https://is.gd/0L1iLa this would in theory let you not need to clone the key, but there's no way to get the key out, even though you _should_ be able to. The real issue, then, is that there's no way to pop the key out in the first place to look it up. That said, if you're willing to take ownership.... pub fn btreemap_pop&lt;K: Ord, V&gt;(map: BTreeMap&lt;K, V&gt;) -&gt; (BTreeMap&lt;K, V&gt;, Option&lt;(K, V)&gt;) { let mut iter = map.into_iter(); let first = iter.next(); let rest: BTreeMap&lt;K, V&gt; = iter.collect(); (rest, first) } Done. Don't even need to do the check for `is_empty`, no need to do any cloning or unwrap.
I published my [Twitter library](https://github.com/QuietMisdreavus/twitter-rs) to [crates.io](https://crates.io/crates/egg-mode) last week, but there's still a lot to add, so I'll be tinkering on that. Gonna try to exhaust all the "lists of users" functions for now (blocks and mutes are up next), maybe move to account settings before finally getting to reading/posting tweets. I'm saving that for last as a bit of a personal joke.
You could cut down the repetition in `impl SymbolId for ...` with a macro. [Example](https://is.gd/nROkdW). You could use some `IntoIter` implementations on `Table` and `&amp;Table`. The `Table` version would be destructive iteration, while the `&amp;Table` one would call your existing `iter` function.
This is a little bit lower level than your question, but an excellent read on the characteristics of various file writing modes/implementations: http://nullprogram.com/blog/2016/08/03/ TL;DR; writes are atomic up to some system limit (commonly 4 kilobytes on linux) so if you stay under that limit you should be okay.
[Image](http://imgs.xkcd.com/comics/wisdom_of_the_ancients.png) [Mobile](https://m.xkcd.com/979/) **Title:** Wisdom of the Ancients **Title-text:** All long help threads should have a sticky globally\-editable post at the top saying 'DEAR PEOPLE FROM THE FUTURE\: Here's what we've figured out so far \.\.\.' [Comic Explanation](https://www.explainxkcd.com/wiki/index.php/979#Explanation) **Stats:** This comic has been referenced 1450 times, representing 1.1952% of referenced xkcds. --- ^[xkcd.com](https://www.xkcd.com) ^| ^[xkcd sub](https://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](https://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_d6a0cjm)
I don't actually know.
Is there a way to `include_bytes!` while ensuring the included array has a specific alignment?
Rust really needs do style notation for chaining those monad style maybe values. Really handy in scala and more comprehensible. 
Thanks for your help! I assumed that c.parent's mutability depended on c's mutability, but I assumed wrong. I'm still not sure why c.next() is borrowing c... I'll have to look more closely at that.
Thanks, but I was actually asking about interior *im*mutability. I was assuming that c being mutable would cause c.parent to be mutable as well. But it turns out I assumed incorrectly: c.parent is already immutable. So with respect to mutability, I already have what I wanted.
There are good reasons for parsers to just indicate more data is needed, instead of returning a closure that will start again from the same point: it avoids lifetime issues (since the closure will be linked to the input), parsers are all simple functions, and going again through a contiguous buffer is easy for the CPU. The annoying part is in keeping state around for a protocol, and I am trying a few different approaches. Right now, I already have a [useful buffer](https://gist.github.com/Geal/ebfbce9a452da52319af) you can employ. It separates reading the data from actually consuming it, making it easier to work with incomplete data.
no compiler plugins, easy to extend: it's just macros and functions
I do feel compelled to mention that LALRPOP doesn't require compiler plugins ;) Extensibility is an issue, being an external DSL though.
nom seems to be more geared towards parsing binary formats. LALRPOP is more suited to programming languages, with the possibility of using custom lexers. They also let you specify languages in different ways - nom uses [parser combinators](https://en.wikipedia.org/wiki/Parser_combinator), where as LALRPOP allows you to define [LR(1) grammars](https://en.wikipedia.org/wiki/LR_parser). Both have their pros and cons. You can read more on Niko's original blog post about it: http://smallcultfollowing.com/babysteps/blog/2015/09/14/lalrpop/
Importantly, binary formats are rarely even context free!
so for example, with cargo you can just run `cargo build`, but if you had to write a `Makefile` (the think that tells `make` what to do) to do something comparable, you would have to do the following: write code to parse a `Cargo.toml` file to figure out dependencies. write code to download the right version of that dependency to a cache directory. then parse the Cargo.toml of each of the dependencies to see what *they* depend on (then download them). then call `rustc` with all the right command line arguments to build each dependency. then call `rustc` with all the right command line arguments to build the code in your crate. Cargo does all of this for you, do you don't have to write code yourself to do these steps
Realistically, you wouldn't rewrite Cargo in Make - you'll just write a targeted Makefile that downloads and builds your specific dependencies. But that's also a huge problem, because then every project has their own version of the same logic, each with different quirks and hacks, implemented in Make, a language that doesn't exactly lend itself to readability or code reuse or verification.
I'm pretty sure it has no builtin rules for .rs files. And Cargo doesn't understand Rust itself either, it understands how to invoke rustc.
Well, you can add a rule for it to the builtin ruleset. And while Cargo doesn't really understand Rust syntax, it understands the Rust build process, and the Rust packaging concepts, and things like features and #cfgs and whatnot.
Make is basically a thin wrapper over a POSIX-y shell. It takes a description of a dependency graph, each node containing shell snippets, and calls them in the right order to reach a desired target. The desired target can be anything in the graph, user-specified, by the way. Cargo is a specialized tool that downloads and compiles a Rust program and all its dependencies. It already knows how to build anything on Crates.IO out of the box, and it's more opinionated than Make about project structure. It also doesn't depend on the presence of a Bourne Shell clone (custom build steps are implemented in Rust code).
The main thing make does is look at modification time of the source files and the produced file for each step in the build, and if any source is newer, it re-runs the step. This way, if you touch a C file, it recompiles the object file (but none of the others). It then looks at the link step, notes that one of the object files is newer than the target application so re-runs the link. Make is used here to provide incremental compilation for C/C++ - only compiling units that need it. Note that if you didn't need that magic behaviour you could have written the whole thing as a simple shell script. In the Rust world, the system is very different. You just hand rustc (the compiler) the entry point to your crate and it handles all of the building/linking steps and manages the incremental compilation aspects (incremental is a bit of a TODO at the moment). Cargo's purpose is 'merely' to make sure that all the relevant source is present and to call rustc appropriately. [Cargo is capable of calling other build commands (http://doc.crates.io/build-script.html), but they're really about making sure that everything is in place for a rustc call] To put it another way, make and cargo solve completely different problems. Make is a platform allows you to put together a series of commands in a big tree of dependencies to make sure you get what you want. Cargo is a tool that sets up an environment to call rustc, that knows how to build an entire thing.
I'm continuing my work on my serial terminal emulator GUI, [Gattii](https://gitlab.com/susurrus/gattii). I tried to publish to crates.io last week, but I'm building off git repos, which I learned you can't do if you want to publish. So this week I'm going to add file sending support. It works if there are no errors, but the UI doesn't provide much status and almost no error checking is done.
[Rustboy](https://github.com/VelocityRa/rustboy), a Game Boy emulator.
I agree, and comparing Cargo to Make is a bit like comparing apples to oranges; they are not replacements for each other
That is really easy to do, all you have to do is draw to the root window :) Use the image crate to load the picture, then open an XCB connection, use `xcb_util::image::create`, iterate over the pixels of the image, use `xcb_util::image::Image::put` to put the pixels in that image, then use `xcb_util::image::put` to draw it on the root window.
Glad to hear it :)
&gt; Instead it looks like I would define a type app_string. Rust has a different order than C and Java for this. In Rust, it's `name: type` rather than `type name`. This makes it more similar to other languages, just not the one you're used to :) As for why it's App { app_name: app_name } there, the literal syntax tries to mirror the syntax of declaring it. Either way would be consistent and inconsistent at the same time. We could be consistent with `let` or consistent with `struct`. We decided to be consistent with `struct`. So why aren't they consistent with each other? Well, we could use `=` in `struct`: struct App { app_name = String, } but that's now inconsistent with `let`, but in a different way: // real rust let foo: Bar = baz; // with = for type declaration, two equals? let foo = Bar = baz So, short answer: language design is hard.
You'll probably have to either use a trait object ala `Box&lt;Iterator&lt;...&gt;&gt;`, or specify the iterator type on `Storage` as an associated type, so that the compiler can reason about the result of `into_iter` depending on the `S` it has for a specific `Compiler`.
It seems that the channel iterators disagree with you, by blocking when nothing is available, and returning `None` when the channel is closed ([example](https://doc.rust-lang.org/std/sync/mpsc/struct.IntoIter.html), [example](https://doc.rust-lang.org/std/sync/mpsc/struct.Iter.html)). I suspect the line in the documentation that states that an iterator need not return `None` forever is primarily to simplify iterator implementations, and not to expose useful behavior. 
Sorry about that :( What browser are you using? It's just pulling from Google fonts, so I would have thought that if it downloaded ok it would have rendered. Weird. Does the rest of the text show up?
Thanks for the comments, that made it much clearer to me :)
Thanks for the kind words, but I can't take credit for the documentation, as 99.9% of what exists today was done by the original author, and I have just updated it as necessary for the changes I have been making. Great job w/rust-notes, I'd never seen it before but it looks great!
That is an excellent point but I'm unsure why you stated html and programming languages. html is just text and I'm pretty sure the Perl 6 parser is written with that grammar.
Trying to write a little application to generate images to help me visualize non-euclidean spaces for use in D&amp;D. The maths are a bit tricky, but nothing special. 
I was wondering why `drain()` wasn't there, and thought that might have been it....
I believe you are trying to do something like this: pub fn start(&amp;self, data: String) { Iron::new(move |_: &amp;mut Request| { Ok (Response::with(&amp;data[..])) }).http((&amp;self.host[..], self.port as u16)).unwrap(); } [Response::with](http://ironframework.io/doc/iron/response/struct.Response.html#method.with) is working with a [Modifier](http://ironframework.io/doc/modifier/trait.Modifier.html) trait and if you check documentations you will see Modifier is actually implemented for `&amp;'a str` not `String`. Converting `String` to `&amp;'a str` should work. And also [Iron::http](http://ironframework.io/doc/iron/struct.Iron.html#method.http) is using [ToSocketAddrs](https://doc.rust-lang.org/nightly/std/net/trait.ToSocketAddrs.html) trait. You can also see implementation of ToSocketAddrs for `(&amp;'a str, u16)`. Converting `self.host` to `&amp;'a str` and `self.port` to `u16` should be enough.
Nice. It seems similar to https://github.com/carllerche/bytes/blob/master/src/buf/byte.rs which I have liked and used in the past.
I think it is because html has context. I have oft seen it said that it is impossible to safely parse html using regular expressions.
Varargs is already in Rust via the macro system (or close enough). I agree with named arguments and default arguments though.
Oh wow, I haven't used them!
Awesome!
Working on finishing docs for v0.1.0 of [statrs](https://github.com/boxtown/statrs). Just one more distribution left and hopefully the cargo crate should be up before the end of the week :)
By that argument, so are named and default arguments.
I'm on my phone, so I can't really elaborate more. I'll expand on this when I get home (if I remember). So, it's not always the case that a binary format is context-sensitive, however they are more likely to be. The canonical example of a context-sensitive "language" is "a^n b^n c^n", I.E, three consecutive strings of equal length. So parsing "aaabbbccc" requires a context-sensitive language. A language like "na^n" (so "3aaa") is also context-sensitive. This isn't common in programming languages or human-readable file formats, but is in binary formats. It's not hard to work with, but a parser generator like LARLPOP isn't really designed for you to direct parsing based on what has already been parsed. A parser combinator, like nom, is much more amenable to this, as you can pass extra data to combinators if you wish.
This is the kind of thing that makes a good language great.
Would the name of the arguments be required to be in the type signature? The way I imagined it, was that it's just a little syntactic sugar that gets resolved pretty early in the compilation process. That way it's 100% compatible with the positional form. But my knowledge of the compiler is relatively limited. Is something preventing doing it like this, or making it difficult? 
`I` should instead be an associated type on `Storage`. This isn't just a roadblock you have to get around, the code you have tries to say the wrong thing. You say that the `IntoIter` is a `CompilerIter&lt;I&gt;`, where `I` is _any_ `Iterator` of the right item type. However, your code constructs a `CompilerIter` of _the specific iterator_ returned by `self.storage.iter()`. Your types say that I could turn a `Compiler&lt;S&gt;` into a `CompilerIter` of _any_ iterator I want that yields the right type, but that's not what the code does. The code turns a `Compiler&lt;S&gt;` into a `CompilerIter` of `S`'s iter.
Out of the standalone wallpaper setters only feh has been able to do it in a way that works with a compositor, so it cannot be that easy, or there wouldn't be so many setters which don't work :). Thanks for the details!
most programming languages do not have a regular grammar, and as such, cannot be parsed by regular expressions. It's not a matter of being text or not.
I'm not sure comparing the size of those parsers is honest enough here. This nom CSV parser was written to test a lot of features in nom, like custom error messages, and it rewrote one of the basic combinators (separated_list). LALRPOP will definitely make it easier to parse languages where tokens are space delimited. It's something that I want to tackle in future versions of nom. About the point on context sensitive grammars: in most binary formats, you have a context-free grammar, with local context-sensitivity, like a Tag-Length-Value chunk. That is kind of easy to do, in parser combinators or other methods. But it gets ugly with formats where what you parse (and how you parse it) can depend on data than can be far before what you're looking at (or even after). Things like deciding whether you parse an integer as little endian or big endian depending on a tag in the header. I wrote nom to get the nice frame of parser combinators to abstract buffer manipulation (the biggest source of errors in binary formats), but with the ability to insert stateful, ugly code when needed, because most formats did not emerge from nice, clean theory :)
Another interesting aspect of comparison would be compile times. In early days of LALRPOP I remember hearing comments of it generating impractical amounts of source code in some cases.
I've been using future-rs to rework my audio crates (cpal/rodio), and to split CPU processing of my personal project in small tasks. I wanted to write an article about it, but I really don't have time to do so. The library works nice, but it's disappointing that it's not "zero-cost". The fact that everything is `'static` means that you will have to use `Arc`s everywhere, and the fact that the `Task` struct contains an `AnyMap` (I hate the principle of an `AnyMap`) will probably deter any C++ programmer. 
Nothing prevents erasing them before type checking, but it makes closures second-class citizens.
This looks like a promising feature! There's a sweet spot between forcing unnamed and forcing named arguments. Just look at Obj. C code for iOS where all arguments are (forcibly, afaik) named, it's syntactic noise. When you have no named arguments, the code becomes too terse imho. Just naming those non-intuitive arguments goes a long way!
Could you expand on that and explain what that entails exactly? I'm not sure I understand what it means exactly.
It is not, at least not the bad kind. The undefined behavior that safe Rust code must not expose are loopholes that let compiler optimization writers make assumptions about code generation - assumptions that can lead to memory unsafety. This behavior might rather be considered 'unspecified' - it does something sane (in the sense that it does not lead to memory unsafety), but exactly what is left to the implementation for efficiency reasons. That said, Rust tries to avoid these situations where it can because they do lead to bugs, but its a difficult tradeoff for these core abstractions when they have performance consequences. 
There are some difficult tradeoffs, but the library does provide alternatives to capturing `Arc`s, the most important being [task-local data](https://github.com/alexcrichton/futures-rs/blob/6a1950e6cd91cb7fd4f27b19d0090f81dc957a05/TUTORIAL.md#task-local-data). The 'zero-cost' claim is a slight stretch, but it's hard for me to imagine a design with less cost, so it might be more accurate to say 'the smallest cost possible to implement a futures abstraction'. This library really is crazy efficient. The only significant cost the library imposes is a single allocation when resolving an entire chain of futures (and even that could conceivably be eliminated). How one deals with managing state is up to the user, and it totally can be done without any additional allocations. But that technical efficiency does come with an ergonomic cost for sure. Mostly that you can't capture anything rooted to the stack. But it's hard to imagine an efficient futures library that would allow references into the stack - they must be invalidated in order to achieve concurrency. So perhaps if you had references to the stack you could package up the *entire stack* in the future, but then you are basically back to green threading and all the tradeoffs that entails. This lib appears to hit a sweet spot, but time will tell. 
To elaborate a bit on what /u/brson says, there's a difference between "unspecified behavior" and "undefined behavior". To use Wikipedia's definitions: * Unspecified behavior is behavior that may vary on different implementations of a programming language. * undefined behavior (UB) is the result of executing computer code that does not have a prescribed behavior by the language specification the code adheres to, for the current state of the program (e.g. memory). This is a subtle, but important difference. And the really crucial bit comes down to a detail of the C spec: someone writing a compiler is allowed to assume that no valid program contains undefined behavior. Which means if they run into it, they can do whatever they want. See https://blogs.msdn.microsoft.com/oldnewthing/20140627-00/?p=633 for example. http://blog.llvm.org/2011/05/what-every-c-programmer-should-know.html is also a really fantastic resource on all of this. On the other hand, unspecified behavior is code that will work, it just might not give you the exact results you were expecting.
Yay! It's about time you got on the Rust train, Sandstorm!
I'd prefer it if if in these cases we'd stick to a `fn poll(self) -&gt; Result&lt;Item, Self&gt;` (not actually `Result` but isomorphic to it). Why not leverage the type system to make it impossible to use a resolved future or a finished iterator (ofc the ship has sailed on this one)? The argument generally goes that with a reference you can store the `Future/Iterator` as a member. But in reality, given that after it's been finalised you can't use it, a more honest type for your member is `future: Option&lt;T&gt;` anyway, right? Then `future.take().poll()` is a reasonable thing to do.
There's one comment that's fairly relevant there: &gt; I would definitely agree with the above for C code but not for Rust. Rust allows nesting functions and &gt; that alleviates most of the above concerns. Not to mention the availability of closures. I haven't been able to find much documentation on nesting or inner functions. (Just a [stack overflow question](http://stackoverflow.com/questions/26685666/a-local-function-in-rust).) It makes me wonder a bit why this isn't known better.
Yay darude.
&gt; Builder pattern seems pretty obviously one of those design patterns that exists to paper over lack of syntax. What is the syntactic alternative? Named arguments? They have disadvantages: * It encourages redundantly checking invariants for every function. * Changing argument names is a breaking change.
Thanks for the mention /u/killercup! This wasn't quite ready for posting on reddit yet, but I'd be more than willing to answer questions regardless :). In any case, we'll have a blog post with lots more details about this soon, so stay tuned!
&gt; What do you mean by this? I'm not saying that _nobody_ will, but I'm going to assume that people will possibly use something a bit higher-level up the abstraction tree. For example, at Twitter, (in my understanding) most devs don't write futures, they use Finagle. Which includes futures, but also some higher-level constructs as well. That is, futures are a primitive. Sometimes you're working with primitives, but often you're working with abstractions built on top of primitives. &gt; Another thing that is not clear is how to allow for zero-cost futures As mentioned at the start of the post, &gt; futures do not require allocations to create and compose, and only zero or one allocations to resolve.
Thanks for the response! We've explored a bunch of possibilities of how to remove `'static`, but unfortunately we've always come to the conclusion that it either hinders consumers too much or it's unfortunately too unergonomic. That's not to say that it's not possible, though, as this library is still a work in progress! We're very open to exploring more avenues before solidifying. Also, to clarify, `AnyMap` was never used nor is it used now. I think you may be referring to [`TaskData`](http://alexcrichton.com/futures-rs/futures/struct.TaskData.html), but access has *never* gone through `Any` and it is implemented differently now than before in a way that doesn't even use `Any` at all.
Why do you have error handling at this abstraction level? I would have pushed that into the response type, which could be a `Result` if the call can fail.
I am against this. Not only does it make the parameter names part of the function signature, it goes against any kind of pattern matching (how would a `fn abc(TupleStruct(a, b): TupleStruct) -&gt; Something` work?). It's also redundant with builder patterns and option structs.
An excellent point. Futures solve a very hard problem, but not everything *is* a very hard problem. Finding the line at which the ergonomic cost of Futures makes sense vs. not is key. This is why Tokio uses Futures as part of the `Service` trait: an inherently multi-threaded interface, but keeps them out of the event loop, which is inherently single threaded. Implementing protocols is just a case of efficiently shuffling bytes. I find it much easier to just write the code than to have to constantly deal w/ boxing up state and passing it around to closures.
Our servers atm are written like: we have 8 CPUs, 8 threads (each pinned to a CPU) and a single event loop per thread. Each event loop is listening to a unix domain socket (in the future a shared memory abstraction). It would seem that we would be able to refactor our system to use future-rs and we wouldn't have to use Arc. Do I understand things correctly or does future-rs break this model? (perhaps by spawning worker threads in the background...) Thanks. 
Appreciate the design comment!! I ended up getting the `CompileIter&lt;I&gt;` to work, but i fear it's still a bad design - mind commenting on it a bit? Firstly, i rewrote my `Storage` a bit, to include IntoIterator type. Here is the signature: pub trait Storage: IntoIterator&lt;Item = Result&lt;(String, String), Error&gt;&gt; { } It has no notable associated types or methods. Then, i basically just added `type IntoIter = CompilerIter&lt;S::IntoIter&gt;;` to the `IntoIterator` implementation for `Compiler`. Example: impl&lt;S&gt; IntoIterator for Compiler&lt;S&gt; where S: Storage { type Item = Result&lt;String, Error&gt;; type IntoIter = CompilerIter&lt;S::IntoIter&gt;; fn into_iter(self) -&gt; Self::IntoIter { CompilerIter::new(self.storage.into_iter()) } } pub struct CompilerIter&lt;I&gt; where I: Iterator&lt;Item = storage::IterResult&gt; { storage_iter: I, } impl&lt;I&gt; CompilerIter&lt;I&gt; where I: Iterator&lt;Item = storage::IterResult&gt; { pub fn new(iter: I) -&gt; CompilerIter&lt;I&gt; { CompilerIter { storage_iter: iter } } } impl&lt;I&gt; Iterator for CompilerIter&lt;I&gt; where I: Iterator&lt;Item = storage::IterResult&gt; { type Item = Result&lt;String, Error&gt;; fn next(&amp;mut self) -&gt; Option&lt;Result&lt;String, Error&gt;&gt; { None } } Do you think this is incorrectly designed then? Or is using `IntoIterator`'s `S::IntoIter` exactly what you meant? Much appreciated!
Yeah but there is not a single useful future type, there are many, and you want to be able to abstract about all of those. It is probably possible to do so without monads (e.g. the future trait might be enough), but monads just make it straightforward to do. EDIT: I guess my thinking is/was backwards. If you have HKTs then monads is the obvious solution to abstracting over all futures (and many other things). That doesn't mean one needs HKTs to abstract over all futures, but whatever alternative solution one chooses is just going to not be straightforward to those used to solve problems with HKTs available.
&gt; futures do not require allocations to create and compose, and only zero or one allocations to resolve. There are some applications (basically anything latency sensitive) that, to be able to use futures, require futures that never ever allocate. There are future types with this guarantee that can be used there, but in the absence of these, people classically resort to use a callback based approach to be in full control. Boost.ASIO is able to abstract over different future types to support these use cases. &gt; That is, futures are a primitive. Sometimes you're working with primitives, but often you're working with abstractions built on top of primitives. Of course, sometimes a higher level API is better, and I agree that futures are a primitive, but they are _the_ primitive for asynchronous operations, just like `string` or `vec` are primitives, and are going to be part of any API doing anything asynchronously. This is at least what happened in C++, where futures are everywhere, from the coroutine TS (which requires coroutines to return an awaitable type, which is basically a future), to parts of the Concurrency and Parallelism TS (e.g. HPX uses futures to make all the standard algorithms asynchronous, which allows to run `sort` on the GPU, or on a different node of your cluster), ... In fact a problem with C++ is that it doesn't have a future Trait (or concept), but a `std::future` type, which means that a lot of APIs provide at least an easy to use `std::future` API for convenience, and a faster API which might be a bit less convenient (e.g. using their own custom future type or callbacks). Rust doesn't have this problem, but for the Future trait to be widely applicable it must be guaranteed that it is possible to at least implement it for types that never allocate (some C++ applications like AFIO go all the way to making the future methods `const fn` but something like this won't easily work in Rust).
~Anything that has length fields embedded. PNG. ustar.
How do I `join` or `select` from many futures of the same type (e.g. a `Vec` of futures)? EDIT: yes, `collect` for `join`, `select_all` for `select`! Can I `join` or `select` two (or more) futures having different types without dynamic dispatch (e.g. a tuple of 3 or 4 Futures)? EDIT: yes for `join` with `join2`, `join3`, `join4`, ..., no idea for `select` =/
Even though the associated type is on `IntoIterator`, rather than `Storage`, the important thing is that it is a specific type associated with `S`, rather than _any_ type. Also, to be clear - `CompilerIter` taking `I` is right, because using `new` you can construct a `CompilerIter` from any iterator. Its the `IntoIterator` impl for `Compiler` that specifically passes `CompilerIter::new` the iterator associated with `S`.
See cross-post link :) TL;DR can a garbage-collected language hit performance goals? If not, maybe Rust?
Awesome work! This will make learning rust much more easier and accessible, that's the real problem with rust.
MUDs have been around for longer than processor speed has been measured in GHz, a GC probably isn't going to be the bottleneck for such an application :-)
TL;DR: `rustc` is now an IDE. Fantastic work, folks.
Very cool, though the unextended error messages look more Elm inspired in form than the extended explanations
FYI, your blog is *so* close to being usable without JavaScript but could you set your code block background to black using CSS?
Is the syntax of the LALRPOP example valid Rust Code, or is it transformed/compiled into Rust code?
This is nice, though I wish there was more notice of the change since it breaks some tools (like editors). Of course I understand that it's nightly and can basically change at any time, but it's still annoying because so many tools depend on nightly rust either directly or indirectly and assume that the error output format at the very least won't change without notice. But other than that I think this is a great improvement.
Whelp, looks like I might be back to using nightly/beta for a while.
I've been using nightly for ages now just for these! :)
Thankfully, CSS can handle that using some slightly more advanced selectors, e.g. differentiating `p&gt;code` vs `pre&gt;code`. :)
I've heard this a few times :D
Yeah, what's up with that? Why did nobody do a #[plugin(idris)] yet? How hard can it be? 😜
Ahh, ok that makes sense. I guess the lack of an address bar or something in the screenshots made me think "oh wow a GUI app!". Still very exciting, cool work! :)
Well, it means you dan't have to explicitly free memory you allocated on the heap at the end of the scope (always error prone). Having a single owner for data also means that you are free to destroy it, which is super useful for safe concurrency. Reference counters also use it to bump their counts up and down. They are also super useful for other resources, like file handles. It's not a new idea - comes from C++, under the unhelpful name, RAII. Rust just puts it in the type system so you don't mess up!
? Is still not stable. There are still lots of questions around Carrier, etc.
It depends entirely on what side-effects the conversion has. If it's a simple wrapping/unwrapping operation then it will likely be elided. If other stuff has to happen in the conversion, it will likely not be elided, and depending on the conversion and what invariants the `B` type has to maintain, the data inside may be moved around or in a different form. [If you look closely](https://doc.rust-lang.org/std/convert/trait.From.html#implementors), you might notice that there are not many `A -&gt; B -&gt; A` conversion impls for `From` or `Into`. I can find only three such `impl` pairs: * `impl&lt;T&gt; From&lt;Vec&lt;T&gt;&gt; for BinaryHeap&lt;T&gt; where T: Ord` and `impl&lt;T&gt; From&lt;BinaryHeap&lt;T&gt;&gt; for Vec&lt;T&gt;` This is not side-effect free as the `BinaryHeap` has to heapify the array. That means moving stuff around in an allocation that is more or less opaque to the optimizer. Going back to `Vec` is a simple unwrapping operation, however. * `impl&lt;T&gt; From&lt;Vec&lt;T&gt;&gt; for VecDeque&lt;T&gt;` and `impl&lt;T&gt; From&lt;VecDeque&lt;T&gt;&gt; for Vec&lt;T&gt;` The `Vec -&gt; VecDeque` conversion seems simple at a conceptual level, but the conversion may resize the `Vec` if its capacity is not a power of two. This branch *may* be optimized out in the uncommon case where a `Vec`'s capacity at the conversion can be determined at compile time, but this is not a guarantee. `VecDeque -&gt; Vec` is similarly difficult to optimize as the vector would have to be rearranged if, for example, the dequeue is in a state like this: 5 6 7 8 9 - - - 1 2 3 4 T H Where the **H**ead pointer is at a later address than the **T**ail pointer (with `-` denoting empty space). Conversion back to `Vec` would require moving the elements around so they are in a linear order: 1 2 3 4 5 6 7 8 9 - - - This operation can likely be elided if the `VecDeque` was not modified between conversions and thus was still in the original order of the `Vec` it was created from. * `impl From&lt;Ipv4Addr&gt; for u32` and `impl From&lt;u32&gt; for Ipv4Addr` This conversion is entirely indempotent. The internal representation of `Ipv4Addr` is `u32` so it's a simple wrap/unwrap operation that would likely be elided. In the end, it all depends on the conversions involved and the optimization level. At first glance, some conversions may seem to be non-elidable but through aggressive inlining and induction, the optimizer may very well determine that the conversions can be elided.
It sounds like the futures crate would certainly be appropriate! The futures crate itself actually has no assumptions about a runtime, instead it forgoes all of that work to an event loop to later get implemented. The futures-mio crate is an example of exposing futures at an I/O layer, but there are certainly many other possibilities for how to drive an event loop with futures on it as well. To clarify, neither futures nor futures-mio spawns external threads on your behalf, so you've always got full control over the threading story.
The following should work (the inline code tags aren't wrapped in pre's): pre { background-color: hsl(0, 0%, 8%) !important; border: 5px solid; border-radius: 5px; padding: 5px; } And yes, they look horrible :) (white text with a black shadow on a light gray background).
This you could add some move closure examples as well? 
One of the great parts about this futures crate is that `Future` is a trait rather than a type. This ends up being the foundation for being "zero cost" as you can always implement the trait in whatever way is most suitable for your needs. Along those lines it's actually possible for futures to never allocate throughout their lifetime except at the very beginning (the `Task` to drive the future). Even that, though, can be foregone in special circumstances if truly necessary.
Yeah we've considered a signature like `fn poll(self)` in the past, but unfortunately it's not very ergonomic to work with nor is it object safe. The conclusion we reached was that a method with `&amp;mut self` that could be called after resolution was *far* more ergonomic to both implement and use while also enabling the trait to be object-safe (e.g. allow `Box&lt;Future&gt;`). These wins seemed worth it over the compile-time guarantee that futures were never poll'd twice. Additionally, you basically never call the poll/schedule methods yourself anyway, so this is just a concern for the core event loop abstraction (e.g. solve once, everyone benefits)
This looks very interesting. I'm just now working on infrastructure in [xi-rpc](https://github.com/google/xi-editor/tree/master/rust/rpc) for nonblocking calls that use a completion callback. Having the RPC return a `Future` looks like it might work. One hitch is that I'm most interested in communicating over stdin/stdout, and there doesn't seem to be a standard way to get access to nonblocking i/o on those. I'm seeing a bit more overhead than I'd like using multiple threads, and channels to communicate values from one thread to another, so I'm thinking an event loop could help, especially as a lot of what I'm doing is inherently single threaded. I'll continue to roll my asynchrony by hand, but will also keep a watch on futures to see what kind of promise it shows for solving my (not typical web-like) problems.
Thanks for the kind words! Not directly. But I'm hoping that at least one text stack for at least one of the front-ends at least incorporates the ideas, for the speedup.
The Dec c compiler had awesome error messages just like this. Glad to see rust doing it.
Quite interesting, thanks! Working with `mio` directly can be tricky so it is nice to see `tokio` trying to make it simpler. But even then, without a good tutorial it's harder to understand what is required to use! I greatly appreciate. Something I wish there was more is how to scale those abstraction. What if I want to serve many clients concurrently? Can I store the server state outside of the closure? Can code be shared between clients and servers? I see that you bind to port 17. Why are you doing so? Aren't ports lower than 1024 reserved to root?
That approach isn't object safe though, which is important because futures are often passed around boxed.
Direct Github link: https://github.com/suhr/vmjk Pretty interesting, but am I only one who has noticed that the keyboard layout is ISO (as opposed to [ANSI](https://en.wikipedia.org/wiki/Keyboard_layout#Mechanical.2C_visual_and_functional_layouts) where `\` is in the different position)? ;)
The example implements the [Quote of the Day](https://en.m.wikipedia.org/wiki/QOTD) service which is a "well known" service and lives on port 17.
When will neomake get support for parsing this? Can't switch until neomake can parse it
It is probably more idiomatic to use Option. Then you can write app.name = None to destroy the field struct App { name: Option&lt;String&gt; } With regards to ownership you can often write your code so that each object only has one owner at a time (also look up mutable borrows using &amp;mut ), but if everything fails, then you can use a combination of Rc and Cell.
It is possible to make these fine distinctions between [implementation-defined, unspecified and undefined behavior](https://en.wikipedia.org/wiki/Unspecified_behavior). But, from an API user's point of view, it would be better to have: * either a well-defined well-specified behavior * or an implementation-defined behavior, which can vary across implementations but is well-documented by each implementation. For a language/library which would not want to sacrifice safety at the altar of speed, why would we want unspecified or undefined behavior? Not being argumentative, just trying to understand.
It's a bit misleading to say this would get Rust /into/ a game engine, as rather it'd just be exposing some of their types to Rust, for extensions to use. I don't think it'd end up very nice. I say this both as a Rust fanatic and someone working on a game using Godot.
Oh that'd be awesome to use futures in xi! I'd love to help out if you need any assistance :) Right now you basically just need to get stdin/stdout to work with mio to get futures hooked up to them. The [`add_source`](http://alexcrichton.com/futures-rs/futures_mio/struct.LoopHandle.html#method.add_source) method allows adding arbitrary event sources to an event loop and you can also use a [`ReadinessStream`](http://alexcrichton.com/futures-rs/futures_mio/struct.ReadinessStream.html) to get a stream-based interface to the readiness notifications. I believe on Unix you can just set stdin/stdout to nonblocking and then pass them down via `EventedFd` to mio, similar to what [`futures-curl` is doing today](https://github.com/alexcrichton/futures-rs/blob/93ce3f0fca6efe853ddb146976aa81cda127d3d3/futures-curl/src/lib.rs#L549-L573). On Windows, though, unfortunately mio doesn't support custom I/O handles just yet. You'd have to farm out the reads/writes to a separate thread or get support into mio upstream unfortunately.
I've been working on a comprehensive [DNS library](https://github.com/partim/domain) for a while now. It has an asynchronous stub resolver, currently build on rotor and using a home-made future-ish construct. I am right now looking into moving that to use futures either all the way or, ideally, make it possible to combine it with any asynchronous framework.
It's funny; I was actually working on this briefly several months back, but abandoned it as I felt I was working against the declared ethos of the engine. Their position on alternate languages seems to be softening, so perhaps it's worth another look.
As he already mentioned, this essentially is functions, only without the possibility of reuse.
I got to make my first PR to the master branch to help get the the new errors format in place. I am not here to brag about that, as its entry level coding, more to say its so cool that Johnathan and co allowed so many of us to get involved and really feel we are part of the project. This community is really fantastic. Its not very often as an relatively immediate at best programmer, you get to contribute to compiler! You rock, don't change! :)
The API in C++ for `when_all` and `when_any` are just a variadic function, where the single element case takes an homogeneous range, so that you can `when_all(vec)`, `when_any(vec)` for ranges of homogeneous futures and `when_all(f0, f1, f2, ...)`, `when_any(f0, f1, f2, ...)` for the heterogeneous case. That seemed to cover all cases I ever encountered.
I think you are looking for the `cargo new --skynet` command. It's still in nightly and not ready for production use AFAIK.
You're looking for /r/playrust, this subreddit is for the [Rust](https://www.rust-lang.org) programming language
&gt; strict impure I have high hopes for `const fn` powered by miri! :)
I think we can do that quite easily with machine learning given a big enough data set of good rust code. So: Stop lurking on Reddit and write that code! Our future depends on it!
But not in parallel which limits its usefulness
Lately, I have been developing a search engine in Rust in my spare time. This week, I am focusing on polishing it up before properly publishing it. I'll put a post on reddit.com/r/rust once it is ready.
While the implementations on his hash test bed are in C++, the performance results and the discussion on which hash functions to use and when are relevant for Rust as well. Overall City64 and xxHash32 seem to perform very good for small, medium, and large sizes on 32 and 64 bit platforms, respectively. Cryptographically secure hashes are way slower than non-crypthographically secure ones (6-7x slower on medium to large data sizes), but only a bit slower than DOS resistant hashes like SipHash for small sizes, and similar performance or even faster for larger hashes. FNV works well for strings &lt; 8bytes, but not really well for anything else :/ 
I mostly buy what you're saying, but in terms of object safety wouldn't `impl&lt;T: Future&gt; Future for Box&lt;T&gt;` cover most cases where object safety is required? Or is `&amp;mut Future` common too? 
I'm not sure if it'd be worthwhile. The stated goal here is to open up Godot to other high-level languages besides its own GDScript. While Rust has some nice high-level features, it'd be more effective in the *other* side of that equation: as the language in which Godot is implemented (instead of C++).
Define "too much". There are already some attempts to write a game engine in Rust; [Piston](http://www.piston.rs/) and [Anima](http://www.anima-engine.org/) are two examples which I've seen thus far. No idea how they compare to Godot feature-wise or performance-wise, though.
Yeah, this is indeed a problem in clang as well, but I don't see how it would be a problem in your example: &gt; why is that random crate getting used? If you have multiple candidates, rustc should probably give up instead of picking one at random. If you write the name of a function but forgot to qualify it, and in all the crates you are importing in your project, only a single function with that same name exists, what are the chances that you meant a different one? It can happen, but I think that the chances are low. Same with typos, if there is only a single candidate for a variable with levenshtein distance of 1, you could have meant a different variable, but the chances are again low. I think that experimenting with this is at least worth it, but obviously getting the heuristics right is hard because no heuristic will work perfectly for all of the cases.
Ah, now compton's behavior makes more sense.
Thank you so much for the help!
There is not currently a way to donate to the developers, but thanks! &gt; Are those improved somehow? Which error, specifically?
I know this isn't strictly rust related, but boxing is a common pattern in rust to achieve some patterns. This article gives a view into how those are optimised in the Unity game engine.
I donate to Mozilla as a way of "contributing" to Rust. It would be cool if I could at least tell them that the awesome work happening on Rust is my primary reason for donating, but Mozilla seems to give Rust the resources it needs, so it's all good anyway :)
You don't ever want to use `ref` in function arguments. In `borrowing` and `borrowing2`, `obj` is of type `&amp;&amp;App` and `name` is of type `&amp;&amp;String`. This is because `ref` says "please give me a reference to this thing"`, so you're saying that `obj` is a pattern to match against a `&amp;App`, and then will be a reference to that `&amp;App`, hence `&amp;&amp;App`. `ref` is used in pattern matching when you want to take a reference to the inside of something like `Option`. &gt; How can I create a &amp;app reference object for my struct App so I can write: fn zyx(app: &amp;app) {} You were close, it would be `fn zyx(app: &amp;App) {}` &gt; Why would I do that? Well, there are three primary ways of taking arguments in Rust: by ownership, by reference, or by mutable reference. This is the second, by reference. You'd want to do that if you want to borrow your arguments, but not modify them. This is the most common way of taking an argument. You might want to check out the book chapters on ownership and borrowing, if you haven't already :)
As a network programmer, I read it as "reset" but that seems fine to me :) 
If you write any fn which returns e.g. `Result&lt;T, E&gt;` and put `Ok(T);` **with semicolon** the compiler reports type mismatch. Even though in this case, the user most probably wanted to write `Ok(T)`. (without semicolon) It would be really helpful to also output something like: `help: You probably wanted to write Ok(T) (without semicolon)` I believe compiler is able to see that pre-last expression is actually of the correct type.
Ah ha! yes, so there actually _is_ a diagnostic specifically for this, but it doesn't always kick in. This is one of the cases where it doesn't. If you weren't using a generic type, but something like i32.... error[E0269]: not all control paths return a value --&gt; foo.rs:1:1 | 1 | fn foo() -&gt; i32 { | ^ | help: consider removing this semicolon: --&gt; foo.rs:2:6 | 2 | 5; | ^ Could you file a bug about this? I agree that'd be a great thing to improve.
Fair enough :). However most MUDs were written in C, so the competitive performance we're used to was in a non-GC environment. I think the independent variable here is my software skill and time to invest in code vs. game design. I'm unwilling to write in C/C++, but after writing so much ruby am sad to see it fall over at &lt; 100 bots.
/u/retep998 certainly does.
Yes it is vague, sorry about that. `crates.io` (with my definition!) is big, so that's good example! Thanks! By "big" I mean "real", "in use", "well known" (it's vague too haha!)
Ok thanks! I'm thinking about crates too (some crates requires unix tools, I think)
There is also https://clippy.bashy.io/ which is written in Rust and has ["extensive Documentation"](https://clippy.bashy.io/docs/) according to the website :)
Bravo guys! Seriously nice work! :D Looking forward to seeing more details on it in future blog posts :)
Aside from what the others pointed out, there is rarely (if ever) a reason to take a `&amp;String`. Generally you'll want to take `&amp;str` instead (because `&amp;String` `Deref`s to `&amp;str`, it'll look the same for everyone who has a `String`, but allows the function to be used without allocating)
I can't find you a server, but I *can* find you a subreddit: /r/playrust
&gt; Maybe we should file issues? Indeed! Although I think [this issue](https://github.com/carllerche/mio/issues/321) may already be what we're looking for?
This isn't a nightly vs stable change, here it is on stable: foo.rs:1:1: 3:2 error: not all control paths return a value [E0269] foo.rs:1 fn foo() -&gt; i32 { ^ foo.rs:1:1: 3:2 help: run `rustc --explain E0269` to see a detailed explanation foo.rs:2:6: 2:7 help: consider removing this semicolon: foo.rs:2 5; ^ 
Out of curiosity, can you tell us which platform that is?
Thanks! So, if I understand things correctly here, `.from()` and `.into()` are like any other methods, _i.e._ they don't cancel automatically or anything. Is that right? The case that got me thinking about the issue was coordinate conversion, as discussed [elsewhere](https://www.reddit.com/r/rust/comments/4wobby/hey_rustaceans_got_an_easy_question_ask_here/d6b75lw) in the present thread. In that case, a conversion sequence like `A` → `B` → `A` is ideally a side-effect free identity transformation. In practice, there is numerical error, so if the compiler lets the conversions cancel out, the result will not always be exactly the same, but actually better. I don't suppose that there is any hope for cancellation to happen in this case, is there? Do you see any other way to achieve a similar effect?
Excellent! A common, ecosystem-wide futures abstraction is pretty much the last thing required for me to be able to use Rust in production! `async`/`await` sugar would be nice, but having the core abstraction available means that I can start building the libraries that I need now. I will need to try porting my current blocking protocol implementation to this library, to try it out and provide feedback.
dropbox is fairly rust heavy, but i don't know about their website specifically 
This looks really great. I'm sure I could read the code/try to prototype it to find out, but I'm lazy so I'll just ask: a common tactic in high-perf distributed systems is to speculatively send a service request to 2 endpoints and take the first response, to avoid issues w/ a server being down for service or overloaded, etc. On top of that you'd want a timeout, in case neither service finished, so a 3-way select. The wrinkle that a lot of future libs fall over on is that one of the service calls failing quickly should not actually complete the future, it should continue waiting for the other call or the timeout. Can futures-rs handle this scenario? Edit: from the select docs: "If either future is canceled or panics, the other is canceled and the original error is propagated upwards." So, nope, does the wrong thing. What's the use case where this behavior is ever what you actually want? Willingness to throw one of the results away is already implicit in using 'select', so why does an early failure break the whole thing?
You can easily write your own combinator with this functionality. It's just not the semantics of select. Also, select doesn't throw the second value away, it passes it through as a future along with the first value. So use case is "handle either order, but start work as soon as the first value is available".
You can indeed handle this with just combinators! I've written an example below which should do this I believe. Note that the use of boxes here are not required, they're just there to show the demo: fn fetch_either() -&gt; Box&lt;Future&lt;Item=u32, Error=()&gt;&gt; { let a = fetch_server_a(); let b = fetch_server_b(); let first_answer = a.select(b).then(|res| { match res { Ok((answer, _other)) =&gt; futures::finished(answer).boxed(), Err((_, other)) =&gt; other.boxed(), } }); first_answer.map(Ok).select(timeout().map(Err)).then(|res| { match res { Ok((answer, _)) =&gt; answer, Err((err, _)) =&gt; Err(err) } }).boxed() } fn fetch_server_a() -&gt; Box&lt;Future&lt;Item=u32, Error=()&gt;&gt; { ... } fn fetch_server_b() -&gt; Box&lt;Future&lt;Item=u32, Error=()&gt;&gt; { ... } fn timeout() -&gt; Box&lt;Future&lt;Item=(), Error=()&gt;&gt; { ... } Also look like I need to update the select documentation! It's no longer the case that an error cancels the other future.
&gt; Is this library going to make it into std:: in place of the old std::future? If so, when can we expect it to make it? If that were to happen, it would be very, very far off. This is an announcement of a project, but maybe everyone will try it and not like it. There has to be broad community support before something can be considered for std, and so thinking about that on announcement day feels very premature.
Yes, that's right. Futures should always be "nonblocking" -- any long-running computations should be farmed out to a thread pool. We provide some general ways to do this, and will be adding more as time goes on. (This is similar to the pitfall of calling a blocking function that a green thread scheduler isn't aware of.)
Well, if you had a struct containing a pointer into itself, it would need a move constructor to fix up the reference. Do you think there should be an opt-in move constructor the way `Clone` provides opt-in copy constructors?
Ah, sweet! Looks basically exactly like what I'd expect, thanks! 
Fair enough. I'll be trying this one out and giving my support for sure, provided it works as well as it sounds.
I often have trouble wrapping my head around how to do elegant, static polymorphism, even with a composition-based approach. I think when [RFC #1522 (impl trait)](https://github.com/rust-lang/rfcs/pull/1522) is implemented it will make it much easier. But I don't know if there is some kind of documentation resource or good example for how rustaceans use static dispatch in situations where box seems like the obvious answer.
Yeah that's actually one thing I'm really happy about how the `Future` trait turned out, it's quite simple and easy to implement your own custom behavior. For example the [`select` implementation of `poll`](https://github.com/alexcrichton/futures-rs/blob/1f228f205daf6fdd426d7cd41b1306c6ed2b1a17/src/select.rs#L45) is very straightforward, it just polls both future and sees which one comes back with an answer! There's some mild gymnastics to get the types to work out, but that's why it's a combinator :)
Nice work. But I wonder if this could lead to the [callback hell](http://stackoverflow.com/questions/28343904/scala-futures-callback-hell). Does anyone have any information about this problem with future-rs?
To expand on this a bit more as well, the [documentation for `poll`](http://alexcrichton.com/futures-rs/futures/trait.Future.html#tymethod.poll) indicates this in a section "Runtime characteristics" as well. Which is to say that implementations of `Future::poll` are expected to *not* block and return quickly.
If you look at the first example of using the combinators, you'll notice that you don't have any rightward-drift. By and large, this hasn't been an issue so far. (And if it does become one, some form of do-notation or layering async/await should take care of it.)
To build on this: flat_map is such a terrible name if you're not working on lists, oh my gosh. One of many reasons why Monads are a terrible abstraction to force on the world.
&gt; do-notation Bad aturon. Go home, you're drunk. 
So we are 2! I come from php too, and rust is my new default choice (I still have much to learn anyway)
Weren't they working on writing a http2 library for their client?
`ops::ShrAssign`, buddy!
I develop with windows and it works. Most of my pains are in libraries that require DLLs to use.
*never be wrong on the internet* *live by the post, die by the post*
&gt; To build on this: flat_map is such a terrible name if you're not working on lists, oh my gosh. Well, this is entirely your opinion. &gt; One of many reasons why Monads are a terrible abstraction to force on the world. Haskell has no function named `flatMap`, which demonstrates pretty trivially that your objection to the name does not translate to an objection to the concept. Anyway, this conversation doesn't seem to be going anywhere good, so I will almost certainly check out here.
http://b2b.mercatos.com/us/ is powered by Rust.
Are we going to see some usage of futures in crossbeam?
A quick explanation (as I haven't bookmarked my previous responses, *sigh*), is that it would have to be duck-typed and not use a `Monad` trait, *even* with HKT, to be able to take advantage of unboxed closures. Haskell doesn't have memory management concerns or "closure typeclasses" - functions/closures in Haskell are *all* values of `T -&gt; U`. Moreoever, do notation interacts poorly (read: "is completely incompatible by default") with imperative control-flow, whereas generators and async/await integrate perfectly.
Haskell completely threw up its hands in disgust and called it `&gt;&gt;=`. The point is that monads cover such a wildly varying set of things that unifying them under a single name just leads to confusion for concrete types ("oh, that's what &gt;&gt;= means for this type? I guess that kind of makes sense..."). Having and_then and flat_map as separate names makes them so much more clear to people using the concrete type. I would be completely embarrassed to try to explain to people that to do a bunch of operations in sequence, they should *of course* be using flat_map.
Where does the overhead between the raw mio and the futures implementation of minihttp come from? It seems to be ~10% for the single threaded case =/ (EDIT: while ~0% in the multi-threaded case).
Ah, so it's an implementation issue. I thought /u/Gankro was criticizing `do` notation in general. I'm surprised that there's not a way to do it with HKT and `impl Trait`(so that unboxed closures can be returned). I'll have to try writing it out to see where things go wrong.
The fundamental issue here is that some things are types in Haskell and traits in Rust: * `T -&gt; U` in Haskell is `F: Fn/FnMut/FnOnce(T) -&gt; U` in Rust * `[T]` in Haskell is `I: Iterator&lt;Item = T&gt;` in Rust * in Haskell you'd use a `Future T` type, but in Rust you have a `Future&lt;T&gt;` trait In a sense, Rust is *more* polymorphic than Haskell, with *less* features for abstraction (HKT, GADTs, etc.). You can probably come up with something, but it won't look like Haskell's own `Monad`, and if you add all the features you'd need, you'll end up with a generator abstraction ;).
I believe that aaron and alex are at lunch. It's 0.3% in the threaded case. I'm interested to hear this too.
I habe no use for this as far as I can see, though it really looks awesome! Thank for making the rust world even better!
Totally, I hear you on both of those. We'll see what they say!
Are you compiling it with optimizations? `cargo build --release` Cargo makes a debug build by default.
How are you compiling? Make sure you use `cargo build --release`.
I definitely think we'll want to layer this over concurrent data structures. We'll probably start with `std`'s channels.
The event loop has a dispatch table mapping from events to tasks (which contain a future). This is a case of a heterogeneous collection, which is the classic place where you need trait objects (dynamic dispatch) for uniform representation.
&gt; The fundamental issue here is that some things are types in Haskell and traits in Rust. Indeed. The elephant in the room whenever we talk about monads is that iterators (and now futures) implement `&gt;&gt;=` with a signature that can't be abstracted by a monad trait.
It's not about the members themselves, but about members that are references to other members. If you have a struct like this: struct RefSelf&lt;'a, T&gt; { t: T, rt: &amp;'a T, } then it's not possible in safe Rust for `rt` to refer to `t`.
STM32F4 (ARM Cortex M4). I admit to over simplifying in my post, for the sake of brevity. While you can obviously compile Rust code for the CM4 there's a compatibility with the ecosystem thing. We're currently using Keil and ARMCC because that's what you get out of the box. You get an STM provided tool that generates startup code and project for a particular hardware config. You get all the normal IDE things like code browsing and build management (handy even if I actually write all my code in Emacs), and you get one of the better embedded debuggers I've worked with (including ide integration that knows about the various hardware peripherals and can tell you what register are set on your SPI hw etc.). To use Rust in my project I would have to a lot of tooling work and in the end I would still probably have a reduced quality of debugger. I would also definitely have trouble compiling the STM HAL using another compiler, and I'd lose support anyway. The second problem is that this is the R&amp;D phase of a device that will eventually go into manufacture. When we get to the production phase, cost reduction will happen and we may need to swap out the processor for another vendor. Limiting the options because of the use of Rust is unacceptable. It may be that these are not insurmountable obstacles, now or at some point in the future. I believe ARM have dropped ARMCC in favour of a clang based solution anyway. If the vendors move to that then that's a large roadblock out of the way. With things like this to point to, I don't think it'll be too long before the cost/benefit equation works out in favour of Rust.
It makes sense if you look at the types: you map a `T -&gt; Future&lt;U&gt;` over a `Future&lt;T&gt;` to get a `Future&lt;Future&lt;U&gt;&gt;`, and then flatten it to `Future&lt;U&gt;`. This makes about as much sense to me as `and_then`'ing an `Option`, talking about doing an action "after" a plain value like `Option` and `Result` is a bit weird.
I ran your program, and noticed that while it runs excruciatingly slow, it barely utilizes the CPU at all. So I did some debug printing, and it seems to me that network traffic is the bottleneck here. Your program downloads over 128 json objects from the network, and each of those downloads take about half a second.
As Steve mentioned the multithreaded case is basically taken care of, and it's what we've been optimizing mostly. I profiled the singlethread benchmark both with mio and with minihttp, and the profiles were very similar in that the pieces that jumped out were easily optimizable. In general these sorts of microbenchmarks tend to just stress different pieces of the system. That which we optimized for the multithreaded/pipelined case probably isn't stressed in the singlethread/non-pipelined case. Shouldn't be to hard to get the cost back down to 0 though, one of the #1 things I saw in the profile was *moves* of all things!
&gt; My question is basically if it is possible with the current API to preallocate the shared state of a future in a thread stack frame I think that currently due to `'static` this wouldn't be possible, unfortunately. If they were stored somehow in static memory (or on the heap, for example), then it should work!
I guess this means that it is not required by the Future trait, but by a particular implementation. Other event loops might not support heterogeneous taks, or only a couple of tag types (using an enum), and might not need to perform dynamic dispatch. 
Futures are actually a solution to callback hell, because you can easily chain them together.
Rust should really have automatic reference counting.
&gt; Note that ralloc cannot coexist with another allocator, unless they're deliberately compatible. Does this include mmaping memory?
&gt; Either way, it produces the same output as it would have at runtime Yeah, but only if the input is a constant.
Sorry for the delayed response - I've been thinking about this but been away from the computer. I don't think you've made a mess of `self` and `&amp;self`. They are appropriate uses in this case. The only possible exception in my mind is `Point::dist` could take `&amp;self` instead. As written, it'll make a copy of the `Point` on which it is called. If `Point` were not `Copy` then it would be consumed. If you change it to `&amp;self` then you could easily get separate distances to the same `Point` without consuming it or making copies. I happen to think `&amp;self` is more appropriate semantically; you're getting the distance from one point to another, not converting the point into a distance based on another. You can make arguments either way. You're arguably making a mess of `From` and `Into` though. There is a blanket impl of `Into&lt;T&gt; for U where T: From&lt;U&gt;` so it's typical to implement only `From` and get the other for free. It's actually pretty rare that one defines `Into`. For example, the standard library has only two such cases right now. So all the `Into&lt;Point&gt; for Cartesian` is better written as `From&lt;Cartesian&gt; for Point`. I could be misremembering so don't quote me on this, but I believe the only time you typically implement `Into` directly is when you need to convert your type into a generic type from some library, e.g. `impl &lt;T&gt; Into&lt;Vec&lt;T&gt;&gt; for MyType`. &gt; It's not the most efficient, though. I'm not sure how to do that I think the most efficient way is to provide explicit and efficient conversions for each type like I did. There _might_ be a way to express transitive conversions automatically, like given `T: From&lt;U&gt;, U: From&lt;V&gt;` then `T: From&lt;V&gt;` via `U`, but I haven't found it and I have suspicions it might be disallowed altogether due to coherence rules and Rust's general preference for being explicit. Separately, I'm not sure if you were just experimenting, but I don't understand the `RotatedCartesian` or `PolarDegrees` structs. The angular units and degree of rotation seem like they should be separated out. Something like trait AngularUnit{} struct Degrees{} impl AngularUnit for Degrees{} struct Radians{} impl AngularUnit for Radians{} impl From&lt;Degrees&gt; for Radians { .. } impl From&lt;Radians&gt; for Degrees { .. } then you'd have `Polar&lt;AU: AngularUnit&gt;` to allow `Polar&lt;Degrees&gt;` and `Polar&lt;Radians&gt;`. The trickier part I haven't thought through yet is how to delegate a rotation to an angular unit, so that you could have something like `Coordinate::rotated_about_origin&lt;AU: AngularUnit&gt;(&amp;self, au: AU) -&gt; Self`. You might have to go through a `Rotate&lt;C: Coordinate, AU: AngularUnit&gt;` struct or something. Like I said - not thought through :)
Thanks. Any recommendations for how to fix this? I tried putting a delay at the end of each iteration of the while loop (and thus between each call to the API), but that didn't help.
What I mean is that optimizations should not change the output of a program. That would be pretty bad. (It happens when concurrency is involved as the compiler assumes that reordering operations which are not normally order-dependent is okay, but adding concurrency throws a spanner into the works.) As it turns out, we can simulate dynamic data by making the value opaque to the optimizer: https://is.gd/2BEomw In the emitted IR in release mode, you can see that all the operations are performed, since floating-point operations can introduce rounding errors and the optimizer has to preserve those at all costs. Even with integer operations, overflows and wrapping have to be preserved, which depends on whether or not the inputted data is close to the overflow/wrapping boundary and obviously cannot be determined at compile time. In the end, the optimizer does everything it can do while maintaining correctness.
It seems to me that the server you are downloading from is responding slowly. Maybe you could try a different server, if possible. Ideally, you would want to minimize the amount of separate requests, which you have over 128 of currently. Each request has to travel through the network to the server, and back. I can't tell you how to do this, as I'm not familiar with any Hacker News API. EDIT: Take a look at [SirVer's answer](https://www.reddit.com/r/rust/comments/4x9t8r/how_can_i_make_this_program_faster/d6dtq35).
I was using the term "duck-typed" in the sense of statically typed but with no actual abstraction boundaries (i.e. how C++ doesn't have typeclasses and templates expand more like Scheme macros than Haskell generics).
I much appreciate all the attention you're giving this! &gt; it's typical to implement only `From` and get the other for free Right. My reasoning was that it made sense, for a user wanting to implement a new coordinate system, to define a new coordinate system struct and then `impl` the necessary methods _for that struct_. &gt; There _might_ be a way to express transitive conversions automatically, like given `T: From&lt;U&gt;, U: From&lt;V&gt;` then `T: From&lt;V&gt;` via `U` What one would ideally also want is for a conversion from one system to another, and then back again, to be short circuited to become an identity transformation. (This prompted me to [ask](https://www.reddit.com/r/rust/comments/4wobby/hey_rustaceans_got_an_easy_question_ask_here/d6chfzj) about that.) It would be especially unfortunate to perform a sequence of conversions like `A` → `B` → `C` → `D` → `Point` → `D` → `C` → `B` → `F`, to get from some `A` to `F`. &gt; I'm not sure if you were just experimenting, but I don't understand the RotatedCartesian or PolarDegrees structs. Oh, yes, they were just made-up examples of coordinate systems that a user might introduce (with `Cartesian` and `Polar` being assumed to be provided already by the library).
&gt; define a new coordinate system struct and then impl the necessary methods for that struct I agree that makes sense. I was only commenting that having `impl Into&lt;Point&gt; for T` directly is unusual and that those could be replaced with their equivalent `From` impl.
This is extremely exciting...a long awaited feature for me that enables a lot of other long awaited features for me. Out of curiosity, are there any proposals out there for the async/await syntax? Something language level? Macro based? 
We already have functions that can't be reused; just put them in private modules.
Well, the example I linked uses futures in Scala, but the OP on this StackOverflow question had an issue with callback hell.
&gt; println!("Oh no. Blame the Mexicans."); wtf 
There's no `bind` function in Haskell, though, "bind" is just an informal name for the `&gt;&gt;=` operation. Let's not forget that Haskell suffers from acute operatoritis (a.k.a. "Why do my programs look like Snoopy swearing" syndrome, a.k.a. "I can't believe it's not Perl").
Not the subject of the article, but: &gt; This post is not about cryptographic hashes. Do not read below if you need to hash passwords, sensitive data going through untrusted medium and so on. Use SHA-1, SHA-2, BLAKE2 and friends. SHA variants are generally considered to be insecure for hashing passwords and should not be recommended. bcrypt or scrypt are the recommended algorithms for hashing passwords in 2016.
My biggest pain is dealing with libraries that try to build OpenSSL. Other than that, Windows development is delightful!
strcat never wrote one in rust. I have a K&amp;R allocator as well, though it's not public.
&gt; &gt; The fundamental issue here is that some things are types in Haskell and traits in Rust. &gt; &gt; Indeed. The elephant in the room whenever we talk about monads is that iterators (and now futures) implement &gt;&gt;= with a signature that can't be abstracted by a monad trait. I wonder if there would be a trait that is more suited to iterators and other lazy constructs embedded in an eager language. Is there any precedent for this kind of abstraction?
Yeah, I kind of find `flat_map` much more understandable and regular when one looks at it in the context of `map`. `flat_map` on lists is also a good basis for building an intuition off. But I agree that it is a bit bikesheddy.
Just starting my journey in Rust. I started learning Go a few months back, but stopped. I originally learned C and C++ as well as Java in school, so the transition to Rust has been pretty flawless thus far. Looking forward to finishing up the Rust book and reinstating my programming knowledge in the months ahead. 
Ticki can be quite sarcastic sometimes. I merged the change to make the message more appropriate.
It is important to note that the first one, rust-malloc, does not free memory and according to the writer: `As such, it is very slow, not-threadsafe and generally bad. There is little-to-no error checking.` The slab allocator is cool, but not suited to be a general purpose userspace allocator, it can only allocate fixed sizes.
You're looking for r/playrust. However you'll find gamma settings in your monitor's OSD.
Just an idea: could Rust's default hasher use a fast non-cryptographic hash for the first 32 bits? I think this wouldn't harm security, as the first 32 bit hashes are directly enumerable and thus can't be secure, but it has a decent chance of speeding things up. [A Python PEP](https://www.python.org/dev/peps/pep-0456/) found that, for them, ~40% of hashes were for 32 bits or less, so this would probably help a lot of use-cases.
I've never run across a case when I've wanted or needed a struct to contain a reference to one of its other fields. So I'm legitimately curious: what's a use case for this?
I also wrote a terrible (but working) port of the musl allocator: https://github.com/dikaiosune/rusl/tree/master/src/malloc
"Don't ever want to use `ref` in function arguments" is a bit strong, but it's rare indeed.
How about `panic!("Oh no! Out of memory.")`. Can we not all agree that the joke was in incredibly poor taste?
It'd also be fine to just spawn a bunch of threads, and could still use the hyper code. It'd help a lot to make just one Client, which would allow connection pooling. 
&gt; How dependent is the Future interface on the readiness model? Currently it's pretty crucial. We actually prototyped an entirely different completion-based model, but it ended up having far more allocations and overhead, so we went towards readiness instead. I suspect it would be *possible* to implement something like `futures-wmio`, but likely not in a zero cost fashion. Kinda in the same way of mio being zero cost on Unix but not zero cost on Windows. &gt; Do the continuations of a future run on the same thread as the event loop? Maybe! All the combinators (e.g. `and_then`), which I believe you're referring to here, run as part of the `poll` function. This function is intended to run very quickly, so it ends up running on the thread which generated the most recent event. That is, if you complete a future doing some work on one thread, you'll try to finish the future there and otherwise just figure out what to block on next. &gt; How much control does futures-mio give me over the event loop thread? Quite a bit! It spawns no threads of its own, so you're in complete control. &gt; Have you read about, colored-events[0] and how they can be used for multi-core event loops? I haven't yet, but that looks quite interesting! I'll be sure to take a look at that soon. We've toyed around a few methods for multi-core event loops, though. One source is to use `SO_REUSEPORT` for servers accepting connections so you can accept connections into entirely independent event loops. Another possibility is to have one epoll set and a bunch of threads waiting on that set. We've implemented both these possibilities [over here](https://github.com/aturon/async-benches/tree/master/techempower-6) with raw mio, and so far the `SO_REUSEPORT` seems the most promising as it's easy to understand at the event loop layer and easier to implement as well. We're certainly open to more options in the future though and always willing to benchmark more!
Is it just me, or do futures seem like a clumsy abstraction for handling asynchronicity, compared to something like lightweight threads (like channels). I feel like futures make code less readable than channels as they are still trying to hide the ball about concurrent execution, while something like channels puts the asynchronicity in explicit form, so you can reason more easily about it. 
We may still need HKT for it, but maybe there might be a some abstractions that can still afford us the same compositionality as Monads do... I dunno.
What do you mean by "no allocation" and storing futures in an enum? If `Future&lt;T&gt;` is a recursive ADT, how is it possible to construct one without having a heap pointer at the recursion site(s)?
&gt; Ideally, the leakage would be handled transparently and automagically where I as a user can still think in terms of a -&gt; b, but that seems, well, difficult. Why can't you think in those terms? It seems to me that one can still think in terms of function input and output abstractions. Once the right `a -&gt; b` is chosen either then think about the right `Fn`/`FnMut`/`FnOnce` trait to use, or stop if you're not choosing the trait. The environment vs. no environment distinction (I assume you're referring to the function traits vs. `fn` pointers) very rarely comes up in my experience: thinking about/using the traits is usually a better choice than touching function pointers explicitly. (Of course, as you say, half of systems programming is seeing the leaks in normal abstractions.)
Thank you for the PR and the comments! 
No issues here. Just install MSYS2, set your `PATH`, and then just run `pacman -S openssl`. Though this is much easier than compiling stuff yourself with MSVC, this restricts you to the GNU ABI, I think.
The company I work at is going to start a greenfield project in either Rust or Go - we were planning Python as an option but the web libraries I've tried out are difficult to extend, and our usecase is _slightly_ outside a basic use-case of a web server. Don't know about Go yet, but with Rust+Iron I got a working prototype within the hour.
I guess you could use the never type `!` to denote futures that cannot fail, [as soon as it is implemented](https://github.com/rust-lang/rust/issues/35121)
Is there a difference between 'ref' and '&amp;'? What was the reason for adding the 'ref' keyword?
Is there some guide for this? (a working step-by-step how-to about OpenSSL for Rust on Windows)
I agree.
There is an unwritten guide in my head. One day I'll get a written copy of it to share.
It's mainly used in pattern matching, especially when you don't want to or can't move out of the wrapper. For example, `Option::as_ref()` which is implemented like this: impl&lt;T&gt; Option&lt;T&gt; { pub fn as_ref(&amp;self) -&gt; Option&lt;&amp;T&gt; { match *self { // Binding to the inner value by reference, so we're not moving Some(ref val) =&gt; Some(val), None =&gt; None, } } } This couldn't be implemented like this: match *self { Some(val) =&gt; Some(&amp;val), None =&gt; None, } Because that would try to move `val` out of `self` and then return a reference to it, which won't work, both because you can't move out of borrowed references, and because you can't return a reference to a value which will fall out of scope at the end of the function.
Weird, I've experienced completely different errors. I'll check it later, if I'll have time. EDIT: one additional idea could be help saying you probably meant `Ok(x)` instead of `x`, if you accidentally write `x` of appropriate type...
Perfect, thanks, both of those methods are good to know about.
&gt; half of systems programming is seeing the leaks in normal abstractions. lol, it seems we are indeed from different worlds: I'm not speaking of leaking resources, I'm talking about leaking *logic*. In Haskell, when you have a function from `a -&gt; b -&gt; c` and you apply an `a`, you get a `b -&gt; c`, and you may now reason about this `b -&gt; c` just as you would reason about any other. In rust, this is *not true*. Information about the lifetime of that `a` you gave me is now bound to the resulting `b -&gt; c`. AFAIK rust doesn't even support partial application/currying, and this is precisely why.
Ah, and of course, suggestions or pull requests are welcome!
Great idea, thanks! 
They can still be used multiple times within the module though. And there may not be a good name for the block (in the same way that there isn't necessarily a good/useful name for specific closures for instance).
Idris effect system doesn't conform to its Monad typeclass either. Doesn't prevent it from using do-notation at all, it can be implemented purely as sugar.
Thanks! I did some tests in the playground and looked at the 'ref' chapter in the "rust by example" book. It did make the matter a bit more clear. As you wrote, it is used for tuple destructuring and pattern matching. However, if I try to do the following, I get an error match *self { Some(&amp;val) =&gt; Some(val), None =&gt; None, } Here the compiler tells me that it is expecting a 'T' but it found a '&amp;_' (it says it's a &amp;-ptr). I don't fully understand this error and I still don't fully understand the difference. I'm starting to think it's because of how '&amp;' "works semantically".
Sure it's possible? The issue is that the instance becomes permanently borrowed: https://play.rust-lang.org/?gist=77a72a3c2d3a8649b6715a48d6126d28&amp;version=stable&amp;backtrace=0 struct RefSelf&lt;'a&gt; { t: i32, rt: Option&lt;&amp;'a i32&gt;, } fn main() { let mut rs = RefSelf { t: 42, rt: None }; rs.rt = Some(&amp;rs.t); println!("Hello, rs.t = {} rs.rt = {:?}!", rs.t, rs.rt); } prints Hello, rs.t = 42 rs.rt = Some(42)! Requires use of cells to edit though... https://play.rust-lang.org/?gist=b60e2790c12b73cbc57401b9a58fd3a9&amp;version=stable&amp;backtrace=0
[removed]
/u/aturon I'm a little confused by .buffered(32). The way I understand it: The buffer would ask for 32 futures "at the same time". Map would then call process 32 times so that there are 32 futures pending at the same time right? How are we processing it sequentially?
This belongs in /r/playrust.
Well, imag doesn't seem like it is intended for library usage (correct me if I'm wrong), but yes, the issue is similar. The problem of both the GPL and the LGPL is assuming a lot about how programs are assembled and using a lot of lingo from the C world.
1. It relies on some prerelease versions of crates, notably mio. And you can't publish crates with a git dependency.
I just [published](https://www.reddit.com/r/rust/comments/4xd195/ann_filters_a_library_to_build_predicates_and/) [filters](https://crates.io/crates/filters) - a library crate to build filters (for example to use them in `Iterator::filter()`) with the builder pattern. See [the Readme](https://github.com/matthiasbeyer/filters#filters) for an example.
&gt; Well, imag doesn't seem like it is intended for library usage (correct me if I'm wrong), it is partially. The binaries are just frontends to the libraries. The possibility to use the libraries to build a product with them is explicitely given, but the "main idea" is that imag itself implements all the binaries itself. See my other comment on the subject; I don't think LGPL is a problem as it allows static linking.
Quote from the article: "No more "If you really care about security, program in this language". It's one thing to create new tools -- say, programming languages like Rust that stamp out a whole class of vulnerabilities by enforcing memory safety -- but it's another to see how developers use it." I tried to locate slide of keynote "The Hidden Architecture of Our Time: Why This Internet Worked, How We Could Lose It, and the Role Hackers Play"; that sounds fascinating. So far I failed.
Self plug but I am interested in knowning why you didn't go with my jwt crate https://github.com/Keats/rust-jwt You lose random arbitrary payloads but you get static styping and no timing attack (afaik, `jwt` doesn't do fixed time comparison)
a more complete quote: &gt; The answer isn't more standards or theory, but "real-world experimentation," Kaminsky says. "No more 'If you really care about security, program in this language.'" &gt; For example, it's easy to say stop coding in C because of the chances of introducing memory-related vulnerabilities. However, if the developer needs to embed code, then Python as the "safer" language is not an option. It has to be C. By looking at actual use cases, at how developers are working, we learn about these roadblocks and can start figuring out solutions to address them. &gt; It's one thing to create new tools -- say, programming languages like Rust that stamp out a whole class of vulnerabilities by enforcing memory safety-- but it's another to see how developers use it. If they aren't able to do what they need to accomplish with the new tool, they'll figure out a workaround. Which makes almost no sense to me. If C is high level assembly, then Rust is safe, high level assembly, with lots of modern features to make writing code more pleasant. Rust is arguably the most flexible language I've ever seen, since it can be used to write code for microcontrollers, operating systems, web servers, desktop applications, games, and more, and it isn't awkward or restrictive in any of those fields. It feels like a natural fit. I wish we had a first class engine like Unity for game development, but that's not a limitation of the language itself. To me, this article says nothing actionable. Just do real world experimentation, whatever that even means. We've been experimenting for decades, and results are in: do not use C or C++ for safety critical code! In the absence of better options, it is *possible*, but it is highly inadvisable. It takes a tremendous effort and investment to prove your implementation, because those languages maximize your foot-shooting potential. You should arguably be using Ada for safety critical code, since it embeds on microcontrollers and runs on full computers, and everything in between, but I really think people should at least use Rust if they're not going to use Ada, which is an admittedly awkward language. If your web browser of choice is proven to be hugely vulnerable, the right answer isn't "let's avoid telling people to switch browsers, they might need to access the intranet page that only runs on IE6! Let's just do real world experimentation."
Something i have a vague idea on, but i'm not certain of. What does it mean to say zero-cost? Or no overhead? My thinking is that it's not heavy on memory, and that it doesn't have hidden/abstracted costs.
&gt; What does it mean to say zero-cost? It's this: &gt; C++ implementations obey the zero-overhead principle: What &gt; you don’t use, you don’t pay for [Stroustrup, 1994]. And &gt; further: What you do use, you couldn’t hand code any better. &gt; &gt; – Stroustrup The idea is that with this, you couldn't hand-code a mio event loop by hand. And, at least on multithreaded versions, that seems to be borne out.
None of the licenses has a requirement submit changes upstream. They all have a requirement to distribute the source along with the binary form of the program towards the user. (often, a practical way to do this is to submit the changes upstream, but that might not be true) Choose a license has a good overview of the MPL. http://choosealicense.com/licenses/mpl-2.0/ The relevant language in the MPL is: &gt; 3.1. Distribution of Source Form &gt; All distribution of Covered Software in Source Code Form, including any Modifications that You create or to which You contribute, must be under the terms of this License. You must inform recipients that the Source Code Form of the Covered Software is governed by the terms of this License, and how they can obtain a copy of this License. You may not attempt to alter or restrict the recipients' rights in the Source Code Form. &gt; 3.2. Distribution of Executable Form &gt; If You distribute Covered Software in Executable Form then: &gt; (a) such Covered Software must also be made available in Source Code Form, as described in Section 3.1, and You must inform recipients of the Executable Form how they can obtain a copy of such Source Code Form by reasonable means in a timely manner, at a charge no more than the cost of distribution to the recipient; and &gt; (b) You may distribute such Executable Form under the terms of this License, or sublicense it under different terms, provided that the license for the Executable Form does not attempt to limit or alter the recipients' rights in the Source Code Form under this License. Which is close to what the GPL expects. Also don't forget to read the sidebar about recommended usage.
/r/playrust
Thanks, very helpful. I started reading the new book "Haskell programming from first principles" and couldn't recommend it more highly. 
To quote from the RFC: &gt; If a function returns `impl Trait`, its body can return values of any type that implements `Trait`, but all return values need to be of the same type. Source: https://github.com/rust-lang/rfcs/blob/master/text/1522-conservative-impl-trait.md
&gt; I'm not speaking of leaking resources, I'm talking about leaking logic I wasn't speaking about leaking resources either: systems programming ends up caring about the details inside abstractions, i.e. they are leaky abstractions. Like, GC is an abstraction on top of resource management to pretend the computer has infinite memory (no need for the programmer to free anything), but systems/systems-style programming in such languages ends up having to care/work around the GC&amp;mdash;the abstraction is leaking. &gt; In rust, this is not true. Information about the lifetime of that a you gave me is now bound to the resulting b -&gt; c. AFAIK rust doesn't even support partial application/currying, and this is precisely why. I don't think that is why: a partially applied function is, in essence, just a closure that captures `a`, and closures already exist and work well in Rust. I suspect it is more because no-one has pushed for currying in a convincing enough way. BTW, ages ago, Rust used to have partial application like `foo(_, 1, _)` == `|x, y| foo(x, 1, y)` (or `\x y -&gt; foo x 1 y` in Haskell), but this was removed in favour of the `|...| ...` syntax, i.e. proper closures. Haskell gets away with every function looking the same because it is willing and able to compromise on performance in some cases, i.e. everything can be a (dynamically allocated) pointer, and it's ok for things to be left as dynamic calls, depending on what the optimiser sees, while, as a systems language, Rust tries/needs to give total control to the programmer (another example of an abstraction leaking: different sorts of callable objects have different behaviour, and a systems language needs to expose that).
This seems an important improvement for Rust. Adding the calendar demo ( https://github.com/eddyb/rust/blob/d92e594c3801a8066f95305c87e53a7ecfb24e9b/src/test/run-pass/impl-trait/example-calendar.rs ) was a good idea, despite it contains two "hacks", a workaround, and it's twice longer (I miss groupBy in Rust). How is the performance of the Rust version (if you generate many months) compared to the D-ldc implementation?
Ahhh got it! I've been dealing with 3 Map deep data structures at work so I can definitely see the advantage of this. Thanks!
... unless you call `.boxed()` which makes trait objects. But generally you're not doing that.
Okay that makes more sense as well thank you!
Just inherited the choice from the original project, I'm sure @kaj would welcome pull requests if the change adds value 👍
In that particular case you can do `rv.take(usize::MAX)` in the other branch. 😁
I guess this will help my [filters crate](https://github.com/matthiasbeyer/filters) I just published today to be even more powerful and enables me to do some really really neat things.
This is my very favorite and most wanted new feature! :D But I've got so many APIs to rewrite if/when this gets stable... at least I have fun writing Rust.
My comment was not really about do notation as much as it was about the usefulness of having a monad typeclass. But that would be inconsistent with the way all other Rust sugar behaves, and I wouldn't be in favor of it (I agree with upthread comments that call it 'duck typing').
It's true that improving the Interator APIs were a primary motivation. If you haven't already, I recommend reading the 'Motivation' section of the RFC : https://github.com/rust-lang/rfcs/blob/master/text/1522-conservative-impl-trait.md#motivation 
Steve take your goddamn day off seriously and stop commenting on dumb programming forums. Also: new types are still happening but they're just virtual dispatched if you box.
Unless rv streams more than usize::MAX elements.
One of many reasons why `flat_map` is a terrible name. You can have monads-as-an-abstraction without it.
Iterators are the best example... Let's say I have a trait like so: pub trait MyTrait { type Stuff; fn get_stuff() -&gt; ???; fn get_stuff2() -&gt; ???; fn get_stuff3() -&gt; ???; fn get_stuff4() -&gt; ???; } I want to you to return a collection of some sort, and I don't care what the concrete type is. Right now, we have two options: * Return a fixed container like `Vec&lt;Stuff&gt;` * Define an associated type and return that. The first is bad because it basically forces you to call `collect` every time the function is called. Say bye-bye any advantage you hoped to gain from lazy iterators. The second is bad because you have to declare every iterator type you wish to use explicitly, So my trait now has to look like this: pub trait MyTrait { type Stuff; type GetStuffIterator: Iterator&lt;Item=Self::Stuff&gt;; type GetStuffIterator2: Iterator&lt;Item=Self::Stuff&gt;; type GetStuffIterator3: Iterator&lt;Item=Self::Stuff&gt;; type GetStuffIterator4: Iterator&lt;Item=Self::Stuff&gt;; fn get_stuff() -&gt; Self::GetStuffIterator; fn get_stuff2() -&gt; Self::GetStuffIterator2; fn get_stuff3() -&gt; Self::GetStuffIterator3; fn get_stuff4() -&gt; Self::GetStuffIterator4; } ... and then implementing this trait will be a veritable nightmare. Whereas with `impl Trait` you can define the iterator types yourself and not have to use associated types to specify them: pub trait MyTrait { type Stuff; fn get_stuff() -&gt; impl Iterator&lt;Item=Self::Stuff&gt;; fn get_stuff2() -&gt; impl Iterator&lt;Item=Self::Stuff&gt;; fn get_stuff3() -&gt; impl Iterator&lt;Item=Self::Stuff&gt;; fn get_stuff4() -&gt; impl Iterator&lt;Item=Self::Stuff&gt;; } And you'll never have to mess with the complexity of nested iterator types.
It's awesome to see Rust continue to add convenient features, instead of falling into the mental trap of believing 1.0.0 is perfect and complete.
It can't be used with traits, so that crate probably can't take advantage of it.
I'm not sure I like the idea of the OIBITS traits leaking through. This seems to break the abstract nature of these return types, making them depend on the actual implementation. If `Send` or `Sync` is really necessary then why not allow it to be specified in the return type like so: fn a() -&gt; impl Trait+Sync {...} This is what's required with inputs: fn&lt;T:Trait+Sync&gt; a(t: T) {} why not with outputs?
You can get it working if you cast your functions as function pointers: d.iter().filter(is_even as fn(&amp;&amp;u32) -&gt; bool).map(sqr as fn(&amp;u32) -&gt; u32) They are function items otherwise, as indicated by the `{fn_name}` after the types in the error messages.
You may not want to use WebSockets in 2016 since [they are not supported in HTTP2](https://daniel.haxx.se/blog/2016/06/15/no-websockets-over-http2/) TL;DR: You can't initiate a Websocket from an HTTP2 connection. Until a spec is written and is agreed upon (not guaranteed to happen), you can either: 1. use an HTTP1.1 connection and upgrade it to a Websocket, or 2. use old polling tricks over an HTTP2 connection. Polling is fast in HTTP2 thanks to multiplexing (no additional connections need to be created).
Thanks for your help! I updated the post and the example. I am unfamiliar with the concept of function items - where are they documented? Also, why are they not automatically coerced? I mean, what is the usecase where you do not want them to be function pointers too?
They are _sometimes_ coerced. I don't know the exact rules, though. I don't know the other answers either. I'm occasionally wondering the same, and I don't remember the motivation for changing to the current system.
I've read (and received) plenty of comments about how Rust doesn't need a certain feature or such a feature would be "impossible", which implies that Rust is perfect as-is. There's no harm in wondering "what if you could do..." and maybe submitting an RFC.
What a day! First futures, now impl Trait! Kudos to all involved, especially eddyb, Alex Crichton and Aaron Turon.
It's also easy to automatically assume Rust's designers considered every possible feature prior to 1.0.0, and that's why 1.0.0 doesn't support certain things. For example, someone in #rust once said that partial/incremental compilation (aka re-using .o files) would be "impossible" (read: too complex to imagine a solution off the top of your head) due to Rust's Generics. A few months later, nikomatsakis submits [this PR](https://github.com/rust-lang/rust/pull/34956). It's better to let people try and/or let the RFC process weed out "bad" features.
&gt; For example, someone in #rust once said They were certainly misinformed in this specific instance, we've always wanted incremental compilation, and knew it was possible. "due to generics" is a weird reason too, after all, other languages with generics support incremental compilation...
actually, on second look, it seems Stream's have an into_future method, but the Stream trait isn't actually bounded by IntoFuture? Is that an oversight?
TIL: Server sent events :).
Hi, I'm the other Way Cooler developer. We are still developing the beta and It's a bit rough around the edges, but the window manager is definitely usable. We also maintain the Rust bindings to wlc, a Wayland compositor library. We're using that and wayland-sys from crates.io. We already have a client library for our IPC in Python and an example program that displays the current layout as a tree. Rust and Lua client libraries are next on the agenda. 
It's not that I disagree with you about the way things *should* be. The community and the core team have and continue to demonstrate openness to constructive criticism and have consistently given due consideration to concrete proposals through the RFC process. Insinuating otherwise because of what some randoms might have said on IRC at one point or another is unfair.
WebRTC will soon be a good alternative. It has some pitfalls but I think in terms of firewall issues they'll be sorted once network admins start allowing it for video conferencing etc.
`use` declarations are relative to the *crate* root, not the module. Referring to a type directly is relative to the current module. mod a { // Start a path with `self` if you want to import relative to the current module. pub use self::aa::Aa as A; mod aa { pub struct Aa; impl Aa { pub fn print() { println!("Hello from aa!"); } } } mod b { pub use a::aa::Aa as B; mod bb { // Start a path with `::` to refer to the crate root in a relative path. type Bb = ::a::aa::Aa; } } fn main() { a::A::print(); a::aa::Aa::print(); b::B::print(); b::bb::Bb::print(); }
Is wayland actually usable already or are there some issues with it? What are the features missing from i3/bspwm? A tiling window manager is a project that sounds like fun though!
Is wayland supported by Nvidia drivers yet? I was using Sway and looking at this but I get such terrible performance on nouveau drivers. Also does WayCooler support multi-head setups. I can't live without my triple head. 
Indeed. I haven't watched the talk, is this article a misrepresentation of his argument? These quotes seem like pointed jabs at Rust, and the article calls it out by name, but Rust *is* the result of "real world experimentation." Rust is certainly not "obtuse, difficult-to-use, theoretically correct but operationally difficult."
AFAIK the disadvantage with a deterministic hash (even a secure one) you can know hash collisions modulo some table size and cause worst case complexity on a hash table. With SipHash, whether two strings collide modulo N depends on the key. 
Ah, if it doesn't run on Sway it'll probably not work with Way Cooler. We use the same library as they do (WLC) so it'd probably interact the same way. Nothing we can do, but hopefully it gets better over time!
The trait system is also a really great form of polymorphism which has several advantages over inheritanced based systems (there are less ways to get yourself in a mess, you can extend other peoples' types, etc). The ownership system guarantees that mutable access is also exclusive access, which can be hard at first but is a very powerful guarantee. All of these features just build off each other to allow you encode much more complex guarantees about your code in the type system than you can in most languages.
Is a composition operator still possible? There was [this]( http://stackoverflow.com/questions/17054978/composition-operator-and-pipe-forward-operator-in-rust/17111630#17111630) in the past. Would love to see more functional code a la [F#](https://blogs.msdn.microsoft.com/chrsmith/2008/06/14/function-composition/). 
There are: `/* */`
It depends on your background. If you come from a high level language, you probably like features like tagged unions, pattern matching, and generics that are ergonomic. If you come from systems programming, you like stuff like the ownership/lifetime system and the mutability and aliasing guarantees that both reduce bugs and potentially enable more powerful optimizations.
Dan Kaminsky is smart. I reserve the judgement until I can see words in context. It could be that there is misunderstanding, or more interestingly, there could be merit to argument but summary does not do justice to argument.
Depends on who you ask I guess. I would say the type system is what enables the Rust compiler to staticly guarantee your code is memory and thread safe.
I wonder if /u/burntsushi would be interested in this.
It looks awesome. Thanks for investing time to create such project.
Associated types are pretty useful. That said, region typing / lifetimes are probably Rust's killer feature.
WebRTC is not an alternative to websockets.
If I wanted to use crate AA in mod a. I need to externa crate at the root not in mod a; then use AA::t; in mod a?
Oh and thanks that has been a great help! I think I understand it now. I think my confusion stemmed from crate vs mod. Assuming mods could declare their own dependencies. Still unsure the best way to structure mods that use external crates that I might want to turn into a crate in the future. at the moment I am putting all my extern crates into the main.rs
Yeah, having `extern crate` decorations at the crate root is really the best way to go. In my projects I only have `extern crate` in modules that are conditionally compiled, like with `#[cfg(feature = "some-feature")]`, where the `extern crate` is optional and enabled with that same feature. In that case, you import from that crate by using `self::crate_name::&lt;...&gt;` in the module that imports it and `super::crate_name::&lt;...&gt;` in child modules.
Thanks so much. I think this clears the haziness I had.
In https://play.rust-lang.org/, is there a way to pass command line arguments? Pressing Run passed "./out" as argument 0, but I would like to pass other data.
It is true for any function.
Note however that they aren't favored by formatting standards.
Does esper only support sending out notifications when somebody POSTs to an endpoint? Or is there an API for sending notifications from the server without a REST request?
Rust and Go are not really competitors are target slightly different areas. I look forward to hearing about your decision.
I wish http2 supported websockets :(
Like any other trait, you'd need to Box them. Unless you use this new syntax which lets you not need the box.... `-&gt; impl Trait` :)
Display server developer here: Wayland does *not* use the same drivers as X does! In fact, Wayland doesn't *have* a driver model, which is why everyone gets to write their own (which, in practice, means that everyone only supports the kms-mesa stack). I'm not entirely sure how it could have been done better, but the Wayland devs have done a reality bad job of communicating what Wayland *is* - an IPC mechanism and some common protocols. In terms of the traditional windowing stack, this means Wayland is X11 - the protocol - rather than Xorg - an *implementation* of an X11 server. Everyone gets to write their own Xorg equivalent (sometimes with the help of some display-server-helper libraries - in this case wlc)
Does it really matter? I have yet to find a good usecase for websockets that I could not do otherwise.
AFAIK Scala's type systems supports nullness , this can lead to representation to an 'illegal state'. Atleast has support for Option Type though.
You could perhaps extend it to predicates over multiple arguments (for some arities of 1..12).
&gt;Is wayland supported by Nvidia drivers yet? Nvidia made a patched version of the Weston compositor that works with nvidia drivers, but that's pretty much the only supported wm 
They can do, without any syntactic sugaring, sometimes, yeah - but with something like async-await this is much less so. When you see the `await`, its very clear that it may be asynchronous in nature - that's it's *function*. And, of course, you can still use channels or other techniques, should they be a better fit for whatever it is you're doing. Indeed, I do this myself - use await-style asynchrony in the simpler cases where brevity matters more for comprehension than explicitness, and more explicit constructs where what's happening is a bit complicated or unusual in some way. 
Scala's type system is so overrated. Sure, you can do a lot with it, but its complicated, full of unintuitive behavior, and requires stupid gymnastics to paper over dumb design mistakes. They just threw in every typesystem feature they could think of w/o caring if they fit together or formed a pleasant, cohesive experience. And the whole castle still falls over on null.
~~Yes, I have.~~ Oh, I had it only in lib.rs, not in main.rs. Interesting. Thank you!
I'm looking at ~~Jenkins~~ Travis' output, and it looks like it did indeed fail. How can I confirm whether this is the case or not?
Cool, looking forward to removing feh from my machine :).
The type system is Hindley-Milner-like, but chooses a slightly different trade-off that makes it more ergonomic w.r.t. lifetimes etc. Rust's type system is Turing complete – as you can see from crates like [typenum](https://crates.io/crates/typenum). However, it is also cleanly separated from Runtime, no reflection necessary. Function-local inference lets you usually write very concise code, while the function signatures give you enough clues to see what's going on.
For what it's worth, I agree with you on this, and I'd prefer Rust would minimize accidental public API guarantees.
&gt; turing complete Wait... Does that mean that typecheck is subject to the halting problem?
&gt; The relevant cases involve multiple crates and the fact that OIBITs already leak through e.g. private struct fields - that is, all the iterator adapters are Send or Sync if their state (base iterator and closure for map, filter, etc.) is also Send or Sync - you'd lose all that if you wrote something similar with impl Trait. But in the current iterator case, it's dependent (and fixed) on the visible types the user interacts with. E.g. An iterator giving `Arc&lt;T&gt;` versus `Rc&lt;T&gt;` is obviously `Send` depending on the item type. With the `impl Iterator` variant it might be intentional, it might not be. I have no way to tell. Is there a way to keep `Send` and `Sync` from being autodetected? Can I at least do `impl Foo+!Send` if I don't yet want to guarantee, or disallow the auto detection? Edit: I should not that I might not be up-to-date on where else Rust is doing it this way. Edit 2: I also just (re)realized that there's of course a problem with how one would specify such a thing syntactically.
No, you need websockets for that.
I've made [a small example project](https://github.com/carols10cents/importing-example) that matches what you've said here as closely as I can tell, and it compiles. I did make some things public so that I could call them from `main.rs`, mostly to get rid of unused code warnings. This might help to compare your code to and see what's different? I'm helping to update the [modules chapter of the book](https://github.com/rust-lang/book/pull/142/files) right now actually. I'd be curious to know if reading this helps you understand what's going on and how to fix your problem, and if not, what could be clarified or expanded!
You've made the statement but not explained why. Perhaps I'm missing something but the only benefits to websockets are: - It's already implemented fairly widely - It uses http to get past firewall restrictions WebRTC is an API, not a protocol. So you've got a pure TCP connection, so that's no overhead and no need for server libraries. The only downside is that you need to run a TURN server but that's not exactly intensive work. You could argue the firewall issues are a benefit too, at least for enterprise. Most consumer products have the ability for upnp to enable NAT traversal, which is on by default for most people thanks to gaming consoles. So basically firewall is non-issue for the vast majority of people and for those who it's an issue, it's an issue because their network admins want it that way or are incompetent. Maybe I'm missing something but webrtc can do everything websockets can do (Mostly better) and more.
Fuchsia uses Mojo IPC, which has Rust binding: https://fuchsia.googlesource.com/mojo/+/master/public/rust/ I posted about Mojo [here](https://www.reddit.com/r/rust/comments/4xbikb/mojo_for_rust/).
Aww man. I was hoping for a way to keep a secret while yelling really loud. 😁
Only the standard library is going to be available, imagine it as them just running `rustc -O yourfile.rs` and running that against input.
I'm trying to do some thread level sandboxing, and I want the ability to reuse that thread in different places. So it won't always be the case that both of the sandboxed functions would run *right* after each other. It may be more like: let a = sandboxed_fn() let b = unsandboxed_fn(a) let c = other_sandboxed_fn(b) and I don't want to spawn the thread over again for both of the sandboxed functions. I'm assuming it isn't possible because the thread's stack size will have to change for the two different functions, but maybe there's a way around that?
It's not a travis thing, it's a buildbot thing. The easiest way to check is to `rustup update nightly` and then `rustup run nightly rustc --version`, or look at http://rusty-dash.com/nightlies , which shows a green checkmark for 8-13's nightly.
A threadpool wouldn't let me run different functions on a single thread - it would let me run a single function without reallocating threads.
&gt; Does it cause the types to grow like iterators do, and will it benefit from the same work to hide those types? Yes and yes, that's why the blog post is showing that syntax rather than the actual, current syntax.
If you have the time, I'd love to get your feedback on https://github.com/rust-lang/book/pull/142 Here it is rendered: * https://github.com/rust-lang/book/blob/333553e44373b15c341c039f82873d89f262b93c/src/ch07-01-modules.md * https://github.com/rust-lang/book/blob/333553e44373b15c341c039f82873d89f262b93c/src/ch07-02-mod-and-the-filesystem.md * https://github.com/rust-lang/book/blob/333553e44373b15c341c039f82873d89f262b93c/src/ch07-03-controlling-visibility-with-pub.md People often find it confusing, I've been working on trying to figure out how to best address the issues.
Agreed, all due respect I think this is a terrible idea. What was the reason behind wanting it to be based on electron? edit: cc /u/snirkimmington
That looks very interesting, thank you!
As I've already said, I forgot to add it in `main.rs`. I guess that having `lib.rs` isn't enough or that mixing `main.rs` and `lib.rs` makes problems. If there is something that should be worth mentioning in the book, then it's certainly relations between `main.rs`, `lib.rs` and the modules.
Woah, I believe I've once filed the [exactly same issue](https://github.com/rust-lang/cargo/issues/2800) before! The patch exists but is currently blocked on the regression; it is probably going to be fixed in the near future.
That's neat. More like a band aid, but better than nothing. I'll take it!
I don't think we currently do any analysis about stack size (the compiler doesn't know about thread::spawn) We just allocate an X MB stack and pray. Just keep the thread waiting on a channel and send it messages with closures or something. Perhaps we could reduce stack size for functions which we can determine have a limited stack. IIRC there's very little interprocedural analysis in rust right now though. But a simple check that bails out on recursion could be possible. cc /u/acrichto and /u/eddyb
Yeah, I had assumed that stack size was allocated based on the function passed to the thread. This is interesting though, and I've just implemented a quick proof of concept based on passing boxed closures to a loop, which works for my purposes so far.
Hi /u/rap2h and /u/Devnought and /u/Maplicant and /u/Spartan-S63 and /u/rjc2013. I did a thing. https://gist.github.com/retep998/eeca39710290d294c81f38e8f8490801
/u/Gankro suggested on Twitter that this may in fact be a case of convergent evolution: initially most of the `Iterator` api-s worked with `Iterator`. They were then changed to use `IntoIterator` so suddenly `flat_map` became a generalisation of `filter_map` even though it wasn't before. This did happen before 1.0 and it makes me wonder if there aren't actually cases where `flat_map` breaks due to some kind of eager inference caveat. Internally the have different implementations, even though I _think_ `filter_map` could just be implemented using `flat_map` and `type FilterMap&lt;...&gt; = FlatMap&lt;...&gt;;`.
As far as I know, the only difference is that `filter_map` provides a better `size_hint` than `flat_map`, since it knows that there must be fewer or equal number of elements than in the original iterator. Edit: Yep, just tested it: https://is.gd/EdnnJx
In theory yes, but in practice you'll likely just crash the compiler, either by stack overflow or memory exhaustions.
That's an interesting point, though the lower bound can't be any better than zero due to the predicate. It looks like `FlatMap` does some funky stuff as far as `size_hint` is concerned and it does a little more work to support `DoubleEndedIterator`. I suppose this would have been a good case for specialisation, but backcompat does tie our hands a little bit (in terms of different specialised associated types).
You do not need to use long polling, you can use server sent events.
It's kind of like awesome using lua as a way to easily script it and extend it which is what the language is primarily for, though I think doing it all in Rust would be preferable.
For a realtime game like agar.io you really need bi-directional, full-duplex communication. Sure you can make it work with HTTP requests but the game will be unplayable.
I do find `filter_map` much clearer in what it does for what it's worth. Not a huge fan of the fact that `Option` is an iterator to begin with.
The risk is that the proxy/firewall notices that there are no bytes going over the connection for a certain time period and closes it. Even though it can't decode the contents of the connection, an intermediary can still notice whether data is or isn't being sent. [Over on HN](https://news.ycombinator.com/item?id=12282898) it's reported as a problem with real-life HTTP2 deployments.
What would a sample that uses ref for something you want to use it for look like?
I'm not convinced agar.io would be possible with SSE. Don't forget some servers have 50+ ppl playing simultaneously, at a typical 30hz update rate this is alot of data that needs to be processed. &gt; Both websockets and SSE are inappropriate for games anyways. You need UDP and you don't get it with either You're right, but when latency and packet loss are low enough it's doable. 
I started to work on a native rust client for X11 based on xcb code, it's in very early stages of development (i'm designing it). https://github.com/jeandudey/xrb
 (0,0) =&gt; "FizzBuzz".to_string(), (_,0) =&gt; "Buzz".to_string(), (0,_) =&gt; "Fizz".to_string(),
True, it shows intent much more clearly, though I was always a bit iffed that I have to call `ok()` on a `Result` for it to work with filter_map. With `flat_map` it also works on `Result` as that one also implements `IntoIterator`. Still, not intuitive.
That's the same with both websockets and sse. You need to send keepalives. Websockets just do it automatically. 
Yeah, something like that. 
I think you would still need something like long polling on Microsoft's browsers, which do not support server sent events. http://caniuse.com/#feat=eventsource
Wrong sub.
I think the real best of both worlds is the way Kotlin handles nullable types. Even if you don't have a vested interest in JVM tech, Kotlin sports a lot of clever concepts without being a kitchen sink of features associated with Scala. =-)
Thanks for pointing that out! Copy/paste error. I've updated the post.
Exactly! Hopefully the rust compiler will gain new functionality to detect when a deref would be sufficient to match the type and emit a more specific error. The compiler error did include a "the following implementations were found:" but it took me a while to connect the dots.
Why do you want to assign to `x` before printing? fn main() { for i in 1..101 {} match (i%3, i%5) { (0,0) =&gt; println!("FizzBuzz"), (_,0) =&gt; println!("Buzz"), (0,_) =&gt; println!("Fizz"), (_,_) =&gt; println!("{}", i), }; } } is a perfectly valid implementation, likely more efficient due to avoid the intermediary string, and suffers no such type issue ;) 
+1 for a better name. Now that we can return Traits, do we even need a specific name? I mean, can't we just clarify the definition of Trait a bit and be done? 
Could I ask why it is not considered idiomatic? This is a common feature in other languages and I don't see why we should have comment characters in every line of a block comment.
While this is an efficient and idiomatic way to solve the problem, I'm not sure I'd recommend it to someone writing one of their very first rust programs. Cow relies on an understanding of a few concepts they might not have encountered yet. 
Deref coercion never works for operators. It might seem like it does because there are some generic impls like `PartialEq&lt;&amp;B&gt; for &amp;A where A: PartialEq&lt;B&gt;`. The result of these is that if the number of references are the same on both sides of the operator, the operator will work. It would be nice if there could be any number of references on either side of the operator, but the impls to achieve this result in conflicting impls because you can follow multiple "paths" between these impls to get to e.g. `&amp;&amp;A: PartialEq&lt;&amp;&amp;B&gt;`. Of course operators could perform deref coercion. I think there are reasons why they don't, but I don't know what they are.
Ah, interesting. Good to know!
No to get all technical on you, but this is true of any turing complete system. :-) Turing equivalence is usually discussed in terms of hypothetical computers within infinite resources; given any finite limit on the e.g. the length of the tape, the capacity of memory, or the depth of the stack, its unlikely (maybe impossible?) that your computer will be equivalent to any particular turing machine unless it is literally isomorphic to that turing machine. Of course the important thing is that programming in Rust's type system would be a frustrating and unproductive exercise.
You can try `Rc&lt;Material&gt;`, instead, which is cloneable. Really just swap `Box` for `Rc` and use it like normal, and also clone it when you pass it around. If the datastructure containing `HitRecord` has a shorter lifetime than the datastructure containing the objects to be rendered, you can use references instead.
you are confused sir (this channel is not for the game rust, it is for the programming language rust)
Reddit pro tip: read the titles of the first few posts to make sure you're on the right subreddit. If you have any questions about or interest in the Rust programming language, feel free to come back and join our inclusive community :) If not, I hope you get your problem with Rust (the game) resolved. As a gamer and aspiring game developer, I know the pain of lagging in game.