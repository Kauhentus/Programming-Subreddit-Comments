The vast majority of Rust APIs have properties like that, and it's a very good thing. If everyone used explicit constructors for new types instead of methods on other types for producing them, then APIs would quickly become sprawling and unmanageable. Like, imagine if you had a regex, and instead of calling `re.find(text)`, you had to call `Match::new(&amp;re, text)` instead. My users would rightfully revolt. Is the latter really "more consistent" in any meaningful way other than to say that every type is introduced with an explicit `new` method? Moreover, this consistency you're after is just literally not possible in general. If you need to build a type that is not defined in your crate, then you can't just give it a new constructor. It either needs to be built as a method on some other type or as a free-standing function. Finally, the Rust ecosystem uses the builder pattern quite a bit, and this runs counter to defining an explicit constructor, since the whole point of a builder is to act as a more elaborate constructor for a particular type. Yes, I get you're just making an observation, but the implication here is that the inconsistency you and the OP have noted is somehow bad. But it isn't. It's idiomatic and encouraged, and to do otherwise, as I've shown above, isn't practical. This is like looking at four tires on a car and noticing that the hub caps are on two different sides of the tire, depending on which side of the car the tire is on. Does this make the tires inconsistent? Sure, in some sense. Is that inconsistency meaningful or somehow bad? No.
The `least` reference may become dangling exactly because it is a pointer into memory managed by the `HashSet` (into an entry in it, in fact).
I thought to use that but then you can't put it in a trait bound :(
Both [std::fs::File](https://doc.rust-lang.org/std/fs/struct.File.html) and [std::io::Cursor](https://doc.rust-lang.org/std/io/struct.Cursor.html) implement both `Read` and `Write`.
I just made a struct named dummy and implemented both do you think it is fine? the read and write methods do nothing but returning Ok(0)
Your method worked great! I made a script to automate the whole thing. Thanks again for your help.
Sure, not a big deal for a CLI. Was just trying to understand what’s sensible in Rust, for now and for future projects.
Let me preface this by saying, I know not everyone here is a fan of React/Redux patterns. That said, I wanted to see how far I could take them in an ergonomic-ish way. Turns out it wasn't half bad. I wanted to offer this code sample as a way of demonstrating many implentations of React/Redux style programming in Rust. This is a simple interface for incrementing, decrementing and updating a counter, but it shows off a number of patterns: * data store with state "immutable-style" reducers and listeners * selectors of state for memoized calculations of state * actions and async actions * containers that help map store to components using a "connect" like mechanism * components using virtual-dom-rs/Percy and how to manage the dom updates I'm really curious for feedback! I may turn some parts of this into a library :) Fair warning: virtual-dom-rs/Percy is still pretty rough, I haven't been able to get its event hook macros working. Maybe a Percy expert can see my problem in: https://github.com/richardanaya/virtual-dom-rs-counter/blob/master/src/components/counter.rs i've only been able to manually dispatch actions and visualize it imperatively.
[`Rc::try_unwrap`](https://doc.rust-lang.org/stable/std/rc/struct.Rc.html#method.try_unwrap). It will only succeed if the `Rc` is unique, as it'd be unsound otherwise.
Update: Here is the source of the panic: https://github.com/tokio-rs/tokio/blob/master/tokio-async-await/src/compat/backward.rs#L87
If you want to use async/await today, consider giving Romio a try https://github.com/withoutboats/romio
IMHO, Routing is a very important part of a SAP, because it helps you to organize the components in the app and contributes to a better navigation experience. 
Woah, are you the hacker they call 4chan?
What i'd love for the community is a really awesome virtual DOM framework that feels like a pleasure to use independent of any framework. Right now I see alot of recreation of the wheel going on and your interface is pretty elegant. My humble request would be that you separate out your vdom stuff + macro into a separate package and show how to use it without your framework. Thanks!
That depends on what you want it to do? Returning `Ok(0)` from `Write::write` 100% of the time will either cause the function to return an error (if it interprets this as stream closed), or loop indefinitely (if it doesn't). That is, if it writes to your dummy `Write` at all. Similarly with `Read`- it'll think the stream is closed (possibly erroring, depending on whether it thinks it needs to read some data). If I were you I'd look at the implementation of this function and see what it actually does with this parameter. Depending on what it's actually doing, your dummy struct could be harmless, could cause an `Err` return, could cause it to not do anything, etc. I can't with certainty say that giving it this is either OK or not. --- With that said, if you want to have a `Write` which discards all data sent to it but does actually allow writing data, I recommend returning `Ok(input.len())` from `write`. That will tell it that the data was actually written.
With a variable called stream, I'd guess TcpStream? It implements Read and Write. Maybe it would help to know more about what the send function does.
You're looking for /r/playrust
Would love to do that. Right now, it's tightly-coupled. The traditional vdom is a simple, elegant approach to read/write, but I ideally, we could move to something more efficient that doesn't recreate the whole vdom each state change.
I think deno is quite an exciting project, I could definitely imagine using TypeScript as a Python replacement. For those who would like to know more, this [recent talk](https://www.youtube.com/watch?v=FlTG0UXRAkE) gives a good overview, inc how and why it uses Rust (shame about the audio quality).
I don't know much about what you are talking about but maybe you will find some answers in this link I saved yesterday : https://www.reddit.com/r/rust/comments/8ygbvy/state_of_rust_for_iosandroid_on_2018
What does this have to do with Rust programming? You’re looking for /r/playrust my man.
Because it would cause tons of breakage in the crates ecosystem and provide no tangible benefit.
When am I supposed to use `cargo crev trust`? Only if I personally trust and vouch for that person or just if I find that he/she seems to be doing the review work reasonably?
What are you expecting the type of `local_a` to be?
Awesome work! So with this: `pub fn attach(&amp;mut self, element: &amp;web_sys::Element, mailbox: Mailbox&lt;Ms&gt;){}` `pub fn detach(&amp;self, el_ws: &amp;web_sys::Element){}` we don't need the horror of `cb.forget()`, right? Any problems in performance using the attach-detach event listeners if you have many of them? Thanks!
Checking that something exists before opening it is not reliable, since conditions can change between the check and the open attempt. It's better to try opening it, and then handle the case where it doesn't exist if that returns an error.
i32
Oh interesting. Do you know why? I'm still pretty inexperienced with Rust so I don't understand why that would be.
Oh you're trying to destructure your actual struct? Just `.a` on the rhs is the ideomatic way to do it. Otherwise you can just dereference the Rc with * and destructure the result
This is the send method of `http_req::Request_builder` which doesn't work for some reason. Seems again I hit the wall with rust
 let mut v = Vec::new(); let mut url = String::from(sandbox_base_url); url.push_str(EP_AUTH); let uri = uri::Uri::from_str(&amp;url).unwrap(); let mut req = request::RequestBuilder::new(&amp;uri); req.method(request::Method::GET) .header("Accept", "Application/JSON") .header(ID, SECRET); let mut _rw = dummy {}; let res = req.send(&amp;mut _rw, &amp;mut v); if let Ok(r) = res { println!("{:?}", r); let resp = serde_json::from_slice::&lt;paypal::PaypalResponse&gt;(v.as_slice()); println!("{:#?}", resp.unwrap()); } else { println!("err{:?}", res); } 
Hi, thanks, I am using GNU toolchain and GDB, it works for a simple app but not during testing in another larger project. Was just wondering if more experienced users might have an idea why...
hi there, don't say "doesn't work" without elaborating. There are plenty of people that can help, but they need to see what you see — when it doesn't work, what do you see? Error messages, compile errors, or errors when running the code, or just the behaviour? Etc.
1: Nailed it. 2: Not sure - I haven't run any benchmarks. They should only need to attach/detach when their attached element is replaced, but there may be some efficiency problems in the way the closures are handled.
There are different trust levels`. `low`, `medium`, `high`, and you can read some comments about which one means what when editing your *trust proof* (after you run the command).
hi I am getting an error on line 148 about not assigning to a borrowed mutable value (an mutable iterator) but I think I already have done that on previous lines. I am guessing it has something to do with creating a new reference but I am not really sure. https://pastebin.com/tDbupcNg also is there any better way to write the match statement to overwrite the previous value or do nothing. I am not sure how idiomatic my attempted solution is 
The problem is that I'm again totally lost. Always this happens when I try to something in rust. Libraries require so many cryptic parameters. For instance i tried to use reqwest after spending 6 hours on the previous problem i wrote and in reqwest i simply want to set headers but i cant because for some reason headers.insert requires a header value and i have no clue why they did this what is wrong with simply asking for string keys and values. Either theres something wrong with me or the library authors. 
[https://docs.rs/reqwest/0.9.5/reqwest/header/struct.HeaderValue.html](https://docs.rs/reqwest/0.9.5/reqwest/header/struct.HeaderValue.html) &amp;#x200B; Represents an HTTP header field value. In practice, HTTP header field values are usually valid ASCII. However, the HTTP spec allows for a header value to contain opaque bytes as well. In this case, the header field value is not able to be represented as a string. To handle this, the HeaderValue is useable as a type and can be compared with strings and implements Debug . A to\_str fn is provided that returns an Err if the header value contains non visible ascii characters. &amp;#x200B; ## Methods ### impl [HeaderValue](https://docs.rs/reqwest/0.9.5/reqwest/header/struct.HeaderValue.html)[\[src\]](https://docs.rs/http/0.1.13/src/http/header/value.rs.html#47-312)[−] #### pub fn [from\_static](https://docs.rs/reqwest/0.9.5/reqwest/header/struct.HeaderValue.html#method.from_static)(src: &amp;'static [str](https://doc.rust-lang.org/nightly/std/primitive.str.html)) -&gt; [HeaderValue](https://docs.rs/reqwest/0.9.5/reqwest/header/struct.HeaderValue.html)[\[src\]](https://docs.rs/http/0.1.13/src/http/header/value.rs.html#67-79)[−] Convert a static string to a HeaderValue . This function will not perform any copying, however the string is checked to ensure that no invalid characters are present. Only visible ASCII characters (32-127) are permitted. # [Panics](https://docs.rs/reqwest/0.9.5/reqwest/header/struct.HeaderValue.html#panics) This function panics if the argument contains invalid header value characters. # [Examples](https://docs.rs/reqwest/0.9.5/reqwest/header/struct.HeaderValue.html#examples) let val = HeaderValue::from\_static("hello"); assert\_eq!(val, "hello"); #### pub fn [from\_str](https://docs.rs/reqwest/0.9.5/reqwest/header/struct.HeaderValue.html#method.from_str)(src: &amp;[str](https://doc.rust-lang.org/nightly/std/primitive.str.html)) -&gt; [Result](https://doc.rust-lang.org/nightly/core/result/enum.Result.html)&lt;[HeaderValue](https://docs.rs/reqwest/0.9.5/reqwest/header/struct.HeaderValue.html), [InvalidHeaderValue](https://docs.rs/reqwest/0.9.5/reqwest/header/struct.InvalidHeaderValue.html)&gt;[\[src\]](https://docs.rs/http/0.1.13/src/http/header/value.rs.html#107-109)[−] Attempt to convert a string to a HeaderValue . If the argument contains invalid header value characters, an error is returned. Only visible ASCII characters (32-127) are permitted. Use from\_bytes to create a HeaderValue that includes opaque octets (128-255). This function is intended to be replaced in the future by a TryFrom implementation once the trait is stabilized in std. # [Examples](https://docs.rs/reqwest/0.9.5/reqwest/header/struct.HeaderValue.html#examples-1) let val = HeaderValue::from\_str("hello").unwrap(); assert\_eq!(val, "hello"); An invalid value let val = HeaderValue::from\_str("\\n"); assert!(val.is\_err()); 
Because types are used to formalize specifications so that they can be reasoned about by the compiler to prevent you from introducing bad logic into your code.
Using `HeaderValue` instead of a plain string is saving you from a frustrating debugging session when your `contnet-type` specification isn't working right.
so the entire API is shaped around a 1% usage which is in the spec. So again wouldn't it be a lot logical to do a special function just for this special case and normally handle everything in easy logical ways. All other http client libraries in other languages i know tries to be user friendly allowing you to set headers this way but it just seems in rust its a custom to complicate things for the average developer
So what you are saying is that all other languages and clients written in those are doing this wrong.
They're doing it in a way that makes applications easier to write but also easier to write wrong. These libraries often make the same tradeoffs that this language did by making both of those harder.
Its 8:15 am right now since 1 am im just trying to send a simple post request to paypal api and get a response back and the say part is that i couldnt even build the client to send the request yet. If i spent writing the thing over a tcp socket by myself without a client it would have been faster. So what is the point of using a library if it makes things slower. 
One thing I haven't seen in the docs is event subscriptions for things like keydown /mousemove on window. &amp;#x200B; I think draco handles this rather nicely. ([https://github.com/utkarshkukreti/draco/blob/master/examples/mouse\_tracker.rs](https://github.com/utkarshkukreti/draco/blob/master/examples/mouse_tracker.rs)) &amp;#x200B; Also the routing addition is great but the approach with a hashmap seems quite limited and doesn't allow for dynamic routing. Again I quite like the approach from draco which provides an easy API for declaring routes, partly inspired by warp: [https://github.com/utkarshkukreti/draco/blob/master/examples/router.rs](https://github.com/utkarshkukreti/draco/blob/master/examples/router.rs) &amp;#x200B; Otherwise, looks like things are coming along nicely.
How long did you spend trying things before reading the documentation? The documentation for [reqwests](https://docs.rs/reqwest/0.9.5/reqwest/struct.RequestBuilder.html#method.headers) gives an example for how to set the headers, and the page for the [header module](https://docs.rs/reqwest/0.9.5/reqwest/header/index.html) lists all of the header keys that you can use.
Because it forms a type, and generic bounds can only be made of trait requirements
Is there some reason that you have to use Rust for whatever it is you're building? If not, and you find the design choices in Rust and its ecosystem this offensive, maybe the answer is that Rust just isn't for you — or at least not right now. And that's okay! From time to time I get excited about TLA^(+). But there's very little I work on for which that level of rigor is necessary or appropriate. And in most cases, it would just slow me down. Does that mean there's something wrong with TLA^(+)? Or maybe something wrong with _me_? No, of course not. It's just not a good fit for the particular problems I have. I'd recommend checking out languages like Ruby or Python. Like every language they have their downsides (especially as systems grow), but for smaller stuff they're both super-easy for getting small projects up and running, and have very little of the kind of "bureaucracy" you'll face in Rust. E.g. using Ruby and `httparty`: ```ruby require 'httparty' response = HTTParty.get("http://google.com", headers: {WOOP: "woop"}) puts response.body ``` Pretty neat, huh?
Thanks for taking the time to read the code and the pointers! I found them very helpful. I have applied the changes you suggested and replied to you on GitHub.
I write golang for past 2 years the idea of having 0 runtime errors is so nice in rust but as you also say the cost is really too high for that just with this level of friendliness I really don't understand how a person can use rust it promises all those nice things but just to get started you need to spend at least a month and then comes the libraries 
He would have to add helper functions for every single spot where a HeaderValue is used which kinda seem bad. Also HeaderValue implements a bunch of traits (From&lt;u16&gt; etc. etc.) which makes stuff really practical. It seems kinda neat actually.
Rust documentation has a search bar at the top that's an absolute lifesaver. If you go to the reqwest docs and search for "HeaderValue" you should get to the docs for that type. You probably want to use `HeaderValue::from_str`. The library is written this way to give users control over input validation; not all strings are permitted in HTTP headers.
That's the thing I love about Rust, though--it gets rid of edge cases that developers have to remember and instead makes it obvious what you need to handle. All those HTTP client libraries you point at as good examples? They're a bug waiting to happen. I work primarily in C#, and I honestly couldn't tell you what the BCL does when a header has an opaque byte value, and I can guarantee none of the code I've ever written considers this case, because it's the first I'm hearing of it. Now, I'm guessing that my code will probably coerce the bytes into a string, but it could also throw an exception and crash. It's a trade-off not in complexity, but in up-front or down-the-road complexity. Rust tends to made your mistake obvious up front. Other languages let you take the happy path and debug the problems later.
This kind of question routinely comes up, and its apparent that go doesn’t suffer from this problem. So either there’s a problem with the documentation, people are being misled about what the language is, or the language isn’t providing the right functionality. It’s not like console input or output is a very esoteric thing. I also get confused sometimes when something specifies that it expects a reader but doesn’t specify whether it needs a bufreader. Should I prefer the risk of double buffering to not buffering at all?
&gt; I really don't understand how a person can use rust it promises all those nice things but just to get started you need to spend at least a month and then comes the libraries Safety and robustness which derive from formality inherently impose restrictions on how you do things - you cannot design an arbitrarily free language yet allow the compiler to analyse it to make it completely safe against some classes of errors (see undecidability of logic systems). Specifically you cannot write a system/compiler that guarantees soundness of arbitrary code without solving the Halting problem. In other words, there are inherent reasons why C, Go, or Python cannot guarantee lack of certain runtime errors at all by design. I would venture and suggest that you're using this project as your starting point in Rust, rather than going through the books. This is completely fine, but you need to understand every language has its design decisions, and sometimes to address certain goals, there are really no other better ways (as in close to impossible).
It's very different from other languages, isn't it? If it helps at all, I've found the pain worth it -- Rust has helped me to understand domains and APIs much more deeply, which in the long run has been hecka rewarding as an engineer. To your other point about time: a month to get productive in Rust is not a bad speed. Rust was originally conceived to help with the large technical debt inside of Firefox, so it's optimized for front-loading learning and later trading cognitive load. The time you save comes later, when you're trying to refactor thousands of lines of code at a time and...things simply work once you get your code to compile again. I shudder to think of doing that with vanilla JS codebases I've worked with professionally. So, my advice is this: give yourself some time and space and cut yourself some slack when things feel slow. Take breaks when learning Rust...because that's better than breaking your computer. :) They'll speed up, I promise!
&gt; I really don't understand how a person can use rust it promises all those nice things but just to get started you need to spend at least a month It's a fundamental trade-off. Rust _is hard to learn_ compared to Go. But after you have learned it, it is (in my experience) no more difficult even for simple tasks than the equivalent solution in Go, and far nicer to use for large projects. I was productive in Go on day 1. Sure, my code broke in production, but by golly I was up and running quickly. On the other hand, it took me _weeks_ to be productive in Rust. But _after_ that painful learning curve I was _really_ productive. You could think of it like walking versus riding a bike. Walking is pretty easy, especially if you've already been doing it for decades. But then you hear about these newfangled "bike" things that let you go really fast, and seem to be all the rage these days. "I'd like to go really fast, too", you say to yourself. So you find a bike, jump on it, and immediately fall over. You try again, make a couple of meters, and fall over again. After ten minutes of this, you throw your arms up in the air and declare "I could have walked a kilometer by now! Why would anybody ride these stupid things?". And it's true. You could have walked a kilometer in that time. And maybe if you never actually need to go all that fast or far, then you don't need a bike. But if you _do_ decide that what a bike has to offer is worth it to you, then there's no getting around the fact that you'll need to put a lot of time and effort into learning to ride the thing. From some of your comments, it sounds a lot like you haven't yet read the book. If you don't want to have a _really bad time_, then I'd suggest that you pause, read the whole thing, and then start hacking again. If that's too great an investment, then Rust is probably not the language for you, because it's the kind of tool that you really do need to learn if you're to have any hope of using it without tearing your hair out. 
Lack of dynamic routes is due to a struggle with lifetimes and storing popstate listeners. Hopefully will get that sorted, using Draco as a guide. Window listeners should be a straightforward addition, but need to think on the API for it. Might take a diff approach from Draco. Considering having an additional optional func passed to `seed::run` that accepts the model, and outputs a Vec of Listeners`. Can you clarify on nesting components?
futures\_cpupool::spawn returns a future that you can and\_then off of.
But there's something very valid in what OP is ranting about and it bothers me that they're getting downvotes and a lot of "this is why Rust is superior"... Yes, here Rust makes it obvious that you're making a mistake _that you're almost certainly never going to make_, but in doing so it makes itself painfully un-ergonomic and the opposite of productive. From the docs for the underlying type, it doesn't seem obvious why this: let mut headers = HeaderMap::new(); headers.insert(HOST, "example.com".parse().unwrap()); assert_eq!(headers[HOST], "example.com"); Isn't couldn't have been considerably more ergonomic as this: let mut headers = HeaderMap::new(); headers.insert(HOST, "example.com"); assert_eq!(headers[HOST], "example.com"); Especially when that `&amp;str.parse().unwrap()` is about as transparent as voodoo to someone starting out. So they try the latter and find themselves getting an error message that leads them to believe what they need to do is: let host = HeaderValue::from_str("example.com").unwrap(); let mut headers = HeaderMap::new(); headers.insert(HOST, host); assert_eq!(headers[HOST], "example.com"); Which is _painful_ without any obvious benefit.
What do you _expect_ this to do? From what I've read of `http_req`, this will attempt to communicate via HTTP with the first parameter, and then write the end result (after doing the communication) in the second. I find it highly unlikely that your `dummy` structure implements an HTTP server...? So, with your code as written, I would expect it to fail, and I see really no way to change it not to do that without knowing what _you want this code to do_. It's very non-obvious what you're trying to do...?
it does but wouldn't that and_then call schedule the chain onto the cpupool future and not tokio...now that im asking im sort of realizing it doesn't matter ?
&gt;\&gt; Like, imagine if you had a regex, and instead of calling &gt; &gt;re.find(text) &gt; &gt;, you had to call &gt; &gt;Match::new(&amp;re, text) But there's a difference here, the operation is using a regex to check a match, which happens to construct a Match object to contain the results of that operation. To use a grammar analogy, the regex is the "subject", it makes sense to have it as the first thing on a line of code. &gt;Moreover, this consistency you're after is just literally not possible in general. If you need to build a type that is not defined in your crate, then you can't just give it a new constructor. Bute fs::metadata *is* defined in the same crate as fs::Metadata. In the vast majority of cases if you're writing a constructor for a type it's because that type is in your crate. I'd say that writing constructor functions for a type that's defined in another crate is a code smell. &gt;Finally, the Rust ecosystem uses the builder pattern quite a bit, and this runs counter to defining an explicit constructor, since the whole point of a builder is to act as a more elaborate constructor for a particular type. This is true, but fs::metadata isn't a builder pattern.
The CpuFuture is going to be running on the tokio runtime.
Well said. 
Hi, I just known that I can put `&amp;` in argument name in order to get the argument value, not the reference, like this ``` fn print_number(&amp;value: &amp;u32) { println!("{}", value); // value is type u32 } ``` I wonder what this syntax is called, so I can read more about it. And when I change to `&amp;value: &amp;String`, I got this error `cannot move out of borrowed content`. Why is it? When the value is moved and is borrowed in this case?
Unfortunately, the error messages are the worst part of typenum, and will never be great. You might be able to improve them in this case by creating a trait like the `IsOdd` you suggested above. I can think of two ways to do that. --- You could create a trait for `Even` and a trait for `Odd` like so: pub trait Odd {} pub trait Even {} impl Even for U0 {} impl&lt;U: Unsigned&gt; Even for UInt&lt;U, B0&gt; {} impl&lt;U: Unsigned&gt; Odd for UInt&lt;U, B1&gt; {} --- Or you could create a single trait with an associated type, like so: pub trait IsEven { type Output; } impl IsEven for U0 { type Output = True; } impl&lt;U: Unsigned&gt; IsEven for UInt&lt;U, B0&gt; { type Output = True; } impl&lt;U: Unsigned&gt; IsEven for UInt&lt;U, B1&gt; { type Output = False; } You will likely run into trouble if you want different impls for the same type depending on whether it is even or odd. If you need that, you may have to split it into three cases like I did (0, evens &gt;= 2, odds). --- Alternatively, you could use negative trait bounds, but they are still experimental. Here's an example anyway: #![feature(optin_builtin_traits)] pub trait Odd {} pub auto trait Even {} impl&lt;U: Unsigned&gt; Odd for UInt&lt;U, B1&gt; {} impl&lt;O: Odd&gt; !Even for O {} This example is a bit strange, as all types will be `Even` except those that are odd.
Thank you very much! That was crucial information! I removed tokio‘s *async-await-preview* and use future‘s compatibility layer now and it works! For anyone interested in what the above code looks like with these changes: *main.rs*: #![feature(async_await, await_macro, futures_api)] use std::future::Future; use futures::{FutureExt, StreamExt}; use futures::channel::mpsc; use futures::compat::Compat; const CHANNEL_BUFFER_SIZE: usize = 16; fn tokio_run&lt;F: Future&lt;Output=()&gt; + Send + 'static&gt;(future: F) { tokio::run(Compat::new(Box::pin( future.map(|()| -&gt; Result&lt;(), ()&gt; { Ok(()) }) ))); } fn main() { tokio_run(async { let (_sender, mut receiver) = mpsc::channel::&lt;i32&gt;(CHANNEL_BUFFER_SIZE); await!(receiver.next()); }); } *Cargo.toml*: [dependencies] tokio = "0.1.13" futures-preview = { version = "0.3.0-alpha.11", features = ["tokio-compat"] }
Oh my goodness I get it now! Just realized it, and then returns a future that will be complete when cpupool notifies it is complete, which will notify tokios runtime! Thanks!
You can also try the blocking mechanism in tokio, as used in tokio-fs.
Hi u/Janonard, I‘ve found the issue that you opened! The tracking issue for tokio‘s *async-await-preview* states: “The current implementation of the async/await support is limited in that it only works with 0.1 futures.” So, it‘s not really a bug, it‘s just not implemented yet. See what Matthias247 wrote and my reply for a solution to use tokio with futures 0.3. 
&gt; why the library authors are overcomplicating the simplest things is this a tradition in rust community? Replace "library authors" with "tech industry" and you've described the last 3-4 years. 
You could just do `“some value”.into()` is that really so much worse?
Alternatively you can define a simple function that allows you to write sudo_execute(&amp;sudo, dry_run, &amp;["/usr/bin/zypper", "refresh"])? 
I came here to learn new rust trick, but wow, Topgrade is so useful from a user's perspective ! Thanks for developing it !
You're looking for /r/playrust
/r/playrust
This guy posted in the wrong subreddit. Go to /r/playrust
This is not /r/playrust
I like it. I might give this a try tomorrow if I do more prng work.
You are using a library that tries to cover 100% of use cases, and it pays with some complexity. You might have wanted to use something that caters to 80% and offer better ergonomics. The existence of the former doesn't preclude that of the latter.
You can make it a trait trait IsOdd: BitAnd&lt;U1, Output = U1&gt; {} impl&lt;T: BitAnd&lt;U1, Output = U1&gt;&gt; IsOdd for T {} 
Yeah I thought I was just misunderstanding how some math op should be used, but I guess that's the route to go.
After a quick skim, I did find several points that could be improved. The usage of raw pointers and libc functions could be abstracted a bit more. There's also all of `constants.rs` which seems pretty much copied verbatim from `ixgbe_type.h`. I think that constants.rs shouldn't be changed too much to start with, but dependence on it should slowly decrease as cleaning is done.
This won't work, it's essentially equivalent to writing this. type IsEven = dyn Rem&lt;U2, Output = U0&gt;; type IsOdd = dyn Rem&lt;U2, Output = U1&gt;; Which is valid, albeit useless. 
Previously https://www.reddit.com/r/rust/comments/a9p7fo/intellij_rust_changelog_89/?st=JQC3Q737&amp;sh=dbd7b11b
Since `i32` is Copy, you can just deref the `Rc` to get a reference to the inner struct, and then get a copy of the `i32` let Foo{ a:local_a } = *rcf; This wouldn't work for a non-`Copy` type, because you can't move out of an `Rc`.
https://docs.rs/tokio-threadpool/0.1.8/tokio_threadpool/fn.blocking.html Seems to be made for this use case and will avoid having two separate threadpools. 
Thanks for working on this. I've been trying out Rust frontend frameworks recently, and so far seed has been my favorite. I particularly like how components are defined as simple functions. If you're asking for recommendations/feature requests, then I have a few: * Convenience macros/functions for specifying an element's id and class attributes. I find myself adding `attrs!{"class" =&gt; "a list of classes"}` very frequently. Something like `class![a list of classes]` and `id!(unique-element)` would be nice to have. * Support for custom tags. I'm a big fan of semantic custom tags for my components. I'll use built-in HTML tags when they're appropriate (section, ul, p, etc), but like to use custom tags (todo-item, image-carousel, etc) to alleviate div-hell. I noticed that you've defined a huge enum of all valid HTML tags and created (generated?) macros for each of them. Perhaps you could add a macro that works like `el[my-custom-tag-name, attrs!{...}, div![...], p![...], ...]`? I haven't looked deeply enough at your code to tell how hard that would be. And the big one: * "Nesting components" Currently all messages get passed to the same update function. In other frameworks I've used, each component provides its own Message type and update function, which allows you to easily compartmentalize update logic in a way similar to how seed currently lets you compartmentalize view logic. This also allows the framework to only rerender subtrees that receive a message from their children, rather than the entire app. A big part of why I like seed is the simplicity of defining components as loose view functions, but the ability to optionally specify a Message type and update function for a component is a powerful tool that my frontend developers are used to. If you want clarification or some help implementing some of these hit me up, I'd be happy to contribute. 
It's part of Rust's pattern matching system, and known as a [reference pattern](https://doc.rust-lang.org/1.30.0/book/second-edition/ch18-03-pattern-syntax.html#destructuring-references). In your example `(&amp;value: &amp;u32)` (despite being somewhat pointless) works, because `u32` is a `Copy` type. Dereferencing creates a new copy of the value. With `&amp;String` however, trying to dereference the string will not work, as `String` does not implement `Copy` and therefore dereference moves it. Moving a value with just a reference is not allowed, so the compiler prevents it. Even if you immediately dereference a reference argument, it's still a reference. 
I absolutely agree about editions. They should be _as needed_ for breaking improvement to the language only. The whole thing with the marketing push and listing a lot of "2018 edition" features in the guide that work perfectly well in 2015 edition is just a big mess.
Found this page [Are we a web yet (templating)](https://www.arewewebyet.org/topics/templating/).
Many Rust libraries try to have an API that makes it impossible to write invalid states. Reqwest prevents you from sending an `Authorisation` header. That prevents bugs in production at the cost of you having to read the docs on how to use it. If you have feedback on how to improve the documentation to make this more obvious and help the next person who expects headers to be a map from string to string, I'm sure the authors would love to hear it!
That one. I have some wrappers, as I'm not used to working with `poll` directly: use futures::{Async, Future, Poll}; use std::result::Result; use tokio_threadpool; pub struct BlockingFuture&lt;F, T&gt; where F: FnOnce() -&gt; T, { f: Option&lt;F&gt;, } impl&lt;F, T&gt; BlockingFuture&lt;F, T&gt; where F: FnOnce() -&gt; T, { pub fn new(f: F) -&gt; Self { Self { f: Some(f) } } } impl&lt;F, T&gt; Future for BlockingFuture&lt;F, T&gt; where F: FnOnce() -&gt; T, { type Item = T; type Error = (); fn poll(&amp;mut self) -&gt; Poll&lt;Self::Item, Self::Error&gt; { let f = || (self.f.take().expect("future already completed"))(); match tokio_threadpool::blocking(f) { Ok(r) =&gt; Ok(r), _ =&gt; panic!("`BlockingFuture` must be used from the context of the Tokio runtime."), } } } pub struct BlockingFutureTry&lt;F, T, E&gt; where F: FnOnce() -&gt; Result&lt;T, E&gt;, { f: Option&lt;F&gt;, } impl&lt;F, T, E&gt; BlockingFutureTry&lt;F, T, E&gt; where F: FnOnce() -&gt; Result&lt;T, E&gt;, { pub fn new(f: F) -&gt; Self { Self { f: Some(f) } } } impl&lt;F, T, E&gt; Future for BlockingFutureTry&lt;F, T, E&gt; where F: FnOnce() -&gt; Result&lt;T, E&gt;, { type Item = T; type Error = E; fn poll(&amp;mut self) -&gt; Poll&lt;Self::Item, Self::Error&gt; { let f = || (self.f.take().expect("future already completed"))(); match tokio_threadpool::blocking(f) { Ok(Async::Ready(Ok(v))) =&gt; Ok(Async::Ready(v)), Ok(Async::Ready(Err(err))) =&gt; Err(err), Ok(Async::NotReady) =&gt; Ok(Async::NotReady), _ =&gt; panic!("`BlockingFutureTry` must be used from the context of the Tokio runtime."), } } } Sample usage: BlockingFutureTry::new(move || { File::open(&amp;path) .and_then(|file| ChunkedReadFile::new(file, None, headers)) .map_err(|e| Error::from_io(e, path.to_path_buf())) }).and_then(move |crf| future::ok(http_serve::serve(crf, &amp;req))) 
/r/playrust
the sad part is that i have finished the book but it may be covers the 1 percent of the things i see in the real world
Thanks, I am aware of it :) . Just need to identify which one is better, especially about performance and features. 
I personally prefer tera's model of inheritance, and its documentation. Handlebars is a slightly inaccurate reimplementation of a javascript library, so finding valid documentation is hard.
I think it IS worth it because most newbies are coming from online links and will be confused / disappointed if the link is dead. Even most non-newbies probably use the online docs &amp; google more often than the installed docs (at least I do).
Open an issue/send a PR? I'm not sure if they are actively working on this, though. The author of the rust code mentioned in the Q&amp;A that they believe the code can be improved further and probably has a few places where it could be optimized more. However, all work was done during the limited time frame of their bachelor's thesis.
Check out https://github.com/djc/template-benchmarks-rs for some benchmarks.
I would like to add maud (https://maud.lambda.xyz/). It's the most efficient, since it's mostly compile-time Rust macros. It has integrations with most frameworks, including Rocket, and it's pretty well documented. Since templating is done in Rust, you have the whole language to do fancy stuff. If you look for performance, it's clearly the fastest, but it's true that it's not the usual templating engine, and it has some learning curve. You will also not have normal HTML in your templates, which can make it more difficult to maintain.
Me too, I'd be interested in porting PureScript's Halogen framework to Rust because I really like it's approach. But it seems to require HKTs :/
Just make a new function with what you think is repeating too much, like: pub fn execute(...) -&gt; Result&lt;...&gt; { Executor::new(...) .args(...) .spawn()? .wait()? .check() } 
It tries, at least. If you understand everything in it and read the library documentation, I've found that's usually enough. Sure the book doesn't cover hyper or reqwest but they have their own documentation.
Thanks 👍 
Maud is not on stable if I remember correctly. Given the template benchmarks we've so far collected, I would also be very surprised if Maud is faster than horrorshow and Askama. Personally I built Askama because (a) I don't want to learn another macro-based DSL, (b) I want to leverage the type system and the compiler for correctness and performance, (c) I like the syntax that was popularized by Jinja and many other template engines for different languages, (d) I don't want my templates in my source code. 
I suggest that if you want to post your code, try to reduce it - now when I open it I see 200 lines of code, so trying to figure out what the exact problem is seems a bit of a daunting task. Also, if you post your code on [rust playground](https://play.rust-lang.org/), then people will be able to see the exact error that you are seeing with one click of a button. That being said, I suspect that you either started your project something like a month ago, or are using an older version of rust. On 6th of December Rust 1.31 came out, which made 2018 edition the default, and your code does compile with that. The problem with your code is that the old borrow checker is too restrictive, and only understands lexical borrows. match feature { ... ForestPopulation::LumberSap(sap_age) =&gt; { let mut newTree = Tree::new(*sap_age); let mut newFTree = ForestPopulation::FTree(newTree); *feature = newFTree; } } Here, `sap_age` is a reference that borrows `feature`. The old borrow checker thus considers `feature` borrowed for the entire scope that you can use `sap_age` in - therefore `feature` is still considered borrowed on the last line of the branch, and so you cannot modify it. There are two solutions: 1. Update to rust 1.31 and add an `edition = "2018"` to your `Cargo.toml` (also, Cargo 1.31 adds that line by default to all new projects). Then your code will compile without any changes. 2. Because you `sap_age` is `i32` (and thus `Copy`), you can change the matching to get `sap_age` by value, instead of by reference. To do that, dereference `feature` in the `match` statement, and remove dereference of `sap_age` in the branch: match *feature { ^ dereference here ... ForestPopulation::LumberSap(sap_age) =&gt; { let mut newTree = Tree::new(sap_age); ^ dereference here is no longer needed let mut newFTree = ForestPopulation::FTree(newTree); *feature = newFTree; } }
Wow thanks a lot. I have not reached that chapter yet. Pattern matching in rust is so cool.
&gt; The next edition shouldn’t be in 2021 because that’s three years on from 2018, the next edition should be whenever we want to implement feature(s) that are not compatible with the normal stable release schedule. On the other hand, the predictability and "bulk" aspect are pretty nice too. Imagine that, instead, each release came with potential breakage; you'd end up with a situation similar to C++ compilers, where to know whether a feature is supported by a given compiler version you have to check `#ifdef __GCC__ &amp;&amp; (__GCC_MAJOR__ &gt;= 4 || __GCC_MAJOR__ &gt;= 3 &amp;&amp; __GCC_MINOR__ &gt;= 7)`... for each feature. It's very painful. This is why there are language versions instead, unrelated to the compiler versions: big predictable checkpoints! C++11, C++14, C++17 and next C++20. It also introduces a bar for breaking changes. The next set will be stabilized in 3 years; this makes you think twice about whether you really need "this" breaking change or not. Maybe it's not worth it? I mean, we could have it stable sooner if it wasn't breaking...
&gt; The whole thing with the marketing push and listing a lot of "2018 edition" features in the guide that work perfectly well in 2015 edition is just a big mess. Uh, hadn't checked the guide. That's a bit weird indeed :/ 
Can't you just use the `header` method on the request builder, instead of the `headers` method?
Maybe I missed something, but why don't you put the whole App beside a trait, something like that: ``` trait App { type Model: Clone; fn new(initial: Self::Model, mountpoint: &amp;str) -&gt; Self; fn update&lt;M: Clone&gt;(&amp;self, model: Self::Model, message: M) -&gt; Self::Model; fn view(&amp;self, model: Self::Model) -&gt; Dom; } fn run(app: impl App) { // your implementation } ``` and the user would implement this trait to have an application.
My understanding is that newer Tokios have a work-stealing threadpool built in, so stuff like tokio-cpupool isn’t as important as it once was for this.
First two done. Examples: let mut attributes = attrs!{}; attributes.add_multiple("class", vec!["A-modicum-of", "hardly-any"]); div![ attributes ] _ let mut custom_el = El::empty(Tag::Custom("mytag".into())); custom![ Tag::Custom("superdiv".into()), h1![ attributes, model.coords_string() ], custom_el, ] We can create the custom El using either the custom! macro, and passing a Tag::Custom as a parameter, or using the ::empty constructor. Ideally I'd like to provide a func that lets the user set up custom macros (eg seed::make_custom_macros("superdiv", "mytag")), but see issue below. Tags are generated; element-macros are created using repetitive code due to inability to both create macros-with-macros, and have these macros take an arbitrary num of args; can do one or the other, but not both, unless using proc macros, which I've no idea how to create. Need to think on the third one. 
I really love Maud. It's a really clean DSL and it's far less painful to write than actual HTML for templated pages where you're more interested in creating a structure than marking up content.
&gt; But there's a difference here, the operation is using a regex to check a match, which happens to construct a Match object to contain the results of that operation. To use a grammar analogy, the regex is the "subject", it makes sense to have it as the first thing on a line of code. Which is exactly the same thing for File and Metadata. The rest of your comment send to have misunderstood my point. Do you really think I was trying to claim that std::fs uses the builder pattern? I was pointing out why the type of inconsistency being pointed out here is not something that one can or should try to achieve in general.
https://github.com/bodil/typed-html There's probably others.
Maybe you can use fxhash instead?
Have you constructed a benchmark yet? If so, what does it say? If not, you should do that first (ideally based on a real workload).
Hey! I tried my (admittedly simple) code with both standard library `HashMap` and `FxHashMap` and found little difference, though of course that could change over thousands of iterations rather than just a few. I think it's probably not worth it to add a dependency for such small gains so I'll use standard `HashMap`. Thank you for your answer!
Nice to see that someone has taken the time to compare all of these directly head to head. I'd written the Rust version, but hadn't felt like taking the time to figure out how to build the C++ version to compare directly. I guess I should have posted the full version of the generator variant in Rust. Note that `IterGen` isn't actually safe, since I'm not doing anything to ensure that the generator is pinned; that's one of the things that needs to happen before `Generator` and some form of `impl Iterator for Generator` can be stabilized. [Playground link](https://play.rust-lang.org/?version=nightly&amp;mode=release&amp;edition=2018&amp;gist=d36c581630550d05c2662a49770df8a2). Here are the relevant parts that allow you to create an `Iterator` from a `Generator` and the `Generator`-based implementation: #![feature(generators, generator_trait)] use std::ops::{Generator, GeneratorState}; struct IterGen&lt;G&gt;(G); impl&lt;T, G&gt; Iterator for IterGen&lt;G&gt; where G: Generator&lt;Yield=T, Return=()&gt; { type Item = T; fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; { match unsafe { self.0.resume() } { GeneratorState::Yielded(x) =&gt; Some(x), GeneratorState::Complete(()) =&gt; None, } } } let triples = IterGen(|| for z in 1.. { for x in 1..=z { for y in x..=z { if x*x + y*y == z*z { yield (x, y, z) } } } } );
Sorry for making it hard. You are spot on about me starting this about a month ago. I did the 2018 solution, as this project is 100% for me to learn rust. thanks for the second suggestion as well that will be handy for future reference.
Rust is not a language to have a simple life. Neither, to be "productive" with it. If you wanna speed of coding, rust is poor choice. Rust is for "avoid hard debugging sessions later".
I'm creating a benchmark right now using Criterion. I've never done this before, so it may take me a while. I'll report on the results soon.
Thanks. I'll try and add this code when I have time.
Okay so I've done a bit of research, and it turns out the problem is `..=`. Replacing `*..=z` by `*..(z + 1)` doubles the performance. I believe the problem with `..=` is that `x..(y + 1)` is not identical with `x..=y`. They are nearly equivalent on most values for x and y, except when `y == i32::max()`. At that point, `x..=y` should still be able to function, while `x..(y + 1)` is allowed to do nothing. Since the other languages don't care about overflow issues at all, I think Rust should be allowed not to care either. On the other hand, the 
This is a programming language subreddit and has nothing to do with the game. You likely want /r/playrust
I think the key thing to understand is that reqwest doesn't reinvent headers, it forces you to use them properly. If a simple string could have been used, you would have been free to set invalid headers (Strings in Rust are valid utf-8) and it would have been impossible to set some valid headers (the HTTP spec allows headers that are not valid utf-8). You could make the function accept a slice of bytes, which would solve the second problem, but you'd still have to deal with the first. You can make the header map responsible for checking whether the header is valid, but that's not really something the map should be responsible for, right? It's just there as a container. Introducing HeaderValue really is the best option, in my opinion. It's the most natural way in Rust to enforce that the headers you're setting are valid. 
Are you planning on making this open source? I'm learning Neon as well and it would be helpful to look at some real projects.
I didn’t know that this page exists, thank you.
Nice this is so great. I long for the day when we have a rust ide from jetbrains like Go has
I this example the actual return type of `get_loop` happens to be `fn(u32) -&gt; u32`, which is a bit misleading. At least it was for me - when I see `impl Fn(u32) -&gt; u32` as return type, I expect non-virtual calls.
&gt; Since the other languages don't care about overflow issues at all, I think Rust should be allowed not to care either. That's entirely true for D. I deliberately kept bounds checking on when compiling the D code and have since found out that disabling it leads to a 30% decrease in run time for the range version. Using explicit for loops does mean no bound checking though. I just made the change you propose above (=z -&gt; (z+1)) and retimed the optimised build. Indeed the code runs faster. In the simple example, Rust is still the slowest (+50% overhead in time), for the lambda one it's now on par with D and C++, and the range example is now the fastest... I'll have to update the blog post. Thanks for giving me more work! ;)
I just sent a PR with this.
&gt; Since the other languages don't care about overflow issues at all, I think Rust should be allowed not to care either. You can do this! You just have to ask for the overflow semantics you want explicitly. See https://doc.rust-lang.org/stable/std/num/struct.Wrapping.html for example.
Calling println in a loop means you're constantly creating and releasing locks to stdout. If you lock stdout once, performance should increase. It's a common performance issue in these small benchmarks (and also sometimes in actual code).
I've updated the blog post. Rust comes out a lot better this time around.
Calling D's `std.stdio.writeln` does the same thing.
This is definitely also true
One of the other improvements that are possible is precomputing x\*x and z\*z outside the inner for loop, since for one or another unexplainable reason rustc doesn't apply the optimization (it should be done by the LLVM backend). 
Interesting. I was wondering where the extra overhead in Rust was coming from. Frustrating that the safer version incurs such an extra penalty; though in this case, using the less safe version doesn't risk any memory unsafety, just a well-defined overflow which could lead to incorrect results, though only very far down the line. And there's nothing preventing overflow of `z` in any of these, other than not ever iterating that far. Actually, after thinking about it, there's no reason for these ranges to be inclusive anyhow. I had used inclusive ranges in the Rust example to match the algorithm in the original C++ version exactly, but for pythagorean triples, `x` and `y` are always going to be less than `z`, so you can just make those exclusive ranges in all of the versions for simpler code and no extra risk of overflow.
This is true, though I tried these examples with a `stdout` lock up front and it didn't change the runtime much for me. It sounds like the use of inclusive ranges in Rust were the culprit this time, as they require more complicated bounds checking to avoid overflow which precludes some optimization.
Here's an unusual issue I ran into while looking at this: [Adding --emit=asm speeds up generated code](https://github.com/rust-lang/rust/issues/57235)
OK, a follow-up question. Given that the benchmark results I added to my other reply show differences in the 100s of _nanoseconds_, should I add the `FxHashMap` crate as a dependency or is it too small of a performance improvement to be worth it? 
I was also wondering if the fact that output of `println` is line-buffered could be an issue, so I decided to check the number of inner loops. The inner loop is executed 227,112,444 times for 1000 triples. Or, 227,112 executions per line printed. I/O may not matter that much, there.
You should probably also use hashbrown with these different algorithms. 
Note that this isn't updated yet for Liquid 0.18 which adds a ton of performance improvements. iirc it is now always faster than handlebars and on average about the same as Tera.
In the Julia language you can iterate over full ranges inclusively without overhead (inclusive ranges are the default). I'm sure Rust can get the same performance. My guess is that the implementation of the inclusive range iterator in Rust can be simplified quite a bit for better performance. For reference a minimal Julia implementation: import Base: iterate struct MyRange{T} from::T to::T end iterate(r::MyRange) = r.to &lt; r.from ? nothing : (r.from, r.from) function iterate(r::MyRange{T}, i::T) where {T} i == r.to &amp;&amp; return nothing i += one(T) return i, i end
println is a really bad performance killer for these kinds of simple demo codes. Relevant Rust issue: https://github.com/rust-lang/rust/issues/50519
Ha! https://www.reddit.com/r/rust/comments/7xslc1/announcing_rust_124/duaszwp/ `rustc` is not tuned for tiny benchmarks by default, you have to force 1 codegen unit. Of course, very few people benchmarking `rustc` know about this.
For *some* of these kinds; note that it doesn't make difference here, because it's not the hot path.
Maintainer of [liquid-rust](https://github.com/cobalt-org/liquid-rust) here ([language docs](https://shopify.github.io/liquid/). ### Reasons to use Liquid - You are already familiar with the syntax due to its use in Jekyll, the default static site generator for github pages - Customizability - Language plugins with access to slighter-higher-than-tokens stream (we might change this to the raw text later for even more power) - Ability to pick and choose which parts of the stdlib are available (generally useful for non-web applications) - Pick and choose your caching strategies. For templates, we don't store those on your behalf so you can cache as desired. For partial-templates, you can choose where they are loaded from and how they are cached. - Optimizing the variables passed to the template. You can put all of them into the liquid-native type or you can implement a `trait` on a `struct`. As of right now, the members of the `struct` need to be liquid-native types but I plan to loosen that restriction in the future. I already commented elsewhere on DJC's benchmarks and that the not-yet-included 0.18 should now be on-par with Tera. ### Downsides From [zola's static-site-generator comparison](https://github.com/getzola/zola) &gt; Cobalt gets ~ as, while based on Liquid, the Rust library doesn't implement all its features but there is no documentation on what is and isn't implemented. The errors are also cryptic. Liquid itself is not powerful enough to do some of things you can do in Jinja2, Go templates or Tera. RE "which parts of liquid are implemented", I've ported the ruby implementation's tests over and [noted all the incompatibilities found](https://github.com/cobalt-org/liquid-rust/issues?q=is%3Aissue+is%3Aopen+label%3Astd-compatibility). RE errors. I wish there were examples so I knew what era this of liquid this comment was from. We've been gradually improving the errors and I'd love to know in what ways people still find them lacking. RE Powerful enough. I assume they are referring to the following: People seem to like Tera's [inheritance](https://tera.netlify.com/docs/templates/#inheritance). This can be imitated in liquid by passing variables around for what files to `include` (once [#310](https://github.com/cobalt-org/liquid-rust/issues/310) is implemented). Someone could make language plugins that can act more like inheritance. Tera's [macros](https://tera.netlify.com/docs/templates/#macros) also get touted. Again, this could be implemented as a language plugin. Seems useful enough that [I created an issue](https://github.com/cobalt-org/liquid-rust/issues/322). Huh, [filter blocks](https://tera.netlify.com/docs/templates/#filters) look neat. [I might want to look into exposing that.](https://github.com/cobalt-org/liquid-rust/issues/321)
I checked the implementation of `Iterator` for `RangeInclusive`; I expected it to be more complicated, owing to the boundary condition, but was still surprised. [It's right here](https://doc.rust-lang.org/src/core/iter/range.rs.html#336-409): #[inline] fn next(&amp;mut self) -&gt; Option&lt;A&gt; { self.compute_is_empty(); if self.is_empty.unwrap_or_default() { return None; } let is_iterating = self.start &lt; self.end; self.is_empty = Some(!is_iterating); Some(if is_iterating { let n = self.start.add_one(); mem::replace(&amp;mut self.start, n) } else { self.start.clone() }) } #[inline] pub(crate) fn compute_is_empty(&amp;mut self) { if self.is_empty.is_none() { self.is_empty = Some(!(self.start &lt;= self.end)); } } Hum, yeah, that's a tad more complicated than for the exclusive version indeed! For comparison, the implementation for [`Range`](https://doc.rust-lang.org/src/core/iter/range.rs.html#217-273): #[inline] fn next(&amp;mut self) -&gt; Option&lt;A&gt; { if self.start &lt; self.end { // We check for overflow here, even though it can't actually // happen. Adding this check does however help llvm vectorize loops // for some ranges that don't get vectorized otherwise, // and this won't actually result in an extra check in an optimized build. if let Some(mut n) = self.start.add_usize(1) { mem::swap(&amp;mut n, &amp;mut self.start); Some(n) } else { None } } else { None } } Okay, let's "optimize" both manually. First the `RangeInclusive` version: #[inline] fn next(&amp;mut self) -&gt; Option&lt;A&gt; { if self.is_empty.is_none() &amp;&amp; !(self.start &lt;= self.end) { return None; } let is_iterating = self.start &lt; self.end; self.is_empty = Some(!is_iterating); Some(if is_iterating { let n = self.start.add_one(); mem::replace(&amp;mut self.start, n) } else { self.start.clone() }) } Then the `Range` version: #[inline] fn next(&amp;mut self) -&gt; Option&lt;A&gt; { if self.start &lt; self.end { let n = self.start.add_one(); Some(mem::replace(&amp;mut self.start, n)) } else { None } } There's one more branch for the inclusive case, it should NOT matter in a loop: once you start iterating, `is_empty` is always `Some` and you are back to `self.start &lt; self.end` only. But apparently it foils LLVM :(
I find it confusing that From and Into aren't implemented for interop between strings and numerics. For example: let x = String::from(34); let y: String = 34.into(); let z = i32::from("34"); Why is this?
[Seahash](https://docs.rs/seahash/3.0.5/seahash/) claims to be pretty fast as well, I would be interested to see how it does in this case
Or perhaps, if I need to allocate stuff on a heap, then libc::malloc would do? 
A simpler form is `{x}`, owing to the fact that `x` is `Copy`. I am not sure that LLVM expects a modification of x/z here, wouldn't rustc tell it that it's a read-only pointer?
So I'd basically call this a "micro" benchmark, in that you've split out a _part_ of your work load and determined how to implement that particular _part_ in the fastest possible way. However, ideally, you'd have a higher level benchmark that timed the actual thing you're trying to do. Does the choice of hashmap make a difference in that benchmark? It might not be the case that any of these hashmap choices is a bottleneck in your full work load, in which case, you can use other dimensions to make your choice (i.e., "I'd prefer not to add another dependency, and since it doesn't give me a noticeable improvement in the overall work load, I can just use whatever std gives me.")
Yeah I got a performance increase with something else when I wrote to a buffer and then printed that out later with rust
The direction string -&gt; numeric can't be `From` since it can fail. The upcoming `TryFrom` would be a possibility, but `FromStr` already exists. The other direction could be done. Instead, there is a separate `ToString` trait and `to_string()` method. I assume this was a conscious decision at some point in the design process, but can't point you to it.
I submitted a pull request just to use num\_cpus to detect an appropriate number of threads, since i've got 8 available in my desktop. Works well tho, nice work.
`From` and `Into` are meant for lossless conversions that never fail - they should never panic (but unsafe code cannot rely on this). So `String` cannot implement `Into&lt;i32&gt;`, because there's no sensible result for `i32::from("not-a-number")`. Instead there's a `FromStr` trait, which allows errors during conversion - you can do `"34".parse::&lt;i32&gt;()` to get `Result&lt;i32, ParseIntError&gt;`. However, `String` could implement `From&lt;i32&gt;` - you can always successfully convert a number to a string. But then there's a question of where should you stop with `Into&lt;String&gt;` implementations. Should everything that is `ToString` also implement `Into&lt;String&gt;`? Maybe everything that implements `Display`? So the answer to these was decided to be simply "no", and you can use `From` and `Into` only to convert between `String`, `Cow&lt;str&gt;`, `Box&lt;str&gt;`, `&amp;str`, and similar things. For other cases theres `.to_string()`, which makes the intention a lot more obvious.
You can submit a PR to that repository to upgrade, if you want. The owner is pretty responsive.
If you're using libc, why not use libstd?
Hey! Yeah, based on my (rough, preliminary) testing of my whole project, `HashMap` choice is not really a bottleneck. `HashMap`s are not used extensively or repetitively enough for it to make much difference. In fact, the download and build times of `FxHashMap` are likely more than saved time for using it, even over 100 iterations. I'll use std HashMap. Thanks for the advice!
Hadn't even considered that. 
Your time calculations are wrong: eprintln!("Loadging file: {} micros", load_time.as_secs() * 1000 + load_time.subsec_nanos() as u64 / 1_000); eprintln!("Doing it: {} micros", do_time.as_secs() * 1000 + do_time.subsec_nanos() as u64 / 1_000); Seconds times 1000 gives you milliseconds, not microseconds, while nanos divided by 1000 does give you microseconds. And the total time is slightly over 1 second for the first case, so you have a seconds value that is being reported as milliseconds, plus the nanoseconds being reported correctly. In the second case, the time is under 1 second, so all of the value is in the subsec_nanos() and you're getting the correct number of microseconds.
So what you're looking for is generally designed such that: You can set the number of "ticks" per second, and it ticks forward one unit of engine processing time per 1000 millisecond/n ticks. If you change the numerator in that equation to a higher value, then the ticker will tick forward slower in real time. In essence, you want to constrain each frame of engine execution to be at *least* that amount of time - things get wonky if you don't constrain the minimum execution time for each frame in your game. So if a frame takes less than your m milliseconds, then it waits for the remaining amount of time left. For example, you'll get objects clipping through walls in physics systems if you don't do this. 
Props on your patient and detailed explanation here. Always good to see this stuff. :) 
Overflow handling is extremely important for correctness. Doing as bad a job as other languages would be a failure. 
This is also why we ship releases every six weeks instead of basing them on features, too.
"hostile to opposing viewpoints" *narrows eyes* I smell... well. Let's be polite. What exactly do you want from the community? I want you to consider what the motivations are for this subreddit and the people who run it - largely professionals, with daily lives, often as programmers or developers of some kind. In essence, this subreddit and the people running it, have the intention of *being productive*. Communities are formed with specific norms and intentions. Or do you really expect communities to be uniform in what they accept and intend to do? That sounds like a demand for groupthink to me. Right? 
It still would not help, though. That is, releases of GCC also have "partial" implementation of a given C++ standard; users simply learn which release of the compiler contains the final version of a given feature and then detect this release version. I've even seen code having multiple switches, to accommodate multiple versions of a feature as the implementation evolved in time before it was settled. It makes for gnarly code :(
Well, the main reason I ask is because I'm writing a smart pointer crate (specifically for raw pointers and FFI). So it can be constructed with a simple `*mut T`, but also wanted to have a way to move something to the heap, or at least create a long-lived pointer to a `T` value passed in and owned by the smart pointer container. Ideas, warnings, suggestions? 
I’m trying to advocate for time based releases, not suggest that we should introduce edition-level changes more often.
Oh, I'm getting "part" of the time. I'm only looking ate the nanosecond part of the time. Makes sense my bad
IIRC, there was a soundness related bug with telling LLVM that a pointer is non-aliased, which rustc worked around by not giving LLVM any aliasing hints. Not sure if that's still the case, but seems to me like it could possibly prohibit LLVM from asuming the value cannot be changed.
I don't know exactly what you're trying to do, but it's not unheard of to provide a crate that has a `std` feature enabled by default. For `no_std` users, they can disable that feature, but still retain access to the stuff in your crate that doesn't need the standard library. [byteorder](https://docs.rs/byteorder) is an example of doing this. Everyone gets the `ByteOrder` trait, but you only get the `ReadBytesExt` and `WriteBytesExt` extension methods if you have the [`std` feature enabled](https://github.com/BurntSushi/byteorder/blob/803136cc2719cc4a5baf562877e01938267a6bd9/Cargo.toml#L24-L25).
This would be for a write pointer, read-only pointers are possibly aliased, however it doesn't matter since they are not written to.
I also seem to remember that `stdout` is line-buffered by default, meaning that a flush (syscall) is emitted every time a `\n` is passed in stream. When printing very short lines, this will result in a LOT of flushes.
&gt; I dont like anonymous naming like this. That's ironic because something like `foo(1, "2", true)` is anonymous naming compared to `foo(bar = 1, baz = "2", quux = true)`.
I really dislike the duration API. Every time I use it I end up copy &amp; pasting from the example in the documentation to avoid exactly this issue. Hopefully as_millis() will be stable soon.
I don't necessarily agree that `.to_string()` is more obvious. It just begs the question of why not use `From`? Is it because `From` came after `to_string()`? We could trivially have `From` as: impl&lt;T&gt; From&lt;T&gt; for String where T: ToString { fn from(i: T) -&gt; String { i.to_string() } } and deprecate `to_string()`. Although this implementation is relying on `ToString` so a bit of chicken and egg going on :)
Stabilization has [landed](https://github.com/rust-lang/rust/pull/57124)! Just a few more weeks and it'll be out in 1.33.
I'm not a big fan of macros. https://docs.rs/stpl/ 
Yes, but it should be possible to do so without pretty much any overhead. I'm guessing what happens now is that rust checks on every iteration, if the thing being iterated over hasn't overflown yet, while you could just check one time, before the for loop, whether or not the max is overflowing.
I have a theory as to why `rustc` compile times are so slow for this tiny example. At least on linux, rustc has a pretty long startup time, most of which is spend while loading LLVM as a dynamic library. To measure this overhead, compare `time rustc --version` (prinit minimal info to stderr) and `time rustc --version -v` (print LLVM's version as well, which loads it). The second one takes about 40ms on my machine. That is, compilation of hello-world is mostly bottlenecked on [this code](https://github.com/rust-lang/rust/blob/6efaef61899f6551914156f69809aa7d5854399d/src/librustc_driver/lib.rs#L202-L213).
Is there an issue on github for this? And also, I understand that a generic next implementation needs to check for oveflow every time, but when compiling a for loop, couldn't a check for overflow on the max of the for loop just be inserted before the for loop? Thus only having to check for overflow once?
I was trying to find out weather this subreddit is poisioned by a particular kind of group think disguising itself as politeness. I wanted to find the norms and intentions of this subreddit and if they conform to my minimum viable values. I welcome dialogue as it is how we can find common ground. But on reddit dialogue is not often sought after or appreciated. The up/down vote mechanic is designed for groupthink and filter bubbles after all. There seem to be a vocal group (likely a minority) in the Rust community laying pressure on projects to change terminology for a supposed offensiveness. I think that is a bad road to go down as this seems to lead to a kind of mob tyranny. Call it childishness, anti corporate attitude or daddy issues but I like things to be a bit abrasive and "unsafe". To often do we see freedoms limited for a supposed safety. The typical "think of the children" arguments. Since few people would thinkg of advocating against that (because who wants to see children hurt?), this is a one way street. Ever increasing censorship and ever more corporate speech. If a single word offends you to no end then good. Take it as a moment of self reflection. Learn from it and gain some emotional maturity on the way. Don't prevent others from gaining the same insight though. Should we be strive to be more polite and cordial? Certainly. But what we consider polite can be wildly different and setteling on the lowest common denominator is not the way to go imo. Take Linus Torvalds and his recent move to be more "polite". I found it to be very much worse than anything else I had seen him write before. Because it seemed like your boss chewing you out in front of the whole staff instead of your friend mocking you amongst friends. Imho "bad" words per number of words is a bad metric to judge politeness. Since to me being honest and friendly are big parts of being polite. Dishonesty and fake courteousness are very impolite to me personally. As I take them to be huge character flaws. Of course as the sayings go: "If you dish it out you have to be able to take it" but also "was sich liebt, das neckt sich" (roughly: who loves each other teases the other)
I replaced `println!` by unsafe call to `printf` from libc (https://github.com/boxdot/pythagoras/blob/master/simple.rs#L15) and now the performance is exactly the same as in C++ and D. However, we should not forget that `printf` is not type-safe at all, but at least now we compare all language in the same way.
Pretty much everyone does this when they see the nanos part. Same happens in java too. 
Soon, the standard hash map will be hashbrown, so that makes it even easier.
To add to this; you can debug print duration to human readable output.
Ok. I get that I'm thinking of an overengineered solution to a problem of overengineering but... It seems like it would be nice if you have a compile time string it would be nice to verify it at compile time and not have to \`parse().unwrap()\`. If you are handed in a string at runtime it seems much more reasonable to verify it before sticking it in the header.
I picked up the pattern `pub struct DryRun(pub bool);` which makes it much simpler in my opinion to know how to flip it.
What's inaccurate about it? I haven't run into anything yet, but knowing what is there would be helpful! (And maybe I should write some of those docs...)
I am not sure with regard to Github; and I would also expect the optimizer to hoist the check, whether for overflow or `is_empty.is_none()`.
Overflow checking can be made very inexpensive, in many situations. I've worked with toolchains that made the cost of overflow checking (on every single operation) basically so small that it was literally unmeasurable. I just don't want people getting the impression that "oh well, overflow checking is hard, so let's just not do it". 
Er, "polite" isn't actually what Torvalds said. He talks about professionalism, not politeness. Let's get to the root here, about what professionalism is, and why it exists in the forms it does. Let's consider some scenarios, and these will be specifically development scenarios given this subreddit. A) One member of a team makes an offhand comment, coming down on people who prefer VIM over emacs. (As a random example). That team member, Bob, later is reviewing another team member's code, who heard that comment, Alice. When Bob makes a review comment that Alice disagrees with, she's going to have in mind the comment made by Bob. Say she feels hurt by it. Telling her to have a thicker skin doesn't help because up to this point, she valued and trusted Bob's insights and observations. We are going to be hurt when someone attacks a part of our identity, especially when that person is respected and liked by us. Is she going to give an honest response to Bob, that enables better code, because of the comment Bob made earlier? B) Consider instead that Alice needs to ask Bob some questions after he criticized VIM users, and that would require her showing him the code at her desktop. Is she going to want to do so after he made that comment? Is she really going to want to be judged? C) Bob is Alice's manager, and he makes that judgemental comment about VIM users. Is Alice going to feel able to bring up observations and critiques to Bob? In essence, professionalism is about enabling people of varying backgrounds, preferences, and concerns, to work productively together with trust and respect. You can't tell people not to feel their feelings. The overall goal is about being productive and getting stuff done. So what you call "censorship", and the way you want dissenting opinions to be aired willy-nilly are all about what professionalism is. The problem is that "opposing viewpoints" has become a code for allowing anti-semitism, bullying, stalking, harassment, and more to suffuse a community. People can and do criticize aspects of Rust here and elsewhere. They do it a lot. That's not disallowed. It helps make Rust better. What isn't allowed is calling someone a bunch of slurs that would get me fired at my day job because you disagree on a minor point. Professionalism is what this subreddit aspires to. Unprofessional behavior creates distrust and disrespect, and makes it harder for people to work together. That is the opposite of what people who like Rust want, by and large. They, and I agree, want a forum where people can talk about these things but without it becoming a bunch of harm to people who don't need it. There are real people on the other side, who have a full internal life. Always remember that. Be compassionate. Please. 
If the rust people believe that writing "cotnent" instead of "content" will lead to a "hard debugging sessions", then i don't want to see their solution to real hard problems and not trivial ones.
I kept seeing mentions of 'placement new' referenced in discussions around different Rust discussion boards. After searching around quite a bit, this is my attempt to distill things down for others wondering what placement new is, and how it works / fits. I'm still newer to Rust, so any feedback would be appreciated!
If your String is a number, why don't you parse it? 15 digits fit perfectly in a u64/i64. With i128 you have up to 38 digits.
Say I have define a method (e.g. a function inside an impl block). Is there anyway to define functions on that function? ```rust struct Test {} impl Test { fn say_hello() { println!("Hello"); } } // This block doesn't work, but I really want it to, for macro fun purposes impl Test::say_hello { fn nested_fn() { println!("Hello from nested fn"); } } ```
Placement new is something that does no longer exist/was unimplemented. It was intended to be a guaranteed way to construct objects in-place, but the implementation ended up being unable to live up to the requirements and thus was removed.
Thanks! That makes sense. It wasn't clear to me whether it was just "removed for now", pending further discussion, or if there are deeper issues that need to be resolved first.
Oh man it's been a long time hasn't it
&gt;This is Rust’s answer to C++ custom allocators in the STL That's [custom allocator support](https://github.com/rust-lang/rust/issues/42774) ([rfc](https://github.com/rust-lang/rfcs/blob/master/text/1183-swap-out-jemalloc.md)). Placement new is (afaict) the same thing as [placement `new`](https://en.cppreference.com/w/cpp/language/new) (scroll down) in C++. Custom allocators don't change how _you_ manage memory, just how the memory management works under the hood (which is necessary for using collections on embedded targets without a heap, writing lock/wait free code, etc). The same rules apply - memory isn't on the stack and persists until its freed, and you don't care where the memory actually is. Placement new is different in that it functions _like_ heap allocation, but you explicitly control _where_ the object goes. It doesn't necessarily go on the heap or stack, you can for example create a fixed size block of memory in your program's text section and allocate objects there with placement new. Here's a table to break it down a bit: | Do I control... | Stack | Default Allocator | Custom Allocator | Placement New | |------------------|--------|--------------------|--------------------|-------------------| | _When_ it's freed | No | Yes |Yes | Yes | _How_ it's allocated / freed | No | No | Yes | Yes | | _Where_ it's allocated | No | No | No | Yes | 
You definitely could add such impl, and it won't break the language and probably won't even break the things guaranteed by the docs (well, probably - I'm not completely sure if there won't be any problems with coherence). I suppose that the obviousness of code is a personal thing. For me `.to_string()` is better when I'm converting to string because it immediately shows that the type being converted to is indeed a `String` - if I see just `.into()` I need to search around for type annotations. And if such conversions as `i32` to `String` were expected from `.into()`, then I wouldn't even have a clue what that `i32` is converted to - maybe to `String`, maybe `Ipv4Addr`, maybe to `[u8; 4]` (because why not - its lossless), maybe something else. I wouldn't like having such conversions because they would simply mean "please coerce the value into some possibly unrelated type". Currently conversions are between pretty similar types, or just to autowrap stuff (`From&lt;Vec&lt;T&gt;&gt; for Box&lt;[T]&gt;`, `From&lt;&amp;str&gt; for String&gt;`, `From&lt;i32&gt; for i64`, `From&lt;T&gt; for Box&lt;T&gt;`, and similar) - for me its a lot easier to read code when I know that I can expect just these things. So that's why I say that `From&lt;T&gt; for String where T: ToString` is a unnecessary impl, and I would be against adding it to std.
Can you elaborate on what would be the use of this? And how is that `nested_fn` is supposed to be called? Like `test.say_hello.nested_fn()`? Or simply accessible inside `Test::say_hello`?
I considered it, but println is only called 1000 times, which takes 10ms on my system (I simply commented out the if condition). Compared to the 500ms difference between the versions println adds practically nothing to the runtime penalty.
That's because printf takes its arguments by value, not by reference.
A few weeks? More like a few months, with a 6 week release train, 4 weeks in a month, and currently being on 1.31, it's ~3 months away
It's supposed to be called like `test.say_hello::nested_fn()`. I'm the author of the rubber duck crate, which adds named arg function call syntax. At this point, it works only for unbound functions, and not methods in impl blocks. I'd like to be able to use the same dumb macro, which calls function::builder() to create a builder object for the args of the function. E.g. `named!(foo(a,b,c="hello"))` gets turned into something like `foo(foo::builder().next(a).next(b).c("hello").build())` And that works great for free functions. I'm trying to replicate that sort of pattern for methods from impl blocks. I'm thinking if that's not possible (which it's not looking very possible), I should be able to define a public but doc hidden builder method in the same impl block. Something like turning named!(foo.bar(a,b,c="hello"))` into `foo.bar(foo::bar_builder().next(a).next(b).c("hello").build())`. It'll make the macro less fun to write, but should still be just as ergonomic from the end user standpoint. 
Hi, I'm new to wasm and I'm wondering why one seems to need webpack and npm to build and run rust/wasm code (that's what the [tutorials](https://rustwasm.github.io/book/game-of-life/hello-world.html) say). Once the wasm code has been generated with `cargo build`, why can't we just open it like `firefox target/debug/some_file`, just like one can do after having run `cargo doc`? Is there some documentation that explains this, which I have not found?
Yup, that's what I remember. The big difference between constness in C and C++ versus rust is that it is a rule, a promise enforced by the compiler (or by the human in unsafe code), while in C++ they are more of what you'd call guidelines. Since the data under a const pointer could change - just not through the pointer itself - it was and is possible to pass a mutable and a const pointer in C where both are pointing to the same location. Which means any hint about read-only pointers is rubbish. For example: char first(const char * const_ptr, char * mut_ptr) { *mut_ptr = 'P'; return *const_ptr; } char hello[] = "Hello world!"; printf("%c", first(hello, hello)); is perfectly valid and prints "P". That `const_ptr` is a constant pointer doesn't mean it can't be changed. And the code has to keep working under every possible implementation of println (since LLVM doesn't know the implementation) including the possibility that a global pointer points to one of the arguments, and so on, so even if the function only takes const pointers stuff could suddenly change, and it can't reliably make sure that its arguments are unaliased. So even though read-only hints can be useful for optimisation they are so worthless and dangerous in practice that they can't be used. In C that is. In Rust they're perfectly valid and usable. However, them being unusable in C means obviously that no one uses them, and therefore they're untested and not ready to see the light of day. Rust has to revert to the next best choice, which are raw pointers.
True. Honestly I'm not sure what Rust does exactly in that context but it involves writes to arbitrary memory locations, double moves, and a `movl %bl, %sil` instruction that I never have seen before, part of a sequence of three moves, two of which hitting memory, only to fetch a value that is always 1. So in this case rustc is quite disappointing. 
Ah, that makes a lot of sense. Your table is really helpful, thank you! Will definitely update the post with clarification.
[The replies from this SO thread](https://stackoverflow.com/questions/222557/what-uses-are-there-for-placement-new) breaks down a lot of the uses of placement new from C++, if you want more information. Most of the time it comes down to allocating memory before you need it to avoid the overhead or locking/waiting of the allocator. 
I have a cargo workspace setup with 3 crates - a binary, a library, and a "builder". The last crate is used only by a build script for the library crate; it generates a bunch of constants and also uses the `phf` crate for creating perfect-hash lookup maps. I have two questions 1. The builder crate spits out a few rust code files for the crate that builds it, however the code does not conform to `cargo fmt`, which means whenever I format code those files always change (which I don't want). Is there some way to either exclude a directory from formatting (I see an argument to specify projects within a workspace but that's it), or somehow run format on the generated code during the build process? 2. The constants are all `u32` and the lookup maps created use that for the key (to look up constant to name). When retrieving values the map requires a reference to a key. In my specific case would it be better to not have to use keys by reference and instead just copy since they're `u32`? Right now this is just musing on my part about optimization and at the moment I'm not yet to the point of testing heavy workloads - I mostly want to get an idea of what options there will be if I have to do something in the future.
Rust 1.33 is due to be released on Feb 28. You can look that up for coming releases at any time on the [Rust forge](https://forge.rust-lang.org/).
Well, `test.say_hello::nested_fn()` is not even valid syntax. And I don't think that you have any other suitable place to put builder function aside from having it next to the real one (just like you suggested as your alternative).
The standard hash map is supposed to help against DOS attacks by default, yeah? Does hashbrown/swisstable? So far I haven't been able to find any material on that.
/r/PlayRust 
Would you mind providing a short ELI5 for those unfamiliar with codegen-units?
It wasn't clear to me at first but it looks like \`cargo fmt\` runs a different command \`rustfmt\` which has more options and configurations. I can probably address my first question with some \`.rustfmt.toml\` files and possibly by invoking this command during build.
Something like lit-html?
[removed]
+1 for Askama. I rolled my own very basic version of it but use the full featured version whenever I'm not just goofing around!
&gt; This may be a pointless exercise, but if we're going to count all GHC extensions we also need to count all &gt; #![feature(..)] &gt; gates. I think this is the nub of it. GHC extensions and rustc feature gates *seem* analogous, but I don't think they really have the same meaning. GHC doesn't have separate nightly/beta/stable channels and a stability mechanism; Rust doesn't have an official normative vendor-neutral language standard. &gt; Yes I was a Haskeller before being a Rustacean and I am still a Haskeller. Same here. :) &gt; I see zero reason not to stabilize GADTSyntax, TupleSections, LambdaCase, or similar syntax-only features. Good point. &gt; It seems to me that the only reason they haven't been stabilized is conservatism. (It's lucky I specified "though not the main one"!) I think the *main* reason very little is getting standardized is the absence of a problem for standardization to *solve*, which is typically multiple competing and subtly incompatible implementations. As long as GHC remains a de facto standard I don't expect that to change. &gt; that makes GHC Haskell an overall easier language You seem to have written this sentence/paragraph from a user's perspective; is that correct? I do agree Haskell (whether Glasgow or vanilla) is easier to use. &gt; does not allow for `μ` What do you mean by this? (That least and greatest fixed points in Haskell coincide...?)
Published as v0.2.1. Check out the new [window_events example](https://github.com/David-OConnor/seed/blob/master/examples/window_events/src/lib.rs).
So ~2 months, since 1.31 was released around the start of december. 8 weeks is a lot more than "a few" anyway, though.
Perhaps I'm misunderstanding your problem, but is this problem not suitably solved with an enum that implements a series of wrapper functions?
Is there any reason not to create the scope in a `Rc` regardless? There's a small overhead involved, certainly, however if you are only infrequently creating them it'll be a blip on the performance radar and will vastly simplify your task.
The hashing function is different than the table implementation itself, that is, what you do once you have the hash.
I think it may be useful to distinguish between (1) memory allocation and (2) object construction. For example, a memory allocator is only concerned about (1) memory allocation^1 and does not concern itself with object construction. In C++, a `new` expression performs *both* memory allocation and object construction, although the implementation of `operator new` only performs the memory allocation and the language automatically constructs the object at the resulting location. Do you know whether Rust intends to follow the same track? I would hope that it could achieve a more decoupled design: allocate memory in some way, then use placement to tell the compiler to build an object *right there*. ^1 *The presence of `construct` and `destroy` in C++ allocators is widely regarded as a mistake; the motivation was supporting [Fancy Pointers](https://en.cppreference.com/w/cpp/named_req/Allocator#Fancy_pointers), however the resulting design is non-orthogonal: all allocators with the same pointer type define the same `construct` and `destroy` method. As a result, those methods are now optional, and `std::allocator::construct` was deprecated in C++17 and removed in C++20.*
&gt; It doesn't necessarily go on the heap or stack, you can for example create a fixed size block of memory in your program's text section and allocate objects there with placement new. Another application is in embedded, where you have memory-mapped registers at addresses given in the CPU's documentation. If you can allocate objects over the registers, you can access them like regular structs.
I haven't listened yet, but I just wanted to say I'm a fan of the show and I'm happy to see another episode released. Thanks for your work on this. 
There is no mechanism to provide LLVM with the necessary information in this case. We can tell that LLVM that a pointer passed to a function is readonly, but that's not how formatting is implemented. Instead the pointer is stored inside a structure, and there is no way to annotate such stores.
&gt; I think this is the nub of it. GHC extensions and rustc feature gates seem analogous, but I don't think they really have the same meaning. GHC doesn't have separate nightly/beta/stable channels and a stability mechanism; Rust doesn't have an official normative vendor-neutral language standard. Your point about channels is entirely true. However, but I think one thing is the same: having to opt into gates is a) annoying, b) doesn't give a stable impression. It feels sort like a sort of gadgety and plug-in-ish.. &gt; Same here. :) :tada: They are both great languages! =P &gt; (It's lucky I specified "though not the main one"!) I think the main reason very little is getting standardized is the absence of a problem for standardization to solve, which is typically multiple competing and subtly incompatible implementations. As long as GHC remains a de facto standard I don't expect that to change; standardizing things is just a nice-to-have with a high cost in effort. I'm interested in 2 effects: 1) not having to start every file I write with a tower of `{-# LANGUAGE ... #-}` like in this case: https://twitter.com/Serious_Brendan/status/973063642915651589, 2) not giving Haskell the bad reputation the aforementioned image does. As for high cost, it's unfortunate that the cost is so high... GHC, being de facto standard, could learn a thing or two from Rust wrt. process. &gt; You seem to have written this sentence/paragraph from a user's perspective; is that correct? I do agree Haskell (whether Glasgow or vanilla) is easier to use. Yes, primarily. Not having control over things is both limiting and liberating; Laziness for example being great for separating consumers/producers, but bad for predictable performance, ... &gt; What do you mean by this? (That least and greatest fixed points in Haskell coincide...?) This: ```haskell data Tree a = Rose a [Tree a] ``` I don't have to think about indirections and whatnot here; that's liberating, but isn't for free..
Thanks! \*Should\* be more consistent in 2019 (doing some things the rest of this week to help with that!).
So many layers ... yes, people are complicated, and we have to work together if we're going to be a community. But really it's mostly working really well. So let's keep going forwards, onwards and upwards, focussing on the big picture / shared goals, resolving stuff as it comes up as best we can, and not get caught up on things. A happy and productive New Year to all!
The closest thing to what you're looking for would be the features of the std::borrow module, including the Borrow&lt;&gt;, BorrowMut&lt;&gt; and ToOwned traits, and the Cow&lt;&gt; type.
Very true, but ime that kind of thing is a two stage process where you have some object like a fixed size byte array mapped to the registers using a .ld file then use placement new to construct on top of the array. But it's been awhile since I've had to do that 
I have yet to test this and it may be technically undefined behavior, but you should be able to use `std::mem::transmute` on a byte array of appropriate size and then write to it by hand for construction. I know you can do that with a union in C but it's still a sketchy practice with padding/allignment
r/playrust
It’s also off topic there. They want /r/playrustservers
Most computers have multiple CPUs, and most programs are pretty big, so it makes sense to break the program up into multiple parts, and have each CPU work on them indepenently. These parts are called "codegen units" because each is an individual *unit* of *generated code*. The problem is that the various parts of a program interact. Imagine one part of a program says: if should_i_do_the_thing() { some_expensive_thing(); } ...and another part of the program says: fn should_i_do_the_thing() -&gt; bool { false } If both parts happen to wind up in the same codegen unit, the compiler can see that `should_i_do_the_thing()` never returns true, and therefore `some_expensive_thing()` is never called, and therefore can skip that whole chunk of code. If those parts happen to wind up in different codegen units, the compiler won't know what `should_i_do_the_thing()` looks like and will have to compile it as an ordinary function call just in case it turns out to do something complex. In a very large program, these codegen unit fault-lines only cut through a few interactions, and *most* of the program will be unaffected, so the compile speed improvement more than makes up for the runtime slowness. In very small programs, like most benchmarks, multiple codegen units don't make compilation much faster (small programs already compile quickly), and if a fault-line falls in the wrong place it can make the resulting program much slower for no obvious reason.
That's interesting, thanks!
It also only takes a tiny amount of logic to overshadow the performance advantage of async IO over threads. Context switching is incredibly cheap and just got cheaper on recent kernels [https://eli.thegreenplace.net/2018/measuring-context-switching-and-memory-overheads-for-linux-threads/](https://eli.thegreenplace.net/2018/measuring-context-switching-and-memory-overheads-for-linux-threads/) Async IO is really only useful for supporting really, really high concurrency (10k+ connections) or in memory constrained environments. Below that you're probably better off just running threads [https://www.slideshare.net/e456/tyma-paulmultithreaded1](https://www.slideshare.net/e456/tyma-paulmultithreaded1) One of the top results in the fortunes benchmark, jlhtttp, which I'm fairly sure is a threaded Java server.
/r/playrust
Oop! Sorry! Will delete and move to there!
Has anyone implemented a cache for Diesel? Trying to do an ORM-based REST API with Rocket and Diesel, but am running into issues with the cache — I'm using a static mutable HashMap. There's almost certainly a thread-safe alternative, maybe even a crate? (I couldn't find one)
Use `.ok()?‘ on a Result to get an Option (which always looks kindda funny since it looks your asking “is it ok?”). Use ‘.ok_or(MyError)?‘ to convert an Option to Result. 
This is great, thank you! If I want to use `ok_or` does that mean I'll need to define custom error types for every use? For example, trying to convert this `OsStr` to `&amp;str` ``` let ext = Path::new(&amp;input).extension().unwrap().to_str().unwrap(); let ext = input_path.extension().ok_or(MyError)?.to_str().ok_or(MyError2)?; ``` 
Try to use rustup to manage your rust version.
I've been using the Failure crate and it's `err_msg` and `format_err` which will create a `failure::Error` and use `Result&lt;_, failure::Error&gt;` as a return type for almost anything that can fail. It's way easier than being specific with all my error types. I'm sure some people would poo-poo about being lazy -- it depends on your use case though. If I were writing a library where these errors would propagate up and could be acted on I'd care, but they're just being used as boolean (fail/didn't fail) and a log message. https://docs.rs/failure/0.1.4/failure/fn.err_msg.html https://docs.rs/failure/0.1.4/failure/macro.format_err.html I may have missed the connection here: Then you can do something like `ok_or_else(|| err_msg("Path must have an extension"))?`. 
Cargo needs to know where to look for your dependency, so you need this in your manifest: mylib = {path="path/to/lib") It sounds to me like your path is incorrect in the build script if you don't want to use cargo. Are you using the build folder or /path/to/crate/target/release?
It's also a private network address. OP might want www.whatismyip.com
The main use that I've been considering is relative pointers like [Jon Blow demoed for his language](https://youtu.be/Z0tsNFZLxSU). The stabilization of `Pin` is perfect for this, but it still raises the issue of how to initialize it. You can't just call `RelRef::from()` because the location it points to is dependant on it's own pointer, so it needs to be allocated on the calee's stack before being initialized.
&gt; https://docs.rs/failure/0.1.4/failure/fn.err_msg.html This is exactly what I was looking for, thank you so much!
Sorry, didn’t phrase it right. My question is that is this already a common pattern to solve this? Because it feels like quite a general problem, where the struct is either not in an `Rc` or in it.
Good point. Guess I’ll have to build it first and observe the performance afterwards.
Slightly offtopic, but since when has .NET core been so fast? How is the CLR competing with compiled languages?
Yeah... Surprised as to how well .NET core has been doing lately. They have also introduced Span&lt;T&gt; and Memory&lt;T&gt; which should help .NET Core in a lot in performance critical areas. Come .NET 3.0, they are bringing in solid ML framework as well. 
Alignment matters in Rust, too, and there is no way to construct an object in a specified location, only to create one on the stack and then copy it there with `ptr::write` (to avoid automatically calling drop on undefined memory).
If it wasn't so deeply entrenched in the Microsoft ecosystem, I'd probably pick it up. Oh well, Rust is fun. Happy new year.
Ah, I see, you're saying there's a way to tell LLVM that a pointer will only ever be read, never written to. Had no idea. But if a read-only pointer could be aliased, couldn't the alias theoretically be a writable pointer?
/r/PlayRust 
You'll get better answers at r/playrust.
Okay thanks 
My String is not a number, no. 
Crates that might be useful: `resiter`, `insideout`
You've got a great attitude. I'm just dipping my toes, considering getting more involved, coming from Haskell. But just at that time, this meltdown happened with sexist posts like the above from a core contributor. Makes me feel very unwelcome. I also have the temptation to read into this, that certain kinds of hostility and prejudice are ok, and will happen again. In contrast, I've noticed that the non-US-based language communities are far more tolerant (Elixir, Haskell, etc.)
&gt;Strings in Rust are just byte collections (in Python they’re collections of Unicode code points). This sounds disingenuous. First, isn't everything "just" bytes? Then, are Python strings implemented much differently from Rust strings? I would be shocked if they have a different in-memory representation, but I'm struggling to find the part of CPython that would clarify this.
So if I put the path in like a dependency it builds the dependency crate. While that does work, I don't want to depend on a crate at all. I want to use a pre-built dynamic library in the form of a `&lt;lib&gt;.so`. I just can't figure out how to properly link to it, or how declare it as a dependency. The path I was using was the relative directory where the `.so` exists.
Aren't rust strings always valid utf-8? Also rust has both bytes and chars(code points) iterators as well as an easy downloaded crate for graphmemes
Tldr: A Rust program runs 10x faster but takes 4x longer to write than Python.
I'm on mobile, but I am almost positive that they do indeed have different in memory representations. The "let's represent strings as just utf8" is a somewhat recent phenomenon in core string types. AIUI, Python's in memory representation for strings can vary, but is generally a sequence of codepoints, i.e., utf32. I believe there are special case representations though! That part of the post is a bit terse in general. It's hard for me to suss out what exactly their point is here. I don't think they are getting disingenuous though.
Doesn't C++-esque "placement new" already exist in Rust? You can't implement vec without it.
My only experience with dynamic linking of rust libs has been through C/C++ which I know works fine using the `cdylib` instead of `dylib` which is not the same thing. Does that change anything? 
Lifetimes and ownership are often the primary consideration when designing a program. Rust does lifetime elision, so most functions and types that you write likely won't need to have explicit lifetime annotations, but they're really still there. You can clone things any time you want to or need to, but some programs will find that the extra time spent copying data is detrimental. It all depends on what you're doing. You'll probably want to pass references around, rather than cloning everything every time. Returning references is also a good idea most of the time; again, it all depends on what your program is doing and how you've designed it.
Dor a moment I thought this would be about compile times :(
I use ferns function logger pass in a closure that calls stdweb's console!(log, ) macro so I can use info!, debug! as usual.
For the reference thing, I usually abide by the C++ rule where if the size of the object is less than a pointer the argument should be by move/value but if it's larger it should be by reference 
That was an interesting read. Thanks for sharing!
&gt; I realize usually in rust everything is compiled statically, but in this situation I need to install a system wide rust library, so a shared library seems ideal. Rust doesn't have a stable ABI yet, so, if you intend to install it system-wide (rather than just sharing a single library among several projects that you always intend to build and distribute together), you need to build a `cdylib` which exposes a C-compatible interface. Otherwise, even the tiniest, most trivial upgrade to `rustc` might render newly compiled stuff incompatible with your existing `.so` file, or a new rebuild of the `.so` file incompatible with existing binaries that link against it. (And not necessarily in a "refuses to work at all" kind of way. Possibly in a "works just fine until you use a certain function, and then you get subtle memory corruption" kind of way.) That, and the fact that the C ABI can't represent all of Rust's language constructs, is why static is the default.
Just make Making webservers little high level and easy 
While I very much lament the current compiler speed, there are some cool things underway that should improve the compile speed.
It's 4x faster to put a package on crates.io than on pypi too
Yes, my understanding is that Seahash is the best of the lot when key size &gt;= 8. And almost matches FNV with lower values.
You can't completely avoid depending on the crate: the compiler still needs the definition of types and function prototypes of your dylib, as well as the code for generic functions (that cannot be instanciated at link time). It's the same for a C library : you need the header files to build a program against it.
THINK TO should help with this, no? Is it enables for O2?
&gt; Doesn't C++-esque "placement new" &amp; "placement destroy" already exist in Rust? You can't implement vec without it. No to both of these. Rust does not have a `new(ptr) T` equivalent, but you don't need it for `Vec` anyway (C++ doesn't use it either).
&gt; RFC Lol wtf there's no issue templates? Seems no effort has been put into the use of GitHub issues
I think there is a typo where you say "impl trait in return position"
As someone who frequently has to wrestle with the compiler, I like having a comprehensive guide/documentation for ownership system once everything related to ownership is stabilized. 
salads.rs
Hello I have one. &amp;#x200B; **pub fn** populate() -&gt; \[\[**u32**; 9\]; 9\] { **let mut** a = \[\[0u32; 9\]; 9\]; **for** i **in** 0..a.len() { a\[i\] = (1..9).collect() } a } &amp;#x200B; This gives the error: &amp;#x200B; error\[E0277\]: a collection of type \`\[u32; 9\]\` cannot be built from an iterator over elements of type \`{integer}\` \--&gt; src\\make\_grid.rs:8:23 | 8 | a\[i\] = (1..9).collect() | \^\^\^\^\^\^\^ a collection of type \`\[u32; 9\]\` cannot be built from \`std::iter::Iterator&lt;Item={integer}&gt;\` | = help: the trait \`std::iter::FromIterator&lt;{integer}&gt;\` is not implemented for \`\[u32; 9\]\` &amp;#x200B; Any help appreciated. &amp;#x200B; Thanks
You mean for the \`rust-lang/rfcs\` repo? The current way RFCs are done, issue templates don't really make sense. RFCs are introduced via a PR, and the only content of the initial comment for a PR is a link to the rendered version of the RFC content. There also is a [template](https://github.com/rust-lang/rfcs/blob/9d89e4f37e657ae123cbd35a9ee5ada991776644/0000-template.md) in the RFC repo to use.
ohhhhhhh
Why not just helping to fix libp2p? P2P is such a complex topic and getting it right requires a lot of effort. I know that is maybe is hard to get into the libp2p source code, but I think it will pay off. 
The ABI should only be an issue across different rustc versions. But it should be possible to compile a dynamic library and link it dynamically to another program using the exact same rustc. I tried to do that once and couldn't manage that (it's not that long ago but I don't remember the details). I think this use case should be allowed.
Good read! But I have to say the contrived analogies were pretty distracting.
&gt;about standards for the ownership system There are not really any standards. You want to pick the solution that makes the most sense from a technical standpoint. &amp;#x200B; &gt;when is it ok to use clone or to\_owned? It is always okay. But it comes at a cost. Both methods usually create a copy of data. You have to ask yourself: Do I need an independent copy, or can I just use borrows/moves to achieve the same thing? &amp;#x200B; &gt;Are lifetimes supposed to be avoided if possible, or are they common? They are everywhere. But they are usually implicit because the compiler can determine them automatically. Only when the compiler can't do that, you have to help it with manual lifetime annotations. &amp;#x200B; &gt;When overall should you prefer talking in references vs owned objects? It depends. You have to ask yourself: What does the function, that I am about to define, do with the object? &amp;#x200B; * Does it just take a look at the object? * Is it a small value like an int or a float --&gt; clone, to create a owned parameter * else --&gt; borrow * Does it change the object? * Should this change be visible on the caller's site? --&gt; mutable borrow * else: Move Made-up examples: &amp;#x200B; `// only takes a look at the Measurements object. Does not modify` `fn get_temperature( data: &amp;Measurements ) -&gt; i32` &amp;#x200B; `// returns nothing and changes the list from the caller` `fn append_to_list( list: &amp;mut List )` &amp;#x200B; `// Moves a Brain struct into an assembly function.` `//The brain should not be used by anyone else after it` `//has been put into the robot.` `fn assemble_robot( brain: Brain ) -&gt; Robot` &amp;#x200B; &amp;#x200B; &gt;When is it okay to return a reference? Only when the livetimes allow it. And this is a typical situation when the compiler needs explicit lifetime annotations. The reference must point to something that outlives the function call. The rust compiler will enforce this.
Why should we downvote this?
I’m working on a [small scripting language called Eko](https://github.com/ravernkoh/eko). It’s currently a work in progress (I just finished function declarations).
Maybe because it's against their pride
Python strings are *complicated*: https://rushter.com/blog/python-strings-and-memory/
Because it is not relevant to this thread.
https://stackoverflow.com/questions/26757355/how-do-i-collect-into-an-array tl;dr - use `Vec`
If your output must be as fast as possible, consider building longer strings and printing them in batches. For example you could write your own "buffered println struct" which would allow you to just format a string into it and then would automatically flush by writing using println!() once after, say, every 20 lines. I just started out learning rust so I really have no idea how things are done in here, but [std::string::String::push_str](https://doc.rust-lang.org/std/string/struct.String.html#method.push_str) seems to be the way to go.
Is the compile speed that bad? Rust is my first compiled language, so, I guess I'm not sure what to expect. Are there projects built in multiple languages so that Rust and other languages can be 'oranges to oranges' compared?
Let me reply to this question. Why I don't to fix libp2p，but open a new repo. First of all, I think a framework must be user-friendly, and I have to write thousands of lines of code to start the test. libp2p is not friendly. Second, you see my current implementation, in terms of multiplexing, it is completely different with libp2p, I don't like the implementation of mutex. &amp;#x200B; Third, the overuse of generics makes the function signature too complicated. I don't know if it should be a rust problem, or if libp2p put too many generic parameters in the implementation process. The generic parameters of libp2p once made me not want to use this library. &amp;#x200B; In the current implementation, I referenced a lot of libp2p implementations in secio(because I am afraid of writing a security question myself.), and modified the parts to fit my channel base architecture. &amp;#x200B; So far, I don't know if my decision is correct, but I found that my current implementation is better than libp2p in terms of control, simplicity, and difficulty of getting started. Although the current implementation does not fully follow libp2p, it is not complete. &amp;#x200B;
Do you really need a crate for `num &amp; 1`? Works only with integers but I'm not sure what you'd call a `real` number as `even`...
can be very bad highly recommend incremental compilation if you're using a newer rust version
You are right, it's a bold claim. I provided several videos which really helpful for me to understand how to code Rust in a short time. It does need your knowledge on other programming languages like C++ / Java. 
Fantastic article, Alan. I think you did a big-bang rewrite. I had a tiny Python program that launched Mozilla-SeaMonkey's editor when it got a command to do so via std-in (that's how 'Native' BrowerExtensions work these days). I was dissatisfied with Python being in the mix there, so I thought Rust would be the way forward. Now I'm a Java-head really who's OK with Python, JS &amp; Ruby. I didn't (and still don't) understand Rust. So I went to UpWork and paid a guy to port the thing to Rust. My constraint was "Python unit tests must pass (unchanged), while implementation moves from Python to Rust". Work completed freelancer paid and given 5 stars review. Imagine we were funded for a paid-programed effort to take Python 'August' to Rust using the existing Python unit tests as a safety net. *Then within that* focus on the **most depended on and least depending** section of code and move that to Rust first (invoking it from Python, and confirming tests still pass). Then ~150 of those incremental code migrations later it's all done - the whole solution is in Rust (even if the tests are still in Python). Bonus, Python bindings for a Rust library ship at the same time. Fro extra fun we could run and record benchmarks at every incremental step. If your expended effort was 40 hours for the big-bang port, what do you estimate the expended effort would be for the incremental approach as outlined above? Feel free to imagine your favorite ever pairing partner for this instead of me. I'm assuming you've done pair-programing before.
I would be surprised to see a "thread-per-connection" model working at scale in the wild, and therefore would be afraid to lean on framework based on such model because of the performance cliff, unless I knew beforehand that the number of concurrent connections would stay low (internal service, for example). It may be that the test harness used by TechEmpower doesn't cause such heavy concurrency, possibly as a limitation of the clients which I think simply spawn N threads with each thread sending a request and waiting for the answer (no pipelining). That would be a limitation of the test harness, which doesn't faithfully represent real world cases. Still, I'd rather presented solutions were geared toward more realistic work loads, and therefore I'd rather they used a scalable architecture regardless of whether TechEmpower actually exercises it or not.
How dare linter tell me that I misspelled something, I'm perfect! Also pretty funny you say that, because I don't think you'd like if it was confused what the fuck cotnent was and instead simply said `Error`. Every single compiler tells you if a variable you are trying to use isn't clearly defined/declared anywhere else. [Look, I found a language just right for you(https://github.com/ErisBlastar/cplusequality) Relevant excerpt: &gt;Any actual errors will simply result in 'error' being printed. It is not the program's job to educate you.
I've become interested in Rust interacting with WebAssembly. As a first toy project, I've decided to implement 2048 and maybe add some autoplay via machine learning. The [cli version](https://github.com/KappaDistributive/rs2048) works as of yesterday and has been confirmed to agree with the official version of 2048. However, there is no GUI as of now.
You know if a friend or workmate was a bit snappy with you one day, I hope you'd ask what's up with them and whether they are okay, rather than blow the whole thing up into a huge issue. That's what I mean by layers. Dig a little deeper. Something was bothering boats on 2-Dec. Maybe there were some trolls circling him looking for a weakness, trying to wind him up. Getting your victim to overreact is a classic tactic. Or maybe trying to stir up opposition and discord within a community, and driving a wedge into any gap that appears. It's easy to get into a siege mentality with trolls circling like this, but that is also playing into their worldview of fear. I guess the answer is to recognise the symptoms of a troll attack and pull the affected people out of that false reality. That seems like a supportive thing to do for our fellow community members. (I guess this may sound discriminatory against trolls, but trolls should also be welcome so long as they leave all their trolling at home -- permanently.)
Yeah because writing some code, making compiler happy, then not having to debug anything because it's perfect:tm: and working on something else is less productive or easy life than *okay boys, start gdb*.
Indeed, and this creation on stack is the issue plaguing Debug builds with Stack Overflows when using large-ish arrays. I would expect this issue to be the crux of the placement new RFC: I have memory, I want to construct a large object there, how do I initialize it *in-place*.
Thanks for your comments! If you have bad experience on debugging a C++ multi-thread program, you will probably understand the value of ownership / borrows instantly. 
That's very unfortunate :(
Even if it would only be called once, it's existence alone prevents optimizations in LLVM, it seems.
If you don't like using libraries, don't use them, it will just move complexity and dealing with it in your own code, solving nothing. For reference, learn C, you'll see what it feels like.
By Rust semantics, if you have a read-only pointer at hand, you are guaranteed that no writable pointer exist to the memory pointed to (unless the type contains an `UnsafeCell`); this is the whole point of borrow-checking. Unfortunately, part of the information is lost when translating to LLVM IR, apparently. Likely because the IR cannot represent the distinction.
I'm making a toy language that will never be publicly released and developing a [game](https://gitlab.com/pi_pi3/arewegamedevyet) live, on [twitch](https://twitch.tv/walterpi).
What's that? Is that a already existing crate? (Sorry I'm on mobile)
That's awesome! Bravo!
I've built a little CLI to scrap lyrics from websites. I'm trying to implement parallelism in it
Thanks. Code organization is a mess, though. I really need to get a better grasp on how to structure interacting components in Rust.
I found another interesting discussion: [https://www.reddit.com/r/rust/comments/27sbgr/oop\_in\_rust/](https://www.reddit.com/r/rust/comments/27sbgr/oop_in_rust/) and a chapter in rust book about OOP [https://doc.rust-lang.org/1.30.0/book/second-edition/ch17-00-oop.html](https://doc.rust-lang.org/1.30.0/book/second-edition/ch17-00-oop.html) &amp;#x200B; It's up to how you discover and model the problem you are solving. In some case, you may think you do need encapsulation to describe your thoughts, sometime it's an overhead. 
&gt; I rewrote a Python project in Rust. The rewrite took a fair bit longer than expected, but the results were good (about 9 times faster and ½ the memory usage). In the process, I learned a fair bit about Rust. Am I the only one surprised by the **9 times faster** bit? I feel like it's missing an order of magnitude in there; with Python generally being 100x slower than Rust, not 10x, in my experience: - Was the Rust program compiled in Release? (it's Debug by default) - Was the Python program leaning on libraries delegating the brunt of the work to C? I couldn't see anything especially bad in `main.rs`; I/O being performed in batch.
thanks for this, and yes, it should have been 1..10
This needs syntax sugar to avoid allocating a big object on the stack. Box::new([0; 0x100000]) could overflow the stack (the argument temporarily lives there), whereas HEAP &lt;- [0; 0x100000] just works. Same for deserializing huge objectd, etc
Working with fixed-size arrays is generally avoided for this and similar reasons. But a relatively straightforward solution would be to use \`copy\_from\_slice\`: [https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=99c8a78a0518df07ad0373b8c006235d](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=99c8a78a0518df07ad0373b8c006235d) (You'll note that I also iterated directly over the elements rather than by index, which is slightly nicer. :) ) Note that this makes a second copy of the data so it wouldn't be suitable for large arrays. Also, if all your data is the same in every row, you could copy them all from the first row too. &amp;#x200B;
What does it do that does add 180kB to the binary? There are whole operating systems that have less than that.
Related question: Is there some overhead to `ok_or()` constructing the error object, only to not actually use it?
That's a good rule of thumb, though I'd relax it slightly to include fat pointers, which are really common in Rust because of slices and trait objects. Another class of objects to consider including are those comprising a known and small number of primitive fields. For example, the struct behind `Vec` is known to consist of exactly three `usize`-wide values, and I'd consider it safe to pass it by value (relying on move semantics to transfer ownership of elements, of course). I suspect that at least some of the reluctance to pass objects larger than a single pointer by value in C++ comes from a time when the compilers were less capable of inlining and eliminating the unnecessary copying.
Thats for the largest test case that likely doesn't fit inner caches. The speed of working in L1 is orders of magnitude better than working in ram or even L3. If the workload forces you to wait on io a lot, you can run tens or hundreds of times more instructions and still be ~ the same speed. This is why genuine memory/time tradeoffs rarely exist anymore. Beyond trivial examples, using more memory usually costs more time.
Yes! Go and D both compile small to medium sized projects in "more or less instant" time.
Why do people profile writing to the console so much? Every time a program outputs kilobytes of console text, I just think that something's broken.
Html5ever is a browser-grade html parsing library, might save you some work.
You would have to do a full audit of the `log` crate to be sure. But from experience Rust's `format!`/`format_args!` macros bloat WASM binaries pretty badly. In general it's clear that binary size has been a low priority somewhere in the dev chain here.
Writing to standard output does not necessarily mean writing to the console. It's not uncommon in unix-like environments to pipe the output of one program to another. For example, you could [pipe the output of a C program to an audio player](https://youtu.be/tCRPUv8V22o?t=176). In that case, you probably want to be at least fast enough to keep up with the playback rate.
TL;DR it will convert between constant-length 1, 2 or 4 byte encoding, as needed for the most complex character in the string. It doesn't use UTF8 because using constant-length encoding means it can easily do random access on the string.
I'm trying to replace hot loops in JavaScript (node) projects with rust (using neon). I managed to get something working in the babel project, but it turns out that their flow type annotations are not actually enforced. This means that you have to defensively code everything, which is really unsatisfying. I might have a look at vscode or the typescript compiler and see if they're easier to reason about. I suspect that I will also be contributing impl Debug for a bunch of the neon core types before the end of the week. I'll see how it goes.
Yeah, but you wouldn’t use `println!` for that kind of output.
Yep, which is why we have [`ok_or_else`](https://doc.rust-lang.org/stable/std/option/enum.Option.html#method.ok_or_else). Clippy will actually suggest this by default, I believe.
That’s what I thought. Swift has a nice solution for this via their autoclosures (basically, there’s an option to function declarations to turn arguments into closures automatically to allow conditional evaluation).
&gt; This is why genuine memory/time tradeoffs rarely exist anymore. Beyond trivial examples, using more memory usually costs more time. Sure, but that should actually advantage Rust even more than Python. Due to its dynamic nature, the representation of Python objects is very flexible... at the cost of multiple indirections. That is, an "object" is actually a handle (indirection) to a table of attributes/methods (each indirectly reachable), so that when in Rust `foo.bar` is just reaching in the same cache line (mostly), in Python you have to dereference two pointers (and pull in two more cache lines).
Yep, Swift has some neat toys in its bin. :)
&gt; A second, and closely related reason was that the difference between writing Rust and Python was greater than I’d expected, and it took me longer to become comfortable with Rust than I expected
Continuing hacking on hdf5-rs. Tons of stuff left to do and no one to help but everyone needs it working asap, usual story ;)
Well, I was *going* to take a week or two to relax after releasing Criterion.rs 0.2.7. I really was. Maybe read a book or something, catch up on some anime... It was going to be great. Then I got to thinking about templating libraries and how none of the ones available really fits my needs for Criterion.rs. I got a bug in my brain to build my own, with a minimalistic feature set, few required dependencies and good documentation. So I spent six hours yesterday hammering out the beginnings of [TinyTemplate](https://github.com/bheisler/TinyTemplate). It's still very much a work in progress (most of the features are not implemented, and there's practically no documentation yet), but the nice thing about keeping it small is that it shouldn't take too long to get it release-worthy and then maybe I can go back to my break.
Thanks! Fixed.
&gt; Aren't rust strings always valid utf-8? Yes. Creating a non-utf-8 string is one of the language's UBs. 
&gt; It doesn't use UTF8 because using constant-length encoding means it can easily do random access on the string. Which doesn't help properly processing strings, but is convenient for improperly doing so and which they didn't want to drop for backwards compatibility reason (Python has always provided O(1) access to "characters" though depending on the type and build they've been bytes, UTF-16 code units or codepoints), and they likely didn't want Swift-style amortised O(1) access for implementation complexity reasons.
&gt; Often a struct might be defined in one place, but the methods for implementing specific traits with that struct are defined elsewhere. For example, if you want to read from stdin, you have to add use std::io::Read; at the top, even though you’re not actually using the Read name anywhere. I'm new to Rust, coming from Python, and i have great difficulty with this. Could anyone explain why this is, and how I should "think" to understand this sort of behaviour. Why doesn't one need to specify Read? 
Can the rust compiler optimize this without relying on llvm?
Not today; as of today rustc does not perform any optimization. In the future, it's possible that rustc may gain high-level optimizations, especially now that MIR would make it practical. In the mean time, it's probably best to tweak the code to make it possible for LLVM optimizations to kick in.
Ending a discussion that's on topic and does not violate any rule is about the worst thing a person can do as a forum moderator. Unfortunately, moderators here do this whenever they feel like it. I still remember when I confronted one moderator for deleting someone else's post (that was not in violation of any of the rules), he or she stated that they can remove any post for any reason they see fit. This is how rotten some moderators are, and whoever is in charge seems either incompetent or pretty content with the status quo. u/anarchorustacean, please notify me If you manage to find an alternative rust community better than this shitty one, or if you start a new one. I'll participate as long as the new community remains technical, on topic, and allows discussions as long as participants are willing to partake in them.
Being spoiled by rustup and cargo, I've decided to try a similar approach to managing Python installs. I've taken inspiration from pyenv, which does exactly that but is written in Bash and thus cannot be used (easily at least) on colleagues' Windows machines. It's not too complicated and quite fun to do. Might interest other colleagues into Rust too!
After some searching I can answer my own question. There is a [no_modules example](https://rustwasm.github.io/wasm-bindgen/reference/no-esm.html) that works without webpack. And while the page says "must be served over HTTP" it actually seems to works both with file:// and http:// with Firefox 64.
I'm making a wrapper for a pretty straightforward web API: GET a bunch of URLs, pass various headers for authentication and caching, recieve JSON objects/lists of known structure that can easily be thrown at serde. I would like this to be usable from sync and async code with as little duplication as possible (an `async fn` equivalent for every `fn` is right out). Since none of the work the lib does is actually about sending requests, just forming them and parsing responses, that seems like a reasonable idea. Any tips on how to go about this? I have half an idea for a MaybeFuture trait, implemented for Results and Futures, that just has a `map&lt;U, F: FnOnce(T) -&gt; U&gt;(self, f: F) -&gt; Self::MapOutput&lt;U&gt;`. Then, when the caller constructs a `Client&lt;T: MaybeFuture&lt;String&gt;&gt;`, they must provide a `Fn(Request) -&gt; T` that's responsible for sending the request and every method returns a `T::MapOutput&lt;some model struct&gt;` - if your HTTP lib returns a result, you get a result, and same for futures. However, I'm running into roadblocks here - MapOutput has to be generic (since different endpoints parse to different structs), and generic associated types aren't implemented yet. The next one I expect to hit is that MapOutput for futures will be an unspellable type, since it will contain a closure. Any ideas on how to proceed from here, or a different path to pursue, are welcome.
`Read` is a trait. For types which implement it, it offers additional methods and API in order to perform `Read`-y things on them. Using those methods requires the trait to be in scope. This is because a type might implement `TraitA` and `TraitB` which are otherwise unrelated, but provide a method of the same name. There is syntax for distinguishing when both are in scope, but that isn't necessary normally.
How am I supposed to use intra-doc links? I tried with just this in my `lib.rs`: /// This is Foo. pub struct Foo; /// Link to [Foo](Foo). /// Link to [Foo](crate::Foo). /// Link to [Foo](::Foo). /// Link to [Foo](self::Foo). pub struct Bar; And in the generated documentation for `Bar` all links come out broken - they always seems to come out exactly the same as written in the source, instead of being resolved to `Foo` struct. 
Indeed, because writing *maintainable* code is what takes time.
&gt; language's UBs Pardon my ignorance, UB? 
Yes, adding the formatting code adds a bunch of functions and additional bindings that take up a lot of space. I should also note that this was measured in a debug build. No compression or stripping of the binary was done. Building with parcel's defaults (instead of wasm-pack's defaults) results in a size diff of about 40k. I should also note that it is trivial to build with `console_log` enabled in debug builds and disable it in releases. All the log statements can stay in place with no size penalty.
&gt; I've worked with toolchains At a guess, not using the LLVM intrinsics, which are made for debugging and not speed?
UB = [Undefined Behaviour](https://en.wikipedia.org/wiki/Undefined_behavior)
Interesting. I suppose static-linking is an option, though if it's only to improve synthetic benchmarks it may not be worth it.
Did you mean ThinLTO ?
Copying from the [stdlib docs](https://doc.rust-lang.org/std/index.html), it looks like you use [square brackets around the type name](https://doc.rust-lang.org/src/std/lib.rs.html#13-18) and [define those links](https://doc.rust-lang.org/src/std/lib.rs.html#165-210) at the end. (Square brackets around a link instead of parentheses means the actual URL will be in a footnote. The actual text in the brackets is irrelevant as long as it matches the footnote.)
That's really cool! I've always wanted to create a scripting language but I just can't (I've tries more times than I'd like to admit). Do you have any tips for me?
Interesting approach. I wasn't aware of fern. I'll add a link to this in the readme in case users want more flexibility. Maybe I should export a `log()` function that can be passed directly to fern. 🤔
Html5ever is used by August
&gt;Is the compile speed that bad? Yes. It's crazy bad right now. Compiling and then running say, a 2d game library from nothing to showing a blank white screen takes something like 90 seconds on desktop hardware. It's crazy.
Makes sense, but it is a really weird thing to benchmark since it consists mostly of syscalls. Why not benchmark a real-world use case?
It’s my first too I came from scala though so the rust times seem somewhat reasonable to me. That being said like scala they can put work into it. 
So, I decided to investigate this range inclusive performance, and specifically whether an alternative representation could be beneficial. I rewrote `RangeInclusive` to have a simpler iterator: struct RangeInclusive&lt;Idx&gt; { start: Idx, end: Idx, } struct RangeInclusiveIterator&lt;Idx&gt; { current: Idx, end: Idx, started: bool, } impl &lt;Idx: Step&gt; IntoIterator for RangeInclusive&lt;Idx&gt; { type Item = Idx; type IntoIter = RangeInclusiveIterator&lt;Idx&gt;; fn into_iter(self) -&gt; Self::IntoIter { RangeInclusiveIterator { current: self.start, end: self.end, started: false } } } impl &lt;Idx: Step&gt; Iterator for RangeInclusiveIterator&lt;Idx&gt; { type Item = Idx; fn next(&amp;mut self) -&gt; Option&lt;Idx&gt; { if !self.started { self.started = true; return if self.current &gt; self.end { None } else { Some(self.current.clone()) }; } self.started = true; // Yes, I am desperate. if self.current &lt; self.end { let n = self.current.add_one(); std::mem::replace(&amp;mut self.current, n); Some(self.current.clone()) } else { None } } } Note the invariant on the `next` method: after the first call, `self.started` is always `true`. I cleaned-up the LLVM IR to make it more legible; just renaming, and moving blocks around: ; playground::compute ; Function Attrs: noinline nounwind nonlazybind readnone uwtable define i32 @_ZN10playground7compute17h059416e7bac56881E(i32 %begin, i32 %end) { start: br label %bb2 bb2: ; preds = %bb6, %start %i.0 = phi i32 [ %begin, %start ], [ %i.2, %bb6 ] %notstarted.0 = phi i1 [ true, %start ], [ false, %bb6 ] %r.0 = phi i32 [ 0, %start ], [ %r.1, %bb6 ] br i1 %notstarted.0, label %bb5, label %bb3 bb3: ; preds = %bb2 %0 = icmp slt i32 %i.0, %end br i1 %0, label %bb4, label %exit bb4: ; preds = %bb3 %i.1 = add i32 %i.0, 1 br label %bb6 bb5: ; preds = %bb2 %2 = icmp sgt i32 %i.0, %end br i1 %2, label %exit, label %bb6 bb6: ; preds = %bb4, %bb5 %i.2 = phi i32 [ %i.1, %bb4 ], [ %i.0, %bb5 ] %3 = add i32 %r.0, 3 %r.1 = mul i32 %i.2, %3 br label %bb2 exit: ; preds = %bb3, %bb5 ret i32 %r.0 } The interesting part, for me, is `%notstarted.0 = phi i1 [ true, %start ], [ false, %bb6 ]` in `bb2`: the condition is completely static! Unfortunately, the assembly is still disappointing, specifically the interaction between `.LBB0_4` and `.LBB0_2` which keep ping-ponging between them: playground::compute: # @playground::compute # %bb.0: xorl %eax, %eax movb $1, %cl testb $1, %cl jne .LBB0_5 jmp .LBB0_2 .LBB0_4: addl $3, %eax imull %edi, %eax xorl %ecx, %ecx testb $1, %cl je .LBB0_2 .LBB0_2: cmpl %esi, %edi jge .LBB0_6 # %bb.3: addl $1, %edi jmp .LBB0_4 .LBB0_5: # Only reached from %bb.0, not part of loop. cmpl %esi, %edi jle .LBB0_4 jmp .LBB0_6 .LBB0_6: retq # -- End function So it seems that LLVM really tries really hard not to split a loop between a first iteration and then the rest, which is detrimental for inclusive ranges.
Might be https://learning-rust.github.io/docs/c3.lifetimes.html
Python has async/await (asyncio) library. Rust doesn't have that (but it will soon, right)? What I'm supposed to use for parallel stuff like requesting HTTP?
I could finally find the time to finish the tutorial about exceptions on bare-metal AArch64 Rust: [https://github.com/rust-embedded/rust-raspi3-tutorial/tree/master/0E\_exceptions\_groundwork](https://github.com/rust-embedded/rust-raspi3-tutorial/tree/master/0E_exceptions_groundwork) &amp;#x200B; Very excited to see this first demo working on a real Raspi3 :) 
Not sure what your goal is, but here's a few crates you might look at: - https://crates.io/crates/alloc-no-stdlib - https://crates.io/crates/mbox Also worth mentioning is that all the pieces for allocators you need are already stabilized in libcore... except for an implementation. The `System` allocator is defined in libstd. I'm wondering if it could be moved into a standalone crate instead?
This is what really distinguishes good devs from bad ones. They'll both make it work but one project will be 10x easier to modify.
Awesome, sorry on my phone so couldn't see.
&gt; (but it will soon, right) Yep https://areweasyncyet.rs/ &gt; What I'm supposed to use for parallel stuff like requesting HTTP? Async/await is syntax sugar for futures. They're a bit annoying to write by hand, which is why we're working on async/await, but you can use them directly in the meantime. hyper is an example of an http library built on futures.
Adding serial port enumeration for FreeBSD to [serialport-rs](https://crates.io/crates/serialport) and probably releasing v3.2 immediately afterwards. Might also try to get compilation for `sparc64-unknown-linux-gnu` working as part of it as well.
&gt;Alternatively, check out &gt; &gt;https://serde.rs/transcode.html &gt; &gt; for a way to do this conversion in a streaming way. I can't figure out how to get it to work. I am getting the error "attempt to serialize struct, sequence or map with unknown length".
Undefined Behaviour. Basically « if you break this you’re on your own, no sense or logic can be assumed »: the compiler assumes UBs don’t happen, nothing checks for them, and because of optimisations being largely heuristic not only is your course uncharted there’s no guarantee it’s even coherent. 
Should I expect libraries to offer auto-completion or "auto-suggestion" in VS Code with the RLS extension? In the following full-code example, when I write `Array::`, I would expect to be offered a list of possible functions, as I do in Python when writing for instance `np.mul` (and get offered `np.multiply`). This does not happen. ```rust extern crate ndarray; use ndarray::Array; fn main() { let a = Array::linspace(0.,3.,4); println!("{}", a); } ```
I thought that might be the case. I figured I was getting into the less documented area and might as well check. Thanks you for the help (knowing that it's not a viable path *is* a big help)
Since LLVM doesn't seem to be willing to help, let's switch gears. The check `self.start &lt; self.end` works splendidly for exclusive ranges, so let's just do that! That should cover the bulk of iterations, and the last one will just have some extra spice: #![feature(step_trait)] use std::iter::{Iterator, IntoIterator, Step}; struct RangeInclusive&lt;Idx&gt; { start: Idx, end: Idx, } struct RangeInclusiveIterator&lt;Idx&gt; { current: Idx, end: Idx, done: bool, } impl &lt;Idx: Step&gt; IntoIterator for RangeInclusive&lt;Idx&gt; { type Item = Idx; type IntoIter = RangeInclusiveIterator&lt;Idx&gt;; fn into_iter(self) -&gt; Self::IntoIter { RangeInclusiveIterator { current: self.start, end: self.end, done: false } } } impl &lt;Idx: Step&gt; Iterator for RangeInclusiveIterator&lt;Idx&gt; { type Item = Idx; fn next(&amp;mut self) -&gt; Option&lt;Idx&gt; { if self.current &lt; self.end { let n = self.current.add_one(); let n = std::mem::replace(&amp;mut self.current, n); return Some(n); } let done = self.done; self.done = true; if !done &amp;&amp; self.current == self.end { Some(self.end.clone()) } else { None } } } #[inline(never)] pub fn compute(begin: i32, end: i32) -&gt; i32 { let range = RangeInclusive { start: begin, end }; let mut r = 0; for i in range { r = (r + 3) * i; } r } Apart from the very last run of the loop, we should have a single comparison. Then, on the last run, we perform some extra work to print the inclusive bound. A cleaned-up LLVM version, which generates the expected code: ; playground::compute ; Function Attrs: noinline nounwind nonlazybind readnone uwtable define i32 @_ZN10playground7compute17h059416e7bac56881E(i32 %begin, i32 %end) { start: br label %bb2 bb2: ; preds = %bb6, %start %i.0 = phi i32 [ %begin, %start ], [ %i.1, %bb6 ] %done.0 = phi i8 [ 0, %start ], [ %done.1, %bb6 ] %r.0 = phi i32 [ 0, %start ], [ %5, %bb6 ] %0 = icmp slt i32 %i.0, %end br i1 %0, label %bb3, label %bb4 bb3: ; preds = %bb2 %1 = add i32 %i.0, 1 br label %bb6 bb4: ; preds = %bb2 %2 = icmp eq i8 %done.0, 0 %3 = icmp eq i32 %i.0, %end %or.cond.i = and i1 %3, %2 br i1 %or.cond.i, label %bb6, label %exit bb6: ; preds = %bb4, %bb3 %i.1 = phi i32 [ %1, %bb3 ], [ %end, %bb4 ] %done.1 = phi i8 [ %done.0, %bb3 ], [ 1, %bb4 ] %4 = add i32 %r.0, 3 %5 = mul i32 %4, %i.0 br label %bb2 exit: ; preds = %bb4 ret i32 %r.0 } Note how the core loop is `%bb2` -&gt; `%bb3` -&gt; `%bb6`, with only `%bb2` containing a conditional jump. Let's see the assembly: example::compute: xor ecx, ecx xor eax, eax cmp edi, esi jge .LBB0_4 jmp .LBB0_2 .LBB0_3: add eax, 3 imul eax, edi mov edi, edx cmp edi, esi jge .LBB0_4 .LBB0_2: lea edx, [rdi + 1] jmp .LBB0_3 .LBB0_4: jne .LBB0_6 mov edx, esi test cl, cl mov cl, 1 je .LBB0_3 .LBB0_6: ret And it does indeed seem better! Notice how the core loop really is only `.LBB0_3` -&gt; `.LBB0_2` (with an unconditional jump at the end).
Borrow checking has no effect on what happens at runtime, it's a purely compile time check to make sure your code is sound. NLL just expanded what the compiler is smart enough to verify. Dropping still follows the normal block scoping rules, so you'll see the behavior you're expecting if you wrap the "a" variable in a block to make it drop early. https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=3fd167a3d14b7aee17b3ad05cbb2114b
Yes. That said, try building the project after you add the dependency. RLS can't autocomplete if you don't actually have the library 
I contributed to rumble: An API to communicate with a Mumble Server. One can use it to create Bots for Mumble servers that can send text messages, switch channels and more. Take a look at the repo: [rumble](https://github.com/Prior99/rumble)
Thanks for the responses. I have looked into these and also tried the ndarray crate (I am coming from python / numpy). As a more general point, my current project is a suduko solver, and so I am working with a 9x9 array, the contents (but not size) of which will change. What would the best representation be? I had assumed mutable array, and I now think maybe ndarray, although it seems a bit heavy weight for what I am doing. Thanks. 
And the amount of time you have to do a project. Writing good code is hard, and takes time. Even if I want to polish the code if I'm requested a new feature on my codebase and it is supposeed to go live in 24h then the code is not gooing to look pretty.
The borrow checker is simply checking to see if you are attempting to use a reference outside what is considered safe, not a guarantee of a variable being dropped. And NLL is giving the borrow checker more information about what is considered a safe use of a reference. As far as I know neither affect when a variable or even reference is dropped, only tell the compiler about unsafe uses of references.
Nll doesn't change the semantic of rust code, thus drop calls are still lexical. If not it would break a lot of RAII patterns. Nll only change the scope of a borrow.
I'd still probably just use `Vec` unless you really need to take advantage of knowing the length 9 at compile time. Plus, what if someone wants you to do a 16x16 sudoku?
I'm not OP, but I've been working through the [interpreter book](https://interpreterbook.com/), translating to Rust as I go, and it has been a very good introduction. 
Looks good to me, I'll see if I can scrape up something to get it. Thanks for the tip!
Did something similar, porting a python script into a rust executable about 2 years ago now. I got very similar numbers. Got run time down from 20 minutes average to about 1-3 mins depending on network transfer and server response time. I'd argue the real speed up was due to optimizing the search method -- O(7n) down to O(n) -- but it's interesting to see someone else get pretty similar numbers
&gt; C++ doesn't use it either. What? How do you think `push_back` is implemented? In VS2017 if you trace through the code you eventually end up in `allocator_traits::construct` which looks something like this: template&lt;class _ObjTy, class... _ArgTys&gt; static void construct(_Alloc&amp;, _ObjTy * const _Ptr, _ArgTys&amp;&amp;... _Args) { // construct _ObjTy(_ArgTys...) at _Ptr ::new (const_cast&lt;void *&gt;(static_cast&lt;const volatile void *&gt;(_Ptr))) _Objty(_STD forward&lt;_Types&gt;(_Args)...); } which is using placement new to construct the object at the end of the preallocated space. How does this work in Rust?
Yeah, but they specify links that correspond to the generated html, whereas I wanted to specify them by item path - it would less error prone this way, and does not break with reexports. I took another look at the [tracking issue](https://github.com/rust-lang/rust/issues/43466), and now I noticed that the last item is stabilization - it actually does work on nightly (previously I only tried on stable).
Interestingly, the `is_empty` field was added because of performance problems: https://github.com/rust-lang/rust/commit/0d7e9933d3cac85bc1f11dc0fec67fcad77784ca :(
Does rust need an infrastructure working group?
Oh interesting. So even though the `a` reference in my example isn't technically dropped by the time `b` is created, the compiler knows that it won't be used any more so it's safe to create `b`?
That's a good point. The compiler *could* optimize out the stack space once it realized that was happening, but needing to rely on the optimizer to notice a redundant move is not ideal.
&gt; I have made this [letter] longer than usual because I have not had time to make it shorter. — Blaise Pascal
a new porn web server using rocket
But if your rust program is already hitting the L3 cache 1 000 000 times then it doesn't really matter that your python program is hitting it 1 050 000 times. Whereas if your rust program never touches the L3 cache then the fact that python hits it 50 000 is gonna make a huge difference. Basically yes python always has bad cache locality, but that doesn't mean that Rust always had good cache locality. 
I'm not very experienced with futures, so just spitballing: Maybe you could have an asynchronous client that would do everything with futures, and then sync client that would simply wrap the async one? Because when you give the async client a sender function that actually runs synchronously and just wraps the result into a future, then the whole async client would actually run synchronously, and so the sync client could just poll that future once to get the result. You would need to write a sync function that would wrap every user-facing async one, but it would be only a very thin wrapper that could be generated with a macro - none of the actual implementations would be duplicated.
Well, you already initialize the board to zero, so you could write the initialization code more imperatively - I wouldn't say it is less idiomatic: fn populate() -&gt; [[u32; 9]; 9] { let mut board = [[0; 9]; 9]; for row in &amp;mut board { for i in 0..row.len() { row[i] = i + 1; } } }
Exactly.
Sure, my point was more that I would expect a *factor* (ie, Python hits 3x more times) rather than an *offset* (ie, Python hits 50,000+ more times) due to the fact that each Python object has multiple levels of indirection whereas in Rust you can pack things tightly together.
Our push_back constructs on the stack, writes it to uninitialized memory off the tail of the vector, forgets the stack copy, and then bumps length to include the now-live bits We don't have the ability to run a by-value object constructor directly on an address; the closest approach would be a by-ref constructor that takes a reference to an existing typed region and `ptr::write`s to its fields, like fn make_there(&amp;mut self); unsafe { mem::uninitialized::&lt;T&gt;() }.make_there()
We have an infrastructure team already.
Where is the error pointing to?
Adding two `usize`s produces `usize`, so you cannot match as if it was a `Result`. * If you wanted "addition that I believe won't overflow", then do just that: `let temp = fib1 + fib2`. On overflow this will panic in debug builds (to indicate the presence of a bug), and wrap around in release builds (because that gives better performance). * If you wanted to check for overflow, use `fib1.checked_add(fib2)` - this will give `Some` if addition did not overflow, and `None` if it did (so you will have to match with `Some(num)` and `None`, instead of `Ok(num)` and `Err(_)`).
That's a very good quote, didn't hear it before.
Yes, I worked on a compiler toolchain that was unrelated to LLVM.
This works because all rust objects are 'move by memcpy', right?
Are you at liberty to detail how overflow checking was implemented, so as to offer good performance? *(I have no idea how complicated the answer is; please don't feel obliged to answer, or to write a full thesis about it)*
Thanks, that's working :). What's the difference between "Some and Num" and "Ok and Err"? They seem to be referenced as options and results, when do you use each?
\`Some\` and \`None\` are for presence and absence; you use \`None\` where you would use \`null\` in a lot of other languages. &amp;#x200B; \`Ok\` and \`Err\` are success and failure.
Man, if only I could snip what people say and save it for later, that seems like a useful pattern to remember :/ (I can see uses for both patterns, namely that in my opinion the enum one is 'easier' to de-structure and encourages more pattern matching, but the NewType is easier to write...)
I wish more was underway to further modularize dependency graphs on crates.io using features or just splitting out crates. I feel like almost all the code that builds in my &gt;200 dependency projects is just stuff that gets pulled in for features I'm not using. This may be more of a community problem at this point. Const generics will help some though because some crates that take a long time are interacting with type-level integers.
No. `RefCell::borrow_mut` takes `self` by non-mut reference, so compiler (with or without NLL) sees nothing wrong with calling it while you are still holding reference previously returned by it. Of course this code panics at runtime because `RefCell` checks borrowing rules at run-time and you can't create multiple mutable references to the same memory.
The enum doesn't make much sense to me if there are only two cases. Then you're just reinventing booleans. But if there could be more in the future then it might be correct.
NLL doesn't affect borrows inside types with `Drop`. If `x` is variable of type that implements `Drop` then `x` is effectivly used at the end of scope hence it borrows until end of scope.
Good to know! I always feared to use \`Mutex\` with NLL, because I wasn‘t sure if there was a possibility that the \`MutexGuard\` was dropped before the end of the lexical scope.
Yes; a move is a memcpy that doesn't run the destructor on the old address
`Some` and `None` are variants of type `Option&lt;T&gt;` `Ok` and `Err` are variants of type `Result&lt;T, E&gt;`
Thanks for the feedback
I program in C and write my own libs specifically because everything else is utter trash (except for Sean Barrett’s libs)
&gt; The ABI should only be an issue across different rustc versions. But it should be possible to compile a dynamic library and link it dynamically to another program using the exact same rustc. Yes. I'm just saying that, with even a bump from yesterday's nightly to today's not guaranteed to remain ABI compatible, it gets prohibitively awkward to ensure continued compatibility between every Rust binary and library that might dynamically link to each other. &gt; I think this use case should be allowed and should be reasonably straightforward. Given how many people I've seen who make mistakes in this area, I have to disagree. It would be comparable to those who think aspects of the borrow checker should be opt-in, because it's too nitpicky in the C++ design patterns they're trying to force upon it.
&gt; you need the header files to build a program against it. ...or `dlopen`. They *could* build a `cdylib` instead and use [libloading](https://docs.rs/libloading/0.5.0/libloading/).
To express it as a trait bound I'm using a crate, yes.
I wouldn’t call it crazy bad. It’s much slower than C but much faster than C++. Surely it could use improvement, but it’s not horrible.
&gt;making compiler happy Is not simply. I'm building my first project in rust and is being reworked from the ground up for the five time already. &amp;#x200B; The same take me in swift 1 day.
I see that `&amp;str` implements `Clone`, but cannot find the source code. I'm interested to see how a reference implements `Clone`. Can someone point me to the source please?
A big weakness of C++ is that it doesn't have this concept, which I call "trivial relocate"; it's the equivalence of memcpy and move-construct + destruct at old location. One of the reasons I really like Rust is that it embraces this definition of move -- there are no non-trivial move constructors, and all objects are trivially relocatable. (Pinned objects are a more complicated story...)
firm agree; we use `new(arena)` at work because construction and transfer are profoundly unpleasant
I'm not seeing how this is like the borrow checker being opt in. I'm saying that if there is a multi-crate project, you should be able to compile the whole project with one compiler and link/swap/plug-in its components together dynamically. I agree that it should not let in linking for a different compiler build, but with the same compiler build, it should work. Now if it is difficult to enable that in the compiler and the work/benefit ratio is too high, I can understand that. If it is difficult to make the compiler both (a) accept linking components built with the same compiler build and (b) reject linking with different builds, I could also understand that. But I do not agree that it is a good thing to refuse linking components built with the same compiler. I'm failing to see how that is a misfeature like ignoring the borrow checker would be.
I'm writing a CLI program to parallely run git commands in cloned repositories within some parent directory. I walk the directory tree looking for git repos, spawn a git `std::process::Command` &amp; get its output, then send the data back to the main thread through an `std::sync::mpsc::channel` to be printed. The problem is I'm getting this error when compiling: ``` error[E0597]: `matches` does not live long enough --&gt; src/main.rs:18:28 | 18 | let git_command_args = matches.values_of("cmd").unwrap(); | ^^^^^^^----------------- | | | borrowed value does not live long enough | argument requires that `matches` is borrowed for `'static` ... 49 | } | - `matches` dropped here while still borrowed error: aborting due to previous error ``` What is the problem here? AFAICT, `matches` and `git_command_args` are `'static`, so shouldn't they live long enough? [Rust playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=a476965fc91371de99b8de417e28f524)
Option is like Result, but without any error data.
Merged! Thanks for the contribution.
Yes that makes sense. I usually use blocks to force the drop rather than the `drop` function.
Then why did Ripple go with C++?
The underlying protocol is the deterministic factor in TPS. So is Go fast enough to hit something like 100K TPS? (I'm going to guess that what is in the transaction is also the determining factor). 
What for? This is all you need... https://docs.rs/is-odd/1.0.0/src/is_odd/lib.rs.html#1-3 You don't need an entire crate for few functions you could write yourself or even copy paste as they are in front of you. Is this fucking Rust or JavaScript?
Then why are you using Rust? Could get away with using C, wrapping rust around C lib that is prone to bugs won't make it any safer.
`.collect()` can't build arrays or slices, only Vecs. Maybe something like [this](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=489b505ad67b8f725f7b9c731d3b1829) although I think it's clearer like [this](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=6f0ddbc92febdcd2c2a086d6f25ccb8e). 
That works at the _value_ level, this works at the _type_ level. Using it at the type level makes it impossible to construct a version of this type where the phantom value is even. There's no runtime check (as with the `is-odd` crate), it's straight up prevented at compile time. The `num` crate is also useless for this, because it deals with _value space_ not _type space_.
I was surprised (and saddened) that there was such a performance difference between inclusive and exclusive ranges--there are plenty of use cases for each. I quickly wrote up a pythagorean triple crate to do the calculation as performed in [simple.rs](https://simple.rs) with inclusive and exclusive ranges. I then wrote a functional combinator version similar to the one provided by OP also with inclusive and exclusive ranges. The difference was I removed println!() from the code (I didn't think benchmarking println!() was interesting, and then, in a more single-responsiblity-principle/functional style, had the function return the set of triples to the caller. I set up the API to the function call to accept a &amp;mut buf preallocated buffer, so the algorithm's performance is just about the computation itself. I discovered: * I must be missing something with `cargo bench`--the functional versions (\[profile.bench\] opt-level = 3, lto = true, ...) take 76ms (yes milliseconds) for exclusive and 190ms for inclusive-- 2-5 *million* times longer than the imperative for versions at 35ns/iter--this was my first time using bench, so I've missed a step somewhere (I'm running 1.33.0 nightly 2018-12-31 on AMD ThreadRipper, Ubuntu Budgie 18.04.1 LTS.) Please let me know if you know what step(s) I'm missing... * Running trusty `cargo test` on my machine shows the following (timings courtesy of CLion test runner): * range inclusive: 212ms * range exclusive: 86ms * simple inclusive: 138ms * simple exclusive: 138ms * Clearly functional combinators can optimize very nicely (as per range exclusive) and can represent negative overhead abstractions over hand-coding when conditions are just right. That is a nice promise that the compiler optimizers are able to deliver on, sometimes. * `..=` seems broken with this degree of performance regression (per range inclusive) to me. It's something I would like to look into more when I get the time. I would have naively assumed that the exclusive range's implementation would iterate while `curr &lt; end` and the inclusive range would iterate as long as `curr &lt;= end`, thus avoiding the +1 overflow issue, but clearly this is not the case. Above code available at https://github.com/U007D/pythagorean_triples.git. Thanks to @SelfDistinction for sharing their findings.
Thanks for correction. I've updated the post. Really what I meant is that they look like byte collections when you check the `len` method.
I'm using the excellent [cpal](https://docs.rs/crate/cpal/0.8.2) crate to process audio. When recording, it will yield data in the form of `UnknownTypeInputBuffer`: an enum which values can be dereferenced into a chunk of i16, i32, or f32 data points. I need the data to be floats for further processing, so I'd like to pass on the f32 values without copying, and convert the integer ones. This is the code I have come up with: ``` let input: Box&lt;dyn Deref&lt;Target = [f32]&gt;&gt;; match stream_data { // pass float data unchanged (and no copy) StreamData::Input { buffer: UnknownTypeInputBuffer::F32(buffer) } =&gt; { input = Box::new(buffer); // rebox } // convert integer input into floats StreamData::Input { buffer: UnknownTypeInputBuffer::I16(buffer) } =&gt; { let max = i16::max_value() as f32; input = Box::new( buffer.iter() .map(|&amp;i| i as f32 / max) .collect::&lt;Vec&lt;_&gt;&gt;() .into_boxed_slice(), ); } StreamData::Input { buffer: UnknownTypeInputBuffer::U16(buffer) } =&gt; { let max = u16::max_value() as f32; let base = max / 2.; input = Box::new( buffer.iter() .map(|&amp;i| (i as f32 - base) / max) .collect::&lt;Vec&lt;_&gt;&gt;() .into_boxed_slice(), ); } StreamData::Output { buffer: _ } =&gt; { panic!("Received output data on an input device"); } } // further processing of `input` ``` It works but feels a bit clunky, especially the type definition and the reboxing. Is there any way to simplify under the constraint of no-copy for the float input data? Thanks!
I'm very new to rust, but looking at your Rust library I noticed that you aren't using the inline macro. It might also speed up performance on large data sets to use the hashbrown external crate for your hashset over there standard library HashSet 
&gt; O(7n) down to O(n) Both are the same thing but I get what you mean.
Good questions * Yeah, it was compliled in release for the benchmark * Not really, I was using ``beautifulsoup`` with ``html.parser`` which is mostly written in Python with a few regular expressions which would be done with C. Also, there's a fair bit of string slicing and concatenating which would presumably be implemented in C in CPython. Other reasons for not being super fast is that: 1. I really haven't tried to optimize the code much. There's a number of cases where I'm coping data when I could probably just be using references. I think this is a fair example of how code might work in a practical environment, not in an ideal one. 2. As I mentioned off-hand in the post. I'm doing unicde segmentation/character counting properly in the Rust version. For the purposes of displaying tables, this is really expensive. There are optimizations to the code that could be done here, but that would add a bit of complexity and I haven't done them yet. 3. I should have mentioned this in the article, but there's a bunch of cases where I'm using `Vec` where I would probably be better off using a fixed array or an iterator. Again, this is related to not doing any optimization. For the most part, if the Python version used an interator, the Rust version sticks everything in a `Vec` and returns that. I'm not sure if this is must slower in practice though. Making an iterator in Rust is a fair bit more work, I think. 
You might also be interested in [Crafting Interpreters](http://www.craftinginterpreters.com/contents.html) by /u/munificent.
I thought this might be it, but still, after successfully compiling, it does not autocomplete. Would you mind compiling the above and trying `Array::eye` or `Array::range`? (as in, typing "Array::e" and seeing if you get suggestions). 
Yeah I didn't take comp sci in College so this is the best way I know how to represent it. Thanks for correcting me!
`git_command` is `Arc&lt;Vec&lt;&amp;str&gt;&gt;`, and that lifetime inside it is not `'static`, which is why you are getting that error. The reason is basically what the compiler points out - it borrows `matches`, which is dropped at the end of the scope - so it is definitely not `'static`. One way to fix this would be create owned strings from matches: let git_command: Arc&lt;Vec&lt;String&gt;&gt; = ^^^^^^ owned strings Arc::new(git_command_args.map(String::from).collect()); ^^^^^^^^^^^^^^^^^^ add this
Thanks for sharing! Code seems very clean. Helped clarify some concepts for me
I just read through the first chapter, looks amazing (and free)! 
&gt; Then I got to thinking The struggle is real
The key point here is that the thing that you can clone is a reference. So saying that `&amp;str: Clone` means that if you have an `&amp;str`, you can use it to create another `&amp;str`. So the thing being copied here is not the `str` - you simply take another reference to it, and only those 4/8 bytes that make up that reference get copied. So actually there's `impl&lt;'a, T: ?Sized&gt; Clone for &amp;'a T` - you can always take new references to anything. That impl is [here](https://doc.rust-lang.org/src/core/clone.rs.html#207-212) - it makes use of the fact that references are also `Copy` (and the impl for that is [here](https://doc.rust-lang.org/beta/src/core/marker.rs.html#694)).
References (`&amp;T` for any `T`) implement `Copy` which implies `Clone`. Alas, this isn't terribly useful, because you usually want the cloned `str`, not a copy of the reference. There's a clippy lint for that, too.
I wonder about the machine-readable RFC's. What about having a machine readable (say, `toml`) file accompanying the markdown RFC text, or even replacing part of it? It could allow indexing in CI, publishing things to a database, and generating a static website. Does such a system exist? If not, it seems like a very useful programme and service.
Experienced players like yourself should go over to /r/PlayRust. T You're currently in the programming languages Reddit
Are you familiar with enums yet? `Ok` and `Err` are the two variants of the `Result` enum. They are not specifically connected with the match expression in any way. So if you don't have a `Result`, it doesn't make sense to provide them as patterns on the match arms. But you can match directly on integer values. let next = match fib1 + fib2 { 13 =&gt; return 13, x =&gt; x, } Check out [Chapter 6 Enums and Pattern Matching, in the Rust Book](https://doc.rust-lang.org/book/ch06-00-enums.html) and [the api documentation for Result](https://doc.rust-lang.org/std/result/index.html) for more information.
ooooooooooooooooohh! That makes much more sense now. Are there any exercises/readings to get better at identifying these sorts of situations, or is it mostly just experience &amp; picking every little thing apart?
The very simple answer is you use Some and None when your variable is of type Option, and you use Ok and Err when your variable is of type Result. If you don't know what type of variable you have, try assigning it to something you know it's not, and read the error messages that show up when you compile. The Unit type works well for this. let x: () = myvar; Or even simpler: let () = myvar; 
Returning an iterator in rust isn't necessarily hard nowadays, using something like ```rust fn foo() -&gt; impl Iterator&lt;Item=Foo&gt; { some_vec.iter().map(f).filter(g) } ``` that will work for any concrete iterator type (such as calling `iter()` on a collection and then a bunch of combinators like `map`). Otherwise you can use a struct/enum and implement `Iterator`'s `next` method but it's indeed a bit more work.
I've been looking for a crate(s) that will allow me to carry out some functionalities provided by FFmpeg, but in-memory and over a network connection. Specifically I want to stream audio-visual files in over a network and begin identifying the file type and stripping out and transcoding audio channels as the file downloads. Does anyone have recommendations for what I should look into using? Thanks.
Refcell is dynamic runtime borrowing and not checked by the borrow checker. It relies on you to use it properly.
This would depend a bit on what the Rust-Python interface was going to look like, and how your code was structured. In my case, there wasn't much point in incrementally porting August. The library itself wasn't super big, and it was very class based. Probably the biggest amount of work was settlingly on a good way of handling what had been handled by the polymorphism in the Python code. Also, the Python code was a little ugly in some spots, and I didn't want to have to port that over. Generally speaking, it depends on what your units are, and if those units are going to make any sense in the Rust version. For example, in the Python code there was a big dict that mapped HTML tag names to element types. In Rust, I just used a function, a map was unnecesary and inefficent, and Rust has the `match` keyword so I don't need to deal with if/elif horror. The code looked roughly like: ```python BLOCK_TAGS = { 'p': BlockElement, 'h1': HeaderElement, 'blockquote': BlockquoteElement, 'hr': HrElement, ... ``` versus ```rust fn from_tag_name(tag_name: &amp;str, attrs: Vec&lt;Attribute&gt;, children: Vec&lt;ElementType&gt;) -&gt; Option&lt;Block&gt; { let tag = match tag_name { "p" =&gt; Some(BlockType::Block), "h1" =&gt; Some(BlockType::Header), "blockquote" =&gt; Some(BlockType::Quote), "hr" =&gt; Some(BlockType::Rule), ... ``` Now, if I had a python unitest with the line `assert(BLOCK_TAGS['p'] == BlockElement)` that would be a pain, and a big waste of time to implement in the Rust version. For the purposes of project management, you don't want your units of work to be too large (40 hours is probably a good max). So if you have a large project, you really do want to port things incrementally. How this works in practice, would very-much depend on the code base. If I was speculatify thow out a number, I'd guess that on average, for every 2 increments, each increment would be about ¾ the work, but the reality of a problem might be quite different.
Do you have any images or any details on what you're using it for? The readme is more than minimal unfortunately
FWIW, main problems I've had were: * Slow collections in the stdlib. * Inability to pattern match "through" an `Rc`. 
Ocaml is garbage collected and thus not suitable for time constrained low level tasks. Rust is targeting system programming tasks.
&gt; Just realized rust lang has grew far past ocaml/reasonml/haskell How do you know? Haskell has a pretty large community AFAIK. &gt; more popular than scala based on the metrics of open source activities in krhelinator Metrics trying to pinpoint the popularity of programming languages tend to be very bad at that, so I wouldn't give this much thought. An example: The rating of [rust-analyzer](https://krihelinator.xyz/repositories/rust-analyzer/rust-analyzer) seems to use totally outdated or incorrect stats, yet this is apparently still the Rust project with the highest score? `rust-lang/rust` is also missing completely - it either should be counted, or stuff like clippy and Cargo shouldn't be considered either, since those are part of the Rust toolchain and are developed by the same teams.
I don't know any specific exercises for this, you just get better at understanding types when you spend more time with rust. Although I can suggest looking a bit more carefully when you see bounds or impls on references. For example, `Read::read` takes `&amp;mut self`. However, you can use it even when you only have `&amp;File`, because there's an `impl&lt;'a&gt; Read for &amp;'a File` - so `read` actually gets called with `&amp;mut &amp;File`, which basically only allows you to get `&amp;File` - but OS deals with synchronization anyways, so you can read even without unique reference. 
Languages like Ocaml and Haskell are heavily functional. The mental model is very different and is usually foreign to the typical developer's background. The reality is that most developers would rather settle with the familiar. Rust adopted concepts from function, but look at a lot of Rust code. It's mostly imperative. Written by people with imperative backgrounds. &amp;#x200B;
Do you know examples when garbage collection would be problematic? I am not doubting this, I just don't know any cases where a GC automatically releasing memory on modern hardware could be problematic.
I have used both Ocaml and Rust for similar projects. From my perspective a lot of the success of Rust has much to do with the community, documentation, onboarding experience and tools. And not so much with the language itself. If you have these things, people are likely to stick around and learn a difficult language. Ocaml is a great langugage, but the onboarding experience is confusing, the documentation is quite poor (libraries specially), the tools are odd (depending on your background) and hard to get into at first. 
I think one of the main reasons is simply because on the surface it looks like a non-functional C based language. Full on functional languages are quite a hard sell. Scala in particular has a syntax that I often felt was pointlessly different. Typically languages designed to replace C++ fail to do what C++ is good at. In fact they also tend to offer less, and very little that is new. One of Rust’s biggest selling point is that it can or could be a C++ replacement at many places. It also offers something new (the borrow checker). That has given it a lot of hype. Rust also got a lot of things right. Nanely cargo. A lot of it’s decisions are aimed at solving professional problema. It manages to do this without being enterprisy.
Games. Drones. Self-driving cars. Anything real-time.
There are intrinsic aspects to the language and its implementation, like performance or safety or how easy it is to embed in other programs. Rust fills a relatively unique niche here compared to Scala or OCaml. I can't really draw a comparison here, but I suspect that Rust also draws a lot of success from the work put into tooling, documentation, building the ecosystem, and working with production users from early in its development. This is also where people talk about corporate backing- Mozilla in this case, Google for Go, etc. Personally I'm coming to Rust from C++, on projects where Scala or OCaml aren't really viable (though I'd love to get more chances to use OCaml). Game engines, bare metal/embedded, etc. I also use Rust for compiler-related projects where there are more language options, but I stick with it because of that second set of benefits- Cargo, rustup, good Windows support, etc. Plus easy integration with my other projects, another language benefit.
Ah, sorry for being unclear. I have heard those mentioned before, but I am wondering what problems one would perceive if these were implemented with automatic garbage collection.
Having a tuple like struct with a single private field and a `get` method is the best way to go here. Plus a `new` function that does validation and returns an `Option&lt;NonZeroFloat&gt;`. That's how the std library's `NonZero*` types do it. You can write a macro to make implementing operators easier.
&gt; but most software applications doesn't seem to need so much performance, especially when JVM and ocaml/haskell are only 2 to 3 times slower than C/C++. I can't really answer your particular question but i really have a problem with this sentiment. Its not originally yours and i don't want to hammer to hard on it but i really see this particular statement more often repeated than i feel comfortable with. From my perspective this is wrong from multiple directions. What does "doesn't seem to need so much performance" even mean? One could argue i have a problem that i can solve in 200 ms with python i don't need the performance to solve this in 5 ms because from a user perspective there is not really a difference. I would say: fair enough, to be honest this is absolutely right. _But_ there are certainly other factors that need to be accounted here and one is wasted CPU time. And we should not confuse this with the perceived time a user has to wait for a result. If we take the numbers from my example, we are using 40x more CPU time here to solve the problem – and again just from the perspective of user-waiting-for-the-result this is negligible. But we are not only wasting 40x more CPU time, we are wasting 40x more energy. And for a laptop user saving energy cold very well be in the interest. So even tough there is no noticeable difference in waiting time, there is very well a difference in Laptop Runtime. And every user is happy about longer battery life! So if we have this kind of choir sung by the majority of software developers we have a problem because most software "doesn't seem to need so much performance" and thus wasting a lot of CPU time and battery life and for the most part, yes i am not particular interested in the difference from 5 ms to 200 ms in a single instance but if the majority of programs have this kind of "waste" it really adds up. Don't get me wrong, but i really feel like Computers get slower and slower. We crank our machines up with more and more powerful CPUs but all we have is software that is just wasting energy into heat. I know this is some kind of Jevons paradox or Rebound-Effect, but i really don't like it and we see this every where. We have increased the efficiency of combustion engines, only to waste it with heavier cars that use more electronics and air conditioners. We increased the efficiency in batteries only to waste it with more demanding mobile phones and never got over two days of battery life on mobile phones. This is everywhere and i wouldn't bother to much – except that the run for more CPU power pretty much flattened out but people learned that it is ok to waste CPU time because, just by a faster CPU. And now we really feel the downside of this from a user perspective – at least i do. I really really feel the battery life of laptops get really stuck even though the battery technology has brought great improvements. Yes we read every two weeks about a breakthrough in battery technology and swipe it away as nonsense – but if you really look into it we did not have a fundamental/revolutionary breakthrough but pretty steady improvements. But i don't see this reflected in improved battery life. I see editors using electron that chew my battery up like nothing. And i don't want to bash on electron here – it has improved quite a bit. But i really feel like my system with 8 cores and 32Gb of ram is not much faster than my machine like 10 years ago. Yes it can compute things faster etc. but simple things like opening a website (don't let me start on the wasted CPU time the "modern" web is producing) opening a file browser, opening a simple application ... it feels like all those applications "doesn't seem to need so much performance" but if your hole system is build with those programs you really start to question this mental model. I really honestly feel my system is like 5 times slower than it needs to be and could run double the time on battery if the majority of programs i use are not so wasteful with its resources, because in the end it DOES matter if the problem could be solved in 5 ms rather than 200 ms even though the user does not immediately sees this as necessary. Sorry for being a little bit ranty and i don't want to sound to much confrontational – i hope i could deliver this in a constructive way rather than just bash on all the things that are not 100% effective. I really like programming in python because its fun and for some problems just saves development time that is often more desirably. If you can program two times faster and deliver a product two times faster by having "just" 2-3 times less performance i think a majority of people would choose the faster route. I would, too. But as developers i think we have the duty/obligation to have this in mind when we develop software. We are not alone on the system of our users and we should care about their battery life and we should even more think about the environment and wasted electric energy and environmental changes we cause if you just waste 3 times more electric energy (the majority of energy is generated by coal) .... we are not just having programs that have 3 times more runtime as i said we just wast 3 times more energy – SAVE THE PLANET :P 
Cool page :) I was looking at hyper but it brings whole Tokyo thing. I guess this should be done natively. How should I approach the problem if I would like to use threads? I guess I can spawn 10 threads instead of 10 `async/await` function.
Rust has been vastly more effective at marketing. You've never heard of the OCaml Evangelism Strike Force, and it's not [due to stealth](https://xkcd.com/125/). Rust has also made some sensible compromises for the sake of accessibility, in terms of syntax and prioritization of developer experience.
So a big one is with C# when doing game dev (unity), eventually you start generating enough garbage that the garbage collector running causes a noticeable stuttering. Its a fairly big job to track down and fix all of this. This is a big problem even with games that aren't that complex The other aspect is that a garbage collector having to be invoked at all necessarily uses some performance, which is also unacceptable for a high performance gamedev style system
Tokio is what you’ll use with async/await too. And yeah, ten threads isn’t bad, you may want to give it a shot.
Games: lagging, increased range of frame times (micro stutter) Drones: crashing drones if the controller is not acting fast enough to correct for certain conditions cars: dead people? i mean, we can't wait on the GC to finish its job to prevent a crash by steering away from an obstacle. 
So that largely depends on what the data looks like. If you think about it there's always going some offset which is essentially the code itself (e.g. function calls or variable lookups). Whether there's also a factor there is going to depend on what your data (i.e. the thing that you're scaling up) is. If your data is vectors of structs for example, then python is going to have more indirection per piece of data which _would_ give you a factor. But this is all about parsing and laying out HTML so your data is going to be strings and complex data structures, so the levels of indirection that are going on are going to be similar. 
Only when C++ code makes heavy use of template meta-programming.
There's no magic to it, you just have to treat overflow-correctness as a worthy goal, and then optimize it like you would any other thing. There are some essentials that you need. First, you need a very efficient idiom for checking overflow, at runtime, when you cannot provide that overflow cannot occur. On x86, that was typically the `INTO` instruction (Interrupt on Overflow), but that single-byte opcode was repurposed on x64, so on x64 we used `JO` (Jump on Overflow). In every function that was compiled where overflow could occur, the compiler emitted a tiny basic block that consisted solely of `_overflow: JMP overflow_panic_handler`, and then inside the method, at every place where overflow could occur, there would be a `JO _overflow` instruction. One single instruction, which had a cost that was so small as to be virtually undetectable, especially because on most x86/x64 architectures these days, branches are initially assumed not to be taken. Then you need to optimize basic things around overflow. Range analysis is pretty important; if you consider an expression like `(i &amp; 0xFF) + 0x1000`, with `u32` operands, then the addition is just never going to overflow. These situations are common enough that optimizing them is important. There are also lots of other idioms that you can optimize. For example, when I was working on overflow optimization in our compiler, we saw one ugly pattern when compiling code generated by an RPC stub. The RPC system would generate methods for measuring the byte-length of an encoded message, something like: struct FooMessage { string s; int32 x; int32 y; } virtual size_t get_message_length(FooMessage* msg) { return msg-&gt;s-&gt;get_length() // for 's' field + sizeof(u32) // for 'x' field + sizeof(u32); // for 'y' field } So the compiler sees "dynamic value + 4 + 4", and has to emit two overflow checks, one for each + operation. But we humans know that this same statement is exactly equal to "dynamic value + 8", and that there is no *visible* difference between `n + 4 + 4` and `n + 8`. So our compiler was able to look at expressions like this, and understand that, in side-effect free expression trees, we could legally rewrite this to `n + 8` and therefore only have a single overflow check. This stuff adds up. So it isn't magic, but you have to deal with it at lots of different levels of the compiler, to get the behavior you want. You have to treat optimizing checked arithmetic as something that is as important as all of your other optimizations. That's a big cultural shift for the C/C++ world.
Interesting point. I just start to read rust book and play rust for a few days. I didn't realized that idiomatic rust codes are not very functional. This indeed could be one of the reasons. Comparably in the scala world, one of the reasons scala has a modest success is that it could be used initially as as a better Java with imperative style, the path to functional style can be incremental... &amp;#x200B; Not sure how true it is: " who have little interest in discovering the benefits of functional. " in the scala world, over time, people developed cats/scalaz/cats-effect/fs2 etc haskell style of pure functional libraries. It doesn't happen at the beginning, but once people taste a bit of benefits of functional, they might be intrigued, as I believe the rustaceans are inherently similar people like to explore new paradigms. &amp;#x200B;
Thanks for the clear answer. I suppose that games like that are already keeping all cores busy? In other words, when 28 or 32 core CPUs are reasonably priced would a GC run still be problematic? (I'm just trying to understand how deep the problem of automatic GC runs wrt to future hardware improvements). In other words is it more a matter of time and hardware cost effectiveness?
If anyone search for answer check out discussion [here](https://users.rust-lang.org/t/issure-with-passing-struct-which-has-function-closure-to-the-future-stream/23685)
I wasn't expecting it to not compile, I was just expecting that `a` would be dropped at runtime before `b` was created, but as others have pointed out NLL doesn't affect when things are dropped.
AFIAK (which isn't that much) all current GC implementations require the main thread to pause occasionally to do GC. There are implementations that pause the main thread for a bit and then do more work on a separate thread, but that still means you have some pauses on the main thread.
Having read through the OCaml discussion I would suggest taking many of their points about Rust with a big pinch of salt. Namely about comments around garbage collection. It depends on your definition. Rust ensures memory cannot be accessed in a dangerous manner (like use after free). Rust ensures memory will be deallocated when it is no longer used, and this will only happen once. For most people this garbage collection as this is what they really want. Rust doesn’t have automatic memory allocation. You have to import `Box` and call it by hand. I’d also argue immutability is much easier in Rust than in other languages.
Yes, but not all of your customers will have the new CPUs. In the end, you would still prefer to give everyone the best performance
Thanks, I found some more interesting stuff about exactly this subject: [https://blog.overops.com/garbage-collectors-serial-vs-parallel-vs-cms-vs-the-g1-and-whats-new-in-java-8/](https://blog.overops.com/garbage-collectors-serial-vs-parallel-vs-cms-vs-the-g1-and-whats-new-in-java-8/) I guess I have some studying to do.
You can't really easily use two languages with garbage collection together. A lot of Rust users are writing Rust code to call from their Python or Ruby codebase.
I am using Rust for the same thing I have used a lot of Scala code in the paste e.g. building Microservices. The benefits are mostly the same as what I see with Scala; A strong type system, the ability to build code correct by design and having the peace of mind that my code isn't going to blow up in the middle night and wake me up. I can think of three benefits Rust has over Scala 1) Performance and resource are very easy to reason about 2) More practical and explicit and 3) Not having to carry around the JVM. In my use case #2 and #3 are really the only two that make me choose Rust over Scala. #1 is nice but in all reality at the scale I'm working at is not as huge of a priority. That said when other teams are using Go #1 helps me sell Rust and is a detriment to Scala since the JVM likes to use lots of memory (I might just be using it wrong). The only downside my team brought up when Rust was introduced to our stack and they had written some was that IDE tooling isn't very helpful and the autocomplete help the IDE could give for Scala helped the team learn quicker. There probably would have been other downsides if I hadn't been around to lay a solid foundation and smooth over the less mature ecosystem. It feels easier to sell Rust to upper management because it has made some very practical choices and on the surface feels much more like an imperative language that most are familiar with. Sorry for the rambling reply I haven't really sat down to organize my thoughts about this and I have many.
Interesting point!
From time to time all GC need to do full sweep (aka “stop the world”), all design in GCs is to reduce amount of time that is spent there or to run it less often. so no matter how much cores you will have, such stops will always be a problem. Another problem is that there exist [Downs-Thomson paradox](https://en.wikipedia.org/wiki/Downs%E2%80%93Thomson_paradox?wprov=sfti1) which mean that with more road throughput the traffic congestion will not decrease. The same goes with CPU power - with more power available we will be: - more eager to use it, so even not-so-efficient algorithms can be used “because we can use it” - we will provide more work for these CPUs, for example better game AI, more realistic physics, better graphic, etc. If you do not think that these are real problems then see on HN what is said about RAM: “I have 16 GB of RAM, so I do not see a problem that my editor is using 2 GB”, etc. We are getting sloppy because technology allow us to be, and this is huge problem. Oh, and if more power mean more work, then more work mean more garbage, and more garbage mean more power needed to collect it, so as you see this is not a solution. 
Won't the compiler automatically inline things when it makes sense? Also, the HashSet doesn't really do much in my code right now, just keeps track of duplicate abbreviations.
Thanks, some of my lists are a bit complicated for that, but that'll work great for a few of them :-) Another thing that might be nice would be a way to join together strings without actually creating a new string. This would be less memory for large data sets, but possibily more memory/processing for really small strings :-/
Garbage collection systems require computational resources to walk memory, find things that are no longer in use, and clean them up. When the garbage collector decides to run, it has to steal these computational resources away from the application. This unavoidably causes a slowdown of the main application. In a game, these slowdowns can manifest as a lot of different things like physics glitches where solid objects that shouldn’t pass through each other end up passing through each other. For drones, like quad-rotor vehicles, the vehicle itself is naturally unstable and relies on constant accelerometer measurements and rotor motor speed adjustments. These speed adjustments are sent to the motors by the flight control software sending electrical pulses to the motors thousands of times per second at a consistent frequency. Any slowdown in this process would at the very least manifest as vehicle instability; twitching, jerking around, etc. For self-driving cars, a slowdown in the simulation could mean failing to react to a pedestrian stepping out in front of the vehicle.
Asking why rust has become more popular than languages like scala, ocaml and haskell isn't a very useful question, since rust isn't really specifically competing with scala, ocaml or haskell. The projects that are using rust usually are projects that would have used C or C++. That being said, there is good work being done on building excellent web frameworks, which will further improve with the addition of async/await. This would put Rust in competition with languages like Java, Ruby, Python, and pretty much every language that has a web Framework. Rust is also a leader in support for wasm, so in the long run, Rust might even compete with languages like Javascript. Rust also incorporates a lot of modern language features, some of which are taken from languages like haskell and scala. As you can see, Rust is a very versatile language. But Scala also has modern language features, and it's also used for backend programming and can even compile to Javascript. But what's special about Rust is that it does all of this while being just as fast as C and C++. Traditionally, programmers had to choose between speed and safety. Safety coming in the form of some form of garbage collector. You talk about garbage collection like an advantage, but we mostly use garbage collectors out of necessity: before Rust, it used to be pretty much the only way to do memory management without the programmer having to do it manually (like in C and C++). Garbage collection incurs some performance overhead, and the latency of a garbage collector is unacceptable for some applications. Rust lets you have your cake and eat it too. In Rust you can have safety and speed (meaning both absolute speed and no gc). This safety is due to Rust's quite unique ownership model and the borrow checker. These same concepts also avoid a lot of problems with concurrent programming. Rust also doesn't sacrifice speed by having modern features. Rust implements these features so that they are just as fast as using traditional programming concepts. (iterators vs for loops for example) On top of all that, I think that the language is really nice to use. I use Rust even when I don't need it's performance, because I like the language, regardless of speed benefits. It is both nice and practical to use while having great features in place for building very reliable software, like excellent error handling and functional programming features. 
Yeah, I feel the same. The discussion on the ocaml forum is pretty much biased (understandably, they have different use cases). I find it weird from one of the Jane Street employees (I guess, it's not that clear from the discussion), they had to change their coding model with ocaml to get good "soft real-time" results. Writing code in a specific way to avoid memory allocations and running a lots of profiling. I feel that Rust avoids that complexity, and makes achieving that easier. So it's devatable if it's worth using ocaml, and writing it ina. weird way, or just go for a language where you can reason better about perf. Like with haskell, you can make it pretty fast, but it won't be smooth ride.
Just keep trying (and of course make the necessary improvements). This has to be at least my seventh time restarting the project already. Also, yeah, the other comments pretty much cover it. The place I learnt most of this from is https://craftinginterpreters.com
That’s good to know :)
It should also be noted that Jane Street uses Rust too.
There is one thing that seems to be constantly forgotten in the debate on GC vs non-GC; programs must deal with more resources than just memory. File-handles, sockets, window-handles and other expensive resources also have a life-time, and a GC offers no help in reasoning or modelling the lifetime of these resources. I've many times seen a program (my own and others) blow up in GC languages due to running out file-handles which was leaked in non-collected memory (many GC implementations does not offer any guarantees the a given chunk of memory will be collected, ever). C++ with RAII (qualifies for worst choice of name in programming history) and Rust lifetimes help model not only memory, but resources in general. It allows you to prevent an API from being called after failure, and state-machines where each action can only be invoked from the right state (both with compile time checking). Memory cleanup just happens to be solved along the way.
OK, let me rephrase that. I don't think Rust currently has a facility for tracking ABI versions and, before RFCing and implementing proper safety measures, making it easy to build dynamic libraries in this world where C++'s unstable ABI changes far less often than Rust's would be akin to having `const` and `safe` keywords rather than `mut` and `unsafe`. Rust is big on "default to safety".
The marketing isn't something that's officially done by the Rust team. The "evangelism strikeforce" (which has a strong presence in this thread) is something that's created entirely by the community because the like the language so much. I think that there is something about Rust that motivates some community members to become part of this imaginary evangelism strikeforce.
This is just an abstract impression, but seems like a reasonable view to me: Functional programming has very concise tricks with some flexible emergent behaviors, which is precisely what makes it so difficult for computers to constrain and create the kind of guarantees that makes Rust so powerful and efficient.
&gt; Where are you coming from to rust? are you coming from C/C++ or some high-level language background? and why choose rust rather than ocaml/haskell/scala? I'm coming from Python, because rust-cpython makes it easy for me to safely write Python extensions, and, on its own, Rust allows me to write resource-efficient, self-contained, quick-starting CLI utilities without the unsafety of C and C++.
I agree, and this is also valid in systems with other constraints. Consider, for example, a daemon that runs something that's not performance critical and doesn't occur that frequently. Maybe it updates the task bar with the currently playing song. 200ms vs 5ms, straight up doesn't matter. It could be 5 seconds and I wouldn't notice. The problem is that, when I am running something performance critical, why do I need to be running that and 5,000 other things, taking CPU and memory for no reason? In the past, C++ has largely been a pretty awful choice for most things. Package management is hard and type and memory safety is not guaranteed, so ordinary tasks like networking and multi-threading become a huge cognitive burden, blowing up the amount of code you need to write and complicating the build systems. Writing a one-off script to query Spotify's daemon and update the taskbar is a weeklong project. That's why I'd write it in Ruby, Node, or Python. What was a 400 line long program with a build system becomes a 20 line script. That, in my opinion, is the greatest beauty of Rust. For me at least, most of the time, it's almost as easy to write little scripts like this in Rust as it is to write them in Node (my most experienced language is JS). Sometimes, depending on the domain, it's even easier. The style is similar, the packages are robust and freely available, and safety is essentially guaranteed. It's no longer a trade off between dev time and performance. You get both. For free. So I think the mantra of "don't prematurely optimize" in this sense used to make a lot of sense, but now, interpreted scripting languages with good ergonomics and relatively poor performance have all but lost their purpose, and that's an excellent thing.
Rust targets and underserved niche. We're still using C for low-level programming. C++ allows us to write slightly higher-level code, but the low-level stuff is just the C subset of C++. Haskell, OCaml, ML, Scala - they all target the same thing - application software. Not only that, but so does Java, C#, C++, and a thousand other languages. Why is JavaScript popular? It's good for the web. Why are Python and R popular? They're good for data science. A lot of work has gone into making functional programming languages better over the years and I really like the innovations of Haskell. But there's a niche that needs to be filled and Rust is helping meet that niche. It's main competitor is probably Go (not Haskell, Scala, or OCaml), but Go seems to be filling more for servers and distributed programming than an in-place C replacement; again, possibly due to garbage collection. Rust takes what we've learned from functional programming and ports it a spot where we really are missing something in the programming world. I wouldn't even say that Rust is functional, btw. It just borrows the safety and correctness obsession from functional-oriented academics in order to solve the most common problems in low-level languages. The name Rust suggests what it is: a thin layer on top of the metal; ML-like languages aren't that. Personally, I think there's another really big gap that isn't fully solved yet: free parallelism and concurrency. Haskell's lazy evaluation and declarative style could lead to a language with automatic parallelism and concurrency, just like improvements in algorithms and compilers gave us automatic memory management. But even in that case I don't think that would fill the same niche that Rust does. So your question sounds to me like "why would you ever use a shovel when hammers are so much better?" Because we're trying to dig a foundation, not build the walls.
Is there much measurable evidence of the Downs-Thomson paradox in action in this way? I'm speaking anecdotally here, but I believe I've stayed in a relatively consistent price bracket on my hardware for a fair amount of time, and I've definitely felt performance consistently improving over time. I used to have computers that would stutter and struggle to display the characters I typed in realtime, but I haven't noticed that problem in a good few years. I would be interested to see if there is a significant degradation in the performance of basic tasks (like typing, browsing the internet, cursor motion, etc) over time. I certain agree that increased available RAM doesn't mean that we should be quite as comfortable using up that additional RAM as we are, but I don't necessarily think that this is as much of a disastrous situation as you're painting it here.
From the [*About*](https://krihelinator.xyz/about) page: &gt; The krihelimeter of each repository is calculated using the number of authors, commits, pull requests, and issues of that project, from the past week. So I guess the shown stats are from the last week? On the other hand your point still stands: Do the stats of the last week really say so much about the popularity of a programming language?
I believe anything that has more C-like syntax is always going to grow faster than something without it. Maybe is just my perception but projects that are programmed with functional languages start that way and stay that way. You would want to port things from Python that don't perform as good to Rust if you need to. But a Python programmer would not turn to Haskell. They probably wouldn't turn to Java unless they want to port they hole application (they just want to do a binding and optimize something). For me it is pretty simple: C-like syntax, C-like speed, GC benefits without JVM. Add-in a few tweaks to remove classes and inherintance problems, remove null related problems with functional-like types and suddently you have a language that appeals to functional people who want to get their hands dirty with low-level stuff. Make it compile and you will appeal to people who are doing really low level stuff. Make it safe and you appeal to people who are running critical applications. For quite a long time you had: * High Speed * Low Resource Consumption * Safe You could only pick two. Rust changes the game here. Rust is way more game-changer than some give it credit for. But for me the most important is that you can easily make it work with C. You can pretty much bind it wich C/C++ with a simple wrapper around the functions you want to access. Rust is not only a good replacement, it can be bootstrapped into existing code-bases at low and high level, while still offering added value.
Ah I see, that makes more sense.
I second this. It doesn't feel like the language team is primarily responsible for Rust's success. Haskell is also a great language, but doesn't have the same "reaching out" effort and so the language doesn't soar through the ranks. For Rust, the community, docs, and such teams are doing great work.
&gt;"thread-per-connection" model working at scale in the wild Most dynamic websites are hosted using a thread per request model with an event based reverse proxy in front. Using async IO at application server level is actually quite rare. &gt;I think simply spawn N threads with each thread sending a request and waiting for the answer Apache wrk is event based and doesn't wait. There are separate configuration flags for threads and total connections.
I hasn’t heard that! Anything public?
I'm speaking as someone who loves Scala, still programs in it frequently, and came to Rust much later than Scala: Rust hits a lot of the same points I love about Scala, which leads to me choosing it when wanting a native binary + little runtime: - Neither language is hamstringed when writing abstractions - They both have powerful compiler that can check useful invariants at compile time - They're both very pragmatic: while you can write your code very 'functionally', the language doesn't get in your way (and even assists!) when dropping down to a more imperative style is appropriate / an improvement *'But what about Scala native?'* ... Rust is closer to what I'd hope to see out of a language like Scala native than Scala native actually is. I care more about the features that make programs correct and maintainable than compatibility with Scala-JVM code... so Rust is OK as a replacement. I still prefer Scala as a *language* to Rust, but Rust hits close enough to home that I'll always turn to it over Scala when I don't want to deal with the JVM :)
I think I understand the use case for adding placement new, but I'm not a big fan of introducing a new syntax for this. I'm even less of a fan of how this feels like a step towards the [initialization nightmare](https://blog.tartanllama.xyz/initialization-is-bonkers/) C++ has. With placement new, would there be any reason for still having the old `fn new` functions for container types? It seems like you would always want to use the function that constructs your type T on the newly allocated space, rather than constructing it on the stack and moving it. &amp;#x200B; An ideal solution that would avoid adding extra syntax would be the language could guarantee that function parameters would elide a stack allocation and move, if it can be proven that the parameter will always be moved. This would be similar to how [C++11 can guarantee copy/move](https://en.cppreference.com/w/cpp/language/copy_elision) elision for certain return values. Unfortunately reordering and eliding input parameters would be far more difficult, or maybe even impossible in many non-trivial scenarios. 
Rust is not on the JVM and it supports parallel execution of threads?
As an aside, more cores doesn't necessarily mean faster games, or any program for that matter. Theres only so much you can parallelize, and especially in gamedev stuff has to run in a certain order. The cost of using and switching between more threads/cores could overwhelm the cost of the work itself too?
1. You should probably expand on this. I'm not going to claim that the collections in `std` are perfect, but they have been heavily optimized. There's always room for improvement of course but they should already be quite fast. 2. `Rc`s internals are an implementation detail which should be kept private. 
THANK YOU! The simple fact that you can invoke use-after-free-like bugs in managed language is a major reason why I don't use Haskell as much as I want to. Forgetting to close a file or unlock a mutex are exactly the same as memory leaks, and can cause more damage.
That's really unfortunate. If I wanted to install a library to be accessible system-wide, what would the recommended way to do this be then? Install the crate to a directory, say `/usr/share/mylib/crate`, and then have people use it via the path? I would use `cdylib`, but doesn't that mean using my library essentially as a `ffi` library?
For rust vs Scala/ JVM based language - start up + warmup time - SIMD instructions - easier optimization of hotspots : I regularly look at the generated assembly. This is possible but more difficult to do that in Java because of JIT - better control &amp; understanding of when static dispatch happens. - ability to build decent generic (no need to box integrals for instance) - my code is less error prone in rust : no null. Safer multithreading. The notion of non mutable reference exists. 
Sorry, this got a bit long, so TL;DR up front: There's a lot of different reasons, but they're about *popularity*, which is often relatively little to do with the functional properties of a programming language, and a lot of do with the non-functional properties of the language, such as community, ease-of-access, relation to different sectors of the programming language world, etc. --- 1. Rust has a big and active open-source community. In fact, I'd probably argue that, outside of perhaps Python or Ruby, it probably has one of the largest recreational communities of any programming language - that is, people enjoy the Rust community as more than simply a tool for answering work questions, but actually see it as a valuable community in its own right. You see this in the cute mascot that has been adopted by the community, and the fact that Rust seems to be one of relatively few programming subreddits that has its own spam subreddit (or at least used to - I can't find a link to it any more). 2. Ocaml and Haskell don't "onboard" as well. It's sometimes a bit hard to get an Ocaml project up and running, particularly when you move to things like dependencies, local environments, etc. Rust's `cargo` tool makes almost all of that a breeze - you have a local development environment that is simple to set up, but actually surprisingly powerful. In fact, I'd argue it's as simple and powerful as NPM, but manages to avoid a lot of the pitfalls that NPM can end up in. I'd argue that Scala also doesn't do this particularly well. There doesn't seem to be a single really good package manager for the JVM, and while a couple of options exist and work well, they are generally aimed more at enterprise software, rather than personal projects or startup strawmen. 3. It retains a mostly imperative paradigm. It's possible to do a lot of quite powerful functional programming techniques in Rust, but the more fundamental aspects of functional programming, particularly when it comes to type theory, are generally put to one side. Instead, Rust is by default a relatively imperative language, and also includes a lot of object-oriented concepts as well to round things out. This means that you don't have the fear-factor that I suspect affects a lot of languages like Scala, Haskell, and Ocaml. 4. Scala specifically seems to be losing a lot of ground to Kotlin, which takes a lot of the "syntax sugar" ideas of Scala, including making Java much more functional, but does it in a way that doesn't do weird things like add a billion and one new operators. It seems to be catching on much more in Java shops, and I think the functional community has never been hugely enamoured with the JVM in the first place, meaning that Scala has big "opponents" on multiple sides. 5. Rust is taking large shots at a seemingly wide array of existing programming sectors. There's the obvious low-level stuff (replacing C/C++), which has typically been the focus of the community. I believe Rust has also seen a certain amount of application in gamedev circles, with one relatively large studio suggesting that they want to work predominantly in Rust for the foreseeable future. However, there's also a lot of surprising applications - Rust seems to be increasingly targeting the webdev world, both on the server-side, and also (tentatively) on the frontend with Webassembly. On the other hand, Scala is very solidly aimed at the JVM world, which is generally server-side web development and some application programming; Ocaml hasn't had a huge amount of niches historically; ReasonML is pretty much specific to the frontends of larger web applications; and Haskell seems to be the domain of PL theorists, who, while an amazingly cool community of people, are not quite as large a group. Other people have talked about Rust's specific strengths, and I think those strengths are true, but I don't think they're why Rust is shining in *this particular metric*. I mean, other top languages include JavaScript and Python, which are both higher than Rust and share almost none of the strengths that people have talked about being specifically Rust's - particularly in the GC and efficiency arenas. The truth is, you're looking at what is essentially a metric of popularity. This doesn't tell you whether Rust is better or worse than any language for any specific project that you want to use it for. I'd argue it's still useful - high ranking tools here will generally have much more documentation, community support, StackOverflow questions, and general "googleability". Those are all desirable traits in a tool that you want to use for more than about half an hour. This ranking might even help if you want to know what skills developers are likely to have, so you can ensure you'll be able to hire people down the line who can actually work on your project. What they *won't* tell you, however, is which tools will produce faster code, which tools have shorter development cycles, which tools have "better" programming paradigms (where "better" here is about the most subjective decision you could possibly make, and couldn't be ranked in any meaningful way), etc. Just because a tool isn't in the top few places here, doesn't mean it's unusable (although I'd suggest being somewhat cautious about tools that rank too low, because (a) there's often bigger reasons for their lack of popularity, and (b) you'll struggle much more to find help on topics, recruit developers, etc).
No idea. I met someone who works there at a Rust meetup.
I feel like much software/libraries only needs to be written once in this world if done well enough. In order to aim for that library or software to rule, a performance difference 1x : 2x cpu (before parallelism), more in memory, seems significant. To start off with this potential is one thing that attracts me to rust. 
Yes. The classical banana peal is mmap. 
Put another way, Rust was designed to be the kind of language you could write a garbage collector in.
I wish it was possible to guarantee this optimization. However, for something as simple as `let x = Box::new(MyStruct::new())`, this would require part of `Box::new` to be called before `MyStruct::new()`, so it can allocate the space that `MyStruct::new()` would use.
Why would you be worried? With eager drops (NLL applied to object lifetimes), the Mutex would remain locked as long as you were using it and would unlock the moment it's unneeded. The most valid point I've heard against eager drops is using the Drop call to mark timing, which seems like a silly thing to do anyways and would be better accomplished with a closure.
You can still `drop(my_var)` manually if you need extra control over where exactly something is dropped in a scope (sometimes important around locks) and the borrow checker will be aware of that (as the drop function will consume what you pass into it).
Is there any difference between arrays and tuples except that arrays are limited to single data types? Are arrays faster than tuples? If not, why would I ever consider an array over a tuple?
&gt; I would be interested to see if there is a significant degradation in the performance of basic tasks (like typing, browsing the internet, cursor motion, etc) over time. The number of opened tabs in my browser definitely increases until its memory usage becomes unsustainable.
But can you open more tabs than you could, say, ten years ago? In my experience that is very definitely the case.
An anecdote: Python is very nice for quick, high-level programming, but is unsuitable for performance-critical tasks. Rust is an excellent complement to Python: its lack of GC, and C-like performance make it a nice companion to high-level languages. With these two tools alone, I feel like I'm not limited by language restrictions, for any task I wish to accomplish. Ocaml/Reasonml fill an intermediate niche, which I don't find as useful.
&gt; Not sure how true it is: " who have little interest in discovering the benefits of functional. " in the scala world, over time, people developed cats/scalaz/cats-effect/fs2 etc haskell style of pure functional libraries. It doesn't happen at the beginning, but once people taste a bit of benefits of functional, they might be intrigued, as I believe the rustaceans are inherently similar people like to explore new paradigms. Isn't just Rust's type system still too weak to reach this point?
Arrays are useful when you need many things of one type. Tuples are more like simple structs than arrays. Arrays aren't faster than tuples, but aren't slower either. Array values can be indexed by variables as in `for i in 0..array.len()`, or iterated over as in `for value in array` whereas tuple values must be explicitly referenced as in `tuple.0`. Arrays can be sized at run time, tuples cannot.
I'm not entirely convinced that Rustaceans are eager to switch to a more functional programming style. One reason is that some functional paradigms, like recursive data structures, are hard to implement and cumbersome to work with. Rustaceans will almost always prefer an array or Vec over a singly linked list. &amp;#x200B; Another reason is that pure functions and immutable data structures can introduce a lot of overhead. This overhead *might* be optimized away, but that's uncertain, and one of Rust's biggest strengths is predictable and reliable performance. Imperative programming corresponds more closely to efficient, hand-written assembly code.
If tuples are more like structs than arrays, is there any difference then between a tuple struct and an ordinary tuple?
Maybe something like: let input = match stream_data { // pass float data unchanged (and no copy) StreamData::Input { buffer: UnknownTypeInputBuffer::F32(buffer) } =&gt; { Box::new(buffer) // rebox } // convert integer input into floats StreamData::Input { buffer: UnknownTypeInputBuffer::I16(buffer) } =&gt; { let max = i16::max_value() as f32; Box::new( buffer.iter() .map(|&amp;i| i as f32 / max) .collect::&lt;Vec&lt;_&gt;&gt;() .into_boxed_slice(), ) } StreamData::Input { buffer: UnknownTypeInputBuffer::U16(buffer) } =&gt; { // snip, see I16 version } StreamData::Output { buffer: _ } =&gt; { panic!("Received output data on an input device"); } } // further processing of `input` &amp;#x200B;
Tuple structs are named types and can have `impl` blocks.
Maybe [https://vfoley.xyz/rust-compile-speed-tips/](https://vfoley.xyz/rust-compile-speed-tips/) can help a little. &amp;#x200B; But the compile time is still a big problem. In our project TiKV, when I want to build a release version for performance test, I have to wait at least 20+ minutes. So we have a joke that "you can have only 24 chances to build TiKV in one day" 
Writing my portfolio website with the backed written in Rust. As a beginner, still unsure how to serve static files in a not stupid and unscalable way. There are few files to serve, admittedly, but I'll be damned if I have to write a case for each resource
How do I get the size of a generic type? fn size_fn&lt;T&gt;(item: T) { const ITEM_SIZE : usize = std::mem::size_of::&lt;T&gt;(); } This gives the error: error[E0401]: can't use type parameters from outer function --&gt; src\lib.rs:3:48 | 2 | fn size_fn&lt;T&gt;(item: T) { | ------- - type variable from outer function | | | try adding a local type parameter in this method instead 3 | const ITEM_SIZE : usize = std::mem::size_of::&lt;T&gt;(); | 
I am not very sure this is the case when I use Chromium.
I've found using the inline macro has drastically increased performance in a few of my programs. I could be wrong but I don't think the compiler automatically inlines because doing so increases the binary size (but usually improves performance)
While I agree that this is a relevant point I think the exposition is biased. In many cases GC pauses can be partially avoided by avoiding allocating many new object, a technique used also in C/C++/Rust, where new allocation can be slow (interesly for the same reason :-) if I understand it right malloc/free is half of a GC and needs some cleanup at times) So to me it looks like C/C++/Rust offer better tools for realtime stutter-free systems and better handle complexity (like pointer spaghetti in C++) but carefully written managed languages can be better than many expect them to be. 
Totally agree. I am really interested in ocaml, but after trying for a week I gave up trying to have build systems and dependencies work together. There are at least four solution and all are terrible compared to both cargo for rust and stack for Haskell (only tried those two, can't speak for python/Ruby/js land)
I think the error message is confusing. It appears you can't use a `const` in that context. Try `let` instead.
I very much like F# and I wouldn't mind for ocaml to be viable but every time I've looked into it the windows support is floating somewhere between terrible and nonexistent and this is a huge deal breaker for me. Rust has had some struggles with treating windows well but it is light years ahead of most other emerging or less popular languages. I was excited about reasonml but it's too directly tied to the JS ecosystem for it to be useful to me. If I was writing a web app I would consider it, but I actually prefer elm which is quite obviously descended from standard ML as well.
The problem isn't actually the garbage, mostly --- it's the stuff that sticks around. A good generational garbage collector, like the ones employed by .NET and pretty much any other production-ready garbage collected language, will have little trouble allocating millions of short-lived values, so long as those values are inaccessible by the next sweep. The problem is when programs allocate values which exist for longer than the first generation but still need to be checked. Because the garbage collector has to iterate through the entire heap (of the particular generation being collected) each time, large generations turn into slow programs. The result is that, for small values and modern garbage collectors, you'll actually usually see better performance by enforcing immutability and allocating new objects rather than editing-in-place existing ones. Note that this is not true of large allocations --- I'm talking more like cells of linked lists than huge arrays.
&gt; and the fact that Rust seems to be one of relatively few programming subreddits that has its own spam subreddit (or at least used to - I can't find a link to it any more). https://www.reddit.com/r/rustjerk/ if you're referring to what I think you are. (I don't use Reddit enough to recognize the term "spam subreddit".)
I'm approaching Rust from significant Python experience because it seemed (and to me still seems) easier than C++ and I like how scoping works and how easy threading is. I'm sure threading is similar to in other languages but in Python I found it pretty nightmarish. I really enjoy Rust, and cargo makes things so easy its ridiculous
We use it at work and the reasons we chose it were (in no particular order): 1) community/documentation, 2) good type system and compiler that helps and not get in the way, 3) familiar feel to other languages our devs know (imperative like) but able to do some of the new stuff our devs are learning (functional like), and 4) fast enough to get what we want done. Background: we are a small web-dev team with people that know PHP, JS, and Python the most. About 2 years ago we adopted Elm for a front end project and loved it so we decided to do Haskell as a backend to mixed responses (some loved, some meh) when we needed to create another backend we decided find something we all could like and rust hit the spot perfectly. The rust eco system still in development but it’s settled enough we can get what we want done with the types and functional concepts we’ve loved from Elm and Haskell, but more familiar to those who are used to “C-like” languages. So far it has been a good experience.
Concurrent garbage collectors are difficult for a lot of reasons. For one, they require synchronized access, so that changes to program state in a running thread can be accurately and reliably observed by the garbage collector. Introducing a whole bunch of atomic operations and synchronization barriers into heavily threaded code can be a serious performance cost, and naive implementations are at risk of writing bad parallel code which is slower than good single-threaded code.
&gt; Is there some overhead to ok_or() constructing the error object, only to not actually use it? That's up to you. If it generates a backtrace or log entries, or connects to the Federated States of Micronesia to upload diagnostics, yes very much so. If it's just an enum variant, no, no more than any other literal value that's just hanging out in the compiled binary.
&gt; Rust is a rather harsh language for amateur data structure implementation.
&gt; And i don't want to bash on electron here Oh, no, please do!
Nah, it's used by my electron text editor and slack, mandated by higher ups to use.
Rust's major draw for me has been that the vision was always very clear&amp;mdash;it is a systems language with a focus on correctness *and* practicality&amp;mdash;and that's basically exactly what I want in a programming language. I *would* prefer if it had better support for functional features (e.g. HKTs), but I think the correct decision for the language's stated goals was to focus on other more pressing issues instead (though I desperately hope that ergonomic HKTs do come along). OCaml and Haskell have been (for me) relatively hard to get started in due to lack of availability/quality of documentation, unusual/poor tooling, focus on pure FP, and a general lack of self-promotion. To be clear, I *like* type and category theory, and I like reading about PL stuff, but that's not a good way to sell a language to me as *productive* (though that's not to say it implies the opposite, either). I would have been much more interested in actually using Haskell or OCaml if I had read articles that talked about solving moderately complicated problems without devolving into yet another explanation of typeclasses or currying. I love Scala, but it's unwieldy&amp;mdash;it suffers massively from a lack of focus. Overlapping/non-orthogonal features have been added onto the language, seemingly without concern for what the language *should be*, but rather based on *what can feasibly be added*. Metaprogramming (via macros) has been in the language for years but in a semi-complete state (only to be ripped out again and rebuilt). It can be relatively slow compared to other JVM languages. I have a lot of fondness for it, but I've stopped using it recently in favor of Rust and Kotlin.
&gt; Rust is a rather harsh language for amateur data structure implementation. I mean, yes and no. The biggest gotcha is when somebody jumps to the unfortunate assumption that C data structures can be implemented in Rust with references. That's going to be a bad day, period. If on the other hand you're trying to implement a pure-functional data structure using `Rc` or `Arc` pointers, well, it's not *quite* a walk in the park, but everything works like you'd expect. However the equivalent problem with C is that mere human beings have a hard time implementing data structures under the assumption that C pointers are easy and straightforward to work with. And Rust *has* C-style pointers and C-style lack-of-guard-rails anyway! Unsafe Rust is comparably harsh to C, and that's exactly the language which this stuff has been taught in for decades. Honestly, the real problem is that we (meaning "amateur programmers") are still expecting 60s and 70s programming texts to have currency when the *real* cutting-edge of C is using slab allocation and read-copy-update and so on - things that work *very well* in Rust with little or no unsafe required. Honestly, I'm finding that with a little imagination `BTreeMap` and `BTreeSet` make excellent (and superior) replacements for linked lists and even possibly for tree and graph structures. And that makes sense - what is a file system if not a huge persistent collection? And what are cutting-edge filesystems? *Big B-trees:* btrfs, zfs, xfs (to some degree). It does require a shift in perspective (your logical "data structure" is implemented on top of one or more virtual-memory data structures) but that decoupling is very good for both correctness and performance. 
I got a little block of code here that's giving me trouble. I'm trying to cycle through all messages currently in the receive channel of a mpsc link. &amp;#x200B; `self.receiver` is of type `mpsc::Receiver` and its holding a tuple that contains a Sender (to get back to the calling thread) &amp;#x200B; the issue that I am having is that the first line seems to be holding onto its borrow of self, even tho `received` is not a reference type let received = self.receiver.try_iter(); for rcv in received { let key: String = rcv.0; let tx_return = rcv.1; tx_return.send(self.track(&amp;key)).unwrap(); } compiler error: error[E0502]: cannot borrow `*self` as mutable because `self.receiver` is also borrowed as immutable --&gt; src/engine/clock.rs:49:28 | 44 | let received = self.receiver.try_iter(); | ------------- immutable borrow occurs here ... 49 | tx_return.send(self.track(&amp;key)).unwrap(); | ^^^^ mutable borrow occurs here 50 | } 51 | } | - immutable borrow ends here error: aborting due to previous error Is received holding onto the reference because the return type of try\_iter doesnt impl copy/clone? Is there any way around this?
Thank you! Can you explain what you mean by using macros to make the implementation easier?
Hi there - so I'm starting out and got through that chapter on string slices and was playing around with them to understand them, but I think I confused myself more. We have the following string literal that's stored in the executable itself after the code is compiled and it has type `&amp;str`. ``` let s = "hello world"; ``` Slicing seems to be a function on `&amp;str`, returning `str`. However, when setting that to a variable, we're told the variable doesn't have a size at compile-tile and to consider borrowing instead. ``` let a: str = s[0..5]; // doesn't compile ``` Why does slicing return a `str`? From everything I've read online, `str` is only ever usable through `&amp;str`, right? Why doesn't slicing just return a `&amp;str` already in this case? Further, when borrowing and slicing, how am I supposed to interpret and read the code? For example, the following: ``` let a = &amp;s[0..5]; ``` can be parenthesized as `&amp;(s[0..5])`. Am I supposed to read this to myself as: &gt; I have a reference `s` (or as I understand a memory address?), and I'm going to look at the first five consecutive values stored from that address `s[0..5]`, and then I want another reference to the resulting `str` (whatever that is). If a reference is just a memory address, and slicing produces a `str`, can we interpret `str` to be like a range of addresses? And that's why we have to get another reference to it? --- Finally for fun (and maybe not necessarily related to slices), I tried to parenthesize the other way - that is `(&amp;s)[0..5]`. This was also `str`, since we're slicing again. What surprised though is the compiler telling me to consider borrowing! ``` let a = &amp;(&amp;s)[0..5]; ``` And that worked! It's as if there's an implicit dereference going on since it behaves just like `&amp;*&amp;s[0..5]`? I've noticed this also happen when printing string references. For example, the following two lines behave the same: ``` println!("{}", "hello".to_string()); println!("{}", &amp;"hello".to_string()); ``` How can I know when Rust will implicitly dereference something for me? Do I have to know or even care? Should I just swallow this quirk for now and continue reading the book and at some point it'll all just make sense?
got it, thanks!
There might be cases where a mutex is used to lock something other than it's contents. That would be kind of weird in pure Rust, but it might come up more when wrapping a C library that isn't re-entrant?
As a fan of both rust and scala- I'll try to give my two cents: 1) Rust is an amazingly well designed language with a very small set of constructs and library. Scala is an amazingly well designed language with multiple ways of doing the same thong and a huge library, both of which get a bit tiring. 2) Rust is developed in the open and this has helped attract the technical crowd. Scala too but the public input in Scala is much less than rust. 3) no offense but somehow rust has gone viral with the "oooo ✨ shiny" crowd. So you hear about it a lot from people who themselves will have trouble with basic stuff That said I don't think that (unfortunately) rust is more popular than Scala. It's like mac vs windows. If you asked on popular programming forums you'd think everybody is on a mac but stack overflow and other more professional surveys put windows ahead of mac. I think rust and Scala could both learn from each other-,rust can learn to not repeat scalas mistakes (and I think it has) and using square brackets for generics and brackets for array index. 
`#[inline]` is a compiler built-in attribute, not a macro. Normally the compiler *does* inline automatically- but only within a crate. So the attribute is useful for non-generic functions that you want to be inlined in downstream crates.
I have seen occasional bug reports where the implementation didn't match the spec, but it appears that these issues have been fixed. The parent commenter may have been working on old information. (Note that `handlebars-rust` has been in development since before Rust 1.0, so this is quite possible.)
Cool! I’ve always loved using it.
&gt; You should probably expand on this I [expanded on it here](https://www.reddit.com/r/rust/comments/4dd5yl/rust_vs_f_hashset_benchmark/). &gt; they have been heavily optimized Rust's hash table based collections are substantially slower than .NET's `Dictionary` and `HashSet` because Rust uses power of two numbers of buckets whereas .NET uses prime numbers so .NET can get away with much simpler and faster hash functions. In contrast the Rust collections either suffer from lots of collisions or the overhead of an expensive hash function. &gt; Rcs internals are an implementation detail which should be kept private. The thing referenced by an `Rc` is publicly available, of course. If you could pattern match over it (as you can in any other language with pattern matching including Swift and Mathematica, which are both reference counted) then Rust's pattern matching would be far more useful because it could manipulate arbitrary trees and DAGs. 
Author of Maud here. While I mostly agree with your points, I'd like to clarify a couple of things: &gt; I would also be very surprised if Maud is faster than horrorshow and Askama. I'd be surprised as well! We should note, though, that while Maud's performance matches that of Horrorshow, both Maud and Horrorshow are somewhat faster than Askama. It's not a big deal in practice but we should be precise about such claims. &gt; I don't want my templates in my source code Maud templates are just Rust. If you don't want them mixed with business logic, then put them in a separate module! I don't want to tell users how to structure their code, and recent developments like React/JSX have shown us that it's not always so cut-and-dry.
So I'm an innocent bystander with an interest in rust and, just judging from "Hello, world" examples it seems like a modern version of C (don't hit me please). I picked up a Rust book today and plan on diving in it and getting competent (and hopefully professional) with it and liked the ease of setting up a simple environment. &amp;#x200B; Having an easy-to-interpret-and-use package manager was a boon for me as well.
Popularity is driven not by the language people are running to but the language they are running from. Scala is popular because many Java programmers want something better on the JVM. Rust is popular because almost all C++ programmers want something better but still without a GC. &gt; Where are you coming from to rust? are you coming from C/C++ or some high-level language background? and why choose rust rather than ocaml/haskell/scala? I'm coming from BASIC, Pascal, Logo, assembly, C, C++, UFI, Standard ML, Mathematica, OCaml, Java, Lisp, Scheme, Scala, Haskell, F#, Clojure, HLVM, Elm and Javascript but I'm not coming to Rust. I've played with Rust a few times and found that the inability to pattern match "through" an `Rc` makes it really hard to write most of the code I write so I cannot see myself making any serious use of Rust any time soon. If that issue gets fixed then there is a small chance I might try it in production but I see only disadvantages over OCaml/F#. 
You could use a macro to generate the body of the struct Something like #[derive(NumericBounds, Debug)] #[numericBounds(high = 0, low = -23)] struct Neg23{} the macro could define the body so you're not repeating yourself if you're making different types with different bounds
This is one of the perils of syntax sugar - it hides the full operation and can make things like this a bit confusing. `&amp;s[0..5]` desugars to `&amp;*s.index(0..5)`. Note the dereference operator in there. `str::index` (the implementation of slicing on strings) does return a `&amp;str`, but because of the deref, `s[0..5]` actually has the type `str`. That's why you need the `&amp;`, to turn it back into a reference. This is why `&amp;(s[0..5])` also works. (I believe this dereferencing is to help other uses of the `[]` sugar; it would be confusing if `my_vec[3]` returned a reference to an element instead of the element itself.) &gt;Why is the size unknown in the first place if we specified the first five characters by s[0..5]? What would happen if you did something like: ``` fn foo(s: &amp;str, size: usize) { let x = s[0..size]; } ``` How big is `x`? We have to know the correct size at compile time since it has to be stored on the stack, but it's only determined at runtime. &gt;It's as if there's an implicit dereference going on since it behaves just like &amp;*&amp;s[0..5]? There is. To see how this works (and to make it more predictable,) let's desugar it. We have `&amp;(&amp;s)[0..5]` -&gt; `&amp;((&amp;s).index(0..5))`. Rust tries to look up an `index` method for `&amp;s` (a `&amp;&amp;str`); it can't find one so it attempts a deref and tries again (finding one for `*&amp;s` as a `&amp;str`.) The general rule is that Rust does this auto deref when calling a method, and it will do as many derefs as it can (and up to one ref) to find an implementation.
As a way to learn, I’m developing taskwarrior alternative with note making capabilities.
You probably meant to post this in /r/playrust. This subreddit is for a programming language called Rust, not the video game Rust.
Ah, I see! Thank you :)
&gt; That's really unfortunate. If I wanted to install a library to be accessible system-wide, what would the recommended way to do this be then? Install the crate to a directory, say /usr/share/mylib/crate, and then have people use it via the path? To be honest, I'm not the person to be asking that particular question. The only situation where I personally have a use for dynamically-loaded libraries is plugins which *do* need to be compatible across versions, so I haven't given much thought to best practices for your use case. &gt; I would use `cdylib`, but doesn't that mean using my library essentially as a `ffi` library? More or less. I've always felt there should be some sort of binding generator which automates the process of adapting Rust to the C ABI and the back again (similar to how `rust-cpython` hides the fact that your Rust and Python code have the much less expressive C ABI in between), but there's no way I trust myself with `unsafe` enough to write it.
Systems with real-time critical timing.
Random delays causing stutters in processing would be terrible for self driving cars. If you can't understand why...
I think you are right. Rust feels a lot more like C, where you are applying advanced C best practices, than C++. 
This is fantastic. Thanks much for sharing, and for the detailed writeup. &gt; Basically I had to write code sort the draw calls by Z coordinate, not just depend on the depth buffer to handle it all. Couldn't you have just put the pollution at a higher Z than the building? Or am I confused?
I’m following the Roguelike Tutorial. https://github.com/ShriekBob/rustrl Plan is to follow it to completion and then move wholesale to a Spec based pipeline. Probably gonna try and wrap as much of tcod-rs as possible cos I might swap to BearLibTerminal. Or tile based. Not sure. Once I’ve got that down I’m gonna turn into a Roguelike Crate and start writing a few small roguelikes. 
Rust has cargo, and lots of libs with good doc. When i need to write stuff more fast/small/predictable than my clojure ability allows me, I use rust. (for now monitoring agents and game engines) I don't see why I would use haskell/scala/ocaml. They have no relevant advantage for me.
I was confused :) Can’t used to it. 
Does it have engine?
Working on [ggez](https://github.com/ggez/ggez/), a lightweight 2D game framework inspired by Love2D. With the closing of [The Great Transform Issue](https://github.com/ggez/ggez/issues/435) I'm pretty sure that's the last major feature for 0.5 implemented. I'd wanted to get at least a release-candidate up yesterday so I could at least say it's kinda-finished in 2018, but that didn't happen. Oh well! But that was still the last Big Blocker issue, and so now it's all down to cleanup, documentation, and fixing whatever bugs I'd forgotten about (and a few that I haven't forgotten about, but are platform-dependent...). I'm probably going to give it a good hard going over in the next day or two, make a release candidate, then settle in to work on the harder bugs. Some of them are unimplemented features in dependencies (`gilrs` and `winit`), so those... miiiight or might not ever get fixed by me. Whew! Frankly, after `ggez` 0.5 is released, I'm going to be taking a long heckin' break from new features. Maybe a semi-indefinite one, I dunno. I really want to work on a website that gives you useful quality metrics for whatever crate you care to look at, and dpc's work on `crev` is fascinating, so that might be my project for the year. On the other hand, `gfx-hal` is released and writing a good 2D graphics engine in it sounds like fun... What I REALLY should do is just make smaller releases for ggez. ¯\_(ツ)_/¯
Could you just extend Cargo to handle Python code too? :D
Depending on the framework you're using there is likely some support for serving static files out of a directory already. Or just throw it behind nginx and have it only direct the non-static requests to your backend.
Rust is the first decent functional systems language. If you are just doing applications programming and you want functional and to avoid the JVM, take a look at Elixir. Elixir makes building beautiful, powerful, concurrent code very easy. It is my favorite language. It is garbage collected though, so you have to watch out for performance issues there. Unlike the JVM though, Elixir garbage collection doesn’t usually cause latency issues. If not optimized memory can and does spike quite high before being garbage collected. 
So the reason the order of drawing is important is because it multiplies the alpha on whats currently there. Since the dam sprite is conpletely covered by the animation, it didnt render at all. http://www.opengl-tutorial.org/intermediate-tutorials/tutorial-10-transparency/ I didnt use a game framework, but used a lot of the same crates that amethyst leverages. Gfx pre ll for rendering, just supporting OpenGL. Specs for the entity component system. Glutin for window/input. Rusttype abd gfx glyph for font. Rodio for sound. 
That's why you use provided syntax support for things like that, ala python's with statement: Lifetime is explicit.
I'm using hyper... I'll look into the documentation! And I've never used nginx so hopefully I can figure out a little about what you suggested
That’s what I did to overcome my fear :D
GC requires extra runtime code. If I want the minimal possible runtime code, it matters
Gist Link: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=95e3873c8b78ee519b25c0e36e169333 My question was pertaining L33 and L34, it seems like the borrow checker complains once I move a field out a struct, it doesn't let me move a field out of it again, it thinks the entire struct is "invalid". I guess this is the borrow checker being aggressive, what is the right way to go about this ?
While this would be a good macro to have, it's not what I meant. I meant something like impl_operator(NonZeroFloat, Add, add); That would implement the add trait for that type
That's the one! I couldn't remember the name, or the X_jerk formulation.
Hmm, I can’t come up with a good example right now, which might suggest you’re right. I thought of cases where two things logically belong together (and shall be done atomically), but what is not expressed through the type system.
Really? You just put your creds in .pypirc and then `./setup.py bdist_wheel upload`, the upload itself takes a matter of seconds normally, I think.
The more you learn, thanks!
I code for a living in Scala, and I started playing around with Java in 1995. Java/Scala have been great for productivity in complex business applications. Rust is not like Java/Scala, despite the fact that Rust syntax and the way you think about things in Rust look and feel an awful lot like Scala to me. C/C++ is powerful and fast. These languages are generally harder to work with, and they allow certain types of difficult to debug errors that simply are not possible in many other languages. C, in particular, lets you shoot yourself in the foot in any number of ways. But if you want the tightest, fastest code possible, running in small memory, with maximum power efficiency, you go with C; and C++ is a close second. Rust gets you very close to C performance (LLVM C compared to Rust), it is closer to C power efficiency than C++ is, but it prevents you from shooting yourself in the foot (it is safer than Java for threaded applications, in fact). You get reasonable productivity, you get incredible safety guarantees, and you get startup-speed/execution-speed/low-resource-usage. Before Rust, the general solution was to code it in anything-but-C unless you absolutely needed C - because C is expensive to do right. With the advent of Rust, I think people are starting to rethink that rule of thumb. For the first time in my career, it seems, low-level control and productivity are no longer mutually exclusive. Interesting times we live in. &amp;#x200B;
Yeah this is really a show stopper for me. I think I'm just going to end up writing my code in C. At this point I've had to write bindings for a C library (the library I had hoped to compiled into a shared library). I was then working on writing a safe wrapper for it (which would also preferably be a shared library). The reason I wanted to be a shared library is there is a C library I wanted to bundle the rust wrapper with, and have the rust compiled when the C library is compiled so they stay consistent. Part of the problem is the C library is huge, and also comes with a kernel module, so it doesn't make sense to statically compile it with the bindings. If the only way to interface with the code is through a ffi api, I don't think it ends up being worth it. I guess I'll have to find another rust project :(.
Oh, okay. Thank you for the clarification.
Not a criticism per say, but did you think about hiring an artist? I think more than anything, that is what would hold this game back.
\`with\` and \`bracket\` do guarantee the resource goes out of scope, but they're completely optional and they still don't prevent a reference from being passed outside the scope.
The ones that stick around have to be accessed frequently just to check them, kind of ruining virtual memory management. And think cells of linked lists of case classes containing a string. That's at least 5 dereferences to different objects right there (the char array part of the string is apparently treated as a separate object). We've got massive amounts of RAM and dozens of cores running at multiple gigahertz, so we can usually get away with this. When you are trying to run something like a large in-memory database, these inefficiencies are killers! :)
The gist you posted does compile on the playground. Maybe locally you are using 2015 edition. The error that I see when compiling with 2015 edition is that you are moving field not out of `Node`, but out of `Box&lt;Node&gt;`. `Box` is a little weird like that - while it is special and the compiler knows what it is, it seems that it still used to be a bit rough on the edges when compiler inserts automatic dereferences. On 2015 you can work around it by manually deferencing the box on line 31 - before you try to move any fields out. 
randomize-2.0.0 is out on crates There's a 2.1.0 to soon follow (bounded randomization is currently missing), but putting out a release on Jan1 was more fun.
I'm a Rust fan and work at an energy tech company, so this is a great combination of worlds. Any plans for a Linux build? I don't use Windows or Mac OS so haven't been able to try it out yet.
Agree. A colleague of mine said just the other day, "The problem with Scala is that you can't really learn the language and ecosystem from online resources." Scala Although there are lots of print books for sale on Rust, and classes are coming along, it is possible to come up to speed and get productive by reading "The Book," optionally the "Rustinomicon," really good online documentation, videos on Youtube from the community, and using freely available IDEs (VSCode, IntelliJ Idea, etc.). I bought the paper version of "The Book" to support the authors, but I read the online version because it is more up to date.
I don't buy this. There are games which have terrible art, but are still fun, and thus popular. Dwarf Fortress is a far out example. Command: Modern Air and Naval operations is less far out (actually, many games published by Slitherine/Matrix fall into this category). Point is: deep gameplay can make up for unappealing art, but great art cannot make up for shallow gameplay. If you are bootstrapping game production, I don't see what the issue is with releasing early versions which do not have great art, but instead are meant to draw an audience around something much more fundamental. 
that's was true some years ago, but modern GC thrends can run concurrently with the user program. They only need to stop the world to snapshot the stacks, so they consistently keep pauses in the order of units of millis. See [ZGC](https://www.opsian.com/blog/javas-new-zgc-is-very-exciting/) or [Shenandoah](https://2018.javazone.no/program/28f62968-0713-4bda-8a37-8b38c9f280ee). These GCs are not perfect. They have slower pauses (therefore better latency) but throughput is usually worse (in worst cases, up to 20% slower).
That's just dealing with the uploading part after you've done all of the heavy lifting converting it into a pypi package with distutils/setuptools/etc. - I find it pretty gross, i.e.: - create __init__.py(s) if not already created - create setup.py - create source distribution ( do not believe they are cross compatible) - potentially a bdist egg, which only is good for your os and your python version - potentially a manifest.in file to include documentation or data files And then you can. Upload to pypi using pypirc All you really have to do to prepare uploading a rust crate is putting the license type (most other metadata fields are auto populated) but of course so far I haven't seen any issues with cross compatibility, distributions, etc. And to conclude, a local python script/library file layout would look pretty minimal to what's required in a pypi package -- which is the reason that I only develop scripts in python 
You touched on this a little bit, but it isn't just faster. Rust compares very favorably with C/C++ in memory efficiency overall. [https://sites.google.com/view/energy-efficiency-languages/results](https://sites.google.com/view/energy-efficiency-languages/results)
Weird. My version is fairly recent. penguin :: ~/projects/my-bst » rustc --version rustc 1.30.0 (da5f414c2 2018-10-24)
That ordering issue is completely normal. Sort opaque objects by nearest first, then transparent objects by nearest last. Most game engines just abstract that away from the user.
Don't look at Rust as wannabe-functional. That's not the design goal. The design goal is to use C best-practices and be able to apply high-level, zero-impact programming constructs on top of them. So, for example, you can apply filter and map to an iterator. But underneath, that is basically a C array of fixed-width structs, linear in memory, no dereferencing; and all the little closure functions are inlined. In other words, it looks kind of like Scala, but it acts like C.
Ah, I knew there was some other common use I were forgetting!
`with` cannot be passed in a structure, somewhat reducing it's use
&gt;They only need to stop the world to snapshot the stacks, so they consistently keep pauses in the order of units of millis. This is what I was referring to when I said "There are implementations that pause the main thread for a bit and then do more work on a separate thread, but that still means you have some pauses on the main thread."
Dwarf Fortress is the Mariana Trench of deep game design. If you think your game is even in the same ballpark as that one, you're simply wrong. Just compare OP's game to [Factorio](https://steamcdn-a.akamaihd.net/steam/apps/427520/ss_2d959b810950c93a61c82ed2e15e586584fd4aae.1920x1080.jpg?t=1530541487) or [Opus Magnum](https://steamcdn-a.akamaihd.net/steam/apps/558990/ss_92c30b0f0d8737cfe8c0b70364bbb3712e2bdd43.1920x1080.jpg?t=1542651953). They're very similar, but which one would you buy? Keep in mind that graphics are pretty cheap, artists are willing to work for nearly nothing compared to a programmer. 2D graphics are also very easy to integrate.
Dwarf Fortress really can't be used as an example because of it's vast complexity that offsets its graphical situation. I'm not familiar with "Command: Modern Air and Naval Operations".
You don't have to statically compile the C library into the Rust build artifacts. It's only the Rust-&gt;Rust links in the dependency tree which have an unstable ABI. There are plenty of Rust bindings which dynamically link against C libraries.
But these pauses are so small they are not really a concern. Different malloc/free algorithms sometimes need to do complex stuff managing their datastructures that "block" (in the sense of not executing your logic) your main thread. The stop the world problem of GCs is when they need to block for a time that grows with your heap size. Of course these new GCs have some other problems: They are not repeatable and know nothing about you logic, so they need to add several barriers and walk through the alive objects, which is, in some ways, a waste of resources
Hah, neat, so you can basically make a function's parameters lazy on demand? Hmmm...
&gt; the damn sprite FTFY
I've used ocaml for games, albeit a long time ago. The GC pauses at the time were noticable in real-time games, like the platformer I was trying to make. The game was jittery, and there just wasn't much I could do about it without abandoning a functional programming style, at which point I felt I might as well just use C#.
No sorry, not yet. My project is quite uninteresting at this point, the library (not mine) is usefull though. 
Has anyone from the OCaml community watched someone new to the language try to do things lately? The user experience is horrible. There are thousands of gotchas and the primary onboarding experiences, like going through Real World OCaml, don’t explain them. Some quick, surface-level examples off the top of my head: * Getting merlin working with your editor is a hard requirement of using the language with real code and is a huge pain * OPAM is more like the archaic CPAN from perl than gem/pip/cargo and recovering from breakages is complicated * Like C++, there are multiple standard libraries and the most popular is not the official one The language’s design itself is wonderful and I have few complaints. I’m all for their type system, for example. But, using the tools and libraries surrounding the language has a steep learning curve only made more difficult by its syntax (for better or worse, C-like syntax is the gold standard for the average developer). Rust, by contrast, has a significantly better and more streamlined ecosystem. If you’re coming from other languages, figuring out the borrow checker is basically the only meaningful barrier to entry. Everything else works as you’d expect. I have no idea if this has anything to do with overall adoption, but it certainly affected the amount of time I spent with Rust over OCaml.
Factorio has bigger and more complex machines, but Opus Magnum can be more elegant in its simplicity and symmetry, so... both?
&gt; implementation complexity reasons The main implementation file of PyUnicode is a 20000-line monster containing multiple repeated template header inclusions for the three string variants. Granted, a lot of that is genuine Unicode complexity (character conversion, case folding, ...), but I can't help but think that the implementation complexity of building index lookup tables would very quickly pay for itself by removing the implementation complexity of writing every single string routine three times, effectively.
Built and ran it, and yeah, it does seem to be leaking memory (I'm on Win64-msvc toolchain). Have you tried to make a minimal program that reproduces the problem? That'd be easier to review than the whole program.
nmv, I just saw the cut-down program on SO. Also leaks memory!
For something like this, it doesn't need fantastic art... it needs *recognisable* art! Dwarf Fortress wouldn't be nearly as good without the huge variety of glyphs it can use. All you really need is to have the different elements be distinguishable from each other. I can clearly see things like rivers, swamps, pumpjacks, and solar panels in the screenshot, but I can't well what anything else is on the screen. The best thing you can do in terms of artwork for this is too make it easy for the player to tell at a glance what's going on and what's what. Being pretty isn't a requirement.
For `NonZero`, it should be possible to use `num-trait`'s `Zero` trait and `Zero::is_zero()` method. Then it can be generic over all numbers.
The Pony Programming Language's ORCA garbage collector does concurrent garbage collection without any synchronization as I understood it. Pony is a very interesting beast, as it provides almost identical guarantees as Rust, but takes a very different approach by providing a garbage-collected runtime with a built-in "cache-aware work-stealing scheduler". It is compiled, uses the actor model for concurrency and is object oriented, although it feels closer to Rust than Java or C++ in that regard. Unfortunately it isn't very popular, but a very interesting project nontheless - especially if you appreciate the guarantees that Rust provides. There are some papers about the subjects in their [website's repo](https://github.com/ponylang/ponylang-website/tree/master/static/media/papers) if you'd like to learn more.
&gt; the order of units of millis Do you mean to say "fractions of millis"? Because a 60fps game has just under 17 millis to spare for each frame; multiple millis simply missing every now and then will cause occasional frame drops or lag, i.e. is not acceptable.
This looks cool, but maybe releasing this on New Year's Eve needlessly limited the exposure for this project.
nope. The longer stop the world phases of these GCs are &lt;10ms. These GCs are great, but they are not designed to be used on games that have to run at 60fps!
Do you have the source available?
If you want to see electron bashed, go to /r/programmingcirclejerk. This isn't the place.
I think it's because Rust is like escaping steam. The boiler is the C/C++ world, where technical constraints don't allow people to escape. The heat are the shortcomings of those languages, most particularly unsafety. (That doesn't mean only C and C++ programmers are affected. So are all those higher-level language developers whose applications have weird bugs because of unsafety in some underlying native library, or simple users whose applications crash.) With Rust showing a possible solution to the situation, there's some steam escaping the boiler with a loud hiss. Programmers see Rust and think, "Hey, maybe things *could* be better, if only **everything is rewritten in Rust**!" And then they go and tell everyone about their vision. Unfortunately, in quite a few cases, and especially during the initial hype wave, this sounded like "Everything is shit, you need to use Rust," which resulted in the backlash you could expect. It is not even relevant to this situation whether or not Rust can fulfill these expectations.
"Only 2-3 times slower" Don't get this wrong but something just broke in me :&lt;
I'm curious to know what in the development prevented release to Linux?
If all of your content is static, you might be best off using a static site generator. Zola (http://getzola.org) is one written in Rust (although you don't get to write much Rust when using it - it's mostly HTML/markdown), alternatively Hugo (https://gohugo.io/) is written in Go and a bit more mature/fully feautured. If you do want to stick with a dynamic Rust backend, then Rocket (http://rocket.rs), Actix-Web (http://actix.rs) and Gotham (https://gotham.rs/) frameworks will all give you static file serving middleware out of the box.
&gt; But these pauses are so small [millis] they are not really a concern that's still too long in some domains (e.g. automated trading or robotic-assisted surgery). Sometimes the pauses are non-deterministicly scheduled and that's even worse than the pause - you take a hit when it could hurt you the most. &gt; Different malloc/free algorithms sometimes need to do complex stuff managing their datastructures that "block" (in the sense of not executing your logic) your main thread. That's why being able to avoid using the heap is important sometimes. These same programs would never allocate memory on the heap in code that has to be fast and deterministic.
Sure! sometimes a GC is prohibitive. 
I found the source and built it on FreeBSD. Seems to work fine except that I had to disable the audio bits as cpal wants alsa, which I don't have installed.
That's an odd simile, considering you can absolutely use Rust for basically anything you might want a compiled language for and have libraries for. I question what you mean by "low-level", given that C++ and Rust generics and standard library containers allow higher-level coding than C with minimal run-time overhead.
You need to define common module above you bin module. Then import it in your bin/whatever.
Had a quick look at the Piston code, but nothing jumps out. There's some unsafe blocks, but no obvious problems. You've got a solid repro, so I guess file a bug with Piston.
A possible example: locking access to a non-threadsafe external C library via a mutex.
2018 edition was released with rust 1.31, which came out on 6th of December. 
Factorio is also a game that could need an artist. I'm colorblind, I've made *so* many mistakes due to the same art being used, differing only in color.
Nice work ! Can you explain your choice of publishing plattform. Im guessing you thought about Steam as well? Im interested to know if i publish something my self. The game also looks suitable for smartohones. Have you considered porting? 
I had a quick Google search to learn more about the issue you are talking about, and if I understood it correctly it's solved. Looks like the uuid's version you are talking about is 0.7 and there is a [merged pull request](https://github.com/diesel-rs/diesel/pull/1861) to update it to that version. Haven't tried to start a project with these libs myself, so I'm interested if this indeed fixes your issues.
I'm not sure what you mean by “under”. If I make move bin so it is src/commoncode/bin... then the compiler doesn't look at it at all. 
* \_\_init\_\_.py is just a standard pythonism, nothing to do with pypi * creating a setup.py file isn't really any harder than a Cargo.toml file. * unless you're shipping binaries, you should be able to just use a wheel distribution, as of a few years ago (hence the previous comment). * I agree, the manifest is a small pain if you want to include that stuff. One unfortunate thing about cargo is that it doesn't support restructured text. It's understandable, because there's no rust version, but rst is a much nicer documentation format. 
This is awesome, @matthieum! Are you willing to submit this as a PR?
I have a string that looks like this \`this is a string\\n\`. They contain \`\\\` and \`n\` separately. Is there a rust function that turns \`\\n\` to the linefeed character? I'm not sure how to google for this functionality. I don't know how to call it... so I can't find it.
Note that Rust won't know that the other states of this float would be unoccupied, so unlike NonZeroU8 where even an Option&lt;NonZeroU8&gt; only occupies a single byte of memory, an Option&lt;MyFromMinusOneToFour&gt; will be larger than a single float. NonZero uses the internal `#[rustc_layout_scalar_valid_range_start(1)]` to give that information to the compiler; that's not an option for stable custom types.
I think you mean *per se*, it's a Latin word hence the pronunciation.
I disagree with the injunction that this game wouldn't be better off without better art - the art is what immediately struck me about the game, and is very poor. Not to say a game can't be fun without it, but when the art is so bad the gameplay has to be stellar.
This is a fun one. There is indeed a portion of the parser that maps "\\n" to "\n". However lifting the `\n` to a newline has been threaded from the OCaML compiler all through each bootstrap Rust version.
Looks nice!!
In regards to your second point; it depends on what you mean. Yes the GC will be invoked, but in a native language you have also have the cost of invoking malloc and free. That has a real overhead. A lot of garbage collectors are actually more efficient than the standard implementations for malloc and free. For example in OpenJDK memory is pre-allocated in big chunks. This is how Java examples can potentially achieve C levels of thoughtput. But of course in Rust or C you can potentially avoid the call to malloc all together.
It would be great if you could include the error message so it's easier to help. Further it would be perfect if you could link to a playground which contains the code you want help with. That way one can easily try out solutions for your problem. 
I will continue to work on [issue 84](https://github.com/wahn/rs_pbrt/issues/84): https://user-images.githubusercontent.com/1074865/50510224-ca70ea00-0a80-11e9-9ac5-e5771da15d6c.png This was rendered using the **C++ version**, lets match the **subsurface scattering** by adding the relevant code to the [Rust version](https://www.rs-pbrt.org/about
IIRC, the executables in the `bin` directory aren't part of your crate's module structure and you pull in the code from your crate using an 'extern crate' declaration the way you'd pull in a third-party dependency. In order to be a valid lib, your `lib.rs` file should be directly under `src`, not under the `commoncode` directory.
Oh ok sorry, the problem its not really an error, its more a problem with the output, sometimes its runing so and sometimes only the half way. [https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=f895240a3539cb5c3a68857f032068f3](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=f895240a3539cb5c3a68857f032068f3)
Well, for one thing, calling `process::exit(0)` in a thread does not exit from the thread, it terminates the entire process. You exit a thread with plain `return`.
Try this: crateroot - src - bin - main1.rs use crateroot::commoncode; fn main() { println!("{}", commoncode::respond()); } - main2.rs fn main() {} - lib.rs pub mod commoncode - commoncode.rs pub fn respond() -&gt; String { "Call me Ishmael.".into() } 
I'm working on a decentralized git-[IPFS](ipfs.io) integration called [nip](https://github.com/drozdziak1/nip). This week I hope to solve a major blocker that would greatly limit the whole project if left unsolved. The issue is that after a push you need to inform the people interested in your nip repo that your repo is listed at a new IPFS address (IPFS = giant P2P hashmap, so changed data means changed address). That info would best be authenticated. IPNS looks like a good match, but the `ipfs publish` command is pretty slow, and I'd prefer more freedom in terms of PKI (e.g. the private key used for IPNS isn't protected with any password, which could easily be fixed with GPG and a custom pubsub scheme). Also, pinning an IPNS node doesn't "follow" underlying IPFS hash changes (See [this](https://github.com/ipfs/go-ipfs/issues/4435) and [that](https://github.com/ipfs/go-ipfs/issues/1467). Then I've gotta figure out how git plumbing works for submodules, which currently throw an error unfortunately. This is on a lower priority because submodules are relatively rare.
oh my lord ... so mutch thanks, in the last 5 minutes, i thought "hmm exit kills for sure the hole programm" ;) so much thanks
I hope there will be a topic on this at TED some day. Thank you!
Is trait B from a library, or is it defined in your crate? I'm wondering why, if trait B requires every field from Struct A, you wouldn't just define the method on Struct A. Alternatively, could you create another struct to contain i and string and any other members of struct A which are needed for trait B, then have struct A contain that struct (as shown below)? ''' struct A { b: Box&lt;B&gt;, other: Other } ''' 
In self driving cars the issue is that your slowdowns are practically impossible to predict. The garbage collector will need to stop everything at seemingly random times. This means that you can't guarantee that at time x your CV algorithms will know if the object is a car happily driving along or a pedestrian standing still. With languages that don't pause for automated GC, this can be a lot more deterministic, especially when you're running on a RTOS that can also make guarantees about scheduling. The differences won't be more than a couple hundred ms at a time, but you'll notice the bumps if you need to go into super safe GC mode every few seconds. Or you'll kill more people, if the dev failed to properly account for GC pauses (which, again, is hard to predict, so not unlikely). Of course, if you fail to free a tiny array every loop, which consequently makes you run out of memory and crash every 3 hours, so QA didn't catch it, that's also bad, but that's why we have rust ;)
`ExitStack` can be used to work with `with` in non-lexical ways, but it brings back the burden of reasoning about your lifetimes.
Modern hardware is increasingly multithreaded, GC involves pausing all managed threads which isn't great for performance. Here's a page on the .net GC that talks a lot about how their GC works https://docs.microsoft.com/en-us/dotnet/standard/garbage-collection/fundamentals
Does one wheel support multiple OSs? Or just the one it was built on? I tried looking online but couldn't find a straight answer.
It doesn't pass ownership of the vector to that function. It doesn't change anything *in* this function, but it makes the vector still usable in the caller code after the call to `sum_vec`. 
"Yet another"? I haven't looked at your post history, but for introductory questions you should be aware of the stickied post dedicated to that topic: https://www.reddit.com/r/rust/comments/abgsuj/hey_rustaceans_got_an_easy_question_ask_here_12019/
It looks like it has *something* to do with `gfx_core`, but I don't know what. Something keeps on pushing stuff into a vector, and then not clearing it or popping anything out. ``` &lt;alloc::vec::Vec&lt;T&gt; as alloc::vec::SpecExtend&lt;T, I&gt;&gt;::spec_extend () ```
Thanks! Probably, i'm in the process of making this stuff more formalized 
The second argument to the closure is a reference, as in `b: &amp;i32`. If you write `&amp;b`, then it's like `&amp;b: i32`, meaning that the second argument is still a `&amp;i32`, but `b` is the name for the dereferenced value. They both work because `+` works with references, too. As to why that argument is a reference, it's because that's what `v`'s iterators return.
I've never used piston, but I think by using your own Events struct you're bypassing the cleanup in [window.event](https://docs.piston.rs/piston_window/src/piston_window/lib.rs.html#281-288). I'd suggest trying window.next() instead, as in the [example](https://docs.piston.rs/piston_window/piston_window/index.html#example).
So, in string literals inside rust, a backslash says the next character is an escape code. Thus, `\t` is a tab, `\n` is a newline, etc. You can "opt out" of this by putting an `r` in front of the string, like `r"literal \n"`, or double the backslash, like `"literal \\n"`. With that in mind, if you have a string with a literal `\n` and want to put a newline character in it's place, you can do the following: let with_backslash: String = r"this is a string\n".to_owned(); let with_newline: String = with_backslash.replace(r"\n", "\n"); assert_eq!("this is a string\n", with_newline); Hope that helps!
Where are you hiring your 2D artists? cause I'm looking for these cheap artist
May I ask why you didn't use SDL? 
I add the same problem on an command line application. From time to time the application fail with a 8go memory allocation error. I have it since I use the rustc 1.32.0-nightly (0b9f19dff 2018-11-21). I didn't investigate more because we had it on a specific server use for test and with the Christmas holidays I didn't already get all the logs. I have no news from that problem since one week but if it appears another time I will test with another version of rustc. This application is running since more than one year and it's the first time we add the problem. 
https://en.wikipedia.org/wiki/Dam ;)
It was something I considered, but the art is a part I want to keep working on and improving. I have comissioned some work in the past, but it can definitely be a higher cost for a game. That said, I would prefer to do that over using an asset store or asset packs, as one cannot dictate the general aesthetic as easily.
Yeah I have the source up on github, just not with an open license :). No plans for an actual linux build, as it can be hard to guarantee it works well across various distros.
For using SLD across the board, one could more easily push to mobile, much like https://michaelfairley.com/blog/i-made-a-game-in-rust/. Though I just prefer to use a graphics API like OpenGL instead. For the windowing I wanted to use something Rust based if I could, so I didn't have to worry about linking C libraries. Turns out glutin worked pretty well. I do have a bug however if you move the game from a hidpi monitor to a lowdpi one, where the graphics dont scale properly. But triggering a resize will then fix it.
Itch.io is easier to publish for. I guess for steam I feel a bit nervous to put it on there? Maybe I should! I've thought about putting it on mobile, I just really dislike the business models available there. It's pretty hard to get downloads for a paid up front game, and for something like this I have zero desire to do ads or micro transactions. I might consider putting it on there, just need to see what the lift is to get gfx-rs working on iOS &amp; Android.
The `iter` method gives an iterator that gives references to the items in the Vec (`into_iter` can be used to give the elements directly). The reason for why `&amp;b` can be just `b` is thanks to something called “match ergonomics”, a RFC passed quite recently, that makes the compiler insert `&amp;` and `ref` where needed
The situation is worse than that. AFAIK, the compiler doesn't even promise to stay consistent with the *same version* of itself. It would make sense of it does. Deterministic compilation is desirable because it makes debugging and incremental compilation easier. But it's an undocumented feature, technically.
It has nothing to with match ergonomics here.
&gt; as it can be hard to guarantee it works well across various distros No it's not &gt; Yeah I have the source up on github Is it public?
Just out of wonder, why does most of the code you write require an `Rc`? (asking because this happens quite often when people try to convert their existing codebase, e.g. in java/python/c++, type-by-type, as is, to Rust, and find out that the borrow checker yells at them, hence the rc, rc&lt;refcell&gt; workarounds and hacks; whereas the proper 'idiomatic' rewrite would often require a change of type hierarchy and/or logic). Also, how exactly would you see pattern matching "through" an `Rc`? Rc derefs into the inner type, so you can always just deref it where needed.
Thanks. Will do for the next time.
I haven't really done game development on linux, but my own experience on using it as an OS has been mixed reliability when it comes to graphics card drivers. A lot more stability in recent years, but this concerns me when you have a number of different distros out there. I feel like if I put it out for linux after just testing on Ubuntu with my GTX 970, I'll be biting off more I can chew. https://twitter.com/bgolus/status/1080213166116597760 also this. Source is here: https://github.com/agmcleod/ld39/
Yeah I've done something quite a lot like what you describe with the module separation in that there are basically just a bunch of templating functions in view files for whatever domain of the system. https://github.com/captainbland/crier/blob/master/src/user_view.rs Admittedly though I also have a few things which look like: ``` Ok(user) =&gt; (Ok(Response::with((status::Ok, render_page("Thanks for logging in!", &amp;navbar_info,html!{("Thank you, ") (user_form.username)}))))),``` And while that's largely laziness on my part (hooray, overly broadly scoped personal projects!), it's quite convenient to do that when you're trying to quickly write a view for a trivial response case and it would be easy to add case-by-case stylisation in these cases which has some appeal if I wanted to do something more like: ``` render_page("Thanks for logging in!", &amp;navbar_info,html!{("Thank you, ") em{(user_form.username)}})``` 
agreed. my pc also feels much slower than older pc-s (and now i have a r7 1700) due to misuse of programming languages. one example is gnome shell. personally i think this was a colossal error when someone took a shitty language and made it a main thing that gives visual feedback to users. anyhow, there is analysis of input lag here [https://danluu.com/input-lag/](https://danluu.com/input-lag/)
Thanks!
I don't see recursive data structures being conducive to functional programming specifically, and I also find that functional code I write in Rust typically doesn't allocate memory at all, or does so very rarely. Linked lists don't seem particularly necessary for such algorithms, and I'm not sure what about them reminds you of functional programming. Usually, I am going from one data structure to another and everything in between is functional, or, if Rust doesn't have the functional primitives, it becomes a mix of imperative and functional code. One instance is gathering data from an octree to a vector, another is parsing data with nom (which is almost always very functional) into structs. I am not particularly experienced in pure functional languages, but I would say that I am curious what about linked lists seems to be associated with functional programming to you.
Yeah, we need GAT and HKT.
Here I write about my personal experiences developing in Rust over the past year. Some of the major obstacles I faced and wasn't able to pass at the time.
Factorio is an interesting case because when it was initially kickstarted it had graphics if a similar quality (and a heck of a lot of assets lifted from other games), but over time the look has evolved to the much better position it is in now. I think the devs do attribute a fair amount of sales to the art updates. 
https://www.arewewebyet.org/ + https://areweasyncyet.rs/ I'd personally wait until `async` + `await` land on stable (possibly some time this year), but you may give Rust a shot nonetheless.
cool thx, how does Rust do async right now? Is it painful? I imagine its writing handcoded promises?
&gt;I think you mean *per se*, it's a Latin word hence the pronunciation. I think you mean *phrase*, it's a short group of words that are often used together and have a particular meaning. Also I'd question your use of the word "hence", since there really is no consensus on the original pronunciation of many Latin words.
I'm currently working on a typescipt parser / compiler for [the swc project](https://github.com/swc-project/swc).
Using dlopen still requires you to know the prototype of the functions you want to use though, and this is exactly the content of the header files.
What are you doing precisely? Node can be pretty fast and rewriting the same thing in Rust won't automatically make everything 10x faster, especially for I/O bound code.
I would like it to work with other escaped characters (e.g. \\t, \\0A) as well, not just \\n.
Trait B would be defined in the crate, but could be implemented by another crate. While you could just define the method on struct A in some situations, when you want to allow custom behaviour, you may want to use a trait. The second solution is would work well in this scenario, but I can't see it working when you want to want to do the same thing with multiple members like: struct A { b: Box&lt;B&gt;, c: Box&lt;C&gt;, d: Box&lt;D&gt; } where traits B, C and D all require one another. I know its not a big issue and there are workarounds, but I do feel like it would be a nice addition to the language.
I’ve played with SDL a good bit and it’s not to hard to wrap the mind around. How hard was it to the learn OpenGL way of drawing to the screen? 
The compiler does optimise it in release mode, usually. But not in debug, ever, from what I’ve seen. So relying on it is not going to work. I’m not sure why you mean by in-place drop? We already have a function for it (two in fact, ManuallyDrop::drop and ptr::drop_in_place). They are unsafe since they don’t keep track of whether it was already called. Rust has no guaranteed calling convention. In practice though, yeah, it usually takes a pointer to fill in.
The memory layout of an array is specified (contiguous elements with no extra padding, the same as in C), but the layout of a tuple isn't, and the compiler is free to play around with element ordering. That's why you can take a slice from an array, but not from a tuple, even if the elements of the tuple are all the same type. That's also why arrays are also suitable for FFI, but tuples aren't.
I'm not sure I fully understand the problem you're working on, so correct me where I'm not getting it. You have an existing library A written in C (or at least, having a C programming interface). You'll write some other software C. And you're asking about a layer that interfaces with both, B. Is C going to be written in Rust only? Or might it be written in another language? The A:B interface would have to be FFI. If C is in a non-Rust language, then B:C also must be FFI. You'd have to write C-language header files for the B:C interface. Then you'd import them into Rust using bindgen and start implementing. Rust only half-supports varargs (calling yes, implementing no) so you might need or want a little bit of glue in C-lang. Pain-level: moderate. Is it a good idea to write B in Rust? I would say, "yes, *if* B is complex enough that you'd benefit from better type or memory safety." Scenario II: you know that C will be written in Rust. The answer here is simpler: write B in Rust and link statically unless you *really, truly* need plugin functionality, in which case you must use FFI and are back to Scenario I. The reasoning is a bit more complex. If/When Rust supports dynamic linking there will still be a tradeoff between dynamism and link-time optimization. Many (most?) Rust library crates expose generics and those can only be optimized at link time (static linking required) or at run time (JIT compiler required). Rust's LTO is already really good. Futures (crate `futures` 0.1) compile to state machines which have virtually identical performance to hand-written state machines. (https://aturon.github.io/blog/2016/08/11/futures/) Very nifty; can't happen without inlining. And that's just using features which are already established. Abstract return types are fairly new. Placement is on the horizon and `async fn` is being actively worked out (like `futures` but much easier to read). The stability of the binary interface tends to lag behind the stability of the programming interface - Rust needs to define "what is a function call" before it can commit to a stable mechanism for implementing it. Do you need Rust-specific features for the B:C interface? Would they help? If so, then C must be written in Rust (or some future language which decides that backwards-compatibility with Rust is desirable) and currently B:C must be statically linked. If in the future dynamic linking is possible, it probably will impose a significant performance penalty on using Rust-specific features across that interface. If you really do want high performance, modern language features, and dynamic linking, then you'll have to accept a heavier runtime with a JIT. If you like Rust's syntax also check out Scala and F#.
On nightly it looks like this: https://jsdw.me/posts/rust-asyncawait-preview/
Not promises, [Futures](https://tokio.rs/docs/getting-started/futures/). Much scarier. 
I think it would be quite fun to build an ML syntax for rust. Just to see how it looks
Thanks :-)
For smaller jobs, Fiverr is very popular. Otherwise, you have to look around just like for any other freelancer. Graphics artists have a very bleak outlook, because most of them end up in marketing agencies, where they are treated like factory workers doing unbelievably dull tasks withs impossible deadlines. Many of them long for exciting and creative jobs like doing the full artwork for a game.
The reason why it still works is because there is an implementation of `Add&lt;&amp;i32&gt;` for `i32` with `i32` as output type. So, you can add a `i32` and a `&amp;i32` yielding a `i32`.
Please correct me if I'm wrong, but isn't the answer to this question the following: If `b` wouldn't be a reference and if it points to a non-copy type then you would move `b`? And because you moved `b` you would break the next iteration?
Enjoyed reading.
&gt; but I would say that I am curious what about linked lists seems to be associated with functional programming to you. Every functional programming class I've had first hand experience with (at three different schools) has introduced linked lists as a core data structure very early on in the course, typically in the form of `car`s and `cdr`s. Recursive data types are a natural representation for functional code because they are amenable to expressing algorithms using structural induction, which is nice for expressing termination arguments. A classical assignment in a programming languages course, for example, is to write a type checker that does structural induction over the types of terms in a program. This is most naturally expressed with a recursive data type. Please consider that I am just trying to provide a perspective here, since you asked why linked lists and recursive data types would be specifically associated with functional programming. Please do _not_ misconstrue this as an attempt to define what functional programming actually means. (In particular, I am not interested in a debate about definitions, but I'm happy to lend perspective.)
In the last year or so, ocaml has an excellent build tool called `dune` emerged as the standard building tool accepted by community. Complemented with `opam`, the de factor package manager, I think the onboard experience now is close to rust on Linux (yes, windows support is still work in progress). Here is the link to how to quickstart an ocaml project. https://ocamlverse.github.io/content/quickstart_ocaml_project_dune.html 
Thanks for letting me know! Really glad to see OCaml trying to address this stuff. I’m obviously not an avid follower of the community, but I’ll give it a shot the next time I’m using the language. :)
Right, I appreciate the comment. I am also not really interested in the debate between what is or isn't functional programming, and it seems that the association here is due to different paradigms of functional programming than I am familiar with. I may have to read about structural induction in functional programming, because that definitely seems to be absent (and probably always will be) in Rust, so I haven't really been exposed to it.
https://www.youtube.com/watch?v=Ll0qHlx_qLg
I'm going to continue working on multi-lingual support for [Zola](https://github.com/getzola/zola), a static site generator. This is the last big feature needed for Zola to be considered mature in my opinion. Design idea can be seen in https://zola.discourse.group/t/rfc-i18n/13/3 and initial implementation in the [`next` branch](https://github.com/getzola/zola/pull/567). I would love to get more feedback on that, especially from people making multilingual sites to ensure I don't miss anything. I will also probably publish the v1.alpha.1 of [Tera](https://github.com/Keats/tera/) a template engine similar to Jinja2/Django. The alpha is not meant to be used by anyone other than Zola since there will probably be some breaking changes between the versions but that will ensure v1 of Tera gets released soonish.
Structural induction is certainly not absent in Rust. :-) I use it all the time. But I work on regexes a lot, which means there's quite a bit of structural induction over a regex's syntax, for example. I'm sure `rustc` itself probably also uses quite a bit of structural induction.
But does it use clean coal?
I think OpenGL is definitely harder. I learned OpenGL mainly by doing WebGL, where SDL isn't really an option. It took some time to figure out how Gfx-rs does things, but with having some basics around OpenGL already, the learning curve wasn't too steep.
Automatically fixing a general warning seems really tricky. For example, if you fix a non_snake_case struct member you may change the meaning of my code if there's a `derive(Deserialize)` at work. Is there something I'm missing?
Is coal clean? 🤔 With the resources that add pollution, there are passive upgrades to reduce the you pay.
Are you referring to compile-time or run-time structural induction? I was referring to it more as a language feature (which I now know would be provability of data structure properties at compile time). It seems from my reading so far that blanket trait impls can partially fill in this gap, but without some combination of const generics and a compile-time solver engine it wouldn't be possible. Maybe `chalk` will become an enabler.
&gt; [how do I] works well across various distros Step 1: Assume every Linux out there is current-day Ubuntu. Step 2: Make stuff work on Ubuntu. Step 3: Earn gratitude from Linux users. Most non-Ubuntu-users will be fully capable of figuring out what's missing, especially if you tell them what it is you assume they have installed. (E.g. Pulseaudio) And those two people who don't will find others who do. That "make it work across distros" challenge is about 90% a myth.
We found that horrorshow is only faster than Askama because it doesn't escape HTML special characters by default. Does Maud escape by default?
I'm not personally aware of a library that does this, but if you want a basic example of how it's done (quickly put together, and not done in-place), here's a small gist: [https://gist.github.com/stisol/25aa6e35eba331fddb1641d7ec39f672](https://gist.github.com/stisol/25aa6e35eba331fddb1641d7ec39f672). `serde_json` has their own implementation as well that's worth giving a look - check the `parse_escape` function at line 728 here: [https://github.com/serde-rs/json/blob/master/src/read.rs](https://github.com/serde-rs/json/blob/master/src/read.rs)
FWIW, A Snake's Tales uses SDL only for windowing + input + audio; all of its graphics are done with raw OpenGL.
We are on completely different wave lengths. :) Structural induction is a general concept that has no specific coupling with language features. You can do structural induction in lisp just as well as Haskell. The latter case will just benefit more from a type system that ensures you have checked all of your cases. My point above was just that structural induction and functional programming go together like two peas in a pod.
&gt; Source is here: https://github.com/agmcleod/ld39/
&gt; Source is here: https://github.com/agmcleod/ld39/
&gt; FWIW, A Snake's Tales uses SDL only for windowing + input + audio; all of its graphics are done with raw OpenGL. Ah thanks for the correction :)
It's something I'd have to support however. This popped up last night: https://twitter.com/bgolus/status/1080213166116597760. It's not the first time i've heard that anecdote, where Linux takes up a large chunk of support requests compared to the revenue it brings in. Like if someone reports a bug on arch linux, im just not going to be able to test it.
Ideally it would be like refactoring tools in eg. C#: in your IDE you get a button where you can select the desired action: In this case there would be 2 actions, apply the suggested fix or add an #[allow] attribute.
That game was released over four years ago. A lot has changed.. why not give it a try?
Nothing has really prevented it. I do have a dual boot of Ubuntu that i use for a bit of machine learning stuff. I just don't trust that's enough of a test to do a linux release.
In general, having your code depend on the spelling is not a good idea, but sometimes unavoidable (e.g. C FFI) or just too ergonomic (e.g. serde). In the case of serde, you can simply [rename](https://serde.rs/field-attrs.html#rename) the field via an attribute.
When a suggestion is emitted by the compiler it's marked with the confidence level the compiler has it is correct (`MachineApplicable`, `MaybeIncorrect`, `HasPlaceholder`, `Unspecified`), and external tools can choose which ones they want to apply based on this information. `rustfix`/`cargo fix` at the moment only applies the ones marked as `MachineApplicable`. If there are suggestions that can be wrong (like the one you mentioned) they will be marked as `MaybeIncorrect`, so `rustfix` will refuse to fix it and IDEs can ask the user.
I never thought about learning OpenGL through web GL. That’s a pretty good idea. Although with Vulkan out I would like to start with that
Linux fanboys are very small in number and a bunch of thankless users. Good calll. 
&gt; On occasion I’ve found myself converting raw pointers into unique references without really caring if those pointers were really unique just because it was so much more convenient. Meaning you're casting `*mut T` to `&amp;mut T`?
&gt; Another sticky point when working with C APIs are out parameters. Out parameters are pointers to uninitialized memory which the C API is supposed to initialize. Rust really doesn’t like these, requiring use of the much maligned std::mem::uninitialized or just taking the hit and initializing with a dummy value. This is pure curiosity from mobile, since I've yet to use it, but does the `MaybeUninit` API help here?
This might be different for Rust based projects due to more robust error handling (especially if you use pure-rust libraries). If you link against dynamic libraries you may still have issues, but then you can simply document the required libraries. Or create an AppImage: https://appimage.org/ (Haven't done this for Rust projects so far, but it works great for https://librepcb.org/)
&gt; Another sticky point when working with C APIs are out parameters. Out parameters are pointers to uninitialized memory which the C API is supposed to initialize. Rust really doesn’t like these, requiring use of the much maligned std::mem::uninitialized or just taking the hit and initializing with a dummy value. This is pure curiosity from mobile, since I've yet to use it, but does the `MaybeUninit` API help here?
Myeah, getting frustrated by such a ticket bombardment makes sense. Though I get the feeling that nowadays Linux+GPU issues boils down to a "nVidia or else?" question. AMDGPU drivers seem to be great today, never had issues with Intel graphics drivers. So... One could e.g. put "tested with AMDGPU on Linux" on the hardware requirements list.
Yes, autocorrect doesn't like programming terms...
In theory, they could check that any constants emit after a fix match the constants emitted after a fix (post macro expansion, which they wouldn't in this case). Really the right fix in this case would be to use the serde attribute to rename it to whatever the serialized name is and then change the field name to the proper format. But I'm not sure how to teach cargo fix about crate specific fixes. 
Do you have any intuition or even data about how many suggestions are `MachineApplicable`?
FWIW I just tried building it on my laptop and it seems to run without any issues! I'm using Manjaro Linux and I've tested it using a Haswell Intel iGPU and a GTX 960m. I can't test it on my desktop PC right now, but I'm sure it runs fine. The only issue I ran into had to do with Bumblebee (a hacky replacement for Optimus) causing a 'NoAvailablePixelFormat' error because it somehow lacks support for the 'Srgba8' pixel format gfx uses. I had run into this before when using gfx myself and I quick Google search seems to confirm this issue. Running it through 'nvidia-xrun' (which allows you to bypass the whole optimus/hybrid graphics shenanigans completely) worked flawlessly though!
Yes, I'm pretty sure that that's simply not valid in some cases. Eg. `(*raw_ptr).field` has different semantics than `{ let temp = &amp;mut *raw_ptr; temp.field }`. The difference is that the latter allows the compiler optimizer to move the _actual_ access around because I just told it that reference is unique. But in my case that is absolutely not allowed! The memory read/write _must_ happen on the line that I wrote it. My code still worked because I'm pretty sure llvm is still very conservative (because Rust had issues with, they disabled some aggressive llvm optimizations), but that doesn't make it right.
Yes, but I'm not a fan of trying to work around language issues with library APIs. It just feels so clumsy and I'd prefer something like the language being able to deal with `&amp;mut uninit_variable`, eg taking the address of an uninitialized variable and having it work right.
I will only be happier with life if I never ever touch a UTF-16 string... but you are right, we should totally have UTF-16 string literals. A procedural macro to handle it actually looks like it would be perfect.
&gt; The cycles rule out reference counted types I'll admit, I've never implemented a tree in Rust, so maybe I'm missing something. But I don't see where the issue with cycles is. I presume the cycles are due to child-&gt;parent pointers, but couldn't you just use Weak&lt;Node&gt; to resolve that?
Thanks!
Hi, I want to create a struct that holds a vector and and iterator for that vector. I think I should hold an Iterator because that struct should have a function that doesn't return anything: the first time I call the function the first element of the vector should be used for something, the second time should use the second element of the vector, etc. A simplified version of what I'm trying to do is on [this rust playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=0f1eadc15deedde372a172cc36b55c4b). Then I tried to use `std::slice::Iter` instead of `Iterator`, and I got [lifetimes issues](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=6cc19052791e591a05509315c4eb9abc). Now I'm stuck trying to write the lifetimes, I have a rough idea about how lifetimes work and I've used lifetimes on some trivial cases but now I can't make it work. I think I can just forget about Iterators and all that stuff by just storing on the struct an index and increment it each time the function is called, but If someone can help me I'd like to learn a little about this. TLDR: Somebody can fix any of the playgrounds I linked?
A search of `MachineApplicable` on the compiler (excluding tools) yields 79 matches, so I guess it's around that number (probably a bit less). I expect most of them to be edition migration lints, even though there are some "normal" lints that are already `MachineApplicable` (like `unused_variables` and `unused_mut`).
I'm trying to use a hashmap with both float and string vecs as the values - the background is that I want to parse a csv into a hashmap with an entry per column. So I created the an enum GeneralVec (below) to use as the hashmap value type. `enum GeneralVec {` `FloatVec(Vec&lt;Option&lt;f64&gt;&gt;),` `StringVec(Vec&lt;Option&lt;String&gt;&gt;),` `}` `fn process_csv(filepath: &amp;str, fields_info: HashMap&lt;String, query::FieldInfo&gt;) -&gt; HashMap&lt;String, GeneralVec&gt; {` `let file = std::fs::File::open(filepath).unwrap();` `let mut rdr = csv::ReaderBuilder::new()` `.has_headers(true)` `.from_reader(file);` `let headers = rdr.headers().unwrap().clone();` `let mut data: HashMap&lt;String, GeneralVec&gt; = HashMap::new();` `for result in rdr.records().into_iter() {` `let record = result.unwrap();` `for (i, token) in record.iter().enumerate() {` `let field = headers[i].to_string();` `let field_info = fields_info.get(&amp;field).unwrap();` `let variable = field_info.variable;` `match field_info.data_type.as_ref() {` `"Float" =&gt; {` `match data.entry(variable.clone()) {` `Entry::Vacant(_e) =&gt; {` `let mut v: GeneralVec = GeneralVec::FloatVec(Vec::new());` `data.insert(field_info.variable.clone(), v);` `}` `Entry::Occupied(mut e) =&gt; {` `match token.parse::&lt;f64&gt;() {` `Ok(f) =&gt; { e.get_mut().push(Some(f)) }` `Err(f) =&gt; { e.get_mut().push(None) }` `}` `}` `}` `}` `"String" =&gt; {` `match data.entry(field.clone()) {` `Entry::Vacant(_e) =&gt; {` `let mut v: GeneralVec = GeneralVec::StringVec(Vec::new());` `data.insert(field_info.variable.clone(), v);` `}` `Entry::Occupied(mut e) =&gt; { e.get_mut().push(Some(e)) }` `}` `}` `_ =&gt; {}` `}` `}` `}` `return data;` `}` However when I build I get the compile error &amp;#x200B; `error[E0599]: no method named \`push\` found for type \`&amp;mut VecType\` in the current scope` `--&gt; src\main.rs:60:56` `|` `60 | Ok(f) =&gt; { e.get_mut().push(Some(f)) }` `| ^^^^` `|` `= help: items from traits can only be used if the trait is implemented and in scope` `= note: the following traits define an item \`push\`, perhaps you need to implement one of them:` `candidate #1: \`ena::unify::backing_vec::UnificationStore\`` `candidate #2: \`smallvec::VecLike\`` `candidate #3: \`proc_macro::bridge::server::TokenStreamBuilder\`` `candidate #4: \`proc_macro::bridge::server::MultiSpan\`` `candidate #5: \`brotli::enc::interface::CommandProcessor\`` &amp;#x200B; I understand that the rust compile doesn't know that all the types in the GeneralVec enum are vecs - how can I let it know this? &amp;#x200B;
&gt; I presume the cycles are due to child-&gt;parent pointers, but couldn't you just use Weak&lt;Node&gt; to resolve that? Yes. The issue with cycles becomes harder to solve in more general graphs with less clear ownership relations. For trees, parent's typically own the children, so parents can have an `Rc` to the children and children can have a `Weak` to their parents. 
FWIW, Go has similar performance characteristics, due mostly to compiler maturity rather than anything inherent in the language. Also remember this is in contrast to Perl/Python/Ruby and other interpreted languages which are regularly 20-30x slower than C for CPU-bound workloads.
Hi Rustaceans at Reddit, I have yet another hashmap question that hopefully you could shed some lights on: I have a well defined input key-value pairs: a u32 value as the key and a String as the value. and all the keys are guaranteed to be unique. The hash function doesn't need to be secured and only need to be as fast as possible. I am thinking about implementing my own hashmap, but wonder what would be a Rustacean's preferred way to solve this problem. Any suggestions/pointers would be greatly appreciated! Thanks and Happy New Year!
That's a pretty terrible thing to say
gfx-hal matches the vulkan API pretty closely. I haven't tried it yet, but the gfx-hal crate got published recently. https://github.com/gfx-rs/gfx
I see, that makes sense. I guess I don't need to worry about this then. And yes I did use bindgen.
Yeah i'm more concerned where I'm running Ubuntu on a GTX 970, and someone is running Arch on an AMD graphics card and it doesn't work or something.
I can always put it out there and hope for the best. I just feel like I would be disappointing users if it's out for linux, but only works reliably on Ubuntu. I guess I'm not sure what's worse. No linux, or only some linux?
I was misunderstanding how things have to be dynamically linked. Since I can statically link the bindings, but dynamically link the bindings to the library. My question is basically answered.
Yep, and there may be issues indeed. I tried to run your game on my Arch + Nvidia + PulseAudio machine and it crashed when initializing the audio sink. I managed to fix the compilation, are you interested in a PR? With the fix (rodio upgrade) the game starts, but there's still no sound. However, that seems to be a rodio library problem, not caused by your game: https://github.com/tomaka/rodio/issues/173
True, but it's important to be clear that you don't strictly need the files themselves.
Yeah. NPM did some Rust rewriting and actually found it slightly *slower* for their use case because of all the work the Node.js team had put into fine-tuning things like buffer sizes. (But they kept the Rust version because it saved them so much pain around tracking down unexpected exceptions.)
if you don't have any dependencies on compiled libraries, it will work cross-platform. Otherwise you have to ship wheels built for each OS/architecture.
You might want to check out [https://github.com/FusionAuth](https://github.com/FusionAuth) as well for comparison. Their API documentation is pretty solid and can give some good perspective on other solutions.
Planning to complete the network based remote control for my Samsung TV. (written in rust of course) 
Just bought the game! I'm a Linux user though, so I'll just build it from source for now :)
How is this an alternative to libreauth? This looks like bash scripts for a server.
I would caution against moving too fast with `MachineApplicable` lints. The existing ones sometimes break code, and I think it would be nice if they were fixed (or demoted) before adding too many more. Here's a sample of some still open issues: [56326](https://github.com/rust-lang/rust/issues/56326) [56038](https://github.com/rust-lang/rust/issues/56038) [55768](https://github.com/rust-lang/rust/issues/55768) [54180](https://github.com/rust-lang/rust/issues/54180) [6465](https://github.com/rust-lang/cargo/issues/6465) [55759](https://github.com/rust-lang/rust/issues/55759) [56685](https://github.com/rust-lang/rust/issues/56685) [56683](https://github.com/rust-lang/rust/issues/56683) [56327](https://github.com/rust-lang/rust/issues/56327) [56684](https://github.com/rust-lang/rust/issues/56684) I don't mind investigating/triaging the reports when rustfix fails, but as people use rustfix more, it will take up more time. Assuming more editors provide built-in fixit suggestions which work with any suggestion, then `MachineApplicable` should be reserved for lints which are nearly guaranteed never to fail. 
&lt;3
Okay cool i'll update rodio in it tonight. I think I will ship a linux build, as im getting a lot of peer pressure from you all ;), and the Rust community is awesome. I should actually see how linux games are usually packaged, as I setup scripts for the .app on mac, and .exe on windows. If it's just the compiled bin + folders cool.
Pull requests welcome :)
AMDGPU drivers are much better, Ill give this a test with my vega56
A box only points to part of the heap. “Box” is a general terminology for this kind of action, so we used it. Depending on what languages you’ve used and how you’ve used them it may not be something you’ve come across, but it isn’t rust specific.
/r/playrust
Ok, you can also apply this patch to save yourself some time :) https://gist.github.com/dbrgn/e18d3916e9ffbc8b6cc95f8f73e95251 Regarding packaging, take a look at AppImage (https://appimage.org/), it might be exactly what you want since it allows packing assets and even libraries into a single executable.
Agree. Cool concept but if I saw this on stream and didnt see the description I would definitely keep scrolling. 
That solved it! Thank you so much. I will point out that several of their examples use a Events::new(), such as [this one](https://github.com/PistonDevelopers/piston-examples/blob/master/freetype/src/main.rs#L72), which I think might have been why I was using it.
Because a [heap](https://en.wikipedia.org/wiki/Heap_\(data_structure\)) is a specific kind of datastructure is my guess.
That sounds like a recipe for disaster; way to easy to partially initialise or forget to initialise something before using, no?
Thanks. Its good to reuse existing namings.
By the way, you should never use an arg type of `&amp;Vec&lt;T&gt;`. Just use `&amp;[T]` because it can do everything that the immutable Vec ref can do, but you can get slides from many things besides Vec. 
The idea is that the compiler would check this (whereas MaybeUninit or mem::uninitialized()) don't check anything right now. The problem MaybeUninit solves some intricate issues with communicating to llvm that this memory is uninitialized, but not quite UB to access, especially an issue in generic contexts.
As someone who is at least partly self-taught, that overloading has always made me upset. "A heap" vs. "the heap" is very confusing for those who want to learn on their own.
Thanks for the write-up! &gt; So it isn't magic, but you have to deal with it at lots of different levels of the compiler, to get the behavior you want. You have to treat optimizing checked arithmetic as something that is as important as all of your other optimizations. That's a big cultural shift for the C/C++ world. I think that you are hitting the nail on the head. The thing is, be it GCC or LLVM, they have been mostly developed to optimize C (or C++) code; as soon as you venture outside of things done in C (or C++), then it's not as pretty. For Rust, two notable examples are (1) overflow checking and (2) aliasing information. I suppose it's pretty normal: if there's no usecase, there's no point in investing. &gt; but that single-byte opcode was repurposed on x64, so on x64 we used JO (Jump on Overflow). Doesn't this inhibit quite a few optimizations? I'm less worried about the run-time overhead of the assembly, and more about the impact on the quality of the code generation. I would expect the presence of jumps to prevent a number of high-level optimizations (code motion? loop unrolling?) and low-level optimization (vectorization?). &gt; So our compiler was able to look at expressions like this, and understand that, in side-effect free expression trees, we could legally rewrite this to `n + 8` and therefore only have a single overflow check. I think coalescing is a very interesting optimization, when available. Did you also consider poisoning? That is, when `r` overflow there's some bit, somewhere, marking `r` as overflowed but operations continue until either: - `r` is passed as argument to a function, - or `r` is used in a branch condition. The main benefit of poisoning is allowing speculation: you can pre-compute stuff that may overflow, and as long as you don't use it, nobody notices. The way I imagine it, you'd have N "check points" in your function, where N &lt;= 64 hopefully... and then on overflow on an operation you'd execute `bitmap |= 0b011100000011` poisoning all "final" artifacts that would be affected by the result of this operation. Maybe using `CMOVO`/`CMOVNO`? 
I was talking about it with @kennytm, who wrote the current version to solve another issue (vectorization of `.sum()` I think). It's not possible to implement `IntoIterator`, but we could put the `is_done` boolean directly into `IntrusiveRange` I think. Never yet hacked on the rust repository, so it would be a good way to kick off 2019 for sure ;)
Can't provide
/r/playrust
Despite finding a workaround, it looks like this was a bug, so I've created an issue for it.
Thanks. I had totally forgotten about that structure 
I think that you need to rethink your code because the type system won't be happy with code like that. You need to specify what happens for example if you're trying to push `String` to `FloatVec`. Will it push `None`, panic or just ignore the value? Basically you must match on `GeneralVec` and call `push` on the actual `Vec`. Alternatively you could implement `push_float` and `push_string` methods `GeneralVec` that do this checking. Other approach is to make the code less generic and use multiple hash tables. You could merge these to after reading all records or return custom `struct` with some nice interface. Hard to say without knowing how this data is used in rest of the code.
Autocorrect? What could possibly go wrong? http://cdn.damnyouautocorrect.com/images/baby-jesus.jpg https://i.dailymail.co.uk/i/pix/2016/05/27/16/34AF292C00000578-0-image-m-152_1464362868750.jpg http://cdn.damnyouautocorrect.com/wp-content/uploads/2012/03/15.png https://media.techeblog.com/images/funniest-autocorrects.jpg
Most of them. All the edition migration stuff is applicable because it has to be, but folks contributed applicabilities to a lot more. Some suggestions _can't_ be machine-applicable, but for others folks just haven't done the work yet, and those are `Unspecified`. There are just 14 of these, and it's not much work to look at those and make them applicable if you're interested in contributing. (Clippy is more 50-50 when it comes to applicability) A far bigger task is adding suggestions in the first place, a lot of lints and errors _could_ have suggestions but currently don't. Would be nice to go through these.
&gt;&gt; but that single-byte opcode was repurposed on x64, so on x64 we used JO (Jump on Overflow). &gt; Doesn't this inhibit quite a few optimizations? In our compiler, no, it didn't inhibit optimizations, because the transformation from "instruction: add with overflow" to "add ... ; jo ..." happened fairly late in the compiler. Because "add with overflow" was represented as a specific instruction type throughout all levels of our IR, from high-level AST to abstract HIR to language-neutral but relatively low-level MIR to machine-specific LIR, we were able to make appropriate transformations based on the semantics of "add with overflow", and then only at the last minute, when we had made practically all of the optimizations we were going to make, would we convert a single "add with overflow" op into the x64 sequence "add ... ; jo ...". So from the POV of nearly all optimizations, "add with overflow" was a single operation. It did not break up basic blocks, for example, even though in the final assembly code there is a JO in the middle of a block. So not only does it not inhibit optimizations, it actually enables more and better optimizations. When you're looking at a code sequence and you know that a previous operation uses overflow checking, you *know* that your range invariants have not been violated, so you can actually generate *even better* optimized code. For example, if you have this code sequence, from a hypothetical binary search: T get_middle_element&lt;T&gt;(T[] items, int low, int high) { assert(low &gt;= 0); assert(high &gt;= 0); assert(low &lt; items.Length); assert(high &lt;= items.Length); assert(low &lt;= items.Length); assert(low &lt;= high); int diff = high - low; // range analysis knows that diff &gt;= 0, because (low &lt;= high) above // overflow is not possible on this op, compiler does not emit JO int diff_half = diff / 2; // range analysis knows that diff &gt;= 0, so diff / 2 cannot overflow // range analysis knows that [0..MAX] / 2 is also in [0..MAX] // range analysis knows that [0..diff) / 2 is also in [0..diff) (conservatively) int middle = low + diff_half; // range analysis knows that low + diff_half &lt;= high // range analysis can *probably* determine that this op cannot overflow, so can omit JO // range analysis knows that middle &gt;= 0 // range analysis knows that middle &lt;= items.Length // range analysis *maybe* can determine that middle &lt; items.Length, because x / 2 &lt; x // so we *might* be able to 100% eliminate this array-bounds check return items[middle]; } Imagine that the "assert(...)" calls at the start of this function are actually hoisted from the caller, and that these invariants are both required by a binary-search loop and re-established within the body of such a loop. In other words, this code is part of the hot loop of a binary-search, and we want to make sure that most or all of the runtime checks (including both arithmetic overflow and array bounds checking) can either be eliminated entirely, or can be hoisted out of the loop and checked only before we enter the hot loop. Checked arithmetic actually lets you generate better code here, because the space of possible program executions is much smaller. If an operation would overflow, then all operations after that are never executed (because you jumped to your panic handler), and so you don't have to think about them. (The same is true in C/C++, but you're just into "undefined behavior" territory.) The key is that you want your compiler to be able to exploit this information, and to move runtime overflow checks as early as possible in control-flow, so that these ops "dominate" later ops. This way, many of your later ops don't need runtime overflow checking. We measured the cost of these JO branches, using a variety of very large-scale tests. (Direct3D-based drivers, filesystems drivers, network drivers running at 40 Gb/s) For most workloads, the cost was *literally unmeasurable*. As in, given a particular machine trace, we could not determine whether the trace was from a run with runtime overflow checks, or not -- the difference was literally in the run-to-run noise. (And yes, we're 100% sure that we were measuring the right code.) And keep in mind, that's checking overflow on *every single arithmetic operation*. We did allow users to disable overflow checking within a specific region of code, by using **unchecked { ... }** blocks. At times, we did find that the cost of checked arithmetic in some specific block of code was unacceptably high, so we allowed our developers to *judiciously* and cautiously disable checked arithmetic in those regions. That helped a lot, and we found that it was only necessary in a tiny number of places. We also used **unchecked** when we simply required modular arithmetic. In the case of modular arithmetic, since unchecked is the actual, correct, desired behavior, it's a fairly different case. The highest "tax" that we saw for arithmetic overflow checking was about 1.2%, and again, that's for enabling it on all arithmetic ops. We simply addressed this as an optimization goal, and we were able to reduce this to "negligible" by making further improvements in our compiler, and again by judicious use of **unchecked**. I wish I could give more specifics about the project, but it has been about 4 years since I was involved in it, and unfortunately the overall project was cancelled. But it did prove to me, beyond a shadow of a doubt, that 1) overflow semantics are vital for correctness, 2) overflow optimization is compilers is straight-forward and has many benefits, 3) the runtime cost of overflow checking can be acceptably low, even for extremely high-performance components. I hope this helps, and I hope this doesn't come across as too strident. It's a subject that is important to me, and too often it seems to be neglected or minimized in language design. I have great hope for Rust doing better than C/C++ in so many ways, and especially in achieving a better balance of correctness vs. performance. 
I believe Erlang GC's trick is to have per-actor heap and deep-copy when sending so that all actors' heaps can GC'ed without any synchronization. Could Pony be using a similar approach?
&gt; The memory read/write must happen on the line that I wrote it. It sounds like you're looking for [read_volatile](https://doc.rust-lang.org/std/ptr/fn.read_volatile.html) and write_volatile?
Yaron Minsky (Jane Street) actually mentions that they use OCaml in algorithmic trading with &lt; 10us response time, which puts their OCaml performance on par with state-of-the-art C++ code. This is apparently achieved by using a specific restricted subset of OCaml in the few modules where this level of performance is required; thus avoiding allocations (and GC pauses) during this particular code path.
Although I prefer Arch-derivates over Ubuntu, I am all in for fixing my distro until Ubuntu-tested stuff works. Even if it is "just Ubuntu", you'll catch what feels like 99% Linux users. &gt; I guess I'm not sure what's worse. No linux, or only some linux? With some Linux, we half a dozen people have more opportunities to get stuff working ourselves, without relying on WINE. With no Linux... well... to this day I'm wondering whether the Steam client can tell whether it is installed on two dozen Play on Linux prefixes.
Besides upgrading to 1.31, add `edition = "2018"` under your `[package]` in Cargo.toml.
Working with uninitialised memory is quite difficult in Rust, but I would expect this to be relatively important for a GC.
It's interesting, the jargon file actually calls this an "arena", so presumably the use of "heap" for dynamic memory area of program comes after the heap data structure was described. I'm sure a real computer historian would know how to track down the usage though.
This is one of the reason for "companion" libraries/executables being written in Rust, of which Conduit is an example. Write the companions in as efficient a language as possible so that most of the server resources are dedicated to the actual application.
Is the speed of the hash map somewhat intentional, to make it unpredictable and thus more secure? There are faster hashmaps available elsewhere. 
&gt; I hope this helps, and I hope this doesn't come across as too strident. It's a subject that is important to me, and too often it seems to be neglected or minimized in language design. I have great hope for Rust doing better than C/C++ in so many ways, and especially in achieving a better balance of correctness vs. performance. Thank you very much for the extensive report. I had never heard of any actual attempt at optimizing overflow-checking for performance before, and was hopeful it was possible; thanks to you I now know that only it is possible, but it's been done before, and with excellent performance results. That's awesome.
Those are mostly bugs with the idiom lints, not necessarily bugs with suggestions. The edition migration was complex to implement and there were a lot of edge cases we had to consciously ignore because it was really hard to deal with.
If all you need from the struct is to use the elements from the iterator, then `vec::IntoIter` is exactly what you need, and how it works is basically by storing a vector and an index, like you said at the end (it actually uses two raw pointers, but conceptually it owns the vector). https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=20562ef3f7cfd19335129f632aa18fa3 If you also expect to access the vector directly, things get more complicated. There's no way to have a reference to another member within the same struct, so storing both a 'Vec` and some iterator to it will not be possible. Storing an index instead would certainly work, and anything else would probably require use of `unsafe`.
It's not obvious, but weak pointers keep memory allocated after a node has been dropped - they have to because that allocation also holds the reference counts. Recursive drop means that dropping a linked list with backwards-pointing weak pointers requires O(n) stack depth. If you have `A -&gt; B -&gt; C` (strong pointers) and drop the last pointer to A, the drop happens like this: - Drop A - Drop B - Drop C - dealloc C - dealloc B - dealloc A If you drop a tree, n is the height of the tree of nodes which are actually dropped. The `drop` method is called pre-order, and memory is deallocated post-order. The size of those stack frames depends on how well they're optimized (ideally as little as one pointer per level). `Rc` is also a lot of overhead for small nodes. Two `usize`counters in the node - which is hugely overkill for most graphs.
You need to put every library (or plugin instance) in its own process, and then use pipes, shared memory, or something else to communicate with the process. That's the only reasonable way you can protect yourself from a buggy or malicious library, that could otherwise write to memory with the host program's data in it.
You can launch a separate process to load and run these share libraries, and communicate with it via some kind of IPC. An example of this is [gaol](https://github.com/servo/gaol), used by Servo, which isolates portions of an application to run in a sandboxed separate process. It abstracts over a couple of different OS specific sandboxing mechanisms. There are also alternatives like [rusty-sandbox](https://github.com/myfreeweb/rusty-sandbox) with a different design philosophy, but doing similar things. You then need to communicate with your sandboxed app in some way, which you could do through something like [Servo's ipc-channel](https://github.com/servo/ipc-channel), though there are also [a number of other IPC crates](https://crates.io/search?q=ipc) you could consider.
Yeah, as long as you're just packaging python code it makes a platform &amp; python version independent package. Unfortunately because of it's long history, it's hard to find documentation about current best practices with Python packaging. 
Yeah, as long as you're just packaging python code it makes a platform &amp; python version independent package. Unfortunately because of it's long history, it's hard to find documentation about current best practices with Python packaging. 
&gt; Given a `&amp;Cell&lt;T&gt;` I’d love the ability to get cell references to fields of `T`. This ability goes under the name of ‘cell field projection’ and is only referenced in a handful of places. I am afraid that's not possible, at least not without adding another marker trait that says "ok for this particular T". The problem is that I can create a `Cell` with any `Copy` type, including an `enum`: #[derive(Clone, Copy, Debug)] enum Copyable { One(u32), Two(u16, u16), } So, let's imagine I can get a reference to... the second element of `Two`. Cause why not. And then I call: `cell.set(Copyable::One(4))`. What's the value of the element behind my reference? For even more fun, use `Cell&lt;Option&lt;NonZeroU32&gt;&gt;`: - grab a reference to `NonZeroU32`. - `cell.set(None);` - congratulations, your `NonZeroU32` now contains... 0. --- You need *locking* to make this work, to make modifications impossible while any projection exists. You could potentially extend `RefCell`, or implement an alternative to `RefCell`, to support it.
That makes sense, but is there a less resilient way that would have similar effect?
Why are there so many error crates in general and so many libraries that use custom error types if there is `Result`? I understand what `Result`is. An enum that returns either an `Ok()` with an actual value to be used or a `Err()` varient containing an error message. That sounds like a super great idea to me. And I can't think of any reason why anybody wouldn't like this `Result`idea. However: Why are there so many error crates in general and so many libraries that use custom error types if there is `Result`? Why is `Result`not good enough? Please try to phrase things as ELI5 as possible. I'm a noob. Thanks in advance!
I've pushed a new version of [zr](https://github.com/jedahan/zr), a zsh plugin manager. Now it supports arbitrary URLs in addition to github author/name shorthand. Any code review or testing is appreciated, and I'm happy to take a look at your code as well (though I am a novice still).
Yep, enums trivially break this. The thing is... this kind of aliasing is the standard and safe in nearly every other programming language! The key difference is that nearly no other language actually has an enum like Rust has. This is frustrating when you actually want this kind of aliasing, you've done it a million times in C/C++ and in that particular instance it's perfectly fine! (working with strictly repr C data types) The point I was trying to make is that there is a use case for this behavior and I don't mind that I would have to approximate this with unsafe fn/macros but even that is currently not possible.
The error crates are there to produce the values that go in the `Err()` variant of `Result`.
&gt; I'm pretty sure that that's simply not valid in some cases. Yes, this was precisely my concern. :) 
I'm sorry, but I don't understand that. Let's use an example: if something_works { Ok(5) } else { Err(String::from("That doesn't work. Please enter a number between 44 and 66!")) } This is just a theoretical example of course, but how would this not suffice? What else would I want to return but an actual specific error message? 
Your errors are not always just going to be printed to a console. You often want a structured error that upstream code can look at without doing a bunch of string parsing.
This is a fascinating project, and also a very different kind of garbage collector from the ones used by e.g. C#. Thanks for pointing me towards this; I'm really enjoying reading about it.
Hmmm OK. So you mean something like http error codes for example? So that in turn means, that the error crates/types don't replace `Result` but the String in Result (in my example) ?
&gt; So that in turn means, that the error crates/types don't replace &gt; Result &gt; but the String in Result (in my example) ? Yep
That makes sense. Awesome! Thanks so much for your help!!
Compile the plugins to wasm and run them using wasmi or wasmer?
Probably not. Within a process there is, essentially, no isolation. 
`Allocated&lt;T&gt;` seems the most accurate to me, if a bit verbose. Perhaps `Alloc&lt;T&gt;`?
You'll be happy to hear that Haskell is getting linear types (similar to Rust) so this will no longer be a problem soon.
Nice! That's exactly what I wanted. Thank you very much!
Good read. I agree about the prelude stuff since I work with ndarray quite a lot amd have to import `ndarray::prelude::*` over and over again gets tedious quite quick. However, most of your comments seem to be quite negative towards Rust. Any positive features you enjoy or are looking forward to in the new year?
I think you might be overcomplicating things. Take a look at: [https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=7c1a9cd844b41461f310cc611b8e647b](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=7c1a9cd844b41461f310cc611b8e647b)! Let me know what you think.
I'd second nginx for all static files. Look up some guides. Also, hyper is a bit more low level. I would probably suggest rocket
Cool, thanks. What was the workaround, BTW?
Darn, not using ggez? ;-)
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/scala] [Why is Rust successful compared with Scala\/Ocaml\/ReasonML?](https://www.reddit.com/r/scala/comments/abwpsk/why_is_rust_successful_compared_with/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Wrong sub
Thank you very much for this wonderful screenshot and concise title. You are cordially invited to learn to read. Regards.
Thanks everyone for the suggestions. I initially chose hyper because I had challenged myself to review http before school started again. So I wrote the basic server from The Book, then was going to use hyper and from there move on to either Nickel or Rocket. As of now I have my hyper server running and serving a couple files, but I'd like to have an expandable solution for when I'll update my site. Maybe as you said rocket might be a good solution in the long run
Filed https://github.com/rust-lang/rust/issues/57279. Feel free to comment on what would have made the error more understandable for you.
I wanted to use Iterators or something like that to learn a little more about Rust features, like JayDepp answer. But thank you anyways! If things start to get difficult with references or things like that I'm going to end up using exactly what you did.
Paul Graham touched on this [here](http://www.paulgraham.com/hundred.html) &gt;I learned to program when computer power was scarce. I can remember taking all the spaces out of my Basic programs so they would fit into the memory of a 4K TRS-80. The thought of all this stupendously inefficient software burning up cycles doing the same thing over and over seems kind of gross to me. But I think my intuitions here are wrong. I'm like someone who grew up poor, and can't bear to spend money even for something important, like going to the doctor. &gt;Some kinds of waste really are disgusting. SUVs, for example, would arguably be gross even if they ran on a fuel which would never run out and generated no pollution. SUVs are gross because they're the solution to a gross problem. (How to make minivans look more masculine.) But not all waste is bad. Now that we have the infrastructure to support it, counting the minutes of your long-distance calls starts to seem niggling. If you have the resources, it's more elegant to think of all phone calls as one kind of thing, no matter where the other person is. &gt;There's good waste, and bad waste. I'm interested in good waste-- the kind where, by spending more, we can get simpler designs. How will we take advantage of the opportunities to waste cycles that we'll get from new, faster hardware? Given the increasing role algorithms play in allocating real world resources - I'd much prefer slower implementations if it means the people responsible for maintaining them have an easier time doing so. The energy used by a program is at worst linear with the number of processes. The energy used by a real-world bug produced by that program is potentially unbound. That's not to mention the security issues that lurk with unmanaged memory access. Don't get me wrong. There is a definite need and use for C, Rust, etc. I just don't trust them in the hands of the average developer. 
`Rc&lt;T&gt;` and `Arc&lt;T&gt;` also allocates a `T`. Even a lone `T` on the stack is also allocated: just stack allocated.
Clean coal is coal that you scrub with a cloth and soap, duh!
That's what rocket is designed for, it's built on hyper internally and rocket code is less time consuming to maintain than hyper 
Maybe if the variable is inside an unsafe block, it can be used before initialized
Thanks for the links, very helpful!
Thanks!
Instead of preludes it could be great to have autoimports in rustanalyzer/RSL. It looks like intellij-rust already has this feature [issue](https://github.com/intellij-rust/intellij-rust/pull/2338)
I love the Idea that I get different useful libraries with rust, but keeping them away from std.
Please read this: https://doc.rust-lang.org/book/ch07-02-modules-and-use-to-control-scope-and-privacy.html
The suggestion [here](https://www.reddit.com/r/rust/comments/abq8vz/memory_allocation_crash_in_piston/ed2oeln), I believe.
Thanks very much for clearing this up - I understand it much better now.
What are const generics? I can't find anything besides the github issues 
Examples of idioms for nested `struct`s/`enum`s with interesting contents like containers and indirection (`HashSet`, `HashMap`, `Box`, `Option`)? I'd like partialeq and clone semantics for these data structures. I find myself implementing these because they can't be automagically derived. I understand why they can't be derived but I just want simple recursive/transitive behavior in order to exhaustively compare/copy the contained fields. Yes, I know this may be expensive. I'd like to have a best practice/example to follow. Right now I have things like this to implement `PartialEq`/`Eq` ... self.some_set.iter().zip(other.some_set.iter()).all(|(lhs, rhs)| lhs == rhs) &amp;&amp; self.some_mapping.iter().zip(other.some_mapping.iter()).all(|(lhs, rhs)| lhs == rhs) &amp;&amp; ... 
Ah thanks a bunch, i'll apply that soon. I haven't heard of appimage, but i'll give that a shot. 
Huh, looks like the link is going to the right place but their site has more details and their docs. Maybe I'm misunderstanding what you are looking for - FusionAuth is a tool for user authentication and management. Their docs define a lot of functionality that auth solutions can incorporate. Thought it would be useful as a reference for you. 
Some crates also predefine a specialized Result&lt;T&gt; which is actually just a Result&lt;T, E&gt; where E is a fixed error type that makes sense for that crate. For example [std::io::Result](https://doc.rust-lang.org/std/io/type.Result.html).
You should try out intellij-rust and see if it's not closer to your c++ and python experience. imo IDE support is the most important thing for rust adoption, and the intellij plugin is much farther along than the RLS.
Oh, thanks for writing that. I tend to use it whenever I forget where I put something in code. Though, the optimizations that I read about at some point in your blog (or readme) are probably wasted at my scales.
You can't do that, as many languages have the ability to change any byte of data allocated to its process. Even bytes that are not related to variables and stuff (like return pointers and code jumps). Some languages even allow you to just write plain assembly, which breaks every and any paradigm you might be programming into. &amp;#x200B; So yeah, the only way you can guarantee the library will not mess up your data is by spawning it in a new process.
Const generics refers to the possibility to use some constants (usually integers) as types, usefull for arrays and matrices in particular, like Matrix&lt;N,M&gt;
I mean to write a blog post in response, as i've been thinking about the name-allocation problem for quite a while, but it's taking too long so to abbreviate: I reserve names for projects not yet public, and names for potential projects not yet begun but on my (long-term) agenda. The reason i squat so aggressively is, alas, others also do so (for examples: [1](https://crates.io/users/mahkoh), [2](https://crates.io/users/swmon)), and squatting is not penalized or even officially condemned. I think centralized name registries are fundamentally flawed, and the answer is ultimately a [petname](https://en.wikipedia.org/wiki/Petname) system of some kind. For Cargo, it means custom package repositories, which feature i believe is in the works. 
This is the RFC: https://github.com/rust-lang/rfcs/blob/master/text/2000-const-generics.md Amongst other things, they will allow us to write code which is generic over the length of an array.
It also fits with some other transport related terminology that Rust uses such as 'crate', and 'cargo'.
I like OCaml a lot, and would love to use it more, but these things always end up making it not worth it, and I go back to Scala. Scala's design is much less clean than OCaml, but it's also a bit more powerful. And most importantly, Scala has a very good ecosystem. The Scala Build Tool (SBT) has gotten much better syntax-wise, and it allows you to set up a build pretty easily that _will work_ everywhere _any_ version of SBT is available. Pretty similar to Cargo, but with the added flexibility that comes with the JVM and platform-independent JAR artifacts (you don't have to compile your dependencies).
Hmmm so that means that there is always only ONE type of error that is being returned if there's an error? (for that specific crate) Why is that a good thing? To make it easier and more predictable for the developer?
Heh, I didn't intend for it to come out so negative! Looking at the new stuff coming to Rust they're nice to have but nothing stands out as a must have feature to the kind of programs I write personally. Rust does have features I enjoy immensely: * Cargo continues to amaze me. I have some experience with cmake (C++) and nuget (C#) and neither comes even _close_ to the kind of project management that cargo offers. Specific experiences with cargo are its dependency management, conditional feature management and build scripts, they just feel so polished it's a joy to use these. This alone is a big reason why I'll never look to go back to C++ for hobby projects. * Error handling is just superb. Rust taught me the distinction between recoverable and unrecoverable errors and the value in handling these separate (return values vs panics). * Enums just blew my mind when I started working with Rust. Patterns that are awkward in C++/C# 'I accept 2 kinds of input for this parameter' or 'I want to return either A or B types' just aren't easy to express and I kept stumbling over these when writing my own APIs. * Non brain dead macros (coming from C macros) are utterly fantastic and enable patterns I wouldn't have dreamed about in C/C++. I heard there was talk about implementing a new kind of macro by example scheme in form of the macro keyword instead of macro_rules. Idk what kind of advantages these new macros have over the old ones, but it's something I may be looking forward to. * I get to have _all_ of this without garbage collection?! (I tend to run my code in places where GC is a non starter). Holy fuck sign me up. This is just stuff I can say on the spot. This is what makes Rust nirvana to me, what makes Rust my personal programming heaven. Sorry if I'm laying it on a bit thick, but Rust changed my understanding of programming for the better.
&gt; Customizable prelude I think this optimizes for writing code instead of reading code. Often i think it is better to optimize the other way so that new readers of a code base have it easier. How will new readers of your code base find where types come from?
Even more confusing when you realize you can have multiple heaps. Windows has an OS API for managing multiple heaps that each have their own set of allocations.
Thanks and I tried IntelliJ IDEA before. Unfortunately at the time I was already used to VSC and there's an uncomfortable amount of keybindings I learned which were different in VSC which were different in IntelliJ IDEA. While I'm sure that I could make the experience similar, in the end familiarity with VSC won out. The experience with intellij-rust autocomplete was much better than RACER could offer, but I didn't explore much further. 
&gt; For quite a long time you had: &gt; &gt; - High Speed &gt; - Low Resource Consumption &gt; - Safe &gt; &gt; You could only pick two. Rust changes the game here. Well, I'd say that's a pretty reductive list of desirable things. How about these other ones? - fast compilation - very fast incremental compilation - platform independence (compile once, run everywhere) - low syntactic overhead, good type inference - etc. You don't always want them all, but they're pretty often needed, and Rust currently does not do well there. OCaml actually fulfills _all_ of these points very well, and the ones above too (but of course not as well as Rust).
That looks interesting and I heavily rely on this feature in my work in C#.
My understanding is that volatile is basically created to handle memory mapped IO. It doesn't quite solve the problem that it's annoying to access fields of structs behind pointers. In my case I wasn't doing memory mapped IO, rather in my driver I was messing around with switching userspace around, which means pointers into a particular userspace are not valid after the switch has happened. I need to ensure that reading the values out of the pointers happens before the switch is done (which is just an API call). Bypassing the annoyance of raw pointers by making them into unique refs could technically lead to llvm generating code which delays reading/writing the variables across the call which switches userspace. It didn't happen in my case, but it could. I could just suck it up and use `(*raw_ptr).field` instead of doing what I'm doing now.
&gt; It's no longer a trade off between dev time and performance. You get both. For free. &gt; &gt; For free? There are most definitely costs, otherwise everyone would just use Rust instead of Python. Here are just two: steep learning curve due to the advanced type system, and slow compilation.
I am unsure about investing significant effort into making more stuff rustfixable, *with current compiler/ide architecture*. I’d rather wait until we can have true “faster then human reaction” on the fly error checking which works with incomplete code. I feel that this will require a significant re-engineering effort, which will lead to different compiler API. If we add a lot of IDE features on top of the current infrastructure, then we’ll have to migrate them.
Fair enough, but I think there's a kind of balance to be struck here. The same argument could be made for not having a prelude at all and just require everyone to import std::string::String and friends. The idea here is that there exists a set of common types/traits that are common in the domain you are working, that everyone reading your code is expected to know and understanding (such as in the case of the standard prelude). The idea is then that there are other domains in which other types are expected to be so common as to not need an introduction. Of course overuse could lead to issues but such is the way with all things in life. Finding the source of these items should not be an issue and solved by whatever viewer you are using. F12, hyperlinked or good ol' ctrl-F.
What's the deal with the RFCs this week ?
Yeah, with better ide support it matters less because you have inline docs or jump to definition..
&gt; I'd argue that Scala also doesn't do this particularly well. There doesn't seem to be a single really good package manager for the JVM, and while a couple of options exist and work well, they are generally aimed more at enterprise software, rather than personal projects or startup strawmen. I don't think that's true at all. The widely accepted standard build tool for Scala is SBT. Here is all you need to set up a reproducible SBT build: // build.sbt scalaVersion := "2.12.8" libraryDependencies += "com.lihaoyi" %% "sourcecode" % "0.1.4", /* ...add other dependencies here */ // project/build.properties sbt.version=1.2.1 With this, you can build the project provided _any_ version of SBT is available on your system. Just type `sbt run`, and it will run your program after automatically downloading the specified versions of SBT and of the Scala compiler, along with all dependencies, as already-compiled JARs (and it will cache these globally).
To elaborate a bit more on my SO comments, I think it sounds like you'll benefit from a new crate I'll be releasing soon called `regex-automata`. I've uploaded the docs to a temporary spot so you can see whether it fits your use case. The docs aren't done yet, but all the individual API items are documented: https://burntsushi.net/stuff/tmp-do-not-link-me/regex_automata/ `regex-automata` was built out of a use case I had for shipping DFAs in a binary with lightweight runtime requirements (so this crate was essentially a yak shave). Basically, it drops support for (reasonably) fast compile times, capturing groups, anchors and word boundaries, but in exchange, you get full ahead of time compiled DFAs that can be serialized to disk. Deserialization is then a constant time operation. There are also a ton of options that let you configure your exact space/time trade off, including two completely different representations of a DFA: dense (fast match, large space) or sparse (slower match, small space). Because of the lower level nature of this crate and a commitment to DFAs in particular, the crate exposes a [`DFA`](https://burntsushi.net/stuff/tmp-do-not-link-me/regex_automata/trait.DFA.html) trait, which will actually let you write your own match routines. Match routines are [pretty easy to write](https://burntsushi.net/stuff/tmp-do-not-link-me/src/regex_automata/dfa.rs.html#84), so it's very feasible to do it. Since you can crawl the DFA transitions using the public API, you can pretty much impose whatever kind of incremental strategy that you want.
It is possible to write an explicit destructor to avoid that recursion. This one https://github.com/kuchiki-rs/kuchiki/blob/66d8e2bcb/src/tree.rs#L134-L194 has an explicit `Vec&lt;Rc&lt;Node&gt;&gt;` stack but for trees (as opposed to arbitrary graphs) I suspect even that could be removed by walking back up through `parent` weak pointers instead.
I can testify that GCC is designed by engineers who mostly care about architecture, not politics. Indeed in such a big project politics shows up but so it does for Clang/LLVM.
&gt;It is a myth that developers sit around dreaming up and implementing data-structures all day. Programming classes often teach the implementation of basic data structures early on. I think this gives people the impression that implementing basic data structures is a big component of what programmers do, even though in reality it's pretty much negligible. A lot of programming job interviews ask lots of data structure questions; no doubt that's also part of the problem.
Huh, I did not know about SBT. I stand corrected.
The number of jobs available in Rust isn't even within an order of magnitude of those available in Scala, especially when you consider Spark. &amp;#x200B; Async I/O is much easier to do in Scala than in Rust with similar performance.
Ah, yes. I think I was confused about the distinction between "lint" and "lint with suggestion". Thanks!
Alright, linux build is up: https://agmcleod.itch.io/energygrid
\&gt; The problem with Scala is that you can't really learn the language and ecosystem from online resources &amp;#x200B; [https://books.underscore.io/essential-scala/essential-scala.html](https://books.underscore.io/essential-scala/essential-scala.html) &amp;#x200B; [https://www.artima.com/pins1ed/](https://www.artima.com/pins1ed/)
You are awesome. I only saw this message just now (I don't use Reddit much/enough), and I'm on my way to bed, but I will read through it properly next time I get some time to sit down with Rust and see how far I can get. It seems that I had gotten the general idea right -- just couldn't figure out the implementation. The beauty is that if `WithFactory` wasn't private, then you could implement your own traits on your on structs and use them (in this context that isn't really needed, but it's a good trick to have for the future). I got as far as your first code block when I was looking into it, but got kind of overwhelmed at that point and gave up. Again, thank you. No questions for now, but I'll take another stab at this. :-)
Not sure where to report this but pretty sure Embard Studios should be Embark Studios.
Working on [img\_diff](https://crates.io/crates/img_diff) a cli tool to diff folders of images. Been since this is my first project I have been using it to explore rust, currently over in [this branch](https://github.com/Mike-Neto/img_diff/blob/drop-dssim-dep/src/lib.rs) I'm trying to come up with a good enough trait/type parameter to avoid having branching code for different image types. &amp;#x200B; I'm currently blocked as I'm trying to change DiffImage struct so it no longer uses an enum. Would appreciate any insight someone can give me as well as a code review if you will ;)
People seem to forget that we are not sacrificing speed or using more cpu time because we don't like going fast. There was a reason to begin with, which is streamlining stuff. I think the important take away is: the reason we use more cpu and ram to do simple things is because **we don't have good tools to do this efficiently** (and here is were Rust would enter I guess, but it's still a long way in productivity in comparison to a language like Scala).
I honour you, lad, and will buy that game for sure. =)
I also was confused with rust async at first ( it took some time to understand how all of that related). Good place to start is to read docs from tokio website as it explains futures and some parts of tokio. Main async lib is tokio, tokio uses futures and mio. Actix is build on tokio. So if you want to learn async you need to start from futures and tokio in my opinion :) Tokio docs are still incomplete but provide good explanation of futures. Also rust on nightly has async/await in the core it simplifies quite a few things but it is not yet in release :( i am waiting for that. 
Wow
Do you have a code example where you'd like to do this? You should be able to derive those traits, and `HashSet`, `Box`, etc. do implement those traits when their contents implement them.
You aren't missing anything. Your approach is more rustic, but isn't required to make this work. A function is conceptually simpler. I actually prefer something along what you have except with update and view in traits of their own (you could have a static site that only has a view).
Thanks! I thought I could answer this quickly but suddenly I wrote this huge comment… 😅 I might actually make a blog post out of it. It's a pretty neat pattern.
Tsk, shame on me, seems like I jumped to the solution. So I suppose the real problem is that the underlying elements in the containers omit those traits. I will work on an example but I expect I'll confirm this theory in doing so.
Thought about this today, you can compare listeners if you use plain `fn` pointers in them instead of closures. The downside is they can't capture any state, but you can work around that by making users give you plain `fn` callbacks and inserting a fancier shim between that and `web_sys` when you register the closure with the DOM. This fits in with the elm architecture as the view isn't supposed to have any state in it anyway.
You can avoid blocking the actix-web requests by doing the potentially blocking operations on a SyncArbiter. See the (database section)[https://actix.rs/docs/databases/] for an example. Unless you need to chain a request through multiple arbiter, then the async stuff shouldn't be too bad.
When you list the JVM as a downside, you also don't like the huge ecosystem of good quality libraries and monitoring tools you get for free with it?
Sounds amazing. &gt;dense (fast match, large space) or sparse (slower match, small space). That does not sound right.
It should be possible for types that have provably separate fields. The [RFC 1789](https://github.com/rust-lang/rfcs/pull/1789) already makes the case for slices. Enums don't have this property but structs do.
Awesome, thanks! Does the Tokio website explain how Futures.rs, Mio, etc relate? Or would I seek that info somewhere else?
Here's something else I noticed, relevant if your structs are generic. It doesn't seem like you should need constraints at the struct definition in order to derive, [but you do](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=213dd3e7cd84652d52544c515ab47a85).
It's a dense *representation* vs a sparse one, not dense *data* vs sparse data. For example, a sparse representation of a DFA state's transitions might be `Vec&lt;(u8, usize)&gt;`, where there are exactly as many elements as there are non-dead transitions. So in this case, each state has a variable number of explicit transitions. In a dense representation, a DFA state's transitions might be `Vec&lt;usize&gt;`, where there are always exactly `len(alphabet)` elements for every state. If you've ever used sparse or dense vectors, then it's the same deal there. e.g., https://pandas.pydata.org/pandas-docs/stable/sparse.html
Also, its short. Having to type “HeapPointer”, or “HeapAllocated” would get tedious quick (Look at C++ with their unique_ptr).
At least in the US, many people are off on vacation most or all of the last week for the various holidays. 
Not sure it covers mio very well, but futures do especially in [**Working with futures**](https://tokio.rs/docs/futures/overview/) **section, you can check this** [Article](https://cafbit.com/post/tokio_internals/) for some more background about how all of that are related (some info is outdated). The important part to understand is that tokio is just a runtime which takes futures in and polls it and when futures are ready it will output some data (which you would code in). Because tokio has so many abstractions it is hard to catch up what is actually happening and makes it more confusing but as soon as you get the basing futures &lt;-&gt; tokio understanding everything kinda sums up :)
&gt; everyone would just use Rust instead of Python That's kind of a silly extrapolation, but I do think the future will see Rust grow much faster than Python, and I happen to think Python will shrink (personal opinion unrelated to this discussion). I think Rust will become more viable than Python in the web server space eventually and nearly as viable in the scripting space, leaving Python precious little domain in the corporate world. Python has plenty of complexity hiding within it, and I think Rust web servers will eventually be almost as easy to write while being way, way faster and crashing way, way less. (ML and scientific computing are another beast entirely, where I think Python will continue to be a DSL wrapper around more performant languages) &gt; For free? When I say "for free", I mean for me personally when I write something, not for any dev ever. There will always be people who just write scripts, and that's fine, but they probably won't be writing much software that I'll be running. I consider slow compilation to be essentially a non-issue for this kind of task. The only time it's a major issue is when you build a large project or crate for the first time. I would much rather compile a very type-safe program than interpret a program that isn't type safe.
He also had the python code as a template for his Rust, so he gave himself a head start on the Rust implementation.
-- Mark Train
Thank you for the example. Initially, for whatever reason, I thought Rust's std lib might provide this kind of functionality. After thinking about it, that would be unlikely. I think there might be away to do it through regex as well. I'll check out the code that you give first. Thank you again.
Adding features to my recently started toy halide clone https://github.com/theotherphil/prism Doesn't do much yet, but you can at least JIT and run a trivial image processing pipeline.
It's a fairly common pattern, too. Plenty of project ship "contrib" modules where the developers make less guarantees about the code quality or stability.
Sure, I understand where you're coming from. In this particular case, my second-choice name grew on me, and now I prefer it - it more accurately conveys the unique twist I'm giving to the problem. Maintaining a public namespace is a hard problem, and I think the crates.io people have found close to a local optimum - a lot of the things people have proposed have strong downsides. I'd be willing to give a petname-type system a chance, but think it would need to prove itself.
Here is a book I found helpful when I started looking at async. https://aturon.github.io/apr/
This is a cool project, thanks for sharing. One thing that would have been helpful while reviewing the readme is some comments that describe the \`new\` methods for \`Action\` and \`GraphPlan\` types. &amp;#x200B; I know data structures in Rust can be difficult, but have you tried implementing this using more shared references and less cloning? 
I am happy. I just wish it would come faster, and be non-optional. I'd also appreciate being able to run it without a heavy C runtime, but that would require making a completely new language.
I'm not sure you're getting any benefits by wrapping the pointer with a Rc. It doesn't save you from throwing "unsafe"s all over your code and all the mutable borrows you seem to need are going to happen in self-contained unsafe blocks, so they would've be released by the end of it anyway. &amp;#x200B; I'm also not sure if you get anything from dividing your unsafe block into 2 parts, as you did in Tree::new. &amp;#x200B; ```rust pub fn new(earth_object_ptr: &amp;Rc&lt;C_Earth_Ptr&gt;) -&gt; Self { let tree_object_ptr = unsafe{ C_Tree_alloc() }; unsafe{ C_Earth_create_tree( *(*earth_object_ptr), tree_object_ptr )}; // Cloning Rc here so the end-user does not have to do this Tree{Rc::clone(earth_object_ptr), tree_object_ptr} }
In the US (where I imagine most of the contributors are), a lot of employees have the week between Christmas and New Year's off.
I just maxed out the "pay the developer extra" option just because of the fact that a) there's a linux build b) it's written in Rust. I heard someone who built products using graphene say "I don't want people to buy it because it's graphene; I want them to buy it because it's better". Although, in these early stages, I think that it's very important for all of us to rally behind anything of quality written using the things we love. And, on top of that, and to graphene-guy's point - the game looks fucking addicting and fun. Thanks for all of the effort this must have taken to produce, and for the balls / (ovaries?) for building it with Rust.
Using native-endian numbers for the length is going to be a problem for use with programming environments that aren't aware of the platform's endianness (such as bash or Python). I feel like it would have been better to settle on either always little endian, or always big endian
That's fair I should mention that as an upside to Scala in some situations. That said of the services I am building my monitoring needs are mostly met by the whichever container orchestration system I'm using and I can only think of one case where the JVM's large ecosystem was absolutely essential (SAML integration). Scala isn't bad but for my use case the benefits aren't as big as the detractors.
Thanks for taking a look and the feedback. I think I backed out of using more refs after some lifetime issues, but it was more me being scared. Taking a step back it feels like I could do more to use references of props, actions, and maybe mutexes as they are immutable anyway. Do folks normally think through all the references and lifetimes as they are writing Rust? Maybe that’s just a sign I have no idea what I’m doing with borrows :-)
&gt; The thing is... this kind of aliasing is the standard and safe in nearly every other programming language! Is that really true? C has it, but notably C++ actually makes it UB, though it'll work on most major implementations.(Though i think only GCC actually guarantees this, under certain circumstances, also depending on if you enable strict-aliasing or not) &gt; This is frustrating when you actually want this kind of aliasing, That said, i believe Rust allows this kind of aliasing. See the [Unions](https://doc.rust-lang.org/stable/reference/items/unions.html) page &gt; Inactive fields can be accessed as well (using the same syntax) if they are sufficiently layout compatible with the current value kept by the union. and the [Aliasing](https://doc.rust-lang.org/nomicon/aliasing.html) page Basically, no strict aliasing. See also [previous discussion on it](https://www.reddit.com/r/rust/comments/73x370/does_unsafe_rust_have_strict_aliasing_rules/)
&lt;3 thanks for the kind words
I know that you're seeking feedback on the Rust code, but my first thought here was for the licensing on the font you've included in your repo. Unless I'm mistaken (which could totally be the case), the font at `src/JosefinSans-Thin.ttf` is [this Josephin Sans](https://www.fontsquirrel.com/fonts/Josefin-Sans). If that is the case, then it's a free font, but the licensing includes the following instructions in its permissions and conditions &gt; Original or Modified Versions of the Font Software may be bundled, redistributed and/or sold with any software, provided that each copy contains the above copyright notice and this license. These can be included either as stand-alone text files, human-readable headers or in the appropriate machine-readable metadata fields within text or binary files as long as those fields can be easily viewed by the user. You might want to add the appropriate copyright notice and licensing to your repository to be in compliance. Additionally, if you wish for this code to be open source you might consider licensing your own code. Both [choosealicense.com](https://choosealicense.com/licenses/) and [opensource.guide](https://opensource.guide/legal/#which-open-source-license-is-appropriate-for-my-project) have some good information on that topic.
Is there an IntelliJ GitHub PR Review that would let you see all that info (like parameter names) in context there, too?
I'm not clear on what you want. You want to have preservation of the whitespace that was between the tokens as they appeared in the source file? 
Exactly, without having to resort to “”
You probably should check out [hashbrown](https://crates.io/crates/hashbrown).
I'm not an expert, but I genuinely don't think that exists, based on what I know of how the token tree and token stream types work.
Brilliant. Purchased. 
They'll read the docs of the customized prelude, of course. It'll show everything that's exported.
[removed]
You can also use locks in an async environment using the [qutex crate](https://crates.io/crates/qutex).
With the single and close-to-useless exception of reading its capacity.
The exception that proves the role, eh?
Wouldn't that be too verbose in the api? Could you provide an example? Closures seem to be the only anony function with easy syntax, and even trivial cases don't compare, as far as I can tell.
Hey, our smart pointers are trying their best, ok
Worth considering, although I feel it's visually cleaner to keep things as standalone functions, and save an indent level.
```rust fn not_fun() {} fn main() { let fun: fn() -&gt; () = || (); println!("{}", fun.eq(&amp;fun)); println!("{}", fun.eq(&amp;not_fun)); } ``` Try that. I didn't try to build it so there may be some syntax errors. On mobile.
Wrong subreddit 
Thanks - it was the eq syntax I was missing. Need to look into that
What I have just been doing is doing the [DAO](https://www.journaldev.com/16813/dao-design-pattern) pattern over the database queries. For example, ``` pub trait UserDao { fn getUser(&amp;self, userId: i64) -&gt; Result&lt;User, Error&gt;; } struct UserImpl { // ref to db connection // impl getUser etc... } ``` Then get the UserImpl struct to implement the UserDao trait which should only be diesel code. Typically, one will not test the diesel code directly as that is the job of the Diesel repo itself to test its own code the point is to isolate that code behind a trait. Then you can write your mocks and stubs against this trait. However, if you are trying to test the correctness of the queries themselves then one will need to consider some sort of integration test using a docker instance or something like that to emulate your production database.
I see. I'm not there yet with integration testing, for now I just want to get through unit testing but the docker idea sounds pretty good. Thanks for the advice. 