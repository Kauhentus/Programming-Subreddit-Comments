I'm having trouble figuring out how to compose `failure` and `tokio`. Here's a function that returns a `Future` for a hash digest of a file: use failure::Error; use openssl::hash::{DigestBytes, Hasher, MessageDigest}; use std::path::{Path, PathBuf}; use tokio::prelude::*; use tokio_io::io::AllowStdIo; use tokio_fs::File; fn digest_file&lt;P&gt;(path: P, alg: MessageDigest) -&gt; impl Future&lt;Item=DigestBytes, Error=Error&gt; where P: AsRef&lt;Path&gt; + Send + 'static { File::open(path) .and_then(move |f| { let hasher = AllowStdIo::new(Hasher::new(alg).expect("openssl hash to be created")); io::copy(f, hasher) }) .and_then(move |(_, _, mut w)| Ok(w.get_mut().finish().expect("openssl hash to finish"))) .map_err(|e| Error::from(e)) } My problem is handling the `openssl` errors that can arise with `Hasher::new()`/`Hasher::finish()`, so for now I'm just using `expect()`, but I'd prefer to learn how correctly to convert the error and return it to the future.
I'm not sure what you mean exactly - you can still put up an enclosing scope in C++, and if the points is shared, of course it needs to outlive the mutex lock.
Do you spawn the same constant number of random rays on each collision?
So well expressed! You've put words to a good chunk of what I've been feeling lately--thank you for taking the time!
Only for the initial rays, not subsequent bounces.
One could use `std::num::NonZeroUsize` (nightly only at the moment). This would require subtracting one when indexing and adding one when creating the index, so the first element can be accessed. https://play.rust-lang.org/?gist=b0e6b1302d090be3be7949b34a8ccff3&amp;version=nightly&amp;mode=debug
It will be to me at least :) Where will the video be uploaded?
Neat! I've thought that we need an ability to say "`usize::max_value()` is not a valid bit pattern", which we can't express at the moment. But adding and subtracting ones works as well!
Leaking memory isn't the issue here. Releasing a mutex while you still have a pointer to the data in it leads to data races, which are undefined behavior.
There was an RFC to add this (search for "trait fields"), but it ran out of steam. I would like to see it, personally! 
You may effectively leak the pointer, sure to use it's unsafe but it may be wrapped in another independent mutex.
What you are getting at might be similar to what Aras tried with his buffer based approach here http://aras-p.info/blog/2018/04/19/Daily-Pathtracer-11-Buffer-Oriented/.
So for the initial rays if the number of rays spawn is a multiple of the vector length (4, 8, ...) SIMD would still work on a per ray basis. For the subsequent bounces you are pretty much out of luck :/ 
I'm not talking about the underlying value, I'm talking about the reference to the value that the mutex guard gives you (if you wrap the data in the mutex, as in Rust). In this case, it's trivial to hold the reference to the *unlocked* value (not just the mutex) beyond the point where the mutex guard is dropped, which is exactly what we were trying to avoid since it leads to undefined behavior if you dereference it while another thread has the lock. It's not just about reusing the reference in the same function, either--you can place it in another data structure, or send it to another thread (both of which are usually discouraged in modern C++, since it's too hard to get the safety right, but these are perfectly reasonable things to want to do and it's routine in Rust).
Obviously it is possible to use a mutex correctly in C++, if you are careful, even without a Rust-like wrapper structure. But the OP was complaining about this being an unfair comparison, and I don't think it is: the Rust version does not require you to be careful in this way to avoid data races, and the C++ version does, and that's not just a function of the way the author chose to present the data structure.
You are right
Congrats on releasing! The [README.md](https://README.md) is really approachable. If a use\-case for Lambda pops up at work, I'll be sure to try this!
Isn't all of this custom C++ used for SVG: https://searchfox.org/mozilla-central/source/layout/svg
Yeah that won't get fixed until specialization makes it through in some form or another. 
How could an RFC be accepted if there's no consensus?
Thank you.
I mean a wider consensus. I have my criticisms but I have no doubt the language team likes when solutions make as many people "happy" and productive as possible.
Try running `rustup component add rust-src` and see if that makes a difference.
yes, assuming you want it to live on the heap.
I was mostly referring to the fact that I was basically trying to reimplement the async/await system without knowing about it already being a part of rust and how it can be used.
My point was, what happens after an RFC is accepted? &gt;Once an RFC becomes "active" then authors may implement it and submit the feature as a pull request to the Rust repo. So up until that point the feature has never been implemented in rustc and vetted in a production-like environment (nightly). &gt;Being "active" is not a rubber stamp, and in particular still does not mean the feature will ultimately be merged; it does mean that in principle all the major stakeholders have agreed to the feature and are amenable to merging it. So all agree the feature is worth a try, but we have yet to "prove" that it is a good fit for stable Rust. So what is the process to prove that this feature is indeed good (by actually testing it in nightly)? and how do we go about rejecting a feature that seemed good at the time the RFC was being evaluated, but the vetting process showed had too many issues? My take-away from all this discussion on `impl Trait` is that people don't know which features of the nightly compiler are serious contenders for stabilization. I know that the process today is to create an issue in the rustc repo, but that doesn't seem very official to me. As an example take `concat_idents!`. To me the consensus seems to be that it is useless, but it has gone through an RFC since it is in nightly. I don't know the actual history of it, but if we imagine it went through the current RFC process we now need a process to get rid of it. And this process needs to be just as official as the one that got it into nightly in the first place, otherwise we cannot trust that the removal process is as rigorous as the addition process. I don't think using new RFCs to cancel out previous RFCs is a good way of going about thatm since we then lose the ability to distinguish between proposals for things to be added, and proposals for stopping things from being added. I made another comment [here](https://www.reddit.com/r/rust/comments/8o0i1b/the_rust_language_and_special_cases/e03eejj/?context=1) about what could be done instead (purely brainstorming). &gt; An RFC is approved, then implemented in nightly, then changes may be made, then stabilization happens. This sounds very much like an automated workflow; after an RFC is merged its basically going into the language at some point - barring some extraordenary discent from the community. I know this might not be what you intended to get across, but it does show how the process may lack a checkpoint between RFC and stable. (A checkpoint where the community comments on everything that might get through). &gt;It’s not clear to me why a build of rust is easier than tossing a feature flag in your code; Using the channel infrastructure was just a stray thought, so don't take that as a serious proposal. I was trying to think of a way to divide nightly into experimentation-oriented features, and features we know are going into stable. Basically we have 2 things to think about when stabilizing a feature: 1. Is it good for the language? (which is the point of all this discussion), and 2. Have we implemented it correctly? Right now, nightly is for both, but maybe we can find a way to separate them. You might say the beta channel is for case nr. 2, but then we need a process to promote stuff from nightly to beta and not have it as a semi-automatic workflow. Lastly, I just want to say that I have only followed/used Rust for 1.5 years, so I'm sure I'm ignorant on many things and my writing will probably reflect that.
&gt; I seem to be struggling to find rust's malloc though. The typical approach is to use `Vec`. It doesn't currently allow specifying alignment beyond the type, but one can allocate slightly more space and then shift the start to be aligned, `Vec::with_capacity(num_elements + alignment - 1)`.
I don't think it's a big pain point. It's boilerplate for sure, but it's easy boilerplate to write or macro away. I often define newtypes that rename internal methods, use pre-computed argument in them, or hide them. So any alleviation of this boilerplate is likely to be too complex to account for these, or it's not going to account for them and now you've got multiple ways of doing newtypes.
no libfabric love?
Ah, Vagrant. Yes, it's definitely a more complete solution, but I suspect it takes long enough (especially if you use *all* those VMs) that you'd want to only use it for CI, not for regular `cargo test` runs while developing.
I would also like an answer to that. Personally, I think all languages should strive for a continuously decelerating rate of change, which logically ends in no changes at all (X years down the road). A programming language is the foundation that solutions are built on. Therefore, the functionality it provides should be enough to allow for all solutions to be implemented on it. Since most problems can solved using some combination of previous problem's solutions, the need to expand the foundation should be rarer the more time goes by. 
Neat!
I think this would do it https://github.com/llogiq/optional
Also note that `#[panic_implementation]` was only added so that the embedded/no_std community has a stable mechanism for this now. The corresponding RFC 2070 is titled: "stable mechanism to specify the behavior of panic! in no-std applications"
If I implement `Deref` for MyStruct(T), does that mean I can use the inner type directly, instead of having to do `mystruct.0`, but still only accepting a type of MyStruct as an argument to a function, and not accepting T?
Yes, but more importantly it means that your struct will be coerced to a `T` any time it's required and this applies to anyone that can access your type, not just you. For example, if there is a function that requires a `&amp;T`, `&amp;MyStruct&lt;T&gt;` can be passed instead. See deref coercion in [the docs](https://doc.rust-lang.org/std/ops/trait.Deref.html#more-on-deref-coercion)) and [the book](https://doc.rust-lang.org/book/second-edition/ch15-02-deref.html). It's mainly meant for smart-pointer-like types and is considered an anti-pattern using it to emulate object oriented inheritance.
So that means I can pass MyStruct for &amp;T, but not MyStruct for T? Thats kinda unfortunate for my use case, which is associating a u32 with some additional methods, otherwise I would just use an alias.
Says it's up to date. 
This is really not comparing apples to apples. A template in C++ could easily wrap a value with a mutex so that access is managed correctly. There is a lot of value in rust having its mechanics uniformly enforced without creating boundaries to using various disparate libraries, but comparing poorly structured situations in C++ doesn't help anyone.
&gt; A interesting real-world example of problems with direct references is, for example, JSON Moving away from a tree to even just a DAG (never mind creating cycles) means you have to deal with problems like exponential blowup though. I think for 99% of cases a plain tree is the right choice.
Correct. The signature of \`Deref\` is \`&amp;self \-\&gt; &amp;Self::Target\`, so it's a reference\-to\-reference conversion. Maybe you want a trait? Like \`trait U32Ext { ... }\` and \`impl U32Ext for u32\`?
While Microsoft has bought Github, I don't think any immediate reaction is necessary. They haven't changed anything or announced any of their plans, and immediately moving way just "because Microsoft" seems a bit premature. While development and the primary source control for almost all of the rust ecosystem is on Github, this shouldn't be too concerning since most developers have local copies, and migrating git to other hosts is extremely easy. Moving community between hosts is hard, but I mean if anything happens to GitHub, copying repositories to any other host is just a `git set remote origin ~~~; git push` away. One other thing to note is that the repository code Cargo downloads and compiles is all hosted on crates.io, and not directly dependent on github. --- With that said, maybe moving the registry off of github would worth considering? As far as I know, [the crates.io-index repository](https://github.com/rust-lang/crates.io-index) is the canonical source for metadata of published crates. It could likely be moved to the same hosting as the crates' contents, if needed.
- GitHub is still GitHub - GitHub has said that they plan on continuing their current service - It's not the end of the world - Microsoft doesn't own your code - Microsoft technically still hasn't acquired GitHub yet (they just announced the intent but there are more gears that have to turn) - There's nothing inherently bad about who pays for and takes home the profits from GitHub - Don't boycott GitHub until they do something to warrant that boycott All of the reasons that GitHub was the ad hoc center of OS are still true. There's no reason to abandon ship just yet. That said, having a git mirror elsewhere is never bad. I'll just quote myself from Twitter: &gt; At least some people are just mirroring their repositories off GitHub. Potentially some have been looking at moving to an OS host for a while and this was a push to do so. &gt; I expect that the worst case scenario is stagnation and developers diversifying their OS host. I already don't like the GitHub lock-in but that's where people are.
Relevant: [bors-ng](https://github.com/bors-ng/bors-ng), the public re-implementation of @bors, only supports GitHub. Rust repositories are much more likely to be using Bors than any other language. If you personally would like to see more Rust repositories off of GitHub, you could help port bors-ng to not be so GitHub-specific.
[It will be one soon.](https://gitlab.gnome.org/World/fractal/issues/253)
Without getting into the substance of the discussion, I'd just like to say that I heard a recommendation from /u/graydon2 many years ago (I think a little before 1.0, when he was handing off Rust) for a book called _[Come Hell or High Water: A Handbook on Collective Process Gone Awry](https://www.akpress.org/comehellorhighwater.html)_. It's about how _actual anarchist organizations_, i.e., organizations where anarchism is a specific end goal of advocacy/action, still need some amount of internal norms / rules to get anything done, but also about the dangers of letting those rules take over. I don't 100% agree with the book (I lean towards the explicit-CoC side), but it seems like the sort of thing that you may enjoy reading and thinking through. It does specifically cover things like false accusations and banning and calls them out as dangers, but it also doesn't go in the direction of "we shouldn't state our norms".
I'm having a bit of trouble understanding how to accomplish what I want to do with `serde`, is this possible within the `serde` framework? I have a struct for an IP network address: pub struct Ipv4Net { addr: Ipv4Addr, prefix_len: u8, } When using `derive Serialize` and serializing to a text formats such as JSON the result is as you'd expect, e.g. `{"addr":"192.168.1.0","prefix_len":24}`. However this isn't the standard format for a string representation of a network address, we'd normally use just "192.168.1.0/24". So what I want to be able to do is write a customer Serialize/Deserialize implementation for text formats, and convert it to that string representation like above (just as it currently does for `Display`). But then for the binary formats, just leave it to produce the expected output. Is that possible in the `serde` framework? AFAICT it seems to be an all or nothing map to the serde types, no way to map differently based on the target format. 
I'd recommend /u/Quxxy's style completely. There are some rough guidelines in [fmt-rfcs/items](https://github.com/rust-lang-nursery/fmt-rfcs/blob/master/guide/items.md#ordering-of-imports), but those only specify ordering "within a group". I found some discussion in [fmt-rfcs#24](https://github.com/rust-lang-nursery/fmt-rfcs/issues/24) about how groups should be imported. Everyone seems to agree with the "std, external crates, local" ordering, but the issue was closed after only establishing the ordering within groups. I've opened [fmt-rfcs#131](https://github.com/rust-lang-nursery/fmt-rfcs/issues/131) for more potential discussion on this. Not sure if that repository is the right place, but I think it would be good to establish this somewhere.
Something that I think really deserves more emphasis, is the ability to use the crates.io ecosystem while writing an OS. I've been playing around with nebulet, and it turns from an insane task where you have to write everything from scratch, to something where you can pick and chose what parts you want to work on and what parts are "good enough".
Are you that egocentric or are you just trolling?
Thank you for posting this! It broke derive_builder today and I would’ve been very confused if I hadn’t seen this post.
Lol of course I'm trolling. No answer is better than a dumb one is the point.
Nice! I had no idea I could setup my own bors.
&gt; I use git integration but specifically chose only to display the branch name in the prompt for performance reasons It should be possible to refactor `roadrunner` to decide what information to get out of `git` based on what prompt components are actually present. Do the parsing first and separate different questions into different commands. In general anything that just hits the index should always be equivalently fast, but anything that needs to run a diff will probably be slow. Of course, the [new fsmonitor integration](https://blog.github.com/2018-04-05-git-217-released/#speeding-up-status-with-watchman) could theoretically make this much faster, but I haven't looked into it at all.
Thanks @thramp! Consider this a cross promotion of the [crowbar crate](https://github.com/ilianaw/rust-crowbar) which is a the crate this is built on top of :) I've used that successfully in a number of projects. Consider lando to be a layer of generalization specific to the API Gateway trigger exposed using the http crate types
Hi guys. I want to write a function which takes structs and executes an operation. These structs are different but they have common attributes so that the function can run. It looks like struct DEM { x: Vec&lt;f32&gt;, y: Vec&lt;f32&gt;, } struct SPH { x: Vec&lt;f32&gt;, y: Vec&lt;f32&gt;, } struct SPHDEM { x: Vec&lt;f32&gt;, y: Vec&lt;f32&gt;, } fn get_sum(ob1: ?????, ob2: ????) -&gt; f32{ let sum = 0.; for i in 0..ob1.x.len(){ sum += ob1.x[i] + ob2.x[2]; } sum } fn main(){ let ob1 = DEM{ x: vec![1.,2.,3.]; y: vec![1.,2.,3.]; }; let ob2 = DEM{ x: vec![1.,2.,3.]; y: vec![1.,2.,3.]; }; let sum = get_sum(ob1, ob2); }
@Eyedropers I haven't taken a close look at SAM yet mostly because engineers at my company have already adopted serverless framework as a default for writing aws lambdas. I'll have to take a closer look. My general goal here is to give rustlang an easier adoption curve where I work with a common set of tools. I hadn't seen your plugin before. Very neat. I have to give most of the credit to my to [serverless-haskell](https://github.com/seek-oss/serverless-haskell). Reading though its source code was my intro to writing serverless plugins :)
Thanks for documenting both the cases where you improved performance or did not. Really interesting read.
You can use [this trick](https://www.reddit.com/r/rust/comments/8nwr87/tricking_the_hashmap/) to make map to work with two kind of keys: original and borrowed (at the cost of potential dynamic dispatch) without resorting to unsafe. However, this won't help with the second part. First, you cannot really have a reference to a key &amp;String in a HashMap if your HashMap key type is String. Any modification of the map would move keys around and references will be broken. You can [use get_key_value](https://github.com/rust-lang/rust/issues/49347) (requires nightly) to get the reference to the key, but that's a second lookup. Also, once you retrieved reference to the key from the map, you cannot modify that map (because, for example, removing the mapping would invalidate the reference even in case resizing the map would not). With unsafe, you can get the `&amp;str` from the original key and put it into KeyARef, then insert the key, but that whole operation should be wrapped into one function, so you can tie that KeyARef lifetime to the map lifetime. And prohibit operations to replace / remove anything from the map since those could invalidate that reference, too.
In nearest_common_ancestors 2, why not use a B-Tree (or any other self-balancing tree)? Is there too much overhead?
How do you expect this HashSet to work, if you go and mutate the Foo in a way that changes Foo hash?
Notice that provided implementation doesn't work if string length is exactly 3 characters (or shorter).
What is your Rust project?
&gt; I tried also replacing the Option&lt;ast::Name&gt; in Literal with just ast::Name and using an empty name to represent “no name”. That change reduced it to 16 bytes, but produced a negligible speed-up and made the code uglier, so I abandoned it. An Option[NonEmptyString] seems like it may be useful.
\&gt; This difference is very likely due to CPU frequency scaling differences between Linux and Windows. It does make benchmarking on my Linux laptop a bit unreliable though. You can adjust the frequency scaling on Linux. This should work: \`sudo cpupower frequency\-set \-g performance\`
The ScopeTree is a tree of scopes within the program, i.e. the structure of the tree explicitly reflects the structure of the program. So a self-balancing tree wouldn't be appropriate -- you don't want existing child/parent relationships to be changed just because you inserted a new node!
I think OP was referring to the vector of seen nodes: &gt; nearest_common_ancestors 2: A different part of the same function involves storing seen nodes in a vector. Searching this unsorted vector is O(n), so I tried instead keeping it in sorted order and using binary search, which gives O(log n) search. However, this change meant that node insertion changed from amortized O(1) to O(n) — instead of a simple push onto the end of the vector, insertion could be at any point, which which required shifting all subsequent elements along. Overall this change made things slightly worse.
Oh, right, thanks for pointing that out. I haven't tried a B-tree, but I suspect the overhead would also be too high. The sets of seen nodes are really short-lived and mostly really small, so a dumb `Vec` with linear searching is hard to beat.
No, the situation you describe with a template is likely exactly what /u/wrongerontheinternet is imagining: something like `mutex_value&lt;T&gt;` storing a mutex and `T`, with a `mutex_guard&lt;T&gt; lock()` method where that return value has an unlocking destructor and implements `T &amp;operator*`/`T &amp;operator-&gt;` (etc.). Given that, consider: int &amp;break_it(mutex_value&lt;int&gt; &amp;m) { return *m.lock(); } Obviously that's contrived and relatively easy to notice... but a more complicated instance will be harder to find (e.g. returning an reference pointing deep into a structure stored in a mutex from a longer function, possibly with various type inference). The draw of Rust's references over C++'s is the compiler stops pointers to that inner data from escaping from the critical section. This is what slide 18 (and this subthread) is getting at.
Yeah this is exactly what I hate in cargo, npm and other packages manager that think GitHub is de facto standard. I don't use git or github but if want to publish to crates.io I must **use** GitHub.
The git-data is easy to migrate of course, but all the things around it is not. Issues, active pull requests, integration with other systems, markdown dialect for documents, wiki etc.
&gt; the public re-implementation of @bors Worth mentioning that @bors is also public and open source: https://github.com/servo/homu
In this case public referred to the instance; you can hook up ng to your own repository without hosting it yourself.
Definitely right there! I think we don't have much to loose staying on github. Migrating git-data being easy just means that not all is lost if something does go bad, I guess?
True, but only for the initial setup. You need to login once and get an auth token, but after that you're free to keep your repositories anywhere and you'd never have to log in to github afterwards. I can agree that some other options would be good, but having a github account is not a large bar for entry, especially since there's no other requirement tying you to github. Benefits of not having to re-implement a secure login system with good two-factor auth probably outweight the downsides of being tied to github in logging in?
I have a closure defined inside a loop, currently working: let mut prog = program::Program::new(...); let some_value = ...; loop { prog.draw(&amp;|x| { ... }); // use x and some_value } // in program::Program implementation pub fn draw(&amp;mut self, f: &amp;Fn(&amp;mut X) -&gt; ()) -&gt; () { ... } I would like to extract the definition of the closure outside the loop, like: let some_value = ...; let closure = |x| { ... } // use x and some_value loop { prog.draw(&amp;closure); } but I'm getting plenty of lifetime and mutability compiler errors. Is it possible to define a closure with a longer lifetime than where it is used? My end goal here is to hide all the stuff inside the loop by another higher order method like: prog.run(closure, ...); And this issue I have is at an intermediate step of refactorization. Full code here if needed: [https://github.com/mpizenberg/computer\-vision\-rs/blob/afe049ae8c5ffb02b332b03567d374111a9f5187/examples/display\_image\_program.rs](https://github.com/mpizenberg/computer-vision-rs/blob/afe049ae8c5ffb02b332b03567d374111a9f5187/examples/display_image_program.rs)
Firefox often ends up using multiple pieces of software for the same thing, in different components. librsvg is only used on the gtk builds and transitively, as GTK Firefox uses GDK-pixbuf, which in turn uses librsvg.
Soooo cool
Depends on how much effort you are willing to put into a wrapper vs. other code that could be more useful to the progress of your project. Personally, I've considered and then opted not to use a newtype at least a couple of times because the time could be spent better elsewhere.
Personally i think its very unfortunate that the whole open source world rests on a private company. That said, i don't see any reason why MS should be more problematic as github now. 
It actually can’t be fixed breaking stability guarantees because `Cow`’s internals are public.
I don't think that matters. Basically Cow is not smart enough to realize that types like &amp;str are immutable and would never cause a copy, so it needs to check and make sure a mutation can't happen. With specialization you would be able to skip that step knowing the type system has done it's job for you.
That’s not true. You can never mutate through a borrowed Cow (no matter the type) and you can mutate the owned variant of a `Cow&lt;str&gt;` (i.e. a `String`)
What I understood was when you got a `[T]` out of a `Cow&lt;Vec&lt;T&gt;&gt;` or a `&amp;str` from a `String`. The conversion may not make it clear.
&gt;Note that rustc-perf uses perf-stat for its measurements Oh, now I get what the different benches on perf.rust-lang.org show
This isn't very pretty, but you can restructure your loop so that it returns the grid as a result: fn main() { let mut over_relaxation = 1.0; let grid = { let mut iter = (0..11).into_iter(); let mut i = iter.next().unwrap(); loop { over_relaxation += 0.1 * i as f64; let mut grid = vec![]; grid.push(over_relaxation); match iter.next() { None =&gt; break grid, Some(n) =&gt; i = n, } } }; println!("Last grid: {:?}", grid); } https://play.rust-lang.org/?gist=835e81337fedf37aa3d476019e8f1b53&amp;version=stable&amp;mode=debug
I needed something to keep me motivated, so I've started something that I need to use, a command line subtitle downloading tool for my TV PC.
Ha! seems it was an annotation missing in the closure, and the compiler errors are not clear at all \(cf [https://github.com/rust\-lang/rust/issues/41078](https://github.com/rust-lang/rust/issues/41078)\). Fixed by: let closure = |x: &amp;mut X| { ... } // use x and some_value
Onur had agreed to join the docs team quite recently, but we hadn't finished actually doing that yet. He's said that he's going to update it soon and let some of us have access so that we can help in the future, but it just hasn't happened yet.
Sure. That doesn't mean that every decision makes everyone happy. Sometimes, hard decisions have to be made.
&gt; Lastly, I just want to say that I have only followed/used Rust for 1.5 years, so I'm sure I'm ignorant on many things and my writing probably reflects that. It's all good! I think you're missing *some* context, but not all. For example: &gt; As an example take concat_idents!. To me the consensus seems to be that it is useless, but it has gone through an RFC since it is in nightly. Rust has not always had the RFC process. `concat_idents` is older than that. Look at its tracking issue: https://github.com/rust-lang/rust/issues/29599 There's no RFC link. There are few things like this nowadays, but they do exist. &gt; we now need a process to get rid of it. ... I don't think using new RFCs to cancel out previous RFCs is a good way of going about that. There is one. A new RFC is one way, but the team can also simply decide to reject the feature entirely. For an example, [see here](https://github.com/rust-lang/rust/issues/29644#issuecomment-251809331). &gt; after an RFC is merged its basically going into the language at some point This is not true at all. "Stabilization" is the process by which we decide if something is stable or not. Removal is totally a possible outcome of the stabilization process. &gt; I was trying to think of a way to divide nightly into experimentation-oriented features, and features we know are going into stable. Ah, gotcha. Well, that's what the unstable book, the `B-unstable` and `B-RFC-approved` tags are on the repo. They're not *super* easy to get into, but all of the information is there!
A core team member lives in Iowa. He doesn't really reddit though.
Is there a theoretical “fastest” compiler that we can get closer and closer too with more work? If so, how can we know when there are no more optimisations do be done?
There must be a better way to traverse scope trees than that. It feels like this should be effectively O(1), O(n) at worst. For O(n) I think you can just track scope depth, and you know only scopes with equal depth can match, so you don't have to keep a buffer of all seen scopes. For O(1) I'd need to think about it harder, but I don't think it should be implausible.
Yeah that's probably the easiest thing to do.
You could define a trait that exposes the common aspects of the structs that your function needs: trait HasSlice { fn x(&amp;self) -&gt; &amp;[f32]; } Then implement the trait for each of your structs, and define the function to be generic with bound `HasSlice`: fn gen_sum&lt;T: HasSlice, U: HasSlice&gt;(obj1: &amp;T, obj2: &amp;U) -&gt; u32 { let sum = 0.; for i in 0..ob1.x().len(){ sum += ob1.x()[i] + ob2.x()[2]; } sum }
Yeah, I was thinking about that. It should probably take the whole slice if the iterator runs out.
cool, I also just installed it since we use matrix for work.
That's great to hear!
Not sure yet, but I will post a link on twitter.
There's a kind of MS virus of incompetence which infects some stuff they touch. Some excellence as well, but also the huge risk of getting the other virus. For example, they could integrate the MS login system, or get MS licensing involved. Our login could be going through so many domains that even MS itself doesn't keep track of their renewals.
The locking and unlocking can be better than what is shown here, by using a `std::lock_guard` or a `std::unique_lock`, which don't require an explicit `unlock()`. However, there is no way to associate the `data` vector with the mutex, so it's an easy mistake to try to modify it without locking the mutex. You can prevent that in Rust. 
My main concern there is that I have no smartphone and Microsoft had decided to make them mandatory for logging into existing accounts in the past. (eg. Hotmail)
&gt; but having a github account is not a large bar for entry Currently. Here's hoping Microsoft doesn't repeat the Hotmail trick of "You already have an account. We'll let you back in when you tell us the number of the mobile phone. We don't believe people without mobile phones exist."
When you ask an Executor to run a future ("spawn" it) you get a SpawnHandle back. Normally if you drop that handle it will automatically cancel the future for you, but you can call its forget() method to discard the handle without cancelling the future.
&gt; Sometimes, hard decisions have to be made. I'm not sure what we're even arguing about here. My issue is that sometimes discussions are "killed". When RFCs are actually "rallying points" for the community to come and discuss, then they shouldn't be killed. I just want to counter the optics of people in power pushing things through. It's fine when not everyone is happy after a good discussion. The issue is if it was a good discussion. Saying "we can't discuss that" when the process says we can minimizes how good the discussion is to me. Look at the module/path changes. It started out as a push to get rid of `mod` and `extern crate`, had lots of animosity, but a solution was found after things were looked at as a whole (modules, external crates, item visibility, paths, prelude, ...). Imagine if the process was to try and change them bit by bit without looking at the big picture. I simply think if an RFC is part of a wider design that also includes a previous already merged RFC that it should be possible for former decisions to be adjusted to fit the big picture, and that merged RFCs aren't set in stone. Honestly, to me everytime I read "hard decisions have to be made" it hurts these days. Everyone knows that that's true, so that phrase tends to only act as a signal to end discussion. And if we're discouraging use of "explicit" we should discourage that as well. These days I read variants of "don't discuss that" so often it's frightening.
How come that `Option&lt;ast::Name&gt;` takes 24 bytes and `ast::Name` — 16 ones? [playground](https://play.rust-lang.org/?gist=b1d6b466a789acda7e9d9a06f5bf7f56)
Yep, these are all valid points. I was thinking of some way of including conditional logic that is not too complicated.
Exactly
&gt; while also being consistent with the zero cost abstraction principle: it only costs you if you use it, and if you hand-rolled it, you couldn’t do better. But if I hand roll it, I would know the type of a Future so I would not have this problem?
&gt; I'm not sure what we're even arguing about here. I'm not either, and I'm not trying to argue, just be purely informational. &gt; Look at the module/path changes. Yes, this is a good example of things working, and working well. That doesn't mean that every single thing can possibly go this way. `?` was *incredibly* contentious, lifetime elision was *incredibly* contentious, yet both are completely non-controversial today. If the team had gone with popular opinion, Rust would feel quite different. Balancing all of those different concerns is the exact problem the team has to solve. If we purely wanted to listen to everyone, Rust would simply have votes on every proposal, with reactions, and then it'd be done. &gt; it should be possible for former decisions to be adjusted to fit the big picture, Again, this is absolutely *possible*. But "possible" doesn't mean "successful." Some things are decided, done, and over with. Re-litigating them over and over just wastes time. &gt; These days I read variants of "don't discuss that" so often it's frightening. This is always true, though. Let's take an extreme example: Imagine an RFC that suggests that we remove the curlies and semicolons and have significant whitespace instead. Discussing this is technically *possible*, but in reality, that's never going to happen. RFCs on this topic are doomed from the start, and it's better to say "this isn't going to happen" than it is letting someone's hopes get up, put in a lot of work, and then eventually have it rejected.
Core team member here. There has been zero discussion about this, but my personal take: Rust doesn't dislike Microsoft. In fact, if anything, we have historically tried to support Microsoft *more* than the average open source project: 1. Windows is a Tier 1 platform 2. I, as a core team member, switched to Windows in order to try and gain experience to better support things. I'm not the only one. 3. VS: Code and the RLS are the foundations of our IDE support. etc etc. I don't see why we'd suddenly change position here. 
First make sure you can use \`rustup\` from IDEA terminal. If it works but IDEA doesn't see it you will have to exit IDEA and remove \`.idea, \*.iml\` from the project directory. Then open IDEA and Import your project. 
You can already mirror the index elsewhere: http://integer32.com/2016/10/08/bare-minimum-crates-io-mirror-plus-one.html
Then I don't really understand :( We wouldn't even need a lint for every feature. Just the ones you can't really see from a review. Sorry to be ranty, but the constant "NO OPT OUTS!" from every angle is really starting to hurt. I don't understand it. It causes me pain. No one cares it causes pain. It looks like a custom lint (that would require use of nightly for all contributors at least for checking) is also not solidly doable. And I'd have to maintain my own crate just for that, because I highly doubt clippy would accept it. I just don't know what to do anymore here. I'm feeling a shift from "Don't like it, don't use it" to "Don't like it, leave".
I understand that if you get an impl Trait with an async method, calling it would not box the future.
I came here to comment this! You'd get `O(log n)` search and `O(log n)` insert
[`timepong`][https://git.nzoss.org.nz/tim-mcnamara/timepong] is an SNTP client that works on (at least) Windows and Linux. Running it regularly via cron etc will keep your system's clock in sync with minimal resources.
For `PredicateObligation SmallVec`, I've [written before](http://troubles.md/posts/improving-smallvec/) about how often it's better to preallocate a vector with a large number of elements than to use a `SmallVec`. The increased complexity (and, as you said, cost to move) is usually not worth the one-time allocation cost. `malloc` is fast.
Why not just use \`HashSet\` there? Is it also slower?
Interested to read that blog post when it comes out
I'm trying to compute the singular value decomposition of a matrix. I'm using `linxal` and `ndarray`. The documentation of `linxal` is a bit confusing, since I'm not very experienced with reading Rust documentation. This [link](https://masonium.github.io/rustdoc/linxal/svd/general/trait.SVD.html) mentions the `compute` method, which takes three arguments, the first being the matrix, and the latter two being booleans indicating whether you want the left and right singular vectors or not. I thus attempted to compute the singular values of a matrix `red`. 
Hello everyone. I am well aware of this situation and I am really sorry but I've been really busy with my daily job recently. I actually prepared, built and upload [a rustc update](https://github.com/onur/rust/pull/8). It requires some minimal testing and I will do it this evening (in couple of hours) when I am home. Unfortunately the PR you mentioned doesn't update rustc. Docs.rs is using a slightly modified rustc and I didn't have much time to send this patches to upstream. And for this reason, when I need to update rustc I have to merge upstream changes and rebuild rustc for 6 different platform from scratch. Upload is also taking few hours (3rd world sorry). This is super time and energy consuming. If anyone willing to help we can start sending patches to upstream. This is actually most important patch I've used in docs.rs: https://github.com/onur/rust/commit/aebd6b58806be68a1a8c36c400a79037ea1ff0a5 We already started implementing some docs.rs specific changes to rustc and rustdoc as `unstable-options`. This changes doesn't require any RFC process since they are only used in docs.rs. rustdoc team is also willing to accept this patches. So, I only need some help to port this changes to upstream. Then I can start using rustup in docs.rs and it will make everything so much easier and provide frequent rustc updates. Also you can always find me in irc.mozilla.org #rust-docs rather than GitHub.
&gt;`conca_idents` is older than that I had a hunch that was the case. &gt;"Stabilization" is the process by which we decide if something is stable or not. I was wondering what that process was. I can see that the tracking issues also have a FCP and core team review, so maybe the only thing lacking is better "advertising" of what is going on in stabilization. As has been said before, maybe have the TWiR guys include a dedicated section for the goings on in the stabilization process. Thanks for your replies, I will now stop bothering you :)
I personally prefer your current approach. User generally will copy-paste `cargo install usvg-cli` just once to install `usvg` binary, so I don't think there is enough worth to warrant increased complexity. As an alternative you can use `usvg` for CLI tool and something like `libusvg` for library crate.
There is an [algorithm](https://www.hackerrank.com/topics/lowest-common-ancestor) for finding lowest common ancestor in `O(log n)`, would it speed things up more? Basically, you just store not only one parent for each node, but also all ancestors with depth differing in a power of 2 (so there is an `O(n log n)` precomputation, but that can be done lazily), and then do a binary search.
It’s not a bother! In fact, I think it’s been extremely helpful. I wonder how much of the recent weeks has been due to people not understanding (or misunderstanding) how the process works, leading to confusion. I also wonder if that’s a function of Rust’s growth. You (and others) have given me a lot to chew on, and I think we can improve stuff here, so thanks!
[`ast::Name`](https://doc.rust-lang.org/nightly/nightly-rustc/syntax/ast/struct.Name.html) is not the same thing as a `String`, it's just a `u32` index so it takes 4 Bytes while `Option&lt;ast::Name&gt;` uses 8. [Playground](https://play.rust-lang.org/?gist=c42f0bf92068a50837c8fd18c0d8c9fa&amp;version=nightly&amp;mode=debug) (the post is talking about the [`Token::Literal` variant](https://doc.rust-lang.org/nightly/nightly-rustc/syntax/parse/token/enum.Token.html#variant.Literal) which contains more than that)
The Rust compiler adds an extra byte in order to identify the variant of the enum.
Yes, right now this isn't that great. You pretty much have all of the current workarounds listed here. My understanding, though I'm not on the Cargo team, is that bin-specific deps are a feature we want, but it needs someone to take charge and make it happen.
I did try that and it was slightly slower, because a non-empty HashSet always requires an allocation. In contrast, the vector used is actually a `SmallVec` which doesn't require an allocation if there are fewer than 8 elements.
The idea of dynamically dispatching the future type is interesting, but it sounds like a whole lot of language-level machinery just to support this one use case. I don't think that's a very good idea. Perhaps if this idea of dynamically dispatching associated types would be implemented as a more basic building block that could also be applied to other use cases then I'd like it more. But even then I feel like it should only be added to the language after a lot of careful consideration and broad consensus that it's a good idea.
As above, `HashSet` always requires a heap allocation.
I read that post when you wrote it, but I disagree with "malloc is fast" as a blanket statement. A bunch of the speedups I've made have come from avoiding calls to malloc.
But what about `path = "../"`?
But when size is more than 8, it uses just a \`Vec\`, right? How about using \`SmallVec\` when size is low, but when is becomes bigger switch to \`HashSet\`?
You also won't have this problem if you don't box the `T: Foo` before calling the async method on it. When you're calling the async method on a `T: Foo` instead of `Box&lt;dyn Foo&gt;` it wouldn't box the returned Future.
I haven't looked at your code but I could imagine doing something like this: let prompt: ParseResult = parse_prompt()?; let git_info = if prompt.git.includes_untracked_files() { load_heavy_git_info() } else if prompt.git.diffs_against_index() { load_index_git_info() } else { load_branch_git_info() }; where `git_info` sort of looks like: struct GitInfo { branch_info: BranchInfo, index_info: Option&lt;IndexInfo&gt;, heavy_info: Option&lt;HeavyInfo&gt;, } that is, it makes optional info... optional. You could actually make it a bit nicer by putting that `if` expression into a `load_for_prompt` function: impl GitInfo { fn load_for_prompt(prompt: ParseResult) -&gt; Option&lt;GitInfo&gt; { ... } } and your `git_info` just looks like: let git_info = GitInfo::load_for_prompt(&amp;prompt); 
Right, I do agree and I do certainly try to avoid malloc where possible but I think that I often see code that introduces significant complexity in order to avoid malloc entirely when the problem could have been better solved with something else (like reusing allocations or doing smarter preallocation of buffers).
You could compare to other languages that can compile very fast like C or JAI or Go, but those languages have vastly different language specs that are in part designed to be compiled fast. Probably one of the biggest things that could be done is a custom backend that doesn't use LLVM, and outputs less optimized code, but outputs it very quickly. 
IIRC you can use `usvg = { version = "x.y.z", path = "../usvg" }`.
Did Boats switch to gitlab in the wake of the Microsoft acquisition?
I've always though that associated types in dyn types should be dyn types by default. The reason it's not done this way is because the cost of a dyn type is not zero, so it's not a zero cost abstraction AFAIK. Still the cost already got paid by the first dyn type. The logic follows: associated types are by default at least as abstract as the type containing them: impl for impl types, dyn for dyn types, unless you explicitly bind the type at the trait declaration level. It simply is part of the cost (with an opt out).
Yhanks.
No, Boats’ blog has been on gitlab for a long time.
The compiler for the language that Jonathan Blow is working on (not released yet) can compile ~ 70k lines per second. He said he didn't even start optimizing it yet and he expects it to get around 4 times faster by the time it's released.
I don’t remember why, but I had an unused lifetime parameter before and had a field `_a: &amp;’a ()`
What happens if I wanted to allocate the future in a pool? Could the heap be replaced with arbitrary allocator (allocation trait)?
What happens if I wanted to allocate the future in a pool? Could the heap be replaced with arbitrary allocator (allocation trait)? At least #[async(boxed)] has the advantage of letting me decide where the allocation happens. Curious on your thoughts.
gtk-rs is supported by the Gnome foundation and has frequent hackfests around it. https://github.com/gtk-rs I'm sure they are very happy about any contributions.
If C\+\+ has any competition \(I think it doesn't\), the rust is closest. It's purpose is to be at as high level as possible, without losing performance and possibilities of low level languages. Second purpose is to make it safe \- from memory leaks, and data races. But it has a cost of being pretty hard for beginners – even for those who knowe other languages.
Systems language just means you're in charge of memory management. Even then, it's a slight reach. In Rust you have to be *aware* of how your memory is being managed, but you don't really *manage* it. Check out [Rocket](https://rocket.rs/) and [actix-web](https://github.com/actix/actix-web) if you want to stick with web backend development. You'll still be working in Jinja/javascript for the front end.
By the way, that vector is actually unnecessary if you know depths of nodes. First, just lift the lower node up to be same depth as the other one, then move through parents until they are equal. Also, there is an algorithm with O\(log n\) complexity which may be even better
Rust can do web services! Take a look at [Are we web yet?](https://www.arewewebyet.org/): &gt; Rust has a mature HTTP stack and various frameworks enable you to build APIs and backend services quickly. While increasingly more databases drivers become available, ORMs and connections to external services (like search or worker queues) are still scarce. Looking farther, it doesn't necessarily get better. Though there is significant support for base needs (like data compression or logging ), a lot more web-specific needs are still unmet and immature. The edges are still rather rough in places, but I've been hearing plenty of good things. Perhaps try something like [Actix](https://actix.rs/) if you're interested in web services to see if it fits your needs? The performance certainly is great, but you might find Rust a bit restrictive at first coming from a more dynamic language. If you really want, you could even [start using Rust on the client side to replace JavaScript](https://rust-lang-nursery.github.io/rust-wasm/background-and-concepts.html). It's very new stuff, but there seems to be a huge drive towards Rust + WebAssembly recently. Keep an eye on it at the very least.
I've done kind of all sorts of things in Rust.. Some data analysis, interfacing with devices, game servers, even "scripts" for updating software and the like. I think Rust can be pretty fine for a web backend that requires solid performance combined with good safety guarantees, though it's certainly not my first-line choice for web backends in general. Code components and libraries can be written in Rust and then included to for example a Node project.
&gt; But if I hand roll it, I would know the type of a Future so I would not have this problem? The problem withoutboats refers to is multiple implementations of the `Foo` trait. Without dynamic dispatch, you couldn't assign futures returned by one implementation's `foo()` to another, because each impl derives a unique state machine. From the [async/await RFC](https://github.com/rust-lang/rfcs/blob/master/text/2394-async_await.md#return-type-of-async-functions-closures-and-blocks): &gt; The return type of an async function is a unique anonymous type generated by the compiler, similar to the type of a closure. You can think of this type as being like an enum, with one variant for every "yield point" of the function - the beginning of it, the await expressions, and every return. Each variant stores the state that is needed to be stored to resume control from that yield point.
There are still a lot of other data structure tricks to try. For instance you could precompute for every node its height in the tree. Then given a and b, trace up from whichever is lower to get them to be the same height. Then follow both up in parallel until the common ancestor in found. This strictly reduces the number of comparisons you have to do between nodes, and eliminates the need for any "seen" data structure. A trick that builds on that, and gets you logn asymptotic behavior while still doing well for the common case, is to store for every node its height as well as it's power-of-two ancestors. Then you can do the steps I mentioned in the last paragraph in more of a binary rather than linear search fashion. I suspect the common case is small enough that this may bring marginal benefit (but it's worth a try).
I think it will mostly depend if your web services have computationally heavy tasks (or you want to squeeze last drops of performance from cheaper server) sensitive to latency, if you think that answer is yes, then Rust can be an excellent choice for solving those parts. For IO-bound (network+database) web-services performance benefit can be ignored in most cases and Rust should be considered mostly if you want improved service reliability. Another possible application is improving front-end using WebAssembly, currently wasm feels quite limited and it shines only if you have some heavy computations on the client side or if you work with canvases a lot (e.g. 2D games).
I'm finally done with my exams! Time to start reading Programming Rust, really looking forward to something else than Java.
[removed]
It should be possible to add that to [optional](https://docs.rs/optional/0.4.2/optional/). You would need an impl of Noned something like: impl Noned for String { #[inline] fn is_none(&amp;self) -&gt; bool { self.is_empty() } #[inline] fn get_none() -&gt; String { "".to_string() } } 
Instead of precomputing, you can calculate height and ancestors lazily. This can be useful if new nodes are being added to the tree.
You can do `PhantomData&lt;&amp;’a ()&gt;`, `&amp;’a ()` is the same size as a `usize` but `PhantomData` is zero-sized
Reminds me a lot of [https://flow.org/en/docs/lang/variance/](https://flow.org/en/docs/lang/variance/)
It's not quite as nice as `Tagged(my_index)` but you can do: ``` use std::marker::PhantomData; struct Tagged&lt;T&gt;(usize, PhantomData&lt;T&gt;); impl&lt;T&gt; Tagged&lt;T&gt; { fn new(idx: usize) -&gt; Tagged&lt;T&gt; { Tagged(idx, PhantomData) } } fn main() { let tagged = Tagged::&lt;u64&gt;::new(0usize); // or let tagged: Tagged&lt;u64&gt; = Tagged::new(0usize); } ```
You can use it for anything at all. If your web services need to be both fast, and secure, Rust is a great choice for sure. 
I used to be in your case: I was a web developer working primarily in JavaScript. I started learning Rust when version 1.0 was released and I after a year or so, I did one personal CLI project with it. And then a few others. And now, I'm working full-time with Rust. On a robot, something that I could never have done with JavaScript. So, my advice is : learn Rust, it's worth it even if you have no use-case for it right now. At worst you'll learn many things about system programming, at best you may find interesting career opportunities thanks to this new knowledge. It will be challenging in the beginning, but it will eventually click. It's not that hard in the end. A few advices : - read the [book](https://doc.rust-lang.org/book/index.html), and then read the first 3 chapters of [Learning Rust With Entirely Too Many Linked Lists](http://cglab.ca/~abeinges/blah/too-many-lists/book/). IMHO, it's the best explanation about the limitations induced by the ownership system and how to work with and around them. - lifetime notation are just information you give to the compiler (and to developers coming after you), you can't change the lifetime of an object with them. I've never really used Rust for the web yet, because the ecosystem was pretty immature, but [Actix](https://github.com/actix/actix-web) looks like a game-changer in this field and must-have libraries like Serde or Diesel are 1.0 and stable now, so it might be usable without too much hassle. I'm not sure you should start directly with this though, CLI apps are probably easier to start with (and [structopt](https://github.com/TeXitoi/structopt) is so cool !).
Rust is pretty efficient, so you could potentially deploy your web server to places you might not have considered before \- like embedded devices with limited memory for instance. Rust has a strong cross compiling and portability story and it makes this kind of thing relatively trouble free compared to other languages. The other aspect of Rust is the language itself, with its strict typing and the borrow checker. These can help you become a better programmer by forcing you to think about things in a different way.
&gt; it's not obvious that "increase the minimum version you specify so that the minimum version actually compiles on modern Rust" is a reasonable pull request I was going to say that I thought this was reasonable (if not high priority), but reading the rest of this thread I'm starting to wonder if there are people who think this is actively unreasonable... am I reading the tone here correctly?
Rust is (probably) going to be *really* nice for web services in around 6 months - 1 year when async/await lands. Expect an express.js like framework, except ridiculously fast and with a lot more safety guarantees.
Does anyone have any idea on when non-lexical lifetimes will be released into stable?
I'd like to add that VSCode with RLS does not work great for me on Windows. The main things not working are jumping to a definition, and seeing the usages of a definition across files. Lots of errors also don't display. Otherwise though it's ok.
Yes, it's the same concept!
I foresee Rust being an awesome solution for web services and RPC services once the ecosystem matures. There's nothing stopping us from building RESTful web services right now. [Diesel](https://diesel.rs/) is a really nice crate for interfacing with SQL databases. [crates.io](http://github.com/rust-lang/crates.io) is a pretty sizable SPA/API using [Conduit](https://crates.io/crates/conduit). If you look at the code, you'll find that it is quite familiar if you've used any go web frameworks. For a high performance web service you may want to look at [actix-web](https://actix.rs/). It is maturing quite nicely. I really like the [extractors feature](https://actix.rs/#extractors) of actix for super simple request body handling. I am waiting for async/await to land before going all in as Futures are a bit ungainly without that feature. GRPC and GraphQL crates are still works in progress. There's a promising project called [Tower](https://medium.com/@carllerche/announcing-tower-a-library-for-writing-robust-network-services-with-rust-67273f052c40) that will be something like go-kit or Finagle. It is being written to support a product called [Conduit](https://conduit.io/) so I expect when Tower is released it will be pretty solid and production ready. It is an exciting time for Rust. The ecosystem is expanding rapidly. The community and governing body is really [open and accepting](https://aturon.github.io/2018/06/02/listening-part-2/). I am pretty confident Rust is going to take over the world. It is good time to learn Rust and get ahead of the curve. 
You can also have a look at: * `cargo-edit` to quickly modify your Cargo.toml * `cargo-graph` or `cargo tree` to show deps * `cargo-modules` to show the relations within you crate Both as command line tools to use while developing and as examples how to parse Rust sources to show certain information...
Ahhhh cool, I'll have a look! For any future readers, I was looking at the Racer source code &amp; that's been v useful, they have functions for fetching a list of all the source roots of the project's dependencies. I didn't appreciate that the cargo registry is actually just a github repository with loads of funnily named folders! 
\[rXinu\]\([https://github.com/robert\-w\-gries/rxinu/](https://github.com/robert-w-gries/rxinu/blob/master/src/task/scheduler/preemptive.rs)\) is the Rust re\-implementation of the C kernel that was used in Marquette University's Operating Systems course. I just got priority preemption working! As always, credit goes to \[blog\_os\]\([https://os.phil\-opp.com/](https://os.phil-opp.com/)\) for getting me started.
Nice!
I just don't agree that it is right to call an abstraction zero cost when it is possible to hand roll a more efficient implementation. Even if your hand made code is less generic and doesn't work for "any foo()".
You would not know what type it is- this only applies to trait objects, which means you're already using dynamic dispatch to call the method, which means it could have any return type that implements `Future`.
I would look for opportunities to write small tools which will make your life as a developer (or a human being more generally) better. Particularly things which need to do one thing well, with minimal interaction, and especially where speed or efficiency are also desirable. Command-line tools might be fertile territory for ideas. As might 'sidecar' type daemons. For example, the first thing i tried to write in rust was a log shipper - a program that would tail a log file, parse the entries, and send them to a Logstash server using the Lumberjack protocol. I wanted it to be able to read logrotate configuration, so it would integrate seamlessly with a standard UNIX logging setup. 
&gt; No one cares it causes pain. I personally disagree about opt-outs in the language because my experience in C++ has been that this leads to fragmentation of the community; this does not mean that: - others disagree about opt-outs, - or that I do not care about your pain. I am sorry to see you pained by the introduction of these features, and I certainly wish for a solution that will ease your pain. I simply disagree that opt-outs are the right way about this. &gt; It looks like a custom lint (that would require use of nightly for all contributors at least for checking) is also not solidly doable. Custom lints would really be my favorite solution to the issue here, since it basically allows anyone to do as they please without the community having to vet each and every idea. They also have broad possibilities, after all many projects are likely to have "some" invariants that could benefit from automated checking, so I hope they become a first class citizen in the future (I seem to remember a WG was created for Verification, this seems like something they could push for).
I've started assembling a patch for the commit you linked. With luck I may be able to post it tonight (in several hours from this comment).
I wonder how people come up with `PhantomData &lt;fn (T) -&gt; ()&gt;` trick. Is looks like a well-known idiom, but where it is described? Are there more idioms like that? Where to find them? Or it's just been discovered, one by one, and some years later we'll see a book called "More Rust idioms" (similar to [this one](https://en.wikibooks.org/wiki/More_C%2B%2B_Idioms))?
Is there a way to specify that type parameter `T` must be a trait (i.e, that &amp;T is a trait object)?
&gt; I am well aware of this situation and I am really sorry but I've been really busy with my daily job recently. It's okay, take it easy, no need to work yourself to death :) Life has a way of getting in the way; be it work, family or friends, and that's FINE :)
Python's "context manager" is only necessary in python because it has really limited lambdas. In any language with decent multi\-line lambdas \(such as ruby\) you don't even need such an abstraction because they fall naturally out of just normal lambda usage. God i hate python. Such a brain\-dead language.
I was literally struggling with this with baby’s first crate last night. The clunkiness of it made me convinced I was doing something wrong to have gotten in this situation.
&gt; “Object safety” is the set of restrictions that a trait must meet in order to be allowed to be turned into a trait object, Rust’s solution for dynamic dispatch. These restrictions aren’t arbitrary, but they also aren’t really cohesive: they’re basically the set of things that we can do at compile time, but we can’t do at runtime. To be honest, I would really favor a change of approach here: - Allow **all** traits to be turned into trait objects, - Restrict which associated items can be accessed on trait objects. I know ways to exclude methods for trait objects (`where Self: Sized`), but no way to exclude associated constants and types... and it's always annoying to have to split a trait in two, separating the object-safe part from the compile-time only part.
I don't think there is going to be a huge impact for Rust, at least on the short term. Long term consequences might or might not be huge, depending on how much M$ truly loves Open Source now.
I would also favor a general rule (for every trait) over a specific rule for the Future trait, though it's not clear to me whether this would prevent changing the Future trait in the future if returning abstract Unsized Types become possible.
How did async(boxed) give more freedom than this? You decide where the allocation happens by calling an async method you know will allocate: in that case, because its tagged async(boxed); in this case, because its on a trait object.
I'm talking about handrolling a vtable based dynamic dispatch mechanism like a trait object. You couldn't know the future type in that case.
My blog is in on GitLab because they let me use hugo to generate my site, which I prefer because it is a static binary. I would prefer to use GitHub if it would give other static site generators the same level of support as jekyll.
This doesn't work for associated types in general: 1. Associated types could have no required bounds, meaning there's no trait to make the trait object. 2. Associated types could have an object unsafe bound, meaning you can't make that trait into a trait object. 3. Associated types could have multiple object safe traits in the bound, we do not allow trait objects of multiple traits. 4. Even if the bound is object safe, there must also be an impl of `Trait for Box&lt;dyn Trait&gt;` in order for this to work.
I've responded to the parent with the reasons that we cannot dynamically dispatch associated types in general.
I agree that just making the `where Self: Sized` automatic might make trait objects more ergonomic, but how would that solve the problem here? The implication of that alone would be that async methods are not object safe, so you can't call them on trait objects.
Oh, sorry for not being clear, it doesn't solve *this* problem at all :)
The log portion of Raft is probably the most important one. Log compactification is non-trivial and each strategy has tradeoffs.
Returning an unsized type is not such a great solution because then the return type of all async methods becomes `impl Future + ?Sized` in order to make generics work correctly. We could invent some additional leaking mechanisms to make concrete cases Sized again, but even then in generic code you would have to assume pessimistically that the returned future may not be sized in case `T` is a trait object. The only way that really seems viable to me would be if there were no restrictions on unsized types on the stack, making `?Sized` go away entirely. That would be a happy day, but I don't know how to get there.
I think we want to look at trait objects holistically after the 2018 release and try to figure out how to make them less painful.
The thing about Github is really the social aspect. Seeing the recent surge of Mastodon, I believe a true open source Github alternative shall be a federated social web interface for git. 
It works fine and is interpreted by cargo/crates.io to be a `usvg = "..."` dependency, as long as you have the `version = "..."` the sibling comment mentions: https://doc.rust-lang.org/cargo/reference/specifying-dependencies.html#specifying-path-dependencies
Does it have type system, generics and macros? 
What's the highest point release Rust has ever reached? I think this is the first 1.x.2 I've seen.
&gt; The only way that really seems viable to me would be if there were no restrictions on unsized types on the stack, making `?Sized` go away entirely. That would be a happy day, but I don't know how to get there. I could see manipulating `?Sized` types on the stack rather easily. It seems easy enough for monomorphisation to introduce the necessary boilerplate; it could even be as simple as automatically boxing to get started, then progressively eliminate it. SafeStack has proven that having two stacks (control &amp; data in their case) did not introduce significant overhead, so having a special growable "Unsized" stack on the side is a viable option for example, it's easier than returning an Unsized value directly on the current stack and would be strictly "You don't pay for what you don't use". However, I do not see an easy path to storing `?Sized` types in arrays beyond boxing; mostly because each `?Sized` item could have a different size. I've done some prototypes using an array of offsets + an arena, which works pretty well, but is a *complete* change of layout and even if the array has a fixed size (say 4) the arena is itself Unsized. At least in Rust (unlike C++), moving the objects in the arena (compacting) would not involve dreading a throwing move-constructor... &gt; That would be a happy day, but I don't know how to get there. Me neither :( 
Oh, that'd be great :) Though to be honest, I'm more excited by value generics and a broader scope for `const` functions and values.
Ah, cool!
Yeah, I'm... hoping they don't do that. I wonder how hard it would be just to add other OAuth options to crates.io.
This is the highest, you're correct.
&gt; I'm primarily a JavaScript developer but far prefer working on backend Check out the [Actix](https://actix.rs/) actor / web framework. I've had good luck with it (even w/ HTTP/2.0 support). It would allow you to build a more performant backend with greater reliability. JavaScript may have more libs today w/ NPM, but Rust may soon be a strong competitor with Cargo.
It is. There had previously been 3 point releases [`1.22.1`](https://github.com/rust-lang/rust/blob/master/RELEASES.md#version-1221-2017-11-22), [`1.15.1`](https://github.com/rust-lang/rust/blob/master/RELEASES.md#version-1151-2017-02-09), and [`1.12.1`](https://github.com/rust-lang/rust/blob/master/RELEASES.md#version-1121-2016-10-20).
I am not really an expert in Rust or Async, but I was hoping this would be interesting feature in the embedded space. So, how will this impact now that you need to use heap?
When a compiler is generating code it will produce anonymous types for _Future. But when you hand-rolling implementation for your specific use case, you know the concrete type for _Future, so you don't need dynamic dispatch, right?
&gt; Cachegrind’s output showed that the trivial methods for the simple BytePos and CharPos types in the parser are (a) extremely hot and (b) not being inlined. Sigh. (If you don't know, the supposed reason for this is so that BytePos / etc can be changed without recompiling the parser.) I *really* wish compilers did better with this sort of thing. Generate two versions of the function, one with trivial functions inlined, one not. At linktime (or start-of-execution?) choose which one to use. If a dependency changed? Fall back to the generic version. (You can extend this trick too to e.g. globals that appear to be readonly.) &gt; Note that the order of elements within this vector is important, so de-duplication via sorting wasn’t an option. There may be a faster method than creating a hashset. Sure it's good asymptotically, average-case at least, but hashsets aren't the best in practice. Their cache behavior is terrible, for one thing. Try instead creating a vector of indices, sorting it (or rather, sorting with the key being originalvector[index]), then deduping based on that. May or may not be faster.
MIR optimizations should also help here. We produce a lot of code that LLVM has to chew through.
IIUC, Futures are supposed to be zero cost, but async/await is not making a zero cost guarantee. 
Great to see this fixed! Looking at the [issue](https://github.com/rust-lang/rust/issues/51117) I am a bit surprised, though, that this bug (two mutable borrows allowed) was not higher on the priority list and was even considered to not be fixed with a patch release? Am I mis-judging the severity of this?
Thank you for this, and thanks for docs.rs! This is just about figuring out a way of making sure we as a community have a way of helping maintain the infrastructure when individual contributors need to take a break, for whatever reason.
Thank you for this, and thanks for docs.rs! This is just about figuring out a way of making sure we as a community have a way of helping maintain the infrastructure when individual contributors need to take a break, for whatever reason.
Well, it is quite severe, but I sort of thought we were closer to the 1.27 release when writing that comment, IIRC. It also only affected code written against/since 1.26, so it was fairly recent that this could become a problem.
Thanks for sharing! Could you add in your README information about what makes it fast?
Don't feel discouraged from using it for web services. I am building web services using Rust and the rough patches are tolerable and there is a lot of really great things the language brings to the table for this space. I highly recommend [actix\-web](https://github.com/actix/actix-web).
yep--thanks for the feedback, made the code blocks the same width as the text. Sorry about that!
yep--sorry about that, updated to make the width of the code blocks the same as the text. thanks for the feedback!
[@bors's latest tweet](https://i.imgur.com/Yt4t39v.jpg) [@bors on Twitter](https://twitter.com/bors) - ^I ^am ^a ^bot ^| ^[feedback](https://www.reddit.com/message/compose/?to=twinkiac)
Interesting! &gt; Read you data first, then finally write to a new structure at the end of a function What are the mechanisms for doing this efficiently? Feels like you'd be copying a lot of data by doing this, esp. w/ the data structures in the stdlib.
Even after 1.27 release this should have been backported to a 1.26.2 anyway. 
Neat. Thanks!
Reading over it, it looks like it was about 24 hours form 'found the bug' to 'decided to backport'. Doesn't seem overly cautious to me. Basically gives enough time to make sure the fix works. 
It might be nice to have something a little more direct, e.g. `struct Tagged&lt;Tag #![unused(invariant)]&gt;(usize)`. Ideally with an attribute like that we could preserve the `Tagged(0usize)` syntax.
I read through this thread and am persuaded that `to_owned()` is the more semantic &amp; clear function to use in the case. While `myvec.get(1).unwrap()` is technically not a `String` (it's a &amp;String), it seems like the interpretation of `to_string()` is that it is used to turn non-string values, like ints and whatnot into strings, rather than changing the ownership. `to_owned()` makes this clear. thanks all for weighing in here! 
Thanks, I'll put that on my shopping list.
delete
Awesome thanks!
We don't generally backport fixes to previous releases. It's not out of the question, but we currently make no guarantees about releases older than the most recent stable.
strongly, not "stringly" typed pipes and args to binaries. stringly typed IPC being just a subset one opts in to. that combined with a Plan 9 style extreme file centrism would be an interesting experiment. good luck - never know there might be posts like http://opendotdotdot.blogspot.com/2006/03/linus-torvalds-first-usenet-posting.html some day pointing right here :)
Thanks for docs.rs! Definitely ask the moderators for `docs.rs` flair, so it's easy to tell from a glance at your comments that you're the author.
If I had to guess I'd wager stabilization won't start until after the [A-NLL issues](https://github.com/rust-lang/rust/labels/A-NLL) and [WG-compiler-nll issues](https://github.com/rust-lang/rust/labels/WG-compiler-nll) are resolved (maybe just the NLL-sound and NLL-complete ones), and then it should actually be 12 weeks from then. Here is the [tracking issues](https://github.com/rust-lang/rust/issues/43234).
You don't need to use the heap unless you're calling an async method in a trait object. In most cases, async fn still does not allocate.
If you want zero cost, wouldn't you steer away from trait objects to begin with? Maybe I'm unclear on what you're trying to do -- you want to write a trait that contains a fn that returns a nameable type that impls Future, e.g. `fn f(&amp;self) -&gt; NameableFut`? Because you wouldn't be able to use async fn to impl such a trait, since async fn returns an unnameable type. Furthermore, if you know the type to return, why do you need a trait at all?
Why would you hardcode a specific Future type into a trait fn? It'd be nearly impossible for implers to impl such a trait.
I think you just want \`where T: Trait\`. Possibly \`where for\&lt;'a\&gt; &amp;'a T: Trait\`.
&gt; GitHub is still GitHub &gt; Don't boycott GitHub until they do something to warrant that boycott On the other hand this is a wake-up call for people to reflect upon if they want to rely/use so heavily a proprietary service, regardless of who the corporate overlord owning it happens to be. Github is still Github, but maybe it wasn't all that great even before the news broke. For the record, I do think that MS *might* end up being pretty good owner for Github. Time will tell.
The covariance/contravariance relationship between subtyping and functions is fairly well studied in object oriented literature. If someone is already familiar with it in that context, I could imagine them coming up with it in Rust too. All of that is to say, I bet it' been independently rediscovered lots.
So you'd basically use the 'hash' as bitset? How would that work?
Yeah I guess since Rust is supposed to always (except in rare cases) be backwards compatible there's usually no reason to backport bug fixes. Upgrading to the latest version _is_ the bugfix :P
Note that there are obviously many small Vecs to search here, which would fit in cache, so being cache friendly may very well trump big Os.
Author here. I understand the sentiment, but the Randen permutation is less of a 'modification' than a straightforward combination of well\-studied components. Simpira v1 was vulnerable due to two new 'features': structured/sparse constants (which we avoid with fully unique and dense constants) and a weaker Type\-1.x structure. By contrast, Randen uses a generalized Type\-2 Feistel with 16 branches and an odd\-even shuffle, which has been analyzed in multiple articles since 2011. Preliminary reviews have not found any fault with our indistinguishability proof, but yes, we're looking forward to the final reviews.
You may be looking for /r/playrust
Ah I see.
We've had a lot of discussions about needing some sort of LTS policy at some point. I personally hope we adopt one in the last quarter of the year. We'll see!
No, that would not guarantee that "&amp;T" is a trait object (has size of two pointers). I think, I found a way around it (make my code to work without that assumption).
Make your entry into Rust easier by setting a clear set of goals for what you want to accomplish in the next month. Try porting a project, maybe? I've had great success with learning languages by porting work from one to another. 
I think something like type CovariantNoDropchk&lt;T&gt; = fn() -&gt; T; struct Tagged&lt;T&gt; { marker: PhantomData&lt;CovariantNoDropchk&lt;T&gt;&gt;, } would be a 80% solution, which does not introduce new language machinery. 
That was actually a proposal, but it was decided that it’s a simpler mental model to use `PhantomData`. You just ask “do I conceptually hold a `fn(T)` or do I conceptually hold a `T`?”
In C++ you could use something like https://github.com/abseil/abseil-cpp/blob/master/absl/base/thread_annotations.h, which can be checked by Clang. If you have this code: #include &lt;vector&gt; #include "absl/base/thread_annotations.h" #include "absl/synchronization/mutex.h" struct ConcurrentInts { std::vector&lt;int&gt; ints GUARDED_BY(mutex); absl::Mutex mutex; }; int main() { ConcurrentInts data = {{1, 2, 3}}; // unsynchronized access detected by Clang data.ints.push_back(4); { absl::MutexLock l(&amp;data.mutex); data.ints.push_back(5); // l released at end of scope } } And you compile it with `clang++ -Wthread-safety`, then you get: main.cc:15:7: warning: reading variable 'ints' requires holding mutex 'data.mutex' [-Wthread-safety-analysis] data.ints.push_back(4); ^ 1 warning generated.
&gt; - GitHub has said that they plan on continuing their current service Every company that's ever been acquired has said this. Hopefully it's true this time! I definitely agree that there's way too much inertia for any changes to happen quickly. 
It's sort of weird that GitHub is used as the data store for crates.io anyway, isn't it? 
Could we allow multiple trait objects by introducing disambiguation rules a la Eiffel?
Is it possible to have a struct respond to being moved into a thread? As in: struct Responsive {} impl Responsive { fn on_move() {} } let r = Responsive {}; let handle = thread::spawn(move || { r.stuff();}); handle.join(); 
Looks like jai (which I just looked up) has types and limited function-level generics: https://github.com/BSVino/JaiPrimer/blob/master/JaiPrimer.md
Yes, but try leaking the reference instead of calling through data.ints directly. Or even worse, leaking the reference *several function calls deep*. clang's analyzer can only do so much; at some point, without modular annotations like lifetimes, the proof complexity just explodes.
Check `rand` version, it should be the same version as for `ed25519_dalek`. Trait `Rng` from `rand v0.4` is not equivalent (without semver trick) to the trait with the same name from e.g. `rand v0.5`.
This work of you guys is a godsend! Generating bindings for libsel4 really is a pain, and an automatic way to do it like this is awesome! I'll be waiting patiently for you to release some more of your work! :) It seems that your libsel4-sys is really tightly integrated with cargo-fel4. Is there any way to use libsel4-sys without cargo-fel4? For example, using the library in threads that are not the root sel4 thread?
That did solve the problem but then it makes OsRng not work for the other methods that need it due to being the incorrect version. I will make a separate thread regarding this issue. 
In what sense?
There is no callback for being moved. You could `impl !Send for Responsive {}` and then have a method to call to make it sendable again: let r = Responsive {}; // it would be a compiler error to move `r` to another thread let sendable = r.sendable(); thread::spawn(move || { sendable.into().stuff(); });
&gt; It would be a known fact that calling an async fn on a trait object means a heap allocation, just like its a known fact of using all of those APIs. I don't think I agree with that logic. An API is one thing--it's user code and people generally expect to have to read its documentation or code to find out what it does. But async is a builtin language feature and keyword, is it not? Rust doesn't have *any* builtin language features that implicitly allocate at the moment, AFAIK; it used to have box, but (1) it was removed and (2) the only purpose of `box` was to signal an allocation, while async certainly does not signal an allocation (in fact, it usually *won't* allocate). I do understand that you currently can't do better, but I still think there should be *something* clearly signaling "hey, this allocates!" More generally, even beyond the allocation thing, I find the idea of a completely different method being called on the trait object than the original object rather unsettling. Especially one that's generated by what sounds like some almost macro-ish magic. I wonder whether the macro approach just needs to be tweaked, so that if you want an object safe version it can generate a second "boxed" version that replicates the boxing you describe in your post?
I switched rand to 0.4 and curve25519\-dalek to 0.16 and now it works, though it is a shame that the newest versions of these packages will not work without this situation coming up. If anyone knows a better way to deal with the original problem than switching to old versions of these packages please let me know, though for now I will just continue working with the old versions of rand and curve25519\-dalek. 
The problem with multiple trait objects is figuring out a representation. There are a lot of trade offs between multiple vtable pointers, combined vtables, etc: having to do with compile size, struct size, indirection, etc, and having to do with our existing hard requirements about coherence, separate compilation, etc. Multi-trait trait objects are not a hard never, just a tricky problem. (Most of the other bullet points I don't have any idea how we could solve, though.)
`box` wasn't removed, it's just unstable. To this day, [this](https://doc.rust-lang.org/src/alloc/boxed.rs.html#242) is the implementation of `Box::new`. I don't think the division you draw between an API and a language feature is meaningful. Either way, users have to read documentation to find out what it does. Many of the APIs that allocate in std do so nonobviously - [Mutex](https://doc.rust-lang.org/src/std/sync/mutex.rs.html#121-130) for example contains a Box because of different platform requirements about address stability. The definition of zero cost abstraction is: 1. You don't pay the cost when you don't use it. 2. You couldn't implement it more efficiently. Both hold true for this.
Will PhantomData ever go away? Coming from a Haskell background, I like to willy-nilly add phantom type parameters to my types. If PhantomData must stay, could a kind soul give me a brief explanation why?
Would this LTS policy help distros like Debian and projects like Firefox align on a version of Rust to ship?
You're absolutely right; `libsel4-sys` is tightly bound to `cargo-fel4`. As it stands today, the kernel is managed by `libsel4-sys`, as we need its source to do the Bindgen work. While it may be possible to use `libsel4-sys`, I can't imagine it'd be much fun :( As for your second question: yes! There are some magic names involved^1 for invoking your task: During root-task generation, `cargo-fel4` looks for an [exported function](https://github.com/PolySync/cargo-fel4/blob/8e47260b9daeb49fe1f4f19c5e89d3e0f2315d14/src/generator.rs#L173) called [`run`](https://github.com/PolySync/cargo-fel4/blob/8e47260b9daeb49fe1f4f19c5e89d3e0f2315d14/src/generator.rs#L186). However, we imagined that you could compose process trees by having, in your task's implementation of `run`, more child threads start up. Right now this is cumbersome due to the passing down of a thread's TCB from its parent. Which, I think, is not possible without walking through the `BOOTINFO` struct and confirming whether or not the calling thread has sufficient permissions to a CSpace (eek). The good news is that this is the first thing on the docket for next steps. We'd like for consumers of fel4 to not have to wade through all the boilerplate of starting a thread, but instead, have a nice Rusty way to build up the necessary configuration to a TCB, and fire it off.
&gt; Mutex for example contains a Box because of different platform requirements about address stability Will this be able to become a `Pin` in the future?
Do you know how to activate instantiateStreaming work on a server like CreateReactApp? (Ref Hello World page on the site you linked) or make instantiate work with bindgen or stdweb? I've asked and searched everywhere, with no success.
Probably not any time soon, given that any LTS schedule we start with is likely to still be so much shorter than something like Debian. I’d imagine.
So there's actually more C than there was before? 
So ~6000 lines of C turned into ~18000 lines of Rust. I wonder why there's so much more code. 
No, you are free to move the mutex around so it has an inherently unstable address. you could invent a different mutex based on pin but it'd only be usable with scoped threads.
I think we have a fundamental disagreement about the role of language features vs. libraries (I *don't* have to read the documentation to find out if a language feature will allocate in current Rust), but in any case the second thing is my bigger gripe--Rust doesn't generally overwrite trait definitions in the way you've described at all, and doesn't generate methods in such an ad hoc manner except via the mechanism of macros. I don't understand why async functions are so much more important than the other things for which we use macros that this needs to be a language feature--especially since, AFAICT, everything they are doing could be replicating by generating a second function with the boxed interface.
You should probably be able to create a perma-pinned mutex from a static and treat it as always pinned, which would be usable from non-scoped threads as well (I'm not sure how this API could look, though).
There actually used to be a bunch of variance types, and even more variance choices than there are today. They were removed in favor of the current inference-based approach because that was deemed too complicated and no one really understood it anyway.
PhantomData doesn't prevent you from adding phantom type parameters. It just forces you to be explicit about their relationship to the rest of the type, which is important for things like auto-derived traits (especially unsafe ones).
I find that to be a higher cognitive load because when I'm reaching for `PhantomData` I'm not thinking in the context of holding anything at all. I'm thinking in the context of controlling variance and/or drop checking, which I'm currently forced to do by pretending I'm holding something that happens to have the properties I want. In fact, I'm usually thinking "I conceptually and specifically do not want to hold anything here" since what I'm after is a marker/phantom type. So there's an extra indirection there where I have to remember a set of things that easily express variance and then filter it to what I want instead of just saying "invariant no drop checks please". On top of that, it's not something that is reached for often, so I typically have to relearn the set of things each time before I can filter appropriately.
I suspect such a distribution strategy is doomed to failure. Rust projects will use features from the latest stable, and people won't be able to build their dependencies or Rust binaries (like alacritty or ripgrep) using the version of rustc from their distro. There will be much complaining, and hopefully users will find their way to rustup instead of forcing the Rust ecosystem into a multi-year release cycle to match projects like Debian. I'm just not sure there's a lot of value to be added by trying to make the 6-week _stable_ release cycle mesh with a multi-year LTS release cycle.
What about a heap?
It will be listed, but it should say `ignored` when you run `cargo test`, it shouldn't actually run it. However, I highly recommend not using the nightly\-only built\-in benchmarking stuff. [Criterion](https://github.com/japaric/criterion.rs) is significantly better, and still easy to use.
A named type alias is a good start, but I'd give it more like a 50&amp;#37; solution since it still requires either exposing the fact that `PhantomData` is used or creating a boilerplate \`new\` function and affects syntax negatively like when pattern matching. In my mind the only things one can possibly do w`ith Phantom`Data is create an empty one or ignore that it's a field, which makes them nothing but boilerplate, so it'd be nice if I didn't have to do that at all.
&gt; running 28 tests test tests::asterisk ... ok test tests::ampersand ... ok test tests::at_symbol ... ok test tests::backslash ... ok test tests::backtick ... ok test tests::caret ... ok test tests::create ... ok test tests::comma ... ok test tests::dollar_sign ... ok test tests::exclamation ... ok test tests::forwardslash ... ok test tests::greaterthan ... ok test tests::double_quote ... ok test tests::iterate ... ok test tests::multidigit ... ok test tests::lessthan ... ok test tests::newline ... ok test tests::minus ... ok test tests::period ... ok test tests::numbersign ... ok test tests::plus ... ok test tests::percentagesign ... ok test tests::question_mark ... ok test tests::single_quote ... ok test tests::tilde ... ok test tests::unknown_character ... ok test tests::verticalbar ... ok test tests::underscore_in_name ... ok test result: ok. 28 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out Running target/debug/deps/bzip2-86e88341c65057ee running 1 test test bzip2_all_valid ... ok test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out Running target/debug/deps/gcc-dbee6510b79835b6 running 1 test test gcc_all_bench ... ok test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out Running target/debug/deps/readme_example-dbf72902e31e0c00 running 1 test test readme_example ... ok test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out Doc-tests token running 0 tests test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out 
You could switch the other crate to use the newer rand. Then send a pull request, chances are it will be very welcome.
Put your file in `benches/bench.rs`. Example: benches/bench.rs
Ah! thanks! I think I will go with Criterion, it looks pretty nice. It was pretty easy to modify what I had to get it working (and of course it had to go into benches not bench!)
I enjoyed the footnotes. :D
It's statically typed, automatic type inference, compile-time (and run-time) type information. In terms of meta-programming, it has compile time polymorphism (similar to generics or templates, but simpler and better). It can execute arbitrary code at compile time, and allows hooking in to the compiler's pipeline. It has no garbage collector and no guarantees around memory safety, but it provides tools to help manage memory manually, e.g. arena allocators and resettable temporary storage. Jon doesn't buy into a lot of "modern" programming techniques, so his idea of what makes a good language are divergent from the rest of the mainstream languages. You can sort of get an idea about his programming philosophy from this talk he gave last year: https://www.youtube.com/watch?v=De0Am_QcZiQ He's building a game (and game engine) in this language as a way to test his ideas and prove that they work.
I don't mind the inference so much as I mind how indirect it is and that it affects how I can use the struct in non\-variance\-related situations. I've followed Rust for a few years now but I'm not familiar with any prior art when it comes to handling variance. If you run into a variance\-related problem then at some point you have to understand variance and whatever mechanism the language gives you to control it. I don't have opinions on having more variance types/choices but I'd definitely prefer a control mechanism that was more direct and didn't affect creation/usage of the struct, especially when there's only one thing you can reasonably do at the creation/usage site (construct/ignore a zero\-sized type that won't exist at runtime anyway).
I actually find it rather useful--it forces me to think about the precise relationship I have to the type. Much more than once, while I was writing unsafe code having to think about the kind of PhantomData I wanted to carry helped alert me to potential safety violations.
I disagree with Debian's strategy for packaging Rust, for the record. Personally I like Fedora's strategy better, where they always have the latest compiler in the supported releases. 
Having Clippy on stable is one of the number 1 things I would like to see fixed with Rust. I'm very happy to see there is a plan to get this resolved.
&gt; it forces me to think about the precise relationship I have to the type That's the thing for me - I usually don't have a relationship to the type. All I usually want is a tag that behaves in some way with respect to subtyping/variance. For example, the only relationship I have to `fn() -&gt; T` is that they share the same properties I care about in this regard. I'm not holding a function, I don't behave as a function, and I'm not going to use it in a function context. The only thing is I happen to share the same variance as the function, and going through `PhantomData` only obscures that in my mind. Renaming it with `use ...PhantomData as SameVarianceAs` or whatever helps convey intent, but doesn't address my usage complaints.
That's kind of analysis is important, but it's not sufficient to correctly estimate performance in cases where any performance difference between a BTreeMap and a HashMap would matter. First, log2(n) is really small. It's never more than 40. And keep in mind that with a b-tree the log isn't log base 2, it's more like log base 16. And that means that log(n) &lt; 10, *always*. At that point, any more arguing theory is silly. If you care about performance, you need to benchmark.
But Rust has more than just variance: it also has auto traits. When you're writing an unsafe type, if you semantically have ownership or shared access or pinned access to another type, you'd better mark it correctly in your PhantomData, because if you don't odds are good that you'll get an unsafe impl that is incorrect for your type. Rust even has wrapper structs (like Unique) that exist solely in order to make auto trait implementations easier, which no one would ever remember to use otherwise (to the detriment of usability and safety).
No way! That's awesome (and kind of surprising as well)!
True! And a good point. I'm not suggesting to get rid of `PhantomData` or anything. I'm just noting that having *only* `PhantomData` makes my uses more cumbersome. &gt; you can always just write structures with names like "InvariantLifetime" for this use case That helps intent, yes, I agree. But that doesn't help my usage sites where I still have to explicitly handle zero sized types that mean nothing at all. 
There's three of us if you count the core team member that Steve mentioned :) Understandable! Its looking like there probably aren't enough Rust users in Iowa for a meetup anyway. If there is enough interest in the future I would be more than happy to attend a meetup anywhere in the state really.
I can use rustup from the IDEA terminal.
Super brief: * Are you building a CLI tool and don't care too much about structured errors? Then you should probably just use `failure`'s `Error` type. * Are you building a library? You might find `failure`'s `Fail` trait useful along with `derive(Fail, Display)`, but you can also just implement `std::error::Error` and `fmt::Display` by hand. The latter is what I tend to do, although I some day hope to move to `failure`. For a brief intro to error handling: https://doc.rust-lang.org/stable/book/second-edition/ch09-00-error-handling.html In depth treatment of error handling, with no external crates: https://blog.burntsushi.net/rust-error-handling/
&gt; Rust projects will use features from the latest stable, and people won't be able to build their dependencies or Rust binaries (like alacritty or ripgrep) using the version of rustc from their distro. I had a similar belief, and the answer here is basically, "folks that stick to Debian's package repos don't get the latest version of your software. They need to use the most recent version that works with their compiler." It is possible that the Rust ecosystem moves too fast for this to work. But Debian's standard model has never really been a good fit for me personally.
I think clippy on stable is especially important for newcomers who are more likely to make the mistakes that clippy catches (not to imply it isn’t a valuable tool for seasoned rust users). Some new users are unwilling or unable to go through the process of installing nightly and keeping clippy updated to target their current nightly even when they could stand to learn a ton from the program’s advice. 
Neat 
Is there an option for just a one time payment? I worry about recurring payments like Patreon because I feel like I'll forget. Also I'd rather just throw 600 dollars to someone upfront once in a while than 5 dollars every month for a year - and I assume they'd rather get money up front too.
Is there discussion around this somewhere? I don't plan to participate as I'm not involved with the product but I'm interested in the software lifecycle process and would love to see how other organizations manage these sorts of things (bugs, releasing, etc.).
There's no built-in way to do this, but it seems you can create a pledge then remove it after billing (see [this page](https://patreon.zendesk.com/hc/en-us/articles/204606215-Can-I-make-a-one-time-payment-)) to get the same effect.
Hm, alright. I'll give that a shot, thank you.
Your comment was helpful, until the last two sentences. You don't have to rag on shit, it was just a question to help me bridge a gap in my knowledge. 
Actually, I should add: I'm not so sure that up-front is preferable to monthly. I know that at least some of these folks live in places with relatively low cost of living (compared to major tech hubs in the US, for example), where reaching a steady monthly income from Patreon might be enough to fully support them.
Can I suggest adding https://www.patreon.com/sebcrozet to this list? Sébastien works on (among other things) a great stack of crates spanning lower-level maths (linear algebra and associated data structures) all the way up to a high-level physics engine. 
That's a good point. I'll consider the monthly thing, I just dislike the idea of an indefinite payment that I will certainly forget (this has happened before). But I guess I can try to set some kind of reminder to track these things with patreon - they must support something along these lines.
What does it mean to block nightly? It's also mentioned in the linked pull request but I didn't quite follow. Also it says that the idea would be for Rustup to pull in a clippy component when compiling (rustc?). Is the idea that for each stable version of rustc a version of clippy for that would also be available? It sounds like that would be more maintenance and upkeep, challenging.
It might help if you show the full error as well as the second attempt with its error as well. 
Continuing to work on my neural net library. Got stuck in a theory rut with back-propogation and finally think I understand it. Writing a lot of unit tests right now to make sure it really works as I expect it to. 
That would require dependent types to do statically, which Rust doesn’t support. Generally I would suggest using a trait for functions that can take many types and do the same thing with them, but we could probably suggest something more specific if you elaborated on your use case.
I've added the second error message now.
It means that if clippy doesn't compile there won't be a nightly that day. This is the case for RLS/rustfmt/etc. The PR initially blocked nightlies on clippy but that part was removed.
&gt; What does it mean to block nightly? I'm assuming it would mean that changes which would break clippy would be held back from entering nightly builds until clippy gets fixed, thus ensuring that every nightly release available through rustup would have a working clippy available. &gt; Also it says that the idea would be for Rustup to pull in a clippy component when compiling (rustc?) I'm not sure what you mean by the "(rustc?)" part, but it'd be similar to how the ability to install RLS and rustfmt via `rustup component add` is currently in a preview phase. &gt; Is the idea that for each stable version of rustc a version of clippy for that would also be available? It sounds like that would be more maintenance and upkeep, challenging. I don't see why. The vast majority of nightly builds have a version of clippy available and each stable version begins life as a nightly that gets extra testing.
Using Windows 10 on an AMD FX-8350: test tests::bench_best_0 ... bench: 52 ns/iter (+/- 4) test tests::bench_best_1 ... bench: 52 ns/iter (+/- 4) test tests::bench_best_2 ... bench: 46 ns/iter (+/- 3) test tests::bench_best_3 ... bench: 24 ns/iter (+/- 1) test tests::bench_best_4 ... bench: 671 ns/iter (+/- 22) test tests::bench_best_5 ... bench: 3,617 ns/iter (+/- 179) test tests::bench_best_6 ... bench: 1,997 ns/iter (+/- 171) test tests::bench_fsm_0 ... bench: 168 ns/iter (+/- 10) test tests::bench_fsm_1 ... bench: 127 ns/iter (+/- 7) test tests::bench_fsm_2 ... bench: 124 ns/iter (+/- 3) test tests::bench_fsm_3 ... bench: 108 ns/iter (+/- 2) test tests::bench_fsm_4 ... bench: 854 ns/iter (+/- 82) test tests::bench_fsm_5 ... bench: 3,600 ns/iter (+/- 119) test tests::bench_fsm_6 ... bench: 2,385 ns/iter (+/- 279) test tests::bench_fsm_psuedovec_0 ... bench: 92 ns/iter (+/- 30) test tests::bench_fsm_psuedovec_1 ... bench: 53 ns/iter (+/- 2) test tests::bench_fsm_psuedovec_2 ... bench: 49 ns/iter (+/- 2) test tests::bench_fsm_psuedovec_3 ... bench: 32 ns/iter (+/- 3) test tests::bench_fsm_psuedovec_4 ... bench: 678 ns/iter (+/- 37) test tests::bench_fsm_psuedovec_5 ... bench: 3,759 ns/iter (+/- 87) test tests::bench_fsm_psuedovec_6 ... bench: 1,994 ns/iter (+/- 107) test tests::bench_regex_0 ... bench: 1,360 ns/iter (+/- 390) test tests::bench_regex_1 ... bench: 829 ns/iter (+/- 85) test tests::bench_regex_2 ... bench: 820 ns/iter (+/- 221) test tests::bench_regex_3 ... bench: 505 ns/iter (+/- 77) test tests::bench_regex_4 ... bench: 16,347 ns/iter (+/- 5,366) test tests::bench_regex_5 ... bench: 46,269 ns/iter (+/- 16,688) test tests::bench_regex_6 ... bench: 36,315 ns/iter (+/- 15,455)
Seconded, and there's also https://www.patreon.com/redox_os.
This is like fractions of a millisecond. Your probably seeing the effects of the OS , scheduling and hardware caches. I tried running it repeatedly and got fluctuating results with no changes. cargo rustc --release -- --emit llvm-ir generated the same ir for inline and nothing, and it was inlined. inline never had a call so it didn't seem inlined. Though if I'm wrong Id be very interested in hearing why heh
I write mostly python... It is easy as hell to get something you want to happen to happen. It isn't efficient though. And that's fine until it isn't. I am currently learning rust because I wanted to lean a low level language that allows me to control hardware with more granularity, and I really like the fact that simple, procedural bugs that plague systems programming just won't compile. The idea that humans aren't good at stuff like that and if a machine can do it a machine should do it appeal to me a lot. Wanting to compile bad behavior should be explicit, not implicit, and rust works that way. If you're writing web stuff, stick to what you're used to. But either now or later you're going to want to write embedded systems stuff, or just a non-browser GUI that runs smooth as butter. You'll want to learn a low level language then, and rust to me is the best candidate right now.
How long until clippy is integrated with RLS? ;)
If what you're trying to do is print the enum, then perhaps you should do something like implementing `Display` or `Debug` for the type, that lets you turn it into some kind of string.
Things to rule out: * caching effects of the data (pages hot already? that is, do different tests share a data set in memory and is there an unlucky one that runs first and faults the pages in?) * caching or alignment effects of the code * dominating effects like system calls: your process is rescheduled, system behaviors can introduce a bias in benchmarks, or unpredictability &gt; Even altering unrelated code caused it to become fast. You should compare disassembly between the good and bad cases. e.g. `objdump --disassemble --source foo.o &gt; foo_good.dis ; edit / build_bad ; objdump --disassemble --source foo.o &gt; foo_bad.dis`. Zoom out to the whole executable if you need to see differences. &gt; EDIT: I’m not sure what the best way would be to get asm output for the different builds and compare them, but I think that would be beneficial. Yup, that's the key step here. Experts encounter puzzling symptoms like this and are only able to muddle through by reasoning about differences in the disassembly. I'd encourage you to avoid spoilers if someone here does it for you. This exercise is a valuable one. Try it yourself, think about why it looks this way, try some other changes, make some hypotheses. Then come back and compare notes with anyone who's volunteered to do the same.
Patreon sends me two mails each month. A receipt and a VAT invoice. Which mskes me unlikely to forget about it, unlike some other monthly stuff I have agreed to.
When I ran the two variants many times, they were consistently within their own ranges. So I don't think it's due to fluctuations. 
Best 5 change significantly if you change the code like shown in the post?
Thanks for the tips! Will get back to this in the morning. I'm glad to see people have taken an interest in it.
&gt; You can stop weeping and screaming now, you’re not in the first year of your computer science course anymore. So. First class in the Java subject. Walk in, sit down. First class is usually boring as hell; I've been programming since I was, I dunno, ten or something, so it's just going through the motions for me. There's the usual administrative stuff. Then the lecturer decides to give the class a taste of Java. He writes Hello, World. Five lines, nothing fancy. "This is not Java, this is bad." Half an hour passes, over the course of which I can feel my blood run cold, and my soul begin to die by inches. Hello, World goes from an inoffensive five lines to five files, spread across multiple screen's worth of classes and interfaces. The code has become so abstract, there's a goddamn Factory class in there. I'd tell you how the rest of the class was reacting, but I found myself unable to tear my eyes from the scene of horror unfolding before me. Finally, his unholy work done, the lecturer turns back and says: "This is Java, *this is good.*" To this day, I maintain that I heard someone behind me sob at that moment. Or maybe it was me, I don't know. --- (Also, this was the same lecturer who gave a final exam that required us to write a Swing UI. On paper. With no reference. Without ever telling anyone they had to *memorise the Swing API*.)
Haskell doesn’t have subtyping, I don’t believe, and so doesn’t have this problem 
I used to work at a company that had a simple CRUD app with maybe 15 screens at the absolute maximum. Pretty simple, but the guy who wrote it fancied himself an enterprise architect and so it was over 60k lines of C# code split into something like 20 projects, with several layers of interfaces, dependency injection in every controller, custom annotations to control behaviour at endpoints, absolutely everything coated in a thick layer of runtime reflection. As one would imagine (since the only faster code is less code) this was dog slow, and so to fix this he added several more layers of custom annotations and runtime reflection to implement caching. Not just any caching, however, but cursed caching that had the magical ability to always do exactly the opposite of what you wanted. It would cache only parts of web pages and give no indication as to whether it was fresh and no ability to clear the cache. Then, just to make sure we could never put reigns on the monstrosity he had wrought, he left the company. The company failed about 6 months after, although it was poorly-managed and would probably have failed even with the most beautiful and simplistic architecture in the world.
I'm guessing your eyeballing it. Here is how I tested it. I commented out all of the test except the one you want to know about. `bench_best_5` I ran cargo clean between two runs, one where `get_bounds` had inline and one where it was inline(never) I ran a little bash line to get these numbers `for x in {0..50}; do cargo bench 2&gt;&amp;1 | grep best_5 | awk '{print $5}' ;done | tee out_inline.txt` and `for x in {0..50}; do cargo bench 2&gt;&amp;1 | grep best_5 | awk '{print $5}' ;done | tee out_inline_never.txt` In [10]: with open("./out_inline.txt") as f: ...: total = 0 ...: lines = 0 ...: for l in f: ...: total += int(l.rstrip().replace(",","")) ...: lines += 1 ...: print total ...: print lines ...: print total/lines ...: 138773 51 2721 In [11]: with open("./out_inline_never.txt") as f: ...: total = 0 ...: lines = 0 ...: for l in f: ...: total += int(l.rstrip().replace(",","")) ...: lines += 1 ...: print total ...: print lines ...: print total/lines ...: 140468 51 2754 After 51 runs between both, there's no significant difference. If anything the non inlined would be slower. The instructions generated are the same between different builds from what i can see. A couple hundred ns is can be a couple cache misses. [Latency Numbers Every Programmer Should Know](https://gist.github.com/jboner/2841832) plus what ever else is going on your computer. 
&gt; Is there a more generic Error type that isn't so IO specific but has more general ErrorTypes? &gt; [...] &gt; I feel kind of strange using an input output oriented Error structure for something that is a internal 'business logic' related error Why don't you write your own Error type (or multiple), tailored to your business logic? You can `impl From&lt;std::io::Error&gt; for YourError` for convenience. 
I think if Rust is proving anything, it's that having a rubber duck that can offer up advice on its own instead of the usual worried silence is a good thing.
That's good to know, that definitely makes a big difference.
/r/rust_trivia
I think RLS can already show clippy advices if I remembered correctly.
Yup, it is for some time now! You can check https://github.com/rust-lang-nursery/rls/issues/149 for a tracking issue regarding clippy integration (which needs some ironing but works in general).
Then try reimporting project like mentioned above. I had the same issue.
Released [atomic-array](https://crates.io/crates/atomic-array) and [atomic-ref2](https://crates.io/crates/atomic-ref2) which together map several classes of [java.util.concurrent.atomic](https://docs.oracle.com/javase/10/docs/api/java/util/concurrent/atomic/package-summary.html) into Rust. Had a need from a recent project and wanted simple, safe utilities.
Ah, so it's not like `racer` where the functionality is always built-in to RLS, it's a thing that works if you already had access to clippy separately?
You would have to have `&amp;'a ()` in order to construct your instance
Strongly agree! /u/sebcrozet has put in an impressive amount of effort into the gamedev and linear algebra ecosystem in Rust, and moreover has even put himself in a financially disadvantageous position by working part time in order to be able to spend more time on his Rust projects. I'd be happy to see more people supporting his work.
The sooner you can get Clippy into rustup the happier I'll be. I can live with it being nightly, but it would be really helpful to have it match the compiler.
If we give infinite money to Leonardo Yvens now, do we get GATs tomororow? 🤔 
Usually you don't make fields public in presence of `PhantomData`.
Hi, so I'm walking through the tutorial and I have a question about "consuming adaptors" of iterators like `filter`. Is it possible to use them in an immutable way on a reference. Concretely, I know I can do this, consuming the array: let legal_age: i32 = 18; let v: Vec&lt;i32&gt; = vec![6, 18, 27, 16]; let authorized: Vec&lt;i32&gt; = v.into_iter().filter(|x| x &gt;= &amp;legal_age).collect(); // cannot use `v` anymore since "consumed" by into_iter() // how to filter an immutable value, creating a new one?
You can add `T: From&lt;i64&gt; + From&lt;&amp;'a str&gt;` bound to convert either variant to `T`. Or any other bound that gives you functions to convert any type from enum to `T`.
I’d file a bug with them, it’s probably unintentional.
My experience with newcommers (also me back when I started) is that they are pretty willing to use nightly and often do so without much consideration about what that means. Nevertheless, having clippy on stable should eradicate quite some headaches for newcommers, be it through its lints or not having to install nightly.
Not yet. There will be an RFC at some point.
Can you give a more concrete example besides printing the value of what you really want to do?
I want to write a method which wraps a `Read` for computing a hash/checksum (data is passed through). It seems straightforward, but when I looked at the implementation of `BufReader::with_capacity`, I saw unsafe code (`initializer` method). Do I also have to handle this in my wrapping implementation?
I'm just trying to show by example that this particular feature should not be advertised as zero cost. But looks like nobody else agrees with that.
The point of Debian rust packages is to compile Debian-packaged Rust code. And Debian-packaged Rust code will always compile with the Debian rust (and cargo, and librust-foo-dev) packages. That allows Debian to ship software written in Rust.
I can't personally help with the implementation detail of this plan, but the concept itself seems sound. Submit an RFC? 😁
Zero cost usually means "you don't pay for abstractions you don't use, and abstractions you do use couldn't be hand-coded better." I don't believe that `Box&lt;dyn TraitWithAsyncFn&gt;` could be done better than by converting the trait's async fns to return `Box&lt;dyn Future&gt;`. And in the unboxed case, e.g. taking an arg `impl TraitWithAsyncFn`, the futures returned from the trait's async fns are unboxed, so that couldn't be done better either.
I mean `Box&lt;dyn TraitWithAsyncFn&lt;_Future = MyFutrue&gt;&gt;`. If I know the type in advance i only have 1 dynamic dispatch, not 2, and i don't have an extra Box.
The futures returned from async fns are unnameable (because the compiler desugars the fns to return generators) so you'd never be able to write that if it's using an async fn. And if it's not using an async fn then none of this discussion applies.
&gt; The futures returned from async fns are unnameable Yes, I know that, I even said it in [my previous post](https://www.reddit.com/r/rust/comments/8opgfq/async_methods_ii_object_safety/e05uw4h/). I am comparing async fn feature to hand-rolled code that only using old Futures. Why does it not apply? I can imagine a use case when somebody porting their old Futures code would incur a cost when they port it all to async fn.
I kind of wish Rust would offer the same level of control over trait objects as Louis Dionne's dyno library: https://github.com/ldionne/dyno Being able to configure exactly the trait object layout would make some design decisions that are currently unthinkable possible, e.g., for example if you can configure the trait object with a small object optimization to make them a bit fatter, returning "boxed" futures might not require a memory allocation in most cases.
I wanted to say that `1147404288` might actually be too large to fit in a float without loss of precision, but that doesn't seem to be the case: http://rextester.com/RIDNK33214.
Ist 16_777_217 the largest integer that fits into f32? 1_147_404_288 is far larger than that. I think you confused f32 with f64.
you could do `cargo rustc -- -D warnings`, i think? instead of `cargo build`.
That looks like a (pretty significant) issue in the formatting code: if you provide an explicit precision to the float it prints properly fn main() { let number: u32 = 1147404288; let float32: f32 = number as f32; let back_again: u32 = float32 as u32; println!( "u32: {}\nf32: {:.0}\nback_again: {}", number, float32, back_again ); } prints u32: 1147404288 f32: 1147404288 back_again: 1147404288 
You have missed something important. An f32 can only store 23 bits precision, or a precision of 1 in 8388608. Since your u32 is 136.78125 times as large, the print function knows that the real value can differ at most 128, and thus for convenience the conversion to string rounds to the nearest hundred. Technically it's not a compiler bug, but an stdlib feature.
No it can be stored as (in bytes) 4E 88 C8 00
Great thanks for finding a work around. But we should probably report it haha
It can be stored as 4E 88 C8 00
It looks like printing of floating-point numbers is being limited to eight significant digits. If you print using `println!("f32: {.0}", 1147404288f32);` you get `1147404288`. This looks like a bug to me, especially because the `f32` closest to 1147404300 is *not* 1147404288 but is 1147404304. I do not really know how the number of significant digits is chosen.
but byte representation an float aremt the same or am i missing something? 
That's right, check here: https://www.h-schmidt.net/FloatConverter/IEEE754.html
I think there's something else at play here. If you use a slightly different value... fn main() { let number: u32 = 1147404287; let float32: f32 = number as f32; let back_again: u32 = float32 as u32; println!( "u32: {}\nf32: {:.0}\nback_again: {}", number, float32, back_again ); } prints: u32: 1147404287 f32: 1147404288 back_again: 1147404288 Without the formatter, you get... u32: 1147404287 f32: 1147404300 back_again: 1147404288
&gt; An f32 can only store 23 bits precision 24, with the 24th being implicit. The smallest integer f32 *can not* exactly store is 16777217 (2^24 + 1).
No, a bitset is different. Mine's a list. First I need to clarify that I'd rushed through this too fast and dropped the factor O(log n) from the range tree lookups, so one might as well use a compacted tree format and do the straightforward thing. The idea I was thinking about seems to be very similar academic solutions, but they drop the O(log n) by allowing upfront preprocessing, which we don't want. The basic idea is that every time you add a new sub tree, you take the previous bitlist and you add one bit to the end. · /|\ 0 1 0 / \ \ 01 01 00 | / \ 010 001 000 | | 0101 0001 Then you can take two arbitrary subtrees' hashes, and calculate their probable shared ancestor. lhs 01010101110101 rhs 01010101101100100 lhs ^ rhs 00000000011001100 ctz |-------| 9 This overlap may include extra parts that overlapped by chance, but on average there will be only one. The problem with this method is then you need to fine-tune this depth and extract the actual scope, which means range lookups, which means a tree search, which means you might as well just traverse a tree in the first place. Avoiding the chained hash maps is a straightforward win though. 
If you get that value as the result of a calculation, you only know that the real result is somewhere between 1147404224 and 1147404352 (assuming that limited floating point precision is the only source of numerical error). IMHO it does make sense for the default format to round that to 1147404300. Printing 1147404288 would pretend that the number of significant digits of that number is larger than it really is.
The hex they're giving is the IEEE single-precision representation of 1147404288.
It is currently built-in, but from what I understand it’s still nightly-only (like Clippy).
&gt; I don't think this is a printing issue. See my edit, the internal value of the float is correct, which suggests that it's a printing issue (I think) 
Yes.
16777217 is the first integer which can not be stored exactly in an f32 (it's when the conversion starts having holes), but OP's integer is one of those beyond that limit which does get stored exactly in a single-precision IEEE float.
Is it correct, though, to say there is no stable release with a bug this grave that doesn't have the corresponding patch release?
Thank you for the suggestion. I implemented the depth idea, it worked really well: https://github.com/rust-lang/rust/pull/51394
Thank you for the suggestion. I implemented the depth idea, it worked really well: https://github.com/rust-lang/rust/pull/51394 
I would like threat the struct Foo `{ctor: u32, a: u32, b: u32}` as an enum: enum Foo { A = 0 (u32,32), B = 1 (u32), C = 2, REST = 3..2^32 (u64) } So I can pattern match on it. I also need to be able to convert `REST` to `u32` and `u32` to `REST`. I am aware that this is not possible to do in rust, but maybe there is some trick to get the advantages of pattern matching **(jump table, automatic conversion of u32,u32 to u64, etc.)**. I could do enum Foo { A (u32,32), B (u32), C, REST(u32, u64) } but that would increase the size of struct Foo to `32*3+8` (I suppose the size of constructor is one byte). And I want to keep the size of struct Foo as small as possible. Is there any other way?
I believe so, as this feature was made stable in 1.26.
[/u/Eingaica's comment is not uninteresting](https://www.reddit.com/r/rust/comments/8oz9va/can_someone_verify_that_this_is_a_compiler_bug/e078970/), it's odd but it might be an explicit choice to avoid hinting at more precision that exists: technically, while 1147404288 stores exactly all you actually know at the usage side is that the source was somewhere between 1147404224 and 1147404352.
The largest odd integer that can be stored exactly as f32 is 2^24 - 1 = 16_777_215. For numbers divisible by 2 but not by 4, the largest that can be stored exactly as f32 is (2^24 - 1) * 2 = 33_554_430. That can be stored exactly because it is stored kind of like 16_777_215 &lt;&lt; 1. The number in question, 1_147_404_288 is equal to 8_964_096 &lt;&lt; 7, and is can be represented exactly as f32, as can be the next f32 which is 8_964_097 &lt;&lt; 7 = 1_147_404_416.
I'm not sure I agree since back_again is different from the original value with the integer I chose. fn main() { let number: u32 = 1147404287; let float32: f32 = number as f32; let back_again: u32 = float32 as u32; println!( "number: {}\nback_again: {}\nback_again - number: {}", number, back_again, back_again - number ); } prints: number: 1147404287 back_again: 1147404288 back_again - number: 1 
That's because 1147404287 can't be stored exactly in a f32
Yeah you only know that the value is between these two, but 1147404288 would be the correct interpretation, because 2^30 * 1,068603515625 = 1147404288 and not 1147404300. 
I didn't get it from a calculation I read it from a file, thus I would like to be able to print it out again. But I understand your argument
Well, winapi really needs sponsorship or more maintainers at least. I remember a month or two back the project looked pretty abandoned, people was waiting *for months* to get their PR *merged*. I mean, what could be more difficult than to click Merge? But now it looks more active, tbh. Fortunately. Because a team proposal was [rejected](https://github.com/retep998/winapi-rs/issues/268#issuecomment-196563081) for no reason.
Oh right, good catch. Scratch my line of thinking.
This seems to be a problem with the formatting, not with the `f32` datatype itself. The number is stored correctly in `float32`, the number `1147404300` does not exist in IEE-754 anyways. I have played around with some other numbers, and it seems that somewhere around `3e8` the formatting function starts rounding to tens, and at around `3e9` it starts rounding to hundreds. 
&gt; 1147404288 would be the correct interpretation No, it's the *precise* interpretation, but after decimalisation it *implies* a precision which does not exist: generally speaking, when humans read numbers trailing zeroes to the integer part are assumed non-significant.
I don't think it's so clear that 1147404288 is the correct interpretation. The library is using the least number of significant bits such that converting the printed number back to f32 gives the correct value. This makes it consistent with printing numbers with fractions. The only difference is that since this is a large number, the digits 11474043 have to have two zeros added at the end, whereas with fractions, the printing just stops after the least significant digit.
Then I'd rather have it written in scientific notation, like f32: 1.1474043e+9 or something like that. Reading `1147404300` makes me think that's the actual value, while reading scientific notation makes me think "hmm, this is a float, the actual value might be rounded, let's be careful".
This has literally nothing to do with storage. Drawing inspiration from a really stupid joke: &gt; A guide is giving a tour around the fossil museum. As they approach one of the fossils, the guide says: "This fossil is 20 000 017 years old." &gt; "How do they know that?" One of the visitors asked. &gt; "Well, when I started working here 17 years ago, they told me this fossil was 20 million years old." According to scientific notation, your float has the value 1.1474043E9. Not 1.147404288E9. Not 1.147404300E9 (as you are assuming). Floats have a limited precision and even though they can represent certain values doesn't mean you should rely on that. So ignore IEEE754, or floats, or bit representations, because the answer to your question has nothing to do with that. This is entirely a scientific notation issue. PS So does `println!("{:.0}", 1147404288.0f32);`, as mentioned in another comment.
I'm starting to use rust for AI stuff (like tree exploration) that needs raw speed (when go is not fast enough for that). So anything that is not system/kernel programming but needs raw speed / fine memory tuning might be a good fit for rust.
You could keep the "raw" definition that you would use where you want the size to be small, and an enum for the places where you want to match on it, and define conversions between the two. [Something like this.](https://play.rust-lang.org/?gist=18ad34d4fd38ffdfc0332f0f21d6b83c&amp;version=stable&amp;mode=debug)
Yes, that's unfortunate. But since most floats are results of calculations, I still think the current behaviour makes sense. For integers, you can use the `{:.0}` workaround that has already been mentioned. I don't know if there is a simple solution that also works for fractions.
Fair point. Python (3) does that: &gt;&gt;&gt; 1147404288.0**3 1.5105997402875323e+27 
That unsafe code is there only to initialize the buffer - it creates a vector of bytes with uninitialized values, which is unsafe, and then uses the initializer to possibly zero them out. If you just want to pass all read operation through to the wrapper reader, you don't need to do anything special or unsafe.
Yeah, async/await is going to be *great*! Futures work just fine, but are kinda hard to learn and frightening for newcomers.
Thats cool. Is it also possible to treat Vector&lt;u32&gt; as Vector&lt;Foo&gt;? 
I'm having some difficultly writing idiomatic code. I'm currently writing a particle system. It looks something like pub struct Particle{ ...} enum Message { None, RemoveSelf } impl Particle { fn update(&amp;mut self, dt_ms : u32) -&gt; Message fn display(&amp;self, displayHandle) } Vec&lt;Particle&gt; particles; Now I want to iterate over these and call update. And remove ones that return a Message::RemoveSelf. I looked into retain: particles.retain(|mut p| match p.update(dt){ Message::None =&gt; {true}, Message::RemoveSelf =&gt; {false}, }); but retain can't offer a mutable. so I have to loop twice - one for p in &amp;mut particles. And another retain. Is there a better solution to this problem? I think particles should be mutable, unless i am misunderstanding something about how copies and writebacks work. Also I wanted to display every particle. I reached for map: particles.into_iter().map(|p| p.display(displayHandle)); but this doesn't work as map is lazy?! I have to use yet another for loop for this. It seems inelegant and weird. Am I doing this wrong yet again? Another question that might resolve some of the above: If in the above display issue I had display return &amp;self. would I be able to use .map and collect it back into the Vec? What would be the performance impact of this, if any? 
Well, have you tried to not consuming the vector? let legal_age: i32 = 18; let v: Vec&lt;i32&gt; = vec![6, 18, 27, 16]; let authorized: Vec&lt;i32&gt; = v.iter().cloned().filter(|x| x &gt;= &amp;legal_age).collect(); ^ ^ | but it makes it iterate over references, | so we add .cloned() to clone individual values use .iter() here because it takes vector by `&amp;self`, whereas .into_iter() takes `self` // can use `v` here, since .iter() does not consume it If cloning individual values is expensive, you might want to remove `.cloned()`, and instead collect to `Vec&lt;&amp;_&gt;`. However, this will borrow the original vector immutably.
perfect, glad its that easy!
I think you can use something like fn func() -&gt; impl ::std::error::Error
Or create your own type based on it
you can put `#![deny(warnings)]` on top of your `main.rs`
I wrote an algorithm to simulate Hawkes processes as part of coursework. I was using Python before that, with the [numba](http://numba.pydata.org/) library to get as much performance as I could, but once I started introducing structured spaces (different data types in the same array, such as strings), or wanted to define classes, it became unmanageable or impossible. The experience was quite nice
The goal with filtering is not to copy the all initial data ;) I finally found an answer though, I can copy at the end with an addition map like below: // Use one ".map(|x| *x)" to create a copy and keep input immutable. // can use ".map(|x| x.clone())" for types that are not Copy let v: Vec&lt;i32&gt; = vec![6, 18, 27, 16]; let authorized: Vec&lt;i32&gt; = v.iter().filter(|&amp;x| x &gt;= &amp;legal_age).map(|x| *x).collect(); println!("v: {:?}", v); // v still available
This is a pretty "handwavy" question, but maybe someone can send me in the right direction nonetheless. I don't (yet?) need specific code hints, but some sort of overview which things I need and what I should look out for. I have an algorithm that starts from a `Vec&lt;String&gt;`, parses the first 8 bytes of each `String`, and then does some grouping of the results following some specific rules. Right now, I made an iterator out of the `Vec&lt;String&gt;`, and the algorithm calls out to several functions that call `iterator.next()`, parse the String and then do the proper bookkeeping needed depending on the parse result. Functions might call `iterator.next()` several times, if need be, but after some time, they return, and then the next proper function will be called. The point is, I'm getting the `Vec&lt;String&gt;` "somewhere else" (it's an RPC API that delivers it), and now I'm thinking about how to go about reading the lines from a file. The file will be quite large, and performance is paramount, so I'd like to do this in kind of a "streaming" fashion (I'm lacking the vocabulary here). I'm imagining something like * Thread A reads the first 8 bytes of the file, sends them to thread B. Then it searches for `\n`, reads 8 bytes, sends them off... and so on * Thread B receives 8 bytes, parses them, sends the results to thread C. Thread B also might group several batches of 8 bytes for more performance(?) * Thread C uses the parsing results to to the grouping. If the information is incomplete, it just waits for more parsing results to arrive. I've said "Thread" here, but I'm not sure I need/want threads, it's just sort of a mental model for me. The main point is that I want the work to happen in parallel, i.e. I want the parsing to already happen while the file is still being read. What's the way to go about this? Should I look into using threads for this, like I described? I've somewhere heard about "streaming iterators", is that something I need to look into? Thanks for any pointers :) Of course, if you can point out examples to similar things or useful libraries, I'd be grateful, too.
The actual code is here: https://doc.rust-lang.org/src/core/fmt/float.rs.html#106 If you don't provide a precision, then the formatter uses the representation that implies the fewest number of significant digits. When represented in decimal, that means specifying to the 100s place. If you want an "exact" representation, specify a precision.
This works. Although I would prefer not having to write it in the source code.
I found out it should be \`cargo rustc \-\-bin crate\_name \-\- \-D warnings\` (for my binary). I'll go for this solution, as it is more elegant than the other one.
`.cloned()` does exactly the same thing as `.map(|x| x.clone())`. Also, for small `Copy` types it probably won't matter whether you clone them at the start or at the end.
After some trial and error, I recommend you also pin the rust version. This puts you in control of when new warnings break your code. i don't have examples yet for gitlab but maybe you adapt my github example: https://github.com/crate-ci/stager/blob/master/.travis.yml
wow this is awesome. Can it be added to the [compiler's collection](https://github.com/rust-lang/rust/blob/master/src/test/run-pass/weird-exprs.rs) of weird code?
You can't pick what layout you want for enums, so the conversion wouldn't be zero cost. However, this shouldn't be a problem as LLVM should have no problem inlining the conversion function and then optimizing matching to basically work on the raw struct.
Me too :)
It is also possible to just set environment variable \`RUSTFLAGS=\-D warnings\`, then run \`cargo build\` as usual
Well, my project relies on the nightly branch, so I'll just stick with the latest and see what happens. Thanks for the advice anyway. I might use that in future projects.
&gt; I mean, what could be more difficult than to click Merge? You're really undervaluing the work that goes into a Merge. The act itself is small, but there's quite a bit of stuff to check and do and investigate if you want to keep your project functional.
1. Instead of retain, you can use [`drain_filter`](https://doc.rust-lang.org/std/vec/struct.Vec.html#method.drain_filter) ([which `retain` uses internally](https://doc.rust-lang.org/src/alloc/vec.rs.html#908-912)), something like this: for _ in particles.drain_filter(|p| p.update(dt) == Message::RemoveSelf) {} 2. You can use [`for_each`](https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.for_each), which is not lazy. However, using for loops for side effects is idiomatic Rust. 3. Definitely not idiomatic. Just use a for loop.
sure it can! Trailing commas can make it even stranger: fn main() { println!("{:?}", (|&amp;__@_:&amp;'_ _,|-&gt;_{[(|(..,_,__,_,):(_,_,((),),)|__..__)(__)]})({&amp;({||('"');}&lt;=*&amp;(),&amp;[..=..],((),),)})); } 
Is your .wasm file being served with the `application/wasm` content type from CreateReactApp? I'm not too familiar with it, but you do need that content type for `instantiateStreaming` to work. Alternatively you can use the 'no modules' option like this: $ cargo build --target wasm32-unknown-unknown --release $ wasm-bindgen target/wasm32-unknown-unknown/release/PACKAGE.wasm --out-dir . --browser --no-modules That will build your code, do some basic optimisation from `wasm-bindgen` and output the new files to the current directory. Then in your HTML, you can just do: &lt;script src="file.js"&gt;&lt;/script&gt; &lt;script&gt; wasm_bindgen('./file_bg.wasm').then(() =&gt; { wasm_bindgen.main(); }); &lt;/script&gt;
Starting this week, "Final Comment Period" section now also includes tracking-issues and PRs from [rust-lang/rust repo](https://github.com/rust-lang/rust/labels/final-comment-period). Additionally, "New Contributors" section is removed in favour of [Rust Contributors](https://thanks.rust-lang.org/) site. (And also because it didn't seem fair to list contributors from just one Rust repository).
&gt; Because a team proposal was rejected for no reason. The proposal wasn't rejected (or at least it doesn't say so anywhere in that thread). &gt; I mean, what could be more difficult than to click Merge? Blindly merging PRs would quickly result in winapi becoming an unmaintainable mess. I mean, it *would* be nice if PR authors get quicker feedback telling them what they can do to get the PR merged (be it a code change or waiting for another planned change that should land before the PR), but "just click merge, duh" really doesn't cut it.
`RUSTFLAGS` is too fragile; if any of your dependencies throw warnings, they will also cause your build to fail. `cargo rustc` only does it for your package.
Super! Years ago I implemented a very similar thing for C\+\+ (`FooVec`s that can be indexed only b`y FooI`dxs) and use it everywhere, so I've been looking for the best way to do the same thing in Rust. The blog post says that it is mostly for readability (and it does help a lot) but I actually have occasionally caught real errors at compilation time this way.
In most of projects - definitely. But in bindings? There must be no work at all - bindgen.
ty
I don't think the work in winapi is done by bindgen, even if it might help somewhat. This is out of scope for me, so I might be wrong, but you're still undervaluing the work of the maintainers by suggesting they need to just "click Merge" for a PR. Even if the PR was auto-generated by bindgen, a maintainer still has to review quite some things, and the PR might not be on top of the priority list.
The compiler error is this: ``` = note: candidate #1 is defined in an impl for the type `failure::Fail + 'static` note: candidate #2 is defined in the trait `failure::Fail` --&gt; /Users/mitsuhiko/.cargo/registry/src/github.com-1ecc6299db9ec823/failure-0.1.1/src/lib.rs:152:5 | 152| / fn root_cause(&amp;self) -&gt; &amp;Fail where 153| | Self: Sized, 154| | { 155| | find_root_cause(self) 156| | } | |_____^ = help: to disambiguate the method call, write `failure::Fail::root_cause(...)` instead ```
&gt; [compiler's collection](https://github.com/rust-lang/rust/blob/master/src/test/run-pass/weird-exprs.rs) of weird code Heh, the strangest part of that is the function names!: &gt; strange(); funny(); what(); zombiejesus(); notsure(); canttouchthis(); angrydome(); evil_lincoln(); dots(); you_eight(); fishy(); union(); special_characters();
[ministat](https://github.com/thorduri/ministat) is handy for this sort of thing.
seems to be more about the magnitude than the precision though... &gt;&gt;&gt; 1147404288.0**1.7 2520681631020628.5 &gt;&gt;&gt; 1147404288.0**1.8 2.029970110758693e+16
I get that the operation is the same, I was pointing at the fact that it is not done at the same step. If your filter keeps only 1/10 of the data, it's way more advantageous to have the copy done after filtering than before.
Only if the func only can return one type of error. Otherwise, fn func() -&gt; Result&lt;(), Box&lt;::std::error::Error&gt;&gt; ...is what I tend to use in application code.
Does hyper support request pipelining? AFAIK it is now standardized in HTTP/2, meaning that a lot of the difficulties with it in HTTP/1 should not apply. I am mostly interested in the client-side.
You're not wrong but I don't think that's a counter argument. Visibility mostly affects the scope of my usability arguments. They're still there even if at a smaller scale. And for the case of tagging with a phantom type I'd argue that being able to not have the private field would be a net increase in usability.
&gt; Hello, World goes from an inoffensive five lines to five files, spread across multiple screen's worth of classes and interfaces. How the heck is that even possible ? How can someone even purposefully do something like this ?
You seem to be very knowledgeable about the performance characteristics about memory allocators. Could you give me some tips on some tools that I could use to analyze / profile / collect stats related to memory allocations done by my Rust software? This seems like something interesting that I should explore more.
*This* is your brain on *Java.*
`while return {...}` is such an amusing construct
I was worried and then I was blown away - replicating a constant value *in the length of the array* for the `[u8; $x.len()]` type is something I wouldn't have considered. I *really* hope we can soon make progress on the "lazy normalization" front (preferably including `const` generics), to allow array type lengths to depend on generics.
That's honestly pretty impressive, that there have been so few times where they needed to do point releases.
Hm, `cargo rustc` also doesn't apply for build scripts and dependencies from same workspace. So, it would be needed to call `cargo rustc` for every project from the workspace, and also somehow deny warnings on build scripts somehow (is it even possible?) for such a repository to be completely warning-free.
What will it do? Just return and that’s it, yes?
He certainly had to be smoking something else...
If the requests you make are HTTP2, hyper allows you to easily make them over 1 connection at the same time. If HTTP1, hyper will keep it to 1 request at a time. My opinion after a lot of HTTP1 experience is that it's not worth it at all (for example, all browsers have disabled their HTTP1 pipelining support).
http://play.rust-lang.org/?gist=e53776ff939a18654035c7a80bdd7d43&amp;version=stable&amp;mode=debug
A lot of these have interesting backstories. https://news.ycombinator.com/item?id=15495027
I dream a monad syntax alla Scala, i.e. by reusing `for` blocks: // trailing 'do' for procedural use for b in a.b, c in b.c, d in c.d, f in d.field *do* { println!("{} {} {} {}", b, c, d, f) } or // trailing 'yield' for expression use let result = for b in a.b, c in b.c, d in c.d, f in d.field **yield** { (b, c, d, f) }
What does the @ sign do? I'm so lost here
Turns out it's a 'pattern binding' \-\&gt; [https://doc.rust\-lang.org/book/second\-edition/appendix\-02\-operators.html](https://doc.rust-lang.org/book/second-edition/appendix-02-operators.html)
I got pretty close, except I expected the debug formatting to contain something like &gt;!RangeToInclusive&lt;RangeFull&gt;!&lt;. Also that's a very interesting way to generate &gt;!true!&lt;.
This should be a test case for `rustfmt`. 
&gt; Even if your hand made code is less generic and doesn't work for "any foo()". How useful would this construct be? Consider that you'll need to create an enum with a variant for each await point, and each variant needs a field for each variable alive at its corresponding await.
Now add `Deref` coercions into the mix, and reborrows suddenly get even crazier. You can reborrow your reference into another reference of a different type!
Wow this is an entitled and insulting comment. Merging a PR takes significant work. Loading the context of the PR, providing constructive feedback to help the contributor to be able to improve the contrib to the required level of quality, repeat until the PR is mergeable, etc... this all takes significant time. If you think you can do a better job, help out or fork the project and do it yourself.
I agree, 1.0 is not the same as 1, just like 0.5 is not the same as 1/2
The println was just to do something with it. I wanted to be able to get the inner value of an enum variant and do something fun with it. For the i64, some maths, for the &amp;str, manipulation or storing it.
I answered above, but it's really just to see what I can do with enums: For the i64, some maths, for the &amp;str, manipulation or storing it.
I tried this: enum myEnum&lt;'a&gt; { stringSlice(&amp;'a str), integer(i64), } fn stripOut&lt;'a, T&gt;(input: myEnum)-&gt; T where T: From&lt;i64&gt; + From&lt;&amp;'a str&gt; { match input { myEnum::stringSlice(T) =&gt; T, myEnum::integer(T) =&gt; T, } } fn main() { println!("{}", stripOut(myEnum::integer(5))); } But it's not converting it to `T`.
I answered above, but it's really just to see what I can do with enums: For the i64, some maths, for the &amp;str, manipulation or storing it.
I think it does a decent job already.
It's a shame drain_filter is a nightly, I'm trying to only use stable. At this rate I should just make a reverse for i &lt; loop, and remove as I go.
Sticking a ".0" on the end automatically when the formatter rounds seems easy and quite desirable?
This time, with no suggestions nor votes, I simply decreed im to be crate of the week. If you are fine with that, you're welcome. Otherwise, please head over to [the rust-users thread](https://users.rust-lang.org/t/crate-of-the-week/2704/402) and add your suggestions. Btw. suggesting your own crates is OK (but you should say that it's your own creation).
Because it allows you to write code you literally can't write today. Zero cost isn't about every feature having zero allocations. It's about enabling new code to be written in the best way possible. If someone wants to write a trait that can be implemented with unboxed futures using future combinators with closures, that simply can't be done today, because such futures are unnameable. So today futures in traits are either boxed or hand implemented without future combinators. Async-fn-in-trait will enable that code to be written with zero allocations, with ergonomic combinators. That was all explained in the first post of this blog series. This post digs into how such traits could also support dynamic dispatch. *But it still assumes the trait writer wants to use unnameable futures!* As long as that is something the code author aims to do, this feature is zero cost.
How do I create a superscript in a documentation string? It used to be that `^` would do it, as one is accustomed to in Markdown, but it doesn't seem to work any more.
&gt; generated the same ir for inline and nothing Doesn't this make it an open-and-shut case? If it's producing the same IR, it should produce the same machine code, which means the behavior at run-time is 100% identical so any profiling differences are environmental. Please correct me if I'm wrong. I actually wish the original example didn't include benches or external crates or I'd just slap it in Godbolt and have a field day. Otherwise I'm worried about compromising the test.
Still, what is evil about that Lincoln log?
I'm not sure if this gets you closer to what you're asking for, but you can implement From for the Options of the more concrete types you're trying to work over: https://play.rust-lang.org/?gist=31ed0f4d935dd88b2164c2cb328f124f&amp;version=stable&amp;mode=debug
Performance is great! On my benchmark, it is literally an order of magnitude faster than `aeson-pretty`, the JSON prettyprinter written in Haskell and distributed in Debian that I have been using to prettyprint a large JSON file (1.1M lines formatted, 0.28s for `pp`, 2.8s for `aeson-pretty`). Ability to read from a pipe would be helpful if easy to do. Please include `json` in the name of the tool: that's one of my pet peeves with `aeson-pretty`. `jsonpp` or `ppjson` are fine names.
I'm in the early stages of making a FOSS Dropbox sync client with a couple friends! It was inspired by getting fed up with my **/target/* and **/rls/* folders getting synced all the time. So I looked into how feasible it would be to make a new client with .dropboxignore support, and it looks very doable.
I think you will need to use &lt;sup&gt;&lt;/sup&gt;
Did you read the post? I said that removing almost any other code broke it in my tests, so removing all of them but the one definitely breaks it. And I said that most people haven't been getting my range results, but I know my results are occurring.
I really wish the example was much smaller too, but any changes broke the effect.
I don't know if it was particularly evil; like the other functions in that test case, it's an example of an edge case that you'd probably never encounter in normal code. The `&lt;-` operator on that line was just used for explicitly moving a value in ancient Rust, and there's pretty much no reason for a `log` expression to be on the right-hand-side of one of those. Rather than evil, I'd say it's just misunderstood. :P
I guessed that it would print `[&amp;[..=..]..&amp;[..=..]]`. I'm not sure why it doesn't print the ampersands.
Have we moved away from Markdown and gone with HTML instead?
ELI5?
I know it's great to have a short name like `pp`, but there's already at least one other tool with that name. Any chance for a rename? Great work by the way :)
I really enjoy his style of writing, very engaging and informative at the same time. Does he have a blog or something?
I think it's more of a "why would you even write that?" sentiment than pure evil, but given that I assume it crashed the compiler at the time, well... :)
Yes, I am aware of the problems with HTTP/1 and pipelining, so I am OK with `hyper` not implementing it. However, for my usecase, it is important to absolutely minimize the time delay between sending a bunch of requests to a REST API and receiving all of their responses. I was hoping that `hyper` would allow to do proper pipelining when talking to HTTP/2-capable servers once HTTP/2 support is implemented. I have not looked at `0.12` yet. &gt; If the requests you make are HTTP2, hyper allows you to easily make them over 1 connection at the same time. Is this true pipelining, though? Will the second request be sent to the server immediately, before the response to the first request comes back, or will hyper wait for the response before sending the next request over the same connection? Ideally (assuming the server can handle that), I'd like to properly pipeline my requests (not just reuse the same connection) -- send out a bunch of requests in a row over the same connection immediately without waiting for any responses. I will be looking at hyper 0.12 sometime in the next days/weeks and I will try to figure this out, but do let me know if it is supported :)
Markdown [supports inline HTML](https://daringfireball.net/projects/markdown/syntax#html) by design. 
The rust formatter is actually pretty cool in this respect: it's not easy to have a fast algorithm that determines the shortest string decimal representation that will convert to the exact float that produces it. AFAIK the rust one is not perfect, but might be one of the best of any stdlib for any language.
I still don't completely get how it works. Is it relying on the constant `str`s happening to end up in memory next to each other?
Does it support pipe opperator?
Wow, I didn't know that, or I had disregarded it! To my surprise, I don't find anything about `^` in the Markdown specification. I was sure it was part of it!
This is a benchmark to test ambient occlusion rendering of 3d geometry via raytracing. It's an attempt to have a simple benchmark that can be ported to different languages easily. the repo here is for Rust specifically and compares a few methods: * scalar: ie just serial naive code * vector: using simd instructions that allows the processor to do multiple calculations at once , but still single threaded * scalar_par: same as the naive serial method, but using Rayon now to spread out the load over multiple threads * vector_par: Same as the simd vector method above but now also multithreaded. It's a good way to see what kind of gains you can get.
I do! It’s here: http://troubles.md/
Man, sometimes Rust really does my brain in. I am *boggled* by how similar the scalar and vector parallel code is.
Consider using [`serde-transcode`](https://serde.rs/transcode.html) for the implementation instead of `Value`. That will let you preserve the order of map entries, avoid keeping the input data in memory all at once for large inputs, reduce memory allocations to almost none, and ideally improve performance.
I was looking for exactly this, thank you so much! serde is awesome!
Perhaps some kind soul who is fluent in English and German will provide English dubbing or sub\-titles?
How is this allowed? I thought the while condition must be a bool. Shouldn't return be !?
It's^a^nonstandard^reddit^extension! 
&gt; ambient occlusion rendering of 3d geometry via raytracing I suspect that's what /u/greppable777 was asking an explanation for (I for one have no idea what these words mean). &gt; It's a good way to see what kind of gains you can get. I'm not sure I understand what is being measured. Is it the performance of floating point operations with/without SIMD and/or Rayon?
I don’t really see Big differences, how come one is faster than the other? Code looks the same.
Presumably `!` is coercible to, or a subtype of, every type; otherwise, how would expressions like let x = if condition { 1u8 } else { panic!("oh no") }; typecheck?
&gt; I would strongly prefer a prettifier that respected the order of JSON tokens in the input stream. Indeed. I have json files where it is common to have a "name" or "type" properties to the top of the object, because that's what a human look for to know whether it's this object or the next they are interested in.
No. If I understand correctly, it declares a new const whose type happens to have the required size, by adding up the lengths of the inputs (which you can do now because `len` is const), copies them in by transmuting the original `&amp;'static str`s to `&amp;[u8; N]`s, and then transmutes back to `&amp;'static str`. 
The `..` in tuples I did definitely not know was allowed, but I guess it makes sense in destructuring tuples, such as: ``` let (x, y, ...) = (1, 2, 3, 4); ```
One thing I'd like in a pretty printer, is to AVOID splitting an object or list across multiple lines if they are short enough. For example, imagine the following json blob: "property": { "low": [0, 1, 2], "high": [7, 8, 9] } "Prettified" with a basic prettifier: "property": { "low": [ 0, 1, 2 ], "high": [ 7, 8, 9 ] } Sorry, but that's objectively worse. It wastes so much screen estate! Of course, pretty-printing "small-enough" objects/lists in a single line naively may be a real performance hog (lots of duplicated work), but hey, at least that's an interesting problem to solve :) *Note: a possible approach would be to (1) pretty-print everything without newlines/indent and (2) have a minimal json-parser to hop from `{` to `[`, to `"` so as to annotate the parse tree with the size of the "compactly formatted" output and (3) finally do the pretty-print, using the pre-computed size of the node to check whether to use the inline output or not.*
I'm not allowed to use non-standard extensions, say my parents. :-|
Two things are at work: 1) Instead of using a serial iterator which has the CPU perform one set of operations on one set of data at a time, it uses a parallel iterator which has the multiple CPU cores perform the set of operations on multiple data elements at a time. 2) Instead of working on the data in serial which uses generally single registers to operate on single data elements, it uses vector operations which combine 2, 4, 8 or more (depending on vector width) data elements into a vector and then uses the CPUs vector operations which do all those operations on the multiple data elements in the same clock cycles. When you combine them, you get dramatic speedups. That the code is very similar is the fantastic part: Rust's abstractions have allowed the expression of the parallel calculation to be very close to the simple linear single datum version. Of course, this is a the sort of task that's generally considered "embarrassingly parallel" in that there isn't much dependency between data elements so the speedup is very good. For less parallel problems, you wouldn't see as great as a speedup and it's not necessarily clear that the expression of such would be as similar between the serial version and the parallel vectorized version.
&gt; I don’t really see Big differences That's exactly the point, there are no big difference. The meat of the difference is here `(0..h).into_par_iter().for_each(`: instead of using sequential iteration, this uses the Rayon library to have parallel iteration. Rayon figures out how to efficiently distribute the work to multiple cores automatically. **Magic!** *Note: I do find surprising that the speed-up achieved is greater than the number of cores on the machine; since normally the best you can do for linear work is... linear scaling, and most often the overhead of synchronizing bring this down a bit.*
If only libui was not so limited. No list boxes, no skinning, no tray icons, no keyboard shortcuts, only the most basic events. Pretty much trying to do anything non-trivial you would hit a show stopper.
Neat, but it won't read from stdin? And the input file has to have ".json" at the end of the name? Could be better :)
Can you give examples of the breaking changes you're seeing?
If I understood correctly, it's still relying on the layout of fat pointers to not change, which AFAIU isn't guaranteed and kind of scary. This is still very cool none\-the\-less!
I think it’s the SIMD stuff, that gives another Nx speedup over the number of cores.
Thanks :)
I'd be pretty surprised if slice fat pointers changed, personally... 
Which I look forward to eagerly. OP's expression using this form: try { println!("{}", a.b?.c?.d?.field); } Much cleaner than the alternatives (except perhaps do-notation).
Given that they're talking about embedded, you end up using a bunch of nightly stuff. The most recent panic change, while *absolutely* a thing we want, is breaking, which means you need to update all your deps. It's just part of the whole nightly bit.
Ah so I can expand on that. So in this virtual world you have 3 dimensional geometry. In this case the spheres and the ground. But to see that you need to look through a virtual camera. This camera essentially collects samples across its field of view by shooting rays out to see what it hits. There is usually one ray at the very least per pixel but often more. The rays go outwards and trace to see where they hit something. When they hit an object they can either return that information as a pixel value or continue to calculate and even spawn more rays from the hit point. Here the rays are checking if the hitpoints would be hidden from an ambient light like a sun in the sky. They do this by generating more rays at the hit point to find their distance to the nearest objects. The pixel values returned are how much light an area can receive. This is called ambient occlusion, checking what parts of the image are occluded from ambient light. The recursive casting of rays can be very slow especially for naive solutions. But their processing is very simple hence they make for great benchmarks. What's being measured in these Brigette benchmarks is just how long it takes to calculate all the rays for the image. This is a function of the resolution and anti aliasing samples (essentially more samples per pixel) Let me know if you want more details on any of this. I'm not the creator of this but it is in my domain
Exactly. I understand why, but I still feel like there should be a better way. Crates.io notifications to crate owners if the dependancies get too far behind?
thank you for your detailed feedback! I released a new version that addresses most of your concerns! It includes: - reading from stdin - correct ordering - custom formatting - improved perf (most notable with gigantic files probably) If you have more formatting options in mind I'd be more than happy to accept a PR :)
&gt;Note: I do find surprising that the speed-up achieved is greater than the number of cores on the machine; It's likely because of hyper-threading
I like the name for its brevity. I think I'm going to leave it that way, thanks for pointing out the issue anyway! :)
fixed in new release ;)
the new release does!!
Can anyone break this down for the rest of us?
Interesting suggestion! I will think about it!
&gt; fn root_cause(&amp;self) -&gt; &amp;Fail where Self: Sized, I've never seen such a `where` clause before, where is that feature documented? It seems almost like it's adding an implicit bound to `Fail`, which smells kinda totally bogus. I note that removing that `where` clause in your testcase makes everything work perfectly, though I'm not sure if that's exactly useful to you: http://play.rust-lang.org/?gist=67109716a294cd53a45e9a7a6a99a66e&amp;version=stable&amp;mode=debug
I'd probably just ask folks to setup a cron on CI that builds current master with the latest nightly. I think this is very simple with Travis CI at least. There's a knob to turn in the TravisCI repository settings, and then you just need to have a `nightly` entry in your `.travis.yml`. The cron will run every day.
Can you run 100 tests each and show the average like that from your runs of the benchmark? 
thank you all for your review and kind feedback. I released a new version that addresses most concerns!
Have you considered using Rust's arbitrarily large decimals, then? I don't remember the name off the top of my head, but it should be easy to find.
What were you using to visualize the running threads in your shell?
How does this compare to \[jq\]([https://stedolan.github.io/jq/](https://stedolan.github.io/jq/)) ?
Maybe you only want `deny(warnings)` for your CI builds, so you don't have to deal with these strict failures during your normal edit-build-debug cycle? If so, consider using a feature. Add this to your `Cargo.toml` file: [features] fail-on-warnings = [] And then at the top of your crate, add this: #![cfg_attr(feature="fail-on-warnings", deny(warnings))] Then you can do a normal `cargo build` to get the normal warnings. But if you want to do a strict build, you can do `cargo build --features=fail-on-warnings`
What's the best way to work with physical quantities, with their dimensions and units? The candidates I have found are [`dimensioned`](https://github.com/paholg/dimensioned), [`uom`](https://github.com/iliekturtles/uom), [`units`](https://github.com/Boddlnagg/units), [`runits`](https://github.com/jesse99/runits), and [`simple_units`](https://github.com/willi-kappler/simple_units), though some of them are old and might have died off. It would also be great if it would let you parse a string like "(GeV/c^2)^-2" into the corresponding unit.
It just hit me in the shower that you could avoid relying on unstable internals (apart from the layout of `&amp;str` and `&amp;[u8]` being the same) by transmuting a pointer to the first element
/r/playrust
Doesn't compile? [http://pix.toile\-libre.org/upload/original/1528316453.png](http://pix.toile-libre.org/upload/original/1528316453.png)
Thanks; I'll give the no modules option a try. I've found zero info on how to configure the CRA dev server to serve .wasm as application/wasm, despite it being a ubiquitous tool.
I'd prefer not having to change anything to the source code to enable this. It looks like a hack to me.
Can they use [deps.rs](https://deps.rs)?
Yes, it uses HTTP2's multiplexing to allow many requests to be sent on the same connection while the responses or body streams are in various states. We use it for the high performance needs of the [Conduit](https://conduit.io) proxy.
The code works only for rust &gt;= 1.26, which version are you using? :)
A `match` expression over an enum is an "unwrap" in and of itself. What you're asking for is not possible since `T` is unconstrained, while `MyEnum` can hold just two specific types. There has to be some outcome in the case that `MyEnum` does not hold `T`. Usually either `None` or a panic. There is some support for dynamic typing through the [`Any`](https://doc.rust-lang.org/std/any/trait.Any.html) trait (`is::&lt;T&gt;()`, downcasts) but `T` must be `'static`, so it would not work with `&amp;'a str. In most cases it's best to simply handle all the cases at the use site: fn use_myenum(e: MyEnum) { match e { MyEnum::StringSlice(s) =&gt; do_stringy_thing(s), MyEnum::Integer(i) =&gt; do_inty_thing(i), } } There is the option of implementing `From&lt;MyEnum&gt; for Option&lt;i64&gt;` and `From&lt;MyEnum&gt; for Option&lt;&amp;str&gt;` as others have suggested, but type inference struggles with things like `let i: i64 = MyEnum::Integer(1).into().unwrap()` so it doesn't end up any more convenient. If you really want to "unwrap" in the `Option` sense then the quickest way is: let integer = MyEnum::Integer(5); println!("{}", if let MyEnum::Integer(i) = integer { i } else { panic!() }); but I've rarely found the need to do that.
Is this stable?
Thank you for working on TWiR !
If you're using the jemalloc allocator, current Linux, OSX default. Then you can get some info via the [jemalloc-ctl](https://github.com/sfackler/jemalloc-ctl) crate. Which exposes some of the extended [jemalloc](http://jemalloc.net/jemalloc.3.html) API. There's a ton of info there and it's hard to tell which parts are relevant sometimes. The crate seems to be missing the profiler API which can trace allocations and heap dumps, it should be possible to add that feature to the crate, but I haven't needed it in Rust yet. Allocation tracing is really handy for tracking memory leaks, especially with with the default random sampling, cause that's fast enough to run on production with only a few percent loss in performance. But it's hard to leak memory in safe rust, which is why I haven't needed it yet. Setting arena's can also be handy for stats, but I don't think that's exposed in the crate either. There's some higher level explanations in the [jemalloc wiki](https://github.com/jemalloc/jemalloc/wiki). For general performance analysis in Linux [Brendan Greggs Blog](http://www.brendangregg.com/) is probably the best source for info. I also gotten a lot of use out of [perf](http://www.brendangregg.com/perf.html) in particular. There's also [valgrind](http://valgrind.org/) which has some powerful tools, and I like [rr-project](https://rr-project.org/) to debug anything really gnarly. Though I mainly use valgrind or rr on C code, I've only done some basic testing against Rust. With allocations in particular some performance issues that I've run across are, excessive locking either by the allocator, or the OS as it modifies the page table (adding or removing virtual memory). I work on many core servers, and this is were performance can get horrible. The 100x lose was from C code with way too many threads (thousands) all doing heavy allocations. jemalloc in particular is very strong in heavily threaded environments due to a lot of lock avoidance in it's design. Memory fragmentation can waste a lot of memory for long running processes. One recent example I saw (on rails)[https://www.mikeperham.com/2018/04/25/taming-rails-memory-bloat/]. Showed a massive difference between jemalloc and libc's allocator. Rust does provide much better tools for avoiding allocations especially vs an interpreted language like rails. It's possible to fragment any allocator with the wrong allocation pattern. But it's very hard to accidentally produce a problematic pattern with the arena scheme jemalloc uses. It can take a while for fragmentation to show up. Note that the Linux system allocator is usually quite fast until it starts having to deal with fragmentation.
 The modified one could be any modification but I just removed inline | unmodifed | removed #[inline] ---|---|---- Average bench | 2131.14ns | 1743.69ns Standard deviation | 35.33ns | 21.16ns All runs for unmodified [2081, 2080, 2170, 2100, 2105, 2118, 2123, 2116, 2102, 2093, 2086, 2134, 2108, 2107, 2104, 2129, 2095, 2092, 2106, 2101, 2100, 2088, 2155, 2094, 2123, 2108, 2101, 2144, 2183, 2118, 2143, 2100, 2099, 2091, 2115, 2100, 2180, 2112, 2118, 2085, 2136, 2098, 2113, 2106, 2105, 2115, 2091, 2112, 2106, 2165, 2139, 2229, 2108, 2160, 2103, 2154, 2172, 2158, 2116, 2145, 2116, 2111, 2195, 2107, 2124, 2102, 2129, 2134, 2124, 2097, 2119, 2114, 2170, 2110, 2107, 2194, 2179, 2148, 2196, 2153, 2210, 2268, 2220, 2151, 2140, 2132, 2153, 2153, 2142, 2145, 2136, 2155, 2169, 2146, 2148, 2139, 2145, 2172, 2146, 2177] All runs for modified [1726, 1758, 1727, 1723, 1726, 1739, 1726, 1737, 1728, 1730, 1729, 1728, 1732, 1744, 1741, 1736, 1731, 1766, 1740, 1749, 1725, 1759, 1726, 1822, 1741, 1728, 1745, 1734, 1772, 1765, 1729, 1766, 1769, 1760, 1765, 1741, 1734, 1730, 1732, 1770, 1763, 1741, 1730, 1754, 1726, 1734, 1734, 1765, 1728, 1744, 1765, 1747, 1731, 1724, 1754, 1726, 1755, 1804, 1831, 1729, 1740, 1737, 1729, 1813, 1773, 1754, 1732, 1728, 1734, 1732, 1731, 1738, 1745, 1734, 1729, 1727, 1729, 1743, 1759, 1734, 1736, 1731, 1729, 1732, 1730, 1730, 1741, 1728, 1753, 1729, 1729, 1731, 1741, 1726, 1784, 1771, 1771, 1759, 1728, 1735] [Script used to do this](https://gist.github.com/JuanPotato/83c9d01010bcbe23cbc2714b0dc7cd5f) 
1.24.1. It is default version for latest Linux Mint (and Ubuntu).
We did move from "no specified markdown" to Commonmark. The previous one (hoedown) did support this extension, but it's not in the Commonmark spec yet.
Looks like htop
That determination has to be made for *all* trait objects for the same trait though; seems really hard to decide what optimizations to apply to such a fundamental trait as `Future`.
Ah, yeah you can use that to name a part of a pattern, [like so](https://play.rust-lang.org/?gist=85befd5e95829d96f898d78a9131a4de&amp;version=stable&amp;mode=debug).
Time to publish an update then, haha. 
If every implementation of that method would have *the exact same implementation* (which is the implication of returning the same Future type) why are you even dynamically dispatching it?
Since you are sitting on the super generic `pp` name - are you planning to support other formats as well? YAML and TOML don't need pretty printing, but maybe `pp` can detect (by filename or syntax, or if necessary by a flag) if it's input is XML or RON and prettify according to the format?
yep, it was htop. I had to change some of the settings (hit F2 to get there) to get it to use the tree view &amp; color the threads differently.
Fortunately, `libui` is now actively developed again and is improving rapidly. One reason I haven't put much thought into stabilizing `iui` is that I'm waiting until `libui` 1.0, which will only come once many more features are implemented.
Interestingly, this reveals a bug in the warn (unused_parens) lint. If the parens are removed, the parser confuses the while body for a block expression.
Yep, in type parlance it's a bottom type.
&gt; scalar 6266ms 1.0x &gt; scalar_par 2443ms 2.5x It doesn't have anything to do with SIMD since it's comparing two scalar implementations, but as /u/Verdonne said, it's probably because of [hyper threading](https://en.wikipedia.org/wiki/Hyper-threading): for each physical core on your machine, you have two logical ones. Then a dual core CPU can theoretical run up to 4 threads in parallel (I don't know the details, but in practice, it's not exactly as good as having 4 real cores).
**Hyper-threading** Hyper-threading (officially called Hyper-Threading Technology or HT Technology, and abbreviated as HTT or HT) is Intel's proprietary simultaneous multithreading (SMT) implementation used to improve parallelization of computations (doing multiple tasks at once) performed on x86 microprocessors. It first appeared in February 2002 on Xeon server processors and in November 2002 on Pentium 4 desktop CPUs. Later, Intel included this technology in Itanium, Atom, and Core 'i' Series CPUs, among others. For each processor core that is physically present, the operating system addresses two virtual (logical) cores and shares the workload between them when possible. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Interesting yours is consistent. I tried with a mac and cant recreate it. I did notice the actual binary seems different. But the llvm ir and assembly looks the exact same. if you run `cargo bench --verbose` theres a line 'Running `rustc --crate-name testing...' you can copy and paste that and get the command to compile, then just add on to the emit argument and check for your self.
This is great! I will be looking at the API and docs of the new 0.12 release. Thank you for making this awesome library!
&gt; * add polonius compare mode &gt; * make borrowck use polonius output What's polonius?
I've been looking over the vector and the scalar code. When looking over the vector and scalar code I'm not really seeing where the SIMD based code is being called. I've looked over the src and as far I can tell all of the vector code uses types with a "xN" postfix attached to them. However, I'm not seeing any of those called\used in the actual vector.rs file. I can see some feature flags in vector.rs, but I'm not quite sure how that leads to "xN" to be used. I'm sure I must be missing something, so if any could possibly better explain it to me I'd love to know because I'd love to be able to experiment with SIMD in some of my numerical codes I'm writing in Rust.
Looks like atomic\-ref2 is unsound. Consider what might happen when you load from \`AtomicOptionRef\`. First, the pointer is loaded from \`AtomicPtr\`. Then, another thread calls \`swap\` and exchanges it with a new \`Arc\`, thus dropping the old \`Arc\`. Finally, the fist thread calls \`Arc::from\_raw\` to construct an \`Arc\` from the dangling pointer (the one the other thread has destroyed). Building \`AtomicReference\` in a language without GC is a notoriously difficult problem, unfortunately.
I really dislike it when PRs have no information about the change whatsoever. It happens surprisingly often. I mean: if you worked on the code, probably for hours, how hard can it be to write a few sentences of explanation for your PR? :/ (I don't want to sound like I don't appreciate your work. I do, a lot! But yeah, recurring annoyance for me...)
ok..I'm sorry, you need rust 1.26 :/ you could install it via rustup if you want to! :)
&gt; For numbers divisible by 2 but not by 4, the largest that can be stored exactly as f32 is (224 − 1) × 2 = 33_554_430. That can be stored exactly because it is stored kind of like 16_777_215 &lt;&lt; 1. The algorithms used are Grisu3 and the dragon family of algorithm. Last time I checked they are used by Chrome, Firefox and the Julia programming language. We could argue that no implementation is perfect, it depends on what you value the most and what do you do with the resulting strings. But I thinks as the default in a standard library, this is perfect. I think we can really thank @lifthrasiir for this amazing work! 
It's a new version of the nll lifetime analysis. https://github.com/rust-lang-nursery/polonius
Isn't async/await used with futures? 
[removed]
The problem is how to sort out the naming collisions, because crates user == Github user.
Right, I can see that being a problem. 
A step-by-step transformation that might help you understand the code. Original code: fn main() { println!("{:?}", (|&amp;__@_:&amp;'_ _|-&gt;_{[(|(_,__,..):(_,_)|__..__)(__)]})({&amp;({||('"');}&lt;=*&amp;(),&amp;[..=..])})); } Rename some variables: fn main() { println!("{:?}", (|&amp;x@_:&amp;'_ _|-&gt;_{[(|(_,y,..):(_,_)|y..y)(x)]})({&amp;({||('"');}&lt;=*&amp;(),&amp;[..=..])})); } The pattern `(_, y, ..)` is equivalent to `(_, y)` when applied to two-element tuples. fn main() { println!("{:?}", (|&amp;x@_:&amp;'_ _|-&gt;_{[(|(_,y):(_,_)|y..y)(x)]})({&amp;({||('"');}&lt;=*&amp;(),&amp;[..=..])})); } `@` is a pattern binding, which is useless in this case. Also, the lifetime annotation is useless, so eliminate that. The `-&gt; _` is a closure return type annotation which is also useless and eliminated. fn main() { println!("{:?}", (|&amp;x:&amp;_| {[(|(_,y):(_,_)|y..y)(x)]})({&amp;({||('"');}&lt;=*&amp;(),&amp;[..=..])})); } `{||('"');}` is a block that computes a closure, throws it away, and returns `()`. fn main() { println!("{:?}", (|&amp;x:&amp;_| {[(|(_,y):(_,_)|y..y)(x)]})({&amp;(()&lt;=*&amp;(),&amp;[..=..])})); } Eliminate the redundant `*&amp;` fn main() { println!("{:?}", (|&amp;x:&amp;_| {[(|(_,y):(_,_)|y..y)(x)]})({&amp;(()&lt;=(),&amp;[..=..])})); } For some reason, `() &lt;= ()` is a less-than-or-equal comparison which returns true. fn main() { println!("{:?}", (|&amp;x:&amp;_| {[(|(_,y):(_,_)|y..y)(x)]})({&amp;(true,&amp;[..=..])})); } Remove some redundant braces fn main() { println!("{:?}", (|&amp;x:&amp;_| [(|(_,y):(_,_)|y..y)(x)])(&amp;(true,&amp;[..=..]))); } Reformat so it's easier to read fn main() { println!("{:?}", ( |&amp;x:&amp;_| [(|(_,y):(_,_)|y..y)(x)] )( &amp;(true,&amp;[..=..]) ) ); } Transform the fourth line so it's easier to read. (I'm not sure how to explain this step in simpler terms.) fn main() { println!("{:?}", ( |&amp;x:&amp;(_, _)| [x.1 .. x.1] )( &amp;(true,&amp;[..=..]) ) ); } When calling the closure, the two occurrences of `x.1`become `[..=..]`. fn main() { println!("{:?}", [[..=..] .. [..=..]] ); } Note: `..=..` is a half-infinite inclusive range with the type `RangeToInclusive&lt;RangeFull&gt;`. The range of the printed expression is `[Range&lt;[RangeToInclusive&lt;RangeFull&gt;; 1]&gt;; 1]`
https://www.reddit.com/r/rust/comments/8p013f/rusty_riddle_what_does_it_print/e08irvu/
Yes apparently inspired by https://github.com/kusti8/proton-native which uses `libui` under the hood.
[GitLab Ultimate and Gold now free for education and open source](https://about.gitlab.com/2018/06/05/gitlab-ultimate-and-gold-free-for-education-and-open-source/)
Another meetup I'm out of town for :(
Honestly, that stuff should go in the commit message.
&gt; - Allow **all** traits to be turned into trait objects, &gt; Restrict which associated items can be accessed on trait objects. That’s how it used to work, at some point not long before 1.0, but [it was changed](https://github.com/nox/rust-rfcs/blob/master/text/0255-object-safety.md) when dynamically sized types were introduced. The reasoning was that it’s useful for the bound `(dyn Trait): Trait` to hold, so that you can use trait objects with functions that are generic on `T: Trait`… but that only works if `dyn Trait` provides access to the entire API of `Trait`, not just a subset of it. I suppose that adding implicit `where Self: Sized` bounds where necessary (as boats suggested) could work, but it might be confusing. I’d prefer an approach where as many language features as possible are made object-safe – including associated types as proposed here, a subset of generics, and more – so there’d rarely be a need to make the distinction. That’s not easy to implement, though.
The problem with commit messages in Rust is that, while I love bors and think it should be how CI works for all projects, it does create a lot of noise. Like 2 thirds of the commits on rust-lang/rust are “bors merged ...”
ppnj Pretty printin' JSON. Plus: Kinda sounds like PB&amp;J. It's bad, but it's short, sweet, and funny.
I gave it a shot: https://gist.github.com/Florob/ed756ba944b8b094d079d594322ad2c6 Though /u/noop_noob also did a good job in the sibling.
It looks like you have old docs. The correct ones are here (presuming you're using the latest version of `linxal`): https://docs.rs/linxal/0.6.0/linxal/svd/general/trait.SVD.html#method.compute It's so common that projects forget to update the self-hosted docs that going to docs.rs is a good idea in "am I taking crazy pills" situations like this. 
Alright, will check it out
Is there a specific reason why Bors doesn't group the commit messages from the PR and set the commit author explicitly to the contributor(s)?
I like this a lot.
I'm excited to see new work in this space. Robigalia's vapor is just about done dissipating.
I'd call it a bug in the formatter. (It's working as intended, but the intended behavior is undesirable in cases like this.) For digits after the decimal point, it makes sense to truncate once you've output a value that will round-trip back to float correctly, so e.g. it can print "0.3" instead of "0.299999999999999989". But truncating *before* the decimal point never makes the number any shorter, and (as in this case) produces a surprising value for some floats that represent exact integer values. Probably better would be to switch to scientific notation for these cases, which would make it clear that precision might have been lost. "1.1474043e+09" wouldn't have been as confusing. 
The difference is that `no_std` exists, and almost everything that’s in `std` but not `core` is possible to reimplement yourself, without having to depend on any particular runtime. One exception is the ability to move out of `Box`, which can’t be replicated in user-defined types, but that’s considered a flaw which will hopefully be rectified with some form of `DerefMove`. Another is `box` syntax, but in theory the final, stabilizable version of that will have to at least provide a feasible path to support custom allocators in the future. The same, I think, should apply to the auto-boxing you’re proposing. Shipping with `Box` only is okay, but there should be some sort of general plan for how it can be made compatible with custom allocators in the future (including, notably, allocators that care about their `self` pointer rather than being global). On the other hand… I don't think it would be that hard to support stack allocated returns at all. You "just" need to stick the required size in some slot in the vtable (as well as the vtable of the return type). Given a normal call: trait Trait { type Foo; fn bar(&amp;self, baz: Baz) -&gt; Self::Foo; } fn f(my_trait_object: &amp;Trait&lt;Foo=SomeFoo&gt;) { let foo: SomeObject = my_trait_object.bar(baz); } The existing 'desugaring' for the method call (if you consider ABI behavior) is something like: let foo: SomeFoo = uninitialized(); (my_trait_object.vtable.bar)(&amp;foo, my_trait_object, baz); However, if the associated type were not known, it could instead become: let foo_size = my_trait_object.vtable.foo_size; let foo_vtable = my_trait_object.vtable.foo_vtable; let foo_storage = [0u8; foo_size]; // i.e. alloca (my_trait_object.vtable.bar)(foo_storage.as_ptr(), my_trait_object, baz); let foo: &amp;FooTrait = transmute(TraitObject { data: foo_storage.as_ptr(), vtable: foo_vtable }); Unless I'm missing something, this doesn't seem significantly harder to implement than the implicit Boxing that you proposed.
An async fn returns an `impl Future + 'ret`, where `'ret` is the union of all input lifetimes. `Box&lt;dyn Future + 'ret&gt;` implements that bound, but `&amp;'a mut dyn Future +'ret` does not (even if you pin it).
I'm having trouble understanding something about dereferences. I was trying to do one of the sample excercises at the end of Chapter 8 of the tutorial book hosted on the website. The one about finding the mean, median, and mode of a list of integers. I got it working, but I'm a bit stumped about one part. fn mode(list: &amp;Vec&lt;i32&gt;) -&gt; i32 { let mut map = HashMap::new(); for i in list { let count = map.entry(i).or_insert(0); *count += 1; } let mut max_value: i32 = 0; let mut max_key: i32 = 0; for (key, value) in &amp;map { if value &gt;= &amp;max_value { max_value = *value; max_key = **key; } } max_key } Here's the function I wrote to find the mode. It borrows a Vec\&lt;i32\&gt; and creates a HashMap that contains the number of times each number appears in the list. Where I'm confused is why I need to dereference value once and key twice in order to save them to the max\_value and max\_key variables to later return. Am I missing something obvious here? Is there a better way to do this?
This is the entire diff, which leads me to think its something with memory and alignment, time to learn how to use perf. 74,76c74,76 &lt; @panic_bounds_check_loc.15 = private unnamed_addr constant { { [0 x i8]*, i64 }, i32, i32 } { { [0 x i8]*, i64 } { [0 x i8]* bitcast ([11 x i8]* @str.12 to [0 x i8]*), i64 11 }, i32 219, i32 13 }, align 8 &lt; @panic_bounds_check_loc.16 = private unnamed_addr constant { { [0 x i8]*, i64 }, i32, i32 } { { [0 x i8]*, i64 } { [0 x i8]* bitcast ([11 x i8]* @str.12 to [0 x i8]*), i64 11 }, i32 230, i32 8 }, align 8 &lt; @panic_bounds_check_loc.17 = private unnamed_addr constant { { [0 x i8]*, i64 }, i32, i32 } { { [0 x i8]*, i64 } { [0 x i8]* bitcast ([11 x i8]* @str.12 to [0 x i8]*), i64 11 }, i32 231, i32 9 }, align 8 --- &gt; @panic_bounds_check_loc.15 = private unnamed_addr constant { { [0 x i8]*, i64 }, i32, i32 } { { [0 x i8]*, i64 } { [0 x i8]* bitcast ([11 x i8]* @str.12 to [0 x i8]*), i64 11 }, i32 220, i32 13 }, align 8 &gt; @panic_bounds_check_loc.16 = private unnamed_addr constant { { [0 x i8]*, i64 }, i32, i32 } { { [0 x i8]*, i64 } { [0 x i8]* bitcast ([11 x i8]* @str.12 to [0 x i8]*), i64 11 }, i32 231, i32 8 }, align 8 &gt; @panic_bounds_check_loc.17 = private unnamed_addr constant { { [0 x i8]*, i64 }, i32, i32 } { { [0 x i8]*, i64 } { [0 x i8]* bitcast ([11 x i8]* @str.12 to [0 x i8]*), i64 11 }, i32 232, i32 9 }, align 8 10847,10848c10847,10848 &lt; ; Function Attrs: uwtable &lt; define internal fastcc void @_ZN7testing10get_bounds17h62cdf28873d0c3a1E(%"core::option::Option&lt;(&amp;str, &amp;str)&gt;"* noalias nocapture dereferenceable(32), [0 x i8]* noalias nonnull readonly %string.0, i64 %string.1) unnamed_addr #1 personality i32 (i32, i32, i64, %"unwind::libunwind::_Unwind_Exception"*, %"unwind::libunwind::_Unwind_Context"*)* @rust_eh_personality { --- &gt; ; Function Attrs: inlinehint uwtable &gt; define internal fastcc void @_ZN7testing10get_bounds17h62cdf28873d0c3a1E(%"core::option::Option&lt;(&amp;str, &amp;str)&gt;"* noalias nocapture dereferenceable(32), [0 x i8]* noalias nonnull readonly %string.0, i64 %string.1) unnamed_addr #6 personality i32 (i32, i32, i64, %"unwind::libunwind::_Unwind_Exception"*, %"unwind::libunwind::_Unwind_Context"*)* @rust_eh_personality { 
I'm having a bit of trouble on how to rectify this error * error\[E0495\]: cannot infer an appropriate lifetime for lifetime parameter in function call due to conflicting requirements I'm trying to work on implementing a quad tree for some object collision. How I generally envision this working is the top level struct containing a vec (maybe a set) of all the item currently in the try. I was thinking the tree nodes would then hold references for these items so I could do quicker lookups on what a given item might collide with. The QuadTree itself, B here is a generic bounding box with an associated item type T pub struct QuadTree&lt;'a, B: 'a&gt; where B: Partition { items: Vec&lt;Box&lt;B::T&gt;&gt;, root: Box&lt;QuadNode&lt;'a, B&gt;&gt; } The insert method fn insert(&amp;mut self, item: B::T) -&gt; Result&lt;()&gt; { if !self.includes(&amp;item) { return Err(QuadError::OutOfBounds) } self.items.push(Box::new(item)); let boxed = &amp;self.items[self.items.len() - 1]; match self.root.insert(boxed) { Ok(_) =&gt; { Ok(()) } Err(err) =&gt; Err(err) } } My real question here is I want to first place the item into a box. I then want to get a reference to this new box and insert that into the root of the quadtree so I can later do lookups. My understanding right now, is that boxed goes out of scope of the insert method, but the QuadTree has a lifetime of 'a so there is conflicting lifetimes. Also any other suggestions would be great. Still very new to rust
That only reports out of date dependencies. [Dependabot](https://dependabot.com/rust.html) will attempt to update them. On a given schedule, it will - Update your `Cargo.lock` if present - Attempt to update the deps in your `Cargo.toml` I've been using it on 5+ crates for a month or more and been finding it useful.
&gt; There's a knob to turn in the TravisCI repository settings TIL
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/actix] [TechEmpower round 16](https://www.reddit.com/r/actix/comments/8p6g57/techempower_round_16/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
This is a great explanation; thanks!
This is the spiritual successor to [`assert_cli`](https://github.com/assert-rs/assert_cli). We plan to move `assert_cli` to being higher level, see [Issue 41](https://github.com/assert-rs/assert_cli/issues/41). Next up is making it easier to setup file system fixtures and write assertions on the result of programs.
Congratulations Actix! In case anyone was wondering, the builds published for round 16 don't include the hyper 0.12 update, which was merged 4 days later on the FrameworkBenchmarks repo. The next CI build should have those results.
…Oh, you want to be able to move it. I'm not sure why I didn't think of that. And now I see [this discussion](https://www.reddit.com/r/rust/comments/8opgfq/async_methods_ii_object_safety/e05rqxm/) elsewhere in the thread, which seems to sum things up well. Hmm… well, that's obviously a longer-term project at best. Though, suppose that at some point in the future, the full unsized rvalues RFC was implemented, and then we hypothetically found a way to support unsized return values for all functions. Would we then say that ideally `async fn`s *should* return `impl Future + ?Sized`? Is there anything that can be done to make the current design forwards compatible with that, or does it pretty much have to be a breaking change of some sort (or not done at all)? On the other hand, after thinking about it, it seems to me that while implementing unsized return values would be possible, it would probably be a bad idea. This is because it would inherently require memcpying the value from one location on the stack to another. For one thing, the source would have to be a dead stack frame, which of course presents significant implementation difficulties and would call the idea into question by itself – but even ignoring that, the performance cost would be problematic. There's no way to do it with zero copies other than having the caller guess the required size, which would have all sorts of problems. And while memcpy's performance would probably be fine for most objects, it "doesn't scale" to objects that happen to be large, i.e. store a lot of data inline, something that would probably surprise users. Probably it would be better to just allocate and, in the future, take advantage of custom allocators to provide more efficient allocation mechanisms. …Though, today, passing large objects by value already requires a memcpy (although this is pretty suboptimal), so would it be *that* surprising? In any case, for now, I do think it makes sense to consider how custom allocators could be accommodated in your design.
What type do you expect `stripOut` to return? Rust needs a static type for every value, including values returned from functions. Could you elaborate on what exactly you expect this type to be? I'm asking because I don't see a reasonable return type for your `stripOut` function. Usually, instead of providing some generic "get something out" method, you'll provide methods for retrieving each specific type, like this: enum myEnum&lt;'a&gt; { stringSlice(&amp;'a str), integer(i64), } impl&lt;'a&gt; myEnum&lt;'a&gt; { fn as_str(&amp;self) -&gt; Option&lt;&amp;str&gt; { match self { myEnum::stringSlice(s) =&gt; Some(s), _ =&gt; None, } } fn as_i64(&amp;self) -&gt; Option&lt;i64&gt; { match self { &amp;myEnum::integer(i) =&gt; Some(i), _ =&gt; None, } } fn display(&amp;self) -&gt; &amp;std::fmt::Display { match self { myEnum::stringSlice(s) =&gt; s, myEnum::integer(i) =&gt; i, } } } fn main() { println!("{}", myEnum::integer(5).display()); println!("{}", myEnum::integer(5).as_i64().expect("we know this is an int")); println!("{}", myEnum::stringSlice("hi").as_str().expect("we know this is a string slice")); } https://play.rust-lang.org/?gist=33e32ca6284c76effdb36fa49723f8cc&amp;version=stable&amp;mode=debug
I don't have the documentation in front of me but under std::fmt there's pretty printing formatting, including scientific notation for floats. 
With infinite time, I really wanted to contribute to Robigalia. Now I can apply that desire to this :)
This works for me.
I'd expect future compiler optimizations to mess with layout of any stuff that isn't #\[repr(C)\]. Like the current optimization of Option\&lt;Box\&lt;T\&gt;\&gt; being just a pointer.
Very impressive results! Hopefully Actix will become #1 in all categories in the future! :-)
There are definitely things we will never change and the fact that the first word of a slice type is a pointer to the slice data is one of them.
You can't do this. Rust doesn't let you have cyclic- or self-references like that. You *have* to remove the references. You can do this by replacing references with opaque IDs (such as indices into the `items` array), or by using `Rc&lt;RefCell&lt;B::T&gt;&gt;` (assuming you need to be able to mutate the things stored in the tree; drop the `RefCell` if not) in both `items` and `QuadNode`. IDs are probably a bit cleaner, assuming you don't need to worry about removing things from the tree; otherwise `Rc&lt;_&gt;` will be simpler to manage.
`list` is `&amp;Vec&lt;i32&gt;`. To iterate over it, `for` has to pass it through `IntoIterator`. The applicable `impl` for this, from [the `Vec` documentation](https://doc.rust-lang.org/std/vec/struct.Vec.html) is: impl&lt;'a, T&gt; IntoIterator for &amp;'a Vec&lt;T&gt; type Item = &amp;'a T Thus, the type of `i` is `&amp;i32`. [`HashMap::entry`](https://doc.rust-lang.org/std/collections/struct.HashMap.html#method.entry) takes the key by value. `or_insert` then takes the value by-value (which is `0`). This means `map` is of type `HashMap&lt;&amp;i32, i32&gt;`. Again, iterating over `&amp;map` means going through `IntoIterator`. The applicable `impl` for this, from [the `HashMap` documentation](https://doc.rust-lang.org/std/collections/struct.HashMap.html) is: impl&lt;'a, K, V, S&gt; IntoIterator for &amp;'a HashMap&lt;K, V, S&gt; type Item = (&amp;'a K, &amp;'a V) (You can ignore the `S` parameter; it's for overriding the default hasher, which you aren't doing.) Substituting for `K` and `V`, means that the iterator produces `(&amp;&amp;i32, &amp;i32)`s, which means `key` is `&amp;&amp;i32` and `value` is `&amp;i32`. You deref `&amp;&amp;i32` twice to get `i32`, you deref `&amp;i32` once to get `i32`, which is what you need in order to store the values in `max_value` and `max_key`.
You can put extra bounds on `Self` on functions, and if the implementing type doesn't match those bounds, it doesn't have to implement that function. It also means that using `T: Fail` without a `where T: Sized` would not allow calling the method either. The specific `where Self: Sized` is extremely useful because it lets a trait by object safe by having the non-object safe methods just not applicable for trait objects.
An issue was filed on this: https://github.com/rust-lang/rust/issues/51402 An and older issue I filed on similar: https://github.com/rust-lang/rust/issues/45251 In fact, I think this is actually just a bug in the confusing behaviour that for trait objects, they effectively get inherent methods with the same name as the trait methods.
Soon we'll start hearing about the C7M problem
fwiw, if you're looking for projects that have done married pretty with compact, I've used [this pretty-compact library](https://www.npmjs.com/package/json-stringify-pretty-compact) previously and thought it did pretty much exactly what I wanted.
Actually wait, no. That issue that was filed is just the same issue I already filed. There was no issue filed on this specific instance. I would expect that the trait object wouldn't have the psuedo-inherent method for the Self: Sized bounded method, but it seems to still do so.
We (TechEmpower) really appreciate the interest and excitement from the Rust community. Thanks to everyone here who has made performance a \*big deal\*. It has yielded very impressive results in our synthetic tests and I believe very strongly that means real\-world applications will be the real beneficiaries of that effort by Rust's champions. Please also don't miss the [blog entry I wrote up about Round 16](https://www.techempower.com/blog/2018/06/06/framework-benchmarks-round-16/).
What is this problem, google didn’t helped me to find answers by myself 
I think it's a combined reference to the C10M problem + the numbers at the top of the chart in the link.
I made it weirder still: ``` fn main() { println!(r#"{:X?}"#, __=(|&amp;__@_:&amp;'_ _,|-&gt;_{[(|(..,_,__,_,):(_,_,((),),)|__..__)(__)]})({&amp;(!(({"\"__\\\\\ \'";(||{('\"');()})();}&gt;=*&amp;())|(|__|__&amp;__)((()&lt;())|(()&gt;()))),&amp;[..=..],({-0__.%0.;},),)})); } ``` This is fun!
Oh sorry it was an obscure reference to the [C10k problem](https://en.m.wikipedia.org/wiki/C10k_problem]. The top 10 or so systems are very close too each other at 7 million responses. It looks that no one can go faster than that, implying a limitation with the current architecture (hardware/networking/OS/...) that application-level frameworks will be unable to address
Ok. Is this kind of dereferencing something that happens often in Rust code or is there a better way to accomplish what I'm trying to do here?
Bummer!
It's not out of the ordinary or anything. Rust is a language with pointers, and sometimes you have to dereference them explicitly. There are other ways you could do it (`.entry(*i)` so you're not storing a double-pointer, which is usually just inefficient, `(&amp;key, &amp;value)` to deref in the `for` pattern match), but you're still having to remove the indirections.
Should techempower also provide CPU, I/O stats during the test to confirm that the limitation is no more at the SW side but at the HW/kernel ? 
Ok thank you for your help.
Why is the parent of inner thread 2 not inner thread 1? Does thread::spawn implicitly switch the parent or something? After all, inner thread 1 does run the code to spawn inner thread 2.
Logically this makes sense. I didn't consider that. Could you help me come up with a test to prove this? I tried playing around with AtomicOptionRef and I wasn't able to break it. I'm probably not understanding something here though.
That's the first time I hear of a rust feature that results in loss of precision and isn't marked unsafe.
One more pass: fn main() { println!(r#"{:X?}{}"#, __=(|&amp;__@_:&amp;'_ _,|-&gt;_{[(|(..,_,__,_,):(_,_,((),),)|__..__)(__)]})({&amp;(!(({"\"__\\\\\ \'";(||{('\"');()})();}&gt;=*&amp;())|(|__|__||__|__)((()&lt;())&amp;(()&gt;()))),&amp;[..=..],({-0__.// %0.;(|_:[();0]|{})([]);},),)}),_=""); }
&gt; networking This is likely to be part of the problem: the blog post linked above points out that these are getting pretty close to the 10-gig ethernet connection the server apparently has. The plaintext response is ~140 bytes (1120 bits), meaning the maximum responses per second is something like 10e9/1120 &amp;approx; 8.9 million. There's still some space to improve, but not much!
The title could be more precise: it's a frontend for PostgreSQL only. 
Theoretical throughout is about 7.5, there is ticket discussion https://github.com/TechEmpower/FrameworkBenchmarks/issues/3538
That was really quick turnaround! Thanks very much. The new version appears to about 1.5x slower on my testcase, but as it's still blazingly fast I don't care. Any chance of getting a sorted flag in case I want dictionary keys in sorted order? Can't hurt to ask. The suggestion to have a compact flag to try to fit objects that fit on one line into one line was a good one, I think. I know it's a lot of work. It's your program and nobody can tell you what to name it, but several of us have independently wished for a name that describes what it does. Just sayin'. :-) :-) This program is now my goto for prettyprinting JSON files. Thanks huge for writing it.
I've seen people linking to "test runs" identified by a GUID. Where do those come from?
I would not exactly call it rapidly because the project does not seem to be managed very well. Lots of interest, lots of PRs, but the maintainer seems to want to do everything himself so he is getting swamped. A real shame.
Ah, a *web* backend. :-) There are lots of alternatives to both Diesel and Rocket. Actix and Gotham seem popular as Rocket alternatives, but there's a dozen others. It's a very popular space right now. If you wanted to share a little about your use case, I suspect someone could suggest some other potential add-ons.
Well, what kind of service are you trying to write? You mentioned libraries for accessing sql databases and a framework for writing HTTP servers. Do you also need json output? Do you need to send emails? Do you need websockets? How about logging?
This is genius! &gt; (|__|__|__) 
Shame there isn't a LoC stat too, I suspect it does well there too!
According to my understanding, it seems that plaintext max concurrency is 4096. Data table shows that peak results is achieved (with actix) at lower concurrency. It should therefore not be a C7m issue (c for concurrency) but more a maximum platform/test capacity
AFAIK std::simd will be in Rust 1.27
Very nice! I am just wondering what exactly is actix-raw (in contrast to actix-(web))?
Really interesting results. However, when looking to data table results , it looks competitors have different results in the high concurrency column (for plaintext). In the low concurrency you are equivalent but ulib makes 40% more in the 16384 bench 
Tooltip says "raw database connectivity". My guess would be that it doesn't use any existing Rust database adapters (which are all blocking atm, and are the bottleneck) and instead rolls its own thing.
This sounds reasonable, but in the case of Plaintext? 
ELI5 how come `tokio`'s event loop doesn't 100% the CPU if it's polling? If it sleeps after a round of polls, how does it decide how long to sleep? Also, I thought the whole point of async constructs is to do computations after some kind of an event (a-la interrupts, at least as far as I understand them) and polling essentially goes against that?
**Disclaimer**: I'm not super familiar with tokio's internals. This is based off what I do know and implementing my own event loop. &gt; [..], how does it decide how long to sleep? It doesn't. It just tells whatever underlying API it's using to wake it up when something happens. At *that* point, it figures out which task that "something" happened to, then polls just that task. The exception would be where it has a deadline/wake-up timer set, in which case it will ask the underlying API to wake it up when something happens *or* after a limited amount of time has elapsed.
It looks like [the code](https://github.com/TechEmpower/FrameworkBenchmarks/tree/master/frameworks/Rust/actix/src) uses the full actix\-web pipeline for the actix version, but directly writes byte strings into a buffer for actix\-raw.
[C10M was formulated back in 2013](http://c10m.robertgraham.com/p/manifesto.html): &gt; Today (2013), $1200 will buy you a computer with 8 cores, 64 gigabytes of RAM, 10-gbps Ethernet, and a solid state drive. Such systems should be able to handle: &gt; - 10 million concurrent connections &gt; - 10 gigabits/second &gt; - 10 million packets/second &gt; - 10 microsecond latency &gt; - 10 microsecond jitter &gt; - 1 million connections/second &gt; Such systems already exist. We call them "hardware" or "devices". But in recent years, devices have become just software running on general purpose computers. Rip open a network device and you might find a personal computer inside. The limits of scalability aren't our hardware, but the software, specifically, the operating system.
Thanks! &gt; It just tells whatever underlying API it's using to wake it up when something happens. Do you happen to have some links to read about this bit?
Really interesting results. However, when looking to data table results , it looks competitors have different results in the high concurrency column (for plaintext). In the low concurrency you are equivalent but ulib makes 40% more in the 16384 bench 
I think you weren't able to break it because it leaks memory - `T`s (as in `Arc&lt;T&gt;`) are never destroyed so it's always safe to access them. :) use atomic_ref2::AtomicOptionRef; use std::sync::Arc; use std::sync::atomic::{AtomicUsize, Ordering}; static DROPS: AtomicUsize = AtomicUsize::new(0); struct Foo; impl Drop for Foo { fn drop(&amp;mut self) { DROPS.fetch_add(1, Ordering::SeqCst); } } fn main() { let a = AtomicOptionRef::&lt;Foo&gt;::new(); a.swap(Arc::new(Foo)); a.swap(Arc::new(Foo)); assert_eq!(DROPS.load(Ordering::SeqCst), 1); }
&gt; Transform the fourth line so it's easier to read. (I'm not sure how to explain this step in simpler terms.) I believe that's a kind of [beta reduction](https://wiki.haskell.org/Beta_reduction).
The property of being warning-free is a property of the source code. Therefore, I think the property of failing to compile if there are any warnings should also be a property of the source code.
If someone else doesn't, you'll have to dig into the source code.
Note that the operator is just 2 dots, which may be confusing for some.
This is not for PosgreSQL only, work for mysql is underway and right now you can use this for opening sqlite database too.
/u/epage has been doing tremendous work on these crates -- in regards to the implementation as well as project management! It was so cool to see Ed, Aaron, and a few other people [crank out](https://github.com/assert-rs/predicates-rs/pulls?q=is%3Apr+is%3Aclosed) PRs for predicates-rs at the impl days after RustFest Paris! I'm very excited to see the assert_cli group of crates moving into the direction of 1.0 quite quickly :)
I was not aware of `jq`. It looks amazing! I'd like to use a metaphor to compare `jq` to my program: If `jq` were a (real) tool, it would be a swiss army knife: a battle-tested multi-tool with an insane amount of features. My program on the contrary would be a biface: dead simple, produced in less than a day, better than nothing.
Hi @sidpshanker. I would say this is possibly the most efficient way as you never copy data; or at least that is the objective. An example I've done recently is some trade object that needed some market data retrieval via a https service and then some calculations. I just split that up into 3 different data structures, which follows Rust's composition model. In pseudo code * Read trade data and identify what market data you need * Request and receive a MarketData object * Populate mut HashMap with &lt;Request,MarketData&gt; * Attach &amp;MarketData references to a mut TradeResults * Repeat until you have all the market data * Return a TradeResults object that just has references to entries in the HashMap In this example the original trade object in untouched, and there's no copying of data; there is one instance of data referenced from whatever needs it. Something along those lines
Wrote a [small compiler plugin](https://bitbucket.org/TechPriest/wstr_plugin/src/master/) (yeah, Nightly) for Windows\-style widestring literal generation (obviously) in compile time. [This article](https://llogiq.github.io/2016/05/17/flamer.html) was super\-helpful during AST generation research.
You convinced me to rename my tool, as the name pp should be used for something more sophisticated and general. The tool is now called `jsonpp`. I think this name describes perfectly what it does and is not too long. I renamed both the crate and the GitHub repo: https://github.com/flo-l/jsonpp-rs https://crates.io/crates/jsonpp 
Some benchmarks would be nice, maybe with small, medium and large files (~1KB, ~1MB and ~1GB or so). I'd be happy to accept a PR if you want to join in ;) Sorting would require the tool to load the whole JSON file into memory first. But nonetheless its useful, maybe I'll implement it. Compacting should be doable too. I renamed it, see my comment on the thread directly.
Does it? I think `&amp;Future` just gives you the address of the object, but `Box&lt;Future&gt;` would always allocate it dynamically on the heap, and maybe we could have `SmallBox&lt;Future, 24&gt;` that stores objects with `size_of::&lt;T&gt;() &lt;= 24` inside.
I think `std::arch` will be in Rust 1.27, I haven't heard anything about `std::simd`.
Hmm... This makes me wonder if Redox could find production usage in the IoT space in the not too distant future.
\&gt; However, I'm not seeing any of those called\\used in the actual vector.rs file. The explicitly vectorized ambient occlusion algorithm is called here (\`ambient\_occlusion::vector\`): [https://github.com/gnzlbg/aobench/blob/master/src/vector.rs#L41](https://github.com/gnzlbg/aobench/blob/master/src/vector.rs#L41) \&gt; I've looked over the src and as far I can tell all of the vector code uses types with a "xN" postfix attached to them. The code of ambient\_occlusion::vector\` looks pretty much like the scalar code (\`ambient\_occlusion::scalar\`) except that, as you note, it uses different types suffixed with \`xN\`: [https://github.com/gnzlbg/aobench/blob/master/src/ambient\_occlusion.rs#L49](https://github.com/gnzlbg/aobench/blob/master/src/ambient_occlusion.rs#L49) All the methods of the \`xN\` types are explicitly vectorized, so the code looks like the scalar code, but it operates on \`N\` values at a time, instead of just 1. 
Could you be more specific and provide examples? For instance, what does it look like now and how should it look in your opinion?
Just curious.. how does it compare to the ispc implementation?
actix seems to be the best choice atm.
Tokio is based on [mio](https://github.com/carllerche/mio) that abstracts over the necessary operating system APIs. You might want to check out [this talk](https://www.youtube.com/watch?v=CjQjEMw-snk).
I mean, I've never used actix so I could be wrong, but I poked around the docs &amp; saw in the root level there were 9 modules exposed. \`registry\`, for example, contains 2 traits and 2 structs regarding the registry. This is great for development, but when I'm actually using some library, I don't care what the structure of the project is! I just know 'okay, actix has this 'registry' thing, and I need this trait right now'. What normally happens, is even if I know the name of the trait, I have to ooopen the docs, fiiiind the trait, aaaadd the use for that module or trait, THEN I can add it to my program. This is like 20 seconds of work that could have been 0! Yeah, there's implicit documentation in modules, but this only helps new users, and new users don't stay new for long. Whenever I use some C library, I find that very quickly I start to remember the function names &amp; signatures, such that when I want to use the library I don't even have to look at the docs, just remember the concepts. Whenever I use some Rust crate, I am constantly looking at the docs for stuff I use very commonly! As a concrete example from a framework I have used, `iron` has objects for errors, requests, and responses. `iron` is a web framework - each endpoint you define takes a Request, and returns a `Result` containing a `Response` and an `IronError`. You'd think I'd be able to write these pretty well - nope! Every new endpoint I defined was at least 60 seconds of searching through docs to find the right modules, or whether what I needed was exported in the prelude. In terms of namespacing, I refuse to believe your crate is SO huge that you actually need internal namespacing. Look at the OpenGL C API, one of the most bloated APIs out there - they're pretty fine with just the 'gl' namespace. This is a half rant &amp; half question on why this is needed, I feel like a lot of rust right now is focused on making everything as neat as possible, rather than thinking about the useability of stuff for getting shit done. Again, never used Actix though, just seen a few posts about it recently &amp; decided to have a look around
&gt; Diwata is a database interface for PostgreSQL First sentence in the README on github 😉
From memory the PR message is what ends up being used when it gets merged into master. I wrote a PR message that was out of date by the time my PR had got through code review but didn't realise so quite it ended up being an inaccurate message.
Put in the README that you're willing to transfer it to whoever asks, then push a new version. Now wait. I am willing to hold crates like this, if you want to transfer it to me and be done with it.
I feel like this may just be a culture thing; you come from a language with no module system, so of course you'd be used to not having one! That said, there is always a design question about the right way to organize things, and that doesn't mean that every crate is right about it. One good reason why this is needed: the module is the privacy boundary in Rust. This matters not just for internal organization, but for unsafe code: technically speaking, any module that uses `unsafe` internally has to be audited in full to make sure that the safe code upholds invariants too. So people try to keep modules that have `unsafe` small. *that* said, re-exporting can make it appear to the user like there's not smaller modules, so it's not necessarily a good reason to expose many small modules in an API.
Did you try `use iron::prelude::*;`? That seems to be addressing your needs.
Yeah I'm way in favour of many small modules for development, but exposing via `pub mod` in the root rather than just `pub use` and keeping the module structure hidden results in a pretty lousy experience (see rant)
We should make a crate of a webapp to manage abandoned crate names. Then rename that webapp and put its old crate name in its database so that it could be self contained.
Yeah, and prelude is great, except when there's something that's not in the prelude - and I'm no superhuman, so I can never remember what's in or not in the prelude! If the prelude just exported everything, that would be great (except you might as well just export everything in the root crate at that point)
Failure is a strict improvement over `std::Error`. There are no drawbacks to using it. If you are writing a library, you can just use it to generate boilerplate for you. You can use `#[derive(Fail)]` to autogenerate your custom error types. They will be fully compatible with `std::Error`. If you are writing an application, you could find the `Error` type useful. It allows you to combine arbitrary errors from the different libraries you are using (as well as your own errors), for easy logging, etc. I don't see why anyone would want to resist using `failure` for new Rust projects.
What are the (hypothetical) repercussions of transferring a name for people who depended on the old package? Would the new crate that takes the name just bump to a new major version?
I feel less strongly that the root is always better. It entirely depends on size. Do what makes sense, IMHO.
It's less of a problem with bors, and more that this is how merging works by default in git. GitHub has some options to prevent these merge commits from appearing, e.g. "Squash and Merge" (which does a fast\-forward merge so that no merge commit is created), but most projects don't use them (some don't know they're available, others have decided they prefer merge commits). In other words, if not for bors, you'd just see the names of Rust maintainers on those merge commits instead.
I thought this might be a good moment for an inquiry: There's a category of "pedantic" lints for "Controversial or exceedingly pedantic lints". Would `clippy` accept lints for language restrictions? Some examples: * The ability to say "no `as` conversions in this module" to require use of explicit, checked conversions. * The ability to warn/deny defaulting binding modes in pattern matching. * The ability to disable whole constructs, such as `try` or `extern fn`.
The problem is that 'what makes sense' to the rust community seems to be super neat organisation, where the OCD inside you says 'well, if you want a `Request` object you need the `request` module!' Sometimes the gut reaction of 'what makes sense' is pretty bad, a lot of the times the nicest APIs would be traditionally thought of as 'bad' Let's take, for example, a linear algebra library (for working on vectors &amp; matrices) You COULD expose an API that works only with your custom Vector3 type you define in your library - or, you could duplicate your API surface to work not only with Vector3 but with [f32; 3] and [f64; 3]. The programmer inside you recoils at the second option, but in terms of useability this is the most friendly API out there - once you think through the 99% use case, you realise that the chances are your crate isn't the only one being used in isolation, and you're probably gonna want to be passing around vectors in different formats (like f32 arrays or f64 arrays). Basically, my point is that 'what makes sense' to current crate authors is super frustrating to me, the consumer of said crates, &amp; potentially we should think a little harder about useability of the API, not just how 'neat' you can pack everything up.
I’m not sure that it’s so broad. Some packages do this, some don’t.
I probably wouldn't make any new crates based on Gotham until [they find a maintainer](https://gotham.rs/blog/2018/05/31/the-state-of-gotham.html). Rocket and Actix are both good choices, although Rocket requires a nightly compiler. (This really makes Actix a strong contender\-\-the latest version of Rocket and the latest version of Diesel actually can't be used together at the moment because of a bug in the Rust compiler that breaks the latest version of Diesel\-\-I've had to pin my Rocket services to the 2018\-05\-15 compiler with Rocket 0.3.10.) You could also use a crate like [postgres](https://crates.io/crates/postgres) if you don't like ORMs. OP will probably also want some of the following: [serde](https://crates.io/crates/serde) and [serde\_json](https://crates.io/crates/serde_json) for returning JSON payloads [jsonwebtoken](https://crates.io/crates/jsonwebtoken) for sessionless authentication [bcrypt](https://crates.io/crates/bcrypt) if you need to hash passwords [chrono](https://crates.io/crates/chrono) for timey\-wimey things [uuid](https://crates.io/crates/uuid) if you prefer those over incrementing numerical IDs [reqwest](https://crates.io/crates/reqwest) if you need to make HTTP calls to other services [log](https://crates.io/crates/log) or [slog](https://crates.io/crates/slog) for logging
Oops, I need to update my docs then. I was working on PostgreSQL primarily. I had a mysql module before but I removed it since adding new features becomes a drag when you have to remove compile time errors for all the database platform implementation.
&lt;3 What do you all think about showing some more stats, like for example, resident memory usage?
I think it's enough to pay it some attention / try to move away from it, unless people actually like it this way 
Lockfiles mean that nothing changes immmediately. The registry is immutable so they can’t modify old versions.
Yes, however major versions are expected to have backwards incompatible changes so it's not much of a problem.
 &gt; Is there a reason for not just sticking a load of `pub use` in the root of your crate? I do this for the most common structs and traits. The rest are kept in modules. E g. Which one is better. The (imaginary) C function `snd_pcm_status_get_info` or the Rust one `snd::pcm::status::get_info`? Not much difference at first, but in Rust you now have all these choices: use snd; snd::pcm::status::get_info(); use snd::pcm; pcm::status::get_info(); use snd::pcm::status; status::get_info(); use snd::pcm::status::get_info; get_info(); ...depending on what's more readable in context. If you have a lot of status handling maybe `use snd::pcm::status::*` makes most sense, if not, maybe `use snd` (and qualify every function call) makes more sense. &gt; The difference is that in java, you have super sick tooling such that you don't need to remember which package everything is in If it doesn't come naturally which structs are in which modules, maybe the crate should reorganise itself so it comes more naturally. That said, let's hope that one day we will get "super sick tooling" in Rust too :-)
I mean, the idea of being able to choose is nice, but in your examples this is WAY more typing. Also remember that each one of these is a scroll to the top of the function / file, then finding your way back. A big part of the reason i love rust is because it DOESN'T let you choose. Imagine a world where the compiler didn't warn when you weren't using a snake case variable or function name - we'd have no standards between crates, and people wouldn't know whether they needed to type get_info or getInfo. I think the problem is that humans are dumb, &amp; we rarely make good choices - when you limit the choices someone can make, you pretty much always have a nicer experience. To go with your example, I would go for the imaginary C function any day. I don't need to remember the module structure of a crate to use that function - i just need to remember the function's name. This is important, because that crate containing the get_info function isn't even close to the only crate I'm using in any given project - I can't possibly remember the structure of your crate, when I only need it for 1000 lines out of the 20k I'm writing. I CAN, however, easily remember the name of a function I use often - regardless of what it has prefixing it. Also, I refuse to believe that your lib is so large you actually need 3 nested namespaces;)
Here's the [announcement](https://users.rust-lang.org/t/crates-rs-a-new-faster-crate-index-website/17876) where you can get more info about the project. I'm not the author btw.
Problem with that is that you don’t always want to squash - it can sometimes be helpful to see how an implementation progressed even after the PR has been merged. It’d be nice to effectively rebase master onto the new branch, but I don’t know enough about git to know if that’s possible
This is correct. std::simd is built on std::arch, and is also available on crates.io as stdsimd, so while you won’t be able to use std::simd on stable in 1.27, you will via that package.
Clean and fast. Nice!
Love the idea and want to find the time to build a rendering! :) In the meantime, you can get raw dstat capture from the logs server. Here is Actix as an example: [http://tfb\-logs.techempower.com/round\-16/final/actix/json/stats.txt](http://tfb-logs.techempower.com/round-16/final/actix/json/stats.txt) As I think about the rendering of stats in the results web site, top of mind is which stats are most interesting for a tabular summary view. I'll get a GitHub issue added to propose some ideas and see what opinions the community can offer. If you have some initial thoughts there, I'd welcome those!
Awesome! If you open the issue, cc me on it, steveklabnik, and I’m happy to chime in there. Thanks!
Depends on the construct. There still needs to be a valid reason for someone to wish to disable it
Thanks for the suggestion. But no, since I'm really not doing any calculations, I'm " just" parsing a binary file 
I don't think giving back crate names should ever be done if the name was used to publish crates. Yanking and pointing to the new name is the way to go.
Very cool! If it were open source, I'd send a PR to change the link color to something that doesn't look like my brower's default, though :)
No search option?
Woot! I love the categories right on the home page, I love how fast it is, I love opinions and experimenting!
Algolia would probably allow you to use their search platform, similar to what they did for the [yarn website](https://yarnpkg.com/en/packages). It's super fast, probably better than anything hand-rolled could every be. 
Yes that helps quite a bit. The only other question I guess I have is I'm assuming that the compiler knows what appropriate SIMD instructions set to call based just on the feature flag? If so that's really cool, and I can't wait to start trying out all of the awesome work you've put into portable packed SIMD vector types.
Well, of course :) The question would then be what reasons are "good enough". E.g. for `try`, is the wish to force some code to write out `Ok`s enough? I think in all cases I'm interested in it falls into one of two categories: * Lint some things to hint that alternatives might be better to use. Example: Code requiring conversions doable with `as` that you might not want to do with `as`, and that aren't just things like getting a `u8` to `usize`. * Lint some parts to require more annotations in code. Like making sure there's visible `Ok`s and exact types in patterns. I could imagine use cases for these where you'd want to totally forbid them, but also cases where you'd want to just deny or warn and allow specific parts to opt back in.
Wow, this looks really cool. I am really excited by this. Always preferred CLI, but mainly because the UIs was db-specific — having a generic one seems better. It seems like the UI needs some polishing, but that's understandable. Thanks for writing this!
I've updated the repo with ISPC aobench benchmark, and updated the readme with results in both CPUs I've used. ISPC is ~2x faster than Rust.
&gt;The only other question I guess I have is I'm assuming that the compiler knows what appropriate SIMD instructions set to call based just on the feature flag? That's it. It does so depending on the architecture you are using (`x86`, `arm`, `ppc`, `mips`,...), the `-C target\-feature=`s enabled globally at compile-time, and the `#[target_feature(enabled=...)]` per function.
We do the former already. The latter -- depends. File an issue.
FWIW pedantic is for possibly-controversial but still something there's general agreement on. Turning on clippy_pedantic shouldn't give you a non-rust-style experience. Restriction lints are more for highly specific things people may wish to turn off in some contexts.
I like the categories. It would be great if it's main distinction from crates.io was that it let you easily explore and find crates when you're not really sure what you're looking for.
`AtomicUSize`?
Thanks. I did the best I can with the UI, it's more of a functional look rather than aesthetics, since I'm no CSS expert. I'd hope to have contributors that can make the UI look better.
In my actual code, I need to volatile read a u8 and those Atomics aren't stable yet.
* the trait bounds only say that you convert &amp;str to T and i64 to T, you still need to actually perform the conversion, incidentally you should probably call your inner values something other than T as that's confusing: match input { MyEnum::stringSlice(v) =&gt; v.into(), MyEnum::integer(v) =&gt; v.into(), } * your input type is missing a lifetime: stripOut&lt;'a, T&gt;(input: MyEnum&lt;'a&gt;) -&gt; T * at one point you need to say *what* T is as the usage context doesn't filter it out that much, there's no type in the stdlib which implements both From&lt;i64&gt; and From&lt;&amp;str&gt; though 
Maybe [vcell](https://github.com/japaric/vcell)?
I'm assuming it's written in rust?
I like a clean design, but this is a bit dull.
I'm rather confused by this library. It's not sync (and can't be sync unless both `get` and `set` are made unsafe), so you can't have a shared reference to it on another thread, making the volatile reads pointless.
I'm curious, how did you get into working with robots? And what kinds of robots? :)
I transferred ownership to /u/steveklabnik1 . I hope someday somebody will create great software and publish it as pp ;)
In the [Table of PhantomData patterns](https://doc.rust-lang.org/nomicon/phantom-data.html#table-of-phantomdata-patterns) it says: &gt; (*) If contravariance gets scrapped, this would be invariant. Why would contravariance be removed from the language?
Personally, I much prefer the current approach compared to what you're suggesting. In any Rust module I write, it's very easy to figure out where a symbol is coming from regardless of what editor I'm using because a simple string search will show the corresponding import. 
For anyone here who hasn't read the [announcement thread](https://users.rust-lang.org/t/crates-rs-a-new-faster-crate-index-website/17876): &gt; Yes, of course! The things that stop me from open-sourcing it right now: &gt; - it needs to make &gt;64000 requests to crates.io 1, and I don’t know if I can unleash such traffic on it, and what’s the policy for redistributing its data. I know the team is preparing to make official data dumps, so that will help. &gt; - I know documentation is #1 thing people expect from crates, and I currently have none, and it works only on my machine :slight_smile: &gt; - I’m going to pitercss_conf 14 this week, so I don’t have time to fix the above issue. https://users.rust-lang.org/t/crates-rs-a-new-faster-crate-index-website/17876/18 There are offers of help in cleaning it up but the author is [unavailable for a week](https://users.rust-lang.org/t/crates-rs-a-new-faster-crate-index-website/17876/26).
Do the cpus you used support avx512? I think stdsimd only supports avx1 atm. I think that would explain it?
&gt; I'm curious, how did you get into working with robots? I was looking for some freelance work, and I created a profile on a freelance website. I was initially looking for a JavaScript job, but I mentioned my limited Rust experience in my curriculum. I was contacted by two companies for a Rust job, including one developing a robot with Rust since 2013 (the project was started with Rust 0.8). And here I am. &gt; And what kinds of robots? :) Medical analysis : you give it biological samples, it performs the analysis and gives you the result after some time. 
eh? 1. How would my approach change that? 2. You shouldn't have to do a string search, it just pulls you out of doing your work, I want to be able to write like 10 lines of code without having to zoom around the file doing boilerplate imports, right now I can't even write 1 You could just as easily say 'Yeah I don't care for a static type system, because REGARDLESS of what editor I'm using a simple if statement will clear up any ambiguities'
Turns out I've actually read some of your articles before! Keep up the great work, thanks.
Feels a lot faster than crates.io, nice!
So does that mean that the `stdsimd` crate will compile on stable Rust? Was the `repr(simd)` stuff stabilized or how is that possible?
My understanding is that it does, though I’d have to double check. I may be forgetting about the repr(SIMD) bit... 
Glad I could be helpful. 🦀👍
Unless I'm misunderstanding you, you want to write this: use some_crate::*; at the top of your file and be done with it. I'm saying that I don't prefer that because when I read code like this: fn do_the_thing(x: &amp;FooBar) -&gt; Baz { ... } I don't have any clue where `FooBar` and `Baz` are coming from. If you use a glob import, I can't just search the file and find out. Since I don't use glob imports, I can use position my cursor on `FooBar`, press `*` and perhaps `n` a few times until I come across a line like use some_crate::example::{FooBar, Baz}; which then gives me the info I want. &gt; I want to be able to write like 10 lines of code without having to zoom around the file doing boilerplate imports, right now I can't even write 1 I usually just write all the code first without imports, then do a compile and add the imports guided by the errors, and then compile again. It's really not bad at all. &gt; You could just as easily say 'Yeah I don't care for a static type system, because REGARDLESS of what editor I'm using a simple if statement will clear up any ambiguities' I don't really understand what this means.
Main motivation, is to avoid heap stored objects (for cache coherency). Since this is mainly (but not only) desired for containers, such vector, it may seems that alternatively it could possible to just make specialized version of vector; which on each object reallocation update iterators pointing to moved objects. But, since iterators, as objects can be moved too, we would just not have address of iterator to update. So such "stable" iterators, if implement vector with trackable\_iterators now, would have to be stored in heap... At the end this vector traverse would be fast, but access through this "stable" iterators would require redirection through pointer in heap.
No, I want to write use some_crate::{Default symbols, self}; &gt; I usually just write all the code first without imports, then do a compile and add the imports guided by the errors, and then compile again. It's really not bad at all. This is very slow, if you've ever programmed in a language where the culture is to group libraries under 1 namespace (like Go, LISP, or C), it's much easier to program difficult problems, because you spend more time thinking about the actual problem rather than getting frustrated about having to deal with all the cruft.
Given how rare such a case is, this seems a lot of effort for such a fringe concern.
I think we'll just have to agree to disagree. I've used those languages before and found them very unpleasant to work in. I typically use languages with nested modules/namespaces and I really don't find it distracting or frustrating to import what I use.
If the crate only had a 0.1 release and nobody beyond yourself was using it then... I think it is kind of an ugly thing. There should be a way on crates.io to somehow "free" the name. For example, by allowing someone else to use it as long as the first version they publish is a major version bump over the last one published.
&gt; I think we'll just have to agree to disagree. This is really important though, I wouldn't really use Rust for any ACTUAL project right now just because of how bad it feels to use - which is a shame, because the core principles of the language &amp; all the compiler warnings are amazing. &gt; I've used those languages before and found them very unpleasant to work in. How?
There is a crate which has integer atomics for stable rust: https://crates.io/crates/integer-atomics
Thanks for sharing. Wish more people thought on these lines than going after infinite scale for their 10 reqs/s application. These days servers can be bought with 3.5 TB of memory and close to 100 CPU cores.. if debts step out of their horizontal scaling craze, they can make use of shared memory, and make most things much faster cheaply. 
Is it normal for Clippy to not always compile as it is now? I actually just tried `cargo +nightly install clippy`, which generated errors. I would normally assume that there is something I should have done differently, but perhaps that is not my fault this time, is it?
Whoa, nice find! I just added it to my personal repos and got a bunch of PRs right off the bat. Definitely going to use this in the future!
You have to have an up to date nightly. But sometimes even then it won't compile and you need to wait for another release. All this work will fix this problem.
Trying to understand this a bit better: what does `getpwuid_r` return in `char *pw_dir;` if `$HOME` isn't set? Thanks!
If you want faster feedback on PRs for winapi, I would suggest donating to Peter's [Patreon fund](https://www.patreon.com/retep998)!
My issue is that "Error" and "ErrorKind" are very common in Rust. If you import everything from diesel, and everything from Rocket, you have name collisions *before you ever write any code*. Yes, in trivial cases you could export/import everything. But it breaks down so quickly, and namespaces become very important.
That's an odd comment. Not sure what's going on there.
My nightly is the latest one. I shall try again tomorrow. Once I get it to work, do I risk breaking it by updating again?
I'm not saying that namespaces are not important! Namespaces are super good, and I'd take them any day over the C world of just prefixing functions with crossed fingers. BUT, I'm NOT suggesting we just ``` use some_crate::*; ``` I'm suggesting ``` use some_crate{self, ...}; ``` I highly doubt your project is so huge you actually need multiple namespaces to differentiate your types - if you do, why do you have 20 error types in 1 crate called 'Error'?;)
According to [this man page](https://linux.die.net/man/3/getpwuid_r) it returns `NULL` for errors.
This assumes buying hardware doesn't it? Otherwise, given a cloud provider, that's going to cost a lot. It also ignores issues of availability. Hard drive dies, now your whole service is down. Any system that wants reliability needs at least two separate hosts executing it. I'm a bit confused as to why the results are surprising as well. Of course there's overhead to a distributed system vs a single thread when you can fit everything on one host - distributed systems never claim to be faster than a single system for that workload, from what I've seen.
I think the expectation would be that this will be a long term concern, especially as Rust grows in popularity. I also expect Rust to be an exceedingly long-lived programming language, so we should sort this out now in a forward-looking way.
&gt; This is really important though, I wouldn't really use Rust for any ACTUAL project right now just because of how bad it feels to use - which is a shame, because the core principles of the language &amp; all the compiler warnings are amazing. It doesn't feel bad *to me* to use. This is entirely subjective though so I don't really know what more to say about this. &gt; How? I very much dislike the implicit nature of these languages whether it's the lack of static typing (LISP), the lack of strong typing (C, Go), the lack of of proper modules (C), the lack of proper error handling (just return `null` in C or `nil` in LISP), etc. I find I spend too much time having to consult the documentation to figure out what a function expects and what it returns compared to languages like C#, Java, Rust, OCaml, etc. I realize C, Go, and LISP work well for lots of people and that's ok but they don't work well *for me*.
That's what I wondered ... how is "not having `$HOME`" represented in `getpwuid_r`, and does it count as an error?
The surprising part (to me) was that the overhead was so large that 128 cores wasn't enough to improve upon the single threaded performance. I would have guessed that the overhead wouldn't be that bad. Moreover, when people published these implementations, they only compared their implementation on a single core against their implementation on many cores, concealing that fact.
It works on safari, that is so good to know
- I don't think it IS totally subjective - in my example, you spend a lot more time typing, and without a concrete auto-import system like Java (although we may have this in RLS right now(??)) you have to revert to docs WAY more often - I'm specifically talking about the module systems in those languages, not the type systems
That's a fair point - the differences are definitely a bit more stark than one might intuitively imagine.
Personally I don't think any code which contains "\r\n" is a fair comparison for these benchmarks. At this point you are not benchmarking an HTTP library but a TCP server. IMO it would be better to remove this version since it is misleading. The `actix` and `hyper` versions however have very clean code and impressive results.
Yeah but also you get new features if you update, so you should still update every now and then.
&gt; I don't think it IS totally subjective It is though. You think this is an issue and I don't. &gt; you spend a lot more time typing I don't personally. I type quite fast and I fix missing imports after I finish writing the code, not during it so it doesn't interrupt the flow of what I'm doing. &gt; you have to revert to docs WAY more often Either I know the name of the thing I'm using, in which case adding the import is easy, or I don't know the name at all, in which case I'd have to look at the documentation anyway. How do you program against a type when you don't know the name of the type?
Hello, im coming from a C++ background and am just starting to port a project to rust. One feature that is relative useful in C++ are variadic templates and to a lesser extend variadic functions. Are there variadic generics in rust? And if not what is the best way to implement something similar? Macros? Thank you 
i dont think performance matter for top contenders, they all are fast. This is just a game for places :) i added raw benchmark aster `aspcore` added similar one. 
I like that it works with JavaScript disabled for two reasons: 1. I have a strong dislike for any site which reinvents templating that shouldn't need JavaScript using JavaScript. (TiddlyWiki gets an exception, because it's a wiki written as an HTML quine that you can carry around on a thumbdrive.) 2. crates.io and the TiddlyWiki ecosystem are the only places I can think of where search engines like DuckDuckGo and Google are more or less useless.
&gt; It is though. You think this is an issue and I don't. This happens with loads of objective problems, I don't understand how this 'proves' it's a subjective problem &gt; I don't personally. I type quite fast and I fix missing imports after I finish writing the code, not during it so it doesn't interrupt the flow of what I'm doing. Yeah, but you still undoubtedly have a longer turnaround time between coding &amp; seeing your results (and this is very important, especially in fields like game development where seeing results is your main form of testing) &gt; How do you program against a type when you don't know the name of the type? The problem is I know the name of the type (because memorising thename of a type is very easy, see the examples in my op like `Request` and `Response`, but remembering the author's specific module naming convention is very hard. Remember, I'm not just using 1 crate - I'm using many different crates, so if different authors conceptually name modules differently there's no 1 standard way to divine the name of a module structure without just brute-force remembering the module that the type / function is in!
It would be great to have LoC in there...
Maybe, but it is a linear amount of effort. Once we have a "system" in place we can better handle this stuff as crates.io grows.
Sounds interesting, which freelance website did you use? :)
&gt; This happens with loads of objective problems, I don't understand how this 'proves' it's a subjective problem We're talking about how Rust makes you and I feel. Our feelings are inherently subjective. Neither of our feelings are "correct" or "incorrect". &gt; Yeah, but you still undoubtedly have a longer turnaround time between coding &amp; seeing your results (and this is very important, especially in fields like game development where seeing results is your main form of testing) Yes, but I don't consider "turnaround time" to be a particularly useful metric to me. I'd much rather spend more time upfront if it means less time debugging issues later. Again, this is subjective. I don't value turnaround time but you do and that's ok. &gt; Remember, I'm not just using 1 crate - I'm using many different crates, so if different authors conceptually name modules differently there's no 1 standard way to divine the name of a module structure without just brute-force remembering the module that the type / function is in! If you're using many different modules, there's more of a chance of collisions anyway so you'll have to manually import the names to resolve collisions ;)
Indeed, especially on the Plaintext results, I feel quite strongly that precise rank order is not important, though it is obviously fun for the participants to jockey for first. But I encourage readers to recognize that there is a group that are highly motivated by plaintext performance. And when those same frameworks also excel in tests such as Fortunes, you can build an application confident in knowing your framework is doing its absolute best to stay out of the way, consume the very least CPU cycles necessary to perform its functions, and leave those cycles for you, the application developer. As it should be. Incidentally, I do plan to add tiers of some sort to the results view to try to drive this notion home visually. See [https://twitter.com/BrianHauerTSO/status/999401726179401728](https://twitter.com/BrianHauerTSO/status/999401726179401728)
Yes, it would. We've tinkered with some ideas there in the past, including ideas ranging from self\-reported (honor system) LoC counts to adding special markers in comments above and below the code that implements our tests. Something like the latter would have a side\-benefit of being able to open the relevant code as a "pop\-up" over the results, which is something I'd really like to see. Nothing yet has materialized here, though. But I agree!
&gt; I don't need to remember the module structure of a crate to use that function - i just need to remember the function's name. To me, that is the same. Namespaces are part of the name. I'm unconvinced that `crate_name::function_name()` is superior to `crate_name::function::name()`.
&gt; We're talking about how Rust makes you and I feel. Our feelings are inherently subjective. Neither of our feelings are "correct" or "incorrect". We're talking about making a language that allows us to engineer solutions to problems, we can totally be correct / incorrect in this, and one way can certainly be 'better' than another for a specific task! &gt; Yes, but I don't consider "turnaround time" to be a particularly useful metric to me. I'd much rather spend more time upfront if it means less time debugging issues later. Again, this is subjective. I don't value turnaround time but you do and that's ok. But that's the thing, a module system isn't like a type system in that it doesn't prevent loads of bugs in the future - my suggestion literally JUST reduces friction, and I cannot see it introducing bugs in any way in the future! &gt; If you're using many different modules, there's more of a chance of collisions anyway so you'll have to manually import the names to resolve collisions ;) I'm already manually importing the names, like i said earlier, I'm NOT doing `use crate::*;` ;)
I have implemented a web service with `actix-web` and found it to be a good foundation. Currently, I am implementing a second project using `actix-web` and diesel for accessing sqlite without locking myself in. If your backend needs startup parameters or you want to create accompanying CLI tools, `structopt` is fantastic for argument parsing. As mentioned by others, `chrono` gives you a sane API for date and time stuff. I started using `slog`, also has a learning curve but is powerful in its functionality. `dotenv` lets you configure your service by a cheap `.env` file in properties syntax. `serde` is _the_ solution to most of your data exchange problems. I also like `tera` as a template engine. Benefits from Jinja2 syntax support in IDEs and has, at least in my case, sufficient performance. Just for an impression, loading a page in my rudimentary backend, decrypting cookie, confirming signature and checking auth, accessing 10 values from an sqlite db, rendering a table in HTML from that and pushing it back to the browser (using TLS), I get a 'measurement' of 1ms in Firefox dev tools and 849_253 ns from my logging middleware. I'm _so_ curious about real-world performance, once a beta runs on a server out there.
Because modules are organised based off of some programmers mental organisation Names are also organised like this, but names are shorter, and if you actually look at the names being namespaced, they don't NEED the prefixes given by the modules. Let's go back to the `actix` crate - there's a trait called `Registry'. What module is this in? Well, if you guess the root, you'd be wrong (although I mentioned this trait earlier, so no points for being right either;)) It's actually in the `registry` module. Tell me, how do I decide whether to use `actix::Registry` and `actix::registry::Registry`?
I love how dependencies are listed by feature and target! Awesome!
This paper is from 2015 but still very relevant. Don't miss the various accompanying [blog posts](https://github.com/frankmcsherry/blog/blob/master/posts/2017-09-23.md). Incidentally the [https://github.com/frankmcsherry/COST](COST codebase) was recently refactored too.
Oh, I assumed the number of cores announced took hyper-threading into account if available. I didn't expect a 28 physical (56 w/ hyper-threading) monster to be used!
Unfortunately, Rust does not have variadic generics. It's fairly common to define a family of types for a variety of arities: `MyType1`, `MyType2`, `MyType3`, etc. Often you can use a macro to generate those definitions, so that it's easy to go to whatever maximum arity you want to support.
A place where I can find all of the no_std libraries! \o/
Do you mean that you want parsing to truly be parallel with I/O, or just interleaved? Because if you just want it interleaved—read a line, process the line, repeat—then the `BufRead::lines()` iterator should do what you want. If you want it to actually be parallel, that's harder. You'd want to have a separate I/O thread and parsing thread, and maybe the I/O thread could pass `String`s to the parsing thread over a channel. You'd still possibly want to use `BufRead::lines` for the I/O thread. If you want to process lines in parallel with other lines, look at Rayon for parallel iterators.
Do some degree I think you're right. It's not obvious whether `actix::Registry` or `actix::registry::Registry` is the better choice. For me, I tend to reexport if a struct has the same name as the module. Some coding guidelines/standards wouldn't hurt here. (Maybe there already are some, and I just don't know about them.) &gt; Also, I refuse to believe that your lib is so large you actually need 3 nested namespaces;) This is a design choice. Many small modules also help with documentation. I can look at rustdoc for a module and find the right function to call, quicker than I would if everything was in root and nowhere else.
A google spreadsheet and google forms does this extremely great.
I looked into this a few weeks ago and I have to say the udemy offering for Rust is not that great. There are many great courses for other languages/frameworks, but unfortunately not yet for Rust.
&gt; For me, I tend to reexport if a struct has the same name as the module. The problem WOULD be fixed with a standard, but even if there WERE guidelnes, people wouldn't follow them (people use tabs in python even though pep8 specifies spaces) &gt; This is a design choice. Many small modules also help with documentation. I can look at rustdoc for a module and find the right function to call, quicker than I would if everything was in root and nowhere else. The problem is that actual docs help way more than this, once you have even a semblance of docs this stops being important Also I don't know what you mean about the 2nd bit, there's a search functionality with the cargo docs page 
What if the new crate is API compatible but evil?
Great post! As someone who had to write about 5 different little parsers for the last half year, I'm very sympathetic to a good parser generator library. On the API side, it seems very close to what I remember from an experience of writing an IntelliJ parser (PsiBuilder &amp; co). I don't remember all the details, but I remember things around comment and whitespace management (through some declarative left-/right- binding IIRC?). This is what I miss in [pest](https://github.com/pest-parser/pest), which I currently use. It has pretty basic whitespace/comment management, but only if you don't care about them. On the other hand, I like the lack of separation between lexers and parsers in pest. I understand the advantages of separating these two, but it always looked artificial to me. 
&gt; The problem is that actual docs help way more than this, once you have even a semblance of docs this stops being important Rustdoc *is* actual docs! For very big projects with many crates you might need an overview to show how different crates interact, but more often, you don't need more docs than rustdoc. Just have a tutorial with a few examples at module level. E g, in the `snd::pcm::status` case I would ideally write a short intro to status handling on the module level doc page and a few examples on how the different status functions can be used together. &gt; Also I don't know what you mean about the 2nd bit, there's a search functionality with the cargo docs page But that requires you to know the name of the function. E g, I might want to find out the current pcm timestamp, but my search for "timestamp" turns out empty. By reading the module level documentation for the relevant module, I figure out that it's called "tstamp" instead. 
wow, I'm really impressed by what categorization can do! It's like seeing the light. 
Hmmm, maybe compromise on a full exported module system then a `prelude` module that actually has EVERYTHING exported, not just what people think are 'common'. This way I could just write something like use crate::prelude as crate; and be totally happy!
If there are no name collisions, you can actually do this yourself: pub mod crate_prelude { pub use crate1::*; pub use crate1::mod1::*; pub use crate1::mod2::*; // etc } 
&gt; If the crate only had a 0.1 release and nobody beyond yourself was using it then The policy should conservative and enforceable, and that requires drawing a clear line. &gt; If the crate only had a 0.1 release and nobody beyond yourself was using it That's as unclear a line as it gets. The first condition is arbitrary. And the second is is impossible to verify, not to mention that it breaks one of the basic assumptions on the internet (once something is out, it's out forever).
Don't lie, this is fucking horrible;)
Mentions "it's been around for 12 years". I don't think anybody really counts from when graydon first cooked up the idea, I'm not even sure it was called "rust" then. It was released officially in 2015, so we're 3 years old.
This is very cool. Is the site open-source? I'd be interested in learning more about the stack and how it manages to be so fast.
Did you actually take the Udemy course or just basing it off other reviewers ratings?
Another possibility would be to set up search using Elasticsearch. AFAIK the fastest solution currently available to build a search engine.
&gt;On the API side, it seems very close to what I remember from an experience of writing an IntelliJ parser It is almost a direct copy of that API :) An interesting difference is that in Rust the AST-over-ParseTree approach is significantly more ergonomic because trait system is more flexible than Java’s single inheritance, and allows for static polymorphism. The code for listing file symbols for example looks much better, because the visitor is heavily type driven.
I don't imagine that move constructors are a thing that will happen in Rust, as a lot of the current semantics of the language are based around the assumption that moves are always a simple `memcpy`. There has been some work towards making self-referencing structures work in other ways though, mainly through immovable/pinned types (see https://github.com/rust-lang/rfcs/blob/master/text/2349-pin.md)
Would be cool to default exit value 1, as an automatic warning to operator staff that an application’s exit code is not to be trusted.
[removed]
My use case is just doing something for fun. Probably making a dummy forum ala Reddit. I just want to practice getting the full stack experience
I noticed there is an "Implementation Approach" field, with options "Realistic" and "Stripped". In the FAQ you define: "A Stripped test implementation is one that is specially crafted to excel at our benchmark". I think this designation fits `actix raw`. BTW, your approach of comparing various metrics for many frameworks, thus gently pushing them to improve, is great. You make the internet a little bit faster :)
My plan is to just build a dummy web forum to practice things like security, full stack web development, and more database practice
I find that the organization into modules really helps with reading the documentation. It's very hard to make any sense of the output of \`rustdoc\` if it's essentially just a long list of types. The logical grouping of types into sensible modules lets you more easily digest the documentation. The time it takes to figure out or write the right import is honestly something I've never even considered a problem. Usually what slows me down when coding is certainly not my typing speed, but rather the time it takes to come up with a solution to the problem at hand. Anyway, as others have pointed out, the organization into modules serves an important aspect with respect to encapsulation: being able to make some things accessible only for types within a certain module is essential to build good APIs which avoid leaking their implementation details. The fact that virtually nobody \*additionally\* exports all of their types into the root just suggests that this isn't a big deal to people, I suppose.
I actually consider this a feature. "this library treats warnings as errors" is totally a library property. This interacts with the `--cap-lints` flag of rustc(1) nicely, which is on in release mode and set to `warn`, making sure that your code doesn't break by future warnings.
But no one needs a webapp for that. We had multiple tries to actually provide something caring about ecosystem here (for example structured handover of unmaintained crates) and didn't end up having enough hands for that. In the end, if you want to start such a thing, don't build software, get a spreadsheet and an email account.
Very good call and thanks for pointing that out. I'll get that captured as a GitHub issue. &gt; BTW, your approach of comparing various metrics for many frameworks, thus gently pushing them to improve, is great. You make the internet a little bit faster :) My heart, it is warmed. Thank you!
I don't think there are any plans for trackable objects. However, there are plans for (guaranteed) immovable objects, using the `Pin` type/trait (I forget which). Would that help?
I think crates.io itself could be a lot faster without ember, too! And maybe with actix.. I'd like to see crates.io getting faster and incorporating some of the things of crates.rs. Maybe crates.rs can be used as a beta-testing ground for ideas that are being evaluated to see if they should be included in crates.io
I wonder if it would be possible to have a trait `Move` that allows you to access the address of the moved object after it's been moved (so as to track it as above). Presumably with some optimizations disabled for types that implement `Move`.
I started with Pluralsight (free for me at work), but The official Rust documentation is definitely worth going through in tandem with the video course or afterwards.
&gt;I also haven’t seen deterministic LR parsers with great support for error recovery, but looks like it should be possible in theory? [lrpar](https://github.com/softdevteam/lrpar) has support for error recovery. It might be useful here.
I think sharing this link is relevant: https://github.com/rust-lang/crates.io/pull/1173
Basically just [this](https://askubuntu.com/a/29364) , and you can test with it by `sudo su mytestuser`
I think one great improvement is that the domain name doesn't support the [US military anymore](https://gigaom.com/2014/06/30/the-dark-side-of-io-how-the-u-k-is-making-web-domain-profits-from-a-shady-cold-war-land-deal/) Sure russia isn't that much of an improvement, but it's an actual place with actual people and not just a military base.
I'd want what's the fastests, screw the resources :) Problem is, I don't have a good idea what that would be... Channels might be a good idea, but I came across scoped threads from crossbeam, that sounds a bit like I want it. But yeah, thanks for rayon, that's actually probably a good idea, parsing is totally parallelizable... I'd still need to worry about that reading though.
Yeah but then you have to run ES..
Is there a way to keep, or retrieve, the old versions if it turns out that the update breaks things?
I actually checked out Pluralsight and it's the same instructor as the Udemy one, i'm wondering if it's the same exact course? I have to sign up with a payment method just to get the free trial with Pluralsight so I can verify if they are exactly the same, atm :\-/ I'll likely go with either just to get my feet wet and dive in deeper once I know the basics of the language. 
&gt; Problem is, I don't have a good idea what that would be... Guess you should try it a few ways and see ;)
You’d already need a webserver to run crates.rs, setting up Elasticsearch would be possible in most cases. I’m suprised I am already getting downvoted for just giving a suggestion.
Running elasticsearch is a real pain in the ass. Orders of magnitude worse than a webserver. Many orders.
Interesting! Their approach for finding the minimal set of edits to get some valid code seems really neat and sophisticated. However, I am not sure that finding repairs is actually the right problem to solve here. The IntelliJ-style approach of just skipping tokens while you are in a loop seems much more robust to me, because it is stupidly simple.
Nice. I'm definitely bookmarking this to check up on. Have you considered letting us install this through cargo --install ?
Thanks, I will give it a try!
Well, it looks like it just returns `/home/` + `mytestuser`. (The syslog user also claims to have `/home/syslog` for instance.)
&gt;t needs to make \&gt;64000 requests to crates.io 1 Anyone know what this means?
In context of vector with non\-invalidatable iterators: If, pin forbids move, and at the same time will allow clone()... And iterator will be "immovable" but "clonable" \- then in clone trait it is possible to send/update new iterator address on each clone. But syntax of iterators will be awkward, since you'll need to clone() everywhere. And I suspect, that since iterator will be immovable you can't have vector of iterators.... (if suppose, that vector cannot have pinned elements)... N.B. But it is possible that I misunderstand Pin trait...
I thought rs was Serbia?
You can always install older nightlies via \`rustup install nightly\-yyyy\-mm\-dd\` and \`cargo install clippy \-\-version ....\`
Can macros be used to read an external data source and generate code based on that, all at compile-time?
Agree, I really like it, and it would be nice to see on crates.io too!
Thanks! I hope I won't have to attempt it! :-)
Yes, but using the nightly compiler(at least for now). cargo +nightly install diwata_cli diwata_cli --db-url postgres://user:passwd@localhost:5432/dbname -p 8000 --open
I agree. I think the actual modules exposed to the user should be semantic to them and not for the codebase. Generally that should just mean putting pub use in various places and probably keeping the underlying modules private (pub(crate)).
Russia is .ru and .su (and also .рф)
We also need a system for when, eventually, someone refuses both to give up a name and maintain a project. The RustCrypto project is a good example of that where a few sub-crates are named differently to the rest because of that issue.
Can't you just cast a `&amp;[u8; N]` to `&amp;[u8]`?
He scrapes the website to build his mirror.
Try Rust by Example 
&gt; Tell me, how do I decide whether to use actix::Registry and actix::registry::Registry? Does the concept of a registry have more than just a `Registry`? Then the module is appropriate. If not, then it's not.
I noticed one thing which I presume is a bug. The "Next Page" link at the bottom when browsing a category sends me to crates.io :D 
[This comment from carol10cents](https://github.com/rust-lang/crates.io/pull/1173#issuecomment-353228688) in particular is quite enlightening as to why Ember is used!
How am i meant to know that when i'm programming Here's how it works: I think 'I need a Registry' I think 'I need to import Registry now' I think 'Welp, let me check the docs because i have no idea off the top of my head what module it's in' Even if I knew an appropriate module name, that doesn't mean the author of the crate has put it there!
Presumably on startup it reads in every crate from crates.io, so if there were multiple instances running (if it were open-source and several people decided to run their own copy) then it would create a huge number of requests and potentially ddos crates.io
The front page currently has the md5 crate, which is a broken cryptographic algorithm. There’s a case to be made for including it in the site, but wouldn’t it be better not to show it on the front page?
I think this is a tooling issue more than an organization issue. In Visual Studio C# I can auto import the proper namespace by simply having my cursor over an unresolved name and hitting Ctrl + . I think having that kind of tooling makes it far easier to not break your train of thought just to deal with imports. It would also make it less annoying to deal with nesting of modules.
Do not use hover menus; they break for keyboard-only users, people with muscle problems, or people trying to use your interface in anger. Use a checkbox menu, or a target menu, or [hovers combined with some nasty heuristics that require JavaScript to compute](https://css-tricks.com/dropdown-menus-with-more-forgiving-mouse-movement-paths/#article-header-id-4).
Thanks- kind of makes me glad I didn’t kill my Packt sub looking superb
It's WIP, but we (Lyken) have been working on [a Rust GLL parser generator](https://github.com/lykenware/lyken/tree/master/gll) ([latest dev branch](https://github.com/lykenware/lyken/tree/gll-dart/gll)) for a while now. (if you want, you can go to `../gll_tests`, relative to that URL, to see how we're testing it) It already has a traversal API which lets you choose for every node if you want to require an unambiguous parse, or handle all the ambiguities (within that node itself, not its children). We've been stuck in refactoring hell since starting this endeavor, because the papers we started from talked about C and goto's, despite GLL being so beautiful once you start abstracting and orthogonalizing it a bit. It's fundamentally a non-deterministic VM with *very* lightweight threads (a handful of "words" of state, "stack frame" included), executing a parser program, which creates parse nodes as an output side-effect. We could even create an IR for the parser program, and then generate Rust code from that IR (allowing e.g. optimization passes instead of special-cases here and there), but we haven't gotten that far yet. One of the major next features to add (aside from documentation) is "disambiguators", or better put, "validators", which get to filter every node in the tree. For example, say you have a `E = E * E | E + E | Ident;` grammar. That, under GLL, results in an ambiguous parse for `a * b + c`, where the nodes are either grouped as `(a * b) + (c)` or `(a) * (b + c)`. A GLL precedence validator would take a look at the `(a * b) + (c)` node, see that all the children that are also binary operators (i.e. `a * b`) have higher precedence (`*`) than the node itself (`+`), and accept the node as valid (at least assuming no other validator disagrees). It would also look at `(a) * (b + c)` and reject the node, as the `b + c` child has lower precedence (`+`) than the node itself (`*`), making it an invalid parse (sub)tree, precedence-wise. By employing [precedence relations](https://en.wikipedia.org/wiki/Operator-precedence_grammar) instead of numerical "precedence levels", asymmetric relations can be used to encode associativity at the same time! But precedence can be encoded in CFG, so this isn't too exciting. Something that can't use CFGs for is Rust raw strings - even if you require that the same number of `#` is found before the opening `"` as after the closing `"` (by treating it like balanced parens), you still have an ambiguity in e.g. `r##"foo "## bar"##` (assuming the rest of the code somehow "fits" with both parses, which is potentially quite-unlikely/impossible-for-now). With a GLL validator, we can allow the short (`r##"foo "##`) parse and disallow the long (`r##"foo "## bar"##`) one, by simply checking if the contents of the string contain the closing sequence, `"##` - the parse is valid only if they don't.
What I normally do is use Twitter Bootstrap with a little patch of my own design to make the click-driven menus degrade to `:hover` if JS is disabled. If I remember, I also tune them so that Tab-based traversal works in a comfortable manner.
Though I don't think this is true of any supported Rust platform: exit value 1 is success on some such as OpenVMS.
I've found step 3 to be what I jump to 90% of the time, and it's extremely quick. I go to docs.rs/actix, press 'S', type what I'm thinking, and then I have the full qualified path. If I already have the tab open, which is likely if I'm working with a large library like actix, it's even faster than that - just alt+tab,S,"registry". It's.. not a large cost.
[http://intorust.com/](http://intorust.com/) \+ [https://doc.rust\-lang.org/book/second\-edition/](https://doc.rust-lang.org/book/second-edition/).
I took another look at the things you listed and I noticed that you can do LEB128 *extremely* fast if you have access to `pdep` instructions. I don't suppose the Rust compiler can use `stdsimd` and `is_target_feature_detected`? input 0000000 0000000 0000000 0100100 1001001 0010110 0101010 1001011 pdep 00000000 00000000 00000000 00100100 01001001 00010110 00101010 01001011 add 01111111 01111111 01111111 10100011 11001000 10010101 10101001 11001010 mask 00000000 00000000 00000000 10000000 10000000 10000000 10000000 10000000 shift 00000000 00000000 00000000 10000000 10000000 10000000 10000000 or 00000000 00000000 00000000 00100100 11001001 10010110 10101010 11001011 bswap 11001011 10101010 10010110 11001001 00100100 00000000 00000000 00000000 You need a bit more work to handle &gt;56 bits. One major issue is that the current interface writes to a vector, which is a terrible idea if you care about performance. 
Does anyone have a Tokio example of a server\+client that is not request/response but allows both the client and server to send messages at any time? I went through Tokio's examples but coudn't find anything that fit.
Congrats on the book! My experience with Packt was very similar.
I'm curious how .io supports the US military. It's UK owned, and the US leases the island from the UK. The article even says the money just gets re-invested. A crime against the natives, sure, but the US seems like a bystander in this.
But it is a large cost if you're doing this many, many times - you're not factoring in the time taken to find the info on the page visually, tab back and add the use decl (possibly tabbing back and forth to make sure you spelt it correctly), then re-positioning the cursor back at where you were, then rememebering wtf you were trying to do This is why people use vim, because the mouse is a really slow tool to navigate code when you're constantly switching between the keyboard and the mouse - you just don't realise how slow it is until you actually learn to only navigate with the keyboard
So, this is a tooling issue which is born out of a cultural / organizational issue There's a reason why people say Java dev is impossible without an IDE, whereas C dev is fine with something lightweight like Vim - because the culture of java is insanely nested packages with super long names, and you just can't develop reasonably without having an editor fill in what you can't remember. I don't want rust to become like that, when we have a perfectly fine alternative - just organise our packages differently. As soon as we put bulky features like this onto our editors, we start raising the bar for people that can actually develop in rust. Modern Java IDEs are so slow and consume so much RAM because of the features they provide that I just can't do any JVM language development anymore - No, really, I can't. When I do I use Vim, and I'm 5x slower than what I could be.
Hm- I must have a pretty different workflow. Documentation is usually my single other common window besides a cade editor and terminal for compile output. If I'm looking up the name there I don't even need a mouse since I can just take it from the search results- if you already know the type and just don't know if it's `actix::Registry` or `actix::registry:: Registry`, typing "sregistry" should give enough information? There's literally no mouse movement there, just alt+tab,sregistry,alt+tab. Even with browser tabs, browsers have keyboard shortcuts too, right? ^T+docs.rs/actix&lt;ret&gt;+sregistry is still all keyboard and fairly quick.
What is the status of mathematical notation in documentation strings? Is it possible at the moment?
But then you have to copy this out into your editor, and swap between the tabs - the keyboard shortcuts for swapping between tabs are pretty poor once the tabs get high, you need to hit really awkward chords like M-8 and M-9 And this is when you're super on-top of things. If you're just lazily going about your day, programming at an average relaxed pace, you can't take in this information that fast I would love to watch you code, I guarantee you you're spending 10s + inserting imports - even for APIs that you know well.
You're making it sound easy to run elasticsearch. It's not. This undermines your suggestion.
&gt; "property": { &gt; "low": [ 0, 1, 2 ], &gt; "high": [ 7, 8, 9 ] &gt; } Is a better solution, if everything is one line it's quite unreadable.
Ok, so from what I understand after a day of research, what happens is smth like this: 1. The executing function runs all futures. 1. The futures try to read/write sockets/file descriptors, which registers an "interest". 1. The executing function blocks on low level apis like `select/epoll/kqueue`. 1. As soon as an event arrives from the OS, all sleeping futures (only relevant ones?) are awakened. So basically, if I have smth like `std::sync::mpsc::Receiver`, I can't magically register an interest by calling `.try_recv()` without either spawning a side thread, that will call `.wake()` (which nullifies the point of this future) or burning CPU by repeatedly calling `.wake()`. The practical problem I'm trying to solve is stopping a filesystem watcher from the `notify` crate, which relies on `std::sync::mpsc` channels. And since `Select`is unstable (and is en route to deprecation), I don't think I can do that without spawning another thread.
If the data is taken from a dump instead of a DB, you can probably generate static pages for the whole thing
 "property": { "low": [ 0, 1, 2 ], "high": [ 7, 8, 9 ] } Is a better solution, if everything is one line it's quite unreadable.
I don't think there is any reason why Vim couldn't have this kind of functionality when integrating with rusts language server. Just because Java tooling is bloated doesn't mean we have to have bloated rust tooling for simple things like symbol imports.
&gt; infinite scale for their 10 reqs/s application I've had this conversation before and concluded that Reddit would pretty much be a "single-server" application plus a CDN for images and video if the web was remotely efficiently engineered. They only hit 31k pageviews/s average in 2015, which is a mere 310 pageviews/core/s with the stats you give. An appropriate data structure in 3.5TB of RAM fits all of their data (sans the CDN) and should be able to eat through that load with capacity to spare... again, if the web was efficiently engineered. Obviously you want redundancy, locality, database stores, etc. etc., but the point is if you do upfront design for performance you'll find computers are pretty darn fast.
ok, but this is just saying 'Woah dude, why would we bother implement a cultural change in our ecosystem that would fix a load of flaws at very little expense when we could instead develop the fastest and slickest dev tools seen to date, where companies like Jetbrains and Microsoft have failed' I mean, it's pretty insane to say 'hey this isn't an issue, because in theory a dev tool that we might have in the future can fix this problem' Why don't I just say 'hey don't worry planning your code out, soon we'll have awesome refactoring tools that'll just let you move stuff around with no effort'
Personally, I've been won over to the side of [lexer modes](https://www.oilshell.org/blog/2017/12/17.html). I [spent some time wondering about a clean way to in-string interpolation](https://redd.it/8akxie) -- that is, Swift's `"x = \(x)"`, Python's `f"x = {x}"`, and Kotlin's `"x = ${x}"`. The problem is that you have to run your parser inside the brackets to determine where the interpolation stops. Lexer modes solve this in what I consider an elegant manner: you have a lexer for your main language and a lexer for your string language (escapes and all). Your parser talks to the main lexer, and upon encountering a Main::StringStart token, starts asking the string lexer to lex the string. Then, if it encounters a String::InterpolationStart token, it starts parsing the main language again. In this way the complexity is handled by the parser, and the lexer is still just a dumb match against the remaining input's prefix. Depending on how you're reusing lexing from previous lex passes, this might break that. It does prevent the ability to lex the entire input before parsing. But it serves as sort of an in-between ahead-of-time lexing and lexerless designs. Does fall's design handle this type of recursive language for string interpolation? I'm pretty sure you can somehow convince LALRPOP to do so by abusing actions, but that isn't compatible with an interpreted approach.
I can't say much about the code other than I think it's fine, but I much prefer the name "disjoint-set" for this data structure
So ideally the core and basic functionality should be exported on the root. But advanced functionality and complex things (and building blocks) should be in modules, where you explore a module to explain what part of the library you want, even if you don't want the whole thing. Say for example I have a window creation api. I'd expect that all I need to setup a window, run it and do basic operations with it is exposed there. If I want to access OS-specific things, that would be on a module that clearly states it's OS specific. If I want to extend the system (say creation new actions by composing others) I would expect that there's a separate module for extensions. If I want to do very rare actions that generally should be avoided (such as making a window that bounces and runs away from the mouse) those would be on advanced feature modules. Why? Because when I read the docs I can see 8 functions/data/traits, of which I'm interested in 5, or 30.
I believe you're looking for /r/playrust. This subreddit is for the programming language of the same name.
I mean, you've just said a load of stuff here but there's not much justification, why is this 'ideal'? I'm pretty sure I've put forward a good case for a prelude module where everything is exported
I believe getting rid of the lexer/parser separation is pretty good win in this area, as long as your grammar is powerful enough to do lexing in the first place.
I would just like to clarify something about Diesel 2.0: We are not sure when it will happen, or if it even will happen. However, there's a growing set of minor breaking changes which are clearly needed, and cannot be done through a normal deprecation cycle that warrant having this discussion. I would like to make it absolutely clear that we do not expect any breaking change made in Diesel 2.0 to substantially affect users. We will hold any changes in this version to the same standard we hold deprecations (clear and easy migration path, ideally affects a small number of users). If you have `#[deny(deprecations)]` on your code base, you should not expect Diesel 2.0 to be any more difficult than previous versions. However, we are strongly committed to semver (as defined for Rust libraries in [RFC #1105]). For that reason, even though we do not expect the changes being proposed to affect the majority of our users, we will be increasing our major version to indicate that there is code which *could* break. [RFC #1105]: https://github.com/rust-lang/rfcs/blob/master/text/1105-api-evolution.md 
The ideal is the next: *I should have everything I need at hand, and should not be encumbered by anything I don't need*. This basically means that, in an ideal world, I would get a crate, that would have only, and only the functionality I need in their main crate. In reality crates are made for many use-cases, and it's hard to do this. There's a lot of functionality that someone might need, but not me. Now lets talk about the cost of reading code that uses this library. When you read code the fact that there's multiple sub-modules isn't expensive, because the module gives me the full path. It's only expensive when writing code and the path is not initially known. OTOH it's expensive to find the thing when I have to go to a file with a massive amount of items (most of which are just re-published) then I jump to the other file, and so forth, which makes it hard to find out what is the actual function called by the user of the library. Reading docs is a similar thing, I end up having a lot of scrolling to do before I find what I want. Sure search helps, but I'll probably be jumping on different places and this becomes annoying. This is why the second part matters, I don't want to be encumbered by extra functionality that I don't need. Having unrelated code next to the code that I care about, means I can't be 100% sure what truly is the code I care about without working out not even the compiler can (how the author intended things to interact). It's cheaper if modules are small (and correctly focused) because then I know I can just read the whole module and it'd be cheaper than trying to understand what I need and don't before I know what it does and how. So the compromise is that the core functionality, that everyone, or most everyone uses, should be on the root. Everything else should be on modules that specialize on different use-cases. Then I know that if I need functionality related to foo, I am going to be using `crate::*` and `crate::foo::*`, if I'm using specialized functionality related to bar instead, then `crate::*` and `crate::bar*`. I have most of what I need relatively accessible, I do have to type a bit when writing, but not everything. It also helps that I can just specify that I `use crate::bar::stuff` and then just call it `stuff` (because the bar context is implicit in my use case) instead of having to always use `bar_stuff` because it's all in the context of `crate` with no specialization. And IMHO it's what gets you closest to the optimal. It's also how I find myself organizing my stuff. I put boxes within boxes, which contain specific things I only need sometimes, that way I can pull the sub-box out and leave the other ones alone.
If you understand german i can highly recommend this video series: https://www.youtube.com/watch?v=lQ36K1htRDY&amp;list=PL0Ur-09iGhpwMbNiVTBeHmIjs0GuIXhNg There is also a concomitantly git repositry with some exercises.
Choice of backend is unlikely to make a difference. Our response times are already &lt;100ms on all backends (&lt;30ms on most, only ones higher are search when `per_page` is set to the max, and `reverse_dependencies` [which will always be slow](https://github.com/rust-lang/crates.io/blob/0bff27aac914114469d263de8d19202cca4e4e70/src/models/krate_reverse_dependencies.sql)). Dumping ember would definitely make us feel faster, but switching to server side rendering is a massive undertaking. If nothing else it'd have to start with removing some of our unnecessary usage of JS (e.g. the dropdowns for docs and sign in could be done with CSS but are JS instead). It's not a small task, and not one we're even sure we want to make. See https://github.com/rust-lang/crates.io/pull/1173#issuecomment-353228688 for the official statement (not one I personally agree with, but it's the current stance)
Cool! Then you'll surely appreciate stuff like https://docs.rs/bcrypt and https://docs.rs/askama :)
Thanks man. Appreciate the tips
crates.io handles search in &lt;50ms without Elasticsearch.
One paper I just came across that you might find interesting: [Reliably Composable Language Extensions](http://www-users.cs.umn.edu/~tedinski/papers/kaminski-phd.pdf), a PhD dissertation by Ted Kaminski. (Be warned, it's 316 pages of really neat stuff.) In essence, it lays out a systematic approach for defining language extensions such that they can be applied to a base language without prior coordination on the part of extension authors or the base language authors. It enforces various nice properties, such as non-interference between extensions, and is explicitly designed to allow extensions to perform domain-specific analyses in order to produce good code. In the end, the author constructs a language (and compiler) [`AbleC`](https://github.com/melt-umn/ableC), which makes C an extensible base language in this sense.
Sure, but on the opposite I'd rather have the scientific notation as a default and have to explicitely state "I know this value is probably wrong but I 'm ok with it" for the special cases.
ok, but this is just saying 'Woah dude, why would we bother implementing tooling that would be useful no matter what api came our way when we could instead lead a cultural revolution in api design that will go nowhere?' You are asking api designers to fully anticipate the needs of the users which can be pretty hard. Obviously it's a worthy goal but it's not realistic to expect all library authors to follow the same standards. In fact I can guarantee it will never happen.
I really don't think that is a metric that would be useful. It encourages things like ternaries and putting everything in a single function since pulling one line into a well named function adds 4 LOC
There's std::ptr::{read_volatile, write_volatile}, but I suspect the semantics you want require atomics. Volatiles in C (and I assume the rust ptr::*_volatile), only enforce that the memory op occurs and it's ordered relative to other volatile memory operations, but not other code, (The order parameter on the Atomic calls is for that). Volatiles are only really appropriate for doing MMIO. I believe but it is possible with data dependencies, but modern optimizing compilers are very aggressive, so it's hard to be sure it will keep working, though a spinlock is pretty simple. If you're just reading the value then you maybe able to to use the AtomicUsize that overlaps the u8 you care about, and just mask out the other pieces.
So I guess it just needs `str::from_utf8_unchecked` to be made const.
It's PostgreSQL's full text search IIRC, which is also an popular way to build search.
Is the number you provided (&lt;30ms on most) include ember.js render time?
Gzipped source size has been an interesting metric for other benchmarking projects. Of course it's also difficult to decide whether library/stdlib/VM code should count when comparing across langs, but it might be a start.
That is server response time. I cannot measure anything else objectively, as it is subject to user bandwidth and CPU speeds
Correct.
Gzipped source seems like it has the same problems, perhaps lessened? do lots of stuff is still smaller gzipped than fn expressive_name() { do lots of stuff } later... expressive_name()
Is there a reason that this front-end couldn't make calls to crates.io's search endpoint? It's not magic, just returns some JSON: https://cl.ly/1C0y0q0G061c/Screen%20Shot%202018-06-07%20at%205.26.58%20PM.png
!! Fascinating
The 'next page' links go to crates.io. a bug?
Thanks for the pointers. As for immutable data structures, I think the book prefers functional lists and maps. Or maybe that's just the way I've interpreted it (and baggage I'm carrying over from OCaml). Mostly I think it's about destructuring lists recursively and being able to restore state as you back up levels from recursion. Particularly for implementing things like frame state and lexical scoping.
Oh yeah, absolutely. To clarify, I didn't mean "you should add a gzipped source metric" but rather "if you are going to add some sort of program size metric, at least try to make it saner."
You need something like [futures-cpupool](https://crates.rs/crates/futures-cpupool) to execute them on one or more background threads if you don't want them to block the main thread.
Btw, you can totally learn Rust just by reading the official book, reading others' code on GitHub and asking questions on IRC, like people did back in 2015 before there was any video material.. :)
Hasn't that been deprecated by tokio-threadpool?
OK, I've started to take a better look at it and there's one UI design no-no that jumps out at me: I wouldn't use tabs for "About", "API Reference", and "GitHub" because they imply that they'll remain when you click one of them. (ie. They imply that you won't need to click Back to pick another tab) 
Quite possibly. I haven't been actively using futures in Rust and had forgotten the crate names, so I just did a quick search for an example.
Well that's fantastic, I thought I was missing some fundamental aspect of how they worked, but good it turns out I wasn't imagining things.
I don't really think counting from the 1.0 release is fair either. Sure the stability it brought was necessary for adoption and "serious" usage, but it's not like the language wasn't around or didn't have releases before that. Personally I have been doing hobby projects in Rust since 2013, and while it was still very much in flux back than, it was fundamentally a usable language with the same guarantees it provides today.
At my job I'm using rocket, diesel and postgres, and tarpc for RPC. For a personal project I use actix (websockets). I recommend all of the above, also yew if you want to do full-stack Rust :)
&gt; There's a reason why people say Java dev is impossible without an IDE, whereas C dev is fine with something lightweight like Vim - because the culture of java is insanely nested packages with super long names, and you just can't develop reasonably without having an editor fill in what you can't remember. Again, that's a subjective thing. I agree that you need an IDE for Java and don't need one for C... but I use Rust perfectly well in Vim without RLS. (Probably because I started on Python, which has an approach to `import` paths similar to Rust and generally short function and class names, also similar to Rust.)
[combine](https://crates.io/crates/combine) is (partially at least) this -- it's a parser combinator library with functions instead of macros.
How do you handle scaling around off-peak hours? Take the system down, bring up a cheaper one? I feel like this ignores the cost of running for peak load 24/7.
That’s good to know. I have been super impressed with the ecosystem of crates too. Such nice build tools 
LoC could be gamed, sure. But that doesn't mean it isn't valuable. Code has to be merged into techempower, and if it is artificially written to optimize LoC that could be pushed back/requested to be refactored to be more idiomatic. My thinking is that you can only push the numbers so far anyway... they'd nevertheless give a interesting heuristic. I am not saying they should be something to sort by... and the benchmarks should not be about optimizing LoC, but IMO it'd be an interesting thing to have in those tables when comparing perf.
These are great: https://www.youtube.com/playlist?list=PLJbE2Yu2zumDF6BX6_RdPisRVHgzV02NW https://www.youtube.com/playlist?list=PLJbE2Yu2zumDD5vy2BuSHvFZU0a6RDmgb
The state-of-the-art on practical Strassen's implementation can be found in the paper "Strassen's Algorithm Reloaded" and a number of followup papers. Actually: Strassen's algorithm is already practical for small matrices. https://www.cs.utexas.edu/~jianyu/papers/sc16.pdf Enjoy!
Oh wow. I will take a look at that. Thanks.
How is the number stored in the file?
I'm assumong you're asking about Latex Support. AFAIK, there are no plans at the moment to do this. Super and Sub script should be available using html tags though. I'm gonna investigate how hard it's gonna be to integrate MathJax into rustdoc. I'll edit this comment with what I find.
Please don't encourage this kind of thing.
So after investigating, I found that due to being able to return \`impl Parser\`, this is very much what I am looking for. I notice that its design doesn't have separate traits for Complete and Partial parsers, as they state: "If partial parsing is not supported this can be set to \`()\`" for the \`PartialState\`. They could implement \`Partial\` for \`Complete\` and then have a way of expressing that at the type level, but other than that this is almost exactly what I was hoping for. I will definitely be giving it a spin. My only gripe is how (in their example) you have to use a 'I::Error: ParseError\&lt;I::Item, I::Range, I::Position\&gt;' bound on your combinator functions. That is a bit ugly, but I don't know if its possible to get rid of it currently (it cites rust\-lang/rust#24159). I am also not sure to what extent they utilize closures to build parsers, but I guess I will find out soon enough. I will switch a parser I was working on over to combine to see whether nom or combine gives better error, which one is faster, and which provides the best abstractions for streams vs complete data (my guess is combine will win there).
Do note that futures are intended to be multiplexed on a single thread by spawning them on an executor. You don't need a separate thread to drive each future; you don't even need a thread pool!
I always try to group tightly coupled things together so structs directly above their impls. If your structs are loosely coupled, split them into separate files.
That can be true even if the maintainer doesn’t change.
I don't know if mentioning users work in edits, so /u/steveklabnik should be able to tell you more about this.
I don't know if mentioning users work in edits, so /u/steveklabnik should be able to tell you more about this.
Clippy on stable is important because I can't use it. If you check nightly broke clippy in june 2, there is a patch but a bug in crates.io makes impossible to update it. It's known, sometimes it breaks, but if you can't even install (I tried older versions) how can you use it?
Scaling for performance and scaling for reliability are two different things. This paper only discusses scaling for performance.
Often you can just try to compile without the correct import and it will suggest the correct import, and then you just copy paste from the error msg.
You could read an `AtomicUSize` and cast the usize to u8, until `AtomicU8` is stable..
&gt; I'd much rather see another programming language take over Solidity's lead in smart contract programming. One candidate to me is Rust. I disagree that Rust is a good language for smart contract programming, because I want functional correctness, formal verification, lack of logic bugs for smart contracts. Rust's correctness/safety properties are somewhat different and do not address functional/logic concerns. But he is on the Qtum team and I am not, so what do I know.
I understand that. I was referring to the idea that every company building distributed systems for workloads that may not justify it still have a completely valid reason for doing so - reliability.
If I'm using a struct + impl like a class, I treat them like a class and put them together. It's generally types, structs, then structs + impls, then free functions, then the most major impl or an entry function. When it becomes more complicated I'll also split into separate files. I'm actually really eager for features like `non_modrs_mods`. Splitting a file into a module or a number of smaller files always felt cumbersome before, which increased the friction of doing these types of refactors. A coming module imports refactor will fix the issue of having to copy over imports. I'm relying a lot on nightly-only features to organize my code at the moment, but it makes it feel much easier when editing a codebase without an IDE.
Thanks for investigating. It doesn't necessarily have to be LaTeX; I tried putting in MathML code, but that wouldn't render. Forking is too non-standard for me. I want to stick to the main stream and have things to work on &lt;https://docs.rs/&gt; and everything. It's interesting to know that it's possible, nonetheless.
flirf.rs is a decoder of the [FLIF ("Free Lossless Image Format")](https://en.wikipedia.org/wiki/Free_Lossless_Image_Format) image format, written in Rust.
**Free Lossless Image Format** Free Lossless Image Format (FLIF) is a lossless image format (current version FLIF16 specification) claiming to outperform PNG, lossless WebP, lossless BPG and lossless JPEG 2000 in terms of compression ratio on a variety of inputs. FLIF supports a form of progressive interlacing (a generalization of the Adam7 algorithm), which means that any partial download of a compressed file can be used as a reasonable lossy encoding of the entire image. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
You're now running a single server. (Probably two for redundancy.) I feel like you can afford to run this overnight.
Thank you bro 
I work with the people behind SGLR. If anyone has questions about it I can try to answer it myself or send it to someone more knowledgeable in my group :)
Both names are in wide use.
My setup right now is emacs with rust-mode, cargo.el, and racer. Works pretty well, has nice auto completion and jump-to-definition. Sorry for the lack of links, I’m on mobile.
I followed your idea and implemented a generalised framework. Previously I implemented something else with out generics where I had a ownership problem, so I thought to change the design. Even in the new design I had same problem, and I am stuck for 15 days. I want to get mutable references of the attributes of struct, which is not possible. What is the idiomatic way of solving this problem? Updated design is at https://play.rust-lang.org/?gist=2d3726c253d66839510e0e723f48b657&amp;version=stable&amp;mode=debug
Cloud is just someone else's server that you pay extra for(to ward off the load of managing power,cooling, physical operations etc). I believe you can get such beefy servers even on public IAAS. So, "costs a lot" is an economic choice you make as a company. There are many co-located leased space providers that would turn out cheaper in the long run than the public IAAS. But, that's another discussion. Issue of availability: Of course scaling for availability is different from building out a distributed system. You'll have to take availability needs into account while you do capacity planning. I was mostly ranting about unnecessary complexity &amp; overhead from distribution for distribution sake. Of course there are legitimate cases where distributed systems are necessary. 
I mean it's a potential one line diff and a config option for doxidize in the future, but let's see what Steve thinks.
&gt; I think doc.rs uses Rustdoc or some variation, but that has been archived, so yeah... Hang on a second, does that mean that docs.rs does not generate the documentation in the same way as it is generated when I run `cargo doc` myself? Should I not expect the documentation to look the same on docs.rs as it does on my computer?
nom uses macros only as a syntactic sugar. It is obviously possible to use functions directly (I did this many times). A parser is a function. Macros help to pass the input most of the time, or to deal with errors, but nothing more. There is not a lot of magic behind that, except when some code generation is required. I also know this is on the nom’s roadmap to remove more macros. I don’t have details about that, nor I know the motivations behind that. cc /u/Geal
I see your point about the first quote, but the second quote simply is a reason not to use the alternative (tokio-cpupool) crate.
https://www.malt.fr/, it's a French one. I doubt it helps you, unfortunately.
I would advice to add [REDME.md](https://REDME.md) to the project, with examples. It always attracts attention :)
Just do not borrow twice :) https://play.rust-lang.org/?gist=8c3ff65afc879d3c93dd874c805300d0&amp;version=stable&amp;mode=debug
Would you be able to make a blog post somewhere, to sum up what you've learned of this experience and have some pros and cons of both libraries ? I would definitely read that !
Could a slice store its size inside the pointer itself?
this is a clever solution, and perhaps intuitive to some, but as a newcomer to rust I would probably never think of this. should I be training myself to think of solutions in this way, or is there a simpler way to do it?
Even simpler way is to borrow **once** whole structure but its members individually.
This could be too generic for particular purposes. Something like this could help too. fn advance(&amp;mut self, dx: f32, dy: f32, dfx: f32) { for i in 0..self.fx.len() { self.x[i] += dx; self.y[i] += dy; self.fx[i] += dfx; } } 
Yep!
Ah, cool. Thanks!
It should generate the same output, When I said "some variation" I was talking about cosmetic differences with ordering of things on the page. Now that you mention it, I'm not so sure about the markdown standard each one uses any more...
Yes and no. Being able to have two mutable references to a structures fields like this is something that's being actively worked on I believe. But in saying that, the style of thinking that led to their solution is something that will help your programming in general since a struct should have some rule that needs to be consistent across it's state. If you just want to pass the date together I'd just swap the struct for a tuple and destructure it with pattern matching. 
&gt; Remember, what you're competing against is me typing actix::Registry when I want a Registry, then actix::X when I want an X. No docs needed. This still takes a lot of time dude, more than the docs suggestion probably! (especially since you're fishing out the import errors from all the other errors / warnings in the compile) What if I said to you - 'Don't bother putting trait bounds or lifetimes in, just let the compiler give you suggestions' - it'd take AGES to get a change running
will be very exciting to see hyper or actix-web running on redox ;)
Wait, no, python's imports are a world apart from rust In python, you write something like `import cv2`, then just use that for everything in the library. In rust, you'd be doing something like use cv2::image::{Image, imshow}; use cv2::filter::{Blur, apply}; use cv2::filter::roi::Roi; or some bullshit, where every time you need a new type or function you'd have to re-import at the top. I'm not saying that a module system is bad, I'm saying tha crates with nested module based APIs (like a TONNE of crates have right now) are really frustrating to use. &gt; In fact, until now, I was under the impression that I was doing a pretty good job of understanding and defending the "no IDE needed" viewpoint when it came up in RFCs. I'm not quite sure I understand this, but the point is that some guy was suggesting this wasn't a culture problem, but was instead a tooling problem - if we let it become a tooling problem instead of changing how we design APIs, then we WILL need IDEs, because we'll just become like java.
I mean, right now I'd argue they're not really bothering to anticipate the needs of users with regards to what they export and where - currently, we have a system where people are just exporting modules which leads to far slower development. On the other hand, they could just re-export everything in a `prelude` module (and I mean everything, not what people do right now and just export a couple 'common' things, however they define that), and development would be much easier. Imagine the perfect world where I could stick a `use crate::prelude as crate;` at the top, and whenever I needed a symbol I'd just use `crate::symbol`, regardless of where I was in the file. No need to check what I'd already imported, no need to figure out what module I was already in. #THEDREAM
Yeah, this has been raised before, I think a nice compromise is to have the module system exposed as normal for documentation, with a `prelude` module in the crate that has everything exposed. That way I can just do `use crate::prelude as crate;` at the top and when I'm writing code just write `crate::X` when I need X, and never have to worry that `X` is not in the right module. In this system, I don't HAVE to worry about whether `X` is 'related to foo', or 'related to bar'. Remember, in your `crate::foo` example, as a developer I don't even know that the crate HAS a `foo` module. I don't want to have to remember this! How do I know that `foo::Foo` is where `Foo` is, and not just in the root, like `crate::Foo`?? In the actix example, there's a type called `Registry`. Guess which module this is in! I guarantee that's a google of the docs, because `Registry` is actually in the `registry` module. How am I meant to know that when I'm programming, given that I'm also using 10+ other crates?
A libOS seems to be a [library operating system](https://lwn.net/Articles/637658/) - operating system functionality provided as a library.
Canonical already did it (I am not sure if it is Debian\-compatible though).
Well, you can do some stuff with [include](https://doc.rust-lang.org/std/macro.include.html) and [env](https://doc.rust-lang.org/std/macro.env.html) macros. To do more complicated things, you can use [build scripts](https://doc.rust-lang.org/cargo/reference/build-scripts.html).
Excellent work. I am hopeful that my retrospective won't be too far away!
This is the best I could do. https://play.rust-lang.org/?gist=2f81a67ab55d950428ee0a50a821fe76&amp;version=undefined&amp;mode=undefined I am very curious what the actual best answer is.
Well, this solution is totally specific to the equation I wrote. In practice I have around 10 variables and I need mutable and immutable references of those. like https://play.rust-lang.org/?gist=dafc5cb5fac16546a38721acee4f9878&amp;version=stable&amp;mode=debug 
What form factor is that?
Actually I want to keep it little robust so that I can add any number of different bodies into my simulation. I changed my problem and added one more equation. Can you generalise your code for this? https://play.rust-lang.org/?gist=dafc5cb5fac16546a38721acee4f9878&amp;version=stable&amp;mode=debug
Wait, this is now my solution https://play.rust-lang.org/?gist=60ee56fcde3d89ea7a4a2709123d4793&amp;version=stable&amp;mode=debug For this example, you can just split up the scope where you have each mutable reference so you never have more than one at a time.
I'm currently using a build script and just compose a string of Rust source code, which I print to a file. I'd prefer something more principled, instead of having it be up to me to adhere to the Rust syntax. I was thinking that I could either use macros (but I don't know enough about them to tell) or find a library that lets me build an abstract syntax tree, and converts it to code for me.
Reliability can be handled by failovers. It doesn't need distributed handling of single actions.
Although I couldn't actually give it a try (Qt currently not installed), this looks promising. Cheers!