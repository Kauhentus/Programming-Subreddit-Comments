I think they just TIL'd the opposite.
ML is [a programming language](https://en.wikipedia.org/wiki/ML_%28programming_language%29) from the early 70s! Rust draws on lots of ideas that it founded, by way of Ocaml and Haskell.
Alright, using `RefCell` solved the second problem. I need to read up some more on RefCell, but if I understood it right is basically moves the borrow checking from compile-time to run-time. The issue was that `foo` can potentially give out a `&amp;mut` to the same object multiple times. I still need help with the first error though. [Here](https://ideone.com/eeCUL1)'s an updated version of the code.
uuuugh that's what I get for posting before breakfast.
I have spent quite some time with Scala, and the linearization of traits has never been a problem. Overuse (IMO) of `implicit` is much more of a problem for me when working with Scala APIs. Figuring out which traits are in scope in Rust has a similar feel.
The most obvious workaround is to wrap it in your own newtype (e.x. `struct MyIpAddr(mio::IpAddr)`) and add the required conversions and implementations to it. This can be tedious, but sometimes necessary.
Yeah, if those language can do it, we should expect Rust to be able to do it at some point. REPLs are the fucking bomb.
Keeping a reference to a value along with the value itself is tricky in Rust. FWIW, there's also https://github.com/Kimundi/owning-ref-rs but it's relatively limited.
Scala actually has fewer reserved keywords and sigils than Rust does, it's just very extensible. https://doc.rust-lang.org/beta/grammar.html#keywords http://www.scala-lang.org/docu/files/ScalaReference.pdf section 1.1 It's probably a more consistent core language as well. At the talk, one of the attendees commented that he (unfortunately) would be reluctant to teach Rust in his college course because of things like the difference between signifying a move into a closure versus a move into a function call. 
All of that stuff is direct FFI, but I'm sure somebody will make a safe wrapper some day :D
Really? Coming from Rust you think Scala syntax is "noisy"? My first complaint as a Scala dev when I started learning Rust was how noisy and littered with symbols Rust syntax can be. Both languages have their noise. At least it's not Haskell...
You could definitely speed it up by inputting the key outside the loop, and reverting back to that state instead of resetting entirely. You can also iterate over only u64s that can be converted to [u8; 8] instead of doing to_string, and could even stagger inner loops where you input the `i` bytes one at a time,reverting back to the last inputted byte if the hash doesn't match.
Scala is the C++ (the very worst subset of it) of the JVM world.
Thank you for all your answers. Will look into that.
It's also one of the most powerful features and Scala would be a much weaker language without them. Implicit conversions are discouraged (even by Martin himself) and implicit parameters should mostly be used for as a form of type classes (see for example [Scalaz](https://github.com/scalaz/scalaz)). Rust's trait implementations can be used to get some of the functionality of implicit parameters/type classes, but is not as powerful as Scala's solution. Regarding linearization of super types, this is something that is worked upon in the new Scala compiler called [Dotty](https://github.com/lampepfl/dotty). This will have full support for union and intersection types making the Scala type system both simpler and more powerful.
I'm aware of rusti, and I mentioned it in the talk. I think a good built-in REPL is part of table stakes for a modern language. I don't think rusti is there yet, and based on the rusti readme and the repl rfc, I think the author would probably agree with me.
Surely someone has considered the C++-style `using` solution to this. What's the status/consensus on that? Supposing I'm using crate1 and crate2, both of which implement Serialize for mio::IpAddr, I could do the following to specify which implementation I want: using crate1::Serialize for mio::IpAddr
Yeah, it's definitely prone to some subtle bugs, that may vary in devastation. It has to be done in a way that avoids breakage from dependencies, which is the really hard part. That's why I thought about having a warning when things could go bad. The implementation could also be `unsafe` for extra discouragement. I haven't worked all the logic out, though, so it may not be worth it at all.
Having things in the AUR (like an up to date version of racer and the rust source) is really nice for staying up to date without futzing with versions myself. Just don't tell IT that I'm running a rolling release at work.
Hum, not really: $ cargo install cargo-extras Updating registry `https://github.com/rust-lang/crates.io-index` Downloading cargo-extras v0.2.2 binary `cargo-check` already exists in destination as part of `cargo-extras v0.1.0` In order to upgrade, you have to manually uninstall it before.
&gt; Don't ever program lisp then, if you can't handle a closing paren 2 characters away from an opening paren. I try to avoid it for the same reason. Also I was presenting my own experience datapoint. Things like `))` or `||` could cause your eye to gloss over. I wonder if there was a study comparing readability of parenthesized to non-parenthesized text.
More documentation is always helpful, but probably just a list of which features from Akka are supported would suffice to help newcomers. :-) Thanks for that example. One way to document things would be to provide RobotS versions of the Akka examples from, say, https://www.coursera.org/course/reactive. I am tempted to do this if I can free up some time.
Data-oriented design! Instead of having a lot of entities {x,y,vx,vy,m}, you instead have an array or `Vec` for each member. Should allow the compiler to do more auto-vectorization. When you have done this, you can set rustc's `--target` parameter to better match your CPU. If your CPU supports it, it should enable the compiler to use the AVX instructions which should provide an even better speedup. But the resulting binary won't be as portable.
The Rust list of reserved keywords doesn't include the symbols that Rust effectively reserves. The Scala list of reserved keywords **does** include reserved symbols, and is still shorter. So, yes, the count is unfair, but not in the direction you suggest. Edit - yeah, I agree it's not the best comparison, but it's at least a factual comparison, as opposed to "Blah I don't like the way this one language looked for the couple hours I spent on it"
Seconded. When the benchmarksgame entry was changed to do all operations apart from each other (which needed added intermediate storage, but allowed LLVM to easily vectorize), performance rose about 25% (disclaimer: on my machine).
I suppose you could send PRs to the upstream crates to fix this. Perhaps one way to do this would be to make the use of serde instead of rustc-serialize a [feature](http://doc.crates.io/manifest.html#the-[features]-section) in the upstream crate, so that both are supported.
It's complicated :x
Sure, for huge functions or in cases where I have to pass down parameters deeply through many calls structs have their place. For smaller functions however they are overkill and in any case more cumbersome for users of an API. Also much more boilerplate.
Indeed. In this case, you miss the whole stack-allocation-thingy, which is a Bad Thing for a systems language. Dependent types would be awesome, but I'm not sure how feasable it is. If the `const` prefix turns out to be necessary, _maybe_ we could use `let`for runtime values? And add runtime checks (like bound checking) when the compiler cannot prove that an invariant is filled statically? Someone proposed to use # instead of `let` in a constant's name. Perhaps we could use @ for runtime type constants and conditions. I think Liquid Haskell uses that symbol too. Then, @ and # would combine like the following #[some_compile_time_thing] @[index &lt; U::@len] fn get_stuff&lt;T, U: Array&lt;T, @len:int&gt;&gt;(arr: U, index: usize) -&gt; &amp;T; That is, # means macro &amp; other static goodies. @ may have a runtime cost if the compile cannot prove everything's right.
I [recently asked a related question](https://www.reddit.com/r/rust/comments/3v2k37/can_a_library_support_both_rustcserialize_and/?ref=share&amp;ref_source=link) about dealing with both `rustc-serialize` and `serde` from a library-author's perspective. There are also some good comments in [this recent post](https://www.reddit.com/r/rust/comments/3v4ktz/differences_between_serde_and_rustc_serialize/?ref=share&amp;ref_source=link). In short, I believe there's no good workaround for dealing with the `rustc-serialize` and `serde` dichotomy right now. For [my library](https://github.com/couchdb-rs/couchdb), I've decided to punt and force applications using my library to use `serde` because “it' the future.” But this is exactly the problem you're stuck with. Sorry.
Hence why I recommended a heuristic so that such long names would still trigger the warning. Apparently as others said though I'm mistaken about the warning being triggered in this case anyway. I suppose I should file an issue with the `crypto` crate instead.
Any ideas where to start? Should I be thinking about asking the WiFi hardware drivers directly or is there some higher level API to be accessed? 
&gt; At least it's not Haskell... What does this mean? Haskell syntax is much cleaner than both Rust's and Scala's. And I'm primarily a Scala dev.
Probably what Im looking for is OS X, Linux/Unix and Windows kernel APIs to get this information, anyone that has said information will be welcome
It's the same one, and they haven't announced who exactly is getting the job. Applying again wouldn't make sense.
For certain things like this, I wonder if it would make sense for Rust to be somewhat opinionated about serialization. It would be nice if there was a unified API that both `rustc-serialize`, `serde`, and `future-serialize` could implement, and the end user could decide which serialization library to be used to satisfy calls to the unified API. `serde` is great now, but I would hate to end up in a situation where there are better technologies available in the future, but nobody uses them because `serde` is too entrenched. Either that or the problem described in this post just happens forever.
&gt; Is there a conceptional issue with that? Does this count as violating aliasing rules or something? Yes, definitely. If you have a `&amp;mut World`, you can now get two `&amp;mut Sphere` for the same sphere, one by accessing spheres and one by accessing foo. And that's part of Rust's memory safe foundation, that you can only have one `&amp;mut` to the same object. &gt; benefit of continuous memory for the Spheres :( Ok, I didn't know this was important to you. Here's how I would reason about your problem: If you need more than one reference to something, those references need to be immutable (i e `&amp;`, not `&amp;mut`). That usually means that you cannot change the object, but both `Cell` and `RefCell`s are ways around that, where `Cell` is usually preferred because of less overhead. For you, that means, that if the only fields you need to change in a Sphere are `Copy` values such as e g integers and floats, then you can just mark those individual fields as `Cell&lt;i32&gt;` instead of `i32`. And after that, just get rid of everything `&amp;mut` and replace it with `&amp;`, this includes `do_something(&amp;self)` and `Foo&lt;Cell&lt;&amp; Collider&gt;&gt;`. 
Using pyenv and pip after cargo and multirust.... It hurts me. Rust tools are really impressive.
I don't know, it was just an idea. It just *really*, *really* irks me to see `Md5` and `Sha1`. As a crypto-enthusiast, I've never named either of those algorithms that way, so it just comes across as unnatural. Hence why I finally settled on just eventually filing an issue with the crate itself instead of pursuing the heuristic idea.
Its interesting to see this come up. Scala has an even worse problem with this, apparently, and the proposal to add a standard Json AST for library use generated quite a bit of heat: https://github.com/scala/slip/pull/28 If serde is really becoming the standard, then this is in theory just interim pain and will hopefully not last long. Famous last words. 
Why? What are you using them for?
That was Ocaml and F#. :) Haskell and Standard ML use a "case" and "of" keyword, but my point was that pattern matching is an old idea. Rust draws a lot of ideas from a lot of languages, although I think it's noteworthy that Rust was originally bootstrapped in Ocaml.
Probably thinking of stuff like the nightmare that is [the lens library](https://hackage.haskell.org/package/lens-4.13/docs/Control-Lens-Operators.html) Good ol' `&lt;//~`, `&lt;&lt;&lt;&gt;~` and `^@?!` operators.
Are you still interested in this? I put something together today. It's not exactly rich in features, but if you'd be willing to add some features, it could be great really fast. It currently only has a sequential implementation but it's built with parallellism in mind. I'm going to be very busy in the coming months, but if you wish I could upload it to Github and you could take a look at it?
On linux, you'd probably read a few files out of `/sys/class/net/$dev/` but you'd have to do some research to find the correct files.
&gt; I'm using serde, as it's the preferred library for new and future serialization work. Maybe so for string based formats, not so for binary. Where CapnProto and my own *[incomplete]* [lense](https://github.com/trustless-fox/rs-lense) project which are faster and better.
Yes, this is true. In my case I'm specifically working with data, so a Thrift/Protobuf/Cap'n Proto-style system doesn't come into play.
Ahh yes, that makes sense. Well, these kind of problems are exactly what Rust was designed to detect, albeit I could not interpret the error messages. I tried with a RefCell, but I can't use a `Box&lt;[RefCell&lt;Sphere&gt;]&gt;` and copy the elements to a `RefCell&lt;Collider&gt;` for the broad phase (`foo`). An enum specifying the array and an index into it seems like the best way to go about it then.
For Linux, you could take a look at what the `iw` utility does: https://git.kernel.org/cgit/linux/kernel/git/jberg/iw.git/tree/ Grep for either `signal` or `bitrate`, depending on what exactly you're looking for. It looks like it won't be pretty if you want to get it directly from the kernel - it might be easier to just parse the output of `iw` or `iwconfig`, depending on what's available on the system. There might be something useful in `/proc` you could use too.
One thing I thought of (about which I commented earlier) is that we could use the `#` sigil to mean _static_ (static value and static annotations), so we have: #[my_annotation(args)] MyType&lt;#my_const_arg&gt; And `@` to create runtime verifications and runtime associated values such as `len` on a `Vec`, which would give: @[assert(index &lt; @len)] fn get&lt;V: Vec&lt;T, @len&gt;&gt;(...) -&gt; ...
You can implement `Deref&lt;Target=InnerType&gt;` on your newtype to get a hacky sort of inheritance. It lets you call any method defined on `InnerType` on your wrapper, as well as automatically coerce references.
 self.0.METHOD()
Try `self.0.foo()`.
Yep. Thanks. So I have to use `X.0.foo()` anytime I want to call a method on it? I'm sure there's a good reason for it, but that's kinda annoying.
What is the difference between a decimal, a double, a float and a single? is it just the amount of data that can be stored in each? 128, 32, 64, 16 bits, respectively?
Yeah, but just having seen the capitalization of `XMLHttpRequest` I want to never have anyone make this kind of a mistake again
double is double float, not used in Rust Rust uses `f64` and `f32` both of which are base 2 Decimal numbers are base 10 and guarantee lossless representation numbers people are used to like 0.2 (there is no way to accurately represent 1/5 in binary, it's a repeating decimal fraction)
Previous discussion: https://www.reddit.com/r/rust/comments/3uwtz6/using_wayland_from_rust_first_steps/
Requiring an import for each function argument is about as objectively unergonomic as it gets. And definitely not the same number of keystrokes.
Another option: if mio is interested in having its data types be serializable, you could submit a pull request. :-)
It's well-known as amazingly elegant and entirely incomprehensible. To this day, I do not know how it works. Something about describing holes in the data structure to do multiple updates at once to avoid creating 7 billion copies (because it's functional and you aren't mutating).
I also came from a C++ background but I've had extensive experience with OCaml. Rust is pretty much what I've been looking for. For lack of better words, for me it's a C++ alternative with ML syntax. Although I've been playing with Rust since pre-1.0, I've finally gotten to the point where I'm moving production systems to Rust. They're real programs that make real money.
Please do. I've ported a machine code based LGP system to Rust. Although it's closed source, I'd love to put some of the features in a public project.
Yeah, but that's going through an intermediate library not written in Rust. The cooler thing to do is write a Rust library that has a Direct2D backend.
Your multithreaded rust version doesn't actually start from zero. See the data.i is incremented before anything is even computed that should be the line above.
See [this](http://rustbyexample.com/std_misc/process.html) ([here](http://is.gd/KdCqWg) on play.rust-lang.org)
Yeaahs, you're right! Starting 1 cycle ahead.. off by 1 errors always find a way, thanks.
That's the problem with Scala. There's a lot of guns pointing to your own feet.
On `let x; {... x = {}; ...}` and `let x = {... let tmp = {}; ... tmp };` I'm actually really curious about what happens under the hood in the first case. What is `x` to the compiler before the assignment and how is its scope extended beyond its initialization (and type binding) point in the smaller scope?
I'm working on porting more rust to the [gba][1]. This week I wrote an example using the input system. Feedback is welcome, I'm not really sure what's the best way to abstract all this hardware stuff. [1]: http://csclub.uwaterloo.ca/~tbelaire/blog/
Linearization is how you solve the diamond problem in Scala. Remember that Scala's traits also include field inheritance.
There's nothing unusual about the scope or type. borrowck is responsible for checking that variables get initialized properly, see [this](https://github.com/rust-lang/rust/tree/master/src/librustc_borrowck/borrowck#moves-and-initialization).
It's more idiomatic in Haskell to work with immutable data. While in Rust I could write r.foo.bar.baz = new_value; arbitrarily reaching into nested structs (with public fields), in Haskell the equivalent code looks like r { foo = Foo { bar = Bar { baz = newValue } } } which is not so nice. Not to mention that record updates are not function applications (you have to write your own), so yes, it really isn't composable in the way that Haskell users might define composable. So idiomatically you'd write functions that do the updates and compose those... which is where lenses come in (auto-generating those functions with TH). 
I didn't expect someone finding my blog post by an other mean than the reddit thread I opened 2 weeks ago! :O
By all means, feel free to hack around with it! It would really help me improve my crates to get more feedback from users. :)
This article was badly written, and it looks like very little real research was done to confirm facts or even ask if this is something Mozilla could even hope to do. (I posted my criticism of the article itself in a comment on the article itself)
Why would you want Denial Of Service?
Also seen in C++ as the 'Recursively Recurring Template Pattern'. Just to give it another name for future searchers :)
It's very easy but occasionally you have to [jump through a few hoops](https://play.rust-lang.org/?gist=ef220d80540b08a94fbb&amp;version=stable), depending on what you want to do (your code doesn't look like it encounters any of those though).
Sure, that takes you closer, but it's semantically strange.
I'm relatively new to Rust, so I'm not sure if this is of any use to you, but [here](https://github.com/m-decoster/RsGenetic) it is. I just wish I had more time to work on it at the moment.
I'm relatively new to Rust, so I'm not sure if this is of any use to you, but [here](https://github.com/m-decoster/RsGenetic) it is. I just wish I had more time to work on it at the moment.
Hey, I did the nbody solution! But yeah veedrac and llogiq did pretty much all others 
Excellent post with great timing, I've been struggling with Rusts type system implementing something that is (usually) inheritance heavy and this lit a few sparks 
&gt; Pretty much every platform that has hardware for a "double" has it as 64-bits (and similarly 32 for single/float). So yes, you are technically correct, but in practice it is not a concern. Don't compilers often fail to configure the x87 FPU for strict double precision, performing double computations on 80-bit EPD values within said FPU?
I'm guessing your Ryan Scheel? If not sorry for hijacking your comment. It's very naive to think Mozilla's goal isn't to make money. There's a massive difference between wanting to make money and wanting to make profit. Mozilla is non profit but they have employees to pay and lights to keep switched on. If they didn't want to make money they wouldn't have made google their home page and yahoo after. Mozilla can't embrace and further an open web if they're bankrupt and if they can make money by promoting Rust then they should do that and if Mozilla did what the article suggested it'd prove to everyone that Rust isn't dead on arrival and it will supported until the end the end of time. I think the Servo/Gecko part of the article was badly phrased and is more of a philosophical question than anything. If Gecko components are rewritten in Rust and incorporated into Servo then Gecko is replaced piece by piece by the Servo components, is it still Gecko or is it Servo? It doesn't particularly matter what it's called tbh, as long as it's fast and more memory safe. 
I would suspect the i vs j conditional in the loop can be a hurdle. Duplicate the loop and do iterations over ..j and j+1..; use iterators here
Yes, although it does take a while to recompile all my crates after that. Any better fix?
The types themselves are 64 bits in size, even if calculations aren't performed with 64 bit "precision"/according to IEEE754-2008.
Servo already lost that cause when they added their first line of code. That said, it can still be *sufficiently* secure for many applications.
Nah, it spent a good chunk of its time being unable to compile, thanks to pre 1.0 Rust ;)
All LLVM needs to know for immutability is that live stack slots' contents don't change along any control flow path. Before initialization (in a flow sensitive sense), the stack slots aren't live, so there's no concept of immutable or mutable for them. So they aren't ever actually mutable in any sense that matters to the backend.
Well, I don't know the optimizer works, but there's no difference (that I see) in the LLVM IR between mutable and immutable variables. All variables are mutable in your sense since there is always a period in between growing the stack to hold them and construction of the object in the stack space, but perhaps someone else can give you more details.
Ergonomics. People usually remember those traits (and may possibly use them already), so there's no need to look further and no risk of naming collision.
I don't mean to put you in a *box*! I was just noticing all the *references* that you were making to it, and I wanted to give some special attention to the *constant* effort that you've put into this community.
See also /u/comex's [type-system Brainfuck](https://www.reddit.com/r/rust/comments/2o6yp8/brainfck_in_rusts_type_system_aka_type_system_is/) from a year ago. 
I doubt there's a better fix - the old version was no longer usable, you needed a new version. Only way I can see to fix the issue is to recompile after wiping the old version.
Yep. Such is the curse of no ABI.
&gt; Mozilla is non profit To be clear, Mozilla is two entities: the Corporation and the Foundation. Corporation is for-profit, Foundation is non-profit. Rust, Servo, and all of Research are part of the Corporation.
No that's DoS. 
No, just regular cargo.
You're missing forest from the trees. Issue here is readability not grammar size. Scala could have 30 symbols and it would still be worse than Rust, because in Rust you only get special sigils, like `+=` or `&gt;&gt;` or `||` for operators. In Scala [anything can be an operator*](http://www.scala-graph.org/guides/core-initializing.html). * It's not operator techincally, however in practice it's virtually indistinguishable from what you'd call an operator. In truth it's an infix function name. But then again, Rust could claim that it also doesn't have operators, just syntax sugar for compiler recognized trait. If you look at Rust code `X ~%+# Y` you know you're either inside a macro or a cat has been sitting on your keyboard. If you look at Scala code you - Just. Don't. Know. And trying to figure out what `~%+# ` is a task in itself. 
Thanks. Sounds like good reading. Added to my list. 
Technically the problem says to start from 1, not 0, anyway.
Not on everything, it seems. While it works using GNU on Windows, `cargo-script` doesn't have a target for MSVC and the compilation fails.
Indeed. It's now fixed!
Two questions, does Redox need its own version of Rust? (the one in the submodule), also does it use Cargo to manage dependencies? (such as in the userland apps) edit: actually I found [some Cargo.toml files](https://github.com/redox-os/redox/search?utf8=%E2%9C%93&amp;q=filename%3ACargo.toml&amp;type=Code) but it appears none of them pulls external dependencies. Is Redox self-contained, not depending on anything on crates.io? Is this by design?
Strange. This should only happen when switching compiler versions. Perhaps a bug in cargo or rustc? What version are you on?
Apparently it's built into wayland? Super + S (So winkey + s by default)
`s/Array/List/` ;)
AFAIK, Redox can't use Rust's standard library yet. It can't therefore use external crates that would depend from the stdlib neither.
This is amazing! I remember there was some kind of "standoff" where there was a compiler feature that the Redox needed and a Redox dev said "fine, I'll maintain this patch myself". But I'm not remembering exactly what it was. I'm wondering whether this issue was solved.
I think it's expected that Redox development would be done outside the OS itself. It would be amazing to run rustc itself on Redox though. But I mean, being able to use the stdlib in Redox GUI programs for example.
FWIW, since this heterogenous list is resolved/populated entirely at compile time, random accesses to an element (e.g. the last one) should be possible in O(1) time, i.e. it will have properties of an array.
Thank you for your hard work!
I feel this clarification is itself a bit misleading. Whilst technically correct, since the Corporation is wholly owned by the foundation it is forbidden by regulation to pay shareholders or do pretty much anything else a for-profit corporation would usually do. AIUI, it exists to make it possible to do various financial things that a strict non-profit cannot, however, in the key point of whether it pays shareholders (private or public) vs re-investing all its income into the business, it behaves like a non-profit. Therefore, I think the first intuition - "Mozilla is a non-profit" is more accurate (although less precise) than making a distinction between the Corporation and Foundation. In programming terms, there is a for-profit component of Mozilla, however, the whole aggregate *is* a non-profit.
Just realized I hadn't replied to this, oops. It's fine, thanks for giving me a huge boost in motivation for working on it (haha).
That was quite the RFC/PR walk. :)
Some of this looks oddly familiar... :P Nice post, I wouldn't call it an evil hack though. We're probably not using the type system as it was intended, but we're doing anything dangerous or scary.
A bit late to the party, but I wanted to showcase this library. Here is a multithreaded version that's using Niko's (new? upcoming?) library `rayon` (https://github.com/nikomatsakis/rayon) for instant parallel processing. Single-threaded takes 2.0s, this parallel one 0.7s (4-core i5). A primitive "spawn N threads and join them" solution is about 0.6s, so in this case you pay ~16% overhead for this nicer API. extern crate crypto; extern crate rayon; use crypto::md5::Md5; use crypto::digest::Digest; const INPUT: &amp;'static [u8] = b"yzbqklnj"; const N: usize = 10_000_000; fn check(i: usize) { let mut buf = [0u8; 16]; let mut hash = Md5::new(); hash.input(INPUT); hash.input(format!("{}", i).as_bytes()); hash.result(&amp;mut buf); if buf[0] | buf[1] == 0 { if buf[2] &amp; 0xF0 == 0 { println!("Found 5-zero hash: {}", i); if buf[2] == 0 { println!("Found 6-zero hash: {}", i); } } } } fn check_parallel(from: usize, len: usize) { if len &lt;= 1000 { for v in from..from+len { check(v); } } else { let half = len / 2; rayon::join(|| check_parallel(from, half), || check_parallel(from + half, len - half)); } } fn main() { check_parallel(0, N); } 
I guess you're right about that. I may add it's likely we'll discover the NSA funded / hacked / had or has plans to fund or hack hardware contractors to include backdoors into CPUs and the like. It's all pretty hopeless to be honest; the attack surface is just too unwieldy.
Here are two other Rust projects in Google’s GitHub org. Both are by Raph Levien, and like Hat they are not official Google projects: * https://github.com/google/pulldown-cmark/ * https://github.com/google/font-rs/
Well yes, but they don't seem very much alive unlike this one, which flashed among the trendy ones earlier today: https://github.com/trending?l=rust&amp;since=daily And it compiles with 1.7, no problem.
Yeah, I didn't even told you everything: I'm also red/green colorblind. The is no "right" color, but there are wrong ones, for sure.
I'll try to come up with something then, if it's all it takes.
Some of them should be broken, but not all of them. &gt; my main question * https://github.com/rust-lang/rust/blob/89ec45ced42a37a77ff1b71be5b903eb313a77e7/src/libcollections/str.rs#L330 * https://github.com/rust-lang/rust/blob/c4b16384f101bbe28dc4eec0651a61cb9d5274ac/src/libcore/str/mod.rs#L1618 So the answer is no, it does not. However, you could know without looking at the source: slices never re-allocate, they're pointers.
&gt;However, you could know without looking at the source: slices never re-allocate, they're pointers. I knew this rule of thumb, but I wanted to be sure.
You may click the very link mentioned in the post ("By the moment of writing this article, it is comparable to ..."), [which has updated results.](http://benchmarksgame.alioth.debian.org/u64q/which-programs-are-fastest.html)
It's vectorized, but not well because of bounds checking. Yay. You can figure out how well it optimizes by looking at the generated assembly in playground. Mark the function of interest with `#[inline(never)]`, then click "Release" to actually optimize, then "ASM" to see the assembly output. Ctrl+f your function name, and read the output. Well-vectorized code will generally be a lot of instructions referencing xmm or ymm registers, with no branches except for the two needed for the loop.
Thanks! I think I can see where to locate things now. There is a .section tag which appears at the start of each function? And I see also that for dot there is no panic bounds check stuff but for my sum method there is. I guess this is because I am using the holding value to index into the sum.
The LLVM optimizer is a deeply mysterious beast. Just flail around with your code until it generates assembly you like the shape of. Oh, and profile. That's good too.
I am currently doing profiling for Collenchyma and our related crates. I mainly use `perf` with this this script (see the usage instructions in the README): https://github.com/autumnai/collenchyma/tree/0912715af4a8417e87fedc08583ed54af2ec587c/perf This will both generate the perf dump which you can use with `perf report` and a flamegraph which is usually much more helpful. With the flamegraph I then look out for everything that is taking up a lot of time. I would recommend reading [this post](http://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html) as a intro to flamegraphs. One quick tip: A lot of functions will be inlined by the usual optimization levels. If you are interested in making the performance of one function more visible while profiling either reduce the optimization level or put a `#[inline(never)]` before the function definition.
Personally I would shorten the function body to something like `*opt = opt.take().map(|old|if replace(&amp;old, &amp;new) { new } else { old });` because I don't really like the `if if` part. Edit: As /u/desiringmachines points out, the suggested code is incorrect. Should be something more like *opt = match opt.take() { None =&gt; Some(new), o =&gt; o.map(|old| if replace(&amp;old, &amp;new) { new } else { old }) };
If you are on Linux, you can follow [what I did](http://llogiq.github.io/2015/07/15/profiling.html).
perf on Linux, instruments.app on osx.
Since (iron) rust is red I find it appropriate that that's the color used for a core concept of the language. &lt;/joking&gt; &lt;ducks for covers&gt; 
`iter_mut()` (and `as_mut()`, which is equally valid here) don't allow you to replace a `None` with a `Some` in-place. That is a functionality that the std library doesn't expose (I've no opinion on if it should or not).
Unlike the example code, this doesn't replace the option if it is None.
None of those things really affect these microbenchmarks, since they're written at the lowest level possible. No smart pointers in sight. The only thing these microbenchmarks really measure at this level is how fast libraries are and how clever the implementers are, given implementers have a *ton* of freedom to do different things.
That post is quite old. People have now spent more time optimizing the Rust code (and its dependencies, like `regex`) than before, so the timings are changed. If the benchmarks were fair, this would not matter so much. But they're not (the implementations are very different and have very different timing characteristics), so it does.
[It's been about 6 months.](https://github.com/rust-lang-nursery/regex/pull/91) Mostly done by the glorious /u/burntsushi, and [he explained how he did it](https://news.ycombinator.com/item?id=9781973). Amusingly (or, sadly), these changes didn't get to the `regex!` macro yet. Actually, perhaps unintuitively, [`regex!` is slower than `Regex::new` for now](https://www.reddit.com/r/rust/comments/3b2i0f/psa_regex_is_now_slower_than_regexnew/). This is primarily because (IIUC) `regex!` still uses full-blown NFA, whereas the improved `Regex::new` uses DFA which is faster.
For the author, creating custom structs for every named parameter is a lot more overhead. Even if it makes no difference to users, it would be lovely to not have that barrier to entry, which discourages this kind of naming quite a bit. But even so, it *does* make a difference to users! The caller has to import each struct for every "named parameter", _and_ the order of the parameters still has to be right, which means more time spent juggling parameters instead of writing actual code.
I mostly agree with you, but I also think that finding a good "combination" is a bit rabbit hole. For some people, Rust can also be seen as so complicated that the average programmers cannot actually utilize its low-levelness to optimize their programs. Probably the other extreme in this spectrum would be Haskell. A typical argument I've heard from the proponents of such high level languages is that the average programmers are not well-versed in optimizing programs, so programs should be written only concentrating on semantics and correctness, and introduce some language features like pure annotations to make programs easier to optimize by the compiler (Hell, Haskell doesn't even have deterministic execution order to make it even further!). Then the "sufficiently smart compiler" chimes in and does better optimizations than human. And yet, I don't see this claim to be completely proven, as there are many corner cases around compiler's automatic optimization. So I think the answer is somewhere between those two extremes. Moreover, a good balance may differ per target audience. I'm exactly not sure if Rust is the optimal balance for the *average* programmers. Whether or not Rust succeeded to meet the balance for general audience would be very hard to prove, only time will tell.
That looks fine to me, what is the problem? Is it actually slow? Have you profiled? Are you compiling with `cargo --release`? :)
You could do it with mem::replace, couldn't you?
Also there are more optimized versions of many Rust entries (most of them by you) in the tracker.
Well the trait can't "allow" that, since not all functors have a concept of "empty value" (e.g., Either A B, Pair A B, non-empty lists). But you're right, I did misinterpret the original question. In this case, I think the best that can be done is remove that `replace` parameter and just put an `Ord` constraint on `T`.
What's your hobby OS like? Do you target hardware or a hypervisor?
Thanks for the tip with write_all, I shall have a play. `abomonation` looks hilarious. I doubt that perf is really going to be an issue, this a control plane daemon, but I just like to do the best thing without becoming unreadable.
"Science advances one funeral at a time" - Max Planck See also [How Math Works](http://www.smbc-comics.com/index.php?id=3947) by Zach Weinersmith.
Even today, the assembly programmers aren't wrong. Assembly is the only way to reliably get certain behaviors and performance characteristics. Things like SIMD have only made this more true. Manually invoking SIMD intrinsics that map 1:1 to assembly instructions isn't exactly winning on abstractions, beyond not managing registers (and managing registers might be why you need to use raw ASM anyway).
Here's a theory. 1. The average duration of tenure is about forty years. (Average age of tenure is 39, life expectancy at 39 in the US is 80) 2. The average software developer career is about forty years. (Average age of retirement in the US 62, assume programmers started around 22) 3. Therefore, from the moment a revolutionary computer science discovery is made in academia, it takes forty years for it to become mainstream enough in academia to be noticed in the industry, and another forty years to be mainstream in industry. Using that model, functional programming should be mainstream in industry around 2017, and dependent types around 2054.
They were wrong then as they are now (except some minor caveats). The original K&amp;R C was designed to basically be a portable assembly and it closely matched assembly instructions specifically to address such concerns. Was it *exactly* the same performance? Depends on the exact use-case. Compared to "regular" hand optimized code, C was just as fast. Compared to an assembly expert that used the knowledge of the specific rotation speed of the very specific hardware storage device (rotating magnetic drum of some sort) to skip jump instructions, the C version was probably a negligible percent slower. This is truly a neat trick but I wouldn't say it is significant enough in general to justify the general statement. As for today, at least for Intel processors, assembly is virtual. Intel CPUs have a hardware VM that translates Intel CISC assembly op-codes into one or more internal RISC micro-codes. So mapping 1:1 to Intel assembly isn't enough to infer performance characteristics. it is entirely possible and depends on the specific hardware CPU model that one op-code takes more cycles compared to an equivalent sequence of other op-codes that might be better optimized to a more efficient sequence of micro-codes that take less CPU cycles. CPU makers today optimize their HW for compilers and not human programmers and outside of some special cases hand writing assembly doesn't make sense. The caveats to the above would be when the PL lacks support for new HW features such as SIMD. This is solved by either non-standard extensions to the language/compiler or by actually adding language level support. I'm sure that modern languages such as Rust and D will eventually fully incorporate support for that. So it's a matter of time and not some intrinsic advantage of assembly. tl;dr - Assembly still exists and has its use cases but that doesn't mean that assembly is inherently faster. Edit: link to source story (this is actually even before assembly!): http://www.catb.org/jargon/html/story-of-mel.html 
Indeed, but they _should_ affect real-world code where you want to do something fast, without crashing your system, at least that would be my naive guess. It would be interesting to see a comparison of Rust's primitive references, `Rc`, `RefCell`, etc. VS C++/boost smart pointers.
[removed]
Here's my take on it: a.enum { color: rgb(165, 135, 0); } a.trait { color: rgb(0,146,146); } a.mod { color: hsla(108,45%,38%,1) ; } a.struct { color: rgb(73,0,146); } .unstable { opacity: 0.75; } a.macro { color: rgb(165, 109, 145); } .deprecated a { text-decoration: line-through; } 
So I have a library that does a similar kind of thing, and it makes heavy use of unsafe code simply because it's easier than trying to convince the optimiser of the various facts you know. I wrote the `dot`function using mostly unsafe code [here](https://play.rust-lang.org/?gist=39612fdce10eb441f9a2&amp;version=stable). It's very vectorized and more importantly no longer has a chance to unwind (that is, it won't panic). More generally, optimisation is about using assumptions to avoid excess work. The compiler uses various assumptions like "a pointer deference will never fail" or "the value I load from a memory location is the value last stored to that memory location". Since all variables start life, pre-optimisation, as memory locations, you can see how that last one is useful when optimising. The thing is, there's a limit to what the compiler will consider, sometimes it's a strict mathematical limitation (e.g. the halting problem), more often it's a practical concern (i.e. requires a slow analysis that doesn't really pay off in most cases). Sometimes it's simply the optimiser not handling something. For the last case, you have to remember that compilers don't see your code the way you do. You see `let len = cmp::min(u.len(), v.len())` and know how `u.len()`, `v.len()` and `len` all relate. For the compiler, it requires looking through a lot more information. Most of the time this kind of stuff doesn't matter, but for tight loops, often you need to more direct in the assumptions you're relying on. In my re-written code, I'm using `len` directly, as you're ultimately relying on `xs.len() == len` being true. I'm using raw pointers and raw pointer offsets too instead of `xs = xs[8..]` since the optimiser can't seem to avoid the bounds check there. Likely due to it going via `RangeFrom` instead of a regular index. The LLVM IR isn't too hard to understand once you get used to it. It's definitely more understandable than ASM, as it has a 3-address form instead of a two-address (meaning no having to remember which register is overwritten). Personally, I tend to look for function calls that indicate a compiler-inserted panic, like a failed bounds check, then trace back to see where it's being checked and how I can avoid the check. Tracing back is fairly easy, as blocks list their predecessors (the blocks that have jumps *to* that block). Finally, using `unsafe` should be a "last-resort" tactic. However it shouldn't be a dreaded and/or strongly avoided tactic. It's less "nuclear option" and more "diplomacy has failed". This is because you've now taken on some of the responsibility of making sure the code is correct. The good new is that often you're using unsafe because you know it's correct, you just can't convince the compiler.
It was really the Copenhagen interpretation that he was upset with - this idea that, given the famous cat in a box, it could be *both* alive and dead. I think a lot of physicists at that time assumed that the statistical model of quantum theory would eventually resolve into a clear, physical model, and the Copenhagen interpretation just seemed too accepting of chance being the sole arbiter of quantum-scale physics.
This sounds like my gold opportunity to learn the language :)
Well, one could argue that makes it a good benchmark :)
:-)
You could also get rid of the Box there. But yeah, you probably would want some sort of type alias too.
No idea why it's slower without more profiling. I'd suggest callgrind to see where the program is spending its time.
My guess would be that statically known window size can elide some bound checks in this loop (by assuming that both windows are of equal length): for (a, b) in s1.iter().zip(s2.iter()) { ... } Lack of bound-checks may also enable vectorization, but I'm not sure if it kicks in in your code. Also (style nitpick), you can simplify your outer loops with `enumerate()`, eg.: for (i, s1) in seq1.windows(window_size).enumerate() { ... } ------ Edit: I've looked at the assembly, and both my hypotheses have turned wrong: 1. LLVM is able to assume that both iterators have the same length. This is how the hot loop looks like. (It has one weird issue though, notice how the `cmpq %rax, %r8` is executed twice in a row) ([I've used simplified version](https://play.rust-lang.org/?gist=31a8874a3dc1119e2460&amp;version=nightly)): .LBB0_9: cmpq %rax, %r8 je .LBB0_11 movzbl (%r14,%rax), %r15d movzbl (%rdi,%rax), %ebp cmpl %r15d, %ebp sete %bl movzbl %bl, %ebx addq %rbx, %r12 incq %rax cmpq %rax, %r8 jne .LBB0_9 2. The code with constants is not vectorized. But it is fully unrolled! So, while the version with constants should be faster, I see no reason for the argument-version to be slower than a C one. The easiest method for profiling such short hot loops for me is `perf` (use it for C, Rust, anything): $ perf record program with args $ perf annotate And you jump straight into disassembled hottest place of code, with nice colors and percentages. Bonus points, if you add debug symbols to `release` Cargo profile, the assembly will be annotated with source code. --------------------- I've written simple version in C and it seems that both Clang and G++ vectorize the code. I'm not sure why Rust doesn't, since it can elide bound checks...
When this post was written, you couldn't get rid of the box :)
It was huon
 What does being fair have to do with anything? Einstein famously said that he doesn't believe that the old guy plays dice - referring to God and to the statistical nature of quantum mechanics. That's a known historical fact, not my personal opinion of him.
Looks pretty good to me, though there's some weird formatting with the if/else setting lim, try running rustfmt
1. [Here](https://doc.rust-lang.org/std/option/enum.Option.html#method.and_then) is the documentation for `and_then()`. Basically, yes, you understand it correctly. 2. You should never use `&amp;Vec&lt;T&gt;`. `Vec&lt;T&gt;` is already a pointer; the difference between `Vec&lt;T&gt;` and `&amp;[T]` is that the former is an owned pointer and the latter is a borrowed pointer. Function arguments should always be `&amp;[T]` because you can get that from `Vec&lt;T&gt;` (or possibly other data structures) with no cost, whereas to get `Vec&lt;T&gt;` from `&amp;[T]` requires an allocation.
We do have [typenum](https://github.com/paholg/typenum) though. And you can always write other functions for atring conversion that don't suck. What's missing?
&gt; Function arguments should always be &amp;[T] because you can get that from Vec&lt;T&gt; (or possibly other data structures) with no cost, whereas to get Vec&lt;T&gt; from &amp;[T] requires an allocation. And an `&amp;Vec&lt;T&gt;` is a double indirection (pointer to a Vec to its data) whereas `&amp;[T]` is a single indirection (pointer to data)
&gt; I have a bit of trouble understanding the logic of and_then(). Does it apply the argument function to the value if it's Some(x)? Yes, and the argument function returns an Option so the sub-operation can either succeed or fail. val.and_then(f) is equivalent to this: match val { Some(v) =&gt; f(v), None =&gt; None } (in fact that's exactly how it's implemented)
There is also [We Love Rust](http://www.weloverustlang.com/).
submit a PR! we love getting them :)
Yeah, I mean you'd think one would learn from the past. But alas, angle brackets are the herpes of software development: they just keep spreading around anything that has had even passing contact with c++
[Integration of the new state of my wayland libs into glutin.](https://github.com/tomaka/glutin/pull/674)
As others have mentioned, `&amp;[T]` is called a slice, and you can check out it's documentation here: https://doc.rust-lang.org/stable/std/primitive.slice.html
&gt; The first concept is deciding which of two T's is better, which has nothing to do with optionals at all. One of the T values might not exist (the one in the Option), so it's not completely independent of optionals. Second, I can't/don't want to put an Ord constraint on T, because T is usually a simple tuple of the form (index_of_currently_best_solution, quality_of_currently_best_solution). Obviously I only want to compare based on the second element, and introducing a new type with custom Ord implementation for every use is overkill. Third, as desiringmachines already mentioned, it might have to replace a None value. This is the least problem, since it's just an assignment to the mutable reference.
 fn read_file_lines(f: &amp;mut File) -&gt; Result&lt;Vec&lt;String&gt;, io::Error&gt; { let read = BufReader::new(f); let mut lines = Vec::&lt;String&gt;::new(); for line in read.lines(){ lines.push(line.unwrap()); } return Ok(lines); } `line` here contains a `io::Result&lt;String&gt;`, [right](https://doc.rust-lang.org/std/io/trait.BufRead.html#method.lines)? This means that your function `read_file_lines` returns a `Result` with `io::Error` as its error condition, but whenever it encounters an actual I/O error it panics the program instead of returning this error in the `Err` variant. If panicking is really appropriate, the function could just return `Vec&lt;String&gt;`. If not, it could just be written like this (also correcting other style stuff): fn read_file_lines(f: &amp;mut File) -&gt; Result&lt;Vec&lt;String&gt;, io::Error&gt; { let read = BufReader::new(f); let mut lines = vec![]; for line in read.lines() { lines.push(try!(line)); } Ok(lines) } This functions stop whenever it encounters an I/O error in `line`, and returns `Err&lt;encountered error&gt;`. [`try!` is rather neat](https://doc.rust-lang.org/stable/std/macro.try!.html). More details about error handling is found in [this chapter](https://doc.rust-lang.org/book/error-handling.html). Also, the comment that says this loop is unnecessary still applies, so lets turn it into an one liner (using [this trick](https://stackoverflow.com/questions/26368288/dealing-with-result-within-iter)): fn read_file_lines(f: &amp;mut File) -&gt; Result&lt;Vec&lt;String&gt;, io::Error&gt; { BufReader::new(f).lines().collect() } Or, if we wanted to specialize the `collect` method (not needed here): fn read_file_lines(f: &amp;mut File) -&gt; Result&lt;Vec&lt;String&gt;, io::Error&gt; { BufReader::new(f).lines().collect::&lt;Result&lt;Vec&lt;_&gt;, _&gt;&gt;() } It reads: create a buffer, take its lines (which is an iterator of I/O results) and collects the iterator into a result of a vector: it's either `Ok&lt;strings you want&gt;` or `Err&lt;first encountered error&gt;`. Whenever you put a `_` in a type, the compiler try to infer what should go there, based on context. But if we found this type confusing, we could write down the type explictly: fn read_file_lines(f: &amp;mut File) -&gt; Result&lt;Vec&lt;String&gt;, io::Error&gt; { BufReader::new(f).lines().collect::&lt;Result&lt;Vec&lt;String&gt;, io::Error&gt;&gt;() } Note this is considered [bad style](https://aturon.github.io/features/let.html#use-type-annotations-for-clarification;-prefer-explicit-generics-when-inference-fails.-[fixme:-needs-rfc]). Whenever the human reader of your code might be confused by the types, instead of over-specilizing the `collect` method, you should just use type annotated `let`, like this: fn read_file_lines(f: &amp;mut File) -&gt; Result&lt;Vec&lt;String&gt;, io::Error&gt; { let result : Result&lt;Vec&lt;String&gt;, io::Error&gt; = BufReader::new(f).lines().collect(); result } But note how absurd this code is: it just states the return type of the function twice, which is unnecessary. If you drop the type annotation it returns to the cleaner version. Also, note that you committed this style mistake when you wrote down your `lines`: let mut lines = Vec::&lt;String&gt;::new(); This is not needed: you can just write: let mut lines = Vec::new(); And let the compiler infer it's a vector of strings. If you think a person reading your code might be confused about what is stored in the vector, don't specialize the method but instead write a type annotated let: let mut lines : Vec&lt;String&gt; = Vec::new();
That `Build Script` error means handling a piece of newly compiled code (mostly non-Rust, probably C) failed in some way. This could implicate your gcc toolset. If you rerun cargo with `--verbose` you might learn more about the cause. I've had a similar [issue](https://github.com/rust-lang/rust/issues/30178) recently but it was caused by a broken compiler. In your case, a simple `cargo clean` could be enough. (provided you were using a different version of rustc in the past in the same `.cargo` directory) I remember one case from a few months ago (having used two different rustc's) where `cargo clean` didn't help and I had to start with a fresh `.cargo` directory. If you want to test this option without deleting your existing cargo files, set the `CARGO_HOME` variable to something else than the default. 
&gt; I'm also sure that once future-lang is developed in 2020, we Rustaceans will argue the same - how future-lang is more complicated and slower than the established mainstream Rust which is used in so many code-bases and cannot be replaced. A very good point that we should all bear in mind.
If you write `foo.f` or for example `self.bar` in a closure, the variable captured will be `foo` or `self` respectively, not just the field. You can remedy this with explicit captures: In lieu of actual capture lists, just use `let c = &amp;foo.c` and use `*c` in the closure, same with the third closure, put `let f = &amp;foo.f;` outside and use `f` in the closure. At that point it will work well. By the way, your `as_mut_slice()` calls are completely redundant and should just be removed. Of course accessing a vector as a slice is a fundamental operation that does not require a feature gate. [Result (playground link)](http://is.gd/8tWlcW)
I spend some time maintaining my [pending PRs](https://github.com/rust-lang/rust/pulls/petrochenkov), but mostly busy searching for a job, ugh.
Shameless plug: https://www.reddit.com/r/rust/comments/3vuob8/rustycheddar_a_compiler_plugin_for_automatically/
Have you looked at exercism.io? There's at least a bunch of exercises that allow you to experiment with idioms and get feedback on your code.
IIRC, this behavior is on purpose. I remember reading it on reddit, but sorry, I can't remember the reason.
It's because the compiler can't make assumptions about when the closure is being called and whether foo.c is still the same. As c is part of foo, it might change until then. To make sure this doesn't happen the closure captures a reference to foo instead, making it impossible to also borrow it as mutable at the same time.
rustc has no "release" mode. "release" is a cargo concept. The rough equivalent is turning optimisations on when compiling using rustc. Cargo requires at least some of the parts of a project to function. Cargo just runs rustc though.
Numbers in the size of an array, e.g. for linear algebra libraries (which can only happen as of today if specialisation and const fn land).
So, is it normal to have a large binary, even passing opt-level=3?
I don't get it, is it because (supposing the struct is packed) `foo.c` might be smaller than one word and a write to `foo.f` might also write the machine word where `foo.c` is stored? I think the logical extension to the current Rust behavior should be: if you borrow `foo.c` mutably, you can't borrow `foo` (of course) but you can borrow `foo.f`, since they are disjoint. But if you borrow `foo.c` immutably, you can still borrow `foo` (as long as it's immutable). And you can borrow `foo.f` mutably (disallowing the `foo` borrow), etc. Arrays could have this kind of sub-borrowing too.
Regarding color blindness, there are applications that create a "filter" in the colors showing them as if they were seen by different forms of colorblindness, in such a way that if two colors would be confused by the colorblind person, they are mapped to similar colors by the filter. An example is [Palleton](http://paletton.com/) (you insert your color scheme there, and click "vision simulation" in the bottom right). It can also simulate low quality monitors and so on. Also: Palleton is excellent for *designing* a new color scheme for Rust documentation (it can select colors that match well, look good and are well distinguished, etc; see the menu in the top with "triads", "tetrads", etc). So, from a designing point of view, I disagree that there are not many distinct colors.
Rust already allows you to do individual borrows for the struct members. The problem is the closure though. The closure has a reference to foo and since it's a closure, the dereferencing isn't going to be done immediately, but instead whenever it is being called. So it can't just evaluate foo.c early. So the closure has to capture a full reference to foo. Creating a reference to foo.c outside of the closure and capturing that should work.
You should try ISPC. It is a language that maps to very fast SIMD.
What about character escapes?
[GitHub issue](https://github.com/rust-lang/rust/issues/19004) has been around for a while. As bjz points out there, you can manually borrow the field, and as Diggsey points out, fixing this is non-trivial since the naive fix would increase the closure size unnecessarily in some cases.
I replied to someone else above recommending looking at ISPC, it is a C variant that allows portable but vectorized programs.
There was something similar on here a few days ago: https://www.reddit.com/r/rust/comments/3vd8sj/rusty_result_type_in_python/ The thing is that I don't believe replicating the match statement in Python makes sense. Many things (like `unwrap_or(default)`) are not necessary at all in Python. So this is a simple wrapper class that can hold some state and knows whether it's `Ok` or `Err`, that's all. No `isinstance`checking going on (although I might add that later on).
This week marks a big change for the book. Historically, I always had [a vision for the docs](https://air.mozilla.org/rust-meetup-december-2013/), but with 1.0 coming, it was a struggle to just get everything documented and keep it from breaking, let alone put things together in the way that I wanted. One way in which this manifested is in the split: https://doc.rust-lang.org/book/ &gt; After reading this introduction, you’ll want to dive into either ‘Learn Rust’ or ‘Syntax and Semantics’, depending on your preference: ‘Learn Rust’ if you want to dive in with a project, or ‘Syntax and Semantics’ if you prefer to start small, and learn a single concept thoroughly before moving onto the next. Copious cross-linking connects these parts together. Two paths through one book isn't good. And "Learn Rust" never really came together in the way that I wanted it to. It only even came into existence the week before 1.0! I just didn't have time to make things right. And then, it was time to work on API docs... So, after doing a lot of chatting with a lot of people, including at the work week last week, I've come up with a reorganization of the book that makes me happy. It's going to be great. I'm really excited about it. But it also means that there's gonna be a lot of change, and things might not be coherent in the meantime. I don't want to land a PR to change 300 pages of docs! As such, I've made https://github.com/rust-lang/book. I'm going to be working on the book in that repo. Right now, it's empty. But that won't last long. :wink: This will let me work on things without screwing up the existing book for people, in the meantime. You'll notice one more thing: I'm using [mdBook](https://crates.io/crates/mdbook/). This may be for good, or it may just be for development, but I'd like to think that it's for good. We would have to sort out how to make it work with the build process. Or, the book might end up living in this repository permanently. There are still some unanswered questions. But in the worst case, things will just stay the same: once we're happy with the new state of the book, we can import it back into tree, and things should Just Work. But maybe we can make them better, too. Oh, and this also means I'm going to be a bit more liberal with pull requests to the book itself, as it's a bit less important that the markdown be perfect, that it absolutely not introduce new forward references, or anything else. see also https://internals.rust-lang.org/t/better-support-translations-for-the-book/3003
Rust statically links everything by default, so that probably accounts for that difference, yes. `opt-level` doesn't really have much to do with binary size.
So far it's very Spartan, but it's also not finished. Strings can use double quote if it's proceeded by a back slash. No other chars are checked when reading a string except EOF. I'm not 100% familiar with the json spec so idk if that's standard. It's more of an exercise to learn life times. 
Language/compiler support doesn't have to be either raw SIMD intrinsics or autovectorization. The D language for example supports vectorization semantics at the language/type-system level. Last time I checked (long time ago) the state of affairs was that some types of vectors had special treatment (based on type and size) and there was a growing amount of library code to implement various operators on them with efficient hand-written algorithms. so things likes: int[4] arr1 = ...; int[4] arr2 = ...; int[4] arr3 = arr1 + arr2; would produce the expected result using templated code in the stdlib for the + operator. I don't know the exact state of affairs now in the D community but the point was that a language can be extended with semantic knowledge of vector types. It's all about providing the compiler the semantic information it needs in order to generate well optimized code. So again, given the above prerequisites the difference in performance is negligible and not worth the cost in programmers' hours and loss of abstraction and portability for almost all use cases. Rust is behind in this aspect - it needs to first grow support for generics over integer values and specialization before such a design can be considered. This also requires the discipline to not add impls in the std that will later conflict with such a design. This is why D uses a separate operator for string concatenation so that the + operator will always have a consistent behavior. 
&gt; So again, given the above prerequisites the difference in performance is negligible Do you have any examples of actual, nontrivial D programs using this functionality benchmarked against equivalent code using intrinsics or assembly?
I'm vacationing pretty heavy right now, but planning on merging initial asmjs work, reviewing the FAQ, adding RUSTFLAGS to cargo.
Not that, but `cargo build --target=asmjs` 
It's behind a feature gate so it's nightly only, luckily /u/manishearth has a [copy of the full nightly docs](https://manishearth.github.io/rust-internals-docs/test/fn.black_box.html) on github.
Why are you using a boxed slice, rather than just returning the vector?
How about an unsafe method where the user is expected not to change the hash?
It's not unsafe. You can do this today with RefCell.
That and "spooky action at a distance".
I've started working on a state machine/generator/await/async [syntax extension](https://github.com/erickt/stateful) with /u/eddyb. It's super early, but can currently express something like: #![feature(plugin)] #![plugin(stateful)] #[state_machine] fn yield_() -&gt; usize { let x = 0; yield x; let y = 1; yield x + y; } fn main() { for value in yield_() { println!("yield_: {:?}", value); } }
I momentarily forgot about what Rust's definition of safety is.
Happy to take patches to serde_json :) I've been waiting for specialization to do something like this for serde_json in order to keep it fast for stream processing and for reading from a slice.
Still improving Chomp. Currently I am working on bounded versions of the combinators, allowing for an upper and lower bound using the standard range operators (this includes actually supplying `size_hint` for the iterator). I am also adding a simpler way of parsing arbitrary slices without any buffering (essentially Attoparsec's `parseOnly` function). I might be able tog get an `Applicative` version of the parser working too, but I need to make sure that it composes well (got some odd experiments which seem promising). Once I have finished the bounded combinators there is only the risk of overflow in the `decimal` parser (since it uses an unbounded number of digits) left to provide an alternative to which does not have any risk of overflow.
It's really `return` in the prototype but I'm treating it as `yield` until we can come up with a better scheme. Maybe we could have the macros `yield!(expr)`, `goto!(expr)`, and `await(expr)` which we reinterpret as our state transition operators?
The current issue is in using a string wrapper class so I can just throw sub-objects into the standard Btree. Which means you have to bring in sip harsher, and a string wrapper class. 
That sounds amazing! Is it out in the open?
You *can* box it if you like, but usually it will be stack-bound.
Ariel in chat mentioned that this is due to wonky rvalue stuff. Probably a bug. (may disappear post mir?)
I don't.
I'm preparing for the celebrations. Apart from that, I have just scripted around clippy a bit and now lints can have doc comments which will be copied to the wiki. So this week I'll add missing doc comments to clippy lints. Apart from that I want to finish my ternary logic code and perhaps some type data structure work. Edit: I just pushed [ternary](https://github.com/llogiq/ternary).
(You can also use a `let` for the pattern match, to avoid having to indent/scope things.)
Good questions. Will try to answer them. &gt; does Redox need its own version of Rust? The rust folder, which is just a submodule of the Redox fork of the Rust repository, is used for the libraries, such as liballoc, libcollections, libcore, and so on. Redox supports these (but not the standard library yet). &gt; does it use Cargo to manage dependencies? Some parts do, some do not. Most of the codebase do not (including the kernel). We do, however, have plans to go with cargo at some point, but currently, it's not possible due to lack of control over options for rustc. &gt; Is Redox self-contained, not depending on anything on crates.io? Almost. It does depend on the core libraries of rust (libcore, liballoc, libcollections, and so on.), but not anything else, though in the future, we might get Octavo, Toml-rs, and some other dependencies. &gt; Is this by design? No it's not. It's due to the fact that we do not yet have support for std (mainly because of not having libc yet).
That's right. But we do have libredox, which is spoofed as `std` and supports quite a large part of std, one-to-one.
That's true, we don't have rustc *in redox* yet. They're just cross-compiled outside redox.
The issue is not solved. We don't support x86_64 yet, due to this. It's because rust don't have a `naked` flag. I've written about this before: --- **PR (implementation)**: https://github.com/rust-lang/rust/pull/29189 The proposed *#[naked]* attribute indicates that the specified function does not need prologue/epilogue sequences generated in the compilation process. It is up to the programmer to provide these sequences. In basic terms, this makes rustc capable of doing all the low-level stuff which C is able to. Such as: - Possibility of implementations of green threading. - Interrupt handling. - Context switching. *All in Rust, without needing external files.* --- This is especially important when dealing with low-level stuff, such as OS development. We for example need this in Redox (an OS written in pure Rust) to get x86 support working. So this is really an important feature to us. ------ An example for the curious would be a custom interrupt handler function: #[cfg(target_arch = "x86")] fn interrupt_handler() { asm!("iretd"); } This function will currently generate code as: interrupt_handler: push ebp mov ebp, esp iretd ret (truncated for simplicity) With the #[naked] attribute (which is implemented with PR #29189), it will bypass the prologue and epilogue sequences. And the output will be like: interrupt_handler: iretd ---
Is the workaround of defining the naked functions in their own asm files acceptable? Also, IIRC, there was a concern that adding non-asm Rust code in a naked function would lead to undefined behavior, due to unintended stack manipulation. Are you okay with a restricted form of naked functions, that can only contain inline asm?
It would be nice if the `fn work(on: RustProject) -&gt; Money` section mentioned the new [Rust Community Job Board](https://rust.jobboard.io/).
It took me a while to finally get it into an interesting state. I'm here to answer any questions.
"Amend 0809 to reduce the number of traits" link is wrong.
Oh, so actually when one says that it doesn't support libstd, you mean there are some libstd interfaces that aren't available, but there's still a `std::something`. libredox and libstd shares a lot of code, right? Also, if the OS is entirely written in Rust and the applications too.. does it make sense to include a libc for the sake of libstd? Or maintaining a separate library that doesn't rely on a libc is just too much work for little gain? (actually: is is possible to write a libc in Rust?)
My guess is that it's a bug in how we handle rvalues. Can you file a bug at https://github.com/rust-lang/rust/issues ? I'm sure /u/nikomatsakis would be interested to have a look at it.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/humanresourcemachine] [\[x post from \/r\/rust\] Something a little light-hearted: A parser and interpreter for Human Resource Machine solutions](https://np.reddit.com/r/HumanResourceMachine/comments/3wucq5/x_post_from_rrust_something_a_little_lighthearted/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
I'm pretty sure the following requires `lock` to be destroyed before `stdin`, but the same error is shown: let handle = io::stdin(); { handle.lock().lines() .map(|line| line.ok()) .collect() } Edit: In fact, the following code appears to demonstrate that "`lock`" would in fact be destroyed before "`stdin`" even without the extra scope: struct LoudDrop(&amp;'static str); impl Drop for LoudDrop { fn drop(&amp;mut self) { println!("Oh no, demise of {}", self.0); } } fn test() -&gt; Option&lt;String&gt; { let foo = LoudDrop("stdin"); (LoudDrop("lock"), None).1 } I would say this is exactly what should be expected, because variables are dropped in reverse order of declaration, and it would be very strange if the final statement of a block would be an exception to that.
Yeah, that's one of the reasons I posted it here so I can get this kind of feedback. Sounds so obvious now but for whatever reason I didn't think of that before. 
It's been tracked since this was introduced: https://github.com/rust-lang/rust/issues/22323
There's been 2 or 3 (or more) attempts at generic numerics in the standard library, but none of them felt *great*, so the decision was made to let it be worked on outside the standard library. This is along the lines of Rust's choice in other areas too: external crates have significantly more flexibility than the standard library, and so not setting functionality in stone by stabilising it in `std` is nice. &gt; Are there plans to integrate what is in the num trait into the stdlib? The team has always said that we'd consider bringing in functionality into the standard library if there was a particularly good reason to do so, so it is a possibility, but there's no concrete plans. Something more likely is a crate offering this sort of generic functionality becoming [a rust-lang crate](https://github.com/rust-lang/rfcs/blob/master/text/1242-rust-lang-crates.md). &gt; Will the methods that operate on primitive types ever be abstracted behind traits? Can't predict the future, but there's no concrete plans for it right now. If someone were to write [an RFC](https://github.com/rust-lang/rfcs/) that e.g. deprecated `powi` and `powf` in favour of a generic `pow` method, it would be considered like any other RFC. &gt; Are there any plans to expand what is in the num crate/the stdlib to be as comprehensive as Haskell's numerical type clases? This is a question for the maintainers of `num`.
I'd hope that more than one person could read a project at a time, so it's probably more like `Arc&lt;RWLock&lt;RustProject&gt;&gt;`, but really, I suspect a better model with a DVCS is `Arc&lt;Stm&lt;RustProject&gt;&gt;`, where `Stm` is some sort of [transactional type](https://en.wikipedia.org/wiki/Software_transactional_memory). :P
Well, not zero cost. You do have to `call` and `ret` :P
Cool! I've been involved with the effort to build a [Raft library](https://github.com/Hoverbear/raft-rs.git) in Rust, a couple of nice things have fallen out of that which you may be interested in, since I noticed you rolled your own: * cross platform mmap wrapper: https://crates.io/crates/memmap/ * cross platform fallocate wrapper: https://crates.io/crates/fs2/ * mmap-based write ahead log: https://github.com/danburkert/wal (this ones still very much a work in progress) Would be great to get everyone doing this sort of thing using the same building blocks, so that bugs get shaken out faster. The mmap and fallocate wrappers include Windows support, so that should get you one step closer on that front.
Wow, thanks you so much for your help! I Knew It would probably be easier to do it with If's, but I wanted to be "idiomatic" lol I did get that conflicting impl error, and that's how I ended up with this, haha You said you can't use non-constant values on the left side of match arms Could you explain how why it isn't constant? Should'nt the value of TypeId::of::&lt;String&gt;() be known and evaluated at compile time, Since &lt;String&gt; is a constant? Does that mean the entire if/match Structure isn't optimized away? (Since generics are basically copy and pasting for each type that calls it?)
`TypeId::of::&lt;T&gt;()` returns a constant value, but it is still a regular function, so the compiler isn't smart enough to figure out that it will always return the same value for a given type parameter. `Any::get_type_id()` is supposed to be replaced someday with an associated constant, so after that happens, you should be able to match on it. This will probably happen when the `const fn` syntax is stabilized and `TypeId::of::&lt;T&gt;()` is refactored to use it, so you could do `&lt;String as Any&gt;::TYPE_ID` or `TypeId::of::&lt;T&gt;()` on the LHS of a match arm. But it still wouldn't work for your use-case because simply matching on the `TypeId` doesn't convert the `&amp;self` reference to the desired type. The `Any` trait implements downcasting by doing this match and then performing some `unsafe` hackery behind the scenes to extract the `data` pointer from `&amp;Any` and cast it to `&amp;T`.
Isn't const fn recently stable?
Unless I witnessed a truly glorious interface with broad community acceptance for multiple years, I would probably vote against ever including `num` in std. I've never seen generic numerics done "right" for universal consumption, and I'm not convinced that it's even possible. Too many people have too many opinions on what numbers should be. 
I've been using memmap and fs2 and they're both wonderful! Thank you! Wow, i hadn't seen your wal crate. That looks exactly like something I've been planning to write! Looks like i might not have to. :) (It looks like all three could be used to eliminate quite a bit of code in the OP's project as well.)
Wow I didn't know asmjs was on the cards! Awesome :D What kind of timeframe are we looking at for that to be stable?
Ooh! HTTPS, using Let’s Encrypt! Didn’t notice that last week.
&gt; Oh, so actually when one says that it doesn't support libstd, you mean there are some libstd interfaces that aren't available, but there's still a std::something. That's right. &gt; libredox and libstd shares a lot of code, right? Yep. They do. &gt; Also, if the OS is entirely written in Rust and the applications too.. does it make sense to include a libc for the sake of libstd? It's not Rust only. We also want to support all kinds of other programs. We already have a bunch of programs and libraries, which are non-rust, ported and working on Redox.
We already can do this with zero-size structs. There are a few problems though.
As an example, I have found it useful to have a trait just for square root, as I've had things that I want to be generic over that I want to take the square root of that can't implement `Float`. I can't imagine many people would want `Float` broken into a trait per function.
There are two problems with this approach: * you need to have data in the structs you *build* (but you can `impl Default` for the optional parts) * your `Has` needs to behave like a set whereby each element your builder `Has` is contained. The easiest way is to add a `PhantomData&lt;(..)&gt;` to your builder where the tuples contain `Added` or `Missing` markers (which obviously must be generic in your builder type). This can get a bit unwieldy, but you should be able to present a nice interface. The downside is that the resulting errors will be missing impls, which can be misleading.
 type RustProject = Arc&lt;Mutex&lt;RustProject_&gt;&gt;;
A little off-topic, but I find it really nice that we got HTTPS on TWIR thanks to LetsEncrypt :) Cheers!
Now fixed. Thanks for pointing out!
Useful issues for historical context: - [Implement a proper 'numerical tower' in core or std](https://github.com/rust-lang/rust/issues/4231) - [Redesign and document the traits/functions related to numbers](https://github.com/rust-lang/rust/issues/4819) - [Flatten numeric trait hierarchy and allow for user extensibility.](https://github.com/rust-lang/rust/issues/10387) - [RFC: numerics reform](https://github.com/rust-lang/rfcs/pull/369) We tried so damn hard to figure out a good way of making a generic numeric library, and we had many failures. `num` is a relic of those efforts. We tried to represent abstract algebraic properties, but were scared of going all the way. We also tried to be practical, but didn't go all the way. We ended up with a mess of both. The trouble is that 'computer numbers' don't really conform to the same properties of the regular cast of mathematical objects (naturals, integers, rationals, reals, etc), nor to the familiar abstract algebraic structures (monoids, fields, semigroups, etc). The question is whether we try to just go for an approximation of those mathematical objects, or whether we try to find the true mathematical properties of our 'computer numbers', and base our abstractions on that. The former will always be imperfect, and the latter is harder, and could get ugly quick with edge cases, and will probably never be perfect either or be too finicky to actually be useful. &gt; However, one area where Haskell definitely has Rust beat are in the numerical type classes/traits. Even the haskellers haven't figured it out yet. Most people think that the Haskell 98 type class hierarchy is flawed, and there are numerous alternate preludes out there that try to improve on it.
`Provide a syntactic sugar to ...` and `Amend 0809 to reduce the number of traits.` are still both linking to [https://github.com/rust-lang/rfcs/pull/1406](https://github.com/rust-lang/rfcs/pull/1406)
~~Does Rust do tail call optimization? (Look at the generated assembly code). [Some people](http://stackoverflow.com/questions/24023580/does-swift-implement-tail-call-optimization-and-in-mutual-recursion-case) say that Swift does, which could explain benchmark results.~~ ~~EDIT: Actually, wow. If I'm not mistaking, Swift just unrolls the entire loop. Let me upload the assembly code on pastebin, I'll update this comment.~~ EDIT2: * [Rust Assembly](http://pastebin.com/ErJ9zgRR) (`rustc -O --emit=asm fib.rs`) * [Swift Assembly](http://pastebin.com/FLHktQXd) (`swiftc -O -S fib.rs`) I might be incorrect but that is probably why Swift wins this benchmark. EDIT3: on my machine, **Rust 1.5.0 stable is faster** by a second. I'm totally confused by now, probably due to a lack of sleep.
Thanks for posting the assembly code! The Swift assembly is exactly what you would expect (pretty much a direct translation from the source). So, it's still recursive: There are two calls to fib within fib if n&gt;=3 and it returns 1 directly if n&lt;=2. The Rust assembly starts out in a very similar way handling the condition n&lt;=2 and returning 1. But the remainder looks different. There is only a single call to fib within fib but it's part of a loop. It's definitely an interesting transformation. I'm guessing that fib was partially inlined in the Rust assembly version. The assembly code translated back to Rust looks like this: fn fib(mut n: i64) -&gt; i64 { let mut result = 1; if !(n &lt; 3) { // skip this block if n &lt; 3 loop { result += fib(n - 1); n -= 2; if !(n &gt; 2) { break; } // loop as long as n &gt; 2 } } return result; } What the "compiler's thought process" was in turning your fib function into this one I don't know.
Mmmm note that the linked post uses Rust 1.5.0b5 (may be a performance regression?) and Swift 2.2-dev (possibly performance improvements?). I launched a download of [Swift 2.2 snapshot](https://swift.org/builds/xcode/swift-2.2-SNAPSHOT-2015-12-10-a/swift-2.2-SNAPSHOT-2015-12-10-a-osx.pkg) to check. edit: so after having downloaded the snapshot toolchain, here are the results on my machine (2010 15" MBP) | toolchain | time (real) | |--|--| | rust 1.5.0 | 4.54 | | swift 2.1.1 | 5.61 | | swift 2.2 (2015-12-10) | **3.04** | here's the assembly for Swift 2.2-snapshot: http://pastebin.com/BV08VSdF ## OP's results check out^^0, but require downloading a Swift 2.2 snapshot build [0] on OSX El Capitan anyway
I'm getting rust faster by about a second as well (stable and nightly).
It's not so much a new feature as it is a new target. Brian said last week he had the basics going, so we'll see!
&gt; Exception landing pads. Those are just at task/thread bounds right? At any rate is there a "just crash the thread on panic" and strip out any expensive exception handling stuff switch?
You are using an old Swift release, I tried with `Swift version 2.2-dev`
Did you try with `Swift version 2.2-dev`? I tried also with current rust stable and I get alwais the same results
Any feature that substantially decreases intelligibility of code in general is IMO a loaded gun pointing in the wrong direction.
They're using *the current* Swift release. As the suffix notes, 2.2-dev is not a release it's a snapshot of the development tree.
No unfortunately I only have stable swift. (I don't want to install the beta Xcode on my computer.)
&gt; What the "compiler's thought process" was in turning your fib function into this one I don't know. It looks like a way of doing tail call optimization on `fib(n-2)`.
That would be very odd because the code that corresponds to the fib function is exactly the same. See lines 232-258 of http://pastebin.com/FLHktQXd and lines 226-252 of http://pastebin.com/BV08VSdF Frankly, I don't believe you that the former is supposed to be slower than the latter. Something else must have gone wrong. Maybe your testing methodology is flawed. The new Swift version could require less "warm-up time". In that case, the difference in time would not be related to fib.
I added C to the comparison and it is pretty close to Rust, since I don't have Swift 2.2, I get the results where it is slightly slower: $ make bench rustc 1.5.0 (3d7cd77e4 2015-12-04) 1134903170 3.50 real 3.45 user 0.02 sys Apple Swift version 2.1 (swiftlang-700.1.101.6 clang-700.1.76) Target: x86_64-apple-darwin15.2.0 1134903170 5.07 real 4.94 user 0.04 sys Apple LLVM version 7.0.0 (clang-700.1.76) Target: x86_64-apple-darwin15.2.0 Thread model: posix 1134903170 3.32 real 3.26 user 0.03 sys I've put a `fib.c` along with a `Makefile` for `make bench` on [this gist](https://gist.github.com/jansegre/487054963e18a25cce35). My guess is that Swift 2.2 manages to actually be faster than C. (I don't know what sort of black magic goes on, maybe swiftc embeds certain metadata that swift runtime can use to perform runtime optimizations) 
Ah, yeah, good call. I'll give that a shot.
This is what I have now: unsafe { let ptr = try!(CString::new(s)).into_raw(); let mut output_ptr: *mut c_char = ptr::null_mut(); let res = genout(&amp;mut *ptr, &amp;mut output_ptr); let out = CStr::from_ptr(&amp;mut *output_ptr); } I'm going to run it though valgrind after this change. Feels a bit cleaner now.
I've checked the tail recursive version in Rust: &gt; real 0m0.005s &gt; user 0m0.002s &gt; sys 0m0.003s Code: https://play.rust-lang.org/?gist=eb8278e3b9aff8397f15&amp;version=stable This is the "standard" run: &gt; real 0m3.100s &gt; user 0m3.092s &gt; sys 0m0.005s 
That's pretty smart. Granted, there probably aren't many real-world applications for which a compiler could optimize the crap out of a huge chain of the same pure function, but it's kind of neat.
 SNAPSHOT-2015-12-10-a-ubuntu15.10 or SNAPSHOT-2015-12-01-b-ubuntu15.10 or 
&gt; Does the bit of code you outline as identical comprise the entirety of fib including the calling conventions and recursive calls? Yes. &gt; Alternatively, ... Yup, that's probably it. That's the code that differs. The "depth of the unrolling" seems to be the same, 5 (judging by the numbers for `n` fib is invoked with). But the newer compiler did not generate as much `fib` invocations in the main function. It appears as if it was smart enough to figure out that some of them were redundant because fib is "pure".
Thank you. I'm very new to rust. https://doc.rust-lang.org/book/variable-bindings.html says: &gt; left-hand side of a let expression is a ‘pattern’ But to be fair the examples only show let with tuple and the chapter [patterns](https://doc.rust-lang.org/book/patterns.html) has a single example with let (under *Ignoring bindings* again a tuple). My brain didn't make the connection. New code: let Foo { ref mut v, f, c } = foo; v.sort_by(|_,_| c); v.sort_by(f); v.sort_by(|a, b| f(a, b)); 
Interesting. The older Swift version unrolled `fib(45)` into about 32 direct fib calls, never actually calling anything higher than `fib(40)`, which it called once. It called `fib(39)` directly 5 times (6 if you counted the recursion). Whereas Swift `2.2-dev` unrolled `fib(45)` to about 16 direct fib calls, still calling `fib(40)` as it's highest, but only directly calling `fib(39)` 3 times (4 including the recursion). As a point of comparison, actually calling `fib(45)` and following the recursions like Rust does computes `fib(39)` 13 times. I didn't know compilers made optimizations like this. attn: /u/masklinn, /u/grigio
Well it was definitely leaking (thanks for reminding me of valgrind). ==14856== LEAK SUMMARY: ==14856== definitely lost: 256 bytes in 1 blocks ==14856== indirectly lost: 0 bytes in 0 blocks ==14856== possibly lost: 0 bytes in 0 blocks ==14856== still reachable: 443 bytes in 4 blocks ==14856== suppressed: 0 bytes in 0 blocks ==14856== ==14856== For counts of detected and suppressed errors, rerun with: -v ==14856== ERROR SUMMARY: 1 errors from 1 contexts (suppressed: 0 from 0) I ended up creating a struct to hold the pointer and implementing the `Drop` trait and calling `libc::free` on that. pub struct Output { output_ptr: *mut c_char, pub output: String, } impl Drop for Output { fn drop(&amp;mut self) { unsafe { free(self.output_ptr as *mut c_void) }; } } unsafe { let ptr = try!(CString::new(s)).into_raw(); let mut output_ptr: *mut c_char = ptr::null_mut(); let res = genout(ptr, &amp;mut output_ptr); let cstr = CStr::from_ptr(output_ptr); let out = Output { output_ptr: output_ptr, output: cstr.to_string_lossy().into_owned(), }; } Valgrind after: ==17041== LEAK SUMMARY: ==17041== definitely lost: 0 bytes in 0 blocks ==17041== indirectly lost: 0 bytes in 0 blocks ==17041== possibly lost: 0 bytes in 0 blocks ==17041== still reachable: 443 bytes in 4 blocks ==17041== suppressed: 0 bytes in 0 blocks ==17041== ==17041== For counts of detected and suppressed errors, rerun with: -v ==17041== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0) 
The main point here being, that if you want to be sure to do the right thing, better check the documentation of this API call - it might say something about how to use it and whether or not the result should be freed.
Yeah it didn't that I could find. The C API call is `history_expand` in the readline library, FWIW: https://cnswww.cns.cwru.edu/php/chet/readline/history.html#SEC16
What you're seeing here is just a migration of thread from one CPU to another. `fib(43)` finishes at the point marked by the arrow, and from then the only task running is `fib(45)`, which runs for a while on CPU2 and then switches to CPU1. The graph doesn't represent a sharp transition, because the graph is somehow blurred, but at least you can see a nice symmetry between CPU1 and CPU2, which makes them sum up to 100%. When you run this program few more times you will probably see different results. Hope it makes sense now.
You'll need to create a new file object use open. It's kind of annoying that this is not more obvious in the API.
You're at the end of the file, so there's nothing left to read. You need to seek back to the start. https://play.rust-lang.org/?gist=160cc1d245409117685e&amp;version=stable
The problem relates to what happens are your broaden your definitions. Vectors and Matrices certainly aren't numbers, but you can add them, multiply them, raise them to a power, etc. Number systems (complex, real, rational, etc.) support different operations. Finally there are the realistic constraints: changing the order of operations and doing other things can change the result of an operation notably and this is hard, if not impossible, to abstract over. Even integers are sensible to do this, it's not the same to do `(1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1) / 2` than to do `1/2 + 1/2 + 1/2 + 1/2 + 1/2 + 1/2 + 1/2 + 1/2 + 1/2 + 1/2 + 1/2`.
I'm trying out the fst crate for doing some metagenomics work (automatons + fuzzy string matching are pretty relevant there from what I understand).
A Vector is essentially a BoxSlice, but with a different name, similar to how a String is basically a BoxStr. It performs the same function, which is allocating memory around a structure without a fixed size.
An `&amp;Vec&lt;T&gt;` will coerce to `&amp;[T]` if it's passed to a method which takes the latter. Only in very rare cases will you need to explicitly slice it, like if you're creating a subslice. And you can convert `Vec&lt;T&gt;` to `Box&lt;[T]&gt;`, just by calling `.into_boxed_slice()`.
Why don't you just do `Box&lt;[u8]&gt;`? Vec even has `into_boxed_slice` for creating owned slices. If you want to return a slice like `&amp;vec[n..m]`, returning `&amp;[u8]` should be fine because the lifetime of the reference will be automatically bound to that of the parameters unless otherwise specified. Going back to your original question though, if you want to return an owned slice you are probably better off using Vec since (I believe) it has exactly the same performance as a Box&lt;[u8]&gt; if you are not modifying the inner data.
I think it should be noted that in this case, an iterative algorithm is much more efficient than the recursive one you posted. Here's [a gist](https://gist.github.com/yberreby/110e8651ae933efdf16f) showing the code for an iterative version using the [`num` crate](https://github.com/rust-num/num)'s arbitrary-precision integers. On my machine, it can compute the **100,000th** Fibonacci number in just 410 milliseconds: ./target/release/fib-rs 0.26s user 0.15s system 99% cpu 0.410 total If you want to speed up computation-heavy code, it's usually wise to look for _algorithmic_ improvements first.
That wasn't off topic at all.
The readline documentation could have been clearer IMO, but if you look at [the example](https://cnswww.cns.cwru.edu/php/chet/readline/history.html#SEC18) you find something like: char *expansion; int result; result = history_expand (line, &amp;expansion); free (expansion); ...which confirms the theory: 1) do not send anything initialized in (i e, in Rust, just set it to `ptr::null_mut`) 2) one should use `libc::free` to free the resulting output string.
There is a difference however. Vectors and Strings have an extra `usize` capacity variable and can change their size without reallocating in some scenarios. Also once we have `box` patterns and emplacement new its useful to thing of `box`ed DST (dynamically sized types - `str`/`[T]`s) versus `Box::new` boxed `str`types for complex ownership scenarios something like an `Arc::new&lt;str&gt;`.
Putting Dropbox's first rust daemon into production, today! In a nutshell, if Dropbox loses any of your data in the future, you can safely assume Mozilla is to blame. :-P
Ya, there isn't a whole lot of use for it except for some weird ownership/type scenarios or if you for some reason need to optimize out the cost of the 3rd usize in a Vector or String for capacity.
Yes! There's even prebuilt arm toolchains available [on github](https://github.com/warricksothr/RustBuild). I've built a project for raspberry pi (the armv6hf version) using them. With a bit of extra effort, you can even cross compile so you don't need to deal with slow build times :).
In which sense is this too big? use std::io::{self, BufRead}; fn main() { let stdin = io::stdin(); for line in stdin.lock().lines() { println!("{}", line.unwrap()); } } By trying to remove a line, `for line in io::stdin().lock().lines() {` is stuck in a borrowing error I don't quite understand. The integer example is kind of non-idiomatic, and was edited in the stackoverflow answer to make it shorter.
Is there any crate that provides decimal functionality right now? I couldn't find any
Yes, I'll try also with it
In that `else` block, you don't need to call `self.items.insert()`; just overwrite the value of `entry` if it's invalid: let entry = self.items.entry(key).or_insert_with(|| (PreciseTime::now(), fallback(key))); if !is_valid(entry.0, self.item_duration) { *entry = (PreciseTime::now(), fallback(key)); } &amp;entry.1
Well, [decimal](https://crates.io/crates/decimal/)? It's the crate described in the above link. It's not pure rust though (it binds to a C library).
Once the dust settles and we're sure this is looking successful, I'll put a blog post together. My guess is that will happen after the holidays. We might do it concurrently with open sourcing some stuff. All of this depends on how much time I have, and much of that depends on how smoothly this daemon works during the "maiden voyage" here over the next few weeks.
[Ramp](https://github.com/Aatch/ramp) may be of use.
I wasn't actually saying filsmick's disclaimer didn't belong here. Note that I tacitly agreed with "It should also be noted".
Man, that's weird. It doesn't even have to be mutable for that. oO Thank you.
`entry` is an `&amp;mut V`. Making it mutable would allow you to change *where* it points. 
Congrats! Once the dust settles, we'd love to drop by again to get the full scoop.
Complex numbers can be represented (and therefore are) a specific form of vector, vectors are not complex numbers. Some properties that apply to complex numbers do not apply to complex vectors. For example the multiplication of two complex numbers shows the commutative trait, the multiplication of two vectors is not commutative.
Somebody recently opened an issue about not being able to find a cargo package and they're also using multirust. Is this related? https://github.com/phildawes/racer/issues/447#issuecomment-164927466
Yes and no. The rust standard library still needs to target the given OS/architecture pair if you want a full rust stack. I believe for bare-metal without stdlib you just need llvm support, though
Well mine is less impressive..I'm learning rust..any places better than the rust docs? Or just keep going through that?
It is, with [cargo-clippy](https://github.com/arcnmx/cargo-clippy).
If you mean ARM Linux, it used to be problem-free until rust 1.6 and this [issue](https://github.com/rust-lang/rust/issues/29867). Rust 1.5 can also be affected (updated libc 0.2 crate to match 1.6) in some cases but at the moment it's still the best version to use. Otherwise expect compilation errors. 
You might consider using readline's rl_free function. That should guarantee the same allocator that readline used to allocate the memory is used to free it. Odds are this is exactly the same as calling libc::free, but I think it's conceptually more correct.
Is there an easy way to install it? Both `cargo install clippy` and `cargo install cargo-clippy` fail for me.
Are you using nightly? According to the Readme, cargo-clippy requires nightly to build. 
And then you can install the atom plugin and get rustfmt on every save.
Even in the bare-metal case, rustc needs to know some things about the C ABI on that target. There's a minor annoyance in LLVM in that the exact way the C ABI is lowered into LLVM IR differs depending on the target. This goes back to the different ways that GCC targets were implemented; llvm-gcc copied GCC, and Clang copied llvm-gcc.
&gt; Just. Don't. Know. You surely know: the IDE has compiled the code already, and if you're curious, you jump to definition.
I'm aware of [clippy](https://github.com/Manishearth/rust-clippy/issues?q=is%3Aopen+is%3Aissue+label%3AE-easy) (full disclosure: I'm working on it), [servo](https://github.com/servo/servo/labels/E-easy) and [rust itself](https://github.com/rust-lang/rust/issues?q=is%3Aopen+is%3Aissue+label%3AE-mentor+label%3AE-easy). There are probably others.
And it's gotten a few updates this week!
Or vim-autoformat which supports all kinds of formatters.
Upgrading from Rust 1.5 beta to 1.5 stable it behaves as expected.
&gt; the IDE has compiled the code already I don't use IDE, and Scala's IDE are Java based and usually bulky. 
I would even say that the difference is that complex numbers can be multiplied, while vectors cannot. The dot and cross product are actually concepts quite separate from the concept of multiplication.
Perhaps cargo clippy can pick up the path via env var where clippy's output is kept and use it?
&gt; descriptors such as 'single', 'double', etc. cause more confusion than benefit. Yes. Yes they do. There was a reason C99 brought in `stdint.h` and part of me wishes there was a `stdfloat.h` equivalent, although obviously the whole integer situation was a _much_ bigger problem.
You can also use [rust-gmp](https://crates.io/crates/rust-gmp/) or [rust-mpfr](https://crates.io/crates/rust-mpfr/).
It does not have floats though, does it?
Indeed: just copy the contents of the `doc` folder to the `gh-pages` branch and add an `index.html` file containing a redirect, like I did [here](https://github.com/m-decoster/RsGenetic/blob/gh-pages/index.html).
Nothing special, but [RsGenetic](https://github.com/m-decoster/RsGenetic) is almost where I want it to be: * Sequential implementation * Lots of tests * Some selection algorithms * Fast execution In case you're wondering, RsGenetic is a library for executing genetic algorithms. Someone asked on /r/rust if such a library existed, so I decided to make one. As this is only my second project with Rust, I'd love some feedback. You can be totally honest, as long as it's constructive. [Here](http://m-decoster.github.io/RsGenetic/rsgenetic/index.html) is the documentation and [here](https://github.com/m-decoster/RsGenetic/tree/master/examples) are two examples on the usage of the library. I'd love to hear some criticism on: * The structure of my code. Especially: are `sim/seq.rs` and `sim/mod.rs` too messy? * Anything I might be doing that is not idiomatic in Rust. I'm still learning a lot, but I love this programming language so far, so I want as much feedback as I can get. Thanks!
Question: What is this error about? Does rustfmt have a maximum line length that it works with? Rustfmt failed at /Users/greg/src/advent13/src/main.rs:24: line exceeded maximum length (sorry) 
[text_io](https://crates.io/crates/text_io) has a simple scanf-like macro, though it requires a compiler plugin (nightly channel only) to use its full feature set.
You could use [owning_ref](https://crates.io/crates/owning_ref/) to do something similar: extern crate owning_ref; use owning_ref::BoxRef; fn foobar() -&gt; BoxRef&lt;[u8]&gt; { let len = 10; let mut result: Box&lt;[u8]&gt; = vec![0;20].into_boxed_slice(); for i in 0..len { result[i] = 2; } BoxRef::new(result).map(|x| &amp;x[0..len]) } Note that this does *not* free `result[len..]`. (I don't think most allocators even provide a way to do that.)
That should really warrant a warning and not an error, no? It would be rather silly to have a failing build chain because a line was too long. (I know it would be better to just continue the build if fmt fails, but people tend to write scripts like that somewhat naively)
It would be great to have an RSS feed on your blog :)
Indeed :) I'm working on a new backend that will have RSS and some other features. In the meantime, the current mess will have to do.
It may not be an error in the way that one would expect. It still continues formatting and not even the return code is changed. It's really just a sternly worded warning.
I am having some problems: It looks like cargo-extras already had it bundled, but the executable is missing? $ cargo install rustfmt Updating registry `https://github.com/rust-lang/crates.io-index` Downloading rustfmt v0.2.1 binary `cargo-fmt` already exists in destination as part of `cargo-extras v0.2.2` $ cargo fmt thread '&lt;main&gt;' panicked at 'failed to execute rustfmt: Error { repr: Os { code: 2, message: "No such file or directory" } }', /Users/tbelaire/.multirust/toolchains/nightly/cargo/registry/src/github.com-88ac128001ac3a9a/cargo-extras-0.2.2/src/cargo-fmt/src/main.rs:16 An unknown error occurred $ rustfmt -bash: rustfmt: command not found $ ls ~/.multirust/toolchains/nightly/cargo/bin cargo-add cargo-clippy cargo-count cargo-graph cargo-open cargo-rm cargo-watch cargo-check cargo-config cargo-fmt cargo-list cargo-outdated cargo-script How do I go about fixing this? 
It did tell me the path, which I did `ls` above. What did do the trick was cargo install --bin rustfmt rustfmt for some reason.
Also: a Git hook perhaps. Or: a bors/homu routine to reject PRs that aren't formatted with rustfmt. Live the dream!
Looks like `cargo install` should install dylibs for a given binary in their own directory, and `cargo clippy` should have some means to look for libs in this designated directory -- looking for dylibs in the same directory as the binary don't work in most environments (and distro packages wouldn't be able to package clippy, etc)
Failed to run on MacOSX. I installed and executed `cargo fmt` but getting `No such file or directory (os error 2)` What am I missing ? $cargo install rustfmt Updating registry `https://github.com/rust-lang/crates.io-index` binary `cargo-fmt` already exists in destination as part of `rustfmt v0.2.1` $cargo fmt No such file or directory (os error 2) usage: cargo fmt [options] Options: -h, --help show this message This utility formats all bin and lib files of the current crate using rustfmt. Arguments after `--` are passes to rustfmt. $ls Cargo.lock Cargo.toml README.md examples src target 
I wish I knew enough about development to do these jobs :/ Would be nice to get to work on security stuff and Rust at the same time.
For a simple app like this the number of dependencies (and dlls) is staggering - is that normal for rust programs?
Try cargo install --bin rustfmt rustfmt That's what fixed it for me.
Cool, good luck!
I'm going to see if cargo install clippy can work today.
The scope of macros is very narrow (compared to what you want to do with them at least). Right now, the easiest way to solve 1. and 2. is to write a plugin (which you can do in plain Rust). Maybe someday, if generics and const fn get powerful enough, there might be an easier solution, but that's not the Rust we have today. 
Crates are linked statically by default and unused code is eliminated. You don't need to worry about dlls or bloated binaries. Source: I asked for you in [the irc](https://chat.mibbit.com/?server=irc.mozilla.org%3A%2B6697&amp;channel=%23rust). I got responses instantaneously :)
How does code size compare to gcc -Os?
Right, and as I said, I do appreciate that this requires a lot of effort. I'm not requesting that everything will be implemented right now but I do want to see such use cases to be considered for future addition. Rust so far was very good at sticking to basics which is a sign of quality design instead of throwing in many special case features that generate mental overload and complicated interactions. C++ is notorious for this, so is Perl. IMO D fell into the same trap and while it has capabilities that Rust currently lacks (especially in the meta-programming department) it also has a soup of attributes and annotations. I want to see the macro design to be general enough to allow future extensions in the same vein as other core Rust concepts (the ownership model, traits) that allowed later on to naturally derive other features (closures).
Welp, that was it, unbuffering Python and forcefully fflushing stderr gives them similar sys profile to Rust'ss, proof positive that I really need to sit down with dtrace and learn how to use that thing one day, I could have found that on my own. Thanks.
Wow, this is a great tip. Thanks!
You can perform the scan with a `fold` because `fold` takes a `FnMut`. If a string has no characters we know it can't possibly have two characters in a row, so if you handle the empty string case, you can use the first character in the string as the initial character, and the remainder to run the scan on. My (admittedly somewhat crazy) version is like this: let mut chars = word.chars(); chars.next().map(|mut prev_char|{ chars.any(|c|if c == prev_char { true } else { prev_char = c ; false }) }).unwrap_or(false) 
There's also zipping on an offset copy: word.chars().zip(word.chars().skip(1)) .filter(|&amp;(a, b)| a == b) .next().is_some() 
Are we golfing? ;) words.chars().zip(words.chars().skip(1)).any(|&amp;(a, b)| a == b)
&gt; so it should do the same thing "should" and "will" are very different things.
Well, that one is convincing. Still, thanks to all of you. Quite some inspiration. The second part of day 5 has the same task but with one character between the equivalent characters. This is perfectly reusable: fn doublette(word: &amp;str, padding: usize) -&gt; bool { word.chars().zip(word.chars().skip(padding + 1)).any(|(a, b)| a == b) }
Yes please!
We suggest some iterator things. Slowly getting more.
What I was actually trying to do was transmute different size types without having to use a vector. (You can't transmute into a vector) I guess what I mean is, what is the point of restricting array size/ resizing on a box? Isn't it heap allocated? That makes sense for regular arrays, as rust doesn't want to implement dynamic sized stacks, but boxes are on the heap. Why doesn't it act like a C array (besides being on the heap rather than stack)? So if I want to use the stack to return a array, with size determine at runtime, without reallocating, I have to * Create a vector with the size I want * Convert to Box * Manipulate it (Transmuting in my case) * convert it back to a vector * re-sizing the vector (removing unnecessary elements) * If I don't want to deal with the unneeded memory required for a vector, convert it back to a box And you still end up copying size info to convert back and forth unnecessarily, besides being so verbose. I assume the reason you cant allocate dynamically is because when you do Box::new([u8;12]) The array is allocated on the stack then copied to the heap. What I'm proposing is that a box is created with something being allocated inside of it, being able to allocate directly on the heap. So you could allocate an array with a non-fixed size. (Is there a reason you cant slice a box? I assume it has to do with that you have to free the pointer you allocated with) If that is the case, I guess I should be asking why I cant transmute into a vector.
Hmm, I actually just changed to replace because the idea of overwriting a user's files without a backup terrifies me (note that we have had bugs in the past that lose some code, there probably are still such bugs which we haven't found). Not everyone uses Git (they should be, but still...). So, I guess we could make this change if there is enough demand. We could also offer a quick way to delete all the backups or something.
Cool! Performance improvements are starting to be a higher priority since they're needed for some other features (e.g. skeletal animation). There's some options for algorithmic improvements I have in mind, and probably plenty of finer grain/micro optimization opportunities as well. I try to hang out in the #rust channel in the evenings (I'm on US mountain time), my username is Twinklebear.
I'm still having trouble figuring out what you're trying to do. You didn't post a whole lot of code, and gave little context, so everyone here is just guessing what you're actually after. Are you trying to transmute`&amp;Data` to an `&amp;[u8]` to get the byte-representation of `Data`? In that case, you cast the pointer just like you would in C, but you also have to get the number of bytes in `Data` to create the slice (for bounds-checking): use std::{mem, slice}; fn get_bytes(self: &amp;Data) -&gt; &amp;[u8] { let data_ptr = self as *const Data as *const u8; // You can subtract from this if you don't want to give out all the bytes in `self` let size = mem::size_of::&lt;Data&gt;(); unsafe { slice::from_raw_parts(data_ptr, size) } } That will return a `&amp;[u8]` which represents the bytes that make up `self`. It's purely a view into that region of memory; it won't try to free `data_ptr` when it falls out of scope. Its lifetime is also tied to that of `self` by elision, so a use-after-free isn't possible. 
You can handle any of the "contains at least one sequence of X letters that does Y" with the [Windows](https://doc.rust-lang.org/stable/std/primitive.slice.html#method.windows) iterator. It's only available on `[T]`, not `str`, so you'd need to do it on the bytes representation and throw out unicode support, which is fine for Advent of Code.
Continuing to work on [notty](https://github.com/withoutboats/notty). The rest of this week (and maybe after) I'm going to focus on [hocus-pocus](https://github.com/withoutboats/hocus-pocus), the framework for writing apps that use notty features - implementing functionality like both readline and ncurses. This will use features of notty when its running in notty, and an in-app polyfill library doing the best just to emulate those features when running in another terminal. I'm focusing on the readline-like module right now. If anyone doesn't know, GNU readline is a library for writing shell/repl type interfaces. The big difference between readline and hocus-pocus+notty will be that the interface for hocus-pocus+notty will be run inside the terminal instead of inside the app, the benefits of which are most noticeable when using ssh. And of course hocus-pocus apps will have easy access to other features of notty, like writing out raster graphics.
That error means that cargo-fmt can't find rustfmt. Did you add cargo's install directory to $PATH?
Yeah, I've been bitten multiple times due to `stdout` and `stderr` differing buffering strategy in Python (`stderr` always uses line buffering, whereas `stdout` uses line buffering only when the output device is a terminal), which messed up the logs when run in systemd. It was a quite surprising experience before knowing what is exactly going on.
The extra buffer is helpful, but only if you aren't already providing big writes. (Just something to keep in mind). We also have an incoming PR to speed up the line buffering by using a faster scan for newline, which speeds up the non-textual output use case.
Thanks for your comment :) The "lost fight" referred mostly to the fact that I gave up trying to fix it myself, not that the devs didn't answer - I'm aware they're busy and that my description of the problem doesn't make it easy to find the cause ;) If I come across a simpler case that generates the same problem, I'll surely post it, but for now unfortunately I haven't been able to find one. That's actually one of the reasons the problem is so mysterious ;)
My idea is that [the write mode should be settable in rustfmt.toml](https://github.com/rust-lang-nursery/rustfmt/issues/215). This would allow users to set the write mode per-project, without always having to specify the write mode on the command line.
No idea. I haven't done any direct comparisons. I haven't even compiled my code with optimizations so far.
:(
Not in Rust it isn't, but it is in general. Again you generally stop calling them macros but it is possible to do type-based code generation at compile time, it's just not something Rust has implemented yet (they may never do). [This is an example](http://docs.julialang.org/en/release-0.4/manual/metaprogramming/#generated-functions) and [this](http://docs.julialang.org/en/release-0.4/manual/metaprogramming/#an-advanced-example) explains the array stuff I was talking about.
/u/steveklabnik1 the book is pretty awesome already with respect to the language and some parts of the standard library (concurrency, error handling and iterators comes to mind). Is there a plan to cover how to use the standard library effectively in some other part of the book? Otherwise maybe the "effective Rust" section could be expanded with the rationale behind, and how to use effectively, some other very stable parts of the standard library: - containers (vec, hash tables, trees) - strings (and buffers, and slices, and unicode) - I/O (to files and screen, not network I/O) - time The intro docs of the standard library are pretty good, but it would be nice to have something like that in the book as well. 
I think [this website](http://arewewebyet.com/) summarizes it pretty well. I think it mostly depends on what you want your web app to do, for more complex things I would definitely recommend not using Rust at this time, and just go with a nice garbage-collected language with good library support. I have no experience with Swift and Go, but c# is quite nice in my experience. Edit: So ignore the website, bad advice :)
Good to know! Is there a similar website that has more up-to-date information?
So would you then suggest either Go or .NET? I'd rather not venture to pre-gc Objective C style code writing if at all possible.
Sadly setting up C# on a Mac is not all that friendly heh (just tried) but I'll check out Go and see how that go(es) for me...hehehe. Do you have any experience with Go?
Java is quite good at web stuff as well.
How so? I've looked at some of the frameworks like Spring, Play and I think one or two other ones but can't remember much about them.
I would suggest looking at Scala and the Play framework. To me, a strong, static type system with inference and support for user-defined algebraic datatypes is nearly as much of a safety and productivity gain as automatic memory management. Scala has warts. Play is quite big. But if you can swallow that, it's easy to get going and quite pleasant once you learn to work with it, and it's nice to be able to use ADTs to turn aspects of your business logic into type-level problems so that the compiler will catch logical errors. [This is a video that explains the benefits of ADTs](https://vimeo.com/14313378), among other thing, using the OCaml language. It carries right over into Scala, F#, Rust and Swift. The type system is the reason that I rule out Go completely. It's just not good enough.
Ive never been a huge fan of java for the web, given all the xml config and bloat in things like Spring. Glad to see a framework that looks a lot like Sinatra/Express though :)
[ASP.NET 5](https://docs.asp.net/en/latest/getting-started/installing-on-mac.html) is just around the corner, along with the [CoreCLR](http://dotnet.github.io/), which will run in OS X. They are both in pre-release (RC1).
The second SO answer solves the first three use cases well enough for me - it's pretty neat. I guess I will move away from macros to solve these more complex problems. Some concatenation functions will solve the problem well enough I think.
What'd you go for instead? Just pull in libraries yourself? I made a not-small Play app at my old job. It was my first real web programming gig and first time using Scala (though I was familiar with both functional style and Java, so not so much new material, really), and Play was very nice to get something off the ground. I didn't feel it got out of hand after a year and a half of development, certainly not through any fault of the framework, but it was quite obvious that Play wasn't perfect. I'm currently doing dev ops and some PHP, but I have an idea for a part time thing I want to try out, and I was thinking I'd do the first prototype in Scala, at least, since I'm quite familiar with it. If you've tried any other libraries/frameworks I'd love to know about them... although maybe it doesn't belong here.
Well, I don't blame you, it is kind of hard to tell whether the website is outdated without looking at Github. This is a failure of the website IMHO, it should mention a date or a Rust version *somewhere*. &gt; Still, if nobody ever posted that link, when would you get to tell people that it's never updated? :p It is not really like nobody is posting it, because it comes up every time someone asks about using Rust for web development.
If you want something close to the Apple eco system, I would probably pick Swift (I think it is intended to replace Objective C). To me, this languages also seems to be the most interesting one out of your list (after Rust ;) ). Can't really comment on Go or .NET, because I have not used them much personally.
&gt; Swift seems a bit like Python which isn't bad but isn't exactly new. Swift is a very different language and I'd argue is the closest to Rust on your list. The recent open sourcing means it runs outside iOS and I'm sure there are http server libs but I'd be surprised if there was a swift web ecosystem at this point. You can [write web apps in Rust](https://github.com/flosse/rust-web-framework-comparison) but I don't think it's a particularly compelling candidate at this point.
If you stick to Option instead of returning null, you probably will never encounter an NPE, besides go,python, ruby and js all have the null problem. 
This is patently false, Option does not avoid null pointers, as the linked slide shows. And yes, something similar to this has actually happened to me in production due to poorly written third-party Scala library code.
Java is probably the most used web app language, for actual serious businesses that are not startups. You can also look at Scala if you want a more powerful type system like Rust. Clojure you can program both front and back end as well if you are looking at JVM languages. Java you can call directly from the html sites, or call a jar or a war. It is a massive ecosystem that is very heavily used.
If you program scala in a functional or almost funcional style you shouldn't find any NPE. At least I very rarely have them. Of course they appear on the edges of the system and while interacting with java libraries, but if you wrap returning things into Option(potentialMethodThatReturnNull) it will be converted to a Some or None as expected an you will not have problems.
Rust probably wouldn't be a great choice for bog standard web development. You could do it, and it wouldn't be as hard as a web server in C++, but you'll lose a lot of nice features that are common in the more web development oriented languages. The most important thing you'd lose however is a large ecosystem of existing web development code, tutorials, documentation, questions, etc. That said, rust could be used to write web apps, but I would only consider it if you needed to pull more performance than other solutions gave you, or you wanted memory safety to avoid situations with security flaws. An Nginx/apache alternative would work great in rust for example. As for what to use, so far a lot of the replies in this thread so far are solid, such as scala, c#, and elixir. Personally I would recommend using a language such as JavaScript or Python for a web app unless you know you have specific requirements, or you need certainty that your program type checks. I would very much recommend considering JavaScript with node.js if you're planning on making a web app. In recent years it has advanced rapidly, and there's now a massive ecosystem(arguably one of the more important parts of a language) surrounding JavaScript, especially for web development. You lose out on a few of the features in common with all the other languages you mentioned, e.g. static typing, but this can be solved with a transpiler such as TypeScript. A lot of people dislike JavaScript a lot, but I'd suggest at least considering it, since JavaScript isn't as bad as it used to be, and it has become very comfortable to use lately.
For some quick bootstrap it works fine and even for medium sized projects, I mean even for our large works almost fine. The problems appear when for example updating from branch version 2.3 to 2.4 where everything breaks and I'm doing that upgrade on our code base for a few months already. Every corner of the application has to be touched because it's not a minor upgrade, it changes very deeply some core things of Play.... /triggered... /rage... Damn. Right now if we could choose and we are expecting to migrate eventually to a more micro service style application we would like something more light for routing library, JSON library and some slick for database instead of Anorm. So the answer would be more into hand picking individual libraries that work well together instead of a big monolithic framework that imposes its style too much on you.
Gurr. I'd ~~repressed~~ forgot about Anorm.
For the most part, xml config stuff is a legacy of the past. A lot of newer java stuff has moved away from that and towards pure java solutions to do things. The annotation system is relied on heavily to get things done that were previously done through xml configuration files. The only time in my day job that I touch xml is when I'm adding a new dependency to maven. Other than that, I never deal with it.
&gt; What if I don't care about memory management, Okay, so this is something I've been thinking about. Sorry in advance if this turns into a wall of text. Historically speaking, Rust's "advertising/marketing" has been focused on the memory safety aspect, and the lack of GC. Which is great! We've accomplished something awesome. But it's been nagging me... that is the _mechanism_, but it's not the _feature_. Like, most programmers in my circles never manage memory. So "hey you can manage memory easier!" isn't a compelling pitch. But if we look at what memory safety _gives us_, ie, the actual problem it solves, not the mechanism itself, it becomes more compelling: * concurrency errors _at compile time_ * incredibly robust code that should never crash * Predictable and high performance * Not giving up high level features like iterators or closures Those are the kinds of things that make Rust _actually_ compelling. But we've been mostly promoting the mechanism by which we get those features. Because we're working on a language, and so it's easy to get caught up in what people working on a language care about, which is "how does it work", not just "what does it do." Anyway, that's a brain dump. I dunno.
I completely agree with you. I know youve mentioned on twitter before you're not a hug fan of Go. But I think Go, or Ruby or .net even one can just be more productive now on building out a webapp, and have the performance they need. That said, someone might need something super fast &amp; robust, and that's where having rust as an option is super awesome.
&gt; I think Go, or Ruby or .net even one can just be more productive now on building out a webapp, Agree 100%. If for no other reason that the ecosystem being much more mature. I generally find my productivity is directly related to how many other LEGO I can pull in :)
Indeed, Swift and Rust are particularly alike, and very different from Python. To that note, I will add, that I believe Swift has an upper hand on Rust in that it can and will equal Rusts performance, even when in five years where Rust should be very, very fast, because Swift will be so too. That performance will be coupled with a high level feel that can drop to low levels when necessary. I also believe that Rust has the ability to add high level easy-of-writing to the language using macros, which would make it easier for Rust to compete with Swift.
Trailing commas... trailing commas everywhere. Is there a single option that will remove them all at once? (enums, structs, match arms, struct literals..)
Definitely. If OP doesn't know JavaScript, this should absolutely be the thing he learns right now. There is, perhaps, no more important language to know for web development today. 
Haskell, F#, OCaml, Scala, Swift and a number of others exist in this space.
There's not, but there ought to be options for each place (and once you've got a config file, it doesn't seem too bad to have many options in it). If you find any cases which don't have an option, please submit an issue.
If you're trying out Go I have a couple of suggestions for you. Use the Go plugin for your favourite text editor, this should simplify your life a lot. [Gosublime](http://blog.campoy.cat/2013/12/integrating-goimports-with-gosublime-on.html) for SublimeText and [go-plus](https://atom.io/packages/go-plus) for Atom are great. There are also packages available for vim, VS Code and IntelliJ Idea. What these plugins do is that they automatically add and remove import statements, apply the `gofmt` formatter, run linters etc. If you're looking for resources to learn Go, I think the [tour](http://tour.golang.org) and [gobyexample](http://gobyexample.com) are good. I'd also recommend reading [Effective Go](http://golang.org/doc/effective_go.html). If there's anything else you'd like to know, PM me. I'd love to help!
RAII is still manual because you need to manage ownership (and sometimes you need to structure your whole codebase to make it palatable to a single-owner discipline). You *can* use `Rc` everywhere but then you litter your function signatures with useless stuff. There is no silver bullet and I'm not suggesting that Rust should be different. But if everything had shared ownership (like in Java or in Python), memory management would be *simpler* -- in detriment of other kinds of resource management, in which RAII still have an advantage in not littering the with `.open()` / `.close()` stuff.
Swift is definitely in the same performance ballpark for things like application programming or web dev, but Rust's defaults are more towards high performance. For example, idiomatic Swift uses a lot more boxed, reference counted objects with dynamic dispatch. Idiomatic Rust instead uses a lot more stack-allocated objects that you just pass around references to, because the borrow checker makes that a viable option in more situations.
&gt; I've been reading a lot on how Go is not a suitable language "There are only two kinds of languages: the ones people complain about and the ones nobody uses" - Bjarne Stroustrup
Unfortunately, while I can push to the arewewebyet repository, I can't actually publish the website, so it has stalled.
I agree with this. I've found that Rust's strict ownership semantics help enormously to come up with a good structure for my programs. I read a lot of comments about how Rust's ownership stuff is extra work you "don't need to worry about" most of the time, but I am _glad_ to have rustc helping me figure out what my program should actually look like. Its why Rust is so easy to refactor.
I think this work deserves some more attention. So thank you :)
In that case, a `Vec` even makes more sense. `Vec` is the preferred type for managing heap allocations of a sequential list of items. However, if you know your output type, you can create it on the stack with `mem::uninitialized()`, use the byte-slice trick above but for `&amp;mut [u8]`, and write into it that way. You should make sure your struct has the `#[repr(C)]` attribute so the field ordering is well defined (Rust reserves the right to reorder fields for alignment purposes). http://is.gd/OoXU4Z 
With web development, I would say that you should NOT make the programming language your first consideration, unless you're using a web project as an excuse to learn the language in question. If productivity **is** your major concern, then your first consideration should be what, if any, web framework features you expect to have during development. This varies greatly by developer with answers varying from "CGI suits me fine" all the way to "I must have a rich component framework in the UI bound to the server-side object model". Once you've decided which framework, audience, and deployment situation you want to suffer with, your language picks itself. 
Use ... **Rust**: If you want safe manual memory management, low level functionalities, a modern language and become a better programmer but slower development and less libraries for web development (but there is another web framework [Nickel](http://nickel.rs/)). **D**: If you want a language with low level functionalities, GC and a great web framework ([vibe.d](http://vibed.org/)). **Go**: If you want a language which is great for web development, with good libraries (like [Revel](http://revel.github.io/)) but not so well-designed from the ground up. **Elixir**: If you want to learn FP with great concurrency through green processes powered by BEAM, a great web framework ([Phoenix](http://www.phoenixframework.org/)) and a syntax mix of Ruby, F# and Erlang. **F#**: If you want a little more FP feeling, the power of .NET, productivity and an imperative or declarative style, a good web framework ([Suave](http://suave.io/)) but not so good support for unix systems. **OCaml**: Same feeling like F# (because F# is OCaml for .Net) but better suited for unix systems, great module system and a Sinatra like web toolkit ([Opium](https://github.com/rgrinberg/opium)). **Haskell**: If you want the king of FP, HKT, one of the best type system, mind blowing, become a better programmer (same with Rust), great web libraries (like [Yesod](http://www.yesodweb.com/) or type safe web services (!!!) ala [Servant](http://haskell-servant.github.io/)) but you need to relearn a lot because it is different than the imperative languages. All these languages have other pros and cons but I think we can go deeper into the details if you have some favorites.
That's just a random decision made by a scheduler in kernel. There are many possible reasons, even "let's switch cores sometimes to lower CPU temperatue". If you really want, you can pin the process to use only specified cores using `taskset`, you can probably do it on thread granularity too.
Thank you for this list. Seems like it's a toss up at this point between Go, Elixir and Haskell. I don't really have an intention of this beyond just learning a new language and web apps are where I find the most use. Question though, what do you mean by "FP" in your various points?
It means Functional Programming. https://en.wikipedia.org/wiki/Functional_programming
Well, here are the options: https://github.com/flosse/rust-web-framework-comparison From those, I would probably choose to suffer with Iron or Nickel first, though they all seem rather low-level compared to things like .NET MVC. YMMV and all that. You might want to work in a UI framework too just so you're not coding everything from scratch. Good luck!
Both ".NET" (i assume you mean C#) and Go are garbage collected...
But do you use Scala at all? I assumed you wrote a comparison because you know the language, but now I'm wondering if that's the case at all.
Yeah basically C# and ASP.NET heh.
Honestly, no way. Not now, at least. Rust will have its advantages for web dev with its safe type system, but for the time being its frameworks are nowhere near mature enough to be used in production. I'd argue for Haskell - it has the same type safety benefits of Rust, has a few slightly more mature web frameworks, and more importantly, its functional style is well suited for easy high performance web development. That said, you MUST consider how much free time you have. Are you studying at uni? Are you working on other projects? Are you thinking of joining clubs? Do you have a job? Haskell takes a lot of time to get started with so if you already have a busy lifestyle you'll need to accept that it may take as long as a year to get competent at Haskell. If you do have a busy lifestyle, I'd recommend just sticking with Python for now, perhaps with Django. It's already a good solution for web dev, if you want to expand your repertoire of languages you might want to look at something a little lower level, maybe make a simple video game with C++? C++ takes even longer than Haskell to get GOOD at, but with Python as a base getting started in C++ is a lot quicker since at least superficially it's a similar imperative style.
This worked. Thanks !! Got few errors with `line exceeded maximum length (sorry)`. All of these were comments using `//`. I think `rustfmt` should understand `//` and should be able to split into multiple lines :)
I'm a little late to the discussion here, but honestly, I think Rust would be an amazing language for web development. At this point, I'm just waiting on the web frameworks to stabilize. I don't understand the negativity I've seen in this thread about using Rust for web development -- it's a pretty productive language, very high performance, and Rust places a great emphasis on increasing security through having a more strict compiler, and security is deeply important to me these days.
And ironically, the #1 problem on front-end (and now node.js) is memory leaks.
No, it does somewhat more than that. For example it'll shuffle around imports and is opinionated about syntax where there are multiple ways to do the same thing (e.g., `extern "C"`).
Hmm, would we really call what Rust has duck typing? I feel you could make that argument for Go-style interfaces, but not for Rust traits, any more so than you could for interfaces or typeclasses in other languages.
For type info, do you just want typeof()? It ought to be possible to typeof() similar to how it works in C - your macro would include a typeof() call in the expansion which would then expand to the the actual type later in the compilation process.
Nice post.
The meaning of Rust 2.0 *was* discussed at the last workweek, but iirc it was a bit of a standoff without any hard conclusions. I think the strongest contender in those talks was for 2.0 to be a publicity/milestone thing, and not at all related to backcompat (because the strongest opinion was that hard breaking changes simply should not be on the table). I think there was also some discussion of LTS support or summat.
Regarding the first part, the part about Strings: Does it really make more sense to have two different String types, one that exists on the stack (and is immutable) and another on the heap (and is mutable) when you could instead have just one type and then have keywords allowing the programmer to determine where the object goes and whether it is mutable or not? I.e., in C++ you could say const mystr = "hello"; That's on the stack and it's immutable. Or you could say: auto mystr = new std::string("hello"); and get a String in the heap that is mutable. I'm NOT saying Rust's way of doing it is wrong -- I'm sure there's a very good philosophical reason for it, and as someone who is used to C++ conventions, I'm curious what they are. (I know there are some quirks with my C++ example -- I'm pretty sure the stack-based string isn't actually a std::string, but a c string, but you get the point)
I've just posted a comment that addresses some of the concerns, [see my other comment!](https://www.reddit.com/r/rust/comments/3xavud/what_one_must_understand_to_be_productive_with/cy35nse) So, as for your question, `&amp;str` is *not* a stack-allocated string. It is, in C++-speak, similar to `char*` or `std::experimental::string_view` in modern C++. So you may get the idea: `&amp;str`/`char*`/`std::experimental::string_view` are used to pass a "view" of a string from here to there cheaply, without copying or moving. `&amp;str` also has a useful property that `char*` doesn't. It conveys a length. So you can safely create a substring without actually copying anything, like this: let a = String::new("hello"); let b = a[1..4]; Here, `b` is of the type `&amp;str`, and its content is "ell". Translated to C++, it is something like: auto a = std::string("hello"); const char *b_ptr = a.c_str() + 1; size_t b_len = 3; As stated above, modern C++ has a similar capability using `std::experimental::string_view`.
Yes, but what are the real numbers, and not just the theoretical ones? I find that Swift and Rust performes very much alike, for the cases I have tested, but that has not been anywhere near a proper benchmark, so I don't really know. But for fun, lets say they truly are equal in speed at the moment, what about in 5 years? I know Rust will be faster, because the core team says so, but so does the Swift team. Right now Swift doesn't do anything but refcount everything, and do some compiler-optimizations where it can determine how something will end up, and sort that out at compile instead of at runtime, but it's not very deterministic and we as users of the language and compiler can't reason about it in the same way as we can with Rust and the ownership model, but if you were to believe the rumors, and what Craig Federighi said on The Talk Show[1], they are planning to be doing much more at compile time in the future, and only what really needs to be highly dynamic Objective-C style is managed at the runtime. Now, this is only me guessing at stuff. Would be cool if somebody with hands on experience would chime in on the matter! [1]: https://daringfireball.net/thetalkshow/2015/12/14/ep-139
Procedural macros are compiler plugins, no? They are part of the redesign. Besides, "there should be only one way to do it" and this distinction only serves to confuse people. 2 is what I thought the answer would be which is great :) But this leaves my questions regarding ```const fn``` unanswered. 
I'm asking for compile-time type introspection, not C's typeof operator. for example, I want to auto-generate an impl for some trait A: ``` gen!(MyStruct)```. If MyStruct already implements an independent (but related) trait B, I can reuse its functionality for this so I'd like to ask if exists ```impl B for MyStruct```. 
&gt; [you can make a string upper/lower case in-place.](http://doc.rust-lang.org/nightly/std/ascii/trait.AsciiExt.html#tymethod.make_ascii_lowercase) ...in the ASCII subset.
I'm going to make some corrections here so users do not get confused `const char str[] = "hello"; // "hello" resides in stack (could be optimized away into data segment given const-ness)` `const char *str = "hello"; // "hello" resides in data segment` `static const char str[] = "hello"; // "hello" resides in data segment` `std::string str("hello"); // "hello" resides in heap` There are two different types because `String` is owned, and `&amp;str` is not owned. This parallel's `std::string` and `const char *` in that sense. `String` and `std::string` will be freed when their scope ends. `const char *` and `&amp;str`do not.
Does that work for Rust? Given that it's a Mozilla blog, I would expect them to say if it did.
Furthermore, even if you know that the string that you *currently* have is ASCII, there's no way of knowing if the uppercased/lowercased string will still be ASCII without knowing the user's locale.
I had always presumed that 2.0 was just going to be us jettisoning all of our deprecated 1.0-era stuff, but then Alex went ahead and did that last week anyway. :P I dunno what minor stuff is left... we could clean up stuff like https://github.com/rust-lang/rfcs/blob/master/text/1229-compile-time-asserts.md , we could, er, remove numeric literal suffixes if type ascription lands... we could do some spring cleaning on the rustc command line flags and actually have some sort of stability guarantee on them. And maybe we could get rid of `static mut`, depending on how const comes along.
Where'd you read Go isn't for webapps? Usually people are saying Go *is explicitly for webapps*. :) I've been coding in Go for a while now and I love it, and its handing of web systems is great. Not needing a dedicated webserver in front of your webapp is nice, too (though Caddy is an awesome server if you need one). Rust is one I'm watching closely but because a lot of my wants involve cross platform I have been focusing on Go instead, which is really winning the cross platform race against basically everyone else, even Lua/Python.
Ok, the typing and String/str thing has already been discussed, but who here really dislikes the borrow checker? Why'd you call a buddy that has your back a 'fucking buddy that has my back' (FBTHMB for short)? I've been working for some time with Rust now, and never *ever* cursed at borrowck. In fact, every time I got a lifetime error, I was thinking "Thank you, borrowck, for saving me from my own feeblemindedness!"
&gt; As a counterexample, *m = a, where m is not "let mut" is fine, if the type of m is "&amp;mut _". I don't think that's true, there's no way to get a mutable reference to an immutable binding is there? Edit: [Turns out I was wrong.](https://play.rust-lang.org/?gist=24b1382120bc736c6bd9&amp;version=stable)
&gt;Instances of structs passed as implementors of traits are referred to as trait objects. Hm. I thought trait objects are a different thing altogether - a special wrapper used for dynamic dispatch.
thanks!
If I have a struct `Foo` which implements `Bar` and I pass it to a function expecting `&amp;Bar` (or cast it with `&amp;Foo as &amp;Bar`) then it's a trait object.
It should depend on the language that text is written in, not on the user's locale.
You're both sorta right. Trait objects do perform dynamic dispatch, but they always have some concrete type underneath. The primary goal is runtime polymorphism. 
Could you explain the advantage of your `unsafe`-using implementation over a naive one that does not, e.g. [something I just threw together](https://gist.github.com/anonymous/0468e78cf26d34857b73)? Is your performance better, or does it properly handle scenarios where my `RefCell` version would panic? Edit: I think streaming iterators would be nice for this use case, it's a shame there is no support for them in the standard library.
The iterator protocol does not specify behavior if `next` is called after `None` has been returned, so I personally would not recommend just calling `next` twice in a row. In practice most iterators are well-behaved, but in theory they could just panic when you call `next` after getting a `None`.
&gt; How so? I've looked at some of the frameworks like Spring, Play Use [DropWizard](http://www.dropwizard.io) to scaffold a complete Java project. You'll get massively-tested logging, web-serving, metrics recording, database pooling etc. all together, run from a Jetty web-server. Then there's also JMX extensions and remote debugging support for configuring or debugging live apps remotely. Ultimately it depends on how complex your website is: will it be hosted on one server or many, will you need logging, will you want to inspect and reconfigure it without quitting. PHP is the easiest way to get up and running, PHP hosting is everywhere, and documentation is plentiful. For someone new to programming is arguably the fastest way to get a simple website up and running. Facebook also has created it's own "HipHop" compiler for PHP which speeds things up. Go is the next easiest language, and can support more complex websites than PHP. The community is strong and there's lots of documentation, but hosting is a bit rarer I think. Python and Ruby were both strong for a while with Django and Rails respectively, but I'm not sure how popular they are any more. Both are light languages like PHP, with slightly safer and more sophisticated semantics; but neither is particularly fast. Java has been used to build massive, scalable websites for two decades now. The DropWizard scaffolder encapsulates current best practice. However it's quite hard to make a "simple" website with Java: the frameworks assume you want something complex that can scale, which is why Java is overkill sometimes. There's also _quite_ a lot to learn. Java and Go would have similar performance I'd say, and the Go frameworks are maturing quickly. Go is definitely more light-weight than Java. Swift doesn't really have a web-framework library, is still changing quite a bit, and in any event is very similar to Rust. Rust does have web-framework libraries. Compared to Java the language is slightly more lightweight, but comes with different complications. The type-system definitely makes it easier to harden your app (e.g. replacing raw strings with structs). I'd also give a shoutout to F#, which is a lovely little functional language which fully builds on the C# stacks, and has a few of its own. See [http://fsharpforfunandprofit.com/] for more. Personally, for quickie sites, I've tended just to use PHP, and for anything more serious, I've tended to use DropWizard. The Rust space with Iron and Nickel is really interesting though.
Learning Haskell usually takes six months, and the language is beginning to suffer from one too many additions over the years (e.g. three String types, Monads vs Applicative Functors, etc). You can learn the good bits of FP from Ocaml / F#. 
Maybe once the new macro system nrc has been discussing lands, eventually dropping the old macro system?
Just to be super pedantic, that last one should be: `"hello" resides in data segment and is copied to heap`
Thanks, `while let` does look better. I wonder if there is a way to make it even more functional, without any loops. `Iterator` doesn't have a `chunks()` method, though... I want to read a file that follows a certain (simple) format. I probably want a parser, I guess, but I don't know anything about them.
Not shown in the video: The collective groan/giggle ("oh no not integers again") from the area where the Rust/Servo folks were sitting :P I loved the talk, though. Especially the "The core team was too busy shipping software to care about this stuff" part -- it highlights a problem that comes up often in technical communities -- people are too focused on the "technical" aspect to worry about the "community" aspect. Fortunately we're past that stage now, and I love how everyone is trying to maintain the health of the "community" aspect too. And I'm really happy that the main message about Rust at the all-hands talks at Orlando was an overwhelmingly positive one about community. I'd like to see us maintain that image!
&gt; does not specify behavior if next is called after None has been returned, It does specify it: individual adapters may choose to start returning values again if it so chooses.
I quote the documentation: &gt; The Iterator protocol does not define behavior after None is returned. A concrete Iterator implementation may choose to behave however it wishes, either by returning None infinitely, or by doing something else. According to that description, a panic is allowed.
Where is that? http://doc.rust-lang.org/std/iter/ &gt; Individual iterators may choose to resume iteration, and so calling next() again may or may not eventually start returning Some(Item) again at some point. I'm not saying a panic is not allowed. I'm saying that it's not undefined behavior, it's defined behavior. The thing you quoted says that too, though I find its wording to be very poor.
Check out [nom](https://github.com/Geal/nom), it probably does what you want.
By front end you mean javascript? I write a stupid large amount of front end JavaScript for massive web apps. I rarely ever have any problem with memory leaks...like ever. Node js is different, that is not a front end technology. 
borrowck is fundamentally conservative. It can and will prevent correct programs from compiling. Early returns of borrowed data or borrowing branches of a match being classic "fuck everything" situations. Having to add extra scopes or mangle control flow to satisfy it is legitimately annoying.
Impressive, specially when the compiler prevented the bugs on the parallel quicksort. 
&gt;What happens here when you get a (Some(·), None)? I don't care, my file is supposed to have x*N lines. &gt;Still not very extensible to grabbing an arbitrary amount of of lines. Too much typing … The alternative is calling `next()`in a loop and putting the results in a collection.
Nice. But I'm curious: What should I do if I have more than two tasks to run? As far as I see, `join(..)` takes exactly two closures. There is no `join_many(..)` function that takes an array of closures.
Yeah I have serious MIR blue-balls. So much goodness expected to come out of it.
An array of closures would lose the no-virtual-dispatch advantage. You can join_many by doing: `join(|| join(|| a(), || b()), || join(|| c(), || d()))`. It's not great, since it will defer pushing things onto the queue. I could imagine a static dispatch interface like this, though: rayon::joiner() .add(|| a()) .add(|| b()) .add(|| c()) .add(|| d()) .join(); though. Then you could first push all elements in the tail onto the local queue, then start executing.
Ha!
No, **Rust absolutely does NOT have duck typing.** Duck typing is in dynamic languages. I am getting really sick of people misusing that term. Someone on here was doing the same thing last week.
This would be a good time to use [`Iterator::fuse`](http://doc.rust-lang.org/std/iter/trait.Iterator.html#method.fuse). Guarantees you keep getting `None`.
I don't think learning C++ is easier.
as @nwydo pointed out, you can "spawn" N tasks by making 2 tasks that each spawn N/2 tasks. You're better off doing that, actually, because this way, if one of those tasks gets stolen, you've parallelize the work of spawning half your tasks, whereas otherwise you did it sequentially. (Still, it'd be easy enough to accept more closures.)
As a user, I regularly have problems with Javascript running in my browser leaking memory.
It's a common problem with single page apps
&gt; They are amazing for many use cases, similar to Facebook This is really common with this kind of tech. You probably don't have Facebook's problems. Engineering is all about requirements and tradeoffs, and unless you're Facebook, copying Facebook may be worse than not.
Thanks! Didn't know that existed, but it definitely made my code cleaner.
If you can describe the format, I can try to give a small nom parser for it. 
Of course you can always build your own chunking Iterator: pub struct Chunked&lt;I: Iterator&gt; { iter: I, size: usize, } impl&lt;I: Iterator&gt; Iterator for Chunked&lt;I&gt; { type Item = Vec&lt;I::Item&gt;; fn next(&amp;mut self) -&gt; Option&lt;Vec&lt;I::Item&gt;&gt; { let mut chunk = Vec::new(); for i in 0..self.size { match self.iter.next() { None if i == 0 =&gt; return None, None =&gt; return Some(chunk), Some(item) =&gt; chunk.push(item), } } Some(chunk) } } pub trait IteratorChunkedExt&lt;I: Iterator&gt; { fn chunked(self, size: usize) -&gt; Chunked&lt;I&gt;; } impl&lt;I: Iterator&gt; IteratorChunkedExt&lt;I&gt; for I { fn chunked(self, size: usize) -&gt; Chunked&lt;I&gt; { Chunked { iter: self, size: size, } } } This allows you to do `reader.lines().chunked(2)`. However you still have the problem that the individual lines are wrapped as a `std::io::Result&lt;String&gt;`. The best thing I figured out is to do `reader.lines().map(|x| x.unwrap()).chunked(2)`. Sadly `.map(|x| try!(x))` does not work. Does someone know a concise alternative? Question of my own: In my above implementation of a chunking Iterator I specified my trait condition `I: Iterator` at every occurrence, although strictly speaking it is only necessary in the `impl&lt;I: Iterator&gt; Iterator for Chunked&lt;I&gt;` line. I have heard that one should reduce trait conditions only to the places where they are absolutely necessary, but I feel it improved code clarity in this case. What are your opinions?
This is very interesting! I have access to a 20 core server (dual socket x 10 core) so I decided to run the benchmark there to see how well it will perform with more cores. This was done with 1024K samples. thread pool size | Speedup ---|--- 1 | 0.987 2 | 1.840 4 | 3.297 8 | 5.162 10 | 5.691 16 | 6.086 20 | 5.912 40 | 5.848 Trying with bigger sizes does not seem to change the performance much. (This was 40 threads) Samples | Speedup ---|--- 1024K | 5.703 2048K | 5.979 4096K | 6.251 8192K | 6.446 I'm not sure if I am hitting Amdahl's law or some other bottleneck. Quicksort is _not_ embarrassingly parallel, so my guess is Amdahl's law more then a problem with the library. A quick perf dump: 36.12% quicksort quicksort [.] partition::h6711475258950824040 29.73% quicksort [kernel.kallsyms] [k] _raw_spin_lock 11.32% quicksort [kernel.kallsyms] [k] _raw_spin_lock_irqsave 8.88% quicksort quicksort [.] thread_pool::steal_work::h9048cdb1e636e491Blb 1.62% quicksort quicksort [.] quick_sort::h12504552076790674614 1.03% quicksort quicksort [.] main::h3a9cd303e3858ac4iea 
Just because tasks are not stolen does not mean that you have no parallel work. Consider the quick sort example: the first few tasks correspond to big chunks of the array. *Those* tasks will get stolen. But then, once you've divided the array up roughly into Nths (where you have N cpus) you wouldn't expect much more stealing to occur -- but we'll still generate tasks, just in case the workload is not balanced.
The original comment was about the utility of `&amp;mut str`, and Rust strings don't carry a metadata field for language, so you'd have to implement your own custom string type to know what language a string is written in. In the absence of that, locale is the best you can do. (But actually, the best you can do is to not bother ever trying to mess with the case of strings. :P )
The `partition` code doesn't run in parallel, so you might be seeing that and Amdahl's kicking in. Just partitioning the data the first time is probably going to be expensive (you could time that run once). Also, quicksort isn't the most memory bandwidth friendly sorting algorithm. A few cores can't usually saturate your memory bandwidth, but twenty cores can. At that point you are limited by moving data to and from main memory, rather than CPU ops. As I understand it, radix sort is much more bandwidth friendly (and parallelizes too, though more recent research on that).
It looks like someone did one with nom for FASTQ last month https://www.reddit.com/r/rust/comments/3tkc0a/parsing_from_file_with_nom/ Since geaal is nom's author, it might make more sense to ask them for that one. :) 
:) And you've seen this too? http://arxiv.org/pdf/1509.02796.pdf
I'll check it too. :-)
Neat. From the title, thought this was going to have something to do with SIMD though. Which actually brings me to a question: This more or less makes SIMD operations impossible, right? Unless you introduce some sort of explicit support for batching or otherwise pure operations, LLVM would be unable to do automatic loop unrolling and vectorization?
Binary operations are fundamental (in mathematics, and elsewhere, such as this library), because the 2-ary one can be chained to create an *n*-ary one. E.g. the symbol `∑` for summation of many things is mainly for notational convenience: we could write `a + b + ... + z` everywhere instead. Furthermore, a binary split is often exactly what one wants/needs for ([cache oblivious](https://en.wikipedia.org/wiki/Cache-oblivious_algorithm)) algorithms, as it is the most common phrasing of divide-and-conquer. Of course, we're not just interested in the semantics here, and a `many_` variant might be able to eke out a little more performance.
Well we've developed a reputation for being an awesome community, we'd better uphold it. 
Given the nature of duck typing--if it looks like a duck, walks like a duck, and quacks like a duck, then it must be a duck--then we shouldn't be surprised that people familiar with duck typing will take something that looks like duck typing, walks like duck typing and quacks like duck typing, and conclude that it must be duck typing. :)
*Nice*. There've been a few cases where I've had a macro to take an expression, bind to a `const` inside a block, then return a static borrow of said const.
"map takes a closure with one argument, x."
http://devswag.com/products/rust-t-shirt
This is a summary of my recent efforts to code a type-safe tensor library in Rust. Thanks to the awesome people from the community that are always eager to help, the initial version is now functional :)
I mean, he built QuickSort on `join` too but it isn't "Rayon: QuickSort in Rust" :) It seemed from the post (and [project page](https://github.com/nikomatsakis/rayon)) that Rayon was about the `join` method and the smarts behind it ("Rayon's core primitive: join"). If I misunderstood and it is meant to be more about data-parallelism and the `ParallelIterator` trait, I withdraw my complaint and await that explanation!
Avoiding allocation is an incredible trick, but won't this blow the stack for very large workloads?
`char::to_uppercase` returns an iterator which can give you up to 3 characters.
I think its true that learning Rust is easier if you have some exposure to the ML-family of languages. In any case, it is tough at first but definitely worth it, in my experience. My first program was one to download a file from a url, and I never got it to compile :) I'm sure I could make it work now, just haven't bothered. BTW, unless you are some kind of super-learner, if you are really trying to learn all those languages at once, I would recommend you "serialize" them and study one at a time. Go isn't too bad, but Rust has enough detail to soak up a lot of brain-space. I would do C++ last, and only if you have Rust Rage after an extended study of it. One thing that I wish I knew up front is that you don't _always_ have to fight the BC. You can clone(), or put stuff into an Arc/Rc (possibly with a Mutex/RWLock as well in the Arc case). I.e essentially shift the borrow-checking to runtime. E.g. let hm:HashMap&lt;String,String&gt; = HashMap::new(); let my_sharable_hashmap = Arc::new(Mutex::new(hm)) 
The most common example I run into is: let mut v = vec![1, 2, 3, 4, 5]; v[v.len()/2] = 10; A sufficiently smart borrow checker would realize that the subexpression `v.len()/2` can be evaluated completely and release its borrow before beginning to evaluate `v[i]`, but the current implementation requires the trivial rewrite let mid = v.len()/2; v[mid] = 10;
Yeah, `join` is the fundamental primitive, but the goal of the library itself (maybe not the blog post specifically) is to provide the fancier APIs building on top of `join`, e.g. the blog post says: &gt; Rayon’s goal is to make it easy to add parallelism to your sequential code – so basically to take existing for loops or iterators and make them run in parallel. For example, if you have an existing iterator chain like this: It also points out that Rayon is a work-in-progress, so maybe the non-primitive things are not fleshed out enough to be worth the "data parallel" label just yet, I don't know.
Could you try an embarassingly parallel example? E.g. using the `par_iter` stuff demo'd at the top of the post.
Servo usually does not know how to partition the data and must use work stealing, because the trees are heterogeneous and irregular in shape. Yes, it is common.
Basically I'm taking breaks from Rust by doing the easy stuff. :) I already knew C++ from before, so it's easy refresher with the new stuff mixed in. I was gathering that if I wanted a self-referential structure in Rust, I *had* to push stuff to the heap using Box or Rc...? Anyway, when I got to my last stopping point, I think I had it about going, but my ownership was confused. I couldn't modify the contents of a node of the graph since it was owned by Rc (I think?) Letting my brain chew on it in the background. I'll keep at it. Even if I turn out to hate it and never use it again, I can't walk away without first getting something non-trivial to run! :)
map takes an implementor of the Fn trait, which may be a closure, function, or custom type.
"is being passed" rather
I have to put everything over stream, so I calculated the total size, and pass the buffer around to everything, and it just writes the result to it. Thank you so much for your help. I got help from you earlier on something else, and I wanted to know why 'elses' extend the lifetime. I commented some stuff from other modules, but that shouldn't affect it. It compiles as expected without the ```else```s https://play.rust-lang.org/?gist=0f6a00ad9b39d3ff349a&amp;version=nightly (lines 23 - 37) I don't believe you can access string or vector outside of their blocks, so it doesnt make sense why they still are borrowing ```from``` Looks like a bug? Should I submit it?
You forgot to mention the name of the library ;-)
Decent overview article. Comments are incredibly hostile, as is normal for Hackaday (in my experience).
The borrow must last at least as long as the lifetime of the trees in the arena, because of the way you use 'r in your create_tree declaration. [Something like this](http://is.gd/bqH2qQ) compiles. It uses a different lifetime for the parameter borrows vs. the internal borrow lifetimes. This does force the lifetime of the returned Tree reference to be shorter than (or the same as) the lifetime of the Arena borrow. I suspect that's the correct behaviour, though - anything else either is or runs a serious risk of memory unsafe behaviour.
I didn't forget: it's so experimental that it's not really public. In fact, I only got the the SIMD part to actually work in the final (so far) hour that I worked on it, and it required a pile of hacks... that only work in a limited number of cases (the benchmarked case is of course one of them), so I'm going to have to rearchitecture most of it. (I'll be sure to blog/publicise it once I'm more happy with its approach.)
This is really cool. Thanks for your efforts towards this!
Not too bad, actually (yes, I was bored, and decided to read the comments). Though we really should work on getting rid of this meme: &gt; Removing undefined behaviour leads to worse performance. (fortunately, @whitequark, who has expressed strong views on this before on Twitter, corrected them)
Yep, just encouraging you along the way. Figured you'd mention it if you'd published it.
I would presume that only works for const T's that are Copy, which explains why assignment would generate a copy.
Haha yeah.
This looks very cool! I'm a python programmer who likes the look of rust for more CPU /memory intensive stuff, and the idea of easy multi threaded programmes is one thing that appeals to me. And this looks very easy to use! 
Thanks. I'm a bit fuzzy about when lifetimes are &lt;= and when they're &gt;= some other lifetime. So if I take your modified version and add an additional call to create_tree: let b = create_tree(&amp;arena, 1, Some(a), None); then I end up back with the orignal compilation error. http://is.gd/eyo2Wr What I'm trying to do is pass ownership of the arena elsewhere, but only after all references to the contents of the arena have ended. I guess because things in the arena have references to other things in the arena, that becomes a problem.
You're right that rust is absolutely not duck-typed, but duck typing is also absolutely not tied to dynamic languages. Scala has static duck-typing. You very much can specify “this function takes a value which has a function named “durr” defined that has the signature ...”
Scala has this: you can specify “thing with a bunch of methods with specific names and signatures”
More pedantry... The first case is likely in the constant data segment and depending on the compiler may or may not be copied to the stack.
Do the idle threads try to steal task first from the queues of other threads within the same numa node?
I wouldn't doubt the Swift team's capacity to do amazing things with compiler optimization, but devirtualization isn't a solved problem and a sufficiently-smart compiler will only take you so far. The difference that Rusky is alluding to above is that of language defaults, which will govern the runtime characteristics of the language's ecosystem at large. The fact that *all* Rust libraries have to deal with ownership up-front (and that the language heavily nudges authors towards explicitly static semantics) is an enormous feather in Rust's cap when it comes to ensuring that the ecosystem tends towards code that is optimized for, well, optimizability. :) Even if Swift adds some sort of borrow checker someday as Chris Lattner has claimed (and as C++ has claimed, and as D has claimed, and as Nim has claimed...), it remains to be seen whether its library ecosystem will embrace Rust-style features if more ergonomic alternatives have long become entrenched (and I'm personally skeptical that you can cleanly tack on ownership semantics to a language that wasn't originally designed with them in mind, but I'd be fascinated to be proven wrong).
You test for `HOOH` twice. It's a *set*; it doesn't store duplicates.
This could be packaged into a crate that is used as a build dependency to do the right thing, e.g. `deprecated_attr::check_enabled("name of feature");`.
The programmer can opt-in to their own allocations if they know they're going to be doing very deeply nested work, e.g. boxing locals to ensure stack frames are small, or using a library like [`stacker`](https://crates.io/crates/stacker) to ensure the stack is large. That said, the most efficient break-down of a job will be into an (approximately) balanced tree, i.e. guaranteeing a maximum nesting depth of O(log n).
No, I haven't, but that would be interesting, so I will ;) I'm pretty sure that creating a GenericArray has an overhead, since it copies all the elements one by one. Maybe there would be a way to do that better with `mem::transmute` or something... I'll have to try and see.
But they're correct! Undefined behavior lets the compiler optimize away all that unnecessary code. #undefrules #nocodeisfastcode
Fortunately Rust can be just as fast: misuse `unsafe` and you'll hit all the UB you could ever want!
For example image processing.
Wow that was fast ... thank you for your help. 
Well, for my use case is not necesary but yes it would be nice. I would save some space but it's not that important to me. Who knows maybe in the future they will implement bitfields . 
I know the feeling of "never really having learnt" a language as I "learn" most languages by trying to do things and that tends to work well enough. One advantage of Rust and other compiled languages is that you can try to do things in a way you think they might work and then see what the compiler has to say about it. So, embrace compiler errors! They contain a lot of information and google for the keywords or using `rustc --explain` with the given error codes teaches you a lot of Rust. (Once ~~Stockholm syndrome sets in~~ you start to like these warnings, errors, and notes that `rustc` gives you, you might want to read [this](https://pascalhertleif.de/artikel/good-practices-for-writing-rust-libraries/) to learn how to get even more of them. Full disclaimer: I wrote that 😄) Also, if you haven't read it already, the Rust book is pretty great. As you say, you already know a lot of stuff, so you can safely skip a lot of chapters. I think Lua was the only other language that was (for me) as easy to learn from an official documentation/book as Rust.
Don't use hex, use a binary literal. 0b100 &amp; x != 0 way clearer IMO.
Anyone is welcome to do so. I'm just really happy to finally have #[deprecate]. Nice api evolution for everyone
Hex digits are vestly preferred to binary when dealing with bits because they're just as readable for smaller values once you get comfortable with them and they are much more readable for large values. For large binary strings, like 16+ bits it becomes hard very quickly to know what bit you're at. This problem doesn't arise with the hex version. This is why most embedded code in the wild uses hex digits and not the binary string (though it's also partially because ANSI C didn't contain the 0b prefix for bit strings until C99. Tl;Dr - learn to read hex, it's faster, scales better, and is more common for embedded stuff.
I forgot that Rust can use the underscore for arbitrary separation of numerical digits. I like the grouping-by-8 that you've done there, definitely nice to read. Though I'm comfortable with hex and I actually find the 0x1C more readable than the 00011100, so this comes down a lot to personal opinion. I don't know what you mean by binary fiddling in cache, I assume you mean understand hex values, and yes, as I've stated that's a requirement. But like I said, I think that learning that is a good thing. There is a pretty large amount of software that relies on hex for dense bit packing while still being able to effectively display information. Hex editors, visual debuggers, and memory addresses are generally all reliant on hex. It's basically a required skill for embedded programming, which is why it's all around, and I don't specifically see binary strings as being any sort of real improvement as a general over-all replacement for them.
Shifting is endian-independent. If `x: u32 = 0xDEADBEEF` and `&amp;x == 0x100`, then endianness affects whether `*(0x101 as *const u8)` is equal to `0xAD` or `0xBE`, but it has no effect on the value of `(x &gt;&gt; bit_pos) &amp; 1`, which is how you can access a single bit.
Theoretically, would this be better for this simple case? fn get_bit_at(input: u32, n: u8) -&gt; Result&lt;bool, ()&gt; { if n &lt; 32 { Ok(input &amp; (1 &lt;&lt; n) != 0) } else { Err(()) } } Or would this be overkill?
The comments are basically a rundown of the problems with C as demonstrated through a defense of C.
They are supposed to be. It could be that the allocator is re-using the allocations from `mat_det` in `mat_mul`, and the contents of the matrix are in the CPU's cache, making the memory faster to access.
Oh, I had them backwards. That makes it even weirder. Have you tried changing the order of the tests to see what that affects?
Yeah for bulk processing of binary data (reading a dump), hex makes sense. You're usually looking for patterns there, and you're in "thinking about binary" mode. But for some one-off part of some larger application, I don't think it's as reasonable to expect people to grok hex as a way to represent bit patterns. Hex is also useful for things like colours, but the actual bits don't really matter there.
I value consistency very highly in my code, so even for one-off things, I'd prefer to just use hex. Because your argument actually also works the other way, if it's just a small part of a large code base, that one part being slightly more difficult for novice coders isn't much of a problem. Leaving it in hex also introduces coders in that scenario to hex characters in a friendly manner, just a small part of a big code base, it doesn't inundate them with it where nothing makes sense cause it's all in hex. And comments can take care of any of the unfamiliarity. So in essence I'm pro hex because it can be used everywhere readably (in C, hex editors, etc.) and is already used in many places so familiarity with it is definitely important for general intermediate and advanced programmers. 
I think that `Option&lt;bool&gt;` would be better option here.
Regarding cache reuse: Rust benchmarks *should* thrash the cache between each bench.
How do I reorder them? I thought it was always alphabetical?
If you run them separately, are they back at their regular performance? You can filter by name by a string argument to cargo bench. If not, note that all the benchmarks are compiled in the same crate. How many times a function is used may influence inlining decisions. In that sense, there is no right way to do this, the perfect isolation microbenchmark doesn't reflect a real use case very well either.
When run separately the determinant function is the same as when together. The matrix mul is less (as seen above). I couldn't get the name arguments to work. I also thought it might be some overflow error (the matrix I was testing has determinant zero). However I tested using a diagonal matrix and get the same effect on the matrix multiplication.. Edit: Managed to get the name arguments to work. The determinant bench runs the same. The matrix bench runs as if I had ran them all together: me@myPc:~/Projects/rusty-machine/rusty-machine$ cargo bench mat_det Compiling rusty-machine v0.0.7 (file:///home/me/Projects/rusty-machine/rusty-machine) Running target/release/lib-f2e9f38cf4c59f2d running 1 test test linalg::matrix::mat_det ... bench: 2,802 ns/iter (+/- 288) test result: ok. 0 passed; 0 failed; 0 ignored; 1 measured Running target/release/rusty_machine-28dbc84d0e854e58 running 0 tests test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured me@myPc:~/Projects/rusty-machine/rusty-machine$ cargo bench mat_mul Running target/release/lib-f2e9f38cf4c59f2d running 1 test test linalg::matrix::mat_mul ... bench: 1,258 ns/iter (+/- 65) test result: ok. 0 passed; 0 failed; 0 ignored; 1 measured Running target/release/rusty_machine-28dbc84d0e854e58 running 0 tests test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured
It was already false in the 80's when C was only properly available on UNIX. Modula-2, Pascal dialects and C dialects had more or less the same code quality on consumer hardware, hence why they were only used for business software and we used only Assembly when we cared for performance. Of course, current generations think C compilers always generated code like they do nowadays.
&gt; Or is it because we have no meaningful value to put into Err here? This is it. the non-panicking get methods on slices and arrays all return Options also.
Well, I would say that they are copied from the data segment, but your example is a good counterexample.
Because there is no meaningful value in `Err`. Also I think that it is error that user provided value greater than 31 - this is fully acceptable value that returns no meaningful value (or 0). 
Wouldn't providing a `n &gt; 32` be a contract violation, and therefore justify panicking instead of returning a Result? EDIT: `n &gt;= 32`
Yeah, but we should strive to name it better. Perhaps `omit! { .. }`?
I've just noticed that `par_iter()` should already be sufficient – how it's actually implemented doesn't matter too much.
See, that's exactly why I asked this question, so I could learn what is the most idiomatic. Thanks (to everyone) for the replies.
Thanks Veedrac and saposcat. I was reading rust by example and this was not clear. I re-read the rust book and it says that 'they’re effectively inlined to each place that they’re used'. Either way I was surprised by this given what I generally thing of a constant. I think I will be avoiding them in the future. [rustbyexample](http://rustbyexample.com/custom_types/constants.html) [book!](https://doc.rust-lang.org/book/const-and-static.html)
This would be great for `plugin` case, as it could provide graceful fallback to code generators (i.e. in Serde).
I would change get too, personally.
I may look into it later today :)
I can see now why they are unstable! Thanks for taking a look.
I don't think that has so much to do with unstable or not. Writing a program that measures what you want is never trivial, and optimizing compilers like llvm can be pretty mercurial.
C can barely be said to support bit fields because there is no standard to specify how they are packed. That is, you can't do this and sleep well at night: typedef struct{ int if_flag : 1; int set_flag : 1; int reserved : 30; } iff_reg_t; /* Structure is not guaranteed to be as declared */ iff_reg_t new_reg_val; ... memcpy(iff_reg_addr, &amp;new_reg_val, sizeof(new_reg_val)); /* Which bits are set depends on the compiler implementation */ Which is weird to me because *why else would you want to have bitfields!?* Fortunately [rust-bitfield](https://github.com/dzamlo/rust-bitfield) seems to get it right. 
Copy and set of slices are quite common array operations, D language offers them with a short clean syntax: void main() { auto a = new int[10]; auto b = new int[10]; a[] = 5; // Set all items of a to 5. b[] = a[]; // Copy slice a in b. b[$-2..$] = a[$-2..$]; // Partial copy, $ is contextual length. auto c = b.dup; // Whole array duplication. auto d = a ~ b; // concatenation. } Ergonomics for very common operations is important.
Creating a Tree isn't difficult in and of itself - an enum with Node and Leaf variants works fine, using Box for children. The issue here is having a tree in an arena, which makes things more difficult as parents don't own their children.
Thank you so much! Here is my [source code](https://github.com/AtheMathmo/rusty-machine). I'm happy to take a look at anything I may be able to help with so please let me know if I can return the favor at all. I've also sent you a pm with a little more detail of the project structure. 
At first glance this looks pretty well structured/documented. Here are a few concrete nits: * Do you actually need the unsafe block in the Index impl for vectors? Is your code actually any faster than just using the underlying Vec directly? * You have a few places where you call Vec::new(), where instead you could pre-calculate the size and use Vec::with_capacity() - that gets rid of several reallocations that might otherwise be necessary * You don't implement the Clone and Debug traits. I think it's polite to do so whenever possible. * In quite a few pub functions you panic on invalid user input without documenting what the expected user input is, or documenting that you'll panic. I think not using a Result type is probably appropriate, but you might want to use the panic! macro to explain to your library user which constraint they actually violated (after you documented these constraints, of course) * I would keep the tests closer with the stuff they're testing on the file system (you keep linalg and learning pretty separate) * you might want to ensure in your testsuite that invalid accesses properly panic ok, it's 8am and I need sleep, hope I wasn't too far off the mark here and you can get something useful out of it! ninja edit: I can't reddit.
I'm glad you brought this up, because I encountered this problem a few weeks ago: https://gist.github.com/yberreby/b973f71859c1d6a8ffff To this day, I'm still not sure _why_ one benchmark slows the other down. But I asked for help on IRC and we were able to reduce the example that suppressed the slowdown to a call to an `extern "C"`function, done inside `panic` in my case.
This does make me wonder if having (alternative) library functions where heap allocation errors into the normal `Option`/`Result` error checking is worthwhile? 
Every function that can cause an allocation would then return a result. Would make for some ugly APIs. A STD alternative that uses only the stack or provides that type of API for embedded would perhaps work. Some rust libraries are zero allocation so for them it's a moot point.
memcpy should be reachable in safe rust though. Right now it's only that for byte slices (io::Write for [u8]), and writing a slice copy loop that properly lowers to memcpy is not easy in safe rust. Using .zip() on two iterators is a no-go.
Why are you using a `Vec` for `d`? Can't one use an array of length ~~10~~ 20? 
I agree that it's a good idea to have that functionality available; I just don't think it needs additional syntactic sugar.
People here are (for now) generally happy about providing code reviews. Depending on the size of the code you might post it wholesale or post a link to a public repository. Otherwise, there is a codereview.stackexchange site, though I am not sure whether any experienced rustacean hangs out there. Oh, and do not forget that you might want to provide guidelines for code reviews, that is specific areas that you would like feedback on such as proper formatting, code organization/API, ...
Slightly off-topic, but the previous articles in the series have been well received here, so I thought I'd post it. In this article, Rust gets a couple of honourable mentions, so it's perhaps the most relevant yet.
Hello, /r/rust! The only parser I wrote before was a [JSON parser](https://github.com/yberreby/rust-json-parser) that was pretty hackish, but this time I tried to do it right, with separate lexing, parsing and interpretation phases. I haven't had time to implement proper error handling, though, so it panics on error. I figured I'd share to get the community's input and know whether my code is any good :) Thanks!
&gt; though I am not sure whether any experienced rustacean hangs out there. I do!
The secret to writing compact Rust: `unwrap`, `unwrap` everywhere. If you can `unwrap` it, do it.
Well... $ grep -RE '(unwrap)|(panic)' src | wc -l 21 :)
Welcome to the dark side.
If/when Rust gets type-level numbers, you could have a method like this: impl&lt;T, N: const usize&gt; [T; N] { pub fn concat&lt;M: const usize&gt;(self, other: [T; M]) -&gt; [T; N + M] { ... } } And then you'd just have to do this: let d = a.concat(b); Without the aforementioned type-level numbers though, I don't know of a simple way to do it without using a `Vec`.
&gt; Our error model [...] ended up being a hybrid of two things: &gt; &gt; * Fail-fast for programming bugs. &gt; * Typed exceptions for dynamically recoverable errors. ... where we use exceptions for programming bugs and effectively return values for "dynamically recoverable errors". Seems like we're losing out on all the tradeoffs he describes there, except for being able to isolate failure on a thread level. Also: &gt; A nice accident of our model was that we could have compiled it with either return codes or exceptions. Thanks to this, we actually did the experiment, to see what the impact was to our system's size and speed. The exceptions-based system ended up being roughly 7% smaller and 4% faster on some key benchmarks.
Why not just have `*s = blah` at that point? Just like every other pointer.
Where do you NOT hang out? :D
I just yesterday migrated a project from 1.0-alpha to 1.5-stable, and then 1.7-nightly. I was surprised to find that inclusive ranges weren't enabled in any version. Note that the deprecation message for the `range_inclusive` fn says ["replaced with ... syntax"](https://doc.rust-lang.org/std/iter/fn.range_inclusive.html), despite the fact that `...` causes a lexer error. Because I don't like pulling in non-std dependencies, and the num crate's inclusive range iterators weren't discoverable anyway, my project is currently using `(x .. -1isize).step_by(-1)` in a few places. I'd describe this as "less than ideal".
&gt; The difference, it seems, is that Midori used unwinding more than Rust does. Does using unwinding more actually translate to it costing more? I assumed that once unwinding exists in the language, all code pays the codegen costs in terms of landing pads for RAII and whatnot, and then the difference is that Midori gets some use out of that cost for all of their error handling, whereas as far as I understand we pay the codegen costs for exceptions just to prevent process aborts and then pay an additional codegen cost for our actual return-value-based error handling.
In the context of that paragraph I took that to mean that they weren't allowed to open-source it, not that it was cancelled.
To the contrary inclusive ranges (`...`) work in patters.
Doesn't look like it, as of Rust 1.5.0: src/main.rs:2:15: 2:18 error: expected `{`, found `...` src/main.rs:2 for i in 1...10 { ^~~ src/main.rs:2:15: 2:18 help: place this code inside a block Could not compile `inclusive_range`. EDIT: here's a playpen: http://is.gd/7ooh64
It all depends on the frequency of the exceptional case. If the exceptional case is rare, then Midori's implementation (which is more optimized than the usual implementation of DWARF exceptions) will have higher performance. If the exceptional case is common, then explicit error values will have higher performance. The important thing to remember here is that the programming model can be chosen independently of the implementation technique. [Swift's error handling](https://developer.apple.com/library/ios/documentation/Swift/Conceptual/Swift_Programming_Language/ErrorHandling.html) looks like exception handling syntax, but is implemented with return values. I convinced myself at one point that you could do a compiler transformation to convert return value error handling to use exceptions.
only in patterns, iirc
&gt; rusty-est some people joking say "rustic", but "idiomatic" is probably good enough :)
Thank you, the last solution is perfect! How do you get into traits/ generics? I find it very hard to find the one you want.
Thanks! Your explanation of panic vs. Result is very helpful. I didn't realise you could get the behaviour I wanted by removing the pub keyword. Thank you!
&gt; they weren't allowed to open-source it That's really mean. Not allowed to open source it, and (after some time) not receive further funding to continue development.
It seems to me the lexer will be cleaner and more memory efficient if you were to return an `Iterator&lt;Item=Token&gt;` rather than a `Vec&lt;Token&gt;`. If I'm reading correctly, you don't use random access. You should also be able to avoid any lookahead, though you can just use `peekable` if needed for a single lookahead.
It is quite good for extensible CLI tools (mostly due `init` and simple channels). I think that together with Rust it could be awesome tandem. 
You can post it to the /r/rust subreddit. Also: https://codereview.stackexchange.com/ (it's for any language, and hopefully Rust people sometimes see the posts there)
I think you can restructure [this](https://github.com/AtheMathmo/rusty-machine/blob/master/rusty-machine/src/learning/k_means.rs#L140) into an iteratator that yields values `(i, idx_el, distances_el)`, in such a way you can do simply: for (i, idx_el, distances_el) in &lt;some iterator&gt; { // This works like repmat pulling out row i repeatedly. let centroid_diff = c - data.select_rows(&amp;vec![i; c.rows]); let dist = &amp;centroid_diff.elemul(&amp;centroid_diff).sum_cols(); // Now take argmin and this is the centroid. let (min_idx, min_dist) = dist.argmin(); idx_el = min_idx; distances_el = min_dist; } This would save you for doing bounds checking on `idx.data[i]` and `distances.data[i]` (and specially, void the possibility of panicking with out-of-bounds access). I like to do this kind of contortionism not for performance, but for correctness (there is no panicking case in the for loop itself). I'm unsure what goes on `&lt;some iterator&gt;` though. edit: I'm guessing you would need to take mutable iterators on `idx.data` and `distances.data`, then [`zip`](https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.zip) them, then [`enumerate`](https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.enumerate) to get the index `i`. Something like let iterator = idx.data.iter_mut().zip(&amp;mut distances.data).enumerate(); for (i, (idx_el, distances_el)) in iterator { ... }
You should be able to do `.iter().cloned()` **Edit:** since `u8` is `Copy`, you could also do `.iter().map(|&amp;x| x)`
Only a small subset of the tree is executing on a stack at point in time. That is, jobs finish and are popped off the stack in a reasonably timely manner: separate branches of the tree don't interact, and so only one will be on a thread's stack at a time. This rule means the maximum stack use is O(depth of job tree) not O(width) or O(number of jobs) (of course the former can be O(number of jobs) if the "tree" degenerates to a linked list, hence being mostly balanced is important).
&gt;I know Go abandoned linked stacks because of these switches. At first they were pretty bad for us, however after about a man year or two of effort, the switching time faded away into the sub-0.5% noise. That seems like a good data point for Rust to revisit linked stacks. I really think switching away from linked stacks was a mistake, it just didn't seem like there was enough data to support that it wasn't going to be possible to optimize to throw away the baby with the bath water there. Efficient asynchronous tasks (for IO, but also for task based concurrency and just general efficient "coroutines" type stuff) is a huge prize here and I'm sad that Rust can't do it right now. Plus it's pay-as-you-go - you'd only run on linked stacks in "async" functions (and switch back to classical stacks as soon as possible), so whatever the overhead it would only apply to code that opts in to it. 
Sadly you can't do it in *stable* rust, to convert a vector into a slice, you'll need `#![feature(convert)]` and to match on slices, you'll need `#![feature(slice_patterns)]`.
Aww, too bad. I do prefer working in stable. But at least it's gotta land there some time. :) Thanks for the pointer!
Rust has generally made implementation decisions biased by what LLVM currently supports well, which is unsurprisingly a feature set similar to C++. Ideas like integer overflow checking, green threads, segmented stacks, etc. that require a more holistic approach combining language design and code generation changes have not worked out very well.
I guess at least C++ 17 style coroutines will eventually be plausible ( https://github.com/boostcon/cppnow_presentations_2015/raw/master/files/CppNow2015_Coroutines_in_C++17.pdf ), since Clang will have to do it. It seems like efficient linked stacks would be preferable from a performance standpoint, but I guess you wouldn't really use any of this stuff where performance is critical anyway - it's mostly for improving the "glue" code (in terms of productivity, maintainability, not performance). 
If there's no way to duplicate a value (almost always the `clone` method of the `Clone` trait or the implicit copy of the `Copy` trait), there's no way to get two instances of that value. It may be an oversight on the part of the library author, or it may be fundamental. That is, the type may be literally uncopyable, such as `&amp;mut T`: copying that would result in aliasing mutable references, which is not ok (disallowing that is one of the most important pieces of Rust's safety story). If it is uncopyable, you'll have to take an alternative approach, such as using references, or refactoring the code to not try to duplicate the value (having more context for what you're trying to do would help us help you with these).
I work on a team that has some ex-Midori people on it. The reason that they weren't allowed to open-source it was because it was heavily built on top of other Microsoft technologies that were owned by other teams and weren't going to open source. 
Thanks. But it's acceptable to have some open source project heavily depend on some closed source project, specially because this closed source component can be rewritten; or the project can be adapted to run somewhere else.
`RangeInclusive` would often be used *outside* of patterns, and that is the major gripe.
Indeed :D
In this case the closed source component was "the entire C# compiler." This was before project Roslyn, back when the C# compiler was closed source.
&gt;types could literally be all removed and replaced with the right-hand sides of their definitions https://github.com/rust-lang/rust/issues/30503
In order: * Do you mean something like the `abstract type` aliases proposed [here][abstract]? * I'm not sure I'm understanding correctly, but: does this concern still apply considering that Rust has strict, local rules such that, IIRC, for every (trait, type) pair, there is exactly one module (or crate?) which has the right to define an `impl` between them? (I can't remember off the top of my head what things are crate-level and what things are module-level - maybe some things are the former which should've been the latter? cc /u/nikomatsakis) [abstract]: https://github.com/rust-lang/rfcs/pull/1305
Rust's orphan rules are applied at the crate level, whereas visibility rules are applied at the module level.
They're enabled by default because they're enabled by default in LLVM, and they're enabled by default in LLVM because they're part of the ABI. I'm not an expert on x86 these days, but doesn't the x86-64 interrupt handling model allow you to choose an alternate stack for handling the interrupt from the Interrupt Stack Table in your IDT entry? There are legitimate reasons for using the FP registers in kernel code, e.g. to use SIMD instructions or other instructions like the recent AES-NI instructions that happen to use SIMD registers. Clang supports `-msoft-float` for disabling all use of FP registers in generated code; Rust could add something similar.
You could have a function that creates and returns your tuple, it might be annoying to type the return type though, so it's a trade-off.
Eh, not really? Having time to reconsider/redesign is *exactly* what the RFC &amp; stability process is for. An RFC like this landing basically means "we think this is a probably a reasonable fit for Rust now, so we feel comfortable experimenting", but using it in practice might find that it's not quite right, or other changes/more thinking might send us back to the drawing board. Sure, the cycle (if we've actually killed the `...` RFC, which isn't obviously the case) happened quickly in this case, but the teams are only human.
Ok, but in the meantime we're in another 6+ months of limbo on the issue while bikeshedding over the particular syntax occurs. Meanwhile `..` is usable everywhere outside of patterns and `...` is usable only inside patterns. This is what we're stuck with while a more "complete" feature is roundly debated. TBH I don't understand the need for extended syntax anyway when you can just modify the expression with something like `(i+1)..n` which is no longer or less clear. `..` will never be removed for backwards compatibility, and likewise `...` will always be valid within patterns for the same reason, so we're stuck with both. Any extended syntax proposal will likely seek to be compatible with both of these, and so both will remain. So why can't they simply be made to function in each others' contexts in the meantime as an incremental change towards an extended range syntax. Why does this have to be a cathedral?
&gt; I'm not an expert on x86 these days, but doesn't the x86-64 interrupt handling model allow you to choose an alternate stack for handling the interrupt from the Interrupt Stack Table in your IDT entry? Yes. You switch stacks anytime an interrupt to a higher privilege level occurs, in both legacy (x86) and long (x86_64) modes.
The post says that they would only hoist a check to a block that was postdominated by the check, i.e. a block such that all control-flow paths to the function exit go through the original location of the check. This is a far cry from allowing arbitrary hoisting of OOB / overflow checks, but the post doesn't say if that definition includes control-flow from exceptions or not.
So something like `extern "MS-DOS" fn`? Such an addition would probably require an RFC.
I will be working on my [libclang bindings and idiomatic wrapper](https://github.com/KyleMayes/clang-rs) this week. Out of ~240 functions in libclang, I only have 19 that have not had their functionality wrapped in idiomatic Rust. I hope to release it this week!
GPGPU stuff for my game project, with technologies of the OpenGL 3 era (because of compatibility). I didn't have this project in mind when I wrote glium, but now I'm glad I have this library to allow me to do that. My terrain has approximately 380k tiles and I'd like to run about a hundred linear algebra operations per tile and per frame, so GPGPU is my only option. The problem with that method is that it's somewhat a nightmare to optimize, because any transfer between the CPU and GPU can block (waiting for the other party to be ready). 
&gt; whereas as far as I understand we pay the codegen costs for exceptions just to prevent process aborts …which is what catching an exception or handling an error is, in any environment. &gt; and then pay an additional codegen cost for our actual return-value-based error handling …with the benefit that it is *much* faster in the error case. Rust's error handling story was the subject of a lot of debate in the early days with a vocal group in favor of making panics into unconditional process aborts. What's weird about that controversy is that it feels like it became received wisdom in some segments of the Rust community that *everyone* in the programming languages/systems community believes that "panic == process abort" is the right way to go, and that unwinding is a bad feature and should not exist. In fact, this is a very C/Unix-oriented view, and it is not the norm. Systems like Midori actually take the exact opposite approach and use unwinding much more extensively than Rust does.
&gt; You had benchmarks of an early implementation that didn't do all the optimizations that could be done Again, we did most of Midori's optimizations. I don't think the remaining 25% of them would have resulted in the massive speedups we would have needed. &gt; For one thing, you didn't make the static distinction between "things that may need to yield" vs "things that run to completion" and switch stacks so that you only paid the overhead in the small number of functions that might actually need to suspend Do you mean annotating functions that can yield with a separate type (async/await style)? That's a large language feature (and one I would love to see an implementation of in some form in the future!) but it's also irrelevant to what Rust did. Whether a function could yield was not part of its type, and that makes a huge difference to the system in practice. If you don't mean that, then you're talking about a static call tree analysis, which I'm pretty skeptical of because of higher-order/virtual functions. If calling a virtual method taints your function with "might yield", it's going to become a very imprecise analysis. You could try to throw k-CFA and so forth at it, but I'm not optimistic that it would work. &gt; but it seemed to me that a lot of weight was paid to the fact that Go decided to abandon it. Well, yeah, in that the fact that two languages independently tried and failed at linked stacks just makes the case against them that much stronger :) &gt; Plus, as I mentioned the key notion that you guys missed at the time is that you only need to use the linked stacks when necessary (in functions that may suspend), making it pay-as-you-go How do you determine that? The issues I mentioned previously apply. &gt; meaning that bad benchmarks don't matter since they only apply to actual concurrency type code, where the alternative is almost surely way worse (manual polling/events etc. typically performs worse IME). I'm not totally sure what this means, but if you're saying that state machines driven by epoll are slower than green threads, nginx seems like a pretty clear counterexample.
&gt; Do you mean annotating functions that can yield with a separate type (async/await style)? Yes. Or inferring it (a la C++ 17). The point is that most code does not need to (potentially) suspend and thus doesn't need a linked stack. It seems clearer and more rust-like to explicitly say if a function is going to block (and thus would need linked stacks). Analogous to C#'s "async" annotation. &gt; I'm not totally sure what this means, but if you're saying that state machines driven by epoll are slower than green threads, nginx seems like a pretty clear counterexample. I'm saying that the whole system is slower if you need to deal with callbacks or switching on some internal state flags (a la C#'s implementation of async) than if all you do is switch to a thread and execute it when it's ready. 
Yeap, current state of things seems a little bit inconsistent.
A compiler for my scripting language. I'm writing everything from scratch, the lexer and parser are now almost done. I'm going to experiment with thorough optimizations for the scripting language to see if I can outperform some (slow) languages. I know, I'm optimistic.
Note: I think that that second link is about FreeBSD syscalls, it's actually comparing and contrasting to the Linux syscalls in the linked text.
I don't know about you but 4% failure over a year seems way too big to me. We have some pretty hefty C++ code which was originally targeted at GCC 2.x which still builds on 5.x with no breakages. Thats zero problems in 20~ years.
I'm slowly working on [Zone of Control](https://github.com/ozkriff/zoc) strategy game, as always :) . [Refactoring logic](https://github.com/ozkriff/zoc/commit/cefe5e), implementing battle math, etc.. Wrote a [blog post](http://ozkriff.github.io/2015-11-30--devlog-live-again.html) (in russian) few weeks ago.
&gt;why shouldn't `S` be considered part of the signature? The point of that example was that `S` *is* part of the signature. Thus, defining privacy based on naive traversal of the names in the signature does not work. Thus, I devised the "surface" (necessarily distinct from signature) as an abstract concept to ease discussing what privacy means. ---- (This is not meant as an argument against the remainder of your comment. I just wanted to point out that I did not understand your critique of the where clause example.)
I'm not actually sure where the best place to do the optimizations would be. It looks like the current SIL optimizations are combined with the [array bounds check optimizations](https://github.com/apple/swift/blob/master/lib/SILOptimizer/LoopTransforms/ArrayBoundsCheckOpts.cpp). There have been a number of code quality improvements in LLVM for uses of the overflow intrinsics already, and there will probably be more as time goes on. I'm curious about how much Midori's slightly imprecise exception semantics improves optimization potential.
Yeah this would have been super nice in a few projects I've been hacking away at...Hopefully it gets solved soon.
Screenshot for illustration: http://imgur.com/aIzdPAA This is France. The two gray squares near Paris (at the top) are scientific academies and generate that blue thing which represents research. The research is automatically propagated through the country and finds its path through the mountains to the capital, which is located in Italy because for the moment the location of the capital is chosen randomly. You can also see how small the tiles are by looking at the seafront. The entire world is covered in tiles of this size. This system is entirely done by the GPU with three draw calls, one buffer upload, and one texture download. It takes around 0.6ms of GPU and 0.5ms of CPU per frame, but I think there's a synchronization in there (ie. the CPU blocked waiting on the GPU). I need to switch to persistent mapped buffers to get something clean and predictable. 
I'm probably going to publish 1.0 for [jsonwebtoken](https://github.com/Keats/rust-jwt) with full RFC header support. Also going to try to fit some work on [dbmigrate](https://github.com/Keats/dbmigrate), a CLI for SQL migrations, postgres only for now. If anyone has time i'd love to have some code review on dbmigrate and help me figure out how to work for other databases. My idea was a trait (https://github.com/Keats/dbmigrate/blob/master/src/drivers/mod.rs#L7-L13) that other drivers can implement but I don't understand how to make a constructor function that returns a struct implementing that trait, ie a get_driver fn that creates the right one depending on the URL given and returns it.
&gt; Have you published the source anywhere? Not yet, but I'll explain shortly here. The lexer is simply based on the `regex` crate and priority rules. The parser is also implemented in a simple way: for each rule in the grammar, I add a (recursive) function that tries to parse the rule given a vector of tokens. So for example, if I have the rule `Stm -&gt; Stm ; Stm`, I simply write a function fn parse_stm_seq(tokens: &amp;Vec&lt;Token&gt;) -&gt; Option&lt;(Stm, Vec&lt;Token&gt;) { // find position of ';' // if found: parse everything in front of ';' and everything after ';' // if both are succesful (return `Some(_)`), return `Some(Stm::Seq(left, right), c)`, where `c` is the vector of tokens after the right expression } The parser itself simply turns a vector of tokens into a tree of statements and expressions. When it is done, I will write a step for semantical analysis, which will optimize some things. Then, depending on the performance (speed) of these steps, I will either interpret the resulting parse tree, or output byte code and write an interpreter for that bytecode (similar to how Java works, but not quite the same). The most important thing for the lexer and parser is getting the priority right. For example, in the lexer, I want to check for a compare token before checking for an assignment token, because `==` includes `=`. Luckily, the syntax of my language is simple enough to reduce the number of possible conflicts. This makes the lexer a bit easier to write (without having to store previous states and all that). I hope this was clear. I'm always happy to discuss if you want to.
Afaict, the borrow lasts as long as its scope, which would be the expected behaviour. What you might want to do instead is reduce the scope of `arena`, by moving the assignments into the blocks that allocate the trees: https://play.rust-lang.org/?gist=00d4ca37da3ba6178860&amp;version=stable
 - Finishing up my open source mentoring blog post - Reviews, reviews, reviews. All over the place. In Rust, Servo, and Clippy. aaaaaaaaaaaaaaaaaaaaaaaa.
If this is idiomatic, then rust is unsuitable for enterprise software. 
I appreciate you taking the time to provide an alternate view, but I must strongly disagree: 0MQ gets it exactly wrong: - ZeroMQ spawns a thread for each socket connection. The nice robust error handling you are talking about 100% relies on having each socket processed by its own thread - so a panic can be done safely/correctly. - Enterprise software must not use the one thread per socket model. This model was a major architectural blunder for ZeroMQ. For a good description of the reasons why look at the ZeroMQ *author's* page: http://nanomsg.org/documentation-zeromq.html Positive / constructive: I *think* the catch_panic solution is actually going to work 100% for us. What I really need is the ability to do something like: try { ... use some rust library that might panic } catch (...) { ... log/notify/ fall through into the epoll event loop } It *looks* like panic::recover() can provide this: pub extern fn called_from_c(ptr: *const c_char, num: i32) -&gt; i32 { let result = panic::recover(|| { let s = unsafe { CStr::from_ptr(ptr) }; println!("{}: {}", s, num); }); match result { Ok(..) =&gt; 0, Err(..) =&gt; 1, } If it does I'll be super happy. 
Why is this website so broken on my phone. I'm seeing template code everywhere. Why won't anybody, think of the ~~children~~ reader mode!
I tried them out already. Got down from 22 to 14 seconds. *In 100 lines project.* I don't think it's going to be OK even with all those tricks mentioned there - especially that most of them are about reducing genericity and moving stuff to runtime.
It looks fine in Firefox's reader mode?
/u/laumann and I have organized 2 Rust-centric meetups in Copenhagen before. We thought it might be a good idea to have a central point of information for any future meetups, with the next coming up this January!
What do you mean by &gt; These tuples require me to move my variables ? Could you not move a reference instead? Worst comes to worst, you can just use an `Rc` or `Arc`.
The above code compiles in 4s on my 6yr old laptop. I'm guessint you are measuring a full recompile? 4s is still longer than what would be reasonable so sticking the parser in a separate crate and specializing it for the needed inputs so it can be compiled separately is still the only way to really make it manageable unfortunately :/. I'm not really sure what you mean by moving stuff to runtime though?
Yes, `_` means compiler-inferred. I believe this is explained in the book, and the explanation there will be a lot better than anything I can give.
I'm trying to find a bit of time to work on the semver-aware deprecation lints for [clippy](https://github.com/Manishearth/rust-clippy). Also working on typetree, a crate with a type based tree impl (work in progress).
It's on the internet; anyone can participate, and the organizers won't mind. I'm just saying that wasn't the original intention. It seems to just be a bit of Rust fun, nothing location specific.
Totally broken on Safari too.
Just one thing: when you do (something...).collect::&lt;Vec&lt;_&gt;&gt;() It means that `(something...)` is an iterator and that you're collecting it into a data structure. On some cases, Rust will be able to infer what the type of this structure is (in which case no `::&lt;Vec&lt;_&gt;&gt;` is needed). If fully inferring the type is not possible the above code instantiates the `collect` method, but you can still omit details (in the above example it says "collect into a `Vec&lt;_&gt;` but infer what `_` is") On the other hand, if you think the *person* reading your code would be confused by the type, you should use a type annotated let instead: let vector : Vec&lt;_&gt; = (something...).collect() // use vector here (I wrote a longer comment about this [here](https://www.reddit.com/r/rust/comments/3wr2st/rust_beginner_how_idiomatic_is_my_code/cxyjm2u))
Then I think the reason is that you disabled Javascript ;)
Yep. `unwrap` (and `expect` when I feel like typing more) is for errors that are completely in control of the programmer, and hitting one indicates that I made a mistake that I need to fix. Since the input is completely under my control, if my regex doesn't compile or match the input, or if any of the pieces I parse fail to parse into integers, that's a bug in my program. It's just a really bad habit for when I write code that isn't like Advent or Project Euler.
You don't need `convert` - just use `&amp;vec[..]`. [Example](http://is.gd/HYaY62)
I've been having a look at how other languages do comments that have a start and an end token. It looks like in JavaScript multi-line comments are cancelled on the first end delimiter, will have a look at some others and copy the behaviour. OK: Javascript - /*/*Comment*/ G-Code Equivalent - ((Comment) Not OK: JavaScript - /*Comment*/*/ G-Code Equivalent - (Comment)) 
Yeah, languages without nesting don't have a concept of nesting depth - it's just a boolean toggle. It should be even simpler than what I gave to do that (you can even just use a `regex` expression).
&gt; .split(';') .take(1) .collect::&lt;Vec&lt;_&gt;&gt;()[0] You don't need to collect it into a `Vec` if you know you only want one element. Just use `.split(';').take(1).next().unwrap_or("");`.
I am excited to see what the challenge is. They seem to be a candidate vetting system. I wonder if it s a preemptive setup for rust interviews or they are seeing demand for rust interviews? 
This looks pretty good. Since `read_exact` is going to be stable soon, I wonder if it would look better by using it. That is why the first line is there, after all!
Yeah, I thought for a second about how to use the lengths. Since you already have to have code that reads until a newline, I'm not sure of the value of the lengths. If the seconds and third lines were packed tighter or could contain newlines, then reading a set amount of bytes would be the right thing to do. Although then you get into the question of are those numbers bytes or codepoints or whatever...
&gt; Considering that Servo consists of over 800k lines of Rust code and nearly **1.2 million lines of C/C++**, that’s quite a feat! Huh, I didn't know that. I suppose there are some big libraries that are included in that count?
Thanks! This works! I tried gen_3D_vol so far and with upper case "D" it still wants to rip "3D" apart -&gt; gen_3_d_vol. But with the lower case "d" the linter is ok with it.
Does piston use glium?
Thank you for taking a while to look at this :) \#1 and \#2 look great and is way better than storing the first line. \#3 ... makes sense, no need to do that :) \#4 I am not so sure about. I kind of fail to see how this can be any faster. Is there an overhead when using the iterator function or is there any way to find the first occurrence of a character faster than going through all of them from the beginning? I guess I will try to find out how to benchmark programs... Anyway, it is still an interesting way to change this :) Thank you very much, this surely helped me a lot.
FYI, `.ok()` turns the `Result` into an `Option` but doesn't handle the error. If you just want to ignore it, use `let _ = ...`: let _ = io::stdin().read_line(&amp;mut lengths); If you want to assert that there weren't any errors, use `unwrap()`: io::stdin().read_line(&amp;mut lengths).unwrap(); If you want to assert that there weren't any errors and provide a custom message, use `expect(msg)`: io::stdin().read_line(&amp;mut lengths).expect("expected line lengths"); Otherwise, you can actually handle the error: match io::stdin().read_line(&amp;mut lengths) { Ok(_) =&gt; (), Err(e) =&gt; // do something... }
GLX and EGL are two API used for creating OpenGL contexts, and GLX is indeed Xorg-sepcific. But as /u/tomaka17 said, glutin abstracts over that, so an application using glutin should not have to worry about this. Looks like servo is doing some black magic with OpenGL that is outside the scope of glutin ?
No browser (well, accelerated browser) uses raw Skia. In fact, with WebRender I want to dump Skia entirely except for canvas and save-to-PDF :)
This is somewhat tangentially Rust related, but it is obviously highly relevant to Servo, and since this is basically the Servo subreddit I felt I'd submit it here.
Notable quote from the article: &gt; Rust is just plain awesome.
I would really strongly recommend using `unwrap()` over `let _`. The latter _always_ comes back to bite me, and is much harder to debug.
Is it wrong or what?
Absolutely, my mistake. I kinda feel sketchy calling `next()` directly but it avoids heap allocation so that's already one point in favour.
Does `.nth(0)` feel better?
Started working on a compressed sensing library called Incoherence. I'm working through Collenchyma's code/examples to see if I can leverage it, since that would theoretically give me the flexibility to offload computation to the GPU at a later date if I wish. And it supports native CPU BLAS for now, so that's enough to get up and running. As an ex-biologist, my math skills are pretty sub-par, so I'm also reading up on linear algebra so I can grok the algorithms a little better. :)
&gt; So Midori must have done some kind of unwinding to free resources associated with a SIP when it crashed I don't know that "unwinding" is the right term, but I don't really know how rust unwinding works. Unwinding to me implies some kind of stack frame walking, running destructors etc. I don't think that's what Midori did when killing SIPs (or at least, I don't think that should be necessary, but I might be missing something). Presumably they just do analogous things to what Windows does when a process dies w.r.t. OS resources you're using. You just need some kind of flat list of stuff your process owns (or some way of recovering it from other data) that you can walk and tear down as the process (SIP) ends. No need for any special "unwinding" in the process itself, right? Maybe Joe Duffy will blog more about that, but I don't really see why you'd need unwinding for the 'fail fast' stuff - the OS would just notice that SIP X has died and go through and collect any resources associate with X (file handles, memory pages, etc.). You could presumably do something similar for Rust threads (e.g. keep a flat list of OS file handles, etc.). Memory is trickier since it can be shared between threads (unlike with SIPs) so you can't go and deallocate all the pages associated with a thread, you'd need the allocator to play along somehow so you don't kill memory that another thread is also using. This may need some FFI interface so that you can add "on thread panic" handlers (to free up external resources in bulk when the thread dies) rather than attaching it to RTTI style per-stack-frame stuff. But then again, killing the entire process on panic seems fine too. It's supposedly an unrecoverable programmer error, right? No reason why you should let people try to "rescue" it. 
I love that Redox is a thing. I haven't put much time into testing it yet though.
Haskell nests comments, and I feel like it's more useful overall, since you can comment out code that has comments in it, but you really should check the spec. {- Comment {- ed -} out -} 
Stupid but works: let x = (/* some really long tuple that I don't want to retype */); let y = (x, /*something else */); let x = (/* some really long tuple that I don't want to retype */); let z = (x, /* something else */);
&gt; @jackpot51 and @stratact are working on OrbTK, a tool kit for UI widgets, layouts, and other stuff. Works on Linux, too! Sweet! Hopefully this gets its own KISS-UI backend someday!
Very interesting. Are there any other browsers (or I guess other graphical contexts outside of browsers) that do anything like this optimization, or are you making it up as you go?
Is that because Skia is a good fit for canvas, or because WebRender is a bad fit for it? IOW, if Skia is ever relegated to just canvas, would it make sense to replace it with a new component specialized for that task?
Iirc SIPs have special channel memory owned by the OS for message passing etc., they don't share heaps or any random memory. Needing unwinding for just the functions that may throw is very different from having it everywhere. So they can have a cheap system that tears down the whole process in one go on a programmer error and doesn't add any peanut butter cost to the program, and only do unwinding stuff in the comparatively small number of functions that can throw.
&gt; What is "the cost of unwinding" here? What cost? This makes no sense. There's cost to code quality from having to assume that you may need to unwind at any point, plus the bloat of the actual unwinding code. If you can compile most functions without having to worry about any non local code flow (because you statically know there are no exceptions), and its implications for destructor etc. you can do a better job. 
Note that there was a lot of instability around 1.0 which this includes (one-off thing). I think this also includes dependency breakages? 
You may want to look into the `Cell` types for interior mutability (like modifying something owned by an `Rc`).
&gt; This would be orders of magnitude less book keeping costs as you go than if you do unwinding. The actual program code isn't affected. Consider that a destructor that maintains an OS resource that trusts that the OS will free the resource can simply avoid freeing it on panic, and then the optimizer will realize that the landing pad contains no side effects and can optimize accordingly. The implementation strategy of unwinding has nothing to do with it! (I'm skeptical that you'd actually want to do this, though. In practice the cost of zero-cost unwinding is very low. Unwinding gives you the freedom to run destructors if it doesn't impact the performance of your program—which it usually doesn't. It doesn't force you to.)
Implicit in that argument is that the destructor is isolated in a landing pad which makes my whole point! If you don't need to worry about unwinding, destructor are just code that runs at the end of the function. The optimizer can then move this around as it sees fit. A few instructions of it may run right away, a few more a bit later on, some of it may be hoisted in to two separate branches and done differently in each case, some of the destructor instructions may be vectorized with non-destructor code making them "free", etc. Not having unwinding opens up opportunities for better code quality because it's just "normal" code The fact that any function call may lead to an unwind, means you either have to do book keeping in the hot path to track what you've already done so the landing pad code can reconstruct what's left, or constrain the destructor code so it doesn't get arbitrarily mixed in with the rest of the code. This is the peanut butter cost. You pay it even when things don't panic.
In Rust this part on mutability is much less clear-cut than in other languages: &gt; Mutability is a scourge. Most programs should avoid mutating memory at all costs. The discussion on mutability/immutability support in Rust was very intense at one point (dubbed the "mutpocalypse"), so I will not resume it here. But still, it is good to know that, by ensuring at compile time that generally only one owner can change a value at any given time, Rust makes immutability a less central concept than in other languages (while not completely negating its advantages). For reference: http://smallcultfollowing.com/babysteps/blog/2014/05/13/focusing-on-ownership/ .
Well, the time you see there is local time. For instance, if you are at Paris, the time should be displayed as 12 30 PM, Dec 22 2015 to 12 30 PM, Jan 5 2016 A patch will land in couple of hours and after then, the site-wide time format would be HH:MM AM/PM UTC/IST/.. , MMM DD YYYY (Ex: 05:00 PM IST, Dec 22 2015) Disclaimer: I work at DoSelect.
We're looking into it. Send your browser env details to support [at] doselect [dot] com
Likewise.. its a steep curve
can i use it in windows? I try to use the example from the repository and compilation failed. $ cargo build Compiling clang-example v0.1.0 (file:///C:/depot/clang-example) error: linking with `gcc` failed: exit code: 1 note: "gcc" "-Wl,--enable-long-section-names" "-fno-use-linker-plugin" "-Wl,--nxcompat" "-static-libgcc" "-m64" "-L" "C:\\Program Files\\Rust nightly 1.6\\bin\\rustlib\\x86_64-pc-windows-gnu\\lib" "C:\\depot\\clang-example\\target\\debug\\clang_example.0.o" "-o" "C:\\depot\\clang-example\\target\\debug\\clang_example.exe" "-Wl,--gc-sections" "-L" "C:\\depot\\clang-example\\target\\debug" "-L" "C:\\depot\\clang-example\\target\\debug\\deps" "-L" "C:\\Program Files\\Rust nightly 1.6\\bin\\rustlib\\x86_64-pc-windows-gnu\\lib" "-L" "C:\\depot\\clang-example\\.rust\\bin\\x86_64-pc-windows-gnu" "-L" "C:\\depot\\clang-example\\bin\\x86_64-pc-windows-gnu" "-Wl,-Bstatic" "-Wl,-Bdynamic" "C:\\depot\\clang-example\\target\\debug\\deps\\libclang-fab9fb7590da95a2.rlib" "C:\\depot\\clang-example\\target\\debug\\deps\\liblazy_static-f3aa6dfcc7c157cc.rlib" "C:\\depot\\clang-example\\target\\debug\\deps\\liblibc-53b3109cf33a6ace.rlib" "C:\\depot\\clang-example\\target\\debug\\deps\\libbitflags-eec34728826d9769.rlib" "C:\\Program Files\\Rust nightly 1.6\\bin\\rustlib\\x86_64-pc-windows-gnu\\lib\\libstd-8cf6ce90.rlib" "C:\\Program Files\\Rust nightly 1.6\\bin\\rustlib\\x86_64-pc-windows-gnu\\lib\\libcollections-8cf6ce90.rlib" "C:\\Program Files\\Rust nightly 1.6\\bin\\rustlib\\x86_64-pc-windows-gnu\\lib\\librustc_unicode-8cf6ce90.rlib" "C:\\Program Files\\Rust nightly 1.6\\bin\\rustlib\\x86_64-pc-windows-gnu\\lib\\librand-8cf6ce90.rlib" "C:\\Program Files\\Rust nightly 1.6\\bin\\rustlib\\x86_64-pc-windows-gnu\\lib\\liballoc-8cf6ce90.rlib" "C:\\Program Files\\Rust nightly 1.6\\bin\\rustlib\\x86_64-pc-windows-gnu\\lib\\liballoc_jemalloc-8cf6ce90.rlib" "C:\\Program Files\\Rust nightly 1.6\\bin\\rustlib\\x86_64-pc-windows-gnu\\lib\\liblibc-8cf6ce90.rlib" "C:\\Program Files\\Rust nightly 1.6\\bin\\rustlib\\x86_64-pc-windows-gnu\\lib\\libcore-8cf6ce90.rlib" "-l" "clang" "-l" "ws2_32" "-l" "userenv" "-l" "advapi32" "-l" "compiler-rt" note: ld: cannot find -lclang I already installed mingw-w64-x86_64-clang on my msys2 system,
I think you need to split it into two crates: `clang-sys` and `clang` where `clang-sys` will only contain raw ffi interface. This is the convention as described here: http://doc.crates.io/build-script.html #![cfg_attr(rustc_1_5, allow(raw_pointer_derive))] Is this a workaround for some bug in 1.5?
Finally managed to finish this. This *should* bring out-of-the-box support for wayland for rust projects using glutin for window and events management. There are most likely some bugs left, and some edge cases are not yet handled, but it should work for most classic cases, and it'll improve as I keep working on it. :)
In this case, IIRC, it means that the generic type 'S' may not have any lifetimes associated with it--it must wholly own its contents. Or, if more correctly, if there are lifetimes, all are 'static.
The crate doesn't seem to install clang, so you probably need to [install it yourself](http://llvm.org/releases/download.html) since it doesn't come with windows.
The easiest way to use this crate on Windows is to just download the llvm+clang binaries from [here](http://llvm.org/releases/download.html) and drop libclang.dll into somewhere `rustc` can find it. I added it to `&lt;rust&gt;/lib/rustlib/x86_64-pc-windows-gnu/lib`. Edit: I'll add this information to the README.md.
Oh, I thought the `*-sys` convention was for when you had a crate that compiled C code. I am not bundling `clang` with my crate because `clang` is immense and takes 30+ minutes to build on many machines and the only thing my crate depends on is `libclang.so`/`libclang.dll`. Do you think I should still split it up? `#![cfg_attr(rustc_1_5, allow(raw_pointer_derive))]` This is because the `raw_pointer_derive` lint has been removed since 1.5, so if I don't include it for 1.5 builds I get warnings about using `#[derive]` on structs containing raw pointers and if I include it on non-1.5 builds I get an unknown lint warning.
Sure is. We're all hanging in there somewhere...
Thank you, that clears it up nicely. I had been worried that I was somehow coding myself into a corner that I wouldn't discover until later, but based on this I think all will be well.
That is also kinda explained [here](http://rustbyexample.com/scope/lifetime/lifetime_bounds.html). It doesn't explain `'static` but the syntax is explained so the usage of `'static` in that case should hopefully be clear. I don't think it is explained anywhere else in the official docs.
OrbTk is like 'window manager', but highly customizable? 
You can swap out the limitation of "T must not reference anything non-static" for "T (and by implication everything it references) must live at least as long as the Signal", by taking and storing a mutable reference to the signal, instead of taking it by value and moving it into a box. I quickly refactored your implementation to work that way here: http://is.gd/OSpwR0 I don't have enough experience to say for say which will be better in the long term, but my guess is that the references will be less restrictive. You should be able to convert anything that is `T+'static` to `&amp;'a mut T`by adding a second vec for "owned signals", moving ownership of a `T+'static` variable into that vec, and then taking an &amp;mut reference to that element which is in turn added to your receivers, but it feels like a lot of complexity.
Thanks! My original implementation was very similar, but I couldn't get the lifetimes to work out quite right (hence the switch to static). This is actually what I was trying to achieve to begin with, so thanks for taking the time to post it.
MOZILLA
AFAIK `*-sys` crates generally do following things: 1. Search for library. 2. (Optionally) if library is not found try to build it from source. 3. Provide raw unsafe bindings. &gt; This is because the raw_pointer_derive lint has been removed since 1.5, so if I don't include it for 1.5 builds I get warnings about using #[derive] on structs containing raw pointers and if I include it on non-1.5 builds I get an unknown lint warning. Shouldn't you then check if version is greater or less then something instead of equal? Non-1.5 builds can be either 1.4 and less or 1.6 and greater.
OK, I'll split up the FFI bindings soon, thanks for the input. &gt; Shouldn't you then check if version is greater or less then something instead of equal? Non-1.5 builds can be either 1.4 and less or 1.6 and greater. I fixed this, thanks for noticing.
Very happy to see this, thanks :D
If you could just add `margin: auto` to the `main` element on your blog... that would be amazing. Having it centered seems to be a lot nicer than having it left-aligned from my view, but maybe you prefer it left-aligned.
What's the specific setup that's causing the unsafe object warning? This works: struct Foo&lt;'a&gt; { t: &amp;'a MyTrait, }
frank_jwt claims are a string btreemap so you lose types and same for the `private` part of rust-jwt while claims are typed in mine, not much difference otherwise. I was doing some jwt work in go/python so I thought making a jwt library would be a nice first project in rust (and it was!). Didn't see frank_jwt at the time though
I was looking at the challenge. Some things I think may be improved: * make the stub code more idiomatic/run it through rustfmt, * use `unimplemented!()` in stubs instead of nonsense values, * avoid using numbers to represent enums (directions) and corner cases (solution not found). It seems weird in Rust.
I think you misunderstand me. The issue I'm referring to is that if a destructor is "just code", a destructor can get "spread out" all over the function. So if something unwinds it could do so at a point where multiple destructors have run "half way" (and indeed, some constructors may have run "half way" too) and the rest of the destructor code is mixed in with the rest non-destructor code (which you don't want to run while unwinding!). So now you'd have to store some kind of residual destructor logic for each point where the program may unwind (or just make sure that destructors run fully or not at all between "unwind points", which is the more sane option). Duffy even specifically mentions this as a cost for the unwinding in the blog post (it inhibits code movement), so it's not just me saying it here. You could have have a mode for this in rustc I guess. Literally have a flag that disables all unwinding. No landing pads are generated, and destructor code just get pasted in at the end of their scope as if the user had just written the code there. Strip out all special logic that has to do with unwinding. Then you could run benchmarks on this compared to unwinding and see what the extra bloat and other stuff actually costs you. I also think you're confused about what Midori does, but I don't work on Midori so I can only based that on what I've read (which is a lot - I've been following what I can from this since it was Singularity). I don't really understand how you get what you think is happening from the information that has been posted though, you seem to just inject your own assumptions into it. Again, SIPs have their own isolated heaps with their own isolated GCs running inside the process itself (which is not hardware isolated, but still isolated much in the same way windows processes are isolated). Killing a SIP just means giving back the underlying memory pages to the OS and destroying any other OS resources (like files, exchange heap pages, etc.). You don't need to care about GC safe points because the GC *itself* is about to be destroyed anyway! Literally the GC data structures that might be corrupted by interrupting the program at a random location are *themselves* microseconds away from being handed back to the OS and no more GC code will run before that happens, so worrying about it is wasted effort. The benefit of treating panics as "fail fast" and killing the whole process is exactly that the code generation for the program can literally spend zero resources worrying about where a panic may happen - it's handled outside the process entirely (and can even be lazy - the OS could just freeze execution of the process and clean it up when it gets around to it). There are no "safe points", no landing pads, etc. There may still be destructors but they are literally just code that runs at the end of a scope, there's no special tracking of which ones of them have run at a given point (and even if some of them have run partially).
Rust makes you jump through a lot of hoops to write code, with the borrow checker and type system. If you learn C first, you will very quickly realize why all of that is necessary. 
I'm writing some non-Rust code to put together a Blender plugin for my [ray tracer](https://github.com/Twinklebear/tray_rust), so far I've got a somewhat hacky script working that I'll now convert to a proper plugin. Here's a cool sample scene where some falling Suzannes are simulated with Blender's rigid body physics then the scene is exported and rendered with my ray tracer, to produce this neat [animation](https://youtu.be/p3cojzRkmcg). If you want to try it out you can grab the script [here](https://github.com/Twinklebear/tray_rust_blender) and just load/run as a Python script in Blender. It's pretty rough though so it's still kind of hacky and not too user friendly.
I don't see any mention of Rust on that page, can you cite a source?
I guess it is a violation of rules (no memes). Dunno why we have such a rule.
The idea is to avoid nonsense like [these](https://github.com/MrMEEE/bumblebee-Old-and-abbandoned/commit/a047be85247755cdbe0acce6f1dafc8beb84f2ac#diff-3fbb47e318cd8802bd325e7da9aaabe8L351) [threads](https://github.com/apple/swift/pull/17).
It really depends on the time frame when you want to know C and start doing embedded device programming. If the answer is "soon", then learn C first. Rust isn't really there yet for embedded programming. But if you have time and are okay taking the long route, learning Rust first will definitely make you a better C programmer. And C will be easy to learn once you know Rust. But one way or another, unless you focus on one very specific device where Rust is / becomes very well supported, you won't be able to avoid learning C.
You do not need to learn C to learn Rust. I think the only actual advantage of learning C before learning Rust is not that knowing C makes it easier to get Rust, but that a lot of resources that help you understand relevant concepts both languages share (such as pointers, the stack and the heap) are aimed at people learning C.
If you want to compile in #[no_std] mode, you won't be able to use the stable compiler, because the developers haven't decided how they want it to work (and enabling it in the stable compiler would make the current state of affairs de facto standardized; if you don't understand the problem with stabilizing the first implementation, it means you've never used PHP). Stable Rust is perfectly suitable for non-embedded projects like video games, but embedded programming without the stdlib is still experimental.
Well, I'm interested in Cortex-M3 programming with Rust, and to get all that hoopla out of the compiler causes a lot of friction.
Rust does 1) and 2) as you expected. Rust does not do 3), for a few reasons: 1. It wouldn't actually reduce memory usage, because on 64-bit architectures you generally can't pack a pair of a pointer and a 32-bit integer any tighter than two pointers. 2. It also wouldn't be any faster, because moving 32-bit values around is not any faster than moving 64-bit values. 3. It would require special linker support that doesn't exist, including dynamic linker support or a language runtime that hooks into the dynamic linker.
Right, linker support would be required. I imagine the packing wouldn't actually be done right next to each other. Something more like 2 ptrs followed by 2 tags (or more ptrs + more tags if the tags can be 16 bits or 8 bits). That way access would stay aligned but we'd save the extra 4 bytes (33%). Something like http://goo.gl/UwuI6M Of course the addressing gets more complicated now and we'd be gaining some 33% memory at the cost of more complex addressing, which is not a clear win. We also add the constraint that array sizes have to be multiples of 2 or at least waste the space for one extra slot. But I think it would save quite a bit of memory(43.5%) if for example num_implementations_of_trait &lt; sizeof(uint8_t) and we could use a byte per tag. 
&gt; This term, one of the open source project choices available was to refactor the OpenGL usage in Servo’s graphics stack, effectively adding support for EGL to all Linux-based systems. Adding EGL support now is a somewhat late, since with Vulkan coming out it should be done with WSI already... It would make it more future proof.
Glad we got you excited! DoSelect started out as a skills assessment and recruitment tool for companies. We are launching our developer engagement platform shortly, so you can see these public hackathons as practice for candidates. Our assessment engine is capable of assessment on Rust, though we are yet to see a growing demand for Rust from the teams that are recruiting. P. S. I work on product at DoSelect.
I never learned c from a book so I cannot recommend one. But I think using an old edition risks learning things that are now UB.
Learn C first. Rust abstracts a lot.
Nice, I'm [reading about it](https://stackoverflow.com/questions/24405129/how-to-implement-fast-inverse-sqrt-without-undefined-behavior). If the issue is pointer aliasing, using `memcpy` was suggested (what may not be apparent is that `memcpy` may be optimized out by the compiler). The other problem is that the C standard doesn't specify the layout of floating points, but that merely makes the code not portable (undefined behavior is different from platform-dependent behavior) Yeah, K&amp;R is definitely lacking a discussion on pointer aliasing. Which, by the way, is what Rust is about. edit: regarding the "the C standard doesn't specify the layout of floating points" stuff (or the layout of pointers, etc), things like pointer tagging and NaN-boxing [are widespread](https://wingolog.org/archives/2011/05/18/value-representation-in-javascript-implementations) and are likely employed by the browsers we are using. Of course, this kind of stuff is very unportable.
You could start with some assembler. C is just a basic set of macros around raw assembly. And Rust is, arguably, a set of macros around C (with some extra checking thrown in). As for which you should learn first, it depends a lot on what kind of person you are. If you just like to get in a car and drive - then maybe Rust is good to start with. On the other hand if you have to know *why* about everything then starting with some assembler, then some C, then some Rust is probably a good progression.
 $ brew install multirust $ multirust default nightly $ multirust update nightly And then you can quite easily use `#[no_std]`.
I interviewed with them. Hopefully they won't mind me sharing this here.
When can I use it in production?
Maybe ask [here](https://github.com/rust-lang/rfcs/issues/1005)? It's not something that I need so I haven't been following closely.
&gt; I'm interested in automobile interface programming, which is in MISRA C Do I take that to mean that you intend to do this professionally? Then learn C. It's the standard language in industry for embedded, and especially for something as entrenched as automotive, they're not going to jump to a brand new language like Rust any time soon. To do so would require compilers to be written for all of the various embedded processors that are out there (surprise, not everything runs on an ARM), and then would probably require development and approval to create safety standards equivalent to MISRA.
This is the book I learned C from, as well.
&gt; Maybe rust does support this way of working ? (I haven't checked lately), It does, very much so.
In stable Rust, apparently 1.6.0. So less than six weeks. You could also use beta instead of nightly right now.
Still, a certain threshold of elaboration is expected and the answer doesn't match the question.
How do *you* want to turn an array of vectors of bytes into a single string? Do you want to concatenate them? Do you only want the first vector and throw out the rest?
Are you aware that `get_raw` returns _an array_ of `Vec&lt;u8&gt;`s? I.e., there can be multiple strings for each header key. If you _really_ want an `Option&lt;&amp;str&gt;`, you have to reduce the list of header values to one string (by concatenation or just taking the first one, for example). You could do something like this to get the first value (untested): self.raw.headers.get_raw(name) .and_then(|vals| vals.iter().next().map(String::from_utf8))
We are running a rust daemon in production at beget.ru.
Neat! Thanks for sharing.
Not a direct answer to your question, but what is wrong with just exposing its response object directly? What I currently see is that you're destroying hyper's strongly typed header structs by wrapping them with a simple string-to-string mapping.
The main difference to me seems that you could have instead spent those hours being productive or doing something you enjoy. "Waste a lot of time being frustrated so that you appreciate not having to waste time anymore" is not a compelling argument to me.
Why does memchr need its own module? Wouldn't it be fine to put in `std::mem`?
Some great advice in this thread. Personally, I came from a Java and Python background and jumped right into Rust. There's a learning curve, but progress is always consistent.
&gt; I convinced myself at one point that you could do a compiler transformation to convert return value error handling to use exceptions. Then perhaps there should be something like `#[errors_are_uncommon]` hint to the compiler to automatically transform `Result`-based errors into unwinding, just as an optimization (with the same high-level semantics of current `Result` approach). That would be exceptionally flexible, and help reduce the abuse of panics for recoverable errors. Or even, a compiler flag to make the compiler decide by itself whether it should optimize for one case or another (but that seems dubious).
Sounds cool; can you give us any (non-confidential) details? Like, which features of Rust made it stand out from other options?
Hey, thanks for your advice. I'm a Rust newbie, that's why I posted this question here. `raw` is meant to be private, so the public API is what you see in `impl`. Not sure if this is "Idiomatic Rust". Yes, I'm "destroying" it, to abstract my domain model from low level details. To me hyper is just an adapter, while my `Response` should expose only meaningful methods for my library. What's your suggestion to keep strong typed and to hide this implementation?
Very true. 
Is the specification available online? I remember having an issue with this.
I'll honestly envy anyone that can grok Rust without having learned C and not experience the pain points.
So the GPGPU thing is to process the game logic (spreading of research, through what, diffusion?), not just to render the graphics we see? What are you using, OpenCL?
What is your compiler going to emit, x86 asm, LLVM IR, some bytecode? Also, what's your memory model? Is everything allocated on the heap? I found the [duktape](http://duktape.org/) design very nice if you're going for a simple VM.
I just wouldn't hide it. Since you're writing an API client, direct access and manipulation of headers probably won't be necessary too often (if you have a good abstraction) and if the user does need to reach down into the API client's guts, why obstruct their way?
C is terrible; MISRA C is worse, although if you follow it faithfully you'll probably have fewer bugs in the end. You'll probably have to learn to use it effectively if you pursue a career in embedded systems programming, especially automotive embedded systems. Modern cars are essentially a networked distributed computing system on wheels, and most of the computers are tiny microcontrollers that either monitor something via sensors or actuate some physical control. Most of these run C firmware today, with a smattering of C++ and assembly code. Some of them could conceivably run Rust code, but don't expect it to happen at a large scale anytime soon. Expect to learn a lot of different CPU architectures; x86 is nowhere near dominant here, although you'll see it in some head units. ARM is getting big, but you'll see just as much PPC and stuff you've probably never heard of before like Renesas V850. Learning C won't teach you how a computer works, and there's very little in C itself that will teach you how to structure programs well. People say this because they happened to learn how a computer works in temporal proximity to learning C and they observed apparent similarities and probably ended up conflating the two more than is justified by the C specification. Rust, on the other hand, has a number of features that will help you learn how to write reasonably bug-free and well-structured low-level code. These skills should translate fairly easily to C, although you'll find yourself replacing language features with programming idioms/patterns and conventions (e.g. naming conventions for namespaces, patterns of using header files to simulate modules, etc.). It's easier to learn these things when there's tools that understand and support them. For example, it's a lot easier to write structured programs in assembly language if you become familiar with the concepts first in a language that provides them natively like C. Rust provides structuring for concurrency, resource management, and program modularity that you can become familiar with and then mimic in a language such as C that doesn't provide them natively. Learn Rust first, and get a copy of the platform ABI specification for the platform you're writing on. This will teach you all the machine-related stuff that people *think* that C will teach you, but are not so much part of C as part of the omnipresent stuff *below* what's specified by the C standard. You should probably start learning C concurrently with Rust (i.e. after you feel like you have a reasonable grasp on how to write Rust code without fighting with the language, but before you feel you're really good at it) and K&amp;R is a reasonable place to start (the standard is based on the R part of K&amp;R, which is the latter half after the tutorial, and the language hasn't changed a whole lot from the original ANSI C version) but you should also get copies of the various C standard specifications if only for Annex J, which lists out all the things that are either unspecified, undefined, or implementation-defined (these are large lists! Get the compiler and platform ABI manuals you're using for the resolutions to the implementation-defined parts). The last pre-publication draft of each version is available online for free in PDF form. You'll probably want something a bit easier to digest than the specifications for actually learning newer bits of the language not covered by K&amp;R; I haven't read a lot of these but Harbison and Steele's "C: A Reference Manual" is decent and covers up to C99, which is probably as recent as most of your automotive ECU compilers are going to be for a while. It also discusses overlap between C and C++, which can be important in some coding houses. Lots of automotive code these days is mixed C/C++. I'm not aware of any good books that include changes in C11, but it's got some potentially very important changes regarding multithreaded programming, which previously was not really part of the C specification at all. You'll also need some guidelines on how to do MISRA compliance; I don't have a lot of guidance for you here aside from the fact that you'll probably have to buy the MISRA standard and a linter tool for your C programs, neither of which are particularly cheap if you're not already employed to use these things. Be warned; MISRA will tell you NOT to do a lot of things that K&amp;R teaches. When you say automobile interface programming, if you're talking about hardware interfaces (rather than human interfaces) then you'll probably want to find some good material on software implementations of control theory concepts. Quite a lot of embedded code, especially in an automotive environment, is writing control systems, and knowing both the theory of control systems and how to translate the mathematics there into concise and efficient programs is probably more critical than (and largely separate from) what language you're using. You'll probably also want to learn a bit about the OS layer that's fairly common in the automotive world, which is specified by AUTOSAR. These specifications are huge and rather difficult to read, although they're freely available. I don't really know of any tutorial material, though I'm sure there's some around. The OS part of AUTOSAR is based on a previous work called OSEK, which has a much easier-to-understand specification and several open source implementations. It's a very simple OS based on interrupt-driven run-to-completion tasks, with optional "enhanced tasks" that can sleep/block waiting for events, which can be generated by interrupts or other tasks. It's mostly statically configured and doesn't require a lot of complex run-time code, so it's a reasonable OS to start learning on or possibly write your own implementation of in Rust! If, on the other hand, you're talking about the human interface part of automotive development (i.e. head unit stuff) you should probably get familiar with embedded Linux and QNX, which as far as I know are the most common platforms (with QNX being the big incumbent player in high-end systems, and Linux starting to make major inroads starting from lower-end graphical head units and moving upmarket from there). MISRA compliance is often not a hard requirement here, although that could change with concerns over hacking, and frankly a lot of the GUI stuff is done in Javascript/Actionscript and is not terribly different from writing desktop or smartphone apps. Most of all, no matter what you do you're going to have to develop a great skill at reading and cross-referencing a LOT of manuals. A lot of higher-level programmers can get away with a vague understanding of their language and platform along with an IDE and Stack Overflow. You will not be able to get away with that in automotive systems embedded programming. You'll be able to replace some of Stack Overflow searches with vendor support calls, but that's neither free nor very time efficient. Mostly you'll have to refer to all the manuals and books I mentioned above, along with several others relating to the specific system you happen to be working with at the moment. Many answers will require synthesizing information from several manuals, datasheets, specifications, and schematic documents. I get a weird sense of satisfaction from doing that sort of thing; I expect if I didn't I'd have found another line of work long ago. Or maybe I'd have just ended up making a lot more support calls!
C today is not at all a basic set of macros around raw assembly. If you approach it this way, a modern C compiler will surprise you in potentially nasty ways. You could start learning some assembly language, but which one? A single vehicle model might have 3 or 4 different CPU architectures in its various controllers. It would be quite common for none of them to be x86; it's possible none of them would be ARM either. C and Rust are not that far apart in terms of how abstract the low-level parts of the language go; i.e. you can't portably/reliably get a lot closer to the machine in C than in Rust. Rust just provides more error checking and scales upward better towards higher abstraction. This means that you can learn *from Rust* how to write better C code.
One of the C standards committee's primary goals is to ensure that existing C code bases will not be broken by new versions of C. Part of the reason the specification is such a mess is to preserve compatibility with existing code that runs on bizarre architectures. K&amp;R is, however, *stylistically* old and encourages a style of programming that's now typically seen as difficult to understand, error-prone, and sometimes even completely disallowed by lint checkers. Aside from that, however, it remains a remarkably succinct yet thorough coverage of the parts of C that it covers, so it's not a bad place to start. Just don't expect to *end* at K&amp;R these days.
No, count counts the number of iterations, the fold does a sum of the results of all the iterations. Unless there's multiple folds and we're looking at different ones.
I'm working on mutable views into disjoint areas of a 2D array. I am seriously going nuts about this. I am completely stuck with [this problem](http://is.gd/bzBvxL). I just want to wrap those aliases in a cuddly, safe coat and never ever let them go outside. If only I could tell those A's to end their lives as soon as the t dies. I just want them to die. Please.
Ah ha! Dave changed the text since I read the post.
You don't need to change the ownership semantics; `&amp;'a mut T` is akin to `Box&lt;T + 'a&gt;`. https://play.rust-lang.org/?gist=f35ceb6c3e13ecd8a9e9&amp;version=nightly
`.fold(0, |sum, line| sum + line)` can be written as `.sum()` although I'm not sure that's in stable.
No, the value is unspecified if it wraps, and it may or may not trigger a panic. We should probably clarify that. Obviously we would not compromise memory safety for this. Bounded unspecified behavior is not undefined behavior.
This looks fantastic. The thing keeping me back though so far has been nodejs' expensive ffi cost. JS implementations of things are sometimes faster :/
glibc uses an sse2 on Linux x86
http://code.woboq.org/userspace/glibc/string/memrchr.c.html
It's more of a cmp operation than memory manipulation. 
&gt; The thing keeping me back though so far has been nodejs' expensive ffi cost. With node-ffi? This doesn't do that.
FCP https://github.com/rust-lang/rust/issues/27739#issuecomment-165603735
I started with K&amp;R and didn't like it. I got much farther into Sams Teach Yourself C in 21 Days. I just liked the style better.
Yes, GLSL for GPGPU. OpenCL works exactly the same, except that it's maybe more convenient to use and can work on the CPU as well. 
If you have some legitimate examples that show this is a generally useful method, you can open an issue on https://github.com/rust-lang/rfcs. Otherwise, this can be implemented as an extension trait in your own code or an external `Option`-utilities crate. I don't see one on Crates.io, so maybe you could jump on that.
&gt; I implemented Buffer.equals and Buffer.compare for nodejs in C++, since that's where most other Buffer methods are, there isn't much of a speed improvement for doing buf.equals(other) versus a JS version. I'd have thought Node stdlib's `Buffer` was already implemented in C++. Isn't this the case?
I might be tempted by let mut data = Some(1); if data.map_or(false, |num| num &gt;= 5) { data = None; } --- IIRC, there was a suggestion of some `bool → Option()` method somewhere that never got anywhere, but it would let you do, perhaps, let verified_data = data.and_then(|num| (num &lt; 5).of(num)); Aside: Hrm, that's making me think of (a &gt; b).and(|| c).or(|| d) as a humorous/flexible alternative to if a &gt; b { c } else { d }
I don't know if it's any better, but you could do something like let verified_data = data.iter().filter(|&amp;&amp;num| num &lt; 5).next();
It seems strange to iterate over something that can only hold a single value. I think the IntoIter is useful elsewhere when using Option but I usually wouldn't call it myself
`Option` is a container that can keep from 0 to 1 elements, a special case of static vector keeping from 0 to N elements. This is certainly not the only interpretation of `Option`, but still one of the most useful ones.
That's an interesting way to think of `Option`!
&gt; I wonder why they removed it Seems pretty clear to me: lack of use.
What about VFPv2? ;) Seriously, the name is rather unfortunate; especially if someone is going to use neon instructions in the rust part.
Which is bad, but as I said, all we can do about that will be merely show.
It just so happens that I wrote a small helper method that solves this problem earlier today. It turns Iterator&lt;Result&lt;T,E&gt;&gt; into Result&lt;Vec&lt;T&gt;,E&gt;. trait IterUtils { fn try_collect&lt;T, E&gt;(self) -&gt; Result&lt;Vec&lt;T&gt;, E&gt; where Self: Sized + Iterator&lt;Item=Result&lt;T, E&gt;&gt; { let mut v = Vec::new(); for i in self { v.push(try!(i)); } Ok(v) } } impl&lt;I&gt; IterUtils for I where I: Iterator {}
No license can stop from using software in any applied area, even military. If you somehow can restrict usage of Rust then, nothing will stop other people from creating language without that restriction. You are obviously digging in the wrong direction - think how can you make something greater and useful with Rust language.
It doesn't have to be a CStr, in fact I want it to be a String. It's layed out in the stream in the same way a C string is, though (meaning null-terminated). I don't see how from_ptr will help unless I buffer the stream, which I would prefer not to.
This is what you want: let c_str: &amp;CStr = unsafe { CStr::from_ptr(ptr) }; let buf: &amp;[u8] = c_str.to_bytes(); let str_slice: &amp;str = str::from_utf8(buf).unwrap(); String::from_utf8 if you want an owned copy of the data instead of a string slice. Looks like you already have the bytes, so you can probably just use the third line instead. Make sure your string is properly encoded as UTF-8, otherwise this will fail.
Sorry, I missed your "streaming" aspect initially. If you have a pointer to a null terminated C string, then CStr is *the* way to convert from C to a Rust String. (See [to_str\(\)](https://doc.rust-lang.org/std/ffi/struct.CStr.html#method.to_str) and [to_str_lossy\(\)](https://doc.rust-lang.org/std/ffi/struct.CStr.html#method.to_string_lossy)) Rust Strings are: 1. Length-prefixed and not null-terminated 2. UTF8 and not ASCII If you actual goal is to end up with a rust String, you will have to collect all the bytes of the string from your stream and then convert it. You can't do it on the fly because you don't know how long your string is until you read the last (null) byte. If your goal is to just end up with an array of bytes (Vec&lt;u8&gt;), then use Xirdus's approach. But I don't believe doing what you actually asked for is possible without buffering/collecting from your stream.
Why would you go from a CStr -&gt; &amp;[u8] -&gt; &amp;str instead of using the to_str() or to_str_lossy() methods of your CStr?
There is no reason to hold the rust community and developers responsible for ill-intentioned software that *might* be made using rust and would otherwise simply be developed with other tools. If rust was specifically designed for evil purposes, or only applicable to them, then you would have a point.
There's been PRs merged to make it work on Linux, and possibly windows too.
Strikes me as similar to refusing to embrace flatware, since steel is often used in killing people. 
Yeah that was almost a year ago, and pre 1.0, so some stuff got tweaked :(
It's treated the same way in Scala.
Having never used Node before, why would I want to use it with Rust? Or is this just for people who are already using Node for other reasons?
Nice! This was exactly what I wanted, thanks a lot! edit: Is there a way to let potential errors escape the map? I'm guessing the underlying issue is that you can't clone/move the error value, hence ignoring it and using 0 instead works. Simply cutting the string short in case of any errors seems kind of hacky (even if it works).
Aside from the problem of being incompatible with notions of "Free Software" and "Open Source", I see a difference between something like an anti-military clause and a Code of Conduct, even if they seem similar. It is unnecessary and perhaps damaging to take a stance on controversial issues that are not directly related to the functioning of the community. (I am not saying that killing people is controversial. But a reasonable person could argue that "the military" and "killing people" are not synonymous. I'm just saying that these are issues with two sides.) I think of a CoC as something that defines who "we" want to be. OTOH, an anti-military clause (or similar) tries to define who we want other people to be. Noble or not, this seems like an exercise in futility, and a distraction. The tenth tradition from 12-step programs, adapted for the Rust community, might look like this: "Rust has no opinion on outside issues; hence the Rust name ought never be drawn into public controversy." 
It's probably impossible to get that into Rust's license after all this time already, so discussing that is pointless regardless of whethet it has merit or not. For library implementors who don't want some military or the NSA or other intelligence organization to use their work for what they perceive to be evil, it would be nice to have a ready-made license that could be used. Dunno what would work, maybe something modeled after the typical bunch, MIT, Apache, (L)GPL or whatever but with no military, no mass surveillance clauses. Then it's up to library implementors to choose this license and whether they want their work to help these causes or not. Maybe you could help with the license work.
I'd use a buffered reader so that read_until is available, then it's simple.
Moreover, I hope that using Rust could give the military applications less room for error – because error can be costly up to and including loss of innocent life.
Unwrapping `c` causes a `cannot move out of borrowed content` error :/
I believe `if let` was introduced after that. If you're willing to write up an RFC and argue for it, maybe `filtered` could be brought back.
As much as I appreciate the sentiment, I am afraid that this would actually cripple Rust. I will thus offer 3 counter-arguments: 1. Inefficiency: there is little point including a clause that either cannot be enforced or which laws just waive. 2. Repellent: non-standard licenses are costly to examine, and therefore repel industrial (and their lawyers), it would be a drag to adoption 3. Cynically: as mentioned, with Rust helping driving more bugs out, we might be better off if military systems end up using it. As much as I loathe killing, I still prefer when hospitals/innocents do not suffer "accidental losses" in war areas. All in all, Rust, like any programming language, is only a tool. Whether that tool is used for honorable or nefarious purposes is hardly the fault of the tool itself.
It's mostly for those writing Node.JS/Electron apps who want to speedup some operations or implement lower-level abstractions.
Oh I see. Thank you.
I don't think r/rust is a good place to debate this, but it is not inherently obvious that we are better served by a more accurate application of military force.
Oops, yeah, brain fart on that last bit, drop flags are unrelated. I agree that if Midori wins it's because it lacks fast transfer, just not sure that if Rust also gave it up, it wouldn't be interesting to disable unwinding, which you asserted at one point. But maybe you're right that separating out 'ExcDrop' in this hypothetical world would mostly achieve the same. (One exception: binary size.) Though, for that matter, it's not really hypothetical - normal old hardware process isolation and IPC might be a bit slower than Midori SIPs, but not so much that it isn't a viable choice over threading in many situations. And you can do that in Rust today; it's just that Rust's safety guarantees make it less advantageous than in C or even Midori. The bookkeeping in question is then just what's done by the kernel. 
I was using it and nobody asked me...
If you look at wikileaks reported incidents where civilians were harmed, malfunctioning or non-functioning IT systems are often part of the problems cited.
Take a random sampling and test against those instead. 
Here's another way that I prefer: self.bytes().take_while(|&amp;c| match c { Ok(n) =&gt; n != 0, Err(_) =&gt; true, }).collect::&lt;Result&lt;Vec&lt;u8&gt;, _&gt;&gt;(); or, if you're feeling particularly fancy, let zero_or_err = |&amp;r: &amp;Result&lt;u8, _&gt;| r.ok().map_or(true, |n| n != 0); let res: Result&lt;Vec&lt;u8&gt;, _&gt; = self.bytes().take_while(zero_or_err).collect(); 
First things first: `Core` shouldn't have a lifetime parameter. [Your basis should be this.](https://play.rust-lang.org/?gist=aeef18ec7afa9392e2c9&amp;version=stable) Then note that you're asking `ViewMut` to borrow from `Core`, but be able to do so multiple times. This means you actually want *interior mutability*, so you want to be wrapping the inner `Vec` in an [`UnsafeCell`](https://doc.rust-lang.org/std/cell/struct.UnsafeCell.html), which turns off no-alias rules on a type, and making mutable borrows through that, or if possible using the prebuilt [`RefCell`](https://doc.rust-lang.org/std/cell/struct.RefCell.html) at the loss of a small amount of efficiency and a fair amount of flexibility (as you'll have to reborrow repeatedly and won't be able to borrow separate *elements* separately). [Here's a quick example with `RefCell`.](https://play.rust-lang.org/?gist=c23e86842d381a8b40c7&amp;version=stable) `UnsafeCell` will be better but will take more work... and, well, `unsafe`.
Thank you for taking the time to write this proposal. Since you're obviously coming at this from a utilitarian standpoint, I'm going to assume you've already submitted a similar proposal to more widely-used open source languages with known military users. What was the response you got from Python, for example?
Actually, I'm a little confused about how to do this with `UnsafeCell`. Given a multiply-borrowed `UnsafeCell&lt;Vec&lt;u8&gt;&gt;` where some stretch `a .. b` is "private" to this borrow, how does one get an `&amp;mut [u8]` over that stretch? Using `IndexMut` would go through a `&amp;mut Vec&lt;u8&gt;`, which if I understand correctly isn't valid. Using `Index` might work, but then we end up having to cast `&amp;[u8]` to `&amp;mut [u8]` (probably through intermediate raw pointers to avoid them existing simultaneously), which seems dubious. Using `as_ptr` will work, but it seems like a lot more effort. Hints?
Rust does have a very liberal license and I believe it's legitimate, or at least legal, to just take the existing code and add such additional restrictions on it, if you so desire. Just as you could (say) make a proprietary derivative of Rust, without consulting everyone. Unfortunately the definition of open source / free software requires the absence of such restrictions. Everything in the open source ecosystem relies on not having such restrictions, and it is part of the qualification for being adopted, used, linked-with, redistributed (eg. by debian, pkgsrc, homebrew, github itself, etc). Most people will therefore interpret the addition of such restrictions as prohibitive to their involvement in such a fork. Developers of Rust generally intend to be working on something that is actually open source. In many cases their employers, schools or other institutional affiliations require it. So they would all redirect their efforts to a repository fork that does not contain such restrictions, and carry on. The restrictions would therefore not just be ignored by the intended targets of the action (military users); they would be ignored by the developers of Rust itself. If the owners of the rust-lang/rust repository were to add such restrictions, that repository would cease to be used by most of the community. I'm sorry, but this is just how open source development works. If a license isn't usable for most of the developers, they will switch to a fork with a license that is. Licenses are not retroactive, and do not control what the whole community does. As an open source community, Rust is almost certain to always stick to an open source license. I wish this was a way to achieve the peace of mind you are after, but unfortunately it's not.
Whatever you might think of the proposal, Rust has a pretty clear Code of Conduct. &gt; We will exclude you from interaction if you insult, demean, or harass anyone. That is not welcome behaviour. /r/rust has made a name for itself for being nice, and we wish to keep it that way. Make whatever arguments you wish, but make them respectfully.
Oh, awesome! Thanks a lot. I didn't even think about interior mutability. Thinking about it, I'm confused now. This whole lifetime business just showed me that I do not actually understand it. If we have a lifetime parameter on an impl, what are we expressing with this? Isn't it kind of a "The following stuff is defined for Views with a lifetime parameter 'a"? So, what is the benefit of introducing 'b? impl&lt;'a&gt; View&lt;'a&gt; { fn new&lt;'b&gt;(c: &amp;'b Core) -&gt; View&lt;'b&gt; { ... } } What is the difference to the following? impl&lt;'a&gt; View&lt;'a&gt; { fn new(c: &amp;'a Core) -&gt; View&lt;'a&gt; { ... } }
Thank you too, of course!
You're using his own political context to justify what I feel was an entirely unwarranted "bit of hyperbole" - an understatement in its own right to begin with. Programmers in the military are not out to "hurt and kill brown people", this is ignorance at best, and a complete, wilful misrepresentation at worst.
&gt; What's there to respect? The sentiment, the discomfort, the intent. I.e. not "laugh it out of the room". The OP doesn't know how to accomplish what they're after, but we don't laugh people out of the room for being new/inexperienced, or trying to do something in a way that won't work. That's not how this community works. &gt; My alternative proposal is for OP to find a more effective means of social status signalling against the military That's fine. Making fun of or laughing at the OP for picking an ineffective means to express their discomfort is not.
People should review this account's comment history before deciding to engage!
&gt; spoken unfairly of those in the military So .. if anyone's gone over to criticizing the military as a group of people-who-do-certain-things, it's me. Though I think if the military can't handle being informed that they have been involved in killing people in the past, they might want to reconsider their view of history. But that's beside the point. The OP said _nothing at all_ about _people_ in the military. They said they were uncomfortable contributing to military _systems_ and _technologies_ that can entail killing and mass surveillance. I think there's very little to debate there, and I've known many people who -- despite working side by side with military people they liked, respected and admired -- decided that they weren't comfortable with the _technology_ they were building, and opted out of building it. This is not an unusual stance to consider. Most engineers have codes of ethics about the uses of their technologies. Google "military engineering ethics" and wade in. It's a real consideration. It's just not one we can fix through licensing, here and now.
There's one in progress! Not sure what the state is, though: https://github.com/murarth/rusti
It's not really a REPL, though. If I recall correctly, it recompiles everything whenever you type something, which ought to be too slow as soon as you start doing anything non-trivial.
You can use closures and do it like this: fn add_two_args(a: f64, b: f64, c: f64) -&gt; f64 { a + b + c } fn main() { let a = 2.0; let add_one_arg = |one, two| add_two_args(one, two, 1.0); let c = add_one_arg(a, 1.0); // c would be four }
The new ORC (on-request compilation) LLVM JIT could be useful here, but you'll probably have to wait for MIR to be used in the rust compiler before you can do on-demand translation to LLVM IR.
You could do what the other comments suggest, but I feel you might have to rework your strategy. Trying to hammer currying into Rust ain't gonna go well for you.
 let data = match data { Some(x) if x &lt; 5 =&gt; data, _ =&gt; None, }; Verified playground: http://is.gd/zyyU3O
Am I right in guessing that it's mostly a matter of doing what [Cling](https://root.cern.ch/cling) does?
Is there any reason why I should avoid currying as described above? My situation is that I have a set of optimizing algorithms (gradient descent, etc.) which require as input a function which outputs derivatives. However these functions also need varying inputs depending on the model - training data with/without labels etc. There may be another way to work around the problem (and I'll keep trying to think of it). But right now this seems like the most natural method. [For reference](https://github.com/AtheMathmo/rusty-machine/tree/master/rusty-machine/src/learning).
As I understand, yes.
By the way, the usual term for reducing the number of arguments is [partial application](https://en.wikipedia.org/wiki/Partial_application) (also, when you have full partial application what you get is [currying](https://en.wikipedia.org/wiki/Currying), which is what Haskell or OCaml does) If I understand correctly, Rust doesn't employ currying by default because of calling convention issues. Building abstractions using partial application looks unidiomatic in Rust, even though it's very common in eg. Haskell.
This is the right answer.
What is the intended effect of restricting the military's use of modern technology? Having a complete disparity in military capabilities generally means that fewer people die. From a purely utilitarian point of view, the US should retain its status as the sole superpower. I don't understand the sentiment that military power or the ability to kill people is always a negative. The military is an extension of political will and can be used justly and unjustly. [Just war theory](https://en.wikipedia.org/wiki/Just_war_theory) covers the philosophical background for the debate. The tl;dr for Just war theory: &gt; Just War theory postulates that war, while terrible, is not always the worst option. There may be responsibilities so important, atrocities that can be prevented or outcomes so undesirable they justify war. 
It's true that if you add a license clause to prohibit some "fields of endeavour" (as Debian puts it), currently a lot of the open source community will shut you out. But it's also true that many open source community members are here for moral reasons, i e, to make the world better. And it hurts them when they see software they've written, being used for things that makes the world worse. I'm not saying that Rust should immediately relicense to something else, but I'm glad that somebody has the courage to at least ask the question. And certainly it is a hard problem to figure out a definition of "not making the world worse" that fits enough people and is legally viable, but hard problems are for solving, right? :-)
Interestingly, this does work: fn main() { fn add3er(x: i32) -&gt; i32 { x + x } println!("8 + 8 = {}", add3er(8)); } I didn't know you can nest plain functions in Rust.
idk so user hits enter &gt;save state run change a panic happens &gt;A terrible fate has befallen you! panic or undo? 
still i didn't know a better way for this let add_offset_to = |std::time::Duration: b| { if neg { b - offset } else { b + offset }}; since std::time::Duration doesn't implements std::ops::Neg any more (`offset` and `neg` gets set by command line)
Could you post a complete snippet? At http://play.rust-lang.org for example (click "shorten" and copy the url)
I don't understand, isn't this: let add_offset_to = |b| { if neg { b - offset } else { b + offset }}; trye!(write!(&amp;mut outfile, "{}\n{}", i, add_offset_to(trye!(b)))) The same as this? let offset = if neg { - offset } else { offset }; trye!(write!(&amp;mut outfile, "{}\n{}", i, trye!(b) + offset))
That might be a terrible experience, but isn't a bad REPL better than no REPL, assuming no permanent bad design was added to support the bad REPL?
Oh, the issue here is that durations can't be negative. That's why `Neg` doesn't make sense for `Duration`, right? Yeah, in this case, either: let b = trye!(b); let duration = if neg { b - offset } else { b + offset }; trye!(write!(&amp;mut outfile, "{}\n{}", i, duration); Or continuing with your closure scheme would work. Personally I prefer doing away with that closure if all it does is to abstract over some `if`. It's the right abstraction in Haskell or other functional languages, but doesn't feel quite idiomatic in Rust. edit: I'm more a functional programmer than other paradigms, but when at Rome, do as the Romans do.
hm like for b in dec.by_ref() { i += 1; let b = trye!(b); trye!(write!(&amp;mut outfile, "{}\n{}", i, if neg {b - offset} else {b + offset})) } don't know what i like more
Works too, perhaps add a line breaks if that line seems too long. (now what I don't do is omitting whitespace around braces) PS: you probably should run `rustfmt` on your codebase.
can you show me how you would format the code didn't tried `rustfmt` and will not try it know (tooooo drunk) 
Loool! As said [here](https://github.com/rust-lang-nursery/rustfmt#installation), you do `cargo install rustfmt`, then (provided you have `~/.cargo/bin` on your `PATH`) do `cargo fmt` on your repository. It will overwrite all your files with pretty-printed source code. Also: you can control the settings of rustfmt by creating a file `rustfmt.toml` on your project (it dictates the style choices of the project). [Example](https://github.com/rust-lang-nursery/rustfmt/issues/262) &gt; Some people like to fit a whole function (decl and body) on one line. They are wrong, but we should support it as an option.
&gt; cargo install rustfmt oh didn't know `cargo install` have to update my rust 
never used multirust just have one install in ~/opt and the source in ~/src for racer
The weird thing is that the panic points to [this line](https://github.com/rust-lang/rust/blob/master/src/libcore/result.rs#L738) instead of the line on your own source code that called `.unwrap`. That's.. unhelpful. Anyway, do this: replace every `.unwrap()` call on your program with `.expect("custom error message")`. Example let window: PistonWindow = WindowSettings::new("test", [400, 600]) .exit_on_esc(true).build().expect("couldn't create window"); Do it for the other two instances of `unwrap`, then compile and run again. tldr: It's unclear whether `unwrap` should be used even on small snippets.
almost the same error thread '&lt;main&gt;' panicked at '2: NotFound', ../src/libcore\result.rs:757 Process didn't exit successfully: `target\debug\test.exe` (exit code: 101) 
Yes, seems like none of them. Wrote expect in all 3 cases and none came up if I understood your question correctly. 
Is there än efficient data structure for this? Sounds quite advanced to have to look up collisions in 2d space. 
I would note that closures are, in general, to capture arguments. Here, in the absence of captures (since 1.0 is hard-coded), you can use a bona fide function: fn main() { let a = 2.0; fn add_one_arg(one: f64, two: f64) -&gt; f64 { add_two_args(one, two, 1.0) } let c = add_one_arg(a, 1.0); }
Oh wow, how did I miss that. Looks like I had corrupted font files in fonts dir. After replacing them this problem is gone, but I get another error: Compiling randowser v0.1.0 (file:///C:/Users/xxx/Rust-projects/test) Running `target\release\test.exe` Process didn't exit successfully: `target\release\test.exe` (exit code: -1073741819) Segmentation fault I tried cloning and building conrod git and I get the same error, so now the problem is not in my code. But I still don't know what is wrong.
Lol, a segmentation fault. This is a bug *somewhere* and I'm 110% sure that the fault is not in your code, because your code doesn't have `unsafe { }` blocks. I suppose you can file [an issue](https://github.com/PistonDevelopers/conrod/issues) on Conrod, but the bug itself might not be on Conrod but on other dependency. I think you would need to have a debugger to properly debug this. But you can try to litter you code with `println!("Reached point A")` in some points, to pinpoint exactly what line is hitting this segmentation fault. (change the printed message so that you know that, if it prints a given message but not the next, the program is crashing between those two `println!`s). Once you isolate this line, you can search for its source code, it must be calling some `unsafe` bits that is crashing the program like this. Another thing: `piston_window` uses Glutin by default, right? You could perhaps try to change it to SDL2 or something. Not sure how to do this or even how to install SDL on Windows though.
pinging /u/mitchmindtree
I often use the repl in Ruby to sketch out non-trivial things and to connect to running systems. It's one of my favourite parts of the language.
It is not manual, if it is validated automatically by the compiler. 
p.s. join us at /r/stm32f4
I remain unconvinced that adding lifetimes onto a language post-hoc will produce nearly as good a situation as Rust has. I think it will produce much the same situation as [JS's split between continuation-passing and promises](http://journal.stuffwithstuff.com/2015/02/01/what-color-is-your-function/). In particular, the swift (and ObjC) std libs require the use of Arc for various tasks. If an API needs ARC for a temporary mutable borrow, you could in principle be changed to use lifetimes, but this won't be usable by anyone who was passing it an already shared ARC. Edit: It's also unclear to me how one could get the useful guarantees like "don't worry about guarding against iterator invalidation". Perhaps this could be provided to structs (which are copy-only right now)? Then they'd have to provide struct-iterators, which further bifurcates and complicates their story.
Actually, I'm a bit positive about this. Of course, I'd prefer it to be done the other way, opt in to pervasive ARC per-module with seamless interfacing. This can somewhat be done with a language that transpiles to modern Rust today. This is something I'd like to see happen; which is why I'm pretty positive about the Swift thing. &gt; In particular, the swift (and ObjC) std libs require the use of Arc for various tasks. If an API needs ARC for a temporary mutable borrow, you could in principle be changed to use lifetimes, but this won't be usable by anyone who was passing it an already shared ARC. This can be fixed with escape analysis or other sugar. _shrug_
So, me personally, I also built a separate target file for my OS; and then just copied the libcore for it into the place where my regular Rust libraries are.
Yes, hence "the new", ie, the thing that takes the position of manual memory management.
No, it can't. The old API permitted you to legitimately share and mutate. The new API forbids it. It is a breaking change.
And if it isn't some problem with freetype, it probably isn't with glutin either, since window creation is (apparently) okay.
structs in swift are already value types, so they can add borrows to those without breaking anything, *I think*. It's just that no interface expects to share structs around, so entire new ones will have to be added, and they can't interact with ARC interfaces, because those all use classes (so... continuations and promises).
(Valuetype was the wrong term to use, I guess `noshare`? Basically, "don't put this in an Rc") Yeah, things get murky. But I do feel it's still doable without much exposed ickyness.
There's also that afaict they are willing to have fairly significant breakage between major versions still. I imagine they could fix many of the apis with relatively little breakage. 
&gt; Yeah, things get murky. But I do feel it's still doable without much exposed ickyness. I feel like it'll bifurcate all packages into ARC ones and manually-managed ones. I don't know how to go from aliased/mutable ARC objects to unique references without dynamic checks and failure.
I agree, I feel it might end up a bit like D's GC vs no-GC issue. 
I recently did a bunch of UnsafeMutablePointer stuff in Swift. It basically turns the language back into C. I'm still surprised at the performance overhead that ARC introduces.
`@T` was basically `Rc&lt;T&gt;` with cycle collection (on task death), while `@mut T` also incorporated interior mutability. IIRC, `@mut T` was basically `Rc&lt;Cell&lt;T&gt;&gt;` iff `T: Copy` and `Rc&lt;RefCell&lt;T&gt;&gt;` otherwise - but in the latter case, the equivalent operations to the `.borrow()` and `.borrow_mut()` calls were automatic, hence the nightmare /u/pcwalton mentions. To give an example, if `a: @mut Vec&lt;T&gt;` then `let b = a; a.push({ b.push(x); y });` would panic just before the `b.push()` (allowing that code to run would be a violation of aliasing^mutation rules in Rust). In modern Rust, it would be `let b = a.clone(); a.borrow_mut().push({ b.borrow_mut().push(x); y });` which at least lets you grep for `borrow_mut` and insert debugging information. Also, because `RefCell` is a library type, it's possible to make a variant of it (IIRC someone has done this already) which stores a backtrace of the source of the last mutable borrow or so, making `RefCell` panics so much easier to track down.
To help the splitting issue it seems like they could make it legal to pass ARCs to functions expecting borrowed references, perhaps by monomorphizing behind the scenes (or even explicitly being generic over the reference type). Then they could change a lot of the library functions to expect references without breaking ARC-based code. I'm not familiar with Swift or the guts of borrow checkers, so this might be nonsense.
Pry or irb? 
I'd like my build system to automatically build and link against the patched libcore. Think of it as me trying to push Cargo's limits. :P Sucks that it's probably not feasible. I wish there was a way to specify lib directories in the `.json` target file.
Damn, that's probably it.
Agreed. I've been wondering about how that is supposed to work in these systems, and my best guess is that they simply don't allow references into shared state, so you have to take references into structs (and perhaps the equivalent of boxes), and not classes. Functions must then perhaps have to be generic over both references and ARC classes, such that safety is maintained for shared state. 
C++ supporting it would be a "I told you so" moment..
I'm a bit unclear, are you saying that you had to use unsafe pointers in Swift to overcome ARC slowness, or are you implying that ARC still has overhead somehow even when using only unsafe pointers?
I believe pcwalton has been betting for a few years now on a standardized C++ borrow checker by C++20. :P
The original mention had "C++21" in it.
Is C++21 the next revision after C++17? I thought it was just every three years at this point.
Wouldn't a similar, maybe even more prevalent problem be borrowing a thing by accident and then being unable to use that thing while you're in that lifetime?
By that way of thinking RC and GC are also manual, because developers need to manually decide when to use weak and phantom references. 
This is a subreddit about the programming language Rust, you might be looking for the sub about the game: /r/playrust 
Working example here: http://is.gd/BeNSnA. First of all, you can get the `K` type from HashMap by specifying K and V when implementing the trait: impl&lt;K, V&gt; ComponentContainer&lt;K&gt; for HashMap&lt;K, V&gt; { fn all_keys(&amp;self) -&gt; Vec&lt;K&gt; { self.keys().cloned().collect() } } You can see that `HashMap`'s `.keys` [returns type Keys&lt;K, V&gt;](https://doc.rust-lang.org/std/collections/struct.HashMap.html#method.keys). It is an iterator over the keys. The additional _bounds_ `: Clone + Eq + hash::Hash` were imposed by the things we are doing with the `K` in this method: the `HashMap`'s `keys` requires that `K: Eq + hash::Hash` (clearly described in compiler's error message) and because we later clone the key value, it also needs to be `Clone`. Now, however, the `ComponentContainer` has to be generic over the `K` type. If we don't want to include the `V`, it has to return something else than `Keys&lt;K, V&gt;`. I have chosen to return `Vec`. However, to create `Vec&lt;K&gt;` we need to clone the returned key list. In example, I have chosen to also clone the key contents with handy `.cloned` method. There can be various other ways to do the same, I have chosen this example as the most straightforward and with least surprises when actually used.
Thanks for your quick response and your detailed answer! While this works we have an additional heap allocation because of the conversion from *std::collections::hash_map::Keys* to *Vec* - Is it possible to avoid that?
Thanks for your response. This works only for *keys()* implementations which return *std::collections::hash_map::Keys* - This is not the case for other collections.
You might want to take a look at [eclectic](https://crates.io/crates/eclectic/) which defines a set of set and map traits.
How does this work for you? https://play.rust-lang.org/?gist=490f93268da890269d64&amp;version=stable
This seems to solve the iterator issue
This looks good! I have tried something like that but ran into lifetime issues. Only downside I can see is the additional lifetime parameter which is necessarry for all functions which want to accept those container traits. Thank you very much! Great help!
Here's a version of this with generic Keys: http://is.gd/35iqRs
You can use an associated type on the trait which will be the iterator of keys. Unfortunately this can get extremely wordy, which is why abstract return types are often desired. https://play.rust-lang.org/?gist=9396cef2391f5783206f&amp;version=stable Someday, hopefully, these kinds of collection traits will be a part of the standard library.
I have trouble thinking of how that would work in a non-VM context. Recompiling an old AST/source file and re-running everything from scratch would not make for a true REPL.
It's a shame that BitC never really went anywhere. Shap seemed to often fall victim to analysis paralysis, which is even worse when the problems you're dealing with possibly have no ideal solution. The linked post by Shap makes a bit more sense when viewed in light of the history of regions in the typed PL community. Work on regions began with the Tofte-Talpin region system for ML, which was largely motivated by finding an alternative to garbage collection. Regions were associated with the scope of a single expression, and were LIFO-nested according to expression nesting; allocations were associated with regions. They developed a whole-program (or perhaps more accurately, whole-module) inference algorithm that would infer region annotations for functions and expressions. They did have region polymorphism (which is tricky from an inference point of view), but they did not have subtyping, instead opting for a more traditional equality-based ML approach. Because all allocation occurred via regions, some regions had potentially unbounded size. Cyclone made a few big changes. It added subtyping, required annotations on functions, and had to refine the notion of region to match a block-based imperative language. There were region names and variables, and pointer types could be parameterized by the region of the pointee. Regions still had runtime significance; there were bounded stack-based and unbounded lexically scoped regions that were deallocated when leaving a scope. Rust uses a similar model to Cyclone, but tweaked it to match the modern C++ programming style. Regions no longer have runtime significance; the automatic deallocation of regions in Cyclone is replaced by destructors. Unbounded regions are replaced by containers or arenas. The main drawback here is that by losing support for unbounded regions as a first-class concept, the use of arenas is less ergonomic compared to Cyclone and most Rust code does not use them.
But you would still need to cross compile libcore manually if you're using a custom target file... What I really want is a rustc that just compiles libcore on the fly when it's needed. 
Hey, I'm the author of [rusti](https://github.com/murarth/rusti), the sort-of-REPL which is the best we have right now. What's necessary to implement a full-featured REPL is some more cooperation from the `rustc` crate. Access to information about definitions and types would eliminate the need to redefine `static` and function items as it currently does. Some API for code modification will probably be necessary to do things like prevent local variable bindings from being dropped prematurely. There are a lot of unanswered questions for weird, Rust-specific situations (like the one mentioned here about panicking), but these are the big hurdles. As for developments, the MIR/HIR progress (being written by people who are much more awesome than I am) looks very interesting. I'm not sure exactly what all the plans/goals are with that, but it looks like it may be useful for outside code wanting to meddle in rustc's affairs.
The problem with Qt is indeed that it's C++. If you're writing C that's not too much of a problem, just use the ++ parts that you need, you've already got a compiler for it and you can otherwise just continue to use C (C is almost, but not really, a subset of C++). However, writing bindings to C++ libraries for any other language but C and C++ is next to impossible. If it's done, then by writing pure C bindings. (C being *the* lingua franca of CS) That said, proper EFL bindings would be really nice, and the library is pure C. Also, retained mode, in case you don't like conrod for being immediate. EFL is actually retained down to the canvas level, it's pretty much a scene graph like approach.
In this case the last statement is `Ok(())`. The if statement is completely separate. If you didn't want to write `return` at all, you could write something like this: if file_len &lt; 1 { Err(io::Error::last_os_error()) } else { Ok(()) } That makes the if/else statement the final statement, therefore you can omit the return keyword.
I don't understand why floating point support is in libcore in the first place?
I think we'll need more specifics to be able to help, e.g. the definition of the `Matrix` struct or even where you're struggling with the code you've written if you've got that far.
You could look into [Numerical Recipes in C](http://www.it.uom.gr/teaching/linearalgebra/NumericalRecipiesInC/), the chapters on Gauss-Jordan elimination or LU decomposition (you can use either to compute the inverse) Nalgebra implements matrix inverse too, so you can look at their implementation [(DMat docs, look at trait Inv)](http://nalgebra.org/doc/nalgebra/struct.DMat.html)
Because the only time it's problematic is if you're writing a kernel, most targets are fine with it. We are the trailblazers who are running into the problem, how most OSS gets improved :)
You can simply replace cargo-generated `.git` subdirectory with the one you pulled and add/commit the files.
I have a [working example](https://github.com/AtheMathmo/rusty-machine/blob/master/rusty-machine/src/linalg/matrix.rs#L701) using LUP decomposition. It doesn't do very good safety checks but that sounds like it's fine for your purposes! Alternatively you could take a look at this [rosetta code page](http://rosettacode.org/wiki/LU_decomposition). I based my implementation on it. Good luck!
Do you think disaggregating float from libcore would be a reasonable request to make from the Rust developer community? I still feel a bit green for writing RFCs and making significant structural changes to the core API, but maybe with a bit of community support and/or mentorship it could work out.
I suspect you'll find that the lifetime goes away as a parameter, but you have to write horrible things like where for&lt;'a&gt; T: ComponentContainer&lt;'a&gt; because the actual lifetime `'a` you want to use isn't a lifetime that can be named when the function is invoked. Edit: Also, when I tried to do this recently, I ran into all sorts of [ICEs](https://github.com/rust-lang/rust/issues/29997). It seems like, at the moment at least, Rust has a hard time with exotic associated types (maybe combined with closures?). It's fun to try out, but I got to the point of head-desking pretty quickly.
I was thinking of this: http://doc.crates.io/build-script.html#overriding-build-scripts. But I haven't tried it yet.
Damn, this actually sounds feasible. You're saying I should put my compiled `libcore.*` as a static dependency of `rlibc`, and then override the `rlibc` build script. I'll try it tomorrow. I don't see this strategy scaling to fancy dependencies that have their own build scripts, but then that's not a problem I'm likely to hit anytime soon as an OS developer.
If you plan on using this in any production system I would suggest you find or write a wrapper around blas and lapack. Matrix algebra gets extremely complicated on floating point hardware and there are a lot of things that need to accounted for. Also chances are you don't actually need to invert the matrix. If you are trying to find Ax = B there are much better ways to find x than inverting A.
Writing a simple sed/awk clone as my first nin-trivial Rust program. Using the nom library of parser combinators to turn expressions into AST. Having a bit of borrow checker midlife crisis since getting stuff to compile seems to become harder and harder...
Note that (if I recall correctly) LU decomposition is more efficient and has less numerical problems. It's probably the best way to invert a dense matrix.
Maybe it's different in Haskell, but in Python I almost always regret using a REPL instead of writing a script, which is easier to modify and run again. I like using the REPL for debugging though.
libcore doesn't have a .toml file, and even then I don't think it would be understood as a dependency of rlibc :(
This sounds like a lot of work for something that will (hopefully) be patched eventually!
[Here, have a crate with it.](https://github.com/notriddle/rust-optionkit)
Still, I think your question should be "how do I patch libcore" rather than "how do I use arbitrary `.rlib` files".
From the compiler errors (reference must be valid for the block suffix following statement 0 at 4:65 but borrowed value is only valid for the block suffix following statement 1 at 6:25) I think this is because: The HashMap's memory extends from statement 0 to the end of the block, so all functions should live at least this long. `fun` doesn't live that long, because it only starts living at statement 1. I think this *should* work in practice (especially since reordering makes it compile), so maybe it's worth opening an issue? Maybe someone else can clear this out better than me, though. There is probably a reason for this to fail.
I believe it's because "fun" technically goes out of scope before "fns" because of the way the rust compiler works. First in scope is last out of scope so in this case, fun goes out of scope just before fns does.
Yes, it will work because the order of dropping resources(clearing memory) is last declared dropped first. In this case, when `main` function ends, `fns` is dropped first before `fun`. So `fun` does still exist longer than `fns`. use std::collections::HashMap; fn main() { let fun = |v: i32| v; let mut fns: HashMap&lt;char, &amp;Fn(i32) -&gt; i32&gt; = HashMap::new(); fns.insert('F', &amp;fun); }
Yes, the compiler is not smart enough - there is a [long standing issue](https://github.com/rust-lang/rust/issues/6393) which I assume covers this case too. Meanwhile, in addition to the other comments, here's how you can specify that both `fun` and `fns` have the same lifetime. Notice how the first row declares both `fun` and `fns` (the funny type signature specifies the type for `fns` while letting the `fun` type be inferred): let (fun, mut fns): (_, HashMap&lt;char, &amp;Fn(i32) -&gt; i32&gt;); fns = HashMap::new(); fun = |v: i32| v; fns.insert('F', &amp;fun); 
Ahhhh that makes sense. I did not even notice that. Welp.
It's arguable that re-ordering to make it work is too much magic. Yes, it might be a little less easy at first, but knowing that things get dropped in order always is nicer, imho.
Thanks for the links, learned a few things from there. I have seen that LU decomposition is more efficient so I will probably end up using it.
Your code looks nice, but maybe you can find a way to take out the unsafe blocks. I took a look at the Rosetta Code page and it gives a good explanation of the problem, but I will look into using a library instead.
If re-ordering is too much magic, then I think a better compiler warning would solve the problem, as it would suggest re-ordering with consideration of drops. I haven't done any compiler hacking, but would it be possible to suggest re-ordering drops for dependent lifetimes? And possibly pointing out which variables are the ones responsible for the lifetime.
I would agree that a better diagnostic here would be awesome. I have no idea how hard it would be to add.
&gt; So, Rust drops things in reverse order to the order they were declared. Except, strangely, for [struct fields and array elements](https://github.com/rust-lang/rfcs/issues/744).
That doesn't mean you have to write RFCs. There are issues open with "wouldn't it be cool if we had a library that did X?" That should give you ideas.
Perhaps open an issue with your example so that someone more involved can see if that's doable to add as a suggestion.
Another way to think of it (in addition to the other comments), is that an when an `if` expression does not have an `else` case, the only possible type that can be ascribed to that expression is `()`. Think about it: let result: Result&lt;_, _&gt; = if file_len &lt; 1 { Err(io::Error::last_os_error()) }; If we are expecting from the first branch to see a `Result`, what value can we assign to `result` if the predicate returns false? If you do: let result: () = if file_len &lt; 1 { return Err(io::Error::last_os_error()); }; That's fine, because `return` or `(;)` both have a type of `()`. The other way to handle it, as others have mentioned, is to have all branches return a value.
~~I think the biggest thing for me would be being able to use sum type constructors as parameters to macros.~~
Think it'll happen anytime soon? Cargo seems mature.
What do you mean by that? The constructor for an enum variant behaves like a normal function (e.g. `Some as fn(i32) -&gt; Option&lt;i32&gt;` compiles), so AFAICT what you want is already possible.
Thankyou! a couple google searches lead to this https://developer.mozilla.org/en-US/docs/Mozilla/Projects/SpiderMonkey/JSAPI_User_Guide
Incremental build system :)
Why do generics need explicitly defined traits anyway? if i have fn halfofsize&lt;T&gt;() -&gt; usize { mem::size_of::&lt;T&gt;() / 2 } Why do I need to explicitly point out that T is sized? Shouldn't rust be able to infer that? I'd use generics a lot more if I could write them like macros. Just erroring if you don't use them that right. I never save any time using them, having to look up and fuss with traits.
Rust doesn't infer the type of item signatures at all, it would seem unusual if it inferred the bounds on type parameters.
To clarify, I was talking about [integer template parameters](https://internals.rust-lang.org/t/pre-rfc-integer-templating/2974), not [traits for integer types](http://rust-num.github.io/num/num/traits/trait.PrimInt.html). Mostly I want code that can be generic over `[T; N]` for all `N`.
Higher kinded types and type-checked `do` notation without too many macros.
A language spec.
Also note that since we pass `gc-sections` to llvm, as long as you don't use floats, they aren't compiled in and everything works. I've got rust [running on the GBA][1] with no issue, and that has no float support. [1]: http://csclub.uwaterloo.ca/~tbelaire/blog/posts/gba-rust-1.html
It’s a design decision that changing the *body* of a function should not be able to cause downstream code to stop compiling. Instead, the *signature* of the function should contain everything necessary for typechecking, so that it’s clear whether a given change affects the function’s type.
It would be extremely convenient for the compiler auto-generate the line and needed imports, e.g. Operation(s) for T can not be applied due to missing traits Suggestion: fn double&lt;T: Add + Sub&gt;(a: T) -&gt; T { a + a - a } std::Ops:Add would need to be imported // If Sub was already in scope error: binary operation `+` cannot be applied to type `T` note: an implementation of `std::ops::Add` might be missing for `T` error: binary operation `-` cannot be applied to type `T` note: an implementation of `std::ops::Sub` might be missing for `T` This doesn't seem that helpful, but when you are using many traits, It would be extremely convenient.
I'll argue it. I don't know many traits, but I know most of the standard library. Trait bounds don't even show where they are used in the function, so they just add noise to me. Burden of proof, could you post an example where the code would be confusing without the bound? If the problem is the body breaking downstream code, a ```#feature``` could be required for implicit traits in Libraries , or using ```T: _``` There would be no problem with executables, as they are compiled together.
The fact that you have to make an explicit change to the function signature makes it easy to identify potential breakages, and provides other valuable documentation directly in the source of your program.
But semicolons play an important role in determining what an expression evaluates to, which is not true of languages in which semicolons are optional.
So why not be implicit in executables, and in Libraries, require a ```#feature``` or have to define the type ```&lt;T: _ &gt;```
Rust's typechecking algorithm makes use of the explicit typing of function signatures when inferring the types of bodies. The way this has made it easier to implement features in the type system is considered a greater advantage than the advantage of bounds inference, which to be honest I see little use for (I feel like you would get 9/10ths of what you want if the `num` traits were standardized and you didn't have to specify every single arithmetic trait to be generic over ints).
There is a [postponed RFC](https://github.com/rust-lang/rfcs/pull/99) that would extend patterns to allow this syntax, along with some other extensions.
You basically have two options: either ignore non-unit expressions when you're expecting a unit expression or require an explicit `()`. Swift does the former; F# (with its indentation syntax) does the latter.
[This example file](https://github.com/servo/rust-mozjs/blob/master/examples/callback.rs) might help as a starting point. Like many crates built for Servo, mozjs hasn’t had much work put into documentation or other things to make it usable beyond Servo’s immediate needs. (Sorry!) But we’re very open to suggestions or PRs for remedying this.
The latter is a breaking change and seems like a more frustrating syntax, the former seems like it could let errors slip through or be caught in a way that makes debugging them more frustrating.
Working MSVC linking.
semicolons are optional in Scala, and play a role in determining what an expression evaluates to
Nobody mentioned HKT yet? Also I still want a stable ABI that exposes at least _some_ safe types beyond what's currently FFI-safe, but that's pretty involved.
Woah, this is a big RFC. I can only wonder if it will be ever reopened.
Really sorry for the inconvenience! Just wanted to let you know, we've fixed it. :) P. S. I work on product at DoSelect.
I don't know. Stack based programming in FORTH is pretty easy.
I think that `?` would solve most of my problems.
At some point in the future it would be great to have IDE-support for * Semiautomatic ampersand insertion * Semiautomatic asterisk insertion * Semiautomatic `ref`insertion * Semiautomatic `mut`insertion * Semiautomatic `pub`insertion * Semiautiomatic parenthesis insertion (guided by indentiation) But I fear that some of these are hard problems.
Say there are two traits that implement a `foo` method that returns a `bar`. What does this mean? fn&lt;T&gt; (t: T): bar { t.bar() } (This is actually different from non-local analysis, it's actually an ambiguous case) Non-local means that the compiler has to know every trait in scope and figure out which trait you actually want. This means knowing about things outside of the function being compiled other than what the function explicitly mentions. Somebody else probably has a better description.
If changes to the body can change the signature, then you have to typecheck the function body, and in the process you have to recursively typecheck every function that is called from the function body. In the absence of the ability for the function signature to change based on the body, then a caller doesn't even need to consider whether or not any of the functions that it calls has changed as long as their signatures are identical, and checking for matching signatures is a trivial and non-recursive operation. The function's code can be generated independently and in parallel and then cached with much greater potential for reusability in future compilations.
Your writing is fine but you'll want to focus your question to make it more approachable if you want a good answer. Try to distill your problem down and detach it from the rest of your project. Make a generic example that could apply to anything. In the process, you might even see a solution yourself.
Any type system contains an informally-specified, bug-ridden implementation of half of dependent typing. For a language like Rust you would mostly want them behind library curtains but OTOH things are suddenly so much easier when you're not trying to fight the inevitable. ...just nurse Idris a bit and then, once it reaches slaughter weight, gut it. One thing necessary for practicability for the broader audience that's still lacking, or at least underdeveloped, certainly is domain-specific type inference.
People starting to put the commas at the beginning of the line. struct Point { x: i32 , y: i32 } There's a reason the Haskell community, by large, switched to that style: Because stuff is aligned it's trivial to see whether something is amiss.
&gt; but the BCPL rule for optional semicolons revived by Go is really quite nice What about Haskell's rule for optional semicolons?
&gt; MIR and incremental compilation. Will it reduce rustc recompilation time? I've heard contradictory statements.
The drop order is well specified, since code has to actually depend on the side effects of running destructors. If anything, borrow scopes are actually more open to potential reorderings, since ending a borrow does not cause any side effects.
HKT!
First step: write a rustfmt option for doing this (if there isn't any) Second step: push projects to adopt this convention (by including an appropriate `rustfmt.toml` in their Cargo project, in such a way that running `cargo fmt` will pretty-print the whole project to use this convention) Third step: push rustfmt to make this a default (you may need to bribe some key developers) Fourth step: total world domination