Yes, unfortunately you cannot do this as there is a blanket `impl&lt;T&gt; From&lt;T&gt; for T` in core and if `T::Error` is `SuperError&lt;T&gt;` then they will conflict.
You have to register with your email address to download free copy. That email can be used to spam you later with future "offers".
I seem to recall someone mentioning traits at some point. There seems to be a simpler approach. Would it be possible to simply define the following? pub struct v128 { ... }; // opaque vector of 128 bits pub struct v128i { ... }; // opaque vector of 128 bits containing signed ints pub struct v128u { ... }; // opaque vector of 128 bits containing unsigned ints pub struct v128d { ... }; // opaque vector of 128 bits containing floating points // Impl bitcast conversion for v128 &lt;-&gt; * and v128i &lt;-&gt; v128u ... // Impl relevant bitcast conversions from typed vector types into untyped ones impl From&lt;i8x16&gt; for v128i { ... } // bitcasts impl From&lt;v128i&gt; for i8x16 { ... } // bitcasts // Impl intrinsic *once* constraining by the least specific vector type, // but ensuring proper match of arguments and result. pub fn _mm_slli_si128&lt;T: From&lt;v128i&gt; + Into&lt;v128i&gt;&gt;(a: T, b: T) -&gt; T { _mm_slli_si128_internal(a.into(), b.into()).into() } fn _mm_slli_si128_internal(a: v128i, b: v128i) -&gt; v128i; I've (tried to) read all 246 posts (this thread is insane oO) and haven't seen this seemingly simple proposal yet; I'm unclear whether it's because it's a particularly bad idea or it's because nobody really tried to cater to these poor Intel intrinsics. 
Such a function could be implemented in a generic manner (perhaps by a `__mm_128` type that has `From` and `Into` impls for all source/target types), right? Or would inference failure make this useless?
Sounds pretty cool. 😎 Puns aside, I wanted to get back into sound programming (e.g. writing a guitar tuner), and this could be a viable building block (though FFT is usually too slow for that). Once I've finished my next blog post and an RFC I've just started to write...
A bit of context. I'm a self taught web developer, working mostly on ruby (on rails) projects and this was a refreshing change. I was annoyed that in the browser I could easily filter between images or stylesheets with firebug, but when looking over the rails logs with tail + grep it felt painful.
Ah yes, indeed only binary should commit the `Cargo.lock` file. As for "sane" vs "reasonable", as a non-native speaker, this flies way over my head :p
And then someone is user land can make VectorMultiply and whatever else using the intrinsics, publish it on crates.io, and done. We can then have competing crates and experience help drive a better higher level (official or not) crate. 
Can we find the exact numbers so we know if and how much Rust regressed?
&gt; I've shown that numbers on the website are misleading No you have not. The numbers on the website are what they claim to be. &gt; performance that users will get across different platforms and devices http://benchmarksgame.alioth.debian.org/dont-jump-to-conclusions.html#not-prophesy &gt; at least relevant and valid for their setup I'm all for people [making measurements](http://benchmarksgame.alioth.debian.org/play.html#languagex) on the setup [**their programs** will use](http://benchmarksgame.alioth.debian.org/dont-jump-to-conclusions.html#ultimate-benchmark). I'm all for people [making **better measurements**](http://benchmarksgame.alioth.debian.org/for-programming-language-researchers.html). 
Considering that the backdoor would require to spend time on parsing the AST, would it be possible to notice a systematic delay in compile times between unrelated compiled code bases?
Too tiny IMO. That parse step is quick. If anything it's the folding that will get to you, since that walks the entire tree. But I don't think that it would be that noticeable, even on a large codebase -- the compiler walks the entire tree many, many times; what's one more?
I think you should test the ones that are twice as slow, you tested one that is almost the same as C++ and got the same results like in the link provided.
OK i found it In january 2016 Rust k-nucleotide was 9.89s, now it's 22 seconds. regex-dna was 2.05 now it's 2.34. n-body was 14.60 s now it's 22.92. As i told you it's clearly a huge regression. Edit: spectral-norm was 3.48 now it's 4.00 
There are too many changes that affect the compiler's performance in more drastic ways. This attack could've been hidden in with the Type Obligation Forrest, and we'd write it off as a part of that change.
I see - what about scrambling the namespace signatures beforehand? Assuming the backdoor would try to hook into the codebase by the specific module names.
Cool stuff – I believe that's the first quine I've seen written in Rust.
Thanks. We'll need to look closer to know, but with ongoing development, these things can happen. For example, llvm version upgrades often cause both some programs to optimize better and some to do worse.
Hmm, my travis build, linked in the post, seems to have pulled down a newer one. Maybe there's some caching gone awry?
Ouch!
For small library additions like this, you send in a PR to https://github.com/rust-lang/rust . I'm on mobile, but should be in src/libcollections/vec.rs, if memory serves.
Yep -- I mention DDC in the post. Rust doesn't have a second compiler at the moment (and doesn't have deterministic builds) so it can't be used to protect against this, yet. 
This syntax works if the size of the slices is constant. It will fail if you try to use a non-const variable to declare the size: https://is.gd/mQ713F
It was mentioned, but not linked, in the initial version. But someone asked me to link it pretty much immediately after publishing and so I did.
huh. that's interesting.
I'd suggest not implementing your own byte-by-byte reading and parsing. It looks like it's doing a lot of extra work for every byte handling a format string and using asserts that aren't really necessary. While I didn't profile, I expect that's the bulk of the slowdown. Use a `std::io::BufReader`, call `lines()` on that, then parse from each line with a combination of `split_whitespace()` and `parse()`. You can also pre-size the vector holding the heights to reduce reallocations/copies, which will mostly only matter on larger inputs. Another small win would be to return early in the event that `k == 1`, which saves time by not parsing all the numbers from the next line. I wrote a quick benchmark for your code and an implementation with my suggestions and saw a 5x speedup for the tiny sample data on the site. As a side note there's no need to write your own `min` function. You can use `std::cmp::min` for two values or `std::iter::Iterator::min` to find the smallest element in an iterator.
See: https://www.reddit.com/r/rust/comments/5g0op1/getting_explicit_simd_on_stable_rust_large/dapwbh8/
Right... except... I lied a little in my previous comment. It would actually be *really* hard to do this if all we did was expose the intrinsics. That's why the current consensus in that thread (if one exists) is to stabilize the most basic cross platform API possible. So this would give you, for example, `Mul` and `Add` impls on the various vector types that we define. To state this more clearly: if all we did was expose the intrinsics and nothing else, the current `simd` crate probably couldn't feasibly work on stable Rust. Here's a bit of explanation with more details: https://internals.rust-lang.org/t/getting-explicit-simd-on-stable-rust/4380/209
Thanks for the pointer. Indeed that is better than type casting. However, I would like to avoid touching `main` and `greeting` at all if possible. 
I don't know the actual technical reason, but closures don't get quite the same inference as other types. The issues is that, unlike everything else, the compiler has to work out *which* of the callable traits the closure should implement, and how to capture the environment. The practical upshot is that, yes, the compiler is *not* all that good at inferring the type of closures once you start putting layers of indirection between the closure and where it's used. The way I usually get around this is to use a hinting function that gives the compiler the information it wants. Something like: fn hint_greeting_fn&lt;F: Fn(&amp;'static str) -&gt; String&gt;(f: F) -&gt; F { f } You should then be able to use use `hint_greeting_fn(|msg| msg.to_string())` instead of the raw closure. Note that this doesn't change the closure in any way, or do any allocations, it *just* gives the compiler the information it needs in a way it can actually use it.
Will do, thanks!
&gt; None =&gt; panic!("Item not found."), //is this how im supposed to do error handling? In general, it's a bad idea to panic in a library, since it's difficult for users of that library to recover from that. Better to have your function return a `bool` or `Result` that lets the user check whether or not it succeeded.
Looking at a "side channel" (increased run time for adding backdoor, increased code side for hiding the backdoor...) can give you suspicions that something is amiss, but I fail to see how it proves the code is doing malicious. Perhaps the code is oddly slower in some specific circumstance due to something else. After finding the micro-slowdown, perhaps someone can bring forward a patch saying "Oh my LLVM optimizing pass was buggy and messed with your AST code, here's a fix. Apologies" The more paranoid among us may point out that just because someone could cover up that finding with a "bugfix" it doesn't mean a real backdoor wasn't found. Perhaps the buggy optimizing patch was "buggy" in the sense it highlighted the operation of the backdoor, making it more easily identifiable -- and the "bug" was "fixed" by the same person that inserted the backdoor in the first place. After all, we are talking about how dangerous is trusting trust itself...
Since, according to [/u/Quxxy](https://www.reddit.com/u/Quxxy), what I want to do isn't currently possible, I will settle for doing it this way instead.
that code is my second attempts. those macro i got from [oli-obk/rust-si](https://github.com/oli-obk/rust-si). my first attempt is this, (now using your suggestion, `std::cmp::min`): use std::io; use std::cmp; fn main() { let tc: i32 = read_line() .trim() .parse::&lt;i32&gt;() .unwrap(); for i in 0..tc { let mut inp: Vec&lt;i32&gt; = read_line() .trim() .split(" ") .map(|x| x.parse::&lt;i32&gt;().unwrap()) .collect::&lt;Vec&lt;i32&gt;&gt;(); inp[1] -= 1; let mut seq: Vec&lt;i32&gt; = read_line() .trim() .split(" ") .map(|x| x.parse::&lt;i32&gt;().unwrap()) .collect::&lt;Vec&lt;i32&gt;&gt;(); seq.sort(); let mut ans: i32 = std::i32::MAX; for j in inp[1]..inp[0] { ans = cmp::min(ans, (seq[j as usize] - seq[(j - inp[1]) as usize])); } println!("{}", ans); } } fn read_line() -&gt; String { let mut buffer: String = String::new(); io::stdin().read_line(&amp;mut buffer); return buffer; } still, i got the same result, TLE. that's why, i thought the iterator kinda slow?
Then it depends on how hard it is to do successfully.
I guess it's a balance, from cheap easily-discovered backdoors to the expensive and undetectable. Calibrate your paranoia according to how much you think those incentives can afford to create.
I have a struct like so. struct RfidReader&lt;'a&gt; { context: libusb::Context, handle: libusb::DeviceHandle&lt;'a&gt;, } The `DeviceHandle` holds a reference to `context`, thus the lifetime. I thought putting the two in the same struct would make it work, but the compiler says that `context` doesn't live long enough. Is there a way to make this work?
&gt; n-body was https://users.rust-lang.org/t/why-is-rust-n-body-benchmark-slower-than-c/5951/7
&gt; i thought the iterator kinda slow? Doubtful. I just benchmarked that code and it's a little over twice as fast as your previous implementation. It's still about twice as slow as my implementation, and mine uses iterators even more than yours. I won't quite have the time to profile to find out where yours is slow for a couple hours. But our implementations are similar enough that I'm surprised at the difference. I'll try to look more into later, but the main differences I see are that I'm using `lines()` and `split_whitespace`, not using `trim`, and I'm making sure my vectors get allocated up front. I also don't have great input data for my benchmark, just the short sample on the site. Do you have bigger inputs you can share so I can run more accurate tests?
thanks for investigating it! i don't have input data. i just submit it to spoj. the reason i'm using trim is to remove newline, maybe i can use [pop](https://doc.rust-lang.org/std/string/struct.String.html#method.pop) instead.
You'd probably have to define a new error type. Another approach is to specify the function to do nothing if it isn't in the vector. Note that a serious problem in your current implementation is that it takes a `T` instead of an `&amp;T`. `&amp;T` would be better because it wouldn't require the caller to give up ownership. Note that if you're trying to get this accepted into `std`, you'll want to think very carefully about the motivation for adding this method and, more importantly, what its *use cases* are. (That it exists in Python won't be enough!)
finally i got AC. this is the code: use std::io; use std::cmp; fn main() { let tc: i32 = read_line() .trim() .parse::&lt;i32&gt;() .unwrap(); for i in 0..tc { let mut inp: Vec&lt;i32&gt; = read_line() .split_whitespace() .map(|x| x.parse::&lt;i32&gt;().unwrap()) .collect::&lt;Vec&lt;i32&gt;&gt;(); inp[1] -= 1; let mut seq: Vec&lt;i32&gt; = read_line() .split_whitespace() .map(|x| x.parse::&lt;i32&gt;().unwrap()) .collect::&lt;Vec&lt;i32&gt;&gt;(); seq.sort(); let mut ans: i32 = std::i32::MAX; for j in inp[1]..inp[0] { ans = cmp::min(ans, (seq[j as usize] - seq[(j - inp[1]) as usize])); } println!("{}", ans); } } fn read_line() -&gt; String { let mut buffer: String = String::new(); io::stdin().read_line(&amp;mut buffer); return buffer; } i change the trim().split(" ") to split_whitespace(). in the same algo, my c++ 4.3.2 implementation got 0.07 from spoj. with rust i got 0.38. :(
America and England and a few other English-speaking countries have issues over the stigma of mental health. There are some efforts trying to avoid words like sane = reasonable (since someone who is insane may be perfectly reasonable in many contexts). just like how psychopathy != killer. The stigmatization of the condition has traditionally hindered viable treatments.
That'll put them off for about as long as it takes to develop a match-by-structure approach..
In Rust, struct fields cannot be marked `mut`. Instead, they always inherit the mutability of the containing struct. Does that help?
I'm using ncurses-rs (the last two lines of the toml file), or do you mean why am I not using something like cursive? I did write my own wrapper for libreadline because the existing implementations I found didn't expose the entire api. The reason I went ncurses + libreadline rather than something else is because I wanted it to feel like the *nix programs I'm accustomed to like htop. libreadline in particular, allows the use of emacs keys, of which I'm quite fond of.
Neat! ARM Rust (once it's stable) + [Rust SysFS GPIO](https://github.com/rust-embedded/rust-sysfs-gpio) seems like a great way of programming the Pi.
On the other hand, `mut self` allows you to express builder as a state machine.
In these cases (wrapped "context" objects from C libraries) the lifetime dependency of other objects is made with good intention, but often leads to usability problems like yours. You could lobby the libsusb crate to adopt a strategy similar to rust-zmq, which [recently switched contexts](https://github.com/erickt/rust-zmq/commit/62ef789ce0b1d3466aec08d73700e4a75c15b121) to be `Arc&lt;RawContext&gt;` (therefore living as long as the longest living socket). The Arc can be replaced by Rc if the context doesn't have thread safety.
Apart from throwing away the loop counter and rolling your own, this is fine, as diwic said it should be compiled to a memmove equivalent.
*otoh* "Rust performance" is not a well-defined term and OP gave a clear context.
What's your project about? What's the license? Github link? (Seriously though, you're looking for /r/playrust)
My mistake. I missed the link.
Exactly - had a quick look at racer on crates.io and there's no way of telling that it is a program, not a library. I suppose we cope by immediately following the doc/repo links, but that can soak up a lot of time if you're browsing for some functionality. _Discoverability_ is such an important thing; how can we re-use the discoveries of others? Otherwise it becomes easier just to roll your own...
Hi, as someone tons of mental health problems (depression, ADHD, OCD; though the stigma is much worse for my mother which has schizophrenia, most people I know freaks out when they find out about this...), I wouldn't think twice before using the word "sane" to describe code that does the obvious right thing (but only if it's obvious and not a contentious point). I wouldn't associate something being a "sane project", "sane code", etc to any quality of the author of the code, and I think "reasonable" may accidentally convey another meaning. Often something is reasonable because it's a compromise accepted by everyone with a stake at the matter (so if I'm pushing for a compromise I may say I may say people to be reasonable and understanding each other's demands), but the sane thing to do is sane because it's obviously the right thing! Another crystallized terms I wouldn't mind using is **"sane defaults"**, **"sane environment"**, **"sanity checks"**, among others: the first means the defaults aren't something odd that most people will have to configure to get rid of (which is an usability problem), sane environments generally are platforms without buggy or missing functionality (for example, if an OpenGL program is running in a sane environment you don't expect it to immediately hit a driver bug and crash), and sanity checks are checks to determine if the environment (or something else like the fields of a data packet) is sane. The use of those terms is well established and don't refer to the sanity of anyone in particular, so I think switching to a synonym misses the point of using a thoughtful language, which is to respect other people. Now (and here's the part I fully agree with you), I think that using a term like "sane developer" may be very offensive, specially things like "wtf, no sane developer would do this.." when reviewing a PR, which implies there's a problem with the contributor and not with their code (people here that review PRs, don't do this!). And I agree with the article you linked too, since it talks about labels we apply to other people (like calling someone "crazy" instead of something more informative like "illogical"). But I try to go further and not label people with negative adjectives and instead label their actions. Like, don't call a contributor disruptive, say their actions were disruptive; the former implies it's an innate quality while from the latter it follows they can change their behavior to stop being disruptive. A real-world example in which I remember that many people said a piece of code wasn't "sane" was that, during the Heartbleed fiasco, OpenSSL was found out using its own custom buggy malloc (IIRC because on some obscure platform malloc has performance issues). For example, in a comment on [this slashdot thread](https://it.slashdot.org/story/14/04/10/2235225/heartbleed-coder-bug-in-openssl-was-an-honest-mistake) &gt; Ditto. Writing a custom malloc is insane for a sensitive security library like this... specially when it is done so carelessly. &gt; The fact that OpenSSL won't even work using regular malloc() suggests that there're more issues waiting to pop up here. But perhaps it's a poor wording of this issue. I find that writing "insane" makes things more personal and open to misinterpretation than saying OpenSSL wasn't doing the sane, obvious thing it ought to do. On the other hand, the OpenSSL developers were endlessly mocked for the choice of replacing the system allocator, an uncivil behavior we shouldn't engage (and fortunately the Rust community has very high civility standards, perhaps the highest I've ever seen in a free software community). That's a situation I find comparable with people that [mock the programmers of code posted at thedailywtf](https://blog.codinghorror.com/whats-wrong-with-the-daily-wtf/), without realizing we all write bad code sometimes: &gt; This is a repeating theme: On the one side are the great programmers, and on the other are the people endlessly bound to give TheDailyWTF source material. &gt; Do people really think such a schism exists? Is the impression that great developers are infallible, never creating any bad code at all, ever? Are bad programmers just stumbling from one WTF to another? &gt; Of course not. &gt; I fear the output of any developer who claimed that they've never written bad code. I would fear them because they're either bald-faced liars – believing that simply saying it repeatedly will somehow convince others into this fiction – or they're completely blind to their own weaknesses. &gt; Every developer in the real world has had bad days, brain faults, or bad interpretations of new languages, environments or libraries. It's simply a given of the profession. ---- PS: perhaps using the word "sane" to describe a piece of code doesn't offend my sensibilities because English isn't my native language. I was thinking here that perhaps if the term were in Portuguese (I'm from Brazil) I could feel it's in bad taste. But alas, I can't think of any Portuguese analogue for this so that's an untested hypothesis.
The central problem with shopping for wheels: if it's too hard to find or adapt the wheel, we'll just reinvent it. I feel a little guilty when I do this, but guilt is a useless emotion.
Yeh I'm getting the same results updating rustup too. My travis builds are fixed though (hurray!). I even went and explicitely installed `nightly-2016-12-02` and got cargo from `2016-11-26`.
this is my c++ implementation: #include&lt;cstdio&gt; #include&lt;algorithm&gt; #include&lt;vector&gt; using namespace std; int main() { int T, n, k; int h, ans; vector&lt;int&gt; v, dv; scanf("%d", &amp;T); while (T--) { scanf("%d %d", &amp;n, &amp;k); k -= 1; v.clear(); for (int i = 0; i &lt; n; i++) { scanf("%d", &amp;h); v.push_back(h); } sort(v.begin(), v.end()); ans = 1000000000; for (int i = k; i &lt; n; i++) { ans = min(ans, v[i] - v[i - k]); } printf("%d\n", ans); } return 0; } 
As an aside, I think your rhetorical question: &gt; what if, tomorrow, a very popular Rust library is published with code that deletes all your files, can it delete all my files? Is actually quite an important thing to ask.
That's true, unfortunately a lot of people think that the benchmark game is the holy grail...
This is the compiler's recursion limit, which is used when expanding macros. There's no runtime penalty associated with it (except that your program might be built with some fairly deeply-nested error types, I guess).
&gt;Apart from throwing away the loop counter and rolling your own I don't understand what you mean. 
* **k-nucleotide**: Once upon a time, [I had a blazing fast version of this](https://github.com/TeXitoi/benchmarksgame-rs/pull/21). However, thankfully /u/igouy updated the rules to make several of the tricks I used disallowed. Unfortunately this means Rust is stuck with a fairly old version whereas the C++ version uses things like a specially-designed hash function. The difference here is artificial, and Rust's could be updated to get much closer. One thing Rust will probably need is to use an external crate; the built-in `std::HashMap` doesn't handle poor hashes particularly well. * **n-body**: C++ uses explicit SIMD, Rust does not. The recent regression is likely just due to the fragility of automatic vectorization. There isn't an easy solution beyond stabilizing SIMD, but that will happen... eventually. * **binary-trees**: I don't know that much about this one, but TBH it's an odd benchmark and probably not very meaningful. * **spectral-norm**: This goes back to SIMD again. * **reverse-complement**: The loss here is fairly mild, but it's worth noting that C++'s sets affinities. I don't know how much that changes the resulting timings, but it *could* mean the programs themselves are equivalently fast. I don't know without testing. On the other side, we have * **regex-dna**: BurntSushi made a seriously awesome regex library, basically. * **mandelbrot**: The win here is not particularly large, but the programs are pretty different; Rust tries to fake SIMD whereas C++ uses OpenMP. So basically none of the differences matter IMO. It's still worth getting SIMD for its own reasons, and bringing k-nucleotide, binary-trees and reverse-complement up to par with the others are perhaps worth the PR.
i try to implement using your suggestion. this is my code: use std::io; use std::io::Read; use std::cmp; fn main() { let mut buffer: String = String::new(); io::stdin().read_to_string(&amp;mut buffer); let mut inp = buffer.split_whitespace(); let mut seq: Vec&lt;i32&gt; = Vec::new(); let mut tc: i32 = inp.next().unwrap().parse().unwrap(); for i in 0..tc { let n: i32 = inp.next().unwrap().parse().unwrap(); let mut k: i32 = inp.next().unwrap().parse().unwrap(); k -= 1; seq.clear(); for j in 0..n { seq.push(inp.next().unwrap().parse::&lt;i32&gt;().unwrap()); } seq.sort(); let mut ans: i32 = std::i32::MAX; for j in k..n { ans = cmp::min(ans, (seq[j as usize] - seq[(j - k) as usize])); } println!("{}", ans); } } i still got the same performance, :( look at [this](http://www.spoj.com/status/AMR10G,findingo/).
&gt; A couple of things we could do are: &gt; [...] &gt; Make rustc builds deterministic, which means that a known-trustworthy rustc build can be compared against a suspect one to figure out if it has been tampered with. Why isn't rustc deterministic at the moment? What would need to change?
I'm trying the first example of the guessing game in the rust book I wanted to check the effect of removing the .expect() from the read_line function, so I comment it. use std::io; fn main() { println!("Guess the number!"); println!("Please input your guess."); let mut guess = String::new(); io::stdin().read_line(&amp;mut guess);//.expect("Failed to read line"); println!("You guessed: {}", guess); } when I compile for the 1st time I get the expected error message: warning: unused result which must be used, #[warn(unused_must_use)] on by default --&gt; src\main.rs:12:5 | 12 | io::stdin().read_line(&amp;mut guess);//.expect("Failed to read line"); | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ however I don't get it if I compile again, even if I modify the file elsewhere (for example by adding a println!("test" ); somewhere before or after). I'm a bit surprised by this behavior. Did I miss something ? 
The latter, as I'm wanting to add in alpha conversion to eventually only generate 1 type for something like `(\x x x) (\y y y)` I'm sure if I was more familliar it'd've worked out, I'd eventually like to learn how to use nom, especially if I get into compiling something a little more syntactically complicated like prolog into rust type expressions (My next thought is a simplified Scheme, which I feel is one I'd rather handroll for)
It's also that I made this wheel as a learning exercise &amp; so replacing it isn't quite needed now, but I'd like to replace it as a learning exercise
Do you know why builds are non-deterministic, and if there's any work underway to fix that? It seems like a really important feature imo, especially for a language which emphasises security.
I like the idea to integrate with doc.rs better. I also miss help(module) in python very much when I want to know the definition of a rust class or function.I wonder if there has already had a cargo sub-command for this requirement :)
Sure, I can elaborate. &gt; This is a mutable variable binding. **When a binding is mutable, it means you’re allowed to change what the binding points to**. So in the above example, it’s not so much that the value at x is changing, but that the binding changed from one i32 to another. In the struct example, `a` points to a value of type `Point`. Of course, a `Point` is also composed of two `int` values, but `a` is not bound to the fields. Thus, to be consistent with that statement from the book, because `a` is bound to a `Point`, I was only expecting the `mut` keyword to indicate that `a` can be rebound to new `Point` values/allocations, not that interior values of the `Point` value that `a` is already bound to can change. Anyways, my point is that what `mut` does seems to fundamentally change once structs are involved which makes me think I perhaps didn't understand the semantics of `mut`in the first place.
As the error states, you *must use* the `Result` returned by `read_line()`. You could put the code into its own function returning `io::Result` and use the `?` operator (or the older `try!` macro, which does the same thing)` to short-circuit the method. Don't forget to return `Ok(())` in the successful case.
So the thing is that Rust won't assume the concrete type of something based on trait bounds. `|| "Hello world".to_string()` will resolve to the same assembly no matter what so there's no issue there, but `|msg| msg.to_string()` might mean `|msg| str::to_string(msg)` or `|msg| SomeTrait::to_string(msg)` or whatever. In most cases you can infer the concrete type of `msg` from the use of the trait (passing an `&amp;str` to it, for example) which resolves the issue, but here this would involve enumerating all the types that implement `Into&lt;Greeting&gt;` and taking the intersection with the types that `|msg| msg.to_string()` could represent, which is technically possible but leads to confusing semantics (such as adding new `impl`s causing existing code to fail to compile).
I assume X and Y are indices, so `for i in X..Y` gives you a loop counter without an extra `i=0`/`i+=1`.
Still doesn't work. Installing nightly-2016-12-03 manually gives me cargo from 2016-11-26, on both macOS and Linux. Travis builds are still failing too.
A `mut` binding not only allows you to change what that binding points to, but also to take a `&amp;mut` reference to the object. Which means that you can call `&amp;mut self` methods, update fields, and probably more that I forgot. This might need elaboration in the book (/u/steveklabnik ?).
That's a great suggestion for a badge, which is another feature of crates.io we're working on. However, it's out of scope for the particular problem the survey is investigating, which is how to order crates by default. EDIT: Actually, this might be solved by categories, depending on the categories we decide on and how crate authors categorize their crates.
I think I will return None if the item is not found, and Some(index) if it is.
Few small things: * `|ref x| **x` is the same as `|x| *x` * the return value I think makes the most sense is `Option&lt;usize&gt;` (the index the element was) * you should take a reference argument since all the method needs is to compare to it * to be more specific, use something like `&amp;V` where `V: PartialEq&lt;T&gt;` so that you can do `remove_item("a")` for a `Vec&lt;String&gt;` (actually, other slice methods like `contains()` should probably do that) 
What are goldenfiles?
I apologize for the documentation. I am improving it, but there are still a lot of combinators without examples. Recently, I concentrated on integrating long articles in the documentation: https://github.com/Geal/cargo-external-doc/ It will allow me to write manually an index of combinators by category, instead of the big alphabetic list generated by rustdoc
Thanks for the tips! Also, could you explain your last bullet point for me?
The book seems to be wrong here. Mutable variables act just like C++ variables, in fact printing the pointer shows that the binding does not change, only the value changes: let mut x = 5; println!("x is {}, address is {:p}", x, &amp;x); x = 6; // this prints a different value but the same address println!("x is {}, address is {:p}", x, &amp;x); The address is the same in both `println`s indicating that the binding does not change. To change the binding from one `i32` to another, you don't use `mut`, you use a new `let` statement. let x = 5; println!("x is {}, address is {:p}", x, &amp;x); let x = 5; // this gives the same value but a different address println!("x is {}, address is {:p}", x, &amp;x); 
Curiously, after updating again, I still only have cargo `3568be9 2016-11-26` (while my rustc is `c80c31a50 2016-12-02`).
Reproducible builds are really, really important for security; without them, hacking a single machine can compromise every user of the software. Especially if this isn't a huge amount of work (but even if it is), this should imo be high up on the to do list. Do you know if this is a priority of the rustc team?
&gt; k-nucleotide: thankfully /u/igouy updated the rules to make several of the tricks I used disallowed. The intention was not "to make several of the tricks [Veedrac] used disallowed." The intention is to show the performance of the built-in / library hash table implementation provided by a language implementation. &gt; whereas the C++ version uses things like a specially-designed hash function. *iirc* a specially-designed hash function can be used with std::HashMap and in the k-nucleotide program [the default hasher is replaced by fnv](http://benchmarksgame.alioth.debian.org/u64q/program.php?test=knucleotide&amp;lang=rust&amp;id=2) ? &gt; perhaps worth the PR [When chosen as a reference ?](https://www.rust-lang.org/en-US/faq.html#performance) :-) 
Awesome, thank you!
The two are one and the same - the tricks I used (most of which were taken from other programs) undermined the ability for the benchmark to benchmark what it aimed to. You might not have been targeting my submissions specifically, but you were targeting the same set of programs. To be clear, I think the rule change was a good step in the right direction. &gt; iirc a specially-designed hash function can be used with std::HashMap and in-the k-nucleotide program the default hasher is replaced by fnv ? Yes, the program currently uses FNV, but FNV is still a lot slower than the hasher C++ uses. Rust's can copy the same hasher, but Rust's hash map probably won't work well with hashes that weak. 
In addition to security, it's also important for performance when building large codebases. Deterministic builds let you cache build artifacts and perform incremental builds.
Dual_EC_DRBG is deliberately written in a way such that a backdoor could exist. You can't prove that anyone actually has the key to the backdoor, short of it being leaked, but that is a rather extreme standard of "proof". It is backdoored by any reasonable standard. The analogy would be if someone found code in the compiler that makes a network connection, checks if the result is signed by a hardcoded public key, and then executes it. You can't "prove" that it is backdoored because the public key could just be random bytes, in which case no backdoor exists. But it looks exactly the way it would look if someone did try to add a backdoor. This is the situation with Dual_EC_DRBG. In fact, the design of Dual_EC_DRBG is mathematically equivalent to encrypting your secrets with a hardcoded public key. The only question is whether somebody knows the corresponding private key, or whether this design somehow happened by chance and there is no private key.
No, it's because without them, there's no way to tell if the machine that produced a binary was malicious, and that's really, really important if you want to distribute software. I know a lot of current software doesn't support deterministic builds, but things are definitely [moving in that direction](https://wiki.debian.org/ReproducibleBuilds), and I expect (hope) that in a few years, you can get a whole OS that way. If Rust doesn't have them, it makes it basically impossible to verify that a given program written in Rust (as distributed) is secure. I suppose you can say "well just compile it yourself", but irl that's impractical and almost no one will do it. If builds are deterministic, only a couple of people need to do it and you can know the produced packages are secure. Some other benefits are [enumerated](https://wiki.debian.org/ReproducibleBuilds/About) on the Debian Reproducible Builds project page.
It allows you to use any type that could be checked for equality with the contained type, not necessary the contained type. In the example given `Vec` is holding `String`s, but the function is called with `&amp;str`. These are different types, but can be checked for equality with each other. The signature of the function in this case needs to be fn remove_item&lt;V&gt;(&amp;mut self, item: V) where V: PartialEq&lt;T&gt;;
I'll change that, thanks!
A mud-puddle can be an oasis; in a desert. Unfortunately it's easier to complain about what other people think, than **provide them with something better** than [a starting point](http://benchmarksgame.alioth.debian.org/dont-jump-to-conclusions.html).
Not a library, but [it's happened before](https://github.com/MrMEEE/bumblebee-Old-and-abbandoned/issues/123).
This is pretty accurate in the sense that it's "fuzzy." Beefing up types like `Vec` and `String` with non-controversial methods probably doesn't require an RFC, but we'll still put it through an FCP before stabilization.
 I use SIMD extensively in C++ land if that means anything.. IMO the answer is simple... Expose all the intrinsics, exactly as C++ does, using the same identifiers(_mm_mul_ps etc) *this will also make it much easier to port C++ SIMD to rust. Then people can build abstractions on top of them. 
Oh man, I literally just watched the Rust Fest talk about that yesterday. I really want to find some nice cases to start playing with quickcheck. 
&gt; To be clear, I think the rule change was a good step in the right direction. I appreciate that. &gt; Rust's can copy the same hasher, but Rust's hash map probably won't work well with hashes that weak. In which case, the Rust program would be shown as-slower because it was in-fact slower? :-) Sounds like a valid difference worth showing.
I think you're looking for /r/playrust
The only way to have a self-propagating piece of code like that is for the compiler to be compiled by a compromised compiler. If that compromised compiler isn't itself, then whatever compiles the compiler's compiler would have to be compromised too, and so on down the line until you get to something self-hosting.
Sweet, glad I could help it make sense.
My 2005 ACSAC paper was mentioned, however, it doesn't link to my later 2009 dissertation on the same subject. The 2009 dissertation doesn't *invalidate* anything in my 2005 paper, however, the 2009 dissertation adds much more. The 2005 ACSAC paper only applies to a common a special case (where a compiler self-compiles as its parent), while the 2009 paper applies to an arbitrary parent. Also, while the 2005 paper gives an informal argument that it works, the 2009 paper provides a formal proof. Finally, while the 2005 paper only shows one example (tcc), the 2009 paper adds additional demonstrations, e.g., it shows that it *does* detect a malicious Lisp compiler (as expected) and that it scales up (because it works on gcc). It's not *wrong* to point to the 2005 ACSAC paper, but I thought it'd be important to know that there's even more information available. 
AFAIK Crater runs only happen when a compiler developer asks for one to happen. They're not automatic (it costs a non-zero amount of money for each), and they're definitely not excessively frequent (though when they *do* happen, there may be a few in quick succession). The only crates that really need to worry about Crater inflating their downloads statistic are those that have a lot of dependent crates on crates.io. pcap-config doesn't have any dependent crates on crates.io, as shown at https://crates.io/crates/pcap-config/reverse_dependencies (contrast with https://crates.io/crates/libc/reverse_dependencies ). What makes you suspect that these numbers aren't from people actually using it?
For some crates, it makes sense to host the documentation off-site, e.g. for heavily platform-dependent crates like libc.
I've never tuned a guitar in my life, but I guess this would require some sort of pitch detection. I remember doing a pretty simple PD algorithm and it involved STFT (short time Fourier transform) over overlapping frames of the signal. With short enough frames (say, 2048 samples) speed shouldn't be an issue. BTW, I just found an in-depth article on pitch detection and shifting: [Pitch Shifting Using The Fourier Transform](http://blogs.zynaptiq.com/bernsee/pitch-shifting-using-the-ft/). Might be of some use to you :-)
This is way too broad and reads like "please rewrite my code for me". If you have specific questions about a snippet of code or a concept we can certainly help with that. Start rewriting small bits at a time and make sure those pieces still work. Then when you run into a situation you can't resolve you can ask for help about your exact scenario.
It looks like you didn't finish your thought, that might be why people are having trouble answering. Mind editing your post to add the missing bits?
So the [last time Rayon came up on Reddit](https://users.rust-lang.org/t/calling-rayon-users-opinions-wanted/7290) I learned that there was a tuning parameter. From the thread, it looks like [a new scheduling algorithm landed](https://github.com/nikomatsakis/rayon/pull/106). In the lazyweb spirit, does Rayon need any tuning these days?
&gt; This might need elaboration in the book (/u/steveklabnik ?). In general I'm not working on the first draft of the book right now; if someone wants to send in a PR, please do!
I believe that they're *always* run for betas and releases though. Not automated, but part of the process.
I'm glad my suffering brought someone joy :'D
This is my first real debugging, so I'm glad you enjoyed it!
could crater send something like a "useragent" that gets ignored on crates.io side?
Thanks for the blog post. A few months ago I needed to do something similar in parsing HTML tables into CSV and also picked Rust to do it. Take a look and see if it helps you move your project along: https://speck.phacility.com/diffusion/HTMLTBLTOCSV/
I can already see how hard this would be to implement, but would it be desirable to make it a distributed application like SETI@home and just run it every day, or with every PR if enough people were signed up?
Booted fine for me. I'm on arch. Installed `qemu` and `qemu-arch-extra` (not sure if that was needed). Then ``` qemu-system-x86_64 --cdrom livedisk.iso -m 1G ``` booted no problem. In fact, the biggest issue I had was finding the qemu keyboard combo to release the mouse. 
This may actually be of great help. &gt;Processing of tables stores the webpage and table contents in memory until written to disk, instead of streamed to file as processed By this, are you saying I can still access/extract data without needing to write to a tempfile? If so, this may be perfect for what I need. `kuchiki` is nice, but it's not thoroughly documented, and it's API is a little clunky for my taste. If this has a much better API, I might use this in lieu of `kuchiki`. Thanks!
I wonder if this is optimized away; it seems like something the compiler would maybe be able to figure out on its own. In any case, your solution would be the best option :p
I just woke up, so I might be missing something obvious... but why can't you do this: macro_rules! catch { ($($tts:tt)*) =&gt; { (|| $($tts)*)() }; } 
Huh. Well now I feel dumb, but thanks. Atleast now I don't have to rely on nightlies
Thanks, didn't realize that. I posted this from a phone when I usually am on a desktop. 
I'm no Rust expert, but [the function for finding the path to the goldenfile directory](https://github.com/calder/rust-goldenfile/blob/master/src/mint.rs#L98) looks pretty scary. At the very least, it looks like it's making a big assumption about where Cargo puts test binaries, and there doesn't seem to be anyway to override that assumption from outside the library. I don't know if Cargo provides any convenience functions for helping tests access test-data, or promises about where tests will be run from, but it seems like that would be worth looking into... and maybe filing a bug against Cargo if it doesn't. Sure, tests that require external test-data are not pure-functional, but they're sometimes very, very convenient.
I love hearing stories like this. It really provides insight to those learning it for the first time which is an invaluable resource. Also prime usage of Watamote. It gives the post a nice /r/anime_irl feel I wasn't expecting.
I thought I was on /r/anime for a second here due to the picture.
my guess, they aren't allowing for optimizations. :( by the way, thanks a lot for this discussion. i think i learned a lot here.
I thought it was a good discussion too. A little bit of searching makes it appear that the version of Rust on SPOJ is quite old. It might be worth contacting SPOJ to update it and pass correct flags to the compiler.
Apologies, I did try rust-beginners but didn't get a response. I'll update the OP with the exact issue.
I would vote for this, because it looks like the if comes after pattern matching, otherwise. I was confused reading it, not that I am any sort of rust prodigy.
If you don't change the file, cargo won't actually rebuild; try `cargo build --verbose` to make sure you're not mistaking the two situations.
Yes it could.
You'll likely want to post in /r/playrust
I should preface this with the fact that it has been over ten years since I last did any VB6 coding. Thankfully, the Dim x as y syntax is basically obsolete in rust = just using let whenever you need to add a new variable is enough. That by itself will probably make the code more legible. Most of the code looks like it could be easily translated over - things like for x in 1 to 12 would become for x in 1..13, but since both languages are capable of simple imperative code, the first pass would probably just be a direct translation. After that, a round or two or refactoring would probably make the code far easier to maintain. After that, you can start thinking about things like optimisation.
Have you taken a look at [ReadDir](https://doc.rust-lang.org/std/fs/struct.ReadDir.html) at all or [std::fs](https://doc.rust-lang.org/std/fs/index.html)? I think functions in here will help. I'm not sure it's exactly what you want but if you look at the seahash function I used for my website [here](https://github.com/mgattozzi/mgattozzi/blob/master/src/update.rs) you can see how I iterated through all the files in a directory, though even that could be cleaned up a bit. It's not fully robust code. You could also look at the md.rs file in there as well since that shows how it reads in pages from the pages directory and places then into the site folder it creates and renaming them
... but the rust library seems to have worked perfectly if "200" without a space is actually not conforming to the standard. The only bug seems to be with the server. Nice story never the less :)
I looked up the macro documentation but it's not entirely clear what `$tts:tt` means ("either side of the =&gt; in the macro declaration"). Could you explain what it does?
It's a [token tree](https://danielkeep.github.io/tlborm/book/mbe-syn-source-analysis.html#token-trees) capture.
What I'd be tempted to do is turn it inside-out, actually. 1. `include_str!()` the goldenfile 2. Create `struct GoldenFile { gold: &amp;'static [u8], pos: usize }` 3. `impl Write for GoldenFile` which, if and only if the written buffer is equal to `self.gold[pos..(pos+buf.length())]`, `pos += buf.length(); return Ok(())` 4. At the end, compare `gold.length()` and `pos` 5. If no write errors occur, and (4) succeeds, the test passed - no files needed! EDIT: If you wanted to be a little bit more ambitious, you could make `struct GoldenFile&lt;R: Read&gt; { gold: R, pos: usize, expect: usize }`
Strictly speaking, yes, `httparse` is just following the standard. However, there comes times when complying to the standards so strictly reduces usability. Case in point, neither the browser, `curl`, nor `requests.py` panicked when getting the exact same page with the exact same status. One of my engineers demonstrated a default module in golang that handled the response without batting an eye. The HTTP 1.1 standard says that return codes need to be a number followed at least by an SP character, before the CRLF. The reason phrases that follow, like "OK" or "Forbidden" or "Not Found", are only suggestions, and not actually formalized. Even so, I think the latest commit on `httparse` sums this up nicely &gt;RFC7230 says there must be 'SP' and then reason-phrase, but admits its only for legacy reasons. With the reason-phrase completely optional (and preferred to be omitted) in HTTP2, we'll just handle any response that doesn't include a reason-phrase, because it's more lenient, and we don't care anyways.
Yes, that's a reference to [redis-cell](https://github.com/brandur/redis-cell). The only module ATM that I know of that could potentially be a part of Redis is [neural-redis](https://github.com/antirez/neural-redis).
That's how all of my previous VB6 ports started. Refactoring's quite fun on the first pass. :P It's just the iterating through directories bit. I've got a C# implementation going on in the GUI code for now.
I threw away what I was writing before posting this so I didn't have it on hand. It's difficult to debug Rust DLLs from C# GUIs unless I should've had some sort of test harness to put the code in (in this case, stick the code in a Rust EXE).
Yes, but it doesn't regenerate docs on a regular basis like crater does, so it literally adds one download for every crate version.
Could you please provide some example?
I'm really exited about this. This opens up for efficient bignum implementations on 64-bit architectures which in turn opens up for fast asymmetric cryptography written in rust (without any C bindings).
God, the amount of churn required to introduce those integers is amazing; I'm sincerely amazed of est31's determination!
Book's probably talking about values living behind references, so the pointer itself being in a mutable variable doesn't let you mutate the value behind the pointer, but you just have a value right in the mutable variable with no pointers involved.
In my laptop, Go code (with `package` and `import` added as needed) results in: 5.34 real 5.21 user 0.04 sys ...while Rust code with `-O` option (would correspond to Cargo's `--release` by the way) results in: 0.97 real 0.95 user 0.00 sys ...so it's indeed a matter of appropriate options. Just in case, Go version used is 1.7.4 and Rust version used is 1.13.0; both done in macOS 10.12.1. It's a typical compromise between compile time and run time---it's very hard to lower both so Rust chose to optimize for run time (when told so) while Go chose otherwise. On `Some(1)`, Go [`strings.IndexAny`](https://golang.org/pkg/strings/#IndexAny) would return -1 when there is no match, which is not typesafe as one can mistake -1 for the actual index. Rust has an [`Option`](https://doc.rust-lang.org/std/option/) type for representing such case.
Holy damn! Compiled as 'release': 0.574 sec, 0.579 sec, 0.566 sec I didn't realize there would be such a discrepancy between debug and release! Thanks guys!
in general, I would expect Rust to be slightly faster than Go, maybe twice as fast. This instance is a little better than my expectations.
That's fair. I was really aiming to give a high level view the usage of the libraries and some real world use cases where different techniques apply. Honestly, I kind of expect that the core maintainers will probably start publishing more details about the depth that your asking about, I don't think I actually know the depths of both libraries to get into that much detail.
Looking at your changes, the "however" you note also misses another problem: when an elimination leads someone to be elected, you redistribute the resulting surplus before checking if anyone else is elected as a result of the elimination. The premature distribution is not only a problem when electing those who were elected with first preferences. Specifically, in the Tasmania election, by the official rules Lisa Singh is elected in [count 352](https://angrygoats.net/senate2016/#/scenario/tas/count/352) before David Bushby's surplus is distributed because the elimination of Richard Colbeck got her 4 more votes than quota. Your code however distributes David Bushby's surplus before checking who else is elected as a result of Richard Colbeck's elimination, and so elects Catryna Bilyk before Lisa Singh. If you like I can try to figure out a patch :)
Use this: const CHARS: &amp;'static [char] = ['0', '1', '2', '3', '4', /* you get it… */ 'x', 'y', 'z']; fn to_str_with_base(mut num: u64, base: u64) -&gt; String { if num == 0 { return "0".into(); } let s = String::new(); while num != 0 { s.push(CHARS[num % base]); num /= base; } s }
Use the data-encoding crate. http://ia0.github.io/data-encoding/data_encoding/
FWIW, whenever SIMD is stabilized, this is exactly the case for the [jetscii crate](https://github.com/shepmaster/jetscii#searching-for-a-set-of-ascii-characters): #[macro_use] extern crate jetscii; fn main() { let part_number = "86-J52:rev1"; let parts: Vec&lt;_&gt; = part_number.split(ascii_chars!('-', ':')).collect(); assert_eq!(&amp;parts, &amp;["86", "J52", "rev1"]); } Then you can search for up to 16 characters in parallel.
I'm also really excited to see this! A while ago I was working on an in-place transposition algorithm that required 128 bit integers for efficiency. When this hits stable I'll finally be able to merge that work in! Edit: Here's [a post](http://athemathmo.github.io/2016/08/29/inplace-transpose.html) about the work. The part that needs 128 bit integers is the arithmetic strength reduction.
Thanks! Most of the work in the PR was done by nagisa. My contributions were that I rebased, fixed intrinsics (they were basically just drafted and riddled with bugs), and made sure everything works (still doing that, the PR isn't merged yet and just now bors told me that it can't build for windows). I've learned much along the road, its true when people say that you only learn when you challenge yourself.
Is there any advantage to doing that?
Good job though! You learned something new and we get 128bit integers. I'm excited :D
Yes. The documentation is ok, but definitely lacks critical details in some places. I will try and go back and submit some PRs for things that I figured out or should be clarified. Also, I think for most use cases there are going to be higher level abstractions in the tokio-proto and other crates in the tokio system which most people will want to use.
&gt; Sounds like a valid difference worth showing. I appreciate that, but here's the problem from my point of view. The Rust standard library makes the (correct, IMO) decision to trade away performance with pathologically bad hash functions in favor of performance with normal hash functions, since hash functions of the former type are rarely used outside of benchmarks. However, this hurts our k-nucleotide numbers. This puts us in the position of having to choose between the real world and the benchmarks game.
 const CHARS: &amp;'static [char] = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']; ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ expected reference, found array of 10 elements = note: expected type `&amp;'static [char]` = note: found type `[char; 10]`
When you're writing benchmarks, also take care that your loop isn't optimized away. The [benchmark support](https://doc.rust-lang.org/book/benchmark-tests.html) wraps the closure with a hint to the optimizer for you. I tested this case, and it looks like the compiler is behaving **fine**, but it's something to watch out for.
When doing bignum arithmetic you want to have as large a limb (digit) size as possible on your architecture. The reason being that primitive arithmetic on a limb (basically) is constant time. So for example a 64 bit addition is as fast as a 32 bit addition on a 64 bit architecture, so in that case if you would have a limb size of 32 bit you would need to do two 32 bit additions to add a 64 bit number. Most CPU architectures have a multiplication instruction that has double word precision, e.g. you can multiply two 64 bit numbers and get the result in two 64 bit registers. In assembly language it's trivial to use a multiplication instruction and handle the overflow, but in higher level code you need to be able to access the 64 high bits of that multiplication. When we only had u64s in rust there was no way to multiply 64 bit numbers without throwing away the high bits. Now that we have u128s we can access the result of u64 x u64 as an u128. Previously we needed to either use u32s or do two multiplications of u64s (one for the high part and one for the low part of the result). Hope that's clear, please ask if you have any more questions.
[This is an example](https://gist.github.com/SethDusek/69e2e1bb4bf0979db072333d89982c86). Basically the point is that you don't have to create a new function every time you want to handle errors
Yes, you can design your API to be hard to use incorrectly. For example, HTTP builder could prevent you sending body before header. Or you can imagine something like `FileBuilder::readonly()` disallowing you to call `.append()` on it. Etc.
Since you're sorting the vector in-place, it doesn't seem safe to hold a reference into that vector that outlives the call
My PRNG crate utilizes 128 bit internal state for its 64 bit generators. I have been toying with the implementation lately but stopped until real u128 are supported.
Although one advantage of using a plugin would be that you won't have to add an Ok(()) at the end and it'll just automatically insert the Ok similar to [this](https://github.com/rust-lang/rfcs/blob/master/text/0243-trait-based-exception-handling.md#optional-match-sugar). But I can't seem to figure out how to do that
I wrote a big comment over on HN too: https://news.ycombinator.com/item?id=13100758
It's a link to the PR, which is still open. Once it's merged, it'll be in the next nightly.
This is not a pleasant or necessary metaphor. I hope you reconsider it.
After some [initial confusion](https://www.reddit.com/r/rust/comments/5ef884/why_cant_this_method_be_called_more_than_once/), I decided to heed the advice of /u/Veedrac and stick with an `unsafe`-based design. Only three usages in all! The `owning_ref` crate's `StableAddress` trait helped generalize the structure to intern many different types (`String`, `Vec&lt;T&gt;`, `Arc&lt;T&gt;`.) There's even a specialization for "doubly-stable" types (e.g. `Arc&lt;Vec&lt;u8&gt;&gt;`). The backing map type is parameterized. Both `HashMap` (default) and `BTreeMap` are supported out-of-the-box. The ID type (default `usize`) is also parameterized, so that more memory can be saved if the domain size is known a priori. 
I'd considered that, but thought that the wordplay on "internment" was too good to pass up (and it's one of my favorite films!) If you have any alternatives, I'm happy to hear them.
It would be fine, if we could have u128 support soon. So we can experiment and gain some experience. The RISC-V people want to have a 128-bit CPU in a few years. Yes, thats right, a 128-bit CPU with 128-bit registers that is primarily designed for warehouse scale computer systems. 
Does (num % base) as usize work ? and let mut s, so s.push works 
Yes, moves would be impossible. I'm not sure if `StableAddress` is enough to make it work in general. It might be possible to have it work for heap-managed data but I don't know if can ever work with stack-allocated data. Since moves are just copies the reference field of the copy would erroneously refer to the original, which would then be dangling when the original disappeared. Rust would either need some kind of copy constructor logic to fix the reference (which I'm pretty sure is never going to happen).
Some people might associate it with actual internment camps, such as the Japanese internment during WW2. I'm not saying you're insensitive for using that name, but I can see where others might.
If you have better suggestions, I'm open to them. Open an issue/make a PR.
Well, I mean ["string interning"](https://en.wikipedia.org/wiki/String_interning) is the common term already. (I think jdubjdub was referring to the fact that prisons are notoriously awful places.) I could have called it a "pool", but that term is so overloaded already, I wanted something more unique.
Does this work leverage any particular hardware support for 128-bit arithmetic? If not, how is this preferred to just implementing, say, 256-bit arithmetic? Bigger is better right?
Better suggestions: literally anything else. For example, crate "generic_interner", struct "Interner".
That was my first thought, actually, but I sorta feel guilty for taking "too common" names from crates.io (already got `deque` and `minimax`). Plus, `generic_interner::Interner` is so redundant and uninspired. If it's just the "Prison" part everyone finds objectionable, I could use the name "ArenaSet" that mentioned in the docs. I guess "Solitary" could be "StadiumSet" or something.
&gt; Two bugs in a single day. One in httparse, and one on my company’s custom webserver. Wait, I thought you concluded that not accepting the lack of a space was correct behaviour.
Though correct and adherent to the standard, other http clients like curl, requests.py, and even my web browser handle this just fine. The HTTP/2 spec now even makes the reason phrase entirely optional. So, yes, technically accepting it isn't kosher, it is convenient. 
Since the rental struct could potentially be moved at any time, the borrow portion of the rental must remain valid across moves. This can only be guaranteed by allowing objects that deref to fixed memory addresses to be used as owners. This usually means you store the owner object in a Box or Arc, as the heap address it refers to will remain the same across all moves. The owner can itself be a reference, which is fine since the target of a ref can't be moved out from under you while a reference exists. In this particular case, an Arc is used so that other rental structs can share the owner.
Ha, I did, but I'm kind of liking that typo. I wasn't keen on the "arena" metaphor anyway because this structure has a very different use case (even if some implementation details are similar.) Once/if someone comes up with a better metaphor, I'll put out a 0.3.0 release :)
This is something I've been wanting for a while, hope we'll be seeing it soon. Julia's the only language I know of that officially supports `int128` fully. I think C and C++ also can access `__int128`, but basic things like printing didn't seem to be working last I checked.
Honestly it's really hard to say without profiling, because I'm betting almost all of the time is being spent in the string allocation. EDIT: I came up with a version that eliminates allocations in the loop, and the time to run on my machine went from 9.7s to 4.24! use std::fmt::Write; fn count(s: &amp;str, m: char) -&gt; usize { s.chars().filter(|&amp;c| c == m).count() } fn main() { let mut sevens = 0; let mut buffer = String::with_capacity(10); for x in 0..100000000 { write!(&amp;mut buffer, "{}", x).unwrap(); sevens += count(&amp;buffer, '7'); buffer.clear(); } println!("{}", sevens); }
done!
A possibly more efficient way to do this would be to use division and remainder directly in a loop (the condition might look something like `if x % 10 == 7`), avoiding the string/allocation middleman. This applies to both languages.
It just occurred to me that this would have been a better approach overall. 
Oh nice! If they respond and it is optimization flags I'll be interested to know how your solution time improves afterwards.
Just pasting your code onto my system and compiling it with go v1.7.3 and rustc v1.13.0, the Rust code outperforms the Go code consistently with 5.72s versus 6.40s for the Go code... so that's interesting. Using the optimization that /u/dbaupp mentioned, the execution time for the Rust code drops down to 1.35s instantly. fn count7(num: u64) -&gt; usize { let mut total = 0; let mut num_tmp = num; while num_tmp &gt; 0 { let remainder = num_tmp % 10; if (remainder == 7) { total += 1; } num_tmp -= remainder; num_tmp /= 10; } total } fn main() { let mut sevens = 0; for x in 0..100000000 { sevens += count7(x); } println!("{}", sevens); } 
Yeah, I"ll probably take another pass at the readme. It's hard to formulate a good description of the library since it's so abstract.
Very good finding.
I've had similar things happen in C++, where a 100x speedup is not unusual for heavily templated code that can be inlined in release mode. I'm thinking of the [Eigen library](http://eigen.tuxfamily.org/index.php?title=Main_Page) in particular. (I'm still waiting for an Eigen equivalent in Rust. Ndarray is getting there, but I'm particularly looking for the fixed-size types used in graphics, like Affine3f and Matrix4f.)
## How to speed it up by only(!!) a factor of **390224x**: If you want to make it **really** fast, you might as well cheat and use math to your advantage, rather than manually counting each and every "7" in the number. [Rust Playground link.](https://is.gd/3EOoGF) (note: the bounds of the search have been divided by 10 to fit within the timeout limit on the Rust Playground. *make sure to select Release mode!*) &gt; count7: 7000000, took 0.26691324 seconds &gt; &gt; total7: 7000000, took 0.000000684 seconds //more traditional, simple solution fn count7(num: u64) -&gt; usize { let mut total = 0; let mut num_tmp = num; while num_tmp &gt; 0 { let remainder = num_tmp % 10; if remainder == 7 { total += 1; } num_tmp -= remainder; num_tmp /= 10; } total } //use math to calculate the number of 7's in all of the numbers leading up to some maximum fn count_total_7s(up_to: u64) -&gt; usize { let mut num_digits = 0; let mut up_to_tmp = up_to; let mut last_digit; let mut total_sevens = 0; while up_to_tmp &gt; 0 { num_digits += 1; last_digit = up_to_tmp % 10; // two = ld + (&gt;7)*10 // three = ld*20 + (&gt;7)*100 // four = ld*300 + (&gt;7)*1000 // five = ld*4000 + (&gt;7)*10000 let some_sevens = if num_digits &gt; 1 { last_digit * (num_digits - 1) * 10u64.pow(num_digits as u32 - 2) } else { 0 }; let special_sevens = if last_digit &gt; 7 { 10u64.pow(num_digits as u32 - 1) } else { 0 }; total_sevens += some_sevens + special_sevens; up_to_tmp /= 10; } total_sevens as usize } use std::time::{Instant, Duration}; fn get_duration(dur: Duration) -&gt; f64 { dur.as_secs() as f64 + (dur.subsec_nanos() as f64 / 1_000_000_000f64) } fn main() { let mut sevens = 0; let start1 = Instant::now(); for x in 0..10000000 { sevens += count7(x); } let count7_time = get_duration(start1.elapsed()); println!("count7: {}, took {} seconds", sevens, count7_time); let start2 = Instant::now(); let total7 = count_total_7s(10000000); let total7_time = get_duration(start2.elapsed()); println!("total7: {}, took {} seconds", total7, total7_time); } 
&gt; No math tricks.
The [bytecount](https://crates.io/crates/bytecount) crate is the best way to achieve this. It is the fastest, most simplest method of counting specific characters in a string.
ARef looks pretty good, nice work. Reminds me that I should probably automatically impl more traits on the structs I generate; will look into that. The primary differentiating factor with my lib is that the borrow does not have to be a reference at all, rather it can be any type that qualifies as a borrow, or in other words, any type that has lifetime params. With rental, the borrow could be a tuple or struct that contains any number of refs that borrow from the owner, or a Box of an Arc of a ref to the owner, or what have you. Because the "fake" lifetime can appear anywhere in the type signature of the borrow, a lot more machinery is necessary to make sure it's safe, which is mostly what the macro is for, to act as pseudo-type-constructor. If rust had HKT, then this could be expressed as a trait and the macro would be unnecessary, but for now this is the best I could manage.
It's half-half. If you're talking about the news and not the PR, it's simply because they're still up-to-date.
Since people don't seem to like the name, may I suggest honoring a doctor instead? An internist is a physician, and a subtype of internist is an infectious disease specialist. Since interning is in part about preventing replication of data, this seems to have nominal sympathy. How about John Snow?
Last week, we found some further improvements to [bytecount](https://github.com/llogiq/bytecount) &amp; I extended benchmarking to stable and beta. This week, I'm trying to get travis to test all combinations. Also I have a new RFC around stack allocation, will push soon.
You might want to split this into two functions, remove_first_item and remove_all_items for the cases where the vector has multiple of the same item
I love the name exactly for the reason you stated. It's witty and quite precise. 
The fancy algorithms only make sense above ~32/64 bytes. For tiny strings like this, you'll want `naive_count`.
*We need to go deeper!* use std::fmt::{self, Write}; struct CountSevensWriter(pub usize); pub fn naive_count(haystack: &amp;[u8], needle: u8) -&gt; usize { haystack.iter().fold(0, |n, &amp;c| n + (c == needle) as usize) } impl fmt::Write for CountSevensWriter { fn write_str(&amp;mut self, s: &amp;str) -&gt; Result&lt;(), fmt::Error&gt; { self.0 += naive_count(s.as_bytes(), b'7'); Ok(()) } } fn main() { let mut sevens = CountSevensWriter(0); for x in 0..100000000u64 { write!(sevens, "{}", x).unwrap(); } println!("{}", sevens.0); } 
&gt; I don't know how Go strings work Go strings are pretty similar to Rust's, working on codepoints. `strings.Count` counts decoded codepoints (which Go calls "runes"). However since it's a specialised process it has special cases for various situations, including single-byte needle where it just performs a linear bytewise scan: https://golang.org/src/strings/strings.go#L75 &gt; but the chars() iterator has to do UTF-8 decoding which is probably a bottleneck in this process On my system, rust's UTF8 decoding throughput is well over 400MB/s (source and destination in memory) for ascii.
&gt; It's important to state right up front that a string holds *arbitrary* bytes. It is not required to hold Unicode text, UTF-8 text, or any other predefined format. As far as the content of a string is concerned, it is exactly equivalent to a slice of bytes. https://blog.golang.org/strings That said, this is only a minor cost. It's the slower allocation (due to lack of GC, I presume) plus the extra copy in the formatting that seems to cause it.
Great post, I had already looked into slog, thought it could be interesting but failed to understand how it could be used efficiently.
I'll try to get [rust-zmq](https://github.com/erickt/rust-zmq/) closer to a [0.8.0 release](https://github.com/erickt/rust-zmq/milestone/1), and get [stdio stream support](https://github.com/alexcrichton/tokio-process/pull/1) for [tokio-process](https://github.com/alexcrichton/tokio-process/) fully working.
The problem is the lifetime of your files, created in this line: let file : File = File::open(file_path).expect("oaeuoe"); A reference to `&amp;file` is only valid for the scope of `file` (the lifetime of `file_path` is irrelevant once the file has been opened). Therefore, if you want this code to work, it's not a question of adding lifetime annotation, but storing the `files` in the vector you return, instead of references to files that will be destroyed when the function returns. I think the simplest way is to return a `Vec&lt;File&gt;`: fn create_buffer_list(file_list: &amp;Vec&lt;&amp;Path&gt;) -&gt; Vec&lt;File&gt; { let mut buffer_list : Vec&lt;File&gt; = vec![]; for file_path in file_list.iter() { let file : File = File::open(file_path).expect("oaeuoe"); buffer_list.push(file); } buffer_list } or you could return a `Vec&lt;Box&lt;Read&gt;&gt;` if that's really needed (not sure why but I guess this looks more closely to your initial code?): fn create_buffer_list(file_list: &amp;Vec&lt;&amp;Path&gt;) -&gt; Vec&lt;Box&lt;Read&gt;&gt; { let mut buffer_list : Vec&lt;Box&lt;Read&gt;&gt; = vec![]; for file_path in file_list.iter() { let file : File = File::open(file_path).expect("oaeuoe"); buffer_list.push(Box::new(file)); } buffer_list } (also, you might want to prefer using `file_list: &amp;[&amp;Path]` in your function argument, since it is more flexible than requiring `&amp;Vec`.)
If he insists on only exposing a `Read` interface and you don't want to pay the price of trait objects, the best alternative is to create a `struct NewType(File);` and only implement `Read` for it, forwarding to the inner instance. This is what the proposed `impl Trait` type syntax would desugar to.
Was assigned a task on webrender. . .how exactly do I build it? Do I do this (from the README.md)... `To use a custom webrender with servo, go to your servo build directory and:` Edit servo/Cargo.toml Add at the end of the file: `[replace] "webrender:0.11.0" = { path = 'Path/To/webrender/webrender/' } "webrender_traits:0.11.0" = { path = 'Path/To/webrender/webrender_traits' }` Build as normal Do I have to build all of Servo or can I just do `Go to the servo directory and do ./mach update-cargo -p webrender` (from the top of the README.md as well)
Just asking, are you by chance using an AMD machine? IIRC there are/were some weird performance problems. EDIT: [Found the original thread](https://www.reddit.com/r/rust/comments/54dowq/amd_performance_crippled_by_rust_compiler/) and it seems it was an issue with jemalloc and some Linux kernels and not specifically AMD CPUs.
Oh damn, I misunderstood the code sample which I assumed `i` would be in range `X..Y`. In any case the same applies, you're relying on the compiler to omit bound checks and it isn't as simple as hoisting them out of the loop as this may change the program's behavior when it does panic. There are some workarounds for the optimizer such as splitting the loop in two parts (is that what you're trying to say?) but better would be to either use unsafe constructs or assert that `i` will always be in range before the loop (same thing, really).
Cool! I've been using [exa](https://github.com/ogham/exa) as a `ls` replacement for quite a while now, it also has `--tree` option. (Whoah, I just saw that I made a PR to exa in March 2015 a.k.a. before Rust 1.0. Crazy.)
I'm afraid I don't follow? The OP is trying to implement memmove by copying elements of the slice/array individually. This involves bound checks. I'm replying to what you said: &gt; as diwic said it should be compiled to a memmove equivalent. These bound checks may be optimized away. If they're not optimized away (eg. the range being copied is an argument and the function isn't inlined) they will happen in every iteration of the loop. This may inhibit optimizing to a `memmove` under the hood. That's my point. Either be explicit (use `memmove` explicitly, so `ptr::copy`) or tickle the optimizer such that it will get rid of the bound checks because it cannot do so automatically.
Note that there is already a popular rust project called [Oak](https://crates.io/crates/oak). I'd recommend a different tree for your purposes.
Okay, interesting! I'm still trying to understand how you're able to do this. So your `RentRef` looks a lot like `OwningRef`, but there's an additional `rent` function that seems to be able to map from, say, `&amp;'a Foo` to a `Bar&lt;'a&gt;`. Would it be safe to just add a `rent` function to `OwningRef` and get the same functionality, or are there major differences that makes this impossible? 
Side note: Prefer `&amp;[X]` over `&amp;Vec&lt;X&gt;` as function parameter types. The former is more general. It also applies to subslices or slices that are not managed by a `Vec`. A reference of type `&amp;Vec&lt;X&gt;` coerces to a reference of type `&amp;[X]`. So you generally don't have to change the code that invokes the function. :)
FYI: fn foo( x: &amp;'a mut Bar) -&gt; Foo&lt;'a&gt; or fn foo(x: &amp;'a mut Bar&lt;'a&gt;) -&gt; Foo&lt;'a&gt; Is more like a move, to simply borrow, but modify state... while keeping interior lifetimes valid fn foo&lt;'a&gt;(x: &amp;mut Bar&lt;'a&gt;) -&gt; Foo&lt;'a&gt; The first to examples will invalidate the original `let mut something: Bar` the compiler will act like it is moved. The last example the original `let mut something: Bar` will exist. :.:.: What you're looking for is (*I think*) fn foo&lt;'a&gt;( x: &amp;mut Vec&lt;&amp;'a Path&gt;) You want the `Path` to remain alive, while modifying the Vector(?)
Right you are, https://is.gd/3rpHM5
That's a good demonstration. Now we can't use a demonstration to prove our `unsafe` code correct, but it's a start.
I would indeed like to keep the `Read` interface. What "price" are you talking about?
Is it really important to _hide_ the fact that which you return are in fact `File`s? If not, just do as I show below and return `Vec&lt;File&gt;`. Returning `Vec&lt;File&gt;` doesn't mean that `Read` isn't available any more! Using `Box&lt;Read&gt;` you 'erase' the fact that you are really working with `File`s and instead say 'this is some opaque object which implements `Read`'. The way you've implemented this privacy is by creating a [trait object](https://doc.rust-lang.org/book/trait-objects.html). The price you pay is a memory allocation for each `File` and every time you want to read from it will use dynamic dispatch. Quite a hefty price to pay for that privacy. If you insist on hiding the internal details of the returned files, here's how you can achieve that without memory allocation while still getting the performance of static dispatch: pub struct Buffer(File); impl Read for Buffer { fn read(&amp;mut self, buf: &amp;mut [u8]) -&gt; io::Result&lt;usize&gt; { self.0.read(buf) } } And then return `Vec&lt;Buffer&gt;`. Populate it like so: `buffer_list.push(Buffer(file));`. Is this worth it? Depends on what you want to do... 1. You just want to act on the opened files: really, just return `Vec&lt;File&gt;`. 2. You are making some library and want to hide the details of the returned vector, wrap it in a newtype as shown. 3. You have to satisfy some external interface expecting `&amp;[Box&lt;Read&gt;]`, very unlikely... It sounds like you really want 1. where all this extra complexity is unecessary.
Your proposed add64 is already part of Rust but under the name `overflowing_add`, see [here](https://doc.rust-lang.org/std/primitive.u64.html#method.overflowing_add). Sadly there is no equivalent to your mul64. 
I've been working a bit on my [`rouille`](https://github.com/tomaka/rouille) web framework recently. I'll try to make progress towards releasing a version 1.0 of the library. 
Partially-generated AMQP parser and client. The last week worked on using nom as a parser-- which works fairly well after a few hiccups-- though that code isn't pushed yet, due to the lack of decision on the data structures. I keep having trouble figuring out is how to organize structures so that they can be zero-copy when possible, but also allow owned instances to be used and sent with the API. Someone in #rust mentioned that they basically had two structures that could convert between each other, [like so](https://github.com/sfackler/rust-postgres-protocol/blob/master/src/message/backend/borrowed.rs#L240). But that leads to a lot of duplication and potential complication for enums that are user-presentable. I'm leaning towards having a struct that purposely makes everything owned, like [so](https://is.gd/vlAtAI)-- but I'll need to work out some details on how the Table and List types work. After that, hopefully I can finally get to more fun stuff: like publishing a workable client, seeing if using a Vec/Hashmap pool in the parser is worthwhile, and experiment with allowing Vec/Hashmap to be generics instead.
What's the fastest way to print to a terminal? I remember reading that if a program is going to be doing heavy terminal IO that the println! macro can slow the program, but I don't remember what the preferred method is.
Is the speed of terminal printing a limitation that is blocking you, or just a theoretical concern?
I am so excited for this RFC, you have no idea. Thanks to /u/chriskrycho for all his work on this!!! The TL;DR is that all new features must have documentation of some sort before they can become stable, and all new RFCs will have a "How do we teach this?" section.
Rust has fairly light weight testing. Just put something like the following at the end of your file #[test] fn my_test() { assert!(... my test ...); } See also http://rustbyexample.com/meta/test.html https://doc.rust-lang.org/book/testing.html 
&gt; I'd love to see a feature for recommending alternative crates. I.E. If I fork a crate, I'd like to put a link on the old one to suggest the new one. Not as an official Endorsement, just that some user thought you may want to look at this other thing. I think that it is hard to find out what alternatives are out there. Hm, interesting. I'm not sure how to implement this exactly, since not everyone who forks a crate on github to fix something or play around would want their fork recommended on crates.io, necessarily. And if the alternatives aren't official endorsements, we'd have to communicate that clearly. Also it's taking away some amount of control that a crate owner has over what appears on their crate's page... I'll think on this some more. &gt; Also a way to search tags. I need to find a crate with the tag to see what else has that tag. I'm not sure if this is linked anywhere, if it's not it's not obvious and we should fix that, but there is [a page that lists all the keywords](https://crates.io/keywords).
It should be better, because the compiler understands the operations and can optimise them better.
Yes, it is basically two u64s, but it is also backed by instructions https://m.reddit.com/r/rust/comments/5gfijk/comment/dasmiir .
So the `?` operator will have official documentation, along with macros and all compiler flags?
I am working on letsencrypt-rs to add [DNS validation](https://github.com/onur/letsencrypt-rs/pull/6).
I'm not sure how soon features that have already gone in will be documented, but this will at least ensure features going forward will be documented before stabilization. I'm sure the docs team would love help catching up! [Check out This Week In Rust Docs](https://guillaumegomez.github.io/this-week-in-rust-docs/) and all the things linked from there to get involved. The more people that can help even a tiny bit, the more documentation will get done sooner!
Oops, I didn't know about that. Thanks for mentioning it.
Sorry, I was referring to the other half of my original comment.
https://en.wikipedia.org/wiki/Gamma_function
Veedrac wrote "… Rust's hash map probably won't …" Maybe it will, if someone tries. &gt; This puts us in the position … Yet somehow C++ escapes this dilemma ?
Hooray! As I wrote elsewhere: [...] library code needs docs to be usable – not only for the contained know-how, but because it forces you to think like API user [...] The same reasoning applies to language features.
One thing you'll want to do is use `write(ln)!` on `stdout.lock()`, it'll save acquiring a lock on every `print(ln)!`. For many small writes, a `BufWriter` wrapped around that should save on syscalls.
Multiplication on x86 (and probably other platforms) has always returned a double-sized result, and most languages/compilers would throw away the upper half, giving you no way to access it.
I also use https://crates.io/crates/cargo-info a lot
&gt; A mud-puddle can be an oasis; in a desert. wise words ;-) &gt; Unfortunately it's easier to complain about what other people think, than provide them with something better than a starting point. Don't get me wrong, benchmarks are important. And the OP showed that s.th. was going on worth investigating. The thing is that especially the benchmark game is seen as s.th. ultimately which it is not. It only can show directions or trends. If in doubt write your own benchmark optimized for your special purpose. And the game was criticised in the past. For ex. pypy is missing and some programming languages allow to solve problems in a much nicer way but were rejected, etc. Lastly because the game is cited so often it is important for Rust to show good results.
Why would they do that ? 
Writing a breakout clone to get into Rust. Finished loading in the level maps (stored as xml), currently working on displaying them, after that implementing the general logic. Due to some insecurities it is currently a private project on gitlab, will be moved to github when it's finished.
I recently wrote a simple crate for reading environment variables as well: https://crates.io/crates/envoption. It has functions for optional and required variables, and returning a default value if the variable is absent. Originally, I wrote this since *envy* was using version 0.7 of *serde* which seemed to cause conflicts with other packages using 0.8, but I see that *envy* has also updated to use 0.8 in the past few days, which is great!
I'll try make my first project working stable. It is a parser library for Excel (xls) which will get data, formats, colors &amp; styles, optional save its to json format.
"If you are not embarrassed by the first version of your product, you've launched too late." - Reid Hoffman
You're looking for /r/playrust. This subreddit is about [the programming language Rust](https://www.rust-lang.org/en-US/).
Thanks 😁
&gt; the benchmark game is seen as s.th. ultimately which it is not Are you speaking about how you personally see the benchmarks game? &gt; benchmark optimized for your special purpose As the website says: [*"Your application is the ultimate benchmark"*](http://benchmarksgame.alioth.debian.org/dont-jump-to-conclusions.html#ultimate-benchmark). &gt; And the game was criticised in the past. If I criticise your work, does that mean there's certainly something wrong with your work -- or could it be that the criticism is invalid ? &gt; … some programming languages allow to solve problems in a much nicer way but were rejected … As the website says: *"Measurements of "proggit popular" language implementations like Crystal and Nim and Julia will attract attention and be the basis of yet another successful website (unlike more Fortran or Ada or Pascal or Lisp). [So make those repeated measurements at multiple workloads, and publish them and promote them](http://benchmarksgame.alioth.debian.org/play.html#languagex)."*
If you want to visit Pittsburgh, or more likely skype or google hangout, I'd be up for it!!
The older version is more complete, but IMO the explanations in the new book are better. The intention is to replace the old book with the new book once the new book is complete. If you're looking to expand your Rust knowledge, I'd recommend working back and forth between the two until you run out of new book :) I'd be most interested to hear from people new to Rust about the new version standing on its own, so it doesn't matter to me if you've read the old one or not. If you happen to find that you like an explanation in the old book better for a topic the new book has, I'd be interested to know that though!
Any other fellow Python programmer having hard time learning Rust? How much time did it take you to start being somewhat fluent in Rust?
Everything will eventually have documentation. It's just a massive, massive, massive amount of work. I'm always looking to help onboard more people who want to help out.
That's not really a coercion, it's a re-borrow. It does feel like one though.
IMO common, self-describing names should map well to generic libraries. Multiple implementations of a non-opinionated API would only fragment the ecosystem and waste implementor effort. You shouldn't feel guilty for taking common names; as long as you're open to working with other people to make a what-it-says-on-the-tin library, all you're "taking" is the responsibility of managing the project to make a solid deque, minimax, or interner implementation. Inspired names fit opinionated libraries. Since libraries with different opinions may compete in the same problem space and it's impossible to characterize the way a library is opinionated concisely enough to put into the package name, labelling such a project creatively works well. /Just thinking aloud; I don't have experience running any popular libraries.
Working on a gui for competitive pokemon teambuilding, you select 6 pokemon, and it shows what pokemon counter your team, sorted by most weak to, and with a colored border indicating the ranking of that pokemon. It will also suggest what you can change to improve your team. Very wip, just started trying out gtk on rust, idk how your supposed to build it on windows, but so far linux is fine
I've added link to your article in slog's README. Thanks!
In case of troubles look for my on gitter, and make sure to review "Introduction" and "Getting Started".
that's basically what my second function does, but generalized over all possible positive numeric inputs, not just inputs that are all a 1 followed by trailing zeroes. Would your formula handle "1218" correctly?
Oh hey, I was there for that. Glad to see httparse got patched for you! Hopefully you can try again soon. I might have to keep this link around, so I can remember how absurd this was. I love how you wrote this up; thanks for following up like that! (Sorry for not responding to your PM on IRC; I usually don't lurk over the weekend `&gt;_&gt;`)
Hey misdreavus! Glad to hear from you again! Yep yep, it's working now, just trying to get the parsing bit all worked out. In my initial tests, it's already much faster than snap.py Thanks again to you and the rest of chat!
Used to do Python in 2004-2005 professionally. Since then, a number of other Langs, but mainly Java. I took the scenic route to Rust, writing [lints](https://github.com/Manishearth/rust-clippy) (awesome way to start, because framework handles ownership in 90% of cases), [small libs](https://github.com/llogiq/optional), [(procedural) macros](https://github.com/llogiq/overflower) and two RFCs (third one's in the works). I'd say after about two weeks, I was comfortable writing simple Rust code. At three months I no longer considered myself a newbie and after about half a year I annotated my lifetimes like a boss. 😎
&gt; everything in my launch.json and just left this: &gt; ``` { &gt; Thank you! That solved it. I searched everywhere to see what target meant, and nobody could say that it was simply just the executable program. 
I finally got around to adding logging instead of println statements to my website server. Thanks for writing this out to remind me to do it and how!
Here's a version of Version A that works: [link](https://play.rust-lang.org/?gist=a04ea0e8df2c4255ab2d28cecfd61db4&amp;version=stable&amp;backtrace=0) The trick was to get rid of the explicit lifetimes on 'func'. Obviously, having explicit lifetimes there forces those lifetimes to be the same as the lifetime of 'data_ptr', but I'm not sure why that's bad.
Probably 
Does slog integrate with the log crate? It would be great if slog could install a log handler that would convert every free-form log message to msg="log message".
You can do `write!(&amp;mut sevens, "{}", x)`.
Ah great. I am able to write simple Rust code after 2-3 weeks of learning but I'm still very unsecure about my knowledge of lifetimes, trait objects and more advanced stuff such as macros. I'll just keep on learning.
Wrong subreddit. You should post in https://www.reddit.com/r/playrust/
Yeah, I expanded it manually because I timed `write!` to be slower. However, just now I checked the actual macro expansion and the two are equivalent, and new timings aren't showing the difference I saw before, so I've put it back how it was. Thanks for the suggestion :).
C++ escapes this dilemma by having a hash map that is a bit slower in general, but masks the effect of poor hash functions better, whereas the Rust HashMap doesn't try to do so, instead offering superior performance with good hash functions. The upside for C++ is that those poor hashes are easily calculated, which makes the whole benchmark faster than a well-chosen hash function with Rust's HashMaps. The problem in the real world is that those poor hash functions are subject to DoS attacks. So C++ has worse real-world performance, but better microbenchmark performance.
I haven't tried that, but apparently there's a [slog-stdlog](https://crates.io/crates/slog-stdlog) crate with [an example here](https://github.com/slog-rs/slog/blob/34b248ea869769311e694bf7f8be6f76fd708d0c/crates/stdlog/lib.rs#L140).
Don't fret – lifetimes will come up soon enough, and if you get stuck, ask here or on IRC. We're here to help. Also you don't exactly *need* macros to write Rust; but some things get easier with them.
_**We need to go deeper!**_ use std::fmt::{self, Display, Write}; use std::mem; pub enum Alignment { Left, Right, Center, Unknown, } #[allow(dead_code)] struct FakeFormatter&lt;'a&gt; { flags: u32, fill: char, align: Alignment, width: Option&lt;usize&gt;, precision: Option&lt;usize&gt;, buf: &amp;'a mut (Write+'a), curarg: std::slice::Iter&lt;'a, ArgumentV1&lt;'a&gt;&gt;, args: &amp;'a [ArgumentV1&lt;'a&gt;], } #[allow(dead_code)] pub struct ArgumentV1&lt;'a&gt; { value: &amp;'a Void, formatter: fn(&amp;Void, &amp;mut std::fmt::Formatter) -&gt; std::fmt::Result, } enum Void {} struct CountSevensWriter(pub usize); pub fn naive_count(haystack: &amp;[u8], needle: u8) -&gt; usize { haystack.iter().fold(0, |n, &amp;c| n + (c == needle) as usize) } impl fmt::Write for CountSevensWriter { fn write_str(&amp;mut self, s: &amp;str) -&gt; Result&lt;(), fmt::Error&gt; { self.0 += naive_count(s.as_bytes(), b'7'); Ok(()) } } fn main() { let mut sevens = CountSevensWriter(0); { let args = &amp;[]; let mut formatter = unsafe { mem::transmute(FakeFormatter { flags: 0, width: None, precision: None, buf: &amp;mut sevens, align: Alignment::Unknown, fill: ' ', args: args, curarg: args.iter(), }) }; for x in 0..100000000u64 { x.fmt(&amp;mut formatter).unwrap(); } } println!("{}", sevens.0); } I'm getting pretty close to the speed of iterated division. Not sure if there's anywhere further to dig to though...
May I suggest you put a link to introduction and getting started directly in the README ? Maybe directly after the "Features" section ? To tell you what I did when I landed on the slog page, I read the features, look quickly at the screenshots, and then clicked on the first "help" linked, which is a link to the example code. I quickly look at the code, but found it confusing because I had not the "mental model" in place. And then moved over (I know short attention span). I think it is important to have the first Get Help link as simple and clear as possible. Maybe it is easier to get the example immediately if you are familiar with other logging utilities like log4j, but I am not. So having a gentler introduction might help. 
Reversing the string at the end would be more efficient than the "prepending" approach, but still not optimal.
I'm thinking about pine, fir and ash... I want to pick a short and easily spelled one :)
`?` is already stable, so it's not affected by this RFC in that way.
My biggest gripe with library docs is they provide a couple of examples at the beginning of the docs and maybe in the repo. Explain how and where something should be used within the API. Use example code everywhere you can. It doesn't have to be a full example, just a snippet perhaps from a full example. If it's a snippet don't let the full example go to waste, give a link to it to provide the user more oversight. *That's* what documentation is for; oversight! Documentation should give you enough oversight and understanding so that you can become a master in 2 weeks with a bit of experimenting, not 2 months of experimenting!
Thanks! Is there a way to do this for very large arrays, like with a macro? (I'm trying myself). EDIT: [Apparently there is](http://stackoverflow.com/questions/28656387/initialize-a-large-fixed-size-array-with-non-copy-types), but it's unsafe.
Hello, I know this isn't a technical Rust question, but I think it still applies. Does anyone know of any open Rust programming positions in London (or around) ?
MaidSafe is based in Scotland and has an open position for a Rust engineer. It's not exactly close to London but the position can be remote and being in the same time zone is definitely a big plus. 
Clippy doesn't seam to help in this case. Output is pretty much the same as compiler. Thankfully some humans came to the rescue!
Great stuff, tnx! Lifetimes are the bane of my rust journey. Since I don't understand it, my approach to it is the good old trial and error method. If I had one wish for the rust compiler, it would be the ability to elide all lifetimes, thus making lifetime notations obsolete.
[Pine's also taken](https://en.wikipedia.org/wiki/Pine_(email_client%29). As is [elm](https://en.wikipedia.org/wiki/Elm_(email_client%29), in case you're wondering :).
I recommend using filter: https://play.rust-lang.org/?gist=6461940e3dd36c188cc52ff1d79be39d&amp;version=stable&amp;backtrace=0
Oh, so you first create a pull request, and then comment on lines within that, is that correctly understood?
I would like to know how to convert `Vec&lt;Option&lt;T&gt;&gt;` to `Vec&lt;T&gt;` (containing only the Some values). In Haskell there is a function called [catMaybes](http://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Maybe.html#v:catMaybes) and I'm quite sure something like that exists for Rust too, I just don't know the name of the function.
This week, and next week, and ...: [Advent of Code](https://github.com/birkenfeld/advent16) :)
Do you mean the [Extended Enums](http://smallcultfollowing.com/babysteps/blog/2015/10/08/virtual-structs-part-4-extended-enums-and-thin-traits/) proposal?
No, you appear to have misunderstood me. C++ optimizes for weak but fast hash functions, which helps winning the k-nucleotide benchmarks but in real-world applications may hide the effect of said weak functions until your app gets HashDoS'd. On the contrary, Rust performs worse with weak hash functions (which is why we've not simply copied C++'s approach), but has better performance with stronger hashes, which you *should* be using in any real-world application. Note that even the FNV hash which the Rust k-nucleotide entry uses has undergone extensive statistical testing and deemed fit for medium-sensitive applications. Together with the per-map salt, it provides a reasonable performance-safety tradeoff for shorter keys. 
Ok - [this was fun](https://github.com/birkenfeld/rulsp). One of the important things when optimizing is to avoid allocations. So: * Replaced `Vec&lt;AtomVal&gt;` and `&amp;Vec&lt;AtomVal&gt;` by `&amp;[AtomVal]` to save allocations of Vec. * Replaced `Vec`s in the int ops by iterators. * Replaced `String` by `Rc&lt;String&gt;` in symbols to avoid cloning the symbols on env hashmap insertion. * Made `Nil` a thread local `Rc` value which can be cloned instead of allocating new `Nil`s. * Avoided allocating a new Symbol each time when checking for `&amp;` or `recur`. * Removed the unnecessary return values from `env_set` and `env_bind`. Also, when dealing with `Rc`, `clone()` is relatively cheap but passing references still beats it. So: * Replaced `Env` by `&amp;Env`, `AtomVal` by `&amp;AtomVal` where possible. * Removed a lot of other `Rc.clone()` calls that were unnecessary. Odds and ends: * Replaced `unwrap_or(c_nil())` by `unwrap_or_else(c_nil)` to save function call for the Some case. * Reordered match arms to put more common cases first. * Idiomatic code: `map(|x| x.clone())` -&gt; `cloned()` and so on. * Added a few `#[inline]`s which I was surprised did something. * Made `c_symbol` take a `&amp;str` for pure convenience. That got the `100000` count case from 1.3 seconds to 0.6 seconds. Ah, and `[profile.release] lto = true` can squeeze out another 10%. Allocations were down from 295k allocs/22M bytes to 91k/11M. (Note: to have useful allocation statistics data with Valgrind, use the `alloc_system` crate.) After all that work, I discovered the redefinition of `+` in `core.clrs` - commented it out, and got from 0.6 to 0.07. :D (Only twice as slow as Ruby, which is pretty good I think for the amount of code that is interpreted.) 
Perhaps Maple ;)
Willow?
I was aware that there're definitely too many allocations but I would have never thought there would be SO MUCH difference. https://media.giphy.com/media/EldfH1VJdbrwY/giphy.gif Thank you /u/birkenfeld!!! I'll shamelessly go through every point and try to implement that stuff before looking into the fork. :) &gt; After all that work, I discovered the redefinition of + in core.clrs sorry about that! I was playing with avoiding having to iterate through all args to add something in Rust and instead move that stuff to lisp but now I know it only made it even slower... Once again. Thank you and appreciated.
I wrote some quick code: https://is.gd/utuKxQ `get()` method returns `Ref&lt;&gt;` so the `RefCell`'s ownership tracking works properly.
Instead of rewriting the getters/setters on every struct that needs lazy fields, why not write a `Lazy&lt;T&gt;` type that takes a function and evaluates it when you first try to get its value (I'd do this with a method of the type, but you could also implement `Deref` if you don't mind hiding non-trivial execution cost behind derefs). EDIT: Looks like [lazycell](https://github.com/indiv0/lazycell) could be a useful basis for this
Can you please explain how this relates to Rust?
Nice! Rust needs more early, concrete examples like this for beginners. Fizzbuzz might be a good place to teach people to use `use MyEnum::*;`, to bring all the variants of an enum into scope. You can chose to use it inside certain functions if you don't want it throughout the module.
new to rust as well, but wouldn't it be a better idea to take a String as an argument for your read line function?
Is the only possible real-world application of a hash table, one in which the app is exposed to be HashDoS'd ? &gt; Rust performs worse with weak hash functions (which is why we've not simply copied C++'s approach) If someone contributed a Rust k-nucleotide program that simply copied C++'s weak hash function, we could see how much worse. (The show part of show &amp; tell.)
So a mutable string as an argument, and the read_string_from_io function would borrow and edit it? So it would be the same style as io::read_line(&amp;mut buffer). Am I right? I do have a reason for doing it my way, but you're right (and it would match better with the existing function). 
Not at all!
You should be able to collapse the two RefCell's into one (e.g. via a three state enum like: `Value(T), Intializing, Intializer(Box&lt;FnOnce() -&gt; T&gt;)`), which should help reduce how much overhead there is. I believe this is basically how `lazycell` works.
Strangely, when I try installing ripgrep with the suggested flags: RUSTFLAGS="-C target-cpu=native" cargo install --features 'simd-accel avx-accel' -f ripgrep I get an error while building kernel32-sys – and I'm on XUbuntu/crouton/ChromeOS, not Windows. What gives?
It depends on what you want to do. In this case the implementation is reasonable given the context. Instead of always needing to create a string to pass in it just returns it which can be cleaner. Honestly this is an OK choice in terms of Rust. If this was a REPL though I would do what you suggested and clear the buffer each time. This way you're not allocating memory every time. If it's just a one off function this is fine. op might want to consider returning a result because you might want to handle it differently depending on the place you're using it.
I don't understand your question, could you restate it? What do you think is wrong with doing this? This would just be a convenience API. You can get the same behavior (with a redundant `unwrap`) with the API that LazyCell has today: if !self.foo_cell.filled() { self.foo_cell.fill(Foo::new()); } self.foo_cell.borrow().unwrap()
When I encountered the problem, I wanted a simple function to handle the error(to print and halt) and return a String that the user typed. Yes, this isn't the best code :P Like you said, for a one off case, this should be fine. Thanks for the comment! 
I think it's because I first learnt programming with Pascal (a bloody terrible thing to learn because.. ) and now I see functions as contained units, all the code for it to run should be confined and precise. Making the string before calling the function seems backwards to me. Although, if this is how you're meant to do it, I'll have to change my habits won't I :P It's a good learning experience anyways.
yes (writing this from memory, may not be 100%) use std::io; fn read_ln(mst: &amp;mut String) -&gt; String { io::stdin.read_line(mst) .expect("Failed to read line") let nmst = mst.to_string(); return nmst }
less than how you're meant to do it, more what I'm used to. Using C and Go up until now has skewed me to write code that way.
I'm gonna make a counter argument, although i know you did say it's not 100%. You're passing the reference mst, but you only use it for the buffer in read_line(), why not create that string when needed, instead of forcing the user of the function to create the string. [Note: If you're reading multiple lines, this would be more efficient because you just pass the same buffer around, yes. But you have the freedom to write good code for the situation, why not do it?] 
As I read into the API I wonder if this is the right solution. My way of seeing a lazily filled cell is that I know what the contents will be when I declare it, I just don't know if I really need that data (and therefore if we need to make the effort to build it). That way a different api would look like: let sloth = lazycell::LazyCell::new(foo::new) let map_of_data = lazycell::LazyCell::new(|| { let mut m = HashMap::new(); m.insert('expensive', expensive_operation()); m.insert('cheap': 'cheap_string'); m }; if bar() then { sloth.borrow().do_stuff(); } else { do_stuff_with_map(m); } Then we'd only initialize the value we use, depending on the value of bar, but we can use the variable set when we specify it.
This would require the LazyCell to store the constructor function, which would add a second type parameter to the type.
Rc&lt;T&gt; and the like impl Deref to make "transparent" all T operations that don't need to take ownership of the T.
I'm not sure that I agree with your decision to create this function, since read_line is already part of the standard library, but I guess that's just personal preference. I would however recommend returning an io::Result&lt;String&gt;. By printing the error and panicking, you're preventing users from handling the error differently.
I'm truly grateful for your help! Indeed, I'm from cpp and not adapt to some Rust traits like Result and Option. I'll take your advice and appreciate your review again.
Unless you really need to satisfy `From`, this operation is usually exposed as a `map` method. (e.g. `Iterator::map`)
Huh. I'm on a Debian computer so I just tried it and got a clean install using nightly `3568be9 2016-11-26`. I updated to `daf8c1dfc 2016-12-05` and now I'm getting errors, though mine are about `OUT_DIR` not being defined; `kernel32-sys` builds fine. Interesting... maybe it has to do with the nightly version?
That is a brilliant article! Being new to Rust, you've explained the concept between String and &amp;str really well, as well as some lifetime topics. Thanks! 
Haha. The JacaScript and Rust ones are great :)
The Rust one indeed got me.
I have to say... C++ deserves a bit more credit (esp C++11 and higher) but I guess it's all about perception. But Haskell and Rust got me :)
You might want to check out [nalgebra](https://crates.io/crates/nalgebra) if you need linear algebra in Rust. You can check the documentation to see what they did if you are still interested in working on your own. Their source has a lot of macros though, so it might be difficult to see what is going on.
This is lovely, I wonder how swift would have been portrayed. 
I think THIS is a great example of the Rust community and how it reacts to humor. Not being offended in any way and equally laughing about their own tools and others. Great! :-)
Yeah I did recognize early on that the bolding wasn't the best way, I'll give it a try on my next one! :)
What's the status of moving capnp-rpc towards futures-rs etc? That would hopefully make integrating it to preexisting event loops easier
I write primarily Python for a living and I look almost exactly like that except I don't have any tattoos and I don't smoke anymore. Also, I'm not blond and don't wear V-necks. OK, maybe it's just the hair do. from comic import PythonHipster class Me(PythonHipster): def __init__(self, coffee=None, cigarette=None): super(Me, self).__init__(*args, **kwargs) self.coffee = coffee self.cigarette = cigarette def speak(self): print("My formatting is my syntax.") Guess I didn't really need to subclass anything. But aren't we all just objects in a global scope? 
I have a short writeup from the Cologne event: https://fnordig.de/2016/11/29/novemb-rs-code-sprint-weekend-2016-retrospective/ We didn't advance that far, but still had a lot of fun. I did not yet hear back from the embedded development folks though. I try to reach out.
I don't get the PHP one. But whatever it means, it isn't mean enough.
yes, much, much bigger. That or we would've had to also distribute some runtime components separately; either way the &lt;64kb magic is lost :)
As far as I can tell this is an issue with specialization. See [RFC 1210](https://github.com/rust-lang/rfcs/blob/master/text/1210-impl-specialization.md) and [Issue 31844](https://github.com/rust-lang/rust/issues/31844).
I just spent who knows how long reading quite a lot of github issues. Props to all the hard work that's landing this week!
You might be running into the problem that https://github.com/rust-lang/rust/pull/37445 fixes - is the compiler using a ton of memory? I would definitely load that csv file at runtime - I don't think it would be noticeable at all.
The JS one is wonderful. The Rust one seems a lot like my grandfather. I would have preferred C++ with : "Resistance is futile, you will allocate! " that is more appropriate for its look.
Ah, yes, I see how I screwed up the placement of the reference. Thank you for your answer, it has cleared things up immensely. I figured it was one of those "You COULD do it, but you probably shouldn't," things, but I just wanted to double check.
&gt; `copy_from_slice(_)` got faster for small slices Ah not quite; copy_from_slice is not changed, the merged PR doesn't do what its title says.
He's a server, and not a great personality.
Is it actually possible to be as mean as php deserves? or will you overflow some variable somewhere (after checking it against the maximum value of that same type to make sure it didn't overflow).
64kb magic indeed &lt;3
What gets me is the suit of armor, and 3 shields!
The immediate problem is that rust doesn't know enough about the type of `t`. `.collect()` can return a lot of different things, and a lot of different things can be used in a `for`-loop. Maybe you'll make progress if you specify `t`'s type.
Okay, but if I specify the type of t let t:Vec&lt;&amp;str&gt; = l.split(|c| c=='[' || c==']').collect(); I get e new error: the trait bound `std::vec::Vec&lt;char&gt;: std::iter::FromIterator&lt;&amp;str&gt;` is not satisfied [E0277]
Released a PostgreSQL foreign data wrapper that communicates with a Rust library like this https://github.com/komamitsu/treasuredata_fdw/blob/master/bridge_td_client_rust/src/lib.rs. I struggled to get results from Rust in C asynchronously, but it finally works with great performance using std::sync::mpsc::Receiver. There is a large room to improve the FDW itself, though.
The `?` proposal was supposed to come with this `catch!`, right? Or something like it. Then it was dismembered.
Ah, that's what I thought it might be, but it wasn't clear.
Try using iterators instead of explicitly looping and pushing: let ary = input_str .lines() .flat_map(|line| line.split(|c| c == '[' || c == ']')) .map(|substr| substr.chars().collect()) .collect::&lt;Vec&lt;Vec&lt;_&gt;&gt;&gt;();
I don't get half of these.
I spent whole time writing [PN532](https://github.com/Funcoil/pn532-rs).
The issue was pretty interesting. Good read.
I had a job doing erlang for a while, and I loled pretty hard at that one.
I wonder about Lisp
&gt; 404 Here's a [working link](https://github.com/Funcoil/pn532-rs).
At the end of it all, it's just that ridiculous codegen where the compiler is emitting a memcopy of length "&lt;boolean conditional&gt; as usize"... hehe this PR leaves a loose thread to be revisited another time for sure.
https://github.com/rust-lang/rfcs/pull/1808 &gt;Except they actually aren't, since it hasn't been implemented yet (or maybe it only works on Windows?). OSes normally provide a 4kb guard page below the stack, so most stack overflows will crash anyway (and trigger that error, which comes from a signal handler), but a function with a big enough stack frame can skip past that page, and I think it may actually be possible in practice to exploit some Rust programs that way... I should try. Yeah, not implementing stack probes seems like a mistake. This is something people may not realize - I didn't realize it until it was pointed out the other day. From what I understand rust had another way of handling stack overflow before but it was removed due to performance issues, but stack probes were not implemented beforehand. Seems like a significant oversight. Perhaps rust would benefit from a dedicated SDL/ security team built into the process.
Yeah! Thank you very much.
Fixed. Thank you!
/u/petertodd I'd be glad to know your opinion on Rust. How do you like it, whether you created some project in it, etc.
&gt; Couple this with the fact that we don’t really have a common type or trait like Integer to cover all these types (yet another irksome quirk of Rust), covering all possible combinations of Rational number interoperation with integers would be a combinatory explosion! That is bit awkward, but luckily [num](https://crates.io/crates/num) crate has [Integer](http://rust-num.github.io/num/num_integer/trait.Integer.html) trait that eases the pain.
What brought D down, at least for me, was a lack of focus. They had two different compilers, two different "standard" libraries, a GC, then an optional GC, now they want to have no GC in the standard library but also to still have a GC, and they're not even done yet because half the standard library is GC'd and half isn't, and you can't even tell which half without reading the docs or disabling the GC and looking for breakage. Here's [Alexandrescu explaining the issues with D and comparing it to Rust and Go](https://www.quora.com/Which-language-has-the-brightest-future-in-replacement-of-C-between-D-Go-and-Rust-And-Why/answer/Andrei-Alexandrescu) from the perspective of one of its lead developers and evangelists - it's a really interesting read.
Note: Never used D, or even looked at it that much. D tried to be more usable than C++. And from what I hear, it largely succeeded. However, Rust tries to be more stable than C/C++, which is unique in language as close to the hardware as Rust. Additionally, the security and code quality benefits* of Rust make it a better first choice for large software without time constraints. \* Totally my opinion, but it's a lot harder to write bad Rust that actually works than it is to write bad C that technically works.
Was curious as to why the author wrote GCD using recursion, as to my knowledge rust doesn't support tail call optimization. But when I compiled the [recursive version](https://is.gd/uALktN) and an [imperative version](https://is.gd/fRSmJl), I found that they generated extremely similar llvm, both of which use llvm's tail call feature. Yay for zero-cost abstractions! Edit: you -&gt; author
I didn't quite get the Erlang one, can you explain?
http://erlang.org/doc/man/supervisor.html
I agree, even though programming in D was like a whole new world of ideas opened to me. Especially the power of compile time programming, which is, alas, not possible in rust. [This](https://forum.dlang.org/thread/vmjgxzuypcyigtkuyrsz@forum.dlang.org?page=1) current thread on D's forums displays your critique very good, I think. 
Note: I never used D, I just followed it for a while during development. D looked like it took too much inspiration from C++ for my taste. I had a lot invested in C++, and I didn't want to spend lots of time replacing it with "a better C++". I think the thing I like most about Rust is Cargo, as building C++ projects (with dependencies) was such a PITA. If C++ had Cargo, and Rust did not, then I would not be using Rust, even though I think the Rust language is a huge improvement over C++. Edit: Much reformulation.
Explaining each one (I did not understand some too, but after the comments in here I think I got xD): * Python: Always well dressed, and in fashion. (due to the obligatory formatting) * R: For science, since it's the language niche * Java: A Butler, serving you and cleaning the garbage. (garbage collector) * JavaScript: has everything but in an Ad-Hoc way. And the community always likes to remember that JavaScript was inspired by Lisp, and it's usual to follow functional ideas (underscore, react...) * PHP: A "Server", with a pun of "GET" - an HTTP verb and in PHP you have easy access to GET parameters. Also, wants to please even when it lacks solid design/knowledge (the waitress is sweating) * Haskell: Pure and mathematical, similar to the "divine". * Perl: An explorer, out of fashion. "Interesting times" theme, looking for adventure. Also, the community called themselves "Perl Monks", maybe there's a monastery at the top of the mountain ;) * C: Old, and in favor of manual allocation. * C++: C, but with new implants. * C#: Modern, and powerful. (with Unity T-Shirt, the game engine that made C# popularity explode) * Ruby: Young (the girl), powerful and from Japan (Mecha). * Assembly: Raw and the draft that everything starts. * Erlang: The failure treatment in the language is based on letting process crash and die, the virtual machine brings them back from death ;) * Go: A young language, very normal and nothing special, that Google stands like "You WILL love him". * Rust: Takes safety very seriously (3 shields), with chipper brilliant error messages. 
No problem, thanks for your involvement!
&gt; JacaScript In Portuguese jaca is a huge fruit that if you eat too much of, or eat it hot, it will sink your digestive system to hell and turn you into an undead being. But it tastes great in the beginning. Kinda like JavaScript.
One for the static type analysis. One for the borrow checker. One for move checks? Also the haha at the end is just how rust is, both correct/strict and nice!
That's exactly it- when people say "Rust doesn't support tail call optimization," they mean the compiler doesn't ever guarantee it, not that it will never happen. As far as I'm aware, the `rustc`-level implementation just unconditionally generates non-tail-call-optimized code and hands it to LLVM, which will do the optimization 1) if it can and 2) if it deems it beneficial.
I agree, good interpretation! 
It's notable how even though I'm the maintainer of python-bitcoinlib, I used rust to explain the concept of a fake single-use-seal... Even if all I'm doing is making prototypes of cryptographic systems that I *know* will be thrown away I find Rust worth using because its strong typing and traits system does a much better job documenting what is actually going on than Python does. Frankly my main issue with Rust is just that there aren't yet any Bitcoin libraries for it with good RPC functionality equivalent to my own python-bitcoinlib. But I'm sure that'll get better, particularly if I sit down and write them myself. :) As for actual projects with Rust, I haven't released any yet, but I do have a few things in private repos that'll get released soonish.
I'll challenge the implicit premise: by what metrics do you consider Rust to be more popular than D? Rust undoubtedly has more hype due to being shiny and new, but D is over a decade older than Rust, so I'd be surprised if there wasn't more D code out there in the wild than Rust code (for the moment, at least).
There are lots of articles outlining how Erlang and OTP work, this one is the best I've run across: http://ferd.ca/the-zen-of-erlang.html
&gt; It's not necessarily better in your eyes, but it wasn't an oversight: I'm not trying to assign blame or anything. These things happen. &gt; 'not implementing' it isn't necessarily as an active a choice as you make it sound. Perhaps we should have morestack until then? &gt; https://www.rust-lang.org/en-US/security.html This covers vulnerability disclosure, which is awesome (although what I'd really love is to see rust featured in the moz bug bounty program, which has been an excellent program), but it doesn't formalize any kind of SDL.
Hey. Rust is on the list. A lot more than you can say for many languages.
To *me* it's that D just didn't seem like it improved enough over C++ to be worth the switch. D is better than C++ sure, but there's a huge cost to switching languages and I better get a ton out of it if I'm going to take that risk, and D just doesn't seem to do that. The struct/class division, the hard-to-avoid GC, the null pointers, all seemed to taint the experience enough that I'd rather just stick with C++ and not take the risk of using something new. Rust seems better in this regard. You genuinely get something *much* better if you make the switch. The biggest issues I have with Rust are the lack of built-in (i.e. special syntax, special compiler optimizations) async/await style constructs, and the borrow checker being a bit overzealous. For the latter, I'm not sure how to fix it. One option would be to add a GC that you can drop in whenever lifetimes become hard to structure in the way the compiler wants you to. If you did a concurrent or incremental GC (a la Go) to keep latency down it may not be a big deal, and since the language has robust support for non-GC memory management it would hopefully bypass almost all of the GC issues in the first place by just using it a few orders of magnitude less. You'd probably also have a per-thread GC (with a global one for things that need to move around), which further helps. 
Thanks for the summary /u/japaric, this is exactly what I was looking for. I've done embedded development spanning the whole range of what you described, from non-realtime application-level systems on Cortex-A cores to hard realtime control systems on bare metal. To me, Rust is already pretty easy to use in the application-level scenario with cross-compilation and remote debugging being the only real system requirements in addition to standard desktop development. At the bare metal/RTOS level there's still a lot of work to be done on abstractions and APIs. I think with the right APIs and interfaces we can get an equivalent or better level of abstraction than C++ while maintaining similar performance/overhead to C code for embedded applications. Using Traits for Rust implementations can also make implementation for new platforms much less onerous than current approaches. In particular, I think we need to consider how to organise the embedded Rust community under a single banner. It would be useful to have a way to "tag" bare-metal compatible crates which only depend on other bare-metal crates. We need to increase the discoverability of these embedded-compatible crates for embedded developers. The approach for platform support also needs improvement; per-platform support crates are a good start, but like the previously mentioned "embedded crates" they really need to be more discoverable and ideally there should be some way of marking which platform(s) they support, kind of like the existing platform/feature flags which can be used to switch code on/off. As you say, the most pressing concern is code reuse, and having standard "embedded" traits for core utilities and peripherals. It would be nice to see `num` traits for embedded systems, embedded `Iterator`s, embedded data structures, etc. ported from `libstd` to a `libmetal` that doesn't require dynamic allocation or any dependency on libc, kind of like [ETL](http://www.etlcpp.com/) does for C++. I currently don't have time to work on any of this, but once my masters thesis is submitted and I'm working again I plan on devoting a few hours to sitting down and trying to achieve some of the above.
Can confirm. Work at enterprise. Use Java. But I wear colorful shirts so I feel a little better during the day...
That was a great read, thanks a lot!
How did you like it? I've gotten into elixir and I'm loving it.
&gt;Ruby: Young (the girl), powerful and from Japan (Mecha). The girl is Ruby, the Mecha is Rails. There is even arrows. Signifying that Ruby is what it is mainly because of Rails.
Additionally: I feel like the split between `nostd`/`std` code could be as significant for Rust in the embedded space as the GC/no-GC split ended up being for D in high-performance programming. It needs to be easy not only to write `nostd` code, but also to assert (at compile time) that none of your code or its dependencies drags in `libstd` or dynamic allocation. All the better if this can be asserted by the dependency management system (cargo). There needs to be a significant amount of code available which is usable/useful in the `nostd` scenario.
I'm working on a library called [`duct`](https://github.com/oconnor663/duct.rs) that makes this easy. It's not totally stable yet, and not documented (though it maps very closely to the [Python version](https://github.com/oconnor663/duct.py)), but it works today: #[macro_use] extern crate duct; fn main() { cmd!("echo", "hi").stderr_to_stdout().run().unwrap(); } The "right way" to do something like this, which `duct` is doing for you under the covers, is to create a double-ended OS pipe and pass the write end of it to both stdout and stderr. The standard library's `Command` class supports this sort of thing in general because `Stdio` implements `FromRawFd`, but unfortunately the standard library doesn't expose a way to create pipes. I've written another library called [`os_pipe`](https://github.com/oconnor663/os_pipe.rs) to do this inside of `duct`, and if you want you can use it directly.
Not enough emphasis on this part. &gt; two different "standard" libraries Exploring different programming languages is something of a passion of mine, and this made D so incredibly frustrating to use. For a very long time, this made certain combinations of libraries impossible to use together in an ecosystem that was small enough to lack alternatives. I remember at one point trying to GUI up program and realizing I'd have to switch the standard library of the rest of my code.
Ah, true! Hadn't noticed it, but it's definitely that! Red hair (ruby) xD
Basically mostly what everyone else is saying. D primarily suffered from two major issues, the most damaging of which was a series of significant rifts in its community and user experience. The first rift was the proprietary vs. free compiler. So right out of the gate you've got confusion about which compiler to use. Then once that was mostly resolved you had the big stdlib split where half the community used one standard library and the other used the other one and neither was compatible with the other. No sooner had that been resolved then there was the split about GC vs. no GC vs. optional GC. So yes, D has a very disturbing track record of coming off schizophrenic. Aside from that D also lacked a compelling story. By that I mean it sort of walked the middle ground between C++ and C#/Java. C++ has always been something of a train wreck of a language, but when used carefully is still largely safe if a bit cumbersome. Additionally C++ has an awful lot of momentum and network effect behind it. On the flip side, if you care more about the ergonomics of the language and not quite so much about performance C# and to a lesser extent Java are quite compelling. D tried to be a more ergonomic C++, without getting bogged down performance wise the way C#/Java did. The problem is that it was only slightly more convenient than C++ and sat somewhere in the middle of C++ and C# performance wise. It wasn't a big enough performance improvement over C#/Java to justify swapping to it from either of them, nor was it enough of an ergonomic improvement over carefully written C++ to justify migrating from it. Rust in comparison isn't looking to be a better C++ (although it arguably is), but rather primarily aimed at C. It quite clearly stated at the outset that it was targeting systems programming; that is bare metal, embedded, and low level use cases, as well as high performance scenarios. Importantly it isn't trying to replace something like C#, the kinds of scenarios you're likely to reach for Rust are ones where you literally can't use C# due to the runtime library. Additionally Rust has a clear advantage over C and a slightly lesser one over C++ in terms of type and memory safety, which provides a compelling reason to pick it over either of those two. In other words Rust has a very clear niche it's aiming for and provides compelling reasons why you would want to pick it to fill that niche vs. your other options. 
Thank you very much for sharing! I like the type system as a documentation too. :) Of course, lack of libraries is problem of all new languages. I wrote some tx deserialization code myself but mostly to learn more about how txes are serialized. I wanted to publish it with some blog post, but didn't have time to finish it...
Check the "Call for participation" section of the weekly [This Week in Rust](https://this-week-in-rust.org/blog/2016/12/06/this-week-in-rust-159/#call-for-participation) magazine.
The intent has always been to implement stack probes in LLVM but it's fallen off the radar. It's indeed embarrassing that we haven't followed through, so patches would be quite welcome. Technically it's not very difficult. The main problem is convincing LLVM to accept the feature. It's conceivable we could carry a patch out of tree but it really does need to be upstreamed so that e.g. linux distro's can benefit. The old stack overflow protection was also not well supported in LLVM - it worked on x86 on the popular platforms.
Python and Rust are my favorite p.languages. What's your opinion on optional types in Python? I liked the new typings coming to python. I think it softens a lot of dynamic typing negatives.
Not here, it isn't :)
&gt; The intent has always been to implement stack probes in LLVM but it's fallen off the radar. The problem is that in the time since the old feature was disabled and until the new feature is enabled users are *vulnerable to stack overflows. In combination with another bug this may lead to an exploitable vulnerability - particularly relevant in the Alloca discussion, I think. The old feature shouldn't have been enabled until the new feature was enabled, in my opinion. The question now is what to do about it. To me this seems like a big deal since rust prioritizes memory safety. I think it would bother me if it took a CVE or proof-of-exploitability to deal with this (but I have a feeling that that's what will happen at this point even if work were started now). *edit: To be clear, I don't know the direct exploitability here. In the presence of alloca it seems much more scary, without it it may require a second bug or the presence of unsafe. Maybe /u/comex will be able to answer this.
I never used D but my guess is mostly that safe Rust is perfectly safe.
You're welcome! I hope you keep having fun with Lisp and Rust :)
That looks pretty good to me :) what is the irc channel?
I'm building with dependencies that require a couple of environment variables be set for a cargo build to succeed. Is there somewhere to apply that via cargo (maybe in a local cargo .config file)? I've scanned through the cargo docs and haven't found anything that jumps out at me. Right now I have a makefile that sets the env variables and calls "cargo build", but it seems there must be a better way? The specific dependency is openssl on macOS Sierra, I need to setup paths to the openssl include and lib. But I think in general it would be nice to know if cargo can apply env variables into the build environment.
If you want something very simple to get your feet wet, you can help me finish [humanize_rs](https://github.com/LeopoldArkham/humanize_rs)
I'm usually in #clap-rs on IRC.mozilla.org although I'm not always at my computer. I forgot to mention clap-rs on Matrix as well! I'll get notified even on mobile via matrix and gitter, in case anyone needs a more time sensitive response ;)
Yeah I think I will help you :)
Thanks, I just take a look at all issues and I have some difficulties to find myself in all that code. I will need some time to understand all of that
Follow I-unsound and I-wrong in the issue database and there are many soundness bugs. It's how it is, it's an expressible low level programming language, and there are still bugs in rustc.
Yeah, I'm aware that there are others, though I don't recall the severity of them in regards to direct exploitability.
Absolutely, that's why I wouldn't recommend just digging through the code from scratch ;) If there's an issue that interests you or you want to work on ping me or make a comment on the issue and I can walk you through which parts of the code may need to be changed or what may need to be changed.
I think you'll find that: assert_eq!(Rational::new(-1,-2), Rational::new(1,2)); will fail. 
THIS so much. I was a big D fanatic for a while because I wanted a statically-typed, compiled, garbage-collected language. (With decent generics, which rules out Go. :p) But when you start using D, you find that some of the library bends over backwards to avoid doing allocation, even though D is supposedly garbage-collected, making it a bit of a pain to use the APIs. (IIRC I ran into this when using the Range "API".) I was initially turned off by Rust's verbose syntax, but when it finally clicked that I didn't actually want a *garbage collected* language (I just wanted a *memory safe* language), the fact that Rust had a single, consistent way of dealing with memory made it win out over D. Add to the fact that I find Rust's traits a lot better than D's magic UFCS, hand-wavy-compile-time-duck-typing nonsense. 
Damn, if only I had known. I have a tessel2 lying around together with the RFID module and want to have it working in Rust as well. I hope I can make use of your library soon.
Thanks! That seems like it would work just fine with my side hobby project, but for the general case does that induce maintenance to support multiple build environments? I was sort of looking to specify a local .cargo/config file, or equivalent, because it seems like baking the location into build.rs would require changes for anyone who has a different dependent library path than mine (or a lot of resolution logic into the build.rs)? Maybe I should try specifying a build.rs via the .config file?
Another metric, companies which are actively advertising usage on the language's community page: https://dlang.org/orgs-using-d.html (I count 17, and recognizable names include Facebook and eBay) https://www.rust-lang.org/en-US/friends.html (I count 48, and recognizable names including Dropbox, Mozilla, 3DR, Canonical, Chef, etc) Of course other factors might influence this measure. Rust community members may have more actively solicited known commercial users to participate in their "friends" page. D users may be using it for projects which they don't want to advertise, or may be in general quieter, or place less importance on convincing others that it's a good tool to use. EDIT: On another metric, D is perhaps "winning": https://github.com/search?utf8=%E2%9C%93&amp;q=language%3Ad shows 4,377 repositories https://github.com/search?utf8=%E2%9C%93&amp;q=language%3Arust shows 2,492 repositories Of course, D has been around for longer. But I can't really be bothered to actually calculate the "repos per year" metric in any meaningful way :).
In addition to the other reasons mentioned *(GC, stdlib, focus,...)* D felt at the same time cleaner and more elegant than C++, but also dirtier, more hacky than C++. To pick an example for both, improvement and urkh, let's talk about compile time evaluation. D's compile time evaluation is ridiculously powerful. Where you need a gazillion `constexpr` and specially designed algorithms or loads of `template` black magic just everywhere in C++, or where you need nightly and `const fn` with many restrictions in Rust, D just gets shit done silently, no special annotations needed. Thus, this feature is used just everywhere. And I mean everywhere. And now comes the downside called `mixin`. Imagine you can do `include_str!` together with compile-time string processing in Rust - No compiler plugin cheating! This is basically what `mixin` does. Take a string and insert it at compile time as more D source code. And this string can be built using compile time evaluation. In D, you implement operators using this. You build a generic function `binOp` or whatever it was called, which takes a string as a template parameter. In that function, you use this string as the binary operator. This looks something like this *(Pseudo code, I haven't used D in ages)*: int binOp&lt;string op&gt;(int other) { mixin "return this" + op + "other"; } People even use `mixin` to e.g. generate CRC lookup tables. *"But wait!"*, I hear you saying, *"Isn't this pretty damn cool?!"* Yoah, somewhat, except that it isn't. At the time I learned some D, it already felt too dirty, but now with Rust, I understand why. `mixin` is just this: strings. Rust's macros on the other hand follow a strong type system. Whatever your macros spit out will always be valid code. There are many more dirty things in D, like `static if`, and there is even more to just `mixin`, but I'll leave it with that example. *(And I don't quite remember all the other details.)* **Edit:** I just remembered those gazillion tons of WTF operators, like 89736756 operators for array indexing and slicing, or those 123235668 special WTF operators for floating point WTF.
Why would you want one?
For me, there was nothing in D that made me think "Yeah, I would want to use D in place of Java or C++". It was a cleaned up C++ and that was pretty much it. It also did not help that there was no standard and for quite some time, D was closed source. Even now it is source available and not truly open source, that tends to kill innovation (see Ada). D came to the table too late to be a closed source language.
Always think how your users are going to use your API, e.g. in your case do you want them to write `rect.2 - rect.0` or `rect.x2 - rect.x1` to get the width of your rectangle? The former is a tuple, the latter an aptly named struct.
Note that this may be quite hard to actually exploit/weaponise in reality: it relies on having a large stack frame where only things at the end are accessed before the exploit occurs, as Rust has a guard page. Of course, experience shows that any sort of undefined behaviour is likely to be exploitable with enough effort.
I think it would be a great proposal, and I think it might actually happen. Like TypeScript, where the community writes/generates type info for library interfaces.
D was not different enough from C++, and many of it's advantages went away with C++11 and the evolution of the Boost libraries. That is just my take on things. What is the point of a language that both has a garbage collector that is hard to get away from AND can still easily leak memory? D's syntax was more of just cleaned up C++ syntax and less new+awesome features that programmers crave.
/r/playrust
Thank you
It seems like much of this is just going to take time to mature, but it certainly sounds like everything is either in progress or planned. &gt; Iterators are usable in no_std code and some of the num traits should work in no_std environments, I think. My understanding was that since the [Iterator](https://doc.rust-lang.org/std/iter/trait.Iterator.html) trait is in the `std` crate, it isn't available in `no_std` environments? 
Yup, it was supposed to, but the design was never finalized for this (there were multiple ideas in the RFC), and as far as I know, it's never going to be implemented. Especially not the catch match, since that'd break the idea of rust being an expression-based language I'm guessing
Definitely. There are plenty of reasons while there might be way more D code in actual production (&gt; 15 years of D, and being less Open-Source centric which does not expose the true scope). Might be there's still more D code being employed into production today, who knows. Though it does feel like Rust is popular, people adopt it, talk about and so on. And in business and life: perception of popularity tend to increase the objective popularity. I think it's not really important which language is more popular, as identifying which missteps to avoid that made DLang not really take off as it potentially could. And it seems to me the lesson is "Community, community, community. " (aka "Developers, Developers, Developers" ;) ) * Think about market needs + Don't just improve old ways. Create new ways, tailored to new applications and markets. D as a C++ improvement wasn't big enough fundamental change - a lot of people are mentioning it. Go is tailored into network, distributed, scalable systems and is succeeding, even though it's not half as nice language as D is (IMO). Seems to me that Rust's reliability, security and safety story can easily appeal to embedded and security minded (especially cryptofin) industries and any businesses where safety without sacrificing performance matters * Have a vision and "story" that caters to targeted markets and don't let the community split over technical details. In Rust case, we should work together to make sure main story is acceptable for all users, and we unite over consensus, as opposed to having a lot of incompatible choices. Unity is worth compromises. Example would be async io. Community decided that futures are the way to go, so we should follow. As an author of mioco, I can clearly see that some people might have a preference toward green-threads. Hopefully [coroutines adapter that would let use mioco-style coroutines as `futures::Future`](https://github.com/alexcrichton/futures-rs/issues/223) might satisfy everyone without breaking the ecosystem. Another warning example of fracturing the community would be Python 2 vs Python 3 war. If there's ever a Rust 2, it should come with a great interoperability story, and hopefully we can avoid it altogether, and stick with Rust 1.9999. 
What parts of D's compile time programming do you miss in rust? Not a D user myself, so I'm curious. I've done some pretty intense template hacking in C++, but so far don't miss it in rust.
Not D specific, but integer generics are a huge missing piece. Also, const fn isn't stable yet.
I'm not sure exactly how hard it is - like I said on Twitter, I'd like to try it for myself - but one data point: last night I tried compiling one of the example programs for toml-rs. I picked it to check first because it's a pretty small library, but its parser is based on recursion and trivial to crash with a stack overflow. In the output binary (release mode), there were (IIRC) three functions with a stack frame size of 0x1000 or higher, meaning they could potentially byoass the guard page. I did not verify which, if any, were actually part of the decode path, and the biggest one was like 0x1400, not a lot of slack. Thus it's reasonably likely that none of those functions is exploitable, at least in most situations (e.g. race conditions where the function will crash after writing a bit of garbage below the stack would be really hard to exploit, but potentially doable if you have something like a JS interpreter running on another thread). However, like I said, toml-rs is quite small. If it's the first thing I looked at and there were 3 potentially exploitable functions, I bet a more thorough search of Rust programs could turn up a lot. It makes sense that Rust programs would have a relatively high number of big stack frames, because idiomatic Rust encourages putting structs on the stack. In itself that's a good thing, but we need stack probes. :)
Doesn't Rust somewhat go that way with a "core" library and a "standard" library? Whenever you take this approach, there'll always be someone who generally just wants "core", except "that one or two things from standard".
The difference is that standard is re-exporting core + some extras. This was two, completely different, incompatible standard libraries. With the layered model, it's easy to use no-std and then manually add some stuff from std back in. Things will still work. With the Tango/Phobos split in D, you'd have to pick which "standard library" you wanted to use, and then you'd be cut off from every library which chose the other one.
Wrong sub-reddit. Please delete? 
Iterators [are part of core](https://doc.rust-lang.org/core/iter/trait.Iterator.html), and core is then re-exported by `std` ( as are `alloc` and `collections`)
[removed]
npm described their use of rust as follows: &gt; Replacing C and rewriting performance-critical bottlenecks in the registry service architecture. 
Oh, well that makes it a lot easier :-) Would that mean we would need to run with no `core` in an environment with no heap?
Just to clarify things about some of the features you have mentioned. &gt; without beeing generic or boxing the whole thing This has been fixed for a while now (might not be stable yet) &gt; no CTFE Ongoing, if not complete. You can use const fns (not yet stable) in many places already. &gt; There are no compile time variables This does seem to be a feature that will eventually make it into Rust; though there are other priorities right now. I haven't seen any opposition to it. Hope it happens! (Writing an rfc for this has been in my todo list for a while) &gt; search for "static if" The final plan for macros 2.0 is much closer to much of what D's static if gets you. This will take time to be designed and implemented though. 
The core library is more of a minimal version meant to be easily portable for things like operating systems and embedded development. The standard library has everything the core library has and more but has additional run time requirements.
To be clear, I'm not saying "oh the feature is in nightly just use that", I'm saying "oh the feature is in nightly and will be stabilized soon". Certainly constfn and impl Trait will be stabilized soonish. (Not all nightly features are actively on track for stabilization, but these are). Yeah, you shouldn't have to use nightly to use a feature. I was just saying that these issues with Rust are slowly being fixed and should be gone in the future :)
Yeah, when I first saw his leg day comment I was quite convinced that he hadn't done much Rust programming. Don't get me wrong, Alexandrescu is brilliant and D is an amazing language (omg so much metaprogramming). But I think he's wrong here, and operating off of misconceptions. Rust certainly emphasizes memory management a lot when being evangelized. There's a good reason for this -- memory management that is low level but safe is something pretty radical^1 that Rust brings to the table. But as far as "Rust puts safe, precise memory management front and center of everything" I think that's a bit inaccurate. We don't compromise memory safety with design decisions, but the language design far extends just memory safety issues. I don't see most of the "design real estate" being spent here. ^1: as far as established, production languages go -- this has existed in research languages before
Should have been "ten", I typed that on my phone. :P
Not in the current plans, no. Some of it can be made to happen on top of the model. To be clear, Rust's metaprogramming will never be as good as Ds. The compilation model is just different. But we can come close.
Ah ha!
Oh, I see. No, Rust doesn't have that, and probably won't. Like I said in my other comment above: &gt; To be clear, Rust's metaprogramming will never be as good as Ds. The compilation model is just different. But we can come close.
In the sense that it has an implementation and is generally considered very important, but not in the sense that there's an actual timeline yet.
I think enough people want it and the general sentiment seemed to be of the "we should stabilize this soon" kind. I was under the impression that it is planned to be stabilized soon barring any issues.
No, this is different. "std" is build on top of core. core is full of stuff that doesn't need allocation, etc; suitable for embedded programming. "std" reexports a lot of core (though you can use it directly if you want alongside std? There's no point to doing so though). "core" is not a general purpose standard library, it exists for a specific purpose. If you're doing embedded programming you can't use many libraries that use std, but that's due to a constraint of your platform. This is different from Tango and Phobos, which are different standard libraries entirely, and each library uses one of them, and these libraries may not compose with libraries using the other.
I've had a lot of fun writing a Rust implementation of the GitLab API. Crates.io has version 0.4.0 (had a couple of API changes) which allows many (read-only) queries, for example listing projects, issues or merge requests. I'm relatively satisfied with the API, except how it's working with strings. Without knowing what the best would be, I simply take `String` everywhere. I'm something cleaner might be possible, but I did not have time to investigate this more. Ironically, it's hosted on github and not on gitlab. https://github.com/nbigaouette/gitlab-api-rs/
I'm not sure how you'll get the vars to escape from the build script process.
Yes I compiled with `cargo --release` I'll see what flame can tell me, thanks!
- I understood Java to be represented by a corporate en-cultured salary-man. Often Java is defended for it's utility in the corporate setting, and it's praise is often associated with slightly older developers whose interest is less in languages but in serving the enterprise. They also like the reward that comes from climbing the corporate culture. - Haskell I think you're close, but it's more "enlightened" rather than "divine." - For Ruby. The school girl and mech are certainly a nod to Japan. But I think it's supposed to show that Ruby is a language build on the goal to create a language that would make the user happy because it was both enjoyable and productive to use, which is contrasted with Rails the heavy framework on which most Rails work is done. - Go I also took it as the behemoth Google creating a rather small and friendly language. I see Google as holding it's breath because it would like to force it on everyone but has to let Go speak for itself. Which is a small, well kept, but young languages which is just trying to be a good helpful boy instead of changing the world. - Rust believes a good defense is the best offense ;) It's also a paladin who believes in attaining order through law. Who is here to spread the divine gospel of the holy trinity. Ownership, Borrowing, and Lifetimes.
The Iterator crate is actually [defined in the core crate](https://doc.rust-lang.org/core/iter/trait.Iterator.html) and re-exported in the std crate, which depends on the core crate. Several traits / structs that appear in the std crate are actually just re-exports like the Iterator trait.
I wonder if the js functional bit is also meant in a &gt; yeah it falls apart if an ant farts by it, but technically it works.
Yeah, dependency management is what killed D for me. I was somewhat active in the community and gave up eventually because it was too hard to actually build something starting out. I actually started building a package manager, but I had to build so much of the infrastructure that it wasn't worth it. Cargo rocks and makes me so much more productive, so I've starred a few projects with it. Here's hoping Rust gets decent GUI support by the time I'm interested in making a game!
Nearly every comment here is about some deficiency in D. I am not sure that is what made Rust more popular. In Rusts favor * Backing by Mozilla * No GC * Rigorous Design * Way more programmers now * Security a bigger deal Rust is at the right place, at the right time being developed by the right people in a very open way that only encourages more development. All of these things play into giving Rust even more momentum. The correctness of a Haskell or ML with the speed of C and the some of the best tooling around, the ideas that Rust has brought fourth are here to stay. The thing that displaces Rust has to be better than it, in ways that Rust has already defined. 
Making the value of an integer a bound. In other words, write this function: fn foo&lt;T, const N&gt;() { [T; N]; } (Straw man syntax on the generic, obviously)
You can fill in the type parameter like /u/burkadurka mentioned if you want to take a split specifically and you know what kind it's going to be. Taking any kind of split is harder, because the `Pattern` trait that it wants you to use when you parametrize isn't stable. Often though, all you really need to do with a split is iterate over it. In that case, you can define your function to take any iterator, and passing in a `Split` will [just work](https://is.gd/xP6Pnc): fn print_any_iter&lt;'a, T: IntoIterator&lt;Item = &amp;'a str&gt;&gt;(iter: T) { for s in iter { println!("{}", s); } }
You definitely won't hear me arguing.
Ashley talked about npm experimenting with rust internally and said a news might be coming soon, in a changelog podcast a month or two ago. Didn't know it was coming so quickly.
I'd love some help with my serial port terminal program, [Gattii](https://gitlab.com/susurrus/gattii). I've added some issues, but the main thing I'm doing is working towards feature parity with Cutecom, the de facto serial terminal app on Linux. Note that you'll need a serial device and something to communicate with to do this development.
Yes. It's quite hard to keep up with amount of new crates being released. For notable crates, we do have Crate(s) of the Week.
Ada has had an open source compiler (GNAT) for most of its history now.
Do you have a link? I couldn't find anything with a Google search.
Cargo and crates.io are just awesome and is one of the major point that will make me probably switch from C++ to Rust :)
To be fair to previous commenters, aside from "Security a bigger deal", that list is basically just a rephrasing of all the reasons already stated for why D lost out to Java earlier on. 1. **Backing by Mozilla:** D failed because Java had "Backing by Sun" and, later, C# had "Backing by Microsoft". 1. **No GC:** D failed as a systems language because it had a GC and it was too C++-like to occupy any major applications niche other than the one Java (and, later, C#) had locked down. 1. **Rigorous Design:** In the interval before it was opened up, D suffered heavily from the community reinventing everything in order to get open versions. This made things haphazard and resulted in fragmentation. 1. **Way more programmers now:** D was initially developed by one (admittedly very smart) man and didn't have much of a marketing budget. Java SE had a major company behind it and Sun was *very* good at appealing to pointy-haired bosses..
&gt;jaca You can sometimes find it the States as "Jackfruit"
Its not an issue in the Week 159 - the link on the cargo website is wrong.
If somebody were to demonstrate an actual attack on a piece of Rust software using this, that would force us to fix it real fast. A profitable place to look might be cargo's graph resolution algorithm. It might be just as much work to write the LLVM patch to fix it than to write the attack though.
Did you do benchmarks before/after? There is an instance [where someone noticed a 20% slowdown after this change](https://github.com/rust-lang/rust/issues/37939). So far, nobody has taken up the issue...
This series of article is really great! Thanks for doing them. I started learning Rust very recently, and having this kind of article that exposes to the library ecosystem is a boon! Interestingly, using the `regex` library recently, I used `lazy_static` by following the crate's documentation… I have a better understanding to why it was needed now ^^.
Also Rust supporting ML features 
Procedural macros RFC in Final Comment Period, that's great. I am just curious about the possibilities. Let's say we would wan't to implement interceptor for any trait (including those from other crates). Would be possible to have something like this: #[derive_interceptor(SomeTrait)] struct SomeTraitInterceptor { pub fn InterceptCall(...) {...} } and `derive_interceptor` could get AST or `TokenStream` for the `SomeTrait`to be able to implement it?
Well, why not? :D
Looks very promising! I've bookmarked the playlist, and will certainly watch it. Thanks!
&gt; these tutorials Ah, the Aussie guy, yeah? I've watched his videos, and they're quite nice, but most of them seem a bit basic. Perhaps I'll have to wait for future videos. 
Was it this Episode? http://bikeshed.fm/89
Yes, that is the one! &lt;3
Okay, after having read all the comments in here, I seem to have covered most of the free stuff available. I'll probably take a look at Dmitri's course on Pluralsight as well (as suggested by /u/loneraver), but I have a feeling that maybe it's time for me to get down and dirty with real code. Do you have any suggestions for easy pickings that I can start off with? I am an experienced developer, but in terms of Rust, I'd say that I'm novice-to-intermediate. Thanks!
There was [an RFC for this](https://github.com/rust-lang/rfcs/pull/457) back prior to 1.0, but it was rejected. At this point, users have to guess/already know that their compiler is out of date and install a newer one.
Not sure if you are kidding, but please have a look at /r/playrust
Overall I like it. Less clutter and I don't find it hard to spot them
I'm intrigued, but the fact~~s~~ that ~~? is unstable and~~ still doesn't work with `Option` yet is keeping me from switching. I think it decreases readability somewhat but I think that's just because I don't use it normally and I'm still used to looking for `try!()`. Edit: [`?` is stable now?](https://is.gd/m0843N) When did that happen? You'd think I would have noticed.
It already is! (Instructions in the issue).
Ah, I do remember reading that announcement. To be fair, it doesn't specifically mention that it was stabilized, just that "Rust gained [it]". I remember that at the time I just thought, "Well duh, it's been in the language for a while, that's no surprise." 
I was the user who suggested `truncate()` before I realized that with your original `trim()` it wouldn't work (because trim also trims from the front). But you could use `trim_right()`. As for why `read_line` takes a mutable String and returns a number of bytes is that it can be much more efficient. If `read_line` returned a line, it would have to allocate a new string for every line. With the mutable buffer, you have a lot of possibilities that are easy on allocations: * create a buffer once, and clear and reuse it every time you call `read_line`. Since the string keeps its capacity, reallocation is only necessary when the lines get larger than any previous. * call `read_line` multiple times, which *appends* the new lines (this is where the returned value can become useful). In general, being able to pass in mutable growable buffers is good because it allows you to preallocate a suitable capacity, if you know good values beforehand. A `read_line() -&gt; String` API has no chance of doing that. `println!` equivalent macros for input can be found in the [simple input](https://github.com/oli-obk/rust-si) or [scan-rules](https://crates.io/crates/scan-rules) crates.
He saying "keywords/tags assigned by crate users (in addition to the authors)" which I would like to see as well.
Something which forces you to nightly or build scripts isn't a small dependency. Also worth noting there is a mental overheard cost to introducing any new macros, as knowing rust is no longer enough to know what it's doing. 
Instead of having a second pass of unmarking dependent nodes that don't care about a change, why not just figure out how to characterize these changes and only propagate invalidation along edges that care about them? For the example of adding `mut` to an argument binding of a function, the situation can be generalized as either being dependent for external usage or dependent for internal usage. If you add `mut` to the left side of an argument, then this should only propagate along edges that affect implementation details, such as typechecking the function body. This would then become a property of every kind of AST node; e.g., function argument patterns (names, binding properties, destructuring) are internal, their types are both external and internal.
&gt; Not in stable is equal to not in the language if the resulting product aims to be stable. Well, the only guarantee you want is that any addition won't break previous code, not that addition of new features stops altogether.
*chuckle* Touché.
Yes, it seems it does.
&gt; He saying "keywords/tags assigned by crate users (in addition to the authors)" which I would like to see as well. This worries me since then the crate owner has less control over what appears with their crate on crates.io :-/
I don't see the point, but then I've never liked those constructs in JS or Ruby or wherever, they feel like pointless cutesy. I much prefer a basic assertion construct which can reach back into the asserted expression and display the relevant contextual information à la pytest.
Yeah. On a very quick skim, it's about how the C11 memory model, as currently specified, is vulnerable to pathological interactions with the Power and ARM ISAs to expose places where consistency guarantees don't hold. I'm not an expert, but I believe that rustc lets the relevant bits be handled internally by LLVM, so Rust would be vulnerable too.
As a new Rust user, I think it would be great to know if a crate requires nightly vs stable Rust. Maybe this is more of a crates.io thing than a Cargo.toml thing?
What would it take to get `rustc` to be proven? I have a MISRA spec sitting on my desk, but if we have an automated MISRA compliance checker in the build system they haven't told me about it. Having Rust's guarantees about memory safety, especially in a non- or barely- dynamic system, would (I think) provide a nice means of automated safety checks with formally verifiable behavior. ____ I'm starting out on basic device drivers, so Rust vs C should have very little, if any, difference in compiled performance. I'll only have a small, static amount of internal state and things I'm permitted to touch, and won't create data. Plus the place where I'm working is a research lab, and one of their (side) missions is experimenting with new tech on CubeSats. Those are (relatively) cheap and, apparently, get lost with some frequency, so that would be a good place for trial. ____ I agree completely that it would take a non-trivial amount of effort and proof to convince a significant switch in languages and tools, which is why I would like to start out doing a reimplementation of my work in Rust and being able to demonstrate (if it succeeds; if this doesn't work then I learned something and will continue working in C) equal performance with improved safety and productivity aspects.
I was using "proven" in the management sense rather than the formal sense. (ie. "We're waiting for someone else to risk it first.") However, the fact that there's no MISRA checker you're aware of bodes well for your chances, as does the "experiment with new tech on CubeSats" side mission. To anyone else, what was that project (which originally had a funny name) to build a transitive static analyzer framework thing for Rust? (Since it could possibly eventually be capable of answering questions like "Tell me which of my calls can `panic !` so I can manually audit whether a non-panic-capable implementation is viable.")
I know some of our missions run on top of VxWorks, which is written in C and has a fair amount of POSIX compliance. I would hope the underlying OS is safe. I can't speak for sibling code, as I haven't seen it. But I know we ship both monoliths and modules, depending on how much work customers want to do in-house.
I guess the problem that I have with the approach of waiting for proof is that every vulnerability report also has to be accompanied by exploit code. This means more work for a security researcher to convince the maintainers of danger, and it means that security vulnerabilities will be mislabeled as bugs until proven otherwise. In my opinion, if there is the chance that it is a security vulnerability, the assumption should be that it is exploitable, unless someone has very good reason to think otherwise. Of course, that can't *always* be the case and this is why I think a dedicated security team - people with experience writing exploit code (like comex) - who can make that call would be useful. Someone who can say "This is definitely exploitable" "This is probably exploitable" "This is probably not exploitable" "This is definitely not exploitable" so that the maintainers can prioritize appropriately. &gt; It might be just as much work to write the LLVM patch to fix it than to write the attack though. Yeah, that's a good point, although it's also very different skillsets. I might be able to exploit it, I doubt I could write a patch for it.
There is the more recent [RFC 1709](https://github.com/rust-lang/rfcs/pull/1709), which is still open.
As /u/onuras explained, the link is correct. Maybe it would be less confusing if crates.io pointed to https://github.com/ticki/tfs/tree/master/seahash?
Yes, but it's not on the radar just yet.
Unless you disable overflow checking or use `Wrapping&lt;T&gt;` and `Saturating&lt;T&gt;` everywhere, all arithmetic might panic. Likewise for indexing, unless you use `get` everywhere (that is the name of the option-returning indexing method, is it not?). And allocation. 
Nice! I was looking at the opcodes and wondering why there wasn't a jump. That's when I realized that the load_program had to be it. Anyway, it's not like the spec is crystal clear, it's probably deliberately written like this to be misleading.
I like elixir a lot better, though I hesitate to judge erlang-the-language based on my one experience with it on the job. The code base I worked on did not seem to be following erlang best practices, was written by a few devs who were working on the codebase whilst simultaneously learning erlang, and was a big mess of spaghetti code. :)
I'm not surprised you do. I find elixirs syntax to be much more user friendly just like I did with ruby. I'm hoping elixir continues to be adopted so that I can find a job in it within a couple years.
POSIX has nothing to do with safety. It has to do with being UNIX-like. And VxWorks is not very UNIX-like.
I brought it up solely to indicate surface area, which can correlate with bug risk. I have yet to do anything but read manuals and attend meetings, so I don't know how the actual code will end up looking. It *appears*, though, that we use an interface that looks at least a little UNIX-y. There are ioctl and open/read/close functions and file descriptors and everything is in C, so there's at least surface similarity.
Is it necessary to transitively mark the change? I was thinking you could change the meaning of the "changed' state. In the given example, it would be: 1. `AST(foo)` has changed, mark all **direct** dependencies as *potentially* changed 2. Re-compute each *potentially* changed dependency: - if changed, mark its dependencies as *potentially* changed - otherwise, do not propagate the change 3. When you run out of *potentially* changed dependencies, you're done In this case, it means that whether `FnSig(foo)` is depended once or a thousand times, the run-time does not care. The one difficulty I foresee is avoiding running in circle because of cycles. 
Crate authors and crate users will still be free to use whichever of categories or keywords they prefer :)
Also I've just [created a new issue](https://github.com/rust-lang/crates.io/issues/489) for pornel's suggestion!
No, the codegen is more like C. Specifically, Rust doesn't generate heap allocations that don't originate somewhere in the Rust code.
Does it make sense to integrate this in rusty-dash.com?
I guess that makes sense. Ownership and lifetimes is meant to allow that sort of code gen without having to use traditional functional data structures and garbage collection.
Not this window. My bad.
I guess I'm late to the party, but in case anyone still happens to come here, I created an interactive essay (2 years after writing the blog post that OP shared here) with more stuff on coeffects: http://tomasp.net/coeffects/ 
&gt; To anyone else, what was that project (which originally had a funny name) to build a transitive static analyzer framework thing for Rust? I guess you are referring to metacollect (previously nsa): https://github.com/llogiq/metacollect
You're more chaining procedural code blocks right? The call chains are usually unrolled in rust instead of building up a functional data structure.
Lisp was depicted indirectly through all the other languages
I look forward to Rust having a formal semantics so that useful properties about programs can be proven. :-)
In the meantime I just have the do said proofs manually. Woo!
Yes, it is. We had to do it in class (on C code) and it sucked, a lot. It's kinda doable in some embedded contexts, when you can have guarantees about constant-time performance, but it's never ever fun.
More microcontroller reading never ever hurts. Thanks again!
Yeah, that's the one. Thanks.
What have you tried so far? Are you looking to implement a complex grammar, or something Lisp/Forth-like? If you know some Python, [this](http://norvig.com/lispy.html) is good practical guide for implementing a really basic interpreter. It uses a limited subset of Scheme, wouldn't be difficult to customize, and should be relatively easy to follow along using Rust. I think the hard part about building your own language is deciding what you want it to do, and drawing up the syntax/semantics. That's not really a Rust issue, though.
Wait, what? How do they use crater on crates that only compile on nightly? Isn't compatibility the whole point?
No idea about that, but I do know `std` and `regex` use the technique for searching Unicode tables.
Out of curiosity: were crossbeam devs involved? TL;DR: **Please take with a grain of salt, I'm not an expert. Feel free to correct me.**. When accessing multiple atomic, mixing [SeqCst](https://doc.rust-lang.org/std/sync/atomic/enum.Ordering.html) with other orderings some properties that should hold, can't be guaranteed on Power (and Arm?) architecture. Rust library `crossbeam` was explicitly mentioned as an affected example. The exact properties guaranteed by atomics are part of C++11 standard and were carefully designed to map into what actual CPU architectures provide. The authors of the paper explain the problem, and propose an alternative model RC11 (for Repaired C11) of atomic guarantees that is mostly backward compatible and fixes this and other misteps of original C++11 atomics standard. They explain it, give a proof of correctness. AFAIR Rust relies on LLVM to implement atomics, so there's not much to do about it for Rust devs. The whole industry and CS academia will get to some consensus and implement fixes in compilers, I guess. Unless you're one of the ~~miserable~~ passionate devs who deals with complicated atomic operation sequences, you don't have to worry about it.
Or the forge, though that's harder as it's static.
Crater knows history. Let's look at the output of a crater run: https://gist.github.com/brson/35160eacfc9d4c1a704e (this one is very old, I was lazy and just grabbed the first one I found) Note that "regressions," "root regressions," and "broken" are different things. nightly-only crates would be under "broken", and so not really considered when looking at changes that might break stable code. There are some packages on crates.io that were created before 1.0, and never compiled on stable Rust, so this kind of analysis was always needed.
Someday we'll have good docs for nightly stuff; it requires one of two things: 1. We catch up on documenting all stable stuff, so I can start working on nightly stuff. 2. Someone wants to specifically take the time to work on documenting unstable stuff.
I think you are in the wrong sub. Are you looking for /r/playrust?
[removed]
I'm a lurker here on r/rust, but I feel uniquely qualified to respond as a fellow software engineer in the aerospace industry. I write software that's scrutinized by the FAA at the highest level of safety (i.e. failures result in loss of life). It's written in C. I'm only 2 years in and, while I admire your enthusiasm, this isn't happening anytime soon. I'll stand over with u/ssokolow in wet blanket territory. Rust is exciting to me because I believe it could one day replace C in industries like ours. But don't hold your breath... Ours is a cautious industry, and rightfully so. I personally feel safer knowing the level of scrutiny that our software is subjected to. But the same caution that buys you all that confidence makes it difficult to justify innovations. Switching from tried and true C, which has been an industry standard for decades (with a brief period of Ada), is an impossible sell right now. I will say that the transition will never happen without enthusiastic young engineers like yourself pushing the envelope. I encourage you to share the virtues of Rust with your colleagues, but you must be practical. Decades of inertia is no joke, and software written by companies like ours has the potential to wreak serious havoc.
I like to think of benchmarksgame as a measurement of the maximum possible speed of languages, even if in some cases, significant tradeoffs have to be made in favor of performance. It's a benchmarks "game", where languages compete for maximum possible performance, even if the solution to the problem is not "advisable" in real life. With that in mind, I don't consider the "cheating" solution off the table. If C can get away with whatever it does, I would like to see if Rust can too. I consider allowing these kind of hacks a good thing in a performance-oriented language like Rust. There _could_ be real-life cases where super fast hashing is needed, but safety is not a concern. EDIT: pedantic people: s/languages/language implementations/
I wouldn't consider preventing HashDOS objectively correct. It makes sense for languages like Rust and Swift to try to do it, because they have strong a emphasis on safety (and yet, Swift does not yet have any meaningful HashDOS prevention). But there's something to be said about it being an incorrect default for 99% of situations. I'm also becoming increasingly unconvinced that properly defending against it is possible with open addressing. Too many weird correlations. Also: confused that apparently C++ uses open addressing??? I thought the guarantees it provides around iterators effectively mandates chaining.
The dependency graph is acyclic IIRC, so that last bit isn't an issue.
_NEVER go full Haskell_
xargo can help with the cross-compilation of libcore, but not compiler-rt. `-Z no-landing-pads` isn't the right way to do that anymore, you should use `-C panic=abort` instead.
nice `unique` joke :D
Also see [closed RFC 1619](https://github.com/rust-lang/rfcs/pull/1619), [this issue](https://github.com/kbknapp/clap-rs/issues/740), [this discussion](https://users.rust-lang.org/t/cargo-lock-libraries-and-ci/7698), and i think u/burntsushi has more thoughts :)
Gaisler LEON3 amirite? Used it a lot on ground based Satellite comms stuff.
Another good one: https://github.com/BurntSushi/ripgrep/issues/271
/r/playrust
&gt; (i.e. failures result in loss of life) I'm flying a CubeSat. Failures result in loss of &lt;$100,000 &gt;this isn't happening anytime soon I agree *completely*. I have no intention of trying to get an entire project written in Rust; my goal for the moment is to get a single, relatively simple, component written in both purely as a demonstration. Even if I did get it working, I certainly wouldn't expect it to fly for some time; and I absolutely expect I would have to get a lot of scrutiny on that piece, and then on the build system, done before it would ever leave the property. &gt;I personally feel safer knowing the level of scrutiny that our software is subjected to. I certainly hope I'm not coming across as "the compiler does it for us; to hell with manual auditing" because I'm not advocating that at all. I'm all for manual auditing. But you know what's even better, is when the machine can audit too. `rustc` has better auditing capabilities for safety than `gcc` does, so human auditing of code that has survived `rustc` is more likely to be spent purposefully than is human auditing of code that has passed `gcc`. I just want to see us move past C in my lifetime; preferably in my working lifetime. I'm 23; I'm not holding my breath. But the journey of a thousand miles begins with a single step, and I sure would like to at least find my boots. Maybe look outside in a few months. Stroll around my block in a year. Set out on a short journey in a few. And then maybe walk to Mars in a decade or so. But first I gotta find that boot. &gt;Decades of inertia is no joke True &gt;and software written by companies like ours has the potential to wreak serious havoc All the more reason to move on from C, wouldn't you say? :p But yeah. Wet blanketing is fine. I would never dream of doing something rash like, I don't know, [putting NodeJS in space](https://www.reddit.com/r/javascript/comments/4iogzk/nodejs_on_a_satellite_means_anyone_can_be_a_space/).
The difference is whether to use prime sizes, which offsets the effects of weak hash functions, or use power-of-two sizes, which performs a bit better with a good hash function, but simply cuts off bits, so the effects of weak hash functions become painfully obvious. Which is as it should be, because it will motivate users to employ better hashes.
What a nice writeup. I'd been wondering about k-nucleotide performance for months, since its performance numbers struck me as odd. Good to now know, thanks!
For displaying asserted expression, any macro which wraps whole assertion can be used. Or, alternatively, may read and parse file partially and show that assertion. Currently 'must' shows information by reading file based on source of fail!(), but not position of exact assertion. For example, when assertion failed, it prints error: assertion failed -&gt; tests/failure.rs:10:xx | 10 | val.must_be(2).or(fail!()); | ^^^^ (with monospace, it's under fail! macro call) "1" must be 2 It will be expanded to full assertion on future, and will use syn to parse file. It's delayed because syn does not support parsing expression and ignoring invalid input after that expression. I mean, to parse statements like assert_that!(val).must_be()....; I have to substring exact statement (from assert_that! to last ;), and pass it to syn. For context-sensitive comparison with assert! style syntax, it needs compiler plugin as it needs to run after type inference. Without assert!, it would be easy with impl specialization (unstable) "A".must_be("B"); // uses diff cli style vec!["1"].must_be(vec!["2"]); // uses slice's diff mechanism 'a' must be('b'); // uses PartialEq trait assert_eq!(a, b); // alternatively, but it's compiler builtin macro And, dirty example of trait would be trait AssertEq&lt;Rhs: ?Sized&gt; : PartialEq&lt;Rhs&gt; { fn assert_eq(&amp;self, other: &amp;Rhs); } impl AssertEq&lt;str&gt; for str { fn assert_eq(&amp;self, other: &amp;Rhs) { // diff text } } // do something similar for vec/slice/array impl&lt;T: ?Sized, Rhs&gt; AssertEq&lt;Rhs&gt; for T where T: PartialEq&lt;Rhs&gt;{ default fn assert_eq(&amp;self, other: &amp;Rhs) { // simple diff using PartialEq } } Edit: formatted code
I'm wondering how the aerospace industry (which I worked in for a short time as a student / new grad) got from languages like Coral66 and Ada into C? (Or was it a migration from assembler to C by engineers/projects that never went down the Ada/Coral path in the first place?) 
Saw this on Twitter just now. At least one of the two people working on this works on tor...
Awesome, I had a thread asking about this very thing 2 months back. https://www.reddit.com/r/rust/comments/567tq2/does_the_rust_team_maintain_a_list_of/ I'll edit it with this link, in case anyone finds that old post when searching.
I am 99.9% certain that `std::unordered_map` uses chaining, for precisely the reason you describe. I'll check when I get home.
&gt; What have you tried so far? I've played around with bnfc and LLVM. I've also done a tutorial where I wrote a simple Lisp in plain C. I've also been playing around with RPython from the PyPy project. &gt; Are you looking to implement a complex grammar, or something Lisp/Forth-like? Something a bit more complex ideally. I would like to start small just implementing things like global variables and such and start adding in functions and scoping as I get more experience. Forgive me if I'm not using the correct language, but I'm new to compilers. I want to experiment with building a strongly typed language with garbage collection. I like Python's syntax, but I feel like there is a lot of sugar there which isn't entirely necessary. I'm a big fan of enforced white space, but I feel like that would be difficult to implement. I believe D is similar to what I'm talking about, but I'm not trying to make something super serious. &gt;If you know some Python, this is good practical guide for implementing a really basic interpreter I'll take a gander at it. &gt; I think the hard part about building your own language is deciding what you want it to do, and drawing up the syntax/semantics. I kind of have a good set of ideas in my head already. I'm sure those will change as the reality sets in, but I just want to see what I can get away with.
I'm pretty inexperienced myself, but my understanding is that the US Department of Defense pushed Ada in the 1980s. My guess is that it was slowly abandoned as it became clear that commercial support for C is unrivaled, meaning hardware platforms, software drivers, APIs were all provided in C. [Wikipedia](https://en.wikipedia.org/wiki/Ada_(programming_language%29) seems to agree: &gt; In 1991, the US Department of Defense began to require the use of Ada (the Ada mandate) for all software, though exceptions to this rule were often granted. The Department of Defense Ada mandate was effectively removed in 1997, as the DoD began to embrace COTS technology. Similar requirements existed in other NATO countries.
Really? I thought we were corrosion enthusiasts. 
There is an old saying in China: defeat the enemy without combat. No compilation is fastest compilation for most of us. I asked if sccache can be used to speed up rust compilation a couple of days ago. Happy to see such a discussion :)
Is D used in any big name projects? I don't see any reason why Rust won't continue to take off. "C++ with security" is a really big deal. 
If you're using an identity function for your hashes to win a benchmark -- well let's just say the word pathetic comes to mind.
Why not choose both??
Pathetic or not, it helps to cement C(++)'s perception of 'fast'.
I'm pointing out there's a factual error, I don't think how many collisions there are changes that.
Yes it does, it's an extra cache miss.
(Rust Newbie) I'm using a cargo build script to generate constant definitions from a published standard. In addition to the constants I want to provide lookup tables so the struct constants can be looked up by name or value. The lookup table would not need changed while running so it's all immutable, however it would reference constants defined elsewhere. The current manner I'm building lookup tables is just by generating code that creates a map and inserts values which needs to be created when the application starts. There are quite a few constants (I think ~5000, I hadn't actually counted but total lines generated is ~15k). Would it be possible (and possibly faster) to instead create the table in the build script and serialize it to a file which would be deserialized on startup (possibly using `include_bytes!()` macro)? I suspect it might be but haven't gotten around to learning serde or cargo's benchmark stuff yet. I'm also hesitant to start using serde while it's still dependent on nightly (would rather stick to regularly updating stable and not have to manage multiple versions of rust).
The problem with porting a faster hash map for the benchmark is that "custom" solutions are ruled out. It's not clear from the rules whether a fast hash map library in crates.io would be acceptable as a third-party library. It's a hole in the library ecosystem if a faster but unsafer hash map is unavailable.
Have a look at [phf](https://github.com/sfackler/rust-phf), it's designed for creating hash tables at compile time. For usage on stable, have a look at the `phf_codegen` example. You `include!()` the generated file which gets substituted into the invocation site at compile time like it was copy-pasted, no need for deserialization.
/r/playrust
I fail to see how it's exciting. Compiling the dependencies of one's project is usually done only once. I don't think that's what people are talking about when they complain about long compile times. EDIT: Also with my Internet connection the binaries would probably take longer to download than to compile. 
You're right. Sorry for being defensive here. Will fix once I had enough coffee.
Might be quite interesting for Travis CI and co, though, where having the exact dependencies already built on the machine is not too likely.
You'd pass it as a slice of slices `&amp;[&amp;[i32]]`. There's no way to parametrize over integers in generic types yet. But you'd probably be happier with one of the existing crates for working with matrices. From the top of my head I remember ndarray, nalgebra, cgmath.
Sorry. Didn't read this carefully enough. Thought it was just a cache for crates.io.
[removed]
&gt; Compiling the dependencies of one's project is usually done only once. I don't think that's what people are talking about when they complain about long compile times. It's part of the problem. When I start a project with a non-trivial dependency chain, I have to sit there between 2 to sometimes even 10 minutes waiting for the initial build to complete. And I like to start new projects often for experimentation/fun/whatever. This gets annoying fast. In C/C++, this problem doesn't exist, because dependencies are compiled once, then exist on the system pre-compiled. What I would like to see is cargo locally caching dependencies, so the same dependency with the same configuration doesn't have to be built more than once.
While I basically agree with everything said in this blog post, I do think that the most impactful answer (from a PR standpoint) would be to submit a 'cheating' Rust implementation as well. 
I think requirements w.r.t. drop order are sufficiently rare that it doesn't make much sense to devote language features to it. You can use something [more specialized](https://play.rust-lang.org/?gist=4e284666b1eedfb84fb983771d68c956&amp;version=stable&amp;backtrace=1) than `Option` for more usability.
I think in a safety-oriented language it's good to have HashDoS-preventing associative containers in the standard library. I also accept them to be used by default. But in my opinion this is acceptable in a system language only if the language and its standard library (or easy to reach, commonly used and handy external libraries) offer a very easy to use and short to write way to opt out of that safety and regain the missing performance.
cc /u/igouy wrt. the article --- I think focussing on HashDOS is missing the big point. Defense against a rare, and somewhat obscure attack that doesn't apply to most cases is a fairly weak case. In my eyes what's important is that benchmarks should *generalize*. This benchmark doesn't generalize because it encourages usage of a hashing scheme which would be appropriate in only an extremely narrow set of cases; in most others, including ones directly related but over a slightly different subset of keys, it would result in excessive collisions and *natural* O(n²) behaviour. Admittedly, making benchmarks that generalize is *really hard*, and when you're dealing with a dozen plus languages it's *extremely hard*. If you enforce FNV hashing, you make languages like Python suffer, since they don't support custom hashers efficiently. If you enforce the built-in hasher, languages like Rust seem slow compared to languages like C++, because C++ chooses atrocious default hashes whereas Rust chooses extremely good ones, and languages like C don't even have default hashing schemes! I think a genuinely good benchmark really needs to discourage bad hashing schemes naturally, by making the benchmark itself averse to them. Potentially you could even do what machine learning people do - have a public and a private dataset. If there's enough variation between the two, that makes overfitting a lot harder. Ideally a benchmark should be thoroughly audited for fairness, but doing so requires a lot more time than I think any one person is willing to sink into the project.
That's what I thought after looking a bit more myself. Thanks for looking!
This is a wayyy better than the idea I proposed on IRC awhile ago, which was to have a mesh of rust users who participate in cloud compilation. :)
&gt; My CPU implements the SPARC v8 (IEEE-1754) architecture, which Rust does not target. Do you know how good is LLVM support for that architecture? I think I can easily enable the SPARC backend in rustc then you'll be able to experiment with an out-of-tree custom target (e.g. `sparc.json`)
So, As you said, rustc translates Rust source code (`.rs`) into LLVM IR and passes that to LLVM. LLVM then generates machine code. Nightly rustcs are statically linked to LLVM. In theory, rustc can support any architecture that LLVM supports. In practice, to keep binary size and compilation times of rustc (for development) down, not all the architectures (that LLVM can target) are enabled in the LLVM that ships with rustc. &gt; Out of curiosity, why does rustc need machine targets? If you meant stuff like `x86_64-unknown-linux-gnu` which shows up in the output of `rustc --print target-list`. Those are "targets" and each target carries an [specification](https://github.com/rust-lang/rust/blob/1.13.0/src/librustc_back/target/mod.rs#L198-L350) that tells rustc how to e.g. perform conditional compilation (`#[cfg(..)]`), how to link executables, etc. apart from carrying the [LLVM triple](https://github.com/rust-lang/rust/blob/1.13.0/src/librustc_back/target/x86_64_unknown_linux_gnu.rs#L20) which what controls most of the codegen options.
I don't suppose there's an ability to have rustc dump LLVM IR (similar to how GCC can dump assembly), and then feed that into a system LLVM? I presume that, if not, there's a good reason for it, but I'm still in "curiosity killed the cat" mode.
A false choice; fast and wrong isn't very useful.
There is: `rustc --emit=llvm-ir`. But you still will probably want to specify a target via `--target` because of conditional compilation and what not.
&gt;To me, D is the answer to "What if we could re-invent C++, but have all the features work together well instead of being tacked-on". It's well-designed for that purpose. It also brings way better metaprogramming to the table, among other things. I don't know if I agree with this - I remember a lot of stuff in D felt tacked on - like the GC and nogc. And while Rust may be targeting everyone, it makes quite a few specific tradeoffs and in the direction of safety and zero cost abstractions since that's it's stated goal - which is great ! If D had this kind of focus it would have been a lot more successful, but D is just all over the place and doesn't really do anything significantly better than the rest (metaprogramming maybe). If you want a high level language with a runtime go with C# it's immeasurably more mature, even the rather new .NET core is light-years ahead of some community project. You can still get close to D level of native interop with value types, pinvoke, you can even AoT compile to native code - so it's lower level than JVM in this regard. If you want a low level language with deterministic latency - GC (their implementation in particular) is a huge no-no that just adds pitfalls all over the place which they eventually realized and added nogc mode but that's just fighting with a design flaw after the fact. And even when you go past that the only distinguishing aspect of the language is the meta-programming which is quite nice - but that's not nearly enough to switch from something mature and tested as C++. Rust fixes a lot of issues C++ has out of the box, and then it adds compelling reasons to use it on top of that - even if it's not mature enough yet it has potential to get there - D never had this, it was messy from the start.
Use `_blank`, not `blank`, for your link targets. The current behavior means I click a link, it opens a new tab, I click another link, it replaces the tab I just opened!
It's worse than that for the benchmarks game website though, the guy running it only affects solutions he likes. I've looked at benchmarks that I think I could beat using rust by getting better cache locality, but they wouldn't be 1:1 with the C++ version so I don't know if they would be accepted.
Wouldn't there still be an extra pointer indirection?
25519 is an ECC Crypto scheme. All hail DJB.
Soon https://github.com/ctz/rustls
I don't see this as Rust becoming more reliant on the internet. Cargo isn't losing the ability to compile crates locally. I fully expect that people with limited internet connections will be able to configure Cargo to prefer not to use the global cache (heck, using the global cache might even be opt-in). I also don't think that this will lead to crate authors not caring about compile times. In fact, I think it might be the opposite! After all, in order to prevent DoS this theoretical build farm will have to impose upper bounds on compilation times, which means that crate authors who let compilation times explode will cause their users to lose the benefit of the cache.
&gt; The big difference between RSA and ECC is strength. A 256bit ECC key is considered as secure as an 3072RSA key. I would say the big difference is performance. The small ECC key will have better performance with similar strength to the larger RSA keys. However, once we are in a post-quantum world, ECC is (reputedly) significantly weaker than RSA. There's a lot of skepticism around D-Wave systems, but big players (Google, three letter agencies) are definitely taking them seriously. Post-quantum seems very close, if it is not here already. NIST already has post-quantum recommendations and is urging government agencies to move.
Thanks! This is some really useful stuff. I appreciate your help.
Yes, thus my remark about co-primes. It's unclear to me whether current C++ hashmap implementations use co-primes however.
It should be pointed out Linux Developers raised these concerns in 2014 https://lwn.net/Articles/576486/ That the C11 TSO memory model is junk for actual hardware. 
If or when Rust gets a Gc of its own, I suspect it will probably feel a little tacked on too. On one hand, the designers have been excellent about maintaining future design space, so that later features fit in nicely, but maybe Rust just hasn't had the time to build up tacked on features.
The rotation operation you describe here isn't exactly a pure rotation; members at the front are removed instead moved do the back, and the shifted members are replaced. The problem actually gets a lot easier if you can use a pure rotation - no Default/Copy/Clone/etc ~~bounds are needed because it reduces to a `memmove`~~. For example, rotating a slice of any `T` could work like so [[playpen](https://is.gd/pnMUEQ)]: impl Rotate for [T] { fn rotate(&amp;mut self, positions: usize) { self[..positions].reverse(); self[positions..].reverse(); self.reverse(); } } If you require the version that leaves new valid elements in place of the shifted ones while dropping the elements in front, it seems like you would need multiple implementations for different bounds of `T` (eg `where T: Copy`). It's not that you would have to implement it for every type individually, but you could implement it for most types with those few different bounds.
In this particular case, it's safe to do this for any rust type! To actually answer the question though, you should be able to use specialization to achieve this in cases where it's needed.
Isn't drop order defined to be declaration order right now? It'd be nice if this was clearly documented, and honestly it'd be a lot more intuitive if it was reverse declaration order, since this is consistent with the way things drop from the stack. I've encountered it while trying to use unsafe tricks to own a reference to a thing and that thing in the same struct (obviously while also taking measures to ensure address stability).
&gt; It's not clear from the rules whether… Whether writing a custom hash table specifically for k-nucleotide, and then putting the implementation in `crates.io` and saying *It's a library* would be the same as writing a custom hash table specifically for k-nucleotide? ;-)
I found a [discussion](https://github.com/rust-lang/rfcs/issues/744) in the issue tracker where different people mention relying on the current (unspecified) drop order in their libraries (sfackler, carllerche, ...)
You may be interested in this [issue](https://github.com/rust-lang/rfcs/issues/744)
Really? how do those calls to reverse compile down to memmove? I think it might make sense to just port the simple c++ std::rotate algorithm directly, could save up to 50% of swaps depending on the position chosen: [playpen](https://is.gd/ouBr92)
It is a library to do arithmetic with points on a specific curve. Just like integers, you can “add” points on that curve, there exists a “0”, and for every point P, there is a point “-P” such that P + -P = 0. (This is called a [group](https://en.wikipedia.org/wiki/Group_(mathematics\)).) Traditional cryptographic algorithms, such as the Digital Signature Algorithm and the Diffie-Hellman key exchange protocol are based on a certain type of arithmetic that you can do with integers (not addition actually, but multiplication modulo a prime number). It turns out that the rules above (being able to “add” and “negate”, and having a “0”) are all that is required for these algorithms to work, and it is possible to create similar algorithms that use points on the curve instead of integers. This library is a building block that implements the arithmetic operations, but not the cryptographic algorithms. A note on security: the security of these algorithms relies on the equation “n * x = y” being hard to solve for an integer n. (n is called the [discrete logarithm](https://en.wikipedia.org/wiki/Discrete_logarithm).) In the case of points on the curve you might wonder what it means to multiply a point on the curve with an integer. If you can add points, then you can also multiply with integers: n * P means P + P + ... + P, for n Ps. In normal integer arithmetic, solving n * x = y is straightforward, but for the way integers are used in DSA and DH, we haven’t found a way to efficiently solve the equation on a traditional computer. This is because the best known way to solve the equation relies on factoring integers, and we haven’t found an efficient way to do that on a traditional computer. On a quantum computer we *do* have an efficient way to factor integers, which breaks DSA and DH. But so far, no efficient way to solve the equation over this elliptic curve is known.
Why do you need the path shenanigans? A path, if not absolute, should always be relative to the current working directory already. And `image::open()` supports passing `&amp;str`. Finally, a `to_luma()` method exists on the whole image - although that doesn't allow you to demonstrate a closure :)
have yall heard about nix &lt;:)
I'm also very curious about this. Crates.io (like every other repository) is in a position of trust. For source distribution this is already tricky but source can be audited and they keep old versions around so if anyone does anything wrong at least there's evidence of exactly what happened. Binaries are a lot harder to audit and there is very little to be gained by looking at them, so shenanigans will be much harder to spot. One solution would be to have the crate author upload a build, but then have the crates.io infrastructure also perform a build for verification that the binary matches the source. This needs reproducible builds of course.
I always enjoy seeing new users write about their experience. It helps when I want to write articles targeted at them in a way they can understand. I'm curious about this part: &gt;The interesting aspect is that instead of passing around a reference to a room, I clone it. The room you enter, the current room is a cloned instance of the room in the vector of rooms that makes up the world. Is there a reason you're doing a clone? I'm guessing it's a lifetime issue you've run into?
Every setback is a learning opportunity. I guess I'll have to learn how LLVM's SPARC support is, and maybe how to get a v8 target added, and then how to turn that on in rustc. Or why rustc bundles a static LLVM rather than using system; I'm sure there's a good reason I just don't know what it is.
With *, when `image` 0.11 or 1.0 comes out, anyone who installs your crate will get that new version. Because the Rust ecosystem (or at least the Cargo ecosystem) strongly recommends using semantic versioning, it's highly likely that the APIs in `image` will change, breaking your crate. By specifying 0.10.4, your users will always get the version you've tested against. (I think @steveklabnik1 suggested you use the fully qualified version number because every minor bump in the 0.x series is regarded as a breaking change, just as if it were a major bump.)
To add to the readme: * Only ~70% of the spec is wrapped * The example can only be run on linux with x11 at the moment. glfw-rs doesn't expose low level function at the moment that I need. This is an early release, if anything in the library bothers you please tell me. Also feel free to ask any questions. If the examples crashes for you please open an issue. Edit: I got asked how this would compare to [Vulkano](https://github.com/tomaka/vulkano/) Vulkano is a more "high" level vulkan api that makes Vulkan "completely?" safe. Ash is just a low level Vulkan binding that also takes care of loading.
I am surprised by this comment: &gt; Currently ash can be used without any unsafe keyword. I have looked at a few other c wrappers and it seems this is common practice. But Ash is not particular safe and I am thinking of marking every function unsafe. I would expect any function that may invoke Undefined Behavior (aka, be memory/type unsafe) to be marked as unsafe and others to be safe. I think it would be fine if all functions were unsafe; I'd certainly prefer a safe function to be marked `unsafe` than the reverse.
I'm not sure if this is what you need but maybe a [RefCell](https://doc.rust-lang.org/std/cell/struct.RefCell.html)? It helps and allows interior mutability which is covered in this [great article](https://ricardomartins.cc/2016/06/08/interior-mutability) and I think this might be what you're aiming for possibly. If it's not well you learned something new anyways!
For regular readers of this sub, I'd suggest skipping to around 16:30 for where the Python shows up.
:-) Writing something specifically for the benchmark is an obvious violation. But if some specialized hash map (like the one for Java, which eschews Java generics and hard codes key and value types) motivated by better timings for the test, yet generally useful, showed up, would that be disallowed? As it is, I see this as a deficiency of the library ecosystem. Most of the Rust I've tried out in small benchmarks is in spitting distance of C and C++. 3 times slower is quite odd. 
If "people" misquote out-of-context then the website provides more than enough context for you to quote back to them.
I hope someone goes in and makes something like the `cpython` crate but based on cffi instead.
Yeah go for it! If you have questions about what I did ping me. There's quite a few patterns that could get cleaned up and made better and I'm pretty sure the code does not compile.
Why are you using GLFW? Doesn't Vulkan have its own window management built in?
No. Vulkan core is a very low-level interface to the GPU itself, details of the operating system like window management are outside it's scope. Window management is provided by the [WSI plugin](https://www.khronos.org/registry/vulkan/specs/1.0-wsi_extensions/xhtml/vkspec.html#wsi). Since you can productively use Vulkan without it, I would not argue if bindings for it were in a different crate than Ash.
You're right of course. The example code was meant as a quick rotation example that worked for any `T` because of Rust's move semantics. Removed the bit about `memmove` since it was misleading (and actually on further consideration, I don't know how you'd implement a rotation with `memmove`).
So why use GLFW, instead of using WSI (whether in Ash or in a separate crate)?
Yes, it is just a very thin wrapper around Vulkan. The only reason I used glfw was to make the triangle example cross platform (without much code). But I realized that the rust glfw bindings were missing a few ffi functions.
You're asking for [specialization](https://github.com/rust-lang/rfcs/pull/1210/), which is implemented but not yet stable.
Ah thanks, this is great. I'm giving an 'Intro to Rust' talk next week at work - we're a Python shop so I wanted to show some Rust-Python interop. I may just rip this talk off wholesale.
Will let you know if I have questions after digging in! [I did manage to get it compiling with a few changes.](https://github.com/daviswahl/Rusty/commit/2f68c4cb909dbae4d573668ed4c9b4b7882a697a) Fun stuff so far.
Not a compiler expert, but I imagine your loop is getting optimized away?
I mostly complain "I have two projects that have the same crates.io, why am I downloadig the registry twice" ;)
Added a remove metric functionality to rust-metrics. So I can integrate it with my longshot economic resource allocation vm that is supposed to the align the programs inside it to a user. Next is integrating and putting metrics in various places.
Well, this is what sccache is designed to do. So, all the usual stuff, checksums, etc. Installing rust itself involves downloading a binary from the Rust project, so it's not like this is something unusual. &gt; if I understand this idea correctly, I'd be downloading a binary compiled by some random Rust user anywhere in the world. Not necessarily. Since it's a cache, you could have any number of them, maybe for example the "official" one would only be uploaded by the same build farms we use to produce rustc. But really, these details would be dug into once there's an actual proposal for this as a feature, it's still too early to say exactly.
I think it's already possible using #[no_mangle] and a C ABI.
Have you considered [winit](https://github.com/tomaka/winit)?
=&gt; /r/playrust
Check out ArcadeRS. It covers the development of an astroids like game step by step. Handling input, structuring the program, and dealing with the borrow checker are all covered. http://jadpole.github.io/arcaders/arcaders-1-0
I've found that splitting up the state usually helps, here are a few examples: Case: Running a simulation Problem: Updating each entity requires looking at all the other entities Solution: Split up the state of entities in the time direction - for example, one list for entity positions and velocities, one for entity accelerations. Each time-step becomes two passes, one updates forces and calculates acceleration based on the positions and velocities of the entities, while the other pass integrates those accelerations into the position and velocity. Case: triggering an action Problem: an entity might trigger the "shoot" action - this action does damage to another entity causing it to die, etc. This cannot be done by calling methods, since you'd have to prove somehow that the entities were different. Solution: instead of trying to process actions immediately, add actions to a queue, and refer to entities from actions by ID avoiding any problems with references. The queue can then be processed separately with full mutable access to all the entities. Case: deleting an entity based on certain criteria Problem: this may require resizing a vec which requires mutable access to the whole vec. Solution: use flags to indicate "dead" entities, and clean up dead entities after the fact. All of these cases are simpler facets of a full entity-component-system, something which is used by most modern game engines regardless of the language used. The reason is that each of these cases is actually hiding a real underlying problem: - Running a simulation: in C++ it would be quite easy to access fields which may be modified by the update, resulting in using a mix of old and new fields. This can cause unpredictable behaviour where entities update differently depending on where they are in the array. - Triggering an action: it rapidly becomes impossible to trace the execution flow with direct method calls, and you can end up with unforeseen re-entrancy or broken preconditions that cause use-after-free and other nasty bugs. - Deleting an entity: iterator invalidation bugs!
&gt; I found that the easiest way to check whether an object file was compiled using GCC or Rust was to check if the string ".rs" appears in it: &gt; &gt; grep -lF .rs *.o &gt; &gt; (Of course this might have false positives, so if you have a better approach please let me know!) I'd guess the ".rs" is coming from panic messages, and maybe also debug file/line tables. If you're compiling with debuginfo, there's the producer string, something like: $ readelf -w /path/to/file | grep -w DW_AT_producer &lt;c&gt; DW_AT_producer : (indirect string, offset: 0x0): rustc version 1.15.0-nightly (ebeee0e27 2016-12-04) A more indirect sign might be rust's style of mangled symbols, assuming you haven't marked everything `no_mangle`. A regex something like `^_Z.*h\x{16}E$` would detect the hash at the end, which C++ doesn't have. Or just look for `^_Z` mangling in the symbol table at all, if you're only deciding between Rust and C.
Your second item reminds me of [this](http://prog21.dadgum.com/23.html) series of articles on programming a hypothetical pacman clone in a purely functional language. The author comes to the same conclusion: don't mutate state directly, queue up state changes for later.
It emits a [`phf::Map`](https://docs.rs/phf/0.7.20/phf/struct.Map.html) value; if you look back at the build script, you'll notice that it just straight-up writes the `static` declaration to the file before invoking `phf_codegen`: write!(&amp;mut file, "static KEYWORDS: phf::Map&lt;&amp;'static str, Keyword&gt; = ").unwrap(); The key can be anything that is [`PhfHash`](https://docs.rs/phf/0.7.20/phf/trait.PhfHash.html), although you'll probably want `String`. Then the type can actually be any value. The [`phf_codegen::Map` builder](https://docs.rs/phf_codegen/0.7.20/phf_codegen/struct.Map.html) takes just a string for the value; it's expected that you'll use some struct literal there, or you can use string literals. Obviously the type should match the declaration that you wrote to the file previously. As for what it ends up looking like, it's formatted for human readability, but it's full of implementation details for the PHF table. In my [`mime_guess`](https://github.com/abonander/mime_guess) crate, I have [a long list of string keys to string values](https://github.com/abonander/mime_guess/blob/master/src/mime_types.rs) which ends up being used to generate [two different PHF tables](https://github.com/abonander/mime_guess/blob/master/src/gen_mime_types.rs). It ends up looking like [this](https://gist.github.com/abonander/30f67dd25050c6688e1c445579e84fcd). You'll notice the key type is actually `UniCase&lt;&amp;'static str&gt;`, which implements `PhfHash` in the `unicase` crate. It's just a wrapper type for `String`/`&amp;str` which will use case-insensitive algorithms for ordering and equality.
I really wish it was more common in the work world in python shops for senior python devs to extend code with libraries built with more performant languages like C/C++/Rust. Performance and parallelism really are the major places where cpython suffers, but it doesn't have to be an issue at all if good libraries existed that you could call from python but were written in something like rust. And there are some really simple libraries you could write that everyone would use and be able to benefit from. I remember some script I took over that was using datetime.strptime lots and lots of times (log files I think?) to parse isoformat datetimes. I profiled the five-minute script and that was the bottleneck... I wrote some python code to parse out the digits and just invoke datetime(year=int(year_str), ...) and I sped it up something ridiculous like 12 times as fast. Something like this could be written in rust and probably end up being lightning fast.
Having to `clone()` is always a sign that you need try a different approach. Perhaps you can separate each of those items into their own unique structures instead of conglomerating them together into a megalithic structure. Additionally, I'd avoid vectors as much as possible, and try to go for stack-based arrays whenever you can. The `arrayvec` crate can be very helpful for this. The stack's always going to be faster than the heap -- much faster. Surely a room has a limit to the number of certain elements that can be inside of it.
Honestly, Halt and Catch Fire is also responsible for my interest in learning Rust haha
Yeah there are plenty of ways to do it, I just chose one. I might want to say also, that there is another character at the end of buffer (\r) after the truncate, which caused some bugs for my latest post, so I now suggest using trim() instead of truncate. Thanks for the comment.
I'm working on a game in Rust right now, and facing some similar issues, except that I've never made a game in C++ (or anything else, actually.) Whenever you run into these kinds of problems, it is easiest to just use IDs or indexes instead of pointers. This is fundamentally safer, because if you move things around, you don't invalidate any references. Another option is the `Rc` and `Cell` types. These are a little more verbose, but they're probably more efficient and less prone to logic errors. The last option is to just use raw pointers. I haven't had to do this yet, except when dealing with dynamically reloading libraries and storing the referenced symbols in the same struct. However, the overall architecture is structured as /u/Diggsey proposes: I just queue up all my state mutations and them apply them after the entity processing is complete. I just call them `Delta`s. Entity processing can produce Deltas, any of which may be merged together, and when all processing is complete I'll just have one big Delta that I can apply to the previous game state. This will put everything into its new state, create anything new, and delete anything that needs to go away. (I'm also going to try this out as infrastructure for a client/server design, where I can take these Deltas and merge/compress them for transport between machines, so I'm not sending the whole game state every time.) I really don't know enough to say this is the ideal way to do it, but it is working fine so far.
Cargo (marvellous jewel that it is) isn't really optimal for little play programs. One can hack it by creating a dummy bin project with all the dependencies of interest, and put those little test programs in src/bin. A little scripting can create a more ergonomic workflow. I understand why Cargo doesn't centrally cache libs, but there's no reason why it can't do both.
`rotate` is a bit of an unfortunate name; I actually don't care what happens to the moved from items. They can be left uninitialized. &gt; you would need multiple implementations for different bounds of T (eg where T: Copy) Could you explain how one would implement this? It sounds like you're talking about specialization (in a C++ sense) but I get multiple definition compiler errors if I try to redefine `rotate` with different bounds.
I want to implement a structure for a generic fixed size sparse grid. So, 64x64 grid cells in an array. They start off empty and some of them will be filled. My first implementation was this: struct Tile&lt;V&gt;(Box&lt;[[V; TILE_SIZE]; TILE_SIZE]&gt;); impl&lt;'a, V&gt; Tile&lt;V&gt; where V: Default, V: Copy { fn new() -&gt; Tile&lt;V&gt; { Tile(Box::new([[&lt;V as Default&gt;::default(); TILE_SIZE]; TILE_SIZE])) } // ... } This works great for enum types where I can simply make an 'EMPTY' value and implement the default trait to say what it is. But I want it to work for boxed pointer types too! I figured I could just change V to Option&lt;V&gt; and drop the dependancy on Default and Copy: struct Tile&lt;V&gt;(Box&lt;[[Option&lt;V&gt;; TILE_SIZE as usize]; TILE_SIZE as usize]&gt;); impl&lt;'a, V&gt; Tile&lt;V&gt; { fn new() -&gt; Tile&lt;V&gt; { Tile(Box::new([[None; TILE_SIZE as usize]; TILE_SIZE as usize])) } // ... } But I can't even initialize the arrays now! 49 | Tile(Box::new([[None; TILE_SIZE as usize]; TILE_SIZE as usize])) | ^^^^^^^^^^^^^^^^^^^^^^^^^^ trait `V: std::marker::Copy` not satisfied I can't add V: Copy because Box doesn't implement the Copy trait. Is there any way to initialize my array of Nones? Am I going about this the wrong way somehow?
&gt; Accusations of bias aside, the rules preclude writing a custom hash just for the benchmark. What Rust could benefit from would be a set of hash maps customized for performance rather than for being DoS resistant. The hashers described could, IMHO, be considered a "custom hash for the benchmark".
Why doesn't the allocator API have a wrapper for `calloc()`? Allocating and zeroing isn't that uncommon of a pattern in Rust, and using `calloc()` (or, on Windows, `HeapAlloc` with the `HEAP_ZERO_MEMORY` flag) could often be a performance improvement over independently allocating and zeroing memory since it could use pages that have already been zeroed by the OS.
Oh neat. Have started preparing a pull request for adding conversion of keys of the internally-represented Edwards form (for Ed25519) to keys of the Montgomery form (for Curve25519), as well as performing key exchanges with Ed25519 keys if I have time. So, like, if anyone else was doing the same, please let me know to prevent duplication! I'm new to this library BTW, not maintainer etc
Using Option::map together with the `?` operator is a pain. If you are mapping over an optional type, you can't use `?` inside the closure. Is there a "failable" map method that has a signature like this? Option&lt;T&gt; → (FnOnce(T) → Result&lt;U, E&gt;) → Result&lt;Option&lt;U&gt;, E&gt; Or if there isn't (I couldn't find one), would it be feasible to introduce?
Sounds like a good idea to me. Not sure if adding methods to Option requires an RFC these days, but at the very least you can open an issue at https://github.com/rust-lang/rust
What is really amusing, is that using a queue has been the golden solution for multi-threaded applications for a while now: "don't communicate by sharing, share by communicating" is a very old adage at this point. It's quite interesting to see that it applies so well to mono-threaded situations as well. In order to make it easier to debug, I'd advise adding a "cause" field or two to the queued event. It makes it easier to understand why a unit is dying when you have a field that says "X damage by fired gun from Y".
Linking libpython is a problem. Extension modules are supposed to leave the symbols unresolved, then the extension module should work with any compatible CPython binary, even statically linked ones. But I'm not sure how to do that with cargo, and I want to support embedding Python into Rust applications at the same time. Pypy: At some point I should look into whether the `cpython` crate could work with `cpyext`. Cython has some limited support for pypy, so this should be possible. I agree that for simple usages where the Rust code doesn't need to directly store/access/create Python objects, safe bindings around `cffi` would be better suited than the `cpython` crate. (This would then presumably involve generating .py-files that wrap the generated C ABI in a safe manner?)
libpython only has a stable ABI on 3.3 and later. 
Dude, you're awesome. Thanks so much for all of your hard work!
Are you looking for `vec![0u8; _]`? jemalloc can and will give you freshly mmaped pages. However, zeroing data isn't the most widely required use case, as it turns out...
In a timely coincidence, here's my RustFest talk from September on using Rust with `ctypes` and `cython`: https://www.youtube.com/watch?v=xhoONTb_PdI
At least if you are aiming to compete against finely-tuned C software, eliminating cloning as much as possible is mandatory.
I am unclear whether translating another language tutorial would be that useful. The thing is, not all languages present the same difficulties. In Rust, ownership/borrowing is central and must be addressed sooner rather than later, and no other language tutorial will delve on this because other languages mostly do not care (even when they should, aka C/C++). Similarly, traits (generics vs traits objects) are also central in Rust, and I doubt a Python tutorial addresses this because Python is a dynamically typed language where this distinction does not make sense. --- On another note, there is also the issue of audience. You do not present the same material to a beginner (who has yet to learn to program at all) and a senior (who already has a few languages under her belt). What's the expected audience level of Think Python and LPTHW? Because it's important to keep your audience in mind when communicating.
I don't always use clone. It's only used in one instance which Ill delve deeper into in another blog post.
ArcadeRS is a great tutorial, though I wonder whether it isn't getting a bit long in the tooth. Most of the tutorial steps are over a year old, and progress has stalled on step 13 with no further updates since March 2016. The last adjustment to library dependencies was in January 2016 according to the front page, nearly a year ago. I'm not sure it's the place to point people to if they are interested in learning "current" Rust, at least not without caveats.
&gt; The hashers described could, IMHO, be considered a "custom hash for the benchmark". By "custom hash" here I meant "custom hash map data structure" and not "custom hash function". Sorry for the confusion. 
How much of that is needless copying, I wonder? `std::vector` access is unchecked. (It's not always clear in C++ when you're copying or moving; Rust definitely has the benefit of being explicit). The real killer is probably going to be non-locality of access and general cache non-friendliness.
Are discrete logs really easier than factorization? The attacks against both have the same time complexity, and it is likely that there is some underlying connection that makes them equally easy though none has been discovered yet. 
Couldn't you just use `std::mem::read()` along with `std::mem::forget()` in your own `Drop` impl?
To suppress the members' destructors, you need to take ownership of them, but `drop` takes the value by `&amp;mut`. `read()` =&gt; `forget()` will duplicate the value and skip the destructor on one, but not do anything to the original.
I won't bother responding for obvious reasons (you can't read, and you're rude). :)
* Discrete logs are more difficult then factorization. * Time complexity and _actual runtime_ on modern hardware are totally different concepts. Vector inserts are `O(N+1)` while adding to a linked list is `O(1)`. Yet the later can take 100-10,000x longer on modern computers. * There is a fundamental connection, all NP-hard problems can be mapped to one anther. But this is for a *general NP solver* which theoretically *can* exist, but it hasn't been *proven* to exist yet. Actually modeling the behavior of real algorithms on modern computers is a constantly evolving field. Right now modeling the memory subsystems (cache obliviousness) and runtime (time complexity) sit in different corners and don't interact much. 
I think you're looking for a different subreddit, this one is for Rust the programming language.
[removed]
I'm pretty sure you can't get around transitive propagation in one way or another because something might reach a downstream node via a path that has no changes at all. I'll write about the concrete change propagation algorithm I have in mind in the next post. It does not have problems with cycles (although there should be none, as DroidLogician mentions) because it can stop marking when it encounters a node that is already marked. 
I agree. As much as possible != always, that's all I'm saying. 
I didn't describe it very well. This is part of a 2 level structure where I'm using a `BTreeMap` mapping from position to `Tile`s. Then the tiles contain 64x64 individual grid cells with data in them. I want the grid to be able to be arbitrarily large. And operations are usually really local, so I'm hoping splitting it into little patches will give me better cache locality. Mind you, it might end up running faster to just use a `BTreeMap` directly and avoiding the overhead. It'll be interesting to see. To your questions, I'm `Box`ing the Tile object so the BTree isn't allocating and copying 4096 cells worth of data around in its nodes. (I'm also really curious if thats the right call - I'll need to benchmark it.) I could use a `Vec` but its sort of awkward because when I set any cell I'd need to expand the vec to 'reach' the x,y position being set. I suppose in theory that'll free up a little memory but I suspect that lots of varying size allocations + growing will thrash and fragment the allocator. (Or I could make fixed size allocations with Vec, but then whats the point? The size invokes extra state that I don't want.) Anyway, I'm using unsafe (gross) but I got it working: fn new() -&gt; Tile&lt;V&gt; { use std::ptr; let mut data: [[Option&lt;V&gt;; TILE_SIZE as usize]; TILE_SIZE as usize]; unsafe { data = mem::uninitialized(); for x in 0..TILE_SIZE { for y in 0..TILE_SIZE { ptr::write(&amp;mut data[x as usize][y as usize], None); } } } Tile(Box::new(data)) }
well, you told Rust that m will have a lifetime of 'a and then you try to assign something that lacks the lifetime of 'a. Rust rightly told you not to do this. Copy or clone it? create it on the heap and hand it over? just don't leave it on the stack and expect to pick it up after the function call is over.
You can use the `nodrop` crate to manually drop your fields too without incurring the usability penalty of Option
There are a couple of options, but first `println!("{}", val)` should be something else (`val` is not in scope). I'm not quite sure why this example doesn't work, especially when the following do 1. Move `special` out of Foo as a free function. [Playground](https://play.rust-lang.org/?gist=431b3d16146f42c98565aa8ceb56fb15&amp;version=stable&amp;backtrace=0) 2. Move `special` into its own trait any provide a blanket implementation. [Playground](https://play.rust-lang.org/?gist=2a7217ab55a68f29c7c5b9a3f2ba7522&amp;version=stable&amp;backtrace=0) 3. Change the associate type of Foo to a type parameter. [Playground](https://play.rust-lang.org/?gist=c5c13da811646cb8c5b79d474add0159&amp;version=stable&amp;backtrace=0)
That actually looks fantastic. My plan was basically to following along the HandmadeHero series, except using sdl2+rust, as a way to learn rust. Even if slightly outdated, my mad g00gl3 skillz should help with that.
Do you definitely have a connection to the internet?
Well, I'm posting here, so... Sorry, not trying to be a smart-ass. Yeah, I made sure my connection was fine. However, apparently it just took a very long time to download, I left it there for about 10 minutes and it finally resolved. Sorry for wasting your time!
Yeah, if a crate has a lot of big dependencies, it can take a long time the first time you build it. Looks like 'rand' is such a crate!
AFAIK, the very first time you use cargo to download a dependency on a new system it takes forever (I seem to remember this being the case as I've searched for it at the beginning of my Rust learning journey)
Yes, that is correct (which hopefully will be in ~7 weeks!)
I don't really know how these are designed in C++, but afaict they fit together reasonably well in Rust. Allocators define strategies for allocating and freeing memory; placers define strategies for initializing memory (primarily to avoid temporaries). In practice placers may need to call out to allocators to get a slot, but that's fine. As a concrete example `let x: Box&lt;i32, MyAlloc&gt; = box 0` is a natural combination of the two features (Box provides a placer that is generic over allocators). Note: I haven't followed these proposals in several months, and I don't remember where `box` vs `&lt;-` vs `in` or whatever landed. But it's all just different syntax for the same core ideas.
Is there a good implementation of a nom parser for floating point numbers including scientific notation? That would be very useful for me.
&gt; `drop` takes the value by `&amp;mut`. Ah, shit, so we can't move out of the structure... I forgot about that.
The signature I came up with would be `&amp;own Interior&lt;Self&gt;` but perhaps only `Interior&lt;Self&gt;` would suffice. The `Interior` wrapper would be there to "strip away" the `Drop` impl (but keep access to fields/variants) so that doing nothing would recurse on the fields but not on `Self` itself. That or `Drop::drop` could just not drop its `self` argument, and we could have the unconditional recursion lint take it into account so `let this = self;` would warn about the infinite recursion. A bit sketchy though.
What I would do: have some sort of built-in destructor `delete: Self -&gt; ()`, and have the default implementation of `drop` be to just call `delete`.
Oh nice, that's a relief! I wonder how it goes once you start adding joins and selects and stuff ;) I wonder if it would be useful if you could be able to extend Rust's error reporting with domain specific error messages. I think I remember seeing Idris having some similar functionality...
A bit off topic, but the Diesel website ([diesel.rs](http://diesel.rs)) is amazingly good. It's pretty, lightweight, and informative. I love the examples at the bottom. Nice work!
The language knows about drop, it can have different (de)initialization semantics. The ideal design involves a different class of function that "act" like drop, so drop impls can delegate to them and you can be confident in no infinite recursion or leaks (for instance File::drop wants to call File::close without File::close needing an explicit call to mem::forget, which at the time was Unsafe). Swift has a mirror of this system in its initializers. As is, taking &amp;mut self is a decent 95% solution. 
Good point
I thought you used specialization. Is that just helpful and not necessary?
[Tales of Maj'Eyal](https://te4.org/wiki/Stone_Warden) does! Unfortunately, it's not written in Rust, but it is open source, so maybe someday. ;)
Rust actually has a bit of this: https://doc.rust-lang.org/1.5.0/src/core/iter.rs.html#520 It just wasn't ever proposed as a proper feature.
Maybe a `fn drop2(self) -&gt; Option&lt;Self&gt;` that gives you the opportunity to `forget(self)` instead of returning it, but due to having to consciously return *something*, makes it lesss likely you'll accidentally assign `self` to a local and then forget about it. Or just `fn drop3(&amp;mut self) -&gt; bool` involving fewer non-built-in-types I guess. :/
&gt; There is probably more efficient ways to go about this, but this is the current reality. Possibly building a `HashMap&lt;&amp;str, usize&gt;` per result set that maps column names to indices. This adds a little overhead by needing to build the hashmap, but would allow O(1) lookups. That being said, I contend that `row.get("user_id")` is probably the fairest benchmark of real-world usage, because it's more similar in human readability to the diesel way of doing field lookups (`row.user_id`).
I mean, why can't `vec![0; _]` or `vec![None: Option&lt;[null optimized type]&gt;; _])` or `Vec::new().resize(_, 0)` just get a pre-zeroed allocation. This does impact performance, such as in construction of `BufReader`, and in `Read::read_to_end()`/`read_to_string()`, which zero-extend the allocation before reading into it because they can't trust `Read::read()` not to try to read the buffer it's given (though this could be alleviated with write-only references).
I'm trying to reproduce these benchmarks using the [provided code](https://gist.github.com/sgrif/0d948fb06fab9b1f6db51c6fddf93023), but it doesn't appear to work with the current nightly &amp; I can't find a reference to the specific version of rust used. Also, the post doesn't appear to note the exact schema that was used. I can guess some of it, but it'd be more straight forward &amp; comparable if it was simply provided. 
rustc_on_unimplemented let's library code specify custom messages to display if a Trait isn't impl'd. In this case we add the suggestion to add .iter() for Iterators.
Got it. (Here's a link to the right line for everyone else https://doc.rust-lang.org/1.5.0/src/core/iter.rs.html#327)
I guess then trying to get LLVM target support for XB1 will be your best bet. Rust » C++ transpiling is a non-starter because of UB in the latter that cannot simply be defined away.
Shouldn't the post be "libpq is 30% faster than rust-postgres in benchmarks" then?
&gt; and it is likely that there is some underlying connection The answer is more complex than that. There are certain connections that make ECC more easily attacked - for certain curves. DJB lays out the various curves and their susceptibility to those attacks here: http://safecurves.cr.yp.to/ You'll note the NIST curves fail several of his tests. 
Whatever suits you – if you already use vi, emacs, sublime, atom, VS code, eclipse or IntelliJ, go on; there are extensions for all of them.
I made a small success on this. I decided to use compiler plugin and drop support for stable. It's not published on crates.io because it contains many unimplemented!(), but I could get diff of string with color and table-like diff of slice. https://github.com/kdy1997/must/blob/35a247fc4b758e729d6a5437472b323ff8c07578/must-assert/tests/simple.rs 
We use libpq but will have our own driver in 0.10
There is one semi-niche case that requires specialization. Basically if you have let's say `MySpecialInt(i32)`, and you want to have a struct like: #[derive(Queryable)] struct Foo { id: i32, some_int: MySpecialInt, } you can do that by providing an impl of `FromSqlRow&lt;Integer, DB&gt; for MySpecialInt`. However, if you wanted to use `Option&lt;MySpecialInt&gt;`, you would need `impl&lt;DB&gt; FromSqlRow&lt;Nullable&lt;Integer&gt;, DB&gt; for Option&lt;MySpecialInt&gt;` which Rust won't let you write because it violates coherence. I provide a blanket impl that is: impl&lt;T, ST, DB&gt; FromSqlRow&lt;Nullable&lt;ST&gt;, DB&gt; for Option&lt;T&gt; where T: FromSqlRow&lt;ST, DB&gt;, DB: Backend + HasSqlType&lt;ST&gt;, ST: NotNull, but that impl also violates coherence without specialization. That's a case that not many people need to be able to do, though. So we don't require specialization. That impl is only present if you have `features = ["unstable"]`. I will probably start refactoring the code base to use specialization more widely with that flag (always with the current code if that feature isn't there), but I have tried to avoid introducing any features that require specialization until it has a timeline for stabilization. It also turns out that the for of specialization that was accepted is much less helpful to us. We need lattice or negative bounds for the majority of our use cases.
This is the kind of dodging the issue that caused our first argument to reach what it did. My comment was composed primarily of quotes from your web-page. If you are dismissing this as unsupported opinion, it is not a criticism of *my* biases. A vague, single-line footnote that speaks in absolutes on a page people are unlikely to visit is exactly as insufficient as it sounds. As a person in a position of authority, you have the power to shape people's ideas about reality. People frequently reference the benchmarks game in ways that we both know are incorrect. I cannot remember the last time I saw the converse - perhaps I never have. You need the moral impetus to take responsibility for the effects caused by your work, even if sometimes that is hard. 
Another interesting example (`ManuallyDrop&lt;T&gt;`): https://github.com/rust-lang/rust/blob/f3af8c8505255555023e4cb7c6c4f297ce22d80d/src/librustc_data_structures/array_vec.rs#L216
Ooh which ai competition, is it open?
Yes
It's a very friendly protocol IMO. Remember that this is only 31% of the 8% of overall request time, so 2.5% of the overall benchmark runtime. There also isn't really any other code that could be running - it's not doing much else besides parsing responses from Postgres!
&gt; It is better to be upfront in the first place, and not present unanalysed data that looks like it shows things it does not. The website is upfront and the data looks like it shows the things it does show. If you wish to suggest otherwise then **provide more than innuendo** -- show examples that demonstrate your claim.
Yeah, I made an issue in the tutorial Repo here: https://github.com/jadpole/jadpole.github.io/issues/33 This is regarding the changes to the SDL2 API. As far as I can tell, no progress has been made on this issue since my last post there.
 EDIT: Er... I thought the post was about a new release of Cargo. nvm... 
Might be useful to add some kind of progress bar in there.
Ah yes, the Haskell function `traverse`. Its kinda awkward to add methods for each case separately since we don't have higher kinded types in Rust, but I can see this being the most useful case by a large margin, so it would make sense to add it.
That's sad to hear, Rust is supposed to be used in real-time situations so the fact that you seem to be having performance issues is a serious problem. Seriously though, check out /r/playrust
Have you tried soaking it in vinegar? (but srsly, /r/playrust)
Just a random side comment: why not make `TILE_SIZE` a `usize` to begin with, to avoid having to cast everywhere?
More minimizing... trait Foo { fn foo(&amp;self) {} } fn special&lt;'a&gt;() where &amp;'a (): Foo { (&amp;()).foo() } impl&lt;'a&gt; Foo for &amp;'a () {} Rustc cannot see that `&amp;'a ()` implements `Foo` for *all* `'a` - so long as there is a `where` clause constraining it to implement `Foo` for a specific lifetime. Edit: filed an issue https://github.com/rust-lang/rust/issues/38308 
Which rust tutorial/guide do you recommend for C/C++/C# programmer?
Thanks for doing that.
Aww, that's too bad! Thanks for investigating it!
Huh, I don't know if you deliberately said that in a cynical-sounding way but I can see some real benefit there. Saying "look, we know the margin of error is huge but you can tell it's at least in the same league as these known-to-be-fast languages" would be a real plus point for an industry environment.
Hmm, that's an interesting point, about try opting *out* of error handling in Rust, and opting *in* in other languages. But, speaking as an old C++ dev (and I have the gunshot scars on my feet to prove it), I've never confused the C++ 'try' with the Rust 'try'. Maybe just because the syntax is so different. 
i thought that `try!` is pretty much deprecated in favor of `?` also: help update your text editor’s highlighting to fix that lack of visibility if you experience it!
Only if you aren't using nightly/until union is stabilized ;)
Interestingly, Swift `try` has a similar meaning to `try!` in Rust.
As Ubuntu bases on debian testing and unstable, the packaged crates like ripgrep or rustup should arrive on ubuntu as well. Its great news to hear about this!
&gt; How will this affect Rust on Ubuntu? Packaged Rust libraries and applications should work unmodified on Ubuntu, and this policy should also work for Ubuntu without any changes. Also, anyone wanting to upload their Rust application to a PPA could use debcargo to do so. &gt; As a side note, it would be nice to get something into Debian core that required rust. I'd love to. I suspect that Firefox will start out as the most prominent package incorporating Rust. But I wouldn't find it at all surprising if, by the next Debian stable release, we have some system daemons or administration tools written in Rust.
From what I've read over the years it has influences from loads of languages. This is from https://doc.rust-lang.org/reference.html#appendix-influences &gt;SML, OCaml: algebraic data types, pattern matching, type inference, semicolon statement separation &gt;C++: references, RAII, smart pointers, move semantics, monomorphization, memory model &gt;ML Kit, Cyclone: region based memory management &gt;Haskell (GHC): typeclasses, type families &gt;Newsqueak, Alef, Limbo: channels, concurrency &gt;Erlang: message passing, thread failure, linked thread failure, lightweight concurrency &gt;Swift: optional bindings &gt;Scheme: hygienic macros &gt;C#: attributes &gt;Ruby: block syntax &gt;NIL, Hermes: typestate &gt;Unicode Annex #31: identifier and pattern syntax 
I have been trying, but I'm frankly skilled at and c as well as c++ but only nominally so at Rust, figuring out how to build a C parser in nom has been kicking my butt.
And Midori's, though since that never left Microsoft it doesn't really matter.
A little more context: The first Ruma crate to be released was [ruma-identifiers](https://docs.rs/ruma-identifiers). ruma-signatures was the final blocker to being able to release [ruma-events](https://github.com/ruma/ruma-events) to crates.io, because one of its structs has `ruma_signatures::Signatures` as a field. Once ruma-events is published, that unblocks [ruma-client-api](https://github.com/ruma/ruma-client-api), which will allow Matrix's client API to be extracted from Ruma itself, and will allow someone to start working on a client library for Matrix written in Rust, in parallel with development of the server!
cargo-release doesn't generate changelog at the moment. It's pretty lightweight and you can start to use it in any of your project without any prerequisite.
Which came first, C++11 lambdas or Rust lambdas?
C++11, by a long time. We still had way different lambdas until like... 2013?
Yeah.. but C and C++ libraries already have this problem (you can pass parameters to `./configure` to enable/disable stuff at compile time). In practice distros tend to compile features liberally, to satisfy every program that depends on the library.
Why do you think it's acceptable to dismiss program measurements that make you look bad by just saying that benchmarks are misleading, without giving a good alternate explanation for them ?
A slight aside: testing binary format parsers like this is made a lot easier with the [`test-assembler` crate](https://crates.io/crates/test-assembler). [We use it extensively in `gimli`'s unit tests.](https://github.com/gimli-rs/gimli/blob/40f5d27/src/cfi.rs#L2889)
Sorry, I should really say it doesn't apply to benchmark game-style benchmarks (as opposed to benchmarks that are explicitly written to be as fair as possible, for example for compiler writers to check codegen). Constantly chasing after explaining away benchmark game results is, frankly, a fool's errand. It already annoys me somewhat that people use these close-to-meaningless results as an excuse to bash on languages they have a pre-existing bias against (Haskell gets beaten up here especially), and by legitimising them we just end up in a vicious circle. It's like the TIOBE - it's a fun thing, but we shouldn't have to defend ourselves. Same reason the UK's former prime minister never commented on the allegations that he put his penis in the mouth of a dead pig. This post was super interesting, mind, so I don't begrudge its existence. We just shouldn't worry if we start slipping away on these benchmarks.
How can I run benchmark with `iter` function called for only 1 time? The code I want to benchmark will change the internal state (memory) of the program, so the bencher must rerun the setup/tear down code in order to run the code inside `iter` again. I looked at the document but can't find any options to change that.
&gt; I'm going to feel terrible though if we go through all the trouble of getting Rust to target SPARCv8 and then I wind up not using it Nah, don't worry. Adding targets to rustc is relatively easy and don't require too much maintenance. I'm sure we got quite a few targets that *very few* people use at all. &gt; I'll also be able to get at least some useful contribution done here You can! Could you tell me what toolchain do you use to build C apps for this platform? And where can I get/build it? (I hope it's gcc based and that I can build it myself using buildroot or something)
I don't actually have a build environment yet; there was a delay getting my dev machine and there aren't any to spare. They have me reading the docs for [VxWorks](https://en.wikipedia.org/wiki/VxWorks#Development_environment) 6.8, and I know we have Linux build servers, so I'm betting GCC. Not sure how available the whole system is, though.
I should have mentioned this earlier, but if you want the safety of Rust while using any C toolchain you desire, take a look at http://www.ats-lang.org/
I'd really like to see an init system written in Rust!
Why not just mem::transmute the data? let mut web: WebSocketFrame = unsafe { mem::transmute(frame) } 
Ｈow might this overlap with [termion](https://github.com/ticki/termion)? Do you foresee any coinciding development taking place?
This is awesome. Last time I had to do something like this in Python I had to basically reimplement a big part of its readline wrapper, and it was a pain.
This is an exciting project. I love it.
I'm not a beginner in programming anymore but not that good that I would count me to the ones calling themself seniors. I learned xBase++ because of the company I'm workin for and now want to learn Rust in my freetime. I always need a little push for reading tutorials or books or the same in my freetime, but i think a good tutorialseries going into all would help all. Maybe divide it into 3 parts: -&gt; Beginner: Going through the essentials of Rust and learn the basics of all. -&gt; Intermediate: Get a bit deeper then the basics maybe some exercises to redo the basics on a small line to keep newcomer use what they learned in the first part. -&gt; Professionel: Going deep into the language and pointing out what can be done and what not. Heavy focus on different aspects not said in the first to parts and exercises where you need to think about what you do. I'm the type that learnes best when using things over and over again and that need exercises that fits the level of where I'm currently in my learningprocess rather then just going through tons of dry theory. That's why i currently have some kind of hard time learning rust in my freetime, because i couldn't find tutorials matching my learningcurve.
&gt; Except Rust lambdas infer how to capture each variable (I think C++ doesn't do this). They don't. Rust lambdas just provide much simpler capturing modes, it's either capture by reference (default) or capture by value (`move` closures) for all captured variables, period, end of the story. The only inference is that capture by reference may capture `&amp;T` or `&amp;mut T` variable-by-variable depending how they're used (old Rust used to have explicit denotations for these). Basically, Rust's `||{}` is C++'s `[&amp;]{}` and `move || {}` is `[=]{}`, and Rust does not provide per-variable capturing specification, to do that you use a move closure and capture references (by value) which you created outside the closure.
Ah, I see. Good point! Reason I ask is I'm currently writing a rust program which parses static data. i.e. I know the size of each and every element in a binary. So I simply transmute the structure at the start of my program. I was wondering if nom would be of any use but it looks like it wont.
Rust does infer by-move, e.g. `|| drop(x)` compiles fine without the `move` keyword. That keyword is required because escaping closures may need to capture by-value even if the variables are only used by-reference in the closure body.
Agreed, audience is everything. And pitching at people who already know how to program presents interesting challenges - the dynamic typing people need to learn stack/heap and immutability (since they've been swimming in a sea of mutable references. C people get those things, as do C++ people (although it may be _harder_ to learn Rust if your head is too full of C++). And just about everyone has a mental model of 'classic object-oriented programming' that needs to be recalibrated.
Using another DSL (Diesel Query Builder) ontop of an existing DSL (SQL) means that I don't have the flexibility to optimise out queries. Ontop of this it's a bit hard to hand to a DBA who might understand SQL but not Rust specifically, or use any countless examples on the web. I come from a world of Java, so the only comparison I can make of where I'd like to see Diesel heading is something like [mybatis](http://www.mybatis.org/mybatis-3/), which allows you to write your SQL in an XML file and have it serialize out objects based on what you provide it. If I could map a raw SQL to a struct, and not have to write boiler plate to manually construct it, that would be the end game. I doubt that rust has the language capability to do that at the moment though (reflection is needed etc..) Diesel is great if you're after an ORM, want to do the pure rust thing, or have a pretty trivial schema. I have a feeling my use case is very much niche in this regard.
&gt; In other words, a tuple shows a queue-like drop order, unless one of the expressions in the tuple constructor panics. In case of a panic during construction, the drop order will be stack-like! I don't see anything weird here. Consider these equivalent snippets: // 1 let tup = (PrintDrop("x"), PrintDrop("y"), PrintDrop("z")); // 2 let x = PrintDrop("x"); let y = PrintDrop("y"); let z = PrintDrop("z"); let tup = (x, y, z); So of course if constructing `z` panics, the `y` and then `x` are dropped in reverse order. You're not dropping a tuple, since there is no tuple yet! There is no "reversal of drop order" for tuples, structs, or slices. (Although, of course, it's a valid question whether tuples should be dropped from left-to-right.)
As far as I can see, it is not possible. Might be worth a feature request (e.g. having `#[bench(1)]`).
I don't think Python forces you to write cleaner code - rather it makes it easier to write cleaner code, by emphasizing style and conciseness. Heaven knows I've written my fair share of ugly horrendous drivel in Python.
Sort of forces you in the sense of indentation errors, but it's a small and subtle way to make reading code much easier than say other languages. I've written a fair share of disgusting code in Python but it somehow looks much more pleasing than the garbage I often wrote in PHP or C. 
You call it a Proof of Concept. Can you elaborate a bit about its status? Do you envision stabilizing this implementation or do you mean this PoC more as an inspiration for somebody else? I also have to say; I have been looking for this library for so long, and considered starting it multiple times (for different environments), but decided it wasn't the time for me to shave that particular yak that time. So happy to see this implemented!
I don't understand how rust can work within the rules of the debian packaging at all. They only ship one version of a library which in Rust land won't get you very far.
My personal experience from showing Rust to people is that `error_chain` together with `?` is something people pick up on instantly.
You mention immature ecosystem. What did you miss mostly?
My intentions originally going into Rust would have been to contribute libraries but soon I started to realize some of the libraries that I loved using would have been a farcry for me to make myself at my level of expertise haha. Some of the libraries I missed were QT, Boost and various other more mature graphics libraries for game development (cocos2dx). I'm aware there are bindings but many are incomplete still or many libraries are still not stable/ready for production use. That comes with time of course so I just want to wait it out a little :) 
(Off topic, but well...) Also, doing binary libraries would (as far as I understand) only really make sense if they are dynamically linked, but I don't know how that works with Rust? I mean, yeah, you can export a C ABI .so file but how do you make a compiled dynamic library with generics and so on, since if I have e.g. fn foo&lt;T&gt;(bar: T) -&gt; T { bar } it will monomorphize to different compiled code according to how the caller uses it?
&gt; That's not true. For libraries for which there's a need to have multiple versions due to having dependencies that rely on each, they will ship multiple versions. For instance, there's libgtk2.0-0 and libgtk-3-0 in Debian stable right now. Oh I do not dispute that. I dispute that individual version pins would be allowed. This means that dependencies within libraries are required to be lose.
Take the url crate. People not only pin to patch versions, they at least pin to minor versions. Yet that crate changes quite a bit. A project of mine has 4 diff. versions of the URL crate. That won't fly in Debian. 
Given that many essential and hugely used libraries are still not reaching a version `1.x.x`, we can't really consider the ecosystem mature.
Can confirm, my course in lisp did that for me, and learning Haskell helped as well.
Does Github's Atom have any up to date rust syntax highlighting plugins? If so, please link one.
Yep. They will probably patch crates to use less strict dependencies and submit the patches upstream or, if the latter does not accept them for any reason, keep shipping them downstream.
I'm planning to dust off my [old experiment of writing a http server](https://github.com/Etenil/irontray) in Rust; clean it up and remove the plethora of compilation warnings and add fastcgi support to it. I'm a rust beginner so any suggestion to improve my code is welcome btw.
I think this will cause major issues. It does not work at all in python land when debian unvendors python dependencies. 
It would be nice if you could tell in the README why it is so much faster. Are you using a specialized hashing function? If so, does it have the same guarantees as `Map`?
Funny, I'm also working through the book and writing it [in rust](https://github.com/JayKickliter/monkey). For peeking, I'm using a `Peekable` iterator.
Nice! Adding a port setting to the config would be nice though. EDIT: Looks like the port is set by a flag.
By that standard Go is your Daddy ( 688359 golang packages in 169242 projects indexed, last updated 37 mins ago *rofl* ) The issue regarding packages goes beyond how many are active or complete. Its more how the language gets used. This whole how bad D is at thing ( no wonder as its a Rust Reddit area ) show more of a insecure nature. Maybe i am only a D newbie but it seems like the D Community is less focused on fighting flame wars and is more about actual development. Another big difference is that D Community is indeed more closed. Less people share code because from my perspective, people are more interested in developing proprietary systems that they can earn money with. And sharing is not exactly caring with it hurts your money income. ;) If one has to rate communities about how rabbit its users are, then surly we can state that: Go &gt; Rust &gt; D. If you want a active repository with loads of packages to "sample", sorry, but you can not beat Go on that. Its growing faster and faster and in comparison Rust is massive lagging. Even when we remove the junk or incomplete packages ( and there are a lot of mature / professional packages in Go ), its still way more easy to prototype your program in Go, then in Rust or D. The real issues only come later when packages are not developed anymore and the developer has no idea how to fix the issues. I have always been the mindset: Only use packages when it takes you too long to make yourself. If people spend years on working, testing etc them. Sure, use them. But if you can write it in a few weeks... The nice thing about lots of packages is simple: Lots of code that you can look at, to see how things are done. Just pick the right tool for the right job. Most of these tools can do the jobs you want to do. It all depends on how "specialized" the job is. Just my perspective and like always another post on actually a useless topic in the grand scheme of things.
Why FastCGI? Plain HTTP such as you get through a reverse proxy is fast; FastCGI is slow. I can’t think of any reason why any Rust thing ever would need FastCGI unless it was trying to be a drop-in replacement for an existing FastCGI component, in which case that *might* work, but you’re losing some of the benefit of Rust.
There's a difference between being able to write an init system, and it being done. For starters an init system must be very portable. I'm sure someone will make a toy init system in rust but it won't go anywhere.
1.x.x is just a number though. Not every project maintainer is going to use 1.x.x to mean that the library is production-grade. What really matters is if the libraries have what you need. You're not forced to update libraries either if you do not need to. The `Cargo.lock` file ensures that dependencies are locked to a defined version.
Get the Fuchsia port of xi-editor checked in (the Rust side of the code is written but not yet tested. Start getting the Rust toolchain into the [Fuchsia](https://fuchsia.googlesource.com/fuchsia/) tree. I've also been noodling with a partial rewrite of pulldown-cmark.
You should keep an index (the kind you get from `char_indices`) as the record of the current position. Only with byte indices do you get constant time random access slicing and char access (for example as used in `peek_char_eq`). Edit: Oh looking closer, there's a logic error in using the same kind of index variable to compare against str.len() and to use in chars().nth(). One is byte index, other is `char` (not character, but specifically `char`) ordinal. Edit 2: To underline the consistency here, let's break it down. **“byte indices” (offset from start of string)** This kind of integer is used by *all* string methods. `.len()`, `.char_indices()`, indexing/slicing (`&amp;s[i..j]`), `split_at(i)`, returned from `s.find(x)` and so on. **“char ordinal”. Strings don't use these.** The number of `char` in an iterator. But this is the kind of integer you get from the general methods of an iterator, if you apply them to the `chars()` iterator. For example iterator methods `.count()`, `nth()`, `position()`!
I'm quite sure systemd doesn't care about portability as much as C. At least interest in non-linux is definitely not high priority. :)
Well I was thinking of trying to place it in front of *php-fpm* or something of the sort. But I could try proxying as well. Perhaps it's an easier goal as a first step.
There are other potential reasons it can be good. For instance, if there is a set number of states an enum can be in, such as opcode in this case, it would be nice to parse that out and error on incorrect data. Or for instance, if there are some dependent fields, in this case extended length being dependent on the length field, you might want to parse that out and not have to store both fields and manually convert them to length at every access. Lastly you'd have to use repr(c) and repr(packed) even if those things aren't true for you, in which case you will lose some potential optimizations the compiler could devise.
First off - thanks again for your great work on these bindings! I should have given more of a shout-out in my talk. PyPy support with `cpyext` would be great - what do you think the level of effort would be for that? The reason I prefer the bindings approach rather than `cffi` in general is that you can preserve much more type information. But as for the linking issues, that exists even with C extension modules - wheels don't have enough ABI information to be fully compatible and you often end up needing to build from source anyways.
&gt; Currently I'm holding off on learning Rust still because of its immature ecosystem compared to C++ Um .... ecosystems only get better if more people learn and build for them. I feel like this is a crutch statement. Unless Im missing something deeper.
The count of dependencies motivated the automated packaging tools. And any other practices we run into, we can deal with as we encounter them. We also can work *with* upstream to improve practices; I've been submitting various upstream patches to improve practices in ways that make them more compatible with distribution packaging.
I largely meant that the libraries I need and am used to don't have a real alternative in Rust at the moment that is stable/production ready. It's just about giving the ecosystem time to build more libraries and become more feature complete and stable. It's honestly hard to adapt when you have to recreate the wheel (or don't know how to either). I'm not at the level of my knowledge where I'll be able to actively contribute high quality code yet (for any language yet :) ) so I don't want to necessarily try yet and instead prefer an existing and mature ecosystem in the meanwhile
You can avoid a lot of allocations by removing that `literal: String` from `Token`, and inside `TokenType` changing `Ident` to be `Ident(String)` and `Integer` to be `Integer(i64)`, and all the punctuation doesn't need to have its literal form stored at all.
It will assign `ch` (inside of the some-body) to a char `=`. You can use this to do something like... match x { num @ '4' | '5' | '6' =&gt; { /* use num */ } } That isn't a great example but hopefully helps to get the point across.
Thanks for your response :) Think I'll give nom a go next time I'm parsing.
Has anybody explored using MIR as a target for other languages? I'm just curious if there is anything out there to potentially learn from. 
What is MIR?
Rust gets its entire machine model from C.
I'm going to avoid reading this as I am also going through the book. I've diverged a lot from how the go code was written to avoid copies and whatnot. Enjoying the project a lot. I'll give this a read once my parser is finished.
If you have a choice though, instead of using a custom binary representation that you have to manually parse/deserialize, I might recommend you use a well known binary format such as msgpack or protobuf. The latter isn't very well supported in rust unfortunately, but I use it in almost every other language. You can use serde for msgpack though.
It's a PoC as basic features like detecting non-TTY input/output or restoring the original mode are lacking and I'm not 100% sure if it works everywhere all the time. So it's the 10% percent of effort that yields 90% of the result. Another one is that I am very busy and I can't commit to adding the missing 90% of effort right now. I just wanted to show it works. You and anyone interested are welcome to help or take it over if have a need for a library like it.
Rust's mid-level intermediate representation...a compiler phase that happens after type resolution that is something like a very pared-down rust that is useful for optimization and analysis. 
Yes, in the sense that it is possible and I hope that if people are interested they will be able to add it. I can't dedicate enough time myself ATM.
Why not write a compiler in rust? That seems like a better idea.
&gt; I would expect more on the order of 500,000 per second or more. That's a completely ridiculous number. Assuming a 3GHz CPU that'd be 6 cycles per insert, on 8 bytes that's not remotely enough to even generate a constant 0 valued hash let alone a full SipHash-2-4[0]. And that's just the very first step of the process, then you have [the entire bucket lookup and actual insertion to run](https://doc.rust-lang.org/src/std/up/src/libstd/collections/hash/map.rs.html#513). [0] http://bench.cr.yp.to/results-auth.html
Any time you don't know some syntax, https://doc.rust-lang.org/stable/book/syntax-index.html is a good place to check. 
I don't understand the philosophy of Debian universe. Why go through all this pain? Why take this complicated route when there are simpler ones? Sure, each user might have to spend a little more time when taking the simpler approach, but I feel like it doesn't justify all the extra work the Debian community needs to do for this. (May be this comment is a bit unwelcome here?)
&gt; Feedback welcome! * indent code blocks by 4 spaces to properly format them on reddit * your comparison should probably use a dedicated integer hash (or at least FNV) rather than the general-purpose SipHash used by default
I've almost always seen by name outside of Rust as well.
3Ghz would be 3 billion cycles per second, which would be 6,000 cycles per insert. 
Whelp, right you are. Oh well, that stays.
If you had benchmarks, I'd ask you to to compare two solutions: - Using `peekable` and peeking using `peek` - Using just `chars()` and peeking using `iterator.clone().next()` A chars iterator is just two raw pointers into a string, copying it is super cheap.
Sorry, thats a batch of 2000 random numbers. Updated to make that more clear.
Not everyone has the luxury of time to spend building libraries.
So each insert is a vector of 2000 random numbers or each insert is a random number and you did 2000 of those?
`self.ch` is a `Some(char)`, whereas `ch` is just a `char`.
What the compiler is _written in_ and what the compiler _targets_ are two different questions. For example, `rustc` is written in Rust, but the frontend produces LLVM-IR. Someone could write another compiler in Rust that produces asm directly.
&gt; Feedback welcome nitpick, but you've got a leftover [`println!`](https://github.com/JesperAxelsson/rust-intmap/blob/master/src/lib.rs#L70) that will hurt perf in production when there are duplicate keys
Oh dear! Thanks.
It's a vector of semi random numbers. I use a fixed seed at the moment. Should probable use a random seed and print it for debugging, but that's work though. ;)
&gt; a fair number of its transforms aren't constant time Could you point you an example?
I have a question about HashMap. I was doing some performance testing and was curious about how an identity hash would perform. And for some reason it perform really well! I thought the collisions would degrade performance a lot. My measurements with a vector filled with 1000 semi random u64: test tests::u64_get_built_in ... bench: 21,569 ns/iter (+/- 1,610) test tests::u64_get_id_hash ... bench: 6,265 ns/iter (+/- 537) test tests::u64_get_murmur_x64 ... bench: 10,890 ns/iter (+/- 1,447) test tests::u64_get_u64hash ... bench: 7,081 ns/iter (+/- 861) test tests::u64_insert_built_in ... bench: 31,917 ns/iter (+/- 3,228) test tests::u64_insert_id_hash ... bench: 8,958 ns/iter (+/- 959) test tests::u64_insert_murmur_x64 ... bench: 17,843 ns/iter (+/- 2,745) test tests::u64_insert_u64hash ... bench: 13,823 ns/iter (+/- 2,175) Does anyone know how it does it? Tried reading the codes but was not clever enough. Source: https://github.com/JesperAxelsson/serious_hashes/blob/master/src/lib.rs
The hash function has nothing to do with it. If you have only 15,555 inserts per second on single integers, that is not good performance, it is terrible. &gt; One of the biggest slowdowns from the built in SipHash is that it uses a random number as a seed which slows it down a lot How would the hashing work if random numbers are introduced?
I have to second Qt (QWidget, not qmlrs) and add SQLite-compatible schema migration for Diesel and a high-level web framework (if not a Django competitor, at least a [Pyramid](https://trypyramid.com/) competitor). I'm not a game developer and don't typically write CPU-bound code, so these two holes in the ecosystem limit my use of Rust to command-line utilities. (My main interest in Rust is the ability of the type system and monadic error handling to take some of the load off me as a writer of unit tests.)
Actually, you landed on a very important distinction. A bad language design tries to force bad programmers to write good code (see Java). A good language design tries to make it as easy as possible for good programmers to write better code (see Python). **EDIT:** In hindsight, this was poorly thought out. However, I'll leave it unmodified in the interest of honesty.
Ah yeah, I took that as "well obviously that doesn't work because...". Have a good week 💖
&gt; How would the hashing work if random numbers are introduced? Eh, good point. I haven't actually read the source. But yeah, there is something funky going on there. I made a murmurhash that where about double the speed of the SipHash. Might be that the DoS protection slowing things down? Even with an id hash I got better performance then the built in hash, which is really strange. Id hash should be fulll of collisions. I made a question about it [here](https://www.reddit.com/r/rust/comments/5hw5vi/hey_rustaceans_got_an_easy_question_ask_here/db40ev7/) 
I'd be curious to know what you guys prefer to use between `getopts`, `docopt` and `clap`. I personally discovered `getopts` via the trpl book and settled with it, though I have had very simple requirements so far.
&lt;3 &lt;3 &lt;3
Fair enough. My implementation is a naive Vec&lt;Vec&gt;&gt;, I actually didn't expect it to be faster then the built in one. And certainly not as much. Their use cases is not the same though.
The previous init systems where never very advanced. Linux has always lacked in this regard. My point is that while rust has its places and I am sure it will be used in the future, I don't think a init system for Linux is one of them. 
which here makes no sense because you can use `'='` in the match arm anyway…
How cool is that! I'll be checking out your implementation! I've also stumbled across the `Peekable` iterator. Did you actually implement it? Edit: Just found it browsing your code. I'll definitly try to adapt that!
Same way it works for static linking: the up-stream library contains a serialised AST of generics and macros, and when you link against the library the compiler de-serialises those ASTs, monomorphises them, and sticks the object code into *your* crate. So you can never link against a library compiled with a different version of the compiler, either statically or dynamically.
Instead of code in `.text`, generic stuff like this is written as metadata in a `.rustc` section, to be monomorphized by the caller. It's very similar to the challenges of C++ template libraries, where a lot of the actual code has to live in headers and get instantiated by the caller. And yes, this is an ABI headache.
Totally. I'm really liking the book and I'll have a repo as I make progress. After I'm done I plan on scrapping it, starting from fresh, and doing the same thing with a different language (and compiling it this time, not interpreting it). Should be a lot of fun.
Wow! That's a bold undertaking, to say the least! :)
I'm doubtful that you'll ever find Qt on Rust in any decent capacity. Qt just happens to be the least-portable GUI TK. It's the same story for every other language that isn't C++. There's great support for GTK and others though.
I knew there were something i forgot. In my project it is used as a read only cache that is cleared and created when it needs updating. Will try to implement a get_mut method though. Still quite new to rust and my first time implementing a hashmap.
I agree that it would be very tough, and probably isn't gonna happen any time soon. But I don't think its impossible, especially in the future.
Methods that you are missing are: - get_mut - keys - values - values_mut - entry - iter - iter_mut - drain - is_empty - shrink_to_fit - reserve Traits that you are missing are: - PartialEq - Eq - Clone - Debug - IntoIterator - FromIterator - Extend You could possibly also add the ability to use a custom hasher as in HashMap. Although for my use cases, `get_mut` is all I need. The rest is just for completionness-sake.
&gt; If I could map a raw SQL to a struct, and not have to write boiler plate to manually construct it, that would be the end game. Diesel allows doing exactly that. That's the ORM portion, which is independent of the query builder portion.
&gt; A bad language design I don't agree. Java is pretty well designed IMO, which is part of why it's so popular. I personally don't like Java, but that's not because the design is poor, but because I'm not the target audience. Most popular languages are designed pretty well, but they all have their warts. There *are* a few that I think are designed poorly, such as PHP, but that's because they weren't designed for a different problem than people wanted to use them for (e.g. PHP was designed as a templating language, not a full programming language).
&gt; Do you have a specific example in mind? So far, I had a good track record on my Python packages. The great urllib3 unbundling from requests comes to mind. Badly broken pip etc. This might get you down the rabbithole: https://github.com/kennethreitz/requests/issues/2870 &gt; Upstream projects with questionable vendoring practices Those are not questionable vendoring practices. They are internal dependencies. You would not force a standard LLVM on rust either.
The point is that most of the time you don't *need* the literal string value of the token, eg when you see ';', that's TokenType::Semicolon, and you don't lose any information by discarding the string ";". The only time you store extra information inside the enum is when there is more information, so for StringLiteral, Integer, Ident you store extra information, but for punctuation like Semicolon you don't need anything else and get to skip unnecessary allocations. So you might not even need a separate type `Token`, and could just make `enum TokenType` be called `enum Token`. For all of: Token { token_type: Assign, literal: "=" } Token { token_type: LeftParenthesis, literal: "(" } Token { token_type: Comma, literal: "," } Token { token_type: RightParenthesis, literal: ")" } Token { token_type: LeftBrace, literal: "{" } Token { token_type: Plus, literal: "+" } Token { token_type: Semicolon, literal: ";" } Token { token_type: RightBrace, literal: "}" } Token { token_type: Semicolon, literal: ";" } Token { token_type: EndOfFile, literal: "" } The literal field is totally redundant, as that is the only possible value it can take for the given enum.
&gt; I don't agree. Java is pretty well designed IMO, which is part of why it's so popular. I don't want to get into a long list of reasons I think Java is poorly designed, but I will grant you that it's definitely a matter of perspective and my experience is weighted in the direction of the more elegant languages like Python and C. &gt; There are a few that I think are designed poorly, such as PHP, but that's because they weren't designed for a different problem than people wanted to use them for (e.g. PHP was designed as a templating language, not a full programming language). To quote a conversation among friends of mine: &lt;1&gt; they don't speak english &lt;1&gt; (at least it doesn't seem that way from their language design) &lt;2&gt; it seems you're implying that PHP is a language and has a design I'm assuming this is in reference to `T_PAAMAYIM_NEKUDOTAYIM` being the canonical token name for the double colon operator, as expressed in syntax errors messages and the documentation: &gt; T_DOUBLE_COLON :: see T_PAAMAYIM_NEKUDOTAYIM below Seriously, though, PHP's [pathologically organic](https://eev.ee/blog/2012/04/09/php-a-fractal-of-bad-design/) approach to evolving the language is everything Perl-haters caricature it to be. (See also, [errors in PHP 7](https://eev.ee/blog/2016/12/01/lets-stop-copying-c/#comment-3029798692))
I used clap for an in house cli tool which I wanted to provide subcommands with various options (some general, some subcommand unique). It was really straightforward with clap.
It looks like it doesn't include `?`. We should see if we can find a docs person who can fix that. I don't know where we would find one though. &lt;3
`peekable` is just putting the value on the stack, which is also super cheap
I like clap a lot, but I also find myself wishing there was a sensible way of combining parameters from arguments, environment variables, and configuration files. Has anyone come up with such a strategy? The way I think it should work in general is Arguments &gt; Configuration &gt; Environment. As in, environment variables represent an "ambient" configuration, configuration files represent a setting specific to the program, and arguments are an immediate override of any default or pre-set configuration. It's easy enough to implement this sort of hierarchy, but it requires a fair amount of repetition and boilerplate for each override chain.
Oh, good job! Will take a look at these tomorrow. :)
:)
&gt; I'm still waiting for Swift to demonstrate that it's suitably viable for non-Apple platforms I think that's done now with [Swift 3.0, which had significant changes to the standard library](https://swift.org/blog/swift-3-0-released/). Check out the [core libraries](https://swift.org/core-libraries/) to see if what you need is in there. The last time I read a complaint, file i/o wasn't available, but that's now in 3.0. 3.1 is supposedly coming out in Spring 2017, which will likely have a bunch more changes and stabilization. That being said, I still think that Rust is still a better bet because it has those things now, plus memory safety and a bunch of other stuff. Memory safety is one of the huge draws for me, and that's still a "TODO" item for Swift. &gt; Or, in couched in negative terms... But that's a complaint against the standard library, not the language or the ecosystem. It's definitely important, but I have more fundamental complaints, like the lack of a standard package manager (I guess there's maven), which really divides the community. That's now pretty much become a requirement for me in choosing a new language because it solves so many problems.
This seems handy. Having said that, it would be nice to be able to have the prompt permanently fixed at the bottom of the screen. Having it jump around seems annoying. That might require some kind of curses/termbox usage though...
Learning scheme is fun too!
But this does. Why?
You're not willing to let people see that **your accusations are false**. Show where you think there's "a contextless grid of numbers" or your silence will show you cannot.
Another :+1:. Posted a [feature request](https://github.com/kbknapp/clap-rs/issues/764). Although I won’t end up using the feature in the project I needed it for as I already turned the project in (today), it is still very nice to see it being worked on in such a timely manner.
We really do need some kind of good Rustic GUI library though... And it would be a big project that requires a lot of expertise with oh so much room for bike-shedding run awry.
Gonna join the +1 parade - definitely a fan myself!
That's too simplistic of a view. The languages generally regarded as "worst" are those that don't try to force anything on anybody. They end up with twelve different ad-hoc ways to unreliably pretend to solve problems that shouldn't exist. In this space, it's better to have a sub-optimal opinion than no opinion at all.
A lot of people just need to build working software, and it's unfair to admonish them for not wanting to sacrifice their time and effort to advance a language ecosystem along the way.
Hard to say without a sample and some error messages. EDIT: maybe it's due to /u/thiez forgetting `Item=`?
Still working on an ergonomic AMQP library. I'm hoping to get a basic client implemented, even if all it does is connect. I'll probably take a look at some other ergonomic libraries for ideas. I did manage to tackle most of the tackle most of the tasks from last week: the Value/List/Table structures organization, the (not yet published) parser crate, a few test cases (and Travis CI). Depending on my mood and research, I'll either make progress on the client or write more tests.
&gt; You would not force a standard LLVM on rust either. Both Debian and Fedora, and probably more, do indeed use the distro LLVM.
Ah yes, so then `Item=&amp;Path` should work just as well, then, I'd imagine.
It WAS well-designed. For the 90s it had a very nice VM and a saner type system than C. But now it's very dated.
There is no "one way to do it" in Haskell and Haskell is considered the "best" language by its enthusiasts.
How does the C machine model differ from say, Pascal machine model?
Hum, ok, but I still don't really get it. Say you have a library `cryptofoo` with some generic functions, and a program `bar` that uses it. They both compile, it runs nicely. Except there is some critical vulnerability in `cryptofoo`, so a new version is published in emergency. If I understand correctly, the binary `bar`, if it isn't recompiled, won't be able to benefit from this security update (unlike a program dynamically linked against a C ABI library) since the caller will have monomorphized the previous version? Plus, it might make the program completely crash, so it seems the problem is not just binary incompatibility between two versions of the compiler, but also quite possibly between two versions of the same library (I imagine most crates' semantic versioning is for source compatibility only).
Why not Vec&lt;PathBuf&gt;? [PathBuf](https://doc.rust-lang.org/std/path/struct.PathBuf.html) is to &amp;Path is like String to &amp;str
[removed]
Ah thanks! I have a need for lookup tables where all entries are known at compile time and the keys will all be `u32` or `&amp;str`. The fastest possible insert and retrieve times would be best for that situation without having to worry about HashDoS etc. since the maps will be immutable.
Package golfing.
&gt; yinz ಠ_ಠ
Agreed, and I think every language should be evaluated in terms of the technical situation at the time. Unfortunately, Java has been slow to modernize, which is really sad and I mostly blame Oracle.
I won't argue with that. To be honest, I sort of took that as so obvious that, without thinking, I eliminated it from consideration for the same reason you don't let the existence of Somalia be used as a red herring when discussing how messed up your own country's government is.
My hope is that, by the time I run out of more pressing projects in Python, Rust, and so on, either the Node.js or WebAssembly ecosystems will have produced a data-binding framework that means I don't have to keep reinventing Angular/React/etc.-like things simply because I insist that my pages continue to work near-perfectly with JS disabled.
You do have a point there. I just think that neither that nor the fact that it's a product of the '90s fully justify some of the warts in the type system and standard library. (Not to mention the "AWT is too crippled, Swing is painfully non-native, and SWT and JavaFX arrived far too late in the game" elephant in the room.)
You're looking for /r/playrust
For isomorphic applications, you can take a look at [WebSharper](http://websharper.com/)... I'm waiting for v4 to stabilize. 
I strongly feel this is the wrong precedence for three reasons: 1) Explicit is better than implicit. 2) An environment variable can and does apply to more than one application 3) Any environment variable that needs to override a configuration can simply be provided as a command line argument to always ensure it is used. Of course, my experience is more with deployed software rather than programs used directly by end users. Maybe that is just an argument for making this sort of precedence configurable by the developer should such a parameter unification library/tool/whatever be created.
You can find an implementation of FNV in crates.io ([Link](https://crates.io/crates/fnv)), in case you want to compare. 
Thanks! Will take a look at it later.
Is there a guide for replacing individual files/functions in a large c/c++ project? For instance, should I call the compiler directly or use cargo somehow? What's a good way to make header files into something rust can use (manually or otherwise)? Assume that there are a lot of runtime requirements and it's not easily extracted into a clean library.
I've always interpreted the intention of environment variables in 12 factor to be switching a very high level "run confuse for this environment" or maybe "here is where the masters are". Now, I don't mean to presume, but given that 12 factor was written before Docker, doesn't the 12 factor model heavily imply VM's running multiple services? Wouldn't it make more sense for a docker based deployment to just provide any configurations as the arguments for the single service instance being started? Furthermore, my preference for truly shared configurations in a multi-host (or multi-container if you prefer) environment is a distributed KV store that ensures you don't accidentally leave a bad environment variable in a Dockerfile or have a single hardware node that has had an init script misconfigured (because not all of us are so lucky as to live with disposable environments where ops practices good hygiene).
You can be "functional" in many ways even approaching a problem like FizzBuzz.
I decided to finally make the leap and start learning Rust. Recent conversations have me feeling nostalgia for my first programming language, so I think I'm going to try to implement a BASIC interpreter in Rust, very similar to what I used on the Apple ][ and in GWBasic.
Nit: `ident @ pat` is a pattern itself, and `|` separates multiple patterns (and currently not a pattern itself), so you need `@` for each pattern. ([Book](https://doc.rust-lang.org/stable/book/patterns.html#bindings)) Thankfully the compiler will correctly give an error for the original code ;)
Are you going to base your client on tokio/futures? That's what I'm currently struggling with (I'm using rust-amqp parser).
This is great! Thanks! One of lacking things in the Rust community (I, find, anyway) is the lack of video content where you can see people working through problems. There are lots of videos about the basics of the language and various tutorials about the language.. and thats great, but this sort of content is lacking. Things like ferris' Rustendo N-64 emulator series were amazing from my perspective. Even though I've been programming for almost a decade and working with Rust in my own time for about 9 months, watching someone sit there and work their way through _an actual problem_ in Rust is incredibly helpful. As for watching them live - I probably wouldn't.. but I certainly do enjoy these sorts of videos "on-demand". Thanks again!
If you want to keep the literal for some reason, a common pattern is to implement `From&lt;&amp;'a str&gt; for Token` and/or `From&lt;Token&gt; for &amp;'static str`, so you can convert them into each other. 
http://docs.diesel.rs/diesel/expression/dsl/fn.sql.html There's not a ton of examples because as mentioned it's not the main focus of the library
Previously: https://www.reddit.com/r/rust/comments/5hteej/rumas_second_supporting_crate_has_been_released/
Possible improvements I see in the code ([end result of applying them](https://gist.github.com/anonymous/ad12c2e4f3ba1f90cc6addc1fb2348ce)): * no need to explicitly type `parse` calls since the variables are concretely typed by the return annotation, `foo.parse()` works just fine * the Python callback can take in a `&amp;str`, the cpython crate will handle the translation from Python string to Rust string without you needing to do so by hand (just replace both `PyString` by `&amp;str` and call `parse_isoformat` with `datestr` directly) * you can use `try!` or `?` to correctly handle import/attribute errors rather than `unwrap` them (which would panic! the entire thing instead of properly bubbling up an ImportError or AttributeError or whatnot) aka let dt_mod = try!(py.import("datetime")); let dt_dt = try!(dt_mod.get(py, "datetime")); hell you don't even need the module (though it's demonstrative which is nice) so you could go as far as: let dt_dt = py.import("datetime")?.get(py, "datetime")?; * likewise dt_call, that entire match statement can become dt_dt.call(py, args, None)?.extract(py) * `parse_isoformat` is similar, the various let val = match foo.parse() { Ok(val) =&gt; val, _ =&gt; return Err(message); } can be replaced by let val = foo.parse().map_err(|_| message)? * incidentally you could have a free function converting the `&amp;str` to a `PyErr`[0], which makes the "python body" just: let dt_mod = try!(py.import("datetime")); let dt_dt = try!(dt_mod.get(py, "datetime")); let args = try!(parse_isoformat(datestr).map_err(|e| to_py(py, e))); dt_dt.call(py, args, None)?.extract(py) (nb: you can replace the `try!()` calls by a `?` postfix in rust 1.13, but I find the try version slightly more readable in this case) * in fact I think you could just use `PyErr::new` which is pretty terse and could be inlined in the map_err callback: let dt_mod = try!(py.import("datetime")); let dt_dt = try!(dt_mod.get(py, "datetime")); let args = try!(parse_isoformat(datestr).map_err(|e| PyErr::new::&lt;exc::ValueError, &amp;str&gt;(py, e))); try!(dt_dt.call(py, args, None)).extract(py) * month, day, hours, minutes and seconds should probably be u8s not i32s (should not make a difference but hey) * `timeit` (from the python stdlib) is probably better hand-rolling your benches Also check that you compile with optimisations (`cargo --release`), on my machine the Rust version is ~~\~3~~ 4~6 times faster (depending whether I use your bench scaffold or convert it to `timeit`), but I do get a ratio of 1~2 if I use the debug build. [0] it needs a `py` object for the conversion so you can't just implement From/Into on a custom struct, and the custom struct is thus useless
I don't think the `urllib3` case is representative of the *rest* of the Python packaging ecosystem, does it? I would agree that each packaging team do maintain a handful of tricky packages which require more-than-average efforts for unbundling, but you sounded like they are the norm more than the exception. Please clarify. And I am not questioning the pain sometimes necessary for producing bundle-free source packages. I maintain my fair share of packages with various degrees of such unbundling required but very few required more than a patch or a good discussion with upstream.
/r/playrust?
Is your source code available somewhere?
&gt; &gt; library packages are just distributing the source code (without .so files or anything) &gt; &gt; They only provide the shared libraries, no source code. I was referring to the draft, not about the "standard" definition of library packages, but about the rust library packages. To quote from the draft: &gt; Library crate packages must ship the source code of the crate in the /usr/share/cargo/registry/cratename-version/ directory. &gt; &gt; Motivation: At the time of writing, Rust does not have a stable ABI. So, we can't reasonably ship compiled versions of Rust libraries. Instead, library packages ship source code, and application packages build all the library crates they use from source. We will revisit this point, if the situation changes. Did I read that wrong? And yes, I understand that if a Rusty library `foo` is to be distributed, it can be distributed as _both_ a `librust-foo-` package and a simple `libfoo-` package. But distributing as `libfoo-`, as a simple shared library, does not need any of the policies defined in this draft. Which brings me back to "How FF will be able to use a `librust-foo-` package?" (I see why Debian wants `librust-` packages in general (from the other reply), but not the way you put it. I might imagine that during FF installation, `librust-` dependencies will be compiled to generate the shared objects?)
Hey, I again wanted to take and I've update my work now not to include anything static. I had to create a union of Variable, Floor, Ceil, ... for representation. I also have a generic branch which allows the user to specify what is the "unique" representation of a variable.
I've started implementing this in this pull-request: https://github.com/chr4/writing_an_interpreter_in_rust/pull/8/files As soon as I introduce `Indent(String)` and `Integer(i64)`, I'm getting the following error. Apparently, `match` only works when all `enum` items are of the same type or so? error[E0308]: match arms have incompatible types --&gt; src/token.rs:49:15 | 49 | let tok = match ident { | ^ expected enum `token::Token`, found fn item | = note: expected type `token::Token` = note: found type `fn(std::string::String) -&gt; token::Token {token::Token::Ident}` note: match arm with an incompatible type Hints apprechiated. When just using `Ident` and `Integer`, the tests run through, though, which is bad. This is because I don't check for the `literal` anymore at all... *Edit: Ah, I need to use `_ =&gt; Token::Ident(ident.to_string()),` when creating an `Token::Ident` obviously!*
I see, thanks for the correction!
This is up to taste. I prefer operations as types and chains of operations as seperate types. Futures provide that in a very clear fashion.
With these improvements it is very nice. re: benchmarking, my preferred way is `pip install pytest pytest-benchmark` then def test_pure_python(benchmark): benchmark(my_pure_python_func, my_args) def test_rust(benchmark): import my_rust_lib benchmark(my_rust_lib.func, my_args) Now, someone write a working `setup.py`! (You may assume that Cargo is installed).
Shouldn't this be compared to a [BTreeMap](https://doc.rust-lang.org/std/collections/struct.BTreeMap.html) instead of HashMap?
This is in the works. [Here's the issue for config files](https://github.com/kbknapp/clap-rs/issues/748) and also [the issue for env vars.](https://github.com/kbknapp/clap-rs/issues/712) There's a few implementation details to work out, but these features are coming ;)
Design quality and fitness for a particular purpose are orthogonal. For example, I think Bash is poorly designed even for its time, but I still use it a lot. I think lisp is a beautiful language, but it doesn't fit the requirements of any of my projects. I want to use the best designed language that fits my project's requirements.
&gt; I've also been noodling with a partial rewrite of pulldown-cmark. I still want to integrate this into rustdoc someday... Oh, and if you (or anyone else) is interested, getting an implementation of GitHub flavored markdown, specifically, would be a good thing, for reasons I can't fully share at the moment.
(For those of you who don't know, this is a colloquialism from Pittsburgh, the hometown I share with Carol &amp; Jake.)
Very nice video, really like the back and forward arguments for doing something in different ways. Note on your 'jump' implementation. You could simply initalize 'jump' to '1' and remove the '+1' and '-1's.
impl returns haven't stabilized yet, have they?
For the Go way to scale, they have to keep the stacks small and be able to relocate them, both expanding them and shrinking them according to use. This requires enough knowledge of on-stack data to relocate pointers that point internally to the stack. Go already has this for free because it has a GC. If you are not able to do this in the language, then you're stuck with inefficiently-large stacks for coroutines, or else virtualising the stack on the heap with the kind of async/await approach, or else doing that manually with state machines (which is effectively what actors are in an actor language). I think if you can't do "full Go", then for efficiency, the only thing left is what Futures is implementing. One day it might get some kind of async/await sugar on top to make code look sequential. Personally I'm really interested in how this is shaping up, and all in a distinctly Rust style -- like everything that Rust does, for some reason!
No and there are still open issues: https://github.com/rust-lang/rust/issues/34511 It's not going to be in a plannable future (in the sense that there has not been a final vote on the implementation yet and it isn't in way through the beta towards release). It is accepted as an RFC, though, it just has no date of landing, yet.
`cargo build --release` generates a shared library in `target/release/`, something like `libfastdtparse.so` (linux) or `libfastdtparse.dylib` (OSX) (I guess it would be a .dll on Windows?) Remove the `lib` prefix (and replace `.dylib` by `.so` on OSX) and move the file to your pythonpath (e.g. `.`). (`cargo build` does the same thing except it generates a debug build in `target/debug/` instead)
The article mentions Rust multiple times, but I don't think all statements concerning Rust is correct. Isn't Rust's solution to attack assumption 4, "Two different versions of a package cannot be installed simultaneously"? In fact, he goes on to say: &gt; I mentioned above that there can't be two definitions of printf built into a C program, but languages with explicit module systems should have no problem including separate copies of D (under different fully-qualified names) into a program.
This is too true. I really wish there were more videos like this out there. Ferris' videos made me check out Rust and fall in love with it. I understand some people find them helpful, but I'm not a big fan of the video tutorials that pretty much just goes through the same basic stuff that's in the book. I don't need a video explaining the syntax for arrays or what a vector is, etc. I want to see people working through something, explaining as they go, making mistakes, troubleshooting said mistakes, suggesting things that could be used instead and other things like that. I probably wouldn't watch it live either, simply because I don't really want to schedule my watching habits. I would however definitely enjoy watching VODs, just like this.
Awesome. Thanks. That was the missing piece for me. 
Rust will allow you two have two versions of transitive dependencies, but not depend on two versions of a dependency directly.
The proof uses transitive dependencies, so the proof as written is invalid. (Can it be saved?)
On the other hand, futures are a decent core abstraction for a more synchronous-looking syntax to build on. There are examples of this in other languages, like `@inlineCallbacks` for Twisted, or Haskell's do-notation for monads. Some people have already experimented with macros like this for Rust.
The differences between commonmark and github-flavored are pretty small, I think it's doable.
Have you tried using futures and tokio-core in practice though? I've tried to do multiple different projects with it, and it's not really practical or usable.
Good to know! That looks like a great resource.
HUGE thanks to [jonhoo](https://github.com/jonhoo) for swooping in, taking an interest in the `addr2line` example program in `gimli` and turning it into its own proper library and executable, and doing lots of performance work! There's still some more things to try, but given that the current results put us way ahead of the canonical binutils `addr2line` I felt it was a milestone worth sharing :) I suspect we are also faster than `atos` on OSX based on previous experiments (which was before gimli's `addr2line` was feature complete), but I don't have fresh numbers there.
I'd assume not, since it has to preserve order and HashMap doesn't.
&gt; 12 factor works perfectly with docker. Generally speaking I want my containers to be immutable which means that their config files and argument flags are generally baked into the container. I then inject in environment variables based on where the container runs, e.g. inject the staging database connection string when the container is running in staging, etc. I completely agree that deployment artifacts should be immutable. But what I do is bake all the possible configurations into the artifact (sans secrets and a master "environment" variable). That way it's *always* clear what the configuration was for a certain artifact in a certain environment (as long as you have a master artifact that specifies each sub-artifact and the shared configuration). &gt; The easiest way to inject these values into a running container is to turn them into environment variables and start the container with those variables set. I've found it much easier to emit configurations from templates, personally. The added bonus being that they're *very* easy to introspect, since they're just sitting around in a file, or if I need to, I can just re-emit the configuration. &gt; You have to explicitly pass environment to Docker, I'm not sure why this is an issue. I'm actually not allowed to use Docker at work, but I was trying to give an example of how an environment variable could be set badly. I imagine if I *do* get to use Docker, I'll have to work off of a base image that operations provides. And they're not always so smart about sensible defaults or making things clear and obvious. I'm not saying that it doesn't work the way you prefer. I'm just saying I have what I feel are pretty good reasons for doing it the way I am. And, like I said, maybe this is just a good reason for this sort of parameter preferencing to be configurable as well.
Noice!
&gt; The overhead of learning futures is worth it. For most people they will be working with prebuilt low level Future implementations. I think so too, but it's true that they're not easy for a newcomer. Speaking of which: I've been trying to graft `trust-dns` onto `tokio-socks5`. The name resolution part in the current code uses threads and blocking I/O from `std`, and looks like this: ... }).and_then(move |(conn, buf)| { pool.spawn(futures::lazy(move || resolve(&amp;buf))) .map(|addr| (conn, addr)) }) ... So far, I have this: ... }).and_then(move |(conn, buf)| { let (name, port) = try!(name_port(&amp;buf)); // &lt;- io::Result&lt;(Name, u16)&gt; let (stream, sender) = UdpClientStream::new(*dns.clone(), handle.clone()); let client = ClientFuture::new(stream, sender, handle.clone(), None); let query = client.query(name, DNSClass::IN, RecordType::A).map(move |response| { get_addr(response, port) // &lt;- io::Result&lt;SocketAddr&gt; }); // ... now what? Ok((conn, *dns)) // to make the compiler happy; I actually want the result of get_addr() as the second element }) ... Various attempts to use `and_then()` or `then()` on `query` ended with type errors; I can `spawn()` it on the loop handle, but I don't think it should be necessary, since `ClientFuture::new()` already does that. Besides, I'm not sure how I could get the resolved address out of `spawn()`. Any hints? Or should I structure the code differently, e.g. by constructing the future outside of that closure?
What issues were you having? I haven't done a _ton_, but it was fairly straightforward.
Is there a github repo I could look at? I'll look more closely at you post later today when I have more time. At first sight I think you want to return the result of the query().map() from the and_then() with possibly a flatten() following that.
Yes! Considering different implementations, and trying to be flexible knowing that a change in requirements is coming without overengineering, is the best part of these. Great practice for professional programming!
You might be interested in perfect hashing - this looks like an implementation of what you need: https://github.com/sfackler/rust-phf. I've not actually used that crate, so I can't vouch for it specifically, but perfect hashing is very interesting for this sort of thing.
The repo (branch: futures-dns) is [now up](https://github.com/inejge/tokio-socks5/tree/futures-dns). Here's also a link to [the relevant lines](https://github.com/inejge/tokio-socks5/blob/futures-dns/src/main.rs#L314-L322). Thanks!
Hi! /u/llogiq is correct! [Here's a repo with my slides](https://github.com/carols10cents/rust-out-your-c-talk), and the slides have speaker notes. [Here's where my resulting code is!](https://github.com/carols10cents/zopfli) Please let me know if you have any questions! 
Actually, the proof only requires for it to be _possible_ to specify packages (or package version) that cannot be installed at the same time, which is definitely possible with the `links` attribute in the Cargo.toml
&gt; Please don't -- publish your own measurements. If you insist. I was [going to](https://github.com/RDeckers/rust_c_comp/tree/master/src) but never got around to it. Taking a closer look at the website it seems we might also be going for different things &gt;So read the description and write your own idiomatic program, without programming tricks. I'm more interested in seeing both: how fast would idiomatic code be and how much work is it to then optimize hot-spots, forgoing idiomaticcy (is that a word?). &gt;Back in the day -- nsieve &amp; nsieve-bits Cool, but slightly different: my version is not a sieve but a ` while primes_found &lt; n {find the next prime by trial division}` style loop. Conceptually more simple, but it's a good test I find of how ergonomic a language is to write strict optimizations in. The rust version can look very nice using functional style programming, but it's not even close to the current version in terms of speed. I had a nice SIMD+Parallel version of the C code in the repo I linked, but that was before I learned to use versioning software/backups so I ended up losing it :(.
I had to test this ofcourse, being curious and all. ;) Seems the linear order is actually faster for some reason: u64_get_built_in ... bench: 45,244 ns/iter (+/- 3,420) u64_get_id_hash_linear ... bench: 7,048 ns/iter (+/- 335) u64_get_id_hash_random ... bench: 15,185 ns/iter (+/- 955 u64_get_id_hash_random_order ... bench: 7,090 ns/iter (+/- 736) u64_insert_built_in ... bench: 62,935 ns/iter (+/- 3,235) u64_insert_id_hash_linear ... bench: 14,826 ns/iter (+/- 818) u64_insert_id_hash_random ... bench: 27,906 ns/iter (+/- 1,215) u64_insert_id_hash_random_order ... bench: 14,811 ns/iter (+/- 2,066) Might be a issue with my benchmarking though. hash_random is random numbers. hash_linear is numbers `0..count` hash_random_order is numbers `0..count` in a random order
And for convenience, you can use cargo add (install via cargo install cargo-edit), and it will do that process automatically when you type 'cargo add image'
As reported by `time`, my solution takes 0.084s, so your **0.39ms** would be less than 1% of my runtime; very impressive! &gt; with zero heap allocations. Do I read correctly that this limits the code to a maximum of 32 instructions? 
Are you perchance using Python 2? The package as provided is a Python 3 extension module (the default for rust-cpython), you need to change Cargo.toml to enable the `python27-sys` feature if you want to build against Python 2.
Ah yes, I was defaulting to python2. Following: # pip3 install pytest-benchmark The test worked: $ py.test-3 perf_test.py -&gt; ----------------------------------------------------------------------- benchmark: 2 tests ----------------------------------------------------------------------- Name (time in us) Min Max Mean StdDev Median IQR Outliers(*) Rounds Iterations ------------------------------------------------------------------------------------------------------------------------------------------------------------------ test_rust 1.6280 (1.0) 43.3640 (2.39) 1.7662 (1.0) 0.3014 (1.0) 1.7480 (1.0) 0.0510 (1.0) 795;5201 71711 1 test_stdlib 9.0500 (5.56) 18.1450 (1.0) 9.3961 (5.32) 0.7119 (2.36) 9.3015 (5.32) 0.1180 (2.31) 5;12 196 1 ------------------------------------------------------------------------------------------------------------------------------------------------------------------ (*) Outliers: 1 Standard Deviation from Mean; 1.5 IQR (InterQuartile Range) from 1st Quartile and 3rd Quartile. ============================================== 2 passed in 1.60 seconds =============================================== 
Now try it with a bunch that all hash to the same bucket, like `(0..count) &lt;&lt; 32`.
The first stage is iterating through instructions and returning errors if the instruction is invalid. The valid instructions are checked for possible optimizations and pushed to a stack-based `ArrayVec`. You could expand the number of registers up to 26 if you just increase the size of your registers array to 26, but because all the inputs only contain four registers, from 'a' to 'd', a size of 4 is all you need for this problem set. You could probably also add a check to see if the register is invalid during the collection phase though.
Isn't that where you can use `Io::split()` to get independent read and write halves for a socket?
RAII came from C++.
Can you show some code? For example, what sorts of things do the enemies need to know about the players for? Does your screen struct look something like this? struct Screen { enemies: Vec&lt;Enemy&gt;, players: Vec&lt;Player&gt;, } or something else? EDIT: Also, by "players", do you mean the paddles in Pong, and "enemies" are the ball(s)? Or are the players the human paddles and the enemies are the computer-player-controlled paddles?
Have you made any comparison to the elfutils `eu-addr2line`?
Not the submitter, but I took it to mean something like: struct Enemy&lt;'a&gt; { friend: Option&lt;&amp;'a Friend&lt;'a&gt;&gt; } struct Friend&lt;'a&gt; { enemy: Option&lt;&amp;'a Enemy&lt;'a&gt;&gt; } struct State&lt;'a&gt; { friend: Friend&lt;'a&gt;, enemy: Enemy&lt;'a&gt; } fn main() { let friend = Friend { enemy: None }; let enemy = Enemy { friend: Some(&amp;friend) }; let state = State { friend: friend, // error[E0505]: cannot move out of `friend` because it is borrowed enemy: enemy }; } I would just use `Rc` for this purpose. Not very elegant, admittedly.
https://bitbucket.org/johannestaas/fastdtparse/pull-requests/1/applied-improvements/diff For your pleasure.
&gt; coroutines that look like synchronous code ... appear to be way more natural, readable and concise Futures look just like the synchronous code - with a bit of `do` notation from Haskell. cf. https://www.youtube.com/watch?v=ozN6XxsAF84 =)
&gt; This requires enough knowledge of on-stack data to relocate pointers that point internally to the stack. I think that lifetimes means that most Rust developers are aware of how big their stacks are and when they are shrunken. I don't think it's so crazy. If anything I think that the solution needs to be a hybrid that allows creating full threads, multiplexing tasks on them, and communicating between tasks, and threads, using futures, channels, etc.
Yeah that did it. Was about 63 times slower.
Nice post. &gt; Unfortunately I do not understand why it rejects the code, it doesn’t explain what are the requirements that are in conflict, and its suggestion is completely unhelpful I don't understand either. Could some explain why [this fails](https://is.gd/crGXf2) ? Small question: why did you chose not to go with [libpnet](https://github.com/libpnet/libpnet) to build your networking stack? Edit: I guess the answer is in smoltcp README: &gt; Its design anti-goals include complicated compile-time computations, such as macro or type tricks, even at cost of performance degradation.
I fiddled around with this for a while but couldn't get it to work. Another source of inspiration would be [this setup.py](https://github.com/getsentry/libsourcemap/blob/master/setup.py) from the recent Sentry [blog post](https://blog.sentry.io/2016/10/19/fixing-python-performance-with-rust.html).
Yeah pretty much exactly that. If I implemented Copy on the State, that means that as long as the Friend and Enemy had the same lifetime as State, it should pass theoretically right...? Not sure. Would be a huge shame to resort to something like Rc imo, might just be me though.
Nice! I was just starting to work on my own snappy until I found about this. Thinking I might try working on a leveldb implementation?
Does that code snippet not work if you add 'a to &amp;self? It could be that it currently gives &amp;self a different lifetime than the reference into one of its fields.
I'm working on a 6502 Emulator, Assembler and Disassembler. I've always wanted to implement an emulator and I'm finally close to achieving that goal. The Disassembler and Assembler are complete.. albeit a little messy. The lexing and parsing is all written by hand (not generated) and so its quite verbose ... but very complete. It errors at every possible level with quite a bit of detail - which I'm proud of. The Emulator is coming along nicely. I am [working my way down this list](http://www.atariarchives.org/2bml/chapter_10.php). Sometimes I've jumped ahead to implement instructions that help me test the code. On the train to work I finished implementing the compare instructions (specifically, `CPX`, `CPY`) and the decrement instructions (`DEC`, `DEX` and `DEY`). Even though some of it is a little bit messy.. I am pretty proud of what I have so far. I've written tests at every level of the code - I'm even writing integration tests now. Here is an example of an integration test I just wrote: #[test] fn INTEGRATION_CPU_dex_decrements() { let asm = " LDX #$05 LDA #$FF STA $0100 LOOP: DEC $0100 DEX BNE LOOP "; let mut cpu = Cpu::new(); let mut assembler = Assembler::new(); let bytecode = assembler.assemble_string(asm).unwrap(); cpu.load(&amp;bytecode[..], None); cpu.step_n(20); assert_eq!(0xFA, cpu.memory[0x100]); } It essentially stores `0xFF` at memory address `0x100` and decrements it 5 times. Then checks the result at `0x100` is `0xFA` after the test runs. Its exciting to write tests like these. The reason this is so exciting to me is that its lexing, parsing, assembling and executing the code.
It does work but completely negates the purpose of adding a lifetime in the first place; simple `pub fn payload(&amp;self) -&gt; &amp;[u8] {` would do the same thing.
I guess I don't understand the purpose of the lifetime then. It would read to me as the reference outliving its data. Edit: I haven't read the blogpost yet, just the code snippet :)
Here's how the trait would look with "HKT" (syntax quibbles notwithstanding). I haven't written it out with trait families, because it takes more code, but in principle it would be the same: trait Ptr where Self&lt;'_&gt; { type Target; fn deref&lt;'a&gt;(self: Self&lt;'a&gt;) -&gt; &amp;'a Self::Target; } impl&lt;T&gt; Ptr for &amp;'_ T { type Target = T; fn deref&lt;'a&gt;(self: &amp;'a T) -&gt; &amp;'a T { self } } impl&lt;T&gt; Ptr for &amp;'_ mut T { type Target = T; fn deref&lt;'a&gt;(self: &amp;'a mut T) -&gt; &amp;'a T { &amp;*self } } // And using it pub struct UdpPacket&lt;'a, P: Ptr&lt;Target = [u8]&gt;&gt; { pub buffer: P&lt;'a&gt; } impl&lt;'a&gt; UdpPacket&lt;'a, &amp;'_ [u8]&gt; { ... } impl&lt;'a&gt; UdpPacket&lt;'a, &amp;'_ mut [u8]&gt; { ... }
&gt; Arguably, AsMut&lt;[u8]&gt; should extend AsRef&lt;[u8]&gt;, but it is too late to do this backwards-incompatible change. Is it really too late? Rust is still quite young.
Thanks for the info! Someone else had mentioned this to me but I didn't put 2 and 2 together at the time. I wasn't fully aware what perfect hashing is so that wasn't the aspect of the library I was looking at. Having guaranteed no collisions would be really great, but I'm also hoping to have simple fast computation of the hashes as well for doing lots of lookups. This sounds just right. Thanks again.
Well, they're stable, and this would break any implementation of AsMut that doesn't have the corresponding AsRef. It is my understanding that such changes are frowned upon, to say the least.
I suspect that `yield` is fairly straightforward to compile to a state machine that can live on the stack, but how do you make a Sized `yield from` or work around it?
Am i blind or it's slower on this benchmark ? It seems binutils wins if you compare memory usage AND completion time to others.