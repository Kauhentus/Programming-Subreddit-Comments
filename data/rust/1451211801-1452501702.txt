This is something that an Emacs mode could do. Perhaps `rust-mode` could get an option to automatically insert semicolons when you press the "enter" key in the end of a line that could have a semicolon. That is, pressing enter after the line `fn f() {` wouldn't insert a semicolon, or pressing enter after the line `ob.method(a,` (since `fn f() {;` and `ob.method(a,;` aren't valid syntax). But, inside this function, pressing enter after a line that forms the end of a complete expression (even if the expression spans various lines) would add the semicolon. Perhaps this would require running an external library to partially parse the program though (perhaps racer could be adapted to do this, pinging /u/phildawes) Add this to some theme that gives less emphasis on the semicolons (by graying out them for example) and you get a less semicolon-heavy experience. edit: would also be cool to delete trailing semicolons whenever you delete a line (For example, you are at the beginning of a line and there's a semicolon on the previous line; pressing backspace should also delete this semicolon). Then, it's perhaps better to also add semicolons when you insert new lines in the middle of an existing line (not just at the end), provided that the point where you inserted the new line can syntactically have a semicolon.
A properly supported interpreter that rivals ipython. Rust would be missing nothing from my point of view after that. *edit* Oh, and stable compiler plugins!
Would you be interested in writing/enhancing a TOML parser? https://users.rust-lang.org/t/looking-for-a-project-how-about-a-toml-parser-that-writes-nice-files/3719/4
Const parameters would fix the same problem, I hadn't thought of that.
Some selected answers (might add the rest later) 1\. Line 28 is very idiomatic. It's also cool you avoided instantiating the `collect` method, but instead relied that `State::inst_tape` is a `Vec`. 3\. What about having different methods for incrementing, decrementing, etc. and have it be like fn step(&amp;mut self) { match self.inst_tape.get(self.inst_ptr) { Some(&amp;Inst::IncPtr) =&gt; self.inc(), ... } } (I'm not sure this is prettier) 5\. Of course! It guarantees that safe code can't produce invalid instructions, and therefore you don't need to have a check for them "just to be sure" (which, in `match` arms, would be like: `&lt;range of invalid instructions&gt; =&gt; `[`unreachable!()`](https://doc.rust-lang.org/std/macro.unreachable!.html))
It's actually specced well, the spec is just badly written. The end result matters, not how the spec expresses it. &gt; Of course, such state isn't even well-defined, because it depends on the exact parsing method being used. The spec assumes some (LA)LR(1) parser generator being used, because that's the way things were done back when it was first written. The Haskell98 rules were also somewhat broken, the changes in newer revisions reflect that. *Every* implementation disregarded the standard and just did it the way it was supposed to be done. I somehow doubt there has ever been an actually 100% compliant Haskell98 implementation. (And pure Haskell98 is atrocious... for one, there's no hierarchical modules). The *general idea* however, that is, defining layout syntax in terms of translation to bracy syntax, is very, very, nice. 
async / await and yield semantics support in the language based on compile time generation of underlying state-machines
&gt;2) Implementing Deref _is_ the easy/terse way, but you might not want to do it, because it means that different newtypes will be non-distinct if they have the same inner type, which removes some of the benefits of newtypes. True. Good point. &gt; Alternatively alternatively, you could make it a record with just a field called "inner", or "pointer", if you want to make it more readable. I like this. I think `inner` makes the purpose clear while keeping the benefits of newtypes. &gt;3) Making step take `&amp;self` and return `Option&lt;State&gt;` or return bool is much preferred to setting a flag on self. Ideally, if there's a way to do something that decouples state and behaviour, you want to do it that way. Rust designs itself to support a functional mindset, so if you want to write idiomatic code it's best to think functionally. I did think about this, my concern was: what if step is called after it's in a terminated (and thus invalid) state? But now that I reconsider, it does seem like I could simply check that the instruction pointer is still in bounds. I'll do that. &gt;4) I would write it like &gt;Why were you writing "as u64 as i64"? When I wrote it, I didn't want it to be sign extended. But now that I reconsider, full 64 bit -1 might be better. (Since that's usually what the real EOF is - actually it's 32 bit, but you get the idea.)
It should reduce sequental cargo builds time, shouldn't it?
I added another example. In part a lot of the other parameters can be hidden in the model struct itself which I'm passing in as self. If that isn't included the functions could look a lot more different.
Travis is currently failing due to a regression on the nightly rust build. It builds without problems on stable rust.
Ahh thanks for helping out and pinging me /u/protestor, sorry I missed this - looks like you helped to sort it out either way :)
Agreed, variadics and genericness over integers are needed just because of this [horror](https://doc.rust-lang.org/nightly/std/hash/trait.Hash.html).
The simplest would be `Box&lt;FnMut(i32)&gt;`, which entails a heap allocation, but gives you a concrete type.
If you want a non-generic type, you don't really have any other option that I can think of.
How would this be done? Each module compiled to a separate object I guess. You could do it easily by just serializing the AST (may change with a new version of rustc, but then it will just recompile the .rs instead of using the .o), but most of the compile time is in the llvm/bytecode generation which would still have to be repeated.
That's a very different situation, from what I understand.
Its ok ;-) And thank you. Hopefully people will get more mindful of it in the future. 
leg day.
I really hate this, I'd rather come up with a syntax that does away with commas altogether :(
Not even Haskell does not allow eliding those, it only does for `{;}`, not `{,}`. Lists also still have commas, after all. Maybe we'll get struct Point where x: i32 -&gt; Point y: i32 -&gt; Point some time?
From the PoV of someone who started tinkering with Rust just a few weeks ago: 1. Adoption of one of the various try-catch proposals to reduce the clutter of error handling code. As it currently stands, both the "Go way" (lots of `if let`) and the "JavaScript way" (promise-like method chaining) of dealing with `Option`/`Result` are really cumbersome, while the `try!` macro is quite limited. 2. Enums that define an implicit anonymous trait which is implemented by its variants, allowing for simple dynamic dispatch, plus Methods specific to enum variants (like in Java). 3. ~~Ability to refer to the whole pattern when matching (if it doesn't exist already), like foo@ in Haskell.~~ It's already there, yay! 4. User-defined attributes for better AOP (and because I love Python decorators).
This isn't exactly borrow checking.
Its kind of broken and umantained at the moment, but with a little love that project has a huge amount of potential.
I want an IDE with type-inference based suggestions/auto-completion, full-featured refactoring support, `use` management, the works...
Came to post this. Don't get fancy unless you need to (or as a learning experience) - simple array indices are way underrated.
I would want this too, if it wasn't because Rust's semicolons have a special semantic meaning.
The problem is that Rust is an expression-oriented language, thus semicolons are an inevitable part of the language.
Erick and eddyb were working on a proof of concept (plugin?) for this, fyi.
What would `?` actually do? Isn't it just equivalent to one of the combinators?
Would it be plausible to compile each _module_, rather than each crate, and only recompile if the module was changed? Would that have an appreciable effect? Edit: Also do I understand the current state of affairs correctly? Does the entire crate get recompiled each time there is a change?
I disagree. As a functional language it's much, much too verbose and as an imperative language it's just way too complex. If I have the luxury to choose between Rust and a language with GC such as Go, the latter will always win. There are just too many rules that don't make sense. Flexibility is something you have to be careful with but Rust is all about flexibility. It makes the mental model quite large.
`?` is equivalent to `try!`, but with the long-term intention to be valid for `Option` types and other types that meet a certain trait bound.
What are the advantages of that?
Ah ok thanks, so it's not just sugar for something already possible in the language?
More lifetime elision&lt;'a&gt; so you&lt;'a&gt; don't have to&lt;'a&gt; sprinkle&lt;'a&gt; your&lt;'a&gt; code&lt;'a&gt; with `&lt;'a&gt;`s just because&lt;'a&gt; your&lt;'a&gt; struct&lt;'a&gt; (or trait+'a) has a&lt;'a&gt; borrowed reference&lt;'a&gt;.
If it's independent of both, how do you efficiently go from the handle to the object?
You do need some kind of map. It wastes a few CPU cycles, but the table itself should be small enough to stick around in the L3 cache. And there are a number of important benefits. For some reason I can't find the articles I remember about handles in Entity-Component systems, but here's Vittorio Romeo's C++ implementation of this: https://www.youtube.com/watch?v=_-KSlhppzNE
Hi Xirdus! I'm sorry to bother you here, but I don't know how to reach you otherwise. It seems you keep coming on IRC while I'm absent, so I can't help you in time. Next time you have an issue, can you drop me an email (see https://github.com/Geal for my current email address), that way I can answer when I'm available :)
Crates are the current unit of compilation, so a change to a crate causes it all to be recompiled. The work on incremental compilation is looking to change that, and once it does, then you can actually do even better than just caching modules.
Good to know.
The proposal is still fuzzy, its first implementation will likely just be sugar for try. Goodies can come later. :)
Seconded, I think it should be possible to come up with an easy rule for eliding lifetime parameters on structs in simple cases, which would help avoid a lot of the refactoring hazard caused by introducing references into a struct. However, the harder battle will likely be to convince people that such convenience is worth the implicitness. 
Right. A simple case should be when the lifetime isn't even used: the problem isn't so much the initial `struct Foo&lt;'a&gt;`, it's having to change (e g) all of `impl Trait for Foo` to the noisier `impl&lt;'a&gt; Trait for Foo&lt;'a&gt;` even though you're not actually using the `'a` anywhere inside the trait implementation. 
Wrong sub, check out /r/playrust instead
This is the wrong subreddit. It's about the Rust programming language, not the game. EDIT: Are you looking for /r/playrust ?
Well make sure you have the development environment installed right. Are you using Cargo? Give us some more information on the problem. Just screwing with you dude, you got the wrong subreddit. 
Lots of sugar pieces. Things like being able to use default arguments, or having named parameters, or even just varargs without having to resort to macros. I love Rust, I'd just love to love it even more... :P
 let javascript = "puts('Test Iñtërnâtiônàlizætiøn ┬─┬ノ( º _ ºノ) ');".to_string(); You guys.
I've used sync channels for audio before. Sending a buffer of 16 samples at a time works quite well. You can use larger buffers for lower overhead at the cost of higher latency. I wouldn't recommend unbounded channels; you'll have issues with both memory usage and latency being unbounded. See https://github.com/monsieursquirrel/organn/blob/master/src/basic_types.rs
Examples are actually managed by Cargo as well, you can use `cargo run --example &lt;example name&gt;` to build &amp; run an example. So `cargo run --example teapot` will build and run the teapot example for glium.
Wow that's great. It works. Thank you very much!
[removed]
how does it fix the same problem ? curious
I'm making a RTS game, and I found that unless you're using more than 500 points, there is actually no need for a KD-Tree (which is usually faster than a quad tree). The naive implementation of comparing each point to each point is better. If you have more than 500 points/bots, then you'll probably find that the time spent "rebuilding" the tree is miniscule compared to the benefits. Here's a benchmark using 500 points: KDT build time: 0.053959ms KDT search time: 0.300469ms Naive search time: 0.33739ms Improvement: 0.9519282 KDT in range: 686 (To ensure correctness) Naive in range: 686 Here's a benchmark for my code using 4096 points: KDT build time: 0.612015ms KDT search time: 7.08858ms Naive search time: 22.421328ms Improvement: 2.9116359 KDT in range: 16720 (To ensure correctness) Naive in range: 16720
Without knowing why you want to do what you're doing, I'm finding it difficult to help. If you want to, say, create trait objects, so that you can call the optimize function without knowing at compile time which struct you're calling it on, then you need all of your functions have the same signature. You could do this in your example by changing `compute_other_grad` to fn compute_other_grad(&amp;self, _: Vec&lt;f64&gt;, data: &amp;Matrix&lt;f64&gt;, _: &amp;Matrix&lt;f64&gt;) -&gt; Vec&lt;f64&gt; and then passing it an empty `Vec` and `Matrix` for the unused arguments. If you don't need to do that, and are going to be calling a version of optimize that takes a different function signature for each struct, then you could just make `optimize` a member of each struct and do away with traits altogether. Or, if you're going to end up with a few different flavors of optimize, but still want to use traits, then you could make multiple optimize traits.
You can also use a Cow for this: http://is.gd/sVWH2w Unlike String, Cow doesn't necessarily create copies, but comes with a slight overhead when dereferencing.
You can solve this with [associated types](https://doc.rust-lang.org/book/associated-types.html). Here is one example. (I don't know if this specific design is right). Suppose we had a trait `Optimizable` so that `NeuralNetwork : Optimizable`. Add to it two associated types: `Input` (to mean `Matrix&lt;f64&gt;` in the case of `NeuralNetwork`) and `Output` (`&amp;Matrix&lt;f64&gt;` perhaps?). pub trait Optimizable { type Input; type Output; fn optimization_fun(&amp;self, input: T::Input) -&gt; T::output; } pub struct NeuralNet&lt;'a&gt; { layer_sizes: &amp;'a [usize], weights: Vec&lt;f64&gt;, } impl Optimizable for NeuralNet { type Input = Matrix&lt;f64&gt;; type Output = &amp;Matrix&lt;f64&gt;; // perhaps the output should be `Matrix&lt;f64&gt;` instead fn optimization_fun(&amp;self, input: T::Input) -&gt; T::output { ... } } Then you could write your optimization code like this: pub trait OptimAlgorithm&lt;T : Optimizable&gt; { fn optimize(&amp;self, start: T, f: &amp;Fn(T::Input) -&gt; T::Output) -&gt; T::Output; } pub struct GradientDesc { alpha: f64, iters: usize } // the impl here edit: okay and the impl is like this: impl&lt;T: Optimizable + Clone + Mul&lt;f64, Output=T&gt; + Sub&lt;T, Output=T&gt;&gt; OptimAlgorithm&lt;T&gt; for GradientDesc { fn optimize(&amp;self, start: T) -&gt; T { let mut optimizing_val = start.clone(); for _i in 0..self.iters { optimizing_val = optimizing_val.clone() - optimizing_val.optimization_fun(optimizing_val.clone()) * self.alpha; } optimizing_val } } Note that some of the bounds `Clone + Mul&lt;f64, Output=T&gt; + Sub&lt;T, Output=T&gt;` you might want to move to `Optimizable`, to reduce the noise on the impls of the optimization methods. Also: it makes no sense to `.clone` inside `GradientDesc`: if you want to clone, clone it outside the function and pass the clone itself (to go with the Rust philosophy: "don't pay for what you don't need"). Also I'm not sure why you need to `clone` inside this loop.. this seems incredibly unidiomatic for a Rust library. edit: also, this code above is all wrong: I can't decide whether `OptimAlgorithm::optimize` should return `T` (that is, return a new copy of `NeuralNetwork`) or return `T::Output` (that is, `Matrix&lt;f64&gt;` - the matrix of synaptic weights?) or even return nothing (and mutate `start`). Actually that's something for you to decide. But I think you should *avoid* those `clone` calls as much as possible.
How would it be used? `let x = f()?;`? 
Would yo mind opening a bug on cargo for that? we should mention it.
For a more verbose version of this comment, see [this Stack Overflow question and answer](http://stackoverflow.com/q/32300132/155423).
Const parameters are types parameterized by compile-time values. Dependent types are types parameterized by run-time values. So const parameters enable a subset of the type enabled by dependent types. The features the other poster wanted must be in this subset (`[T; N]`s come to mind as an obvious example).
If you know Haskell, associated types work like functional dependencies in type classes. That is trait A { type B; fn method() -&gt; B; } is like trait A&lt;B&gt; { fn method() -&gt; B; } but, instead of being polymorphic in `B`, it forces each type that implements the trait `A` to choose a concrete `B` (you choose the `B` at the `impl A for ..` and not at the call site of each method of `A`)
One more thing, There's a lot of libraries that make heavy use of Rust's type-level features. For example, take [nalgebra](http://nalgebra.org/). It implements pretty generic operations like [QR decomposition](https://github.com/sebcrozet/nalgebra/blob/master/src/linalg/decompositions.rs#L41). It *sometimes* use associated types ([like this](https://github.com/sebcrozet/nalgebra/blob/master/src/traits/geometry.rs#L68)). There is a library for doing machine learning in Rust that implements stuff like gradient descent. (I guess there are more than one), [rustml](https://daniel-e.github.io/rustml/rustml/). It uses bindings to OpenCV (but oddly enough, didn't isolate said bindings to another crate - a big no no in my opinion) and has [GradientDescent for NeuralNetwork](https://github.com/daniel-e/rustml/blob/6c9f1291f3f5ec8c096131727806e7b6d7739386/src/nn.rs#L31). It doesn't use associated types and doesn't try to be generic at all: pub trait GradientDescent { fn gd(&amp;self, input: &amp;Matrix&lt;f64&gt;, targets: &amp;Matrix&lt;f64&gt;, p: OptParams&lt;f64&gt;) -&gt; Self; } impl GradientDescent for NeuralNetwork { fn gd(&amp;self, input: &amp;Matrix&lt;f64&gt;, targets: &amp;Matrix&lt;f64&gt;, p: OptParams&lt;f64&gt;) -&gt; Self { let a = p.alpha.unwrap(); let mut n = self.clone(); for _ in (0..p.iter.unwrap()) { let v = n.derivatives(input, targets).iter().map(|x| x.mul_scalar(-a)).collect::&lt;Vec&lt;_&gt;&gt;(); n.update_params(&amp;v); } n } } You may want to talk with the author about those issues of API design. (I understand that you want to have a generic API so that you implement gradient descent not only for `NeuralNetwork` but for other stuff) edit: not sure if relevant, but, other libraries you could study: * [leaf](https://autumnai.github.io/leaf/leaf/index.html). not heavy on generic/type leve stuff but seems to be on the same problem domain * [rustlearn](https://github.com/maciejkula/rustlearn/tree/ec6b666cffc136f5bf63903212358e6df5b2f9c7) also on the same problem domain. It is [not trait heavy](https://github.com/maciejkula/rustlearn/search?utf8=%E2%9C%93&amp;q=trait) (so I suppose it doesn't contain much generic code) but has some matrix stuff and seems related? * [cgmath](https://github.com/bjz/cgmath), a small vectors library (more like for OpenGL and stuff). This one has [trait bounds on `Self`](https://github.com/bjz/cgmath/blob/f60e85b61ab4b5d3706d36d5f466c9ef5711a1d2/src/angle.rs#L58). In a trait, `Self` is the type that is implementing it, so if you add a bound on it (like `Self : Copy`) you're saying that only certain types may implement the trait. Useful to transport type bounds from a generic parameter to a trait (like passing that `Clone + Mul&lt;f64, Output=T&gt; + Sub&lt;T, Output=T&gt;` into `Optimizable`: just make it so only types that implement those traits can implement `Optimizable`). I think you can also have bounds on associated types (so that the associated type must implement some trait, such as `Mul&lt;f64, Output=TheAssociatedType&gt;`) * [num](https://rust-num.github.io/num/num/index.html). This one has a new numeric trait hierarchy ([like this](https://github.com/rust-num/num/blob/85b9ac58bf420bcf063ef7bf676d9b55cf007df0/src/traits.rs#L22)) but no associated types and other fancy stuff. perhaps not related though, not sure why i added it
oh thank you! sorry about that
Thanks again for taking so much time to help me. All of the various libraries you referred are definitely going to be helpful. I'd seen a few of them before and they do a lot of what I'm trying to achieve very well. However they also tend to produce quite specific solutions for problems. That may well be the way to go but for now I want to see if I can be generic enough that it's very easy to prototype ML models and just put bits of the jigsaw together. I'm a long way off... I was cloning in the loop as I was fighting the borrow checker trying to mutate the optimizing_val and decided to be lazy for now. I also don't want to mutate the model itself within GradientDesc (to try and keep the optimization and modelling as separate as possible). In my head I want to be able to attach an optimizing algorithm to a model (with some sensible default) and then use it from the model with something like: model.train(data: Matrix&lt;f64&gt;, targets: Matrix&lt;f64&gt;); This would the instantiate the struct for my optimization algorithm of choice and run optimization (using the generic interface you've shown examples of above). You've certainly given me a hell of a lot to work with and I'm feeling a lot more confident that I can do what I want to now! Edit: Oh and one more thing. You mentioned moving constraints to the Optimizable level. Can I do this with associated traits, e.g. type Input where Input: Sub&lt;Input, Output=Input&gt;; 
I cannot change struct Text {}, it takes a &amp;str.
I cannot change struct Text {}, it takes a &amp;str. 
It's not dependent typing that's hard to understand, it's what people do with them: You get a system that is turing complete (well, as long as you can prove your stuff to be terminating), that's expressing constraints on other code, of course it's not going to be easy to grok if you're not used to that. Other language implement such things, well, in their type system, but inside the compiler. That's why I mentioned the library curtain. Also, it's not just about *dynamic* terms. Things rust currenttly does with macros even though there's no need (because the syntax is valid rust) such as println can be done type-safe with dependent types. And sometimes the term you want to pass is actually just a type. It's really about *joining* the term and type level, you don't have two separate languages, any more. In fact, people should use that as introductory example, not vectors.
https://github.com/erickt/stateful, in case anyone wants to have a look.
To be fair, @mut did have debugging aid with backtrace too, enabled by RUST_DEBUG_BORROW environment variable. Debugging would have been *more* nightmarish otherwise.
[Diesel](https://github.com/sgrif/diesel) has been announced ([reddit thread](https://www.reddit.com/r/rust/comments/3ur9co/announcing_diesel_a_safe_extensible_orm_and_query/) and [blog post](https://medium.com/@sgrif/announcing-diesel-a-safe-extensible-orm-and-query-builder-for-rust-fdf57966a16c#.i6pw39k2e)) a couple of weeks ago and seems to be rapidly developed. Also the developer is one of the maintainers of Rail's ORM, so he has a ton of experience in that domain. I haven't used it myself, but if I should need an ORM that is the first one I would look at. **Edit:** As mentioned by /u/daogangtang [rustorm](https://github.com/ivanceras/rustorm) is probably more mature. If you need to do *"serious work"* **now**, I guess you should probably opt for that :)
It's a local variable, it's on the stack. Local variables are *always* on the stack, no exceptions (that I can think of off the top of my head :P). What happens in the abstract is this: * You enter `main`. * `main` allocates 4 bytes on the stack to store a `Foo`. Let's call it `Foo_0` * `main` calls `Foo::new`, *passing it the address of `Foo_0` in a hidden argument.* * You enter `Foo::new`. * `Foo::new` allocates 4 bytes on the stack to store a `Foo`. Let's call it `Foo_1`. * `Foo::new` stores `42` in `Foo_1.n`. * `Foo::new` copies `Foo_1` into the storage of `Foo_0` in `main`, and returns. * Control returns to `main`. * `main` reads `Foo_0.n` from its stack frame, and prints it out. One complication is the existence of "named value return optimisation"; this basically means that because the compiler *knows* it's going to have to copy from `Foo_1` into `Foo_0` at *some* point... it's actually free to just not bother creating `Foo_1` *at all*. Instead, it just writes directly through the hidden pointer argument to `Foo_0`. 
You're looking for /r/playrust. This is the subreddit for the programming language Rust.
You may want to go over to /r/playrust - here's the programming language, not the game.
Thanks! That really clears up my confusion. I didn't know about return value optimization, but now I do. TIL! :)
I worked as system programmer in wargaming.net and in [Zillion Whales](http://zillionwhales.com), but have never done whole game myself. Rust is interesting to work with, it's the least of my problems)
I don't have experience with that but my guess is that it either delegates the task to some C code (possibly glibc) or uses embedded assembly language, depending on the library in question.
Aww man, I thought the name HurtWorld was going to be a jab at fighting the borrow checker.
you can use `Regex::new("(?i)abcd")` to match, but you'll need to replace manually, if I'm not mistaken.
There's a nice detailed explanation on how rust's std mpsc channels work in the src [here](https://doc.rust-lang.org/src/std/sync/mpsc/mod.rs.html#118) (not sure why this isn't a part of the official docs). FWIW, I'm also using these channels to communicate with a real-time audio stream and I haven't noticed any issues - as far as I understand, they are lock free (as long as you use the `try_recv` method on the `Receiver` instead of `recv`). It would be very interesting to see some benchmarks comparing rust's channels and other available 3rd party lock-free queues like [crossbeam](http://aturon.github.io/crossbeam-doc/crossbeam/index.html)'s [`MsQueue`](http://aturon.github.io/crossbeam-doc/crossbeam/sync/struct.MsQueue.html). What are you working on btw? I find it super exciting to see other audio folk experimenting with rust :)
@bondaly This saved my day! Thank you!
I haven't used regexps in Rust yet, but, from the API docs, it looks similar to how you'd do it in Python. The docs I'm looking at don't make any mention of how you're getting an implementation of [Replacer](https://doc.rust-lang.org/regex/regex/trait.Replacer.html) for a closure, but you need to return a new string which will be inserted in place of whatever `&amp;Captures` matched. That means iterating through the matched string and using something like a `HashMap` or indexing into a pair of strings to look up the replacement for each character. (eg. Find the index of `w` in `QWERqwer` and then retrieve the character at that index in `ABCDabcd`) Given what I remember about the performance characteristics of string indexing and the lack of a Rust literal syntax for `HashMap`, I'd suggest writing a simple function to be used equivalently to Python's `string.maketrans('QWERqwer', 'ABCDabcd')` which returns a ready-to-use HashMap.
When you look at the implementors section of `Hash`, there are at least two quite worrying things there: * All the implementations for functions are manually defined. This limits the use to functions with at most 12 arguments (generic A-L), which you can say is pretty reasonable, since writing functions with more than that does not seem like a smart thing to do. Nevertheless, manually maintaining all that isn't smart either. Variadics could allow shortening this to just &lt;Args...&gt; (C++ syntax) and maintaining far less definitions. One problem here might be the `extern "C"` functions, since Rust variadics might not/probably will not be compatible with `stdarg.h` from C. * The implementations for tuples are the same, limited to 12 tuple members, and in them the order of `where` trait restrictions is seemingly random: `where L: Hash, B: Hash, D: Hash,` etc. Somebody got bored when writing this? * As for integers as generic types, this has probably been explained in this thread before, but basically they would allow shortening the implementations of `Hash` for arrays of `T` to just one `[T;N]` where `N` is any integer. Now they are limited to arrays of at most 32 members, which isn't reasonable.
&gt; (Disclaimer: I don't know if rustc actually does this. I'm just assuming it does or will at some point in the future.) Rustc does not do RVO. LLVM does.
How would you deal with name conflicts in external dependencies without `as` here?
That's easy. Just look at how Go solved modules / packages. C2 has also a very interesting approach in dealing with modules. I think that nested modules are only complicating things without *any* benefit and I also think that the `use` keyword is too liberal. Really in Go you only have one way of doing modules, there are no filename modules. I think this could be integrated with crates in a very easy way. I also think that the toml file in crates could be used for building instructions, that way you can keep all the building instructions out of the language. When you have modules at directory level things simplify a lot and it results in much cleaner code. I also like the way how Go organizes module imports, with a `import ( each module in one line )`. It just clears up lots of code. In both Go and C2 you can use the `as` keywords (or similar). But I said it before, there are more things in Rust that could be simplified without performance cost. The syntax for instance has some historical artifacts and testing is too liberal. In Go you only have one way of testing and that works quite well. This too can be integrated in the crates as well and it would make it a lot easier to understand. Then you don't have testing code in ordinary source code anymore, so you know where to find your tests, there is nothing hidden.
"Efficient code reuse" - https://github.com/rust-lang/rfcs/issues/349
Agreed, the comments are great...would be really nice if they were part of the docs. If anyone needs a slightly faster bounded SPSC queue, I wrote a simple one which is backed by a ring buffer (instead of a linked list like the std version): https://github.com/polyfractal/bounded-spsc-queue I haven't compiled it recently, but it *should* still work fine (yay Rust 1.0+)
I've finally found time to continue working on my hobby compiler for a Rust-inspired language, [RusTiny](https://github.com/msiemens/RusTiny). I'm currently working on finishing the instruction selection which requires me to do a liveness analysis on the IR registers. To that end I intend to implement the algorithm described in [Fast Liveness Checking for SSA-Form Programs](http://www.rw.cdl.uni-saarland.de/~grund/papers/cgo08-liveness.pdf). It's gonna be fun!
Thanks. That's very cool. Is there any project boilerplate for Diesel + Nickel/Iron?
That syntax isn't valid, but you can do this: &lt;https://play.rust-lang.org/?gist=01e8080e73b6f2d3a3ba&amp;version=stable&gt;
I started making progress with a smaller spider monkey project. Got a compile examples to compile. Working on cloning the ASYNC Javascript extensions I wrote [link](https://github.com/valarauca/Async-Rhino) in Rust/SpiderMonkey instead of Java/Rhino. Rust Channels will be a godsent just have to get the JS/Rust interface working better. 
The right way to do that is to insert the semicolon when the user starts typing a second statement adjacent to another. For example (where the | represents the text editor cursor): fn some_fn() -&gt; T { fn_call()| The newline doesn't insert a semicolon automatically: fn some_fn() -&gt; T { fn_call() | So I can either close out the function (returning the result): fn some_fn -&gt; T { some_call() }| Or insert another statement: fn some_fn() -&gt; T { some_call(); let| Not quite sure how possible it is, though.
I just checked and I definitely compiled it in release mode. I will try your method once I have a bit more time, [I simply used one of the inputs that was supposed to be solved](http://paste.ubuntu.com/14241449/). Are you making sure the result can be found? It seems to me that you aren't and that simply checks which of the programs can die faster.
Looks like it's not a bug.
Yup. Still pretty shoddily documented, but I'll take what I can get.
I'd be curious to see how a simple grid compares to these.
I made a compiler plugin called [tql](http://git.tuxfamily.org/tql/tql.git) which I developed to explore an ORM with compile-time SQL generation. This compiler plugin allow you to write SQL query using a Rust syntax, like: sql!(TodoItem.filter(done == false).sort(-date_added)[..10]) using an API similar to the one of Django. You can look at the [examples](http://git.tuxfamily.org/tql/tql.git/tree/examples) to see more examples. However, this is alpha-quality and should not be used in production. And I don't think I'll update it in the future. It was just a proof of concept to see what is possible using a compiler plugin.
Had no idea you could use infinite ranges. Cool. 
Instead of writing out the match expression in a case where the error path just returns `None` like this: match users.filter(name.eq(target_name)).load(connection) { Ok(x) =&gt; Some(x.collect()), Err(_) =&gt; None } Can't you just use `map` like this?: users.filter(name.eq(target_name)).load(connection).map(|x| x.collect) An `Option` type is a functor in Rust, and as such, can be mapped over, can it not? At least according to [this post](http://hoverbear.org/2014/08/12/option-monads-in-rust/) that seems to be the case. I am new to Rust, but I know that this is what you'd do in Haskell, using `fmap`.
&gt; QTDDTOT Questions That Don't Deserve Their Own Thread http://www.urbandictionary.com/define.php?term=qtddtot Generally a weekly, stickied post for "little" questions.
Huh, weird. 
Someone should prototype this and try it out. I was pretty dismissive of Go's semicolon insertion rules at first, and I'm still not sure whether they work in a language as expression-oriented as Rust. (I'm a fan of Lua's rules when the language can get away with them, which basically boil down to making an `EXPR EXPR` rule that's identical to `EXPR ';' EXPR`, then mopping up the corner cases like `a '\n' (b)` with newline-sensitive hacks. I don't think these rules would work in Rust, though.) You have a very good point, though, and I'm now curious whether they work. (It should be an easy experiment--you could even implement the rules as an editor plugin and see how many false positives you get.)
I started some work on my Rust ORM but didn't have much time to finish it. However I have plans to continue to work on it in next month. You can find it here: https://github.com/phonkee/treasure
I don't know what you're talking about regarding `println!`, the arguments passed to `println!` are not valid Rust, they're valid in the string interpolation DSL of `format_args!` and this is why `println!` is a macro. I really have no idea what the relation between `format_args!` and dependent types would be, so an elaboration would be awesome.
That would be nice. In the meantime, I've always found people on the IRC channel to be quite helpful.
Yes it would
Ther'es also `-ffreestanding`, which implies `-fno-bulitin`.
Over the last week, I started up http://intermezzos.github.io/ , and hopefully will get a lot more done on it this week. Other than that, just the usual. writing all kinds of docs.
Do we have a Rust group in Skåne too?
A call to `println!` is, if you take away the `!`, syntactically just like any other function call. ...the thing that wouldn't work with a normal function right now is varargs, though, yes, but that's orthogonal: Rust doesn't curry functions by default (or there's some way to do that but my rust-foo isn't good enough). [Here's a type-safe printf in Idris](https://github.com/mukeshtiwari/Idris/blob/master/Printf.idr). It's doing the exact same thing as the rust macros: Parse a string, from that, compute what types the arguments are supposed to have. Haskell could do the same "calculate a type containing lots of `-&gt;` to have varargs" thing, but it cannot do that by analysing a string. You'd have to give in a thing that can be taken apart by typeclasses, that is, Haskell has to dispatch on types, not terms, there. This is what "types can depend on terms" means: The string is a term, and that first argument is specifying the type of the whole function. The code of `printf` and `toFunction` is actually executed at run-time, the rest of the code is executed at compile time, calculating the type the run-time code is checked against. It's no wonder that that's where it's run: You see it called in the types of `printf` and `toFunction`. It's not a hard concept to grok... it's just that most people are way too used to there being a strict separation between term-level and type-level stuff. What that code is doing certainly isn't hard, and it's also not obfuscated (at least not if you can read functional code).
So what you're saying is that you could implement the `format_args!` DSL by parsing the string literal as type parameter expansion rather than using a syntax extension? Conceptually (if not syntactuically) something like `format_args::&lt;"foo: {}, bar: {}"&gt;(foo, bar);`. Presumably you could do the same thing for DSLs like regexprs or pattern matchers. But why? You can already create a well-typed string formatter using a syntax extension. Dependent types are elegant and beautiful in the way that they unify compile-time and run-time execution into a single coherence, but that's not a reason that mainstream languages should adopt them over systems which are more restricted (and consequently more accessible).
The channels weren't designed to optimise the worst case latency, so it depends on how low you need to go. Certainly there can be heap allocations when pushing data to the channel (which can lead to system calls to ask for more memory etc). If you want something that's allocation-less, try [mio](https://github.com/carllerche/mio). Slightly harder to work with initially - but you might want it anyway, if you e g want to wait on either an file descriptor or a message coming through the channel. 
I'm not sure if it's for the username, but the `Regular` badge comes with a "rename" privilege.
Writing an NES emulator as an excuse to learn emulation, rust, OpenGL, and audio generation. Finally got my first opcode working correctly-ish. It's humbling when you realize how complicated even this old CPU from the 80's must have been. Really liking Rust so far. It writes like Scala and runs like C. I mostly work with JVM languages, so it's nice to have a native language that I can tolerate. I haven't had much trouble with the borrow checker, surprisingly. When it kicks in it's usually pretty obvious that I'm trying to do something dumb. I did find the module system to be harder to understand than I would have liked - took me a full hour to figure out how to sort some source files into a separate folder and still have everything compile. It'd be better if I could use src/$folder/$folder.rs instead of mod.rs, because otherwise I end up with multiple mod.rs files and I have to page through multiple tabs to find the one I want. Not sure if the documentation is bad or I'm just too used to the simplicity of Java packages. The built-in test support is pretty limited, but I'll have to use the language more before I decide what I want in a test framework. I look forward to experimenting more with macros, never really done metaprogramming before.
Huh, didn't know about this. It'd be cool to have it there, but I don't know how to add it.
The problem is that your function returns Option&lt;Path&gt;, and Path is an unsized type (= a type that doesn't have size known at compile time), and can't be directly contained as a stack value. Use PathBuf instead. It's a type where the buffer containing the path is heap-allocated, so it can be thrown around easily.
Thanks for the support! Do you know of any Cargo geeks I can CC on this? E: &gt;In my OS, I'm using `make` to drive Cargo. `nasm` builds the assembly, Cargo builds the Rust, `ld` links it all up. I'm hoping to replace the `ld` part by Cargo using the `linker-args` part of the target file. There's only a tiny little bit of assembly, and the only knowledge of the Rust stuff it needs is which symbol to jump to. I can probably push that to link time.
I would support this. Often some threads feel dumb because it's a one sentence fix and somebody just didn't know the right term to google. 
These comments are not really actionable. You've taken the fact that the book has a section called "Complex imports" as evidence that Rust's imports are too complicated, but that section just documents Rust's aliased importing, which is a feature you admit Go and C2 also have. Your proposals range from very possible (more flexible use syntax) to totally unworkable (defining the build structure in Cargo.toml which wouldn't work with Rust's compilation module). They also don't seem consistent, in that some proposals suggest a stronger convention-based approach and some suggest making the system more configurable. None of this is evidence that Rust's module system is poorly engineered or not thought through.
IIRC, Alex is on vacation till next year.
Like [this](https://github.com/nrc/patterns)?
Can't access from my android browser, says certificate is missing. Only mention it in case author is from the site. Was the same last time and I don't have issues with any other sites over ssl.
Looks good!
Anything that depends upon non-core libraries--and in particular libraries that wrap/abstract Operating System calls--are in danger of getting out of date if the `cargo.toml` file is set to use a `[*]` for the version. There was a big library issue with this when a windows library make a breaking change (with semantic versioning done appropriately) and dependent projects still broke because they weren't tied to a specific version. The community is slowly working though these issues (including the main one: don't use `[*]` for a dependency version). 
Well the exact current problem I'm facing tends to not be using `*`, but rather package A 1.0 requires package B 0.1 requires package C 0.5, and package C 0.5 is broken on Rust 1.5, but upgrading to the fixed version of 1.1, means breaking changes were made, so then you need to rework stuff in package B, and in some cases even propagate up all the way to A, and then to your primary program that requires package A. 
I don't have much of a problem with it actually *if* it's a fallback for platforms you don't support natively. I don't like using C as your main compiler backend.
&gt; Does Rust have built-in, overload-able copy constructors? Nope, that was an explicit design decision that separates it from C++. All copies and moves are plain bitwise copies. The equivalent of a copy constructor is the `Clone` trait with the `.clone()` method, which is completely customizable, but isn't ever called implicitly.
The problem here are method calls. It's possible that `X` is a valid statement but the user is actually in the middle of calling a method `X.method` (perhaps in the other line). So even the rule to "insert `;` after pressing enter" is heuristic and fails when you're going to continue the next line with `.method`. But inserting `;` in the middle of the text is even more obnoxious.
What is the status of const fn? I have a domain-specific library that has been on hold for about half a year waiting on the stabilization of compile time functions. Is there any sign of it being stabilized soon?
I don't want to start a flame war here. Please get to the arguments. Have you looked at the modules in Go and C2?
Would love to hear more about Stylo. What does it mean to use Servo's "style system" in the context of Gecko?
I think you need to port the libc crate then, at least partially. You could start with the functions that the linker is missing.
I love when slippery concepts of statistic are explained with plain code for actual people. This kind of posts are really helpful. Thanks!
I've already done that for iron. Still have logger pulling its dep from the registry.
The first thing I note is that `Family` should really be a struct with named fields. You don't actually uphold the conditions of `Ord`, which is required to be &gt; * total and antisymmetric: exactly one of a &lt; b, a == b or a &gt; b is true For you, `!(a &lt; b) &amp;&amp; !(a &gt; b)` does not imply `a == b`. Instead define `eq` in terms of weights to get this. Note that in a larger program you should probably write a wrapper class to implement a custom ordering. I recommend using `#[derive(Debug)]`, rather than doing it manually. Your `Soo much better than panicing.... ;)` comment is right to sound sarcastic. I'd just `unwrap` here, or spend the time on an *actual* error message. Don't use `to_string` on `str`s; it's generic, which makes it slow. Prefer `into` or `to_owned`. `unwrap` *is* an assertion; it seems redundant to have two. `len() &gt; 0` should be `!is_empty()`. I'd change `Box&lt;Error&gt;` to `io::Error`. 
Have you placed the `paths = [...]` in `my_project/.cargo/config`? Reading your comment, it sounds like you may have placed that inside your projects `cargo.toml`, which is incorrect. Also note that it's plural: `paths` not `path` (ask me how I know! Took me ages to figure that out myself :( ) Finally, if all else fails, I've sometimes resorted to patching the checkout in `~/.cargo/registry/src/github-&lt;somenum&gt;/&lt;crate&gt;/`. But this is very much not a good practice so I wouldn't rely on it :)
For me the solution would be to depend on major versions of stable libs, eg 1.* 2.*
Off-topic but generally I find it a bad idea to use different DBE in development and production. This also prevents you from using some DBE specific optimisations. 
qemu can boot a raw kernel -- just pass `-kernel &lt;filename&gt;`. Plus, it supports bits of multiboot 1, so you'll get similar info from qemu as you would grub.
Sure, but I would call it the obvious middle ground, rather than a solution. E g, what if some crate changes its log crate dependency from 1.* to 2.* without changing its API? That wouldn't count as a major version change (or would it)? Still, it could cause logging from that crate to go to nowhere, just by you running "cargo update"... Still, it's probably the best middle ground. At least for now.
I test with both DBEs. The "SQLite for testing" is sort of analogous to testing locally before pushing to the CI server. It won't catch everything a full CI run will, but it often brings some advantage in simplicity or speed that makes it more feasible to run more tests more frequently. As for optimizations, when I said "PostgreSQL for production", I wasn't being quite accurate but I was too tired to realize my mistake. I tend to write apps where easy-to-use, cross-platform installers for offline are mandatory while SaaS-style operation is a desirable but typically optional extra. There's no way I could provide or expect a local PostgreSQL install for Windows users, which means that if I didn't have an ORM to insulate me somewhat, I'd sacrifice scalability altogether and use SQLite.
I'm not sure, but to me it looks like your problem with the UI is, that Conrod is an immediate mode GUI. This means that it (mostly?) doesn't save any state, and just shows what you tell it to show. As you create your variables in the 'set_ui' function, the UI will always show the same state (and you won't be able to see any changes). So you will have to create them somewhere else, then pass a reference to the function you pass to 'ui.set_widgets'. It would probably be a better idea to declare the set_ui function as closure, and then using that closure to mutate the variables holding the path and folders. Something like this (not tested): // First create the variables (your 'state') let mut folder_vec: Vec&lt;PathBuf&gt; = Vec::new(); let mut path_string = "C:\\".to_string(); for event in window.ups(60) { ui.handle_event(&amp;event); event.update(|_| ui.set_widgets(mut |ui| { // Do your update stuff here, referencing the variables 'folder' and 'path_string' // Basically move the things from 'set_ui' here // or add more arguments to set_ui, then call it from the closure })); event.draw_2d(|c, g| ui.draw_if_changed(c, g)); } As for the second problem, I've no idea where it's coming from, but you could try using '.expect("error message")' instead of '.unwrap()', to get a better idea of where the error is coming from. (It does pretty much the same, but gives you the given error message instead).
/u/Machtan is correct - `set_ui` is being called in an update function so the `folder_vec` and `path_string` locals are constructed brand new every time it is called. You should store your `folder_vec` and `path_string` somewhere prior to when the event loop begins - otherwise they'll only last as long as the event in which they are instantiated. The second error looks like an IO related error, probably related to something in `get_dir`. Conrod doesn't do any IO after initialisation (definitely not during the `Button` widget instantiation) so I doubt it's the problem. Even though `get_dir` might be working fine in a separate project, that doesn't necessarily mean it will work fine in another, as IO interactions can occur relatively to where the program is executed. An alternative to using `.expect("message")` as /u/Machtan suggests, you can do a backtrace to get the call stack of the moment at which the program panics by setting the `RUST_BACKTRACE` variable when running, i.e. `RUST_BACKTRACE=1 cargo run`
Messing with a spaced repetition flashcard program, akin to Anki (but with more data-friendly import options...). Just about to get the actual scheduler done, and it's not very fun to play with the same 20 cards over and over again. My Rust might be leveling up soon, though.
That seems pretty bad. I'm going to let the rest of the core team know about this and make sure this doesn't happen again. Thanks.
+1 adding the path to ~/.cargo/config always works for me. Hugely improves over manually tweaking dependency paths.
&gt; 99.9999% of the time, you want to take a &amp;[T] instead of a &amp;Vec[T]. Why is that?
If you only have a few known styles of capitalization in your code base (e.g `oLdsOmetHing_functioN` does not happen), it’s much easier to do exact-string replacements separately. For example `x.replace("old", "new").replace("Old", "New")`
It's a wider type to be able to accept. For example, an array (`[T; N]`) can act as a slice (`&amp;[T]`). Other code is more likely to give you a borrowed slice as well. There's also some argument that `&amp;Vec&lt;T&gt;` involves double indirection; once to get to the `Vec` and another to get to the item. I'm not as sure about that particular though. The only reason I know of that you would want a `&amp;Vec&lt;T&gt;` is to be able to call `capacity()` on it, and it's rare to want to do that and not be able to manipulate the vector, requiring it to be mutable.
I'm Kai and I'm not really a new contributer...
&gt; Similarly, reexporting symbols of another crate, then doing a " breaking change " upgrade of that dependency, is a breaking change, even if you don't change any other parts of your API This seems like something that may not be widely enough realized. If you re-export symbols your semver becomes coupled to the versioning of the lib you re-export from.
What's the benefit of using GHCI REPL compared to writing a small driver program as a separate .hs? The GHCI REPL doesn't look too useful given the syntax of Haskell in which indents are (usually) meaningful: - https://www.quora.com/Why-cant-functions-be-defined-on-the-GHCi-repl
&gt; And in some cases, they also cause bugs that might be hard to catch - e g, if you end up with different versions of the log crate, suddenly logging is broken from half of your program (the compiler happily links in two log crates without even giving you a warning). This sounds like a bug. There is probably a very obvious reason why this does not work, however if something does not work it should not be SILENT.
25 Pull Requests for Christmas? Sounds about right.
&gt; to avoid writing out all four cases of pairs of bools. Match statements on tuples of booleans can help clear up this case when you need to do it though. &gt; Only once you change `Box&lt;std::error::Error&gt;` to something else. Nope - that error happened immediately after I cloned your repo. Note that you import it, but then use a full path in the `Box` value. Thus the import isn't needed. You could have also used `Box&lt;Error&gt;` to use the import. &gt; when I tried to derive `Display` Gotcha. Yeah, you can't derive `Display` because the compiler doesn't know what a good human-facing display would be. &gt; To make it nice to work like cat and read from stdin when passed the path '-' [...] I would like to split that reading code up a little still. Any ideas? I'd make the loading function accept a generic type that implements `Read`, and then have the decision one level up. [Here's an example from my code](https://github.com/shepmaster/sxd-document/blob/5b8b3599a9c8eb950842d7e77276379ca822b55b/src/bin/open.rs#L48-L53). &gt; I find it easier to read when I can keep the types of everything in sight. I actually find the standard practice of leaving off all the types somewhat confusing. Maybe that'll subside when I become more comfortable with everything It might get better with time / experience. I find it annoying to have to retype the same thing over and over, especially if I change the type. Having smaller functions will likely help. For example, in `load_families`, the `collect` goes directly into the function return type. &gt; Does `try` work to break out of map closures and early exit still? Yes, it does. Specifically, the `try!` will return from the closure, and the implementation of `FromIterator` for `Result` will stop at the first error returned by the iterator. &gt; I've moved it so that assertions should only fail when internal invariants are broken, not for invalid input. It's always a tricky thing to decide which code is which, but I think this is the right guiding principle. &gt; Is there a nice way to do that with `last_family` and `next_family`? I couldn't see a way, but that doesn't mean there isn't. I also looked at that, and I couldn't figure one out either. Mostly because of the mutation of the heap inside the loop. There's nothing *wrong* with mutation, it's just good to scope it tightly to help us humans reason about where things may change. &gt; Since splitting it up, I can't pass `num_people` in nicely, so that's axed, so this is moot. You could return a tuple of `(Vec&lt;Family&gt;, usize)` from the load function, or create a little struct for that. It's certainly not impossible, but still of questionable value. &gt; I much prefer the positive test, "are there things?" than "is it not the case that there are no things". I can see this. I do a lot of Ruby work and really enjoy the glut of methods that cover the positive and negative cases, which really leads to a readable program. The one I miss a lot is `select` and `reject`; I can never remember what `filter`does! ^_^ &gt; I'm also a little confused by the double map happening in your load_families. Wouldn't you end up with a Vec&lt;Result&lt;_,_&gt;&gt;? That nice little bit of code is from [`Result::from_iter`](http://doc.rust-lang.org/std/result/enum.Result.html#method.from_iter). It steps through an iterator of `Result`s. If one is an error, it stops and returns the error. Otherwise it collects into any other collectable container. `Option` has the same thing.
&gt; Not sure if the documentation is bad or I'm just too used to the simplicity of Java packages. Well, I don't think that they're bad, but since so many people struggle with modules, empirically, they must be :( It seems like a lot of people assume they work kind of like whatever language they're already using, and it creates confusion. I still haven't figured out what it is about modules that makes this happen. &gt; It'd be better if I could use src/$folder/$folder.rs instead of mod.rs You can use `src/$folder.rs`.
Rust uses minor version numbers.
Well, it _does_ work, as long as you don't try to use the types from the different versions with each other.
I could maybe see 'concise', but why 'consistent?'
&gt; it's generic, which makes it slow. To elaborate here a bit, it's not exactly that _generics_ are slow, it's that it could be faster if it took advantage of it being `&amp;str` specifically, which it can't do. Yet...
I am specifically referring to "logging is broken from half of your program". It seems like their is an unpredicted interaction for the log crate.
I think there are at least two things Rust could do better: 1) Not all crates are equal. For the `log` crate, having two versions is really really bad. For, say, `tempdir`, having two versions might be just fine. As a strawman proposal, how about a lint like `#![allow(link_duplicates)]` (that should probably default to `warn`)? This would be defined at crate level (i e, a crate attribute), and potentially overridable on the `extern crate ` definition for the caller. If you think this sounds good to you, maybe I should write an RFC about it? 2) I was going to write something but found an issue was already filed: [Cargo is overeager to pull in multiple copies of a crate](https://github.com/rust-lang/cargo/issues/2064)
There are [several definitions of mod](https://en.wikipedia.org/wiki/Modulo_operation). There's a good chance, that OP wants something different from what Rust does, for example modulo in mathematical sense (always positive).
If it helps, it kind of clicked for me when I read the rules laid out in this Github issue: https://github.com/rust-lang/rust/issues/8215 I'll try using src/$folder.rs - I'd assumed that that would only work for single files and that if I wanted a folder I'd need a mod.rs file.
Is [this](http://is.gd/cbcjIT) not what you want?
Yep. What happens is that `env_logger` (or whatever frontend you use) only initializes one `log` crate. The other `log` crates remain uninitialized, so all logging from crates using other log crates is discarded without warning.
They seem to have a youtube channel https://www.youtube.com/watch?v=sBkN513ShkQ
Thanks for the elaboration! I'll check it out and see if I can't make improvements. &gt; I'd assumed that that would only work for single files and that if I wanted a folder I'd need a mod.rs file. Ah yeah, it's either `src/foo.rs` or `src/foo/mod.rs`.
I know that CSS is one of the bug reasons that things like adblock have used gobs of memory in the past so I'd imagine it does hold some significant bottlenecks
Nice article. I've been wondering for a while about how the following is a bad API and have no idea how to write a good API for this in rust: let xs = vec!(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12); let ys = vec!(12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0); ks::test(&amp;xs, &amp;ys, confidence); What if my values are stored in a vector of `(x, y)` tuples, or in a vector of twice the length but with the data interleaved. Do I need to copy the data into separate vector to use the API? If `ks::test` is taking slices there is not much that I can do. Is there a way to avoid this? I guess that if the API would take a "zip" iterator instead, where the `xs` and `ys` are zipped together, then the user could build this iterator in whatever way it wants, e.g. by zipping two vectors in the case above. If the values are interleaved it could zip two iterators with a stride, or if the data is in a vector of tuples it could just pass it an iterator into the vector.
People switching from a browser that feels slow is a real thing. Browsers are never fast enough. Matching one CSS selector against one node is not expensive, but a document can easily have hundreds of selectors and tens of thousands of nodes. This performance of this stuff is significant enough that people have written at length about how to manage it: https://developer.mozilla.org/en-US/docs/Web/Guide/CSS/Writing_efficient_CSS Then, of course, browsers add new optimizations and “best practices” need to be adapted: http://calendar.perfplanet.com/2011/css-selector-performance-has-changed-for-the-better/
If you’re ok with a lifetime parameter, you can also borrow a dynamic-dispatch closure: struct ClosureHolder&lt;'a&gt; { pub the_closure: &amp;'a FnMut(i32), }
I wouldn't be opposed to some kind of switch that made Cargo emit an error if you have multiple versions of a dependency. I still think Cargo does the right thing by default though.
Wrong rust. You're looking for r/playrust.
That comparisons is almost fair: `let x` declares a private binding, whereas `fn foo`, `trait Foo`, `const` or `static` all declare public or semi-public (that is, `mod` visibility) bindings, which follow the design principle of requiring explicit typing.
Holy fuck you're right. My bad ;_;
Would be pretty cool if they added some way to do the mathematical modulo to the standard library.
I have had experienced this too. Just checked out `logger` as well and modify its `Cargo.toml` to use your own version of local `iron`. It should do the trick.
We aren't really focusing on memory usage, mainly on rendering performance. Restyling (including shaping and frame/renderobject construction) is often about 50% of rendering time, and I think it'll grow to more and more as there's lots of low-hanging fruit elsewhere. So optimizing the time profile of restyling is an extremely important goal for us.
So by consistency you mean less stuff? I can buy that.
Consider placing your tests inside a ```#[cfg(test)]``` block, but more importantly, having lots of small tests that can be run automatically by cargo test. E.g., #[cfg(test)] mod test { use super::*; #[test] fn test_generates_assignments_only_two_people() { let people = vec![Family { people: vec!["Abba".to_owned()], first: true, }, Family { people: vec!["Betty".to_owned()], first: false, }]; let expected: Vec&lt;String&gt; = vec!["Betty".to_owned(), "Abba".to_owned()]; assert_eq!(find_assignment(people), Some(expected)); } #[test] fn test_generates_assignments_three_families_one_couple() { let people = vec![Family { people: vec!["Abba".to_owned()], first: true, }, Family { people: vec!["Betty".to_owned(), "Bob".to_owned()], first: false, }, Family { people: vec!["Cathy".to_owned()], first: false, }]; let expected: Vec&lt;String&gt; = vec!["Cathy".to_owned(), "Bob".to_owned(), "Abba".to_owned(), "Betty".to_owned()]; assert_eq!(find_assignment(people), Some(expected)); } #[test] fn test_infeasible() { let people = vec![Family { people: vec!["Appa".to_owned(), "Alph".to_owned(), "Alex".to_owned()], first: false, }, Family { people: vec!["Bob".to_owned()], first: false, }]; assert_eq!(find_assignment(people), None); } } (Don't forget to make Family and find_assignment public!) Rust makes unit testing of your code so much easier by making it possible right there in the same file for you, and the ```#[cfg(test)]``` ensures all tests get compiled out of release builds. You're using rustfmt now, which is great - no need to worry about what does and doesn't need indenting etc, and you've cleared the compiler of all warnings. ```find_assignment``` has an interesting return type of ```Option&lt;Vec&lt;String&gt;&gt;```. I'd argue it should return ```Result&lt;Vec&lt;&amp;str&gt;, SomeErrorEnum&gt;```, or even better ```Result&lt;Vec&lt;&amp;Person&gt;, SomeErrorEnum&gt;```, where Person is either a struct owning a String, or ```type Person = String```. Result makes more sense over Option in this situation as there are multiple reasons for us not getting a valid output from ```find_assignment```, and we should clearly communicate that. As for the ```&amp;str``` instead of ```String```, it's much cheaper to pass a view into some pre-existing memory (we already know the name "Betty" for example) rather than create a new String object entirely. This is very similar to what shepmaster mentions with using &amp;[T] instead of &amp;Vec[T]. Consider using match find_assignment(people) { Some(assignment) =&gt; { print_list(&amp;assignment); } _ =&gt; { println!("The list of people had no valid solution"); } } at line 45, the current ```if let Some(assignment)``` reads awkwardly, and ```match``` is more Rust-y. Shout if anything's not clear! :)
I compare Vec&lt;T&gt; to &amp;[T] when used as a part of a struct. Element needs to be Sized, so [T] is not an option. But &amp;[T] requires a lifetime specifier (if im doing it correct), when Vec is just a sized pointer that I can own.
Published package systemd_socket (https://github.com/viraptor/systemd_socket) for plug&amp;play socket activation from systemd. It's a port of sd-daemon code, so that there's no extra ffi / linking required (works on stable!) and it mostly reuses interface of https://github.com/jmesmon/rust-systemd Currently it relies on some modifications in nix, but as soon as they're merged, it will be published to crates.io. There's also a PR in iron (https://github.com/iron/iron/pull/415) to allow taking pre-created network listeners so that you can have iron with systemd-activated sockets.
Glad it helped!
Your Git web thing doesn't render markdown, so here is your readme: --- | SQL | Rust | |:----------------------|:--------------------------| | SELECT * FROM Table | Table.all() | | SELECT field1 FROM Table | Table.only(field1) | | SELECT field1 FROM Table | Table.defer(pk) // Exclusion de champs. | | SELECT * FROM Table WHERE field1 = "value1" | Table.filter(field1 == "value1") | | SELECT * FROM Table WHERE primary_key = 42 | Table.get(42) | // Raccourci pour : | Table.filter(primary_key == 42)[0..1]; | | SELECT * FROM Table WHERE field1 = 'value1' | Table.get(field1 == "value1") | // Raccourci pour : | Table.filter(field1 == "value1")[0..1]; | | SELECT * FROM Table WHERE field1 = "value1 AND field2 &lt; 100 | Table.filter(field1 == "value1" &amp;&amp; field2 &lt; 100) | | SELECT * FROM Table WHERE field1 = "value1 OR field2 &lt; 100 | Table.filter(field1 == "value1" || field2 &lt; 100) | | SELECT * FROM Table ORDER BY field1 | Table.sort(field1) | | SELECT * FROM Table ORDER BY field1 DESC | Table.sort(-field1) | | SELECT * FROM Table LIMIT 0, 20 | Table[0..20] | | SELECT * FROM Table | WHERE field1 = "value1" | AND field2 &lt; 100 | ORDER BY field2 DESC | LIMIT 10, 20 | Table.filter(field1 == "value1" &amp;&amp; field2 &lt; 100) | .sort(-field2)[10..20] | | INSERT INTO Table(field1, field2) VALUES("value1", 55) | let table = Table { | field1: "value1", | field2: 55, | }; | table.insert() | | UPDATE Table SET field1 = "value1", field2 = 55 WHERE id = 1 | Table.get(1).update(field1 = "value1", field2 = 55); | // ou | Table.filter(id == 1).update(field1 = "value1", field2 = 55); | // ou | let table = Table.get(1); | table.field1 = "value1"; | table.field2 = 55; | table.update(); | | DELETE FROM Table WHERE id = 1 | Table.get(1).delete(); | // ou | Table.filter(id == 1).delete() | | SELECT AVG(field2) FROM Table | Table.aggregate(avg(field2)) | | SELECT AVG(field1) FROM Table1 GROUP BY field2 | Table1.values(field2).annotate(avg(field1)) | | SELECT AVG(field1) as average FROM Table1 | GROUP BY field2 | HAVING average &gt; 5 | Table1.values(field2).annotate(average = avg(field1)) | .filter(average &gt; 5) | | SELECT AVG(field1) as average FROM Table1 | WHERE field1 &lt; 10 | GROUP BY field2 | HAVING average &gt; 5 | Table1.filter(field1 &lt; 10).values(field2) | .annotate(average = avg(field1)).filter(average &gt; 5) | | SELECT Table1.field1, Table2.field1 FROM Table1 | INNER JOIN Table2 ON Table1.pk = Table2.fk | #[sql_table] | struct Table1 { | pk: db::PrimaryKey, | field1: i32, | } | #[sql_table] | struct Table2 { | field1: i32, | fk: db::ForeignKey&lt;Table1&gt;, | } | Table1.all().join(Table2) | | SELECT * FROM Table1 WHERE YEAR(date) = 2015 | Table1.filter(date.year() == 2015) | | SELECT * FROM Table1 WHERE INSTR(field1, 'string') &gt; 0 | Table1.filter(field1.contains("string")) | | SELECT * FROM Table1 WHERE field1 in (1, 2, 3) | Table1.filter([1, 2, 3].contains(field1)) | | SELECT * FROM Table1 WHERE field1 LIKE 'string%' | Table1.filter(field1.starts_with("string")) | | SELECT * FROM Table1 WHERE field1 LIKE '%string' | Table1.filter(field1.ends_with("string")) | | SELECT * FROM Table1 WHERE field1 BETWEEN 1 AND 5 | Table1.filter(field1 in 1..6) | | SELECT * FROM Table1 WHERE field1 IS NULL | Table1.filter(field1.is_none()) | | SELECT * FROM Table1 WHERE field1 REGEXP BINARY '\^[a-d]' | Table1.filter(r"\^[a-d]".is_match(field1)) | | SELECT * FROM Table1 WHERE field1 REGEXP '\^[a-d]' | Table1.filter(r"\^[a-d]".is_match(field1, db::CaseInsensitive)) | | CREATE TABLE IF NOT EXISTS Table1 ( | pk INTEGER NOT NULL AUTO_INCREMENT, | field1 INTEGER, | PRIMARY KEY (pk) | ) | #[sql_table] | struct Table1 { | pk: db::PrimaryKey, | field1: i32, | } | Table1.create() | well it didn't quite render as expected (Reddit doesn't allow multi-line input in each table cell; had to render with [this](https://www.reddit.com/r/tabled/wiki/table-format#wiki_multi-line_answers) trick) I suggest hosting some online documentation somewhere. Also uploading the crate to crates.io
Perhaps you could could create the window with [glutin](https://github.com/tomaka/glutin) and render with [glium](https://github.com/tomaka/glium)? See [this example](https://github.com/tomaka/glium/blob/master/examples/blitting.rs), to run it type git clone https://github.com/tomaka/glium cd glium cargo run --example blitting edit: it pulls a lot of dependencies, but perhaps you can disable some of them with feature flags.
As a stand-alone program it's not that useful; it's useful because it integrates directly into emacs, where you have your current file open + a REPL buffer which has everything in that file accessible to it. So as you're writing `whatever.hs`, you can at any time just tab over to the REPL buffer and try out your code in its current form instantly.
Actually I was going to recommend Piston and its 2D thing (it's in the example in the front page of http://piston.rs methinks), but OP wanted something that isn't Piston.. But something optimized around "old school" graphics but with the highest possible performance (?) would be cool. Also: never heard about persistent mapped buffers, [here is a link](http://www.bfilipek.com/2015/01/persistent-mapped-buffers-in-opengl.html) for anyone curious. My issue is whether Mesa implements it, and for what drivers. edit: yes it is, extern crate piston_window; use piston_window::*; fn main() { let window: PistonWindow = WindowSettings::new("Hello Piston!", [640, 480]) .exit_on_esc(true).build().unwrap(); for e in window { e.draw_2d(|c, g| { clear([1.0; 4], g); rectangle([1.0, 0.0, 0.0, 1.0], // red [0.0, 0.0, 100.0, 100.0], c.transform, g); }); } }
Thank you for your efforts, Phil!
So, a Rust SFML-like library ? Would be awesome ! :D In the meantime, there is still http://www.rust-sfml.org/ if /u/bkv wants to have a look.
Generalising will require bounding by some trait(s) and supporting references can happen by tuning the impls, like how that standard library implements `Eq` for `&amp;T` when `T: Eq` and `Iterator` for `&amp;mut T` when `T: Iterator`. E.g. one might have (short because I'm on a phone) fn test&lt;I, T&gt;(iter: I, ...) where I: IntoIterator&lt;Item = (T,T)&gt;, T: KsTestValue { ... } trait KsTestValue { ... } impl&lt;'a, T: KsTestValue&gt; KsTestValue for &amp;'a T { ... }
Be aware that SDL2 comes with the basic caveat that it's only for per-pixel graphics. If (or rather, when) you decide you want to use a shader to do dynamic effects, you'll be forced away from the render api and into pure opengl. Although https://github.com/grimfang4/sdl-gpu can help with this, you end up using the sdl-gpu api, not the render api. The render api for SDL2 is still, basically, a super-charged version of the SDL1.2 api; ie. for blitting blocks of pixels to the screen, and nothing more complex than that. &gt; Everything else is runtime-detected. This is also somewhat misleading. The SDL2 runtime support is chosen when you compile the library. Very few of the components (eg. image loaders) are loaded at runtime based on availability. 
&gt; Not if the dependencies are in a tangled tree of packages somewhere down the line and you have to patch the temporary copies of all those packages just to have your project built. No, I'd still rather do this than have no idea why my program is broken. I can usually make those kinds of patches quickly. I think that -- if I did not know the issue -- the multiple `log` version problem could easily take me a few very frustrating days to track down. &gt; Also, pulling several version of a package shouldn't make your program broken, as it only happens whey the different packages share no surface API. That the log is broken in such a setup is most probably a fixable bug of the log crate. Any case where a crate uses a singleton, including non-obvious singletons like PID files, will probably cause either some kind of break, some weird behavior, or excess resource use (i.e., loading multiple copies of something that only needs one copy). I gather that's why `log` is broken: it requires a once-per-program initialization (to set up a singleton log sink), which ends up only getting called on one version of `log`. Crates that use no global state should probably work, but it's not like Rust enforces Haskellian purity.
Is there a demo written in Rust using OpenGL 1.5? I'd like to test it against [glshim](https://github.com/ptitSeb/glshim) on my arm SBC.
You're using the manually unrolled dot product &lt;3 &lt;3 Edit: by the way, a tip, if you use "default-features = false" on your num dependency then you don't have to compile bignum and rustc-serialize (assuming you don't use them).
You could introduce interior mutability through a RefCell to solve this problem: http://is.gd/dRfcCh This solution introduced the interior mutability in the Table datastructure, but you could also put it in the Data objects instead if you wanted to. **Multiple Owners**: Rc (Thread Safe: Arc) **Ownership Cycles**: Rc + Weak (Thread Safe: Arc + Weak) **Reference Cycles with Value Types**: Cell (Thread Safe: Mutex) **Reference Cycles for any Type (Runtime Safety)**: RefCell (Thread Safe: RwLock) **Reference Cycles for any Type (No Overhead)**: UnsafeCell
In this case, you need to move the functionality of `Data::link_to` to the link function (which I assume should move into being a member function of `Table`?). Something like this: if t.get(leaf).and_then(|l| if l.linkable { Some(()) } else { None }) .is_none() { return false }; let root_d = if let Some(d) = t.get_mut(root) { d } else { return false }; root_d.links.push(leaf); 
&gt;Be aware that SDL2 comes with the basic caveat that it's only for per-pixel graphics. For what it's worth, this is precisely what I'm looking for :)
Thanks, glutin should expect a bug report related to this [issue](https://github.com/rust-lang/rust/issues/29867). And BTW, this tiny example was compiling for incredibly long before it failed, and the longer parts belonged to the completely redundant wayland bits. Incredible!
I haven't measured recently so I'm not sure. I suspect it might not make much difference since the current caching is centered around loading/masking files, and these will be in disk cache anyway. This and process setup/teardown are a relatively small % of total search overhead iirc. As racer starts using more of rustc for type inference I'm expecting caching to become a much bigger deal though.
Yeah, it did exist. https://github.com/rust-lang/rust/commit/308c03501a9a49d058f2ad76dd17a4e593ce7be7#diff-242481015141f373dcb178e93cffa850L179
Wait, VSCode started to support plugins? If so, it's a great news for me!
I think nearly 2 months away? Something like that :)
I absolutely am! Thanks :) I'll also put that flag up - will definitely help. I will want to use rustc-serialize at some point to serialize the models but that's on the horizon.
Oh my. And I was questioning myself what to choose: Atom or VSCode. Plugins are a great argument for latter. Thanks!
Perfect. Thank you!
No, Rust doesn't support reference polymorphism like that yet. 
Not sure, but it looks like it's here https://leanpub.com/dataflowbook/read
I love when slippery coding practices are explained by implementing a plain statistical concept for actual people.
Indeed, great work! Developing Rust wouldn't be the same without Racer. 
I assume Rust being such a moving target is a little harder to write about. Also, deciding what goes in and what not, is probably somewhat difficult to figure out. No news and progress reports is something else entirely, however. I am a backer and I wish we got more updates.
- [Post on users.rust-lang.org](https://users.rust-lang.org/t/new-rust-meetup-in-cologne-germany-on-2016-02-03/4110) - [Meetup.com](http://www.meetup.com/de/Rust-Cologne-Bonn/events/227534456/) - [Rust Cologne on Twitter](https://twitter.com/RustCologne)
At second look, yeah there are separate crates, but, it might just be that I'm a dum dum, but the suggestions are mostly way over my head. Thanks for the comment though.
&gt; (though they tend to be pretty large orders!) You aren't kidding! Those suggestions range from over my head to way way out of my league. It's a good page to keep bookmarked though. I'm sure some more noob friendly suggestions will pop-up from time to time. 
In case its not clear, I think specifically what's not obvious to people (and definitely wasn't obvious to me) is that you can have `src/foo.rs` and `src/foo/bar.rs`, and `bar.rs` is recognized as a submodule of `foo.rs`.
How's it going?
Any time!
Hm. Okay. I'll think about that.
I'd link to a .gif, but apparently [that's frowned upon](https://www.reddit.com/r/rust/comments/2sn91h/8_trillion_subscribers_we_did_it_reddit/cnr5875?context=1). Specialization (and partial impls) will really allow Rust's type system to shine, allowing for some crazy abstractions. This is probably important enough to warrant Rust 2.0, but (amazingly) it's backwards compatible, so it's a moot point. Many kudos to aturon. ... if only that RFC would land ...
- [Implementing specialization!](https://github.com/rust-lang/rust/pull/30652) - Helping out on /u/steveklabnik1's new book effort. - Starting a lock-free skiplist implementation in [crossbeam](https://github.com/aturon/crossbeam).
&gt; Helping out on /u/steveklabnik1[2] 's new book effort. The Rust book, not the OS Book. Unless you have some spare editing time ;)
I'm used to Ruby, where the language itself doesn't let you, but Rails tries to, and it's complicated and a huge source of confusion.
Someday I'll finish the hypermedia one :(
I use conditional inclusion of modules already.
It would be interested to see a summary of all of the features in the trait system and their interaction with the rest of the language (particularly lifetimes). I'm curious how much of the complexity in the language is due to impl resolution.
Specialization is a feature I've been looking forward to for a long time. Surprising to see it so soon!
Doesn't atom have plugins? VSCode's plugin interface is minimal but usable. Basically you use the pseudo cmdline/file search to update/install plugins and open up a json file to configure the plugin.
&gt; I feel like I should build a small 2D engine with sprites batching (texture arrays + PMBs =~ one million moving sprites per frame), text rendering Look at WebRender :) It's not actually Web-specific, it has no dependencies on HTML and CSS as languages, and it only requires an OpenGL context. I think it'd make a great starting point for a lot of 2D graphics needs—anything that is describable via CSS (which is a lot)—and we're really committed to making it as fast as possible.
Yeah, I'm taking it as a loss. I backed it with enough to get a PDF and paper back copy of both the Rust book and the previous book he wrote on reactive systems. I was asked for my address the day it closed. I have received neither the PDF nor paperback of the reactive systems book. Seeing as how i thought we were promised progress on the new book, I'm betting nothing will show up for the Rust stuff either. Learned me a lesson. First and last time I use kickstarter. :(
Just a question, does impl specialization constrain the design space in such a way that some other requested feature would become harder to implement? (no feature in mind, I'm just trying to understand the cost of merging impl specialization) My understanding is that it throws away parametricity, right? (but I'm not sure what it is good for)
wowee zowee I'm excited for this. Next, integer types!
What is integer types?
Integers as types. Ie an array of a certain size where the size is available to the type system.
Are those somewhat simmilar to dependent types?
 let gift: Gift&lt;Christmas&gt; = aturon.recv(); gift.unwrap()
Roughly 90% of $4,473. Say $4,000. 
I *think* you're looking for /r/playrust ;-)
Some crates use a lot of conditional inclusion. Se for example [my `wayland-sys`](https://github.com/vberger/wayland-client-rs/blob/master/wayland-sys/src/lib.rs#L46-L56).
[removed]
Good point to keep in mind, as Rust marches forward.
Can anyone explain it in a layman sense? Greatly appreciated!
It would let you do something like trait Trait {} impl&lt;T&gt; Trait for T {} impl Trait for i32 {} Currently, those conflict as the second impl is already covered by the first. With specialization, the second takes priority as it is more specialized, so it becomes valid code. It will be really nice for generic programming.
It's the idea that if code is generic on type `T` then it can't inspect the details about `T`, and must treat all such `T`s the same. Or, regarding to bounded polymorphism: if a code is generic on type `T : SomeTrait`, this code can only inspect details about `T` regarding their implementation of `SomeTrait` (and not something that is unrelated). The right search term is [parametricity](https://en.wikipedia.org/wiki/Parametricity). It's highly valued in the Haskell community (eg: see [this](http://bartoszmilewski.com/2014/09/22/parametricity-money-for-nothing-and-theorems-for-free/) and [this](http://www.well-typed.com/blog/2015/05/parametricity/) or [this](https://www.reddit.com/r/haskell/comments/36zo7p/new_blog_post_parametricity_tutorial_part_1/), etc) The observation here is that with ad-hoc polymorphism, you can write a generic method or function with different implementations for each type. For example, in C++ you can overload a method for integers, then implement it for floats, for strings, etc, writing different code in each instance. But with [parametric polymorphism](https://en.wikipedia.org/wiki/Parametric_polymorphism), you treat all such types the same (you write the code once, and it is valid for all types). So we write (in Rust) fn myfunction&lt;T&gt;(value: T) ... And this is parametric (it's the same code for all `T`, we don't special-case our generic function). Of course this kinds of limits what you can do with your `T`. So [bounded parametric polymorphism](https://en.wikipedia.org/wiki/Parametric_polymorphism#Bounded_parametric_polymorphism) comes to the rescue. Suppose that you want to print a value of some unspecified type. So you can, in Rust, add a trait bound to your generic (polymorphic) function: fn myfunction&lt;T : Display&gt;(value: T) ... That way you can print your `T` (but still otherwise treat all such `T` the same). Note that bounded polymorphism lets you to have different behavior for different types, as long as you write this behavior in a trait impl, but it doesn't let you specialize trait impls themselves. That is, you can't first write something valid for all `T`, then write some specific version valid for some concrete type. Impl specialization brings some degree of ad-hoc polymorphism back to the otherwise mostly parametric system of Rust (the side note note that there are compiler intrinsics that already break parametricity anyway, like the `size_of` that was cited)
FWIW, the discussion on walk_dir is here: https://github.com/rust-lang/rust/issues/27707
Not quite. Dependent types bridge the gap between compile-time and runtime whereas generics over values are still purely compile-time. For example, this requires dependent typing: fn create_array&lt;T: Clone&gt;(value: &amp;T, n: usize) -&gt; Box&lt;[T; n]&gt;; The key point is that `n` is only known at run-time and yet end-up being part of a type.
There's a couple different crates on Crates.io that replace `fs::walk_dir`: https://crates.io/crates/walkdir/ https://crates.io/crates/walker/ It's up to you which one to use. I don't see any crates which implement directory copying. Might be an opportunity to publish a crate of your own if you're interested. I'd suggest having an option for the user to provide a progress callback closure of some sort; that way they aren't required to commit to 100% blockage of client code while the copy executes, or so they can get progress reports through a channel if they want to run it on another thread. Addendum: a version of `fs::copy` with the same progress callback option would be neat too. 
I received links to 'Dataflow and Reactive Programming' after the kickstarter closed. Look for deepfriedcode.com in your email. 
Thanks, this was helpful and interesting. It seems as though even non-parametric languages ought to have (and often do have) a culture of parametricity. What I mean is that by convention all functions (except a known few like size_of) should behave parametrically although it is technically possible to implement non-parametric ones. This allows specialized faster implementations while still allowing parametric reasoning in almost all cases. You would need dependent typing and a lot of work to be able to prove that you optimized implementation for a specific type behaves identically to the general version.
Seems like something that should go in separate crate to me!
I think that it should be an unwritten rule that a specialized impl should always have the same behavior as the most general one. Perhaps it's faster, or uses less memory, but overall it should produce the same effects. &gt; You would need dependent typing and a lot of work to be able to prove that you optimized implementation for a specific type behaves identically to the general version. Yeah, unfortunately the compiler can't check that the specialized impl behaves the same way. In general we should still trust the specialized code does the right thing (if only for the programmer's sanity) [unless memory safety is at stake](https://doc.rust-lang.org/stable/nomicon/safe-unsafe-meaning.html).
I spent some time with /u/acrichto trying to puzzle out exactly this question. I think it's mostly orthogonal except for features that care about specifically which impl code might be allowed to run (as opposed to just treating the trait system like a black box). So something like HKT should be just fine, but the old dropck (that depended on parametricity) is not. BTW, we already removed the reliance on parametricity for dropck (see https://github.com/rust-lang/rfcs/pull/1238), partly in anticipation of specialization, and partly because we were unsure of the overall viability (since it's a property that could easily be broken in other ways). There are also positive interactions with future features, such as the [use of specialization to support efficient reuse aka virtual structs](http://aturon.github.io/blog/2015/09/18/reuse/).
Thanks. Actually it not interacting with HKT in a bad way is nice too. Actually, perhaps it's too unrelated, but something about drops: will drop flags ever be removed? Is it considered an ugly hack or something unavoidable? (does C++ rely on drop flags?) It kinds of affects the memory layout in undesirable ways (IMO), all while Rust promises delivering control over low level stuff. edit: actually it seems that the flags [might move to the stack frame](https://doc.rust-lang.org/nomicon/drop-flags.html) (possibly just for functions that actually employ uninitialized valies? "pay only for what you use" and all). And yeah it's totally unrelated to impl specialization.
Also, it's worth noting that in Haskell, overlapping instances leads to incoherence -- see the end of https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/type-class-extensions.html#instance-overlap This does not appear to be the case for Rust, due to our compilation model: we monomorphize, which means that the use of `showHelp` in the downsteam code will result in a fresh monomorphization, which will be aware of the specialization. This, together with the orphan rules, means that specialization does not affect coherence in Rust.
It does. I use it with rust linting, racer, and cargo build shortcuts, and like a hundred others.
Allowing incoherent instances would be `-XIncoherentInstances`. Which you should never, ever, use, it's down-right evil. IIRC GHC actually keeps core around in its `.hi` files so that things can be monomorphised and inlined in using modules, too.
If you take a look at the above link, you'll see that you can get incoherence strictly using -XOverlappingInstances.
It may be I have missed something: I thought specialization was about specializing a method's implementation, however from your various comments and the mentions of interference with type inference it would seem there are other changes available. Can you specialize something else than method's implementations, such associated types or constants?
Quick question on the hematite project: Is the aim to have a similar server system to vanilla minecraft where it runs as a survival server, or to have a plugin like system and provide survival as a default?
Thanks for the plug! I'm the main author of libpnet - if you've got any questions, feel free to ask. What specifically are you working on, /u/0x4B454B, if you don't mind me asking?
I believe the goal is to re-implement it to be indistinguishable feature wise, then supplement it with additional features as it grows. So all existing config files, world files, chat commands, interface should be 100% backwards compatible, a "drop-in" replacement for the vanilla.
They could still do it like that while providing a default implementation but the ability to do game logic however you want, rather than over-writing game functionality in the java equivalents.
It took me a long time to realize how important Piston would become for the Rust community at large. Image rendering, audio processing, font rendering, and GUIs are immensely useful even for people who aren't doing gamedev (including large parts of Servo), and I'm grateful to all the contributors who have so enriched the Rust library ecosystem via these efforts! Very happy to see the pure-Rust TrueType library shaping up. How soon until it's a drop-in replacement for Freetype, and does it work just as well on Windows? Clearly we need to start working on porting flate2 next. :)
Couldn't agree more! But I guess you might have answered your own question with the dead tree remark.
Personally coming to rust, this is well needed. But the `default` really irks me.
There doesn't seem to have been much progress recently on Turbine. I'm wondering what the progress (and timeline and goals) on that project is. 
The idea is that multirust, multirust-Rs, and Rustup will all eventually merge together.
Can't wait until it is. I'm really happy with how the first few chapters are shaping up.
I don't remember ever seeing this macro.
Are you looking for the `if` expression? let a = if x == y {z} else if d == boo(x) {2};
Is that really more simple than using if? let a = if x == y { z } else if d == boo(x) { 2 };
Merging all that stuff into the "all version manager" could be worthwhile. https://github.com/schultyy/avm
You could use a builder style pattern, where your users `set_alg()` and `set_link()` on the builder struct. This toggles an `Option&lt;T&gt;` from `None` to `Some(user_supplied)`. When `build()` is eventually called, if the option is None it uses a default, otherwise uses what the user supplied. Then a fully built `NeuralNet` is returned. let b = NeuralBuilder::new(); b.set_alg(Foo) // Uses alg: Foo and link: Default let nn = b.build(); --------- Alternatively, you could have a `new()` that takes both `alg` and `link`, but provide the user with `NeuralNet::default_alg()` + `NeuralNet::default_link()` helper methods which make it easy to build a new NeuralNet. let nn = NeuralNet::new(foo, NeuralNet::default_link()); --------- Or you could have a new function that accepts Options and checks those. If None, use a default. I don't particularly like this, since it's messy and conflates what an Option is for. let nn = NeuralNet::new(Some(foo), None); --------- Or you could set the defaults and then provide ways to mutate the current algorithm/link, but that introduces mutable state and may not be what you want (e.g what happens if someone changes the algorithm mid-execution?) let nn = NeuralNet::new(); nn.set_alg(Foo); There are probably other ways to structure it, but that's what first came to mind :) 
There are no constructors in Rust. new is just a function like any other, so you can make one function that creates a NeuralNet with the default types and one that takes custom ones.
First off, thank you for your hard work and a happy new year :) As you have mentioned about the `lattice` rule, I find it much more appealing and expressive. From my understanding, as [withoutboats](http://github.com/rust-lang/rfcs/pull/1210#issuecomment-142529621) mentioned, most conflicts and concerns would not occur/or be negligible. In terms of *why* `default` irks me, is much more superficial - it gives out a vibe of *disjoint edge-case verbosity* very akin to C++/Java OOP inheritance patterns. Which, although not inherently bad, feels out of place for Rust's explicit yet lean syntax. 
Thanks for the help! I think I'll implement your helper methods as well as /u/CryZe92 's suggestion below.
One last note here: you _are_ using polymorphism: those generic types, `T` and `U`, let you be polymorphic. 
While the author talks about the possibility of supporting more things in the future, it is clearly very node-centric. You even install node versions with `avm install 4.1.2`. I don't see how it will serve as a viable replacement for multirust.
That sounds like a good idea, but aren't the inheritance plans in direct conflict with it?
I'm not sure about such plans. I saw some blog posts about specialization in relation to inheritance, but I glanced and I didn't understand a lot. Actually, I'm reading the RFC, and sure enough inheritance is a goal. That's interesting. I think that using specialization for anything other than "doing the same thing but with better performance" fails at the principle of least astonishment, because you naturally expect, that if some parent impl is written as valid for all `T`, well, it is. If it weren't valid for all `T`, it shouldn't have been written with an unbounded `T` but instead some marker saying that the `T` may be further specialized (and the specialization is allowed to deviate in behavior). It's useful at least to understand APIs: knowing that `Debug` has a specialized impl for `&amp;str` is just a minor implementation detail (that might be in the API, but might not, or be de-emphasized). But knowing specialization of some trait that actually implements some inheritance scheme is critical, and should be clearly marked as such in the API. I'm thinking that those two cases should even carry some different syntax or something.
I'm not sure, but `src/foo.rs` and `src/foo/mod.rs` being the same makes sense to me.
Using bare libc raw sockets is what I have been doing, works reasonably well: https://github.com/zokier/arpmasqd/blob/master/src/main.rs#L141 (the code is still otherwise heavily WIP so don't pay too much attention to it)
Curious, what benefits would using nix give over using libc directly? It's not immediatedly obvious to me
How did partial impls and default impls ever get mixed in the same RFC? They are very different features with very different use-cases and semantic implications. 
Ahha, I see. That's definitely a legitimate concern, though I'm not sure how best to avoid it.
Partial impls are not terribly useful without a specialization hierarchy to resolve overlap. And I don't think the semantic implications are so different -- in either case, after selecting an impl you wind up walking up the specialization hierarchy to determine the definition of an item you'd like to project. To my mind, partial impls essentially "fall out" of the rest of the specialization story, which is part of why I included them together in the RFC.
Thinking more about this... Perhaps if adding a `default` method allows overloading, could you say it's a *mutable* method? So would using a `mut impl` function prefix make more sense? I guess this would make sense to a `mut trait` too? With that logic, instead of phrasing: &gt;"A default method can be overloaded with another method" You have: &gt;"A **mutable** method can be overloaded with another method?"
That's... not bad! Do you want to drop by the [RFC thread](https://github.com/rust-lang/rfcs/pull/1210) and make that suggestion? If not, I'd be happy to do so and cite your comment here.
[Done](https://github.com/rust-lang/rfcs/pull/1210#issuecomment-168258266). Thanks for the idea!
The specialization order for partial impls is resolved once at coherence-time. They are basically semantic sugar (as such, they don't really have scary semantic implications), a feature used for ergonomic purposes, so we want them to work smoothly. General poset specialization breaks parametricity with all the annoying interactions and ugly/scary semantic implications that brings. They are also basically a trick to gain performance, and we don't care that much about them having sharp edge-cases for implementors because of that.
Holy shit my bad, thanks for letting me know.
Presumably, the command syntax would change?
Thanks for the kind words! It has been the vision from the start to engage people and evolve useful libraries. We are very focused on not being a closed project, but building the community! There has been over a hundred people contributing one or more libraries, and I also had the opportunity to work on external projects. It has been a lot of fun (and a lot of hard work)! The Rust community has grown quite large so Piston is not that big in comparison anymore, but we will continue pursuing new ideas like we did from the start! Freetype is a large and complicated library, it has even won awards, so a pure Rust alternative probably means some of the features, but not all. We'll see!
There is nothing you can express using multiple crates you can't express with one crate as far as I know. The internal dependency graph can even be cyclic, as in `pub use super::*;`
The readme says "Multirust-rs is a reimplementation of multirust in rust", but it doesn't say what multirust is. For someone coming in cold, it doesn't offer any clues as to *why* they want to be using multirust-rs, or what they might want to use it *for*.
What do you mean by "Rust channels"? Are you talking about something other than the channels in the stdlib?
That looks tiny indeed. :) In what ways is your language Rust-like if it doesn't have a borrow checker or traits?
Now there's a blast from the past. :P Are you expecting to be able to connect an actual Dreamcast to an instance of your server?
Connecting the versions before Blue Burst is much simpler since they do about 90% of the game relayed between the "party host" and the joiners, but for now I am focusing on Blue Burst since it is a superset of everything else. Sylverant is another server which already implements cross-version compatibility for everything, even the trial versions, but it is written in C.
Or maybe it's a smart pointer that autoderefs to an Option. :P
C++ basically uses drop flags, but they don't have automatic compiler flags. It's all adhoc library code. Everything is basically unsafe_no_drop_flag, and move ctors are on the hook to somehow mark the struct as moved (e.g. unique_ptr nulls on move, and the dtor checks for null).
"The *only* Rust books of 2015"
yes, that would be helpful.
See also [this](https://public.etherpad-mozilla.org/p/rustup-new-experience).
This was [the `cond!` macro](https://github.com/rust-lang/rust/pull/6333), contributed by /u/bjzaba in May 2013, and [removed](https://github.com/rust-lang/rust/pull/9291) a few months later.
I can't wait to start learning rust. I was going to start soon since I'm between jobs. I thought I'd go through the basics then try implementing Game of Life, probably using Piston to do the basic 2D pixel graphics. Hopefully one day I'll know enough to help with Piston! I've always wanted to help with something open source, the hardest part is finding something that I want to contribute to that I can contribute to.
Hah! That was a while ago indeed!
Gotta start somewhere :D Honestly though, it's more that the syntax is Rust-like (or Rust-inspired). It's more about learning about compiler engineering than creating a Rust clone.
A little bit offtopic, but what about a community made Rust book? We saw a lot of interesting and useful blog posts in 2015 (for example about macros), we may ask the authors to write a chapter for an Advanced Rust book. I miss a book which describes best practices (i.e. Error handling (From, enums, etc.)), common mistakes (lifetime issues, higher-ranked lifetimes, Rc&lt;&gt; hell), etc. The community may review the book and propose changes, clarify some parts. The result would be a high quality book for advanced Rust developers :) What do you think?
I think OP meant to say method overloading.
Is funny, for me the best one is [Why Rust?](http://www.oreilly.com/programming/free/files/why-rust.pdf "Why Rust?"), that is not listed.
I actually don't see so many. I love multirust as a version-switcher because of the number of commands it gives, but I don't see a lot of a difference. EDIT: giving this some more thought, I think that _most_ of the stuff these managers do is actually generic. Env management, installation organization and env switching. Most of their difference come from their external interface. If you aim towards a world where you work with python, ruby and nodejs (say, for something complex, like your jekyll-driven site) and maybe write some parts in Rust, having to align 4 installation procedures and strategies and environment mangling schemes is a really frustrating procedure.
Because the println macro automatically dereferences anything. Most of the time you want the value and not the pointer/reference to be printed. So it's convenient that println does this. If you want to see the memory address instead you can put :p in between the brackets. For reference see [println!](https://doc.rust-lang.org/std/fmt/index.html) [std::fmt](https://doc.rust-lang.org/std/fmt/trait.Pointer.html)
Woah pure rust TrueType sounds incredible. Something like that would have such impact on security.
I'm writing Rust code on Windows and that gave me no apparent reason for why I should be using it 
If you want to debug pointers, you probably want to use `{:?}`.
Is "Programming Rust" even released yet?
What does this even imply ? "println!("address of address of address of x is {:p}", &amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;x);"
Happy New Year! Why not start 2016 with some kernel hacking? :) This post continues the [Writing an OS in Rust](http://os.phil-opp.com/) series. We will ditch our initial page table, that just identity maps the first gigabyte, and switch to a new table, which only maps what's needed and sets the right flags (e.g. NO_EXECUTE for data sections). We also make our kernel stack safer by creating a guard page for it. Please point out any issues in the [corresponding repository](https://github.com/phil-opp/blog_os).
I haven't actually started learning Rust yet, but is there a reason lines 97 to 120 aren't all just in a single unsafe { }? Because I do embedded software and having to repeat that every single line would drive me completely mad.
Your question is very hard to understand. You state &gt; This doesn't even come close to working &gt; &gt; `decoder.read_str()` fails but you never actually state what the error *is*. You also switch between `d.read_str()` in your code and `decoder.read_str()` in your prose, which makes it sound like you might have copied the wrong code and actually mean to refer to another piece of code that calls the variable `decoder`. If someone were to check out your code [at revision bb78cc](https://github.com/archer884/watcher/tree/bb78ccdb55103bdc2da75a4ea69891865fe494da) and compile it, they would get this error: error: this function takes 3 parameters but 0 parameters were supplied [E0061] assert_eq!(Ok("passthru".to_owned()), decoder.read_struct()); ^~~~~~~~~~~~~ This seems like a pretty straight-forward error message to address, but it's in a completely different section of code. It does have a `decoder` variable, but not a `read_str` method, so it doesn't match your prose either. Perhaps you want to revisit your problem, take a little bit of time to polish up your question, and ask it again. Then maybe someone can help you. \^_\^
A couple suggestions. use winapi::winuser::*; use winapi::minwindef::*; use winapi::windef::*; use winapi::winnt::*; As a convenience you can just do `use winapi::*;`. let mut v: Vec&lt;u8&gt; = UTF_16LE.encode(c, EncoderTrap::Strict).unwrap(); Why do you still have this line? You're using `OsStrExt` to convert to wide strings, so there's no need to have the encoding crate hanging around. //better way to make a default object of this? Sure, just do `let mut msg: MSG = mem::zeroed();`. To get the hinstance, since you're not in a DLL, you really can make it a lot simpler by just doing `GetModuleHandle(ptr::null())`.
Rust's `&amp;` _is not_ like C's `&amp;`. In C `&amp;x` is the memory address of `x`. In Rust `&amp;x` is a reference to `x`, which is kind of similar to a pointer to `x` in C. So in Rust `&amp;&amp;x` is a reference to a reference to `x`, which is similar to how you can have a pointer to a pointer to `x` in C.
You can have as much indirection as you need. &amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;i32 is a valid (although not particularly useful) type
&gt; but you never actually state what the error is. On the contrary, I said exactly what I think it is: the method requests a string, but the value isn't a string. Here's the message I get from the test when I attempt to use `read_str` in the test: running 1 test test greetings::tests::can_decode ... FAILED failures: ---- greetings::tests::can_decode stdout ---- thread 'greetings::tests::can_decode' panicked at 'assertion failed: `(left == right)` (left: `Ok("passthru")`, right: `Err(DecodeError { field: None, kind: ExpectedType("string", "table") })`)', src/greetings.rs:55 I *can't* get that message from the actual code because the Decoder trait doesn't call for a Debug implementation and, as such, it's next to impossible (or at least I know of no way) to get any kind of error message out of it, so I didn't include the message because I only *think* that's what's wrong. Ok? The `read_struct` call in the test is not related; I was playing with it to try to understand what I'm supposed to be doing to make the decoder work, but I *realize* that isn't the correct way to call the read_struct function. That's why I highlighted the trait implementation in my link to GitHub instead of highlighting the test code below. :| I don't understand what you think I can "polish up," here, particularly without being able to return any meaningful error from the implementation for Decoder. If you know of some way to do that, please tell me. 
Often times, you try to group together all the things that make an unsafe action safe, even if some of the lines don't require the `unsafe` block. For example, if I had a function that was unsafe if i passed it a value larger than ten, I might write this: unsafe { assert!(v &gt; 10); awesome_function(v); } Because of this, I might actually advocate having multiple back-to-back `unsafe` blocks at times. I don't know if that applies here, as I don't know what makes the functions in question unsafe. 
My original comment doesn't seem to make a whole lot of sense anymore as he appears to have updated the paste.
well yes, because you told println! to do that.
Hey, now that I've had time to read everything, I can safely say that you provided exactly the piece I was missing; I thought I needed to call read_struct or read_struct_field instead, but I couldn't tell how from looking at the documentation. That in mind, what is the zero for? The other parts make sense now, but I'm not clear what that value does. I started won that GreetingCore route initially sometime yesterday afternoon, but the version I tried produced a *lot* of extra code on down the line, with Core versions of other structs that depend on this, so I decided it was better to bite the bullet and learn how to do the actual implementation. I can see now, of course, that your technique for doing the same thing is actually much cleaner than what I was doing, and that's an excellent thing to know in itself; I wasn't aware you could use that GreetingCore::decode thing internally there... I assume that decode is, in that case, provided by the derived implementation? (I had inline debug statements, but because the trait doesn't allow for Debug implementations on the Error types returned by the Decoder, they weren't all that helpful; I wrote the test just to get around that and see what the error message was.) Thank you for taking the time to make all this clearer! This is exactly the missing piece I needed.
Why don't you simply let d: T = unsafe { mem::transmute_copy(&amp;x[3]) }; I'm not sure if Rust currently does TBAA (and if not, whether it will in the future), but it's probably best to avoid `transmute` when you can use `transmute_copy`. Regardless, I like that `unsafe` does not change the semantics of the language. The `as` keyword behaves exactly the same in and out of an `unsafe` block.
&gt; let d: T = mem::transmute(x[3..(3+size_of::&lt;T&gt;())]); A slice, as you've mentioned, is `Unsized`; you cannot store them directly on the stack, because the compiler wouldn't even know what assembler to emit if it was supposed to let you do it. The code, as you wrote it, doesn't even make sense. let d: &amp;T = mem::transmute(&amp;x[3..(3+size_of::&lt;T&gt;())]); That actually makes sense, but it's still wrong. A pointer or reference to a slice is two words long; one word has the pointer to the first item, and the other has the size. You can't transmute it to a reference to a sized type, because that kind of pointer is only one word long. let d: &amp;T = mem::transmute(&amp;x[3]); I think what you want is to just take a reference to the first value of the chunk of your array that you want to transmute, and transmute that to a reference to then type you're trying to synthesize. 
Cool! I hope we can close [RFC Issue #613](https://github.com/rust-lang/rfcs/issues/613)! I don't know much about Actor programming, but I do know that one of the common complaints of Akka is that it is not strongly typed. Your readme says that RobotS is inspired by Akka, so does it follow in its lack of typing?
Many of the projects are small experiments or depend upon each other in order to be worked on and matured - I think for these reasons it's fine to have a lot of immature projects progressing in lock-step, but I think you're absolutely right that it does make the organisation seem a bit fragmented. There's also probably quite a few old abandoned ones which we should get rid of! It would be nice if you could customise the landing page for a github organisation, so that rather than just displaying a massive list of the most recently committed repos we could display a big dependency tree of the whole ecosystem, with links to each repo at each node along with their build status and version (for an idea of maturity). /u/long_void Perhaps we can do this on the main piston.rs site, or a separate wiki page linked to from the description? It could be a fun project to write a program that automated the generation of this tree/dag visualisation for a given user or organisation. I've seen lots of dependency graph visualisations going around, but I think it'd make a huge difference providing links and badges for each node too.
Damn, I fall right into this shortcoming then :) Messages are indeed dynamically typed, but it is mostly because static message typing is a bit limiting. I actually started with a typed version but quickly realized that a lot of features that developers got used to (such as routing actors, dead_letter, proxy actors, always knowing the sender of a message and many more) cannot be implemented (or at least are very hard to) with statically typed messages. It seemed like a reasonable idea to first get an untyped (with which I am more familiar) version to get something rolling. If you are interested in seeing what a strongly typed actor system would look like, you can take a look at [akka typed](http://doc.akka.io/docs/akka/2.4.1/scala/typed.html#typed-scala) (still experimental though). I'll maybe try to reimplement it as typed actors but that would not be in the near future...
So basically what the compiler does is it creates 10 pointers under the hood in memory/stack when you do something like `let x = &amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;5;`, right? Or is there something more subtle going on
I've elaborated on the visualisation idea [here](https://github.com/PistonDevelopers/piston/issues/1022).
&gt; It just seems to be harder to manage structs that borrow. Yup. Borrowing is conceptually much more complicated than owning. Such is life :)
I mean, your second example is almost valid Rust already: let a = if some() { 3 } else if foo() { 1 } else { 3 };
&amp;4 is *not* the address of 4, it's a *reference* to the value 4. An address would be `&amp;4 as *const i32`. References are guaranteed to always point to a valid value, so most things (like formatting) will treat a reference exactly the same way they will treat the value itself. If you print a pointer instead of a reference, you will get what you expect: fn main() { println!("{:?}", &amp;4 as *const i32) } Results in: 0x7f2baf00ec60
In rust, references are guaranteed to be valid - so most things will treat references exactly as if they were the actual values. Rust's auto-deferencing will make sure that if something expects `i32`, and you give it `&amp;i32`, or `&amp;&amp;&amp;&amp;&amp;i32`, or an `i32` value behind any number of references, it will be automatically dereferenced and the actual value will be passed. A reference is *not* a pointer - in rust, a pointer looks like `*const i32`, and will print the address as you expect it too. See my comment above.
&gt; you cant "extend" into a buffer writer with a iterator. Just to reiterate, you have an iterator of `u8` which you collect into a `Vec&lt;u8&gt;` and that's what you are writing to the stream? &gt; Any advantage in re-writing it The thing that comes to mind is that a `BufWriter` has a fixed size buffer. When the buffer is filled, the data is flushed to the underlying type and the buffer is reused. If you collected 2^30 `u8` into a `Vec`, then you'd be using 1GiB of RAM, *then* start writing the data. With a `BufWriter`, you'd [use 64KiB of RAM by default](https://github.com/rust-lang/rust/blob/1.5.0/src/libstd/io/mod.rs#L277) before it started writing the data. This may also cause a latency / throughput tradeoff, as the time-to-first-byte might be lower with the `BufWriter` strategy. It all depends on your data and usage, of course.
You mean `{:p}`? `{:?}` also derefs
&gt; thus (in the future?) be allocated on the stack. I don't think this would be the case. It would mean that you must at some point box the function's return value, and the function would then write directly to that box. You _can_ allow stack DSTs via alloca/etc like C99, but ... you _really_ don't want to :)
&gt; It just seems to be harder to manage structs that borrow. I think this is just as true in other languages, however rust just makes it obvious at compile time *before* you start making pointer spaghetti (this meal was my specialty back when C++ was my kitchen ;) ). I totally agree though, I think rust encourages a much more composition-based approach. I've personally found this to be a great benefit in the long-term scalability of my systems. &gt; The lifetime syntax on structs for borrowed stuff is also really verbose and annoying. I totally remember feeling this way for a while, however I've actually grown to love it! The more I've worked with lifetimes, the more I realise that they're not just *"this thing I need to deal with so that the compiler can guarantee safety"*, they're actually an incredibly expressive tool for creating much clearer and safer APIs. Being able to say that `Foo` should only live as long as `Bar` is a very valuable tool - not only for safety, but for describing the purpose and flow of an API too. An example: I recently wrapped a C API which used a lot of global functions for initialisation and termination of different stages of its use. The original API needed extensive documentation on every function, stating which global functions had to have been initialised or terminated before being called. It also returned error codes to indicate when one of these initialisation or termination requirements hadn't been met. Using rust, I was able to provide a completely safe abstraction on top by wrapping the API in types and adding very specific lifetimes (that's right, *I voluntarily added lifetimes!!*) so that no part of the API could be called outside of the intended "initialisation" and "termination" stages. We get multiple wins here: - the user no longer has to remember the order in which the intialisation/termination process happens as it is safely wrapped behind the scenes. - new users can't accidentally shoot themselves in the foot in the case that they haven't yet extensively studied the docs for the library. - I can remove the old error codes entirely. I think this is an area where rust might be able to offer even greater performance than C and C++ in practical applications, as many run-time checks can be moved to compile time. TLDR: I used to feel the same way, but not only did lifetimes in structs grow on me, I now welcome them :)
&gt; I've personally found this to be a great benefit in the long-term scalability of my systems. I'd be interested to hear more details about this. Maybe you could also clarify what "long-term" means in this case, as Rust 1.0 wasn't even a year ago yet ;-). I really appreciated your example of wrapping the C library; that makes a lot of sense to me.
Sure! You're right, long-term isn't *that* long yet. I started working with rust in mid-2014, I think the version was 0.10.0 at the time. The primary project that I've been working on is somewhere around 50kloc, not including any of the OS libs i've been working on, so not huge. I think the greatest benefit of the "composition-based" approach has been greater modularity. I find that rust's ownership model encourages me to: - really think about the hierarchy of my system. i.e. what is *really* the necessary behaviour of each type, who should govern what process, etc. - pass references to data *only when its actually needed*. I used to have a terrible habit in C++ of storing pointers to other data even if I only needed to use it occasionally - this is something that rust makes very awkward to do. At the time it just seemed easier to hold onto them, but little did my naive self realise how much complexity, ambiguity and seg-faulty-ness this could cause. By the time I found rust, my C++ version of the project had become incredibly hard to refactor and maintain. Keep in mind that this is also largely due to my newbie-ness at the time and that pro C++'ers know to avoid these patterns. The difference is that rust consistently taught me what to look out for at compile-time, rather than a year into an unmaintainable mess via seg-faults. Now, my project is finally approaching its commercial release, and I still find it just about as easy to work on and refactor as I did when I started it :) This is something I don't think I could have said had I continued with C++, though I'm well aware that there are probably experienced C++ers out there who are capable of doing so. Edit: removed a bunch of unnecessary parentheses (I have a bad habit of this).
Personally I would love to see both typed and untyped implementations live side by side, allowing one to pick whichever was more appropriate for the task at hand.
Yes, that is true. I guess I should stress that it isn't that similar at all to a *C pointer*. I mean it is definitely the same in memory, but references act very different in rust than raw pointers in rust or pointers in C.
Hah, so simple. I guess I initially was focussed to much on getting rid of "all that RefCell stuff". Thanks!
You could also do `println!("{:p}", &amp;4)`.
&gt; Because the println macro automatically dereferences anything. It's not the println! macro doing it, it's simply that `Debug` (and a bunch of other formatting traits) is implemented for `&amp;T` to call `T`'s operation.
I've somewhat come to the same conclusion as you, but perhaps not as black and white. I also think that the `&lt;'a&gt;`s are more annoying than useful. But for short-lived structs, say, if you have a `Vec` that owns its data, then an iterator for that vec (which is in itself a struct) could borrow the vec, and then return references to every element in the vec. For long-lived structs, say players and enemies in a game, you might want to do `Rc` (and often `RefCell`) instead of borrowing pointers. Or use some other system, such as storing array indices to some big array instead of storing pointers to the structs.
Thanks a lot! I agree with nemaar, the OSDev wiki is the best practical resource. If you are interested in OS theory as well, read the [Three Easy Pieces](http://pages.cs.wisc.edu/~remzi/OSTEP/) book.
I think you're being a bit too candid here. There are some real theoretical concerns with the concept of a data type entailing a borrowed reference, and your own analogy highlights this quite well: if you "give your friend a reference to your house", that entails a proof that *your house will outlive your friend*. When you say it this way, suddenly the theoretical concerns are very apparent - at best that is non-trivial, and tbh it's probably just plain logically wrong.
I can empathise with this - I think its best to only encourage `RefCell` as more of a last resort, as it subverts one of rust's nicest qualities: the compile-time no-aliasing guarantee.
&gt; Dealing with this panic at runtime sounds much worse than convincing the Rust compiler to validate some borrowing code at compile time. Sure, if you can convince the Rust compiler to validate your borrowing code, but in practice, this will often get difficult. (YMMV.) And then `Cell` and `RefCell` are the only ways to mutate inside an `Rc`. (And `unsafe`, but I would assume you think that's even worse.) The problem is not as bad as it sounds though, if you make the `RefCell`s small enough. E g, avoid doing `Rc&lt;RefCell&lt;Player&gt;&gt;`, instead do `Rc&lt;Player&gt;` and e g: struct Player { id: i32, // Player Id never changes during the Player's lifetime score: Cell&lt;i64&gt;, items: RefCell&lt;Vec&lt;Rc&lt;Item&gt;&gt;&gt;, } ...now the only chance of panicking is borrowing `items` mutably, so only borrow it for very short periods of time (e g when you're about to insert or remove an item).
I agree that a finer-grained `RefCell` is better than wrapping it on the whole thing (and that's why it's the verbose `Rc&lt;RefCell&lt;T&gt;&gt;` is accepted, instead of something like `RcCell&lt;T&gt;`; it's probably better to have `Rc&lt;T&gt;` with internal cells in `T`)
The current implementation of lifetime inference in `regionck` already doesn't give useful error messages in many cases, but there's no reason to believe that it's a fundamental limitation. The interaction between lifetimes in different functions is very limited (ignoring the strange way that local functions and closures are implemented, or at least used to be implemented). It doesn't make sense to directly compare concrete lifetimes between distinct functions, or even distinct invocations of the same function. The most relevant things are likely to be the choice of bounds between distinct lifetime parameters (including equality). Type inference for subtyping is famous for generating types that have a lot of variables and bounds between them, so the inferred types might not be what you expect, even after sanitization.
About error handling: [this chapter](https://doc.rust-lang.org/book/error-handling.html) is very comprehensive, and is based on [this /u/burntsushi's blog post](http://blog.burntsushi.net/rust-error-handling/). There's also an [style guide](https://aturon.github.io/) in the workings (which should eventually become official?), that is a starting point for `rustfmt` rules. The prime "advanced Rust" book is the [Rustonomicon](https://doc.rust-lang.org/stable/nomicon/) but it covers more the unsafe parts (and how Rust features interact with memory safety). I think that we need a book on "idioms". /u/nick29581 wrote [a blog post](https://www.reddit.com/r/rust/comments/3ptlgq/design_patterns_in_rust_blog_post/) laying out work on what should become one (the WIP is [on this Git repository](https://github.com/nrc/patterns)). The majority of it should codify what is already standard practice (like the [builder pattern](https://github.com/nrc/patterns/blob/master/patterns/builder.md) that is used by a number of libraries) Besides it, I think domain specific books (such as "web programming in Rust") would be excellent.
&gt; Most languages that employ a GC don't bother with forbidding aliasing and let all references be mutable, without this being an error (well they have other kinds of troubles because of this, but I'm not sure what). IMO this is one of the biggest issues with GC and one of the reasons why I feel like GC is generally a hack-ish solution to memory safety (particularly in non-pure languages) as it doesn't solve the underlying, core issue which is the aliasing of mutable access to data. IMO aliasing mutable access to data isn't just a memory-access problem - it's a massive logic problem that is extremely difficult to reason about when prevalent in a language. `Rc` is another type I'm very wary of as it subverts another of my favourite qualities about rust: always having a clear owner for all data. Normally if I have some kind of heavily inter-related system or graph-like design, i'll opt for using an actual indexable [`Graph`](http://bluss.github.io/petulant-avenger-graphlibrary/doc/petgraph/graph/struct.Graph.html) type so that I still have a clear idea of who owns what, how long everything lives, etc.
What's the trouble with mutable aliases? Supposing a single threaded program. My reasoning is: if you already don't care about memory leaks, is treating your memory as "infinite" and the GC as merely an optimization; and if you accept your program is going to be single threaded; then mutable aliases probably aren't doing any harm. What a pervasive GC buys you is not having to manage memory at all, it's a simpler programming model (unlike Rust, in which you need to think about the lifetime of your resources). And if the program is short running - say, a small script - leaking memory isn't a problem, because the memory will be freed when the program exits anyway.
You are using the standard Android browser?
In this case, because all entities are so heavily inter-related, I'd opt for using an indexable `Graph`-like type that owned all entities in one big contiguous array under the hood (I guess this might be how an ECS works, though i've never used one so I'm unsure). Each entity would be described as a variant of an `Entity` enum, i.e. enum Entity { Player(Player), Enemy(Enemy), Item(Item), } and each kind of relationship or interaction between entities would be described as a variant of an `Interaction` enum, i.e. enum Interaction { Attack(AttackKind), Dodge(Probability), Kiss(AmountOfPassion), } During a frame I'd first update all the interactions so that all `Edge`s correctly represent the current state of interaction between all `Entity`s. Then, I'd evaluate and apply all of the interactions by traversing the graph in whatever way makes sense for my game. Finally, I'd render my fancy game to the screen :-p I'm sorry I don't have any magic parameters for the `Enemy::do_frame` function to take - it would be too much like trying to shove a square through a circular hole, when we should really have made the square a circle in the first place :-p
Note that methods to read node or edge indices return `Option`. For example, [`node_weight`](https://bluss.github.io/petulant-avenger-graphlibrary/doc/petgraph/graph/stable/struct.StableGraph.html#method.node_weight) return `None` if there is no node with that index number (perhaps because you stored the index somewhere and later changed the graph). It seems that walking the graph using an abstraction like Walker works to not handle `Option` directly, but only if your problem can be expressed by walking through the graph (often you have an ad-hoc problem that doesn't match this interface) Another issue: this graph assumes that all nodes are the same type, while a structure using `Rc` describes the data types in a more precise way. Sure, you can have an enum to describe all possible data types, but this is still not expressive enough to say that objects of type `A` can only contain references to objects of type `B`. It seems that by using this graph interface, some problems that are easily expressible with reference-counted pointers may become too complex.
You're right, I was being too general in the parent comment. I was making the assumptions that the user doesn't want to be limited to a single thread and cares about memory leaks.
[`arrayvec`](https://crates.io/crates/arrayvec/) should support collecting. That's about as close as you're likely to get.
In all cases that I've used a `Graph`, I normally write a wrapper API around the `Graph` which limits the behaviour and makes the guarantees about the graph that I require. I.e. never being able to delete a certain `master` node, or only being able to add certain `Edge` variants between certain variants of `Node`s. &gt; Note that methods to read node or edge indices return `Option` If you've designed you're graph's API so that you're certain your `master` node will always exist within the graph, you can safely index the graph just like you can index a `Vec`. Normally though, you just walk the edges of the graph the same way you might walk the `children` or `parents` `Vec` of your `Rc` node. I think it's also probably much easier to clone, serialise or move a `Graph` than a large number of connected `Rc` nodes. Normally I can just use derive i.e. `#[derive(Clone, RustcEncodable, RustcDecodable)]`. `Rc` also requires dynamically allocating each node separately, whereas a `Graph` only requires two allocations - one for all nodes and one for all edges. Perhaps `Rc` is the better option for use-cases with a very small fixed number of nodes?
I dunno; I don't *like* `Rc` but I also don't know how such `Graph` wrapper would look like - do you have any code examples? I think that code using `Rc` would tend to be simpler if you have a limited number of nodes (and also have a simpler programming model perhaps?), but with bad runtime behavior - you need to consider it might panic if you do the wrong thing. Now, many programming errors (like out of bounds array indexing) also panic, this may not be too bad in practice.
If you don't wish to expose that you have a `RefCell`, you can flip the logic around and accept a closure that can be given the borrowed value: impl Foo { // ... pub fn within&lt;F, T&gt;(&amp;self, f: F) -&gt; T where F: FnOnce(&amp;[u32]) -&gt; T { f(&amp;*self.bar.borrow()) } } fn main() { let x = Foo::new(); x.add(5); // iter over x.bar non-mutably let sum = x.within(|z| { z.iter().fold(0, |a, v| a + v) }); println!("{}", sum); }
Thank you for being so helpful!
Perhaps you want to use an [ArrayVec](http://bluss.github.io/arrayvec/doc/arrayvec/struct.ArrayVec.html)? extern crate arrayvec; use arrayvec::ArrayVec; fn main() { let output: ArrayVec&lt;[_; 3]&gt; = [1, 2, 3].iter().map(|i| i * 2).collect(); let output_array = output.into_inner().unwrap(); println!("{:?}", output_array); } There is a note about `into_inner`: &gt; This function may incur unproportionally large overhead to move the array out, its performance is not optimal. So you may want to just leave it as an `ArrayVec` as long as possible.
This [classic blog post by Niko](http://smallcultfollowing.com/babysteps/blog/2013/06/11/on-the-connection-between-memory-management-and-data-race-freedom/) talks about the trouble with mutable aliasing. Roughly, multiple threads are not the only case when you can be mutating the same object "at the same time".
thanks for calling out the bit that makes the most sense :) There's gonna be a ton of differences in the next iteration. Should have a draft up next week, hopefully.
&gt; Supposing a single threaded program Well... http://manishearth.github.io/blog/2015/05/17/the-problem-with-shared-mutability/
There's also boxed slices for arrays of a size that is only known at runtime, but they can't be resized like a vector. You can turn any vector into a boxed slice by calling [into_boxed_slice()](http://doc.rust-lang.org/collections/vec/struct.Vec.html#method.into_boxed_slice) on it.
&gt; Think of it like “for any lifetime ‘a” rather than “the particular lifetime called ‘a”. This points out the "universal quantification" i.e. that `'a` is a *parameter* of a generic `struct` and the generic `impl` adding methods to it. But the actual words "parameter", "argument" or "generic" do not appear in the text, which worries me, because we might be inadvertently making lifetime parameters seem different from type parameters, when they are both used for generalizing code, to all possible scopes, and all possible types, respectively. FWIW, I've been previously asked to write [a ~~short~~ tiny introduction to lifetimes](https://www.reddit.com/r/rust/comments/3tfsz1/im_reading_the_book_the_lifetimes_part_is_very/cx6qbot) which didn't end up in any docs, but I (and /u/steveklabnik1) would be glad to get feedback on.
Looks like you would be passing a ```Fn(&amp;L, f64) -&gt; f64``` as a ```Fn(f64) -&gt; f64``` in that case. I think you could solve this by binding the link attribute to the function with a closure that would look somewhat like this: ```|x| self.func(x)```
&gt; I'm not sure that I like the idea of the word "promise" here In this context, I think he meant "promise = guarantee" as in you make a contract with the compiler that the compiler enforces on you. 
Because an iter from a vec is and ExactSizeIter rust will allocated the vec with the right capacity when collecting into it. If you want to avoid the heap entirely you'll need to use the arrayvec crate or wait for type level ints. 
Thank you for putting so much time and effort into making the book what it currently is. It's so nice to have all* the gotchas nicely outlined I one place. 
&gt; finally cracking iterator and operation efficiency Where can I read more about the sweet tools and techniques you figured out?
You can also just drop the `self` parameters in the trait definition. pub trait LinkFunc { fn func(x: f64) -&gt; f64; fn func_grad(x: f64) -&gt; f64; fn func_inv(x: f64) -&gt; f64; } 
How would I call the functions in this case? I tried: &amp;self.link::func which didn't work.
Can't remember exactly syntax and I'm on my phone but there's a universal call syntax that you'll want to look into.
But what if we want a person to outlive its parent?
Hehe, in the blog post I want to write but never do, sorry! :-) Mostly, use the slice iterator whenever you can! [(code to special case for the contiguous case)](https://github.com/bluss/rust-ndarray/blob/56e3cb8259ae3e727e9dee4b54223dc836dac87d/src/lib.rs#L1539-L1540). Then use the current best practice for [zipping slices](https://users.rust-lang.org/t/how-to-zip-two-slices-efficiently/2048) The array iterator is an enum with two cases, either it's using the slice iterator, otherwise, the custom array iterator impl. I was really happy with the result. So for the common contiguous-contiguous case, arithmetic operations are autovectorized, and simple assignment loops can be memset and so on.
The issue I have is that I'm using a generic type in my NeuralNet struct to represent Baz/Bar. Because of this I don't think I can use this same syntax? A [playpen example](https://play.rust-lang.org/?gist=f5377f79d99de9223022&amp;version=stable) detailing the problem (throws a compilation error).
Not sure I follow, but if there is no `Enemy::do_frame` function, where is all that logic handling (e g, what an enemy can do every frame) located? In a wrapper around `Graph`...?
I'd be interested to get your thoughts on whether something like [timely dataflow](https://github.com/frankmcsherry/timely-dataflow) would help here. It is generally meant for more exotic things, but it has a few properties that could be nice: 1. Its [communication crate](https://github.com/frankmcsherry/timely-dataflow/tree/master/communication) handles communication for you, transparently serializing and deserializing structs that implement [`Abomonation`](https://github.com/frankmcsherry/abomonation) (which, erm. which really can be most structs, despite the scary language). This means that you get to write things as if they were typed queues, sending something like `WorkUnit` and receiving `CompletedWorkUnit` back. 2. You get machine and thread-level parallelism out of it, for the cognitive overhead of one! 3. When you wire things up as dataflow, the coordination mechanisms allow you to work on multiple frames at the same time, and receive notification when a frame is done (has no outstanding work items). There isn't any magic about work-stealing at the moment, but it is something I could code up. It has been prototyped before. It seems like the "tricky" part is re-doing the logic as dataflow rather than master-based. I think you would just have all of the work unit descriptors (8x8 or 2x2 blocks) start as a distributed stream, which are then exchanged across the workers using your favorite exchange function (a hash? a space filling curve?). This stream leads into a `unary_stream` operator that just maps each work unit to completed work, and then the completed results get exchanged back to a specific worker or... * you could make the reconstruction filtering data-parallel too, with workers shipping partial results to each other, rather than through the master. It just gets added as another data-parallel dataflow step after generating the values at each pixel. * the final compositing could also be done in a data-parallel way (perhaps like above), where regions get written out by independent workers. Your scaling limits *could* be Amdahl's law sorts of stuff, if the master ends up doing non-trivial work in a frame, and this would work around that. * It's all low latency, so you could totally try out low-res interactive ray tracing, like Teller was trying to do back in the 90s. :D
You call these 'static' functions (impls without &amp;self) on the type itself. If you don't take &amp;self in your functions, there's no need to store a T. http://is.gd/kOTvOZ
Also, I think that an "iterator that can insert values while iterating" is just a specialized kind of iterator, just like a [`DoubleEndedIterator`](https://doc.rust-lang.org/std/iter/trait.DoubleEndedIterator.html). So maybe you don't need to reimplement the whole `Iterator` API, just add a new trait that has `Iterator` as supertrait. So one could have something like pub trait InsertableIterator : Iterator { fn insert(...) -&gt; ...; }
For 1: It might be simpler to just offload the work to some other threads on the master so it can continue reading from other workers. I haven't done any benchmarking on this part of the code so I'm not sure how much of a bottleneck it ends up being. If that turns out not to work well some partial merging on the worker end could be worth a look. For 2: I started thinking of a similar thing when writing the post, since that's exactly what is done to load balance threads on a node (the window size is just 1). This would definitely be a lot easier than work stealing, picking N well might require a bit of experimenting but this might not actually be too bad to try out. I'll see about implementing this, thanks for the comment!
&gt; use the slice iterator whenever you can Ah, gotcha. It's good to know that core stuff like that gets optimized nicely. Thanks for the reply!
This is really interesting! I'm not familiar with the paper but will have to read it and think about this some. I think it should be possible to basically eliminate the master process and have things be a pure dataflow of blocks -&gt; samples -&gt; final image where parts can be worked on by any worker. All the "master" would really need to do at that point is be the one who sets up and launches the data flow at the start. Then it could become a worker or would already be a worker, just with some extra responsibility.
Iterators must internally store "where" they are at the collection (you can manually call [`next`](https://doc.rust-lang.org/std/iter/trait.Iterator.html#tymethod.next) on the iterator, each time you want to advance it). [Here](https://github.com/rust-lang/rust/blob/4744472fe03cceb81747ac9c7c64ae5fe6340c0b/src/libcollections/linked_list.rs#L78) is the mutable iterator for LinkedList. You could call an `insert` method directly on the iterator, and it has enough information to insert an element in that position (and also add a `remove` method). I guess that *also* adding a utility method to iterate over the whole iterator would be good, running a function that either kept, inserted or removed elements. But this method could have a default implementation in terms of `insert` and `remove`; and some users of the iterator could want to iterate over it manually (without consuming the whole thing for example).
By "candid" I mean informal, or sloppy with respect to the underlying theoretical semantics. And I contend that even the revised examples poorly model the English semantics of an "Invitation". We may just disagree here, but to me it seems far more accurate to model an `Invitation` as entailing an `location:Address`, not a `&amp;'a House`. When I write an invitation, I don't put my house in it, nor do I "give the invitation ownership of my house" or "give my house ownership of this invitation" or anything silly like that. On the contrary, I just write an address in the invitation. When someone goes to that address, maybe my house is there, or maybe my house was burned down, or maybe it's not a house at all but a public park, or maybe I lied in the invitation because I don't want my ex to show up! `location:Address` has no logical difficulty with any of these situations, whereas `location:&amp;'a House` cannot properly model any of them. My broader contention is that I have difficulty conceiving of *any* circumstance in which I would make a struct with an owning reference. Off the top of my head, I can see very little theoretical basis for such a thing. EDIT: *structs with a borrowed reference. Obviously structs with owning references make perfect sense, lol.
I could probably bang something up for you, but just to sketch the idea: The computation you have is currently a not-too-complicated dataflow `A` -&gt; `B` -&gt; `C` where * Instances of the `A` operator generate a stream of `Instructions`. At the moment, there is one instance that produces instructions, but it could be any of them. * The `Instructions` are exchanged to instances of `B` operators who each call `Scene::load_file(...)` and `Config::new(...)`, and eventually produces a `Frame`. So, the result of the `B` operators are a stream of `Frame`s. * This stream is then "exchanged" again, where all data go to some specific instance of operator `C` (e.g. worker 0) who gets to be the master. As it receives frames it calls `render.add_blocks` and once it receives all of the frames (a "notification", in timely terms) it does the hunk of code under the `if *num_reporting == self.workers.len()` test. Or it could emit the whole `render` object in case some downstream code wanted to use it (e.g. a framebuffer, without doing a round-trip through the file system). I could bang together the rough structure of this, if you like. I'm not sure I understand enough of what happen on the workers between setting up a `Config` and getting results back, but ... maybe you just run the config and block on the results?
For example, here is a mock-up of what it might look like, where I've ignored most of what you've done except the `Instructions` -&gt; `Frame` logic, and the `Frame` -&gt; `render` logic. https://gist.github.com/frankmcsherry/66306876547b408f309e Edit: If you looked at it recently, it twitched around a lot as I fought with gist formatting. Also, it doesn't do arguably cool things like "use notification" to tell when it it done, it just counts down like your code did. You could use notifications to break the work up into finer grains, which is what would open up the potential for load balancing, but it's non-trivial. 
`Vec` has a [`retain`](http://doc.rust-lang.org/std/vec/struct.Vec.html#method.retain) method which allows you to remove elements while iterating.
Thanks, Steve. I think maybe the word "guarantee" or "contract" may fit better as azerupi suggested.
Thanks for this. I will do some research and then fix up that section.
It's all good. It's the most nitpicky-ist thing ever.
I think you meant to post [here](https://www.reddit.com/r/playrust) instead.
Oh yeah thanks
I notice you split up the libraries up into separate repos - this is not required and usually not done; see e.g. https://github.com/alexcrichton/ssh2-rs where is a subdirectory https://github.com/alexcrichton/ssh2-rs/tree/master/libssh2-sys
Ah yes, I see. It just seems a little inelegant to have to create a local binding to the iterator, rather than writing it as a single expression.
Perhaps a better protip is, hit `?` to open the help and see what key shows a permalink.
Before, you mentioned that the `Enemy` needs to consider all other `Player`s, `Item`s and other `Enemy`s in its "`do_frame`" method. Instead of calling a `do_frame` method on the `Enemy` with all the other entities and interactions as the arguments, we set those interactions (but for all `Entity`s) as `Edge`s on the graph, and evaluate them (aka "do the frame") by walking the graph, reading the edges between each `Entity` and updating the `Entity`s as necessary. This step could be a method on a wrapper around the graph, or it could simply be a function that takes the graph as an argument. This gives us easier control over the order of interactions and the resolution of conflicts, as we consider all `Entity`s evenly during this stage (rather than limiting ourselves to what each single `Enemy` has access to each `do_frame`). We also get finer grained control over the access to each `Entity` or `Interaction` as we only index into each when necessary using the indices produced by our walk of the graph as we go (here's a [`Walker`](http://mitchmindtree.github.io/daggy/daggy/walker/trait.Walker.html) abstraction that I use for walking my own graphs). This means we also don't require any unsafe for aliasing pointers and we don't need to sacrifice the compile time checks for runtime checks (like `Rc&lt;RefCell&lt;T&gt;&gt;` does). I used to have a similar problem to the one you mention in my audio DSP graph API - I tried for so long to make a nice API around shared `Node`s that stored `Vec`s of parents and children nodes until I realised that what I was really trying to construct was a directed acyclic graph. It turns out there are humans that know a lot about what you can and can't do with graphs and there are some *really* nice abstractions that incorporate this knowledge (I always rant about [petgraph](http://bluss.github.io/petulant-avenger-graphlibrary/doc/petgraph/index.html) for this). In my own experience, performance, node ownership, interaction logic, graph cloning and serialization all got a lot easier once I started taking advantage of these.
Custom DSTs. This would allow for types like &amp;[[T]], &amp;[[[T]]], or whatever you would like to design. Also, it would mean we could get rid of `std::repr` entirely.
Nah, I don't need any other shortcuts! ~~640K~~ `y` is enough for anyone. \^_\^
&gt; Now I'm wondering, could some Rust GC library offer safe mutable aliasing without breaking something in the language? `Cell` does this for POD types. We could pretty easily have sugar around `Cell` or `RefCell` to make mutable aliases more palatable. In fact I've been in favor of this for a while. However, the idea doesn't seem to have gotten a lot of traction, probably because multiple mutable aliases aren't wanted often enough to warrant language changes to make them nicer. I'm totally fine with this. Unfortunately it's not possible to have multiple `&amp;mut` references to something. For one thing, generic code that operates on `&amp;mut T` relies on those `&amp;mut T`s being distinct for memory safety. You'd have to have a new type of reference that isn't *actually* an `&amp;mut` reference.
&gt; probably because multiple mutable aliases aren't wanted often enough to warrant language changes to make them nicer The trouble is, a lot of people is settling to using `Rc` with non-POD types. Perhaps there isn't demand among Rust developers, but relying on a dynamic borrow checker is already the status quo. Hopefully most libraries won't use `Rc` internally (or will use it only for POD types). &gt; You'd have to have a new type of reference that isn't actually an `&amp;mut` reference. It seems that `Cell` is itself one such type (well you can mutate its value). Why isn't it possible to write [`set`](https://doc.rust-lang.org/std/cell/struct.Cell.html#method.set) for `RefCell`?
I've been bombarding this subreddit with questions and I'm really thankful for all the help I've got. I'm glad to say I can finally contribute something different! Thanks to all the help from the community I've finally got a somewhat usable machine learning crate. I'm aware there's a lot of other awesome ML tools here already but hopefully mine can be of use to somebody. I'd love some feedback from the community and a little help in choosing a direction for future development. I want this to be as useful as possible - for me that means some nice data handling and plotting integration. I'm almost ready to take a look at those... Thanks again for the advice and patience!
I opted instead to go with something closer to your first example. It feels a bit more Rust-like? Thanks for all your help with this issue! I think I'll give the sub some rest before I throw some of my other questions out there :)
ProgressBar library so you can use progressbars more simply in your code if you wanted
I was thinking more like the embedded sensors and the main engine computer type stuff, but for the infotainment system I guess it would make sense.
If you refer to the programming language, the answer is yes. If you mean the game, seek an answer on /r/playrust instead.
We need to train a ML model to evaluate new ones automatically.
No, I'm saying mostly the opposite of both. Any number of nested references are valid (a reference pointing to a reference is fine), and each layer is a pointer (people saying that references aren't pointers are being a little confusing: semantically they're different to C pointers, but they're *exactly* the same as pointers on the machine/at runtime). This means `x = &amp;&amp;&amp;&amp;&amp;p` is a pointer to a pointer to a pointer to a pointer to a pointer to `p`, i.e., in expanded form: let p = ...; let r1 = &amp;p; let r2 = &amp;r1; let r3 = &amp;r2; let r4 = &amp;r3; let x = &amp;r4; Each of the `r`s and the `p` are an memory location on the stack, so there's a whole pile of values put on the stack (the `r` and `x` references are all pointing to the stack). To get to the `p` value from `x`, the computer will need to execute `*****x`, reading the value each reference points to to reverse the chain of references. By "optimize out most of the pointers" the parent means that the layers will be collapsed, reducing the amount of indirection, e.g. the comment is suggesting `&amp;&amp;&amp;&amp;&amp;p` might be compiled to be the same as `&amp;p` on the machine. In other words the implication is that the chain above becomes: let p = ...; let x = &amp;p; However, this isn't generally possible: the compiler has to differentiate between `&amp;p` and `&amp;&amp;&amp;&amp;&amp;p`. For instance, consider this program: fn main() { let x = 1; let y = &amp;&amp;&amp;&amp;&amp;&amp;x; println!("{}", y); } The optimised assembly for that code starts with (viewable [on the playpen](https://play.rust-lang.org/?gist=614b9798b4e4d2c727ab&amp;version=stable) by switching it to [Release] and clicking [ASM]): movl $1, 116(%rsp) leaq 116(%rsp), %rax movq %rax, 96(%rsp) leaq 96(%rsp), %rax movq %rax, 88(%rsp) leaq 88(%rsp), %rax movq %rax, 80(%rsp) leaq 80(%rsp), %rax movq %rax, 72(%rsp) leaq 72(%rsp), %rax movq %rax, 64(%rsp) leaq 64(%rsp), %rax movq %rax, 104(%rsp) The `mov... X, M`s mean "store `X` at memory location `M`", and the `leaq M, Y`s mean "save the address of memory location `M` into `Y`". The first line is storing `1` on the stack, the next is taking the address of that value (the 116 "address" appears in both), the third line stores that address into a new location on the stack, the fourth takes the address of that new location, etc., repeated several times until the last line. (In other words, the layers of references stayed as many layers of references, they were not collapsed/compressed at all.)
Oh right, you can put the other computations in there as side effects.
I feel like things are going to change a bit with generic integers, which will allow for much smoother matrices. At that point I think a central Matrix data structure would help a lot to unify crates like these.
Yes, if we had a standard matrix implementation then introp between crates should be pretty easy. D's new std.ndslice (http://jackstouffer.com/blog/nd_slice.html) might be a good source of inspiration here, given that it's also aiming to be a faster, type-safe alternative to numpy. bluss' ndarray (https://github.com/bluss/rust-ndarray) looks promising.
Do you have any guidance on what sorts of problems suit timely dataflow patterns? The crate is waaaay up on my list of "cool things I want to try and use", but I don't know enough about how dataflow in general to know what it should be used for. That's a horribly vague question, sorry :( I understand if there isn't a good answer...the real answer may be that I need to just play with it some to get a feel for how it works.
Interesting, this looks really nice. It might be an easier way to implement the windowing method mentioned by /u/matthieum as well with the stream of instructions. I've bookmarked this and will try putting together a timely backend!
Dynamically sized type, like `&amp;[T]`. These are types that are `T: !Sized`. Basically, `[T]` could be any size, from `0` to `usize::max_value()`, which means that it has to be behind a pointer, and there's a length field *in the pointer* to tell you how long the `[T]` is. If we had custom DSTs, we could put any sort of data in the pointer instead of the length field.
Maybe a new Mozilla irc channel for rust-ml ? Just as they've added a new one for networking.
I'm keen to help out - a unified array library will be a huge help with more than just machine learning libraries. When you say you don't want to focus on linalg - do you mean separating this from the ndarray library or just focusing development away from linalg for now?
It's a bit tricky to answer, but I understand the question. :) Dataflow (without the "timely" part) is most common where there is a good deal of data-parallelism, so that various bits of data can be processed without needing fine-grained coordination. You are roughly giving up control flow (and that expressive power) to get a simpler model that can be more easily parallelized; some times this is a good tradeoff, sometimes not. For some problems it is obvious (one way or the other) whereas other problems need a bit of a re-think. Timely dataflow is small steps toward re-introducing some of the control-flow constructs like loops, without giving up the scalability of the infrastructure. It makes dataflow expressive enough to emulate a PRAM, so it's a super general model of computation (you have compute nodes, memory, and a network; tada!), but it's not yet clear what the least painful / most productive programming models are. 
Actually--going purely by the TIOBE charts--C++ has generally been on decline the entire last decade, while good ol' C has been holding strong. The one uptick is around a 2014/2015 timeframe, so I guess C++14 really did generate a lot of interest, although that seems like it may have been temporary.
Unfortunately, it'd be impossible to guarantee the invariant that every `&amp;mut` pointer is the only way to mutate what it points to with a non-panicking `set`. That invariant is necessary for memory safety; it's what prevents iterator invalidation, for example.
I feel like this advice isn't necessarily true when the optimizer comes into play. Do you know how that factors into the equation?
It simply returns the already allocated heap pointer.
No, this is accurate. What would the optimizer do with a value that it couldn't do with a pointer?
Mioco is getting close to a new release. The API is nicer than ever (passing MiocoHandler and wraping event sources has been removed), user scheduler can now use custom data assigned to co-routines.
&gt; the example program will be turned into a no-op It always amuses me how often this happens when I'm looking at the LLVM IR or assembly to answer a performance question. `#[inline(never)]` is often my friend to help me try to keep a little bit of sanity when diving into such.
I am not aware of any official guidelines but personally I would recommend passing types that satisfy `Copy` by value unless it is very large (which is a bit vague, but I would say anything smaller than three pointers is not large). Anywhere else: by reference where possible. 
Even if *you* will never need to use `x` again, somebody else might; borrow whenever possible. Also, try not to have implicit copies, because `x` may stop being implicitly copyable. The compiler is smart enough to replace references with copies, so don't worry about `&amp;u8` parameters.
Well, yes, wrk is a poor choice. It's just a tool that easy to run. But I doubt tcp echo is a better choice. (By the way, what do you use to test that?) I believe that I need to implement HTTP client in rust just to build a better load testing tool :) First, it would be nice to have a lot of keep-alive connections that are just idle. Second, it should fragment messages at an average TCP packet boundary. Third, the size of response should be much bigger, etc...
And indeed, anything larger than a pointer gets passed by-ref under the covers today (except for fat pointers).
Woops, yeah, I meant copy when I said move sometimes :p A call on a small struct might be more equally or more expensive on a 64 bit machines, but might not on 32 bit machines or ARM. Would it be better just to let use a reference, and let the compiler choose? I though that sometimes it would optimize it out, and just clone it. So I guess the question is, does passing it by reference give the compiler any more flexibility to do what it wants? I've been always using immutable borrow when I can, then mutable borrow, then cloning as a last resort, because I figured that the compiler will know it can copy the data, but might not be able to figure it can do it the other way around. *** Also, to clarify, its better to use vec = mutate_vec(vec); rather than mutate_vec(&amp;mut vec); ? *** On a somewhat related note, Is there a list/ reference of best practices for rust? I didn't ever really use iterators, until I saw that they didn't have to undergo bounds checks while being memory safe. Edit: I assumed that was *one* of the purposes of abstracting pointers/ref. You could tell the compiler what is, and was is not used later, and what needs to be copied, and it could figure out the best way to handle the memory.
Why must it respect it if its called, but not if its inlined? Is there any reason it needs to be respected in the first place? Is there not a way to let the compiler, "do what it wants"? I assumed that was one of the purposes of abstracting pointers/ref with a borrow. You could tell the compiler what is, and was is not used later, and what needs to be copied, and it could figure out the best way to handle the memory. Is it better to copy a Vec, or is it better to use a mutable reference, since it is 192 bits. Does that count as "word size" as Droid said, or does that only apply to primitives?
When you pass a reference it always puts the value behind a pointer. Clones are explicit; the compiler won't pass a copy if the method is expecting a reference. Large `Copy` values (larger than a machine word, I think) are `memcpy()`'d into the stack frame, and large returns (`Copy` or not) use out-pointers behind the scenes. You're not sacrificing much data locality if you pass a reference to something on the stack, of course. It's just convention to take `Copy` types by-value. For non-`Copy` types, taking a mutable reference is almost always preferable over passing by-value and returning by-value. One exception is if you're doing method-chaining with one of your own types, like with the builder pattern. There's a couple different projects that are working on nailing down design patterns and conventions in Rust. There's the [official Rust style guide](http://doc.rust-lang.org/nightly/style/), as well as @nrc's (whose Reddit username escapes me) [Rust design patterns](https://github.com/nrc/patterns) which covers Rust design conventions as well as antipatterns.
Thanks for all your help. I've asked a lot of questions here, and you've always taken time to explain it thoroughly. I really appreciate it.
for an `enum` what's the difference between a copy and a clone? I assume a clone would just be a memcpy
Perhaps having some generic interface so that one can swap implementations or something (like how Piston 2D graphics have backends for Gfx or Glium)
This is what I would do.
To `Copy` or not is a question which hasn't been conclusively answered so far. Some prefer to implement it wherever possible, others (like the author) only when in need. I think the right question to ask when deciding whether to `#[derive(Copy)]` is if we actually want move semantics (which implies the latest binding is always the canonical representation of some value unless `.clone()`d) – otherwise our code doesn't gain anything but a number of pointless `.clone()` calls.
There's a misunderstanding in this blog post. Copying (or equivalently, .clone()ing) an integer type like the enum amounts to the same work as `let x = 1; let twice = (x, x); // `x` is copied!`. It can't even sensibly be said to have a cost, it's all values in registers.
I've found the issue. It was stupid. The answer is posted in [Stack Overflow](http://stackoverflow.com/q/34540673/155423)
Last week I finished writing a bytecode parser (https://github.com/YorickPeterse/aeon/blob/master/src/bytecode_parser.rs) for my programming language (https://github.com/YorickPeterse/aeon). This week I'll have to write some documentation &amp; tests for it, followed by (hopefully) a basic compiler.
The beginnings of a matchmaking simulator for Blizzard's game Heroes of the Storm. https://github.com/jimmycuadra/hotsq
The awesome list currently has only two: https://github.com/kud1ing/awesome-rust#machine-learning
I really think `mem::replace` should be in the prelude, it's such a useful thing to use and so many newbies take forever to find out about it because it's hidden in the scary `mem` module with evil beasties like `transmute` and `uninitialized`.
GitHub says I contributed 170 commits so far. Here's my setup. * I use ./configure, no arguments. I don't do debug build. * While developing, I build to the first stage by "make rustc-stage1" and test. * I just pulled (from before the new year) and ran "time make -j4 rustc-stage1". It took 12 minutes. (18 minutes of CPU time, so 1.5x parallel speedup.) I am curious about others workflow too.
There's a runtime clause in the libgcc license, see here: https://gcc.gnu.org/ml/gcc/2001-06/msg01279.html
That's good to know. That's probably really useful if you don't want to immediately put back a value. That's not really the case in my situation, but still good to know for the future :)
&gt; The General Public License restrictions do apply in other respects; for example, they cover modification of the file, and distribution when not linked into a combine executable. `libgcc_s_dw2-1.dll` is not part of the executable. I've never been able to find an iron-clad statement from an actual lawyer or someone representing the FSF in an *official* capacity to say that the GPL doesn't apply in this case. I've seen it asked several times, and I've seen people say *both* "of *course it doesn't apply" and "of *course* it applies". [See also this thread on the topic](https://users.rust-lang.org/t/minimal-executable-size-win-32-bit/1900/14?u=danielkeep).
You *should* be able to remove this dependency by using the MSVC-based toolchain instead of the MinGW-based one. Also, I've never seen *any* aggressiveness about using Rust for open/closed source project.
Thanks for the quick response! Unfortunately, I'm a bit of a dummy when it comes to the lingo (or whatever it takes to confidently comprehend that paragraph), so, if it's not too much trouble, could you clarify something for me? &gt;...unlimited permission to link the compiled version of this file into combinations with other programs... What exactly does "link" mean here? Anything other than shipping the .dll with the .exe?
To link means to put the .dll stuff statically inside the .exe. If you plan on distributing the .dll, you're legally obliged to provide at least a link to the unchanged sources (in the package, for example inside a LICENSE.txt) or if you apply changes, you'll have to make the changes open source.
Good call, haven't considered that. I'm guessing that woud entail installing MSVC version of rust, but the only stable version is 64bit. There's a 64bit nightly version - if I were to use that, and only use the stable features, am I right to assume that I would basically be using the stable version? As to the open/closed source thing, I was mostly just trying to preemptively "when in Rome", "I come in peace"... just in case. :)
I think "she'll be right mate" is a *really* bad attitude to have once money's involved. I broadly agree with your assessment of previous cases, but that's really no guarantee *at all* that you won't run afoul of that *one* project where the contributors *will* want to screw you over. At the end of the day, the thing with `libgcc` is just a frustrating distraction that really shouldn't even be an open question. Avoiding it makes the issue go away *for certain*.
If you're worried about using non-stable things, you could always test using the 32-bit Stable MinGW toolchain, then build for release with the 32-bit Nightly MSVC toolchain
Ok, thanks! I think I'll be doing this. I could take a risk with linking the .dll, but I think that'd be irresponsible with regards to the 3rd party. 
Just to be clear, there's really *nothing* you can do with *your* code to imperil a third party's code. At least, I *hope* not. I'd like to believe the law hasn't gotten *that* insane just yet...
That's true. But what do you suggest? Reimplementation of all of the stdlib calls used? That's probably a lot of work in there. Hard linking and relying on the licensing terms as understood by the developers themselves still seems like a sensible option to me. If you're still in doubt, ask the developers on the GNU mailing list yourself.
Normally I'll do ./configure --enable-ccache Then make -j8 When you have 16GB+ of RAM its likely best to do your compilation within a ram disk. mkdir /mnt/ramdisk mount -t tmpfs -o size=8G tmpfs /mnt/ramdisk tar xf rust-lang-sauce.tar.gz /mnt/ramdisk I'm running i7 16GB of ram and I can do a full build within 8-15 minutes. Depending on patch level/kernel configuration.
Yeah, I've jumped through some hoops to try and get non-allocating code, but it's really hard. let mut output = String::new(); let mut left = Some(0); for (ix, ch) in input.char_indices() { // If we have a previous offset as a left bound. if let Some(left_ix) = left { // Check the range up to but not include the new character if let Some(val) = dict.get(&amp;input[left_ix..ix]) { output.push_str(&amp;val); left = None; } } if ch != 'G' &amp;&amp; ch != 'g' { assert!(left == None, "Unconsumed prefix"); output.push(ch); left = None; } else if left == None { // Start a new pattern with the g or G left = Some(ix); } else { // Expand the existing pattern } } output https://gist.github.com/abcdf0dd608ba764417e Which would have been a fair amount simpler if I just kept a `String` and pushed characters onto it, rather than trying to get the indices to slice into the input string to get a `&amp;str`. There's no way to expand a `&amp;str` as far as I can tell, which would have been useful here. (Solves https://www.reddit.com/r/dailyprogrammer/comments/3x3hqa/20151216_challenge_245_intermediate_ggggggg_gggg/)
&gt; Part of me wants a good non-core crate to abstract away some of the difficulty Could you be more specific on what exactly this non-core crate might do?
Javascript work continues (Or didn't over the holiday). Realized I don't know HTML as well as I thought so inlue of updating doc.servo here's a [blog post about where to find references for Rust Javascript bindings](http://a1612.blogspot.com/). Eventually I should just bite the bullet and put the mozilla docs into the moz-js source so it renders properly as a rust doc, but currently that's a pipe dream.
I think you're basically right. Either `remove_tags` need to take `(&amp;mut self, ValueId)` or `(&amp;mut HashMap&lt;TagId, Tag&gt;, &amp;[TagId])`. In the latter case, you can still have it inside `Aggregate`, like this: impl Aggregate { fn remove_tags2(h: &amp;mut HashMap&lt;TagId, Tag&gt;, ids: &amp;[TagId]) { unimplemented!() } fn clear_tags(&amp;mut self, id: ValueId) -&gt; bool { /* ... */ Self::remove_tags2(&amp;mut self.tags, &amp;val.tags); } } ...and since it is not a `pub fn`, it will not be exposed through the API. 
I think `from_raw_parts` is correct here. If there's no re-sizing, then that looks fine to me. You also need to implement `Drop`, right?
Yes, my experience has been similar to yours. Boxing (incl. String and Vec) and the borrow checker are two things that many people, myself included, will have the most trouble with when learning Rust. However, once you become accustomed to it, it's not so troublesome, and the benefits you gain are well worth it--the Rust compiler catches more potential runtime errors than `gcc -Wall`. We probably do need better examples for when to use boxed structures (e.g. `String`) and when to use their raw counterparts (e.g. `&amp;str`), as mentioned in another comment. The advice presently is to use String or Vec when you need "ownership" of a variable, but that also requires understanding ownership in Rust--so that statement is not very friendly to Rust beginners.
I suck at licenses, but hey on commas... iirc Use one whenever you want to pause in a sentence, but the sentence is not over. Also in lists like item0, item1, item2 and item3. 
Actually, it's not too difficult to extend `String` with more methods. A String is just a `Vec&lt;u8&gt;` with the guarantee that its contents is valid UTF-8, so you can do things like: trait StringExt { fn foo(self) -&gt; Self; } impl StringExt for String { fn foo(self) -&gt; String { let v = self.into_bytes(); /* Do something with v */ String::from_utf8(v).unwrap() } } ...without extra allocations. 
Use [`Vec::from_raw_parts`](http://static.rust-lang.org/doc/master/collections/vec/struct.Vec.html#method.from_raw_parts) in `drop()` instead. The `Box&lt;[T]&gt;` type is a little bit underloved and there are some things that had to be done indirectly using `Vec`. Another way to do it would be to transmute `*mut [T]` into [`raw::Slice`](http://static.rust-lang.org/doc/master/std/raw/struct.Slice.html), so you can use regular `into_raw` and `from_raw` methods from box, but that's not a pretty solution. I hope that there would appear a simple and idiomatic way to convert between `*mut [T]` and `(*mut T, usize)` and back. I don't even see a way to get a data pointer from `*mut [T]` other than transmuting or `(raw as &amp;[T]).len()`.
Not quite. Here's how it works: If you're making a list, place a comma after every list item but the last (eg. "Me, myself, and I"). The other major use is for certain cases of attaching a dependent clause to an independent clause. An independent clause is something that could be a sentence by itself. For example "I like her". It has a subject (I), a verb (like), and an object (her). A [dependent clause](https://en.wikipedia.org/wiki/Dependent_clause) is something that can't stand on its own, like "In a platonic way". ("In a platonic way... what? What's in a platonic way?") There's one order in which you can combine these clauses which requires no commas. "I like her in a platonic way". If you want to reorder things, you use a comma to cue the brain that it needs to context-switch or shelve the clause in order to parse the sentence. For example, "In a platonic way, I like her". (In other words, punctuation like commas is used to prevent the brain from having to make multiple passes over longer or more complex sentences to successfully parse them.) You can also place one clause inside another. Then, you need two commas: "It was an ordinary, if somewhat brusque, winter day" ("It was an ordinary winter day" is a perfectly valid sentence on its own, then you stick "if somewhat brusque" in the middle rather than on the end, so you need to context switch away and back. Two commas.) The pause is a cognitive side-effect of what the comma really means and the easiest way to determine whether you need a comma is to try stretching or shrinking the pause to see which feels more wrong. (For example, "Give or take, of course." Without the comma and its associated pause, a literal reading is "You should give or take 'of course'" as if "of course" is some object that can be given or taken.) You use an ellipsis for a pause without context-switching. For example, "I think he's a... difficult person." Finally, if you're sticking together two independent clauses (things which can stand on their own as complete sentences), then you don't use a comma (the name for that mistake is "comma splice"). Instead, you use either a conjunction (for/and/nor/but/or/yet/so. FANBOYS for easy remembering.) or a semicolon (;). A semicolon indicates that tone of voice is supposed to substitute for a conjunction. (eg. "I like this but she likes that." "I like this; She likes that.")
You can solve this problem by swapping the `val.tags` vector with a new empty vector, thus taking ownership of it and at the same time achieving the same result as the call to `val.tags.clear()`. It is also necessary to introduce an inner scope so that the borrow of `val` does not last for too long. Here is the code : fn clear_tags(&amp;mut self, id: ValueId) -&gt; bool { let mut tags = Vec::new(); { let val = if let Some(v) = self.values.get_mut(&amp;id) { v } else { return false }; mem::swap(&amp;mut tags, &amp;mut val.tags); } self.remove_tags(&amp;tags); true } Don't forget to `use std::mem`.
I am absolutely thrilled by the recent uprise in ML related projects in Rust. E.g. just stumbled upon [meta_diff](https://github.com/Botev/meta_diff) (increases speed to develop cross-platform Machine Learning algorithms), But to get to a level of popular ML frameworks in other languages, I agree with /u/andrewbrinker, we should increase collaboration and exchange. Speaking as the author of Leaf, we would love to contribute some of the HPC abstractions (CUDA, OpenCL, CPU) and knowledge to other frameworks.
&gt; (Then does it again, I think, though I’m not sure what’s the point of that last one.) The third pass is to see whether the second compilation pass introduced errors. If the second and third pass result in the same binary, everything's fine. See [this](https://www.reddit.com/r/programming/comments/2wy2qe/gos_compiler_is_now_written_in_go/covds1l?context=3) for a quick explanation.
&gt; things like slices Hmm. That is essentially what an `&amp;str` is. If `s` is a `String`, then `&amp;s[n..m]` is a slice of `s` from the byte range `n..m`. &gt; characters at an index If `s` is a `String`, then `s.chars().nth(n).unwrap()` retrieves the `n`th Unicode scalar value from `s`. This is an `O(n)` operation however, and it is a *good* thing IMO that it is not as simple as `s[n]`. &gt; split at substring (not just char) `"abcfooxyz".split("foo")` works as you might expect. (The `split` method [is polymorphic on the `Pattern` trait](http://doc.rust-lang.org/std/str/pattern/trait.Pattern.html).) &gt; string matching (probably not possible via crate) There are various simplistic (but fast) substring matching functions. e.g., `contains`, `starts_with`, `ends_with`, `find`, etc. Regular expressions are supported using an external (but official) crate `regex`. &gt; string concatenation (str or String agnostic) Depending on what you're doing, one approach is string interpolation. e.g., `format!("{},{}", s, t)`. Otherwise, you have the `join` and `concat` methods available via the [`SliceConcatExt`](http://doc.rust-lang.org/std/slice/trait.SliceConcatExt.html) trait. Unfortunately, you cannot concat/join `String`/`&amp;str` directly, but it is essentially free to convert any `String` into a `&amp;str`, so it shouldn't be a problem in practice. All of these things work on Rust stable. For what it's worth, I think the string handling in Rust is best in class. Performance trade offs are explicit in the API. The type system guarantees valid UTF-8 in safe code. Slicing is cheap. Dealing with strings correctly is a difficult task in any programming language, but Rust manages to rule out a ton of foot guns in the process. (I have experience with dealing with Unicode in other languages, like C, Java, Python and Go. Rust is my favorite thus far.) I think the major missing piece of strings in Rust is more extensive Unicode support and support for other types of non-Unicode character encodings. However, the ecosystem is filling in these gaps. (There are various small unicode crates on crates.io and the `encoding` crate is making progress on supporting alternative character encodings.)
I'm not an expert, but from my understanding boxed structures are not the only items that can be allocated on the heap. Likewise, someone can correct me on the following if I'm wrong. In Rust, every variable has to have a known size. This is why when you declare a variable as a slice `[T]`, you have to declare it with a fixed size. Boxes (being the generic term) are Rust's way of dealing with this--a Box is essentially a pointer to a structure of an unknown size, and it allows you to work with dynamically-sized structures including strings and arrays without needing to know their size ahead of time. `String` and `Vec` are probably the two most commonly used types of boxes, but there are other similar structures, as well as the generic `Box` type.
Alright, thanks a lot for the explanation! I guess I'll have to mull over this a little bit more, then. Edit: After reading /u/protestor's [sister comment](https://www.reddit.com/r/rust/comments/3ze3ig/my_tryst_with_rust/cyltusz), your explanation now makes a lot of sense. I was wondering how a program could work with a single pointer (and nothing else) when the size of a boxed structure is unknown, but I didn't even know about fat pointers. So, again, thank you!
*cough*/r/playrust*cough*
Back to work, which means I've got some profiling of my metagenomics binning tool ahead of me. Promising results so far (much faster than a trie-based impl in C++ I had last year), but not fast enough for the intended applications. Time to actually get my Rust debug environment put together on the work machine.
&gt; retrieves the nth Unicode character To breifly be That Guy, "character" isn't a thing in unicode. This will return the nth Unicode Scalar Value, which may or may not be a 'character' to humans who don't need a spec to figure out what's going on . :)
Thanks.
All fair points, but from a beginner's perspective I think that a lot of the perceived difficulty of Rust's string support comes from the power and safety that you describe. Most languages let you cheat if you want to code something up quickly, and while that results in inefficient code that will break horribly with no warning on any non-ASCII strings, many developers clearly find it easy to accept those tradeoffs, especially in a prototype. Further, they are tradeoffs that are made by default in many developers' first (and second, third, ...) languages and they may have never encountered them before (or have done so in a limited fashion). In my personal experience, dealing with Rust's String/&amp;str can be a somewhat painful first step outside of the toy example code one writes when first learning. Personally, I'm growing to like that Rust forces me to have better hygiene with allocations and with potentially invalid UTF-8, but it's something that I've lazily been able to mostly ignore without too much consequence in other languages.
&gt; any minor performance hit involved is what I would have accepted as business as usual in other languages I work in. This is a sentiment I've been having a hard time articulating.
Taking such a generic name has a really high cost, though.
It actually can get moved (this happens in generic code, to say the least), but yeah, magic.
Submitting to /r/rust because this contains the techniques I'll be using to try and make mentorship on rustc better in the coming weeks, and I'd like others to pick them up too. I'd also like the general community to get better at this :)
[How 'bout a reference-counted copy-on-write string?](https://github.com/servo/tendril) No ownership fights.
Good idea, can be slightly simplified using `mem::replace` like this: fn clear_tags(&amp;mut self, id: ValueId) -&gt; bool { let mut tags = { let val = if let Some(v) = self.values.get_mut(&amp;id) { v } else { return false }; mem::replace(&amp;mut val.tags, Vec::new()) }; self.remove_tags(&amp;tags); true } 
This is more elegant indeed. I guess my C++ background may have influenced me towards the `swap` function.
&gt; I'm not sure what the policy is on including external crates like this in docs is though. :-/ Current policy is only crates under `rust-lang`. However, I _might_ be willing to make an exception in some circumstances. It's just very difficult.
Apparently using a comma before the and/or in a list is regional. http://www.grammar-monster.com/lessons/commas_in_lists.htm Brit's don't put it in before the conjunction, Americans do in general. Bikeshedding heh. (I'm American, but I like to leave that comma off)
Full support. Maybe /u/fgilcher can help introducing a rust-ml irc.
`char` excludes certain codepoints. That restriction is not present in `String`. /u/ybx is correct, in my understanding.
No, when you go from `(ptr, len)` to vector, you just pass the same `len` value both as capacity and length. That works because when going the other way (vec to `(ptr, len)`) you used `shrink_to_fit` (you can also use [`into_boxed_slice`](http://static.rust-lang.org/doc/master/std/vec/struct.Vec.html#method.into_boxed_slice)), which sets capacity to be equal to lenght.
&gt; To breifly be That Guy, "character" isn't a thing in unicode. I hope you weren't That Guy who called the method `chars()` then :)
&gt; I think having more educational materials for Rust strings is definitely needed, at least in part because Unicode is so hard. Also lots of blog posts that present the pros of Rust's string handling (e.g. no rewrites to support non-ascii), and why Rust is not the chief cause for the complexity of doing Unicode right.
It's a second edition, basically. It's under its own repo, "book", in the rust-Lang org. It's got significant structural changes, so I'm doing the work out of tree so as not to destroy the existing book in the meantime. It's going to be better in basically every respect.
It's not exactly a "new" book, it is more a complete overhaul of the current Rust book, but the changes will be so important that we refer to it as the new book, until it replaces the current one ;) https://github.com/rust-lang/book.
`rune` works. :-)
&gt; The fun begins when you start building new strings from slices of old strings and trying to tell the compiler how long the references to the different bits an pieces are supposed to stay alive for. This goes for other objects too. I spent literally hours fighting with the compiler trying to get it to compile what seemed like not terribly complicated code. There seems to be an issue of misunderstanding lifetimes. A common misconception is that an object would live longer if only the right lifetime were given. On stackoverflow one regularly sees users asking how to get the right lifetime annotation in situations such as these: fn foo() -&gt; &amp;Bar { let b = Bar::new(); &amp;b } That the user was working under the wrong mental model would explain why they could not make progress even after hours of trying. The other thing is that unicode strings are simply hard. 
&gt; Maybe we need to emphasize that “String is the StringBuilder”? You can allocate it up front and keep modifying it. Is it just documentation or is there a missing API? I remember trying to manipulate strings being the point where I created messes when I was learning Rust as well.
Feel free to look at [notty](https://github.com/withoutboats/notty) to see how I did it (and contribute if you have better ideas :P). Obviously notty has some extra complexity because of features, but Unicode complexity is nontrivial part of it. In fact: not only is there no longer a 1:1 relation between bytes and cells (or even _code points_ and cells, since some graphemes are multiple code points), you also have to handle things like what happens when a user types an extension character (like an accent)? What the user expects to happen is that the cell to the left of the cursor is modified in place by that accent, but many existing terminals are actually broken and do some totally wrong thing here.
Boxing (in general) means allocating on the heap. So if you're designing a VM, a "boxed float" means it's allocated on the heap and you pass around the pointer, and an "unboxed float" means you pass the float by value. So, for example, in Haskell `Int#` is the ["unboxed integer type"](https://downloads.haskell.org/~ghc/7.8.4/docs/html/users_guide/primitives.html). Now, in Rust, `Box` isn't the only way to box a value (you can also use `Rc`, etc). Note that even if something is "unboxed" it doesn't mean it's actually on the stack: it may be part of a large struct and this struct may be on the heap. The problem is, if the struct is on the heap and contains itself a boxed value, to access this value we need to follow two pointers: one to get to the struct, another to get to the value. The other point that was also commented is that an [unsized object](https://doc.rust-lang.org/book/unsized-types.html) can't be unboxed. For example, you can't usually have a value of type `str`: you can have `Box&lt;str&gt;` (that isn't the same as `String` - `Box&lt;str&gt;` is dumber and can't [push more text](https://doc.rust-lang.org/std/string/struct.String.html#method.push_str)), you can access a `&amp;str`. Pointers to unsized objects (either `Box&lt;T&gt;` or `&amp;T` or anything like it) are stored as [fat pointers](https://www.reddit.com/r/rust/comments/3xx9ty/where_can_i_read_more_about_fat_pointers/): one word to the memory location and another storing the size of the pointer. I'm trying to find specific documentation on that, but I'm not finding. As a site note, you can't have a string on your stack, because for it to go on the stack you need to know its size, but string literals (of type `&amp;'static str`) aren't actually allocated in the heap, they are in a [read-only section](https://stackoverflow.com/questions/2589949/c-string-literals-where-do-they-go) of the program. If you create a `Box&lt;str&gt;` or a `String` out of a string literal, you will copy the contents to the heap.
Strings / &amp;str do indeed have 1-to-1 correspondance with sequences of `char`s, it's quite neat. It also means you can compare two `str` by byte representation or char sequence and have the same result.
They don't leak, it's rather by design. Byte offsets for searching &amp; slicing are simple and efficient (O(1) slicing), and of course searching methods return the same kinds of offsets that you use for splitting and slicing. Rust provides a correct and basic string API which allows efficient libraries to be built on top of it.
Very interesting, thank you. I will try the "rustc-stage1" target. In the output of "make tips" it says that it will build up to stage 1. Do you know whether it will pull in the "blessed snapshot" again for stage 0 or will it use a previously build compiler for that?
If you could remember, it would be better if you could tell what you thought.
That's true. I noticed that stages 1 and 2 were substantially slower than the build from the blessed snapshot for stage 0. Thanks for that. 
[Already out there](https://github.com/tomprogrammer/rust-ascii), assuming this was the kind of thing you were thinking of.
&gt; (previous?) nightly requirements of serde Serde doesn't _require_ nightly, but it is nicer to use.
Published [rust-tldr](https://github.com/rilut/rust-tldr) both to [tldr](https://github.com/tldr-pages/tldr) and [crates.io](https://crates.io/crates/tldr/). I also want to try to improve/rewrite my [rust-cheat](https://github.com/rilut/rust-cheat), but I'm on my finals, so yeah I think I can't really do anything on Github this week. Are you on Linux? Give it a try! `cargo install tldr` On Windows &amp; Mac? Yes you can, but you need to follow some instruction first.
Total Rust noobie here; started writing https://github.com/llambda/rreverse as a learning exercise, which is a port of https://github.com/thampiman/reverse-geocoder a Python reverse geocoder. Reverse geocoding is taking a latitude/longitude and converting into a location like the city. I would like to benchmark the two, of course, because everyone loves benchmarks, once I get further. Fortunately, a nice CSV parser and KDtree implementation was already written, and folks in the gitter chat helped me figure out where I got stuck borrow checker. So far really liking Rust, it's a totally different experience from JavaScript and Java which is what I primarily write, and C I haven't written in 10 years. JavaScript, and even Java, I am usually making a small changes, and re-running my app/web page, often through a step debugger, and seeing what broke, then making a small tweak, and repeat. Rust feels like once the compiler is happy, 99% of the time, everything is just going to work. Especially with eliminating null pointers and thread type safety issues which are of course need to be tackled in Java. Rust feels more...predictable...unsurprising. It gives me something of an odd "boring, but in a good way" feeling that I never experienced with a programming language. I am enjoying learning something new and the community is fantastic. I feel very humbled reading about the fascinating kinds of things folks much smarter than me are building.
If possible, borrow. If the caller owns the value and you're borrowing, the caller can call you. If the caller borrows the value and you're taking ownership, the caller is screwed.
The one exception is for Copy values like i32, which should never be borrowed because there's no reason to.
I know, and I'm fine with that. But these are all points that would make changing the internal representation to, say, UCS-4 a breaking change. As opposed to e.g. Python where the internal representation of a Unicode string is unspecified, and even varies between individual string objects in newer versions.
You guys probably wanted to go to /r/playrust - this channel is for the programming language - even though I wouldn't say you couldn't have fun with that, too.
Yes and the internal rep is fully exposed by .as_bytes() and there is no way to work around it (unless a magical garbage collector can let you create a new utf-8 vector and pass a slice of it).
That doesn't mean much. Hell, `u8` gets moved in generic code without a `Copy` bound. There's just not enough information about the type.
Even at generic boundaries where there is perfect information, they get moved: fn main() { let mut x = 0; let y = &amp;mut x; borrow(y); borrow(y); consume(y); consume(y); //~ERROR: use of moved value: `y` } fn borrow&lt;T&gt;(t: &amp;mut T) {} fn consume&lt;T&gt;(t: T) {}
When I started glium I was very enthusiastic about this. I was maintaining a clean list of issues and was happy whenever I received a pull request. The problem is that to work on glium you need at the same time an intermediate-advanced level in OpenGL and an intermediate-advanced level in Rust. At the moment few people in the world have both. You also need a lot of motivation to understand all the hacks used in glium to bypass some limitations of the Rust language (namely, the problems with visibility and the lack of HKTs). I also noticed that many people who have some knowledge of OpenGL prefer to run their own solution. I think it's similar to how intermediate C/C++ programmers think they don't need Rust because they don't make mistakes anyway. Only advanced C++ users really understand that it's too hard to correctly do things manually. Of course the design of glium has tradeoffs, but for the moment all the alternatives I have seen are in my opinion strictly inferior. Because of all this, I received several PRs to fix typos, do some minors changes, but in a year and a half I received only two PRs that changed some actual code that does stuff (in other words, OpenGL-related code). The problem is that it takes time to sort issues, understand issues, answer questions, etc. When I have some time for glium I usually spend in one of the urgent necessary fixes, and not on sorting issues and documenting the project. I just think it's not really worth it. What's the point of getting small fixes and cleanup in a library, if that library has critical bugs? 
&gt; I think there are some unstable stuff in the book Yes, but it's at least segmented off in its own section. And there's different levels, too: documenting unstable features is one thing, but documenting internal layout stuff is a great way to get people to start depending on it...
For OSX, does `/usr/local/opt/openssl/include` not work? That way you don't have to worry about a specific version.
The thing is Rust's default string is utf-8 encoded which was designed to allow efficient text manipulation at a byte level to be semantically correct. E.g. algorithms such as sort and find do not need to decode to array of char but can operate directly on the underlying buffer. I do agree that USVs and all other concepts defined by Unicode exist for a reason but they are not general purpose primitives IMO. In the same manner, other concepts such as grapheme clusters and glyphs also exist for a reason but are not provided as primitive types in Rust. Rather, the expectation is that these belong in special purpose crates - "unicode" and "font". Unless I need to implement specific language aware algorithms such as "toUpperCase", there really is no need for such concepts. Hence my original question - what is a valid use-case for ```char``` as a general purpose built-in primitive in the language and why do USVs differ from the other concepts such as mentioned above? IMO the only reason for this is because all languages copy from C where a string is an array of *ASCII* ```char``` and therefore char is a primitive type. In Rust this analogy obviously does not apply because utf-8 is a variable-length encoding of *bytes* and not a random access array. 
If we're renaming things, then we need to find any other name at all for what `char` is. :)
The 32-bit MSVC builds work mostly fine. The main problem, IIRC, is that they they abort on `panic!` instead of unwinding the stack. Hopefully that will be fixed soon, since i686-pc-windows-msvc is by far the most important target for desktop applications. See https://github.com/rust-lang/rust/issues/25869 and the pull request that will hopefully fix it: https://github.com/rust-lang/rust/pull/30448
&gt; The thing is Rust's default string is utf-8 encoded which was designed to allow efficient text manipulation at a byte level to be semantically correct In what way are you thinking this differs to the 2- or 4-byte encoding schemes? &gt; algorithms such as sort and find do not need to decode to array of char but can operate directly on the underlying buffer This isn't true. [Sort](http://www.unicode.org/reports/tr10/) is [language dependent](http://www.unicode.org/reports/tr10/#Example_Differences_Table) among other complications and [find](http://unicode.org/reports/tr10/#Searching) is similar. In terms of other complications, composed vs. non-composed characters are a big one, e.g. `ä` and `a\u{308}` (i.e. `a` followed by [COMBINING DIAERESIS](https://en.wikipedia.org/wiki/Diaeresis_%28diacritic%29#Character_encodings)) should probably sort/search the same, and searching for `a` probably shouldn't pick up the second one. Just doing a plain byte operation will not satisfy either of these. (Normalising text first does help resolve some of these problems.)
Have you tried the `regex_dfa` crate? It did much better than `regex` in some regex tests I made a while ago.
"Log Angeles" meetup! 
I was not aware until just now that Guava was about eliminating data races! Interesting. Paper: ftp://ftp.cs.umanitoba.ca/pub/IPDPS03/DATA/13_02_071.PDF 
This can be written as: let mut output = String::new(); let mut start = 0; for (ix, ch) in input.char_indices() { if let Some(&amp;c) = dict.get(&amp;s[start..ix]) { output.push(c); start = ix; } if ch != 'G' &amp;&amp; ch != 'g' { assert!(ix == start, "Unconsumed prefix"); output.push(ch); start += ch.len_utf8(); } } 
If I understand correctly, notty just models a single character-cell grid with no scrollback? I wanted to model the way libvte and Terminal.app work, where resizing the terminal horizontally causes lines to rewrap, rather than cropping or padding. I also wanted to support VT100 double width/height lines like xterm, but those lines behave oddly when it comes to wrapping so I decided to ditch the wrapping bit (although wrapping is probably more practically useful to more people than double width/height). If I ever get anything working, maybe I'll revisit that decision.
&gt; whether the OP's Rust code has the new memchr optimization I can't speak for OP, but I did run it locally on `rustc 1.7.0-dev (5d4efcb13 2015-12-21)` which seems to have [that commit](https://github.com/rust-lang/rust/commit/8ad12c3e251df6b8ed42b4d32709f4f55470a0be) in it. I even saw it in some profiling: | Running Time | | Self (ms) | Symbol Name | |--------------|--------|-----------|-------------------------------------------------------------------------- | | 7205.0ms | 100.0% | 307.0 | `main` | | 5529.0ms | 76.7% | 284.0 | - `re::Regex::captures` | | 381.0ms | 5.2% | 381.0 | - `str::from_utf8` | | 232.0ms | 3.2% | 42.0 | - `vec::Vec$LT$T$GT$::reserve::reserve` | | 175.0ms | 2.4% | 1.0 | - `fs::File.Read::read` | | 146.0ms | 2.0% | 31.0 | - `memchr::memchr` | | 134.0ms | 1.8% | 121.0 | - `je_sdallocx` | | 82.0ms | 1.1% | 9.0 | - `fmt::write` | | 82.0ms | 1.1% | 82.0 | - `_platform_memmove$VARIANT$Nehalem` | | 41.0ms | 0.5% | 41.0 | - `iter::Filter$LT$I$C$$u20$P$GT$.Iterator::next::next`| I wish there were some low-hanging fruit in `str::from_utf8`, I see this in my profiling in SXD and know that it's likely to already be very optimized.
notty implements scrollback. It even has the code laid to support horizontal scrollback, but that's not being used right now. (fyi the grid is implemented on top of a VecDeque). It crops and pads on resize for similar reasons to you - its ambiguous how to "wrap" an image. I'm also not certain that wrapping on resize is actually what you want most of the time. notty doesn't support double sized lines, but I'm not opposed to it.
Ownership is an extremely common concept among C++ developers. Rust just give the compiler the annotations it needs to check that we're doing it right.
Just a heads up, for new contributor 'Alberto Corona' the username is `0X1A` as in hex for 26 not `OX1A` ;)
If there's one thing I've learned in my years helping on #rust its that you have to start *somewhere*. At some point, somebody will have to learn how to make a PR, learn how to rebase, how the review workflow goes. Some people are highly self-motivating and will figure it out on their own. Others might be less so. The single most important thing to do is to make people as comfortable as possible asking questions. I always reassure people that their questions aren't stupid. I think the other important thing is good, consistent moderation. What I've seen on Rust has been good, as people aren't immediately kicked/banned for breaking the rules, they are politely and firmly told that they have crossed a line. This is important for newcomers as they don't know the rules for the community. Sure we have a written code of conduct (and that in itself is important), but if you expect everybody to read it, you're living in fantasy land. Most of the time I see people say something that's not appropriate, get warned, and then apologize and continue on.
I love long and in-depth series like this. :) It's almost like getting a textbook for free!
Hopefully, I'll turn it into one. Once the shooter is done, I'd like to write about building an RPG with a simple map builder (with maybe some logic programmed in a dialect of [Lichen](https://github.com/jadpole/lichen), although this is also a young project that I might eventually kill). This would allow us to see how to load data and manage resources more effectively than by simply wrapping them in `Rc`. Then, I'd like to do something involving glium and networking, maybe a 3d, p2p ripoff of street fighter or a _really_ simplified clone of Quake (as in, with bounding box based collision detection). I'd also like to explore compiling to the web with Emscripten (I think it can already convert Rust binaries, although this is still experimental), and having Rust on both the client and the server. Then, I'd rewrite them and combine all three in a single ebook. This is thinking _way_ ahead though, so although I'll definitely finish ArcadeRS 1.*, I'm making no promise about future games.
Scala example is missing.
I don't think this issue is taken seriously enough. The time it takes to compile rustc is, for me at least, the _absolute only_ reason why I don't contribute to rustc development. I have limited free time and several OSS projects I contribute to and waiting a couple of hours while rustc compiles and tests run is waaaay past the point of "worth it" for me. I just take my hack time elsewhere. I understand it's a hard problem and that people are working on it and all that. But it has been like this for literally years.
Does this work on Dokku?
Personally, I tend to use it for programming.
it very well might! Please let me know? If dokku expects things to look like they do on Heroku it should.
Ha ha. Fixed now. :P
Thanks.
`str::from_utf8` is likely running through the 10M lines of the file to [check that every single character is valid UTF8](https://doc.rust-lang.org/src/core/str/mod.rs.html#237-240)
Snarky, unhelpful answers aside, Rust is most practical as a replacement or supplement for C and C++, when you want to get as close to the hardware as possible but still want the niceties of higher-level languages like functional programming and foolproof memory management. It is especially suited for embedded platforms where resources are at a premium and luxuries like garbage collection and a heavy runtime can't be accommodated, or whenever these things would use too much memory or CPU to be useful. This makes it suitable for high-performance software like operating systems, game engines, the cores of web application servers and browsers, realtime applications like stock trading bots, etc.
Thanks, fixed. BTW, I'm a complete newbie in Scala and any help in improving the code is very much appreciated :) 
I'm very interested to see efforts to integrate Rust with other products. You mention a few issues such as the lack of posted version strings and multirust configuration, have you filed any bugs for these?
That's not particularly helpful, there are domains where Rust excels and domains where it doesn't. I presume OP is asking about where Rust fits into a programmer's toolbox.
It doesn't support captures at all, and it doesn't even support finding the right matches. But it does support finding the *beginning* of the right matches, and this can be used to dramatically reduce the work that a full regex matcher has to do. I have an experimental fork of the `regex` crate [here](https://github.com/jneem/regex) that has the same features as the usual `regex` crate, except that it uses `regex_dfa` to find the initial match. On favorable examples, this speeds up searching [quite a bit](http://bl.ocks.org/jneem/raw/3f08ade195796358d027/?data=%5B%7B%22y%22%3A%20295351074%2C%20%22ratio%22%3A%200.008334599792245889%2C%20%22bench%22%3A%20%22.%7B2%2C4%7D%28Tom%7CSawyer%7CHuckleberry%7CFinn%29%22%2C%20%22x%22%3A%202461633%7D%2C%20%7B%22y%22%3A%20125978857%2C%20%22ratio%22%3A%200.12771945533685863%2C%20%22bench%22%3A%20%22%5Ba-q%5D%5B%5Eu-z%5D%7B13%7Dx%22%2C%20%22x%22%3A%2016089951%7D%2C%20%7B%22y%22%3A%201631615%2C%20%22ratio%22%3A%200.9981981043322107%2C%20%22bench%22%3A%20%22Tom%7CSawyer%7CHuckleberry%7CFinn%22%2C%20%22x%22%3A%201628675%7D%2C%20%7B%22y%22%3A%20137662852%2C%20%22ratio%22%3A%200.014245985547357395%2C%20%22bench%22%3A%20%22%28%3Fi%29Tom%7CSawyer%7CHuckleberry%7CFinn%22%2C%20%22x%22%3A%201961143%7D%2C%20%7B%22y%22%3A%201724976%2C%20%22ratio%22%3A%200.920081206926937%2C%20%22bench%22%3A%20%22Tom.%7B10%2C25%7Driver%7Criver.%7B10%2C25%7DTom%22%2C%20%22x%22%3A%201587118%7D%2C%20%7B%22y%22%3A%201620677%2C%20%22ratio%22%3A%200.9712465839892835%2C%20%22bench%22%3A%20%22Huck%5Ba-zA-Z%5D%2B%7CSaw%5Ba-zA-Z%5D%2B%22%2C%20%22x%22%3A%201574077%7D%2C%20%7B%22y%22%3A%2084880712%2C%20%22ratio%22%3A%200.0620566425031873%2C%20%22bench%22%3A%20%22%5C%5Cs%5Ba-zA-Z%5D%7B0%2C12%7Ding%5C%5Cs%22%2C%20%22x%22%3A%205267412%7D%2C%20%7B%22y%22%3A%201646883%2C%20%22ratio%22%3A%200.96746763431282%2C%20%22bench%22%3A%20%22%28%3Fi%29Twain%22%2C%20%22x%22%3A%201593306%7D%2C%20%7B%22y%22%3A%2099739%2C%20%22ratio%22%3A%201.0025065420748154%2C%20%22bench%22%3A%20%22Twain%22%2C%20%22x%22%3A%2099989%7D%2C%20%7B%22y%22%3A%2062666721%2C%20%22ratio%22%3A%200.06938462601226575%2C%20%22bench%22%3A%20%22%5C%5CbF%5C%5Cw%2Bn%5C%5Cb%22%2C%20%22x%22%3A%204348107%7D%2C%20%7B%22y%22%3A%2071127171%2C%20%22ratio%22%3A%200.10740621189615428%2C%20%22bench%22%3A%20%22%5Ba-zA-Z%5D%2Bing%22%2C%20%22x%22%3A%207639500%7D%2C%20%7B%22y%22%3A%201664874%2C%20%22ratio%22%3A%200.9960153140718156%2C%20%22bench%22%3A%20%22%5Ba-z%5Dshing%22%2C%20%22x%22%3A%201658240%7D%2C%20%7B%22y%22%3A%207366270%2C%20%22ratio%22%3A%200.1817939065497192%2C%20%22bench%22%3A%20%22%5B%5C%22%27%5D%5B%5E%5C%22%27%5D%7B0%2C30%7D%5B%3F%21%5C%5C.%5D%5B%5C%22%27%5D%22%2C%20%22x%22%3A%201339143%7D%2C%20%7B%22y%22%3A%20243419316%2C%20%22ratio%22%3A%200.01944975065166973%2C%20%22bench%22%3A%20%22.%7B0%2C2%7D%28Tom%7CSawyer%7CHuckleberry%7CFinn%29%22%2C%20%22x%22%3A%204734445%7D%2C%20%7B%22y%22%3A%2094501284%2C%20%22ratio%22%3A%200.04505000164865485%2C%20%22bench%22%3A%20%22%5C%5Cb%5C%5Cw%2Bnn%5C%5Cb%22%2C%20%22x%22%3A%204257283%7D%2C%20%7B%22y%22%3A%20131818244%2C%20%22ratio%22%3A%200.0326143094426292%2C%20%22bench%22%3A%20%22%28%5BA-Za-z%5Dawyer%7C%5BA-Za-z%5Dinn%29%5C%5Cs%22%2C%20%22x%22%3A%204299161%7D%5D), but at the expense of some extra time to compile the regexes.
No, it doesn't support captures -- have a look at line 29 in that file you linked. The reason it lists the captures in all the tests is because I just copied them from the `regex` crate.
Ah. Sorry, shame on me for not checking better then.
Debugging internal state as opposed to external user provided input? I'm talking about regular printf-style debugging, e.g. "variable x = 42". 
Do Servo uses it or may it be useful in any way?
I might be wrong but ldc and gdc have much better code generation with respect to D. I couldn't test it myself since I don't have the data used for this. What was the compiler flags used for D?
Fixed, and my apologies for the typo! The process of mapping new contributor names to GH usernames is shockingly manual, and I'm sadly not really sure how to improve that. Also, assuming that you're Alberto, I'm also in Chicago, and would be happy to grab a coffee &amp; share some stickers at some point :-)
Working on compartmentalizing and improving the safety of the garbage collector for https://github.com/Marwes/embed_lang. It may be some time yet but I hope to eventually have it as a separate library so other projects can use it.
`len: vec.len()` Nitpick: I think you need to divide `len()` by the size of `T`.
That's definitely true: DMD has faster compile times, and as the reference implementation it has the newest features available first, but the codegen is not comparable to LDC and GDC.
I just "run as Scala console" from IDEA. It loads the app to REPL as I understand, then I call `Main.main(Array.empty)`. It performs equally on very first run and the subsequent runs.
Wow, thank you very much for the detailed explanation, and for the examples! I had read the unsized types chapter a couple of times before, but with my limited understanding (I'm coming from Python and don't have a CS background) I wasn't able to grasp its significance, especially not in this context. Things are a little bit clearer now.
&gt; I don't think this issue is taken seriously enough. The majority of work on the compiler since May has been focused on compile times. I sometimes worry it's been _too much_ of our focus. &gt; it has been like this for literally years. It has always been slow, but it has been _massively_ sped up. Like, it used to take me two hours and now takes me 30 minutes. I feel your pain too, but it has been getting better and will continue to get better.
Doesn't dokku just make Dockerfiles under the hood now? Last time I used it the Heroku buildpack for something was lacking the ability to build GIS libraries (GEOS, GDAL, et al) and so I just made a custom Dockerfile. Worked quite well.
&gt; check that every single character is valid UTF8 Yup, which is a very important guarantee, and definitely should be run on every unknown piece of data (like reading a file from disk) that wants to be a string. That's exactly why I wish it were faster, though, as it has to be inserted in all sorts of paths like that.
Those are on my list, but I haven't posted bugs yet! That will be on the agenda later today. Thanks a lot!
Don't you think it can be improved (SIMD-ified I guess) to quickly scan ascii data? Assuming we want to optimize for that. Edit: lol, working on it. I guess finding a good tradeoff between the ascii vs nonascii strategy is interesting.
That commit doesn't affect any code that's used in the benchmark, I think Edit: Oh it does! it affects lines() via .read_line and read_until, which uses memchr
&gt; The Rust code is as twice as long as F# one, but it's handling all possible errors explicitly I don't think this is fair, since the Rust code was formatted using a very questionable style, leaving lines with tons of whitespace. He should format with `rustfmt` first to compare this. edit: now, one thing that makes Rust verbose is the `From` boilerplate and the granular `use` at the top.
On Sunday I started to write a rust port of [VTFLib](http://nemesis.thewavelength.net/index.php?p=40), which I will hopefully end up placing in a crate with other source engine tools.
For example, [here](https://www.reddit.com/r/rust/comments/29ywdu/what_you_dont_love_about_rust/cipvbpc): &gt; I would wish for more syntactic sugar and everytime something gets removed from the language, i just hope that the reasoning everything is done to have the possibility to do that in the langauge is true. I would wish for more syntactic sugar in the language, akin to say python, where i can do array[2:] or array[::-1] or whatever. Also, currently my rust code uses a log "as_slice" and "to_string", which looks noisy.. Rust already has a facility for such syntax sugar (just implement `Index` &amp; friends for `Range` or other types), even if not included in the stdlib yet; and I think the proliferation of `as_slice` was alleviated (the `to_string` problem remains however; this always involves an allocation and Rust wants to be explicit here. But perhaps there could be some sugar)
Have you tried compiling Rust from AUR with a package like [`rust-nightly-bin`](https://aur.archlinux.org/packages/rust-nightly-bin/)?
I'm sad [this](https://www.reddit.com/r/rust/comments/29ywdu/what_you_dont_love_about_rust/ciq78tr?context=3) wasn't adopted :(
No, I haven't. Don't those packages actually install Rust? I don't need to install it (I already have it installed via multirust), I just want to be able to make changes to the source &amp; re-compile so I can contribute back to the project.
&gt; lol, working on it. 😻 You are a **machine**! 
Fair enough. edit: I think that for such short snippet it's better to use `Box&lt;Error&gt;` instead of defining custom error types.
&gt; using a very questionable style I guess my personal style is "very questionable" then, as it closely matches what the author had :-) When I read over it the first time, it all looked pretty reasonable. The only big things I saw were the use of actual tabs and a few closing braces / parens that should have been moved to the next line (which would make the code longer). I found it pretty readable, overall.
I was commenting to something like this: let name = name. ... But either I read incorrectly or the author updated the code :P I supposed that `rustfmt` would make it more compact, but I was wrong.
Yes, this compile from source and install it. Perhaps you could try this package just to see if it compiles without installing it (saying No at the end) so that you could see how to compile it on Arch using the PKGBUILD (if it works).
Not quite rustfmt territory, but the `match` could become `if let`.
Join us at #rust-machine-learning on irc.mozilla.org I can hand out mod bits after my holidays.
The gains are pretty easy for pure ascii. Not much I know can be done about non-ascii, except trying to keep the ascii path from slowing it down(?) https://gist.github.com/bluss/bf45e07e711238e22b7a This isn't even simd-ified, it compiles to reads of u64 words.
As requested here: https://www.reddit.com/r/rust/comments/3z8msk/another_machine_learning_crate/cylowd9 I can give out mod bits next week (I'm on holidays). Send requests to flo@andersground.net. Or post them here.
Thanks, I'll give it a go. Owning Json types doesn't seem like a hugely terrible idea.
I'm just frustrated because I want to hack on rustc but can't stand the wait time. :)
Thanks, this is indeed much nicer. I just pushed it to GitHub Pages.
&gt; This isn't even simd-ified So this [isn't even it's final form](http://knowyourmeme.com/memes/this-isnt-even-my-final-form)? I **am** surprised I didn't see this submitted as a Pull Request yet. \^_\^
It's probably the areas where the library ecosystem is more developed (because, leaving Rust's strengths aside, a large part of modern programming is selecting the right libraries and gluing them together effectively). So, say, if you want to write really fast web services, Rust is probably practical (or will be soon enough). Also, I'm thinking that the first Rust niches will be "things that could benefit from C++ levels of performance, but we don't use it because it's too unsafe or not really practical", that is, expand the applicability of low level languages. That would be kind of a reversing on the trend of the last 10 or more years, which was more like, "use ALL dynamic languages!". edit: another point are domains when, besides wanting performance, you also want to write parallel code. Here, Rust's increased safety is a big win.
Thanks for the benchmarks update with LDC :)
Can you please update the compiler flags used for the LDC and DMD builds?
The benchmark measures a mixture of times, not only the regex time. At least the D version is non-idiomatic: * The I/O allocates one string per iteration * The last `join` in the pipeline computing `res` allocates without a need, should be replaced with `joiner`. * f should be `File`, not `File*` (unlikely to impact anything) The first two are likely to impact measured time significantly. To benchmark the speed of regex proper, only time spent in that should be counted.
Me too. Since I don't have to do it that often myself, when I do it's extra annoying 
FWIW, the Rust version is allocating a string per iteration too, although it is using a standard method (`std::io::BufRead::lines`). I think we should endeavor to be more charitable. This post is not a benchmark. In fact, it does not even claim to be one. It's an exploration!
https://doc.rust-lang.org/book/closures.html
By the way, nothing in nightly is working in play.rust-lang.org. Now, I thought that `FnMut(T) -&gt; R` wasn't a type, but [a trait](https://doc.rust-lang.org/core/ops/trait.FnMut.html) (implemented by all functions and closures that take a `T` and return a `R` -- and also capture their environment mutably?)
Several of the other answers are great if you're asking what types implement the `FnMut(T) -&gt; R` trait, but in case you understand function traits and are asking about the distinction between `FnMut(T) -&gt; R` as a trait and as a _type_: all traits are also names for dynamically sized types, which are the type of a trait object. This is why you can take closures as `&amp;mut FnMut(T) -&gt; R`, and you can take `Box&lt;Write&gt;` and so on. If you don't know about trait objects, read [this](https://doc.rust-lang.org/book/trait-objects.html). You cannot instantiate a value of type `FnMut(T) -&gt; R` directly in any way that I know of.
All traits are also valid types as a part of how they are used in trait objects, but they cannot be instantiated afaik.
I think it's noteworthy that the D regex engine is largely the work of a single developer, who wrote it as a [Google Summer of Code project](https://github.com/DmitryOlshansky/FReD) in 2011. I can't speak to what's been improved in the standard library version since then (if anything), but it was already tremendously fast in 2011 -- a remarkable piece of summer-project engineering on the part of a very talented student.
Also, immutable `Fn` closures can be used where `FnMut` is needed. `Fn` ⊂ `FnMut` ⊂ `FnOnce`
Thanks so much!
That describes the trait, but not its use as a type.
That's true of the `FnMut` trait, but not when used as a type.
Ah right, my mental model for trait objects was very different. I sort of feel like this kind of use should be a compiler error, since the impl is nonsensical for a bare trait object type, isn't it?
Yeah. There's a testcase where it regresses performance though, 10MB of ja-wikipedia's article dump (mostly 3-byte utf-8 encodings, some ascii in between); perf regresses 15%. Want to make it a win for that case too, if possible, before submitting.
Ah, that `len_utf8` method seems interesting. Having that does simplify it nicely, thanks! :D
I think I'm going to solve all of my programming problems by just mentioning them to you from now on. Then I just have to wait a few days and *bingo* I'll have a nice solution.
Here's a slightly modified version for Scala. I replaced the mapping and explicit group handling with a simple `collect` call, but I had to change the regex slightly (add `.*` to the end) since doing it this way expects a perfect match per line instead of a partial one. I also did things a little more idiomatically and eliminated an unnecessary `toList` call. Altogether it reduced runtime from ~6s on my machine to ~4 while still reporting the same results. import scala.io._ import scala.util.matching.Regex object Utils { def time[A](f: =&gt; A): (A, Long) = { val s = System.nanoTime val ret = f (ret, System.nanoTime - s) } } object Search { def search(file: String, pattern: Regex): Iterator[String] = { Source .fromFile(file) .getLines() .take(10000000) .collect { case pattern(x) =&gt; x } .map( _ split Array(';', '"', ' ') filter (_.nonEmpty) mkString ".") } } object Main { def main(args: Array[String]) = { val (length, elapsed) = Utils.time { Search.search("big.txt", """\{.*(?&lt;name&gt;Microsoft.*)\|\].*""".r).length } println(s"Found $length lines, elapsed ${elapsed / 1e6} ms") } } 
yussssssssss Thank you.
There's a couple different ways you could reference a `Vec`. If it's the way you're thinking of, like `&amp;Vec&lt;T&gt;`, then that's a reference to the `Vec` itself, i.e. a pointer to the `Vec` struct on the stack, which contains the data pointer to the heap allocation and the length and capacity values. An `&amp;mut Vec&lt;T&gt;` is the same, but the reference is mutable so you can call methods that take `&amp;mut self`. A `Vec&lt;T&gt;` can also be dereferenced to a slice, `[T]` which is usually either represented as `&amp;[T]` or `&amp;mut [T]`. This is slightly different, as a slice is actually a fat-pointer: one data pointer (copied from `Vec`'s internal pointer, or an offset of it if the slice doesn't represent the whole `Vec`) and a length value. So what's passed around looks kinda like this: `(&amp;T, usize)`. Both of these actually use raw pointers in their implementation instead of references, since they have to do pointer arithmetic in a lot of their methods--mainly those that have to do with indexing, slicing, and iteration. `Vec&lt;T&gt;` uses `*mut T` since it owns the allocation. Slices ostensibly use `*const T` but their layout is defined by the compiler (as they're a built-in type) so it doesn't really matter.
This seems only marginally useful though, and almost entirely without good use case once arbitrary `Foo + Bar` trait objects are implemented. Is there a use case that I'm not seeing? I think in most cases impls like this are written in error.
&gt; In what way are you thinking this differs to the 2- or 4-byte encoding schemes? It doesn't. It is the same complexity, i.e surrogate pairs exist. The idea of string as an array of a primitive ```char``` type is too simplistic for Unicode. The primitive here is the ```String``` itself. It can be encoded using one of several variable-length encodings and it can be interpreted at several different levels of abstraction as sequences of code-units/code-points/graphemes/grapheme-clusters/glyphs/etc.. Some of which are also variable-length! &gt; This isn't true. Sort is language dependent among other complications and find is similar. This is complicated :) Rust's std provides a basic ```String``` type and there are find/sort/etc that are language independent. This is correct for basic text manipulation when searching for *exact* (rather than equivalent) sub-string matches (byte-level identical). Handling equivalent strings of said search term is left to user-code/3rd party crate. It is a reasonable design and as you said so yourself, when the user wants to match not only identical but also equivalent sub-strings than they should either normalize the code in advance or use a higher abstraction that needs more inputs (locale, language, collation, etc..). Both options are semantically valid and can have use cases. For example if I want to compare frequency of composed vs. non-composed characters in my text I explicitly do not want to normalize my text nor do I want to use the language dependent search that treats them as equivalent. 
Good point! I couldn't get it to work, but the explicit type makes everything makes sense. Imo in a perfect world, it would have to be written as `impl&lt;'a, F, T, R&gt; Bark for F where F: FnMut(T) -&gt; R + 'a`, but then we run into [#25041](https://github.com/rust-lang/rust/issues/25041). (We could get back to the shorter version with the syntactic sugar that /u/desiringmachines mentioned a few posts up, which would be reusable in lots of other places).
That's a different thing, too, so I don't agree it necessarily should be that instead.
I really like these talks, but I wish that there were more talks that covered the "we don't have enough time to talk about this" stuff :)
What part of the reasoning behind the decision to have methods and to not have glob imports by default did you disagree with?
Another interesting tidbit that I just discovered after messing around. The `regex` crate is actually using a *backtracking* algorithm in the OP's code. This happens because the combination of both the regex and the search text is very small (since the regex is run per line rather than the whole input at once). If I force the issue and make the regex crate use the NFA engine instead, runtime **doubles**. Wowzah! This actually makes a lot of sense because the backtracking engine is specifically used because it handles capture groups more efficiently. Namely, instead of maintaining a set of captures for every state in the NFA and copying them around all the time, the backtracking engine maintains one set of captures and saves/overwrites them on the stack. (We maintain linear time guarantees by keeping track of all visited states, which has unfortunate memory requirements. Therefore, the backtracking engine is only used on small regexes and small inputs.) /u/vaskir - I think you're the OP, right? This should serve as a cautionary tale for how implementation dependent regex performance is. Just because you have a language that is "supposed" to be fast doesn't mean the regex implementation will do everything optimally!
Actually I just realized that Rust is only holding its own here because of its backtracking engine, which is [only used on a subset of regexes/inputs](https://www.reddit.com/r/rust/comments/3zh95h/regular_expressions_rust_vs_f_vs_scala_vs_d/cynh9xz).
Sure, I wanted `format!` to behave a bit different - namely to have following behavior, in addition to its other behavior. Also I don't need the alignment shtick. // This identical to format!("Example {} ", name); format!("Example {name}", name); This isn't possible, if I understand correctly, you need to do following: format!("Example {x} ", x = name); 
Actually `RefCell` doesn't implement `Deref`, because `borrow()` can panic and it would lead nasty suprises.
Note that `try`/`catch` is not exceptions, it's just fancier functionality built around `try!` (in fact, a basic implementation would be a macro expanding to an immediately-invoked closure, but this doesn't handle some cases). Everything good about algebraic data types for errors still exists, e.g. `Result`, errors-are-values, more explicit control-flow etc. In fact, `try`/`catch` requires all that to work. (In any case, the two other proposals are designed to be improvements for the use-case Rust is targeting, even if they make the language not as "pure". The goal is high-performance, low-level... basically precise control, not some abstract sense of 'cleanliness'. The interesting part of Rust/why it might be worth knowing is mainly the memory model: most other aspects of the language are better, or at least clearer, elsewhere.)
Propagating a failure is still explicit inside a `try`/`catch`, so I think so? But I'm not really sure what catching a non-wrapped value would mean, maybe you could expand?
&gt; \#[deny(missing_docs)] Ooh. Thanks for that tip. I hadn't noticed that one, and it's great for code hygiene. One tip for anybody newer, if you want to apply it to the entire file, you need to use &gt; \#![deny(missing_docs)] (note the exclamation mark). I put that in my lib.rs and suddenly I'm told exactly where my docs are missing. Also, some people might prefer &gt; \#![warn(missing_docs)] instead
Quoting from the "Motivation and overview" section, at the beginning of the [relevant RFC](https://github.com/glaebhoerl/rfcs/blob/trait-based-exception-handling/active/0000-trait-based-exception-handling.md) Rust currently uses the enum Result type for error handling. This solution is simple, well-behaved, and easy to understand, but often gnarly and inconvenient to work with. We would like to solve the latter problem while retaining the other nice properties and avoiding duplication of functionality. We can accomplish this by adding constructs which mimic the exception-handling constructs of other languages in both appearance and behavior, while improving upon them in typically Rustic fashion. Their meaning can be specified by a straightforward source-to-source translation into existing language constructs, plus a very simple and obvious new one. [...] The most important additions are a postfix ? operator for propagating "exceptions" and a try..catch block for catching and handling them. By an "exception", for now, we essentially just mean the Err variant of a Result. Seems pretty clear to me that this spec is about making it less verbose and easier to handle errors with the current Rust error-handling paradigm (Result type, etc.), and not a paradigm change towards actual exceptions as implemented in other languages. 
Ty fixed. 
I meant to draw a comparison between "not having exceptions" and "not having `try`/`catch` syntax". In Rust, as opposed to Java or C#, any function that could cause control to go to the `catch` branch must return a `Result&lt;T,E&gt;::Error&lt;E&gt;`. In other words, the possibility of errors is explicit in the return type, and must be handled in one way or another by the caller--and `try`/`catch` is merely syntactic sugar of more verbose error handling code.
I may have to redefine "early" to mean the first 24 months...
At first glance, it seems a bit odd to have unwinding in the language and have to deal with all of the ensuing complexities, but also have `try` / `catch` syntax that doesn't invoke unwinding.
Probably worries about implementation/spec complexity.
Just an idea: You may get simpler code by using a channel instead on an iterator. Then you don't have to worry about lifetimes. http://rustbyexample.com/std_misc/channels.html
`foo()?`seems like a great improvement here and now, but perhaps `try` / `catch` should be postponed till the dust has settled.
If `catch` isn't a keyword yet, it should be though (`try` seems more complicated, not sure if it can be done backwards compatibly because `try!` should continue working) ... actually, another problem: the existing `try! { }` has a different semantics than the proposed `try { }`. That's confusing.
&gt; We might finally get to wave goodbye to drop flags. We can dream, anyway. Do you mean storing them in the stack frame instead of a hidden field of the object? (does Rust still have drop flags in a hidden field [as the Rustonomicon says](https://doc.rust-lang.org/stable/nomicon/drop-flags.html)?) Or, perhaps, only storing a drop flag if the compiler can't determine drops statically? (aka "pay for what you use"). Just to clear up, [this](https://github.com/rust-lang/rfcs/pull/320) and [this](https://github.com/rust-lang/rust/pull/23535) was actually merged, right? I'm seeing the LLVM output of [this](https://play.rust-lang.org/?gist=2480629a59259cacd869&amp;version=stable) struct S(u32); impl S { fn hehe(&amp;self) -&gt; S { S(self.0 + 1) } } becomes ; Function Attrs: uwtable define internal i32 @"_ZN13_$LT$impl$GT$4hehe20h41bb72fd63efb8ddkaaE"(%S* noalias readonly dereferenceable(4)) unnamed_addr #0 { entry-block: %sret_slot = alloca %S %dropflag_hint_13 = alloca i8 %self = alloca %S* store i8 61, i8* %dropflag_hint_13 (...) So it looks like `hehe` is allocating a `%dropflag_hint` in the stack (but wait, I didn't implement `Drop`, where does this hint comes from..). I must be misunderstanding something. It seems to be related to [this](https://github.com/pnkfelix/rust/commit/dce1c61e977deed5e0a499f4247d5da870809692). --- ... actually I think I found some more reference, on [this](https://this-week-in-rust.org/blog/2015/08/03/this-week-in-rust-90/) TWiR: &gt; [Nonzeroing move hints](https://github.com/rust-lang/rust/pull/26173). This patch adds drop flags to the stack as part of the [non-zeroing drop RFC](https://github.com/rust-lang/rfcs/blob/master/text/0320-nonzeroing-dynamic-drop.md). It does not yet remove the drop flag from objects.
Didn't expect to see this on reddit! You're welcome :)
Ah, that makes much more sense.
Github repo says: &gt; Latest commit f93aa5c on May 18, 2015
[removed]
Really can't wait for trait return types. It'll go far to clean up the multiple types that handle iteration. 
Thinking about it. Maybe I should return two vectors, one with only the pointers (to be used in C) and the other with the CStrings.
I'm trying out the proposed `?`, and having this prefix form just ends up very subtle and hard to find. Look at this code, the `?`'s are next to invisible to me. fn into_matrix_mut(self) -&gt; Result&lt;BlasArrayViewMut&lt;'a, A, D&gt;, ShapeError&gt; { if self.dim.ndim() &gt; 1 { self.contiguous_check()?; } self.size_check()?; Ok(BlasArrayViewMut(self)) } The `foo()?` cases from the RFC are much longer lines in reality. In different code, I have multiline expressions that are wrapped in `try!()`, and with `?` it's not an improvement — the prefix form is actually easier to read and understand. I think this sugar is like `try!`, hard to learn or explain without going through how it desugars. Having it one step further removed doesn't help.
Well, nothing changed since then. Most of the named libraries still aren't post 1.0 nor marked as stable.
Actually, I just saw that miniz is a data compression library. So, is it included in *every* Rust program? Can the program at least call the functions of miniz for their own stuff?
&gt; does Rust still have drop flags in a hidden field as the Rustonomicon says[1] ?) Currently, yes &gt; Do you mean storing them in the stack frame instead of a hidden field of the object? Yes, this is still the plan. It's waiting on MIR/HIR.
[PR'd](https://github.com/rust-lang/rust/pull/30740), but I don't even know if tests pass yet. 
There are a number of new libraries that the site says we don't have things for, and stuff has still matured a lot even if it's not 1.0.
"What's there is looking good" is separate from "what could be there". Also, remember, Rust isn't batteries-included, so while those things are important, I personally don't think that it's that big a deal that they're missing. These kinds of libraries are notoriously hard to get correct.
We will get there. It's just the way that conferences work. They need to provide enough value to attendees, and since the chances are high that they haven't even heard about Rust, let alone know it well, it's gonna be intro talks for at least the next year. That said, RustCamp is a notable exception. You might want to watch some of those videos, as some of them went a lot deeper.
/r/playrust ?
&gt; Look at this code, the ?'s are next to invisible to me. Isn't this somehow the point? If you want to look for them you will find them and otherwise they're almost invisible. The important thing is, that you can't really introduce errors that way, because if you forget a `?` you will get an error, because `Result` return values have to be used and if you wrongly add one the return value of the function has to fit. I can understand that people want to see everything, have the complete control, but if you can't get harmed it's first of all more information to consume, without IMHO that much additional help. 
I'll be checking those video's out. Thanks for the tip.
Can you explain what you mean by exploratory analysis? If you mean sort of like exploring data via something akin to iPython notebook, that's where I see the big gap between rust and Python. Lack of libraries is not too big of an issue to solve, and it's clearly something multiple people are willing to tackle (we have 10+ crates for this area). But I don't know how easy something like ipynb is to do in rust.
I imagine /u/ehiggs means the stage of ML where you are engaging a new problem or new data. In this stage you generally want to iterate over different ideas quite quickly. Though I think that connecting Rust to an 'exploratory language' would certainly help. I think in fact that Rust is in a strong position to handle these things itself. Of course, the tools aren't there yet...
Great write up! Hopefully we can start working on a game plan with the new IRC.
Yes, exploratory stuff is always going to be tricky without a REPL. Having said that, a lot of the time my exploratory analysis in Python consists of incrementally modifying and re-running a script, and I can't be the only one who prefers to work that way.
One thing that is really important for me (and I have implemented in rustlearn) is sparse matrices. Do you see those as part of ndarray, or would you prefer for them to be separate?
Agreed, and to that I'd add special matrix operations and packed storage. I ended up recoding lots of the blas / lapack functions (instead of using [rblas](https://github.com/mikkyang/rust-blas)) precisely because I wanted to take advantage of triangular or symmetric structure. These fundamentals are important before we can move forward.
I see. Thanks.
I believe `?` will also work with `Option`.
No plan for that, there's a [different sparse mat](https://github.com/vbarrielle/sprs) project that I know of. I'm not sure there's any benefit to have ndarray have the functionality in the same type (probably not a good idea), but ideally we'd have a common trait?
&gt; it seems that libc needs Rust nightly You should be using the external crate, not rust's private implementation. (but you can't do that on playpen)
Oh, that's nice. I haven't. I'm just using the accelerate bindings with the blas (not rblas) crate, and implementing my own Matrix/Vector structures. Thanks.
That depends - if the call sequence is like fn wrapper(args) { ptr = make_cstring_array(args) call_some_ffi_that_uses(ptr) } and the pointer isn't used from C afterwards, using `into_raw` makes everything more complicated for no reason.
Hmm. We just have that `ptr` now is owned by C (and you need to call `free` to free it). I think that's more intuitive than having a pointer that is only valid while `args` is in scope (but the compiler won't prevent you for using anyway). edit: ~~anyway, I think that if he is going to return a pointer to a function parameter that is not bound by a lifetime, he should better put an `unsafe` marker on the function.~~ actually no, merely creating a pointer doesn't violate memory safety
See also [this project](https://public.etherpad-mozilla.org/p/rustup-new-experience) by the core team, which aims to greatly simplify all this.
&gt; We just have that ptr now is owned by C (and you need to call `free` to free it) Nope - you cannot call `free` on it. You have to get it back to Rust and drop the CString returned by `from_raw`. So we can't really say that it's owned by C anyway. That's why, if you don't need the char buffer to outlive the Rust wrapper, I think it's backwards to use `into_raw`.
it implements command invocation and soforth, which is presumably the context given `argv`.
[Sure enough](https://doc.rust-lang.org/std/ffi/struct.CString.html#method.into_raw), thanks! &gt; The pointer must be returned to Rust and reconstituted using from_raw to be properly deallocated. Specifically, one should not use the standard C free function to deallocate this string. But wait.. how to pass ownership to C then? Is it possible *at all* to pass ownership from Rust to C?
&gt; edit: actually, this code is crashing Panicking, I guess? Free capacity isn't taken into account by `into_boxed_slice` (otherwise you could access noninitialized elements), so your array's length is zero. Suggestion: fn string_array_to_c(strs: Vec&lt;String&gt;) -&gt; *const *const c_char { let mut res = strs.into_iter().map(|s| CString::new(s).unwrap().into_raw() as *const _) .collect::&lt;Vec&lt;_&gt;&gt;(); res.push(std::ptr::null()); Box::into_raw(res.into_boxed_slice()) as *const _ } As for borrowed vs leaked pointers (`as_ptr` vs `into_raw`), see the other thread.
Not sure if there's a better way, but you can always call `malloc` from Rust (and then memcpy from a String::as_bytes() slice).
I also prefer working that way. I almost always regret doing analysis with the REPL.
Elegant. I like that!
With syntax highlighting they probably will be more visible
Haskell and SML, not to mention Idris, have REPLs, and no one could call them weakly typed or free of constraints. I don't know if people *expect* a REPL for Rust, but REPLs are certainly *nice*.
* Trying to land the three [rust-www PRs](https://github.com/rust-lang/rust-www/pulls). * Catching up on pings * Getting the [emscripten port](https://github.com/rust-lang/rust/pull/30629) to pass tests. * Adding [RUSTFLAGS to cargo](https://github.com/rust-lang/cargo/pull/2241)
Not the OP, so I wasn't talking in the context of the VM, but in a more general sense. Still, the VM could be enhanced with something like memory segmentation. Even if this is not the case, I don't see how `u16` for the `MEM_SIZE` constant would be cleaner that `usize`. It wouldn't even help you ensure that you don't have unused memory or try to load a program that is too large, since addresses for this VM are 15 bits long; this would have to be checked separately (and explicitly). The only reason I can think of right now that would be an advantage of `u16` is that it's using less memory.
Yep, good idea, just added latest, stable, beta and nightly Latest is stable :-) https://github.com/waxzce/rust-docker/commit/97a9870e36c908aa17a202950f6c72977198cc5f Images will be available tomorrow 
&gt; The trouble is that with this, one will need to re-implement the CString::new logic (put a NUL in the end). I agree. Also, a general method must check for inner NULs. (As I said, maybe there is already a better method?) 
I was using `u16` everywhere (since the instructions specify 15-bit values) but the compiler required that I use a `usize` to index the memory and register arrays. When opening the file, I only need extra code in the case where it's *not* Ok. So using `if let` would look like: if let Ok(f) = File::open(filename) {} else { fatal!("cannot read program file {}", filename2); } Which is kind of awkward. Thanks for informing me about range patterns. It looks like they only work with literal values, though: Compiling vm v0.1.0 (file:///E:/Code/synacor-challenge) src\main.rs:69:19: 69:20 error: unexpected token: `(` src\main.rs:69 0 ... (MEM_SIZE - 1) =&gt; a as u16 ^ The `&amp;[u16]` syntax lets the function take any reference to a collection of `u16`s? That's useful. Yes, all memory values are 15-bit. At worst, `add` will act on 32767 and 32767, giving a sum of 65534, which still fits in a `u16`; then the modulo turns it into 32766, a 15-bit value.
Oh, so *that's* why `try!` wasn't working. Thanks, now I understand what it means (and how it's different from C++/Java/Python). So now the `load` function explicitly returns `Ok(())` when it succeeds, and error handling is moved to the caller: let filename2 = filename.clone(); match vm.load(filename) { Ok(_) =&gt; {} Err(_) =&gt; { fatal!("cannot read program file {}", filename2); } } (Do I really have to clone the filename? I wouldn't expect opening a file to own the name from then on. And can this be done with an `if`, so I don't need that empty `Ok` handler?)
It should be good to go. One of the design goals for rust was this exact situation. 
note that non-zeroing move hints were disabled by default (due to bugs) in https://github.com/rust-lang/rust/pull/27582 I don't think anyone has had time to try to turn them back on.
Actually, Microsoft offloads text rendering to GPU in their Office 2013 and upper products. I think this way it faster to render on GPU because of more parallel processors available. Also you can render glyphs to cache texture and draw them later from it for small glyphs. This way you can get smooth sharp zooming, but should do something for subpixel rendering for sharp small glyphs.
In general, Rust _defaults_ to static linking, but you can choose dynamic if you prefer.
Great post! One meta-note is that I like how the references are side-by-side with the main text. &gt;An earlier discussion on “pinning” established that any object referenced by a conservatively scanned root cannot be moved by the GC. Therefore, integrating with a GC that does not support object pinning (such as a fully-copying collector) will require we scan the roots precisely, not conservatively. Would it simplify requirements to say that Rust only allows GC implementations that support pinning? &gt;What should we do about unsafe pointers *mut T and *const T. For example, it is not uncommon for library code to convert boxed data Box&lt;T&gt; to a *mut T or vice versa; that is an ownership transfer. Is it reasonable to require that Gc&lt;T&gt; types that are in a box cannot be transmuted using into_raw()? Perhaps by using an "anti-bound" Trait ("NoRaw"?) in the bounds for that function. Similarly, transmuting back to a Gc&lt;T&gt; via from_raw() would be forbidden. Obviously this means that you can't pass pointers to GC memory over FFI, so perhaps that is too onerous of a requirement. (Disclaimer: I'm no expert on GC, but I have written apps that mix native (c++) and GC-managed code) 
I'm planning to support multiplying a sparse matrix with an ndarray array eventually. Currently it's implemented with some quite limited matrix type I've coded. It looks like it shouldn't be too hard.
Nice post! &gt; pinning I think there is a lot of mileage to be had from non-moving GCs. People don't discuss them these days and all production GCs are moving but I've built systems using non-moving GCs and they work really well. You don't need to reload references after GC points. You don't need to worry about external code making assumptions about GC allocated heap blocks not moving. Pathological performance for typical generational GCs is ~3x slower than necessary which you don't suffer with non-moving GCs. Life is easier and, provided you have value types and reified generics, performance is basically the same. If you aren't already familiar with it, make sure you check out the Very Concurrent Garbage Collector. &gt; conservative...BDW Don't do it. Conservative GCs suck. BDW plagued the Mono project and it is only now (after almost a decade!) starting to recover by fixing its rotten core. It makes no sense to use conservative GC when you're starting from scratch. The throughput is bad, the latency is just awful and the added complexity is needless. 
In this particular case, since you are only using a single integer value (zero), it would probably be better just to use `Option&lt;&amp;'static str&gt;`, and instead of returning zero, just return ` None`. Of course, this won't necessarily be the case in general.
One thing I noticed is lines such as `let mut data: String = String::new();`. You can just let type inference do its thing there. And functions that take `&amp;Vec&lt;T&gt;` should usually just take `&amp;[T]` instead, and deref coercions will still let you pass a reference to a vector to such functions. Your `Scanner` performance is O(n^2 ) because your `peek` operation is O(n).
&gt; And functions that take &amp;Vec&lt;T&gt; should usually just take &amp;[T] instead, and deref coercions will still let you pass a reference to a vector to such functions. What's the advantage? I could pass a slice into other types of collections? &gt; Your Scanner performance is O(n2 ) because your peek operation is O(n). This is one reason why I want to change from using a String for lexing to using a stream of some sort.
Well you could pass a reference to an array, or a slice of an array. In theory you could make your function generic over any iterable type, but that is probably not worth the trouble in your case. Instead of storing a stream you could just sort the character iterator, possibly made `peekable()`.
&gt; The if let boilerplate in the tc_* type checking functions is pretty ugly. How could this be improved? I think the issue here is that you're repeating case analysis, which almost always feels wrong. The best solution that I see is to define a new type for each variant in `Stmt`, and change your methods `to_stmt_assign`, `to_stmt_read`, ..., to accept the corresponding type instead of `Stmt`. By accepting `Stmt`, you're committed to doing case analysis again to extract the associated data. If you switch to using a new type for each variant, then you only need to do your case analysis once, which will feel better! From briefly looking at your code, it actually looks like the `else` bracket in each of your `tc_stmt_*` functions is never reachable if your code is correct. It would probably be clearer to communicate that with `unreachable!`. (But I think my suggestion above is better, because then it will be statically verified and there's no need for `unreachable!` at all.) Generally speaking, it is OK to panic for an invariant violation (i.e., a bug). Other than that, your code looks pretty good to me. Pretty much exactly what I'd expect to see in a Rust implementation of a tiny language. Nice work. :-)
How does Haskell handle type inference? E.g. &gt;&gt;&gt; let nums = [1, 2, 3].iter().collect() &gt;&gt;&gt; nums.len() ???
Is there a handy source that's not out of date? What's the actual current web situation?
On the subject of the tc_* type checking functions, it's odd to me they all have a prefix. The fact they're all called on an instance of `TypeChecker` should be enough
How offline are we talking? Bottom of the ocean? Nothing but a 2G mobile connection?
After browsing the prelude I am of the opinion that you should name this language `In`.
I'll have a wifi connection from time to time but roaming isn't cheap so spurty. Can't go stack overflow / Google random issues.
[Here](https://archive.org/details/stackexchange) is an 8 Gb archive of stack overflow posts. If you have intermittent wifi, you won't need to worry too much about pre-downloading cargo packages. You can learn just fine without them. After installing `rustc` (at least on Windows), you'll have the book (README.html) and standard library docs (std/index.html) located at install_dir/share/doc/rust. Any cargo libraries you download, you can generate docs offline from the source with `cargo doc`.
But if you need smthing by value it won't help you, I guess.
Yes, but I figured the string had some meaning. The "string or zero" looked suspiciously like a C idiom of using zero or NULL as a return value to indicate "no value", which is why I brought up `Option` as Rust's alternative to that.
Update: This only works on specific nighly builds of the rust compiler. nightly-2015-12-26 is known to work.
I was under the impression that it's not possible to bound returned values on traits until abstract return types have landed. Is that wrong?
Noted :). I have been annoyed by needing to write `in` after every declaration myself so the syntax will likely change for this, likely by having an indentation based syntax as Haskell has. (If I compare it to rust, the reason `let` is needed though is for basically the same reason as the semicolon in rust. (`let id = |x| x; 1` == `let id x = x in 1`). I do write `let ... in` a lot more often though as it fills the role as both `let` and `fn`.)
I think there was a postponed RFC basically along these lines...
I see. That makes sense, thanks.
[Sure you can](http://is.gd/JRjc2y). Abstract return types let you specify that a _concrete_ (not generic) return type implements a trait. This is for cases where the actual type of the thing is unwieldy or anonymous; i.e. when you're returning a complicated iterator chain or a closure.
&gt;making error handling more localized. That's not even true. If you want localized error handling then you don't use `try!`, you just match on the `Result`. Or you use `unwrap_or`. `try..catch` is intended to get multiple errors to reuse a single error handler. In my experience, you either want to handle each error individually (not using try catch to merge error handlers) or you just want to report an error and give up (log and bail out of the function). So `match` and `try!` handle these. I think it would be best to keep things as they are for now and see if something can be done to design a consistent system for handling the error handling patterns we actually use: log-and-bail (`try!` does this); retry; transactions-with-rollback (Go manages this with `defer`. Rust can kind of do this by overriding `drop` but I haven't see it used so much in application code); and whatever else. I dunno, maybe people want a spark-alike in Rust and so a lot of the iterator patterns like `map` and `partition` could be network operations which could fail and at that point you want the ability to use chained `?` and really just give up. I could see that being useful. But it wouldn't use error handling. It would just be for error reporting.
Great to have these back in the works again. :)
Any plans for a machine-learning-themed meetup? I just gave Leaf (https://github.com/autumnai/leaf) flairs to /u/mjhirn and /u/Hobofan94 today, maybe they would be willing to be speakers. :)
Because for me it is very hard to see and very surprising to see that one, quite hard to see, character change whole state of my program. Like someone said in that discussion: `try!` encourage line by line style. I am pretty sure that `?` will be hard to notice, even with syntax colouring. IMHO almost all one character sigils are made to be "invisible" to our brain.
Well there is our [upcoming talk about Leaf and Collenchyma](http://www.meetup.com/Rust-Berlin/events/227321071/). I'm not sure if that is going to be streamed or recorded for people not able to attend (taking place in Berlin).
&gt; One meta-note is that I like how the references are side-by-side with the main text. You can see how I added this feature to my blog at a separate post: http://blog.pnkfx.org/blog/2015/11/27/cherrypicking-tuftes-of-hair/ Update (*): &gt; Would it simplify requirements to say that Rust only allows GC implementations that support pinning? I think that would be characterized as *over*-simplifying the requirements, because one of our goals is to integrate with the Spidermonkey GC, which does not support pinning and has no plans to add such support in the future. (*): ugh reddit doesn't support multiple replies from one person, so I am forced to either edit or reply to myself
I have a feeling you may have run into an [XY problem](http://meta.stackexchange.com/a/66378) here, i.e. you're asking how to do something by returning multiple types when in reality there's probably a much better solution. Maybe it would help if you explained what problem you are actually trying to solve?
&gt; Don't do it. Conservative GCs suck. Yes, Conservative GC's are not good, but that wasn't actually what I am proposing there. Hopefully the promised followup post will come, but the main idea was to employ conservative GC technology solely for figuring out how to scan objects on the *Rust Heap* to find the roots embedded within them. This is very different from Conservative GC like BDW, and it is also quite different even from Bartlett style mostly-copying GC. It would solely be about finding roots that are embedded in objects on the Rust Heap that are reachable from the stack. (We don't want to just scan *all* of the Rust Heap because some blocks allocated there may be owned by dead objects on the GC Heap. I discussed this matter in the previous post, see http://blog.pnkfx.org/blog/2015/11/10/gc-and-rust-part-1-specing-the-problem/#precision
Example, java. It has 4, that I can think of, representations of dates and time. The first of which has been deprecated almost since v1.0. I really like that rust isn't pulling the universe into the standard libraries. In fact, I love the fact that the standard library is not part of the language definition. Let's never create the crazy breast that is the java standard library.
Gyah! Oops. You're quite right... 
It's not wrong, it's just not as convenient as I'd like ;-)
I've started a [ticket](https://github.com/rust-community/talks/issues/7) to track organizing one yesterday :) I reached out to th Autumn folks a bit ago, but it's tricky to get the timing right for a remote presentation since they are in Berlin. 7pm PST == 4am there. But hey, if /u/mjhirn or /u/Hobofan94 are up for presenting at 4 or 5 in the morning, I'd love to have them speak too :)
Yeah it doesn't, which is why I am also asking for the state (is it still open for discussion or will it never happen?) :-) I also asked because maybe I didn't find something or there was an obvious work around I had not found.
I was just so distraught over you and your rainbow coat going back east. It took a while to get things back in order :)
If I use this approach it says: error: The attribute `dialogs` is currently unknown to the compiler and may have meaning added to it in the future (see issue #29642) src/lib.rs:31 #[dialogs(inline)]
`doc`, not `dialogs`.
This is lovely. I may have to steal this hand drawn style of slides.
That's irrelevant. The *attribute* is called `doc`.
Oops. My question is in that case how to remove the extra namespace when I have multiple modules inside a crate. How do I use multiple files in `src` without having the module namespaces? I am toying with [iup](http://webserver2.tecgraf.puc-rio.br/iup/) and iup uses a standard C namespace IupNameOfFunction. When I organize my files in Rust I get iup::module::function instead of iup::function (a single namespace).
Someone else [covered this in this thread](https://www.reddit.com/r/rust/comments/3gufil/getting_your_crate_in_shape/cu1jdf1). Hide the module, re-export the contents.
Thanks!
Just came here to say I really enjoyed the slides!
Yup, exactly. Thanks.
Couple of notes: 1. "Unfortunately, the tutorial has not yet been written." 2. "The only example currently available is hello world." 3. Gitlab instead of github. 4. Not much information about what it is and why anyone should care. Why it is (or could be) better then the system I am using now? If you are leader of this project, your No 1 job should be getting interest and contribution from other people. So far this project doesn't do well in in this front.
It's being worked on.
Jep. ~10 years after Joda was developed.
&gt; The only problem I see is that people new to rust might have trouble finding them. People even miss things in stdlib and use libraries where they don't need to. It's a common problem.
All Rust installers ship with docs. multirust even gives you a nice command for opening up the docs corresponding to your current toolchain.
It's possible to log into gitlab with a github account. I also think github is inferior to gitlab. I'm not really willing to compromise on this yet, and I don't really expect it to impact the rate of contributions - sending a pull request on gitlab is just as easy as sending one on github. The tool here is git - the repo host really shouldn't matter. If it comes up more often, I might re-evaluate. (I also just plain prefer gitlab to github, and my prefered way would be patches sent to the mailing list, but nobody else develops software that way. If I had the resources/time I'd instead use a self-hosted Phabricator instance, but that's a lot of work I'd rather spend writing code)
I talked with /u/Hobofan94 about it and we concluded, that if people in SF want to hear about it, we are down to give that talk. We are considering wearing a pyjama, though ;) Shortishly after the one in Berlin would be best.
I believe this is a repost of https://www.reddit.com/r/rust/comments/3zgx9e/lambdaconf_2015_in_rust_we_trust_alex_burkhart/
There's higher levels bindings to iup if you're not trying to create your own. [kiss-ui](https://github.com/KISS-UI/kiss-ui) and [iup-rust](https://github.com/dcampbell24/iup-rust).
&gt; My point is that Rust doesn't have optional arguments or function overloading. Using an argument struct with `Default` trait implemented is close enough, isn't it?
/r/playrust This subreddit is for the Rust programming language.
Thank you for supporting free tools! A GitHub mirror can be nice for visibility, but I appreciate holding out and using free tools for the primary development.
He said "seL4" and "Rust". I'm interested already.
It currently warns, since the flag will be added.
The reason I care about Rust is because it provides essentially zero-cost "security"/sanity (as in "not unsafe"). So in that context, what's the performance of Robigalia/seL4 compared to GNU/Linux?
Or at least, no more than two crazy breasts.
Unknown, but I suspect it to initially be much worse, and over time move to only somewhat worse. seL4's performance itself is excellent, but my code implementing POSIX compared to other highly-tuned OS is unlikely to be competitive on performance alone.
now i want a programmer crown
Neither, just me not having lots of experience with this :) AFAIK, you can run Linux in user-space on seL4, and the performance impact is negligible.
I think I've found a solution: http://is.gd/6em6yy I'll roll with this for now :) 
Do you know, at compile time, what type you want `push_to_column` to return? If so, you can do this with a trait with an associated type: pub trait PushToColumn { type Output; fn push_to_column(self, val: &amp;str) -&gt; Self::Output; } impl PushToColumn for SomeType { type Output = SomeOtherType; fn push_to_column(self, val: &amp;str) -&gt; Self::Output { unimplemented!() } } If not, then you can return a trait object as you're trying to do. However, the issue is the size of a trait object is unknowable at compile time, so you can do it with a trait and by boxing the result: pub trait PushToColumn { fn push_to_column(self, val: &amp;str) -&gt; Box&lt;ColumnData&gt;; } impl&lt;T: ColumnData&gt; PushToColumn for T { fn push_to_column(self, val: &amp;str) -&gt; Box&lt;ColumnData&gt; { unimplemented!() } } Edit: I hastily did this second example, and it was borked. It should be good now.
Rust passes arguments by value, so the `mutate` function will just consume the Box, and the caller will not be able to see any side-effects. You'll need to either return `tester` back to the caller, or have your `mutate` function take a &amp;mut Box instead, so that it just mutably borrows the box instead of consuming it.
I'll admit that I'm completely confused. I neither understand the relationship between ColumnData, DataTable and Columnbase, nor do I understand the semantics of push. What confuses me even more is that you seem to use the `Err` Result variant for signalling the caller on how to proceed. You should probably use a self-introduced type for that and not Result. In any case: you cannot take traits like you want to, because ColumnData has no fixed Size (it is a trait). You'd have to at least go for fn push_to_column&lt;T: ColumnData + 'static&gt;(mut cd: T) Finally, you cannot return Traits currently, but you can return trait objects: fn push.... (mut cd: T) -&gt; Box&lt;ColumnData&gt; My (very hacky) implementation from your source is: fn push_to_column&lt;T: ColumnData + 'static&gt;(mut cd: T, val: &amp;str) -&gt; Box&lt;ColumnData&gt;{ match cd.push(val) { Ok(_) =&gt; { Box::new(cd) }, Err(y) =&gt; { panic!("I'm unsure about the behaviour here") }, } } As said in the beginning, I'm confused about the intended flow here, so that's a lot of guesswork.
I love it. My hand writing also looks like a 6 year olds' (...not that it's a bad thing in a typing world...) and now I really want to make slides like this! And you may have just inspired me to write a toy kernel in Rust as well. Never really thought about writing one myself but why not. 
It's a small world - she also authored https://computers-are-fast.github.io/, which [made it to the top of HN](https://news.ycombinator.com/item?id=7736845) [several times](https://news.ycombinator.com/item?id=10445927). It was a very interesting read.
Can't you just add two separate methods, maybe call one of them `or()` and the other `or_else("foobar")` or something?
Those who like the style of this talk might also enjoy Edward Yang's [writings](http://blog.ezyang.com/2011/04/the-haskell-heap/) and [presentations](http://ezyang.com/slides/ezyang15-cnf-slides.pdf). Also, at the risk of being That Guy, a technical mistake in the talk: it is possible to build an OS in Java. Examples include JNode and JX.
This is what introduced me to Rust back in 2014.
Why are we translating all these `%dropflag_hint` values then? And it looks like it's happening for primitive types as well. That doesn't look efficient (both in terms of trans time and unoptimized code performance).
/u/pnkfelix is doing work in this area, but it's basically on-hold until after MIR.
You are right! My method would not work. It looks like /u/fgilcher s code may do the job but I'll need to test it properly later.
Sadly I don't know the type at compile time. I'll be reading in data from file and want to infer the type at runtime.
My god, [that's a gorgeous thesis](http://www.mpi-sws.org/~turon/turon-thesis.pdf).
I did initially have a set up like this but things got quite bloated. However I saw a nice trick in the num crate where macros are used for bulk implementations - that may save me here if I can do a similar thing within my match blocks. Could you explain a little more about how boxing will save me in terms of size? Also how would a serialization-like design work? I would love to have an open list of cell types but for now was just aiming at supporting primitives and strings (for everything else). Edit - Additionally. This design would allow different types in the columns themselves. I want consistent types within the columns but different types within the table. So I believe I'd want: enum ColumnData { Int32(Vec&lt;i32&gt;), ... }
&gt; Using Gc&lt;T&gt; might have less overhead than Rc&lt;T&gt;: every time you clone an Rc&lt;T&gt; it incurs reference-count overhead, while Gc&lt;T&gt; just copies the reference. If `Rc` is faster than your GC then there is something seriously wrong with your GC. A [tracing GC should be several times faster than scope-based reference counting](http://flyingfrogblog.blogspot.co.uk/2011/01/boosts-sharedptr-up-to-10-slower-than.html). &gt; Strongly zero-cost: A unit of code generation that does not use GC should not pay for it. &gt; For example, in the above example of the string parsing module, ideally the code generated for parsing &amp;str values should have the same performance characteristics, regardless of whether it is linked into a program that uses GC or not. This seems to be based upon the assumption that GCs slow code down when, in practice, GCs often speed code up. For example, should the string parsing code be able to benefit from superior performance when using GC? To give an example, Google published a [Loop Recognition benchmark in C++, Java, Go and Scala](http://research.google.com/pubs/pub37122.html). When the Scala was directly ported (line for line) to F# [it ran faster than all of the other languages including C++](https://fwaris.wordpress.com/2011/07/11/googles-multi-language-benchmark-in-f/). The reason is simply that .NET's JIT allows a more efficient implementation of generics that, in turn, makes generic dictionaries and hash sets fast. 
Hmm. When I connect, it works fine, with the following parameters: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, 128 bit keys, TLS 1.2. The certificate is a Let's Encrypt certificate. The chain of trust is DST Root CA X3, Let's Encrypt Authority X1, robigalia.org. What browser/OS are you using? Do you have that root cert in your roots? Do you have it explicitly set to not be trusted?
There has been some related discussion about this recently on the [internals forum](https://internals.rust-lang.org/t/thoughts-on-rust-stdlib-and-c-interfacing/3036/35). The topic of the thread is about Rust using libc but the last couple of posts talk about the binary size "problem". 
- [RFC 1214](https://github.com/rust-lang/rfcs/blob/master/text/1214-projections-lifetimes-and-wf.md) Projections, Lifetimes, and Well-Formedness - [RFC 136](https://github.com/rust-lang/rfcs/blob/master/text/0136-no-privates-in-public.md) No Privates in Public
If you want two orthogonal behaviours like this, write two methods. Eg. a.or("foo") a.else() That said, this looks really suspicios. I'd much rather go with a declarative syntax and variadics: or!(a, b, c).maybe() or!(a, b, c) to mean `a.or(b).or(c).else()` and `a.or(b).or(c)`. Much less cognitive overhead IMO.
I mean, cant you just use macro's? If you can do something like `vec![1,2,3]` do something like `vec = Vector::new();` `vec.push(1);` `vec.push(2);` `vec.push(3);` i don't see why you could not just make your `verex.find(a)` `.or()` `.find(b)` into something like `find!(verex, a | b | .... )`. Thought i might be wrong, i dint allot of macro's yet, i saw libraries use the concept allot, like Nom.
Of course it's much more interesting to see what the size of, say, servo's binary is, but it's still good to know what the overhead is with the simplest of all Rust programs.
Is this the new default in 1.6?
Hmm. The idea of doing any form of conservative scanning of the Rust heap concerns me. Normally conservative scanning relies upon the fact that the destination being scanned for is the GC heap - therefore the existence of a pointer into the heap preserves liveness of the target; or in other words its not possible to have a stale pointer. This is not true of the Rust heap or any manually managed heap in general. How can you be sure that the pointer you're scanning isn't stale and doesn't point to garbage? I don't think this can be done safely 
According to the [tracking issue](https://github.com/rust-lang/rust/issues/27389) it is not stabilized yet. So I assume it is only on nightly?
How many crates are broken by 136? The announcement doesn't actually say.
I might be confused, but I suspect you're conflating two independent tasks: parsing a string with the desired set of emails to fetch, and representing that same set. If the sets are fixed, like in your examples, you can indeed use macros to generate the appropriate representation of the set. On the other hand, if the input is dynamic, you will certainly need to parse it and maybe generate the set representation right away. What I mean to say is, for static examples such as yours, the macro approach will work (and will be useful for testing, I guess), and you'd call it just like you described (`sequence_set!` would generate a set representation). However, in a real-world scenario you will have to parse a string and macros are not the right tool for that job. A parser such as [chomp](https://github.com/m4rw3r/chomp) or [serde](https://github.com/serde-rs/serde) will be useful there (I personally like chomp, but I haven't used either extensively). In that case, you'd have to do something like if let Ok(email_set) = parse_fetch_set(…) { inbox.fetch(email_set); } I hope this sheds some light. P.S.: Just for fun I sketched a very simple representation of the set. // Represents a set of emails #[derive(Debug)] enum EmailSet&lt;'a&gt; { Single(u32), Range(u32, u32), Since(u32), Multiple(&amp;'a [EmailSet&lt;'a&gt;]) } fn fetch(how_many: EmailSet) { println!("how_many: {:?}", how_many); } fn main() { fetch(EmailSet::Single(2)); fetch(EmailSet::Multiple(&amp;[EmailSet::Single(2), EmailSet::Single(4)])); fetch(EmailSet::Range(2, 4)); fetch(EmailSet::Since(2)); } The hypothetic macro `sequence_set` would generate instances of `EmailSet`, which you would then pass to `inbox.fetch`.
Gankro's RFCs often have great titles (if you look at the file name). For example [RFC 1213 I Am Become Death](https://github.com/rust-lang/rfcs/pull/1213) (about API stabilization/deprecation), or [RFC 1232 Rust Is Strong](https://github.com/rust-lang/rfcs/pull/1232) (about removing weak pointers). And the best, which actually got merged, [RFC 832 from_elem With Love](https://github.com/rust-lang/rfcs/blob/master/text/0832-from-elem-with-love.md).
&gt; However, in a real-world scenario you will have to parse a string and macros are not the right tool for that job. A parser such as [chomp] or [serde] will be useful there (I personally like chomp, but I haven't used either extensively). In that case, you'd have to do something like I suppose I could have fetch take a sequenceset type, which the macro will provide (given static values), or they can be parsed at runtime. I see that's how your example works as well. Awesome, and also thanks for this insanely helpful response/ solution. So maybe I'll have fetch take &lt;T:Into&lt;EmailSet&gt;&gt; ? That way they can pass in strings or other types (u32) and I can parse those via the Into trait at runtime. Would that make sense?
Great talk. Having just suffered using C and pthreads for a project, I'm now convinced I need to learn Rust.
&gt; So maybe I'll have fetch take &lt;T:Into&lt;EmailSet&gt;&gt; ? That way they can pass in strings or other types (u32) and I can parse those via the Into trait at runtime. Would that make sense? Hm. I think that does make sense and it's a neat idea, but I'm not sure it would lead to a clean API. The way I see it, `fetch` should return a `Result`, because the server may throw back an error. If `fetch` receives `&lt;T:Into&lt;EmailSet&gt;&gt;`, then it also has to deal with possible errors in the argument (e.g. a `ParseError`), which would broaden the semantics of the `Result` returned by `fetch`. Another consequence is that any other function taking `Into&lt;EmailSet&gt;` would have to make the same sanity checks and broaden their return types, which seems to me like a bad idea. Honestly, this point is debatable and a matter of taste. :)
fetch returns a result&lt;Emails, IMAPError&gt; where IMAPError::ParseError(ParseError) to account for responses being improperly parsed. Pretty much every function has to because they all talk to the network and parse things.
I am. :) I first encountered Lindsey's work on [LVars](http://www.cs.indiana.edu/~lkuper/papers/lvars-fhpc13.pdf), and then later heard about her work with Mozilla on Rust. Her focus on deterministic parallelism is extremely interesting to me, given that I'm in the distributed systems big-data space (Cassandra), and LVars and [Bloom](http://bloom-lang.net/) seemed to be the two most interesting ways of tackling these problems. I do think there are some very promising avenues to pursue in using substructural (as in Rust's lifetimes) types to tackle safety (being defined as [strong eventual consistency and monotonicity](https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type) at distributed systems scale), as well, which is part of the reason I'm drawn to Rust.
Good tip, I'll have a look whether it makes sense.
I guess for this specific case I'll go with your suggestion, thanks. Of course it is just meant to illustrate a general problem which seems to have very different solutions depending on the case (which IMHO feel like workarounds for the fact that there are no optional parameters)
I'll have a look at the Default trait, thanks.
I enjoyed this. BTW shadow makes the background kind of have gradation in color. It feels natural.
I know one of the professors working on bloom, fwiw. I have yet to look at LVars, I only read one blog post on it, but it's interesting for sure! I'm also doing a research project on Rust in the distributed/concurrent systems space (current project idea: software transactional memory, thanks to Niko, but can be changed), so if you have anything you'd like to see happen, let me know! 
`Default` trait and the [update syntax](https://doc.rust-lang.org/book/structs.html#update-syntax) basically allows you to emulate optional\keyword arguments pretty closely.
Quick question: how do the version numbers coincide with breakage?
Cool. We didn't have enough breakage recently.
Those are hacky workarounds that really make using API's more complicated (and its more likely such macros will lead to issues in some cases). I really want rust to have default parameters.
`snprintf` is a very big machine to put a safe wrapper around, it might not be so easy, not easier than writing a new formatting function (or finding one on crates.io).
Nope, but some people are used to this way of working (not me). More options \o/.
All of Servo is built using Cargo. `./mach build` will download an appropriate version of rustc and cargo, then run `cargo build`. That’s it.
&gt; Cargo is advertized as a package manager rather than a software build tool Sincere question: what’s the difference?
Well, neither of them are well-defined technical terms... My impression is a "package manager" focuses on pulling an appropriate package from an external repository with suitable dependencies and it is relatively straightforward to actually build the package, perhaps because of the uniformity of the pulled source tree. On the other hand, a "build tool" is more versatile and is not necessarily tied to a particular programming language.
I got an impression that there are a bit more than "that's it" in Servo's mach scripts: - https://github.com/servo/servo/blob/master/python/servo/build_commands.py
It's just declaring a variable. let x: u32 = 42; let mut y: u32 = 27; ...in Rust is just like: const uint32 x = 42; uint32 y = 27; ... in C++. The biggest difference is that in Rust, the left-hand-side of the `=` can be a pattern, not just a single name. If you have a struct like: struct Point(f32, f32); ...and a function returning a value of that type: fn pick_a_point() -&gt; Point; ...then you can call that function and store the entire Point into a variable and use it: let new_point = pick_a_point(); println!("Picked the point at ({}, {})", new_point.0, new_point.1); ...or you can give a pattern that pulls apart the structure and stores its individual bits in separate variables: let Point(new_x, new_y) = pick_a_point(); println!("Picked the point at ({}, {})", new_x, new_y); 
sure, that will be my workaround, I guess. Issue still stands (this was just a toy example)
OCaml (in which the original compiler was written) also has let bindings. http://www2.lib.uchicago.edu/keith/ocaml-class/definitions.html 
As /u/dbaupp said, there's a little more to it than this, as `let` allows for binding using *patterns* (it just so happens that `let x = foo` binds to `foo`'s entire pattern). i.e. if `foo`'s type is: #[derive(Copy, Clone)] struct Foo { bar: Bar, baz: f32, } #[derive(Copy, Clone)] struct Bar(i32); then we could use `let` to do all of the following: let foo = Foo { bar: Bar(42), baz: 1.0 }; let Foo { bar: Bar(forty_two), baz } = foo; let Foo { baz, .. } = foo; let Foo { ref bar, .. } = foo; let Foo { bar: Bar(ref forty_two), baz: some_float } = foo; etc
All I could learn from the slides was no friend of the author is visually handicapped and in need for text-to-speech software.
I don't disagree.
It's purpose isn't *just* to help the compiler, it helps humans work out where variables are introduced too. (There would be other ways to do this, e.g. the `:=` of Go, but having it as a prefix like `let` is somewhat helpful.) C/C++ doesn't have patterns in the same way as Rust... and, yes, `let` allows the Rust compiler to be a little more flexible/much less complicated: C/C++ cannot be parsed properly without also doing name resolution at the same time, as there's no way to tell whether the sequence of tokens `a * b;` is multiplying variables `a` and `b`, or is declaring the variable `b` with type `a*` without knowing if `a` has been previously declared as a type.
The API seems to only reply with one page of results. For example, the response to this request: `https://crates.io/api/v1/crates/hyper/reverse_dependencies` only includes `algorithmia..cheat`. Is there a way to retrieve the full list?
&gt; Into is only for conversions that don't fail Ah, true. I think FromStr + FromMailboxName (what the list command returns) will cover it. Thanks a lot for the help.
So... *don't* iterate through anything. fn main() { use std::ops::Range; let Range { start, end } = 2..5; println!("{}:{}", start, end); } Prints `2:5`.
&gt; Also, note that there's no particular reason that a keyword in Haskell has the same functionality as a keyword in Rust Well, in this case it's a common mathematical phrase. (Which is also how haskell got `where`.) Sharing keywords isn't all that uncommon really; `if|for|while` tend to mean the same thing. Programming languages tend to loan words from math and each other; they're not … evolved natural languages.
FF/Win7, probably has something to do with the corporate firewall. Also, it works now!
sure sure, destructuring is an (useful) thing, `if let` as well, but i think OP is only interested in the basics for now :D
Definitely not. The subset I mentioned was those that already use Docker regularly.
Ah cool. I'll make use of this.
My personal intuition is usually that the two are operating at different abstraction levels: The build tool is mostly concerned with implementing the transformation from source files to a binary artifact such as a standalone single executable or a library whereas a package manager is one level higher and is responsible to retrieve required packages (= libraries generated by the build tool above, possibly packaged and published). Different languages and ecosystems have different needs with regards to these two steps. E.g. Make is only a build tool - if your executable depends on additional libraries (e.g. DLLs) than you'd need to manage them yourself or via another tool. Maven OTOH is mostly a package manager - it is used to fetch the required artifacts (libraries) for your project from a repository. Rust's model ATM is such that the compilation unit, the crate, corresponds to a single package so the build step is usually trivial (just invoke the compiler once). Hence Cargo mostly manages dependencies between artifacts (crates/libraries). Even with incremental compilation being worked on for Rust, the build complexity is managed by rustc itself unlike say C++ compilers where the logic is managed by an external tool that repeatedly invokes the compiler to produce the intermediate artifacts and the final artifact. 
`winapi` is not a sys crate and doesn't have a `links` attribute, so you can have both 0.1 and 0.2 of it just fine. And for all the actual sys crates I've removed the `links` attribute with 0.2 so they shouldn't conflict with their 0.1 versions anyway.
Others have said explained the meaning in rust, but please let me provide some historical context. In mathematical proofs and lectures there is a tradition of naming a value, expression, function, or what have you by saying "let x be the value 5". This is done as a polite way to introduce a name for something important to the proofs. We say that x is a binding for 5. This tradition was carried over into programming languages by language designers who had very strong mathematical backgrounds such as Robin Milner, one of the creators of ML. Rust draws on inspiration from several languages and ML is one of them. When I see let used in this way I like to imagine the author is saying, "please indulge me for a moment as we create the following local binding for this expression, hopefully you'll see why in a moment". In Haskell we have laziness and polymorphic recursion so let is even closer to the mathematical usage. Practically, it is often used to introduce a bit of pattern matching, which is not a common way to use it in math. I hope that helps.
I never realized that, awesome. I wonder what a 'minimal' Rust would look like, which all currently existing Rust code could be mechanically translated to. All constructs in miniRust should have the same semantics as in normal Rust. Obviously we can choose between (`loop` + `if`) and `while`, and `for` can be rewritten in either of the former. Syntactic sugar for `Range` can easily be dropped. `Deref` and `DerefMut` can be explicit rather than implicit. Can array Indexing always be replaced by explicit use of the `Index` or `IndexMut` trait?
But how to `cargo install` a new version of rustc then? Currently you have to uninstall before reinstalling with cargo. But then you have no compiler to build rust for installing.
https://github.com/rust-lang/rust/tree/master/src/grammar
But if I don't push a new version, it'll let people complain to Rust about the breaking changes they're making so that in the future they'll be much more cautious about this kind of thing. So it depends upon whether I care more about the users of my old versions or pushing an agenda.
Didn't see it was the answer to the const comment. When I read "Not really, you have a C++ vision of the problem", it doesn't help a lot. &gt; That's a pretty rude thing to say, particularly to someone who is right. Rude? How so ? And how can I tell if somebody is right? That's quite a patronizing thing to say. The actual correct answer was also given elsewhere, is that let actually allow the compiler parsing stage to be a little faster and less redundant, since the C++ syntax can be ambiguous. That was the entire explanation. Telling me I'm misunderstanding the problem because I think that way or another is a non-explanation. I did not mean to be rude, and I don't see why you're telling me I don't seem to care. Out of every subreddit I would not have thought to offend people here.
How much more cautious could they have been here?
Thanks!
If this was my case what I would miss most is the #rust IRC. I'd suggest taking note of the harder stuff and talk it in the IRC if you get the chance (having a client helps (instead of using a web client), the traffic is really light, only latency and packet loss may be an issue, I use [irssi](https://irssi.org) though there are some friendlier clients).
As I understand it, robust privacy checking is required if you want to ensure safety at a module boundary while having unsafe code inside. However, it could be a lint instead of an error for that purpose. It'd be nice if there were a way to search crates.io to see how much still relies on `winapi = "0.1"` (my guess is most crates from the winapi-0.1 era have `winapi = "*"`...).
Nice tool! I don't know if you've come across it, but you might be interested in [libpnet](https://github.com/libpnet/libpnet). It's a pure Rust alternative to libpcap, and has lots of utilities for automatically generating bitshifts for packets. In needs a beta or nightly version of Rust right now, it will work on stable when Rust 1.6 is released though!
Again your elegance comes from wanting something like english. That is fine if you want a really high level language, at a low level, such as Rust, I want something mathematical, something I can read and map to assembly, even if loosely. This throws out the idea of full overloading, because this: let x = f(some_value); Can do very different things than this: use some::module::adding::overloads; let x = f(some_value); Even if we add a lot of constraints it means that we have to search around for multiple definitions of the same thing. I may think that `f` does one thing, but it just so happens that the override I'm using does something else entirely. Lets instead make a simpler case. What about optional arguments with default values? That leads to only one definition. * When is the default value instantiated? Is it static or is it done at call? * If it's static how to we handle mutations? Or multiple calls in separate threads? * If it's at call how do we inform the user that he's paying for the cost of creating a new instance. There's no guarantee that this isn't there and hiding it clearly is wrong (especially as it may have unexpected side-effects). * What lifetime do default values have? The most I'd be willing to tolerate is allowing arguments with the Default trait to be called with `_` which becomes `T::default()` or maybe even `..`. So in this case your code would look something like: verex.find("firststring").or(_).find("secondstring") Still I would be strongly against this until a good use-case argument is made to justify the cost. As it allows for things as the above, which I find a terrible api. Simply put: what if I want to find an empty string? For whatever reason, there's no way to do it with `or` so now I have to search and find out that my idea of calling it `find("x").or("")` will not do what I expect it to from other calls, which is more like `find("x").or(_).find("")`. If I had this library I would make it a style-guideline to always use `or(_)` and never use it to search anything.
&gt; We have to safeguard rust's reputation, and the post may spread far and wide. What's amusing is that Rust will probably get flamed because it's breaking backward compatibility, but C++ compilers breaking backward compatibility when introducing new optimizations is considered normal (it was the users' fault for relying unknowingly on undefined behavior), without any announce whatsoever. Sometimes I wonder if backward compatibility in Rust was not over-emphasized ;)
No, we did not. More specifically, it's better to fix soundness holes and well-formedness bugs as soon as possible, while the ecosystem is still elastic enough. Waiting only leads to: - increasing the amount of code to change - increasing the amount of unsound code there's no benefits.
`multirust` is coming!
Let’s look at the `build()` method specifically: * There’s a bunch of code to decide whether or not to pass `--release` to Cargo. We want to make this a conscious choice since release builds are very slow to make and non-release builds are very slow to run. * Then `ensure_bootstrapped` downloads rustc and Cargo if that’s not done already * Then some code to decide what command-line arguments (`opts`) to pass to Cargo. Mostly about cross-compiling and some optional features. * Then there’s a explicit build of OpenSSL, but when targeting Android (when running `./mach build --android`). In other targets OpenSSL is a crate with a build script. I don’t remember why Android is different exactly, but I think it has to do with linking. * Finally we call `cargo build` * … and show a desktop notification, as fgilcher mentioned. So yes, there’s a bunch of Python code that grew over the years, but I maintain that in a default (non-Android) configuration, Cargo does all the heavy lifting of building Servo and mach is a fairly thin wrapper around it.
All of the index’s data is in https://github.com/rust-lang/crates.io-index, if you want to parse it yourself and avoid dealing with the API. I don’t know if it’s documented, though, sorry. But people can probably help/explain if you ask on #cargo on Mozilla IRC.
It's because you wrote it as a template. If you use the template syntax, you are saying that C is a **sized type** that implements the Component trait. By creating a ComponentStore&lt;Component&gt;, you are saying that Component implements Component, which is true, but Component is not sized. To lift the requirement of it being sized, you can just declare C like this: ```C: Component + ?Sized``` But you don't even need the template at all anyway. You can just use Box&lt;Component&gt; as you can see here: http://is.gd/6c5Txz This brings up an interesting question though and maybe others know an answer to this: Is there a way to use the template syntax and only box the value if the type needs boxing? I feel like that should be an associative type of the Sized trait (C::Sized or C::Boxed I guess).
The site seems to be down atm but I think I know what you mean: pub struct ComponentManager { stores: HashMap&lt;ComponentId, HashMap&lt;Entity, HashSet&lt;Box&lt;Component&gt;&gt;&gt;&gt;, } The problem is that the type is very opaque this way, because the ComponentStore doesn't explicitly state what Component it contains when you get it. Thank you very much anyway. this is exactly what I was looking for :D
It does not make any sense to make ComponentStore generic if you are planning to box its content. This will compile pub struct ComponentStore { store: HashMap&lt;Entity, HashSet&lt;Box&lt;Component&gt;&gt;&gt;, } Few notes about ECS. TypeId is not guaranteed to be stable, meaning it might change from build to build, making it impossible to use it for serialization. HashMap/HashSet are probably far from ideal data structured for ECS component storage. For better performance it's better to use Vec or something custom built. Generally you would like your entities to be just indexes into vectors of components struct ComponentList { position: Vec&lt;Position&gt;, health: Vec&lt;Health&gt; } You might wanna take a look at https://github.com/HeroesGrave/ecs-rs It might not be exactly what you want but it'll give you some ideas. 
Maybe a good way is: there'll be two upcoming soundness fixes that will make some code with undefined behavior that fail to compile. That shows really what is happening, the language isn't doing a breaking change, but merely covering up some undefined holes that shouldn't have been allowed in the first place. People who depend on undefined behavior should be aware that this things could change.
You are probably looking for /r/playrust as this sub is for the programming language rust.
Well, that's the cool thing about ```C: Component + ?Sized```. That means you can have a generic ComponentStore&lt;Component&gt; but also optimized, statically dispatched versions of the ComponentStore with ComponentStore&lt;ConcreteComponent&gt;. Unfortunately it's not really a "zero cost abstraction" because the version with concrete components will box the values for absolutely no reason. Update: Actually you could easily get rid of the Box by moving the Box outside the ComponentStore into the ComponentManager like this: pub struct ComponentStore&lt;C: Component&gt; { store: HashMap&lt;Entity, HashSet&lt;Component&gt;&gt;, } pub struct ComponentManager { stores: HashMap&lt;ComponentId, ComponentStore&lt;Box&lt;Component&gt;&gt;&gt;, } 
[removed]
I would like to make ComponentStore generic, because I can do this: impl ComponentManager { pub fn get_component_store&lt;C: Component&gt;(&amp;self) -&gt; Option&lt;&amp;ComponentStore&lt;C&gt;&gt; { //TODO } } I know TypeId is unstable but I found it to be a necessity for the implementation I desire. I also considered the data driven design you propose but I have no idea how to implement it nicely as not every entity implements every component. The game I make is a text adventure so efficient structures are not the priority anyway. I will keep it on my mind, though ;) 
This will work with this then: https://www.reddit.com/r/rust/comments/402qrl/help_boxed_trait_still_needs_to_be_sized/cyr12ne
&gt;I know TypeId is unstable but I found it to be a necessity for the implementation I desire. It's not. You can generate some ID with macros and smth like lazy_static crate. &gt; but I have no idea how to implement it nicely ecs-rs does it. The only approach I found working in practice was to use either macros or compiler plugins and no generics. If you want smth TypeID based you can implement it using Any downcasting. struct ComponentList { comps: HashMap&lt;TypeId, Any&gt; } impl ComponentList { pub fn get_component&lt;C: Component&gt;(&amp;self) -&gt; Option&lt;&amp;C&gt; { self.comps.get(TypeId::of&lt;C&gt;).downcast_ref&lt;C&gt;() } }
As far as I remember ecs-rs used to be implemented with TypeId approach, it used macros to generate some component id based on it's name (or path, i don't remember exactly), that id was stable unlike TypeId. &gt; I want to generate the IDs dynamically. What exactly do you mean by dynamically? If you don't care about it being stable than TypeId is ok. If you want it to be stable you just component!( Position { x: f64 ... } ) that will generate smth like impl Component for Position { fn get_id() -&gt; ComponentId { // simplified ComponentId(Hash::hash("mycrate::Position")) } } You can find exact solution if you'll go through old ecs-rs commits. 
That's a good idea! I'll look into it :D
It was fun to watch!
Isn't this the use case for libsyntax? Or, if you need stable Rust, [syntex_syntax](https://github.com/serde-rs/syntex). Take a look at [how Racer does it](https://github.com/phildawes/racer/blob/c94a17f844c13d2f115a53013c0f9e063bc31f23/src/racer/ast.rs) (Racer is an out-of-tree tool that parses Rust code for the purpose of autocompletion and perhaps other IDE tasks) Now, if you want to roll your own library, perhaps you should study the [Rust reference](https://doc.rust-lang.org/reference.html) and/or take a look at the libsyntax code or something.
Perhaps edit the gist to add a new file for the `Cargo.toml`?
In the match-only version `evil` is computed twice, because it evals to false the first time it is called, entering the second match statement where it is called again (evaluating this time to true). This is the exact same thing that occurs in the if-guard version. The fall-through is just being reified into nested match statements.
There's already a crate which parses it actually.
For this example, yes. But for others, the semantics would change by moving the predicate call up to the first expression. You still need the guard for the arms.
Can you show an example in which you believe the semantics of guards cannot be replicated using nested match statements? I can't think of any, and my intuition is that they don't exist. My intuition is that all control flow expressible in Rust can be reduced to `loop` with `break` and `continue` and `match` with two arms and no guards.
I'm late to this thread, but sqlite support is on the table. Currently planning it as the main focus for 0.7 (0.5 is going to fix some gaps in our extensibility story and 0.6 will be bringing a better story for associations) That said, I second what everyone else has said about not using separate dbs for dev/prod. Diesel is not designed to abstract your back end away, nor should it. Your back end does lots of interesting things that you should take advantage of, but require coupling.
While that should be equivalent it actually is not as `Any` also implies `Sized`. I meant to submit a PR for that a while back but ran into some bugs at the time.
There is a difference between syntax and semantics though - there's nothing inherent about a keyword that imbues it with certain meaning (although it can hint at it). For example `class` means something completely different in Haskell than Java.
Oh, thank you. But wouldn't `Component : 'static + Reflect` also imply `Sized`? If you didn't want `Sized`, you would write `Component : 'static + Reflect + ?Sized` I think.
I'm just going to drop you a link to [this blog post](http://heroesgravedevelopment.tumblr.com/post/112919710664). And from the summary: &gt; - RefCells are not a good solution if you ever need to borrow the internal data. &gt; - Type erasure is not a good solution if you need to manipulate any of the internal data. &gt; - Abusing ownership/mutability escape hatches makes your code easier to write but harder to use. &gt; - Abusing macros makes your code harder to write but easier to use. Using 'Any' and TypeIDs sort-of allows you to work around type erasure but it raises its own problems.
The latter. You can do a variable declaration inside an if statement (that is only valid only in the true case of the if statement). See http://is.gd/qyvV85 for a working example.
Looks amazing! So what's the best way to get all this set up? Seems like a lot of moving parts: YCM, ycmd, racerd, racer, plus racer is going to need a path to my Rust source and there'll be interaction with multirust to find the cargo cache... Is Rust support going to be included in YCM out-of-the-box -- should I just follow its normal installation instructions, and then add racer?
All of that stuff is abstracted away from you as a consumer of YouCompleteMe. Just make sure to pass `--racer-completer` when you run `./install.py`.
Thanks for the kind words, Val! Most importantly, thank *you* for building YCM in the first place :).
Like Joe said, you don't have to care at all about any of the details. A simple `./install.py --racer-completer` will get you set up, though you'll want to put `let g:ycm_rust_src_path = '/path/to/rust/source/code/folder/src'` in your vimrc as well to get completions for the stdlib.
There is also the [recs](https://crates.io/crates/recs/) crate, which also uses typeId and downcasting.
This is so awesome! Really really great that you went to work and made this happen! Of course also mad props to all the people who made/contributed to YCM/racer in the first place. Again though, thanks a lot /u/i_am_jwilm for creating this!
My image editor project and its UI library that uses the internal image editing functions to draw the UI for the image editor itself. Over the last two days I fixed some matrix multiplication errors, found the freetype function I was looking for (which made font rendering less likely to break), and started on the more 'fancy' layout managers. Previously, the layout managers would only operate from the parent's dimensions and custom parameters. The child components couldn't specify a desired size for the sake of simplicity. Now, if you look at the following screenshot, you can see the menu bar at the top has each label fitted to the size of the contained text. http://i.imgur.com/iyvlNT4.png Of course, I haven't implemented buttons or menus yet so they are just labels for now. I was actually supposed to be working on my game (here's a [screenshot](http://i.imgur.com/FSJSZIP.png) of me abusing the "feature" that you can dive while attacking) but I needed to make some new sprites and a big reason for the image editor project was the horrible usability of linux image editors.
As I mentioned, that was a tiredness-induced mis-speaking and it'd be more accurate to say that I like to target both SQLite and PostgreSQL for production with SQLite getting exercised and tested more frequently during development due to it being simpler to set up and tear down. (ie. the SQLite-only testing and PostgreSQL-enabled testing wind up in a relationship similar to single-target local unit tests and multi-target continuous integration tests. One's less thorough but can be run frequently while developing; the other's run only when preparing to push bundles of commits.) As for backend coupling, I don't mind coupling... as long as there's an API that makes it relatively painless to develop generic and then specialize once I actually know where it's most necessary. (Also, if I can't start generic, then I'm likely to just give up on the web-accessible design, glue PyGTK to SQLite3 (since PyGTK is snappier and much lighter on resources than my Firefox), and get on with actually solving problems rather than spending however much time demotivating myself by fighting my way to a black triangle moment with a new backend.)
fyi this was brought up on the [multirust-rs issue tracker](https://github.com/Diggsey/multirust-rs/issues/37) and on the [multirust issue tracker](https://github.com/brson/multirust/issues/77) [twice](https://github.com/brson/multirust/issues/114). It would make life easier for a lot of people if the tooling could handle it.
There's also [an issue for it on rust-lang/rust](https://github.com/rust-lang/rust/issues/19535). Fixing this shouldn't be a lot of work and it would make a lot of users happy.
Chung-Kil Hur is amazing!
Is it this one? https://crates.io/crates/crates-index/
Works like a charm with neovim. Thank you *so* much!
You can even write cargo extension, I suppose.
Even then the concept has certain similarities—it's something related to data types and capabilities in both cases. There are cases too where languages use different words to mean more or less the same thing, like `switch|match|case`. And they're all english words that let you guess at the meaning, unlike dbaupp's example of "thieved in the past" vs "the chair". If a programming language lets its vocabulary be influenced from math, it'll sound kinda similar to other math-inspired languages, the way romance languages or germanic languages share words. (Excluding English, which has … issues.) Still, that doesn't mean that different programming languages can be [transpiled with `s///`](http://i.imgur.com/oNObxMf.gifv), like how OP here seemed to want to fit `let` into a sort of `const` straightjacket. Maybe they'd catch on if a C programmer came into /r/cpp and asked if `auto` is a weird way of writing `void` or asked java people why they don't use `struct`.
You have a use-after-free problem that's not showing up in your tests for one reason or another, likely because the allocator isn't returning the memory to the OS (so you won't segfault by accessing it) and allocations aren't frequent enough for the allocator to reuse that memory (so you won't get garbage data). Interestingly, you should have gotten some kind of error from the double-free that's inevitably occurring, but I digress. In your `to_string_list` function, `converted.as_mut_ptr()` doesn't consume the `Vec&lt;*mut c_char&gt;`, so its allocation will be freed at the end of the scope. You need to `mem::forget()` the vec so that it doesn't free its allocation after getting the data pointer. In that same function, since you take `v` by value, you should call `.into_iter()` on it so your `.map()` takes ownership of each `String`. Then you don't need to call `x.clone()` for each one, which will save you some copying. In `free_string_list`, you're doing basically the right thing, but I would iterate `v` by-value in the `for`-loop as it's more idiomatic: for ptr in v { let _ = CString::from_raw(ptr); } 
Awesome, thanks for response. I've got an app using rust postgress already and thought about migrating to diesel, bit I noticed that there's no mention of the threading / pooling situation. I saw there's a module for r2d2 (not in crates.io unfortunately), but it looks like only one connection is supported. Am I right?
Ok, you'll need to clone the `Option&lt;String&gt;` at that point, since `get_dir` wants such a value by value.
Thanks very much! I hope it can eventually be a stable crate for people to use when they need topology discovery/analysis or cpu binding.
You can find some more details and progress reports at https://github.com/rust-lang/rust/issues/9883
Awesome to see that this is available. I've been following Rust for a long time but haven't really dived into it yet. Having YCM support is just the icing on the cake that will make using it that much easier.
This is also discussed at length in the nomicon (and amusingly, it uses basically the same example as this blog post): &gt; `unsafe` does more than pollute a whole function: it pollutes a whole module. Generally, the only bullet-proof way to limit the scope of unsafe code is at the module boundary with privacy. http://doc.rust-lang.org/nightly/nomicon/working-with-unsafe.html
&gt; to evaluate the safety of types like Vec, we have to look at every single function provided by that data structure, even if it does not contain any unsafe code. "Every single function" isn't really true, those functions (including methods) could be provided in a separate module entirely, in which case they wouldn't have access to that type's private members. It's a good argument in favor of having any module containing `unsafe` have only those public items that actually demand unsafe code to implement.
This is before I talk about the boundaries of unsafe. The term "provided by that data structure" is not clearly defined, so I'd argue that this means exactly "functions defined in the same module of the data structure". I did not want to mention modules and abstraction boundaries just yet.
I think your problem is the following: When the function is over all the local variables are destroyed, so new_word gets destroyed. Because of that &amp;new_word references something that does not exist, and you get an error. ^Just ^started ^looking ^at ^rust ^so ^i ^might ^be ^wrong
You are right. He needs to return it as ```String``` instead of ```&amp;str```.
I believe there are several groups who might be interested in something like this, such as https://github.com/RustAudio and possibly also the audio contingent of people doing gamedev in Rust. I'm not sure if web browsers need a flac decoder, but if they do then Servo would likely be interested as well.
&gt; possibly also the audio contingent of people doing gamedev in Rust Not sure FLAC is of much interest to people doing gamedev, for a game it's a lot of storage space and bandwidth for no gain compared to lossy formats (mp3 or Vorbis), IME the only use game developers have for FLAC are OST, out of 400-odd games in my Steam library not one uses FLAC for game sounds (though several bundle FLAC-encoded OSTs alongside mp3-encoded ones). I assume *Opus* would be of significantly more interest as a low-delay high-quality and highly tuneable lossy audio format, it compares favourably to both mp3 and ogg/vorbis, which *are* heavily used for in-game sound. &gt; I'm not sure if web browsers need a flac decoder *Need* probably not, AFAIK no browser supports FLAC `&lt;source&gt;`, but supported audio formats are essentially unspecified, and the HTML5 spec does give ogg/flac as an example (amongst a dozen others)
I actually enjoyed this blog post of yours long ago.
Two things came to my mind while reading this: 1. There was once a discussion of adding another type of pointer/reference to Rust - an uninit one. This would add the necessary type info of which memory slots were already initialized and which weren't. This idea now evolves and relates to the Allocation/Placement protocols. A ```Vec&lt;T&gt;``` could than become safe(r) by delegating all the underlying raw memory pointers stuff to the allocator and instead of a block of raw uninitialized memory it could hold ```Place```s. 2. The semantic difference between ```MyType&lt;T&gt;``` and ```Vec&lt;T&gt;``` doesn't have to be conceptual only - it could be explicitly encoded in the type system using DbC (design by contract). I remember seeing a crate that implemented this with macros. 
The r2d2 module isn't on crates.io? That's a mistake, I'll put it up there shortly. Connection pooling is supported via r2d2.
Have you tried YCM Rust completion with Servo? Was it slow? If it was, please create a reproable test case and we'll take a look. I expect it to be plenty fast, but I could be wrong. &gt; You can also make goto work by opening the relevant file on a browser. That's both slow (the browser needs to open, the page needs to load etc) and breaks my flow. I want to press a shortcut and have the source open _instantly_ (&lt;100ms) in front of my eyes, right where I was looking, with the same syntax highlighting and typefaces as the rest of the code I was reading. Right here, right now. IMO nothing beats having it open in the same editor.
I *just* was getting it to work and it's pretty fast and awesome! Works cross-crate too. Thanks for this amazing tool! So my suggestion for the browser goto was as a fallback. I want the metadata format for the other tooling benefits and was wondering if YCM could use this for a fallback mode. But it's not so necessary given how fast it is :)
Haven't heard about YCM before, so decided to try it. Well, it is simply awesome. Thank you very much, now writing Rust in Vim is even more enjoyable!
&gt; How can we even say that it is the safe function which is at fault, rather than some piece of unsafe code elsewhere in Vec? &gt; &gt; The intuitive answer is that Vec has additional invariants on its fields. Personally I would be tempted to say that the unsafe code has had its invariants changed out from under it and become incorrect as a result of changes to safe code. If when writing the code for Vec we had specified invariants explicitly (as one might when writing a correctness proof for Vec) we could blame the safe code, but the article seems to throw away the useful maxim that "all memory unsafety can be traced back to an incorrect unsafe block" without needing to. In this example, by safely adding 2 to a Vec's len, we destroyed the invariant that "∀ x : usize . x &lt; len ⇒ HoldsInitializedValue(ptr+x)" (or such). As a result we need to rewrite the unsafe block in Vec's indexing method. With our weakened set of invariants, we can't do proper indexing anymore but we could, for example, always return the first element of vectors with odd lengths (and panic on even-length Vecs) since the first element's initialized status can't be faked with our add-2-to-length method.
Will do tomorrow. It's late now, and I'd like to get it working with Sublime first (the ycmd plugin only works for cpp).
Also, as to `CAS`: we'll likely just steal C++'s ideas for concurrency. I haven't actually looked yet (because that would be putting the cart before the horse), but I'm pretty sure C++ defines concurrency really well.
I express interest! :DD haha
In addition to what other people have said, your code can be made shorter (and allocate less!) by doing something like this: http://is.gd/3tJOuT
The description of GoTo only mentions definitions. Is it also possible to GoTo other things like call sites, implementations, types, help / docs?
How to do profiling like this?
Since GoTo navigates you to the definition, the doc comment is nearby. Granted, this isn't ideal. Many YCM completers support a `GetDoc` and `GetType` subcommand; I hope to add support for these in the near future. `GetDoc`, for example, should retrieve the doc string and open up a split with the docs. This functionality is already built into the YCM vim client, the rust completer just lacks support for it. Since everything is backed by racer, any missing functionality needs to first be available there. /u/phildawes is [actively working](http://phildawes.net/blog/2015/12/29/racer-update-6/) on using rustc for type inference which is critical for good `GetType` support. Enumerating call sites for a method and implementations of a trait is also unsupported at this time.
No sorry, I meant I'm doing completely different stuff (machine learning, numerical stuff). I'm interested in the role of type safety there, too.
Yes, I'm trying to match on opcodes. Some of them include register codes as parameters. (Register - code) A - 111 B - 000 C - 001 D - 010 E - 011 H - 100 L - 101 See how the range is not quite contiguous? 110 is special. So I might have an opcode of pattern 0b00aaabbb where aaa is a register and bbb is another register. How do I neatly create a match pattern that captures all such possible opcodes? I can generate the pattern, then copy and paste it into the match, but if there's a cleaner way to do it, I'm trying to find it.
This post totally inspired me ! Finally, some decent and fast completion ! When trying the *atom-youcompleteme* plugin, I noticed that it uses a custom version of ycmd, which is more than 100 commits behind and thus doesn't support *racerd* yet. However, a [fix for this](https://github.com/Qusic/atom-youcompleteme/pull/41) is on the way.
C (reference decoder) File Name: 01 Colorful..flac Size: 33.2 MB Minimum block size: 4096 Maximum block size: 4096 Minimum frame size: 14 Maximum frame size: 14296 Sample rate: 44100 Number of channels: 2 Bits per sample: 16 Total samples: 11321352 MD5 sum: d9baa49f4aca47fa2400e141dc8b96ac 0.66 real 0.43 user 0.04 sys 0.48 real 0.43 user 0.03 sys 0.47 real 0.43 user 0.03 sys 0.47 real 0.43 user 0.03 sys 0.48 real 0.43 user 0.03 sys ------------------------------------------------- avg: 0.512 real 0.43 user 0.032 sys File Name: 04 - City 2 City ft Belle Humble.flac Size: 46.1 MB Minimum block size: 4096 Maximum block size: 4096 Minimum frame size: 2227 Maximum frame size: 15421 Sample rate: 44100 Number of channels: 2 Bits per sample: 16 Total samples: 13995450 MD5 sum: f11ef9ad441d5cf362b3bb7021337a4c 1.34 real 0.58 user 0.06 sys 0.61 real 0.55 user 0.04 sys 0.65 real 0.54 user 0.04 sys 0.60 real 0.55 user 0.04 sys 0.61 real 0.54 user 0.04 sys ------------------------------------------------- avg: 0.762 real 0.552 user 0.044 sys --- Rust (my implementation) File Name: 01 Colorful..flac Size: 33.2 MB Minimum block size: 4096 Maximum block size: 4096 Minimum frame size: 14 Maximum frame size: 14296 Sample rate: 44100 Number of channels: 2 Bits per sample: 16 Total samples: 11321352 MD5 sum: d9baa49f4aca47fa2400e141dc8b96ac 1.14 real 1.06 user 0.01 sys 1.11 real 1.08 user 0.01 sys 1.11 real 1.08 user 0.01 sys 1.11 real 1.08 user 0.01 sys 1.10 real 1.07 user 0.01 sys ------------------------------------------------- avg: 1.114 real 1.074 user 0.01 sys File Name: 04 - City 2 City ft Belle Humble.flac Size: 46.1 MB Minimum block size: 4096 Maximum block size: 4096 Minimum frame size: 2227 Maximum frame size: 15421 Sample rate: 44100 Number of channels: 2 Bits per sample: 16 Total samples: 13995450 MD5 sum: f11ef9ad441d5cf362b3bb7021337a4c 1.43 real 1.37 user 0.02 sys 1.45 real 1.36 user 0.02 sys 1.54 real 1.40 user 0.02 sys 1.44 real 1.37 user 0.02 sys 1.43 real 1.37 user 0.02 sys ------------------------------------------------- avg: 1.458 real 1.446 user 0.02 sys The reference decoder is about 2.5 times faster than mine. Here is [the code][1] I used, it is basically just benchmarking the decoding itself with no output to a file. The sample counting is just to make sure neither of them stopped early. [1]: https://gist.github.com/sourrust/4d86c3e495600c83190b
This is a great tip in general – it's often said that you should try and do things yourself to improve (be it programming or whatever skill). But often it's not that simple. Studying what masters do is a surefire way to get new ideas and stimulus and can help to overcome blocks.
Yeah, so I'm not exactly sure what you're using mozjs for, but if you want the lowdown on how servo does it's bindings: [components/scripts/dom/webidls](https://github.com/servo/servo/tree/master/components/script/dom/webidls) This is where server gets it's javascript bindings interface from. It generates it using python and put into /target/debug/build/script. Makes it a pain to figure out how to extend servo's binding without modifying servo and its webidl's directly. Using these interfaces looks like this (console.rs): impl ConsoleMethods for Console { fn Log(&amp;self, messages: Vec&lt;DOMString&gt;) {} fn Debug(&amp;self, messages: Vec&lt;DOMString&gt;) {} ... } webidl looks like this (console.webidl): interface Console { // These should be DOMString message, DOMString message2, ... void log(DOMString... messages); void debug(DOMString... messages); void info(DOMString... messages); void warn(DOMString... messages); void error(DOMString... messages); void assert(boolean condition, optional DOMString message); }; [components/scripts/dom](https://github.com/servo/servo/tree/master/components/script/dom) Here's the code that uses the webidl interfaces that servo generates. They are then included inside the window.rs and windows.webidl to be used by the web browser (like how console.log() is also window.console.log())
Racer is a semantic analysis tool for rust that provides completions and definition locations. YCM is an editor integration for vim and client to ycmd. ycmd is a server which abstracts various semantic analysis providers (like racer) to serve a variety of programming languages. YCM and racer are not directly comparable for these reasons. If you want to talk about the difference between something like vim-racer and YCM, then there are some points worth mentioning. * YCM provides a language agnostic identifier based completion engine in addition to the racer based semantic completions and GoTo. * YCM will help you program in other languages as well. Check out the [list of supported languages](https://github.com/Valloric/YouCompleteMe#intro) if this interests you. * YCM provides caching of completions to improve performance * YCM fuzzy searches available completions * YCM's editor integration is *very* mature having been used by hundreds of thousands of people over the last several years. * YCM's rust completion is based on racerd, an HTTP server for racer. racerd provides benefits over calling racer directly including dirty buffer handling, and caching. These points are discussed thoroughly in the blog post. 
I've put the r2d2 plugin on crates.io
This is absolutely the dream. I'll be graduating in about a year and a half; I would LOVE to do something like this afterwards.
There is https://github.com/saviorisdead/RustyCode that uses racerd, so I don't actually see a need for another extension.
So, is there a reason to use ycmd over racerd? Or is it just a tool for Vim users? I'm using Atom with racer plugin, so I was a little bit confused, reading that. PS: I know the difference between racer and racerd.
Rust, C, and C++ all use the same concurrency model, yes. (though Rust excludes a useless access mode, Store) Whether its defined *well* is [another question](http://www.di.ens.fr/~zappa/readings/c11comp.pdf)...
Is this notion of "abstraction safety" the same one as discussed by Dreyer in [these slides](https://www.mpi-sws.org/~dreyer/talks/plmw2014-talk.pdf)? That one seems closely tied to parametricity. Even farther afield: are these "semantic" types in any way related to "computational" type systems like NuPRL? (I know almost nothing about them myself, but sometimes see /u/jonsterling remarking about the inadequacies of syntactic type systems relative to them...)
It will work as you describe on little-endian architectures: https://en.wikipedia.org/wiki/Endianness because the least significant byte is stored first in memory. It's not obvious from your example, because 1,0,1 is symmetric, so you can't see that it's actually reading them in reverse.
Duh! Thank you!
I don't see why a question would be too small for a thread. 
You're looking for /r/playrust
Each format offers different trade-offs. If you think it's likely that someone may want to google your question and find its exact answer, you should ask on Stack Overflow. If your question is open ended or otherwise doesn't fit the SO model, creating a new thread here is an option. Now, most Reddit users are lurkers. So perhaps someone that's learning Rust may find convenient reading a bunch of small questions in the same thread? Not sure about this. It may also encourage one-off questions that wouldn't otherwise be asked here. A side note: in practice the subreddit front page is a finite resource, since new posts displace older posts. But the subreddit has low traffic so, right now, this isn't a concern.
Well, first of all, *no* because you can't do math in macros. Well, that's not entirely true, but it involves things like unary counting, and is really quite unlikely to be what you want in this case. Secondly, *no* because macros have to expand to one of a limited list of things, and "match arm left hand side" is not one of those things. Patterns *are*, but `|` isn't part of the pattern, it's part of the match arm. So that's a *double no.*
Yes, it looks like the OP is using the same sense of "semantic" / behavioral types that are talked about in systems like Nuprl.
Thank you for the link. I hope I'll remember that term.
One trick I used a lot was to think hard about _where the data was coming from_. This helps you understand what can be borrowed and in general how to structure your code to avoid copies.
Isn't metadata already stored in all crates? I thought parsing all of the code again when you have the compiled crates was silly with racer and looked into it a few months ago. With some changes to metadata loader you could reuse rustc's code and get tons of interesting data for tools to use. It would also be the only way for working with closed-source (yet unstripped) crates. I wonder if there is a reason that Racer doesn't use metadata for external crates?
Online version http://duckie.me/
https://manishearth.pastebin.mozilla.org/8856303 The long timings are for whenever I query a heavier part of libscript -- like the bindings or dom folder. Somehow cross-crate is _only_ working with the URL crate (when working from `document.rs`). Not sure why the other crates don't complete.
To test things like this, I recommend always using asymmetric input. If you call your function with `&amp;[1, 2]`, it outputs "1000000001", whereas I'm guessing you want it to output "100000010" (its output for `&amp;[2, 1]`).
IMO, Stack overflow is much more suited to questions while Reddit is more suited to news. I think we should have a sticker thread to pointing to stack overflow.
Perhaps some of the links should have a description or a tooltip. A newcomer may not guess that "The rust programming language" is a tutorieal whereas the "Rustinomicon" is a text about "The Dark Arts of Advanced and Unsafe Rust Programming" 
Once we as a community decide that the 'noob questions' overwhelm the subreddit, we can open a /r/rusthelp sub for them and direct questions there (Source: It works well for /r/java and /r/javahelp).
Dude, reading professional code is an awesome way to learn how to code in a language. You'll run into things that are obvious in what they do, but not obvious in that you can express something so simply. Doesn't matter what language, just figure out what the most well respected and cleanest projects are and scrape through them now and then.
Welcome to checkout these resources: [Awesome Rust](https://github.com/kud1ing/awesome-rust) [Rust Weekly Newsletter](https://this-week-in-rust.org/)
&gt; What else would you expect from a book named after the Necronomicon? You can't expect everyone to understand that reference 
I've just released version 0.1.0 of imageproc. As the README says, this is an image processing library using the image traits and structs from the [image](https://crates.io/crates/image/) crate. Most of the code was written by a Rust novice (me), so any feedback would be great. tafia has already contributed massive performance improvements to several functions. Features requests are also appreciated :-)
Maybe explicit defaults would be a way. Thanks for your thoughts on the matter. I think I understand the risks and benefits better now :-)
Also, on [rust-lang.org](https://www.rust-lang.org/), the book is named “Book”, but on the _All docs_ page it is named “The Rust Programming Language”. A newcomer probably doesn't know that these are the same.
Quote of the week material right here! :D
https://en.wikipedia.org/wiki/Necronomicon &gt; The Necronomicon is a fictional grimoire (textbook of magic) appearing in the stories by horror writer H. P. Lovecraft and his followers.
https://doc.rust-lang.org/nightly/error-index.html This is pretty neat! :)
I know it's part of the book and not a stand-alone document, but [The Syntax Index](https://doc.rust-lang.org/nightly/book/syntax-index.html) should be featured more prominently.
But /r/rust isn't anywhere near that point yet. Better to leave it as is, where the full community can see all questions, and if it does start to pick up to a problematic point, fork off a separate subreddit for such questions.
Cleaned it up and opened a pull request! https://github.com/LuckyGeck/YcmdCompletion/pull/14 No idea if it works for other languages, but it _should_ work. cc /u/Valloric 
It's also worth nothing that StackOverflow has limits about on-topic questions, so questions like "would you recommend ..." for example would get bounced.
Yay! :D
OK, the holidays were quite slow with content posts, but I suspect this will pick up during the next week.
This has been argued before, but in my opinion it would do nothing for actual correctness efforts. Having to put literally *all* code in the `vec` module in `unsafe` blocks doesn't accomplish anything. The rule is simple: if `unsafe` is used in a module, the module is unsafe and you need to Pay Attention. Actual support for expressing invariants *could* be useful, but it's unclear to me how much value you'd gain over just having good tests, in practice.
You didn't read the rest of my comment :P "I'm currently in the middle of defining that." I'm working from an unsafe level up, defining exactly what is and isn't undefined behavior in Rust. Rust doesn't care about allocation and deallocation of memory. `rustc` might, for optimization purposes, but the language itself could care less if your pointer is on the heap, on the stack, or is opaquely from C. To Rust, the following pointers shall be equivalent: let x = vec[0u8; 12]; let xptr = x.as_ptr(); // is equivalent to let y = [0u8; 12]; let yptr = y.as_ptr(); // is equivalent to let zptr = libc::malloc(12) as *const u8; The vector and slice example even live for the same amount of time :)
The type of `b"hello\nworld"` is not `&amp;[u8]` it is `&amp;[u8; 11]` I havent rusted in a while, but you need to convert the byte string to a slice, as someone on IRC, faster responses.
Thank you. I wanted to test performance myself, but I'm hitting endless loops with every file I tested. Here is a sample where the decoder seems to be stuck at sample 49153: https://archive.org/download/unsorted_files/t.flac Both GDB and perf implicate `subframe::parser::encoded_residuals`. There is another loop in metadata parsing which I didn't investigate.
There's an implementation of Bresenham's line algorithm: http://docs.piston.rs/imageproc/imageproc/drawing/fn.draw_line_segment.html. However, this 1. doesn't perform any anti-aliasing and 2. doesn't currently allow you to specify line width. I've created an issue for this: https://github.com/PistonDevelopers/imageproc/issues/97. Edit for second question: I wasn't planning on adding support for text. My reason for starting the library was that there didn't seem to be anything in Rust that contained the basic image processing operations needed for computer vision. I've not actually written any vision applications on top of this yet, and I'm happy to add more image-editor-y stuff instead, but I don't fancy dealing with rendering text. Maybe someone else does though.
&gt; Am I misunderstanding the docs? Can I use a byte array instead of a file object e.g. for testing? The compiler's error message gives the issue: &gt; no method named `read_line` found for type `&amp;[u8; 11]` `&amp;[u8]` (a slice of u8) implements `BufRead`, but `&amp;[u8; 11]` (a reference to an 11-items array of u8) does not, so you need to create a slice from your array with `&amp;array[..]`
 use std::io::BufRead; fn main() { let arr: [u8; 3] = [b'a', b'b', b'\n']; let mut slice: &amp;[u8] = &amp;arr[..]; let mut s = String::new(); slice.read_line(&amp;mut s).expect("unable to read line"); println!("{}", s); }
Good point. On the yet undeployed [website doc page](https://doc.rust-lang.org/nightly/index.html) I have "The Book" mentioned in the description. 
There has been some talk of some things like this, https://github.com/rust-lang/rfcs/pull/1133 for example
The problem I am having has to do with unions, as rust doesn't have them.
I've read it already. Furthermore, it says &gt; If you really want global mutable state, try using static mut or a global UnsafeCell. but using static mut doesn't help here. Thanks for pointer to `*mut gpio::Gpio`. I can work this way but it requires much more unsafe code than is needed. The code for all Led methods is actually safe but now it requires `unsafe` because of pointer usage.
Completely agreed on the Builder pattern. It feels terrible to write and to use. Is there any way around it? I'm thinking of something like a macro that automatically fills a struct's missing values, that is: struct Stuff { foo: Option&lt;i64&gt; bar: i64 baz: i64 } default!(Stuff{bar: 5i64}) translates to: Stuff{foo: None, bar: 5i64, baz: 0i64} but I'm not sure if this makes sense. Thoughts?
You mean like C's `union`? Rust has tagged unions, which it calls "enums".
&gt; I get that things like varargs and keyword arguments have a runtime overhead. Do kwargs really have any overhead at all? It seems to me they can be implemented without any overhead at all, am I missing something? Is there some other deep reason why Rust doesn't have them?
Default values? struct Stuff{ foo: Option&lt;i64&gt;= None bar: i64= 0 baz: i64=-1 } 
I think std lib should be large so people can find a lot of stuff on it battle tested and minus breaking changes without looking at random crates. Also as an example of downsides the rand crate build build script took me a huge amount of time to build on a puny Windows laptop, making my very small applications compilation seemingly frozen. Of course std lib apis need to be stable and therefore require careful design. I think rust has good reason to slim down before 1.0 since it didn't have time to finalize all those apis and should take it slow adding things back in as they are ready but I hope people are willing to add a lot of useful stuff back in. For example I think datetime should be in the std lib when ready. 
Submit it here: https://users.rust-lang.org/t/twir-quote-of-the-week/328/194
As long as you can figure out the size of the largest variant, it's not bad. union UnionType { size_t x; char y; void *z; }; should translate to something like: #[repr(C)] UnionType { _union_data_: usize }; impl UnionType { pub unsafe fn x(&amp;self) -&gt; *const usize { let raw: *mut u8 = ::std::mem::transmute(&amp;self._union_data_); ::std::mem::transmute(raw) } pub unsafe fn y(&amp;self) -&gt; *const libc::c_char { let raw: *mut u8 = ::std::mem::transmute(&amp;self._union_data_); ::std::mem::transmute(raw) } pub unsafe fn z(&amp;self) -&gt; *const *mut libc::c_void { let raw: *mut u8 = ::std::mem::transmute(&amp;self._union_data_); ::std::mem::transmute(raw) } pub unsafe fn x_mut(&amp;mut self) -&gt; *mut usize { let raw: *mut u8 = ::std::mem::transmute(&amp;self._union_data_); ::std::mem::transmute(raw) } pub unsafe fn y(&amp;mut self) -&gt; *mut libc::c_char { let raw: *mut u8 = ::std::mem::transmute(&amp;self._union_data_); ::std::mem::transmute(raw) } pub unsafe fn z(&amp;mut self) -&gt; *mut *mut libc::c_void { let raw: *mut u8 = ::std::mem::transmute(&amp;self._union_data_); ::std::mem::transmute(raw) } } (Based on the output from bindgen) Definitely not pretty though.
You can't do it with macros or syntax extensions, *period*. Macros are expanded before the information you want to use even exists. Theoretically, you could do it by getting the code to compile *without* the method (or with a stubbed-out one), then inject a lint (or some other post-type checking process) to extract the information and store it somewhere, to be used in a subsequent compile. Kinda like how TeX handles forward references.
Have you tried using the `volatile` intrinsics with the pointer? e.g. [`volatile_load`](http://doc.rust-lang.org/core/intrinsics/fn.volatile_load.html) and [`volatile_store`](http://doc.rust-lang.org/core/intrinsics/fn.volatile_store.html) (you might even be able to write a `Volatile&lt;T&gt;` smart pointer using these intrinsics)
In rust's current implementation - variable functions always have their arguments passed in on the stack, along with the number of arguments and some padding, rather than having them passed in via registers. 
Awesome. I'd used Default::default() before but not seen this initialization syntax. Thanks for sharing!
Sure, but you don't need varargs to have kwargs. It would just be a matter of compiling the definition fn_kwargs add(x : i32, y : i32 = 10) -&gt; i32 { x + y } into something like fn add%123189(x : i32, y : i32) -&gt; i32 { x + y } macro_rules! add { (x = $x:expr) =&gt; { add%123189($x, 10) }; (x = $x:expr, y = $y:expr) =&gt; { add%123189($x, $y) }; (y = $y:expr, x = $x:expr) =&gt; { add%123189($x, $y) }; } No overhead, parameters passed via registers, to the compiler it would be just a regular function call. 
This looks cool! Any chance you'd consider adding some more sophisticated interpolation methods like bicubic or lanczos?
Low level Rust programming for raspberry pi GPIO
Wow, you know what, I totally just read kwargs as varargs. Regardless, you're absolutely correct. 
Syntactic elements that get used as much as `pub` and `fn` don't really benefit from being expanded to `public` or `function`, in my opinion. It's not so much that longer forms are good all the time, but that longer forms are good when they're used less frequently.
I'm not against well known and established abbreviations like "xml" but "pub" is not such a well known abbreviation and even worse - it already has an unrelated meaning. Even regular editors like vim, emacs, notepad++, gedit, etc have auto-complete nowadays so why the need to save two characters and make the code less readable like we live in the seventies and need to save bytes?? 
because `pub fn` is shorter to read than `public function`
Interesting post! The move to defining the boundary of unsafety at the module boundary makes me a little uncomfortable - it assumes that the fields which are vulnerable to being manipulated into causing 'unsafe errors' are private, when there is nothing requiring them to be. In effect we are just trading one convention (an unsafe block should establish invariants it relies on) for another one (fields which can affect the invariants of unsafe blocks should be private). There doesn't seem to be much gain to me. This reminds of the debate that has been simmering for a while about whether unsafe blocks should be as small as possible or should include as much code as necessary to re-establish Rust's invariants. That debate seemed to have settled on the former position, which I don't much like. I feel like the latter is right, and furthermore should be extended such that unsafe blocks should establish their own invariants. However, it would be a hard sell convincing programmers to put the body of `evil` in an unsafe block when there is no unsafe code there, in that respect I guess relying on privacy is a better bet. I guess at the end of the day, arguing about the boundaries where a program must establish implicit invariants is going to pretty fruitless. Just got to make some conventions and stick to them.
Atomics can work in their place as their stabilized and likely mean you're not doing something unsafe with pointer aliasing. Instead you'll likely be doing an unsafe transmute from a * to an &amp;. But you'll get better cache coherence as a side effect. 
I'm pretty sure what you are looking for is "cargo features". Basically, you set "optional = true" for some dependencies and then create sets of "features" that activate them (eg one called "nightly"). In your code you can use `#[cfg(…]` for conditional compilation when the feature is active. Now, when testing with a nightly compiler, you run `cargo test --feature "nightly"`. Have a look at the documentation or some popular crates to see how this works. Most of my crates use this to include clippy, for example.
&gt; Checking the OpenGL version and extensions to dispatch between various paths. This is required to ensure compatibility between all versions, and to use more optimized versions of some functions when available. These checks should have 0% branch misprediction though. Couldn't Glium load some function pointers at some initialization code?
Good! Do you know if there's any up-to-date documentation on procedural macros and the `syntax` crate? [This link](https://doc.rust-lang.org/1.1.0/syntax/) seems woefully outdated, but it's all I could find (short of browsing the actual code).
do you have a github account? i'm curious to see your personal projects.