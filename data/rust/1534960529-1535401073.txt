Could a macro create a newtype and give it a basic set of mathematical operations? 
Actix-web branch of my reverse geocoder. https://github.com/llambda/rust-reverse-geocoder/blob/actix/src/actix.rs#L56 Key takeaways: * Actix API is awesome * Actix is fast! * Actix provides performance benefits even for extremely CPU bound work.
I could've sworn there was some RFC or pre-RFC or discussion about a mechanism for specifying that certain values or bit patterns are invalid for a given type and can thus be used to store enum discriminants. But I can't find it right now :/
When reading lots of files with a cold disk cache, disk access patterns can make a tremendous difference. Using more threads than cores can be advantageous. I did [some measurements](https://github.com/ruuda/mindec/blob/75e647abd21b0f690e9ecec49afafa209743cd2f/performance.md) on a program that reads the headers of ~12k files. I used the `walkdir` crate to enumerate files, and tried different ways of distributing them over reader threads, and different ways of interleaving reading files and walking the filesystem. This alone could make a 25% difference in running time!
This isn't related to `Vec`. Think about the contract of the underlying `read` API. You don't know you're "done" until you observe a successful `read` call that returns no bytes. So even though you don't fill the space used by the extra byte, you still need that space to pass to the underlying `read` call to confirm that you've reached EOF. I suppose you could craft an implementation that doesn't cause a `Vec` to realloc, but it would be fairly contorted, and I don't know if it would impact performance overall.
The OP is setting the capacity on the `Vec` to minimize reallocs. If you're going to read the entire file into memory anyway, it's better to just do that instead of inserting a buffer. If you can craft your line counter to operate incrementally (perhaps only requiring to fit at _least_ a single line into memory), then yeah, you don't need to read the entire file into memory. This is probably how I would build a line counter, personally. But I've never done it before, so there may be complexities I'm not thinking about. :-)
good idea, i can even test on darwin!!
I was just curious about the effect of calling clear(). After looking through the source I see it doesn't affect the Vec's capacity, only len.
Reading through the example on the GitHub repo, it's almost like you were sitting in the back seat of my car on my way home from CodeMash earlier this year wherein a friend and I were discussing a distributed, cryptographically-signed, web-of-trust library review system after a conversation about the human aspect of security vulnerabilities. The summary of that conversation reads almost exactly like your second paragraph. Devs are charged with vetting but really don't. We build upon libraries but we don't build upon review results. We need a crowdsourced review system to know if libraries are "good" by establishing a variety of aspects of ratings. I'll be watching this closely. I've had that project in my bag for some downtime that will probably not happen anytime soon so I'm glad to see that someone had the idea independent of me and has the time to implement it.
&gt;Why there are NonZero\* types, but no NonMax\* types? From my point of view, NonZero used mainly as an index in a collection. In this case", you have to -1, +1 it on every use. At least according to the RFC introducing the `NonZero`\* types [RFC #2307](https://github.com/rust-lang/rfcs/blob/master/text/2307-concrete-nonzero-types.md), it's because the `NonZero`\* types were introduced to extend an existing compiler optimization for references to pointers and primitive integers. A `NonMax`\* type seems like it'd be part of something "separate", for lack of a better term, rather than using something that was already in the compiler. &gt;Is it maybe, possible to introduce an InvalidValue trait which can be used to define your own "special" value for arbitrary type? &gt; &gt;And for this special trait, we could implement compiler support to optimize memory footprint of Option&lt;T&gt; where T: InvalidValue. At least from what I understood of the conversation in RFC #2307, compiler optimizations based on an `InvalidValue&lt;T&gt;`\-ish type would probably depend on const generics, which could explain why it hasn't been introduced yet. I could have sworn there was some discussion about this sometime in the last few weeks when the other NonZero\* types (or something related) were being discussed, but I can't seem to find it any more :(
I probably should have read more of the RFC before hitting "reply"; this bit from the "Rationale and Alternatives" section answers your first question precisely: &gt; On the other hand, maybe zero is “less special” for integers than NULL is for pointers. Maybe instead of `num::NonZero*` we should consider some other feature to enable creating integer wrapper types that restrict values to an arbitrary sub-range (making this known to the compiler for memory layout optimizations), similar to how PR #45225 restricts the primitive type `char` to `0 ..= 0x10FFFF`. Making entire bits available unlocks more potential future optimizations than a single value. However no design for such a feature has been proposed, whereas `NonZero` is already implemented. The author’s position is that `num::NonZero*` should be added as it is still useful and can be stabilized such sooner, and it does not prevent adding another language feature later. 
I guess? The following is in the [RFC #2307](https://github.com/rust-lang/rfcs/blob/master/text/2307-concrete-nonzero-types.md) "Rationale and Alternatives" section: &gt; Memory layout optimization for non-zero integers mostly exist in rustc today because their implementation is very close (or the same) as for non-null pointers.
Having a little difficulty telling whether or not this is facetious. :P By that same token, one could argue that the turbofish could be removed simply by turning the less-than operator from `foo&lt;bar` to `foo(bar)` and making less-than a particular case of `Fn`, since a comparison is really just a function taking two values and producing a boolean. Leaving aside that both these proposals would arbitrarily exclude implementing `Fn` and `Index`/`Ord` for the same type, just because we can technically do something doesn't mean we should. Anyway, nobody yet has proposed *my* solution to the turbofish, which is to turn the less-than operator into the not-greater-or-equal operator, written `!&gt;=`, which as we all know is mathematically equivalent. :)
A professor at UC Davis teaches a lower division Software Development and Object Oriented Programming course in Rust. It was what introduced me to Rust in the first place.
How about counting in parallel? This dirty hacky version quite has some performance boost on my system: https://play.rust-lang.org/?gist=0d75452588a80ef9264345c06168d12c&amp;version=stable&amp;mode=release&amp;edition=2015
I got much different results using exactly the same code as you. Since you didn't provide your test data for how to produce it, I had to create my own. Here's my methodology: 1. Create test data by running the following (about 3GiB of data created): #!/bin/bash mkdir -p testdata for i in $(seq 1 10); do mkdir -p "testdata/$i" for j in $(seq 1 150); do head -c 2048000 &lt; /dev/urandom &gt; "testdata/$i/$j" done done 2. Compile the `go` code with "go1.10.3" 3. Use `cargo new --bin walkdir-example` to create a folder for the rust version, add `walkdir = "*"` to the cargo toml, use `cargo build --release` to get a release binary (using rustc 1.30.0 nightly). 4. cd into `testdata`, run the following: $ ../go-walkdir-example | sha256sum 9dbdac935739cf14c4504af58d345d2679e1bf3d0f964cf244570d678c17d7d9 - # you should get the same number if you use my test data set $ for i in $(seq 1 5); do ../go-walkdir-example &gt;/dev/null; done; time ../go-walkdir-example&gt;/dev/null ../go-walkdir-example &gt; /dev/null 0.93s user 0.85s system 116% cpu 1.534 total # warm it up, then run one sample $ ../walkdir-example/target/release/walkdir-example | sha256sum 9dbdac935739cf14c4504af58d345d2679e1bf3d0f964cf244570d678c17d7d9 - # same as the go one $ for i in $(seq 1 5); do ../walkdir-example/target/release/walkdir-example &gt;/dev/null; done; time ../walkdir-example/target/release/walkdir-example&gt;/dev/null ../walkdir-example/target/release/walkdir-example &gt; /dev/null 0.05s user 0.06s system 98% cpu 0.112 total As you can see, the rust program is 15 times faster for my sample size / machine / etc. Note, I'm using the 1st of your rust programs because for me it's by far the fastest. The other two are significantly slower on my machine.
Wouldn’t something like the `nom` crate be the right tool for this job? You’re basically just trying to parse a file looking for line breaks. `nom` is supposed to be pretty fast. 
I’m not the author of the blog post, he probably doesn’t know about this Reddit thread. If anyone figures out his username ping him :P
Yes, pretty much.
Is the website down for anyone else? This seems to keep on happening to me.
Hey this is so cool, guess what? I'm taking that section! Just registered a few days ago.
Well, `MaybeUninit` has the definite benefit it being usable with causing undefined behavior in 99% of the cases :D
I mean, you can do this, but if you do then you should also do parallel counting in the go code to be fair. It would be interesting to see the impact of go's green threads here.
We're also hiring!
Probably as no surprise I've also thought about ~~stealing~~ learning from ripgrep's code to make Tokei faster. However the problem is that there's no way to not lose some degree of accuracy. Specifically handling strings in programming languages seems to prevent from being regularly parsed in terms of the [Chomsky hierarchy](https://en.wikipedia.org/wiki/Chomsky_hierarchy). Have a look at the below test case and the output of tokei, loc, cloc, and scc. ## Tokei ------------------------------------------------------------------------------- Language Files Lines Code Comments Blanks ------------------------------------------------------------------------------- Rust 1 39 32 2 5 ------------------------------------------------------------------------------- ## loc -------------------------------------------------------------------------------- Language Files Lines Blank Comment Code -------------------------------------------------------------------------------- Rust 1 39 5 10 24 -------------------------------------------------------------------------------- ## cloc ------------------------------------------------------------------------------- Language files blank comment code ------------------------------------------------------------------------------- Rust 1 5 10 24 ------------------------------------------------------------------------------- ## scc ------------------------------------------------------------------------------- Language Files Lines Code Comments Blanks Complexity ------------------------------------------------------------------------------- Rust 1 34 28 1 5 5 ------------------------------------------------------------------------------- ## Testcase // 39 lines 32 code 2 comments 5 blanks /* /**/ */ fn main() { let start = "/*"; loop { if x.len() &gt;= 2 &amp;&amp; x[0] == '*' &amp;&amp; x[1] == '/' { // found the */ break; } } } fn foo() { let this_ends = "a \"test/*."; call1(); call2(); let this_does_not = /* a /* nested */ comment " */ "*/another /*test call3(); */"; } fn foobar() { let does_not_start = // " "until here, test/* test"; // a quote: " let also_doesnt_start = /* " */ "until here, test,*/ test"; // another quote: " } fn foo() { let a = 4; // /* let b = 5; let c = 6; // */ }
A rfc titled “[Fix the Error trait](https://github.com/rust-lang/rfcs/pull/2504)” was approved recently: - [tracking issue](https://github.com/rust-lang/rust/issues/53487)
/u/boyter
Could you expand more on what `read_exact` is incorrect? How would a race condition occur, unless either getting the file length or allocating memory are non-blocking calls? Could you just allocate a buffer to the proper size you need and then call `read`? This seems much faster than read to end
I'd like a `Non&lt;T, const T&gt;` type, that stores a `T`, but does not contain any value equal to `const T`. 
Woops. Missed that.
As of [this PR](https://github.com/rust-lang/rust/pull/45225), *any* known-invalid bit pattern will be used to optimize enum layout, not just zero. The real reason we don't have `NonMax` yet is that the design work is not finished. There's a lot of discussion in [this RFC](https://github.com/rust-lang/rfcs/pull/2307) and its associated tracking issue, but the gist of it is we really want something more general than just playing whack-a-mole with `NonZero`, `NonMax`, `NonSomethingElse` and we just pushed `NonZero` through first because it's by far the most useful.
This whole situation would have been avoided if we could have just had ML like syntax Foo&lt;Bar, Baz&gt; --&gt; Foo Bar Baz
Is this all proc macros? Like a proc macro you can annotate an inline module with?
I am trying to understand borrowing and i don't get the following example: [https://play.rust-lang.org/?gist=bb36311c46feaea4640a61873936ad3d&amp;version=nightly&amp;mode=debug&amp;edition=2018](https://play.rust-lang.org/?gist=bb36311c46feaea4640a61873936ad3d&amp;version=nightly&amp;mode=debug&amp;edition=2018) &amp;#x200B; Why doesn't collect remove the borrow i have on cache immediately after the collect? I would expect collect() to copy all the elements? How would you propose to handle these kind of cases? Thanks!
Believe me, I've noticed that tokei is more accurate than its competitors. :) That some of the more recent articles haven't addressed line counting accuracy in sufficient depth is a deficiency I've noticed. I've also read tokei's source code, and I think there are wins to be made. But, I won't know for sure until I try. I don't want to go out and build a line counting tool, but I think a proof of concept would make a great case study for libripgrep. I will keep your examples about accuracy in mind though, thank you! Note that ripgrep core has just recently been rewritten, so if you haven't looked at it recently, there might be some goodies there that you haven't seen yet!
The side of the file can change between when you ask what it's size is and when you read from it. Consider what happens when the file gets shorter, for example.
&gt; but you must only be safe in safe code. I think "only" here does not qualify "must" as was the intention, so this phrase actually means that you cannot be safe in code that is not safe. I think it's better to say "you only must be safe in safe code".
Got it. I hadn't considering changing file sizes. The code up on the playground works properly, reading the correct amount of bytes for both the `read_exact` and `read` calls. The implementation that uses `read` is much faster when file sizes vary. There is no practical difference in speed when file sizes in a directory are around the same. The benchmarks I had previously posted (in other comment chain) are indeed incorrect though. I need to change them. 
`collect` does not copy the elements - it just *collects* whatever elements the iterator provides into a collection. In this case, if you had written full type for `removableentries`, which is `HashMap&lt;&amp;i32, &amp;String&gt;`, you could immediately spot where borrowing happens. In your new hashmap elements are just references to the original hashmap, and that's why the borrow is kept active. If you want to actually create copies of the elements, you can add `.map(|(k, v)| (k.clone(), v.clone()))` before collect. Usually `.cloned()` is enough and more succinct instead of explicit map + clone, but sadly it does not work in this case because you need to clone elements of a tuple, instead of the whole tuple.
I think the most awesome way to learn Rust is to join existing projects – I do that all the time (OK, I also started a few on my own). You get an existing codebase so you're not working from a blank slate and you can also often get great mentorship this way. It's a total win-win. To start your search, [this week in rust](https://this-week-in-rust.org) has a list. Also, please avoid noninclusive language like 'guys' – there are a number of non-guys here, and we need all of them. 'Folks", "y'all", "yinz" or "Rustaceans" are a good replacement.
Thanks for the clear explanation! That really helped.
 let mut s: Vec&lt;u8&gt; = Vec::with_capacity(file.metadata().unwrap().len() as usize); file.read_to_end(&amp;mut s).unwrap(); You'll get better performance if you create a Vec with one byte of extra capacity (`Vec::with_capacity(len + 1)`). This is because `read_to_end` will repeatedly grow the buffer and then try to `read` into it, until the read fails. This means it always ends up with at least one byte of extra buffer capacity. Fortunately, you don't need to remember this yourself. As of Rust 1.26, [fs::read](https://doc.rust-lang.org/std/fs/fn.read.html) will do this [automatically](https://doc.rust-lang.org/1.26.0/src/std/fs.rs.html#223-268). 
You'll get better performance if you create a Vec with one byte of extra capacity (Vec::with_capacity(len + 1)). This is because read_to_end will repeatedly grow the buffer and then try to read into it, until the read fails. This means it always ends up with at least one byte of extra buffer capacity. Fortunately, you don't need to remember this yourself. As of Rust 1.26, fs::read will do this automatically.
I really like this idea! This is basically web of trust for signed tests. Then one can measure trusted coverage and at the same time ensure compatibility of dependencies. One idea I'd like to add: The coverage should only mark code from functions which are directly called from the tests. It should not cover indirectly called code. This kind of conservative covering prevents the coverage and therefore trust of untested and potentially dangerous code.
It could be interesting to have a look at gx, which is a language-agnostic, decentralized and content addressed (based on ipfs) package manager. [https://github.com/whyrusleeping/gx](https://github.com/whyrusleeping/gx)
The blog author ran his tests on the linux kernel source, not on random data.
It might be that, thanks!
Is there much more to it than `memchr`?
One word: 4chan. 
Well it's a good thing then that I'm not in charge 😛
I'm also interested in hearing the answer to this. From skimming the docs it looks like there are basically 3 things: - `GROUP BY` and `HAVING` - We intend to support this, but there are backwards compatibility concerns. It'll be supported in 2.0. Discussion at https://discourse.diesel.rs/t/proposed-change-replace-nonaggregate-with-something-less-broken-that-can-support-group-by/18/2 - `CASE`/`WHEN` - No real reason for us not to other than we haven't gotten around to it because it's literally never been requested - Various random SQL functions - We basically don't include any functions in Diesel that don't need to live there. We provide [`sql_function!`](http://docs.diesel.rs/diesel/macro.sql_function.html) to support any arbitrary SQL function that you might want to use. For example, we don't have `lower` in Diesel because it's so easy to write `sql_function!(fn lower(x: Text) -&gt; Text);` in your app and we will never be able to cover ever function supported by every database under the sun. The functions that we *do* support are functions like `max` or `sum` which are generic and have other constraints which benefit from being upstream. The other interesting case is `coalesce` which we simply can't represent safely in Rust today, but is trivial to define with the actual signature you're calling it with. From my point of view the only real gap there is `GROUP BY` and friends. There are a lot of other holes in Diesel's API (joining to subselects, any form of union, CTEs) that we need to address, but specifically if we're comparing to Esqueleto it doesn't look like that library handles those cases either. (Granted I may be missing something -- this was just skimming the documentation but perhaps since the types are the documentation I just missed it)
We all have a tendency to reduce tasks down to the simplest possible instantiation of them. Consider ripgrep for example. Is there much more to it than just looking for occurrences of a pattern? Doesn't seem like it, but 25K lines of code later... It's really about trying to reduce the amount of work per byte in the search text. An obvious way to iterate over lines is to, sure, use `memchr`, but it would be better if you just didn't iterate over lines in the first place. If you look at the source code for tokei for example, there are a limited number of characters that it cares about for each particular language. So if you could make finding instances of those characters very fast without even bothering to search line by the line, then you might have a performance win. This is one of the cornerstones of what ripgrep does, for example. Whether it's an actual performance win or not depends on the distribution of bytes and the relative frequency of matches compared to non-matches. So I don't know.
One possibility to explore is the use of a VM. I know I've found that to matter for memory mapping for example, but obviously that specific thing isn't relevant to your program. But maybe the way Go makes use of the system is somehow different than what Rust does. I think at this point, I'd try to look at an strace output from each of them and see if there are any glaring differences. Maybe there's an extra stat call somewhere? At these scales, that could definitely account for it. &gt; Happy cake day BTW! Oof. I didn't even notice that. 10 years on reddit? FML.
Wow. Exciting. Good luck!
[rust-native-tls](https://github.com/sfackler/rust-native-tls) abstracts platform specifics for TLS. It also has the advantage of being easier to compile/install on windows vs. rust-openssl
I got a potential speed up for you: https://gist.github.com/rust-play/9491af399206d8b6fa9518f6f7e5d93d The `pre_allocate` function will use constant space, re-allocating a buffer only when it begins operating on a file that is larger than the previous max file size. The speed up is only present for directories which have larger file sizes, and larger variation in file size - a 50% increase in speed. Benchmarks for running this code on it's own folder: pre-allocate: 0.45697102 s no-pre-allocate: 0.44426402 s Benchmarks for running this code on my Downloads folder (3.8 GB, 390 files ranging from 50 bytes to 500 MB) pre-allocate: 5.3239822 s no-pre-allocate: 9.392952 s
We'll make a sacrifice to the release gods on your behalf.
Right, but a group of people create a self contained highly connected component then it doesn't make them trusted by anyone else, they have to convince many people in the wider community to trust them too. Also rather than having to piece together metadata to demonstrate brigading you will have cryptographic poof of brigading.
https://www.reddit.com/r/rust/comments/99aiea/_/e4ntpfv thoughts?
I don't understand. Are we thinking about the same problem? I'm trying to protect everyone eg. from crate author inserting ``` install_rootkit(); register_in_a_botnet(); send_all_secrets_to_cia(); ``` What you describe does not seem to address that. It seems to be more of a better testing of commonly used crates, by its users. Which is an interesting idea, but not what I'm aiming at myself.
Cool stuff! Makes me want to watch the keynote, but I think the videos aren't yet up?
All it takes is one insider to breach the web of trust and allow a conspiracy to poison it. Can you honestly say that no highly regarded developer in the Rust community will A) Not cooperate with 4chan brigading (with 4chan standing in for any similar group of ideologically motivated assholes) B) Not have their credentials for the WoT hijacked? 
Thank you, I'm feeling a lot more motivated to work on `std` now!
Nice! Some useful variations: The keynote only lightly touched, but for a lot of scenarios you really want to move the index management out of the arena itself so it can be shared between several differently-typed arenas. (Though that then requires a different approach to tracking free indices, like a stack or queue.) Using a `usize, u64` pair doubles (or quadruples on 32-bit systems) the size of a handle, which in graph-y situations is not great for memory/cache usage. Since a `Vec` can't even hold a full `usize` of elements anyway, you can get a fairly large win just by packing handles in 64 or even 32 bits. This does reduce the space you have for generations, which leads to the next point: Tracking a single generation for the entire arena (or rather "entity allocator" if you split it out from the Vec part) is liable to cause problems if you a) use fewer generation bits as above and b) tend to reuse indices often while keeping handles around for a while. So often it's useful to keep a generation per-index, even when that entry is empty. Fortunately this costs no extra space. This is a good article that goes into more depth: http://bitsquid.blogspot.com/2014/08/building-data-oriented-entity-system.html
Yep! If one is literally just counting bytes, one can analyse more than a single byte at a time, and completely avoid the function call and extra processing of `memchr`. For instance, https://crates.io/crate/bytecount seems to be almost 100 faster than memchr in a loop.
I think crates on crates.io should be put into one of four categories: *Safe*, *Trusted Unsafe Dependencies*, *Unsafe Dependencies*, *Unsafe* *Safe* means the crate and all of it’s dependencies (except the standard library) only contain safe code → green color *Unsafe Dependencies* means at least one (transitive) dependency contains unsafe code, but not the crate itself → yellow color *Trusted Unsafe Dependencies* means all dependencies with unsafe code were trusted (by an appropriate review process) → light green color *Unsafe* means the crate itself contains unsafe code → red color This motivates to put all unsafe stuff into small crates that can be trusted and to not use unsafe carelessly.
1. By default in say gpupg there has to be at least 3 redundant paths to a key, so no, just one isn't enough. So it's a bad WoT implementation if one compromised endorsement breaks the whole thing 2. The point of such a system isn't to guarantee that no one ever acts in bad faith, it's survive it. Let's look bigger bigger picture here: how long will a campaign of bullshit last when every action taken embeds all the information needed to roll the whole clique up? You invest days socially engineering endorsements to write a bullshit code review that is disputed and those endorsements go poof. The work / reward effort is completely asymmetric.
Thanks i will try it today at evening :) 
I'd be interested in comparison of this against [slotmap](https://www.reddit.com/r/rust/comments/8zkedd/slotmap_a_new_crate_for_storing_values_with/) Zero unsafe, `#forbid(unsafe_code)` and quickcheck... I cannot upvote this enough.
Are there any downsides to this approach? I don't think I see any? (It seems much nicer! I love the Own trait.)
Thanks for the advice!
This seems really nice. (I woould bikeshed the `Own` trait name, it seems kinda confusing / an important name, but I like it a lot.)
I'm making a CLI diceware password generator with options to generate a specified number of passwords saved to a text file. I know it's probably not the best idea to actually use generated passwords in this manner but, it's teaching me some core Rust concepts that I'm having some trouble understanding. (For those who haven't heard of diceware: [Diceware Password Generation](http://world.std.com/~reinhold/diceware.html)
Using lenses ergonomically also requires rank 2 types. I believe there was an article on Comonad.Reader which pointed out the possibility of lenses using only rank 1 types but I can't find it right now...
Thank you. This is what I have currently. The game client is done, now I just need to make the server so it can play multiplayer across LAN https://github.com/christiansakai/tictactoe_multiplayer_rust
Wouldn't returning `Pin&lt;Self&gt;` from `pinned` move the value though? Maybe it should be `Pin&lt;&amp;move Self&gt;` instead?
Oh, it looks like I was too eager to reply, and misread what the method actually does. It takes a `T` and returns a `Pin&lt;Box&lt;T&gt;&gt;` or similar, and I thought it took a `Box&lt;T&gt;` and returned a `Pin&lt;T&gt;` or something. Still, it looks like `&amp;move` could come in handy here somehow
See you soon!
I really like this idea, and `Pin&lt;Box&lt;T&gt;&gt;` and `Pin&lt;&amp;mut T&gt;` should work fine as method receivers (and even be object-safe). What was the problem with a wrapper type like this before? I know a wrapper around the *pointee* was a problem because it would require immoveable types, but I remember there being some reason why a wrapper around the pointer wouldn't work either.
Let's say in Yew, in our Component's `update` method we want to be able to do `let resp = self.query(ChildSlot::ChildA, ChildQuery::ChildA(child_a::Msg::Foo(..)));` or get a reference to a child by its slot id and then calling its own `update` method directly: `self.child(ChildSlot::ChildA).update(child_a::Msg::Foo(..));` Maybe we don't really need actual lenses. We just need a way to check at compile-time that the child Component's `Msg` type (`child_a::Msg` here) is used that corresponds to that `ChildSlot` (`ChildSlot::ChildA` here). `Component::child()` could return a `T: Component` chosen by the caller (then `update` can normally type-check against that `T`), so `Component::child()` could transmute/downcast the internally stored child to that `T` but in the Component's `view()` method we'd also have to use the same ChildSlot variant, e.g. `html! { &lt;MyChild #ChildSlot::ChildA: prop=val, /&gt; }`. So internally yew would store a mapping from childslots to children so that it can look up a child by its childslot and downcast it to the type that it's used with in `view()`. In this example, `MyChild` is instantiated with `#ChildSlot::ChildA` in `view()` so it will store that for lookup to allow us to get a reference to the child (with the type `MyChild`) by doing `self.child(ChildSlot::ChildA)`. What do you think of this approach? This doesn't statically prevent accidentally calling `self.child::&lt;MyOtherChild&gt;::(ChildSlot::ChildA)` (it would panic at runtime when we're trying to downcast to the wrong child type) but it'd be better than nothing.. 
I think calling it `Pin`, and naming its method `pin`, would make more sense, and be more consistent with the `Unpin` trait. The proposed `Pin&lt;P&gt;` type could be renamed to `Pinned&lt;P&gt;`, e.g. `Pinned&lt;Box&lt;T&gt;&gt;`
Some context can be found in a discussion from a year ago: https://www.reddit.com/r/rust/comments/6l9mpe/minor_rant_i_wish_rust_had_gone_with_square/
Why not just have a crates.io mirror that only includes 'trusted' crates? If you care about only using them, then don't use crates.io. Everyone else can just use crates.io.
I mean "truly initialized" to mean that the memory behind the T is actually storing data that constitutes a completely valid representation of a T. Of course then you have to define what a valid T is in the first place (is "uninitialized" a valid T?), but since we're moving away from mem::uninitialized toward MaybeUninitialized, it makes sense to forbid "uninitialized" as a possible value of a typical type (and instead it is a valid state of MaybeUninitialized). As for ManuallyDrop&lt;T&gt; specifically: Since ManuallyDrop&lt;T&gt; is a separate type from T, we get to pick what values are valid for it (ie what "truly initialised" means). One reasonable way to do this is to define its valid values as anything of type T plus a distinct post-drop state. (Of course, we don't want to have to represent this latter state in actual memory.) In that sense ManuallyDrop::drop doesn't do anything invalid on its own. It's only when a later call to `.into_inner()` or use through a deref comes along that something invalid actually happens. Of course, that's not very satisfying, since now checking validity is (in general, with the current API) either hard or trivial-but-awful. (Either we have to do some analysis to see that each use of the inner T is actually valid, or we have to disallow any use of the inner T.) So then we are left with the next best option: define valid values of ManuallyDrop&lt;T&gt; as being just all values of T (no uninitialized state). But now any call to ManuallyDrop::drop introduces invalidity to the program. ie, the guidelines proposed in this post simply forbid its use, even in unsafe code. My sense is that the meaning of unsafe in ManuallyDrop::drop is not compatible with Ralf's idea of "safety" (not validity) in the blog post. This means either dropping (pun intended) the safety/validity idea, or reworking the ManuallyDrop API (this might get easier with MaybeUninitialized). What I was leaning toward in my original post would require pursuing this latter option.
Attribute like.
This idea is useful on its own, but seems orthogonal to what OP proposed. Your idea won't necessarily prevent malicious code from being inserted. That is, it addresses *mis*\-management but not *maliciously* managed.
What are the patterns that really need this?
Rust libraries that link to C libraries instruct the compiler how to find and link each C library. gtk-rs [doesn't seem to support anything but dynamic linking](https://github.com/gtk-rs/sys/blob/cdc7f613c2ed1e4f021a2c83d6f51469edc03eb1/gdk-sys/build.rs#L62) at the moment, and would need some modification. Perhaps open an issue on their github? You'd likely have to build gtk from source also as I don't think the os `-dev` packages usually come with static libs.
How is that any different from other pointer types? Why can't you use other pointer types just as well? Thanks.
Futures, once polled, could become self referential, which is a desired property for futures to have. Rust has no move constructors one could use to adjust the pointer-like types to point to the new location, so instead there is a pin API to prevent moving. Types that will not become self referential can opt-out. I believe generators yielding references/something with a lifetime NEED to become self referential in order to yield references/something with a lifetime more than once.
This doesn't make sense to me, the method called own does not involve pinning &amp; there is not a 1:1 connection between pointers that implement `Own` and pointers that can be pinned (any pointer can be pinned. The safe API for borrowed pointers requires a macro)
Previous attempts tried to unify the constructors in ways that never worked out. It never occurred to anyone we could just not do that.
I like the general API but the different constructors are weird. Would it not make more sense to have a trait that is used by one constructor and does the right thing? It seems like a new pattern that a pointer has constructor functions that relate directly to other pointers. 
its not possible because different pointers have different sets of valid possible constructors based on their ownership semantics
Just commenting on hyperfine usage. Did you know that you can run hyperfine as: hyperfine './go' './rust' There's also warmup option, for example hyperfine --warmup 3 './go' './rust'
new_box is one that can be abstracted, which it is below in the `Own` trait (there it is written `Box::pinned` to avoid turbofishing with `Pin::&lt;Box&lt;_&gt;&gt;::new`). But the constructor from `Box&lt;T&gt;` to `Pin&lt;Box&lt;T&gt;&gt;` is only valid for box, `Rc&lt;T&gt; -&gt; Pin&lt;Box&lt;T&gt;&gt;` would be unsound. And the only way to construct a `Pin&lt;&amp;T&gt;` or `Pin&lt;&amp;mut T&gt;` is using stack pinning macros (not shown in this blog post, a variant is in the pin-utils crate), there's no safe function based API for constructing them.
I think we’re talking about different things. I’m asking why Pin has to call `Box::new` at all instead of just accepting a box. 
I see. It’s still a pattern that seems foreign to rust otherwise.
nice, it's working :D
How does dropping something put it in an invalid (not just unsafe) state?. Surely it can't by the definition of valid.
&gt; Types that will not become self referential can opt-out. I was wondering something about this: is it possible to accidentally opt-out for a type which will still be self-referential, or will the type-system prevent you from doing that ?
can you explain the difference between --run-every and --span? I set run every to once a minute but it doesn't change every minute.
If you want a deep dive, there is a long series of excellent and informative posts by /u/desiringmachines which details the motivation and development of this feature: 1. [Async/Await I: Self-Referential Structs](https://boats.gitlab.io/blog/post/2018-01-25-async-i-self-referential-structs/) 2. [Async/Await II: Narrowing the Scope of the Problem](https://boats.gitlab.io/blog/post/2018-01-30-async-ii-narrowing-the-scope/) 3. [Async/Await III: Moving Forward with Something Shippable](https://boats.gitlab.io/blog/post/2018-01-30-async-iii-moving-forward/) 4. [Async/Await IV: An Even Better Proposal](https://boats.gitlab.io/blog/post/2018-02-07-async-iv-an-even-better-proposal/) 5. [Async/Await V: Getting back to the futures](https://boats.gitlab.io/blog/post/2018-02-08-async-v-getting-back-to-the-futures/) 6. [Async/Await VI: 6 weeks of great progress](https://boats.gitlab.io/blog/post/2018-03-20-async-vi/) 7. [Async &amp; Await in Rust: a full proposal](https://boats.gitlab.io/blog/post/2018-04-06-async-await-final/) 8. [Async Methods I: generic associated types](https://boats.gitlab.io/blog/post/async-methods-i/) 9. [Async Methods II: object safety](https://boats.gitlab.io/blog/post/async-methods-ii/) The first post addresses your question. The tl;dr is that you can do it with Arc, etc, but it's pretty un-ergonomic and having to heap-allocate all of your data leads to unnecessary performance losses.
&gt; When we talked about safety, I explained that uninitialized data is not safe at i32. But is it valid? My gut feeling is that it should not be (i.e., validity should require that i32 be initialized to some value), but I cannot demonstrate any tangible benefit and there are good examples for allowing uninitialized data in a u8. Maybe your gut can be appeased with the clarification that there are multiple kinds of invalidity. But i have no idea what to call them now that all safety and validity related words have been used in your article :P. An uninitialized enum can be a valid bit pattern or an invalid bit pattern. The invalid pattern can branch the process into a 'normally not reachable' state. An uninitialized u8 can only ever be a valid bit pattern. and thus can not branch the process into a unforeseen abnormal state. 
Sorry, that sentence painted an incorrect picture of how stuff works. `Unpin` is an auto trait like `Send` and `Sync`. Some type `T: Unpin` can be moved after being pinned safely. From what I understand it is impossible, in safe rust, to create self referential values with just references because you would borrow the thing that owns both the field that will store the reference as well as the field that would be referenced. But that would borrow the owning value, so you can't use that value until you undid it. So you would need to opt in to unsafe rust, at which point you should realize what you're doing I guess. So, it kinda does discourage you? PS. I have not written any significant amount of rust code, so take all this with a not insignificant amount of salt.
&gt;Previous attempts tried to unify the constructors in ways that never worked out. It never occurred to anyone we could **just not do that.** I love when things can be made better by having just having less or not including something.
I will look into Halogen. Coming from Elm and wanting to give purescript a try , i was rather disappointing when i came across its todomvc example. But this looks like a decently thought out framework on first glance. And i share some of your thoughts on Yew. My biggest issue is that if you do everything that you just described , you will in effect have implemented a garbage collected dynamicly-typed? functional scripting language , and at that point why not use JS.
there's a high chance this is the issue. If multiple connections are opened at the same time, there will be a single readable event for `accept()`, but since it's called once, only one connection will be accepted.
Oh, thank you, it definitely looks like what I need.
I didn’t expect such a reaction. Why would it be a bad thing to avoid unsafe code? The goal is to motivate crate authors to put the necessary unsafe portions into separate crates that can be reviewed independently. Also, the point is not to stop using *red* crates, but to be aware that they do unsafe stuff.
yes yes you're right, only reviews, eyeballs and specifically crafted analysis can help for intentional and stealthy malicious intents. I was wondering about the next step : what to do when popular code is or has to be retracted from repos.
Yes I realized that after reading @dpc_pw response, it's not «a way to achieve» op proposal, but merely an attempt figure out what can be done to help code retraction from repos.
--span only has an effect when used with --mode=top or --mode=controversial. Currently, I download and set the first image which matches my query. This means that if you download the best picture of the day every minute (heaven-on-earth --mode=top --span=day --run-every="* * * * *") it will only change if the post with the most upvotes changed. I will add a --random flag soon which should "fix" this. In the meantime, you can use --mode=new.
[removed]
If you were a scented candle they'd have to call it Perfectly Imperfect (and it would smell like summer).
Couldn't we just add inherent impls to all the pointer types that need it? So it'd look something like this: impl&lt;T&gt; Box&lt;T&gt; { pub fn pinned(data: T) -&gt; Pin&lt;Box&lt;T&gt;&gt; { unsafe { Pin::new_unchecked(Box::new(data)) } } } impl&lt;T&gt; Rc&lt;T&gt; { pub fn pinned(data: T) -&gt; Pin&lt;Rc&lt;T&gt;&gt; { unsafe { Pin::new_unchecked(Rc::new(data)) } } } This way we don't need an additional trait, and custom pointer types can simply follow the same API. It doesn't seem like there's any need to be generic over different \`Own\` types. &amp;#x200B; I honestly barely understand the Pin API so there might be some obvious reason this doesn't work.
 &gt; A pointer implements Own if it takes ownership of the data and never moves it from that address again until it is destroyed, even if the pointer type is moved. This sounds exactly like `StableDeref` trait in the `owning_ref` crate. My `Areffic` in `reffers` is similar. And yes, it would be nice to have this in std, so these crates does not have to make their own version of it. 
wat 😅
I really like this approach, being generic over the pointer type itself makes things a lot more ergonomic. All I have to add is bikeshedding the name `Own`, can the trait work defined like this and let the implementer do some extra work to implement it? trait Pinned: Deref { fn pinned(data: Self::Target) -&gt; Pin&lt;Self&gt;; } In other words, what purpose does the `Own::own` method serve other than to allow to auto implement `Own::pinned`? Why not just let the implementer write `pinned` themselves? It doesn't look like much more code than implementing `own` and has the advantage that no unsafe is required (in the trait definition).
&gt; It never occurred to anyone we could just not do that. Sometimes you are in some deep that you don't realize that something that you want isn't something that you need. 
Yes it can work by convention, the main reason to define it in a trait is to allow code to be generic over the pointer type. If your users don't use your trait in constraints or as a trait object, then there's not much reason to use a trait except it can enforce conventions and improve discoverability at the cost of an extra import to use the trait. Basically a trade-off.
In this case, I think the trait only confuses. The name doesn't really tell you that much about what it's for, and the added trait makes it much harder to understand how the pieces fit together. I can't think of any reason to be generic over the different constructors. As far as enforcing the convention, the method signature here is so simple that there isn't a lot of risk of people getting this wrong.
Please if you know of any of these speedups let me know as I have hit a wall in my knowledge on how to make tokei faster. My usual method of doing flamegraphs of how much time is being spent where has been a lot less useful since rayon pollutes the graph to the point of being almost useless, and commenting out the those lines to make it single threaded takes a good bit of effort.
Yeah I'm not sold either on making a trait _just because the pattern is similar_. It feels a lot like premature abstraction. The trait doesn't look sufficiently motivated in the post: _'have the same shape after all.'_ In a reply below I've also done some bikeshedding and simplifying the trait name, but at that point it indeed because just a wrapper for the constructor.
[removed]
Before I started writing Rust, I was writing Python in Vim, so I never really understood why IDE users made such a fuss about completion. Now I'm writing Rust in Kakoune, with completion provided by RLS via kak-lsp (itself written in Rust!) and it is pretty great - intelligent method completion, showing method signatures, automatic formatting, compilation errors and warning showing up right there in the buffer... it's great. It's definitely not flawless; there are times when I type a variable name and hit "." and get completely generic completions (like "std" and "Option"), or times when completion vanishes entirely (like in examples in doc comments) but they're mostly annoying because normally it's great. If the criterion for "1.0" is "...a signal that the RLS is worth using for a majority of users." then it's definitely already there.
Glad to hear! By the way there is one type of issues I forgot to mention earlier, which is actually quite relevant for people looking to learn Rust: "candidate for mentoring": https://github.com/servo/servo/issues?q=is%3Aopen+is%3Aissue+label%3AE-candidate-for-mentoring
I disagree that youll learn more from python to rust than python to go, as both rust and go are very similar in a lot of ways over python. What you will learn from rust is a very different way of coding than youd learn from go. I would almost say the bridge between python to go is a lot easier than python to rust, given that youve got a quite complex memory management system in rust compared to go, while also having a very different style of language in both go and rust over python that youve to learn. Theres a lot of new and different concepts that exist in both go and rust that arent in python. Just my 2c though, its a very open for debate topic
Interesting. What are your thoughts on the recent release of Rustfmt 1.0 and RLS 1.0 for the Rust ecosystem? How do you think that will impact your overall strategy? Will HKT's be of any benefit in your campaigns? What about Constant Generics? Will that assist you in your endeavors?
You want /r/playrust
The most concerning result was that scc misreported the number of lines. I don't know if Go has the same code generation capabilities as Rust, I would say though to try to have a test suite similar to Tokei's ~~or just copy the tests directory~~ so that you can easily test those edge cases.
I think it is to give compilers more lee-way. My impression is that C compilers and the transitively the committee are experiencing lots of pressure for more and more performance without enough people trying to keep the language usable by programmers... frequently the same programmers that ask for more performance, of course. In Rust, raw pointers may be entirely invalid (as in dangling, or pointing to deallocated memory). Only references, and some *unsafe* `raw` pointer methods, make any assumptions about where the pointer points to.
Yeah I never know in which order to put these words in English... it just doesn't make sense to me.^^ Thanks for pointing this out.
/u/Thomasdezeeuw /u/geaal After trying several times i have found that i have to send response to the client to be able to process with another connection. Yes all above things are important as well so i have added them how ever so TCP kinda hangs on one connection till i kill that connection or send Ok, Error response to the client through TCP. Am i doing some thing wrong can any one explain why does server should respond to the client before moving to another one. I would suppose that just reading the data from the socket should be enough. &amp;#x200B; I am still new to Rust and way how TCP work sorry if it is a newbie question. Thank you very much for your help. 
Yes we are, because `mem::uninitialized` returns an uninitialized *value* which will be invalid for (almost) all types -- so according to any rules we can come up with, `mem::uninitialized` is impossible to use correctly. `MaybeUninit` is much better, and the question about references arises when you want to know when it is safe to call `MaybeUninit::get_mut` -- does it have to already be fully initialized, or is it okay so still have some uninitialized data in there and then write through that reference to initialize the data?
Thank you for your guidance! I will look into it.
Uninitialized data is very different from "unknown bit pattern". For example, in LLVM, `x ^ x` can have a result different than `0` if `x` is not initialized. So, just because every bit pattern is okay, does not mean uninitialized data is okay. I talked a bit about that in the section on uninitialized data in a [previous blog post](https://www.ralfj.de/blog/2018/07/24/pointers-and-bytes.html).
lol
Huh, if you consider RLS pretty great you would loss your shit with actually decent autocompletion. 
I have a question about this code snippet: impl&lt;P, T&gt; Deref for Pin&lt;P&gt; where P: Deref&lt;Target = T&gt;, { type Target = T; ... } Pardon any syntax errors in this code, but does something like this work instead? impl&lt;P&gt; Deref for Pin&lt;P&gt; where P : Deref, { type Target = Deref&lt;P&gt;::Target; ... } And if not, why not? What are the advantages/disadvantages of parametrizing the impl over the additional type?
Did you compare the speed difference between the rust and python versions of your app?
&gt; It works with Python, but some things are a bit laggy. I trust you've already profiled your existing codebase? I've been coding Python for over 15 years, I have quite a bit of experience with PyGTK (the precursor to PyGObject) and PyQt, and it's been my experience that, when writing something like your project, the hot code tends to already be in compiled extensions, with performance issues being architectural or algorithmic stuff that would be a problem no matter what language you are using. ...not that I'm against people migrating Python stuff to Rust. One of the projects I have on hold began as a Python prototype and I'm now rewriting it to use Rust&lt;-&gt;rust-cpython&lt;-&gt;PyQt for the final version so the backend can take advantage of Rust's type system for better maintainability.
If I had to pick a trait name, I'd call it `Pinnable`.
&gt; mathematically equivalent Only in total orders; Rust has a `PartialOrd` trait for a reason. How about using "qualified brackets": `($` and `$)`, or `[%` `%]`, or some such?
A quick search on [crates.io](https://crates.io) for 'imap' did not turn up much more than what you listed, so I assume not much is happening there. I'm not familiar with the IMAP protocol, but as far as I can tell from a quick look at the \`imap\` crate, it's quality seems to be good enough to build upon. I assume you're right about marking the mails on the server, but usually the author of a crate will accepts patches for such changes. IDLE support seems to be present using the \`idle()\` method of the client. In general, I would really love to see an E-Mail client written in \`gtk-rs\`. I like geary, but it's missing gpg support, which would be pretty easy to add to a rust-based program once the \`sequoia\` project has reached a certain maturity. Another thing that I would love to see, is support for [jmap.io](https://jmap.io). If I were to write an E-Mail client, I would start with JMAP, because it's trivial to handle (HTTP requests, which is a solved problem, e.g. through the \`request\` crate, and JSON, which is easy with \`serde\_json\`). Later I would extend to IMAP support, either by building IMAP support directly into the application, or by including a JMAP→IMAP bridge (if that's possible).
I tried your version on the playground and it does not compile. The error isn't terrible useful however. I don't claim to *know* the reason why, but I can imagine that only the information up to the `{` is significantly easier to implement than also having to look into the block and teaching the compiler how to infer the missing formation from it.
Nope. But definitely planning to someday.
Keep up the good work! I think there is a silent majority which would be happy to see a 1.0 exactly for the reason you say. The tool is pretty stable, widely adopted and actively maintained, that's more than enough IMO for a 1.0. &amp;#x200B; &amp;#x200B; &amp;#x200B;
Hey, I just did a cargo clean and rebuild of both of my backend and frontend (both in the same workspace), their combined `target` dir is now 1.69 GB (size on disk: 0.97 GB). Btw, the backend was built in debug mode (took 12m 35s) and the frontend in release. If I also build the backend in release, or run `cargo check` it will take some more space (but not that much more, I think).
If you want to parse them as types then yes, there will be ambiguity, but you can parse them as “identifiers” (whatever these will be). In the same manner Rust needs to handle `foo()` where `foo` can be function call or variable function reference call. 
Yeah, pretty much disagree. 1.0 let's people know it is a tool that is ready to start being used. It will be updateable via Rustup and on the "Stable" channel and can receive regular updates and improvements. Also, it can now get feedback from wider usage which will driver forward the improvements.
Yup! I've been happy using it, and thankful for the team's hard work, /u/nick29581's in particular! Worth a 1.0 imo, even if it still needs some improvement.
Hey sushi! You aren't trying to push your own packages now are you? ;)
But it's *not* ready to be used because the user experience is awful (really, it is not good). Tooling is one of the first things people encounter when dealing with a new language, and if the first thing they use is RLS they're gonna run screaming from the language. It's a bad look. And I'd argue that your definition of 1.0 is more likely the definition of 0.1. The futures library is "ready to be used, but not production ready", so it is pre 1.0. I don't know why RLS should be different.
Really, really do not think RLS is anywhere near ready for 1.0. I think this is a bad decision and it will make Rust the language look bad. Please don't do it.
Not saying RLS is flawless or not in need of improvement, but I think a lot of people don't consider that VSCode and the fact that the RLS VSCode plugin is (and has to be) written in JavaScript are themselves a significant part of the bottleneck. On top of that the way LSP works is just not particularly efficient by design... you can only make piping JSON back and forth so fast. On top of that though the non-realtime nature of RLS is certainly less than ideal. I don't think something that relies on continuously getting output from rustc and re-indexing everything will ever be as performant as most people would like. A tool that actually parsed the Rust source files on disk in real time and just sent back the symbol results would be much faster.
dropped a super beginner question on StackOverflow and would be very nice to get some response here or over there: [https://stackoverflow.com/questions/51986710/destructing-array-elements-without-copy](https://stackoverflow.com/questions/51986710/destructing-array-elements-without-copy) &amp;#x200B; Thanks!
Yeah, like C# + Visual Studio, where everything actually works every time, very quickly. It is nice! But a ton of work. &amp;#x200B;
\&gt; RLS VSCode plugin is (and has to be) written in JavaScript are themselves a significant part of the bottleneck &amp;#x200B; Do you know this to be true or are you assuming it? The F# plugin uses a language server and doesn't peg my cpu ever. &amp;#x200B;
&gt; certainly if we started over I would do some things differently Could you elaborate on that? What would be some changes you'd make? I may end up writing a language server for something else and having advice on potential pitfalls would be very useful.
Not quite, I'm not sure if I want to add another layer of complexity by using wasm, though that's only because I'm not familiar with its limitations vs running js directly. At the very least, this awesome crate seems to have solved the problem that I'm having, so I'll definitely check it out. Thanks.
What's the easiest way to use the RLS from spacemacs ?
Are you happy with kakoune ?
The Rust "convention" is for single-method Traits to have the the same name as the method they declare. Coming from Java, I'm often tempted by the -able suffix myself. But then I remember this leads to things like "ExtendedPinnableRemoteFactory" :)
What are those \`.so\` files you mention in the context of having multiple versions of a single crate? Were you talking about DLL hell only in the context of crates using FFI stuff?
That's a pretty cool crate Nick! I got slightly confused (not for long though) because I thought it was somehow related to [https://crates.io/crates/indexing](https://crates.io/crates/indexing), which uses "generativity". :)
To counter this. (many?) other LSP implementations that work with vscode work as expected, fast and with mostly 100% code completion. I don't have experience with a lot of them, but the TypeScript one (obviously) and the Go one work really great out of the box. &amp;#x200B; Although, to be fair, the go LSP is also still experimental, and it uses a lot of third party tooling to work as good as it does. But, Go also has a very fast compiler, which makes this type of tooling a lot easier to implement. &amp;#x200B; All in all, you might be able to "blame" 10% of the problems with RLS+vscode on vscode or LSP itself, but mostly it's just the fact that we haven't had enough time/contributions/compiler support to get RLS to a higher level.
On one hand, it's got a bunch of rough edges, the ecosystem is immature (especially compared to Vim) and it's kind of expected that everybody that uses it builds it from source regularly. On the other hand, I never want to use an editor without multiple cursors ever again, the smooth gradient from "fully manual" to "fully automated" editing is so powerful, and the editing language is so much more comfortable and sensible than Vim's (and this from somebody who had used Vim for nearly 20 years before switching). I wouldn't say I'm *wholly* happy with Kakoune, but I can't see myself using anything else in the near future. Maybe if somebody figures out a way to make the regex crate work on ropes instead of flat byte buffers, I'll try to cobble together something on top of Xi...
\`async\` functions will be able to return self-referential values in safe Rust. The resulting types will not implement \`Unpin\`.
I don't want to beat up on the RLS or nrc so I'll talk about something else: &gt; The Rust roadmap has 'a high quality IDE experience' as a key goal, and that is what I want to deliver, regardless of the version number. Leaving aside the question of whether or not the current state of RLS "deserves" a 1.0, I think there's a bigger issue with "delivering a high quality IDE experience". I'm trying to remember the last time Visual Studio (for C#) failed to give an Intellisense completion. It was a few weeks ago when I was running multiple VMs on my laptop, ran out of memory, and a VS helper processes died. Closing and reopening VS fixed the issue and I got my completions again. Over the last 10 years I've been a C# developer, I can count on one hand the number of times Intellisense failed in Visual Studio. I don't really do a lot of Java but I've dabbled a bit in both Eclipse and JetBrains IDEA and the same thing is true there. Anytime I press Ctrl+Space I get high-quality, correct completion instantly. As I said earlier, I don't want to beat up the RLS. It's a useful tool and it's very encouraging to see the progress the tool has made in the last year. But if the goal is "high quality IDE experience", the RLS isn't there yet. 
I haven't really been following the pinning topic, but I've always wondered why Rust opted for `mut` instead of `mut mov`, where: * `&lt;nothing&gt;`: immutable and unmovable * `mut`: mutable but unmovable * `mov`: immutable but movable (not sure what would be a use-case for this) * `mut mov`: mutable and movable And so I have questions about this pinning stuff in general: * Is `Pin` a trait or some struct/enum? * Does `Pin`ning work on structs/enums or on instances? * Does a `Pin`ned struct/enum still have the possibility to be mutated? Does pin mean "mutable but unmovable"?
&gt;But it's &gt; &gt;not &gt; &gt; ready to be used because the user experience is not good (yet). Let us not let the perfect be the enemy of the good.
Funny you mention JMAP. After looking into the IMAP protocol I really wish more mail providers would support JMAP. IMAP is just a horribly outdated protocol. Unfortunately none of the large providers provide JMAP. But I agree that it will definitely/hopefully be the future. And because you mentioned geary. Geary is close to what I want. I actually want to build a even more minimal dotfile based mail client. However I find autocrypt to be very interesting/more interesting than gpg.
I noticed the same thing. The guy that does the most work doesn't have write access to crates.io and the original author has vanished it seems. Thanks for your link to tokio-imap. What is the benefit of having the async built into the library compared to actually putting it into the actual app? I need some sort of concurrency anyway, because I don't want my GUI to freeze when checking emails for example. (I'm not asking because I criticise, but because I really want to know)
I am currently working through the "A Tour of Go" introduction/course. And so far it's quite similar to Python. The Syntax is different obviously, but there's nothing I really need to wrap my head around before I can really understand it. Rust however seems quite a lot more difficult at least for me. Even simple stuff like String handling is ridiculous compared to Python. Python and Go are almost the same in that regard.
You are trying to send 65508 bytes, which is bigger than maximum UDP payload size. `std::ned::UDPSocket` returns an error, while `socket2` looks like simply truncates the message.
`polyglot` uses `memchr`, which is why it's the fastest on a small number of cores. But one could conceivably do the counting with SIMD as well as the searching, so there's room for improvement.
&gt; (and maybe-Rust-too-but-we're-not-sure-yet?) Wait, what? I thought that *constructing* pointers was always safe? Isnt it in the book/docs, plus you don't need `unsafe` for it?
Thanks so much for your experience/input. I totally agree with some things you noticed yourself as well. I also started using Python Queue for that project and it was a very good enhancement for my project but also for me as a programmer. However there are still little things. The startup for example. It's very fast with PyGObject already, only takes about 100-200ms or something like that. But with Rust this happens almost instantly. (Which is not a surpirse because obviously the Python interpreter needs to start/initialize first) Also I'd really like to learn a compiled language, so that is a big reason for wanting to port.
Thank you, you are right, at some point I seem to have replaced: `String::from_utf8(buffer[..bytes_read].to_vec()).unwrap()` for `String::from_utf8(buffer.to_vec()).unwrap()` 
I've been using slotmap for a while now. It's fantastic. This looks vey similar.
&gt; Believe me, I've noticed that tokei is more accurate than its competitors Unless you want to count lines of code in a Verilog file...
VS Code + RLS is the default for all of my personal rust projects. I think a long 1.0 RC period would be the best course to take. It would make it a lot easier to prioritize features and discover new features that haven't been considered before based on community feedback. After testing the waters a little and knocking off the high pri features, hopefully the community would be confident in using it as part of their dev workflow.
cool, you can copy paste that into your readme :P . Nice little project so far though.
For anyone else reading this thread and not wanting to go look in Vec source code: Vec::clear just sets Vec.len to 0 and does nothing else, when the stored types are not Drop It will run the destructor on all live elements if you're storing Drop types though
Problem with completeness is that it is arbitrary. What is a complete feature-set for me might not be for you
This wouldn't have happened had it been written in Rust. Another point scored for Rust!
Actually, having bugs is A Good Thing For Rust.
I'm not sure what your example is, but it works just fine for me: [playground](https://play.rust-lang.org/?gist=25e8829fdbf0957fc32e4f9aa8d34f2a&amp;version=stable&amp;mode=debug&amp;edition=2015).
Im sorry.. I saw the word "RUST" so I thought it was about Rusted cars and Rusted Metal.. Very sorry for intruding... 
A good alternative I replied below is to drop the `own` helper and just have the implementer implement `pinned` directly, then the name of the trait can be `Pinned`. Of course there is the question whether we _need_ to abstract over the creation of pinned references, the blog post doesn't motivate this choice sufficiently, merely stating that there is a common interface: _'[they] have the same shape after all.'_.
This is exactly what Go was designed for, ie. to not have to wrap your head around anything. You can learn the language in a day and be productive in the first week. It's a boring language on purpose. Which is why I assume you're going for Rust because there's more interesting things to learn, and will probably result in better productivity in the long term.
Gah! I tried `-&gt; Pin&lt;&amp;Self::Target&gt;` and variants. Thank you.
I'm working on RLS, so I might give a bit more insight about performance (and related issues). In terms of general architecture, the idea of piping JSON back and forth sounds... not optimal, but in reality this doesn't seem to contribute to increased latency and does not prove to be a bottleneck. In VSCode, virtually every keystroke is sent as an LSP message (to sync buffer state) and even formatting an entire document, which basically sends the entire buffer contents and receives and updates the buffer (yes, this will need streaming eventually) via LSP does not feel sluggish at all. (Also VSCode JS extension takes negligible amount of time, as well) Because of the current RLS architecture, the biggest bottleneck is the compiler. What RLS does is it basically invokes the appropriate \`rustc\` commands for every available package target in the active Cargo package/workspace. By doing so, the RLS can just retrieve the save-analysis data from the compiler in-memory (in fact, we mostly do only deserialize save-analysis JSON files for external deps once, during start-up), so there's no deserialization overhead. However, imagine you're working on a simple bin+lib package with unit tests in both and edit a src/lib.rs or similar. If you modify and potentially invalidate the library target by doing so, every crate target out of these 4 (checking unit tests is a separate target) must be rechecked, and so RLS errs on the safe side and performs 4 \`rustc\` invocations, even if the only thing you did is press a spacebar. Until the entire compiler model transitions to \[demand-driven\]([https://rust-lang-nursery.github.io/rustc-guide/query.html](https://rust-lang-nursery.github.io/rustc-guide/query.html)) model we're at the mercy of incremental compilation to speed up compilation for some less relevant code changes. A lot of people desired RLS features in unit tests, but if you prefer to decrease latency for the 'regular' development, you could fine-tune for performance and set \`"rust.all\_targets": false\`, \`"rust.cfg\_test": false\`, \`"rust.build\_on\_save": true\` and change \`"rust.wait\_to\_build"\` delay in settings (these are VSCode settings, more info on the LSP config over at [https://github.com/rust-lang-nursery/rls#configuration](https://github.com/rust-lang-nursery/rls#configuration)). As a closing note, I do realize that RLS itself needs more work and there are definitely a lot of things that need improvement and so I don't mean to just ignore that and blame it on the compiler, but rather give more context on why things are the way they are currently and why efficient, semantic completion as a part of 1.0 milestone is still far away. (Fear not - it seems that nrc and kngwyu are trying to come up with a way to enhance racer with save-analysis, which could substantially improve code completion!)
 I took a look at your blog post there. It's pretty tempting I won't lie. I'm happy with my remote job though. Are you guys planning on open sourcing some of that work or is it all closed? 
Have you checked crates.io? 
What do you mean by "hangs"? Can you show full source?
Is matklad's [libsyntax2](https://github.com/matklad/libsyntax2) worth looking at for any kind of integration? Is has it's own experimental VS Code plugin already which while not doing a ton is extremely fast.
I think there's just a lot of tiny-but-significant issues that need to get worked out in various areas before a 1.0 release. For example, Racer currently will always incorrectly claim that the paths for environment variables such as RUST_SRC_PATH don't exist when running on Windows even when they do, because it's written in such a way that it can only parse Unix-style directory-slashes.
How do you know whether it is fast? I ... don't think anybody, except me, is using it? :) 
There’s something called https://github.com/sfackler/cargo-tree but not quite granular enough.
Well, I guess there's two of us then! I just thought I'd try it out. While you're here: one reoccurring issue I've noticed with it so far is that if it encounters a function with parameters beginning like `(self: &amp;Self,`, it incorrectly says the lowercase self is an error and that it was expecting a comma.
Such a great idea / cool hack! Do you have plans to keep refining it until it would be usable in production?
I'll be honest and say I didn't dig into it, but introducing a common AST abstraction that can be used to directly communicate with the compiler would definitely speed things up, at least from what I understand. Instead of completely rebuilding from scratch on entering random characters, the AST delta (instead of text buffer delta) could be then sent to the compiler to reanalyze and possibly rebuild.
Just for clarity what I’m looking to do is reduce compilation of a multi project repo by figuring out if files have changed. I’m looking for a tool that can help me find what files from projects I use. Not so much interested in cargo deps as local deps.
\&gt; But if the goal is "high quality IDE experience", the RLS isn't there yet. True! And releasing a 1.0.0 version will give people the impression that it is ready to be used. Rust should give stable and high quality experiences, I think RLS is not ready for this.
They are an analogy. In Linux, you have one OS, two applications using the same dependency, but two different versions of the same dependency, because one application was not updated. The result is slight bloat and incompatibility by having two installs of the same library in different versions. In Rust, you have your crate *("OS")*, which directly depends on two other crates *("applications")*, which need two distinct versions of the same indirect dependency *(the "DLL")*. The result is slight bloat and incompatibility. So I used the term DLL hell as a name for a specific dependency graph shape, not as in *"there are DLLs/SOs involved in the crates"*.
Anybody know of a good (street, business, mailing) address parsing library? I tried looking on [crates.io](https://crates.io) but mostly just found ip and email stuff.
Thank you for your detailed answer! I have no doubt that none of the LSP/vscode/javascript/JSON are to blame for the current situation. Considering that we're only aiming for human reaction time, they seem like reasonable, practical choices. I tried the haskell hie recently and it worked quite well, others have commented on other language servers, so I'm not worried about that. The recompilation of all targets is indeed one of the things I was hinting at with "It seems to be doing a lot of unnecessary work.". It seemed to me that the interface of my library remaining unchanged, there should be no need to recheck the main bin. And likewise for modules, or even types. My understanding was that if a type changed, any module were it does not appear shouldn't be concerned. Unfortunately, it seems you are confirming what I feared: the rls performance is tied to the compiler's behavior and won't be solved until the compiler is finished rearchitecturing. That's ...sad. And nobody's fault, really. I've know about those efforts for a while, and I've been looking forward to the results, but I'm unsure how advanced those efforts are. That's one of those things that don't get a lot of publicity, unfortunately.
Well, mathematically speaking, a function is just a mapping between elements of its domain to elements of its range, so a `Map` is just a function where you explicitly specify what the return values are for each individual input parameter value...
The suggestions you wrote make the rls much, much faster (considering I don't need unit tests). Thanks alot!
&gt;While you're here: Fixed in https://github.com/matklad/libsyntax2/commit/a077533513de5018e227c740a470e18652b63172, thanks for the report! &gt; I just thought I'd try it out. Be sure to check [extend selection](https://github.com/matklad/libsyntax2/blob/f3e1e6df4202a20fa75543f6847782be515dae06/code/package.json#L60-L64) command! It's like VS Code's expand select, but it actually works correctly with Rust syntax, and is multi-cursor aware. 
This is why it's important to layout the desired features and requirements ahead of time. Had the RLS team defined a 1.0 release last year as having less crashes, with no mention of code completion, expectations would be different here - though naturally, there would be the same discussions as are happening now. 
The plug-in could be written in rust and compiled to asm.js 
Yeah "redefined" is a bad word for this, you're right. SemVer doesn't really define criteria for 1.0.0 in the first place. I should have said something like "The Rust community instead uses it to signify a stable release". On the other hand, people around here *do* use 1.0.0 meaning production ready for PR stuff, but fall back to the technical definition when their release is questioned, which is the case here as well with the part about great IDE support.
This is neat. I suspect there are some easy performance enhancements you can make. You are making a couple calls to `read` that you could potentially eliminate. Rather than calling `read` for each byte at a time - for both finding the end of a line and the start of a line - why not just read some N sized chunk to try to find the start and end of the line? If the start and end are within that chunk of data, you do not have to call `seek` and `read` again, since you already have the entire line's worth of data. 
Yep. And therefore it makes sense to implement `Index` and `IndexMut` for a `Map` and call it with `map(key)`.
I've been playing around with `std::mem::replace()` and I'm confused by something. So we've all tried implementing a singly-linked list before and did something like this. let new_node = Node { data: element, next: self.head.take(), }; self.head = Some(Box::new(new_node)); And to test my knowledge, I've been expanding it in different forms. I've read that `self.head.take` is just shorthand for std::mem::replace(&amp;mut self.head, None) And it works. So far so good. I even tried to expand it to the following and it still works: let mut new_node = Node { data: element, next: None, }; new_node.next = std::mem::replace(&amp;mut self.head, None); self.head = Some(Box::new(new_node)); But the problem arises when I try to change it to the following: let mut new_node = Node { data: element, next: None, }; new_node.next = std::mem::replace(&amp;mut self.head, Some(Box::new(new_node))); and it **doesn't work** all of a sudden. In my head all these code samples should be equivalent, but somehow the last one fails all my tests. I would love some explanation or enlightenment as to what's going on here?
&gt; The startup for example. It's very fast with PyGObject already, only takes about 100-200ms or something like that. But with Rust this happens almost instantly. &gt; &gt; Also I'd really like to learn a compiled language, so that is a big reason for wanting to port. \*chuckle\* Those are actually two of the reasons I started learning Rust... and why I use it for writing little command-line utilities.
&gt; Fixed Whoa, that was fast! &gt; Be sure to check extend selection command I will.
Very cool! I would love to use this when it’s a little more stable!
C++?
Doesn't cargo with incremental compilation already do this for you?
No, not really.
I don't know the exact implications of "transitive trust" but if that means `I trust X and X trusts Y, therefore I trust Y` this is simply not true for me. I love your idea but this would entirely defeat the purpose. If I trust some person to do some reviews, I trust them with their **technical** skill to judge code. That does not mean that I trust their **social** skill to judge people. They are two separate things and a very common misconception even in everyday life with secrets. My best example is this: "I trust *you*, but I do not trust your ability of judging other peoples trustability, therefore please do not share this secret with **anyone** (or better still, I will not share it with you)." Please don't mix them up. Please. Make it optional, if at all, and then add a `depth` field which you can specify to not trust too deep. Thank you!
&gt; so according to any rules we can come up with, mem::uninitialized is impossible to use correctly If the standard library has a function which "used to work" and then it is changed so that it "never works" [correctly], doesn't this amount to a hard backwards compatibility break? I do agree that it would be *good* to do this, and that from a practical perspective we hopefully *could* do it by incrementally moving existing users over to `MaybeUninit` instead and so on, but from a de jure perspective, are we "allowed" to do it? This isn't some irrelevant edge case like `for` loops inside static array sizes or whatever. (I guess it could be justified under the "fixing soundness bugs is always allowed" rule, where we "fix the bug" by deprecating/removing the function, but... we would in a sense be *causing* the unsoundness by the act of defining the rules...)
I vaguely recall reading that on some old mainframe architectures, even loading an invalid pointer into an index register would cause a hardware exception. I don't have any solid info on that though, unfortunately. 
&gt; `self: &amp;Self` Out of interest, when is this syntax useful? Or is it just a personal preference thing? 
I'm also a relative beginner in rust so take what I say with a grain of salt. I don't know anything about rustc internals but I have run into the difference between the two many times in my own code. In the case of `Deref` it doesn't make much of a difference but when writing highly generic code that takes closures and returns an unknown type (for example), adding a type parameter instead of the associated type drastically changes how Rust type inference works. Essentially, type parameters are *inputs* and associated types are *outputs* so how the compiler decides what each one resolves to is different. When you use a type parameter, Rust tries to infer the type based on the call site (from the closure the user passes in, in the aforementioned example) instead of trying to infer the `Fn/Mut/Once::Output` using the impl bounds and a pool of possible implementations. In the latter case, Rust often doesn't have enough information to decide on one implementation so while the code may compile, using it often requires type annotations when trying to actually use the APIs downstream. It's especially important if the type in question has be inferred in the middle of a chain of traits, like when you take a closure, do something with it using other type parameters/associated types, and then return a new type. I'm probably getting a lot wrong here but that is what I've observed. For example, in the below snippet I use the type parameter O2 as the output type of the F closure (yes, this is unfortunately real code): ```rust impl&lt;H, F, P, O1, O2&gt; Init&lt;F, P&gt; for Builder&lt;H, O1&gt; where H: DebugPath, F: Clone + FnOnce(Builder&lt;H, HNil&gt;) -&gt; O2, O2: AsChild&lt;P&gt;, O1: Add&lt;&lt;O2 as AsChild&lt;P&gt;&gt;::Output&gt;, { type Output = &lt;O1 as Add&lt;&lt;O2 as AsChild&lt;P&gt;&gt;::Output&gt;&gt;::Output; fn init(self, func: F) -&gt; Self::Output { let Builder { path, data } = self; let data = data; let new_data = func(Builder { path, data: HNil }); let child = new_data.as_child(); data + child } } ``` Changing the trait bounds to the code below breaks type inference and forces the library consumer to provide their own type annotations. Since a type might Look like `HCons&lt;T1, HCons&lt;T2, HCons&lt;T3, ... HCons&lt;T50, HNil&gt; ... &gt;&gt;&gt;`, it makes the API practically unusable: ```rust impl&lt;H, F, P, O1&gt; Init&lt;F, P&gt; for Builder&lt;H, O1&gt; where H: DebugPath, F: Clone + FnOnce&lt;(Builder&lt;H, HNil&gt;,)&gt;, F::Output: AsChild&lt;P&gt;, O1: Add&lt;&lt;&lt;F as FnOnce&lt;(Builder&lt;H, HNil&gt;,)&gt;&gt;::Output as AsChild&lt;P&gt;&gt;::Output&gt;, ``` When writing complex code with many levels of associated types, it also makes the resulting bounds even more unreadable so often times I end up using a type parameter if I expect to place trait bounds on the associated types. Writing out `O1: ..&lt;Output=O2&gt;, O2: ...&lt;Output=O3&gt;, O3: ...&lt;Output=O4&gt;, O4: ...&lt;Output=O5&gt;, ...` is much cleaner than chaining together all the `&lt;&lt;&lt;O1 as ...::Output&gt; as ...::Output&gt; as ....&gt;::Output`.
High quality ide experience only means vscode apparently 
Define "doesn't work".
And here I thought the Rust community had a reputation for being friendly? It seems it's not true. There's no error message. It doesn't behave the same way. You know the way the previous examples worked? The last one isn't the same, even if it should. Did you even read the whole post?
I hope you not only show what's possible with WASM, but you also mention how hard it is to implement secure protocols manually. Some things I realized while glancing over your code: - Don't use `Math.random`, use [`Crypto.getRandomValues()`](https://developer.mozilla.org/en-US/docs/Web/API/Crypto/getRandomValues) - Your keys have 512 Bits, which is not secure for RSA keys. - You encrypt every character on its own, which wastes computational power. - You do not use padding, which makes this insecure (https://crypto.stackexchange.com/questions/1448/definition-of-textbook-rsa). - Combine the two points above and you can also just do statistical analysis. - Your `e` is random, which is unnecessary. Pick a well-known value, it's public anyways.
There's this post from a few months ago, which isn't super helpful: https://www.reddit.com/r/rust/comments/8cxr86/mailing_address_library_for_rust/ Maybe /u/kodemizer ended up writing a rust address library? You could also wrap libpostal as suggested, the API looks pretty clean.
Glad I could help, however, remember that grain of salt :) &gt; seems like generally in Rust you want to specify incoming associated types as trait bounds That might be too strong a statement. If you take a look at the rest of the [library](https://github.com/akiselev/vtable-rs/blob/master/core/src/builder.rs), all of the structs involved are very generic and the closure output type is used in the middle of the chain of trait bounds - the return type is "hoisted" and added to `Self` before being returned. There is an explosion of possible implementations and by using a type parameter, I'm telling Rust to look only at implementations that makes sense in the context of the callsite. Most of the time, this isn't really a concern unless you're dealing with several associated types and complex relationships between them in a single implementation. For a concrete example of how I use the builder: let builder = Builder::new() // Builder&lt;HNil, HNil&gt; let builder = builder.init::&lt;Path1, _&gt;(|builder| { builder.init::&lt;Path2, _&gt;(|builder| { // builder.add_prop::&lt;Prop1, _&gt;(|| Vec::new()) .add_prop::&lt;Prop2, _&gt;(|| 43.0f32) }).add_prop::&lt;Prop3, _&gt;(|| 32u32) }); builder.some_other_trait_fn(); If the `impl Init` uses a type parameter, Rust knows that the return type must implement `some_other_trait_fn()` which is a huge hint for type inference to cull the number of possible implementations down to those that implement a trait you've explicitly added to your context (with `use ...`). Without that type parameter, `some_other_trait_fn()` doesn't really help because Rust searches for its implementation after it has been able to resolve the `Builder::init()` call, which might be impossible without a type annotation.
I think its a subset of stablederef in owning-ref because I think stablederef includes `Vec` and `String`, which can realloc internally. Not sure I'm remembering right.
Programs are already breaking because of `mem::uninitialized`, see [here for an example in libstd](https://github.com/rust-lang/rust/issues/48493). We're less breaking them than documenting that they are broken.^^ But I get your point. We certainly encouraged people to use `mem::uninitialized()` and there was (is, in stable!) no alternative. We can't put the blame on programmers. We have to find a reasonable way forward. For example, the PR that introduces `MaybeUninit` also makes `mem::uninitialized()` panic when it is called on an uninhabited type -- downgrading UB to program termination. The more similar measures we can come up with, the better.
Also look into sccache
Why don’t we do a 0.1 release, and do a 1.0 release when it is actually comparable with C# or Java, Kotlin, etc ?
I have the opinion that the Rust community is too tentative about marking something as 1.0.0. I'm not even sure semantic versioning makes a ton of sense for a project like RLS. Its public API is an already versioned language server protocol and extensions. As highlighted in the post. Development won't stop. If we truly are facing incompatibilities we would have to deal with them in pre 1.0 in the exact same way as we would have for post 1.0. The difference is public perception on how stable they expect the project to be and which location to communicate the "major version" through. There are exceptions to this. Most notably APIs. It takes a lot of effort to change a widely depended in API in any meaningful way. All in all I'm personally much happier to see version 42.0.0 of a project over 0.42.0.
For what it's worth, I've encountered rustc's `unconstrained type parameter` errors many times when trying to move associated types to type parameters in implementations with nontrivial bounds. I think Rust handles inference on associated types much more conservatively than Haskell does, especially since it doesn't yet handle generic associated types.
A promising idea, but I won't replace setuptools-rust for me until it has the following two things: 1. A well-documented way to configure it for rust-cpython use. (Ideally, one which doesn't require that a command-line argument be passed every single time it's invoked.) 2. An equivalent to setuptools-rust's integration with `./setup.py develop` and `./setup.py check`.
Can't we have those settings as default?
A nice, simple parser and AST you got there. That has value on its own. An alternative to LLVM in the JIT is CraneLift. Not sure the speed of compilation diff but I know CraneLift is built to support the JIT use case. Probably not much value in switching atm.
When you build, dependecy information is listed in `target/&lt;target&gt;/deps/*.d`, in a format that can be used by Make, Cargo, or other tools to do incremental rebuilding.
That's not set in stone yet. The rust reference explicitly state that &amp;T always points to an initialized T. It doesn't distinguish between safe and unsafe Rust. Its unclear of whether RalfJ model will require for references to be valid at function boundaries in unsafe code, meaning that this wouldn't work if that were the case. They defintely need to be valid at function boundaries with safe Rust code.
I like the trait because of the composition and because it can reduce a little bit of repetition as well as neatly encapsulate the individual concepts in play. In fact, I might argue to separate `pinned` into a third trait: `Pinned` or `OwnedPin` with a blanket `impl &lt;P: Own&gt; Pinned for P`. This way a type like `Arc` or `Rc` or `Box` only says "I own and will not move data" with `unsafe impl Own` (as this is the property that the pointer types care about), `Pinned` can create a `Pin` for any `Own` (which is the core operation here, but expressed directly), and then trait that you have to import to use that core operation `Rc::pinned` is `Pinned`, which is a little clearer to me compared to importing `Own` to use `pinned`.
PyO3 is really cool. Too bad I can't actually use it because the code is so wildly unsafe and poorly maintained that I don't even have to fuzz it to get exploits, they're [right](https://github.com/PyO3/pyo3/issues/71) [there](https://github.com/PyO3/pyo3/issues/94) [on the](https://github.com/PyO3/pyo3/issues/159) [bug](https://github.com/PyO3/pyo3/issues/193) [tracker](https://github.com/PyO3/pyo3/issues/198).
I haven’t written kid yet. It’s on my todo list. ;) 
tangentially i always joked that apple Swift was 'objective-rust' , for having some rust-inspired features and being compatible with objective c libraries
Just a quick follow-up: I added [`join_concat`](https://docs.rs/joinery/1.1.2/joinery/trait.Joinable.html#method.join_concat) for doing your joins more efficiently!
In my experience, C++ compilers are **much** slower than rustc. I’m talking 16 minute compiles on a developer laptop for a project of medium size but a significant amount of template code. And anywhere from 1-5 minutes for “incremental” builds depending on the change. None of the rust projects that I’ve worked with have ever taken that long. 
 while let Some(line) = reader.next_line() { let line = line?; This seems backwards to me. I'd expect the entire `next_line` to fail on error, like while let Some(line) = reader.next_line()? {
The last one doesn't work because you're moving the value of `new_node` into that box. Assigning to `new_node.next` at that point is trying to reinitialize the `new_node` local variable but you don't finish reinitializing it and you don't try moving it so it's just eaten. If you go back to just using `.take()` and assigning directly to `self.head` it should work fine. It's a bit more readable anyway, in my opinion.
initial deno presentation: [http://tinyclouds.org/jsconf2018.pdf](http://tinyclouds.org/jsconf2018.pdf) 
&gt; I like geary, but it's missing gpg support, which would be pretty easy to add to a rust-based program once the `sequoia` project has reached a certain maturity. One `sequoia` reaches certain maturity it's going to be easy to hook up to Geary too. It's not that hard to hack on, thanks to being written in Vala instead of raw C.
Defaults are a tricky thing :) Last time we were convinced that current settings should be the default. I think we ought to increase the visibility of those settings, so that people know they have a choice between latency and completeness, in this case.
What build system are you using? I've never found rustc to come anywhere close to how fast my Ninja-based builds with GCC are.
I think my problem is also that it’s a lot of templated code. Boost plus our own libraries with a lot templates plus the actually application itself using all of these libraries. 
I must admit that I don’t understand much about pinning besides the type can’t be moved, and it’s necessary for self-referential structures (which I believe are important). My main concern is the potential viral nature of this, and the extra layer of semantic complexity it introduces. That is, you have to wrap everything in Pin. Something like Send and Sync would seem more natural to me, where a class is marked somehow by the compiler if it can be used with things that require a pinned structure. That being said, a Pin generic is way better than separate variants of Box, Rc, Arc, etc.
Will definitely do so, good point. Thanks for this feedback and taking the time to look at the code. Just opened an issue and will fix.
&gt;A well-documented way to configure it for rust-cpython use. There's `--bindings-crate` which is described in the readme. I didn't test it as I didn't expect any demand for using rust-cpython; The project seems pretty much dead. Could you elaborate on why you use rust-cpython over pyo3? &gt;(Ideally, one which doesn't require that a command-line argument be passed every single time it's invoked.) If there's more people using rust-cpython, pyo3-pack could just autodetect which one to use. &gt;An equivalent to setuptools-rust's integration with ./setup.py develop and ./setup.py check. For the the check part, you can simply use `cargo check` normally (that's 90% of what `./setup.py check` does. For the develop part, you can just use a symbolic link with a normal \`cargo build\`, e.g. the following for linux: ln -s $(pwd)/target/debug/libword_count.so ${VIRTUAL_ENV}/lib/python$(python -V | sed -rn 's/Python (.*)\..*/\1/p')/site-packages/word_count.so In case there's a need have an explicit \`pyo3-pack check\` or \`pyo3-pack develop\`, it's easy to integrate them. I hope that can resolve you concerns.
[cargo modules](https://github.com/regexident/cargo-modules)?
All Rust is advanced Rust. ^(I kid, I kid... but it feels that way sometimes...)
I partially agree with both, initially I also thought of dividing them, returning: &gt; Result&lt;Option&lt;String, Error&gt;, Error&gt; but it seemed a little too cumbersome to use, so i ended up like this. Matching should not be so difficult anyway(?) I'll surely think more about it!
&gt; I didn't expect any demand for using rust-cpython; The project seems pretty much dead. Could you elaborate on why you use rust-cpython over pyo3? pyo3 is a complete non-option for me because it requires nightly compilers. rust-cpython is actively-enough maintained by some people at Facebook that it's very usable.
Thanks for the feedback, very appreciated. You are probably right, I'll try soon ;)
I actually do use Ninja with MSVC every so often as well for what it's worth, and it's definitely much faster than VS builds. Even command-line NMake builds are faster, for that matter.
Yeah, this is the kind of thing I am looking for. Thanks :)
Invalid pointers are safe to construct. You can even have a `*const !` despite there being no possible instance of `!` that such a pointer could point to. You'd only get in trouble if you tried dereferencing the thing. Maybe you've seen discussions about pointer arithmetic causing UB if the result is outside the range of an allocation, but that's a specific rule of pointer arithmetic itself. If you created a pointer to that same out-of-bounds address by casting from a `usize` or something, that in and of itself wouldn't be UB.
I am thinking of rekindling the meetup. Curious about venue hosts
People might have different opinions about whether SemVer is a good idea, but to the extent that we buy into it, it's _directly_ related to this discussion. Here's a quote straight from [their FAQ](https://semver.org/): &gt; If your software is being used in production, it should probably already be 1.0.0. If you have a stable API on which users have come to depend, you should be 1.0.0. If you’re worrying a lot about backwards compatibility, you should probably already be 1.0.0.
Context: this was created by the creator of node, based on what he would do differently if he could start fresh
If that's what their FAQ says, then all things considered, RLS shouldn't already be 1.0.0 right? It's not ready for production, users frequently come to this very subreddit to ask about recommendations because RLS autocomplete doesn't work, and backwards compatibility is a nonissue. &amp;#x200B; Again, you probably can argue it's 1.0 based on SemVer alone, but it would be a very tough argument that is potentially still wrong. It's also tangential to if it's in a state ready to be called 1.0. 
I like that go style import Syntax but does it support specifying commit hashes to insure authenticity or something similar to prevent against malicious imports? 
No Biggie. I just couldn't resist pulling your chain!
Yes that's disturbing to me as well. Looking into it now.
The RLS is basically doing exactly `cargo check`. There is some deliberate latency, we don't start building until you stop typing (so as not to be always building) and that means we wait 1.5s before starting to build. It might be that causing the difference (as well as the things Xanewok mentioned about test targets, etc.). I should also note that most of the time, whether RLS is still building or not is not an issue - you should still get most type info and completions while it is still building, though not 100% up to date (e.g., if you want to jump to def for a name you just typed).
There are some very different angles we could have explored. One option would be to only build on save, rather than try to give live feedback (we could have spiked a release much sooner). Another would be to customise Cargo more to avoid the complex parts of the RLS which emulate Cargo. We might also have tried to build our own 'compiler' (which is the IntelliJ approach, and what Matklad prefers). Possibly we could have contributed to IntelliJ rather than start our own effort (though that has the obvious disadvantage of only supporting one editor, instead of many).
You can import any file over http, so in that way you can link to a specific version or commit (e.g. if from a raw github link). I don't think there's anything RE signing the response for authenticity at the moment, that sounds like it's worth looking into.
&gt; What are you fearing? I'm fearing that people who would benefit from using the RLS will not do so because it is not labelled 1.0. &gt; people expect a 1.0.0 tool to work I totally agree with this sentiment, however, the hard bit is defining "work" what is acceptable for some users is not for others. We're never going to be as good as Visual Studio for C# since Rust is a very different language and because we don't have 1000 full time engineers working on it :-) We could never be 1.0, but I think there is a benefit in using that label, the question is when.
A simplification of the other response would be that you moved new_node into the box before your assignment is evaluated which won’t work.
Wow, he rewrote a lot of that very quickly. Just a few days ago there was ~15-20% still in Go.
Link to recording: https://www.youtube.com/watch?v=M3BM9TB-8yA
All closed source for now.
You mean what I described in my response to theindigamer? It won't require a GC at all. &gt; why not use JS Well, the alternative to Rust in the frontend wouldn't be JS for me, but keeping using PureScript (or ReasonML). I don't want to go back to type-unsafety..
&gt; We're never going to be as good as Visual Studio for C# since Rust is a very different language and because we don't have 1000 full time engineers working on it :-) I'd love to challenge this assumption. The only **really** bad thing from (rust) IDE perspective I think is macros 1.0 with completely non-local name resolution, but they are going away. The bit about 1000 is definitely exaggerated as well. Like, the whole JetBrains was 700 ppl at 2016 :D Sure, writing IDE is hard, and is time-consuming, but its a comparable endeavor to writing a compiler, and you can get pretty far ahead with just a couple of dedicated people.
Thanks. Does it allow constructing lens paths for indexing related isomorphic enum types (such as `ChildSlot` and `ChildQuery`) to be able to get the corresponding variant of each enum with a given lens path. E.g. lets say we have `enum ChildSlot { SButton, SDropdown }` and `enum ChildQuery { QButton(button::Msg), QDropdown(dropdown::Msg) }` and we want to have a type-level lens path to be able to get the slot and query type at compile time, for the path that represents a child. Or alternatively, could we use [frunk's Coproduct](https://docs.rs/frunk_core/0.2.0/frunk_core/coproduct/enum.Coproduct.html) to get scalable sum types for our lenses? We'd have `type ChildSlot = Coprod!((), ()); // in Halogen it'd be: type ChildSlot = Either2 Unit Unit` and `type ChildQuery = Coprod!(button::Msg, dropdown::Msg); // in Halogen it'd be: type ChildQuery = Coproduct2 Button.Query Dropdown.Query -- because of the free type variable in Halogen's Query types` The lens path could be expressed by an array of binary discriminant indices (e.g. `[0, 1, 0]` would mean "descent to Inl, then Inr, then Inl") but this representation couldn't be used at compile time to index a Coproduct :/ Any idea how to make the best of rats (or frunk) to get closer to the goal? 
I'm EXTREMELY excited about deno. I remember when I first heard about it, there were a bunch of people just parroting RIIR... Apparently it worked
I've been using Sublime Text for everything (used Emacs many years ago), I've been looking for a better editor (been anticipating Xi) and started learning vim but it's not working correctly in tmux (the Ctrl+Arrows for moving to next/prev words), and some people say I should use neovim instead, or oni etc. Coming from Sublime I'm used to multi-cursor editing and apparently there's no real working substitute for vim that works with auto-completion etc., is that true? Have you used Sublime and would you recommend switching to Kakoune instead of switching to vim/neovim/oni?
So, I guess all the down-voters believe that it's OK to have the perfect be the enemy of the good? Short-sighted if you ask me. Meh?!?
FWIW there are comments farther down on this post linking to slides and a recording of a talk the author gave about problems with NodeJS.
As others point out, there's been a talk about the shortcomings for node.js. My main problem with node.js is actually the fact that it is not sandboxed, even though it's derived from one of the most secure and safe web browsers in existence. Deno is sandboxed by default. It would have prevented the recent eslint debacle.
I would think you'd want to manage memory yourself for this for performance, so not go.
You've probably thought about this more than I have, but 1.5s seems like a long time to wait to me. I would think something like 200ms would better match the way I write code. Is this a configurable setting in VS code? I would be interested in playing around with different values to see how it affects perceived speed.
&gt;I'm not even sure semantic versioning makes a ton of sense for a project like RLS. Its public API is an already versioned language server protocol and extensions. That's an interesting point. What would a "breaking change" even mean for RLS?
Ryan's presentation introducing deno is titled: [Design Mistakes in Node](http://tinyclouds.org/jsconf2018.pdf), that details many of the issues that deno is attempting to address.
Seems like a no-brainer. This will help to bring in experts into the fold and will alleviate how many hats others are expected to wear. Rust has a lot of potential in security if it can establish itself a proactive track record.
In your case just set \`"rust.wait\_to\_build": 200\`.
I think you're right. It's just that we haven't *really* needed it for this long, and Pin/unmov might not be enough for custom self-referential types. But Pin *is* enough for self-borrowing generator/async fn/closure/block internal states, because those are *entirely opaque* from the outside. So we're adding `Pin` now and trying to avoid making it too magical because it's unclear what its role will be overall.
It's safe in Rust, but it's undefined behavior in C (unless the pointer is one past the end of an array).
[removed]
You're remembering correctly. But it works for `Vec` (and `String`) because accessing the `Deref` target never reallocates, i e, you never get a `&amp;mut Vec&lt;T&gt;`, you only get a `&amp;mut [T]`. Unless I'm missing something, it seems like the same applies to `Pin` (Pin's deref target is T, not P), so `Vec` should be able to implement `Own` too?
/u/fetchan: I would really appreciate your (and anyone interested) to share your opinion about it on the issue I created: https://github.com/dpc/crev/issues/4
That's what the bug tracker is for?
When he announced it, he had said in the announcement that he was considering it already. This wasn’t really a RIIR situation, as it’s usually framed.
There was an [interesting comment](https://www.reddit.com/r/rust/comments/98gqsg/announcing_the_rls_10_release_candidate/e4gazj1/) on this in the other thread as well.
Thanks! I hadn't read that.
Nah, pointer arithmetic in Rust (i.e. with ptr::offset()) has the same conditions for UB as in C.
Not to bash the author but wasn't node created by google's v8 already?
Genuinely curious: how is this useful?
I think (but obviously can't verify) that the downvoters are the ones saying that the current implementation isn't _good enough_ for 1.0 yet, nobody is advocating for releasing a perfect tool at 1.0, but people are saying it should be "good enough" for that label. The problem of course, is that there wasn't a clear description about what "good enough" meant, which is why this debate started once it was announced that 1.0 would be released, which in term revealed what what good enough meant for the maintainers, which is about stability, not so much about "good enough features". I didn't downvote, and I also agree in general with "perfect is the enemy of good", but in this situation, it just doesn't apply, since we're not at "good" yet (although we've already come a long way, and the effort being put in is clearly visible, and appreciated), at least not in my opinion.
Node is a runtime and a set of libraries built around V8, Deno is exactly that as well.
No. Node and Deno were created by the same author... and not Google. Both Node and Deno use v8.
Node uses V8 to execute JavaScript. Node wasn't "created" by V8, I don't even know what that means.
&gt; We're never going to be as good as Visual Studio This was a really disappointing statement. Because really, IDE support means a lot for production usage of language. &gt; for C# since Rust is a very different language Indeed, but there is also C++ with all its pains and flaws, such as module system absence, direct #include, obscure preprocessor, different build systems, and lots of others, but people's been fighting it and making some decent IDEs. Yes, it's took years, but they've made it. 
I'm doing the Futures presentation ( [https://mgattozzi.com/classes/run-await-with-me](https://mgattozzi.com/classes/run-await-with-me) ) with a rough knowledge of Rust. The Executor implementation requires this code: `let mut spawn = &amp;task.spawner;` `let cx = &amp;mut task::Context::new(&amp;waker, &amp;mut spawn);` I tried doing it myself many times before checking the solution. I don't understand it the distinction between mut on the left side and the right side. If I try `let mut spawn = &amp;mut task.spawner` I get an error: `38 | let cx = &amp;mut task::Context::new(&amp;waker, &amp;mut spawn); | ^^^^^^^^^^ the trait \`std::task::Spawn\` is not implemented for \`&amp;mut Spawner\`` `= help: the following implementations were found:` `&lt;&amp;'a Spawner as std::task::Spawn&gt;` `= note: required for the cast to the object type \`dyn std::task::Spawn\`` Of course I don't want to just add and remove code until the compile works, I'd like to understand what's really going on. Can anyone help?
Also, interestingly, for a long time the v8 team didn't particularly care about node.
For writing network I/O code, I think asynchronous style is pretty natural. It's also faster, but that maybe doesn't matter very much for an IMAP client. (Although I know of one user who wanted to burn through many messages fast and thought tokio-imap could help in speeding his code up.)
That's a fairly simplistic view, though. If eslint were run in a sandbox, it might also be less useful. Or the attacks could be aimed at tools that are allowed to use network connections. Or, depending on how the sandbox was set up, an attacker could attempt to override the sandbox configuration. A sandbox might have prevented this particular case, but it doesn't prevent the general case, which is that I want to run a tool on my computer that will require, to do its job, enough permissions to also do malicious stuff. A Yeoman-esque scaffolding generator will pretty much always need network access and filesystem access.
What do you mean?
JS has a garbage collector. Go has a garbage collector
That's why I wrote "not as problematic". The current state is "you delivered the code, you've got everything". Having to ask for network permissions explicitly at least makes clear which apps want them and which not.
It's even on the slides: prototype in go, Rust and C++ considered.
I've done similar things to instrument proprietary libs (and way back did a lot of injection nothing against various games), but previously I've done it all manually. Detour or chiter are great finds! They look like they'll make it a lot quicker next time I need to add some logging to someone else's binaries!
I think measuring futures execution time should be executor (i.e. tokio) feature. It could warn if feature execution takes too long, e.g. by spawning a helper thread which will receive future start/finish signals from executor threads with MPSC channel and will issue warning if future execution takes too long (deadline can be regulated with environmental variable). This way even if future is completely deadlocked we will be able to receive warning, as helper thread works outside of the event loop. The main problem which I see with this approach, is that it will be hard to know which future is blocked, as AFAIK generally you will be able to get only future's `TypeId`. So I think it will be worth to create an issue in [tokio](https://github.com/tokio-rs/tokio) repository for it.
Well maybe "never" is not right, but in the foreseeable future for sure. Writing something like Visual Studio is a mammoth task. It has literally taken decades for C++ and a huge amount of engineering effort, resources which the Rust community simply doesn't have.
There was some discussion recently about differences between capnp and protobufs, so I wanted to do some actual direct comparison benchmarks between them!
Think of `mut` on the left side as telling the compiler to make the local binding `spawn` mutable. You can pass out mutable references to it or rewrite it on the stack. `mut` on the right side is creating a mutable reference to `task.spawner`; you can write through it or pass it to code that wants to write to it. Now look at the value you're assigning to `spawn` and think of what its type is. `&amp;task.spawner` is thus `&amp;Spawner` (ignoring the `'a` lifetime which is irrelevant at this point), whereas `&amp;mut task.spawner` is `&amp;mut Spawner`, a separate type. In most cases, `&amp;mut` references automatically coerce to `&amp;` references where needed, but here it's looking for a trait implementation which is only provided for `&amp;Spawner` and not `&amp;mut Spawner`.
For my current Python client I'm using a module named "imbox". If I download a list of all messages from the server it does so one message after the other, which takes about 2-3 seconds for my current test-mail-account which has about 30 messages in it. How does tokio-imap handle this? Is this parallelized in some way? I tried doing this with a Python Queue, but IIRC the IMAP protocol/RFCs does not recommend to parallelize this. What are your thoughts on this? And how does tokio-imap handle this?
I think this can be used for bridging the UIkit/Appkit to have a native GUI for mac platform. It is now possible to write a library similar to react-native but truly runs native and using the native UI for each of the major OS platforms. gtk - Linux, winapi - windows, objective-rust - macs/ios ndk - android There's https://github.com/andlabs/libui in pure C, but it still has come a long way. If there is a pure-rust equivalent using the [#cfg(feature)] directive, then it would be easier to have a custom UI widget for specific platform a developer who wish to support without waiting for the UI library to add it first.
I think the problem is that this works well in theory, but is there much evidence that this works in practice? It's a similar issue with mobile apps, where unless the controls are very fine-grained, the sandbox effect is almost useless, and if the controls *are* very fine-grained, users get conditioned to just accept anything anyway. In this situation, as I pointed out, there are plenty of node utilities that would require network permissions for whatever reason - the attack could just as easily have been against one of those, and sandboxing wouldn't have solved anything at all. Sandboxing definitely has a place in software engineering, but I don't think it's in this context. When you install development tools on your machine, you are necessarily giving them a huge amount of trust just by doing that. Sandboxing erodes the caution we should have when we install these things, and makes it much easier to assume that the sandbox has taken care of an issue that we as developers should be wary of ourselves. That trust can often be a lie, particularly with less experienced developers who may not understand entirely how the magic sandbox protects them, but have heard from on high that it Just Works. Instead, for development security, we should be much more focused on ensuring that we trust our toolchains - so much more caution around tools like NPM (and Cargo, of course!); being more careful to vet the utilities we download like Yeoman, Eslint, etc; and a community that is more comfortable emphasising security over convenience. I'd argue that the illusion of confidence that sandboxing provides can make it potentially even more problematic than other security tools, if you don't entirely understand its limitations.
Might be interesting to add [bincode](https://github.com/TyOverby/bincode) to comparison. This is what Servo uses for IPC, so it is an alternative to protobuf/cap'n'proto for some use-cases. 
Will do!
To be quite frank, I'm currently not set up for a detailed opinion exchange around sandboxing in that context. I answered the question because someone just dropped "eslint debacle" in it's context and wanted to help out the user below with a pointer in the right direction.
Yeah, sure, I only realised after replying that you aren't /u/est31, who originally made the claims eslint and sandboxing!
One could argue though that NPM has a big advantage by being privately owned, through the amount of resources they have and ways they can gather them.
No problem, Reddit is still terrible in this department.
I have written a (sort-of) [TAS tool](https://github.com/oberien/refunct-tas/) for an Unreal Engine game with support for both Windows and Linux. I handrolled everything related to library injection and function hooking. On Linux I use `LD_PRELOAD` to inject the library, while on Windows I use `VirtualAllocEx`, `WritePorcessMemory` and `CreateRemoteThread` to inject the library (the code can be found [here](https://github.com/oberien/refunct-tas/blob/master/tool/src/inject.rs)). For a function to be called on injection [I use `DllMain`](https://github.com/oberien/refunct-tas/blob/master/rtil/src/native/windows/mod.rs#L14-L24) on Windows and [`.init_array` on Linux](https://github.com/oberien/refunct-tas/blob/master/rtil/src/native/linux/mod.rs#L13-L17). To hook functions I wrote macros ([Linux](https://github.com/oberien/refunct-tas/blob/master/rtil/src/native/linux/macros.rs) and [Windows](https://github.com/oberien/refunct-tas/blob/master/rtil/src/native/windows/macros.rs)), which allow intercepting before the hooked function is run ([Linux](https://github.com/oberien/refunct-tas/blob/master/rtil/src/native/linux/macros.rs#L164-L196), [Windows](https://github.com/oberien/refunct-tas/blob/master/rtil/src/native/windows/macros.rs#L176-L284)) and after the function has run ([Linux](https://github.com/oberien/refunct-tas/blob/master/rtil/src/native/linux/macros.rs#L197-L229), [Windows](https://github.com/oberien/refunct-tas/blob/master/rtil/src/native/windows/macros.rs#L287-L311)). On linux I assume that hooked functions don't have more than 6 arguments, which all fit in registers (System V ABI). Just saving all registers before calling the hook and restoring them afterwards works perfectly fine. On Windows unfortunately `thiscall` is used, which stores all arguments (except the `this*`) on the stack. Thus I need to duplicate the arguments on the stack (I decided to only support arguments with a total length of 0x60 bytes) and do quite some fancy stuff to align the stack correctly, provide the correct return address into the hook-function (which needs to unhook, call and then rehook the original function) and more. This process can be seen (and is documented in comments) in the macro for intercepting before the hooked function is run. If you want you can take a look at the code, the general structure is that `tool` is the interface for the user to control the injected `rtil` (refunct-tas ingame library). Within the `rtil` all native (linux / windows based) code is located in the `native` folder. With my tool I provide a LUA interface for the user to modify game memory and call game functions.
This is an excellent blog post and a good way to move the discussion forward. Good job! &gt;I do believe that although the RLS is lacking in completeness, for most users, the cost/benefit ratio is favourable and therefore it should be widely recommended (if you use the RLS and don't agree, I'd be keen to hear why). I believe RLS is currently best marketed as a technological preview, and I'll try to explain why. My own experience with the RLS in VSCode can be summarized like this: * **performance** It causes excessive CPU usage, spinning up my fans, expending my battery. I am usually miles ahead of RLS in my head, and I have to give it time to catch up and adjust the squigglies. * **completeness** I am genuinely surprised whenever I can get code completion at all * **stability** I don't experience that RLS crashes or fails to respond, yay! But the suggested completions, when they appear at all, are often wrong. (I'm not sure if there is another mechanism in VSCode that puts in "dumber" completion suggestions or if the wrong suggestions actually come from RLS) So, you see, in my experience the **stability** criterion is _not_ yet achieved, and the RLS cannot be used to claim a "high quality IDE experience". As I experience the costs and benefits, I'd say RLS is in a perfect state to be marketed widely as a thing you could use if you don't mind the downsides. I'd call this a technological preview. But it seems like our experiences are very different. It could be that [the project](https://github.com/maghoff/sausagewiki) I am spending the most time on is a degenerate case. It uses multiple proc macros and generates code in build.rs. It is never the less the background for my experience, and in my experience the RLS is not yet "high quality". _That being said,_ I do love the RLS!
Can't you just swap the reactor with one that does what you want ? 
&gt;python 2 That's just bizarre.
what.
there's no go tho?
Here's an example from their github import { test } from "https://unpkg.com/deno_testing@0.0.5/testing.ts" The version is in there
It's funny. I think a lot of those "warts" are the reasons it became so popular. For example, the npm ecosystem / package.json makes it so easy to rapid prototype. 
The interface you need to provide for a node-like API isn't that complicated memory-wise apart from the asynchronicity, so I don't think you gain anything from "go"s garbage collector, and you lose deterministic performance.
That's true, but that just looks horrifically messy. Particularly with a testing tool that I might import in every test file that I write, do I need to go through and update the version in every file every time I want to bump it? What about situations where I end up depending on version 3.4.8 in one place, and 3.4.9 in another - presumably I'm going to end up relying on unpkg or similar tools to handle the semver details for me, otherwise I'm going to end up needing two near-identical copies of the same code, just because I (or one of my dependencies) has a slightly different but entirely compatible version of a particular tool. From reading other threads, it seems like the answer is eventually going to revolve around some sort of `externals.ts` file which basically contains all the exports you could need in the format you've given - basically a free-form `package.json` file. That should avoid a lot of the issues related to duplicating external dependency declarations, and allow all the files to be updated from the same place. But then I've got to manage that by hand, or add an additional tool on top of that to manage the `externals.ts` file. I can see how this system works, I just can't see how this system is an improvement on anything that currently exists, and isn't just several steps back, packaging-wise.
I'm really excited about this and looking forward to seeing its development. I'm not a Rust programmer but I work with Node daily. It's really nice that someone is using the experience of the Node community to fix the many shortcomings of the system—I think pretty much anybody using Node a lot will know what they are. There's also a huge amount of good code now that can be used without having to write new things and include them in the core. Of course, since it's not compatible with Node's package system, it'll be a very long road to adoption. Everybody's going to have to make changes to support this.
I think there was talks about using Go at first, but this release uses Rust
This is release 0.1. I'm sure there'll be dpm install express in a while :)
Yeah yeah, that's what my comment was meant to address, I'm agreeing with their reasoning.
Yeah, I guess. I'm just not sure how much value the HTTP imports are going to end up having, or whether they'll be things that become "cute" but incredibly frowned upon in general Deno code within the next couple of years.
Could you please add short descriptions to each benchmark explaining what it is you are measuring?
IMO the true competition to `capnproto` is/will be [flatbuffers](https://google.github.io/flatbuffers/), the successor to `protobuf`. I've been keeping an eye on the WIP Rust implementation [here](https://google.github.io/flatbuffers/), apparently now at the stage of all-tests-passing.
IMO the true competition to `capnproto` is/will be [flatbuffers](https://google.github.io/flatbuffers/), the successor to `protobuf`. I've been keeping an eye on the WIP Rust implementation [here](https://google.github.io/flatbuffers/), apparently now at the stage of all-tests-passing.
Is it possible to make a native MacOS app at the moment with these bindings?
&gt; Do you have some feedback/experience reports about making a multi platform (linux, windows, darwin, freebsd - i386, amd64, arm, arm64) rust program? Yes. It works just fine. You can test all of these in travis-ci under qemu pretty easily: https://github.com/rust-embedded/cross#supported-targets
*`Let rome = rust_fest.next().expect("no more rust fest - unreachable");`
It's because the V8 build scripts still use Python 2 apparently.
Also an initial implementation for [Flatbuffers](https://github.com/google/flatbuffers/pull/3894#issuecomment-412436860) is WIP maybe you can that as well. 
Did you just call us a lazy iterator?
I just noticed that you're only using `Ready::readable()` (https://gist.github.com/goriunov/a2368b415eb0fb3d204cbf4af858e14f#file-tpc-server-rs-L43) when registering the connection, you'll likely also want `Ready::writalbe()` and perhaps even more flags. You might want to use a simpler command like `nc` (maybe with the `--verbose` flag) to debug this. The browser might be reusing TCP connections.
For me RLS seems to work then crash then next release works. then next release crashes. Testing seems quite poor. And rls-cscode seems to fail to properly restart or watchdog rls leading to no further code completion/etc.
Intellij not.much better than RLS...
I have not been able to get ninja working correctly with MSVC yet. Haven’t really investigated why yet, just tried switching the generator in cmake and it failed when building. I’ll give it another shot since it sounds like it may be worth the effort of debugging. Thanks!
Wow, this information will help me tremendously, especially the Linux part of things. Thanks for sharing!
Get Rust today [for free](https://www.rust-lang.org/en-US/install.html)!
Nice benchmarks and no surprises. ProtocolBuffers with its variable length encoding format requires quite some parsing and unpacking, whereas Cap'n Proto just offsets and reads pointers. It would be very interesting to also add FlatBuffers to the benchmark for comparison against Cap'n Proto. FlatBuffers has a much, much simpler wire format than Cap'n Proto, so I expect writes to be quite a lot faster. On the other hand, I expect reads to be a tiny bit slower, as FlatBuffers adds indirect offsets to compress data with unset/default fields.
Well, let's agree to disagree then. DLL hell is a super pejorative term and relates to dynamic linking and whatnot, which is pretty much unrelated to the problems due to having 2 versions of a given dependency, where things can Just Work(tm) if the dependencies are private. &amp;#x200B; Slight bloat sure is an issue, but having two versions of a given crate doesn't always lead to incompatibilities, according to my experience in Servo. Most often, it doesn't lead to incompatibilities, just bloat.
Panic does seem like a reasonable reaction if RustFest ever were to seize.
 let rome = match rust_fast.next() { Some(rome) =&gt; rome, None =&gt; unsafe { std::hint::unreachable_unchecked() } }
I think it was mostly how they handled how packages were downloaded to the machine
Also not really surprised by the differences in turning objects into bits; I just pushed an additional benchmark that includes build times for the objects and I was surprised to see that capnp was a \_lot\_ slower that protobuf to build the same(ish) object.
&gt;Why would it be a bad thing to avoid unsafe code? Because... that's an extremely vague thing to do. &gt;Also, the point is not to stop using red crates, but to be aware that they do unsafe stuff. Do you mean that they do *safe* stuff, using the `unsafe` keyword, or that they actually do stuff that violates the language's assumptions? That's the problem I'm having trouble seeing past here. If std would be marked red, why would any other crate expect to do otherwise? What good would the awareness do you, if you have to go and read reviews and analyze every crate anyway?
In `cargo-geiger` I did something similar. You could probably copy paste most of it, starting with the function `resolve_rs_file_deps`: https://github.com/anderejd/cargo-geiger/blob/master/src/main.rs#L747 
Any non pinned Rc's would prevent the does-not-move guarantee. impl&lt;P&gt; Clone for Pin&lt;P&gt; where P: Clone { fn clone(&amp;self) -&gt; Pint&lt;P&gt; { Pin { pointer: self.pointer.clone() } } } That would solve it, wouldn't it? Multiple pinned Rc instance pointing to the same value should be fine as long as there is no unpinned Rc.
The other poster nailed it. Like, my post literally says, on the first line: &gt; But it's not ready to be used because the user experience is not **good** (yet). I did not imply that it needed to be perfect. In fact, I said that feature completeness should not be a requirement for 1.0. *That* would be letting the perfect be the enemy of the good. So you got downvoted for not really reading or comprehending the post and then posting a shitty non applicable aphorism in response. 
Thanks, yeah, that makes sense (and seems obvious now...).
[removed]
As I said, just swap the reactor for testing with one that does something differently. For example, you can have a reactor that has threads for executing the futures, but also has a background thread that times how long each future executes. This thread can just abort the whole process if one future runs for too long.
It's quite a lot different from today. I can't think of any other programming language or ecosystem that relies on arbitrary URLs to download resources, and specifies those resources within the body of the code itself. The closest approximate thing I can think of is Go's ability to import from GitHub, but even that doesn't work quite like this. And, again, the Go community seems to be moving towards more explicit ways to manage dependencies. And it never included the version number in the import path directly. I think that's the weirdest thing, tbh - if every import specifies the version number, isn't that going to cause a proliferation of different versions far, far worse than NPM's already fairly difficult-to-manage versioning system? For example, how on earth do peer dependencies work in this system? I can understand that Dahl hasn't got round to sorting out package manager stuff yet - it's a very early project, and I think he's already made it clear that this isn't the sort of problem he's particularly passionate about. I just don't understand why, as a stopgap measure, he'd put in HTTP imports, especially if Deno was, to a certain extent, intended as a "fix" for Node.
The way the graphs laid out is confusing. Sometimes the top graph is for protobuf, sometimes for capnp. Is there a way to force them to have fixed positions? It would make the graphs a whole lot easier to read.
Not at the moment. In order to do that, it's necessary to also create ObjC classes on the Rust side. Shouldn't be too hard to do in theory..
Is there a way to disable Racer-based code completion yet? I don't mind waiting some extra time if RLS can provide more accurate results.
They're memory safe in that you can't access bad parts of memory, but you can still get null errors crashing your program.
He could have been working on it for a while and didn't commit it...
Why? Its reality since Python 3 was introduced?
yeoman is a tool to manage one particular project, right? Then why can it write to files outside of that project directory? Why can it read my `~/.ssh` directory? If it has network access and read/write access only to the project directory, its possible impact is low. Sure, the sandbox suggested by deno is not perfect, but it's a step into the right direction. It's an attempt to tackle the problem, not a "we are giving up because it's hard".
Nice! I'll try to be there :) &gt;!Quanti italiani qua?!&lt;
It's definitely interesting, however I'm not sure if your `InteriorMutable` trait really encompasses all forms of interior mutability. For example, * `Cell` is another form of interior mutability but it can't implement this trait, or at least I don't know what its `RefMut` type would be. * I've also proposed a different, admittedly very weird, kind of interior mutability [here on reddit](https://www.reddit.com/r/rust/comments/6hznoz/a_zerooverhead_refcell_variant/) and that couldn't implement this method either because the `borrow_int_mut()` would need an extra argument. * I suppose `RwLock` can implement this interface but I guess those who would use it would advocate for the trait to have a `borrow_int()` method that borrows the interior immutably. * `UnsafeCell` can't implement it because `borrow_int_mut()` would be unsafe. I don't know any other types of interior mutability, but I feel like the ways that the different types of interior mutability differ from each other is precisely in the way you use them to borrow the interior, i.e., in the `borrow_int_mut()` signature, so I'm not sure how well a trait unifying them all can really work.
Thank you for being interested in my project. 
You might also find interesting to go through the issue tracker looking for easy issues to fix. Having a target would give you a good motivation to look at the compiler itself, and it's a great way of learning how things work behind the curtain.
(For reference, Yeoman is a tool to help "scaffold" a new project by constructing the basic directory structure from some sort of template - that's relevant in some of the examples here.) Because nobody wants to run a tool giving it explicit access to every single folder it's allowed to access. Consider the following: deno --allow-read-write="~/.yeoman/cache" --allow-read-write="." --allow-network yo new-project ./ (I confess I don't know exactly how to run Yeoman!) It is fine-grained, in that it will only ever be allowed access to the current working directory, a necessary cache directory, and resources over HTTP/S, but we've already had to specify the path twice. The length of the command has tripled (quadrupled, even?) to accommodate my explicit setting of what the tool should be allowed to access. Expanding this arbitrarily becomes a lot more complicated - how should I specify to deno what files the following command is allowed access to? cp src/*.js ~/additional/other/*.css /tmp/some-junk-directory (Obviously cp isn't written using deno, but it's a handy demonstration of how a lot of command-line tools, particularly ones that advertise themselves as "UNIX-y" will touch a lot of different parts of a computer system.) Should I specify which files are to be read and which to be written to deno as well? Should I tell it to trust all filesystem interactions - in which case we're very nearly back where I started? Should it somehow parse the command-line and guess which files it has permission to touch? This becomes more complicated when a particular command has more complicated subcommands. For example, Yeoman defaults to only working in the directory you specify, plus a cache directory, and using network connections to download the necessary files. That's three sets of permissions. But I can also scaffold a project using any arbitrary location on my file system. So I'll need to add permissions for that as well, if I want to use that particular feature. My point is not that we should give up on the problem of security, but that sandboxing in this context is specifically an answer that doesn't satisfactorily offer much, at least to the developer downloading tools to use on their developer machine. It does have value in other areas - I could see deno's sandboxing mode being useful if it were embedded to run more arbitrary scripts, and wider sandboxing strategies are a core concept in multitenancy situations (although I'd suspect that deno specifically wouldn't have much to offer here that other, more mature solutions wouldn't do significantly better). However, for developer tools, sandboxing is too complex to be a practical solution. As I said earlier (and demonstrated in this post) fine-grained controls quickly get too complex and will be replaced, at least by users, with more broad permissions (e.g. aliasing `deno` to `deno --allow-read-write="~"` quickly "solves" all the faff I presented here, at the cost of almost completely removing the benefits that sandboxing provides); and broader controls will effectively be useless, as they'll end up being turned on for almost everything. The end user ends up with nothing but the illusion of safety.
I see the difference now: I thought about pyo3-pack as being primary a tool for publishing, while you need a workflow tool. I've now extended pyo3-pack with a develop command (disclaimer: not yet tested on windows) and added an example on how to use pyo3-pack with tox and pytest. This should hopefully create a smooth workflow that also integrates with an ide.
 let rome = rust_fest.next(); assert_eq!(rome, Location::Rome, "variable names can lie");
This is awesome! Just recently I have [complained](https://www.reddit.com/r/rust/comments/8zpp5f/auditing_popular_crates_how_a_oneline_unsafe_has/) about the lack of such a tool. Would using this to verify correctness of Rust standard library primitives such as Vec or BTreeMap be feasible? They rely heavily on unsafe code, but do not use external raw pointers. Better verification of Rust stdlib is another thing that I've recently [complained](https://medium.com/@shnatsel/how-rusts-standard-library-was-vulnerable-for-years-and-nobody-noticed-aebf0503c3d6) about.
Yes you're definitively right about this trait not capturing every possible behavior from interior mutable containers. I should perhaps be more clear about it but I'm not proposing the exact same trait I used in the tmcl library to be included in libcore. I'm proposing a similar trait to `borrow::Borrow` and `borrow::BorrowMut` called `borrow::BorrowIntMut`. It's supposed to fit well for containers where you can safely borrow a `&amp;mut` from a `&amp;` (immutable reference). This includes `RefCell`, `Mutex`, `RwLock` but not `Cell` or `UnsafeCell`. You have several types that is based on swapping values. These includes `Cell`, `AtomicBool` and `AtomicUsize`. I assume there exist some common behavior that could be extracted out into a trait but this is not motivated by my example and I have not had the need for it yet. I've therefore not mentioned any of this is in the post. I believe unsafe cell is mostly a stepping stone for creating safe abstractions and therefore may not be very relevant here. The main things that feels off with this approach is that for instance `borrow::BorrowMut` gives a `&amp;mut` while, due to `RefCell` and `Mutex` needing a way to keep track of state dynamically, would have to hand out fat pointers. There would of course be possible to create a new set of traits that allowed fat pointers (a return value that `impl Deref&lt;T&gt;`) maybe even with a blanket impl from the `borrow` traits. Since `Deref` and `Borrow` are already overlapping in which types they are implemented for, this approach might also "be a bit to much". tl;dr The suggestion is only concerned about types where you can borrow a `impl DerefMut&lt;T&gt;` using a reference to the wrapper type.
Awesome! Unfortunately, the dates coincide with the weekend after US Thanksgiving, so I won't be able to attend. :-(
Interesting idea but I'm not sure it needs to be in `std`...
Here a couple mechanisms for this I find handy. *ExecutorPacemaker* This schedules a trivial task on the executor periodically (say, once a second). Then it waits up to a threshold for it to complete (say, 100 ms). If this times out, it grabs a bunch of debug information (and puts details in logfiles and a summarized form in an aggregation system). The details include stack traces of all the threads associated with that executor (as well as CPU usage over those 100 ms, some stuff from `/dev/cgroups`, etc). Then it waits for completion. Once the closure completes (whether it exceeded the threshold or not), it adds the total time to a histogram which is exported to a monitoring system [rust-prometheus](https://github.com/pingcap/rust-prometheus) would be a good way to do that). The idea is that if all the executor's threads are stuck in some fashion, this will notice quickly and gather detailed debugging information. It doesn't notice if just 1 out of N threads is stuck for a while, though. It'd be better if the executor had something like this built in that checked each thread individually. (And perhaps measured the timings of all the real tasks your system executes, not just the artificial probe ones.) *LocalTimer* This is my favorite thing, but it's a little harder to explain. I use it in a distributed system setting where my local process is just one tiny part of the whole. It handles inbound RPCs which fan out into outbound RPCs and some local work. The idea is to measure just the time in which the local process has no one else to blame for slowness. You start a "local timer" for every inbound RPC (or a sampling of them if your requests are cheap enough that the instrumentation's cost is noticeable). You tell it whenever an associated remote RPC starts and stops. It measures just the time in which the remote RPC count is 0. This is what I call "local time". (If you have sleeps for backoff when handling outbound RPC failures, then you'd also count this as remote RPC time.) The best thing is if you can integrate it with your RPC system so the starts and stops are automatic. Otherwise you'll have false positives if you forget one. Also nice if you have a way to track which threads are working on a given RPC. (My system has some thread-local state; when I capture debug info, I send a signal to each thread, and the signal handler checks this.) I do a few things with these "local time" numbers: * include them in the information I log about each request; useful for correlating with other characteristics of requests. * when local time exceeds a threshold (configurable based on the type of request), gather debug information, highlighting threads known to be working on this RPC. This is similar to ExecutorPacemaker, with one caveat that unfortunately it's too expensive to get the process CPU time right before every single request. I'd love to look at the CPU usage just during the request's "local time", but I have to approximate it by comparing measurements captured periodically (up to a second before the request started), when exceeding the threshold, and when actually completing the request. * a histogram of all local timer stuff exported to the monitoring system, split out by type of request. LocalTimer is totally agnostic as to threading model. It works for async programs. It works for sync programs. It works for hybrids. It just doesn't care. It also can tell you about a variety of problems. Some that I've seen: * Accidentally blocking in a executor that's not meant to be used that way (one that has one thread per CPU or the like). * Surprisingly CPU-expensive work, like accidental O(n^2) algorithms. * Extreme lock contention events. * Kernel problems (we hit some problems where a buggy kernel release was calling this function called `wait_iff_congested` on minor page faults, basically sleeping for no good reason). * Hardware problems (I see this trigger on machines where there are a lot of correctable memory errors). It's really powerful to have debug info captured at the exact second or millisecond you have almost any sort of problem that causes requests to stall.
Sure, I will take a look at rust github issue tracker. 
My vague understanding is that on tier-2 platforms, stable will generally work but might be a little behind on a release, nightly should work but might not. The main reason they're tier-2 isn't any fundamental incompatibility, but just lack of human power to make sure they always work. I've done a fair amount of development on Rust programs targeting both Windows and Linux and a bit of MacOS, and portability is very very good as long as you don't need to stray into realms where platform differences can actually matter: OS-specific process/status info, windowing systems/graphics, and some parts of filesystem stuff (permissions, etc). For a program like `flint` you should be good to go. I've done a *little* stuff on i386 and arm, and the differences are basically invisible.
What a conicidence! Thought about incorporating lua into actix's actors a few days ago and now you come along! Looks great! You have not implemented any sandboxing mechanisms by chance, i.e. does the Lua actor have access to the enviroment (e.g. filesystem and os)?
Heads up that "orthogonal" is misspelled as "ortogonal" in that first paragraph.
Oh, I'll have to dust off my semantic web crate [with the same name](https://crates.io/crates/rome).
If I wanted to build something like it in future, where do I start from today?
How do I integrate V8 with other language? I ask because all I know is crud, and I want to learn how to integrate two or more different softwares.
Pitch forks and angry devs.
Thanks! Will be corrected once page is rebuilt.
Great article, great job! I was able to reproduce this solution on linux but I had to: &amp;#x200B; \- add \~/.nuget/packages/runtime.linux-x64.microsoft.dotnet.ilcompiler/1.0.0-alpha-26512-01/framework path to "link-search" \- add System.Globalization.Native and System.Native references even for simplest program that only concatenated two strings &amp;#x200B; I had also ran into issue with those references, because the file names are not starting with lib (files are 'System.Native.a' and 'System.Globalization.Native.a'). Tbh, I don't know how to solve it, I simply did copy and rename. &amp;#x200B; And I have some strange output to stderr when program ('Symbol \`memcpy\` causes overflow in R\_X86\_64\_PC32 relocation' and similar). But beside that, program works :) &amp;#x200B; It is also worth noting that compiling library instead of binary was way simpler, I didn't have to change anything. &amp;#x200B; I also recommend reading seiri source code. Its build script is perfect and deservers extracting into a crate! (mb some day I have enough time to do it?)
In Esqueleto you can join many tables and have the compiler check that the join statements are between columns with the same type by using the foreign key information from the database schema. https://www.yesodweb.com/book/sql-joins#sql-joins_esqueleto Haskell is surprisingly fast, but I've not done extensive benchmarks. I can just say that running it on low-powered servers was still snappy. On this benchmark, the Hkl (Haskell) entries are not very high. https://www.techempower.com/benchmarks/#section=data-r16 Here it's scoring better but still not stellar. https://www.techempower.com/benchmarks/#section=data-r13&amp;hw=ph&amp;test=json Parallelizing works great though. I've been writing very long SQL queries where the type checking has saved me a lot of time debugging. Subqueries can be functions that can be used to build up larger queries. Also, the same types that are generated from the database schema can be used in the routing of the web application. E.g. if you have a tables with horses with HorseId as the primary key, you can define a route `/horsed/#HorseId` and Yesod will only pass the request to your code when it fits the type. Here's some examples of more complex SQL queries: https://gitlab.com/odfplugfest/odfserver/blob/master/src/TestScheduler.hs 
Or you could just accept that you made a bad post and move on. 
You're certainly right. I've actually put the trait used in tmcl as it's own crate at [crates.io](https://crates.io/crates/interior_mut). In some ways, including this feature in core might be comparable with including futures/async/await in `core`/`std`. It's not *strictly necessary* but if we want async/await/futures to be idioms of rust it's more or less required. I think in a similar way, that if we want library writers to make their libraries to suitable for both `no_std` and `std` it helps having this abstraction available in `core`. 
I'm a little unclear on the rules for this, but would a crate defining `MyPtr` be able to provide impls of std traits on `Pin&lt;MyPtr&gt;`? If not, that could be a problem; it's not clear to me, for example, that `Clone` is always correct to delegate for `MyPtr: Clone`, so `Pin` couldn't have a generic impl of it...
&gt; The main reason they're tier-2 isn't any fundamental incompatibility, but just lack of human power to make sure they always work. Oh, my understanding was that what was missing was build machines for the CI. I wonder if it's a mix of both.
&gt; does the Lua Actor have access to the enviroment (e.g. filesystem and os)? When this question is asked by a user named `rm -f`, I shiver.
From what I've heard, Mozilla already has enough infrastructure that throwing pretty large amounts of compute power at Rust in terms of things like https://perf.rust-lang.org/ doesn't actually make a big difference to them. Compiler hackers who are willing to maintain a port to something relatively niche like OpenBSD are less common.
To be fair, the only one where protobuf wins is the actual initialization timing, all serializing and deserializing is faster in capnp;-)
The numbers I got from running that PR show `prost` beating capnp in 3 out of the 5 benchmarks. I take this as a sign that the benchmarks are flawed, more than a sign that `prost` is faster that capnp.
Certainly worth looking into more!
I haven't yet read the blog post, but I have a very similar experience in Rust lacking better support for interior mutability. My use case is [a library to parse windows PE binaries](https://github.com/CasualX/pelite). I use shared references directly sub parts of the format. Because of this it becomes impossible to provide a mutation API, it is strictly reading only. I really want the ability to provide an API to make changes to the data. I deal exclusively with `Copy` data which is 'pod', in my interpretation a pod type is any type which I can safely reinterpret to and from arbitary bit patterns of appropriate size and alignment. I've considered wrapping everything in `Cell` but it is highly awkward, Cell's generic T requires `Sized` so it can't be used in `&amp;'a Cell&lt;[u8]&gt;`. This restriction and API to conversion to `&amp;'a [Cell&lt;u8&gt;]` are already in nightly but it's not enough. When you have a `&amp;'a Cell&lt;LargeStruct&gt;` there is no blessed way to turn that into a `&amp;'a Cell&lt;Field&gt;`, ie there's no way to turn cell references to cell references of the struct field. Now I'll go read your post, I have the feeling it's a different use case :)
1. Argh, new conventions for where I'm going to find config and cache files! I've already got enough problems trying to work out if they'll be in `~/.progname`, `~/.config/progname`, or somewhere else weird and clever. 2. Sure, except for my calculator that can handle `deno calc 4 / 2`, and suddenly has read/write access to the entire filesystem. 3. This is probably the most practical thing in most cases, but I worry that this ends up leading into the situation we have with a lot of apps, where people just click accept to anything - again ending up with a false sense of security. At least if you don't feel secure, you might take fewer risks!
It's quite disappointing that IDE experience was a goal for 2017, and RLS is still almost impossible to use on large code bases due to performance and not very useful on small ones due to bad code completion. 
più di due ;)
It would certainly make it more ergonomic as some of the type parameters could be moved to an associated type. But I don't see how it would avoid needing an `InteriorMut`/`BorrowIntMut` trait. Or how it could bypass the inconveniences of having this trait defined outside `core`/`std`. Are there any further implications from the RFC i haven't considered?
I seem to remember... that [alacritty](https://github.com/jwilm/alacritty) claims to be the fastest terminal emulator in existence. Otherwise, there's pretty cool stuff done under the Servo umbrella, namely: - [Pathfinder](https://github.com/pcwalton/pathfinder), the fastest font/vector graphics rasterizer. - [WebRender](https://github.com/servo/webrender), the fastest web content renderer. See for example [this video](https://www.youtube.com/watch?v=u0hYIRQRiws) where it achieves 60 fps effortlessly when Chrome struggles to each 15 fps. I think `tokei` was the fastest line-counting tool for a while, but now [polyglot](https://github.com/vmchale/polyglot) (ATS) claims to have it beaten.
btw do you know about the wallpaper crate [https://crates.io/crates/wallpaper](https://crates.io/crates/wallpaper) ?
&gt; even if the only thing you did is press a spacebar. Why not run RLS only on buffer save?
No, implementing a std trait for `Pin&lt;_&gt;` would not be allowed in a crate defining `MyPtr`. Why would implementing clone for `Pin&lt;P&gt;` where `P: Clone` not be correct? `Pin&lt;P&gt;` is a regular struct. But it does not provide any way to access the `P`. So you can't do anything useful with a `Pin&lt;P&gt;` where `P: !Deref&lt;_&gt;`.
Thanks for sharing your experiences with interior mutability. But I think your feeling is correct. The only similarity between our problems is that they need better support for Interior mutability to be completely solved. I see your problem, and how it's quite hard to fix. The conversion into `&amp;'a Cell&lt;Field&gt;` would require `Cell` to have extensive special treatment from the compiler. When accessing fields I've found `RefCell` to often be more accommodating but this, of course, adds overhead and possibility of panic when used in these (self) referential data types. Good luck in finding/creating a solution to your problem 
Qualche italiano c'è :D
Could you give me a hint (a link will suffice) about how to use a Path in order to avoid that panic?
I'd suggest using [rust-cpython](https://github.com/dgrunwald/rust-cpython) instead of PyO3. PyO3 requires nightly Rust compiler and has a [number](https://github.com/PyO3/pyo3/issues/71) [of](https://github.com/PyO3/pyo3/issues/94) [long-standing](https://github.com/PyO3/pyo3/issues/159) [safety](https://github.com/PyO3/pyo3/issues/193) [issues](https://github.com/PyO3/pyo3/issues/198).
How long does it build?
that alacritty claim was for priting and scrolling I think. Delay-wise it was certainly not beating some more primitive options.
Honestly, I was looking at PyO3 just because it was easier to work with. It was intended as a weekend project, though it seems like it's going to take longer than that now :)
I have occasionally come close to saying that the [`csv`](https://github.com/BurntSushi/rust-csv) crate is the fastest there is, but it probably needs some kind of qualification like "featureful" or "robust" or something to that nature. It is certainly faster than [libcsv](https://github.com/rgamble/libcsv), but [csvmonkey](https://github.com/dw/csvmonkey) can edge it out in at least some cases. I tried playing with its approach in `csv-core` a while back, but couldn't get the same results. (Note csvmonkey's very small test suite!) I haven't dove into csvmonkey in enough detail to give an accurate comparison. My guess is that you might be able to claim something similar for `serde_json`, but I'll leave that to others to expound on. :-)
Pretty sure there was a huge controversy with alacritty's claims. Nobody could reproduce their benchmarks. 
LOL!! You take car now and sorry about that.... 
* [grep](https://blog.burntsushi.net/ripgrep/) * [JSON deserialization](https://www.reddit.com/r/rust/comments/6albr0/serde_compared_to_the_fastest_c_json_library/) as long as you know the structure of the data. Bonus points for being 100% safe Rust *while being the fastest deserializer in the world.* When decoding into arbitrary structure it's not the fastest, but still faster than most successful JSON libraries out there.
So excited for this!
I have heard kdb+ be called a lot of things over the years, but "elegant" was not one of them.
I know one controversy is that alacritty has a significantly higher latency than other terminals. Some of this is to be expected since I believe it double buffers and waits for vsync, which avoids things like tearing. Last I heard, there was still some room for improvement there. Most of what alacritty had been measuring was throughput. It is very fast with throughput, I don't know about the fastest in existence though.
Oh, cool! Someone already posted the link here =] (I'm the author of the blog post, hi!)
https://benchmarksgame-team.pages.debian.net/benchmarksgame/performance/revcomp.html Rust currently has the fastest implementation for this reverse-complement benchmarks game
I like the puns too, it doesn't mean they are not terrible =P Even more painful than writing these workflows is choosing a solution to implement them. See https://github.com/common-workflow-language/common-workflow-language/wiki/Existing-Workflow-systems and despair...
This one's just for a game and probably not of interest to you, but there was post on /r/programming a few weeks ago where the OP claimed to have ported the fastest sudoku solver to Rust and then optimized it further (and then compiled it to wasm) https://old.reddit.com/r/programming/comments/928t53/the_fastest_sudoku_solver_compiled_to_wasm/
[Rome doesn’t build in a day](https://www.youtube.com/watch?v=FLGJXbl6g8o)
This seems like a good candidate to be made into a [.net core tool](https://docs.microsoft.com/en-us/dotnet/core/tools/global-tools). 
To fix the System references, it might be better to create symlinks instead of copying them. 
How unrealistic would it be to hope that code like this might work someday? let mut array1 = [0, 0, 0, 0]; let array2 = [1, 1]; array1[0..2] = array2[..]; That's putting an unsized type on the left side of an assignment, but if the right side is the same type, it could check at runtime whether their sizes are equal and then either copy the bytes or panic. Something very close to this works right now with the `arrayref` crate, as long as you know the length of the slice you want statically. But I'm wondering if a built-in language feature could work with dynamic lengths, `str`, and `dyn Trait`.
Sure thing :)
[removed]
My memory of flatbuffers is that it is not appropriate for serializing/deserializing of network messages. I made it work for a side project. But it was clearly mashing a square peg through a round hole.
Romano al rapporto
There are pattern guards: https://play.rust-lang.org/?gist=770bbd4ffb2850a4d106dd0462b7ff76&amp;version=stable&amp;mode=debug&amp;edition=2015
&gt; fastest web framework (actix) https://www.techempower.com/benchmarks/#section=data-r16&amp;hw=ph&amp;test=db 
True
&gt; Edit: Font renderer Does that record still hold? The article's from a couple of years ago, placing it pre-Pathfinder. (Which is also Rust, happily.)
[removed]
I actually found the video interesting anyways.
&lt;shameless plug&gt; If you're feeling particularly bored you can also check out my serialization library - [speedy](https://github.com/koute/speedy), which was explicitly designed to be simple and fast. It serializes to a straightforward binary format without any tricks to make the data smaller. I had a benchmark of my own stashed somewhere, and it was noticeably faster than protobuf (through [prost](https://github.com/danburkert/prost)) and every `serde`-based library I could find. &lt;/shameless plug&gt;
[removed]
[removed]
yeaa didn't click that far - two test cases with this framework not in sight were enough for me
[Tracking issue for RFC 2046](https://github.com/rust-lang/rust/issues/48594). Personally, I'd just tell clippy to go sit in the corner. I have a macro that does the labelled `loop` trick until 2046 is stabilised.
Fastest 2d CPU based simplex noise library is in Rust, probably/maybe. &amp;#x200B;
It sounds like you haven't looked at Diesel in a long time. We've supported multi table joins since one of our 0.x releases (I can't remember which off the top of my head)
Do you know about [`copy_from_slice()`](https://doc.rust-lang.org/nightly/std/primitive.slice.html#method.copy_from_slice)? You can do this in stable Rust right now, although it doesn't look quite as elegant: array1[..2].copy_from_slice(&amp;array2);
Not yet... happy to provide guidance if in turn you contribute a guide ;) I'm always available on the Gitter channel: [https://gitter.im/tokio-rs/tokio](https://gitter.im/tokio-rs/tokio)
&gt; polyglot (ATS) Wow! How many tools are actually written in ATS? I thought—somehow; I have no real reason to think this—it wasn't really used for … anything, I guess.
Although it isn't often the case (and so pattern guards are fantastic for more general use), the conditions in the example are simple enough to be [expressed directly](https://play.rust-lang.org/?gist=770bbd4ffb2850a4d106dd0462b7ff76&amp;version=stable&amp;mode=debug&amp;edition=2015): fn func(i: u8, o: Option&lt;u8&gt;, a: Option&lt;char&gt;) -&gt; String { use std::u8::MAX; match (i, o, a) { (1 ... MAX, Some(3 ... MAX), Some(ref mut c)) =&gt; { *c = 'f'; format!("oo{}", c) } _ =&gt; format!("ow"), } }
There's also the closure abuse: (|| { if i &gt; 0 { if let Some(i) = o { if i &gt; 2 { println!("oof"); return; } } } println!("ow"); })();
Blockchain! https://solana.com
That's like saying Linus Torvalds created Ubuntu because he wrote the kernel.
I'm aware of the differences between lxd and docker, but wasn't aware of how simple it would be to make an image, thanks!
... deduplication engine: rdedup ... I'm not sure if it is really the fastest, but I think it's highly probable.
The `ArrayVec` answer that Shepmaster linked to in there is excellent. Here's a [playground example](https://play.rust-lang.org/?gist=a0f9fedcb1b939917d39d1550949e7a6&amp;version=stable&amp;mode=debug&amp;edition=2015) demonstrating it.
Entity Component System. A simple usage of specs (in Rust) was compared with a naive usage of Unity. Specs was 5000x faster. https://forum.unity.com/threads/benchmark-performance-of-ecs-systems.546836/ Applying burst like the replier suggested correctly, Unity was 9 to 17 ms, while specs is 8ms.
To me the syntax you're suggesting feels a little too... magic, for Rust. I don't have a huge exposure to different languages but I've yet to see one that lets you copy between arrays like that.
Not a migration guide, but this tracking issue links to all the PRs that were required to migrate trust-dns from tokio-core to tokio https://github.com/bluejekyll/trust-dns/issues/385
As I said, I need to bind variables in guards and pass them through functions to check the needed conditions. So this is not possible without nested `match`es, which is bigger in size than the OP. I should have clarified this better I guess.
`fd`. 
&gt; Data parallelism... thingy Have you used `accelerate`? 
Perhaps you could create a motivating example that more closely resembles your code? Or, if it's available online, just link to the original source?
Can I use this version to run futures of nightly or do I need the futures library from crates.io?
What happened to the "Post the cool thing you working on" thread? I quite enjoyed reading those from time to time. 
After some more research and play around i have found out the you can not reregister with the same token (for some reason it does not call next operation if you reregister element with the same token). To verify that i have used slab and just change one small part of the code where i first reregister with old token and with new generated from slab. Everything worked fine after i used slab new key for each action. Is that the right behaviour ? 
I don't want benchmarks, just real live apps. Benchmarks are biased, apps are not. Rust hides complexity from the dev without a GC. I think this is a big selling point. Sure, the devs deserve appreciation but I think it's pretty hard to write something like Rayon in other languages without adding runtime checks.
And \[Here is a PR\]([https://github.com/ChrisMacNaughton/proto\_benchmarks/pull/2](https://github.com/ChrisMacNaughton/proto_benchmarks/pull/2)) that adds quick-protobuf values. It is faster than capnproto for lot of tests,
It's up on Mon-Wed/Thu (Time difference) every week, taken down to sticky TWIR as Reddit only allows 2 sticky posts.
&gt; Low level graphics portability layer (gfx-rs, soon) Why is gfx-rs the best?
You are right, I wasn't clear in my post at all. Here is an excerpt: (what the methods or types actually are I suppose is not relevant): loop { if x &gt; 0 { if let Some(cell) = grid.get_mut(x - 1, y) { if let Some(adj) = ch_to_bin(cell.ch()) { if (adj &amp; 0b0001) &gt; 0 { bin_set |= 0b0100; break; } else if adj == 0b0100 { cell.set_ch(bin_to_ch(0b0101)); bin_set |= 0b0100; break; } } } } bin_set &amp;= 0b1011; break; } 
Not to mention that it doesn't have many of the features other emulators it compares with have.
&gt; I don't want benchmarks, just real live apps. Benchmarks are biased, apps are not. You can't decide on which app is the fastest without a fair benchmark *of the apps* for various tasks under various circumstances. You'll need to make sure you aren't measuring the JIT warmup time for JIT-compiled contenders, you'll have to do cache warmup rounds and so on, just to establish a fair baseline for everyone. For network applications, you need to account for network latencies and packet loss. You cannot simply look at apps under random circumstances and say "oh that's gotta be the fastest of the bunch". &gt; Rust hides complexity from the dev without a GC. I think this is a big selling point. Indeed, but with sufficient determination you would still be able to write a very slow implementation. Absence of a GC does not mean one does not need to optimise their programs. &gt; I think it's pretty hard to write something like Rayon in other languages without adding runtime checks. I can't comment on the implementation detail of rayon, but bear in mind that Rust does lots of runtime checks as well, and that for example `Rc`, `Arc` and `RefCell` defer the entire borrow checking process to runtime.
Soon™️ doesn't really count, does it?
AFAIK, the benchmarks are laid out in the order defined by the user.
There are two issues which will keep me on setuptools-rust and you resolved one of them. &amp;#x200B; What you did resolves the workflow side of things nicely, but it doesn't resolve the fact that it'll reflect badly on my hybrid projects if running \`[setup.py](https://setup.py) install\` either fails mysteriously or produces a broken install and all I can tell people is equivalent to "There is no circumstance where calling \`cargo\` directly can produce useful output. You must run \`build.sh\` instead." &amp;#x200B; (Yes, there \*are\* situations where that occurs in the Rust ecosystem, such as stuff that requires \`xargo\` or projects complex enough to need something like \`x.py\`, but those also require nightly Rust and I have yet to see a situation where someone in the Python ecosystem was required to do that.)
There is always the trick where you just introduce an additional variable, e.g.: let mut perform_default = true; if x &gt; 0 { if let Some(cell) = grid.get_mut(x - 1, y) { if let Some(adj) = ch_to_bin(cell.ch()) { if (adj &amp; 0b0001) &gt; 0 { bin_set |= 0b0100; perform_default = false; } else if adj == 0b0100 { cell.set_ch(bin_to_ch(0b0101)); bin_set |= 0b0100; perform_default = false; } } } } if perform_default { bin_set &amp;= 0b1011; } or something silly with `Option`, e.g.: let mut set = None; if x &gt; 0 { if let Some(cell) = grid.get_mut(x - 1, y) { if let Some(adj) = ch_to_bin(cell.ch()) { if (adj &amp; 0b0001) &gt; 0 { set = Some(0b0100); } else if adj == 0b0100 { cell.set_ch(bin_to_ch(0b0101)); set = Some(0b0100); } } } } match set { Some(n) =&gt; bin_set |= n, _ =&gt; bin_set &amp;= 0b1011 }
Thank you for the heads up. I will have another look. Is it possible to stream results yet instead of loading all result rows in a Vec?
&gt;This is not entirely true. That is a quirk of Cargo, not Rust. The Rust compiler handles it perfectly fine, as you've demonstrated. My mistake, I have changed it to correctly reference Cargo. The system allocator is not the primary reason to use nightly, a bunch of byte wrapping is in the implementation e.g. *to\_ne\_bytes .* Also the underlying FFI using a few features from nightly. &amp;#x200B;
I whacked up abomonation version of your benchmarks (using Rust types, rather than external prototypes) and you get numbers like: ``` basic_read/capnp time: [39.385 ns 40.643 ns 42.263 ns] basic_read/protobuf time: [54.869 ns 55.502 ns 56.180 ns] basic_read/abomonation time: [334.31 ps 337.63 ps 341.03 ps] basic_write/capnp time: [56.530 ns 57.216 ns 58.003 ns] basic_write/protobuf time: [80.567 ns 82.787 ns 85.505 ns] basic_write/abomonation time: [2.8474 ns 2.9356 ns 3.0609 ns] complex_build/capnp time: [404.94 ns 418.50 ns 434.08 ns] complex_build/protobuf time: [297.80 ns 301.58 ns 305.86 ns] complex_build/abomonati time: [90.628 ns 92.171 ns 93.865 ns] complex_read/capnp time: [39.735 ns 40.898 ns 42.423 ns] complex_read/protobuf time: [330.69 ns 334.32 ns 338.19 ns] complex_read/abomonatio time: [1.6751 ns 1.6926 ns 1.7099 ns] complex_write/capnp time: [63.208 ns 64.365 ns 65.718 ns] complex_write/protobuf time: [151.67 ns 157.22 ns 164.38 ns] complex_write/abomonati time: [16.569 ns 16.996 ns 17.477 ns] ``` So that folks don't get too excited, Abomonation is very different than the "proto" things, in that it copies around native Rust types rather than proto-style declarations, which makes it way-unsuitable for many applications (other than "I want this object to show up in another similar Rust process on the same architecture really fast"). You have to type `unsafe` to use it, though, so it's all good.
I'm still posting it every week, but we un-announce it to make place for This Week in Rust, which is deemed a vital resource for the community. If you scroll a bit, you should still find it.
&gt; There is always the trick where you just introduce an additional variable I wonder if this gets optimized as jumps instead of flag comparison, but I don't have the patience to look through assembly 😶 This solution seems best despite the indirection.
Gotcha. Cool. I guess i tend to read the subreddit on the weekends and miss it. Thanks for the heads up! Also thanks for your work on TWIR and rust stuff in general!
I think all frameworks are somewhat punting on database-oriented benchmarks until `async` support comes, probably early next work, since it'll completely change the idioms. As such, there's only really two benchmarks over which comparisons are interesting as far as the speed of the language goes: JSON and plaintext. I find the difference of performance between actix-raw and tokio-minihttp on the JSON benchmark interesting; I'd like to see the difference of approach in serialization that explains the reversal compared to plaintext.
`to_ne_bytes()` is going to be stabilized in the next release, although I wonder why do you use native-endian in the first place.
"Best" is not really meaningful if you don't specify what you consider "best".
Maybe ask someone at Catalyst, they do Perl meetups and such.
It seems like you could trade off having each pinned for a few days at a time or something, or would that get tedious after a while.
Best: "you only need to be safe in safe code" 
To really learn Rust, I'm having a go writing an artificial life system based on [Tierra](http://life.ou.edu/tierra/index.html), initially just the simple early version from 1991 or so, but once that's working I hope to try to make it extensible similar to how the original developed over the 90's. Or maybe make my own developments, we'll see. For now I'm mostly squabbling with the borrow checker and reading about standard libraries that I can use, but the project is interesting to me, and chunky enough to be a "real" software engineering thing requiring actual design so I can learn more than with small toy projects about how parts fit together.
&gt; It seems that this will only accept a single concrete type, since the struct only has a single type parameter, which can't be right. It is right. I liken generic types (be they traits, structs or enums) to *blueprints*. That is, those are not types *just yet*; they are blueprints to create types, and only when all their generic parameters are decided to you get a full-blown type. This means that whenever a type is required, you cannot pass a blueprint instead: it's not a type! So, you cannot have a `Vec&lt;State&gt;` but you can have a `Vec&lt;State&lt;i32&gt;&gt;` or a `Vec&lt;State&lt;T&gt;&gt;` (provided `T` is a `struct`/`enum`, not a `trait`). If you wish to store multiple kinds of `T`s, you have multiple choices, depending on what makes sense for your application: 1. Use multiple `Vec&lt;State&lt;...&gt;&gt;`. 2. Use a `Vec&lt;Enum&gt;` where `enum Enum { A(State&lt;A&gt;), B(State&lt;B&gt;), C(State&lt;C&gt;), ... }`. 3. Use a `Vec&lt;State&lt;Enum&gt;&gt;` where `enum Enum { A(A), B(B), C(C), ... }`. 4. Use a `Vec&lt;State&lt;Box&lt;Trait&gt;&gt;&gt;` where all potential Ts implement `Trait`. Which choice is right really depends on your application. Notably, does it make sense for `State&lt;Enum&gt;` or `State&lt;Box&lt;Trait&gt;&gt;` mean that a `T` can create a `U`; is it sensible? If not, it's not the right point for polymorphism; if yes, then it's likely the right point!
&gt; Fastest alone is irrelevant, the meaningful criterion is best. (Speed may be a factor in this, of course.) Puh-leez. Performance can be measured. Relative to speed, a nebulous term like "best" actually carries much less meaning. The only "best" is "best for my use case." In some use cases, minimal but fast _is_ best. In other cases, being robust or featurful may be more important. 
Wow! Sarebbe carino creare un gruppo/community italiana :)
I believe that the estimate is “within a few weeks”.
Cool, thanks :)
Isn't that what is happening? Like WAYWO Mon-Wed and TWiR Thu-Sun
I think that was the original idea but it seems like TWiR is being left pinned all week now. I honestly haven't been paying attention.
The Ion shell is the fastest system shell. Both due to efficiency and a simpler syntax. Defeats Dash in a number of scenarios, including feature-wise. Method expansions are more efficient than subshells, so if the script contains tasks that can be accelerated via the let arithmetic or a method expansion (highly likely), it will best Dash. At System76, I've been working on the fastest Debian repository tool. Downloads &amp; builds packages in parallel, scans Debian archives in parallel, and generates dist files in parallel. Everything that can be is done in parallel with Rayon. Will be the most feature complete apt repository tool. Currently creates apt-file compatible repos, with Deb repackaging and AppStream support coming soon. Likewise, Distinst is the fastest Linux distribution installer, which is getting faster as many steps are now getting highly parallelized. The only thing holding back install times at the moment is the speed of Apt at installing and removing packages. We may have to write a parallel dpkg &amp; apt replacement. Popsicle is the fastest multiple USB flasher. ISOs are buffered on demand as they are written to each USB target in a separate thread. 
That's what we already do – have the weekly thread pinned Mondays to Thursdays, then pin TWiR.
Ah, but that's not fast. There's a way of doing things that the objc runtime api suggests, and then there's the way clang actually does things. RustKit is generally aiming to do things as clang does.
In OP’s code State is a trait, so `Vec&lt;State&gt;` isn’t going to work, generic or not. You’d need some sort of indirection anyway.
Yes, we know. Unfortunately due to venue availability and time constraints we didn't find another date for it. However, if you can't make it to RustFest Rome, the next RustFest will be around in no time.
This *might* be the fastest cryptographic hash function, but I haven't done enough comparisons to be sure. [`blake2b_simd`](https://github.com/oconnor663/blake2b_simd)
The C bindings bind to TagLib's C API, which don't expose the picture API. I'm in the process of [writing my own C API over TagLib's C++ API](https://github.com/RonnChyran/seiri/tree/libkatatsukicpp/katatsuki/libkatatsuki-sys/libkatatsuki) and using that, but I had trouble getting CMake to cooperate before.
By convention you mean *.d files or dependency information in general? 
Nice. As for quickcheck, there's also [`proptest`](https://crates.io/search?q=proptest), which I think has the right idea in terms of API.
Link goes specifically to the part at the end where they talk about Go 2. Relevance to Rust: * Go's philosophy of "asymptotically approaching boring" makes an interesting contrast with Rust's "stability without stagnation". * The question of how to do a new release that makes serious changes without fragmenting the ecosystem seems similar in some ways to what Rust is tackling with Rust 2018—including not only the need to avoid a semver bump, but also the question of a "marketing release" and what it signifies and whether it's a good idea. * The biggest pain points that Go's users report are all things that Rust has come up with its own answers to.
Yes, that is what I did. The comment is just that there isn’t a profile for it or a solution for custom profiles at the moment as far as I could find.
Trust me, *I know the feeling*.
Leaving aside the difficulties in meaningfully measuring speed... that's exactly my point!
For pulling out the value of a locally computed and queried hash (it is used as an identity function in our broader implementation), the endian-ness is not critical as long as it is consistent.
And there is a PR to add a best proptest_derive to go with it.
This one should be OK. :D
As far as I understand, proptest uses state to decide where to go next, which is beneficial for shrinking. On the other hand, quickcheck is faster in creating test cases. So there are pros and cons to both.
still not entirely certain what a service mesh is
I don't see any immediate reasons to want to write code that is generic over the different constructors, but that doesn't mean there is none. Especially with generic and abstract code, there often isn't a neat simple example for why it's useful. Look at for example the abstract concept of a monad: it's used in lots of code but it's sort of hard to explain why it's useful, at least with an simple or elegant example. All in all, I am strongly against making certain (abstract) code impossible to write in favor of arguable simplicity in the common case. Especially in the standard library. If many different types all implement the exact same method(s) with the exact same signature(s), it should be a trait.
*.d files. Given that it's Unix, there is no official extension; you can name files whatever you want. But people were using *.d as a convention for these dependency files for a long time. The classic (and controversial) paper ["Recursive Make Considered Harmful"](http://aegis.sourceforge.net/auug97.pdf), used this convention in 1997, and the way they discuss it sounds like it was already an established convention.
It requires futures 0.1. We are targeting stable. 
Now we can't mock JS for leftpad anymore
Here is a link to the full announcement on GitHUB: [https://github.com/rust-embedded/wg/blob/master/newsletters/2018-08-2x-psa-cortex-m-breakage.md](https://github.com/rust-embedded/wg/blob/master/newsletters/2018-08-2x-psa-cortex-m-breakage.md)
EPIC FAIL! The program does additional things that are not part of it defined behavior. Not only does it add 1 to a given number, it has the audacity to print out this result to the standard-out.
Thankfully, I [submitted a PR](https://github.com/02sh/add-one/pull/3) for that and a horrible performance regression caused by heap allocation.
Thank you for the diligent weekly updates! I wonder if it would be easy to include a link to all these weekly threads in whatever thread is stickied? Something like: Title: Weekly Threads and Updates Body: TWiR [link] Got an Easy Question? [link] What are you working on? [link]
What exactly is that blinded selection?
You should post a response here when you've got a meeting time set up.
&gt; Nightly Rust is switching to use LLD (LLVM's new built-in linker) as the default linker yesyesyesyesyes &gt; for ARM microcontrollers Oh well, I'm still happy for the ARM developers out there. LLD is a huge (~2x) improvement for my debug-mode build times, hoping to see first-class support make its way into more platforms soon.
Even if all it does is reduce the attack surface, that still makes it safer.
Might want to put humblebrags on /r/rustjerk
To answer the latter question, the `?` operator is handy here. `let f = File::open("hello.txt").map_err(MyCustomError::FileOpenError)?;` 
Having a `From` implementation that uses the ErrorKind to decide on which variant to use would then let you use `map_err` and `?`, so I don't see any downside. Is there a particular issue holding you back from the idea?
In the end, I build my program with target `x86_64-unknown-linux-gcc` with the oldest glibc version possible, using a docker container with some old distro. Looks that it works for computers with versions of glibc newer than the one installed on the build machine.
&gt; also makes `mem::uninitialized()` panic when it is called on an uninhabited type That sounds bad. I've seen code intentionally invoke UB in dead branches to aide optimizations, so this is a breaking change.
I believe let mut f = match f { Ok(file) =&gt; file, Err(e) =&gt; return Err(MyCustomError::FileOpenError(e)), }; can be written: let mut f = f.map_err(|e| MyCustomError::FileOpenError(e))?;
Even if you don't want to match in the from impl... .map_err(|e| { match e { ... } }
Even `map_err(MyCustomError::FileOpenError)`.
`DelayQueue` is going to make my life much more pleasant.
I'm aware of them, and I find TUF them satisfying. It's somewhat orthogonal to this idea, but in practice, I think if this one is working TUP is probably not even needed.
[removed]
A service mesh is basically a middleware between your transport and your application, basically the service mesh sits in between each request and allows to perform manipulations on the incoming/outgoing traffic or can simply be used for monitoring. The most common uses I've seen are * Monitoring: the service mesh sits between every request and your users, so it can log requested path, latency, statuscodes, ... In the "microservice" buzz many service do not have logging capabilities built in so this kind of monitoring should be external, this feature is by the way not useful with applications that have this kind of metrics built in (NGINX access logs) * Traffic shadowing: the service mesh replicates the traffic of the "real" application to other endpoints, this can become useful during testing (it's literally your production data that you are receiving) * Blue/Green deploys: combined with the monitoring stated above, you can progressively shift your requests from a version of a service to the next one, or even weird rules like 99% percent of the users talk with the "stable" version, 0.9% with the beta one, 0.1% with the latest CI built version * Automatic TLS: to secure services transports without having to learn what each service requirement is By the way, this does only secure the transport, the good things like X509 client auth can't be done this way * Super-granular rules: like each subpath under a domain gets routed to a different service It is a very powerful abstraction but PLEASE do not sell it to someone for its CRUD application, it would be like killing a fly with a bazooka. 
Yes. It's besides the point though. Even in places where an Unsized type is accepted, `State` would not be because it's not a type.
We still can, it's not a library :)
Curious: is this the first target to use LLD?
Awesome! Will RISC-V get LLD as well?
for poor (n00b) programmers like me. what the hell is going on here? or should I skip this as many other news.
I think this is a fundamental misunderstanding about setuptools: `setup.py` is basically a pythonic version of `build.sh` (or maybe rather a `Makefile`) and setuptools-rust is just a packaged version of `x.py` for pyo3 and rust-cpython. It's not even possible to use setuptools for any bigger project with native components without monkey patching. For projects with setuptools-rust you need to have cargo installed and rely on some hack to get setuptools-rust installed in the first place because there is an unresolvable cyclic dependency in `setup_requires`, and the implementation is just calling cargo and moving files in directories where setuptools will pick them up. &gt; (Yes, there are situations where that occurs in the Rust ecosystem, such as stuff that requires xargo or projects complex enough to need something like x.py, but those also require nightly Rust and I have yet to see a situation where someone in the Python ecosystem was required to do that.) I had a look at a few popular packages with native extensions. All of them have a highly customized setup.py that contained lots of custom scripting; numpy even has its own distutils (see e.g. [matplotlib](https://github.com/matplotlib/matplotlib/blob/master/setup.py), [pyyaml](https://github.com/yaml/pyyaml/blob/master/setup.py), [numpy](https://github.com/numpy/numpy/blob/master/setup.py), [wxwidgets](https://github.com/wxWidgets/Phoenix/blob/master/setup.py), [gevent](https://github.com/gevent/gevent/blob/master/setup.py)). This situation is far from ideal and setuptools also has some other significant problems. This isn't only my personal opinion, but the basis for [PEP 517](https://www.python.org/dev/peps/pep-0517/) and [PEP 518](https://www.python.org/dev/peps/pep-0518/). I highly recommend reading the [rationale of PEP 518](https://www.python.org/dev/peps/pep-0518/#rationale) on this subject. It's unfortunately to long too include it in this comment. Those two PEPs define a new solution to the problems with `setup.py`: A file called `pyproject.toml` which declares which build system to use and which contains the metadata for that build system. Every project can then choose a build system (such as poetry, flit or setuptools), which also gets its configuration for the `pyproject.toml`. This means that it's not "bad practice and unprofessional to require non-standard commands", but rather something that is recommended on a PEP level. Unfortunately, pip doesn't fully support `pyproject.toml` yet. But once it does, you won't need to care about pyo3-pack or whatever is used for just building the package (see [pyo3/pyo3-pack#2](https://github.com/PyO3/pyo3-pack/issues/2)). For developing I deem it acceptable to install a effectively static binary (which could even be automated by putting pyo3-pack on pypi). (The point I'm trying to make here is not that you must stop using setuptools-rust, but that there are good, well discussed reason to move on from setuptools in general. I also won't stop maintaining setuptools-rust.) &gt; I have no problem using a separate pyo3-pack tool for enforcing the extra invariants and doing the extra work necessary for generating and uploading packages which confirm to a spec like manylinux, but it's bad practice and unprofessional to require non-standard commands if a user wants to build from source or do development work. FWIW these are no extra constraints from pyo3-pack, but according to [PEP 513](https://www.python.org/dev/peps/pep-0513/) you're supposed to enforce those yourself on any native wheel by running something like a [build.sh](https://github.com/pypa/python-manylinux-demo/blob/master/travis/build-wheels.sh) in a cent os 5 docker container and then validate the wheels with by running auditwheel. So I thought people might like it if they could skip that. &gt; "There is no circumstance where calling cargo directly can produce useful output. You must run build.sh instead." ​ `cargo check` and `cargo build` work as you would expect, no matter if you use setuptools-rust, pyo3-pack or handcraft wheels bytewise. It's not possible though to connect cargo with python without a bit of scripting, e.g. the symlink solution in my first response. [rust-lang/cargo#1970](https://github.com/rust-lang/cargo/issues/1970) could improve things to a state where a hacky cargo only solution would be possible, but it doesn't look like it is ever going to be implemented. Also note that cargo doesn't even fully support the linking we need, for which I've filed [rust-lang/cargo#5881](https://github.com/rust-lang/cargo/issues/5881) and [rust-lang/cargo#5928](https://github.com/rust-lang/cargo/issues/5928) and which is why we need the `.cargo/config` workaround on mac. 
Yeah these solutions look great, thanks! Also thanks /u/nicoburns! 
I was going to say that this only works if `MyCustomError::FileOpenError` is a function, but from context it's probably a newtype class. But then [I tried it](https://play.rust-lang.org/?gist=0f90c8c31fbf5206498e45fef023ed1b&amp;version=stable&amp;mode=debug&amp;edition=2015), and it _still works_ on newtype classes. So, whenever I try to use the name of a newtype class as an object, somehow it seems to act as a function object. Is this how they're treated in normal cases? If I write `let x = MyCustomError::FileOpenError(something)`, does the compiler just treat this as an ordinary function call internally? More importantly, where is this behavior of newtypes documented? I'd like to read related documentation to see what else I've been missing. I assume it's somewhere in the rust book, but all I've found is [this](https://doc.rust-lang.org/1.0.0/style/features/types/newtype.html), which doesn't mention newtypes being able to act as a functions.
`mem::uninitialized()` was never supposed to be used for that kind of optimization hint, that's what we have https://doc.rust-lang.org/beta/std/hint/fn.unreachable_unchecked.html for.
People were already complacent presuming that all packages they use are safe.
I wasn't aware nightly was such a blocker for many. Since proc macros are going to land in stable for 1.30, the only remaining nightly feature is specialization, and it seems worth the effort to get pyo3 to stable. I've removed those specialization uses that were easy to get rid of and opened [PyO3/pyo3#210](https://github.com/PyO3/pyo3/issues/210) to track the remaining ones.
Great info. I'm in the same boat (doing a relational lang [https://bitbucket.org/tablam/tablam/wiki/Home](https://bitbucket.org/tablam/tablam/wiki/Home), in prototype phase) and also hit the rust wall a lot. I think will benefit everyone to setup a reddit for how build a lang with rust? I wish to ask a lot of question there :)
Hey, so I've been working on .Net interop myself. There's two ways you can work with C# interop 1) using COM calls and bindings - you need to add COM attributes and interfaces in C# and then write bindings in Rust 2) Hosting the CLR in unmanaged code, and starting the assemblies in the CLR host, and interacting with them that way. Well, there's apparently 3 ways now. I've been working on various sets of bindings and safer layers on top of those bindings, but as you note, its a very large project. 
Enum constructors are functions.
Unless you're working with ARM microcontrollers, you can probably ignore that. If you _are_ working with ARM microcontrollers, some things will break in the coming days, but it's all worth it because it means you won't need anything but Rust itself to build ARM programs any more.
It's not just enum constructors though. It seems to be all tuple structs, yet not ordinary tuples named with `type`: https://play.rust-lang.org/?gist=32f19ee26ec3f5e177f473a5b2f9b024&amp;version=stable&amp;mode=debug&amp;edition=2015 (You can try it with tuple structs that take multiple data, like `struct MyTuple2(i32, i32)`, and that also works).
Avoid it like the plague or cost the project such that you will have to pay really exceptionally talented problem solvers to create exceptionally asinine code
The backward compatibility rules allow fixing bugs, especially if soundness issues are at stake, so I would not fear too much. Rust is, before all, a language for the pragmatic by the pragmatic.
Did I understand correctly that your TablaM is comparable to SQL for instance, but it combines manipulating both structured databases and key-value/document databases (and even more?) under one roof? Sounds interesting!
That is part of the goal, but more simply, is just make relations first-class : &amp;#x200B; [http://www.try-alf.org/blog/2013-10-21-relations-as-first-class-citizen](http://www.try-alf.org/blog/2013-10-21-relations-as-first-class-citizen) &amp;#x200B; Most people always marry "relational" with "data store" and that is ok, but I'm aiming at a lower, simpler level: a general purpose language. &amp;#x200B; BTW, that was how the dbase family operate: &amp;#x200B; [https://en.wikipedia.org/wiki/XBase](https://en.wikipedia.org/wiki/XBase) &amp;#x200B; where the database was integral part of the language. Is similar how is the integrated languages for make stored procedures in RDBMS. Mine is "just" that, but outside the storage so can be used everywhere.
ECS for AST/HIR transformations? Interesting.... The "ideal" IDE-ready "libsyntax2" tree that u/matklad has been tinkering on is based off of an untyped (tree) graph done via indices. For now my toy project is just using a structural AST and a canonical grammar to parse, but I plan to use something like that in the future once matklad's explored the space a bit more or I've spent more time going over what he's done. But now I'm thinking about handling the HIR level (where it's no longer syntax/parse bound but rather you're doing type inference and other expansions) in specs. It just might work....
&gt; Split objects into tables (struct of arrays style), and replace all references with IDs (into those tables). I will love how do this applied to language building. I already have a internal core with both column-format AND row-format and see how can apply it to build the lang will be very neat.
Using COM only works on Windows, which was a non starter for me (although I ran into that hurdle anyways, I bet on the good chance that it would eventually link properly on Linux). Hosting the CLR was too much yak shaving for a simple music sorter app, but might actually be a better solution than using CoreRT. Good luck on your project!
Well, the Rust compiler is written in Rust - it was initially written in OCaml, but it's now rewritten in Rust itself, so if you want to build Rust, you can either use an existing Rust compiler binary you got from somewhere, start from the OCaml compiler version and then keep building newer versions of the Rust compiler, or use a third party compiler like [mrustc](https://github.com/thepowersgang/mrustc) that's _not_ written in Rust. C/C++ mostly does the same thing - GCC and Clang are both C++ programs that you need a C++ compiler to build.
The binary format is designed to be efficient when you have large numbers of the same message in a single block; it is very inefficient at encoding one message at a time.
plz tell what to do, is it just renaming main.rs to lib.rs and make add_one public ?
I think implicit O(n) would be an issue.
Pretty much, though you need to keep a `main.rs` if you want to keep the executable binary. Modules reachable from `lib.rs` are part of the library crate. Code reachable from `main.rs`'s `fn main` is the executable.
`unreachable_unchecked()` was unstable until recently, that's why such code exists. Overall, I agree that deprecating is less bad than keeping a major footgun in the language.
For things like classes and selectors, they aren't normally registered with calls to the objc runtime. Instead, data structures containing appropriate information are placed into specific sections in the output and the objc runtime takes care of everything before the program starts. Objective-Rust actually gets this right. If you look at [HOW_IT_WORKS.md](https://gitlab.com/objrs/objrs/blob/master/HOW_IT_WORKS.md) there, it examines the output of clang to inform the output of the code. RustKit goes a step further and automatically generates bindings for the headers based on how clang views the headers, but RustKit doesn't have a way to generate new ObjC classes within Rust (yet!).
Well, I meant for the `own` method to be removed from the trait, since it doesn't have to do with pinning. Maybe I misinterpreted the purpose of the `Own` trait
Wow, that are some really nice tools! Are they available somewhere or are they just used in-house ? Have you been in contact with the Debian project (or other Linux distributions) ?
Sorry for writing that late, I was busy and forgot about this. Why do you think the drop trick wouldn't be any better? `mem::forget` is no-op, so something like: // This should be reasonably elided by the compiler let drop_trick = ClearOnDrop(&amp;mut next_sep); let accum = if let Some(accum) = func(accum, sep) { accum } else { // Here it drops drop_trick, so it will clear the value, same in the case of panic return None; } // This is no-op mem::forget(drop_trick);
&gt; Have you been in contact with the Debian project (or other Linux distributions) ? No &gt; Are they available somewhere or are they just used in-house ? The ones that I mentioned are open source on GitHub. We do have a few private repositories that contain projects relating to our hardware assembly process for upcoming desktops that we are going to start manufacturing soon. Distinst is useful as a distribution installer, but it's also used by our private Trubble imaging system (also Rust), and there's a few Rust GTK projects that the assembly workers will be using. - https://github.com/pop-os/debrepbuild/ - https://github.com/pop-os/distinst/ - https://github.com/pop-os/popsicle
I see, need to read some more about this stuff, thanks and good luck with the project. :)
I just started an LED project; should I wait to continue? I haven't actually gotten anything done, just started setting up my env for it https://rust-embedded.github.io/discovery/
Excellent write up. Accuracy is why I use tokei!
No, this generally provides no performance benefit over explicitly batching with limit/offset, and would require an API that returns `Result&lt;impl Iterator&lt;Item = Result&lt;T, E&gt;&gt;, E&gt;` which is extremely painful to use in practice
Personally I'm not happy with the accuracy section being just the output of the various programs, but I don't know what would be a better way to have this data be easily comparable. I tried making one big table but markdown doesn't support having one column span multiple columns which would be an a nice way to group and compare the outputs. Without this it'd be essentially be the same as the program output but in a table, which wouldn't provide enough value for the amount of effort it would be to create and maintain.
This is an awesome project. The past few weeks I've been writing a spec for a language I'd love to implement when I get more time. How is this implemented? Are you writing an interpreter? Do you plan to release the code? It'd be really fun to read through this. 
This looks like blatant name squatting, surely someone on the Rust team can swing the hammer on this?
The [crates.io policies page](https://crates.io/policies) says that package ownership will not be handed over for squatting, but I hope that something is done about this. As rust's popularity increases the problem of squatting names with no intent to create and distribute will only become worse, and genuinely good packages will have to be hidden behind less intuitive names purely because of the name they want being taken. This is something that's seen in NPM and it would be a real shame to see the same situation happen with cargo. 
Here is an ECS approach that is being experimented with for use in GUI programming: [https://www.youtube.com/watch?v=4YTfxresvS8](https://www.youtube.com/watch?v=4YTfxresvS8) I believe this is similar to the approach that you are looking for an example of.
I've just made the changes, can you tell me if it's ok ? thanks again.
I have a tutorial series on writing a language VM in Rust you might be interested in. You can find it here: [https://blog.subnetzero.io/project/iridium-vm/index.html](https://blog.subnetzero.io/project/iridium-vm/index.html). Code is at r/https://gitlab.com/subnetzero/iridium
There were issues with certain OpenGL drivers at one point, I'm not sure if those were fixable.
OK, so re-reading the post, it looks like I missed the main point of this trait. The assumption I made was that for any pointer type `Ptr&lt;T&gt;`, you can take a `Ptr&lt;T&gt;`and trivially turn it into a `Pin&lt;Ptr&lt;T&gt;&gt;` just by wrapping it. But that's not true in general: the whole point of `Pin&lt;Ptr&lt;T&gt;&gt;` is that it tells you that the `T` that it points to **will never be moved**. The only pointer types that you can safely wrap in `Pin` are those that own the `T`, which is why you called it `Own` in the first place. Now that I understand that, I see what you are doing: letting types implement the `own` method, and providing a default `pinned` constructor that can be called from safe code. As an alternative that might be cleaner, you could rely on `DerefMove&lt;Target=T&gt;` as proof that `Ptr&lt;T&gt;` owns the `T`, and provide a constructor that takes a `Ptr&lt;T&gt;` and returns `Pinned&lt;Ptr&lt;T&gt;&gt;`. This, obviously, requires that we have a `DerefMove` trait, but I think it's generally agreed upon that it will be added at some point.
This is unfortunately not a unique thing and I think so far the answer is that nothing will be done about it. I saw something similar about a month ago and I got the same answer. Here's my tweet about it: https://twitter.com/Sunjay03/status/906261293497282560?s=19 It's good to keep talking about it. Maybe a policy change will occur. Thanks for bringing this up!
The user's github account was created 7 hours at the time of this writing.
Maybe that's how this is solved, if it's proven unsustainable without actually causing problems this would be solved pretty fast.
yeah so the reason I didn't want to do this was because io::Error can be like tons of things. Like the file could not exist, or later in my function when I go to write I actually didn't have the write permissions, or whatever. I wanted users of my library to have really specific errors to work on, so just catching all io::Error wouldn't work. You could also mean that I could, in the From impl, look at the ErrorKind, but the code from above is what is I think is best, so I'm going w/ that.
Ah, non sapevo! Non sembra pubblico comunque, come faccio ad avere l'invito?
Maybe [crates.io](https://crates.io) could require some prefix (e.g. username- or username\_ or smthg like this) for all new crates? It would be backward compatible and could solve the problem without huge change
I think at this point it’s fair to say that at not having namespaces for packages has become a problem, it’s been a problem on npm for a long time so I’m surprised there hasn’t been a bigger push for it in cargo and the greater Rust ecosystem. Having a few persons being able to squat packages like this in large quantities and have a policy that says that for the most part you do not return them will become a large problem later, if it isn’t now. 
I don't think you need to look at `ErrorKind`. Instead you can include the `io::Error` as in `MyCustomError::FileOpenError(io::Error)` The power of rust's error handling is that you can the error conversion logic is in the errors (with `impl From&lt;io::Error&gt;`), instead of infiltrating logic code.
Damn. My first thought was to namespace all the packages but this is a really good argument against that. I really don't see a solution to this issue now. Though, with this guy, anyone trying to add a package will have to get really creative 😂
Good point. Though now that I think about it, it works today for equality (because of the implicit reference taking, I think), which is also O(n): if slice1[0..5] == slice2[5..10] { ... }
Classic patent trolling.
Java's domain based module naming system doesn't seem so bad now.
Being able to reserve names that also occur in the stdlib doesn't seem good either (e.g. [vec](https://crates.io/crates/vec) vs `std::vec`). Perhaps following in Node's steps and adding [security placeholders[(https://www.npmjs.com/package/fs) might be a good thing to do.
Just because my package would be namespaced doesn't mean I would start using boring names, though. And the couldn't the explicit namespacing could be optional for when the package name conflicts. Is it more complex? Yes. But it allows for first-class crate forking, prevents squatting, etc.
Considering I've never written a line of code in either, it is actually not odd at all!
When you do a benchmark, you want to measure peak performance because only then you get useful results. For example, you don't know whether you need to apply more optimisations to your function if you can't tell how much of its measured runtime comes from outside factors.
Definitely looks like someone running an "exploit" to try and force na issue. I don't see any problem whatsoever with the Rust team kicking him off and freeing the names up manually.
It's a little bit more intuitive for equality between slices to be O(n), since how else are you going to be able to establish equality without checking each element? You can have a fast-path that tests referential equality first (`self.ptr == other.ptr &amp;&amp; self.len == other.len` for the same element type) though that's not implemented in Rust because we have two kinds of equality, partial and full, and the equals operator uses `PartialEq`; e.g. if you have a slice full of `f32` NaNs it's not correct to say it's equal to another slice of NaNs of the same length even if it's actually the same slice. I think the only thing we need to implement the syntax you propose is to have an overloadable assignment operator, but I think that would make the collective language team collapse into a black hole from all the cringing.
There's a lot more that keeps Java's packaging clean of this. Central (the de-facto standard package repository) entries are vetted before being available, and you need a working product before you can grab a name. It makes it much harder to squat with no or low quality work
A friend of mine said he would like if tokei also showed the percentage of comment lines vs the total lines.
This isn't something the crates.io team will be taking action against, as it is fully in line with our policies.
My top-level component is doing the ws communication and receiving msgs at 30fps. Every msg should only affect some small state in some child with a small DOM surface, but in Yew there is currently no way to update a child's state (and causing only that child's DOM to re-render) without also re-rendering the parent's DOM. And the parent is the main App component that includes all children, so it ends up re-rendering the whole app's DOM at 30fps even though each frame, only a very small child state is changed. That's because children can't be accessed from the parent's `update` method in any way, so `update` can't be called on a child. State changes can only be passed to children as attributes in `html!{}` which is only done when the parent's DOM is rendered in `view()`. I'm looking for a way to update child state without re-rendering the parent's DOM (only re-rendering that updated child's DOM) in a type-safe way. The next step would querying (not just setting child state, also querying child state) in the parent's `update` method. Halogen does both of these things in a type-safe way, even "inline" without requiring introducing another `Msg` variant to handle responses.
How far does spam and useless packages like this have to go before they _do_ cross that line? How hard does it need to be for a typical user to find what they need before such packages get removed?
Would it be possible to do generalized auto-batching/pagination (using limit/offset) with an adapter iterator (possibly defined in another crate)?
Nobody put the work yet 
My worry is that if disciplinary action isn't taken against people who routinely squat package names with no intent of filling them, and such packages are allowed to accumulate, that crates.io as a whole becomes harder for newer users to index and enumerate through. If a user isn't familiar with the brands that crates have built up, a large number of spam and unfilled packages will just make their lives that much more difficult as they search for the crate that does what they are trying to accomplish.
after playing a little bit, I concluded it is too much work to have it working, plus `cross` seems not handle darwin. Thank you all 
Last I checked LLD's RISC-V support is not yet stable enough. Most of the documentation related to LLVM and RISC-V uses gcc-ld.
I literally own the `rustup` crate because I myself tried to 'cargo install rustup' and was surprised to get back a 404. I've freely offered it to the rustup team, but they would probably also be willing to yank that from me :P
I don't believe flatbuffers is a successor to protobuf, but something specifically targeted at games. I think a successor to protobuf would be more complex, and not as simplistic.
I have only a cursory knowledge of the subject matter, but this clearly took a lot of love and effort. Also, thanks for the demonstration of Pest in a real-world solution, I've been looking at it for a while. How was your experience with it?
Rust is _full_ of implicit Ο(𝑛) operations: copies and moves! The Rust documentation [specifies that any type that can implement `Copy` _should_ implement `Copy`](https://doc.rust-lang.org/1.19.0/std/marker/trait.Copy.html#when-should-my-type-be-copy). This leads to surprises [like](https://doc.rust-lang.org/1.19.0/std/primitive.array.html): &gt; Arrays of _any_ size are Copy if the element type is Copy. Moves are also implemented as `memcpy`s. In theory, they can often be optimized away; in practice, they often aren't optimized away when you expect them to be. [Combined, these language issues are a major performance pain point of nalgebra.](https://github.com/sebcrozet/nalgebra/issues/334) 
Ammmmm, I get the idea
gcc(-arm-none-eabi)
You can, but you shouldn't need to.
pest is powerful than regex I thought. It's more clear than it. Thanks for pest, so I can write the parser more clearly than regex-based one. And it took a lot of love really, although it was just a homework machine before. But it's more than a homework machine now.
My solution: If you email them and they ask for money to get the squatted crate which has no code, then proving they did that to the core team should give them ample evidence to transfer ownership.
Finding original names for projects is a pain in the ass. Let it be namespaced using the github username/org.
This is great! Is the math involved in balancing various equations relatively straightforward or is there room for optimization for very large equations? 
ah, thanks!
If people who name squat a lot of crates aren't going to be punished / handled, can we at least get an enabled by default filter on crates.io that hides no release crates?
I would say yes, because there is quite a big push in the ecosystem, technology and documentation for Embedded for the 2018 edition. Take a look at the number of things that `block-Rust-2018` in the rust-embedded repo. Most of those things should be better before the end of the year.
Who’s going to write the RFC for this?
[A method to deal with crate name reservation spam](https://www.reddit.com/r/rust/comments/9adfnq/a_method_to_deal_with_crate_name_reservation_spam/)
The code looks pretty good, although there are a few things I would change. So you have quite a lot of let mut foo = bar; while foo { // do stuff foo = bar } why not just write while bar { //
NEVER use absolute paths and never assume something is somewhere. Use environment variables and the registry
So you have a lot of let mut foo = bar; while foo { // do stuff foo = bar; } why not just write while bar { // do stuff } As well as that, you don't need to use as many calls to `expect` when you can use `map` or a similar function. As a side note, RFCs have another meaning in the Rust Community (language design), so I was mildly confused when I read your post. I's suggest tagging it with something like "Code Review Wanted" instead.
[A method to deal with crate name reservation spam](https://www.reddit.com/r/rust/comments/9adfnq/a_method_to_deal_with_crate_name_reservation_spam/)
Would require every cargo.toml in the ecosystem to be updated. Not going to happen. Additionally, you're basically just salting the crate name and adding a complicated voting layer to shave a couple seconds off adding a crate. Moving parts generating compilation warnings (or stopping them if you get rapid switching of leaders) seems like a bad idea. 
I think /r/ron975 answered my question. They had trouble getting CMake to work when expanding the C API for TagLib.
This is an incredibly stupid reason. I want to name my packages how I want and not how someone enforces me. What I like about Golang is that it uses namespaces, so it's really easy to find a library that you use in your project on GitHub or elsewhere.
But where is that work required? Is it literally just switching some code paths, or are there defiencies in one tool or the other that make it unacceptable?
Not going to happen: why? It can be done just by a script. As to the crates not published to crates.io, they are not in the community, why must we care about them. complicated voting layer: Is it really that complex? I don't think so. compilation warnings: yeah, just warnings, they can still compile.
Optional namespacing for _unambiguous_ crates wouldn't work, somebody could maliciously create an ambiguity that would cause build failures and consequently confusion. Instead, though, I believe if a crate had the same name as its namespace you could refer to it by create name and automatically disambiguate: `serde` =&gt; `serde/serde`&amp;mdash;and this might even be possible to do in a backwards compatible manner.
Sigh. This is why there should have been user namespaces.
"Nokogiri" is a saw. It may be "creative" but I reckon the only reason that has any discoverability today is because the library is so well-established&amp;mdash;at least, it doesn't scream XML parsing to me. Rust has stuff like "hyper", which I also think is _too_ creative a name, and "regex", which is a deeply unimaginative name that offers near-perfect discoverability. The top "new crate" on crates.io right now is "abscissa", whose page includes an FAQ, whose only question is, "why the name?", whose answer is incorrect (other new crates: "drug" and "gypsum"). I could even go further and suggest that encouraging special-snowflake names is counter to Rust's underlying idea of explicitness.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/rustjerk] [More best practices brought over from npm!](https://www.reddit.com/r/rustjerk/comments/9adsi2/more_best_practices_brought_over_from_npm/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
It turned out to not matter anyway; there was a pretty substantial correctness bug in my implementation unrelated to this particular issue (see if you can spot it!). Thanks for the advice, anyway!
How about you namespace it and if you earn it you get a namespaceless alias.
The broken glass is amazing. I didn't expect such a relatively simple algorithm could create such a complex imagine.
As long as &gt; Given the number of years we've had this policy without incident, and that other package managers have existed with similar policies for much longer without incident is the default position it is impossible to discuss this matter, because _this premise is wrong_. These policies demonstrably have _not_ "existed for years without incident", in Rust or in other ecosystems. The point of namespacing is not to solve a _typical_ case of ambiguity, because ambiguity tends to be rare to begin with (proof: Maven Central). It is specifically to disambiguate _when ambiguity arises_. &gt; [...] routinely done by established members of the community [...] that is ok I'd love to hear why that's okay; I don't agree that it is. But more importantly, I consider it an artificially created problem, and needing to justify the distinction serves to prove the existence of said problem: there's your incident.
In case one is doing profiling, it's perfectly reasonable. This discussion was about comparing languages. If one is comparing languages, it might be because he want's to know which one to use for which task. If I was building an alternative to `ls` command line tool, any language that'd take 10 seconds to start the runtime would be absolute no-go event if it was the fastest later. If I intended to create a web server, those 10 second's wouldn't matter that much, but should take some score points anyway because if there's a language that is equally fast, but without those 10 seconds of startup time, it'd be better than the first one.
Clion has also debugging support not tried it my self but definitely worth to at 
Not only would that be a nightmare if it succeeded (two popular crates battling over the same name, leaving all our Cargo.tomls changing every day), but it could be trivially subject to Sybil attacks (unless I started skimming too early where they dealt with, but I can't imagine how anyone could).
Why?
 #![allow(unused_variables)] fn main() { use std::fs::File; use std::io; use std::io::Read; enum MyCustomError { FileReadError(io::Error), FileOpenError(io::Error), } fn read_username_from_file() -&gt; Result&lt;String, MyCustomError&gt; { let mut f = File::open("hello.txt").map_err(|e| MyCustomError::FileOpenError(e))?; let mut s = String::new(); f.read_to_string(&amp;mut s).map_err(|e| MyCustomError:FileReadError(e))?; return s; } } Slightly better, but it still has you doing the mapping manually. You could probably do functions that do the mapping themselves and then just refer to them in the `map_err`. Now if the same error happening in different steps of the function it probably implies they should be separate functions, each returning their own specific error that becomes more generalized as it bubbles up (while still keeping the details). Without more context of what exactly is being attempted I can't say if this is the best solution or not. For now I don't see how to add this context without manually doing it.
Also Swift. But Kotlin and Swift are used for their platforms Android and iOS and not core fxa which is only Rust. 
&gt; As to the crates not published to crates.io, they are not in the community, why must we care about them. Yeah, who cares about people using Rust in production on closed-source code bases? Unequivocally proving to businesses that Rust cannot be trusted not to make sweeping, breaking changes to the ecosystem cannot possibly backfire. There certainly wouldn't be a wave of negative press that will be the only thing most CTOs know about Rust for years going forward, sinking most attempts to use the language. `&lt;/s&gt;`
Imho the solution to this problem is [RFC 2141](https://github.com/rust-lang/rust/issues/44931). Redirecting cargo to different registries will allow someone to build a more restrictive crates.io. I also don't really think a haphazard community voting method is beneficial for problems on crates.io. A kind of conclave of vilified individuals would be much more effective. 
For what it's worth we have a popular open source project and the fact that we own the prefix on npm made life much nicer. No longer do we have to deal with community packages picking up the better package names.
You could make it optional. If there is only one package with that name, you can omit it (but cargo still saves the full name in Cargo.lock so if there is one added lated you project will still compile), but if there are multiple, cargo will throw an error and list the possibilities.
Can we have our namespaces and eat them too? Here's a slightly different idea, that could help with discoverability as well: Let's have namespaces, but only few of them. The current one would get called `experimental`, there would be others: `community`, `nursery`, `quality` and possibly `paid`. * `experimental` is the same as today and it'd work the same way. Anyone could publish to it. Squatting like this could get occasionally cleared. * `community` would only contain crates that are known to be useful. I'm unsure how the progress from `experimental` would work, but probably something like voting + quick look from someone from some curator. (Just basic check whether the crate seems to be doing what's in the description and isn't a copy-paste from other crate. Fork could be accepted if it solves some specific problem or the original author no longer maintains the crate.) * `nursery` is basically a marker for "this crate should eventually get to `quality`, but isn't quite there yet". * `quality` should only contain really great crates. It'd have to meet many criteria like widely used, well maintained, several maintainers, full documentation with examples and tutorial, descriptive name, tests, review from security team, etc. * `paid` could be a way to get additional funding for Rust development by allowing people to pay for getting into this namespace. In order for a crate to get into `quality` namespace, first an RFC would be written describing why the crate is considered useful as well as all deficiencies it has and how they should be solved. After this RC being accepted, the crate would get into `nursery` namespace. It'd signal that the crate is considered useful and actively maintined with the intention to make it high-quality. After all issues with the crate are resolved, another RFC would be created that describes why the crate is considered high-quality and after accepting it, the crate would go to quality namespace. The `quality` namespace would thus also be a great answer for those, who see small `std` as a problem.
I totally disagree with the reason from the link. "Creative" name is workaround, and this reasoning forces us to use workarounds instead of real solutions. Could you imagine the mess if names like 'http', 'regex', 'libc', 'log', 'time', 'gcc' etc. etc. were taken and contained some random code? &amp;#x200B; Maybe there is better solution than namespaces, but this reasoning is definitely one of the very, very few things that are explicitly wrong in rust ecosystem.
Awesome timing! I'm about to present at Composeconf a talk on creating generative art with Haskell in the browser. :) Excellent stuff. 
Interested idea, but I'm unsure how dependencies between the namespace could be done 
Penso che ti puoi registrare su https://rust-italia.herokuapp.com/
Wrong sub, mate - you might be looking for /r/playrust
IMO Java had it right here - domains as the convention for package namespaces. Any package manager can then define their tie-break policy as 'prove you have the domain'. The cumbersome nature of packages is not a problem with namespaces but Java's lack a way to alias them. If a package becomes dominant, the community tends to recognise that and refer to them well-known shorter names. And if someone tries to co-opt that branding then there's a clear way to distinguish them. 'Creative' names seem to require too many creative solutions to the problem and that tends to result in the opposite of discoverable and add pronunciation and transcription challenges. These in turn encourage typo squatting without the added barrier of domain registration. At 'use' time you almost certainly want to alias but you're usually only using one crate covering a particular problem domain at a time so you're not likely to find it hard to choose obvious short aliases. Indeed the crate mechanism could suggest an alias - but at least there's a clear way to untangle collisions. The lack of namespaces seems to require creative solutions with non-obvious outcomes or just hope that there aren't bad people with the desire to exploit or inconvenience us. 
That would be really annoying. Have a single, simple to remember name is very valuable. 
So what if someone maliciously attempted to squat *all* crates? Sure, it will take time, but with a botnet and sufficient dedication one could effectively make crates.io unusable. How would that be handled? (To clarify, in case my wording made it sound that way: There is no threat attached to this comment, it is purely hypothetical.)
Oh, they actually can ... https://doc.rust-lang.org/1.6.0/book/strings.html &gt; Additionally, unlike some systems languages, strings are not null-terminated and can contain null bytes.
Some of the names reserved by swmon are Rust keywords like the [if](https://crates.io/crates/if) crate. The "cargo new" command does not allow to create crates with a keyword as a name and has a [blacklist](https://github.com/rust-lang/cargo/blob/db48c63b3641a29a20b572ed516c58c84450e3bd/src/cargo/ops/cargo_new.rs#L131). Perhaps this blacklist should also be enforced on crates.io?
My bad, thankyou &amp;#x200B;
CLion's debug support is absolutely awesome. It bundles GDB Debugger on Linux, but I had no problem [installing GDB as a third-party on Windows](https://www.gnu.org/software/gdb/download/). CLion requires a license though, but you can have one for free if you are a student. Definitely worth checking out.
OK, I should not say "why must we care about them". But the only thing they need to do is just download the new cargo and use the new cargo to compile their projects, then their Cargo.toml will change. I don't think it is hard.
It's apparently not against the policy to do that so I think it would be a good idea. When people can't find a free name for their crate anymore, maybe some change will happen.
[removed]
To second "and things like that": Most debuggers also show thread names (so it's way easier to figure out what you're looking at/for) and htop also can show thread names (its just nice to know why an app is eating your CPU resources). That said, I think it's nice for everyone if the names are descriptive, but there's no need for uniqueness (e.g. "DB IO worker" is fine, even if there are multiple of them) 
I think it'd be fairly easy - just specify e.g. `namespace = "quality"`. Alternatively, one could specify default namespce for whole project. Also, I'd expect `quality` crates to never depend on anything outside of `quality`.
I am playing (again/still) with the idea of running compute intensive tasks on FaaS services via WASM - this post is a step towards the next post about running a TSP solver in Rust vs a TSP solver in JavaScript :) I hope you enjoy the read, but I'd like to improve as well, so if you have feedback - please tell me. &amp;#x200B; Have a great Sunday everyone. 
`OsString` can't be easily displayed and it doesn't guarantee absence of non-zero bytes either. Another string type might be handy.
I find it amusing that there’s all sorts of solutioneering to the problem in this thread but no one has simply decided to reach out to the person responsible to ask them what the deal is.
I would be very interested to hear about your experiences hiring Rust developers in Poland. I'm from the UK but I'm working on a new startup with a Polish partner, who is based in Warsaw and the current plan is to eventually hire a dev team in Warsaw, or possibly Lodz. The main reason I originally decided not to use Rust is because I don't have a good expectation of what hiring will be like. But actually, we're not so far along that we couldn't switch.
Wow that's nice. Do you think you will get to a stable state in the following months? Or is that a years level thing?
I don't think it would be an issue actually. `cargo add` would take care of recording the namespace `github/&lt;user&gt;` or `gitlab/&lt;user&gt;` or whatever; and the user would only get prompted if there are multiple namespaces to choose from.
**update**: the add_one function has been improved, no more BigInt. The input is considered as a list of u8 and only the last digit(s) will have to be incremented/decremented and anything before is left unchanged. This should make for better performance, thanks to github.com/m-ou-se for the PR
Yeah, cargo add could work.
I guest you will have to use bindgen.
The whole Vulkan API exists in machine readable XML. Generating rust bindings from it would be pretty easy.
I wish open source/non-profits would wind down reliance on tech backed by people living in oppressive countries that try to meddle in other free countries who try to destroy alliances between them. [I, as probably many other people who read this sub, though Putin would back off their tech sector, but he demonstrated that he is not interested in that](https://techcrunch.com/2017/03/16/kaspersky-michael-flynn/). JetBrains (developers of Kotlin) is headed by and in large part developed by Russians from their Russian headquarters and are thus under the thumb of Putin. In my opinion it is not ethical to place users of Mozilla's tech to unnecessary exposure by relying on third parties who might be forced to flip because that would serve the goals of some future FSB operation. Any arguments for why the above thinking is paranoid loose their weight in light of the fact that we now know Putin has no bounds in his shameless conduct - I don't need to remind you that he did compromise a good portion of republican party - if he is willing to meddle with a nuclear power he is willing to meddle with smallfish like JetBrains. STOP PUTTING YOUR USERS AT RISK. 
Doh! I was about to push out a new update to scc actually, which resolves all those accuracy issues (short story I am an idiot and didn’t test some situations). Ill probably do it in a few days now. If you are going to compare to it though, could you please use the -c option in it? That turns off the code complexity calculations so at least it should in theory be doing the same work. Or at least point out that its doing that, as it does make a serious impact to the runtime. I guess it also picks up some additional languages so it would be nice to see how many files were compared as well. These are things I was thinking of adding but never got around to. If you want to make your graphs look even more amazing try them out on a 16+ core machine. Tokei and scc walk away from everything else. BTW you can install polyglot just by downloading the binary from the github release page. It has a similar performance profile to loc. Faster on low core counts, but after 8+ it falls behind.
Sorry binding isn't my expertise. How do i do this? Sorry I'm new to this. 
Going to assume this is Vanessa who wrote this, and also wrote polyglot. It’s not ideal, but the thing is so few people ever hit this case its not worth spending the time investigating it? I totally get that you may run into this issue on a daily basis but for most people its a non issue.
I mean, Kotlin is open source, so if anyone wants to git clone, read through the source code and manually vet it, they can. As well as that, I don't really understand how the country of origin of a technology (that is open source) has any relevance to the applicability of said technology.
If you go the parser combinator route https://github.com/marwes/combine supports no_std very well and doesn't need any macro magic
As mentioned above, tuple constructors of structs and enums are functions, so you can write it a bit more briefly as: ```rust let mut f = File::open("hello.txt").map_err(MyCustomError::FileOpenError); ``` But of course, if there's any more complicated logic to determine which error to convert to, it's probably best to split that into a separate function.
What if they don’t ask for money and are silent? 
I've never tried combinatory parsing before, but I'm always up for learning something new so I'll give it a try. I didn't know about this crate, so thanks for the tip!
[removed]
[removed]
JetBrains is Czech, though.
Probably would also be worth putting this up as a question on the LALRPOP issue list as well, so at least they know that there is an interest in this - might be easier for them to gauge how tricky it would be to go no_std!
Interestingly https://crates.io/crates/boost is owned by them but 3 months old. I wonder if there’s some way to get the original owner data for this crate before they moved it to this new account.
isn't it safe if the library is empty?
Good idea, I will make it so :)
It wouldn't solve the whole problem, but at least to prevent people from running bots that register a huge number of crate names: we could require a CAPTCHA when the crate is created on crates.io. It could either be `cargo publish` that presents a textual captcha to the user (generated and checked by crates.io, of course), or we could require new crates to be registered through the browser and have a classic visual CAPTCHA. 
The RISCV team (part of the embedded WG) is looking into it. My understanding is that LLD seems to work well enough after the last LLVM / LLD update that landed in rustc a few hours / days ago. If all seems OK the team will send a PR to change the default linker. You can subscribe to [this issue](https://github.com/rust-embedded/wg/issues/158) to follow along.
Depending on which linker you were using you may be able to remove some if you switch to LLD. But you won't be able to remove all of them because specifying the linker script is mandatory (e.g. `-Tlink.x`). [This PR](https://github.com/rust-embedded/cortex-m-quickstart/pull/41) has the updated linker flags.
&gt;NPM npm doesn't allow squatting. [https://docs.npmjs.com/misc/disputes#exceptions](https://docs.npmjs.com/misc/disputes#exceptions)
Entrato! Grazie ;)
Second one, AFAIK. The ARM Cortex-R targets will soon use LLD as their default linker. Right now they don't have a default linker set which means `-C linker` is required to link binaries. The embedded RISCV target may soon switch to LLD too, if the latest LLVM update in rustc has good enough support. Changing `std` targets like x86_64 Linux would break stable builds w/o a migration plan ([more details](https://www.reddit.com/r/rust/comments/9a7te2/nightly_rust_is_switching_to_use_lld_llvms_new/e4uzf7t)) so it seems unlikely to happen.
The lldb plugin works well on macOS and Linux.
Regarding the flags issues, would it be reasonable to apply a transformation/filter in `cargo`? For example, simply dropping the `-Wl,` part which is unnecessary for LLD, translating known options to their LLD counterpart, or filtering them out if unnecessary, etc... --- As for `libc`, I am afraid the only solution is not to depend on it. Ultimately, it would be great, though I guess there's still a lot of work ahead before this happens.
&gt; the source code and manually vet it, Does Mozilla do that? And did they insure no code is acquired out of band? Do they promise to audit future versions. Do they provide recompiled alternative binaries if I or you don't want to recompile them for ourselves? My impression is that in this industry, outside of OpenBSD folks, you are lucky if people just glance at source before they run it. Cargo doesn't even have dedicated command for auditing and packages, to my knowledge, aren't tied to git commits, expecting one-off project would have dedicated auditing team is unrealistic. &gt; don't really understand how the country of origin of a technology This isn't hyperbole, because Putin is legitimately insane. Just as we can all agree Donald is so is Putin, to a smaller degree. Politicians and media don't say this out loud because there is still small hope that things turn for the better but this community shouldn't pretend otherwise when it materially affects us. Watch eye-opening series of interviews with people who studied his behavior on PBS called The Putin Files to get insight in his worldview. Here's just [teaser](https://www.youtube.com/watch?time_continue=3743&amp;v=Kk9igTqTx9s), there are interviews with others, including a number of people from intelligence services who will confirm this the sentiment. We shouldn't take bets with users, that in his insanity he still would never force JetBrains to covertly subvert their products. We can't make this guarantee for our users so we shouldn't use their products. Imagine how Kotlin would be viewed if North Koreans developed it, all kinds of suspicions would be cast on that code, it would be treated like it's radioactive. It would have to go through the same gauntlet of of analyses TrueCrypt went trough when suspicions were cast that it was compromised. Realize please that Russia is only slightly better than them and that without robust auditing guarantees we can't trust any of code that comes from them. 
[Look at who controls it.](https://www.jetbrains.com/company/people/). Another clue is that Kotlin is named after an island in Russia. Rusts lists "guaranteed memory safety" as one of its goals on its front page, I don't believe memory safety is a goal in and of itself. Memory safety is attempted for the purposes of reaching greater reliability of software AND information security of its end-users. If you are attempting memory safety for its security benefits you then shouldn't undermine it by introducing unnecessary vulnerability vectors that would compromise the goals that memory safety tries to reach. 
&gt; Given the number of years we've had this policy without incident Depends on how you define incident. People have been talking about it as a problem for years. &gt;ther package managers have existed with similar policies for much longer without incident Definitely not
Yes that means there are no existing users. Then it is ok. 
Suggested process: If a crate is reserved but contains no useful functionality: If a fully-developed new crate would like to use the name: If a number of important people agree that the new crate is useful, it is 1.0, and it is used in other crates or projects: Contact the current owner of the crate name. If they respond in 4 weeks, they keep the name. If they don't, transfer ownership, with a new major version. This would majorly favour the current owner, but would allow for change of owner in very restricted circumstances.
For reference on this, based on what parent said GDB directly works, but minigw didn't, however another option you can use is Cygwin (how I finally got it to work). Once it is functional though CLion's support is incredible.
But why do we want to use existing names? There are so many strings possible. 
Yes, I wanted to provide the option of using core and alloc types in the parser and interpreter so that the library can be built as `no_std`. Using the library in a binary crate would then require the binary create to provide an allocator. This all went pretty smoothly up until looking at the parser Rust module that LALRPOP generates; despite using core/alloc types in the grammar definition itself, LALRPOP still generates a parser that relies on elements (Vectors, etc) from the standard library. I looked for an available feature on LALRPOP itself that might cause it to generate `no_std` code, but I couldn't see one. As suggested above, I'll raise an issue on LALRPOP's GitHub repo as a feature request.
This is super cool. Thanks for sharing. 
You can currently use LLD with the musl targets, no linker search path stuff required.
You need a registered domain name certifiably registered to your name to publish an artifact to Maven Central. This kind of limits abuse.
You might want to take a look at [pest](https://pest-parser.github.io) as well. There has been some work on making it `#[no_std]` compatible.
The Pi 3 is actually ARMv8, it's just that raspbian only comes in 32bit.
For anyone following this thread, the LALRPOP feature request has now been posted: https://github.com/lalrpop/lalrpop/issues/397
I second that. I think Arch Linux is a living example of a perfect balance between reliable stable core and wild wild west in AUR. The barrier to entry for newcomers is minimal (just the PKGBUILD script), yet once your package is recognized it gets moved up through community, extra and then core repos. Very sane, pragmatic approach to packaging software. 
To be honest, if nothing is done about this *blatant* instance, someone is bound to do that. And although it will be a pain in the ass, it might result in better policy.
Sounds like a good idea. Might be a nice "first PR" issue for people to contribute.
And as is common in these cases, it requires a lot of damage to be done to get a reaction and change. I'll be waiting for the next wave...
The lldb plugin is what I use on linux
I'm assuming it's [This Issue](https://github.com/rust-lang-nursery/rls/issues/934)
What about importing crates by UUID+name or something similar? That way you would no longer need to worry about duplicate names or abuse. It would require some cargo changes though.
Thanks for the feeeback. My next step is to add a UI to specify the paths.
Thanks for the feedback. I'll keep the RFC in mind for future posts. For not using expect so much, I was just appeasing the compiler / VSCode. I'll look into replacing those with map once I rework the code a bit more.
Awesome! I wrote a crude version of NSGA-II while calibrating a numeric simulation in rust, so excited to try this when I have a chance. :)
Im not an expert on the computational methods used, but the problem of balancing chemical equations is basically the same structure as solving a system of linear equations.
I don't have a solution for that, but my assumption is most namesquatters are doing it to get money. If people do it just to troll people then that would be sad.
Does this require using the mingw Rust platform instead of the msvc platform? Rust binaries I've built for the msvc platform produce .pdb debugging info and AFAIK, gdb doesn't understand .pdb files
I love this. All of my avatars are randomly generated art. I think I'll be using your code to generate them in the future =D. Thanks!
lol, the entire community was saying this before crates.io launched and the Rust team was about as responsive to their pleas as the Wailing Wall. So yeah, this has been discussed extensively, but ultimately he who controls the infrastructure makes the rules, and the infrastructure is not controlled by the community.
It was a lot easy to reject when Rust was younger. Anyone advocating for namespacing knew this would happen. It's not just squatting either. It will also be a problem when some major packages need to fork 2 or 3 times and need to name themselves "${package}-2", "${package}-ng", etc. Linux distributions, Python, npm, etc all have these issues. We need a packages to be namespaced with some light controls on the top level. By default you publish under your username. Core/trusted libs can be granted access to reserved names.
This is cool as hell. What keeps you from implementing macros? 
The problem isn't namespaces, or lack thereof. The problem is the lack of an abuse policy. The goal is to have crates be utterly immutable so it's impossible for old dependencies to vanish. This goal is there for very good reasons but is also inflexible enough to be abused like this. We can fix this with a policy change; we don't need a technological change. pip and npm, for instance, also don't have namespaces, but are more strictly moderated so people have a recourse if names get squatted, or even just packages die and aren't useful to anyone. The fact that these exist also discourages squatting in the first place, since people know it won't work. This is a pretty clear-cut case of abuse IMO, and should be easy to handle. I tried contacting the owner of these crates to ask for a crate and see what happens, but I actually can't find any contact info for them through crates.io or github, so I can only assume that this endeavor is still a work in progress. You can tell the Rust team "you should have seen this coming", and basically we as a community did see it coming and didn't come up with a good response to it -- partially because the discussions always turn into bitch-fests about namespaces. Is this a problem? Yes. Is it easy to solve via policy changes? Also yes. Has any actual damage been done? No. Great, let's fix it and get on with life.
This is already the case, the only way to "register" a crate name is to publish a first release.
&gt;garbage collection is a big hurdle to get right. What about just us RC? Is simply, rust give you this almost for free and exist some ideas to reduce his performance penalty. I wish to know how implemente ARC like Apple, but I think obj-c/swift show that RC is overall great and if work in constrained mobile devices is ok for everything else. Also, do you have read: &amp;#x200B; [http://journal.stuffwithstuff.com/2013/12/08/babys-first-garbage-collector/](http://journal.stuffwithstuff.com/2013/12/08/babys-first-garbage-collector/) &amp;#x200B; P.D: I also interested in use a compiler with interpreter-like capabilities, and ask before about use rust: [https://www.reddit.com/r/rust/comments/94c4m9/is\_possible\_to\_ship\_rust\_compiler\_like\_a/](https://www.reddit.com/r/rust/comments/94c4m9/is_possible_to_ship_rust_compiler_like_a/) &amp;#x200B; But now using it more, don't see rust as a easy target. Too rigid, but wish to be proven wrong, because the other options is target C (and be too easy to shot yourself in the foot and here TinyCC is the main thing) or target a heavy VM like java or .net. And for that what is the point of use rust?
How I got Clion Debugging to work on Windows 10: * Default host: x86_64-pc-windows-gnu * msys2 (based on Cygwin, mingw installer) * [win-builds](http://win-builds.org/doku.php/start) (prob unnecessary, but kept it stupid simple for me) * Might have to manually add directory to PATH for autodetection... but I just manually located the binaries.
Yes. Namespaces, please. 
Because the problem is the existence of name generation spam not why this particular guy is creating some.
Yeah you can't use MSVC for debugging in CLion, which it will tell you right away if you try.
You can definitely implement call/cc in an interpreter in Rust!
Have you considered integration with [rink](https://github.com/tiffany352/rink-rs)?
nom is perfectly compatible with no_std. I use it this way pretty often and I never meet any issues.
Converting to Nom was one of my initial ideas about how to resolve this problem but I wasn't sure about no_std compatibility, so this is good to hear
I guess you would also have to require `StableDeref` to ensure that the value isn't moved when the pointer (or the pointer wrapped in `Pin`) is moved.
To clarify, salting the crate name is not a terrible idea. It's not my favorite, but it would work. A lot of places do something similiar with usernames (append a random 4 digit number to the end). But voting to give one crate the unsalted name is a recipe for confusion. 
One clue: that crate says the author is "Kary &lt;kary@email.com&gt;", whereas most of swmon's other crates don't have any contact information (which, incidentally, pretty much eliminates any presumption that they were reserving names in "good faith"...). 
Contact them how exactly? Despite the inviting (trolling?) descriptions, I checked a few of their crates and most have no contact info in Cargo.toml. We could try [this method](https://stackoverflow.com/a/49277449/1114328) though I'd prefer the contact to come from a Rust team member. 
I was unable to get either gdb or vscode debugger working, but I managed to get Visual Studio's: 1. Run your thing with debug and msvc toolchain 2. In Task Manager, right click on your thing and press Debug 3. Select Visual Studio (I use VS2017 community edition) 4. Proceed as normal
Found a way, https://crates.rs/crates/boost still has the old owner information 😀, that shows https://crates.io/users/ndkimhao as the original owner, which is linked to a not-entirely-empty github account: https://github.com/ndkimhao (with an associated email address).
That's an extraordinary claim. An iterator over results could be reusing allocated memory which helps performance and would send a query only once. When using limit/offset each new batch requires a roundtrip with the database and possibly reparsing of the query or rebinding of query parameters (new offset value). 
What were the issues with using an arena pattern?
I mainly use no_std to target WebAssembly. It requires rustc nightly to get the alloc crate though depending of the combinator you use (like many!).
[removed]
To simplify, my graph has a field \`values: BTreeMap&lt;usize, ArrayD&gt;\` which holds the values of the various nodes in the computation graph. These nodes are associated with functions which take some number of arrays as input, eg MatMul takes \`\[Array2, Array2\]\` and convolution takes \`\[Array4, Array4\]\`. I would prefer to use the type system to make sure the inputs to every operation are of the correct rank (or better yet, shape) but all types in the graph are dynamic arrays, and I link inputs to operations in the graph using the \`usize\` index. This defers the type checking to runtime
One of my use cases is also WebAssembly, so I'm very keen to get this working. I must admit that combinatory parsing and Nom are both very new to me and unfortunately I'm having some trouble with Nom at the moment. It appears that many guides are written for v1, v2 and perhaps v3 and from what I can tell the API has changed considerably since then. Do you know of any good introductory tutorials to Nom v4+?
Don’t worry. I wrote scc after tokei, loc and gocloc were all out. Nothing wrong with additional tools.
Avoiding allocation is otherwise done using a cursor which still requires a database round trip, but one which is more efficient than executing a new query. Data coming in over the network still has to get buffered somewhere, even if it's not in a deserialized vec. The alternative is that if you accidentally cause anything to look at the network buffers before fully consuming your query (e.g. you can no longer do anything with the results of your query while iterating over your batches) you can end up corrupting data. There are APIs that can help with this but they're better suited to an async io situation 
So, that would work for `Box`, but not for `Rc` and `Arc`, right?
In that case they would provide contact info. This is pure trolling. 
Good question. I'd guess that remembering short creative name is easier than remembering nick of a person (likely based on foreign name) + (possibly creative) name. As an experiment, here are crates and authors I remember (I don't check whether they are correct, not spelling of the names, so you can see how many mistakes I do): * serde_json by dtolnay * I think serde is by dtolnay too, but I'm unsure * futures by alexcrichton * toml by alexcrichton * mio by carllerche * bytes by carllerche * ripgrep by burntsushi - but that one isn't a library * csv by burntsushi (very unsure about this one) * rtfm by japaric * copper by japaric * m4 by japaric (not sure about crate name) * embedded_hal by japaric (or some other member of embedded wg?) * syslog by geal * simple_server by steveklabnik And here are crates with creative names (name not obviously associated with what it does) that I can remember: * tokio - async framework * iron - web framework * gotham - web framework * rocket - web framework * hyper - http parser * pest - parser framework * nom - parser framework * rtfm - real time for the masses - real time embedded framework * copper - embedded framework * piston - game engine * conduit - servie mesh framework, I think * clap - command line argument parsing At the first sight it might look like username + crate name wins, but that's not actually the case. First I think that I don't use many crates with creative name, so the second list isn't shorter because of my inability to remember, but because of them not being many. Secondly, in the zeroth case I'd be (theoretically) unable to use any other crate. However, I remember both creative names listed above and "normal" names - many of them listed in the zeroth case, but there are others (websocket, protobuf, reqwest, serde, etc). In other words, I remember more crate names than crate names and their authors. Of course, it's anecdotal evidence, but I'd be surprised if anyone remembered the nick of the author of every crate he uses.
Amusingly, "copper" is a squatted crate name :) Anyway, I see what you are saying but I wonder if this is in part *because* the current situation is that you need to remember crate names and not authors. Another issue is that with namespacing, people might just create organizations, like github.com/serde, and so you'd have serde/serde instead of dtolnay/serde.
https://github.com/rust-lang/crates.io/issues/1480
Nice project. It seems to match the intent of the existing crates rust-objc and metal-rs fairly closely (direct bindings preferring compile time over run-time where possible) so it might be nice to coordinate efforts in this space. From the README: &gt;..or you have to use something like objc-rs or rust-objc to do all your interaction with the Objective-C runtime (at runtime! which adds overhead and reduces compiler safety checks) rust-objc doesn't do everything at run-time, and there have been recent efforts to do more at compile time. For example, see [https://github.com/SSheldon/rust-objc/pull/64](https://github.com/SSheldon/rust-objc/pull/64) and [https://github.com/SSheldon/rust-objc/issues/65](https://github.com/SSheldon/rust-objc/issues/65) Also looking at the Metal bindings, it doesn't seem very different than what [metal-rs](https://github.com/gfx-rs/metal-rs) is already doing with static selectors.
As burkadurka suggests, I think the situation could be much better even for the crates you remembered. Consider Iron, for instance. Iron is actually a large, multi-crate project, and just remembering "iron" is probably not enough for you to get to work on a real project. Here are the packages they list on their official GitHub page: * https://crates.io/crates/router * https://crates.io/crates/mount * https://crates.io/crates/staticfile * https://crates.io/crates/logger * https://crates.io/crates/bodyparser * https://crates.io/crates/urlencoded * https://crates.io/crates/params * https://crates.io/crates/persistent * and... https://crates.io/crates/iron-sessionstorage Every since one of those packages is Iron-specific, despite what I'd guess most newcomers would assume. And if you were searching for "iron" on Crates.io, the first of these packages you'd find with the default "Relevance" sort order world be "iron-sessionstorage" on the second page of results, and then "urlencoded" on the fourth. The abandoned "iron-params" also shows up on the fourth page, before the actual crate for iron, named simply "params". Third-party crates for iron are named inconsistently, mostly prefixed with "iron-" or "iron_", though sometimes other variations. There's absolutely no way to tell if an Iron-related crate on the Crates.io search results page is an "official" Iron crate without opening the result, and even on the individual crate page, the only real evidence a crate is "official" is hovering over the "Repository" link and confirming it is prefixed with "github.com/iron". I don't think this is a good situation, and there are many other examples of this issue besides Iron. Namespaces would fix nearly all of this (overly generic names for very specific packages, hard to find related crates for project, worse quality results occur before high-quality ones due to string matching, no way to tell the difference between official and unofficial crates at a glance), including third-party crate naming consistency if there were "open" namespaces, so iron could keep "iron/" closed and "iron-community/" open.
Ah, I mistaken [a guide](https://github.com/japaric/copper) for a crate. Yes, not having to remember the author is likely contributing factor. But once people start creating organizations, we're at square one. There's already "self-namespacing" like `serde_json`, `serde_derive` etc.
Good point! It's still possible that they forgot, but I consider trolling more probable right now.
I'm not a huge fan of the use of Option&lt;T&gt; for non-optional stuff such as Arc&lt;Instance&gt; and winit::EventsLoop. In my opinion, any non-optional should be created in the same call that creates HelloTriangleApplication. This would also remove a bunch of as\_ref/as\_mut().unwrap() from the code and make it look a lot cleaner. 
Hmm, that's an interesting point. Surely namespacing would resolve this, but only if people didn't intentionally break it by creating multiple organizations. Most people likely wouldn't abuse it.
I agree that parking could still be an issue, but I think it'd be much less of one. It's easier to moderate, in my opinion. There are many credible reasons for someone to need hundreds of packages, but there are very few credible reasons for someone to need hundreds of namespaces. Also, with namespaces the answer can always be "more namespaces". This is more complicated than simply "Cargo and Crates.io should support namespaces", but you could imagine GitHub organizations / usernames or even domain names (think Java) acting as "reserved namespaces". "github/[username]/*" would be reserved for that user, while "[username]/*" would be reserved for a Crates.io user or organization. This system could also be used for integrating "custom package repositories" in a syntactically elegant way (ie, you'd mount your repository to whatever namespace you want). As long as you don't allow users to publish packages containing "/" or nest namespaces, then there's no need for the Cargo team to reserve "github", "bitbucket", or predict other top-level namespaces they might want to create, either.
You may be right, I'd need to think about it more.
This is exactly right 
Ha! The Rust framework that is an implementation of the Haskel Actor model has an embedded Lua scripting environment. I feel like they are trying to win a polyglot bingo!
Maybe it can be a part of plan in the future
[removed]
Data moves any time that you pass it to a function. Rust is pass-by-move. ([playground](https://play.rust-lang.org/?gist=7e703b7a0294e01042b93ff0e3ba1aa2&amp;version=stable&amp;mode=debug&amp;edition=2015)) fn one() { let x = 0; println!("{:p}", &amp;x); two(x); } fn two(x: u32) { println!("{:p}", &amp;x); } It is impossible to move a structure while you have a reference to it. (error[E0505]: cannot move out of `x` because it is borrowed) ([playground](https://play.rust-lang.org/?gist=5b81c0d0bff055bdf0975fba391bb8d3&amp;version=stable&amp;mode=debug&amp;edition=2015)) struct S(u32); fn main() { let x = S(0); let r = &amp;x; println!("{:p}", r); sub(x); println!("{:p}", r); } fn sub(_: S) {} When you "pin" a structure, you're only "thinly" pinning the value. A `Vec&lt;_&gt;` is roughly equivalent to a `(*mut _, usize, usize)`, so what happens when you pin a vector is that those three values can no longer be moved, but the internal allocation is still free to do whatever it wants and move the contents of the vector around. Note that there are two in-flight APIs for pinning. In the currently-on-nightly version, `PinBox&lt;T&gt;` is equivalent to `Pin&lt;Box&lt;T&gt;&gt;` from u/desiringmachine's [latest blog post](https://boats.gitlab.io/blog/post/rethinking-pin/). In the nightly API, the pin family of types directly own the pinned value. In the proposed new API, a `Pin` is a smart pointer wrapper that does guarantees that the smart pointer's `Deref` target is unable to move. The inline data still moves around when passed between functions as is normal. Not quite ELI5, but ELIDKAAP (Explain Like I Don't Know Anything About Pinning). I doubt I could explain something this complicated to a 5 year old. Not that'd get them past where you already are in understanding, anyway.
Oh sorry. I've been busy job searching and I haven't maintained this create in months to be honest since I've been busy with the wasm-wg and the rest of my life. Uh I can take a look at some point but this create is a pretty low priority for me.
Thanks for the correction. Though it doesn't change my point. And, well, the Rust team chose no-namespaces for reasons, and those reasons still seem valid to me. I don't find disambiguation to be a particularly compelling argument alone, though there's times when it's useful. Doesn't change the fact that we could fix this with either a large technical change or a small human change.
When a type is moved out, it's like it was unitialized. So, something like fn something() { let x = ...; f(x); if z { x = ...; } } You need drop flags to decide whether to drop the x at the end of the scope. Not allowing dynamic drop would be seen as an artificial restriction on the language.
The link_section attribute is sufficient to place data into the right parts of the output. Then it's a matter of generating that data, most likely with procedural macros. It'll probably look similar to what Objective-Rust is doing.
There's no operation on `Box` which moves the `T`. However, `Box` allows you to get a `&amp;mut T` which you can then `mem::swap` out the value for a different one, which will then move the value. All `PinBox` does (and all versions of the pinning API) is make it unsafe to get a `&amp;mut`, and to do so you have to swear that you won't [`mem::swap`](https://doc.rust-lang.org/std/mem/fn.swap.html) the value behind the reference (or move it in some other manner). The value which is pinned is non-relocateable because it is in a `Box` or other heap allocation (in the trivial case -- stack pinning is possible in theory if complicated). So your `Pin&lt;&amp;mut T&gt;` (blog post) / `PinMut&lt;T&gt;` (nightly) is, in most cases, a pointer to some heap data, just with the added guarantee that the data there cannot be moved out.
 let stdin = 0; let stdin_fd = EventedFd(&amp;stdin); poll.register(&amp;stdin_fd, Token(0), Ready::readable(), PollOpt::level()) .unwrap(); When stdin will be readable, poll will return an event with Token(0). 
Any plans to implement an api/way to expose rust functions in your scheme?
Yeah I know that it needs to to do those things. What I'm saying is that that use case where it can't be worked around in general is so marginal and easily circumvented that I find it weird that they went through the enormous sacrifice of including those in the type itself. Like you cannot read from a variable either if the compiler can't statically prove it has initialized and dropping is really just a special case of reading an exclusive reference to it. From your example this isn't possible either: fn something() { let x = ...; f(x); // x is now uninitialised if z { x = ...; } g(&amp;mut x) // woops } In that case Rust might as well introduce an `is_intialized!(x)` intrinsic macro which dynamically checks whether something is initialized because in effect dropping is essentially Rust inserting `if is_initialized!(x) { Drop::drop(&amp;mut x); }` I'm looking for a use case that is so grave and can't be easily worked around that it actually warranted putting the drop flag into the type itself originally which is a very heavy sacrifice.
Have you tried contacting ndkimhao? 
I don't see how that's not a special case of reading. The code you gave doesn't work without drop flags because it comes down to. let x = ...; if rand() { f(x); } Drop::drop(&amp;mut x); With the compiler implicitly inserting that call. It's in principle fundamentally the same as any other function that takes an exclusive reference and it's in general not allowed but implicit drops are a special case because they also get an implicit drop flag. But again the point is that I don't see the use case. There is absolutely no need to declare `x` _outside_ of the conditional unless you actually wish to use it after that point which is illegal anyway. In this case you can just move `let x = ...;` into the conditional and all is fine so in what use case does it actually need to exist outside of it?
&gt; All PinBox does (and all versions of the pinning API) is make it unsafe to get a &amp;mut, and to do so you have to swear that you won't mem::swap the value behind the reference (or move it in some other manner). Can we make an analogy to Cell&lt;T&gt; (and interior mutability in general)? It forbids you to have an interior pointer &amp;T, for safety reasons.
The two guarantees are related but ultimately different I think. The biggest difference between `Cell` and `Pin` is that `Cell` wraps a `T` where `Pin` (will) wraps a pointer. A `Pin` is adding guarantees to the smart pointer which it wraps. Really, all it does is remove the `DerefMut` implementation (as well as inherent impls) and provide an unsafe way to access `DerefMut` instead, that disallows you from moving the value.
Well no, it's more like let x = ...; let x_was_moved = false; // drop flag if rand() { f(x); x_was_moved = true; } // ... if !x_was_moved { Drop::drop(&amp;mut x); } 
I've gotten VSCode working on both Windows and OS X just fine. Install C/C++ extension from Microsoft. Install LLDB... somewhere. Then setup your launch config like this. Just update the exe name. [https://pastebin.com/raw/9fc2WjNT](https://pastebin.com/raw/9fc2WjNT) Full breakpoint and step debugging. Works like a charm!
I can see why you wouldn't consider this a bug in a technical way. But I would argue that it's a bug because ths behavior doesn't meat the principle of the least surprise or in other words: tokei might have created an expectation here that it doesn't deliver. IMHO, that's a bug. Your proposal together though may already be a good solution to it. Anyway, no matter if this particular case is a bug or not, I also have the impression that the overall precision of tokei is very good 🙂
&gt; Lifetimes […] (e.g. passing references in via injection, instead of attempting to outlive the owning context) So I have no formal CS education and have only just now read about dependency injection (which this is an application of, I presume). Is the point of this to move the reference up the call stack and thus extend its lifetime?
Know if there's a good way to do build &amp; run?
I don't like this idea as it would force you to use Github. I don't have anything against Github right now and use it, but being dependent on another organization is nothing I like.
Why not just add an else block? This is equivalent of what I’d write in C. let x = ...; if rand() { f(x); } else { Drop::drop(x); }
Because then it's not dropped where you think it is: let x = ...; if rand() { f(x); } // some stuff done // this is where the actual drop takes place If what a drop does can somehow be observed. Having said that I'm still not seeing the use case that made it necessary enough to actually include the drop flag into the type before 1.12.
[Ash](https://github.com/MaikKlein/ash) provides you with direct binding to Vulkan. The things in the top-level namespace are just for convenience, if you want the raw, generated API bindings you can use the `ash::vk` submodule. I recommend using the [`generator`](https://github.com/MaikKlein/ash/tree/generator) branch since that is where we have Vulkan 1.1 support and can generate the bindings from the latest API spec. Ash is so low-level, it's used as the Vulkan bindings in gfx-rs.
[removed]
Hey, let's talk about this. Sent you PM.
RPCS3 describes itself like this: &gt;The world's first open-source PlayStation 3 emulator/debugger written in C++ for Windows and Linux.
Oh yeah, I think it was maybe the 1.0 post or something. It was clearly pretty polished. I spend most of my time in my cave, so I had a good run not knowing someone had already done it. Now I just think it's fun and I love the graphs and benchmarking you're doing. 
&gt; Would simply not be legal I think this would be *extremely* restrictive, especially combined with move semantics. A lot of reasonable-looking programs would be disallowed for reasons that can be hard to explain or teach. And Rust already spends a lot of "weirdness budget" on lifetimes and the borrow-checker. As to drop flags being part of a type’s layout, I assume it was always the plan for this to be replaced with drop flags on the stack eventually.
I initially read this as "City Slimes" and thought that was an awesome name for a game.
I guess I can see that explanation that it's a learning curve thing that was always expected to move to the stack but the way I see it in pretty much every case where drop flags are actually inserted the programmer can always rewrite the scopes very easily to eliminate them.
I think all the documentation are up-to-date. The Gitter channel is active. Also ping me if you need help. If the documentation seems unclear, open an issue. The community is reactive!
The person you are answering to is talking about namespaces. I am against namespaces and your statement that the whole community was for it is wrong. You're pretty hostile here and putting words in my mouth, so I'll stop replying here.
My last project that uses nom with WebAssembly and no_std, https://github.com/Hywan/gutenberg-parser-rs. I’m even writing a blog series about it, https://mnt.io/2018/08/21/from-rust-to-beyond-prelude/.
 I agree - I only did it like that because I wanted the code to look similar to the original C++ where possible, but now I'm thinking of changing it. 
&gt; unlocking marketing leading performance Probably a typo, but I love it.
I think the trait would make more sense if the method took a pointer type and created a pinned type from it. But this is taking raw value and then constructing a pinned pointer from it. Turning this into a trait would be similar to introducing a trait for `new`.
Think to do a rust web back-end REST project. Wondering if I should go with stable or nightly? Nightly I would be able to get handful of features that would be useful like the async/await stuff, some others which i have used in the past. What downsides am I not thinking of? 
Very cool, is the idea to add another real-world test case for gfx-portability or is it just for fun? :)
Are we there yet?
I think you are overstating the gravity of the issue. Drop flags are necessary for the current drop semantics. The problem was that rustc 1.0 had a pretty simplistic compilation model (directly emitting LLVM IR). In this model, it wasn't possible to reason about the control flow enough to add the drop flags properly. So, the flags were put where we could put them: the type. The solution for this was already known: introduce an intermediate representation (MIR). So, the concious decision was made to stabilise the language and fix that flaw later. That's pretty common practice. Otherwise, we'd have delayed Rust stabilisation by 1.5 years. Given that a lot of types are POD in Rust, it also didn't have _that_ big of an impact. 
&gt; I think serde is by dtolnay too, but I'm unsure serde is originally by erickt. I dislike putting library names on people, precisely because ownership moves and there's often collective ownership.
Crative names are also a marketing instrument, and calling yet another http client "http" doesn't help you there.
How would you recommend changing: ``` instance: Option&lt;Arc&lt;Instance&gt;&gt;, debug_callback: Option&lt;DebugCallback&gt;, surface: Option&lt;Arc&lt;Surface&lt;winit::Window&gt;&gt;&gt;, ``` In this case? I'm asking out of curiosity and to learn something new.
&gt;sasik520 There is slight difference between the possibility and the necessity to be creative. There is no evidence that users who are **not forced** to be creative will not be creative.
It would become permanently "symlinked", so that everything still works. I'm not aware of any such tagging system.
&gt; There were equally large chunks of the community who were fine with this decision or wanted to have it that way. [The consensus seems clear to me](https://reddit.com/r/rust/comments/4z6fni/landgrabs_on_cratesio/).
I wrote some rust bindings for RtAudio and used it to play around with passing information into/out of an audio thread with futures. I wanted to do a comparison between idiomatic rust concurrency and C++ for audio applications. Rust definitely wins the ergonomics battle. I might share the source eventually but compiling it is temperamental at best. I think the best path forward is to actually rewrite the RtAudio library in rust... But I don't have the time to do that. cpal and other audio crates in my opinion abstract too much away from the audio callback and don't provide sufficient control over the drivers, at least not for the applications I'm interested in. 
* 1: [yes, combine needs macro magic](https://docs.rs/combine/3.5.1/combine/macro.parser.html) * 2: like everything that feels magic, [nom macros uses tricks that look simple once you know about them](https://github.com/Geal/nom/blob/master/doc/how_nom_macros_work.md) :)
Um... https://github.com/rust-lang/rust/pull/33622
yup, nom started supporting `no_std` very early, since it aimed for integration in C programs with the smallest footprint possible. And you want more features, you can add [alloc](https://github.com/Geal/nom/blob/master/Cargo.toml#L25) to activate parsers that generates vectors of values.
I actually came across this project a little while ago, I didn't realise that you were the author. Thanks for the links, they will be very useful! As for the documentation, yes it is all up-to-date as of 4.0.0 and I have successfully created some simple examples based on that. As a beginner to parser combinators in general I was searching for a good introduction to Nom and those unfortunately all seem to be for earlier versions in which the API has changed considerably. I noticed that you're using 4.0.0 in your Gutenberg parser though, so that's perfect.
Perfect, that's exactly what I need. Looks like your Nom crate might save me here, so I'll have a go at using it to creating a parser for my language. Thanks!
I've been saying the same thing from the start. It's a solution more complex then the problem.
#### [RustConf 2016 - Back to the Futures by Alex Crichton](https://www.youtube.com/watch?v=bcrzfivXpc4) ##### 7,994 views &amp;nbsp;👍134 👎2 *** Description: RustConf 2016 - Back to the Futures by Alex CrichtonOne of the core building blocks of any library is the I/O abstraction it works with, but unfortuna... *Confreaks, Published on Oct 4, 2016* *** ^(Beep Boop. I'm a bot! This content was auto-generated to provide Youtube details. Respond 'delete' to delete this.) ^(|) [^(Opt Out)](http://np.reddit.com/r/YTubeInfoBot/wiki/index) ^(|) [^(More Info)](http://np.reddit.com/r/YTubeInfoBot/)
For `instance` that's quite straightforward (for the others, I haven't yet reached the part of the code which talks about them): you don't need to use the `HelloTriangleApplication` struct before an `instance` is created. Then, instead of creating the struct with empty fields, and then initialize it, you could just initialize the fields in the constructor.
IMO, from what I've seen of Tokio (haven't used it yet) is that its too low level for most people and more libraries need to be built on top of it for most common uses. For example something like Flask (from Python) would cover many people's use cases as most want to do HTTP anyway and then you don't need to build it yourself.
Gotcha!
There's plenty of sad people in the world.
IMO trying to make one language look like another as an attempt to make something easier to learn, while at first glance may seem like a good idea, I think it ends up hindering more than it helps. Instead I'd do something like add a comment in the code tutorial that says like "This is like doing X in Y language".
I have a working implementation of [hygienic R7S-like macros in Rust](https://github.com/etaoins/arret/tree/master/compiler/hir/macros) if you want some inspiration.
That is actually a good things :) I think we need to have some kind of repo where we could collect all of that and be able to find it with out search the internet for a few days and then asking on reddit if there any good (more or less up to date resources), Thanks. going to read trough [Fahrenheit](https://rust-lang-nursery.github.io/futures-rs/blog/2018/08/17/toykio.html) now :) 
\`mio\` is easy. It's basically is a re-wrap of the pattern that powers \`epoll\`. When you know \`epoll\`, then \`mio\` will be very nature for you. \`Tokio\` is an other story, it's a framework that trying to provide a friendlier API of \`mio\`. It does so by hiding the underlying operations, and expose them as a new layer of abstraction. Maybe the abstraction is why you found \`Tokio\` is hard to understand.
Yes it feel like that
What about when someone shuts down crates.io by reserving every single name of 6 characters or less?
The main problem I had learning Tokio is that it hides all the things you are familiar with (select/epoll etc) behind its own abstractions, so you have to build a mental model from scratch. As an experienced programmer this made me *angry*, because I couldn't find a way to bootstrap a model of Tokio from what I already knew it must be using inside, even after reading a bunch of blogs, docs and code. It felt like deliberate obfuscation and pointless abstraction wankery. Note that I'm just trying to describe my initial feelings of frustration here; this isn't anything approaching a fair or objective assessment. I think the idea of Tokio is that tying resources (sockets, file handles, timers) into an event loop (whether epoll or select based or whatever) while also making sure all futures get run is going to require tight coupling between the event loop and the lowest-level resource handling code. Depending on whether you're using edge-polling, level polling, kqueue, completion ports, etc, your low level async_read() and async_write() are going to look very different. Tokio therefore puts it all behind a single interface, which looks very simple and magical. Basically the one thing that made it click for me is that the lowest-level resource primitives "know about" Tokio and know how to ask the event loop how to get woken up at the right time, but that this is essentially a hidden detail that users of Tokio never see. They only see that the read wasn't ready yet, they don't see that the read call invisibly arranged for the task (i.e. a cluster of futures that was composed together before being added to the event loop / executor to be run) to be woken up at the right time.
I actually y like Rust a lot but because of this things it is kinda feels unstable or not ready for me.
This is great. I desperately needed some inspiration. There are plenty of examples that uses Scheme itself to implement macros but they were not that helpful in case of Rust.
Yes you are right, that is probably one of the main problems
One another problem I found is that, when you build something upon that many layers of abstractions, then your code will almost certainly be dead-coupled with `Tokio`. Which made me felt little uncomfortable.
Glad to be of help! The project is still in the proof of concept stages so the documentation and comments are extremely lacking. Feel free to PM me if you need help or clarification with anything. 
Thanks! The process was really fun like you said. This is first time that I'm doing something like this and the project was intended to be a really small subset of R5RS. After achieving my initial goal, I just continued to do more but my initial design couldn't handle the standard's complexity --even though it's a pretty simple standard. Just be careful while taking inspiration because there are some bad design choices there.
I don't think this is true because user code mostly uses plain Futures, it's much less invasive than using e.g. libuv in C.
Im now trying to use it with the non-blocking Async stdin reader from termion - or am I doing something wrong? https://gist.github.com/429d4835ac4663cb9a7445fffaba8050
Yes you are not caling poll.poll anywhere.
&gt; I don't think this is true because user code mostly uses plain Futures That's AFTER the await/async been actually added to the language. AND yes, when await/async is actually in place, my argument above will turn false (some degrees of false at least). Which ... is why I'm currently waiting for them to be added :)
It would be a nice feature but I don't have idea how it can be done. Rust has no reflection, so it doesn't help that the interpreter is implemented in Rust. A naive approach may be building something like FFI or exposing Rust's FFI to Scheme. Another approach may be creating a script that generates Rust code for given module to expose it in Scheme. The latter seems nice and I may consider it, looks fun.
[There's one more, if only need to support Windows.](https://www.xinterop.com/index.php/unmanaged-exports/)
This is how I ended up making it: ```rust pub fn new() -&gt; Self { let (events_loop, _window) = Self::create_window(); let instance = Self::create_instance(); let debug_callback = Self::setup_debug_callback(&amp;instance); let physical_device_index = Self::pick_physical_device(&amp;instance); Self { events_loop, instance, debug_callback, physical_device_index } } ```
I am just wondering does actix run in a single thread or it spawns separate thread for events? 
There's [rust-learning](https://github.com/ctjhoa/rust-learning) a repo of interesting Rust learning material. And I collect interesting Rust blog posts on [Read Rust](https://readrust.net/).
There's [rust-learning](https://github.com/ctjhoa/rust-learning) a repo of interesting Rust learning material. And I collect interesting Rust blog posts on [Read Rust](https://readrust.net/).
I did notice that loc is now easily the fastest counter https://twitter.com/boyter/status/1033888961242877952 with the updates you have been doing. Totally agree its very fun. Certainly more fun than most of the day to day coding I end up doing. Sadly working in Go I think I will never be able to meet the speed of tokei or loc hence starting to play with Rust a bit. Be a long time before I am ever able to get close to either but gives me something to strive for.
In every reddit discussion about parsers in Rust, there's always one person that comes to complain about macros, and to be honest, this is quite tiring. Especially considering the time I spent writing docs and making everything more usable. So, yes, I feel the need to reply :)
The link 404s for me. 
I don't think so? await/async are syntactic sugar. It doesn't change whether you do or don't use futures.
Well this is under the assumption that they are on the stack. My post mostly dealt with it being encoded in the type itself which is a cosniderable sacrifice.
I'm looking for a distributed hash table (DHT) supporting the storage of arbitrary data. This week I am investigating rolling my own since I can't seem to find a good implementation which fits my bill in Rust. Anybody would happen to know of one?
Sure, but the thing been implied here is, `Tokio` is built upon `Futures`, so you're going to use it. And in the future, many IO crate will probably support await/async keyword. When that became true, then maybe you only need to depend your code on the await/async keyword rather than the third-party crate. For me, it's bonus.
After a couple busy weeks of life and summer I'm back to [`uom`](https://github.com/iliekturtles/uom) (type-safe zero-cost dimensional analysis) and looking to get a feature hygiene fix and the thermodynamic temperature/temperature interval changes merged into master so I can release a new version in the next couple weeks.
Yeah the ecosystem is in a state of change right now with the Pin language changes, part of futures moving into std, futures 0.3 in development, tokio-core being replaced by tokio, .... It's very confusing, and we can only hope that things stabilize and then a lot of effort is put into documentation. If you want boring and stable (not a criticism, boring and stable is good if you just want to build something) I would check back again in a year or two. 
I don't think we should say that until there's some kind of message from the development that it's "finished". I think we have an internal inertia towards patterns that are sub-optimal. One of the reasons I've fallen in love with rust is that it values innovation over familiarity.
Actix can run across multiple threads.
Earning money :) I got a requet from a friend to parse some data out of terabytes of the SEC edgar access logs. It's a small job, but I've gotten to use the csv and reqwest crates for fun and profit :)
Would calling mem swap actually be bad? If you have a mut ref, then you don't have any other refs into the object right? Mem swap doesn't change the addresses of what it's swapping, only the contents...
Stuff breaks occasionally on nightly. But if you're on nightly only for syntax, and not for nightly only crates, you should be a bit safer. You could probably get pretty far using warp on stable though.
[removed]
Me? I just starting with rust. Reading the book and trying to setup for bare metal cortex-m. I picked a bad time with the LLD linker switch on the nightly build. 
Since actix-web works on stable, you should be able to get a nightly version of rust and compile everything only when you update your nightly version. The complications come when using nightly only libraries - then you'd have to make sure to have a version of nightly your libraries work with. I'm using nightly for a library I'm working on, but it looks like I'll be able to move to stable when 1.30 comes out (attribute like proc macros &amp; usage). But I haven't any problems. I just got a version of nightly and have stuck with it. OTOH, im not sure what the churn looks like for async stuff in nightly.
TiKV?
Not much code work this week. Working on updating some of the art in my game. Next step code wise though will be to implement a start &amp; end screen.
Well, I have pretty hard time understanding how actors communicate.
Yeah, it was very difficult for me to understand how it works until I've read a blog post explaining it's registering wake-ups automatically and then read a comment that it uses TLS. I'm glad TLS is going to be removed.
Ahh, didn't see the flair, I've used Nom, it's a great library thanks for writing it!
That's why normally mem::swap is safe. But this assumption breaks down if the value contains pointers into itself, because after the swap those pointers will be pointing to where the value used to be, not to where it is now. Up until now this wasn't a problem because there was no way to construct such a value in safe Rust, but that's changing with the introduction of async/await; if one local variable borrows another in an async function and a yield occurs within the variable's scope, the resulting Future value will include storage for both variables, and one will point to the other.
It seems you are not talking the thing which I was talking. I mean, if you've read my previous comments, what you'll realized is I was talking about code coupling, NOT whether or not `Futures` is in the dependence list. The key point is, when you can couple with `await/async`, then you don't have to couple with `Tokio`. Which is something I'm looking forward to. I must remind you that it's very silly and rude to misconceive other people's opinion by not to understand it in the first place. 
I understand now, thanks :)
One can write code decoupled from Tokio's runtime details. &amp;#x200B; For example, \`h2\` only requires futures + AsyncRead / AsyncWrite traits: [https://github.com/carllerche/h2](https://github.com/carllerche/h2)
Trying to come up with another name for Gutenberg, a static site engine: https://github.com/Keats/gutenberg/issues/377
Also, maybe one day Rust will have native support for self-referential types.
I didn't say soon. :)
I think that static checking instead of panic is a big advantage too.
guess that description will need an update soon ;)
Working on gfx-portability is fun! We want it to be useful, we want it to be better than MoltenVK. And we happened to discover quite a few issues on the way to RPCS3, as you can see in the PR description.
There is a bunch of Vulkan examples running, as indicated at https://github.com/gfx-rs/portability/wiki/SaschaWillems-samples DX12 is championed by [@msiglreith](https://github.com/msiglreith), and they are busy refactoring the backend to be more modular and reusable, starting with low-level bindings to d3d12 in [d3d12-rs](https://github.com/gfx-rs/d3d12-rs/pull/1).
I'm currently getting a data file reader library put together that will have similar capabilities to the series of functions in numpy such as loadtxt and genfromtxt. My work so far can be found here: [https://github.com/rcarson3/rust_data_reader] (https://github.com/rcarson3/rust_data_reader). The library so far has mostly the capabilities of loadtxt. It's not super fast right now. I've found when loading in a ~1 GB file of doubles it reaches about ~30-50 MB/s. However, it seems to be working based on my test cases. Although, I do need to fix my easy/fast solution for counting the total number of non-commented lines of code in a file. 
Which blog post?
Alois Senefelder invented lithography. There is no crate called "alois" and also no stuff related to static sites that I could find. Just saying.
&gt; Having said that I'm still not seeing the use case that made it necessary enough to actually include the drop flag into the type before 1.12. OK. If you don't like the examples, that's fine. You're being a little weirdly forceful about this. I recommend taking some moderately complex code and trying to rewrite it so dynamic drop would never be required -- make sure all branches of conditionals consume exactly the same sets of variables, etc. It might be an interesting exercise. The Rust developers always knew that drop flags needed to be moved to the stack -- that's why they did it, after all. It became vastly easier to do so with the "MIR" system implemented in 1.12, so that explains the timing. 
# Better way to get nested vector length? Say I have a 2D tensor : `let tensor1d = vec! [` `vec![1, 1, 1],` `vec![1, 1, 1],` `vec![1, 1, 1]` `];` I can yield the length this way : `let mut vec_size1: i32 = 0;` `for i in tensor1d.iter(){` `vec_size1 += i.len() as i32;` `}` `println!("v:{:?}", vec_size1); // prints 9` However, when I increase the dimension, it gets longer and longer. `let tensor2d = vec![` `vec![` `vec![1, 3, 5 as i32],` `vec![2, 4, 6 as i32],` `vec![3, 5, 7 as i32]` `],` `vec![` `vec![1, 3, 5 as i32],` `vec![2, 4, 6 as i32],` `vec![3, 5, 7 as i32]` `]` `];` `for i in tensor2d.iter() {` `for j in i.iter() {` `vec_size2 += j.len() as i32;` `}` `};` `println!("v:{:?}", vec_size2); // prints 18` There must be a better way to do this but I can't seem to figure it out. 
I understand your pain. I believe the docs can still be improved. &amp;#x200B; Here is the problem though. I am already intimately familiar w/ Tokio, so it is hard for me to see where the documentation falls apart. What would be helpful is for people to go through the existing guides ([https://tokio.rs/docs/getting-started/hello-world/](https://tokio.rs/docs/getting-started/hello-world/)) and take notes on exactly what questions / confusions they hit while reading them. Then, we can iterate on filling in the gaps to try to get a smooth learning experience. &amp;#x200B; If people who are confused here seriously want to try to improve the learning experience, let's put together an issue + gitter channel and work through it. Like I said, it will take a bunch of iteration of people trying to read the docs, documenting where the confusion are, docs are updated to fill the holes, and repeat.
This has been true for years. Rust's greatest downfall - to the point that I've mostly given up learning or using rust - had been it is perpetually a couple years away from usability. It is constantly in flux, and the documentation is always dated enough to be point of not being usable for anything more complex that the basics. The responses are always "just wait a few months when proposals x, y, and z are finished then this will be more usability and documented". 
Well the motivation is that it's hard to see file paths using `-f/--files` when it's only at 80 columns. I would be open to adding to a PR adding a command line flag to set a strict width though.
I don't remember, it was a long time ago. I think Alex wrote it. I'll try to find it.
While agree with that in principle, in the over 150 languages tokei supports there is less than 5 that have overlapping file extensions. What is required to without human intervention determine file type is way more effort than I'm willing to put in for such a small percentage.
That's not a same thing at all. First what I propose is that a crate in namespace experimental could be named same as a different crate in namespace quality. Secondly, becoming quality crate would require approval of wide community and the team members. Having descriptive name would be one of the requirements for being in quality namespace. So that if someone squats experimental, it wouldn't prevent us from having good stable crate with descriptive name in the quality namespace.
Rust doesn't handle these kinds of recursive structures well. Without specialization, I don't think there's any way to implement a recursive `total_leaf_len` for all types. The best you could do is implement a trait for both `Vec` and every element type you intend to use... which is pretty tedious. On the other hand, if instead of `Vec` you use a distinct type for each level (so, `Vec1d`, `Vec2d`, *etc.*), then you could do it pretty easily by implementing a method for each type separately.
Indeed frustrating at times but we are getting closer and closer. It's a good sign that so many are enthusiastic. We want it all right now.
&gt; entire ecosystem It's good to remember that in almost anything, you hear from people who have a problem. Those that don't just don't say anything and ship stuff. So, it's definitely not the *entire ecosystem*.
could you stream your work on tokio (twitch, youtube etc)? i would love to watch and learn from it.
More Dropbox updates, awesome
Mio/tokio has always been this way. Over a year ago I wrote a comment about this - about its poor abstraction lines. I come from a C, C++, and Java world and have writtena lot of high performance networking code. They all have familiar high performance concepts, and Java has an excellent selector and buffer abstraction that keeps you fairly close to the metal. I was blasted by people saying I need to wait because rust and the library were changing and it would get better soon. So I've been toying with mio and tokio since it was just all shoved into mio and everything blind together way too tightly - you had to use mio's event loop and times of you wanted not blocking io - and incredibly poor documentation. Now we aren't much further along. The documentation still sucks, and tokio still is a mess of abstraction that is too demanding. It also seems to be rather slow. I read the code after I made that last comment a year ago and noticed that if I just wanted a fast non blocking epoll loop, I basically need to write it myself. 
In my opinion, it's a matter of documentation for a wider variety of use cases. It's hard to jump from a line-based protocol to anything else. The tokio.rs site was slimmed down a few months ago and I don't think the current examples on the site, or those linked to, cover some use cases that tokio is intended for. My example - I need to do what tokio-proto did. References to tokio-proto were removed from the main site, and tokio-proto is deprecated without replacement. I don't understand how to use the primitives of tokio to build support for another protocol, and haven't found any documentation on that type of use case. My best hope is to dig through how things are done for HTTP and hope the complex parts line up.
But that doesn't make sense, I did this for a similar implementation and it worked: impl Index&lt;ID&gt; for Manager{ type Output = Option&lt;Entity&gt;; fn index(&amp;self, key: &amp;ID) -&gt; &amp;self::Output { if self.mobs.contains_key(key){ Some(&amp;self.mobs[key]) } else if self.items.contains_key(key){ Some(&amp;self.items[key]) }else if self.npcs.contains_key(key){ Some(&amp;self.npcs[key]) } else { None } } } impl IndexMut&lt;ID&gt; for Manager{ fn index_mut(&amp;mut self, key: &amp;ID) -&gt; Option&lt;&amp;mut Entity&gt; { if self.mobs.contains_key(key){ Some(&amp;mut self.mobs.get_mut(key).unwrap()) } else if self.items.contains_key(key){ Some(&amp;mut self.items.get_mut(key).unwrap()) }else if self.npcs.contains_key(key){ Some(&amp;mut self.npcs.get_mut(key).unwrap()) } else { None } } } Or at least, it did at the time because this code worked fine: impl Manager { fn generate_id(&amp;self) -&gt; ID{ let id: ID = nanoid::generate(15); while self[id].is_some() { // &lt;--- this line here specifically id = nanoid::generate(15); }; id } } I'm honestly baffled that something that worked is suddenly not working.
Honestly, "rust is in a transition" followed by "just give it a couple more months" has been the refrain for years. Rust doesn't seem capable of making decisions and following through before the next shiny new idea muddles everything again.
The RLS plugin provides build capabilities (\`Ctrl + Shift + B\`), and can be configured to build on save in your user/workspace settings. Once you configure a launch profile, \`F5\` will run with the debugger
That may sound bad but it's good. Once you get grap on the language you can do anything incredibly easily.
Also, I'd appreciate if someone could explain why pinning is needed in the first place.
Go be a troll elsewhere
I'm glad you like it!
Awesome thanks! I've been using a C++ (Windows) launch config with `cargo run`
Isn't that kind of the point? Not all constructors work with being pinned. The trait groups those that do.
/u/CAD1997's comment has a ton of detail about what Pinning does exactly, so I'll talk just about the other half: Why did we need to invent pinning in the first place? First, back things up a bit. There's a stumbling block that a lot of new Rustaceans run into, where they try to make some kind of "self-referential" struct like this: struct VecAndSlice&lt;'a&gt; { vec: Vec&lt;u8&gt;, slice: &amp;'a [u8] } fn main() { let vec = vec![1, 2, 3]; let vecandslice = VecAndSlice { vec: vec, slice: &amp;vec[..], // error[E0382]: use of moved value: `vec` }; } These structs basically never work out. The language has no way to represent the fact that the `vec` field is "sort of permanently borrowed", and the compiler always throws an error somewhere rather than allowing such an object to be constructed. As we get more experienced in Rust, we lean towards different designs using indices or `Arc&lt;Mutex&lt;_&gt;&gt;` (or sometimes unsafe code) instead of references, and we don't see these errors as much. So anyway, fast forward again back to [the] Futures, and let's think about what this means: async fn foo() -&gt; usize { let x = [1, 2, 3, 4, 5]; let y = &amp;x[3..4]; await bar(); return y[0]; } `foo` is `async`, so rather than being a normal function, it's actually going to get compiled into some anonymous struct that implements `Future` (which some code somewhere will eventually `poll`). The compiler is going to take all the local variables and figure out a way to store them as fields on that anonymous struct, so that their values can persist across multiple calls to `poll`. So far so good, but...what happens when you put `x` and `y` in a single struct? Bloody hell, you get a self-referential struct! We're back to that first example that we said never works! Believe it or not, it's actually even worse than that. At least in the first example, you could make an argument that it's safe to move a borrowed `Vec`, because its contents live in a stable location on the heap. In the second example, we have no such luck. `x` is an array that doesn't hold any fancy heap pointers or anything like that. Moving `x` would immediately turn all of its references (namely `y`) into dangling pointers. As long as local borrows are allowed to exist across `await` statements, some coroutines are going to be self-referential structs. The compiler team could've said, "Alrighty then, we'll just make the compiler return an error instead of letting you borrow like that." But that would've been a constant source of awkwardness for users, and it would've sabotaged the whole purpose of `async`/`await` syntax: That it lets your "normal straight-line code" do asynchronous things. So that's the position they were in, when they designed `Pin`. What's the smallest change we can make to the language, that lets us tell the compiler that we promise never to move a struct like this after we call `poll` on it? That's what `Pin` is.
One of the main motivations is to allow the compiler to translate the async/await interface into one state machine (= a struct with a Future poll implementation) -- including borrows across yield points. These state machines may become self-referential. If they do, the whole state machine may not be moved to another position in memory. The slightly cryptic version of the motivation is [here](https://github.com/rust-lang/rfcs/blob/master/text/2349-pin.md#motivation). While [this is an old article](https://boats.gitlab.io/blog/post/2018-01-30-async-iii-moving-forward/) that uses different APIs, it makes the motivation a bit more clear.
Semantically, each actor has a mailbox that they receive messages in. Each actor will have an event loop type thing where they check for a message and then do something based on the content. An actor can send a message to another. And that's basically it, everything else is implementation details. In actix, the mailboxes are lock free queues. Messages are structs, and without poking further I'd imagine that the actual sending is either copying or moving with multiple-producer-single-consumer concurrency semantics.
The python equivalent of tokio is probably something like twisted, or python 3.x asyncio.
Since you yanked the crate, it still gives the same error as if you tried to install a nonexistent crate. Maybe you should put up a dummy crate with a build script that does `panic!("Yer doin it wrong, try https://rustup.rs")` 
I can't see why it was working. You return values are of different type then declared at function signature
Ah, I see. Thanks! I guess I've never encountered it because I try to avoid asynchronous code wherever possible.
a) /r/rust is _not_ "the community" - Also, reddit is a place known for brigading b) if that is so, how come no one ever could ever be arsed to write an RFC on this? It's a very selective link and I want to also state that carols reply also got a good amount of upvotes there.
Thanks for letting me know. I've skimmed over [the docs] (https://docs.rs/ndarray/0.11.2/ndarray/doc/ndarray_for_numpy_users/index.html) and it seems pretty convenient to use. Do you know any other good crates that might help for audio signal processing? 
You are completely wrong about the documentation. Every language is changing, rust just has lots of gradual releases with small changes. It not being useful is completely wrong as well
Is this useful right now? Given that Tokio is still in flux and we'll have async/await in the near future, is it useful to iterate the docs as they are? For example, the docs have to spend a lot of time explaining core concepts of Tokio before they can get to a reasonable first example. I mean, you can't understand the Chat Server example, which is really basic, without wrapping your head around a lot of Tokio. That makes the Getting Started guide really rough. The Getting Started guide becomes an Advanced guide right from the get-go, because there's no other way to get to a basic example project. But with async/await, my assumption is that you could write a chat example and have it be easily understandable. Which means Getting Started can spend a page or two just explaining Futures and async/await, and then jump right in. Only then following up with Advanced topics like Polling, Tasks, Reactors, Exectors, etc. The internals that you don't need to know until later. So any commentary I give on the docs as they exist today seems fruitless. But if I'm wrong let me know and I'm happy to share my experiences.
And you've been wrong from the start. Tokio is in flux, not "more complex than the problem." There are three very straightforward components: * Futures. An interface for a task that can be run one step at a time. Each step takes as an argument a handle to signal when the next step is ready. A good external interface for async/await. * The event loop. Tokio exposes mio's functionality through the futures interface. That is, tasks like "read from a socket" and wakeup events like "the socket has data." * The executor. This isn't even provided by Tokio anymore, but by the futures library. The thing that receives wakeup events and runs the next steps for their associated futures. The documentation is confusing, and the in-progress churn belies the idea that it's ready for widespread usage, but the architecture is hardly "more complex than the problem."
Language is pure potential
so what your saying is that I need to implement it as this: impl&lt;'m&gt; Index&lt;ID&gt; for Manager&lt;'m&gt; { type Output = Entity; /// Returns reference to an entity for a given 'id' fn index(&amp;self, id: ID) -&gt; &amp;Self::Output { &amp;self.entities[&amp;id] } } impl&lt;'m&gt; IndexMut&lt;ID&gt; for Manager&lt;'m&gt; { /// Returns a mutable reference to an entity for a given 'id' fn index_mut(&amp;mut self, id: ID) -&gt; &amp;mut Entity { self.entities.get_mut(&amp;id).unwrap() } } Then what happens if the key it's looking for doesn't exist?
&gt;It's apparently not against the policy to do that so I think it would be a good idea. When people can't find a free name for their crate anymore, maybe some change will happen. How would our policies save us from lawsuits? If a court instructs [crates.io](https://crates.io) to hand over a package, our policies won't be worth anything.
I had planned to do that at one point in time, just never got around to it. 
People forget that Rubygems once had a host with namespaces and it was terrible.
&gt; everything is reliant on it. It's not, though. All the synchronous stuff is still there and still works. The integration with the ecosystem is not for nothing, either- the experience gained there has led to real changes to the APIs, that would not have been discovered if Tokio/futures were just developed off in a corner. There are real complaints to be made about the documentation and messaging, but they're being made and addressed elsewhere, without your persistent cries that Tokio is fundamentally broken.
&gt; Drop flags are necessary for the current drop semantics I'm curious under what circumstances we can't elide them completely through compile-time analysis.
I read most of this last night and it has taught me so much. Many things that I've never understood before became clear after reading your articles. They are super well written!
I gave up using rust for async I/O due to Tokio. I get the idea and examples, but writing actual projects with it is hard and confusing. Having to use RefCell or clone everything for each future leads to overcomplicated code that makes me wish Rust had garbage collection. When using Futures, error messages are inscrutable. When I write things involving futures, I just keep watching the output of `rust check` after every single line. If it complains, the previous line is likely to be the culprit. Developing that way is slow and painful. Crates using Futures and Tokio are still all in beta, with constant API changes. So you end up spending more time unbreaking and rewriting things rather than developing your actual application. On safety and reliability... Having to constantly convert between Future types and Error types has been causing and is still causing me headaches. If a Future is responsible for handling a client session, when an error is returned, will this break the acceptor loop (so, no more clients will be able to connect) or not? This is never clear. So you end up with something that appears to work, but a malformed HTTP query will DoS the service. I'm a die-hard Rust fan, but Tokio made me discover and love the productivity of Go and Crystal for writing servers.
\&gt; Given that Tokio is still in flux &amp;#x200B; Tokio is \*not\* in flux. The current platform is stable and will continue to be stable. &amp;#x200B; As far as I know, I do not think it will be likely that async / await will hit stable this year. Also, the initial implementation of async / await is not going to handle all cases. &amp;#x200B; Tokio handle production applications today and must remain stable. It cannot shift all at once to an unproven base. Given this, the switch will be gradual and over time. &amp;#x200B; We have a platform that works well today, we shouldn't put everything on hold because something new and shiny is coming eventually.
Yet highly prominent and accomplished hackers like jedisct1 echo their dislike.
https://github.com/zonyitoo/coio-rs and https://github.com/Xudong-Huang/may are very simple to use, but share the same limitation (no code using TLS should be called in coroutines), and that limitation appears to be hard to remove. Having it removed would be a game changer and make Rust and way more pleasant experience for writing async I/O code.
this one deserves to be at the top. seeing as it's the only comment with a real solution. 
Thanks! I'm glad they helped.
Removing that limitation would require pinning futures/coroutines to threads though and that'd kill the performance and scalability.
Isn't it already the case with tasks?
They've helped a ton. It's gave me ideas for projects which is a bonus.
&gt; What is a language but the sum total of its ecosystem? A language with pros and cons, like any other. What would you consider a good language? All languages have some strong ecosystems and some weak, and it is no different with Rust. What IS different though is that Rust is actively addressing what it finds to be weak and actively polling its users to see what they think Rust should consider weak. 
Continuing to implement more of my selenium webdriver client ([here!](https://github.com/saresend/selenium-rs), so I can finally do rusty end to end testing
By all means, actually go into details here for once...
I don't think Rust is good for async I/O yet. But I've successfully created and maintained with a team of several people multiple commercial projects using Rust and everyone who has taken the time to learn it has really loved it. Futures and such are just a little too fancy for the compiler to help all that much at the moment but I think for most of the standard stuff it's great now.
This contradicts Carl's own statement on this same post saying Tokio *is not* in Flux.
TIL: an isomorphic web application is one who can run either on the client or on the server. This [percy crate](https://github.com/chinedufn/percy) looks pretty intriguing!
Kind of like that yeah. With dependency injection you create something outside the short-lived block (lifetime block) so you can work with it longer, instead of creating it inside the short-lived block and attempting to make it live longer outside that block (which is unsafe by Rust's standards without some black magic). So instead of creating struct `Animal` inside fn `feed()`, you create it outside `feed()` and `feed()` just borrows it and `Animal` keeps on living outside `feed()` afterward. Some patterns do this in a manner where your factories are not really "factories" but assembly lines that create whole things out of smaller external parts. DI is useful for other things as well: it allows you to separate logic for things like hot-swapping implementations, better testability, and so on. Instead of doing a `MyThing::new(...)` inside all implementations of a function, you create it outside the function and allow the function to take in `MyOtherThing` as well, assuming they implement the same traits. Type hinting with traits is an important thing to understand when it comes to DI in Rust. Other languages use terms like "interface" in place of Rust's "trait" but they can be used in about the same manner. When I type hint a Trait I can insert _any_ struct/enum that implements the trait, instead of creating dozens of copies of the same function for different structs/enums. Forcing dependency injection of everything is not a good goal, as sometimes it is just simpler and clearer to do a hard-coded dependency. I don't have a CS degree either but working with PHP and spaghetti codebases has taught me that good separation of concerns and implementation swappability (DI/IoC/interfaces) in code is a good thing to aim for. With Rust I've been having trouble with the methodologies I've learned in PHP and Python (static v. dynamic, lower v.higher level etc.), but I think I'm getting there slowly. :)
No.
&gt; That's true for people who knew these things, but for all the rustaceans coming from higher-level language, having a layer between the dev and the low level mechanics in really welcome. The only two languages I've done async IO with are Javascript and Python, so I couldn't agree more with that.
How about [this](https://play.rust-lang.org/?gist=2bb34b426bb748633d8169cd24686cf8&amp;version=stable&amp;mode=debug&amp;edition=2015)? You need 2 impls for every base type (`i32` here), but at least it's recursive enough to work for all dimensions after that.
My thesis has always been simply: The problem: mio is hard to use. The supposed solution: Tokio high level framework that makes async code easier to write. Reality: Tokio is harder to use than mio. Plenty of frustrated users all around in these semi regular threads about Tokio.
That's... not what I asked? People are confused for *some* reason, sure. But it could just as well be poor docs (pretty obviously part of it), changing APIs with unclear messaging, etc. If "the devil is in the details," then *which details*? If my high level description is too abstract to show the problem, give me something *more* specific, not less.
I found it. Definitely the [best blogpost on futures](https://aturon.github.io/blog/2016/09/07/futures-design/) I've read. Maybe if it was updated to match the latest development, it could be good basis for documentation.
channels and timers are available in [mio-extras](https://crates.io/crates/mio-extras)
Not really, I preferred this name because it was shorter. Good point that it'll be hard to find with a google search, but at least its the only result on docs.rs
Hmm, while I mentioned RFC process, I somehow forgot to mention that it'd be managed by the Rust devs. They wouldn't have to review every crate, if some kind of community voting was used to pre-filter them. &gt; People up there are complaining about the Rust team already, Didn't see this yet. Can you point me at some cases? So far I found Rust team very competent. Of course, it's not a guarantee that it will remain so, but it's the best we have so far (I think). &gt; Who would get the money from "paid"? How do we make sure this makes more money then is lost in accounting? Core Rust developers and maintainers + covering expenses of running the server. Economic calculation isn't too hard in capitalism: price is the value of all things one had to sacrifice. Therefore if accounting costs x, then just add that to the price of publishing into `paid` namespace. Also I consider this feature as optional.
Yeah, I meant more how to use it, what are those `Recepient&lt;...&gt;` objects etc. Although, I didn't take lot of time looking into it. I plan to do it in the `future`, though!
How does Tokio 0.3 relates to this? Is 0.3 Tokio with async/await?
Coming from Scala I have no big problems with Tokio. Documentation is not great though. But the main use case is quite straightforward, isn't it? Just set up event loop and run your futures. And i think there can be no argument against futures - they are essential for FP.
&gt; Hmm, while I mentioned RFC process, I somehow forgot to mention that it'd be managed by the Rust devs. Our teams are already at the edge of their bandwidth and now we should come up with a way to manage this? &gt; They wouldn't have to review every crate, if some kind of community voting was used to pre-filter them. How do you make sure that community voting doesn't become a popularity contest? &gt;Core Rust developers and maintainers + covering expenses of running the server. Again: this isn't a well defined set of people. &gt; Core Rust developers and maintainers + covering expenses of running the server. That's overly simplistic. Running an organisation and bookkeeping incurs a base cost. You need to do taxes and accepting money from organisation can be a pain. Someone has to have the time and bandwidth and run all that stuff.
I hear ya, but there are things coming down the pike that, IMO, are complete and total game changers to how you write async code in Rust. It's why I personally haven't gone anywhere near Rust's async ecosystem. It's a hard sell right now, which understandably makes it difficult to find folks to write good docs. There's definitely some amount of misunderstanding here too. Like the GP, I also thought tokio was in flux. But your comment just educated me otherwise. :-) I think I just naturally assumed there would be some kind of move to futures 0.3/futures-in-std, but maybe that move is smaller than I expect? I dunno.
&gt; having a layer between the dev and the low level mechanics in really welcome. Abstractions that actually hide things are expensive. You end up with systems where being productive requires understanding more intermediate layer concepts than there are actual concepts in the problem domain.
Cool post! Probably a bit much for someone brand new if they want to click every link and understand every term before continuing to read the next paragraph, but I enjoyed it quite a lot.
Are there any visual debuggers out there beyond CLion's GDB integration?