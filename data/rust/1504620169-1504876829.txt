&gt; - This recursive version isn't prone to combinatorial explosion, which is nice, but it sets the 0th item of the sequence to `0`, which isn't. The zeroth term isn't zero? I always implememt fibonacci starting at 0 and 1.
Perhaps you can create an issue for rocket, to expose the handlebars instance wrapped in `Engine`
* Oh, I'm not really worried. Corporations will to everything in their power to keep the ability to write crappy software for as little money as possible. * But you can't really compete with the large corporation with billions in their back pocket. The Purism phone is nice, but it's maybe "worth" $200 based on the specs, not $600. A small company that makes a product that, frankly, almost nobody really cares about can't profit from economies of scale. * As end users we're stuck between a rock and a hard place. We need the government to stop the corporations from selling our data to the highest bidder and we need the corporations to stop the government (e.g. Apple TSMs) from peering into our private lives.
&gt; they will then be forced to use a more free solution, ie buy hardware that is in their control. Or they will be excluded from certain parts of society because free solutions then to be way more expensive. (Small market, no economies of scale.) &gt; If you can hack it, someone else can hack it. Generally yes, but I have several advantages. Physical access for one, and whatever I use to replace the vendor software is likely more secure than what I had before.
Fun thought. I mostly work in Java. Part of the nature of Java is that third-party libraries are relatively easy to 'hack' as a deveioper. Even if all you have is a jar file of compiled code, you can get stack traces with class and method names, debug into it, decompile it into roughly readable, and certainly recompilable, source code, easily replace individual classes in the jar file, override the classes in the jar file by putting replacements earlier in the classpath, write subclasses of library classes that were never meant to be extended, and substitute them successfully because dispatch is always virtual, use reflection to poke around inside library objects, and even do load-time bytecode weaving to modify library code as it is loaded. I've resorted to all of these at one time or another, when dealing with bad third-party libraries. I couldn't do any of that if the libraries were native code. I might be able to do some of it if the libraries had significant quasi-source parts (C++ headers, Rust rlib metadata). This is not quite the same thing as exploiting bugs to subvert authority. But it is about a continuum between security and modifiability. 
I did my advanced data structures and algorithms class last semester using rust (keep in mind, it was an undergrad class). I implemented a persistent hashmap, and a persistent BTree, Linked Lists, and touched on many other lesser data structures. I had a great time using rust, however there was a lot of work I had to put in to learn the actual language rather than implementing the algorithms.
As [pointed out](https://www.reddit.com/r/rust/comments/6y7abc/rfc_lets_make_await_a_method_and_possibly/dml7e24/) by [/u/MalenaErnman](https://www.reddit.com/user/MalenaErnman) using `await()` as a method will not work well with `std::ops::Try`. I have replaced the original RFC with ~~[a new one](https://github.com/alexcrichton/futures-await/issues/22)~~ that only proposes to deprecate `wait()`. EDIT: The new RFC has moved to [futures-rs/issues/571](https://github.com/alexcrichton/futures-rs/issues/571).
I had no idea D leaned so much on ranges in its APIs. That's really interesting!
I suppose it could be argued either way. I prefer to begin it `1, 1, 2, 3, 5`, because the 0 term is irrelevant to the rest of the sequence, and it makes terms 1-3 equal to their indices, which is convenient for quickly checking. 
This is really hard to do well inside an application. If i actually wanted to this for real, i would either use [at](http://pubs.opengroup.org/onlinepubs/9699919799/utilities/at.html), or use [cron](http://pubs.opengroup.org/onlinepubs/9699919799/utilities/crontab.html) to schedule a job which ran every minute, checked a persistent file of work to do, and then did it. Or something like that but involving more sockets. 
Or just a while loop: let i = 0; while i &lt; x { // do stuff that you would normally put in for-loop body i += 2; }
There's [webkit2gtk](https://crates.io/crates/webkit2gtk) that is usable in Rust, but it's a binding.
Nothing, still in the reading up on phase, and actively looking for an opportunity to get my hands dirty with Rust. I have made a C++ fiber based OpenCL wrapper that I am wondering if it would be translatable into Rust, but I have not landed on a first project yet.
Isn't it easier to use Python for that?
Working on making a little concatenative (stack-based) programming language called Rivet. The basics work to a point I'm happy with, now it's time for polish and features. It's heavily inspired by [this article](http://evincarofautumn.blogspot.com/2012/02/why-concatenative-programming-matters.html) with a fair dose of Postscript, but its real goal is to be able to sandbox untrusted programs. This way it can be used as, say, a server-side extension language for games (or other things), running user programs while making sure they get at most a fixed amount of memory and execution time. This is a problem domain I really don't see enough good solutions for. Partially 'cause it seems tricky to do well. The only practical and reliable way I can see to set some sort of execution quota is to count (bytecode) instructions up to some arbitrary limit, so that's what I'm doing. 
&gt;though OP probably doesn't care about those I'm pretty sure most people whipping up a fibonacci calculator are doing it because it's a good thing to do right after you do "hello, world!"
Thank you, and also for the answer in the other thread. But when I change the code to use `ExactSizeIterator&lt;Item=I&gt;` I run into a whole bunch of type mismatch issues: https://play.rust-lang.org/?gist=b1cc3abefac1ca6ea42bf95576861c46&amp;version=nightly What do the errors mean and how could I make this work?
&gt; ECS is not the only pattern for managing game state, so I don't know why all current engines choose to have it baked-in at the lowest possible level. Finally someone who agrees with me! I think your approach is really the correct one. 
Is `ulimit` not good enough? `ulimit --help` `-t the maximum amount of cpu time in seconds` 
&gt;for(let i = 0; i&lt;x; i+=2) For what it's worth, I think you're approaching this problem a bit from the wrong angle. AFAIK, the best way to do a for loop in rust is like for i in 0..x { // do stuff } It looks like you have to go to nightly to get your step size (the `i += 2` part, rather than `i++`), but there are easy ways to get around that and I don't often need that. You could also do something like [this here](https://play.rust-lang.org/?gist=c032a4fcf2f143f3351985ed2397e080&amp;version=beta). It's a little more verbose, but it's a pattern that can do some pretty cool things. For example, [here](https://play.rust-lang.org/?gist=4775b30428fa16eff6c47b672cd6e8cd&amp;version=beta) is an iterator over the fibonacci sequence (mostly). The code filters down to the ones divisible by 3 and prints all of those between 1 and 1000, which is possible because [iterators](https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.step_by) are wicked sweet.
https://github.com/kaedroho/kite. Early days, but enough to power a (currently very basic) Elasticsearch clone: https://github.com/kaedroho/rusticsearch
Anyway you need read http://blog.burntsushi.net/transducers/
Those errors come because [`cloned`](https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.cloned) itself requires the iterator to yield references in order to be called. You can try doing the clone yourself with something like `let orig = self.map(|i| i.clone());`, though if the iterator is yielding owned elements then I'm not sure there's a need to clone them at all instead of just moving them.
The problem is that exploits can be used for good or evil, and the bad guys have a lot more resources to hunt for them than volunteer jailbreakers do.
Because Google is heavily marketing the pixel? It's sold as a "phone by Google" and "the best of Android", without even mentioning the bootloader thing. And I heard that some Pixels do have a locked bootloader.
Sure, but whether or not third-party solutions are being developed has nothing to do with whether or not you can jailbreak a running OS using memory overflow bugs. Even if iOS was the least memory-safe piece of software on this earth it would do no good if there was no work done to develop the software taking advantage of these bugs.
Frankly, Rust's support for game development is pretty weak right now. As a language it's absolutely gorgeous to use, but the library support sucks. Probably the best library for games right now, `ggez`, is reliant on SDL2, which is a hassle to redistribute on Windows. Also, it only works for 2D games. You will find that your code is a lot better to work with when you get rid of shared mutable state, though, even if you stick with C++.
I've been working on Rust-on-Rust FFI, in support of trying out interactive streaming data analytics blah blah. https://github.com/frankmcsherry/blog/blob/master/posts/2017-09-05.md I got some help from sebk that opened things up a bit, ergonomically, which was nice. Got me moving again. :)
&gt;Yes, and that's called a buffer overflow. Memory lying around, unfreed, doesn't do much on its own. On the other hand, buffer overflows *can* hurt without other vulnerabilities. Buffer overflows aren't vulnerabilities. They're caused by vulnerabilities. If input isn't being sanitized for a stack allocated buffer and nothing is done to exploit it, I'm afraid that doesn't do much on its own either.
Not if it's multiple VM's running in a single process on Windows not started from a shell where you want to be able to produce meaningful error messages to the user about why exactly their program stopped.
This seems like it's getting dangerously close to the Halting Problem. How is the compiler supposed to know if a function blocks? Can we limit it to just functions returning `Future`? If so, what about functions that don't necessarily return one but are, in fact, non-blocking (e.g. schedule a task in the event loop, but the caller doesn't need to know when it's done, like a timer)? What about functions that don't do any I/O and thus are pure computation, which won't benefit from returning a `Future`? Depending on the input, these could block the event loop for a while, so should we force *all* of these to return a `Future`, though some instances could resolve immediately? If so, this makes it *really* inconvenient to do simple things like accessor methods or the builder pattern. I guess an annotation could work, but then you pollute the entire ecosystem with annotations, and there will be far too many exceptions to the rule to really be useful, especially for things that don't often appear in an async context. I think such a warning would be nice, but I don't know of a practical approach. I'm not trying to shoot it down either, just explore some of the challenges.
I think the best option is WebDriver/Selenium, but [as of this post a few months ago, there are no reliable solutions](https://www.reddit.com/r/rust/comments/6gr2z4/webdriverselenium_client/). I typically do this type of stuff with Python + Selenium since it works today and is well documented.
&gt;Your argument seems to be that leaking memory is as dangerous as a buffer overflow because it makes buffer overflows more dangerous. Ultimately a memory leak is a potential source of an overflow, just like a stack buffer is a potential source of an overflow. The Rust compiler can't predict sources of memory leaks well enough to prevent them completely. It is good at preventing compile-time-assessed accesses out of bounds. But they're the same thing, with different causes and different memory privileges. But the end result is the same. &gt; But that argument requires that you're susceptible to buffer overflows in the first place. No, it doesn't at all. You seem to be confusing the idea of buffer overflow with vulnerability: a vulnerability is what allows for the overflow to actually happen. A buffer allocated on the stack without input sanitization is still susceptible. &gt; Ultimately, it's the buffer overflow that's dangerous and we don't need to worry about those in safe Rust. &gt;As such, your argument has failed to convince me that leaking memory is of equal severity. That's fine. I'm afraid you lack an understanding of the semantics behind this though. If you can leak memory at runtime you can overflow the buffer. At runtime the only protection you have is what's provided by the runtime and the OS.
I noticed from your clap example that `quiet` is a hard "silence everything" flag, while my usual approach is to call that "silent" (eg. `-s`) if it has to be a separate flag and make `-v` and `-q` increment and decrement a verbosity counter, so it's possible to override the use of `-v` in shell aliases without resorting to the nuclear option. (ie. If I use this, I'll probably use `verbose - quiet` with some value clamping.)
`.map` compiles but it consumes/advances the `self` iterator. I used `.cloned` to create a copy of the iterator so it retains its original state. I just realized though that what I'm actually after is only to clone the iterator itself, regardless what the contents are - so I just have to constrain the iterator to `Clone` and use `.clone`, which simplifies the implementation a lot! https://play.rust-lang.org/?gist=326b7afbd76487a0b46f821e3c2a9a3c&amp;version=nightly Thanks again for the help. The whole trait system is still a bit unknown territory for me but I'm slowly making more sense of it.
A while back we had an RFC for adding new communication channels. It didn't go too well, especially because people had different perceptions of what was being proposed. I think having #rust-like channels across communication media would be really good, and it's worth growing these channels organically. If they become large, we can mark them as official. This slack has existed for a while, but it's languished. I want to see if we can grow it. 
(This pattern is called a "destructor bomb". It works pretty well for most linear type use cases, though of course you lose the additional type safety)
Those are just examples. Most of the examples center around the clap code but you are free to structure your arguments however you want. You can change "quiet" to "silence" and change the short option to "s" then add another "quiet" it should work. I've written a gist that shows off what I'm understanding from your explanation. https://gist.github.com/cardoe/06de06f29ea2133d611e157b4ca161fc
There's also [perlin](https://github.com/CurrySoftware/perlin).
I can't recall why anymore, but I do recall at some point realizing that Rust's abort-on-double-panic was the only safe thing to do.
Can I suggest integrating with `termcolor` to use different colors for different log levels? ([My app does that](https://github.com/remram44/dhstore/blob/ff0801dfd69a713a5b22df9ce2f9a9f48fce849e/src/log.rs) and I find that it helps a lot)
For me personally, I don't think I would use this because it doesn't offer that much. For the most part, it's renaming a couple of methods and hard-coding the `Relaxed` memory ordering. The latter part is especially important and should be documented very clearly in your API docs. In general, the "safe" ordering to use is `SeqCst` (and is consequently also the most costly), and I kinda imagine that's the one you probably want to use if you're going to try to hide the memory ordering concern from folks.
I buy phones for $200 slightly used on eBay. They last 1-3 years. Average of 2. So I spend $100/yr on phones. Of course, I use Android. I do my best to not deal with companies that put their business model between everything I want to do.
Needs more API docs. Also using `Relaxed` (as opposed to `SeqCst`) is faster but can lead to subtle bugs, if not used appropriately. I think Rust community would generally advise using the more reliable version, and let user opt_in to the faster one. So in your crate you should use `SeqCst` by default and maybe have `*_relaxed` versions for performance.
Good idea. Thanks for the input. I've created an issue for myself at https://github.com/cardoe/stderrlog-rs/issues/4 and linked you so that you'll get updates when its done.
Sure! I'm `@remram44` though. Shouldn't be too much work, since termcolor is so good!
I'm currently using vscode-rust it's working well after some gotchas in the setup process that was more about learning rust &amp; racer etc that anything else. I've found that (I think racer not the plugin) doesn't get some of the possible completion options from traits etc 
The next year's edition should really include a question on what do people actually use _for_. The closest one here is asking for target platform, but "Linux" may mean anything from high-performance servers to embedded applications. I'm not sure it helps much when it comes to prioritizing roadmap points such as the async server support.
These results give some hard numbers to what I had been observing in the community. I think this will give us a good launch pad for what we should focus on next year! Thanks to the team who took the time to sift through all these answers and put them together :D
Both bug a little bit for me but rust code less. If the formatting doesn't work, just use: cargo +nightly fmt
&gt; Ultimately a memory leak is a potential source of an overflow No it's not, it's a completely different thing than accessing a buffer out of bounds. &gt; The Rust compiler [...] is good at preventing compile-time-assessed accesses out of bounds. Bounds are checked at runtime, not compile time &gt; But the end result is the same. A memory leak causes a program abort *at worst*, a buffer overflow causes an abort (segfault) *at best*. At worst, a buffer overflow can lead to arbitrary code execution, something which a memory leak on its own will never cause. &gt; A buffer allocated on the stack without input sanitization is still susceptible. Susceptible to a buffer overflow, just like a buffer on the heap. There's really not that big of a difference, except in the way I'd have to write my exploit.
Under "What's your preferred way of installing Rust?", I would like to know how "Linux distribution package" compares to last year. I don't see that information shared in the former results. Of course, I am biased as Fedora's packager, and we didn't even have packages yet at the time of the 2016 survey. Given that, 18.9% current distro use seems pretty good!
I was suprised that sublime text was not the number one editor. I never have heard of VIM can somebody explain to me why is VIM so popular with rust users.
&gt; I cried when I learned that arrays have a constant size Rust uses vectors ([`std::vec::Vec`](https://doc.rust-lang.org/nightly/std/vec/struct.Vec.html)) for variable size heap allocated "arrays". Arrays in Rust are stack allocated and pretty much have to be constant size. Many other languages don't have this distinction; Rust is a systems language so allocation matters.
Vim is a text editor first released in 1991; it has a loyal and very passionate following. https://en.wikipedia.org/wiki/Vim_(text_editor) &gt; why is VIM so popular with rust users. Vim is popular generally; I don't think there's any one specific reason why Vim is particularly popular amongst Rust people; there has been a plugin available for a very long time https://github.com/rust-lang/rust.vim/
&gt; Other strong themes included the need for: faster compile times, more corporate support of Rust (including jobs), better language interop, improved tooling, better error messages, more marketing, less marketing, and improved support for web assembly. Should we conclude that these each had less than 8% support? I can't tell if the ones called out were in support of pushing ergonomics, or if it really was that relatively few people feel compile times are an issue.
Yeah, not sure how we didn't post this last year in the blog post so I had to go back and pull up the results from the raw data. 52% rustup.rs, 39.8% linux distros, 14.8% homebrew, 14.1% multirust, 10.9% msi, 9.1% rustup.sh, then 4% for tarballs and pkg
&gt; why is VIM so popular with rust users It's because vim is as hard to master as rust.
Hmm, so that's a big drop for distros percentage-wise, and from the number of respondents, I guess it's also a drop in raw numbers? Still, even if Rust developers will prefer rustup, we still need distro rustc to ship those Rust apps, so I'm not too worried. :)
Ah, yeah, if you don't mind potentially iterating all the values twice that'll work.
Right. I put the top trending topics first, and then the ones that were heavily mentioned, but maybe not quite at the same volumes as the top items. Ergonomics in the post is specifically things that would fall under the language ergonomics initiative, so things like new language features, fewer false positives with the borrow checker, etc.
[removed]
I think the first paragraph is missing a more: &gt; That’s more than 2,000 **more** responses than we had last year!
/r/playrust
"6% of responses talk about the importance of creating better documentation. These covered a topics from helping users transition from other languages, creating more examples and sample projects, helping people get started with various tasks or crates, and creating video resources to facilitate learning." I have actually never seen a better documentation.
Vim and emacs are both old school and popular with programmers. Somethings I really like about both: * highly configurable and have package managers that make it easy to share plugins. This is probably the main reason they stay relevant. * in my particular work style I use an ambient Linux environment as my "IDE". Both of these editors can be run naturally in a terminal inside tmux. * reasonably efficient even with a million plugins running. Some of us hate input lag when editing
I like to point out that sublime text also has a package manager.
As a Vim user, I'm pretty shocked that Vim is number 1. I'd have though Visual Studio Code would have been number 1, and then a bunch of others. Especially when a good chunk of developers are on Windows. Of course you can use Vim on Windows, and I chose to develop on Windows. So that includes myself. In my anecdotal experience however, I am in the extreme minority. Generally Vim users are running Linux, and never run Windows.
Your post made me smile; I run Windows, and use Vim and Code both... I developed https://github.com/vim/vim/pull/1356/ on a Windows machine.
Wasn't there a question like that on the survey? I remember saying that I was using it for robotics and I was super excited to see if anyone else was.
Is there any example of how the source code looked like before the first commit in the rust repostiory? I heard that the syntax of the language was more closer to OCaml: was this true? If so, I'd like to be able to see how it looked because I generated the documentation from one of the very first commits and the syntax looked somewhat similar to what it is today.
I would really like for a better **table of contents** to exist in rustdoc. Some sort of **collapsible tree-view of all modules and types/traits** in the standard library. The actual [text written on the rust docs home](https://doc.rust-lang.org/std/) is useful, but having that top level view would be even more useful IMO, especially because it gives a glimpse into the entire std library, so helps build the mental model of what lives where.
Or were you asking me to rename quiet() to silence()?
lol I feel we are a dying breed. The thing people find hard to get is I actually quite like developing on Windows, and I like using GVim instead of terminal Vim. I still do custom vim plugins, my own vimrc, and write bash scripts to do my work for me daily. I enjoy doing all that on Windows.
I agree! I find rustdocs hard to navigate most of the time. It seems there's some trick to learn, and I guess those with enough experience to grok it stop seeing it as a problem.
Sure, but can its ecosystem compare to vim's? It's tough competing with something that's been out for 26 years
If you have the time, filing bugs over at https://github.com/steveklabnik/rustdoc would be appreciated; the more detail the better. This is my re-write of Rustdoc, and we'll be considering possible ideas like this.
Good idea! [Done](https://github.com/steveklabnik/rustdoc/issues/168).
I find rustdoc great for navigating if I already know what type I need to lookup. I find it difficult when I'm not quite sure what type I want and need to traverse it to figure out what I'm looking for.
They just don't know how to quit. I have to admit that I am using Rust since over a year and I still don't know how to quit.
&gt; How much Rust code have you written? Let's say I've written no Rust, but know C. Personally, I have had the experience of coming to an understanding of `&amp;` as something that basically give you a read-only loan of the data, but that you otherwise use that reference in the same way as a non-reference. Then, something didn't work, and people said "it's a pointer, you see; it has just been automatically dereferenced for you until this point". Ugh. I had build the wrong mental model. Anyway, _I_ feel that it is best to reason in small steps when you're learning, even though you can leap past them once you have understood things and are used to them. I would avoid the simplification of setting _c_ = _G_ = 1 until you have actually learnt relativity, and so on. Edit: Why did you delete your comments?
You need to build a Vec&lt;CString&gt;, then build a second Vec&lt;*const c_char&gt; containing pointers to each CString in the first Vec, then take the address of the second Vec. let mut argstr: Vec&lt;CString&gt; = Vec::new(); // populate this vec however let mut args: Vec&lt;*const c_char&gt; = argstr.iter().map(|a| &amp;a as *const c_char).collect(); let argv: *const *const c_char = &amp;args as *const _; This code probably won't compile, and I may be incorrect about the process (I'm extremely unsure on how the args get passed into the new process memory space), but that should be the general means of turning Rust strings into OS argument sets.
There is no "--disable-magic" switch in rustc that I know of. But, and maybe that is interesting to some folks, I recently discovered `cargo rustc -- -Z unstable-options --unpretty=hir,typed` which prints out your source with type annotations as well as expanded macros. Sadly, it's… well… unpretty.
There was a question about this on the survey.
It's not really a new keyword, the bug I was talking about is just one situation where it would be helpful to have it. The `box` keyword predates that issue, it has been around for a very long time already, it was [introduced as a replacement for sigils several years ago](https://github.com/rust-lang/rfcs/blob/master/text/0059-remove-tilde.md). I think the goal is to have something similar to `new` in C++. I'm not really that knowledgeable about the rationale for using a keyword/operator but there are some further RFCs about box and possible changes/replacements: https://github.com/rust-lang/rfcs/blob/master/text/0809-box-and-in-for-stdlib.md https://github.com/rust-lang/rfcs/blob/master/text/1228-placement-left-arrow.md
That makes me curious why that information wasn't made available in the results.
I checked both VSCode and Vim, but I use VSCode &gt;90% of the time and just use Vim for small edits from the CLI (e.g. when submitting small PRs to projects). I do wonder how many of the Vim votes are Vim-as-a-secondary-editor because I could check multiple boxes.
Are the raw results available anywhere?
I think Rust is hard to learn because it is so *different*. There are so many things where Rust differs from other mainstream languages, and it's usually for a good reason. So yes, our docs are some of the best programming language documentation out there (tip o' the hat to /u/steveklabnik1, /u/quietmisdraevus and all the others) – they have to be, lest users stumble over differences to what they are accustomed to. They are still not sufficient to get users over the initial hump. My personal hope is that once we have clippy on stable (it's on its way, we already can build it as a submodule), we can improve it to the point where new Rust users can get the help they need directly from the compiler.
it is not ready to browse the wild Internet, but if you need it to render a particular webapp that you control, I believe you can use it as an engine for your app.
Hello! I helped build the survey. No, the raw results are not available, and we don't have any plans to share them since people may have put personally identifiable information in it. If you do have questions you want to ask the survey, please let us know. We will run it by our teams, and if it's safe to share, we'll be able to provide a summary response.
No idea, but I'm certain its not malicious.
Where are these surveys advertised? I've missed it a few times now. I check this subreddit frequently and occasionally look at the rust-lang forum (though I find the UI there to be unpleasant).
We just had a ton of responses, so rather than wait until everything was processed, we wanted to cut a post with what we got now. It's my hope we'll make some follow on posts that drill into some of the questions we weren't able to get to in this post.
Here, the blog, on the users' forum, and through Twitter, at least.
[CEF](https://bitbucket.org/chromiumembedded/cef) has a C API which I have successfully used bindgen on (see old version [here](https://github.com/cretz/rust-qt_cef_poc/tree/master/cef-sys)). I strongly recommend CEF as it provides a lot without asking a lot.
Coming from other languages with algebraic data types, something the rust docs don't usually answer for me is the question: for a given type how do I construct it and consume it? Partly this is hard because in rust the ways to create/consume a type are often hidden in traits. I can usually find the new function but in practice I might need to learn about From instead. I'm at the point with rust docs where I read any text blurbs the author provided then go straight to the source and ignore the rendered rust docs. I find on average this is a much better way to get the information I need.
The intelliJ Rust plugin shows types on `let` bindings. Neat feature if you ask me.
Interesting. Not sure how I missed it.
Cool! Can it print lifetimes and dereferences too?
&gt; The beginners are also an impressively large set, with the “less than a month” crowd at just about 18%, meaning we’re currently attracting nearly a 1/5th of our user base size, even as it grows larger, every month. You can't draw that conclusion because survey respondents are self-selected. There's no way to know for sure based on this data, but it's totally plausible that new users responded to the survey at higher rates. In that case, survey respondents would have a bigger proportion of new users than the rust userbase at large.
Try it out, I haven't looked more closely. It's _really_ unpretty
Is it possible to bring in an unstable Rust feature if and only if my crate is being compiled with a dedicated feature flag? I want to be able to compile on stable without issue, but also, if someone is on nightly and wants to use i128, I want to be able to offer functionality for that as well. Cargo informs me that #[cfg(feature = "e128")] #![feature(i128_type)] is wildly illegal, and doing that without the ! doesn't work either. TLDR I want access to the `i128_type`, only if compiling with a specific feature, and without that feature, to be able to compile on stable.
And in function argument lists, closure parameters, and IIRC tuple fields It's WILD
&gt; Needs more API docs. Yes, of course. But I first want to get the functionality correct :) &gt; Also using `Relaxed` (as opposed to `SeqCst`) is faster but can lead to subtle bugs, if not used appropriately. Yes, exactly. The question is: Is this an appropriate, correct use, if only `AtomicCounter`'s `inc()`, `add()` and `reset()` methods are used?* From reading documentation, I believe it should be correct but I'm really not an expert, that's why I'm asking here :) To quote from the [nomicon](https://doc.rust-lang.org/nomicon/atomics.html#relaxed): &gt; For instance, incrementing a counter can be safely done by multiple threads using a relaxed fetch_add if you're not using the counter to synchronize any other accesses. This is _exactly_ what I'm doing here, plus the `reset()` function which should follow the same logic, right? However, from [LLVM's docs I'm not as sure](https://llvm.org/docs/LangRef.html#atomic-memory-ordering-constraints): &gt; There is no guarantee that the modification orders can be combined to a global total order for the whole program (and this often will not be possible). Hm. &gt; So in your crate you should use SeqCst by default and maybe have *_relaxed versions for performance. This is exactly what I'm trying to avoid. The idea of this micro-crate would be: Deal with the memory ordering in the fastest, correct way, so not every rust user that needs an AtomicCounter needs to wrap their head around memory orderings. A zero-cost abstraction, if you wish :) *This is the reason I should probably wrap the usize in a struct, so it cannot be used in other operations
No, it never looked especially OCaml-y. It was _written_ in OCaml, but I aimed for a syntax somewhere in the C/Algol family: some look-and-feel borrowed from Newsqueak/Alef/Limbo, Napier88, Hermes, Sather. The syntax hasn't changed _too_ dramatically. The main differences are: * removed a couple widely-used sigils (`~`, `@`) * changed some things from words to punctuation (`rec()`, `vec()`, `tup()`) * changed generics from square `[]` to angle `&lt;&gt;` brackets * fiddled around with keywords (`alt` -&gt; `match`, `rec` -&gt; `struct`, `ret` -&gt; `return`, etc.) * changed syntax-extension invocation from `#` to `!` (imo a mistake) * switched from statement-y to expression-y (eg. `let x = if a { p } else { q }`) * bikeshedded the semicolon rules until we were blue in the face * large semantic areas removed/reworked (objs, typestates, effects lost; structs and impls added) If anything I'd say it's got a bit _more_ OCaml-y over the years, in terms of appearance. The semicolon rules wound up similarly inscrutable to newcomers and almost-but-not-quite capable of simulating statements, in order to accommodate expressions. The record and tuple tycons went OCaml-wards, probably a win for succinctness. We added leading-single-quote-ident as a lexeme to accommodate lifetimes (I quite dislike this) which mirror type parameters in OCaml. We gained the keyword `ref` (also imo a mistake) and we gained a very rich pattern language, much richer than at first (this part I'm thrilled by).
I install Rustup from the distro packages and then use that to maintain my Rust install; I don't know where that counts in the list or what I put down for it
&gt; For the most part, it's renaming a couple of methods and hard-coding the Relaxed memory ordering. Yes, exactly. The idea is that not everyone needs to try and wrap their head around memory orderings if they "just" want an atomic counter. Yes, I will document this properly, if this approach is correct. (If we end up requiring SeqCst, so be it. I will update the crate and write docs accordingly) &gt; In general, the "safe" ordering to use is SeqCst In general, yes. But is it required for a simple counter? (see my reply to dpc_pw)
Mainly pointing out the usability flaw in implicitly encouraging a "silent" behaviour over a "quiet" behaviour in examples and API design. Remaing `quiet()` to `silence()` or `silent()` would definitely help there, but it's your project, so it's up to you what you choose to do.
You want cfg_attr: https://chrismorgan.info/blog/rust-cfg_attr.html
With [cargo-tarpaulin](https://github.com/xd009642/tarpaulin) my code coverage tool I'm working on integrating syntex_syntax for source code analysis. Some existing tools I've checked have little bits to remove any lines like single close brackets to try and clear up instructions you don't end up hitting that you find using the dwarf tables. However, I think a cleaner solution is to actually use the languages AST to tailor it. It should also lead to better filtering rules, e.g. ignoring the unreachable macro, derived macros, filtering tests or functions with a certain cfg. It will be a big feature drop and it might take me a while to get up to speed with syntex_syntax so I'm aiming for two weeks from now for it to be ready for public release. And it should close out a few issues I have open. Also, not sure where to ask for help but there's an issue I've gotten stuck on [#23](https://github.com/xd009642/tarpaulin/issues/23), the cause is known the solution isn't. It may get resolved when [this issue](https://github.com/rust-lang/rust/issues/35061) in rustc is closed. If anyone fancies lending a helping hand that would be appreciated :)
I am tempted to try Vim. But I feel like it would end up being another thing I use to procrastionate( a word which I never will learn how to spell)
UGH I knew I was forgetting something and couldn't for the life of me come up with what it was Thank you!
I come to Rust from C++ and I can't say that Rust is *very different*. Yes, there are few different features, but other stuff is mostly the same. PS: don't get me wrong, Rust is much better than C++, but not that different.
Is there a scanf equivalent in that? If not what would be the best way to read a file which had 2 ints and 2 floats all separated by new lines into 4 separate variables? 
It might be that we can get away with a small number of annotations in the standard library, and let everything else inherit from there? Just thinking off the top of my head: - IO (read/write/accept) - std::thread::sleep - thread join
Critical statistic that was missed: How many heretics ticked both Vim AND Emacs? (I know I did)
Same here. I personally voted for faster compile times. My current project takes 10s to build in debug mode with CARGO_INCREMENTAL and up to 40sec in release with O3 and LTO.
This is totally off topic now, but I used emacs for years and then one day I thought, "I wonder what I'm missing." I gave vim a good solid year to sink in. Now I use both and love them both. When emacs vs. vim comes up now I just think, "Wait. You're both right!"
Perhaps, and it's certainly a fruitful conversation to have. However, I think there are enough exceptions to the rule that we should definitely brainstorm other solutions to the problem.
Same. Sublime has a huge popularity decrease. Probably because of stalled development (endless beta) and old, annoying bugs (like broken hotkeys in non-US layout). I'm using Sublime for Rust.
You are probably right in a way. There's no really big difference, just oodles of small ones.
I'm happy to anounce release of Mocktopus, the best mocking framework for Rust! What makes it different from other such tools is power and low introduction cost. By adding a few lines to your project root you can mock any function in it. This is an initial release. The crate is fully usable, documented and tested, but it does leave room for tooling development. I invite everyone to contributing, all help is needed!
In absence of an automated tool to help with this, I guess all I can recommend is find examples where these features are in play and try to unravel them by hand. This of course is greatly added by good documentation about those features. When I was learning haskell we had a bot that could do simple transformations on inputs. Like converting to/from point-free form, unraveling a monad transformer stack, and that sort of thing. Perhaps a similar tool could be created for Rust to aid in teaching.
A GC cannot preclude data races. Even if you can live with the cost in runtime and RAM.
... and then you have the burden of gc ... and possibly data races
Read the file into a string, split it by lines (String has a lines() method on it that will just iterate across them), and parse each line as appropriate. let mut f = File::open("filename")?; let mut s = String::new(); f.read_to_string(&amp;mut s).unwrap(); let mut lines = s.lines(); let a: i32 = lines.next().unwrap().parse().unwrap(); let b: i32 = lines.next().unwrap().parse().unwrap(); let c: f64 = lines.next().unwrap().parse().unwrap(); let d: f64 = lines.next().unwrap().parse().unwrap(); AFAIK Rust format strings are solely for generating output
&gt; Not sure how I missed it. busy/distracted while coding cool rust stuff?
Rust + Vim ... not even once! ^(-- a Rust + Vim addict.)
Wow, no mention of [Kate](https://kate-editor.org/). I appreciate that it's a little low on Rust-specific features ([KDevelop should fix that](https://perplexinglyemma.blogspot.co.uk/2017/08/autumn-is-here-wait-this-is-gsoc-not-got.html)), but what's not to like?
I would not recommend it. The tradeoffs Rust makes are not very helpful for someone whose primary concerns will be rapid prototyping/fast ECD cycle/etc. If I were you, I'd use Python.
There are a few crates out there to support scanf-like parsing. [scan-fmt](https://crates.io/crates/scan_fmt), and [text-io](https://crates.io/crates/text_io) seem to be relatively popular. [whiteread](https://crates.io/crates/whiteread) and [scan-rules](https://crates.io/crates/scan-rules) might suit you as well.
Fear not, there is a solution: https://github.com/austin-----/vscode-twitter
&gt;That seems strange for a class focused on data structures... I disagree with this. Data structures are language agnostic; why shouldn't the class be as well? That being said, I doubt OP's professor has `rustup` installed, so using Rust specifically might be an issue.
Firefox nightly, that is, not Rust nightly. For existing nightly users here: It probably already was on for you since we were slowly ramped up the percentage of users with it on, but now it's on for everyone. And probably will ship with 57.
Cool logo, but… hail ~~Hydra~~ Mocktopus?
As /u/coder543 mentioned at the end of their post, I think using and iterator would be the cleanest solution. This could allow the top level dispatch loop to potentially choose multiple different iterators depending on the result. 
Yay! And now the wait for WebRender begins...
Input: fn main() { // Say hello! println!("Hello world"); } Output: #[prelude_import] use std::prelude::v1::*; #[macro_use] extern crate std as std; fn main() ({ // Say hello! ((::io::_print as fn(std::fmt::Arguments&lt;'_&gt;) {std::io::_print})(((&lt;::std::fmt::Arguments&gt;::new_v1 as fn(&amp;[&amp;str], &amp;[std::fmt::ArgumentV1&lt;'_&gt;]) -&gt; std::fmt::Arguments&lt;'_&gt; {std::fmt::Arguments&lt;'_&gt;::new_v1})(({ static __STATIC_FMTSTR: &amp;'static [&amp;'static str] = (&amp;([("Hello world\n" as &amp;'static str)] as [&amp;'static str; 1]) as &amp;'static [&amp;'static str; 1]); (__STATIC_FMTSTR as &amp;'static [&amp;'static str]) } as &amp;[&amp;str]), (&amp;(match (() as ()) { () =&gt; ([] as [std::fmt::ArgumentV1&lt;'_&gt;; 0]), } as [std::fmt::ArgumentV1&lt;'_&gt;; 0]) as &amp;[std::fmt::ArgumentV1&lt;'_&gt;; 0])) as std::fmt::Arguments&lt;'_&gt;)) as ()); } as ()) Looks like it does, but yes, not so pretty. It does preserve comments though, so you should be able to find a particular line's expansion relatively easily. 
why is Android excluded?
What is needed for this to be available on stable Rust? Is it just `proc_macro`?
Anyone have any up to date performance comparisons between stylo and gecko?
Anyone got any tips on a decent setup for Vim? RLS plugins etc? I'm mostly a code user.
oh one more little point - r.e. 'move' not being destructive; So ```move(x)``` leaves ```x``` in a state where certain operations are invalid, but you could re-assign to the value and re-use it. So, basically we just need the analyser to be told exactly which those operations are (something like "after move, only assignment to re-intiialize it is valid".. ) There will be demand for this kind of analysis for optimisation (IMO), e.g. eliding clears/checks; so there would be demand for the necassery machinery to be integrated into the compiler. I still like the idea of a 'destructive move that really consumes it' but it wouldn't be essential
Thanks for your detailed answer.
Huzzah! I'm curious about two things: - What's the "probably" for 57 based on? I.e. what are the looming risks? - Is there somewhere showing performance figures?
What about the raw data minus the freeform questions? (And freeform "Other" responses to multi-choice questions redacted to just be "Other".)
If you never have used a language without a gc, learning one requires some effort. Nobody says that C++ is easy to learn. While I think that "being easy to learn" is a noble goal, we shouldn't have nor give unrealistic expectations: Rust doesn't magically save you from having to learn how to do manual memory management. It makes it safer, but one still has to learn it.
Unfortunately more: `#![feature(fn_traits, get_type_id, proc_macro, unboxed_closures)]`
If purchased through Google, the boot loader is not locked.
This is a big deal. Kudos to all who have contributed to this monumental achievement!
I assumed this was another ML lib, but I was pleasantly surprised. It might be worth getting specific when you advertise it, since the ML crowd has polluted this particular namespace. Looking forward to seeing more docs/blog posts.
You know that you can have both in the same editor right?
Great news! What kind of improvements should users expect, if any?
Currently we ship two style systems simultaneously. Additionally, we've not really worked on optimizing codesize of stylo, and probably won't be able to by the time 57 beta is cut off trunk. Anyway, codesize doesn't matter that much (at this scale) on desktop, but it does matter for Android. We've decided to prioritize getting desktop right and deal with Android later, probably by 58.
Hm maybe there's a way to shuttle this through rustfmt?
Faster page loads and some more features (for example, Servo/Stylo supports `calc()` in more places than the old Gecko stuff does).
_Personally_, I don't find the language hard _to learn_. I mean, you can avoid the borrow checker almost completely by `Box`ing everything (and it's not a big deal, many other languages put things on the heap), and slowly moving to the stack. What I find hard is the ecosystem, where most libraries use all kinds of cool features (and they probably are cool, and I understand why they do it, but it doesn't help me as a noob). For example, all the `#[]` stuff. Oh. And Macros. Macros are great when they're small and localized (`println!` and `vec!` are good examples). But quite often result seem quite magical. 
&gt; What's the "probably" for 57 based on? I.e. what are the looming risks? That something horrible happens during the beta; i.e. there are major breakages that were just not caught by automated tests or the nightly experimentation. Rather unlikely. As long as nothing like that happens we'll make it into the release. &gt; Is there somewhere showing performance figures? We have promising performance numbers, but I don't know if there's a good source that shows the improvements clearly. 
Oh neat, I'm surprised to hear that Stylo actually includes features that Gecko didn't already possess. Are there performance graphs somewhere?
I missed it last year. Unfortunately, it's selecting for people actively following Rust and regular twitter users. I'd like to see a mailing list just for the annual survey, promoted on the website, in the book and on installation of rustup. For rustup ask: would you like to be notified of the annual survey (1 email/year)? (Y)es, (N)o, (A)lready registered, (I) participate but don't like email reminders. The answer could be used to determine participation in the survey.
To quote a certain jedi knight; *This is not the subreddit you're looking for."* Try /r/playrust /r/rust is for discussing a programming language called Rust. Posts like yours are, as expected, quite frequent though.
Well that's embarrassing 
If you want to keep the signature of step() the same, you can also do this: fn play_game(mut game_state: GameState) -&gt; Score { loop { game_state = match game_state.step() { GameStepped::Cont(game_state) =&gt; game_state, GameStepped::Done(score) =&gt; return score } } } 
Yep, Firefox with Stylo enabled feels like Chrome!
Well, while you are here, why not learn a thing or two about programming? :p 
Great crate! I can't wait to test it!
I'm an S2 owner, have 512 megs. Definitely a good idea to not require too much RAM!
&gt; I'm surprised to hear that Stylo actually includes features that Gecko didn't already possess. It's not exactly _supposed_ to, we strived for parity (including making stylo match gecko's preexisting suboptimal behavior in some cases IIRC). But stuff like this where Gecko is buggy but it's not worth fixing the buggy behavior in a style system that will cease to exist in the future is ok, basically.
This isn't RAM, this is on-disk codesize.
Your formatting is a bit irregular. I use VScode with the rust extension `kalitaalexey.vscode-rust` which can autoformat your code on save. Usually I like the default formatting, but it's quite configurable if you don't. &gt; found it a little restrictive Me too at first, but that's kinda the point; Rust doesn't let you do things that could go horribly wrong. &gt; I cried when I learned that arrays have a constant size. What language(s) do you have experience with? It's normal in systems languages (such as Rust) to divide arrays into two types; one which has a constant size (usually called an array) and one which can be resized (sometimes called a vector). Most non-systems languages only have the vector-style of array because they're not so concerned with the cost of allocating memory.
&gt; I disagree with this. Data structures are language agnostic; why shouldn't the class be as well? Implementation isn't agnostic though. My data structures class involved learning about how data structures are implemented in memory. That involved taking care of things like memory leaks, invalid references, how garbage collection works, etc. If I was allowed to write it in SML or OCaml, then my work would be very different than if I wrote it in C. Sure, you can make them in whatever language you fancy, but at that point you're building high level data structures on top of other high level data structures, which in my opinion isn't useful at all. That said, my class was more of a beginner course focused on implementation. If you're more focused on the theory, then language doesn't really matter as much.
Why is that a bigger problem than RAM? Play store limits? I mean when you have an app loaded its code is copied to RAM so that the process can access its own code, no?
&gt; I mean when you have an app loaded its code is copied to RAM so that the process can access its own code, no? Virtual memory, not physical memory. It only hits physical if it's actually getting used. ----- It's a problem because folks have limited on-phone hard disks (not all apps can go on SD cards), so each megabyte counts.
&gt; it'll take years for it to be adopted by the ecosystem. you could equally say it'll take years for Rust to be adopted; for companies with existing C++ source-bases, switching language is a bigger risk than adopting new library features
Just tried this and the compiler complained that lines needs to be mutable. Don't see why that is, we aren't mutating it?
Knew I forgot something. `lines` is an Iterator, and every call to `next()` mutates its internal state.
What could be the reasons for the code breakage experienced by 7.5% of users? I thought Rust was 100% backward compatible.
IMO, this is the right choice. I really look forward to using Firefox as my daily driver on Android, performance is my only blocker. 
Strictly speaking it's required to use type inference for anonymous functions, due to their unnameable types (which D calls Voldemort types due to that). `impl Trait` can only be used for return types.
The link to https://handlebarsjs.com gives me a certificate error. 
Mine: 27mins. =/
Javascript. Which let's you do everything. It let's you do things like "1"+2 and it will out put 3 . It let's you compare strings to integers.Dosen't make you type ; . It let's you add functions as values of other functions. And that is the reason that I chose to study RUST. I wanted a low level language that had higher order functions. 
`next` takes a mutable reference to `lines`, so `lines` must be mutable. `lines` is not a set of strings, but a struct that has some state that gets updated as the iterator is consumed.
nope, syntax errors.
Maybe some attribues to mark the functions/types with these special properties and the places where they can't be used? trait Future&lt;...&gt; { #[has_property(blocking, advice="blocking method, use the await! macro instead")] fn wait(...) -&gt; ...; } fn bar(...) -&gt; Future&lt;...&gt; { } #[disallow_property(blocking, in_closures="False")] fn foo(...) -&gt; Future&lt;...&gt; { bar(...).wait(...); // Compiler will complain: "blocking method, use the await! macro instead" } Of course, the `#[async]` macro will be the one to add the `#[disallow_property(...)]` attribue. We may also want an `#[allow_property(...)]` attribue for macros that do use these things in the disallowed zones(but with the necessary ceramony). This is not necesary for `await!`(because it does not use `.wait()` but in other cases it may be needed. 
https://news.ycombinator.com/item?id=15177789
Ah yes that makes sense. Would it make sense to make content immutable after its no longer needed to be mutable? 
Even setting aside the "spying on me" aspect, I've never used a "smart" tv that wasn't horrible.
Is there a map data structure that uses less memory than BTreeMap?
&gt;There is 0 incentive for them to give users more control. There's the hard cash incentive of people wanting control over their phones and spending their money accordingly.
Windows and macOS + Vim here. I work a lot on Windows simply because my day job is .NET development and I like to play games sometimes (admittedly rarely now). Definitely prefer macOS. Used to love Linux, can't stand spending the time to maintain everything now, hah!
This would be less of an issue if `use package::prelude::*;` wasn't so popular. For many of the libraries this is incredibly convenient though so I'm not immediately aware of a good way to avoid it.
It really depends on the use case. Relaxed gives you sequential behaviour *at the one memory location*. This means that if you assign sequential IDs, they'll be unique. But you can't assume anything about other memory locations from that information. Suppose you're assigning sequential IDs to frames of computer animation rendered. Just because 20 IDs have been generated doesn't mean that all 20 appear in the output buffer. Seems obvious, yes? Well, it's also true if you're counting how many frames have been rendered. The counter may say 20 before all 20 exist in the output buffer. This is because memory may appear different from different cores. So if you hide that detail from the user of your library, you're leading them to believe things that aren't true. Assume that a user of safe Rust is *not a full-on programmer* - smart, yes and mathematically literate, but they could be a mathematician or engineer who wants to think about memory ordering even less than you do. *I think it's ok for atomics to be a bit scary.* They're the scariest thing in safe Rust. 
Thanks, I'll check it out! What are the build steps for Windows btw? :)
Maybe it was cut off by the main thread panicking shortly after? (Because it tried to `send(msg).unwrap()` to the other thread but the Receiver was already dropped.) Could that cause the output to get cut off?
This is only one example! I haven't seen it happen much, if at all.
OP is thinking in Haskell. Saying "make the functions impure" instead of encapsulating impurities within functions that appear pure isn't very helpful. 
I'm doing this now, it kinda works (prints the backtrace) but it seems unelegant, how would you do it differently? { use std::panic; use error_chain::Backtrace; panic::set_hook(box |panic_info| { println!("panic occured: {:?}", panic_info.payload().downcast_ref::&lt;String&gt;().unwrap()); println!("{:?}", Backtrace::new()); }); } Would you repackage the backtrace and print it at the `thread.join()` location? How can you modify the panic_info in the hook specified with `set_hook()`? And btw, how does the default panic hook look like exactly?
It's kinda blowing my mind that someone would be more fluent in functional programming than imperative. Not bad, just *kinda cool* that functional programming has matured to the point it can actually displace imperative. Your code is fine. Best in the thread, imo. 
Minor meta nit: could you (next year) make the screenshots of the graphs at a higher resolution? I'm not sure if it's only because I'm reading on a relatively high-dpi screen, but it just [doesn't look that nice (this image shows the difference between native text and text in the screenshot)](http://i.imgur.com/tN0iMtI.png). It would be even more awesome if the graphcs could be svg images or displayed in another vector-graphic way. Potentially semi-interactive graphs.
Hey, so I've worked a little bit more on the project and I've run into a new set of issues I was hoping you could help me out with since I'm not too familiar with rust or multi threaded programs. A part of the emulator that I've just written needs to use an unsafe pointer for the emulator to have any functionality at all, and having this unsafe pointer makes myMod (now called Emulator) unable to implement the Send trait. Since the Emulator can not implement the send trait it can not be used within the middlware closure. This is an issue because the client needs to send data to the emulator and this ~~can only happen~~ is most reasonably done within the middleware closure as far as im aware. The Client will use a different route using http GET to call a function belonging to Emulator called runBlock() that will recompile and execute a small chunk of the rom and return the emulated cpu's state back to the client. My idea to fix this is to have the server and the emulator running in separate threads, which I'm pretty sure will work, but I'm unsure about the details of the implementation. If I spawn a new thread at the beginning of the main function and transfer ownership of the emulator to the new thread I'm not sure that reading to Emulator.Rom will still work. And I'm not sure if I'm allowed to spawn the thread in the middleware closure. I was planning on using thread::sleep and an infinite loop to poll for actions that need to be taken but if a some sort of listen function exists that would be great. communication between middleware closures and the emulators thread would need to happen either via an mspc channel or a mutex. not sure which will be better yet, I think it might depend on how the current issue is handled. If it makes any difference, I'll be the only one connecting to the server for a while so I'm not concerned with multiple concurrent threads mucking up the global mutable data. I just want it to compile. Also if you notice any glaring logical or structural errors let me know, I'm having a bit more trouble with rust than languages I've used in the past. [heres the GitHub repo](https://github.com/dgibb/moes-n64)
If you don't understand Haskell, you can't use someone's knowledge of it to bootstrap their knowledge of Rust. Also, yes, I do prefer to avoid `&amp;mut` in function signatures when it's not necessary. *Tasteful* amounts of mutability in easily understood loops which wrap pure code. I find that paradigm is better than the oo world of many objects with their own fragmented agenda and not as high flying as Haskell monads. But the pattern here is fold - straightforward in functional style, straightforward in imperative. It needs *one* mutable variable (or struct) and a loop. No pointers, no iterator, no `for` statement. 
TL;DR: If you really don't understand if it's correct, that means you should use `SeqCst` everywhere. :) Long answer: If you have two counters, and you increment them one after the other, do you expect this order to hold on other threads as well, at all times? Or do you care if the counter says N, but only N-1 actions were actually already completed? Probably not, but .... maybe someone will, and not even realize it. Basically `Relaxed`, does not guarantee that things will be visible in the right order on other threads, which is an important detail, and can hit your code hard. I'd be very careful using atomics with anything other than `SeqCst`. That's why it is not really worth hiding that detail from the user - instead of making them scratch their head and ask someone for help, it leaves them unaware and wasts their time and money, when their production systems are crashing or their Bitcoins are being stolen. And it's all in the name of very dubious performance optimization.
Same thing for the Rust Discord server. I started it in the beginning of the year, and it's grown pretty decently. We also have a bot that can evaluate Rust code (which is syntax highlighted) and voice channels in case people feel the need to vocally express themselves. Here's the invite link for the ones that are interested: https://discord.me/rust-lang NOTE: We do have a small barrier when you join the server. You have to go into the #welcome channel and click the little Ferris emoji (have I mentioned we get Rust emojis too?). That's because there's been a very high influx of Rust game people that join the server, and we're trying to find out a good strategy to prevent that. So far this one has mostly worked, but I'm not sure if it's gonna stay. At any case, I'd love some new ideas on how I can prevent the Rust game influx of people :|
Bonus round: detect and use [sixels](https://en.wikipedia.org/wiki/Sixel) / [libsixel](https://saitoha.github.io/libsixel/) when available in current terminal, via one of the [crates that work with it](https://crates.io/keywords/sixel)?
**Sixel** Sixel, short for "six pixels", is a bitmap graphics format supported by terminals and printers from DEC. It consists of a pattern six pixels high and one wide, resulting in 64 possible patterns. Each possible pattern is assigned an ASCII character, making the sixels easy to transmit on 7-bit serial links. Sixel was first introduced as a way of sending bitmap graphics to DEC dot matrix printers like the LA50. After being put into "sixel mode" the following data was interpreted to directly control six of the pins in the nine-pin print head. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.27
[rust.vim](https://github.com/rust-lang/rust.vim) is the "official" plugin. Install rustfmt and configure vim to [format on save](https://github.com/rust-lang/rust.vim#formatting-with-rustfmt). (Note that if possible you probably want `cargo install rustfmt-nightly`; regular rustfmt won't be getting all the latest updates for a while.) If you use Syntastic, rust.vim [works with that too](https://github.com/rust-lang/rust.vim#error-checking-with-syntastic). Assuming you have a recent rustc, you should probably use `cargo-check` as the syntastic checker; [`let g:syntastic_rust_checkers = ['cargo']`](https://github.com/rust-lang/rust.vim/issues/130#issuecomment-326643504) [Recent post on RLS with vim](https://www.reddit.com/r/rust/comments/6rx634/async_autocompletion_for_vim8_and_neovim_via_rls/) - it uses the Rust Language Server for auto completion. Not too complete yet, but has some simple things. 
[Hail Hostel!](http://www.hostelseattle.com/)
Can confirm. I've only got a handful of apps, but I had to uninstall Firefox on mobile :/
HashMap? The load factor is over 90% iirc.
You'd just drop the iterator when you're done with it. Actually as long as you keep the iterator alive, you can't do anything else with the file handle.
It's unsafe. Someone could use `std::mem::replace` to extract just the `(&amp;'static mut T, &amp;'static mut U)`, allowing it to persist beyond the original lifetime `'a`. What problem are you trying to solve with this? I can't really recommend an alternative without having more context.
It's not safe since you are saying that the mutable reference lives for the entire length of the program, when your input only is gaurenteed to be valid for a lifetime of `'a` and almost definetly doesn't live for `'static`. Instead of starting in the middle of a rabbit hole though, let's try taking a step back. What exactly are you trying to do?
I just want to point out that to manipulate the HTML, besides select you can use [Kuchiki](https://github.com/kuchiki-rs/kuchiki)
You need a null pointer to terminate the Vec of pointers.
Yeah I wished for the same thing sometimes when starting out, and I worry a bit whenever new magic gets added to the language. However, having a list of all the magic and an example of how each would be written longhand would go a long way for those trying rust out.
Your welcome to contribute to tantivy if you want to. 
I'll be honest, I looked at outstanding issues and didn't find this one.
What's behind the name Jiyunet?
It's very likely for types to collide with modules. Both can contain items (functions, etc) and you're likely to have a module with the same name as the chief type it deals with. `std::rc::Rc` for example. Code in official Rust style might have `Rc::new` and `rc::Weak`
That sounds super interesting. I look forward to your library.
Also curious about this.
*Jiyū* roughly translates to *libre* in Japanese and the guy who came up with the core idea (no accounts, just identities) originally likes niche weeb 4chan clones. Edit: The whole point is for *free* (literally libre) communication and liberation from censorship.
Good point - I've added some more details to specify that.
For someone who just tried Nightly at work for testing, where would I go to report a styling bug? There's an issue with webfonts in our app that only crops up in the nightly, not in stable.
If you need `&amp;'static` anything, something weird is happening. Exception safety?
Oh oops, I should have clarified that I need to use this in a `no_std + alloc` setting. No HashMaps available there, unfortunately =(
I'm kind of surprised. Rust seems well suited for it as it would need low level things like image loading and sound. Rust itself has proven pretty powerful. I really like lambdas, ownership, and cargo (I can run cargo run on someone's project and *not* expect it to fail). Why do you think Rust isn't used more in game development?
Google web fonts? If so, known issue and it's because of a breakage in the library Google is using. The issue only presents in Nightly because it is stricter about rejecting out-of-spec fonts (on purpose, to help Dev's catch issues). As I recall reading (and sorta understood), the library has been fixed and it's just a matter of Google updating their production servers.
Tomato, potato. Storage.
`while` is like C and friends let mut i = 0; let mut f = (0, 1); while i &lt; n { f = (f.1, f.0 + f.1); i += 1; } return f.1; `for` would be my choice. let mut f = (0, 1); for _ in 0..n { f = (f.1, f.0 + f.1); } return f.1; And although Rust isn't (usually) written (entirely) in functional style, it does have fold. (0..n).iter() .fold( (0, 1), |f, _| (f.1, f.0 + f.1) ) .1 Hopefully that can act as a Rosetta Stone for you. 
Ahh, cool. Thanks for the head's up! I'll have to update the ticket at work tomorrow morning.
Thank you for the detailed response. The reason I have the `step()` function exposed at all is for testing purposes: I have a number of unit tests ensuring that the result of single-stepping the game matches up with my expectations. You're right that, ignoring that desire, I could have inlined the function within a loop. My concern around your proposed implementation is that it exposes the ability to misuse the API, namely: * `game_state.score` would be accessible before any score is actually known * A `GameState` value would still be available for calling the `step()` method even after the game has completed Certainly both of these can be dealt with: the former could be addressed by wrapping the `.score` field in an `Option`, and the latter could be documented and `assert!`ed. But (and this is again my Haskeller bias likely) I'd prefer to express the invariants in the type system. An iterator _would_ be a nice touch, except that it wouldn't allow for capturing a final `Score` value (AFAICT at least).
If you *to* need back it up with something better than "some dude on Reddit said it may be the issue", then this may help: https://github.com/google/fonts/issues/738#issuecomment-321902493 Edit: I accidentally a word.
I alluded to it in my [other response](https://www.reddit.com/r/rust/comments/6y968j/idiomatic_approach_for_recursively_applying_a/dmmde3t/), but digging into the iterator approach: there's a slight mismatch between iterator behavior and what I'm trying to achieve through the type system here. Specifically, I want an iterator that essentially says: * I will produce a stream of `GameState` values * I want the stream to be terminated by precisely one `Score` value Since I'm repeatedly outing myself here as a Haskeller, I may as well demonstrate the Haskell idiom (via the conduit library) that allows something like that: `ConduitM () GameState IO Score`. Which says to me that the specific iterator feature that would need to be present is result values. (Which, for the record, I don't think would be generally valuable enough in iterators to make sense as an addition. It's almost an accident that it works in conduit this way. I can provide more details if anyone cares, but this is already pretty tangential.)
That's so much cleaner than my approach, thank you!
Thank you :). Spend close to ten years doing functional programming, it'll come easily to you too. I'm very appreciative of the functional features available in Rust, but also glad that it's forcing me somewhat to relearn some imperative approaches.
Yes. And if writing to stderr fails we assume your computer is on fire and just abort the process.
At least from personal experience it feels snappier but that's just subjective.
I hope this video is appropriate for this subreddit. Unfortunately, a significant portion of this talk is about Ruby.
&gt; The Rust compiler can't predict sources of memory leaks well enough to prevent them completely. It is good at preventing compile-time-assessed accesses out of bounds. The Rust compiler cannot predict out of bounds access. It relies on runtime checking in the general case with LLVM or hand-written unsafe code for optimizations. There's a reason that `mem::forget` is safe and `Vec&lt;T&gt;::get_unchecked` is not. They're not the same thing at all. The exploit mechanism behind a buffer overflow is that you write to a place in memory that eventually changes the instruction pointer to that location. Or possibly some other value you want to change if you're not going for an arbitrary code execution. &gt; Ultimately a memory leak is a potential source of an overflow, just like a stack buffer is a potential source of an overflow. There are many kinds of overflows, some dangerous some not. You cannot just say "an overflow" without saying what kind. Adding two u8s can cause an integer overflow, but that overflow can only cause logic issues with what uses the value if it doesn't expect an overflow. But that integer overflow cannot cause arbitrary code execution, except in specific unsafe code. So what can memory leaks overflow? Well, obviously it can overflow the maximum amount of memory in the system. But memory allocators are designed for that happening and just fail to allocate or crash a thread or the entire program. What else can a memory leak overflow? &gt; a stack buffer There's really no difference in overflowing a stack buffer and overflowing a heap buffer since to the underlying machinery of memory, there is no difference between the stack and the heap. Sure, there are different read/write/execute permissions, but that's it. &gt; You seem to be confusing the idea of buffer overflow with vulnerability: a vulnerability is what allows for the overflow to actually happen. Buffer overflow is a class of bugs which allows for arbitrary code execution and other kinds of vulnerabilities. It's a kind of bug that exposes the entire underlying machinery of memory. Memory leaking is a class of bugs which does not allow for arbitrary code execution, since it doesn't expose the entire underlying machinery of memory. An attacker cannot write or read to leaked memory without first having full access to that machinery, so on its own, leaked memory cannot cause arbitrary code execution. &gt; If you can leak memory at runtime you can overflow the buffer. What buffer? &gt; That's fine. I'm afraid you lack an understanding of the semantics behind this though. I do understand the semantics. I've never seen memory leaks cause computers to become infected with malware, and I know the reason for that.
Vulnerabilities are much more likely to be exploited by attackers than legitimate users. A better solution which doesn't involve vulns to take control of your device are having a "developer mode" or developer-edition products (or fully open source, developer-oriented products). Chromebooks come to mind. The nice thing is products like this can let you do some developer-ish things to modify the device to your liking, then can irreversibly re-enable security features once you're done (e.g. after you've installed Linux/Crouton on a Chromebook, you can re-enable trusted boot)
&gt; Quick poll: are you more interested in writing Widgets or using Qt Quick? Desktop applications are mostly Widgets, but KDE Plasma has a lot of Qt Quick. Mobile applications are Qt Quick. I've been getting into Qt recently and it was a bit confusing at first. - Widgets seems to not get much more development but have alot more options, focused on desktop support(I think it had some drawbacks but can't recall), it gives the best native and range of components? - Qt Quick unlike Widgets uses QML(though both Widgets can instance a QML view or QML can instance Widgets...), QML has a variety of modules like Layout, Window, Dialogs, Platform(native support for things like menus, varies based on platform and falls back to widget if not available?), Extras(platform extra's where certain features are unique-ish to that platform like MacExtras, WinExtras) And then the two Qt Quick Controls modules 1.0(native components but limited selection and tableview has perf issues) and 2.0(unified UI, no native look and feel, depends on theming, more like other UI toolkits such as Feathers, Piston/Conrod?, Web), the latter has much better performance and a better/growing selection of components, it lacks tableview(which has been in dev since at least oct 2016?) and treeview. The two versions can be mixed(and one needs to note which versions of Qt certain components were released in if not depending on latest Qt(Python Conda only offered 5.6 LTS with no plan for 5.9 until 2018, something to do with building Qt on Windows), the API between similar components of QtQuick 1 and 2 differs which has slipped me up before until I used namespacing imports. ListModel could take a generic list(array/slice) of values to populate, but otherwise needed custom class with role names defined. This took me a while to figure out being new to Python and the variety of snippets I found online(some worked some didn't due to API deprecations on either end). Wasn't clear if I could have a model that wasn't collection based bound to several components(like Redux for state management in React/JS apps). I ended up making a QML Item with properties and nesting Item elements with properties to get this effect. Had to be careful with what names I used for properties or id as they could conflict with QML properties(either on the component/element or one that it inherited from), I didn't get any helpful errors about making such a mistake and figured it out by trial/error...(Not sure if Rust helps here, especially if it's QML, a QML linter maybe?). I also had to be careful with updating the properties else they'd lose their bindings iirc. I guess the solution is to write as much as the UI in Rust rather than in QML?(if that'd avoid these issues) Documentation would be important, PySide2 was pretty poor, similar for PyQt, most of the time I was just trying to use Qt's official docs and google snippets of PyQt/PySide code to see how others were using it(won't be an option with Rust). There is a repo for a QML plugin, QSyncable that's trying to provide model sync between backend and QML with diff approach, it only supports C++ presently, if that's easy to integrate with RQBG might be a good demo for syncing data to view. Another thing I had to figure out was initiating the view/app. QGuiApplication prevented the desired styling(at least with QQC1 native look/feel), had to use QApplication, but if your QML had the Application or Window element, I think you had to use QQmlApplicationEngine instead of a QQuickView(or some other kind). Signals/slots wasn't too difficult in binding to a QML signal and sending data from QML to backend to run a function or emit a signal with data from backend to QML. I had a component with a textfield to display a filepath and a button to open a FileDialog that I had one instance of and a ListView that had several based on model data, having each instance call a single FileDialog component and update the correct textfield wasn't straight forward for me. I also wanted notify the backend to do some extra processing/validation. --- I originally wanted native UI and less CPU/RAM overhead, all issues with Electron. QML seemed nice from a glance, Qt seems to be pushing QtQuick Controls 2, but it's component support isn't quite there yet(and KDE doesn't have theme support for it presently like it does QQC1, there seems to be a repo for 2 though providing Breeze, as well as Kirigami). I'm not sure if QQC2 can support using a system theme(at least not existing ones?), until more components are released QQC1 is probably best for now, or Widgets which I've yet to investigate. I'm not sure why but in Qt Creator, both Widgets and QQC1 projects have a very dated design interface, it seems like a really nice tool for defining components or static UIs though quickly.
Yea, but its a cute kid so it's allowable :) It's also great talk too!
Using `unsafe` is not *inherently* bad, but it's an escape hatch that you probably don't need. &gt; Also if you notice any glaring logical or structural errors let me know, I'm having a bit more trouble with rust than languages I've used in the past. Rust is definitely not a lenient language. It holds you to the highest standards. It takes some getting used to. I'll try to read your comment more thoroughly and look at the code and get back to you. Maybe tonight, maybe tomorrow.
I just started learning QML myself, worth noting Qt Quick Controls 1 and 2 are quite different. 1 I don't think will be getting any more love, but provided native look and feel components, 2 has better performance but not as much components at present(no tableview or treeview) and more of a web/unified like UI to be consistent across platforms(KDE has Kirigami for that, a distro called Liri seems to develop Fluid as another option, and then 3 default styles(default,material,UWP)).
http://howdoiquitvim.com/
I would love this. I've really appreciated some of the hard lessons I've learned before the whole ergonomic push started occurring. Yes I want the ergonomics, but I value learning about these low level realities to the language. Demystifying things is very important to me, and I cannot emphasize that enough.
Here's an implementation of `Iterator` that works with `GameState`: struct GameIter(Option&lt;GameState&gt;); impl Iterator for GameIter { type Item = Option&lt;Score&gt;; fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; { match self.0.take() { None =&gt; return None, Some(game_state) =&gt; { match game_state.step() { GameStepped::Cont(game_state) =&gt; { self.0 = Some(game_state); Some(None) } GameStepped::Done(score) =&gt; Some(Some(score)), } } } } } You're right though: it's possible to make the `GameState` an `Iterator` directly. I essentially end up with an infinite stream of values, and the one runtime assertion is that the stream really is infinite. From the implementation of `next()`, we can see that it's guaranteed to be true: impl Iterator for GameState { type Item = Option&lt;Score&gt;; fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; { Some(self.next_helper()) } } With the `next_helper()` signature being: fn next_helper(&amp;mut self) -&gt; Option&lt;Score&gt; and containing almost identical code to the previous `step()` function. It may simply be that I place a higher value on static guarantees than most, but I don't find the benefits of iterators worth it in this case.
I like `extern crate a as b;` , why remove it?
What's the feature set like atm in tantivy? What needs more work? I'm stretched pretty thin unfortunately, I won't be able to be of much help :(
&gt; Go uses the `defer` keyword for that purpose. But what happens if the `defer` block is invoked while leaving a function in some sort of error condition, *and* the defer block itself errors? (Any code, in really any language, that can propagate errors and do arbitrary things that can cause errors has this problem: what happens if two errors occur, and most either drop an error (Java, C#, Python) or abort (C++))
Is there any plan to eventually fix the bits made to match the "sub optimal" Gecko behavior?
fantoccini looks not bad, ill check it out later, thanks
aye, that can happen! It's a worthy investment if you put a bit of time into it every now and then, but the first week or two it's a time sink.
Unless the task is something very trivial or involves GUI, I think rust is a better choice, cause it is easier to debug. Anyway, I was hoping for some nice framework so I don't have to work with Python.
I was curious about the editor data from last year. You guys noted that "we see healthy growth in VSCode adoption at 34.1% (up from last year’s 3.8%)". I couldn't find editor data from last year's survey.
Neat! We always get mocking questions here from time to time so I know what to point them towards if they need mocking! :D
Right now the main feature missing is having proper analyzers. This should come in version 0.5. Other important features it lacks are : *proper facetting *range queries. 
... a dying breed indeed...
i used cargo rustc -- -Z unstable-options --unpretty=normal &gt; file.rs to make all program in one file for understanding modules system maybe it could help you
I had the same impression.
It's surprising to me, as it means that the editor effort is not going towards the most popular one. I guess the lack of an official LSP plugin sealed the deal, but I find that pretty unfortunate. :(
Approximately what percentage of the "possible speedup" is this expected to fulfill? How does this compare in its potential impact to browsing speed when comparing to WebRenderer?
`Box` all the things? Damn, why didn't I think of that when I was learning rust and struggling with the borrow checker? I probably didn't even have it in my mental model given that boxing is automatic in the languages I'm accustomed to.
fortunately, releasing on stable and testing on nightly is a tride and true method of rust development and works quite well. I for one welcome our octopus overlords. 
Can confirm that page reloads are especially fast. I enabled it myself sometime last week and it felt like night and day.
Just my two cents: I've yet to see a situation in Rust where I want aliasing like this. It... just doesn't come up. Now I'm only a few thousand lines into Rust; have you found a situation where it really helps? In Python I've seen a lot of people abuse import aliasing, sometimes aliasing one package as the name of another well-known package. I wouldn't mind a language that pushes people away from such things.
Are there any benefits of using Slack opposed to the IRC channel? (irc://irc.mozilla.org/rust)
https://github.com/jethrogb/rust-core_collections has versions of the HashMap that works in no_std.
Slack might be easier to join for some. Better UX as well (though this is subjective). In general folks like to use their preferred chat app to interact with a community.
Yep, as the old style system gets removed we'll probably also slowly remove compat things _if_ we can determine that sites don't rely on that behavior. (Adding features requires more investigation and work than we wanted to put in in many cases)
&gt; sometimes aliasing one package as the name of another well-known package Is this something like "import simplejson as json"? I actually consider this one of the great features of Python. For example, one could have API-compatible superfastjson package which you use as "import superfastjson as json".
I'm not sure what you mean by that. What possible speedup? 
In general _anything_ to do with `&amp;'static mut` is unsafe. But the lifetime lengthening here is unsafe too, because you can extract the static mut with a `mem::replace`. _constructing_ a `&amp;'static mut T` to use in the `mem::replace` is unsafe, in general, but it is not considered unsafe to _have_ one (so you may have obtained one through a safe API that does once-locking or whatever)
The LSP is a microsoft thing, so VSCode has the best support, so it makes sense to prototype there. But it's designed so that any editor may add support if it wishes.
That's why I found book #2 so much better than #1. #1 started (like most languages) with the stack, progressing to the heap. Rust is unique in that the heap is head and shoulders easier to work with than the stack. Now it _is_ true that performance will suffer, but: 1. You're learning. Your code won't be the height of efficiency. It's fine. 2. It won't be worse than JVM, where _everything_ except for literal types are on the heap. And you can't avoid it.
No Q&amp;A? Sometimes the best part is in the Q&amp;A :(
Similar, yeah. The one I see often is `import astropy.io.fits as pyfits`, which is _okay_ in that you can still support legacy code, but in this particular case the two libraries are _not_ going to be API-compatible going forward. I never thought of using this as a way to replace a common library with a better implementation. That's cool!
You can do this now within your cargo.toml I am on my phone so an example is not so easy but give me a few seconds random = { version = "0.3", crate = "rand" } Or in your case b = { version = "0.3", crate = "a" }
The horror...
You're right; I forgot that was a real design choice that we really made in our operating systems :/
I improved [endian_trait](https://gitlab.com/myrrlyn/endian_trait); a pair of crates for teaching complex data types how to perform endianness conversions on themselves for, say, serializing binary data directly to a file or network socket. It's got a custom derive that can handle all types of struct, and I'm working out how I should handle enums as well. It supports all the basic primitives: signed and unsigned ints, bools (as single-byte objects, the bool implementation is just the identity function), as well as char and float. With nightly and `--feature e128`, it will expand to include the 128-bit integers, and with stable `--feature arrays`, it will cover mutable slices of any `T: Endian`, and arrays of `[T: Endian; N]` where N &amp;le; 256.
Slack is harder to join (email gated), has less privacy (admins can download backups and read your private chats), and has web/electron UI you can't do much customization to.
In Python's case it is also used to support both Python2 and Python3: try: import httplib except ImportError: import http.client as httplib Let us hope Rust never reaches the point where it needs this... 
I've been waiting for something just like this! Woohoo!
I think this is the Q&amp;A right here! /u/rabidferret, what do you think about asparagus on pizza?
Definitely better this way than having to wait more for preliminary results!
How come the mocking closures are not `move` closures?
like (for example using fake numbers) full servo is supposed to speed up firefox by 3x and stylo achieves roughly 1/3 of that. Not that those are the numbers, but how much of the total gains rust can give (to firefox's speed) do you think stylo is delivering now?
I don't think there's a meaningful way to even estimate that. Speed is a matter of what you're measuring. There are way too many hypotheticals involved here. 
It depends. For `mock_safe` the closure is `'static`, so it doesn't have to be `move` if it doesn't capture anything non-`copy`. For `mock_raw` anything goes like with any other closures.
Haha I've seen and used Jethro's `core_*` libraries, but it's a non-trivial amount of work to get them working on the newer nightly Rust. I'd have to use a nightly from april for the latest version of `core_collections` to work. Maybe I can just copy paste the code from collections directly here?
&gt; have you found a situation where it really helps? It's useful if you want to use a top-level module that conflicts one of your external libraries. (This won't be an issue come epoch 2 though.)
It's interesting, why so many users stick to 1.17 version? Or it was the latest one at the moment of the survey?
Easier to debug? Seriously? I thought that in Rust we have only "printf" debugging due to lack of debug visualizers for msvs/gdb debuggers. 
You can already play with it but it's unstable and feels pretty slow. On some benchmarks it does improve the performance dramatically though: [Firefox Nightly](https://streamable.com/bbrwf), [Chrome](https://streamable.com/9n7ds)
Good point, lots of non-extensible 16/32Gb phones around.
Yeeeees...yesss... the hoards of geeks running Replicant on their phone is really making them switch to a more FOSS stack. Samsung can't stand the user presure.
es compatible is so cool！
That would need to be a dynamic analyzer, incurring a run-time cost per object, and per operation that you want to discriminate between moved from/not moved from objects. Same example as before. Thread a: heap allocates object with an `atomic_shared_ptr`, and shares it to threads b,c,d. Thread c: moves the object from the `atomic_shared_ptr` out. There is still an object in the `atomic_shared_ptr`, which has been moved from. You need run-time storage to mark that the object has been moved from since the compiler cannot know this when type-checking the code that runs in threads a, b, and d. You also need storage per method in this case because of virtual dispatch, multiple inheritance, etc. For example, the object might be type-erased, but the type-erasure method might be parametric on moved from/not moved from. So this needs to be caught at run-time for each case. Seriously, what you are asking for is an even slower valgrind which is impractical for way too many applications. Debugging a memory bug that happens in a game after 5 minutes of playing with valgrind, where each frame takes 30 seconds to render, takes 150 hours. And this is today, without catching all cases of memory unsafety, and catching no data-races. We have had memory safety for decades in form of run-time checkers, garbage collectors, etc. Making memory safety zero cost is really hard, and retrofitting it into an unsound language like C++ is not a problem worth solving IMO. The best you can do is have a lot of CI, with many different build types, each running a particular type of instrumentation, and hope for the best. This accompanied with static analysis on fundamental libraries like the STL should at least give new comers that do not implement their own data-structures a reasonably safe language. But if you want to make C++ memory safe for "professionals" a number of annotations that the state-of-the-art static analyzers require you to write will make you wish you were using Rust. It will be improved, but we don't know if we can improve it enough to be widely useful yet. Exploring all paths (from instrumentation, static analysis, and interfacing with safe languages for critical components, adding an optional garbage collector) is a must when you don't know if a particular bet will pan out.
My gf's phone only has 8gb of space, quite a lot of people have phones like that, every additional megabyte counts. 
&gt; That would need to be a dynamic analyzer, what I have in mind here is static analysis, and outlaw re-using a 'moved variable' in a case where it isn't statically determinable. if reusing the values in a datastruture (moving things in and out, i guess) really is desired , then of course we do really need another 'stronger' ```std::destructive::move``` &gt; and retrofitting it into an unsound language like C++ is not a problem worth solving IMO. ... er... so just adding another library function, ```std::destructive::move(x)``` , is harder than re-writing entire applications in another language... **seriously** ???? &gt; a number of annotations that the state-of-the-art static analyzers require you to write will make you wish you were using Rust I don't think the annotations would be any worse. ```&amp;Foo, &amp;'x Foo, &amp;'y Foo``` vs ```Foo&amp;, ref&lt;Foo&gt;, ref&lt;Foo,2&gt;``` (whatever the naming convention ends up being)
Indeed, both are nightshades, and neither belong in a fruit salad.
Plain `&amp;'static` is actually not so weird; e.g. string constants are `&amp;'static str`. But `&amp;'static mut T` is *very* weird.
Having two panics unwind the stack at the same time indeed doesn't sound safe at all (what should even happen?). But I don't see a fundamental reason why it shouldn't be allowed to panic in a `drop()` function triggered by another panic as long as there is a `std::panic::catch_unwind` that catches it before the end of the `drop()`. It might be difficult to implement in the current compiler, but I don't think it should be unsafe, because in C++ it works (and it's safe).
&gt; Debugging a memory bug that happens in a game after 5 minutes of playing with valgrind, where each frame takes 30 seconds to render, takes 150 hours. I've never seen anything so extreme.. and I've dealt with debugging on a streaming system (as I described, figuring out an OS bug),and porting code to SPUs (requiring DMA transfers to access memory, with no memory protection) you have an array of debug options, and you can usually narrow down where the problem occurs using simpler means than 'something that takes 30seconds per frame'. you run the systems individually in test-beds. (I was indeed able to get SPU code working by writing a sort of 'emulator' on the PC/360 build, with extra checks.. again nothing as crazy as valgrind) A higher level point, optimising code for modern CPUs means *simplifying out* the pointer-chasing behaviour - turning problems on their heads, re-ordering etc until you have simple linear traversals of flat arrays as much as possible. You don't want to be doing system allocation in the inner loops (the streaming system issue I mentioned might have been avoidable through more pre-allocation of VB/texture headers .. regardless it was an isolated system, without complex coupling to the rest of the engine or gameplay code). We don't even need true dynamic memory allocation for games (people get lazy these days and use it because it's convenient, but ultimately games can be done with pools for entities, and precompiled level 'object graphs', and texture-streaming buffers which are really separate from the rest of the program) The more complex 'memory manipulation' is done in offline tools ..which TBH don't even need to be in C++, they don't have the same performance requirements .. it's just convenient to be able to share the same datastructures. Again we get lazy and sometimes do preprocessing in the game engine on loading, and eventually have to strip that out and move it to the tools, to reduce loading times. Replicating the 'precompiled blob' approach in Rust is one of the things I found much more unpleasant than C++... it's doable , it just looks uglier (because of the extra-unergonomic choices within the unsafe blocks)
&gt; I've never seen anything so extreme.. Debugging using valgrind? 
&gt; is harder than re-writing entire applications in another language... You are the only one saying "re-write everything in Rust". AFAIK nobody is arguing for this. If you work on legacy code, either introduce Rust incrementally for new components, or stick to C++. If you have to write a new program, and care about memory safety, consider using Rust. &gt; I don't think the annotations would be any worse. Again. Show me. I write these annotations _every day_. Just because you don't think they would be any worse does not mean that they are not way way worse. In fact, they are so much worse that anybody who has actually write them would tell you that, while they could get better, they would need to get infinitely better to achieve the level of Rust. If that is even possible, we are decades away from achieving this.
&gt; probably by 58. Oh boy, I was afraid it would take much longer to reach Android, that's great news!
&gt; either introduce Rust incrementally for new components, the problem with that is you have to drop back to C interfaces. continuing in C++ you can improve incrementally. &gt; If you work on legacy code, it's never so clear cut. You have overlapping use of systems. Throwing *everything* away is a big mistake.
People have been mixing C, C++, and Fortran for 2-3 decades. C FFI works fine. IMO if you cannot make a clear cut between components, and communicate via a C FFI, then you are stuck with C++. If you don't change your architecture to support this, you will be stuck with C++ forever. This is not bad, C++ is a good language.
&gt; Debugging using valgrind? I've never used valgrind personally. Debugging usually happens at a higher level .. most of my debugging has involved writing some sort of visualisation ('debug lines' etc).. verifying states etc. Thats one reason I want something that works more like a 'productive language' embedded right inside my 'systems language'. The debug code doesn't need to be performant. If the use of pointers is complex... for the reasons I've explained, 'you're doing it wrong'. Games shouldn't be doing complex allocations and pointer chasing mid-frame. The real effort goes into re-working things (and writing pre-processor tools) such that you don't need all that. it was quite ironic to watch C++ (vs C-like) people going OTT with abstracted stl-style iterators ('so you can swap arrays and link lists') round about the same time the platforms lost the ability to handle link lists efficiently (we used to use them more back in the PS1 days, but as each generation passed they got slower). 
&gt; I've never used valgrind personally. &gt; Games shouldn't be doing complex allocations and pointer chasing mid-frame. It can just be an use of uninitialized value. But it doesn't matter. If you never need valgrind, ASan, MSan, to catch bugs in the kind of code you write, you probably would not benefit from memory safety much. This does not mean that C++ can be retrofitted to provide memory safety, nor that Rust should be made more like C++. If you write C++ for a living, I would seriously consider investing some time into learning those tools (e.g. let them run over the code you write, understand their output, understand how to set them up, etc.). They have their problems, but are very very useful. 
regarding allocations in games.. they need to run at a consistent frame-rate, which means a consistent load, which means consistent buffer sizes. if you're doing anything which does an allocation which could fail mid-frame... 'you're doing it wrong'. you make a load estimate upfront, possibly with throttling. e.g. if you are streaming textures.. you're prioritising which textures to fill a known size texture buffer with.. with a 'fail gracefully' fallback of having all the textures in lo-res preloaded (there was hardware support for this, i.e. lower MIPlevels stored at a different location), not allocating willy-nilly 'with an exception or panic on OOM'
You could leak a `Box` into a `&amp;'static mut`.
&gt; regarding allocations in games.. they need to run at a consistent frame-rate, which means a consistent load, which means consistent buffer sizes. Lots of applications that only allocate memory at start up have memory issues, like reading an uninitialized value from some buffer that wasn't initialized, dangling references to stack frames, offset arithmetic, leaks/unreachable memory. Valgrind is not slow because somebody allocates memory. It is slow because it needs to track all access to all memory all the time. Memory allocation happens very rarely compared to memory access. Anyhow, I am not a valgrind tutorial. If you care, inform yourself. There is plenty material about this on the internet. 
&gt; Lots of applications that only allocate memory at start up have memory issues, like reading an uninitialized value from some buffer that wasn't initialized. Or somebody screwing up offset arithmetic. yup and I've never seen these be particularly hard to get right. &gt; Anyhow, valgrind is not slow because somebody allocates memory. It is slow because it needs to track all access to all memory all the time. I get that, it's some sort of instrumented virtual machine isn't it. The thing with the level data is writing visualizers/integrity checkers. You need them *anyway*, because you want to verify that your level data hasn't been corrupted by whatever sorting etc you did to build the things. you want to check that the bounding boxes really do contain their contents, and there isn't some funny bug slowing it down through mis-calculation. you need to verify that the changes actually sped things up. Compared to doing all that, the plain memory bugs are **nothing**
&gt; Compared to doing all that, the plain memory bugs are nothing Until you want to ship your project, and submit it to the AppStore, Sony, Microsoft, or whoever owns the platform where you are shipping, and they let your game run for 2 days in a row, and after 1 day and a half of continuous testing the game they get a segfault, and reject your game. Maybe this only happens with the version of the compiler that they are using, because it is newer than yours, and makes some optimization that yours don't. And you cannot even reproduce this.
How can I build a binary which works on multiple machines? I compiled my project on a Macbook with `cargo build --release` and it runs fine there, but after copying the `main` file from `target/release` to another Macbook the binary won't run (output is just `[1] 95745 killed bin/main`. After re-compiling on the second machine, the binary works properly again. Both Macbooks are running the same version of OSX, and using the latest Rust version `1.20`. Am I missing something in the build step? The only external crates I'm using are `serde`/`serde_json`/`serde_derive`, everything else is in `std`.
I'm very eager to test this once it's available. I'm already using Firefox nightly as my main browser on all of my devices, so just a matter of time...
Ok, chrome has improved that much? Been a while since I last used it.
&gt; and they let your game run for 2 days in a row, and after 1 day and a half of continuous testing the game they get a segfault, and reject your game LOL, we went through this process. we had an in-house test department (actual playability testing being part of the design process) Like I say, reworking things for performance was far more pressure. You have a phase at the end where everything is running at 20-30fps , and you want it at 60fps, and you have to go back and ruthlessly trim everything. (and the big irony r.e. c++ abstracted datastructures was that fast code as I keep saying can't use complex memory traversal.. the amount of pointer chasing has to be minimised.) &gt; Maybe this only happens with the version of the compiler that they are using WTF? we submit binaries.. disk images.., they do build it aswell, but we fundamentally build what will actually ship. Like I say, I know of companies that wrote a custom language to build their game in. If rusts style of memory safety was THE biggest issue, we'd have invented Rust already. I'm sure I've already mentioned jonathan blow's videos to you . Watch it and listen to what he says (and why he rejected rust). He presents the ideas better than I can in these threads.
~~Depends on the target it was compiled for. Macbooks can have different CPUs, so even on the same version of OSX there may be differences. Rust (via LLVM) will usually try to choose the most optimized version for your target platform.~~ I stand corrected. Curious.
&gt; I'm sure I've already mentioned jonathan blow's videos to you . Watch it and listen to what he says (and why he rejected rust). As opposed to you, who has not learned anything about C++ static and dynamic analysis since this discussion started, I had already watched jai's videos. Jonathan Blow doesn't care all the way about memory safety and wants to have his own thing where he can experiment with features like SoA/AoS transformations that he cares about. I think that's a fair stand. 
&gt; Jonathan Blow doesn't care all the way about memory safety but he does care about writing games that *work*. I agree with his stance, that the front-loading of all the issues isn't necessarily a win; the way I explain it is that getting things working requires other kinds of debugging (involving experimental code). he has some ideas on building other forms of memory debugging assists (guards that the debugger will be able to see) Overall his views are more extreme than mine, if anything. I like both C++ and Rust more than he does.
Also just because you call finalize that doesn't mean that some other code cannot panic before finalize is called, in which case this destructor bomb pattern will abort the process :(
To me, the interesting thing about this is the nondeterminism. Although that's not quite the right word for it. In Rust, mutability is idiomatic, and manageable because it can't be combined with aliasing. This is Rust's secret sauce, i think. The natural way to make time flow is: loop { game.step(); // mutates game } Another idiomatic thing in Rust is to use methods that consume self (or other arguments) to encode temporal order in types. This is Rust's, er, other secret sauce. The thousand island to the garlic mayonnaise of mutability^aliasing. The natural way to make time move onwards is: let score = game.score(); // consumes game But what's not immediately clear to me is how you combine these two things in a safe way. You can do this: loop { let finished = game.step(); if finished { return game.score(); } } But there is nothing statically connecting the result of step() to the calling of score(). You could easily write: loop { let finished = game.step(); if !finished { return game.score(); } } And the compiler would be powerless to stop you. You can fix this by sprinkling on some affine types: // these should be in a module, with private zero-sized fields, so they can't be forged struct Coin {} // you need these to keep playing struct Ticket {} // you need one of these to claim your prize at the end enum State { InPlay(Coin), Over(Ticket), } impl Game { pub fn step(&amp;mut self, coin: Coin) -&gt; State { // etc } pub fn score(self, ticket: Ticket) -&gt; Score { // etc } } let mut game = Game {}; let mut coin = Coin {}; loop { let state = game.step(coin); match state { State::InPlay(refund) =&gt; { coin = refund; } State::Over(ticket) =&gt; { return game.score(ticket); } } } } But this is completely grim. What you really want is for .step() to *either* mutate the game and return nothing, *or* consume the game and return a state. But there's no way (AFAIK) to write a type which does that. The consumption or not is fixed at the level of the type. Hence why you have to indirect via an enum of some sort. You can think of /u/Diggsey's [solution](https://www.reddit.com/r/rust/comments/6y968j/idiomatic_approach_for_recursively_applying_a/dmlza8z/) as doing the same thing, but melding the state of the game and the score into the returned tokens. The code you end up with is massively simpler, but it loses that (IMHO nice) idea that the step method just mutates its receiver. 
Perhaps MSI-installs and people forgetting to keep 'em up to date. Or people working on such large projects that the risk of breaking them by upgrading `rustc` feels too high.
&gt; his own thing where he can experiment with features the other major thing to get from his talk is the number one concern being *maleability* ... the ability to evolve code between different use-cases. The rust community sees the world purely in memory safety terms, he explains how it's actually other issues that cause our frustration with C++.. it's irritating syntax issues, and the lack of reflection (just needed at compile time). I actually *like* Rusts syntax (I've seen other C++ people reject rust because they don't like ```fn```..) , but am put off by the extra verbosity within unsafe blocks (beyond merely marking unsafe, which I'm perfectly happy with), and the need to put functions into the trait namespace before you can do *anything* polymorphic (ending up with single-function traits, most notably the operators.. micromanaging traits gets annoying)
Presumably the logo depicts a [Pacific northwest tree octopus](http://zapatopi.net/treeoctopus/)?
Same in JavaScript, it's really nice. import MyChangedName from './example' or import { example as MyChangedName } from './example'
I have a question. what is the difference between. let s1 = String::from("Hello"); let s2 = s.clone(); And let s1 = String::from("Hello World"); let s2 = &amp;s1;
In your first example, `s2` is a `String` clone with the contents of `s1` ("Hello", in this case). In the second example, `s2` is a borrowed String, which will usually be of type `&amp;str` (though it may also be `&amp;String` if you e.g. call a function that expects one with `s2` as parameter) with a (in this case anonymous) lifetime fully contained in that of `s1`.
&gt; For rustup ask: would you like to be notified of the annual survey (1 email/year)? (Y)es, (N)o, (A)lready registered, (I) participate but don't like email reminders. That would be super invasive. I think at most they could get away with an ad for the survey.
That's surprising. On my computer the result on the CSS maze is quite the opposite
Is that with or without WebRender enabled? These are all the WebRender settings I have enabled: * gfx.webrender.enabled -&gt; true * gfx.webrender.layers-free -&gt; true * gfx.webrender.blob-images -&gt; true * gfx.webrend**est**.enabled -&gt; true Additionally I also have * layers.omtp.enabled -&gt; true * layout.css.servo.enabled -&gt; true (I believe this is the default since a few hours). I've only tested it on Windows, so perhaps the results are a bit different in case you're on a different OS.
Wouldn't Android benefit the most from Stylo since mobiles usually get more cores instead of faster cores? There are budget phones with 8 cores already.
Thank you for mentioning perlin. It is probably the only search-engine library in rust already in production. [CurrySearch](https://www.curry-software.com/en/curry_search/) is powered by perlin. But it is probably also the worst documented library. I hope I will find the time soon to enhance the documentation and build some usage examples.
C++ guy here. I believe, you are both right, but probably missing a point or two. As we all know, C++ is huge, no, HUGE. Not only in terms of it's syntax/feature richness or overwhelming legacy (not to mention all those footguns), but in terms of mental models that are used when dealing with C++ code. Two people with different technical background and experience level may think and use C++ very differently. In order to sell them Rust, we need to address very different aspects. Newbie programmer may be excited by expressiveness of Rust and it's ecosystem friendliness, whereas more experienced one, who used to think in terms of contracts and guarantees, would be more interested in Rust's solid ownership semantics, guaranteed and static safety, zero cost abstractions, and so on. Technically, both of them came from C++ world, but with different understanding of things, so basically they should be taught Rust in a completely different ways.
The only point of the `mut` variant is write-access, but making use of this requires statically tracking a single `mut` reference and its borrows; in this case you gained nothing over the `Box&lt;T&gt;` version. Leaking a `Box&lt;T&gt;` to `&amp;'static T` on the other hand... nasty trick, but could solve issues like converting various error types to `Result&lt;T, &amp;'static str&gt;` (with a small memory leak should the program resume, of course). Interesting...
Isn't using anything below i32 an efficiency hit on modern cores?
Oh sorry about that, I have forgotten to commit the windows example build script. I will commit it later today. It should be as simple as ```cargo build --release``` plus copy the CEF dll &amp; resources to the correct relative location (the same dir as the executable). It should be very similar to the linux example: https://github.com/anderejd/cef-rs/blob/master/examples/hello_cef/linux_build.sh EDIT: Beware that the bindings are in very rough shape and are missing at least one critical piece, implementation for cef_life_span_handler is missing and is needed for a correct shutdown. I plan to address that as the next piece of the puzzle but I'm not sure when I will have the time for that. In case you want to dig in, here is the official header file with documentation: https://bitbucket.org/chromiumembedded/cef/src/22d2fc50d1fba0d55cad6d43df95854a9dc34e0b/include/capi/cef_life_span_handler_capi.h?at=master&amp;fileviewer=file-view-default EDIT 2: Windows build scripts for the example crate pushed to github: https://github.com/anderejd/cef-rs/tree/master/examples/hello_cef
I'm not entirely sure what the implications are exactly. But it will depend on what you do with the data ;) If it enables you to pack your data more densely, the reduction in cache misses could overcome any efficiency hit from bitmasking or whatever needs to happen on modern cpus
&gt; The rust community sees the world purely in memory safety terms, Rust is one of the easiest-to-refactor PL's that I've ever used. I've used many, but refactoring in Rust is changing an API, fixing compiler errors, and that's it. I don't even need to run tests to know that the refactor did not introduce any problems. It is also orders of magnitude better code-reuse wise than C++, proof is how hierarchical the ecosystem is. A high-level Rust crate has &gt;100 dependencies, anything like this in C++ would be a maintenance nightmare but in Rust it just works. About the other issues you mention... extra verbosity in unsafe functions is at most a one time cost, and if `emacs`, which probably has the worst Rust support of all editors out there, has already way better auto-complete functionality than most C++ IDEs, I really don't see how this can be an issue to anybody but those writing code in notepad or nano. One doesn't need to put a function in a trait to make it polymorphic, but using single traits has some benefits over plain generic functions, and writing one-off traits is idiomatic Rust. This one time cost is almost zero, I just have a snippet for one off traits and writing one takes zero time. Every time I get a C++ substitution failure error with a 10 possible candidates overload set I think "in Rust this error wouldn't have happened or be a one liner", and this is exactly because of Traits. 
&gt; but he does care about writing games that work. I really don't see your point. Who said anything about writing whole games in Rust? A game is a huge product, and no single PL is suited to all of it. If you are writing your games in 100% C++ or Rust you are doing it wrong. But yeah, if I have to use a rendering engine or a game engine as a black box, I'd rather use one without memory issues. "You will never get this engine to segfault" would be a selling point for me.
&gt; extra verbosity if operators don't matter, why don't we use ```x.get(i)```, ```a.add(b)``` etc in safe code. C expresses unsafe code better, and we've got that embedded in C++. (I would like to see the safe/unsafety decoupled from the operators, perhaps a way of re-binding them, just like any other 'use') there's a load of little irritations like *mut and *const (instead of keeping one of those, which doesn't matter) as the default. &gt; I don't even need to run tests to know that the refactor did not introduce any problems. logic &amp; behaviour, remember performance tuning too. any change can have consequences. I do like most of Rusts syntax choices, but the bit I don't like is building the trait name into the function namespaces. Micromanaging trait bounds gets seriously annoying. This is one of the main reasons (possibly the biggest) that I haven't adopted Rust as my 'main language'. &gt; About the other issues you mention... extra verbosity in unsafe functions is at most a one time cost Someone somewhere still has to think about the unsafe code, and this actually interests me ... all the low level tricks, like the different ways of doing a small vector, where you were content to just use an ```enum``` . &gt; writing one takes zero time. nothing takes zero time. I want the **function** to remain my most fundamental unit, not the *trait*. If I wait for C++ concepts, I'll get exactly what I want: bounds that are optional. Until then, existing C++ remains the model I prefer. (I'd rather have *no* traits/bounds, rather than having them *compulsory*) Polymorphism is sometimes done from a concrete example, as such I've never found it unmanageable (e.g. writing something with concrete types, then observing it's a pattern you're about to replicate somewhere else.. factor a bit out, stick template in front. etc) They give this 'cowboy::draw' example in the docs .. there I would say "use better function names", use the web to establish consensus on what the best names are. Also overloading would have distinguished them e.g. rendering functions usually take some sort of rendering context object , and one would be const, the other mutable. &gt; Every time I get a C++ substitution failure error with a 10 possible candidates overload set yet in Rust I find the level of generic code I can easily do in C++ is much harder. The trait bounds are bigger than the actual functions.
I am going through this book : ,, https://doc.rust-lang.org/book/second-edition/ch01-00-introduction.html "
&gt; if operators don't matter, why don't we use x.get(i), a.add(b) etc in safe code. C expresses unsafe code better, and we've got that embedded in C++. (I would like to see the safe/unsafety decoupled from the operators, perhaps a way of re-binding them, just like any other 'use') there's a load of little irritations like *mut and *const (instead of keeping one of those, which doesn't matter) as the default. Write another RFC about how you want function overloading in Rust, and let people explain to you, for the million-th time, how function overloading is the root of all horrible C++ error messages. &gt; any change can have consequences. If you are refactoring and changing behavior at the same time, you are doing it wrong. &gt; Micromanaging trait bounds gets seriously annoying That's what aliases are for. &gt; Someone somewhere still has to think about the unsafe code, and this actually interests me ... all the low level tricks, like the different ways of doing a small vector, where you were content to just use an enum . I really don't know what this sentence is supposed to mean. If you write more `unsafe` code than you actually read, and you don't have an editor with auto-completion, you are again doing it wrong, and it's your own fault. &gt; nothing takes zero time. It takes one key press on my pretty crappy editor. Again, if this is a problem for you, you are doing it wrong. &gt; I want the function to be my most fundamental unit, not the trait Rust functions are orthogonal to Rust traits. &gt; The trait bounds are bigger than the actual functions. That's what aliases are for. &gt; Until then C++ remains the model I prefer So why are you whining here? Either write RFCs and do something about it, or stop being frustrated because you'd like to use Rust but cannot? What do you expect from whining here about it? That people that do not share your concerns and ideas go out of their way to change the language for you? Function overloading + untyped generics are the root of all horrible C++ error messages. You say it is worth it, I say it isn't, and most Rust users agree that it isn't. Either write an RFC or fork the language and prove everybody wrong, or if this is such a big deal to you, then don't use Rust.
Pardon my ignorance, but what prevents the compiler from further in-lining format strings an the like? I would think that if all the strings are known at compile time, as in the robot formatting example it should be able to. 
There's a summary/roadmap [here](http://smallcultfollowing.com/babysteps/blog/2017/02/12/compiler-design-sprint-summary/). Cretonne in particular should help compiles a lot when/if it's implemented.
&gt; but he does care about writing games that work. &gt;I really don't see your point. safety.. eliminating memory bugs. he does (as do I) want to write games 'that don't segfault'. It's just the demands of that, combined with everything else, yield a different optimal blend. &gt; A game is a huge product, and no single PL is suited to all of it. The vast majority (for the types of game that interest me) is written in C++, and beyond that I remain interested in high performance code in other contexts. His goal is a language thats better than C++ over a wider range of cases (i.e. less need to drop into a scripting language/C#) **I do believe you can build one language for every part, it's just not many people are trying.** **there is a historical example**, (not sure if I mentioned it in this thread), the custom [GOAL language](https://en.wikipedia.org/wiki/Game_Oriented_Assembly_Lisp) used in the PS2 title 'jak &amp; daxtar'. It was designed to handle low level DSP-like 'vector-unit' code (a bit like shaders but lower level) , general purpose engine code, *and* gameplay scripting (including hot-swapping for on-the-fly tweaking).. all in one environment. If *swift* gets move semantics etc, it will be there. (it's started out prioritising 'application code', with low level being secondary or absent.. I did see some 'unsafe' stuff but it looked too verbose). But it remains to be seen what it will look like there. I think the interfacing cost matters. I want to be able to migrate code between use cases ( I mention how you might hack too much setup code in during experimentation, but then you want to migrate that to tools. but I already said tools don't need the same performance, they shift into the 'productivity' use case. Conversely, you might have parts that start as 'gameplay scripts' but which hit performance much more than you'd expect (I've seen this so much), so you need to drop down and optimise fully. Jonathan blows language is built for this exact use case from the ground up, so it's got a higher chance. What frustrates me is that Rust could also be extended/modified to do what I want (it's syntax is a better starting point). It was closer with the sigils. The option for whole program inference (rather than dynamic typing) would be perfect.. together with the sigils that would enable a subset that could eliminate the scripting language use case, IMO. If the Rust team cared about this use case, all the effort he's putting (he does have collaborators) in could have gone into features here. &gt; "You will never get this engine to segfault" would be a selling point for me. you'd still have other problems. segfaults are not the biggest.
**Game Oriented Assembly Lisp** Game Oriented Assembly Lisp (or GOAL) is a video game programming language developed by Andy Gavin and the Jak and Daxter team at Naughty Dog. It was written using Allegro Common Lisp and used in the development of the entire Jak and Daxter series of games. Syntactically GOAL resembles Scheme, though with many idiosyncratic features such as classes, inheritance, and virtual functions. GOAL encourages an imperative programming style: programs tend to consist of a sequence of events to be executed rather than the functional programming style of functions to be evaluated recursively. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.27
&gt; What frustrates me is that Rust could also be extended/modified to do what I want. It was closer with the sigils. Then write an RFC? What's the point of writing in reddit about it? Nobody can hear you screem when you are 100's comments deep in a reddit hole.
Thanks. As someone not in the know, does rustc already do something like gcc's make -j n
You're looking for /r/playrust
Cargo compiles each crate in parallel, and rustc can also do parallel codegen, but I believe it's only enabled for debug builds since generated code can be slower.
Sounds delicious!
Interesting, thanks for that. I'll search around for some more information on that.
Basically it would have to be a very special cased optimization which placed that combined string in the data section of your binary. Since the string still has to go on the heap it would have basically no benefit. If the compiler is able to pre-allocate a string of the right size (I don't know if it does), then that is enough
The tamest of my suggestions, in RFC form, gets 2upvotes, 9downvotes. [https://github.com/rust-lang/rfcs/pull/2063] actually as I type this i note it does appear to have got some interest form the 'ergonomics initiative' chap, maybe all is not lost .. but you can see the controversy in the comments. Elsewhere, I've seen people explicitely tell me "prototyping and final product should be separate" .. which is just a big philosophical shift, and very different to the workflows I've been involved with.
&gt; Corporations will to everything in their power to keep the ability to write crappy software for as little money as possible. They will fail once people refuse to buy such software. &gt; But you can't really compete with the large corporation with billions in their back pocket. The Purism phone is nice, but it's maybe "worth" $200 based on the specs, not $600. The higher price is cost for better product, don't you think? . We need the government to stop the corporations from selling our data to the highest bidder And the very same government spies on people more than anyone else on Earth. Without gov NSA-level espionage would't happen. &gt; we need the corporations to stop the government We certainly don't need corporations to do that. Encryption software is often written by enthusiasts - so-called "cypherpunks". Actually, it was probably those people who did largest part of work on cryptography.
&gt; for the million-th time, how function overloading is the root of all horrible C++ error messages. guess what. I tried rust, *and I found that Rusts trait bounds are more annoying than C++ error messages*, **that's why I went back to C++** I want it optional. In the case where the error message is the bigger problem, great, i'll use the bound. Otherwise (most of my use cases), I'll stick with 'ad-hoc'.
&gt; Rust functions are orthogonal to Rust traits. in C++ you have overloaded *functions*; in rust, you need *traits* to achieve this. so they overlap , in the case I'm talking about.
&gt; fork the language and prove everybody wrong it isn't objective, it is preference. A fork would be unmanageable. the irony is, C++ is better for me than Rust in it's default form, but it would take fewer modifications to Rust to make it what I call "perfect", compared to C++. I like 95% of whats been done in rust, and 5% of choices ruin it. There could be options disabled by default ,which wouldn't affect the experience of people who like rust in it's current form
I've used Chromium lately (v59) and it "feels" slower than Nightly, but that's just my impression.
Does rust need a mocking framework of this style? I've been satisfied simply parameterizing types over traits and using fakes for those types. https://www.youtube.com/watch?v=Xu5EhKVZdV8 explains this alternate philosophy of testing pretty well.
Yes this is something we both omitted: There is considerable variance in those new to learning Rust. However, this does not preclude significant overlap in the concepts people will have to learn. Indeed, I suspect the prime difficulty when learning Rust is to un-learn the concepts people bring from other languages that don't map well (or at all) to Rust.
Yup, most nightly users already had it by default. I certainly did on my three installations. From the source: &gt;Nightly users should not notice much change, since there was already an active experiment that enabled Stylo for most of the Nightly population. 
But you can't use {attributes (https://doc.rust-lang.org/stable/book/first-edition/attributes.html), macros, compiler directives} in cargo.toml.
"Extern crate core as std" helps with optionally no std code.
So you put some work into an RFC, but haven't managed to improve it based on feedback? Like, not even put it in a mergeable state? If instead of discussing on reddit with me you would have been working on that RFC Rust might already be a better language for you if that's what you want.
Clippy &amp; Rust contributor here. An incremental rust build+test takes about 15 minutes on my low-powered Chromebook. Clippy builds in 2-3 minutes (including all tests). One thing that (at least with clippy) makes up for the compile times is the strong compile time checks. Thus the coding cycle is not like in C++ '*code – build – read errors* ×N – *code – build – run – debug errors* (da segno all fine), more like *code – check×N – build – run*.
webrendest was not enabled, but enabling it did not changed result. On my office computer on Windows 7 : - Chromium : 4s - WebRender Nighly : 11s - Normal Nightly : 17s
Similar to what people do with Lua - set a debug hook to count cycles. But there's _always_ going to be an operation with unbounded time so the pros in fact always go back to `ulimit` - not sure what the Windows equivalent is.
&gt; Has anyone who's used it in the field got any numbers with large-ish projects? Trying to gauge whether they order of slowness is along 'it's slow but that's okay' or 'it's slow, I really can't see using this for development.' I have a few medium to large projects, and build times are currently around 27 minutes for the longest running (edit: [the culprit](https://github.com/frankmcsherry/differential-dataflow/tree/master/tpchlike)). Very little of this time is Rust itself; it is mostly that Rust produces an epic amount (gigabytes) of not-much-reduced material for LLVM to work with. The time gets slightly better with tasteful boxing of types to prevent monomorphisation, and worse each time one rewrites a part more generically. This compares with a few seconds for C# to get a similar codebase off of the ground (though obviously, doing lots less work). I would love more attention be paid to compile times, as the one time I did get a Rust dev's ear they were gobsmacked at the size of the data being passed to LLVM. If nothing else, "guidelines for minimizing LLVM work" would be a topic I would love to read about (e.g. "where best to break monomorphisation in your design", "how painful are closures", etc).
That's cute! But the field `tags: String[]` does not look very rustic - should be `Vec&lt;String&gt;` surely?
Strange. I assume the other settings didn't make a difference either? You could report a bug because I find it strange that WebRender is so slow for you.
Well, that's annoying. Is there a way I can tell Rust to create a compatible version so I can create a distributable binary?
You mean `#[macro_use]`? If so, [see RFC 1561](https://github.com/rust-lang/rfcs/blob/master/text/1561-macro-naming.md) which allows macros to be `use`d just like other types.
I had a fairly negative post up above about compile times, but it is worth re-iterating the above (positive) point: I spend very little time waiting for a build, then running my code to see what is wrong. Much correctness debugging happens as part of `cargo check`, which is relatively fast (a few seconds). On the other hand, I do spend quite a lot of time waiting for a build, then running my code to measure performance changes. On the balance, I prefer the long waits with few errors and possible performance improvements to long waits to then debug (me writing C++) or short waits and mediocre performance (C#).
if people wanted that feature, they wouldn't be fussing over the RFC. it explains the idea. the rust community in the whole is full of 'explicit-ists' . it's a personal preference. there's so many issues like this elsewhere. Look how much argument there was over default parameters aswell.
Beginner here, what do you mean by calling qt written in c++? How it is any different from calling c++? It can only call c++ with std + qt libraries but without any other additional library?
Thanks, it is really interesting to know. I started digging into it :)
Thanks for reporting. It has been fixed now.
&gt; I did get a Rust dev's ear they were gobsmacked at the size of the data being passed to LLVM. Yeah it looks a bit sub-optimal at the moment. E.g looking at the llvm output with no optimizations, a function that takes something by value, allocates space it's argument on the stack, makes space for a second one, and copies the first to the second one. (In addition to the stack copy that is created at the caller.) This seems rather... sub-optimal. https://play.rust-lang.org/?gist=c97d83efa7dc125a421fcd0768f607ea&amp;version=stable Of course this would normally (hopefully) be optimized out later by llvm, but it's still work for the compiler. Another issue is with generics, as e.g a function taking a generic argument obviously can't be pre-compiled, so it has to be optimized and emitted by the compiler when it's used. Hopefully work in MIR optimizations can relieve some of this so the compiler doesn't have to spend time doing stuff like inlining `ptr::null()` into [`ptr::is_null()`](https://github.com/rust-lang/rust/blob/f83d20eff734310a0381b7c71e0192988b6b0847/src/libcore/ptr.rs#L488)..
Only once I had learned them in a GC'd language like Scala or Python first.
&gt;Rust doesn't save you from dealing with any of the details, it just makes sure you can't accidentally forget about them. Put that on a bumper sticker. 
&gt; Very little of this time is Rust itself; it is mostly that Rust produces an epic amount (gigabytes) of not-much-reduced material for LLVM to work with. ... The time gets slightly better with tasteful boxing of types to prevent monomorphisation ... Maybe the LLVM folks should consider supporting some sort of type-generic code in the language-agnostic IR, so that LLVM optimizations can be done with less duplicated work? How does the C++ compiler (`clang`) behave in this respect (seeing as C++ also supports generic code w/ monomorphization)?
Yes, it was the latest one at the time the survey was taken.
&gt; not going towards the most popular one. Three things: 1. That makes sense; you don't want to work on what's already used, you want to help out the ones that aren't. 2. I love vim, and so do many others, but saying "To write Rust, you should use vim" cuts Rust off from many, many people. 3. The whole idea of http://langserver.org/ is that you do the work once, and it works for many editors. There's neovim plugins on that page too...
I'm not 100% sure I understand what you're asking for, but does this seem like it would work? https://play.rust-lang.org/?gist=c7c3483669055382f8d699fc592c2be8&amp;version=stable
Wasn't at a computer before, I meant things like: #[cfg(target_os = "linux")] extern crate syscall; #[cfg(target_os = "linux")] #[path = "sys/linux.rs"] mod sys;
I follow that practice as well. It looks like this (haven't tried it yet) doesn't require a trait, and removes a lot of boiler plate. Also should be simpler without requiring deep call graphs to genericize all the way down. (If I read the intro correctly)
That's the point, you have to write a function for every variant.
[Platform Specific Dependencies](http://doc.crates.io/specifying-dependencies.html#platform-specific-dependencies). You can still do conditional modules and changing their paths.
The only problem I see here is that you need some way to tell the function for which flag you're looking (You called it "option". I said "flag" because option is kind of taken as a name for optional types). Everything else is easily solved by iterating over your vector, a match statement and returning Option&lt;AdditionalData&gt;. I know that my comment probably isn't helpful, I'm actually just writing this down to wrap my head around your problem. :)
Very nice! I was waiting for someone to finally take a stab at a proper mocking framework in Rust. Doing it by hand via a proxy trait + the public-type-in-private-module hack gets verbose and tedious really fast. One question though. Is it possible to somehow to write still have stable builds of production code while only compile tests and run them with nightly?
Even though it's technically incorrect, it's common to colloquially refer to internal non-volatile storage as a "disk" or "hard disk". You presumably understood that he was referring to the flash storage and not a mechanical spinning hard disk, and so did probably everyone else who read it.
I understand what you're saying. Perhaps this is some Rust naivete on my part, but: what's the difference between what you're looking for (`&amp;mut` functions that sometimes invalidate the object) versus the enum-based implementation, which consumes objects and then optionally returns them again? Is there a performance pitfall involved, or is it just about the API?
I think you could use a macro to generate all the functions (which would then be the cleanest solution, I think).
By default, it won't do what /u/llogiq said; that's a flag (`-march=native`). I'm not sure what's going on here; given they're running the same version of OS X...
We don't generally do Q&amp;A, no. Q&amp;A *can* be really good, but it can also be very, very bad. We have breaks in between each talk, and most speakers answer questions to people that come up and ask during them; this lets the people who care about Q&amp;A do it, while those who don't don't have to, and tends to eliminate the grandstand-y questions, since there's not a huge audience.
&gt; It outputs six. But I thought the loop should end once i reaches a value that is greater or equal to 5 Huh? This is same in all other languages! You are programming beginner in general, right? &gt; but let's say I want to do this for(let i = 0; i&lt;x; i+=2) How would I do it. Simple but inefficient approach: for i in (0..(x / 2)).map(|x| x * 2) { /*your code here*/ } This simply counts up to x / 2 and multiplies values by 2.
Pretty much what we did, but "Option::One" is not a valid indent, so you have to send "One", which is not very clear, while the macro adds the "Option::". It's semantics, but it's really annoying.
And how do you do that from within a macro?
Yep, but the flag id already exists inside rust. Every enum has a hidden determinant. If we could get a determinant from an enum type and a variant name, we could send it as an argument and use it for comparisons.
What's in your target JSON file, and where is it located?
Disclaimer: I didn't watch the video (yet), so I might not have understood the topic well. &gt; If the compiler is able to pre-allocate a string of the right size (I don't know if it does), then that is enough If you talk about something like `"foo".to_owned()`, then yes, the string will be pre-allocated. If you're talking about `format!("Foo {}", "bar")`, then not. `format!` is bulky and that's why I wrote [`fast_fmt`](https://crates.io/crates/fast_fmt) crate.
Three things: - the type of s2. In the first case it is `String` like s1, in the second it is `&amp;String` which can be automatically converted to `&amp;str` but not to `&amp;mut String`. - the second case causes the surrounding function to be checked for borrowing constraints. As long as s2 exists, s1 cannot be mutated. And the `&amp;` value is not allowed to exist after the function exits. - The first example makes a second runtime allocation for the `str` data, the second has only one. And either way there is compile-time allocation for the string literal. 
[Enum discriminant](https://doc.rust-lang.org/nightly/std/mem/fn.discriminant.html) was stabilized very recently, so it will be in Rust 1.22, the only problem is, there's no clean way to produce an enum discriminant without an actual enum object (because currently enum variants are not proper types).
I wouldn't exactly say I am starting with programming. I have been learning javascript the past six months. Made a discord bot that syncs peoples role to their speed on a site called typeracer. Also I made an image sharing website that you can register/login comment on other images and like them. The fact that I wasn't aware of that . Is I have never had to incremente inside a loop.
As important as it's knowing this is also important to know what would people like to use it for if they are not using it, and what is blocking them from that. Ie. I feel there is too much focus over web stuff and I would like to see more Rust being used in things like numerical/scientific computation, game development, GUI app dev etc. and speeding up certain things would make it more competitive vs. other languages like C++
I mean we could have a simple derive to obtain one, as long as the data inside has the default trait.
Thanks for the input, I'll investigate that. Might need to figure out some sort of portable wrapper for `ulimit`-y things...
Sounds cool, unfortunately it's unusable on my phone. Maybe you could add some flex-wrap magic so it's displayed properly on mobile devices ?
Supporting mobile devices is at the top of the list.
Man... I just spent at least 10 solid minutes convinced that the tree octopus was real, until I saw the Sasquatch reference, and the fact that all this was hosted at Republic of Cascadia. Part of me wishes it was real...a large part of me.
I am following [this](https://branan.github.io/teensy/2017/01/12/bootup.html) tutorial, there is no json, but a linker script.
Should "optionalField" not be Option&lt;String&gt;?
It was a contribution from the community. I will be happy to fix it. Since I am not too familiar with Rust, it will be a great help if you can test and let me know the issues here https://github.com/transform-it/transform-www/issues. Thanks in advance.
We're working on fixing the CSP issues on the page. /cc /u/cmrx64
IMO parallel codegen + separate compilation (run code gen only on change) is the way to go if the code generation takes nontrivial time. Even if it's just for non-optimized mode.
Basically the consensus on that proposal is that it needs more work, unless you do it I doubt anybody will.
Oh, I forgot, https://forge.rust-lang.org/platform-support.html `thumbv7em-none-eabi` is distributed via rustup, no need for JSON.
&gt; The fact that I wasn't aware of that . Is I have never had to incremente inside a loop. Actually, you always did increment inside the loop! You just used a special place to do so. It seems to me that you don't understand how loops work on lower level. (No offense, it's great what you learned already!) Especially, if you learned for loops before while loops. I'll give you quick explanation (feel free to ignore it if you learned that stuff already ;)) for(int i = 0; i &lt; 42; ++i) { do_stuff(); } Is just another way to write this: int i = 0; while(i &lt; 42) { do_stuff(); ++i; } The reason we write `for` instead of `while` is to make the code easier to understand (when one looks at it, he can quickly see how many times the loop will execute). When this code is translated by compiler, it becomes this: int i = 0; loop_begin: if(i &lt; 42) { do_stuff(); ++i; goto loop_begin; } (In case you've never seen `goto`, it makes the program to jump to another line - annotated with `loop_begin` in this case.) From this code it should be immediately obvious to you why the code behaves as it does. Try replacing your own code in similar way and you'll understand it. If you need more explanation feel free to ask.
There is also https://gitter.im/rust-lang/rust for folks who prefer that! 
[RFC #639](https://github.com/rust-lang/rfcs/pull/639) attempts to solve this exact problem (in their case with a SQL crate that has 233 configuration values). It seems like this is not an ideal use-case for enums, unfortunately, as even the macro solution is clearly suboptimal in terms of ~~performance~~ the complex LLVM code generated.
With Rust Qt Binding Generator you write Rust code and the binding exports that so you can use it from C++. So your Rust code has no link to Qt, but it exposes the Rust code in such a way that you can call it as a QObject from C++ code. So the UI code is still C++ or QML. This solution avoids creating a binding of Qt to Rust. Creating Qt bindings for Rust is a lot of work because Qt is very large and you'd need to map the ownership model of the Qt library to the strict Rust rules. 
Even with the new Discriminant&lt;T&gt; thing, you still cannot extract discriminants without an existing initialized variant.
Perhaps try having it accept an `expr` instead of an `ident`?
Rust doesn't accept expr as match patterns.
Depends on the context. You can match an integer against a numerical expression, for example.
Yeah, that was in reference to `format!`. I'm just surprised [this](https://play.rust-lang.org/?gist=48ea34937c6e12378772f2ed6af8d567&amp;version=stable) pattern doesn't flatten to a single static string. I tried expanding the macros, but stopped when I had to copy `concat!` and `format_args!` from `libsyntax_ext`.
&gt; I spend very little time waiting for a build, then running my code to see what is wrong. Much correctness debugging happens as part of cargo check, which is relatively fast (a few seconds). And I think this is a significant weakness of Rust since it encourages more "Big Bang" programming. It's less dangerous in Rust, but I've noticed that I can do 2-3 commits before actually running the code (on personal projects, not production quality). This is obviously not ideal, since there are still quite a few classes of errors that the compiler cannot catch. I still prefer writing in Rust, but the debug cycle is a bit broken by the long compile times. I think compile times need to be cut in half (at least) in order to encourage more manual testing.
Maybe. But Android is also fraction of the userbase. Priorities. We'll get to it eventually.
Wait, what? Webrender is supposed to be slow right now, it works by rendering via webrender and then pipes the result through the regular renderer. Or something like that. I don't think you're supposed to expect webrender perf improvements in nightly for a while.
Would this allow you to use multiple versions of the same crate? IE, one optional feature for one version, and another optional feature for another version?
I guess, but that doesn't help me much, does it?
To feed my ego I like to mention that I have solved the first 14 problems on project euler.
I'm not sure why you're told to not change the data model, but that's probably not a good idea for multiple reasons. 1. You won't be able to call Rust "object-oriented" code from other languages anyway, so you'll have to go through the C API, and at that point you might as well wrap opaque pointers to Rust structs instead. 2. The maintenance costs of forcing Rust into an object-oriented style are extremely unlikely to be less that the costs of changing the data model once.
Did you do: rustup target install armv7-unknown-linux-gnueabihf I don't see it in the tutorial.
&gt; Partly this is hard because in rust the ways to create/consume a type are often hidden in traits. This is a general problem with traits and their impls. For any particular type, its core API may as well be hidden in the long list of trait impls, way below the documentation of intrinsic methods. (The impls also don't have any impl-specific documentation attached to them either). This vastly reduces the discoverability of Rust idioms; the example I always like to cite is `FromIterator&lt;Result&lt;T, E&gt;&gt;` implementation for `Result&lt;Vec&lt;T&gt;, E&gt;` which is to pointed to another newcomer on IRC basically every day.
For development, I type `cargo check` all the time. I save `cargo build` for when I'm ready to test. This won't fix your build times but, for me at least, it can speed up your development cycle.
This is sad because Sublime is still the only usable editor that's neither a GUI-less terminal one (vim/emacs) nor laden with Electron bloat (Atom/VSCode).
Maybe it is something not supported in the proposal. You can comment in the RFC if this is an important issue for you. 
This sort of optimisation is better driven on the Rust side, I think (e.g. I don't think there's a language-agnostic way to specialise generics, since even, say, Rust and C++ do different things: C++ does quite general name resolution during monomorphisation), and in fact is one thing that MIR can (and does) do. Also, I could well be wrong but I think the language model of C++ templates probably means it's very hard to optimise premonomorphisation, especially because one would need to preserve semantics (e.g. compilation should still fail during monomorphisation if it does without optimisations).
It already came up. If you move the control for these things out of the code then that control becomes unavailable for macros.
Given a list of Enums I would just map over it with a function and match all the variants in there. ~~If you need an identifier to the data then maybe a Map&lt;id, List&lt;Data&gt;&gt; is better?~~ Anyway I think Enum doesn't suit your use case very well but without further info I don't know what alternatives to recommend. EDIT: Cross out a dumb suggestion 
FYI, the parallel code generation is typically refered to as "codegen-units" in rustc.
Wow, so many FCPs! 
To be fair, that sampling bias argument can be applied to literally every question in the survey ("people who use vim might've answered at higher rates"). It's definitely something to keep in mind, and obviously means more examination should be done if one was to try to, say, allocate a billion dollars in funds somewhere, but I'm not really sure how it can be avoided for this sort of thing, and the signal (especially the freeform responses) seems useful even if it isn't a complete story.
I suggest you connect with /u/evestera who wrote a paper on exactly this: http://vestera.as/json_typegen/
&gt; Except I have yet to see a phone with a hard disk. You were being pointlessly pedantic, as I attempted to gently point out with the "Tomato, potato." jibe. I would be surprised if a single /r/rust subscriber genuinely didn't understand that /u/Manishearth meant "data storage device" when he said "hard disk", especially since he immediately referenced SD cards. Since you're now doubling down on the pedantry, enjoy the downvotes.
Yeah, if I was tasked with rewriting a project to another language that used a different paradigm but told to keep the original paradigm, I'd refuse. Projects like this should be based on functional requirements (e.g. the wire protocol or whatever), not project design decisions. Write plenty of unit-tests as you go to prove your code is equivalent and rewrite it in the most natural way for the language you're porting to.
I'm not actually sure that `Box` will help much: `Box&lt;T&gt;` is mostly the same as a plain `T`. However, cloning/copying/to_owned-ing things will.
I installed the required package from the tutorial arm-none-eabi-gcc... It freaks me out that this worked yesterday...
Given that my platform is neither linux nor gnu but a cortex m4, does this actually still apply?
One could say, that nothing above assembly is absolutely needed :) Jokes aside, I believe, that if at least one project benefits from a crate, it is not unneeded. I hope, that Mocktopus will simplify architecture and remove boilerplate in well tested code like yours.
``rustup target install thumbv7em-none-eabi`` gives ``error: toolchain 'nightly-x86_64-unknown-linux-gnu' does not contain component 'rust-std' for target 'thumbv7em-none-eabi'``
Yes, this does not require traits, code can be C-grade primitive and still testable. Also yes, you read correctly, macros inject mockability deep down into the project without call analysis.
Thank you, it's nice to read that :) I don't know, I never did that. /u/vitiral wrote above, that it is possible, maybe he could tell us more?
Very cool. While the REPL itself is cool, I've been talking about building a similar lib to improve the sample codegen in our startup's product. Do you have any plans to consolidate the underlying libs into a single library? It's missing 2 things for my use case: 1) instantiating the types you construct using the data from the JSON and 2) more languages. I *might* be able to carve out some time to contribute a bit if there is sufficient overlap in goals.
Actually yes. 1 - different libraries for each transformation - already done. 2 - A unified cli - undergoing. All code is here https://github.com/transform-it
OOP might be reasonably easy to port if: - There's little to no inheritance, or all inheritance is the kind that can be easily replaced with traits and embedding the parent class. - There's few shared objects that get mutated through method calls.
It feels really laggy and scrolling stutters allot, but the [CSS maze benchmark](https://testdrive-archive.azurewebsites.net/Performance/MazeSolver/Default.html) finishes allot faster compared to Chrome. If you look closely at the video of Firefox Nightly with WebRender completing the benchmark you see that it looks way less smooth than Chrome. You see this really well when you look at the counter.
feels very speedy to me IIRC `vgfx.webrender.layers-free -&gt; true` removes the bit you're talking about, I thought. /u/Gankro?
Well, that particular paragraph jumped out at me since it was drawing a conclusion about the user base rather than the set of respondents. Many of the other paragraphs talk about this year's respondents, or make comparisons between this year's respondents and last year's; I think those statements are more valid.
Whoa is amazon using Rust now??
There's been a ton of performance work going into nightly outside of Stylo as well. Feels great :)
The fact that in C++ overload resolution requires monomorphisation to happen means I don't hold much hope. It's also important to consider that a pre-monomorphisation optimization pass could not rely on inlining (since the exact function to inline does not exist yet).
I love the name
https://www.reddit.com/r/rust/comments/6vh61v/this_week_in_rust_196/dm0pmau/
I like the look of the inferred outlives requirements RFC. I had always struggled to make structs with references work---never actually figured it out. The RFC taught me that I was missing the `where T: 'a` clause. But I'll be even happier when that's no longer necessary at all. The implied bounds are great, too. The redundancy with putting `T:Hash+Eq` all over the place always bothered me.
I think Tomaka has a good design sense. You could take a look at some of his libraries. I know Vulkano is one of his more recent efforts and he's actively working on it.
&gt;I got the feeling that compile times seem to be an elephant in the room that no one talks about I just wanted to mention that people definitely do talk about it, and I think most people are pretty aware that it's a problem.
I'm already in Slack all the time for work and other hobby things, so this is convenient for me. I'm not in IRC anymore for anything, which is the main reason I haven't bothered jumping on the rust channel.
I really think piston is a great place to start. Very easy to understand and useful too if you want to make anything with graphics
The code on that article is very nicely rendered, and the article seems well written. I also now know about the insane awesomeness of `indicatif` for CLI progress bars. The only criticism I have is that this code seems convoluted: let mut buf = Vec::new(); let bar = create_progress_bar(quiet_mode, fname, ct_len); loop { let mut buffer = vec![0; chunk_size]; let bcount = resp.read(&amp;mut buffer[..]).unwrap(); buffer.truncate(bcount); if !buffer.is_empty() { buf.extend(buffer.into_boxed_slice() .into_vec() .iter() .cloned()); bar.inc(bcount as u64); } else { break; } }
the proposal is very clear. elide types in impls, like haskell. either the community wants this or not. I see 2+, 9-. explicit-ists are in the vast majority here and will block it (the kinds of people who kept auto out of c++ for 10 years and who still argue against using it) 
Isn't this a significant step backwards and a huge hole in the proposal?
I tried to reproduce your problem using the Point example from `serde_derive` and a late 2013 macbook pro and a late 2008 macbook pro, and it worked fine. Are there any messages in `system.log` or any other places? If you can create a minimal reproduction it might help in tracking it down.
About how many active Android users does Firefox have, if it's not a secret? I suppose it's far lower than the number of total downloads reported by Google Play.
The number is probably public, but I don't know where to look for it
Gotta jam everything in before the impl period starts!
Thanks!
There's nothing like a deadline!
 struct Animal&lt;T&gt; { &lt;base stuff&gt; extras: T } impl&lt;T&gt; Animal&lt;T&gt; { &lt;base methods&gt; } struct CatData { &lt;subclass fields&gt; } type Cat = Animal&lt;CatData&gt;; impl Cat { &lt;subclass methods&gt; } You can use generics with "CatData" to turn deeper inheritance into nesting. 
Wow, that quote!
you're supposed to run: rustup target install armv7-unknown-linux-gnueabihf it includes several armv7 targets, including thumbv7em-none-eabi.
This is a somewhat simplified version: let mut buf = Vec::new(); let bar = create_progress_bar(quiet_mode, fname, ct_len); // only create the buffer Vec once, rather than repeatedly let mut buffer = vec![0; chunk_size]; loop { let bcount = resp.read(&amp;mut buffer[..]).unwrap(); if bcount == 0 { break; } buf.extend_from_slice(&amp;buffer[0..bcount]); bar.inc(bcount as u64); }
gcc isn't part of rust, you just need that installed for it to be able to compile. You need to install the rust target like I showed. Compiling for embedded targets has a lot of separate parts, so it can be confusing.
If discriminant were a const fn (and I think it could be), such a derive would be pretty easy to write, and it would just create a bunch of associated consts. Instead of `Enum::FooBar` you write `Enum::FOO_BAR` and that gives you the discriminant. EDIT: This is only correct if every variant of the enum has a constable default constructor, which they probably don't.
&gt; - a struct with hundreds of Options inside If this is about command line option parsing, that's actually what you want to end up with, though: a struct `RunState` which specifies which options have been set, and if they have been set, what the custom values are. It will always be faster to access data from this kind of struct than to search a vector of enums to see if your option has been set. 
I came up with a better solution than the rough idle I've described in a [different post](https://www.reddit.com/r/rust/comments/6y57zp/is_there_a_way_to_schedule_a_closure_to_be_run/dmkswm6/): Setup an mpsc channel, then calculate the duration until the next timer is due. Use [recv_timeout](https://doc.rust-lang.org/std/sync/mpsc/struct.Receiver.html#method.recv_timeout) with that duration. If you need to add a timer, send it to the mpsc channel (which causes recv_timeout to unblock instantly), then calculate the next duration. For each loop, check if there are any timers due. This way your thread isn't going to due anything unless it has to.
A bug report: There is no such type in Rust as `boolean`. There however is `bool`. Edit: Actually, this seems like an easy thing to change, so https://github.com/transform-it/transform-json-types/pull/2
It's all complicated and messy right now but basically Manish's description isn't accurate. layers-free is currently the only maintained "path" for webrender, but it's a fairly radical change to how gecko works, so we don't even have CI tests running properly for it. If you don't enable layers-free, you'll basically get "what we had, up until we realized layers-free was the correct path forward". This will be significantly slower, as non-layers-free ("layerful") has a significant impedance mismatch with WR's architecture. The way WR works is basically: for each item (text, bullet, border, ..) * we run the layout code to figure out things like position and properties (content, font, color, shadow, blur, gradient, ...) * decide if WR can support those properties (e.g. text isn't handled by WR unless webrendest is enabled, and it used to be that strike-through made us give up even if webrendest was set because we hadn't implemented that feature yet) * if WR can handle it, we build a WR display item and push it into the display list, and it's drawn natively by WR. * if it can't, we ask gecko to render the item to a *recorder*, producing an SVG-like binary format. This is then sent as a "blob image" display item to WR, which WR draws by calling-back gecko to "run" the recording on a given texture. (in layerful mode, we end up having to create layers for the gecko compositing system to "save" the results of step 1 and 2 in, producing the relevant wr commands whenever that layer is requested)
Yes: `extern crate some_long_crate_name as slc;`
I think it would make much more sense for this type to be a wrapper struct instead of a trait, it's not like there are going to be interesting variations on its implementation. In addition, please just follow everyone's advice and use `SeqCst`. `Relaxed` operations aren't ordered with regard to other operations on different memory locations, so suppose you have two counters, `low` and `high`, and you uphold the invariant that `low &lt;= high` by always performing `high.inc()` before `low.inc()`, then you can still break your invariant when multiple threads are involved because of reorderings! I think such behaviour completely misses the mark when the library is supposed to be "something simpler".
Thanks, I'll have a look in the log when I get home. If there's nothing obvious I'll start putting together minimum code, but hopefully there's something obvious happening.
You should watch the video. It has very little to do with `format!` and more to do with what optimizations the compiler can do when `push_str` is called multiple times with literals.
Is it possible to take a struct field as a function's argument? I have a struct with several fields and I shouldn't have to re-write each method if the only thing that changes is the field its applied to. Am I missing something?
&gt; having this unsafe pointer makes myMod (now called Emulator) unable to implement the Send trait. So, some pointers are safe to send between threads, some are not. With `unsafe`, you have to make the distinction. We can use `unsafe impl Send for Translation_Cache {}` to make it work, if it's okay to send that pointer from one thread to another. I sent a pull request in that makes it so the Rust code compiles, although I get a linker error about the `fill` function you have in an extern block. I'm not sure where that function comes from, and neither is Rust. Depending on what you're doing with that `Translation_Cache` object, it might even be possible to convert form a raw, unsafe pointer to a Rust slice and just keep that in the struct. See [this function.](https://doc.rust-lang.org/std/slice/fn.from_raw_parts.html) Just as a style thing, the Rust compiler is rather upset at your naming conventions. I find that it's easiest just to name things in a way that the Rust community finds consistent, but if you really want to just name things whatever, then you should add this to the top of your `main.rs` file: #![allow(non_snake_case)] #![allow(non_camel_case_types)]
Not only is this code convoluted, it also *reads the whole file to memory* before writing. This is a bad idea, because large files will exhaust available memory. Writing `buf`'s contents (as far as available) to the output file would be the correct solution.
Everything I've seen so far from /u/burntsushi is pure gold. Read through [ripgrep](https://github.com/burntsushi/ripgrep) for some dense yet approachable text processing code.
I agree. That's a separate point that I meant to address, but then I got distracted.
&gt; But seen it mentioned that they are similar to C++ and people seem to work with that. the [context-free syntax](https://en.wikipedia.org/wiki/Context-free_grammar) should allow Rust to compile faster than C++ in future, i.e. with more processed in parallel, potentially caching more between builds, by making it easier to figure out which parts need to change.. however realising this sort of thing is complex; I'm not sure what the expected curve of improvements would be
This is a little late, but there is also https://github.com/myfreeweb/freepass in case anyone missed it.
The `error-chain` crate has a `bail!` macro that does some of what you need and it has a `chain_err` function that should help with the rest. And combine those with `?` (or `try!`). let rd = match std::fs::read_dir(dir) { Ok(rd) =&gt; rd, Err(e) =&gt; { eprintln!("Unable to read directory {}, skipping from delete pass; error: {}", dir.display(), e); return } }; Could become: let rd = std::fs::read_dir(dir) .chain_err(||format!("Unable to read directory {}, skipping from delete pass; error: {}", dir.display(), e))?; You could do even better than that, but it would be a decent first start.
It is...impressively elaborate.
Rust chose LLVM, not the other way around. 
that quote needs to be reformatted and framed somewhere
okay, I just wanted some hype if you had any. It's cool!
&gt; It's also important to consider that a pre-monomorphisation optimization pass could not rely on inlining (since the exact function to inline does not exist yet). It seems plausible that it could do something. For example, I have a bunch of generic code where a `P: Push` gets wrapped by, say a counter, to track how many elements get pushed. It does `self.count += 1; self.pusher.push(elt);` in its body. We don't know what `push` will do yet, but this method could still be inlined at its call sites (which are similarly generic in `P: Push`) and if it is seen to always increment its counter, we could then optimize the `if self.count &gt; 0` test that determines whether we've sent anything. I mean, not that this should be trivial, but I have to imagine there is a healthy body of PL research on the subject.
sure, you can use the [cfg_attr](https://doc.rust-lang.org/1.12.0/book/conditional-compilation.html) #[cfg_attr(test, mockable)] on your functions (I think)
But where would the actual printing happen? That's what I do for fatal errors, where I want the errors to propagate up to main() where the program exits, but in these cases, I just want to print and skip the iteration. The second code block is a short function called from a for loop, so "return" is basically "continue". I don't want the loop to exit because of the error.
Security isn't just your use of a device, but also the population at large. It is much like a vaccine, one need to have herd immunity, not just a singular vaccination. https://en.wikipedia.org/wiki/Herd_immunity
**Herd immunity** Herd immunity (also called herd effect, community immunity, population immunity, or social immunity) is a form of indirect protection from infectious disease that occurs when a large percentage of a population has become immune to an infection, thereby providing a measure of protection for individuals who are not immune. In a population in which a large number of individuals are immune, chains of infection are likely to be disrupted, which stops or slows the spread of disease. The greater the proportion of individuals in a community who are immune, the smaller the probability that those who are not immune will come into contact with an infectious individual. Individual immunity can be gained through recovering from a natural infection or through artificial means such as vaccination. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.27
And then you just run `cargo test` with nightly and all the rest with stable?
That guy is a hero. I also find it EXTREMELY useful to read code of the crates I use, at least search it for things I'm confused abot.
One of the nice tgings about rust is that it controls shared mutable state without banishing mutability entirely. While others have provided good suggestions, I'll just mention that you can use imperative, mutating code without worrying about side effects or breaking referential transparency.
&gt; But where would the actual printing happen? You could do it at the top level. If you look at the `error-chain` intro this topic is covered. `chain_err` lets you compose a bunch of log related stuff as you take all the error paths back up to the top level. Then you can dump all that out giving the user a sense of what failed from most specif to most general. 
In VSCode you can use LLDB Debugger to debug in a rather pretty way ;)
(tangentially, I'm guessing rust would let you step through files with an iterator, how would that look, would random access implement seek? EDIT.. i see it in the docs, ok)
Right, but unless I'm misunderstanding you, that's still what I *don't* want to do (print at the top level), and why I felt I couldn't use `error-chain` there as well. I thought about your comment a while, and I figured it out, though. It wasn't very difficult, but I didn't even think about modifying the call site. For the second example in my original post, I just did let meta = std::fs::metadata(src).chain_err(|| format!("Unable to read times for {}", src.display()))?; combined with changing the call site to if let Err(err) = link_file(Path::new(&amp;track.path), &amp;target) { eprint!("{}", error_chain::ChainedError::display_chain(&amp;err)); } Since there were multiple such cases in that function, it got reduced from 14 lines to 3 (4 if you count a line break to make it fit on screen)! I'll have a look at the recursive case now. Edit: Still not sure how to take care of the recursive case. I can't use the same idea since the "catch" code is only executed once, while may be dozens of errors along the way (which aren't chained).
This is the first time I understand the difference between Sync and Send! This quote should go into the docs!
I'm not sure I follow the question. Have a code example that's giving you trouble?
yup, should work I think! Only caveat -- not sure how much your library will affect benchmarks (and not sure if benchmarks trigger `cfg(test)`, I think they do).
ripgrep and the Regex crate are great examples to read through and are well documented so it's hard to get lost. I would highly recommend it. Pretty much anything he writes is a good example to look through. He covers some of those on [his blog](http://blog.burntsushi.net/) and the CSV parsing entry is aimed at beginners who are looking to improve their Rust.
Ah cool. My site uses Terra for templating but I'll try this out! I like having type safety if possible even if the site is mostly static :D
&gt; the example I always like to cite is `FromIterator&lt;Result&lt;T, E&gt;&gt;` implementation for `Result&lt;Vec&lt;T&gt;, E&gt;` Whoa, that's pretty handy! And I think I just proved your point (again) :-P
Everything has trade offs. &gt; Wouldn't the language be cleaner if they weren't? It would certainly be more of a pain to use, for very little measurable gain. Imagine how much friction it would create with people transitioning to the language, and then most people would have to go through the effort of creating their own versions of the current, globally accessible pipes, rather than just having it built in. Those resources are naturally process global. Having to pass them around to every function and every thread would be incredibly tedious. Rust allows you to lock the mutex and gain direct access to the underlying instance when needed for performance, but for the remaining 95% of the time, Rust has the right default approach.
&gt; OP's version is just a bit overcomplicated I disagree, it's not complicated enough. It clearly needs [more enterprise](https://play.rust-lang.org/?gist=766ceee35cfc9d1987532f2155c04304&amp;version=stable)!
I disagree. Not sure if I should say more because it's not like I can be particularly constructive, but it seems like every time I look at Piston from a user's perspective I run into aggravating design decisions. Most recently the SDL wrapper and its handling of the lifecycle of textures and renderers. So, SDL isn't the most Rusty library. Textures may be destroyed by two different calls; either individually or en masse when the parent rendering context is destroyed. Rust is only happy with a single point of destruction. SDL does this because textures may not outlive renderers, and while a segfault is a crude, ineffective tool for detecting unsafe behaviour, it's the best C gives you. The way I'd prefer to see it wrapped is simple: only use one destruction call. If you support freeing a renderer at all (games are unlikely to need that feature, but if your mission is to wrap 100% of SDL...) ensure that the renderer isn't freed until the textures are. This is the only safe approach. Now, do you - a) enforce safety by reference counting, understanding that the run-time overhead is negligible next to the driver overhead of creating a texture? - b) enforce safety by leveraging borrowck? Well, Piston is taking option B. The big justification is "we might want to close a window and continue running but if there's a resource leak, we couldn't." So instead you must prove no leaks. Textures are only borrowed from a texture factory. Since it only lends textures, it's more of a "bank" - this pattern is *really difficult to use*. 
The idea is not to go over the list for each argument, but instead iterate once and act on each argument: for flag in flags.iter() { match flag { Flag::Flag1 =&gt; { // do stuff in case Flag1 is supplied }, Flag::Flag2(val) =&gt; { // do stuff in case Flag2 is supplied, using it's value }, // etc. } } If you can't do that, then a separate function to return the value of each flag from an iterator can do the trick - and they can be generated using a custom derive. But, if you still want something "generic" - as in, a single function that you just need to pass some code for it to do it's magic - I would do something like this: https://play.rust-lang.org/?gist=65d514abc4538094df2cf3b77671f63f&amp;version=stable So, what's going on there? I use `.filter_map()` to **pick** specific values(_filter_) and **do** something with them(_map_). The closure I use uses `if let` to match the specific enum variant I want, and extract the data from it - which I return with `Some()`. Other variants will result in `None` and won't be picked for the resulting iterator. This gives me an iterator with the performed action done on all members of that type. But I only want the first one(don't want to deal with array here), so I call `.next()` to get the first value(or `None` if it does not exist). This gives me an option with the value of the first item of that variant in the vector.
Well, in some programs I use a "configuration` object. Having to pass it around is pain as well, but whenever I thought about turning it into a mutex-controlled singleton everybody was strongly against. I believe that 95% of the programs don't do fancy things with stdin: passing it around wouldn't be a major hassle for them. The remaining 5% could choose to implement a singleton stdin (just like they could opt for a singleton configuration object)...
you're more than welcome to pass the stdin object around, rather than using the global implementation, but I don't think it's the same situation as a configuration object. Some people place configuration in a global. It's not the preferred method, but it's perfectly valid.
BTW, this can easily be encapsulated to a macro: https://play.rust-lang.org/?gist=4d6344c03c5b8052102b927f71fc7f62&amp;version=stable This macro will only work for single-parameter tuple variants - but in case of flags all variants should be either this or units, and units can easily be checked with `.any()`.
Here's a simplified case. I want to be able to run ex.inc(a) or inc(ex.a) instead of having two separate methods. #[derive(Debug)] struct Example { a: u8, b: u8 } impl Example { fn inc_a(&amp;mut self) { self.a += 1; } fn inc_b(&amp;mut self) { self.b += 1; } } fn main (){ let mut ex = Example {a:0, b:0}; ex.inc_a(); ex.inc_b(); print!("{:?}", ex) }
But seriously, a few of these are quite contentious. There's gotta be a balance between meeting the deadline and actually waiting for consensus.
Using non-global I/O is arguably more elegant in theory, but sometimes you do need them for debugging purposes, so for practical reasons it's convenient to have them available everywhere. This is tangentially related to the question of how much "purity" (referential transparency) Rust should strive for. As it is, Rust makes no attempt to be a pure language. You can always get global access to it through other means (e.g. foreign interface), so in some sense pretending that it's not a global presents a dishonest view of the underlying platform.
the fill function was supposed to call libc::memset to fill each page of the translation cache with return instructions so the program doesn't go off executing random pages of memory. I must have forgotten about it and never finished the function since I'm only using a single page for testing purposes, replacing fill with memset should fix the issue. Translation_Cache is supposed be an array of executable caches of binary data. They are populated with 0xC3 at initialization because that is the x86 instruction to return from a function call. You then create a function pointer to point at the the cache and when you call the function the pc will jump to that location and execute your binary data. In C like languages a simple implementation looks something like this int (*Translation_Cache)(int); unsigned char translated_code[] = {0x8B,0x44,0x24,0x04,0x40,0xC3}; //move (ESP+4) into EAX, inc EAX, return int main(void){ Translation_Cache = (int (*)(int)) malloc(sizeof(code)); //create function pointer and allocate memory for translation cache memcpy(Translation_Cache, code, sizeof(code)); //copy binary data to location of function pointer (*Translation_Cache)(0); //execute binary data return 0; } I found an article explaining that It could be done in rust by setting up uninitialized block of memory, writing some machine code into it, then creating a function call pointing to that point in memory. To give you the full picture the call to run the executable Translation_Cache would look something like this fn getBlock() -&gt; (fn() -&gt; u64) { //store executable binary Translation_Cache [0] = 0x8B; Translation_Cache [1] = 0x44; Translation_Cache [2] = 0x24; Translation_Cache [3] = 0x04; Translation_Cache [4] = 0x40; Translation_Cache [5] = 0xC3; unsafe { mem::transmute(Emulator.Translation_Cache) } } let runBlock = getBlock(); runBlock(); I'm not sure If this way is the best way or if there's other ways to do this in rust, that's part of the reason I'm asking for help on here in the first place and I'm open to suggestions. 
(Prefer using the "announcement" link flair over `[ANN]`)
&gt; * Due to the nature of the compile time checks etc, is the compile time always going to be slow? This is actually a relatively small part of compile time, and there are a lot of changes coming soon that will make it even faster. For example, [Chalk](https://github.com/nikomatsakis/chalk) and MIR-based borrowcheck are refactorings of the compiler that clean up a lot of technical debt. Even if they doesn't speed things up on their own they should make it much easier to do optimization work in the future. &gt; * I had heard about MIR (incremental builds) that allow faster development times, are there any other such ideas that are in the pipeline? Aside from incremental builds (MIR is actually the name for an internal representation; incremental builds just happen to use it), MIR should also enable some optimization to be done before monomorphization, i.e. before generic code gets duplicated for all the types it's used with. Since LLVM takes the vast majority of compile time, this sort of thing will probably have the biggest impact.
Is there a proposal to implement this that I'm not aware of? I was expecting a link to an RFC, but instead I got a Wikipedia link...
To be fair, I didn't mean to take a stab at the choice, sorry if I came off that way! I wholeheartedly agree with your first point, ensuring that multiple editors can be used is key. I'd disagree on the second point, the poll still shows that a majority of users are using vim, even with the current focus on an editor with a lower learning curve. And this doesn't mean enforcing vim either, the same way vscode isn't enforced. For the third point, I like the idea of the LSP, but it has hard limits. The use of extensions to the protocol means that the lsp won't work out of the box with a standard lsp plugin. That article points out a few things that are not limited to the IDE side of things: https://perplexinglyemma.blogspot.co.nz/2017/06/language-servers-and-ides.html (And I'll argue that neovim is not vim). I'd prefer an extended libsyntax, as it would mean that every type of editor plugin could get something out of it, not just the LSP. But anyway, I was just surprised to see vim at the top. Thanks for clearing up everything!
Oh, nice! I usually structure things in a way where I don't need it, but that's nice to know. Thanks.
It's all good! Regarding the third, yeah, let's just say we have very different opinions :)
But inability to optimize such cases still has a [cost](https://godbolt.org/g/jtPSS5) ([compare](https://godbolt.org/g/qixCpa)). Yes, in most cases it's quite negligible, especially with correct pre-allocation, but still it would be cool if Rust could do optimizations in similar situations.
Go uses named returns to go around that limitation. I don't have a straightforward answer for Rust, aside from returning tuples where one part is mutated in the `defer` statement itself. :/
You definitely can't pass a field like `ex.inc(a)`. But you can use a trait to define an extension method like `ex.a.inc()` and you can invoke that method explicitly or with a simple macro to clean things up a bit. [Like this](https://play.rust-lang.org/?gist=167d857a906414365d899051b5aa2fd5&amp;version=stable).
It's C struct alignment, since you've specified repr(C). While the struct members will always be laid out in order, they may not be packed together. https://en.wikipedia.org/wiki/Data_structure_alignment#Typical_alignment_of_C_structs_on_x86
If you want the struct members to be unaligned, use `#[repr(packed)]` instead of `#[repr(C)]`. As it stands, `bfType` is only half a word wide, and the other half is useless padding.
Why is asking a question invasive?
I think a fundamental difference is that stdout is a *sink* used primarily for a subset of logging. Code that does output to stdout rarely does so contractually, but as a side channel, and normally has little semantic meaning to the caller. I can easily see why you'd want stdin to be single-ownership, and it might even make sense to separate stdout into single-ownership UI-relevant output and cheat-y debugging output, but at some point the complexity outweighs the cost. I expect any nontrivial TUI is going to be wrapping these interfaces anyway, at which point you've re-introduced ownership and the behind-the-scenes is irrelevant.
That's what we ended up doing, this inside a macro that takes a indent defining the right flag. Also, passing an Option&lt;bool&gt; is redundant in your example, bool would be fine.
thanks, that fixed it
i fairly randomly link things sometimes, but the reason i put this here is: in other discussions I've encountered people who weren't immediately aware of the implications of 'context-free grammar' vs 'context sensitive gramar' which is a trade-off in the rust syntax, i.e. *why* it has a few more keywords here and there ('```let```, ```fn```..') and more specifically the ```::&lt;T&gt;``` needed to introduce type-parameters sometimes (.. some C++ fans will insist these are bad choices, but once you know what C++'s syntax costs us, Rust looks like a much better idea). All I'm aware of for sure is that the team wanted a CFG, with the long term goal of making tooling easier
Oh, i haven't even thought about this. I've the same struct in c and it was 14. My guess it was because of different compilers?
~~Thanks! That's exactly what I needed.~~
[alacritty](https://github.com/jwilm/alacritty)
Respectfully disagree, but not because of Tomaka but because graphics programming is kind of a beast all of its own, so I don't think I'd recommend something like Vulkano as a good project to "read to improve your rust skills". Instead I'd recommend a project where the problem domain is relatively familiar, but still showcases useful features in the language and the idiomatic ways to use them.
The alignment should be standard on a given platform, unless some annotation indicating packing is used. What platform are you on, and what compiler did you use that gave 14? If [I try the same thing in C](https://onlinegdb.com/r1TbOfAtZ), I get the exact same results as Rust gives you.
It's seen from [their repos](https://github.com/wireapp) on Github that iOS client is written in Objective C and Swift, Android client in Java and Scala, and server in Haskell. Looks like they don't use Rust on mobile.
I used msvc-14.1. Now i've tried on a new project and it does give 16. There are weird things going on with the legacy sdk i'm trying to make run :/
It looks like MSVC does [have an option to globally change the alignment used for structure packing](https://msdn.microsoft.com/en-us/library/xh3e3fd0.aspx), so the SDK may be using that.
I just looked at the repo. It would appear the iOS app uses "wire-ios-cryptobox" which provides "cross-compilation" of cryptobox for iOS. The Android app uses "cryptobox-jni" which provides "jni bindings for cryptobox with support for cross-compilation to Android" Cryptobox is written is rust and looking at the cargo.toml files has a dependency on Proteus, Wire's Rust rewrite of the axolotl protocol. Perhaps I'm mistaken as I haven't dug to deep, but this would imply to me that they are using Rust for there protocol library while using Java, Scala, Objective-C, and Swift for the front end and bindings. Edit:phrasing
Do you know why Rust is emitting gigabytes of LLVM IR? Does it reduce down significantly after optimisations? Is a lot of that duplicate code? If it reduces down a lot with optimisations then perhaps the Rust compiler can do easy and fast optimisations such as the JVM does with its "sea of nodes"-IR with rewrite rules applied at construction time. If the issue is duplication then perhaps the Rust compiler can recognise when two instantiations of a type are basically the same and only generate one. For instance you only need one version of the polymorphic identity function per size. In particular all pointer sized types can share the same code for the identity function. If a generic function f&lt;T&gt; calls generic functions g&lt;T&gt; and h&lt;T&gt;, and if g and h have been compiled to the same code for type T1 and T2, then f can also be compiled to the same code for T1 and T2. Perhaps this leads to a virtuous cycle of code sharing.
Those are fair points. 
Nice! Can we have a possibility to have struct and fields marked `pub`?
Yeah, I agree, it's not at all ideal. But you can do something [like this](https://play.rust-lang.org/?gist=68fe2fa4f6b9efb53502247c4c407725&amp;version=nightly) where you hash the value of the incoming determinant and compare it to pre-hashed values of those that are explicitly set. Wouldn't this solve the issue?
You can't to that with Cat. It needs to be a newtype instead, aka `struct Cat(Animal);`
&gt; As it is, Rust makes no attempt to be a pure language. Well, it *does* look askance at mutation through shared pointers. You can do it, but you must either declare thread unsafety (Cell) or memory ordering (Atomic*)
When process-local storage makes sense, it's easy enough to write with some `unsafe` - other languages call that a global. Rust makes it difficult to ignore the thread-safety issues that come with globals; otherwise there's nothing in the language that is particularly biased against them. And compile-time function evaluation will soon make it *easier* to have globally shared mutable state. 
This is from a Rust conference...why not? :)
While I'm a huge fan of Rust (so I'm guilty myself of trying to find excuses to write things in it) I have to ask the question: what is the motivation for re-implementing this library in Rust? Even if you have an extremely good reason to re-implement the library in Rust (rather than staying with C#), the details of the _why?_ may well help to constrain the solution space.
This, right here, makes that thread worth it for me. I'm glad i could be at least somewhat educational while farting around on twitter. :D
"Guardanteed". Great portmanteau.
**Edit**: I should've started this by saying: your concern is totally understandable, given the pace of the RFC repo lately. But I want to assure you that the teams take RFC deliberation very seriously, and have worked hard to avoid exactly the outcome you allude to. I believe that all RFCs currently in FCP have fully uncovered the major tradeoffs at play. That does not mean that every commenter agrees on how to *weigh* those tradeoffs (given how widely we're trying to advertise these discussions and the resulting comment volume, that's totally impossible). But the strongest cases on all sides have been made. I further believe that all RFCs en route to merging have broad community support, though some have seen outspoken dissent. I believe this because the most contentious RFCs are the ones that have also evolved the most through community feedback, and seen support by an ever-growing set of stakeholders. It can be hard to gauge this if you haven't been participating throughout, of course. Regardless, if an argument is raised that was previously missed, that blocks the process. For example, I blocked FCP on [my own RFC](https://github.com/rust-lang/rfcs/pull/2126#issuecomment-326321747) when a commentor pointed out that an argument hadn't been sufficiently dealt with. It has since been [entirely cleared up](https://github.com/rust-lang/rfcs/pull/2126#issuecomment-327578029). RFCs that involve *changes* to the existing language are always going to be especially contentious. We've taken special care here to do many iterations of design work publicly, to reach out to stakeholders, to advertise discussions widely, and to fully engage in the comment threads. What I would ask, given the enormous amount of work that has gone into this process, is that rather than allude to a vague but troubling sense that the teams are not "actually waiting for consensus", you raise specific, concrete arguments or tradeoffs that you feel have gotten insufficient attention. Feel free to ping me specifically, and I will make sure they are addressed.
I'm guessing: Boss said rewrite in Rust but keep the structure/concept so no docs need to be changed and a side by side comparisment can be made. Basically the boss may have looked up that safer than C stuff and doesn't want to cause any changes in existing docs/trainings/service manuals.
It might be easier to start with thinking about `stdout`. If `stdout` wasn't global, we'd need to pass it as an argument to every call to `print!`, and to every function that calls `print!`, and so on. That would be incredibly annoying :-D But just to run with the thought experiment, there are also some ways that creating an "owned `stdout`" could be confusing: - If we make any FFI calls into C code, and that C code calls `printf` or `gets`, it's going to talk to the standard file descriptors. - If we spawn any child processes, they're going to inherit our stdin/stdout/stderr. At the end of the day, the standard pipes are baked into all the operating systems we're familiar with, and that leads to most programming languages representing them the same way, as magical globals. The same thing applies to environment variables, and in a sense to global resources we don't even think about, like access to the filesystem and the network.
Been a huge fan, and glad I could help a bit. Congrats on the release. I'm hoping to throw together a "Porting Python/Bash Automation to Rust" blog post soon, and the compile-time templating of Askama will be a centerpiece as it makes me far more forgiving of the "heaviness" of using Rust for a task where scripting languages usually reign.
Windows data structures like the BMP file-format were designed in the days of Win16, so they're typically designed with 16-bit alignment, rather than the 32-bit alignment that's native to the 80386 and above.
Yeah, i always thought about "artificial neural network" at the first glance.
I guess my example was over-simplified. What if I need to manipulate another field in my struct? ie. if I add a bool to the the struct, and want to change it to true when inc() is called [like this](https://play.rust-lang.org/?gist=6d1ec9d0f1f879c10e6e320ce5be610f&amp;version=stable)? Thanks again for the help.
You are wrong. You do not have a right to control the software because you have the ability to subvert it. You have a right to subvert it because you have a recognized right to control the software.
His [transducers](http://blog.burntsushi.net/transducers/) blog post is a gem. Very inspiring!
The standard library. In particular, reading the source for `Iterator` made me happy, because the source is simpler than the method declarations ;)
Even Haskell has Debug.Trace just sitting there ready to print random debug output outside of IO...
Happens to the best of us. Luckily, Rust is a distraction-resistant language.
To be sure: not all references in structs need that bound, only those that contain a reference to a generic type. This is because the compiler can't be sure whether the generic type is actually instantiated with Type&lt;'ultrashortlifetime&gt;, which doesn't outlive &amp;'slightlylongerlifetime Type&lt;'ultrashortlifetime&gt;. This is not a concern with concrete types. However, of course it should be able to be sure because there's no way you could ever construct &amp;'slightlylongerlifetime Type&lt;'ultrashortlifetime&gt; anyway! So the inference is very welcome.
[brson/stdx](https://github.com/brson/stdx) is what you looking for. It's a list of rust "best" crates judged by their quality, conformance to conventions... See [selection criteria](https://github.com/brson/stdx#selection-criteria) for more details.
&gt; I think it would make much more sense for this type to be a wrapper struct instead of a trait Yes, I think I'll got with the wrapper struct. I agree that makes much more sense. However, I'll also keep the trait, just not implement it for AtomicUsize, but for the wrapper struct instead. &gt; In addition, please just follow everyone's advice and use SeqCst. I think I will provide two flavors: One using SeqCst and one using Relaxed and document how to use them properly. This is more or less what I tried to avoid, but here I am. At least, I have a better understanding of Relaxed and SeqCst now :)
There is no "class hierarchy" in rust. You fail before you start. You have to explain that to the asker. You want rust, you have to pay the price,otherwise abort now.
I just wanted to point out that this feeling might come because things are moving too fast. Not everybody has always time to keep up with all of the RFCs as they evolve and change. For example, reading each module RFC takes me 2 hours. If I have 2 hours allocated per week to read Rust RFCs it means it takes me three weeks t read the three RFCs, and in that time frame, I don't read any other RFC. I am not saying that we should plan for people like me, but it might make it easier to organize my time if I could see some deadlines, e.g., "this RFC is in FCP, if no new issues are raised it would be merged the xx.yy.zzzz". Also, it would greatly simplify my planning if RFCs are merged serially.
&gt; It really depends on the use case. That really hits the nail on it's head! I was only thinking about my specific use case and not what else one might expect from a counter. I will update my implementation &amp; the docs accordingly. Thank you and @dpc_pw for you insightful replies! :)
You can do something like fn inc&lt;F&gt;(&amp;mut self, mut f: F) where F: FnMut(&amp;mut Example), { f(self); self.inc = true; } fn inc_a(&amp;mut self) { self.inc(|s| s.a += 1); } or macros.
The command-line story of rust is real good. I use clap in one crate and like the advanced features such as `+global` and `+conflicts_with`. Recently for a more lighter weight app I used [envy](https://crates.io/crates/envy) which lets you define a struct that is populated from environment variables - for env var configuration of your app.
Sadly my enums require underlying data of various types.
*Danger!* Wild speculation ahead. I'm looking forward to replies telling me how wrong I am :) I don't think we need a "very special cased" optimization: Const eval with miri might actually be powerful enough to see through building that string on the heap. IIRC, miri has its own virtual heap the interpreter uses and that can track allocations as well as their liveliness. So, at least in theory, it is possible to optimize that down to knowing the actual bytes that make up that string on the heap. If you can then find a way to get rid of the heap allocation all together in the binary… I don't know. It'd probably need to return something like `Cow&lt;'static, str&gt;::Borrowed` when const eval was successful and `Cow::Owned` otherwise. But, that would be an optimization valid for all owned/slice like types.
I think you'll just want to ignore the previous bit then and use a macro for that use case. [Playground](https://play.rust-lang.org/?gist=136a990f47f11607e80ba80f214b9a01&amp;version=stable)
Wouldn't that break if he needed 1-byte fields as well? Since `#[repr(packed)]` would then shift by 1-byte offsets, whereas his msvc project seems to be configured for 2-byte offsets?
&gt; the proposal is very clear. elide types in impls, like haskell. Even if the community wants this, an RFC needs to be properly worded, concretelly implementable, explore the design space, document how to tech this, etc. Discussing about these issues is pointless at this stage, because the RFC changes the default template of the RFC repo for no reason, and also proposes a module system in a different file... You have been asked to fix this weeks ago, and you haven't done it. Nobody is going to do this for you. And if the one proposing it doesn't care enough about the feature to even fix trivial formalia, why should anybody care? There are many other RFCs where the RFC authors do care, and with limited man-power, these ones just take priority. 
&gt; cheat-y debugging output Isn't that what `stderr` is? I have to say I never really thought about it carefully, but now I kinda see that making `stderr` global and `stdin`/`stdout` arguments to `main` actually makes sense. If I'm writing `grep`, it's very important that none of my deps clobber `stdout` (but they're free to do that with `stderr`). 
Are you using a procedural macro? You might have a procedural macro `EnumVariant` that creates from: #[derive(EnumVariant)] enum Foo { A(u32), B { key: String }, C } something like this: enum FooVariant { A, B, C } impl Foo { fn get_variant(&amp;self) -&gt; FooVariant { ... } fn is_variant(&amp;self, var: FooVariant) -&gt; bool { ... } } 
Where can I learn more about `nounwind` ? Can I use that to indicate that something cannot panic? What happens if something does indeed panic? Is `nounwind` available on stable?
You'd need to refactor the way you handle optionals to provide a function, much like the handleArray
What's the reasoning behind that?
Keeping configuration in a global can be problematic for another reason: It makes testing more complex since test cases for anything that accesses configuration need to be serialized. If you pass a configuration object around, you can just mock one up on the fly as part of the test case.
Thanks. This looks perfect!
I agree that volume is a problem. This is also discussed at https://github.com/rust-lang/rfcs/issues/2142
In Python, you'd append underscores for avoiding name clashes instead of prefixing them. An underscore prefix screams "private" to me. :) What do you think?
why not... Cat is Animal&lt;CatData&gt;, by definition, right?
Cool! Writing CLI tools in Rust [seems to be](https://deterministic.space/rust-cli-tips.html) popular these days :)
it's a shame this happens, because the trait idea is more pleasing when you build things up around that .. they can still be layered. out of interest.. what does the C# code look like? I've never used C# myself; doesn't C# have extension methods , and so called 'interfaces'instead of c++'s multiple inheritance? Do those map any better to traits? My own experience moving over from C++, I think you can sort of do things 'backwards' (as shown in a post below), in that a base class is sometimes really the 'interface', and the derivations are really more like 'impls', sometimes layered (and in turn you can generically impl to compose) something like this?? c++ heirarchy for gui stuff.. class Window {...} // 1 'the methods any window needs for the gui framework' class GridView : Window { ... } //2 'a window holding a grid of elements' // 'the elements must give overloads to draw themselves &amp; interact.' class TextureBrowser : GridView { .. } //3 ' implements a texture browser as a specific for a grid view window' -&gt;rusty equivalent.. trait Window { ..} //1 'the methods all windows must support for the gui framework' trait GridView { ..} //2 'the methods needed for a grid view' impl&lt;T:GridView&gt; Window for T{..} //2 'how any grid-view slots into the window framework' struct TextureBrowser { } //3 'a specific for a texture browser window' impl GridView for TextureBrowser { .. } //3 (satisfies 'window' due to generic impl)
Could work, but isVariant would require pattern matching, and with no guarantee that the discriminant matches on both sides for optimization. I could force int values on the FooVariant, which match the hidden implementation of Discriminant using the intrinsics... The problem would be getting data out, as in my example. If you have a Foo, of which you know the variant and the type, how to return the underlying data if it exists ?
I am totally up for using some other form. Any suggestion ?
With some unsafe code, we can just make a dummy value? use std::mem::{Discriminant, discriminant, forget, transmute, size_of}; enum AB { A(Vec&lt;i32&gt;), B(Vec&lt;i32&gt;) } fn ab_a_discriminant -&gt; Discriminant&lt;AB&gt; { unsafe { let a = AB::A(transmute([1u8; size_of::&lt;Vec&lt;i32&gt;&gt;()])); let ret = discriminant(&amp;a); forget(a); ret } } Edit: Changed `uninitialized` to `transmute` because there are types, e.g. if you did this with Option&lt;&amp;T&gt;, where the null pointer optimization kicks in, and that forces reading of uninitialized() which is UB.
C++ has these problems, and compiles much faster than Rust. D also has these problems, and compiles infinitely faster than Rust.
No dice... nothing happened. I installed thumbv7em-none-eabi by package manager as well.
Ok, did it, but no change whatsoever...
Well, the suggestion would be to append them: Since `type` is a keyword, it would be converted to `type_`, see [PEP 8](https://www.python.org/dev/peps/pep-0008/): &gt;`single_trailing_underscore_`: used by convention to avoid conflicts with Python keyword
&gt; Do you know why Rust is emitting gigabytes of LLVM IR? IIUC rustc does a 1:1 map of Rust code to HIR, and then MIR, and then LLVM IR, and LLVM does the rest. MIR optimizations like removing dead code, folding duplicated code, etc. could potentially significantly reduce the amount of LLVM IR generated. Whether this is faster than letting LLVM do these optimizations, I don't know. Anyhow, somebody would need to implement and validate these. Also, adding these optimizations to rustc adds more work to be done. If the code is dead or duplicated, maybe it would be better if we could avoid emitting it in the first place, instead of emitting it first, and having to remove it later.
&gt; out of interest.. what does the C# code look like? doesn't C# have extension methods , and so called 'interfaces' (I've never used C# myself) instead of c++'s multiple inheritance? Do those map any better to traits? C#'s extension methods could easily be implemented through the trait system, as they only have access to public fields/properties/methods anyway. Interfaces in C# are similar to traits. I think the biggest difference is that an interface can define a field, while traits cannot at the moment. Like traits, a class can implement multiple interfaces. I don't think that interfaces can currently have provided implementations of functions. In terms of inheritance, classes in C# can only inherit from one class. However, you can inherit from a class which itself inherits from another class. For example: class A {} class B: A {} class C: B {} So you can still end up with strings of inheritance, which is still better than webs.
True, there are many reasons why global resources are discouraged. But these reasons also apply to `stdin` &amp; co. Testing something that uses `stdin` could be harder than testing something that reads from something that implements `Read`.
See example at https://docs.rs/reqwest/0.7.3/reqwest/ instead of read_to_string, you can write to file.
A good heuristic for globals is to consider how likely/useful it is to have two distinct copies of the resource in question in a process. standard pipes are strongly favored by this heuristic because your process only has one of each. Configuration is okay in some cases to make global because there aren't a large number of cases you need multiple configurations (testing and shadowing components are two that come to mind).
If you just want to download a file, I would recommend using the [reqwest](https://docs.rs/reqwest/0.7.3/reqwest/) wrapper around `hyper` (see the link for example code). `hyper` is a low-level async HTTP library, and it's rare that a newer Rust user would want to use it directly.
Thank you for your reply. Is it possible to resume a download with reqwest? I found nothing about it in reqwest's docs.
If you want the code that reads/writes binary data from/to files to be portable, you should not rely on any kind of struct memory layout. This is true in C and C++ as well. The "C standard layout" is system-specific. And in Rust `#[repr(C)]` is for talking with C code only. Differences include padding and endianness. My suggestion would be to check out [serde](https://serde.rs) in combination with [bincode](https://crates.io/crates/bincode). `bincode` allows the use of a custom byte order (little endian, big endian or even weirder custom orders) via [byteorder](https://crates.io/crates/byteorder) and packs everything without any padding and in a straight forward way. I havn't used any of it myself, but it seems it should be compatible to your desired layout. 
&gt; The fact that in C++ overload resolution requires monomorphisation to happen means I don't hold much hope. nitpick: this is true only for dependent template parameters. That is, if any of the function arguments depends on a template parameter, overload resolution happens on the second look-up pass during monomorphization. However, if the arguments to the function are not dependent of the template parameters, overload resolution happens on the first look-up pass. That is, overloads defined after the template are only part of the overload resolution iff the function call is made to depend on a template parameter. This is... bad. Consider this (I am using clang modules here, not the Modules TS): // module aa namespace aa { struct A {}; } // module generic import aa; template &lt;typename T&gt; void cprint(T&amp;&amp; t) { std::cout &lt;&lt; "generic" &lt;&lt; std::endl; } template &lt;typename T&gt; void gprint(T&amp;&amp;) { aa::A a{}; // non-dependent name: look-up happens here cprint(a); // calls only overloads defined previously } // module customization import aa; namespace aa { // defines a new overload that can be found via ADL void cprint(A&amp;) { std::cout &lt;&lt; "custom" &lt;&lt; std::endl; } } // namespace aa // executable 1: import customization; import generic; // that is, aa::cprint is imported before gprint // so it participates in OR during gprint, and // since it is a better match, it will be called int main() { aa::A a{}; gprint(a); // prints custom return 0; } // executable 2: import generic; import customization; // that is, aa::cprint is imported after gprint // so it does not participate in OR during gprint int main() { aa::A a{}; gprint(a); // prints generic return 0; } Note that if we make the function call depend on a template parameter, e.g., by rewriting `gprint` generic to look like this: template &lt;typename, typename T&gt; using second = T; template &lt;typename T&gt; void gprint(T&amp;&amp;) { second&lt;T, aa::A&gt; a{}; // dependent name: look-up happens during monomorphization cprint(a); // functions defined after gprint can participate in OR } then `custom.cprint` will always be called (as long as `custom` is imported) independently of when `custom` is imported because it is always a better match. Anyhow, the important thing is that if you want to speed up template compilation by caching: - there is little that you can actually cache while looking at the `generic` module in isolation, because even for function calls using non-dependent template arguments, the function that will be called depends on what code has already been imported in the modules where `gprint` is used. - whatever caching you do cannot be easily shared between TUs, because the functions that will be called depend on the functions that have been imported before the generic code, and potentially after as well if the calls depend on template arguments. Since this other code can change between TUs, caching would need to take this into account. So all in all, I am a bit more pessimistic than you are, and would not expect any "order of magnitude" improvements in the compilation speed of heavily generic code in C++ beyonds the win's that PCH already deliver (I expect something in the order of 1.2-1.4x speed-ups when enabling modules, and am currently getting 1.2-1.3 speed-ups with clang modules). /u/gabrieldosreis, can you give me some hope?
Great! This is exactly what I'm after. Thanks a ton!
Don't see how this applies specifically to web development.
[removed]
[removed]
Mostly it's aesthetics; i find it slightly easier to think about mutating an object than replacing an object with a fresh version. But then i don't have ten years of functional programming experience! I wouldn't expect there to be a performance difference in release-compiled code, but that might be optimism on my part. There is a small impact on the implementation of Game, because the return-a-new-version-of-yourself involves a move. There are some types which [need to be unmoveable](https://github.com/rust-lang/rfcs/issues/417), and i think that means you couldn't use those in the Game. I believe such things are rare, though. 
I get that it's a joke but I think there's an actual case to be made for making Fibonacci an iterator. It gives a ton of flexibility for almost no increase in complexity compared to the iterative version: [playground](https://play.rust-lang.org/?gist=8dddd54f9ee000f410882c47529ab98d&amp;version=stable). Also pinging /u/petar02 in case you're interested in yet another implementation.
Ironically, one of reasons it has become so important was recurring complaint about usage of GC in standard library. D developers figured out that best way to avoid deciding how standard utilities should allocate memory is not allocate it at all, making everything 100% lazy and leaving decision up to library user. On the other hand, Rust seems to suffer from same problem as C++ - because calling `malloc` is not considered as outrageous as using GC, devs will casually do so even when alternative is possible.
I'd love to, but I'm low on time. `fast_fmt` optimizes pushes too. :)
&gt; I'm just surprised this pattern doesn't flatten to a single static string. I'm not anymore after I learned that it uses trait objects internally. Could you try it with `fast_fmt`? I'm low on time to do it myself but I'd be interested to know, if you can help me with it.
Oh, certainly. I intentionally over-complicated it when playing with number traits to get an idea of how being generic over integer types worked. I also didn't want it to wrap on overflow like yours does on the 94th number, hence the checked addition.
I hadn't thought of the case of unmoveables, that's an interesting case, thank you! &gt; There is a small impact on the implementation of Game, because the return-a-new-version-of-yourself involves a move. I'm not sure I follow, can you clarify? In the code I've written, I didn't really create a new version of the `GameState`, but within my `fn step(self) -&gt; Option&lt;Self&gt;`-style method, modified some fields of `self` and then returned `Some(self)`.
It's saying that Rust gets rid of more overhead for web dev by not requiring Apache or something else that runs PHP on a VM. That is fairly specific to web dev.
The [structopt](https://crates.io/crates/structopt) crate will automate quite a lot of this. I think the OP was imagining writing something like: // once let options = parse_args(); // many times if let Some(flavour) = options.get("flavour", TYPE_STRING) { init_flavour(flavour); } With structopt, i think this would look like: // once #[derive(StructOpt, Debug)] #[structopt] struct Options { // many times #[structopt] flavour: Option&lt;String&gt;, } // once let options = parse_args(); // many times if let Some(flavour) = options.flavour { init_flavour(flavour); } 
For things like resuming downloads, check the [reqwest HTTP header list](https://docs.rs/reqwest/0.7.3/reqwest/header/index.html) and [`Client::execute`](https://docs.rs/reqwest/0.7.3/reqwest/struct.Client.html#method.execute), which you can use with `RequestBuilder` to construct requests with custom headers. This should get you pointed in the right direction.
Great! We would really love a CLI version of this, because we're not allowed to paste some of our internal JSON into web apps. Thank you for building something cool and useful!
This became very clear when playing with `miri` (which is an experimental direct interpreter of output MIR). Granted, the execution speed would win no awards! However, `miri` is intended for future compile-time metacompilation experiments - it would need an interface to libc with `libffi` to do anything 'useful'
My understanding is that every time you pass something as a parameter, return it, or assign it, that's a move. This code has three, plus it takes a move to call it: fn play_game(game_state: GameState) -&gt; Score { let mut result = GameStepped::Cont(game_state); // moves loop { match result { // does match move? GameStepped::Cont(game_state) =&gt; { result = game_state.step(); // moves in and out } GameStepped::Done(score) =&gt; { return score; } } } } I believe the compiler will be able to reuse a single piece of memory across the moves, so there are no physical copies happening, but i think this is still incompatible with unmoveable types. I'm actually not sure about this, though. 
This article seems to support not using a production webserver (like Apache or Nginx)as a proxy in front of a Rust web app. To me, that seems reckless. Yes, using Apache costs some performance, it's handling the details of the HTTP protocol. That is a lot of complex work to handle the protocol correctly. I haven't seen any crates for web servers that are production ready. I'm not expecting them to be production ready anytime soon: the simpler, lower-level libraries are currently being worked on this year. It will be years (and probably more than two years) before production-ready Rust webserver software is available, if there ever is enough momentum to go that far. Am I'm being pessimistic, instead of realistic, about the time needed? I don't think so, but I could be wrong. 
&gt; Testing something that uses `stdin` could be harder than testing something that reads from something that implements `Read`. Absolutely. Although testing is a good reason to make the core logic generic over any `Read`er instead of hardwiring to `stdin`.
You're right. I apologize for being vague and uncharitable. I think the volume is the real problem. You go into an RFC thread and there are 200 comments, some of which are out of date because the RFC itself has been updated several times. The FCP proposal (the one with the checkboxes) is somewhere in the middle. It's hard to see whether conversation died down before FCP or not. 100 comments are hidden because even Github isn't up to the task of reading them all. Has [my concern](https://github.com/rust-lang/rfcs/pull/2115#issuecomment-327606292) already been raised, discussed, and solved and/or deemed insignificant? Probably, but I'm not sure. So I feel like a rude curmudgeon for (1) opposing an RFC that's already headed towards merging and (2) objecting without reading every comment. And then every open RFC is like that... but I know that this meta-problem is already known and will be addressed in the near future.
&gt; That is a lot of complex work to handle the protocol correctly. You don't have to do it yourself. What's important is that having single application and language doing everything brings a lot of opportunities for optimisations for your specific usecase comparing to using separate applications. &gt; I haven't seen any crates for web servers that are production ready. Hyper is used in production, without any more issues than other libraries, therefore its ready, at least by the standards of those who use it (me included). It could be better, but it doesn't mean it's not usable already.
I consider asking a question that blocks on user input while running rustup to be very disruptive. That's like an ad you have to click away when visiting a website, except it's even worse, because it is in your terminal. It would be less invasive to just post a link where you can register for the survey. (It would still be an ad in your terminal though.)
Doh, of course! I was so wrapped up thinking about my game code that I (stupidly) took "move" to mean a move in the game :).
Still failing to properly display Twitter link previews on Reddit.
It's not for any external use, purely something internal to `rustc`. https://github.com/rust-lang/rust/pull/44049/files#diff-c43feebc962041cd0b8d0d5679160afdR477
I'm surprised by how everyone's missing the point and giving this silly advice about using SeqCst. The point of this is that it's a counter, and nothing else. You'd use it for something like tracking metrics, eg the number of tasks performed over time; you need to never lose an increment, because that would make the count inaccurate, but there's no need to synchronize it with any other operations on memory. As the Nomicon says, relaxed is the right ordering for this. Given how simple this is, i'm not sure i'd bring in a crate to do it. But it's certainly something i would pull out into its own struct in my own codebase. If you want to do this at serious scale, even a relaxed integer might not be fast enough - this is why Java has the [LongAdder](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/atomic/LongAdder.html) which maintains unsynchronized thread-local counts, and only adds them up when queried for a total (or something like that). 
If you search the `rust-lang/rust` repo for "rustc_allocator_nounwind", you'll find [this line](https://github.com/rust-lang/rust/blob/d2d50691aadfb9e25c8c3d9a1d71a8c79607c5b2/src/librustc_trans/attributes.rs#L122) which just sets the LLVM `nounwind` attribute on the function. The LLVM Language Reference says this about the attribute: &gt; This function attribute indicates that the function never raises an exception. If the function does raise an exception, its runtime behavior is undefined. However, functions marked nounwind may still trap or generate asynchronous exceptions. Exception handling schemes that are recognized by LLVM to handle asynchronous exceptions, such as SEH, will still provide their implementation defined semantics. From https://llvm.org/docs/LangRef.html (search for `nounwind`)
I don't see anything wrong with the approach you're taking there. It looks right to me, and honestly, this sounds like a fun project! If you run into a specific issue, I might be able to help more!
Still, only in very specific use-cases would that level of performance be required. I love rust, but it would definitely not be my go-to for backend web development. Sure, you'll save some CPU cycles, but it will also take a lot longer to write.
Sure, Python or Ruby packages are much more ergonomic, and PHP is so widespread that it's well supported (good god it sucks though). But if a client really wants or needs speed, Rust is becoming a good competitor.
The Rustonomicon has a [whole section on this](https://doc.rust-lang.org/nomicon/repr-rust.html).
The jump from Python or Ruby to rust is much more dramatic than say, a jump to Java or .NET, which both have far more mature tooling and frameworks and a plethora of documentation. Also, I would argue that it would be irresponsible to do backend web development for your clients in rust unless they were made well-aware that they would have a hard time finding people who could maintain their software once delivered.
Yeesh, that issue is still around? I guess it's about time I find some time to go work on cargo-tarpaulin linker issues again...
Funnily enough I stumbled across Crystal for another reason today. It seems a bit like Crystal has the power of Rust traits without the explicitness (e.g. the `shout` example on [the front page](https://crystal-lang.org/)). Not looked into it much but looks like a neat language!
This is from 2015 and has been [discussed back then](https://www.reddit.com/r/rust/comments/35pn5a/criticizing_the_rust_language_and_why_cc_will/).
[Last posted 2 years ago](https://www.reddit.com/r/rust/comments/35pn5a/criticizing_the_rust_language_and_why_cc_will/).
Sorry, my bad, could'n find by. Deleting.
That solver doesn't seem very fast. Did you compile with release mode? I made a sudoku solver during my holiday too in Go (I couldn't use Rust because I didn't have any internet so I couldn't fetch any crates. Go has a much more versatile standard library. If there's any way to prefetch any often used crates for offline use, let me know). [My solver](https://github.com/Maplicant/sudoku_solver) was able to do a very hard puzzle in 2ms using very straightforward code (I just use an array of length 9 instead of fiddling with bits in an u32) and without any special compiler flags, which corresponds to 1000 puzzles in 2 seconds. Is Go really that fast, or is there some hidden cost to something in Rust that makes it comparable to Go? EDIT: I just looked up what backtracking is. My algorithm isn't really a pure backtracking algorithm. It tries to deduct as much information as possible, by keeping a list of possible options at every square. If I apply a placement to the grid, I remove the option from the possibilities in the other squares. Only when there's nothing more to deduct, my program starts backtracking. Still seems like the actual lesson here is to perfect your algorithms before you start tweaking with compiler flags (not that my algorithm is perfect).
Not my post, I just saw it. $ cargo build --release is in the text. &gt; If there's any way to prefetch any often used crates for offline use, let me know You'll want to look at cargo-local-registry and cargo-vendor &gt; Is Go really that fast, or is there some hidden cost to something in Rust that makes it comparable to Go? Sudoku solving is np-complete; it's impossible to compare without knowing the exact algorithms both used.
Oh absolutely, hence why I said "becoming a competitor". It is arguably better than C++ in a lot of ways, but it doesn't have the maturity to be fully acceptable for production or widespread use. But neither Java and .NET have the same performance, which was what my argument was about.
I tried your solver on my test data, but I get an error from the Go runtime: /tmp/sudoku_solver$ head -n1 very_hard.txt | ./sudoku_solver 12#|3##|### 4##|###|3## ##3|#5#|### ---+---+--- ##4|2##|5## ###|#8#|##9 #6#|##5|#7# ---+---+--- ##1|5##|2## ###|#9#|#6# ###|##7|##8 Solving... fatal error: all goroutines are asleep - deadlock! goroutine 1 [chan receive]: main.(*Sudoku).Solve(0xc42007a000) /tmp/sudoku_solver/sudoku.go:147 +0x207 main.main() /tmp/sudoku_solver/main.go:17 +0x22d Am I using the program wrong? I only replaced the periods with hashes in the input file, is there more I need to do? Edit: If my usage of your program is correct, it seems that your solver can complete only some of the very hard problems, but not all. In my set of 1000 puzzles, your solver can solve 131.
Yep! Resuming downloads is nothing special, you just need to send a `Range` header stating that you want the download to start at some offset instead of 0. That's how it's done in HTTP, regardless of language or library.
You can write an error handler for example for 403 unauthorized and redirect to `/login` in the error handler In your `FromRequest` implementation of User you need to fail with 403 instead of forwarding.
... how often does a set only include nine elements? maybe use a bloom filter instead? (the data structure not the graphics technique)
Backend web apps are rarely constrained by CPU performance. When they are, though, scaling out is usually cheaper and/or easier than taking more development time to fix the issue. This might be relevant for a company of the scale of Facebook and Google, or hobbyists, but for Joe web-developer (that is significantly time-constrained), more ergonomic alternatives make much more sense IMHO. Deployment is also easier and safer if your webserver isn't coupled with your backend code like the diagram seems to imply.
`target-cpu=skylake` Even better: `target-cpu=native`. This one doesn't require you to find out the name for the CPU model you are using.
The source code for the website is public https://github.com/transform-it/transform-www. Be assured that I am not tracking anything except page views. But I understand company policies. I have separate packages here https://github.com/transform-it . You can use them as you deem fit. Or a better alternative would be to run the website on your local server ?
Thank you all for your help! This helped me really much :)
&gt; I now use a u32 to represent a set of 9 elements. Why not a u16? To take this idea even further, it's possible to use even less memory, and represent the entire board as a bitmap, so each cell only uses 9 bits. I'm not sure if this is a good idea though.
Good tip. I really wish `rustc -Ctarget-cpu` had and equivalent to `gcc -mcpu` to short cut for *my machines cpu*. This would be useful if `cargo install` did this + `lto` for you on local builds. Edit 1: It does lol
&gt; my machines cpu that's `native`.
Ah, yeah, that's true.
I had a type like that, and this is what I did: 1. Document that you really should call `finish` before instances drop. 2. In `finish`, set a "finished" flag on the struct. Finish does its work, then returns `Ok(_)` if it worked and `Err(_)` if it didn't. 3. `drop` checks the finished flag. If it it's set, `drop` doesn't have any work to do, and returns (the most common case). If it's not set, `drop` calls `finish` (and in debug builds, prints a warning). If that succeeded, then it returns (the next most common case). Otherwise, it `panic!`s with the error from `finish` (hopefully the least most common case). This way, programmers who read the docs have no reason to fear an unexpected abort, those who don't will see the warning, and, since not every programmer will have the time or expertise to add those `finish` calls, it should still work in the most common case, without being a source of heisenbugs thanks to the message on stdout.
Thanks! `native` isn't listed under `rustc -Ctarget-cpu=help` just fyi.
Can you file a bug?
I actually tried to use `u16` to represent my sets, but my quick and dirty experiment suggests that it makes performance worse. Using a `u64` instead of a `u32` make little difference. Here is a table of timings I captured, running the solver on the very hard problem set 10 times. u16 | u32 | u64 ----|-----|---- 22.25|17.12|17.85 21.94|17.74|18.40 22.50|16.94|18.85 21.97|16.94|18.43 22.39|17.74|18.04 21.70|18.00|18.80 22.25|17.01|17.57 22.61|16.84|18.53 23.09|16.99|17.65 21.78|17.26|18.80
&gt; Much correctness debugging happens as part of cargo check, which is relatively fast (a few seconds). You are correct, but when you're building/running unit-test-heavy code then the long wait for tests to build still hurts.
What external crates do you need for a sudoku solver?
[Sure](https://github.com/rust-lang/rust/issues/44393)
Hi, I have the following code use std::fmt; pub struct MyRange&lt;Idx&gt; { pub start: Idx, pub end: Idx, } impl&lt;Idx: fmt::Debug&gt; fmt::Debug for MyRange&lt;Idx&gt; { fn fmt( &amp;self, f: &amp;mut fmt::Formatter ) -&gt; fmt::Result { write!( f, "Nothing seriously" ) } } fn main() { let start:f32= 1.2; let end:f32 = 5.; let rng2 = MyRange { start: start, end: end}; println!( "{:?}", rng2 ); } I want to understand what *impl&lt;Idx: fmt::Debug&gt;* does. Specifically the *Idx: fmt::Debug* part and why it is required. Thank you.
Could you publish your puzzles somewhere? I might want to try a Sudoku solver for programing exercise and some test data would be helpful.
Belst, could you give me some example codes, I am still new to Rocket.
&lt;3
Look at problems.txt and very_hard.txt at https://github.com/gnuvince/sudoku-rs 
&gt; I also now know about the insane awesomeness of indicatif for CLI progress bars. Makes me wish I had something to code with progress bars somehow.
The first code, s1 and s2 have "Hello" as value. In the second code, it is "Hello World". /// just kidding. a newbie here. have a nice day.
I've not witnessed Rust being slower at implementing websites than any other language though. The ease of developing a website is entirely related to the libraries that you have available. Rust already has some really nice web frameworks, like Rocket, and Ive been able to write complete websites from scratch in it within a day. So, it's got the capabilities. Usual setup is: - r2d2-diesel for pooled database connections via the Diesel ORM - handlebars &amp; horrorshow for templating - rocket for managing routes, states, requests, and responses - serde for serializing / deserializing JSON and mapping database queries to Rust structures and vice versa
Anyone using rust-rdkafka? There's a small chance it has a memory leak, there's a better chance the code my team wrote to use it has a memory leak. Follow up question, what's the best way to go about debugging a memory leak in a Rust program? strace is showing me some mmaps that never seem to get freed: `mmap(NULL, 2097152, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f6ce4800000` I have no idea how to trace that back into Rust though. ltrace crashes the program. so that's cool. EDIT: rust-rdkafka 0.11
&gt; Sudoku solving is np-complete; it's impossible to compare without knowing the exact algorithms both used. Note that this is true of any problem, not just NP ones.
It means the `impl` is generic over a type `Idx`, where `Idx` implements `fmt::Debug`.
It's declaring Idx as a generic parameter and specifying that it must implement Debug for this implementation to be available
It's called trait bound. More info in the book: https://doc.rust-lang.org/1.8.0/book/traits.html
That's true; I was trying to emphasize that the variability here can *definitely* be very very large.
Got a screenshot? Does it work when stylo is off? (layout.css.servo.enabled in about:config) (I'm not clear what you mean by "twitter link previews", i don't ever recall seeing stuff like that) At this stage everything should be working correctly, i don't think there are any known bugs like that.
Less jarring.
Thanks for the help. I am also a newbie.
Thank you. Do all of these have unique solutions?
I do not know; my solver stops when it finds a solution. Edit: Oh, I should mention that very_hard.txt actually only has 20 unique problems that I copied 50 times to get a comparable data set size to problems.txt. I should remedy that.
You probably want `RUSTFLAGS="-Ctarget-cpu=native"`, not `cargo rustc -- -Ctarget-cpu=native`. The former applies the flags to all compilations, even dependencies, the latter only does it for the topmost crate in the deptree. 
The idiomatic way of short-circuiting out of functions on errors would be to make your inner `visit_and_replace` return a custom `Result`, and replace all of your `match` statements where you return on the `Err` case with the [question-mark operator](https://doc.rust-lang.org/book/second-edition/ch09-02-recoverable-errors-with-result.html#a-shortcut-for-propagating-errors-): `x = something()?;` Then, wherever you call `visit_and_delete`, you can do `match` on the result of that call to print the appropriate error. The only complicated here is defining the appropriate `Result` type, which could end up quite verbose if you have errors of different types. This wouldn't work for the cases inside the loop though, where you want to continue.
Man CPUs are weird.... Any idea why this might be?
Alas, for me there is a difference. Last I checked, [bytecount](https://github.com/llogiq/bytecount) got SIGILL on my Core m3-6Y30 with `native`, but ran fine with `skylake`.
I wrote about this and more at [Rust Performance Pitfalls](http://llogiq.github.io/2017/06/01/perf-pitfalls.html).
&gt;The ease of developing a website is entirely related to the libraries that you have available This is the kind of statement that gives rust evangelists a bad name because it fails to acknowledge the trade-offs that rust makes in order to be safe and performant: All the libraries in the world don't save you from having to worry about ownership semantics and compile times, which gets in the way of "just getting stuff done." The trade-offs are well worth it for *a lot* of things, especially where safety *and* performance are required. However, that trade-off isn't worth it for the vast majority of backend web development work, because very rarely are these types of applications CPU bound. 
Where does the time go? My guess would be that it is jumping around in memory in the b-tree. If data was packed together more (larger tree nodes with multiple nodes in contiguous memory) it is likely that the bit shifting would pale in comparison to the benefits of locality. 
Have you not seen a Rocket application? There's zero concerns about ownership semantics. The library takes care of all of the lesser details for you. You only have to worry about getting data and creating HTML pages. Not something that requires much work, to be honest.
Thanks, I've added an addendum with a link to your comment.
It says: let MyRange implement `fmt::Debug` like this, if the type Idx implements `fmt::Debug` already. It could also be written as impl&lt;Idx&gt; fmt::Debug for MyRange&lt;Idx&gt; where Idx: fmt::Debug It's not required for the actual implementation in your example, but would be necessary if you include start and end in the write statement like write!( f, "start={:?}, end={:?}", self.start, self.end) 
Have I seen contrived examples that aren't representative of a real world application? Yes. And even some of those expose some of the less-ergonomic aspects of the ownership model, like having to declare lifetimes.
It is basically saying, "if I have a type `Idx`, and `Idx` implements `Debug`, then `MyRange&lt;Idx&gt;` implements `Debug` using the following impl"
Yep, that's pretty much what I was thinking of. It might also be worthwhile to only do the panic if the runtime isn't already unwinding, so as to prevent a double-panic. It's a fairly easy thing to check ;) https://doc.rust-lang.org/std/thread/fn.panicking.html
you'll have to check what available targets you have with: rustup target list and make sure armv7-unknown-linux-gnueabihf is listed. Here's a page that explains the error you're getting https://os.phil-opp.com/cross-compile-libcore/ . Basically since you aren't using a custom target (a json file) you're using the official target from rustup, which should be armv7-unknown-linux-gnueabihf. It does install correctly for me on nightly, so not sure what the issue for you is. Especially since you said it compiled before, which is weird.
`error-chain` takes care of all those other cases though, and it's a joy to use. :-) For this function, I defined a macro (also used elsewhere). It's not that much less verbose if you count the macro, but the function itself became way nicer. macro_rules! try_or_warn { ($r:expr, $final:expr, $fmt:expr) =&gt; { try_or_warn!($r, $final, $fmt,); }; ($r:expr, $final:expr, $fmt:expr, $($arg:tt)*) =&gt; { match $r { Ok(val) =&gt; val, Err(err) =&gt; { write!(std::io::stderr(), $fmt, $($arg)*).unwrap(); eprintln!(" (error: {})", err); $final } } }; } fn clear_target_directory() { fn visit_and_delete(dir: &amp;Path) { let rd = try_or_warn!(std::fs::read_dir(dir), return, "Unable to read directory {}, skipping from delete pass", dir.display()); for entry in rd { let path = try_or_warn!(entry, return, "Error while reading directory {}, skipping from delete pass", dir.display()) .path(); if path.is_dir() { visit_and_delete(&amp;path); try_or_warn!(std::fs::remove_dir(&amp;path), continue, "Unable to delete directory {}", path.display()); } else { try_or_warn!(std::fs::remove_file(&amp;path), continue, "Unable to delete file {}", path.display()); } } } println!("Removing target..."); visit_and_delete(Path::new(&amp;CONFIG.target_directory)); }
Idx implements Debug?
Lifetimes are effectively never required in a Rocket application. I have no idea what you've seen, but I'm calling complete BS on your statements. Both in what you've claimed to have seen with Rocket, and that ownership is difficult to reason about in the first place. Maybe it's because I'm young (24) and have a malleable mind that made quick work of internalizing Rust's rules, but in my 2+ years of writing software with Rust, it's never been an issue. That's even after I've developed a complete next generation system shell from scratch: Ion. Neither ownership, nor lifetimes are a difficult concept to grasp and exploit in practice. I've been able to write software faster with Rust than I ever managed Go or Python, for numerous reasons. And as for writing web applications with the likes of web frameworks like Rocket, ownership never really gets to get utilized in practice. Only data within a managed State gets to live between requests, so your data is effectively already going to be both short-lived and allocated. Lifetimes don't get a chance to come into play. Rocket has it's own mechanisms for taking care of all those ownership details for you.
What makes you suggest that? Bloom filters are usually used for *much much much* larger sets. They can give false positives, and they involve lots of hashing.
Sure. I don't know how it renders without stylo, but here's an example comparing it to chrome. This is with RES on both. Not sure how it works without RES either. [linky](http://i.imgur.com/5DCb4gK.jpg) EDIT: I disabled stylo and the old rendering engine displays the same thing as stylo, so I guess it's always been *advantage: chrome*
If you don't use nginx/apache what do you use then? Let me remind you almost all web frameworks out there are not using hyper's async features so far. So your whole web server becomes a very easy ddos target without something like nginx in front of your rust server. Also the whole tooling around Nginx is much more mature in general. I think you don't name one of the best points for rust in this deparment. It's by far more expressive without going into a dirty dynamic mess. With the whole attribute system we can write very readable code which performs very good. I don't know much languages who can do the same. In C# you write readable code first but If It performs bad you may change It to perform beter. In rust i rarely end up having this problems.
Can you turn off stylo and check? This could very well be a difference in the RES plugins.
You can use LLVM's leak sanitizer with a nightly compiler: `cargo rustc &lt;cargo opts&gt; -- -Z sanitizer=leak` Edit: also it looks like you need a linux machine to use it at the moment error: LeakSanitizer only works with the `x86_64-unknown-linux-gnu` target Haven't used it before, let me know how it goes.
Shouldn't this be a compiler bug? 
Yeah, it could be a difference between plugins. See edit.
Or backend (llvm), but I'm not sure if it is a bug. I hope it's not a behavior.
&gt;Lifetimes are effectively never required in a Rocket application. I have no idea what you've seen, but I'm calling complete BS on your statements. [From the official Rocket docs](https://rocket.rs/guide/requests/#field-validation) &gt;Neither ownership, nor lifetimes are a difficult concept to grasp and exploit in practice. *Actual surveyed data* shows that Rust's ownership semantics are one of the harder aspects for newcomers to grasp. 
It's too bad that it doesn't use a Rust TLS implementation though. Depending on the system TLS makes the application impossible to compile statically.
That's a shame. I had many situations where I _thought_ I didn't care about Q&amp;A until someone asked a question I never thought about, and I have learned something useful in the process.
Shouldn't rust default to native? If not, why doesn't it?
It unnecessarily restricts the target platform to only your computer; this harms your ability to redistribute binaries.
That's the book for 1.8; you'd want https://doc.rust-lang.org/book/first-edition/traits.html now, or https://doc.rust-lang.org/book/second-edition/ch10-02-traits.html
I'm not sure I understand your explanation and other ones. I'll try again tomorrow.
Hi guys, second question today. I'm reading the rust book here: https://doc.rust-lang.org/book/second-edition/ch03-02-data-types.html It says the following: &gt; What happens if we try to access an element of an array that is past the end of the array? Say we change the example to the following: &gt; &gt; ... &gt; &gt; The compilation didn’t produce any errors, but the program results in a runtime error and didn’t exit successfully. The way I see it: This could be caught at compile time. Why allow something that is going to panic to be compiled in the first place? Shouldn't the compilation fail instead?
Poor example. The annotation in that example is only required due to the Error associated type containing a string reference, and it's not that confusing to figure out, given the example. Lifetime of the error type is being assigned to the lifetime of the input. This also isn't what you'd normally do in the production Rocket application though. Raw strings aren't meant to be thrown around. Instead, you would normally use a dedicated error type, and then there'd be no usage of lifetimes at all in that example. &gt; Actual surveyed data shows that Rust's ownership semantics are one of the harder aspects for newcomers to grasp. You're confusing some concepts here. Primarily, the difference between a feature being difficult to use, versus something simply being a 'harder aspect' for a newcomer to learn. As it turns out, lifetimes are simple to reason about and employ, but no other language has the capability to assign lifetimes, so of course most newcomers are going to find it to be a 'harder aspect', simply due to the unfamiliarity with the notion. And that's especially so given that many newcomers to Rust have Rust as their first entry into systems programming languages. But that doesn't mean that using lifetimes are difficult. If you're developing a web application in Rust, you're not a newcomer to Rust. You've likely already been exposed to the lifetimes concept in the book or in the examples provided. As it turns out, the Ion shell I've developed makes heavy usage of lifetimes where possible, but at no point did I ever find using and managing lifetimes to be hard. And as far as web development goes, my point still stands that no idiomatic Rust web application is going to making extensive use of lifetimes. If you want to see lifetimes in action, you should check out the Ion shell. I've developed my own website with Rocket, which is around 3000 lines of Rust, but guess what? There's effectively no usage of lifetimes anywhere. Basically, your argument is that we shouldn't use bicycles for travel because it's hard to learn how to ride a bicycle. Learning is a one time thing compared to a lifetime of use.
Surely this is down to the affinity to the native word size of the u32/u64 bit types on the CPU?
Wrong subreddit? Maybe you were looking for r/playrust. This is the subreddit for the Rust programming language.
But if you're planning on creating binaries for other targets, don't you have to explicitly specify those targets anyway? Or are you saying that by default rust will create additional binaries for other platforms?
After thinking it over, I made some changes to escaping and released them as 0.5.0. More here: https://github.com/djc/askama/releases/tag/0.5.0
Consider x86. If I compile using Rust today, I can compile a program, and then run it on a wide variety of x86 CPUs. But if I say `target-cpu=skylake`, I can no longer run it on a Sandy Bridge machine, as Skylake is newer, and supports instructions that the Sandy Bridge machine didn't. (I am both being more specific and less here; those are real hardware names, and in the right chronological order, but I haven't dug into the details enough to know if *specifically* these two CPU revisions have slightly different ISAs. The principle holds.)
Probably, but I have yet to check LLVM's bug tracker, let alone create a minimal repro if it isn't there already.
&gt; Poor example. The annotation in that example is only required due to... You're wrong, but even if you weren't, it's still beside the point. &gt;Primarily, the difference between a feature being difficult to use, versus something simply being a 'harder aspect' for a newcomer to learn. Again, beside the point. Even people familiar with the concept of lifetimes and their purpose, i.e., people who've been writing code in unsafe languages for years, find that they get in the way of just getting things done. You refuse to acknowledge the trade-offs here, which is the point I'm getting at.
&gt; It's too bad that it doesn't use a Rust TLS implementation though. It uses `native-tls`, which uses the OS's implementation of TLS on Mac and Windows, and OpenSSL on Linux. At some point, `native-tls` will presumably also support pure Rust TLS when it's ready.
&gt;Hyper is used in production, without any more issues than other libraries, therefore its ready, at least by the standards of those who use it (me included). It could be better, but it doesn't mean it's not usable already. I wasn't clear enough: what I meant was that There is no web server written entirely in Rust that is trusted to handle public traffic without behind a proxy web server.
It probably has some bugs somewhere. I just hacked something together very quickly to finish my logic puzzle book ;) I'll look into it
Will bindings generated this way depend on glib?
Aha! That makes a lot of sense, thanks!! I guess I was confusing general architectures with specific CPUs
&gt; If there's any way to prefetch any often used crates for offline use, let me know Check out cargo-vendor.
In the future it will even be possible to have these optimizations and still keep the program portable to other micro-architectures: https://github.com/gnzlbg/rfcs/blob/1c94fe7c9d0a3433c8a666fdadf13c17b9f7d4c8/text/0000-target-feature.md
Oh totally. Suffering through that right now (plus the `cargo test` grinds for a while, then tells me about one `example/_.rs` that fails, with some of the errors, then aborts).
Yeah, for "count" metrics, `Relaxed` is good enough. However, counters are used for multiple use cases and `Relaxed` is not a good fit for all of them (see comment e.g. claire_resurgent's comment above). Hm, LongAdder seems like a cool idea, but it's too heavy-weight for what I'm looking for. Still, it would fit under the `AtomicCounter` trait :)
If you paid for the early release do you get the finished (at least pdf) version for free? it's been so long since I paid for it I honestly can't remember.
I have this target, it is listed as (installed).
Link to Amazon for those interested: https://www.amazon.com/Programming-Rust-Fast-Systems-Development/dp/1491927283/
No problem! This is also true generally, like, if you say "I want to compile for x86", do you *really* want something that works the whole way back to the 8008? not likely. So generally, people pick something recent-ish as a baseline...
There are multiple implementations for TLS on Linux, so that's part of the problem. A distribution could be using GnuTLS, Mozilla's NSS, OpenSSL, LibreSSL, or even the recently-released kernel TLS, which exposes an interface to use the kernel for your TLS needs.
Maybe I'm just stupid but everytime I look at rust libraries I spend a lot of time going around to understand what the hell the example code uses in Foo&lt;A,B,C&gt; so I know the concrete types used. I tried using futures and I would be pulling my hair trying to get anything to compile because I have no idea what I need to return.
I don't think that RFC will help here. That RFC lets you specifically write code for given architectures in a way that allows for conditional compiling it when unavailable. You still will have to `-Ctarget-cpu=native` to opt in to this.
&gt; Edit: I think I should have posted this under the questions thread. Should I remove this post? (no)
Any reason you used nix, and not mio for polling? mio would give you Windows support. As well as using kqueue in BSD (macOS) and epoll on Linux.
That's because there is no concrete type. It is a generic type where A, B, and C can be any type. You can use your own types with it, so long as it satisfies all of the specified requirements, such as defining a specific trait that the corresponding generic type must have (in order to access methods from that trait).
This book is great. Hands down my favorite Rust text.
That's something it gets from ruby duck typing; though with ruby IIRC it's implemented in the runtime via dynamic method lookup, and crystal supports doing it at compile time (a nice bit of work to implement that).
Yep. If you think of `Idx` as a placeholder for a type, `impl&lt;Idx&gt; fmt::Debug for MyRange&lt;Idx&gt; {...}` would mean "this applies to type `MyRange&lt;Idx&gt;` for any type that is put in place of `Idx`. Having `impl&lt;Idx: fmt::Debug&gt; fmt:::Debug for MyRange&lt;Idx&gt; {...}` means "this applies to type `MyRange&lt;Idx&gt;` for any type that implements `fmt::Debug` that is put in place of `Idx`".
I'm interested. Where can I find out what material it covers? Is there a table of contents? How advanced does it get? How is it different than https://doc.rust-lang.org/book/ ?
So what's the default?
Try /r/playrust/
Maybe its because it will try to store two u16s in a single 32 bit word to save space, but this comes at a cost of unpacking them? Complete speculation though
I would like it if `cargo build --release` defaulted to `native`, and `cargo package` falls back to the more general form. As in, isn't distribution often more intentional? Can we take that intentionality and use it to have an arguably better default for just building something fast for this machine?
does .cargo/config still have what the tutorial says in it (under rust linker configuration)? You should be compiling no_std, so it shouldn't complain about the target not having rust-std.
Very helpful, especially the bits about buffering file IO. I recently read a lot about the best ways to write files in Rust, and this didn't come up so I'm glad I stumbled on your post.
Well, mainly, I knew the `poll` interface and that implementation was fairly simple to do. I think that using `mio` might get me the higher-performing polling interfaces on Linux and BSD, since I could go from `AsRawFd` to an `EventedFD` and feed that into the poll interface. Honestly, since my main goal was to wait on stdin/stdout data from a process, I am not sure how useful it would be on Windows. But, I could probably have an implementation that is looking for the `Evented` trait that might be able to pull in NamedPipe support for Windows. Looking into `mio` further might be good for the next release.
You can see the table of contents, or buy early access, on [O'Reilly's site about the book] (http://shop.oreilly.com/product/0636920040385.do).
I was more thinking of templates used internally. I don't have an example right now, but I've had functions return things like I mentioned from higher level functions. Which is a nice abstraction, until you need to know the concrete type, or want to use another implementation but the documentation is virtually non existant.
Yes. If you don't use GObject, you can in theory get around that but even then all existing bindings (or bindings code generators) depend on GLib. If you use GObject, well, then you depend on GLib directly. Why would using/depending on GLib be a problem for your use case?
I would file an issue on any public functions that aren't documented.
Federico also wrote a great blog post about GObject-Introspection, the part that is actually responsible for having bindings for basically all languages for free: https://people.gnome.org/~federico/blog/magic-of-gobject-introspection.html
You have your choosers and choosees backwards. `'static` doesn’t satisfy those requirements on the `impl`, and *neither does anything else*. The `impl` you commented out is *less specific* than the one you retained, not more!
It's indeed coming soon, but careful not to take the release date *too* literally. ;) &lt; bstrie&gt; jimb: congrats on the programming rust release date! &lt; bstrie&gt; jorendorff: you too :) &lt; jimb&gt; bstrie: Thanks!! &lt; jimb&gt; bstrie: We're really looking forward to having it DONE &lt; jimb&gt; bstrie: I think it's going to be early October, even if Amazon says Sep 25th
Yeah I had a few stabs at it but feel like I made no progress. Also, you are my linker expert! &lt;3 My syntax based stuff should close about 4 existing issues at once though so that's some consolidation
Glad to be helping.
I completely agree that blocking the install on a question would be annoying. I see no reason why this couldn't be asked while rustup is downloading and installing Rust, adding no delay.
NB. The MIR-&gt;LLVM IR step is 1:many (each monomorphisation of generic functions), which is why MIR optimisations are likely to be helpful: they happen before the amount of code blows up.
"Changes to the AVR Target "This release consists mainly of bugfixes and implementations of features required for compiling basic Rust programs. * Enable the branch relaxation pass so that we don’t crash on large stack load/stores * Add support for lowering bit-rotations to the native ror and rol instructions * Fix bug where function pointers were treated as pointers to RAM and not pointers to program memory * Fix broken code generation for shift-by-variable expressions * Support zero-sized types in argument lists; this is impossible in C, but possible in Rust"
Part of the RFC allows for runtime selection of which version of the code to run. And there is already work to make it a simple function tag to get automatic speedup when the machine supports it and fallback when it doesn't: https://github.com/parched/runtime-target-feature-rs
Yes.
Right, but that is _still_ not the same thing as here. Here it's all about _asking_ the compiler to run optimizations, target-feature is about manually optimizing code for certain targets. These two are orthogonal concerns.
It's not redundant (if you're referring to the "foo exists?" println) because Option is part of the definition of [Iterator::filter_map](https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.filter_map).
Unfortunately, there are limits on this kind of type inference. It looks like their closures require type annotations, for example, and they've reported pretty serious performance issues with compiling projects greater than about 40k LOC right now. This isn't to criticize the project, which is doing good work, but these kinds of limitations are well understood by PL theorists. My impression is that they haven't really engaged much with that work.
It might be that the framework itself may not require understanding ownership semantics and borrowing etc, but you can't deny that it's something you have to deal with almost any larger Rust program, on the web or not. I'm about 10x less productive in Rust personally compared to, for example, Ruby, and it's all to do with me having a hard time re-orienting my programming to take ownership into account. It's a lot easier now, but I don't think it can ever be as easy as a GC-ed language is.
I think Windows 8 said "We'll no longer support anything without SSE2 instructions". I think SSE2 is supported on all x86_64 CPUs so it's a safe one to always compile for.
Indeed. It's almost always database I/O latency which constrains any non-Google/FB-scale webapp. It almost always makes sense to choose a language that gives you as much "coding speed" as possible. (Whatever that is language is for you.)
Rust supports 2 x86 targets. The 32bit target is i686 which has a reasonably modern feature set (~1995 Pentium Pro, includes SSE). The 64 bit one is what probably most people use and that gets to assume a fairly modern feature set (~2003 AMD Opteron, includes SSE2). After SSE2 there's been a proliferation of SIMD extensions so dynamic dispatching of different code versions for different CPUs is more interesting for high performance code.
Cargo package doesn't matter, as its source only.
target-feature doesn't have to be manual at all. `#[target_feature(enable = "avx2")] fn foo() {...}` will compile `foo()` with avx2 enabled the same way doing `-C target-cpu=native` would. All the heavy lifting of actually using AVX2 instructions gets done by LLVM with the added advantage you can now compile `foo_avx2()` and `foo_basic()` and figure out which to use depending on the result of `std::target_feature("avx2")`.
It's also Sudoku so each square is a set of 9 elements, as is each row and column. Bloomfilters are not at all the right solution. It's like trying to use a hammer for a screw. You can make it work but it's not going to be easy or worth it.
If you don't care about performance too much, you could just put everything behind `Arc&lt;T&gt;`...
Right, but, again, this goes in the code, which is different from the issue here, which is getting rustc to optimize a program for your machine. At best it's just moving an annotation from a rustc flag to your code, which isn't an improvement because now you have to annotate all your code. target-feature is a great RFC and I was really happy to see it sprout up, but it's not really an improvement on this situation.
Here's my perspective: there are several IETF RFCs defining the HTTP protocol: * [RFC 7230](https://tools.ietf.org/html/rfc7230) for Message Syntax and Routing * [RFC 7231](https://tools.ietf.org/html/rfc7231) for Semantics and Context * [RFC 7232](https://tools.ietf.org/html/rfc7232) for Conditional Requests * [RFC 7233](https://tools.ietf.org/html/rfc7233) for Range Requests * [RFC 7234](https://tools.ietf.org/html/rfc7234) for Caching * [RFC 7235](https://tools.ietf.org/html/rfc7235) for Authentication That's a couple hundred pages of documentation for how HTTP is supposed to work. That's a lot of complexity for a web server to implement. How do we develop trust in a crate (or collection of crates) to handle it correctly? 
With `#[target_feature]` you do not need to use the `-C target` options. In fact, that is exactly the purpose of the feature: to enable target specific optimizations in specific parts of the code. The key trick is that you need to do proper CPU detection at runtime in order to choose the correct functions to call. In particular, nothing stops you from calling an avx2 optimized function on a machine that doesn't have avx. (In the best case, you get a SIGILL for executing an unsupported instruction.) The end user benefit is that I can, for example, ship one binary that works in all x86_64 cpus, but will still take advantage of avx2 when it is available.
When should I expect Rust on AVR to be usable (close to out-of-the-box)?
With enough procedural macro trickery a crate could use target-feature to enable you to say "create as many versions of my program as there are micro-architectures and dispatch between them at runtime" but that would be quite crude. The OP knew what the hot parts of his code were, and annotating those functions with some target-feature tags would give him a better result than `target-cpu=native` because he gets a portable program. So it solves most of the same cases where using better instruction sets makes your program faster while not suffering from the portability downside.
&gt; Do you know why Rust is emitting gigabytes of LLVM IR? Huge compile times and binary blow up are almost always blow up from a lot of generic monomorphisations. &gt; If the issue is duplication then perhaps the Rust compiler can recognise when two instantiations of a type are basically the same and only generate one FWIW, Rust used to try to do that sort of sharing (called type_use) but it was quite buggy. It was also before the compiler had HIR and MIR etc, and a MIR version of LLVM's mergefunc pass might be great and not have the same problems/risks of bugs.
Calling SIGILL is always a bug. The question is "where?"
The Rust community as a whole has better things to do than answer "why does this program fail with an illegal instruction on my friend's computer" 10,000 times, for years and years. No other compiler that produces native code binaries turns on model-specific optimization unless you specifically ask it to, and there's no really great reason rustc should break that mold.
I think I misunderstood what the original comment was trying to say, nvm
Ah, I see, I misunderstood your original comment.
Is this going to be on iBooks?
Do you have DNT enabled in Firefox? about:config (privacy.donottrackheader.enabled). Twitter is/was one of the few sites that actually honored DNT. If you have it enabled try setting ito to false and doing a hard refresh of the page ctrl + f5 or ctrl (cmd) + shift + r.
Wouldn't Option&lt;()&gt; work then?
What about `cargo install`? That seems like it shouldn't be used for building binaries for redistribution, anyway.
Yeah, thanks for all the help, I'm almost at the point where it's executing machine code, I just have one more question for now. I'm getting an error saying "cannot move out of borrowed content" during compilation when it gets to the function call that is supposed to return the function to run the x86 machine code. It seems that the transmute function wont take a mutable reference to self as an argument, only a copy. The only way to get around this that I've found is to return a raw function pointer instead of a function which I think is closer to the C way of doing things. But this way I cant initialize the emulator object because I cant create a raw function pointer until the Emulator is initialized. I tried doing it with placeholder functions but I couldn't get the types to match. So if you know a way to avoid the "cannot move out of borrowed content" error, or how to return a pointer to a function that does nothing I think either way would work. The code should be updated on github, I think the most recent two commits are relevant.
A disadvantage of merging code after the fact is that you still generate the code several times (though hopefully avoid exponential blowup). It might be difficult to handle recursive functions. For recursive functions you essentially want a least fixed point: if you have recursive functions f and g, and f calls f, and g calls g, then you want to optimistically assume that the recursive calls to f and g are equal, and from there show that f and g are indeed equal. An advantage is that you could run optimisations first and two functions might turn out to be the same after optimisations even though they weren't the same before optimisations. Another approach would be to keep a record of everything that you needed to know about type T1 when compiling foo&lt;T1&gt;. Then if you compile foo&lt;T2&gt; you first check if the things you needed to know about T1 are the same for T2 (e.g. size and other generic functions and trait methods called on T1/T2). To handle recursion you could pre-insert entries in the record for T2 so that it gives least fixed point behaviour. I bet it would indeed be quite tricky to implement. This type of algorithm seems closely related to context free grammar parsing, where you also want to share execution paths that are essentially the same, and you also do a fixed point calculation to handle left recursion. Perhaps there is a general underlying principle.
I love rust and write everything with it. Each language has its merit, I believe rust is perfect for high performance, and very robust stuff (robustness means less time spent on bug tracking). But it cannot compete (yet) with more established frameworks in terms of time needed to develop (I just had a great presentation of spring and I was really jealous).
Yes. You would then replace .unwrap_or_else(false) with .is_some().
Reading this on Safari Books Online. Fantastic book.
I need to think on this... I think I'm confusing HTTP processing with TCP processing.
~~FWIW, if you buy on O'Reilly you get DRM-free eBooks in multiple formats that you can load in iBook. Probably also better for supporting the writers too.~~ Apparently individual books aren't for sale any more. Womp womp.
Ah, direct from O'Reilly may be a better bet, but Amazon's (Kindle) DRM seems way worse than iBooks... It limits you to only a certain number of "downloads", even if you're logged into the same account, and wiping your iPad and re-downloading the book counts against your download limit. You can revoke them from old devices but it just seems a lot more restrictive. Good advice on getting a proper non-DRM'd eBook direct form O'Reilly though!
I thought there were some surrogates in UTF-16 that aren't legal on their own?
&gt; ... and why it is required. It's not required. You can write: impl&lt;Idx&gt; fmt::Debug for MyRange&lt;Idx&gt; or impl fmt::Debug for MyRange&lt;f32&gt; 
Sounds draconian, and I've heard stories where they remotely wipe files from your devices as well. Happy to own a Kobo without issues like that, though Kindles are quite nice.
I haven't used glib before. I'm interested in GObject introspection because I've to support users who may develop in different programming languages. But dependency on glib may impose some learning or intalling burden on my users, especially those developing on Windows. I've heard it's not easy to intall glib on Windows?
Rust is not ready for production yet, but it has a lot of potential since there are few nice frameworks already, it is fast and has great stability. Maybe you can "type" or "write" your app faster in PHP but it is much easier to write it correctly when you have the compiler check the sanity of your code.
I wonder how much this might improve performance of the compiler itself if one were to build it and LLVM for their specific CPU. This would be for local development only, of course.
My crate's test suite has a collection of sudokus with solutions as well. All solutions are unique. https://github.com/Emerentius/sudoku/tree/master/sudokus
Your code was extremely close. The main problem is that you were trying to use the entire Translation_Cache struct, instead of just the pointer `page` inside. You could `#[derive(Copy)]` on the entire struct, and it would probably work the same, but semantically it seems better to just use `page`. I made two commits. One to just make it compile as-is, and another to show that you don't need the placeholder function. I don't guarantee anything works, but hopefully it does! EDIT: just noticed the commented out line of code, gonna look at that. EDIT2: okay, no longer uses raw function pointers, and the code compiles even with that line of code uncommented. An `Option` represents the fact that a value could be present or it could be missing. For brevity, I just used `unwrap()`, but that will (safely, predictably, and immediately) crash the program if the value is `None`, so I recommend changing that behavior. Some [example code.](https://rustbyexample.com/std/option.html)
My biggest fear with any book on rust is that it will quickly become out of date because Rust is still growing and changing so rapidly. Are the authors making any commitment to updating it or anything like that?
Have you tried inserting a `let () = output`? You'll get a handy error message that will tell you the type it got.
Is there a way to buy directly on O'Reilly? [Here](http://shop.oreilly.com/product/0636920040385.do) I could only find information about either buying it on Amazon or signing up for the O'Reilly Safari and reading it there. Can you buy an owned, DRM-free version on Safari, without having to buy the Safari subscription?
If this were a "dynamic vs static typing" debate, I'd agree with you. This is more of a "garbage collector vs ownership and lifetime semantics" debate.
I recently used the opt-viewer tool in clang land. It really helps to understand what optimizations are getting applied, and which can not be. Would it be possible to implement something like Clang's -fsave-optimization-record in rustc?
I had been completely behind on news -- I didn't know they had stopped selling individual books. That's a terrible shame and I wish it hadn't come to that. Unfortunately I think this does mean that Amazon and other retailers are the only options now outside of Safari, which is definitely not DRM-free.
Is it possible to write raw strings to files? I'm using `toml` to store a small reminder set. Ideally if a reminder contains linebreaks I want to store it as a multi-line string: entry = """ Reminder Note """ I can turn it into a literal string easily enough, but either `toml`'s serialising or Rust's `write` escapes the line breaks and quotes, so I get a hard to read line instead: `entry = "\"\"\"Reminder\nNote\"\"\""` Can this be circumvented?
Boy that subset emoji sure is wild 
It's very unfortunate indeed. I buy DRM-free ePub on a regular basis. For anyone who is planning on writing books on programming in the future I suggest you go with another publisher such as for example Manning. I have fond memories of a lot of O'Reilly books but with the option to buy them in DRM-free ePub now gone I spend my money on books from other publishers instead.
With the cloud computing model of Amazon Web Services and Microsoft Azure, Lower CPU performance requirements would let you use smaller virtual machines. That being said, if the cost/time of developing is significant, then it may not be worth it.
Yup, it blows me away every time! :) I actually was looking for a funkier equivalence emoji, but got lazy after 15 seconds or so of searching.
Oh rust is getting an animal book? How exciting, I'll add this to my collection
Note that Kernel TLS is *not* the entire TLS stack. You still have to use a library like GnuTLS/OpenSSL for handling the handshake. KTLS only makes a file descriptor that transparently handles the TLS data frames so you can use it with other APIs that need file descriptors.
Remind me 1 month
While I agree with much of your concern, I think you're overly pessimistic. In terms of exploits, etc., there are a lot of goodies in Rust that dramatically reduce my concern for this. In terms of performance, etc, that's different, pick your libraries well.
&gt; Hyper is used in production, without any more issues than other libraries, therefore its ready, at least by the standards of those who use it (me included). It could be better, but it doesn't mean it's not usable already. That may well be, but a directly exposed Hyper + Rocket is all fun and games until you catch a bad case of slowloris.
I think this is a completely misguided sentiment. There's no question that you can pick a better language than php for building a website, but to pick rust against rails, pheonix, django, spring, asp.net...? Heck, even Laravel isn't that bad. There's just no way. It's not a viable choice. It's not going to be a viable choice any time soon. Sure, you can mess around with it and that's fun. You might be able to write a couple of trivial microservices with it, but given rust web frameworks aren't even 1.0.0 yet... you're shooting yourself in the foot to use rust for anything serious at this point. I mean, just look at the version numbers on http://www.arewewebyet.org/topics/frameworks/ Are you kidding me? People saying it's rapidly becoming a viable choice for real projects? /me shakes head.
Here's a discussion about the issue: https://github.com/crystal-lang/crystal/issues/4864
I would be careful to judge a framework by a version number, especially with rust projects where most libraries are under 1.0.0 but are still very stable and usable. What do you think is missing in current rust web frameworks? I encourage you to create an issue on project's github or open a discussion.
In general, you shouldn't make such strong conclusions about performance solely based on whether something "runs on a VM" or not. For one, "runs on a VM" is kind of a broad term. Some VMs are standard interpreters (Ruby, CPython) while some use JITs that are much faster (Java VM, recent Javascript VMs). For example, your "Code Optimization" section isn't true for JITs that do even minimal constant folding. And for some execution patterns, a JIT-based VM can often get better performance than normal static compilation. For example, Facebook originally tried normal static compilation for PHP ([HPHP](https://en.wikipedia.org/wiki/HipHop_for_PHP)) but that was replaced by a JIT that ended up being much faster ([HHVM](https://en.wikipedia.org/wiki/HHVM)) because it made optimization decisions based on runtime execution patterns. If you're trying to make an argument about performance, look to benchmarks -- they're much closer to the thing you're actually interested in measuring.
**HipHop for PHP** HipHop for PHP (HPHPc) is a discontinued PHP transpiler created by Facebook. By using HPHPc as a source-to-source compiler, PHP code is translated into C++, compiled into a binary and run as an executable, as opposed to the PHP's usual execution path of PHP code being transformed into opcodes and interpreted. HPHPc consists mainly of C++, C and PHP source codes, and it is free and open-source software distributed under the PHP License. The original motivation behind HipHop was to save resources on Facebook servers, given the large PHP codebase of facebook.com. *** **HHVM** HipHop Virtual Machine (HHVM) is an open-source virtual machine based on just-in-time (JIT) compilation that serves as an execution engine for the PHP and Hack programming language. By using the principle of JIT compilation, executed PHP or Hack code is first transformed into intermediate HipHop bytecode (HHBC), which is then dynamically translated into x86-64 machine code, optimized, and natively executed. This contrasts with PHP's usual interpreted execution, in which the Zend Engine transforms PHP source code into opcodes that serve as a form of bytecode, and executes the opcodes directly on the Zend Engine's virtual CPU. HHVM is developed by Facebook, with the project's source code hosted on GitHub; it is licensed under the terms of the PHP License and Zend License. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.27
Well, it is the cost of speed. For small or medium size projects I do not think it is a problem. People write games is c++, why can't we write a website with rust?
Great point but if rust becomes more popular it won't be a problem anymore. 
In fact, I've learned about `target-cpu=native` [from the PR](https://github.com/rust-lang/cargo/pull/4191) that attempted to implement this for `cargo install`. But it sadly didn't get accepted. There is a small point though to declining it, which is that you sometimes want to change your CPU, e.g. when you do a hardware upgrade or when you go traveling with your HDD and put it into another computer...
Yeah there is at least one epoch coming up, maybe more, depending on when (or whether) the epoch mechanism will be dropped. That's a huge disadvantage in general of epochs, and not really fault of the authors. Just buy the book, and learn Rust as it is right now, and when big changes come, you'll be much better prepared for them.
`stdx` isn't really built for libraries, it's built for absolute newbies writing applications, and for a list to reference when you're wondering "wow, is this really not in the stdlib?". You can just add the crates you need manually. I don't know what will happen to any other crates he owns.
I want to write a library for newbies to cli's -- in the same vein as stdx. It made the most sense to use stdx behind the hood, but maybe not...
What should a web framework have? I don't know, maybe routing, forms, testing, logging, monitoring (that's not the same thing), middleware, authorization, authentication (also not the same thing), database(s), migrations, hot reloading, scaffolding (if required)... ...but you know what? It's really not that important to have them all. You what's important? Stability. &gt; I would be careful to judge a framework by a version number... Just no. Look, if your project is stable, obey semver and commit to an API with a 1.0.0 release. If it's not, or you plan to change your API, then don't. Lets look at rocket for example... hm... &gt; Rocket is web framework for **Rust (nightly)** with a focus on ease-of-use, expressibility, and speed. Gotham perhaps? &gt; 2017-08-09: today we’re releasing 0.1. *sigh~* It's not about the features. go has bugger all features, but its totally viable as a platform, because there's no chance your code today will be broken tomorrow. I have yet to see a rust web framework that has a commitment to stability, that is actually backed *by being API stable*. That's the problem. The excuse is usually 'well, rust is new and good safe apis are hard, so we have to iterate on the API to find an ergonomic API that people like'. That's a totally fair comment; but it's also like saying 'well, you're welcome to build here, but you know, it is a bit of a swamp, so if your house sinks into it, it's cool, just build a new one on top'. It doesn't cut it for anything serious.
Isn't this a bug?
&gt; IMO stdx was a big part of the ergonomics initiative. I don't think this was the case? stdx was an experiment from a while back, and sort of reduced in activity later on. The solutions for this problem the teams were converging on were related to better discoverability on crates.io. stdx is still a good idea and probably should be maintained, but i don't think it's a part of the ergonomics strategy.
Thank you for your answer! Firstly, I have not said rust is ready for production. I agree with you that rust is not ready for real projects. Although I think it is on a path to being available. We have diesel and serde on stable. I think the fastest path is a compromise between a Rocket's easy to use approach and being able to compile on stable. What I mean is, if we get a framework similar to Rocket but usable on stable, the framework may become mainstream and available for small production projects. You mentioned routing, forms, testing etc. You have most of it out of the box in rocket/diesel. We are missing auth and hot reloading, but those are issues on GitHub waiting for solid implementation. I hope in the future rust will be a reliable language for webdev. It is not ready yet but it is on a good path. 
this is actually a good point people are missing. people use cheap vps for lot of websites, so when they can squeeze more performance with cheaper machine.
Since they closed their store recently you can no longer buy early access releases. Unfortunately now the only way to get early access releases is a Safari subscription.
I'm attempting to use the new associated constants with 1.20 and having a lot of fun with the compiler on them. Take the following simplified `impl`: impl Struct { const DEFAULT: Struct = Struct { path: "/home/Azphreal/tmpfile" } } The type definition on that struct field must be `path: &amp;'static str`, because it's a literal string. However, say in the future I want to do something like this: fn get_or_default(op: Option&lt;String&gt;) -&gt; Struct { Struct { path: if let Some(s) = op { s.as_str() } else { Struct::DEFAULT.path } } } Obviously it's not a very well made example because you can get around it, but hopefully it's enough to get what I mean. The compiler will fight you here because `s.as_str()` won't live long enough (as it's only a `&amp;str`), and you can't deref `DEFAULT`'s value because then it won't fit `Struct`'s type signature. --- On the other hand, if the type definition is `path: String`, you'd have to deref the one in `DEFAULT` with something like `String::new()` or `.to_owned()`. However, trying to do that gets you our good friend error 15: error[E0015]: calls in constants are limited to struct and enum constructors The compiler does note that "a limited form of compile-time function evaluation is available on a nightly compiler via `const fn`". However, I don't think that solves the issue, as a very quick glance at the [associated issue](https://github.com/rust-lang/rust/issues/24111) shows that function *calls* haven't yet been allowed. Short of trying to manually construct a String struct in the function, is there any way to solve this one? 
The biggest strength about this book, (so far, I'm only half way through,) is how well it communicates core concepts of the language. These concepts are the beauty and also the crux of Rust and would probably remain unchanged for some time, so having such a great resource like this book is a tremendous help.
Thanks for the feedback. I'm not sure where I got that impression. Regardless, I see crates like stdx as being a central building block for creating (eventually) a library like Boost/C++ for rust -- a kind of all in one library for industry applications -- as well as similar libraries for clis, data analysis, machine learning, web applications, data base management, etc. The advantage of this is that build tools like Nix, Portage, etc can rally around this library and get it to compile in a range of environments and make packaging and shipping cross-compiled binaries much easier. By streamlining the build, test and interop story of major library collections, rust could be made to be much easier to get into for industry applications. This is like the end goal of ergonomics: an ecosystem of smoothly interacting and well tested crates with proven solutions for build + test + deploy.
The only place I know of to still buy DRM-free O'Reilly PDFs is [ebooks.com](http://www.ebooks.com). But they are very expensive. Sometimes even buying the printed book is cheaper than buying their ebooks.
No, this is by design. `cargo rustc` is specifically for tweaking only the actual crate being compiled, RUSTFLAGS is for the whole tree.
Have you actually developed a web app or website in Rocket? I did and didn't have to use lifetimes outside more than few times and those were very simple.
Unfortunately I think you're missing my point... &gt; We are missing auth and hot reloading, but those are issues on GitHub waiting for solid implementation. Why not make what you already have stable rather than adding new features? That's what I want; not another web framework. Not more features in an existing one. Just make it stable, with a fixed API that doesn't change. You're 100% better off with a solid base of stuff that works 100% of the time no question, instead of 100 features that may or may not work. I know that promising long term API stability is hard, but its **so much harder** the more features you keep adding. What you're generating is a project that is getting *further* from being stable and usable, not closer. What projects need to do is start carving off bits and being like, this module is unstable, we're still working on it... but this *core* is stable and we promise not to break it. Ever. ...if you can't even do that for the tiny microframework web service part of the project, the chance of being able to do it for the whole thing just seem... like a distant dream at this point.
&gt; That's a huge disadvantage in general of epochs I don't see how this is related to epochs? Big changes will happen to rust either way, epochs just provide a way of thinking about them coherently (which is _good_ for books) Without epochs there's no chance for any of these changes to be breaking, but then it would probably still be a similar change just stopping at the deprecation stage.
So what do you propose? We should not stop with a half-baked unusable framework with missing critical features which can't be implemented with frozen API. Handlebars, Cookies, Diesel, Serde are all stable and developed independently. You can assemble your own framework with stable parts. You can use cargo for freezing versions so you have a stable api. Noone makes you upgrade to new version. Yes, there is no complete solution to web problem yet. But we have few stable parts. We are still in the experimental phase when it comes to web frameworks but I don't think we should ditch it or freeze the API at this point. New solutions are always welcomed as an inspiration to existing projects. There are people who fix bugs and keep library stable. New feature doesn't mean your website will break down and you won't be able to fix it.
I have two ideas for improvement: * Provide extension trait `TimeoutReadExt` with blanket impl for all `Read + AsRawFd` (or `Read + Evented`) with method `with_timeout&lt;&lt;T: Into&lt;Option&lt;Duration&gt;&gt;&gt;&gt;(self, timeout: T)`. This will allow people to write: `stdout.with_timeout(Duration::new(5, 0))` instead of `TimeoutReader::new(stdout, Duration::new(5, 0));` * Add clear explanation which error is returned in case of timeout. Currently I'd have to dig into code to know.
Hmm I had hoped to buy other DRM-free books there as well, but looking at their 'formats' page, they seem to have mostly DRM-encumbered books … how can I distinguish them?
there is an existing issue already, waiting for implementation :)
It depends on the publishers, they just sell what the publishers provide to them. There's a sentence on every book detail page that describes what you get. Example for DRM: Read online, or download in secure EPUB or secure PDF format | Example for DRM-free: Read online, or download in DRM-free EPUB or DRM-free PDF format
[Link to RFC](https://github.com/rust-lang/rfcs/blob/master/text/1598-generic_associated_types.md)
It's a C library like any other, and there are binaries for Windows provided in various places. Unfortunately C/C++ dependency handling, especially on Windows, is a disaster. There is nothing nice like cargo available, only many different and incompatible solutions. For your specific case, the best would probably be to ship GLib binaries together with your application binaries. And for building/installing GLib for development, you can use msys (which has packages available), build GLib in Visual Studio, build it manually via other means (autotools or meson based build system), ... What kind of learning burden are you thinking of?
epochs are about a format for breaking changes. They are not some kind of "major semver version" or whatever (like e.g. C++14, C++17 etc): You can stay on an older epoch and enjoy new features orthogonal to the things that changed in newer epochs.
How many chapters on strings are there?
Your claim that V8 does not perform optimizations is incorrect. It will optimize JavaScript as it's run. [See this excellent article](https://blog.sessionstack.com/how-javascript-works-inside-the-v8-engine-5-tips-on-how-to-write-optimized-code-ac089e62b12e). 
yes, that is still there.
Wow, that's old news.
I have a Celeron G1610 (Ivy Bridge), and running `sudoku-rs` with either `native` or `ivybridge` results in SIGILL for me.
At least two type of strings in a system language is not a Rust idea.
Hi, Have a look at [nalgebra](http://nalgebra.org), it has some of the tools you need and intends to have them all AFAIU.
Quick question: Does setting `target-cpu` have the same result as setting all the `target-feature`s manually: RUSTFLAGS = "-C target-feature=+sse -C target-feature=+sse2 -C target-feature=+sse3 -C target-feature=+sse4.1 -C target-feature=+sse4.2 -C target-feature=+popcnt -C target-feature=+lzcnt" Or has it more implications?
I'm just waiting for 6.0, so we can [finally have this optimization again](https://github.com/rust-lang/rust/issues/31681#issuecomment-316830577).
Rust has a lot more than two: String, OsString, CString, Path, Vec&lt;u8&gt;. Double this to include the slice types.
The advantages for Rust are smaller, because it does not link dynamically. Also when using C++, dependencies are so painful that monolithic libraries like boost are more attractive.
There are denial of service attack vectors that apache and nginx are hardened against, like slowloris.
Thank you all for your very valuable feedback! I implemented your suggestions, added hopefully helpful documentation and published this on crates.io: https://crates.io/crates/atomic-counter Let me know what you think :) If you have any feedback, or find any issues/errors/bugs/inaccuracies, please do let me know!
I would argue that `Path` and `Vec&lt;u8&gt;` are not strings. But you forgot all the non-owning equivalents of the others: `&amp;str`, `&amp;OsStr`, `&amp;CStr`.
This isn't higher kinded types btw, although it may be a step towards them.
It took Eigen many years to accumulate the features it has, so you won't find anything like it in Rust, the language and its libraries are just too young for that. Rust is still missing integer generics. You probably want to have a look at nalgebra and rulinalg. If I recall correctly, they don't support sparse matrices. I think you would have to use bindings to C libraries for that.
I know that, but rust still often depends on C libraries under the hood and you can get annoying linking issues. Having tride and true libraries with build system support is essential for rust being used widely in industry IMO.
There are two ways to solve this problem, and it depends on the answer to this question: Will all paths have a `'static` lifetime? If the answer is no, then the struct definition needs to be changed to include a lifetime parameter: struct Struct&lt;'a&gt; { path: &amp;'a str, } And the `get_or_default` function will need to be changed to accept an `Option&lt;&amp;'a str&gt;` instead. Taking an owned `String` wouldn't work here, since it will be dropped at the end of the function. If the answer to the initial question is yes, then the original definition of `Struct` can be kept, but the `get_or_default` function will need to accept an `Option&lt;&amp;'static str&gt;`. This would be somewhat limiting if you want to work with non-literals though. As an aside, you can just implement `Default` for this particular case. It will get you similar functionality to `get_or_default`, by utilizing the struct update syntax with it.
Maybe it would make sense to more or less merge `stdx` with the Cookbook by adding pages for the crates in `stdx`?
How is a filename not a string? `Vec&lt;u8&gt;` is considered a string in Python. By slice types I meant the non-owning types.
I don't think this applies to any of the stdx libraries though. You can also statically link C libraries into Rust.
Oh, I guess I didn't read your comment carefully enough. In that case your original list should have included `PathBuf`, not `Path`. And I'd argue those are not really strings because they are platform-dependent types, and usually resemble lists of string. `Vec&lt;u8&gt;` is a bag of bytes and while there are languages that consider that to be the same as a string, it really could represent arbitrary data.
Sure, if I understand it correctly it's subset of features called HKT or something very similar.
Ah, of course, I meant link to PR.
Well, rust is illustrating some potential limitations with the tools LLVM provides (although this one is probably up to rust to solve anyway). It's not far-fetched to suggest improvements from this.
I ended up just turning the constant into a function `default()`, but I had no idea that there was a trait for it too. Maybe when I have some other time to mess around with lifetimes I'll see if I can make the first option fit. Thanks.
On a personal note, I've worked with Jim and Jason for years and they're both really great people. Some of Jim's C++ test fixture code he wrote for Google's Breakpad library was so fantastic I [ported it to Rust](https://github.com/luser/rust-test-assembler) to use in my Rust projects.
Does the grammar really have any noticeable impact on compile times? I would think the parsing takes a negligible amount of time, and I don't see how it would change anything else. Especially since we know that most of the time spent in the compilation is LLVM doing its job.
[This](https://stackoverflow.com/a/5812498/2551224) StackOverflow post answer it quite well. Basically, most CPUs operate on 16 bit values in 64 bit registers. In order to preserve the 16 bit semantics, the CPU has to do extra work like filling the unneeded bits with 0 or ANDing the 64 bit register when storing it somwhere in memory. Disclaimer: This is just how it got explained to me once, I'm not a pro with CPUs.
In many languages strings are bags of bytes that can represent arbitrary data. I'm not sure what you mean by string, and it seems your definition disagrees with a lot of programming languages.
I'm not sure that C++ compiles much faster. Keep in mind that the compilation unit in C++ is the file, while it is the crate in Rust.
&gt; Does the grammar really have any noticeable impact on compile times? it's the scope for parallelism (compared to C++). C++ should be worse because of the headers. An incremental build system in Rust should have more scope to cache independent parts.. because it's much easier to figure out which parts really are independent. r.e. time spent in LLVM, I still think being able to reason better about dependancies might give scope to break it up into peices whose compilation could be cached (although I know eventually there's scope for link-time optimisations aswell, but maybe it could perform much better in the middle ground .. in the tradeoff between compile times and optimisation setting)
The problem is: *what would it do?* There are two equally valid interpretations that I can think of: 1. The body of the loop runs for each item which matches the pattern. 2. The body of the loop runs *until* an item fails to match the pattern. Very different semantics, but no obvious indication which it is. Python has a similar problem with `for ... else`, whose semantics I can *never* remember correctly. It's a bit like trying to look at one of those double images, and your brain keeps flipping back and forth on which interpretation it thinks is right. So, I'd say: *no.*
Why am I getting this error. http://prntscr.com/gim4zr
Good point. Option 2 didn't come to my mind. While I still think that 2 makes less sense than 1, the potential to confuse those two might be reason enough to reject that idea.
I've [tried](https://play.rust-lang.org/?gist=1a0755c4c7a843a4d86f70611e7ae864&amp;version=stable): fn main() { let v = vec![1, 2, 3]; for p@(i, x) in v.iter().enumerate() { println!("{:?} = ({}, {})", p, i, x); } } and got a _different_ error: `pattern bindings are not allowed after an '@'`. But hey, maybe one day the 65 separate dialects of pattern matching Rust will be cleaned up and unified? One can dream...
FWIW, Haskell allows fallible patterns in list comprehensions and uses the first interpretation (*). It's actually a very useful feature there which often saves a lot of `filter` and `case` boilerplate. The "run until" interpretation doesn't occur to me at all really, as this is a `for` loop which based on iterating _over_ something, not _until_ a condition happens (that's what `while` does). (*) Yes, I'm aware this isn't actually a choice in Haskell, as it stems directly from what "list" comprehensions are, and how failed patterns in `do` blocks trigger the `fail` method of `Monad`. But just because a useful behaviour is emergent, it doesn't mean it is not worth replicating.
This seems a bit suspect to me. What should it do if the item doesn't match the pattern? Skip that iteration (à la `filter` or `continue`) or stop iterating altogether (à la `take_while` or `break`)? None of those seem more correct than the other. Also, it's pretty easy to just combine it with an `if let` and get full flexibility, complete with an `else`. That is, instead of doing for &lt;refutable pattern&gt; in &lt;iterator&gt; { you just do for x in &lt;iterator&gt; { if let &lt;refutable pattern&gt; = x { ... } else { /* handle? break? skip? */ } } This is both clearer to read, in my opinion, and more flexible. Also, requiring patterns to be irrefutable is *not* a special case for `for` loops. Rust does it in other places too: // must be irrefutable, otherwise you need an `if` let &lt;pattern&gt; = &lt;value&gt;; // function definitions are also patterns, and they too must be irrefutable fn foobar(&lt;pattern&gt;: &lt;type&gt;, ...) // Example: fn foobar(SomeStruct { a, b, .. }: SomeStruct) 
Wait, so... pattern matching != pattern matching... okay. Have to read through Rust's syntax definition again.
Was able to help myself. http://prntscr.com/gim9qo
There are only two "dialects": one for refutable patterns (`match`, `if let`, etc), and one for irrefutable ones (`let`, etc). Neither of them permit bindings after `@`. 
I was exaggerating with "65", of course, but there are at least three different variants: * `match` has full featured pattern matching, with`@` bindings and `if` guards * `if-let` and `while-let` don't support guards (and probably not `@`) but allow fallible patterns * plain `let` (and function arguments?) only allow _structurally_ infallible patterns (i.e. `let (x, y) = (1, 2);` because a pair has always the same structure, but `let Some(x) = Some(42);` doesn't even though the compiler could statically prove the pattern is infallible) Where does the `for` dialect fit in here is anyone's guess, though I suspect it's at least close to plain `let`.
&gt; What should it do if the item doesn't match the pattern? As the pattern is in the position of loop variable definition and not in the position of looped over iterators, I'd say that just skipping matches feels more *"correct"* to me. But as I answered to /r/Quxxy the potential for confusion is a problem to be solved if someone wanted to put that into an RFC. &gt; Also, it's pretty easy to just combine it with an `if let` [...] Yes, of course. I thought of `if let` without an `else` being what would happen behind the curtain. You could explicitly write `if let`, but you also could just pattern match in `for` to save yourself two lines of code for short patterns, if you care about those. And not to forget, it was `rustc` who suggested a pattern matching based filter in `for ... in ...`. So... if we all came to the conclusion that such a feature would be stupid, then we might instead want to think about *"fixing"* that error message. &gt; Rust does [pattern matching] in other places too Yipp, I know. This case just caught my attention because `rustc` suggested there being syntactic sugar I never would have noticed. I'd just have called `.filter(...)` without any thought put into it.
&gt; Neither of them permit bindings after @. That's a shame. One could imagine that this could work: use std::error::Error; fn main() { let v: Vec&lt;Result&lt;usize, Box&lt;Error&gt;&gt;&gt; = vec![Ok(1), Ok(2), Ok(3)]; for r in v.iter() { match r { r_@&amp;Ok(ref x) =&gt; { println!("{:?} == Ok({})", r_, x); }, _ =&gt; {}, } } } (and could be written as `r_@Ok(x)` after the RFC for pruning `&amp;`/`ref` from patterns is implemented). As long the usual rules of borrowing are not violated, it looks like a workable feature.
Garbage collector ? I'd rather not.
&gt; In computer programming, a string is traditionally a sequence of characters https://en.wikipedia.org/wiki/String_(computer_science) Yes, many programming languages allow you to put control characters into strings, but most of the time, you only use string types for lists of printable / non-zero-width-whitespace characters. Byte array are usually available as distinct types, even when strings technically allow you to store the same information.
**String (computer science)** In computer programming, a string is traditionally a sequence of characters, either as a literal constant or as some kind of variable. The latter may allow its elements to be mutated and the length changed, or it may be fixed (after creation). A string is generally understood as a data type and is often implemented as an array data structure of bytes (or words) that stores a sequence of elements, typically characters, using some character encoding. A string may also denote more general arrays or other sequence (or list) data types and structures. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.27
HKAT!
I made a fizzbuzz program with good programming habits. https://gist.github.com/anonymous/02a25d1982e6e7812e59282f7183748d
&gt; Now, imagine a future where all these OS and firmware components are written in Rust and are largely free of memory bugs. What if consumer-hostile companies use these safety properties to restrict their users even more, and this time without a possibility to break free from the corporate overlords can you expand on this a bit more?
It seems GAT is also not able to express more complex trait bounds like: fn some_parallel_code&lt;CF&gt;() where CF: ContainerFamily, for&lt;T: Send + Sync&gt; &lt;CF as ContainerFamily&gt;::Container&lt;T&gt;: Send + Sync, { } You can't express this without an extension of the trait bound system. If you however know the exact types you are putting in, then you can write it like so: fn some_parallel_code&lt;CF&gt;() where CF: ContainerFamily, &lt;CF as ContainerFamily&gt;::Container&lt;i32&gt;: Send + Sync, { } However this is already fully expressable in today's Rust: fn some_parallel_code&lt;CF&gt;() where CF: ContainerFamily&lt;i32&gt;, &lt;CF as ContainerFamily&lt;i32&gt;&gt;::Container: Send + Sync, { } ~~So GAT barely adds anything other than some slight syntactic conveniences (which are definitely really cool when it comes to lifetimes).~~ Update: burntsushi showed a situation that is not just a syntactic convenience, but an actual error in today's Rust.
I think I got it, with some help of the imagination. Actually the main problem here is the `for&lt;'a, 'b: 'a&gt;` requirement - it can exist "theoretically", in a form of a trait requirement, but is impossible with the actual runtime objects. Removing nested lifetime makes all work: https://play.rust-lang.org/?gist=fc488fd1a97c36a439f6507ac92e564b&amp;version=stable Yes I know about the commented `impl` with `'static`s, that error is totally legal. I was checking whether passing `'static` lifetimes satisfy 
Non-Mobile link: https://en.wikipedia.org/wiki/Slowloris_(computer_security) *** ^HelperBot ^v1.1 ^/r/HelperBot_ ^I ^am ^a ^bot. ^Please ^message ^/u/swim1929 ^with ^any ^feedback ^and/or ^hate. ^Counter: ^109361
&gt; As the pattern is in the position of loop variable definition and not in the position of looped over iterators, I'd say that just skipping matches feels more "*correct*" to me. Well, breaking the loop altogether seems "more correct" to me. After all for &lt;pattern&gt; in &lt;iterator&gt; is just syntactic sugar for while let Some(&lt;pattern&gt;) = &lt;iterator&gt;.next() , so if the pattern fails to match it should terminate the loop, right? There is obviously no unequivocally correct behaviour here. &gt; And not to forget, it was `rustc` who suggested a pattern matching based filter in `for ... in ...`. So... if we all came to the conclusion that such a feature would be stupid, then we might instead want to think about "*fixing*" that error message. I agree. The error is confusing, and should probably be written in a way that makes it clear this pattern matcher can only be used for destructuring. 
Ugh... forgot that it lowers to `while let`. That makes it quite a difficult decision, indeed, with breaking early being *"more correct"*.
I love this book, way worth more than its weight in gold. Between this, the TRPL, Rust by Example, the Rust docs, and the blog posts, life is good. :)
Yes, what I meant is that, with the exception of some minor optimizations, MIR is basically mapped to LLVM IR in a trivial way. Currently rustc takes Rust codes and lowers it down to LLVM IR in the most trivial way possible, which produces a lot of LLVM IR, and we then just let LLVM do all the work. &gt; 1:many (each monomorphisation of generic functions) Note that this must be the case, LLVM IR works on concrete types. However, when some function of crate A is monomorphized for the same type parameters in crates B and C, we are generating the LLVM IR twice. Is this also the case between different monomorphizations within the same crate?
I guess one way to think about it is whether it will provide the value you seek out of it in the short-ish term that it remains 100% current. Once the basic concepts are clear, it should be relatively easier to keep up with the changes without the book itself. Of course, I understand people buy books for different reasons, so this may not suit you. I felt like this was the best money I've spent on a book this year.
Speaking of this: I'm having trouble with a couple of exercises in the Rust Book. The Chapter 10 exercise building a "largest" trait ends the exercise with "Another way we could implement largest is for the function to return a reference to a T value in the slice. If we change the return type to be &amp;T instead of T and change the body of the function to return a reference, we wouldn't need either the Clone or Copy trait bounds" but I can't get this to work. Either I'm making the reference wrong or I don't know how to do lifetimes right, and the following section on lifetimes never revisits this exercise. I'm also struggling with the Chapter 13 Closures exercise (writing a "cacher" struct) and can't get anything to compile that fulfill "Try introducing more generic parameters to increase the flexibility of the Cacher functionality." Looking for help here XD
Well, let's count string implementations in C/C++. Oops... message limit reached. 
OsString and CString have niche uses for platform and C interop, respectively. Meanwhile, I can't see how Path is a string type at all (just because it has a string constructor doesn't make it a string type, unless one also considers e.g. IpAddr a string type; the semantics of paths and strings are distinct, which is the whole reason we have types in the first place!), and saying that Vec&lt;u8&gt; is a string type seems especially useless given that it's just a featureless sequence of bytes (is LinkedList&lt;u8&gt; also a string type?).
Actually I think it's more like { let mut iter = IntoIterator::into_iter(&lt;iterator&gt;); while let Some(&lt;pattern&gt;) = iter.next() { // body } }
The very beginning of the RFC explicitly says that it adds a form of HKP, and cites a streaming iterator definition that wouldn't be possible to use in today's Rust.
&gt; I want to write a library for newbies to cli's Another one?
&gt; This isn't higher kinded types &gt; it's subset of features called HKT 🤔
Ohhh, it seems that it truly has an effect on the lifetime system, as it seems to be able to avoid the `= note: concrete lifetime that was found is lifetime '_#1r` error. If you move the fact that the streaming iterator iterates over mutable references out of the associated type Item, then it already works. I think it may be possible to abstract over the type of reference by using a "reference family" that you instantiate with the Item. Definitely annoying though. (Update: The reference family thing seems to only move the problem, not actually solve it)
Minor suggestion: If the JSON has an element "friends" that is an array of objects, I'd call the struct you generate for that object "Friend", not "Friends". In other words, while the JSON key name is often plural, struct names are almost always singular. Going from plural to singular in a way that works in all cases is kind of hard (not all plurals end with "s", for example "children" doesn't), but a simple hack that should work in most cases is: does the word end in an s? If so, drop that s. If not, do nothing.
The semantics of the `else` clause on `for` loops in Python is actually pretty simple to describe: it is run when the loop ends without a `break`. That is why I'm waiting for an equivalent clause on Rust `for` and `while` loops, because that would allow me to use the return value on those loops (currently hardcoded as `()`) … does anyone know if there has been any movement in that direction recently?
I feel like we are talking past each other, probably because we have different ideas of what "string type" means. When writing my comment, I thought of it in terms of its implementation, whereas you seem to think of some abstract type with a more or less narrowly connoted use case. (I'm not arguing which interpretation has more merit.) &gt; unless one also considers e.g. IpAddr a string type I would consider `IpAddr` a string type if it was implemented as a string. &gt; the semantics of paths and strings are distinct, which is the whole reason we have types in the first place! The semantics are of course different, there is a reason that they are different types. You don't have to make them separate types though (if you can live with the shortcoming of not enforcing the semantics with the type system), which is what most other programming languages do. &gt; saying that Vec&lt;u8&gt; is a string type seems especially useless given that it's just a featureless sequence of bytes (is LinkedList&lt;u8&gt; also a string type?). Fair enough, but the analogue of `Vec&lt;u8&gt;` is used by a lot of programming languages for strings (i.e. Python 2, Go, Pascal).
Whew! I am a little embarrassed that I did not properly document the error returned in case of timeout. I thought I had done that but, sure enough, I missed it. Oops! I will definitely get that done soon. Also, I love the idea of an extension trait that provides a `with_timeout` method. I plan on prioritizing that since I think it will make code using this much simpler. Thanks!
I'm not sure how that definition helps, because it is so abstract. A character can be anything. You could interpret an integer as a string of bits.
Simplicity isn't the issue: that behaviour just makes *no* sense to me whatsoever. It never has, and every time someone describes it to me, I end up forgetting it because it's so counter-intuitive. On your second point, you can do `let x: i32 = loop { break 42; };` already.
Just a note about the lack of integer generics: This is mentioned a lot in discussions about Rust and numerical applications and it really makes it sound like you can't do efficient numerics in Rust because of this. For the type of application mentioned here (solving dense and sparse linear solvers, eigensolvers etc.) you don't need integer generics, because the size of your linear systems is most likely unknown at compile time. In Fortran, C, C++ or Rust you'd use dynamically allocated matrices and vectors for this task. Something like integer generics is useful when working with many smaller matrices and vectors of fixed size, like in 3D graphics. For the applications mentioned here, you COULD use integer generics to write hand-optimized versions of the relevant algorithms, in terms of fixed-size blocks, for cache optimization. I'm pretty sure that if your goal is not writing new solvers, it's more productive to just link existing solvers like Eigen, Suitesparse, MUMPS or Pardiso (if you don't mind using a closed-source component). These packages are VERY robust and efficient and they are the industry standard tools for this. By the way, Pardiso is written in Fortran, while Suitesparse is written in C, languages without integer generics.
Breaking with a value out of a `loop` is already implemented! Someone managed to (ab)use this to have an `else` clause in a for loop like so: [playground](https://play.rust-lang.org/?gist=b5bf245eee15d947ead7842cfb59eddf&amp;version=stable) fn main() { let result = 'outer: loop { for i in 0..10 { if i == 8 { // Early break out break 'outer 2.71828; } } // Code which only runs if the for wasn't broken out of break 'outer 3.141592; }; println!("Hello, {}!", result); } 
I hearby challenge you to produce a usable streaming iterator interface in today's Rust. :) There is a crate to do it (`streaming-iterator`), but it has some limitations. For example, the type of the value yielded must always be a reference (this sounds closer to your proposal IIUC). I have my [own definition that avoids that problem, but cannot be composed](https://docs.rs/fst/0.2.3/fst/trait.Streamer.html). So yeah, I would personally say that this is a lot more than a syntactic convenience. I've run into different instantiations of a similar problem at least a few times by this point.
And const generics aka being able to implement stuff on [T;n] is close too!