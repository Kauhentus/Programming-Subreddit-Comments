If you click through a bunch of times, you'll find https://github.com/rust-lang/rust/pull/42782#issuecomment-311506979 which is sort of a summary of the discussion over the years.
The type `&amp;T` is called "reference to T" isn't it? I'd say reference to integer, reference to i32 etc
Well one might argue that a function `rand::Rng::bytes(&amp;mut self, size : usize) -&gt; Vec&lt;u8&gt;` should quite possibly exist and that function will internally probably use Unsafe so you might as well write that function and thus use unsafe.
Nothing earth-shattering in here, but I love these kinds of blog posts! One more thing for people to come across when googling, and it's short, sweet, and to-the-point.
&gt; except it forces you to use the shared value inside a closure as opposed to relying on you to release it back Why/how is this better than just using a RefCell and try_borrow_mut()? It seems like they are doing the same thing, just with a different API. But I guess there must be an advantage, otherwise you wouldn't have come up with it. (One thing I can think of is that you can mem::forget the reference given to you by try_borrow_mut, leaving the RefCell in a borrowed state forever.)
No, because in order to be pushed to the stack, it has to exist in the code or data somewhere already. It's cheaper to just reference that instance directly.
This is very helpful. Thanks for making this. Am I missing some piece of Rust magic, or do you need to convert your instance of Server&lt;Start&gt; to Server&lt;Stop&gt; in order to get it to run? 
&gt; If needs_drop conservatively returns true for a type that doesn’t actually have a destructor I think this is confusing because when I think of "conservatively implementing" something then the default value is `false`. It's not an optimization if not implementing it results in broken code (ie a leak).
Even better would probably be to have a `fill_bytes_vec(&amp;mut self, size: usize, vec: &amp;mut Vec&lt;u8&gt;)` so the user can control the allocation. But again, I don't know if the difference in speed justifies the additional API. I guess if you're using a really fast RNG, like a linear congruential, maybe zeroing can become a significant cost. 
How does the design of Tock compare to that of Redox? Are there some low-level design decisions which make these OSs fundamentally different (though both are implemented in rust)?
I always pronounce `&amp;T` as 'and t' in my head. But my brain doesn't want me to put an 'an' in front of something that isn't a vowel.
&gt;For all the reasons to hate Java, having whole stack traces get dumped in the logs with exact line numbers in the code helps tremendously in spotting the conditions of the error. For the most part, sum types are a *far* better way to handle errors. There are issues (libraries can have panicking functions) but it at least gives you a better control. Plus you can get backtraces which is quite nice :)
I pronounce it ampersand T, or simply address of T. I'm a C programmer :-)
[removed]
I get that. My main languages outside of rust are python and C#, so not many `&amp;`'s show up.
Isn't skipping the operation inside a `map` a potential source of (semantic) bugs? I would prefer something like a `RefCell`, where you'll get a more visible deadlock.
Why does capturing a backtrace take tens of milliseconds? That's hundreds of microseconds per stack frame. Is that just libbacktrace stupidity? I would expect it to just use the unwinder to create a list of return addresses, and to only convert the list of return addresses to actual symbol names when we're printing it (and if we're printing an error, this means something failed and a few milliseconds shouldn't kill you).
Is there something stupid I'm missing, or this is just one another reason libbacktrace should die?
And even better would be `write_bytes&lt;W: Write&gt;(&amp;mut self, size: usize, output: W)`, where it writes the desired bytes to any target, including a `&amp;mut Vec`.
As someone who is only interested in things that run on stable, thank you for pushing that through!
`std::mem::discriminant` and `for_each` both look good
I am never going to try and do anything clever ever again. The behaviour underlying the bug in the talk really seems like a bug in either the kernel or the allocator to me. If a program has legally got hold of a pointer that it is allowed to use to refer to some chunk of memory, it seems appalling that its contents can change. Either the kernel should not do that (by initialising the page, with arbitrary values, on read, or zeroing it on initialisation), or the allocator should take steps to make sure that the kernel's behaviour is not visible. In fact, Linux at least *does* zero newly-initialised pages, right? Uninitialised page frames are all mapped to the zero page, and on write, they're remapped to a freshly zeroed page. But hey, i'm just an application programmer, of course i want things to be simple and easy!
map returns an Option to let you know if the operation happened or not, but it would be easy to forget to check that for operations which are purely updates. Perhaps it should instead return an Option-like type which is #[must_use], so you would at least get a warning if you forgot to check. It could even be Result&lt;T, ()&gt;; does that benefit from the optimisation where an Option of a pointer requires no extra space?
No, it's always promoted *and has been since before 1.0* - only the borrow checking understanding it's `'static` took ages in RFC hell (my bad).
Memory leaks are not considered 'broken code' in Rust. It is purely a quality-of-implementation issue. &gt; Rust's safety guarantees do not include a guarantee that destructors will always run. For example, a program can create a reference cycle using `Rc`, or call `process::exit` to exit without running destructors. ... Because `mem::forget`ting a value is allowed, any `unsafe` code you write must allow for this possibility. You cannot return a value and expect that the caller will necessarily run the value's destructor.
Ah thanks.
One thing that is very valuable is to have `Debug` implementations on all data types. This will go a long way to show you the data. Of course it is not comparable to python's capabilities to introspect and monkey-patch at runtime, but OTOH Rust has much stronger guarantees, and one can use the type system to restrict a good number of error classes, reducing the need to debug in the first place. Also note that there is a recent rust REPL project called [rie](https://crates.io/crates/rie).
So...why not? Not that I have anything against Haskell, but what made you choose it, and where do you think lie the benefits for this particular project compared to Rust?
Cool fact of the day: the choice to use `an` or `a` isn't whether the following word starts with a vowel or not, but whether is it pronounced beginning with a vowel sound. The neat consequence of this is that you can tell how someone pronounces "SQL" based on whether they write "a SQL query" or "an SQL query."
I currently don't have enough time to read through all the PRs, so I skim some. In this case, a different heading or some additional verbiage would have cleared up any possible misunderstanding. My apologies.
I'm not involved with either, but from what I've seen, Tock is designed for small embedded applications, while Redox is designed for desktop (and presumably server) use. Part of the design of Tock is to allow isolation of kernel modules (which they call capsules) by taking advantage of Rust's safety, while also allowing for traditional user space process isolation. So, the idea to provide a safe Rust interface in the core kernel for writing capsules which can use no `unsafe` code, allowing for only a very small core which needs to be manually verified, while providing lower overhead than a microkernel type design which uses hardware protection mechanisms between processes.
Syntax checking is somewhere where laziness works quite nicely - I was able to reuse a parsing library with minimal overhead and still get the behavior I desired. I'm not sure what performance a similarly constructed Rust tool would get, though I'm curious to know the answer. But it was mostly just a preference. As you can see, the entire tool is around 45 lines of Haskell. Haskell's CI situation is a mess compared to Rust but other than that I think I would've had similar results with slightly more verbosity.
So when the release notes say the following. &gt;Why? Well, if the thing being referred to is okay to put into a `static`, we could instead de-sugar `let x = &amp;5;` like this: The idea is that literals are always okay to put into `static`, and always do get put into `static`? But for other unbound non-literals they too would be promoted to a `static` de-sugaring?
It’s not a memory safety issue but I’m pretty sure we can say that a collection type that never drops any item is buggy, defective, incorrect, whatever you want to call it.
Shouldn't `set_len` be called after the buffer is written? It might not matter for `u8`, but in case of a panic in `fill_bytes` you will start unwinding the stack (with the default panic impl, at least), causing `buf`s `drop()` to be called which then operates on uninitialized data. IIRC it's generally recommended to leak memory instead of triggering undefined behaviour in such scenarios.
So, for one, `TakeCell` predates `try_borrow_mut` in RefCell. So, in part, it wasn't possible in RefCell at the time, and we ended up just sticking with our interface. We've been keeping it around because we think there is some advantage to the interface. First, we often really want to be able to use `take()`, not `borrow`, for example, we often use `TakeCell` when we're passing buffers down to hardware and won't get them back until some later callback. You can probably do this with a `RefCell&lt;Option&lt;T&gt;&gt;` and use something like `replace(None)`. It's a bit verbose, but would work I think. Second, it's nice to have an interface that just can't panic. Part of the code review process for capsules in Tock is to search for `unwrap` and `expect`. There are a few instances where we still let those fly, but as a rule we bar calls that can panic. It helps to minimize what those calls can be. n.b. this isn't fool-proof: for example, slice indexing can panic, and it doesn't seem practical to avoid that. But fewer ways of panicing is generall better (for us). Finally, even with `try_borrow_mut`, you still end up with a value (`RefMut`) that you can store somewhere indefinitely. There's no guarantee that you'll return it to the `RefCell` (e.g. if you store it in a separate field). It's generally, so far, seemed advantageous to have map only yield a limited-lifetime reference. But having said all that, if we didn't already have `TakeCell` and were facing the same issue again, would we implement it given what `RefCell` already provides? I'm not sure.
Hey, thanks for this! Made a big coding push this week and have delved straight into ECS via Specs. I haven't gotten very far yet but really like the paradigm. How would you implement the concept of teams? At the moment I have a team component with some reusable teams based upon the team component struct. Should input be a generic component that you update on the fly wrt keypresses, to be acted upon by systems during rendering? Any feedback you could give me would be awesome :-) https://github.com/SamHH/osctf-native-experiment/tree/ecs
This is a good summary! The (even newer and shinier) [paper](https://www.tockos.org/assets/papers/tock-sosp2017.pdf) I mentioned in a different comment gives a more in depth description of the OS architecture as well as the motivation and contrast with other OS architectures than is available on the website (for now). Overall I would say that Redox and Tock just address _very_ different kinds of computer systems, and as a result have very different architectures. A cool project to check out, for those curious, is Singularity (it was/is a research OS out of Microsoft Research), and think about how Rust might be helpful for that kind of architecture (they basically had to add linear types to C# to make it work, but Rust already has those!)
The release notes are a tad bit confusing, the rule it's more along the lines "if you can write `{ const X: ... = &amp;expr; X }` in place of `&amp;expr`, and there's no `Drop` involved, we do the promotion. It has nothing to do with literals or desugaring as it's usually understood (syntactical). Rather, MIR is transformed post-type-checking to split off chunks of it which can be computed at compile-time, which involve borrows of temporaries. We may later on do more promotion as an optimization, for values that *are not* borrowed but are still e.g. large constant structs or arrays.
I’ve having trouble following your reasoning, sorry. "Conservative" doesn’t systematically mean `false`, that all depends on what that boolean means. Let’s say you’re implementing a generic collection type (let’s say a custom hash map) with manual memory management. When the collection is dropped you drop the items. Now, sometimes dropping the items does nothing, for example because they happen to be `u32`. In that case, your collection might end up spending time in unoptimized mode looping through items one by one to then do nothing. `needs_drop` allows you to skip that loop entirely. Now I’m making a new implementation of Rust. Maybe in some cases it’s difficult or costly to tell for sure that a destructor is completely a no-op. Or maybe I haven’t implemented this "for real" yet and left that work for later. In the unsure cases, my `needs_drop` implementation should return `true` so that your collection does loop through the items, in case there is indeed some resource to free. Maybe that loop isn’t needed, but "conservative" means doing it anyway just in case. Your collection using `needs_drop` is an optimization. My implementation of `needs_drop` is that thing that might be conservative.
itertools is super great but to be honest I'm pretty happy to have one less reason to import it.
Is there a way to hook `cargo clean` similar to `cargo build`? The reason is to clean the artifacts to a c project build with `cargo build/build.rs`
Would it make sense for an engine to use gfx-hal instead of Ash? And I read that gfx is also used as a testbed for gpuweb. Is the end goal to be able to compile a Rust/gfx application to Wasm/gpuweb?
`vec![0; size]` will call `calloc` or an equivalent function, which requests zeroed memory from the allocator, rather than initializing the memory explicitly. In many cases this is no more costly than allocating uninitialized memory: https://github.com/rust-lang/rust/pull/40409
I'm really excited about this. A full-featured actor model framework has been high on my wish list for rust. Look forward to digging into this soon.
Shoulda known to cc you on the review...
You touched a lot of sensitive topics within this short comment :) &gt; Would it make sense for an engine to use gfx-hal instead of Ash? Problems with `ash` to `gfx-hal` is for the most part the same as [vulkan.h to gfx-hal](https://github.com/kvark/portability): mainly, the fact we use borrowing aggressively, while pure Vulkan is written to work with opaque copy-able handles. Thus, I don't think it's worth trying to meld `ash` and `gfx-hal` together - it will be easier to just ask `ash` to use the Vulkan Portability implementation provided by `gfx-hal` as a C library. &gt; And I read that gfx is also used as a testbed for gpuweb. FYI, prototype is based on Servo, code and examples are [here](https://github.com/kvark/webgpu-servo). &gt; Is the end goal to be able to compile a Rust/gfx application to Wasm/gpuweb? No, it's not the end goal. Major end goals are: 1. Nicest portable graphics for Rust 2. Vulkan Portability implementation 3. GPUWeb prototype Getting Rust/gfx to compile to Wasm/gpuweb would be a nice side-effect of these goals.
That's a technical choice. Ash doesn't provide the level of type-safety necessary to provide a compile-time guarantee that the Vulkan code won't produce errors due to misuse. The purpose of gfx-hal is to abstract over the unsafe API provided by gfx-ll, which is more or less an equivalent to Vulkan/ash but implemented over other backends, to provide that kind of provable runtime safety.
This is great, excited about the ZKP talk =).
Well, when using `error-chain` in production on Linux, I saw (with `perf`) than almost 90% of the time was spent in `error-chain` (actually `backtrace` which is a dependency of `error-chain`). So, this is not only much slower on Mac.
&gt; It could even be Result&lt;T, ()&gt;; does that benefit from the optimisation where an Option of a pointer requires no extra space? [Apparently not](https://play.rust-lang.org/?gist=a4ff2e9dad4a3ded0848095380b9276a&amp;version=stable).
Hey, great post. What did you use to get the metrics for data rate output? 
I just ran into problems with debugging last night when writing a compile-fail test for a lint for [`clippy`](https://github.com/rust-lang-nursery/rust-clippy). For some reason my lint wasn't firing, and I wanted to step through the code in `lldb` as the test ran to see where things were going wrong. But I could not for the life of me figure out how to do this. Does anyone have a good article on using `lldb` with rust? Caveat: I suspect my case was sort of an extra pathological one because of the nature of clippy as a compiler plugin.
Rust doesn't have linear types. Rust has affine types. https://gankro.github.io/blah/linear-rust/ 
So what if I use both forms? ;) I use each in different contexts - but consistently. I use the former form ("a SQL query") if used in the middle of a sentence, but the latter form if used at the beginning of a sentence.
That's quite interesting! If I stumbled across that, I would be quite amused because I always think about this whenever I see it.
In my case, I think it's to do whether the focus of the sentence is on the query, or whether the query is a means to an end for whatever the host sentence concerns. The grammar state machines in our brains are weird. Haha :)
If I'm reading you right, I think there's nothing preventing the `map` closure from calling `set` on the same cell, even though it has a shared reference to the contents?
If you like the video subscribe for more rust and other content 
I like it too :) but Context is important as well, context allows to isolate domain specific functionality from business logic. i.e. Cantext&lt;A&gt; vs FramedContext&lt;A&gt; vs HttpContext&lt;A&gt;
actix does not provide any remote Actor functionality at the moment. I am planing to add remote actors, but that's would be probably separate library built on top of actix.
you are welcome to join the effort to make good library.
Not in any way that's guaranteed.
[This example](https://play.rust-lang.org/?gist=c6f24e722417171e69689dc61a1ae655&amp;version=stable) makes it look like it does?
You’ll probably have more luck on /r/playrust
I am planing to look into async/await but I am very busy right now with validation of basic concepts of actix. But any help is welcome. I can move actix repo to organization if anyone wants to participate in dev process 
Thanks man
That's unfortunate. Can you explain why this is (I am largely ignorant of the workings of LLVM) ? Is this a limitation within LLVM, or a problem that is largely or totally insoluble (eg. you *must* have direct control over the hardware). I would *think* that it is the former, because C is at least partially accepted as a means of implementing cryptography, and it is compiled. There is of course a significant fraction of the crypto community that insist on pure assembly use for this purpose, but my position on this is that it is overly hardline; there is more risk potential for side channel exploitation contributed by the design and implementation of the x86 architecture itself, than of what carefully tuned compilers may contribute.
Some suggestions: * Use [Error Chain](https://github.com/rust-lang-nursery/error-chain) for your errors * Maybe look at a parser combinator like [pom](https://github.com/J-F-Liu/pom) or [nom](https://github.com/Geal/nom)
You *don't* have to repeat `extern crate` in submodules. You haven't shown your actual code, so I have to guess here, but: external crates are *not* global in Rust. When you use `extern crate`, you bring the external crate into that scope, and not everywhere else. Like any other name, if you want to refer to it from another module, you have to specify where it's coming from. Think about it like a UNIX filesystem; just because you mount a disk or drive doesn't mean it's immediately visible from everywhere else in the filesystem. If you think of `main.rs` as the root directory, and `mod1` thru `mod3` as subdirectories, you have something like this: ╶ / "main.rs" ├ mod1/ "mod1.rs" │ └ ... ├ mod2/ "mod2.rs" │ └ ... ├ mod3/ "mod3.rs" │ └ ... ├ mysql/ -&gt; extern "mysql:12.0.3" │ ├ Conn │ ├ Pool │ ├ from_row │ └ ... ├ regex/ -&gt; extern "regex:0.2.2" │ ├ Regex │ ├ escape │ └ ... ├ slack/ -&gt; extern "slack:0.19.0" │ ├ RtmClient │ ├ Sender │ └ ... ├ std/ -&gt; extern "std" // implicitly added to root module │ ├ any/ │ │ ├ Any │ │ └ TypeId │ ├ ascii/ │ │ ├ AsciiExt │ │ ├ EscapeDefault │ │ └ escape_default │ ├ borrow/ │ └ ... ├ main // i.e. fn main() { ... } └ ... Modules are directories. Crates are filesystems that you mount into the filesystem, and appear as directories. Functions, aliases, structs, enums, constants... basically anything that isn't a module or linked crates are like files. ^[1] So imagine you're in a shell, and the current directory is `/mod1`, and you want to refer to `slack`. It's not in the same directory, so you can't just say `slack`, you need to either specify a relative path (`../slack`) or an absolute path (`/slack`). In Rust, you just replace `/` with `::` and `..` with `super`, which gives you `super::slack` and `::slack`. Adding `extern crate slack` to `mod1.rs` is basically just mounting/linking to the `slack` crate a *second time* in a different place. Which would look like: ╶ / "main.rs" ├ mod1/ "mod1.rs" │ ├ mysql/ -&gt; extern "mysql:12.0.3" │ │ ├ Conn │ │ ├ Pool │ │ ├ from_row │ │ └ ... │ └ ... ├ mod2/ "mod2.rs" │ └ ... ├ ... ├ slack/ -&gt; extern "slack:0.19.0" │ ├ RtmClient │ ├ Sender │ └ ... └ ... Also, I'm not sure, but you seem to suggest you're adding `pub mod mod2` to `mod3.rs`. This is also wrong. `mod` defines where a module appears in this tree. Just like any other thing (a struct, a function, a static), it should only appear *once*. Adding `mod mod2` to `mod3.rs` is creating a new, totally different module that's *also* called `mod2` in a different place. That would create this kind of tree: ╶ / "main.rs" ├ mod1/ "mod1.rs" │ └ ... ├ mod2/ "mod2.rs" │ ├ struct1 │ ├ struct2 │ └ ... ├ mod3/ "mod3.rs" │ ├ mod2/ // not the same as the other one! │ │ ├ struct1 │ │ ├ struct2 │ │ └ ... │ └ ... └ ... `mod` *declares* a module, `use` brings the name into scope. What you want is something like this: ╶ / "main.rs" ├ mod1/ "mod1.rs" │ └ ... ├ mod2/ "mod2.rs" │ ├ struct1 │ ├ struct2 │ └ ... ├ mod3/ "mod3.rs" │ ├ mod2/ -&gt; /mod2 │ │ ├ struct1 │ │ ├ struct2 │ │ └ ... │ └ ... └ ... So, rather than making a hard copy of `mod2`, you just want a reference to the existing one. In a filesystem, this would be a symlink. In Rust, it's `use`. The symlink inside `/mod3` would be to `/mod2`, so in Rust you'd write `use ::mod2;`. Because `use`-ing things with an absolute path is so common in Rust, it actually defaults to absolute paths in `use` items, so you can write `use mod2;` (the leading `::` is implicit). If you really want a *relative* `use`, you can write something like `use self::struct1 as an_alias_for_struct1;`. --- \[1]: OK, so things like associated items make this not *quite* work. You have to extend the idea to have files that *also* behave like directories. If it helps, you can think of those as like Zip archives that the filesystem will magically turn into directories for you.
The problem here is that `extern crate` both declares a crate, and _brings it into scope for use_. in short: The "right" solution is to do all the `extern crate ...` in `main.rs`, and in each mod do `use slack;`, `use regex;`, or `use mysql;`. Since the crates have been brought into scope into `main.rs` (the "root" of the module path), each module can bring it into its own scope with a `use top_level_name;` statement. This is one of the aspects of confusion about the rust type system, and something that _might_ be changing in the future (specifically, removing `extern crate`). For now, there are a number of resources which can explain it much better than I can. I'm going to link https://manishearth.github.io/blog/2017/05/14/mentally-modelling-modules/, not because it explains extern crate, but because it explains "the whole system". In rust, modules within a crate and extern crates behave pretty equivalently right now - so things about declaring modules also relate to bringing external crates into scope. Other people most likely have better links for explaining the actual issue, but given that no one else has commented yet, maybe this can be a start.
That is demonstrating that the null-pointer optimisation is occuring: the size of a `&amp;i32` is 8, so the `Result&lt;&amp;i32, ()&gt;` is the same size.
My problem is that while I have an extremely strong, diverse software background and a rather decent hardware background (from designing two layer PCBs with 100+ components or four layer high-density PCBs with a Bluetooth antenna, among other things, prototyping and assembling them, and programming them), my academic performance has always been subpar. Like 3.1 GPA range. I would love to get into the research field, but I generally dislike the stress of academia as a student. I do plan to get a master's in engineering at some point, at a minimum, though.
Checkout this repo https://github.com/mubaris/yes
Ah so it's the implementation and not the usage that is conservative. I think the documentation could be a little clearer.
What's your end goal for this, or is it just a learning rust project? In any case it looks pretty nice, but there are a few points I'd work on more: LexingError looks pretty good, but it seems to entirely ignore the 'cause' internal. In the Display implementation, I'd recommend using [`write!`](https://doc.rust-lang.org/std/macro.write.html) to write the message and the cause rather than just a static message. Another error handling point: Box&lt;Error&gt; is completely fine for binary crates which don't do much besides printing errors, but most libraries will expose a single "Error" enum which contains possible errors for the crate. Some libraries will use the "error-chain" crate for this, but I'd recommend doing it manually to start with. Error chain is nice and quick, but hides a lot of details and can make the errors a bit opaque. Something like the code described [here](https://rustbyexample.com/error/multiple_error_types/wrap_error.html) is used in many libraries and modules, including `std::io` (see `std::io::Error`)
You are welcome to join! 
&gt; There are a lot of reasons two traits might share the one of the same functions but be different Hmm... can't speak for everyone but this seems rare to me. And again, we're not just talking about the same name, but the same *meaning*. If two traits have a function that has the same name and same meaning, but a bunch of other unrelated functions, that's probably a sign that that function should be modeled by a third trait that the other two inherit from.
I'm voting for "ref T" as short for reference. It's also how you would pronounce `ref T` in patterns which does the same thing.
Yeah I think given the way this works in rust, that makes the most sense. It still just seems foreign to me.
Congrats, guys! It's a big step, even if there's still a long way to go. Please pass my thanks on to everyone else working on gfx-rs. It's a super ambitious project and when I first started playing with it in the hopes of using it for ggez I didn't *really* expect it to be usable; I sort of felt like I was diving into uncharted waters. It certainly wasn't a shallow learning curve, but the results have been totally worth it. There's been bugs and friction and occasional weirdness, but you guys are fantastic about finding and fixing bugs super fast and helping people figure out how to do things when they're lost in a sea of twisty trait implementations, all alike. And it just occurred to me: there have been *zero* times working in ggez where someone asked "can I do X?" and the answer was "theoretically but gfx-rs doesn't allow it yet". ggez's graphics capabilities hardly push the envelope, but that's still really cool. And now you're apparently planning on doing things I didn't even think feasible, like writing shader compilers and papering over coordinate transforms? Just amazing. The future looks bright!
This is to avoid an ambiguity when `PrimInt` is implemented that could also work here. For example, it's possible to create a type which when added `u32` will give `u32` back. This is very unlikely to happen, but it technically could happen in unrelated crate. struct Example; impl Add&lt;Example&gt; for u32 { type Output = u32; fn add(self, other: Example) -&gt; u32 { println!("Adding Example..."); 0 } } impl PrimInt for Example { // Yeah, no, not going to implement 25 traits or so. But it's // theoretically possible, so Rust wants to avoid possibly breaking code. }
I totally works and I feel stupid to not have seen this before. However, it is strange that it worked with only one parameter before (without specifying the length). But I still cannot succeed to return an array. The examples I have read said to modify the value of a pointer previously given but I failed to do so. Here is my current implementation in Python : ffi=FFI() ffi.cdef("void print_x(float *x,size_t x_len, float *y, size_t y_len);") C=ffi.dlopen("path/to/lib/lib.so") input_arg=ffi.cast("float *", list_x[0].ctypes.data) output_arg=ffi.cast("float *", np.ones(12).astype(np.float32).ctypes.data) C.print_x(input_arg,12,output_arg,12) And my Rust code : #[no_mangle] pub extern "C" fn print_x(input : *const f32, len_input : size_t, output: *const f32, len_output: size_t) { let x = unsafe { assert!(!input.is_null()); slice::from_raw_parts(input, len_input as usize) }; // output should be changed with the x value } 
I've got two questions first. How to paste my code on playground and share it. Next I tried to make a frequency tool with HashMap because in the rust programming book it said HashMaps are otherwise known as assosciated arrays. And in javascript the way people teach you assosciated arrays is by makeing a frequency tool. I tired to do that in rust and sorta hit a road block. Can somebody tell me how can I do what I want to do. The error message is: immutable borrow occurs here | mutable borrow occurs here I understand what it means but I don't know how to get over it. https://pastebin.com/M8Erf0tR
Funny thing today I was looking how to do for_each in rust.
[removed]
_Typically_ if your advisor says its ok you can enroll into a program. Grades are important as a filter if you enroll into a program without having an advisor, and expect to find one during the program. In that case you are just one number, there is a limited number of places, and grades are just another number that can be used to assign those places "safely" (as in, nobody will get fired if they use grades as the criterion to decide between one student and another). 
So IIUC we could add a `MapCell` trait to std that is implemented for `Cell&lt;Option&lt;T&gt;&gt; where T: Default` and basically provides that `map` function. I think this is worth filling an issue in rust-lang, and maybe even a PR to the [`mitochondria`](https://crates.io/crates/mitochondria) crate as a first step towards experimenting with this in a more general context. 
I just tried this out in Java with IntelliJ and I must say you are right. There is no warning at all. I also couldn't find anything in findbugs or checkstyle. [At least a PMG rule seems to exist](https://pmd.github.io/pmd-5.8.1/pmd-java/rules/java/design.html#PreserveStackTrace). Yay! Then I wrote the same in python and pylint didn't find anything to complain. Then I wanted to try that in go and remembered that you cannot re-throw panic in a way that preserves the original stack trace. So point taken :)
Strange, after doing a `rustup update` the last lines are: ``` stable-x86_64-unknown-linux-gnu updated - (timeout reading rustc version) nightly-x86_64-unknown-linux-gnu updated - rustc 1.22.0-nightly (dcbbfb6e8 2017-10-12) ``` (Note the timeout). Running it a second time shows the 1.21.0 version number correctly.
why isnt it const?
Good point. Actually neither should be public to prevent people from messing with `len` too, causing potential unsafety.
Take the `&amp;mut Point` as an argument, and pass `self` explicitly: struct Point { x: i32, y: i32 } impl Point { fn add(&amp;mut self) -&gt; i32 { self.x + self.y } fn action(&amp;mut self, act: &amp;mut FnMut(&amp;mut Point) -&gt; i32) -&gt; i32 { act(self) } pub fn print(&amp;mut self){ println!("{}", self.action(&amp;mut Point::add)) } } 
Thank you, Your Holy Macroness
Go in peace, my child.
Nice. A [more fleshed out example](https://play.rust-lang.org/?gist=664770921581902d3faccf2e80b1e1b9&amp;version=stable) might helpful for people like me who haven't had their coffee yet. The output: Result&lt;i32, i32&gt;: 8 vs. Result&lt;&amp;i32, i32&gt;: 16 Result&lt;i32, ()&gt;: 8 vs. Result&lt;&amp;i32, ()&gt;: 8 Option&lt;i32&gt;: 8 vs. Option&lt;&amp;i32&gt;: 8 i32: 4 vs. &amp;i32: 8 Pointers are eight bytes; Option&lt;&amp;i32&gt; and Result&lt;&amp;i32, ()&gt; both manage to store an optional pointer without needing any extra space. I reckon you could probably get a Result&lt;&amp;i32, i32&gt; in eight bytes by stealing a low-order bit as a tag bit; is has to be zero for the pointer case, and isn't needed for the i32 case. That only works if you're sure the i32 being pointed to is always aligned naturally, though, which i suppose you don't, even though it probably always will be. 
Why does `Point.add` use a mutable reference? As far as I can tell (granted, I'm a Rust noob), a normal reference would be enough as it doesn't modify the state.
&gt; "Both the methods need mutable access to self, ..." Presumably there's a reason, and the example is over-simplified. 
The argument I would have for that latter bit (won't touch the first bit), is that going too far with implicit coercions gets quite crazy, but also Rust is insanely over-explicit and it's painful to write sometimes. There needs to be a nice middle ground.
I know that the example is simplified, but it still bothers me.
Alternatively, you can make `action` generic: fn action&lt;F: FnOnce(&amp;mut Point) -&gt; i32&gt;(&amp;mut self, act: F) -&gt; i32 Note that, because you don't reuse it, you don't actually need `act` to be `FnMut`, but you can accept `FnOnce`.
I would like to note that header files are going away... soon-ish. modules ts is pretty terrible, but it seems to me like people would like to scrap it, thank god. monadic error handling is coming. deriving is maybe coming, ish, with metaclasses. people (including me) are working on the latter thing. And I don't agree with the coercions thing, at least in theory - in practice in C++ it gets a little crazy, but I think with more sane rules, coercion is completely fine.
[This should do it!](https://crates.io/crates/cargo-do)
- In the playground, you should see a share button in the top toolbar - If you rewrite your code with [the Entry API](https://doc.rust-lang.org/std/collections/hash_map/struct.HashMap.html#method.entry) it should solve your problem
ldd is a cross linker by default (the target is set at runtime, not compile time, of the linker), so with it you'll only need one toolchain and that's it.
Yeah actually I too arrived at a the same conclusion, this way it makes dealing with lifetime a bit easier.
I always say "ref", so &amp;5 is "ref five" to me
`needs_drop`? I don’t know. Maybe it could be but nobody considered making it so yet? Feel free to file a bug.
Yep, for here add doesn't require `&amp;mut`. I just used it to demonstrate the error I was getting.
I'm really surprised at your measurements for Arc::clone are in the hundreds of ns, it's pretty much a `lock xadd` followed by a super predictable branch.
The measurement also includes the time to drop the cloned Arc in the caller (without deallocation of the underlying object, which remains cached). Still, the performance of cloning Arc&lt;String&gt; compared to cloning String is totally different from what I would expect.
I don't see how it would do anything to the executable size. A constant value in your code will be there whether its scope is static or not. It's pure data, defined at compile time.
I also really liked Rouille, my only concern with it was that it's built on tiny-http, which doesn't look very actively developed (at least compared to hyper). I may be completely wrong on that however, was just going by looking at github activity.
Teams, as in groups of players working together? I'd probably have a separate entity per team, with a single "Team" component that carries the information about the team (total score, color, spawn point locations, etc.). Each player component would reference the team it belongs to (via an entity pointer - https://slide-rs.github.io/specs-website/docs/docs/0.10/specs/struct.Entity.html), and probably vice versa. I'm not sure what's the best way to handle input. In PlanetKit I'm doing something like... - I have an `InputAdapter` trait (https://github.com/jeffparsons/planetkit/blob/master/planetkit/src/input_adapter.rs) - I register a bunch of these in my app. When the user presses a key or something it dispatches input via all the registered adapters (search for "input_adapter" and "InputAdapter" here: https://github.com/jeffparsons/planetkit/blob/master/planetkit/src/app.rs) - Specific systems have their own implementation of `InputAdapter` that interpret the event as a system-specific event like "try to move forward", then shove that into a channel that the system will slurp events from. (E.g. https://github.com/jeffparsons/planetkit/blob/master/planetkit/src/cell_dweller/movement_system.rs) This is probably a little over-the-top for getting started, and I'm also just fumbling along trying to find things that work for me. You might be better off looking at something a little more self-contained/cohesive like https://github.com/kvark/yasteroids for inspiration, rather than something like PlanetKit that aims to let you mix and match systems in a bigger project.
Was the same counter used from multiple threads? In my C# test program an interlocked increment, decrement, zero check took 13ns per iteration in a single-threaded scenario (the zero branch was never taken).
&gt; Was the same counter used from multiple threads? Yes, in this case read-heavy multi-threaded usage is the whole point.
I don't think it is a kernel or allocator bug. You cannot expect to get a determinate value if you read uninitialized memory, and any assumptions made, like "I expect the value to be the same if I read at the same address twice" can be unfounded. There are no guarantees. That's why unsafe code can be dangerous; it's very very easy to overlook some remote corner case while being clever. I do not think reading allocated-but-uninitialized memory is immediately undefined behaviour, that is I do not think the program is allowed to for example crash (undefined behaviour lets the program do *anything*). If I remember well, the terms used are that reading uninitialized memory will give an indeterminate value. But it can easily lead to undefined behaviour if you do some decision depending on that indeterminate value.
[Related StackOverflow post](https://stackoverflow.com/questions/40027741/how-can-i-use-a-cgmathmatrix-as-a-uniform-parameter-in-glium/40028032#40028032)
Thank you very much. Here is the end result I can know it's not the most efficient solution. But still, I am proud of it. https://play.rust-lang.org/?gist=049c44f2c5a4332c289b3052fc2bd92b&amp;version=stable
that is very useful advice thanks! Happy there is a new rust REPL project since rusti seems problematic with nightly builds
Interesting stuff :-) One could kind of claim that the Java one is not lock free; because of GC pauses that lock the world. I would kind of argue that if Java is allowed to lock the world occasionally, then your Rust spinlock should be able to occasionally spin, too, in order for the comparison to be fair. Also, I'm not sure about the Java details so forgive me if this is all wrong; but is the Java version really correct w r t reading and writing of `transformed`? Is it possible that if `T` is larger than a pointer, reading `transformed` might occasionally read half an old object and half a new object in case a write is being done from another thread at the same time? And if `T` is an object, that object must in turn be thread safe internally, something you don't require with Rust, since you return a clone instead. 
Yep! I implemented a [wrapper function](https://github.com/urschrei/lonlat_bng/blob/master/src/lib.rs#L126) to parallelise various transformation functions using Rayon, and it worked very well. You just pass in the function, and slices for horizontal and vertical coordinates. I used crossbeam for multithreading initially, but the Rayon version is much simpler, and has a slight performance edge.
&gt; One could kind of claim that the Java one is not lock free; because of GC pauses that lock the world. I would kind of argue that if Java is allowed to lock the world occasionally, then your Rust spinlock should be able to occasionally spin, too, in order for the comparison to be fair. One could claim that, but one would appear to others as a sore loser. In addition, [some JVM implementations](https://www.azul.com/resources/azul-technology/azul-c4-garbage-collector/) do not lock the world to GC, and besides, a JVM instance with sufficient memory could run the benchmark without ever running GC.
I didn't I think your benchmark there is a bit too extreme. The consumer resembles a busy loop fetching the shared state, almost like a spin-lock. Usually, the pinning cost is small compared to the actual work done while holding the pin. Inserting or searching into a concurrent skip list involves a lot more work that the example used. In simpler cases, like queues, the overhead is more noticeable but it's still competitive. There's a lot of tradeoffs when you compare epoch reclamation to Java GC, I won't even try to go into details because I think somebody will do a better job than me eventually. Lastly, the crossbeam pin overhead is reduced these days, there's a lot of work going on in the new repo https://github.com/crossbeam-rs/crossbeam-epoch
From what I understand, GCs that don't lock the world actually do. They just locking the bare minimum. They aren't locking everything for the entire scan. I don't know what Azul do, but there are good reasons why every GC that is similar will stop the world somewhere.
Btw, could `crossbeam::sync`'s primitives be an option here? Not the epochs - just like this: pub struct LazyTransform&lt;T, S, FN&gt; { transform_fn: FN, source: sync::AtomicOption&lt;S&gt;, value: sync::ArcCell&lt;Option&lt;T&gt;&gt;, } 
I just saw that your benchmark is for extreme contention, so I think it's a reasonable number.
Azul gets pretty far with clever use of write barriers, and is indeed usually capable of only stopping individual threads while their root set is scanned. Of course, nothing really prevents all threads from accidentally all doing that at the same time. And there's always the pathological case where application threads generate garbage faster than the GC threads can clean it up. In that case, the world will have to be stopped once the heap is full. But quite honestly, that's essentially just an out-of-memory condition. And let's not forget that Rust's memory allocator isn't O(1) either (by default, on non-embedded systems). Getting a new page from your kernel takes however long it takes, and may very well be serialized behind requests by other threads.
&gt; One could kind of claim that the Java one is not lock free; because of GC pauses that lock the world Sure, but that could be countered by the claim that the Rust version is not really lock-free due to the locking performed by jemalloc. (The Rust version allocates even when returning Arc&lt;T&gt; because it needs to internally box the stored value to provide atomic access.) For this experiment I chose to give both a free pass and concentrate on the lock-free properties in the area which are under the control of the program. For what it's worth, the benchmark tries to create some amount of garbage in both languages, so as to trigger allocation and GC. The broken-down benchmark avoids allocation in both languages, except for the allocation where the `String` is cloned (with a clear note that this is the case). &gt; Also, I'm not sure about the Java details so forgive me if this is all wrong; but is the Java version really correct w r t reading and writing of `transformed`? `transformed` is volatile, so an access to it is equivalent to Rust's `SeqCst` load of an atomic pointer, so it will pick up the whole object. &gt; And if `T` is an object, that object must in turn be thread safe internally, something you don't require with Rust, since you return a clone instead. The Rust version still requires `T` to be `Sync`, so there is *some* constraint on what could be stored - for example, `T = Rc&lt;usize&gt;` should be rejected. The Rust version is designed to be as flexible as possible through the use of zero-cost abstractions. For example, while it is true that the code clones the returned value, you have the choice of using `Arc&lt;T&gt;` to effectively avoid the clone and get the semantics in spirit equivalent to the Java version. This is why I was unpleasantly surprised when the benchmark showed that such use of `Arc` comes with a steeper price than one would think.
`AtomicOption` might be usable, although it is missing a "relaxed" check for None, which would be needed to implement `get_transformed` efficiently. `ArcCell` is the bigger problem - it uses `Arc` internally, so I would expect it to inherit the performance characteristics of `Arc`. Reference counting is generally considered a *slower* alternative to other reclamation strategies - see e.g. page 3 of [this paper](http://www.cs.toronto.edu/~tomhart/papers/tomhart_thesis.pdf). Epoch-based reclamation sounds heavyweight compared to an `Arc`, but it is in fact specifically optimized to be more efficient.
I'd like to note that cloning an Arc is a lot faster on newer CPUs (most ARM cores, intel haswell and newer, amd ryzen and newer). I believe this is because these chips in fact support relaxed memory ordering, which was not the case with x86-64 chips before.
&gt; but is the Java version really correct w r t reading and writing of transformed? Is it possible that if T is larger than a pointer, reading transformed might occasionally read half an old object and half a new object in case a write is being done from another thread at the same time? No. Firstly, in Java, objects are always referred to via pointers; in a LazyTransform object, the transformed field is always a pointer (you can't instantiate generics on primitives - but maybe in Java 10!). Secondly, it's guaranteed that reads from the memory pointed to will see consistent values. The Java memory model is defined in terms of something called [happens-before order](https://docs.oracle.com/javase/specs/jls/se9/html/jls-17.html#jls-17.4.5). The first rule (where hb(x, y) means that x happens-before y) is: &gt; If x and y are actions of the same thread and x comes before y in program order, then hb(x, y). All the writes that are done to initialise the transformed object are done by the same threads that writes it into the transformed variable, and come before it in program order, so they happen-before it. The third rule is: &gt; If an action x synchronizes-with a following action y, then we also have hb(x, y). This depends on another definition, of synchronizes-with, of which the second rule is: &gt; A write to a volatile variable v (§8.3.1.4) synchronizes-with all subsequent reads of v by any thread (where "subsequent" is defined according to the synchronization order). So, the write to transformed happens-before any read from it that sees the new value. The fourth rule is: &gt; If hb(x, y) and hb(y, z), then hb(x, z). That's just transitivity. So, reads from the transformed object in a thread which gets it from the volatile variable happen-before all writes to the transformed object in the thread which put it there. &gt; And if T is an object, that object must in turn be thread safe internally, something you don't require with Rust, since you return a clone instead. True. The usual approach here in Java would be that the object is immutable. If you need to produce a mutable object from the transformation, then yes, you have more work to do. 
I'd like to note that cloning an Arc is a huge lot faster on most newer chips: most ARM cores, haswell+, ryzen+, as I found from my own measurements. There was a noticeable jump in performance of a benchmarking program that heavily cloned lots of Arcs by moving from set of pre-haswell to haswell+ and ryzen chips. Additionally, in certain cases a slower but newer mobile chip i5 6300U would outrun a desktop i5 2500k at Arc clone spam despite the latter performing over twice as better when contention is low.
Change the `*const f32` to `*mut f32` and use `slice::from_raw_parts_mut`.
Note that this paper could be considered somewhat outdated by now as performance of atomics improved a lot in newer compute cores (including x86-64), especially those that support relaxed memory ordering (e.g. ARM cores).
Point taken, but using `Arc` in the benchmark (performed on 2012 x86-64 hardware) seems to confirm that epoch pinning is still significantly faster than atomic reference counting under contention.
I'm embarassed by how helpful this has been XD
This was pretty much meant to be a quick review for me, I've been using rust on and off but it's been about two years since I've written anything serious. Just wanted to be sure I wasn't forgetting anything important. Also looking at the code in the link it makes a problem I was having earlier. I was trying to create an error chain at one point but mistakenly derived From&lt;Result&lt;T, ErrorType&gt;&gt; instead of From&lt;ErrorType&gt; Thank you for your help!
 &gt; (The Rust version allocates even when returning Arc&lt;T&gt; because it needs to internally box the stored value to provide atomic access.) Hmm. So I tried running your benchmark with `String`, `Arc&lt;String&gt;` and `Arc&lt;str&gt;` as Payload, and for some reason `Arc&lt;str&gt;` is quite a lot *slower* than `Arc&lt;String&gt;`. (Here, `String` is ~130 ns, `Arc&lt;String&gt;` is ~380 ns, `Arc&lt;str&gt;` is ~590 ns.) &gt; transformed is volatile, so an access to it is equivalent to Rust's SeqCst load of an atomic pointer, so it will pick up the whole object. I've just read on Java's volatile and it's something quite different from Rust's volatile, so I stand corrected. &gt; The Rust version still requires T to be Sync, so there is some constraint on what could be stored - for example, T = Rc&lt;usize&gt; should be rejected. T must be `Send`, whereas the Java version would have required `Sync` in case it existed in Java. &gt; This is why I was unpleasantly surprised when the benchmark showed that such use of Arc comes with a steeper price than one would think. Indeed.
&gt; Edit: Third question why Rust doesn't have infinitive recursion. Historically, it's because llvm didn't support it on enough platforms, but beyond that, it's got a non-trivial design that needs someone to do it, and nobody has.
&gt; T must be `Send`, This is not true, `T` must actually be `Sync`. This is because `get_transformed` invokes `T::clone` on **the same** cached `T` object from multiple threads without an exclusive lock, which would not be safe for types that are `Send` but not `Sync`. (`Cell&lt;usize&gt;` is an example of such type, and its use as `T` is indeed rejected at compile-time.) The same applies for the [fine-grained lock version](https://morestina.net/blog/742/exploring-lock-free-rust-1-locks#Fine-grained_locking) where `T::clone` is invoked while holding the `RwLock` for reading, and it is the `RwLock` type that which requires a `Sync` type in order to be `Send` itself. Only the first version of the code requires `Send` but not `Sync` because it invokes `T::clone` while holding an exclusive lock. Requiring `Sync` is not a crippling constraint in practice - most Rust types are `Sync` because `Sync` only allows parallel invocation of methods that take a *shared* reference. For example, a `Vec` is both `Send` and `Sync`, it's just that you won't be able to mutate it from multiple threads without the use of mutex or similar. &gt; whereas the Java version would have required Sync in case it existed in Java. Agreed.
&gt; I do not think reading allocated-but-uninitialized memory is immediately undefined behaviour, https://doc.rust-lang.org/reference/behavior-considered-undefined.html &gt; Reads of undef (uninitialized) memory
The absolute number might be reasonable, but what I found truly surprising was that it's actually faster to clone and then drop a 1000 byte `String` than to clone and drop an `Arc&lt;String&gt;`. It is true that the contention is high, but the same contention applies to the allocator, which operates on a global resource. The results were so different from my intuitions about the relative costs of `Arc::clone` and `String::clone` that I had to check them several times before I believed the numbers. At least in this artificial benchmark, jemalloc's segmented locking works exceedingly well, clocking at ~45 ns per allocation/deallocation pair.
&gt;I reckon you could probably get a Result&lt;&amp;i32, i32&gt; in eight bytes by stealing a low-order bit as a tag bit; is has to be zero for the pointer case, and isn't needed for the i32 case. That only works if you're sure the i32 being pointed to is always aligned naturally, though, which i suppose you don't, even though it probably always will be. Is the discriminant required to be directly represented? I know the actual values have to be (so you can't put the tag for a `Result&lt;bool, bool&gt;` in the unused bits), but I'm not sure about the discriminant itself.
&gt; It is true that the contention is high, but the same contention applies to the allocator, which operates on a global resource. Not necessarily. By default Rust uses jemalloc, which has tcmalloc-style per-thread caches (~freelists) of "small allocations" (IIRC up to 32KiB).
£1,114 for a 'full' (3 day) ticket?? Squarely aimed at corporations, I guess.
I gave a talk at this conf last year, it was a lot of fun! http://www.codemesh.io/codemesh2016/steve-klabnik
I've had some experience at work in porting a C++ based JNI (Android) library to Rust years ago. The process was a bit involved as you will need to cross compile libstd by yourself. Nowadays using rustup.rs you can get precompiled libstd pretty easily for all the main mobile architectures. Adding support for more targets is not hard either - In fact I also contributed to rustc's [Android x86 support](https://github.com/rust-lang/rust/pull/27957).
Does [this example](https://github.com/iron/router/blob/master/examples/simple.rs) from the router crate help?
&gt; the RwLock type that which requires a `Sync` type in order to be `Send` itself I was looking at this last week and got confused. Why does `RwLock` require `T: Send + Sync` for the lock itself to be `Send`? Why isn't it good enough that the contents are just `Send`?
Discriminant is a nice thing, but I wonder, why it wasn't implemented using some unsafe Enum trait automatically implemented for all enums. Anyone knows more about this?
Note that Cargo will not automatically add versions to your docs yet! That will require a separate change to Cargo itself, and possibly some discussion about how to handle the currently unstable flag in rustdoc. In the meantime, if you render your own docs to host and want to use this (whenever it hits a nightly), i’d suggest the following: cargo rustdoc — -Z unstable-options —crate-version 1.3.37 (My phone may have turned the double-hyphens into em dashes, so careful if you copy/paste that `&gt;_&gt;`) This will render docs for your own crate only, passing the flag to let rustdoc print the version into the docs. Until Cargo can put the flag in itself, this will have to do for one-off crates. Larger collections of crates that host all their docs together will have to manually add each flag to each doc invocation, and overwrite the `index.html` for the specific crate with the one that has the version in it. Sorry that it’s not that convenient right now, but it’s just a first step to getting this out everywhere. There’s more work to be done to get this to where it Just Works.
See also: * https://manishearth.github.io/blog/2017/01/14/stop-ascribing-meaning-to-unicode-code-points/ * https://docs.rs/unicode-segmentation/1.2.0/unicode_segmentation/
I've been writing a [lock-free embedded database](https://github.com/spacejam/sled) in rust. I hit a ton of issues with the current crossbeam, and have been using coco (effectively the prototype for the upcoming crossbeam-epoch crate) with success. I have barely spent any time performance tuning yet, but it's already clear to me that I will probably need a thread-local object cache. But I'm optimistic that I'll be able to obliterate non-rust embedded database performance due to rust's memory semantics, and being able to share state with the application directly without needing to copy it into a safe buffer first like in other languages. When it comes time to handle the thread-local object caching, I may create a weird fork of crossbeam-epoch/coco to insert the object into the thread-local object cache (or maybe a coco work stealing queue for cross-thread grabbing) instead of the garbage bag, but I'll think more about that when I'm done with a massive storage rewrite of the system.
I don't know. I assume not, because there's no discriminant in the optimised Option case, but i could be missing some subtlety.
I would do it if I could.
Right, `None::&lt;&amp;i32&gt;` has some zero bytes you can pretend are the discriminant, but `Some(&amp;3)` doesn't.
&gt; Perhaps it should instead return an Option-like type which is #[must_use], so you would at least get a warning if you forgot to check. It could even be Result&lt;T, ()&gt;; does that benefit from the optimisation where an Option of a pointer requires no extra space? ...Does it? I wasn't aware that there was any formal spec about how enums were represented. 
If you were actually doing some work in between each interlocked add, the time for the add would completely drop off the map. Memory can only be write accessible to one CPU core at a time, so your benchmark is just spending all its time kicking the atomic int out of the cache of one core back to memory, then loading all the way from memory into the cache of another core. If there was non-insignificant work to do inbetween each atomic add, the cost of those memory operations would probably be hidden because the core could do other work while waiting on main memory.
/r/playrust
/r/playrust
Because multiple readers can access it the same time from different threads.
So how do we effectively slice by rune? I think Go does that by default.
Well done! Made a [couple style corrections](https://play.rust-lang.org/?gist=a07dd5720074bc729b2ef627f3e30e07&amp;version=stable) for you to look at
Do you mean that you would attend if you could? We do offer scholarship places too, for those who would like to attend but cant due to financial reasons? Or do you mean something else :) http://www.codemesh.io/codemesh2017#scholarship-programme
I've designed and built lock-free IPC between Rust and C++ that will be going into production soon. I used: * standard library atomics * eventfd * shared memory * a nice encoding library that we were able to tweak to get us zero copy, zero allocation IPC in both directions. * no drop, no memset/clear, no copy, no clone, no sync/send. 
&gt; I think Go does that by default. No, it's not, it is slicing bytes by default, making corrupt characters in your strings.
I don't know the answer to that for the Tock project; as far as I know, it's just a convention. Which makes me worry when they refer to it as a security feature. There is the ability to use [`#[deny(unsafe_code)]`](https://github.com/rust-lang/rust/blob/master/src/test/compile-fail/lint-unsafe-code.rs) to deny the usage of unsafe code in a module](https://github.com/rust-lang/rust/blob/master/src/test/compile-fail/lint-unsafe-code.rs), though that can be negated by putting an `#[allow(unsafe_code)]` within it in the places you want to use `unsafe`. /u/exobrain, what is it that prevents Tock capsules from using `unsafe`? The papers refer to the language level safety as a feature for running untrusted code in the kernel, but wouldn't you have to at least verify that they don't use `unsafe`? And of course, that also assumes that Rust has no more soundness bugs; because soundness bugs can be used to violate Rust's safety. There are a number of known soundness bugs, and those would all have to be fixed and the language and `core` verified formally to ensure that no other soundness bugs were present before it would be reasonable to say that you could run untrusted code as long as you verify that it does not use `unsafe`. Additionally, while you mention the danger of capsules simply running an infinite loop perform a DoS, could they overflow the kernel stack to cause memory corruption and arbitrary behavior, or is there a guard page and stack probes that prevent that?
/shakes fist That is an oversight.
this is a great writeup, and your blog looks awesome. Thanks for sharing!
This is actually a good question; I wrote the above based on the compiler's error message and inspecting the `RwLock` source. I can't think of a case where allowing allowing `T: Send` (without `Sync`) to imply `RwLock&lt;T&gt;: Send` (also without `Sync`) would be the wrong thing to do. After all, if `RwLock&lt;T&gt;` is not `Sync`, you could only use it in one thread at one time.
Thanks for giving it a read! Trying lock-free Rust was a fun adventure, and trying to explain all that was quite a challenge.
The change was introduced in a [2015 commit](https://github.com/rust-lang/rust/commit/380d23b5d4b9fb8f5f0ebf178590f61528b2483e) that was supposed to remove unnecessary `'static` bounds from stuff that depends on `Send`. For example, it changed: unsafe impl&lt;T: Send + 'static&gt; Send for Receiver&lt;T&gt; { } to: unsafe impl&lt;T: Send&gt; Send for Receiver&lt;T&gt; { } However, in case of `RwLock`, the change mysteriously modified: unsafe impl&lt;T:'static+Send&gt; Send for RwLock&lt;T&gt; {} unsafe impl&lt;T&gt; Sync for RwLock&lt;T&gt; {} to: unsafe impl&lt;T: Send + Sync&gt; Send for RwLock&lt;T&gt; {} unsafe impl&lt;T: Send + Sync&gt; Sync for RwLock&lt;T&gt; {} I am beginning to wonder if this was a search/replace bug that nobody noticed because being able to `Send` an `RwLock` that you can't share among threads is not very useful.
Thanks for the explanation, makes sense :)
I hadn't realized that there was significant work on Tock coming out of UVA. Makes me rather proud of my hometown University. Too bad I'm not looking for a PhD, though.
if you decide to join, just ping me. I'd like to create GitHub organization.
&gt;To my surprise, it turned out that beating Java's performance in lock-free code is remarkably hard That's not really *too* surprising. Concurrency is remarkably hard (as is parallelism) and doing it by hand will only be the most efficient when it's a relatively simple task.
The unicode-segmentation crate.
That's a good point. Looking at tiny-http, it does more or less seem to be in maintenance mode, but one thing I have recently started looking at (particularly for this) was *whether anyone is paying attention*. Even if there is not much actual development on tiny-http, PR's and new issues do still seem to get replies... mostly. I'd probably rate it at 2.5 or 3/5 on "maintenance" compared to the other things I looked at in this article. The Rouille docs do say "Once async I/O has been figured out, rouille will be (hopefully transparently) updated to take it into account", so hopefully this is temporary. It also appears that Tomaka also is the main author of tiny-http, so he would be in a position to do it (if he still had the time and interest).
This is why I'm a card-carrying member of the church of ascii.
You can also use the "is_char_boundary" method to search for the nearest boundary before 250, if you need that kind of human language unfriendly but valid string truncation / slicing. You're guaranteed to not have to look at more than 4 positions before you find a character boundary.
You can deny it via a lint – on mobile right now, but I believe the right compiler arg is `-D unsafe`. For cargo, put this into the RUSTFLAGS environment variable.
Looks interesting but what is the advantage of fluent over gettext? For example, it looks like the support for plural forms isn't nearly as good. On the other hand, the variants system they use for plural forms seems nicer to use than gettext contexts for other cases.
Consider separating construction of MessageContext to separate type e.g. MessageContextBuilder so that it stays immutable once created. Also, why do we need whole parser here? You are given type system and macros, why not use them to your advantage? Also, fluent::context::MessageContext::format method should probably be generic over HashMap&lt;_, _, S&gt;, because in HashMap BuildHasher is part of type. Currently you are limiting users to HashMap with default hasher (SipHash) which is probably not the best idea. Or even better, get rid of using HashMap in user-facing API altogether and provide a more generic way of passing args.
It's a great church, but their international outreach is lacking.
What's the point of having a church if you invite in a lot of people who do nothing but break your rules and make all the good shit about your religion stop working? :D
Thanks for the feedback! This is exactly the kind of canonical rust that we need help with :) I like the idea of immutable MessageContext, but will need to investigate further. At the moment our rust port doesn't do much to its state, but once we start adding more internationalization features, it'll store instances of intl formatters used by the context. We'd like their initialization to remain lazy, which may make the MessageContext inherently mutable :( I like the recommendation to generalize the `format`. We just didn't know that that's how we should do it. If you have time to file issues, that would be appreciated! If not, I'll collect all feedback from here and file the issues myself. Thank you! 
In several of your tests you are wrapping the transformer in a `Mutex`, then all your timed operation require taking this mutex. I don't see how that gives you good lockless timing results(?)
Figured I submit this to the tweet but I wanted to give some shoutouts to the Rust community for making this work: * Nick Fitzgerald for gimli * m4b for goblin [https://sentry.io/We]'re now processing all iOS crash events through rust libraries wrapped wrapped for Python (see [https://github.com/getsentry/symbolic](our symbolic library)) to get an idea what it does. The asterisk are that we still use C++ libraries for C++ and Swift name demangling. We considered using cpp_demangle but that is still too early to use but quite promising. Context for the image: we used to process dSYM files through llvm-symbolizer wrapped into a Python lib. However we effectively only use a tiny subset of dwarf/symbol table data and because of that we ended up wasting lots of CPU and IO doing useless work. Now we generate a custom cache file format out of the original dwarf/symbol table data and process events based on that. Happiness all around because the development experience was great and the end result is significant CPU and cost savings. Ask me anything :)
Ah I see, based on playing with the examples on the website it looked like fluent didn't have any support for plurals but it appears that's just because it doesn't support 'two', 'few', or ranges. When I make a variant for '2' it works correctly. Not sure if that means the JavaScript implementation also doesn't support plural forms fully or if those just aren't supported for English.
I [suggested it](https://github.com/rust-lang/rust/issues/24263#issuecomment-225487140) while implementing the feature but it was [deemed](https://github.com/rust-lang/rust/issues/24263#issuecomment-315773511) too elaborate for now, I guess. It can still be added in later if someone wants to write an RFC.
Have you any thoughts on loading the localization files at compile time? Maybe even doing many of the localization lookups themselves at compile time (e.g. by compiling the HTML templates into one version per supported locale)? This may be out of scope for this library, but it would be nice if it provides APIs which makes this easy to build. One of the cool things, other than the performance benefits, about doing it at compile time would that when compiling you would get the missing localization errors directly and that it should be possible to find unused localizations in some cases.
Have you any thoughts on loading the localization files at compile time? Maybe even doing many of the localization lookups themselves at compile time (e.g. by compiling the HTML templates into one version per supported locale)? This may be out of scope for this library, but it would be nice if it provides APIs which makes this easy to build. One of the cool things, other than the performance benefits, about doing it at compile time would that when compiling you would get the missing localization errors directly and that it should be possible to find unused localizations in some cases.
It also provides clues as to where an author is from. "an herb": American (silent "h") "a herb": not-American (pronounced "h") 
&gt; How is Rust comparing to other systems languages by speed and memory usage? &gt; In places like The Benchmarks Game, they tend to be about equal once the Rust benchmarks have been optimized to the same degree as the C and C++ benchmarks... with one significant exception: Rust doesn't yet have a stable interface for SIMD intrinsics, so SIMD can only be used with nightly builds of the compiler. That's probably the biggest reason that [summarized results](https://benchmarksgame.alioth.debian.org/u64q/which-programs-are-fastest.html) show Rust as slower than C and C++, given that The Benchmarks Game insists on only features available in stable/release compiler versions. (Though, [compared to a wider set of languages](https://benchmarksgame.alioth.debian.org/u64q/which-programs-are-fastest.html), Rust is already the only major language TBG tests that's got performance reliably in the same ballpark.) &gt; Are there optimizations in Rust that are difficult to apply in C[++] or vice versa? Nothing about Rust's design specifically prohibits it from matching or exceeding C and C++ and it's been specifically designed so that precluding optimizations available to C and C++ should only be done in extreme circumstances. (eg. All modern CPUs specify 2s complement wraparound, so Rust removes the "wraparound is undefined behaviour" that the C spec leaves in as an avenue for optimization in favour of specifying is as 2s complement wraparound.) That said, Rust does have two major places where it has the the potential to exceed C and C++: 1. First, when it comes to aliasing-related optimizations. Rust and Fortran both have a theoretical advantage over C and C++ because their aliasing semantics allow the compiler more freedom to reason about the program's behaviour than in C and C++. (Sadly, benchmarks seem to indicate that Fortran loses more than it gains due to other design descisions and Rust doesn't see the benefits yet because LLVM's Fortran frontend hasn't seen much love, so the optimizations not very beneficial to C and C++ need work.) 2. On a more abstract level, Rust makes it more *practical* to design programs that use memory more efficiently, because the ownership system ensures you can write complex systems of borrowing (passing pointers around) rather than copying data without fear of losing track of who's supposed to free memory when.
as I said, that's just because our PluralRules support is mocked in Rust. We intend to support full Unicode Plural Rules categories (one/few/many/other etc.) as well as per-case (0, -1, 1 etc.). Our JS implementation uses recently standardized `Intl.PluralRules` API for that - https://github.com/tc39/proposal-intl-plural-rules In 0.2 I expect to support plural rules fully.
Build times in Rust are a concern right now, but there is a lot work being undertaken to address that. C++ build times are comparable IIRC. Another issue, I think, is the binary sizes being really large. Runtime speeds are very similar. Disclaimer: I don't know what I'm talking about.
low level Fluent tries to stay not opinionated when it comes to when the localization is done. Although we believe that for GUIs localization should happen very late, right before displaying, I can see a case for server side or even compile time. The client-side, runtme localization has the benefit of being dynamic (you can alter the message depending on the conditions like time of the day, available screen width, user selection of languages) and can be updated/changed on fly. So for things like websites and Firefox UI we are moving toward client-side l10n. For command line apps, or some other scenarios where dynamic l10n is not needed, I can see a case of using Fluent at compilation time. I'd hope it would be possible to write a library for that on top of the fluent-rs.
For C you can turn on strict-aliasing for the same rules, however I'm not sure how much in the way of optimizations result from that. I'd expect more aggressive optimizations from icc over gcc/clang. Fortran does seem to take better use of those optimizations. I think exploring aliasing optimization for Rust would make a good research topic.
The binary sizes can be (somewhat) fixed. When I first compiled a release 64-bit linux binary (with only a 50-line main.rs), it was 3.9MB. Yeah. Then I looked it up, and some Cargo.toml editing, plus the gnu `strip` command can get it down 180kB, which is kinda better. Of course, that can still be considered big, but it is miles better. Yeah I have no idea either :D
Laziness can still be accomplished through interior mutability (Cell/RefCell).
The tweet says "ship code with Rust", does this imply that this is the first Rust-based code being run on your own machines (as opposed to the prior Rust code, which I believe was a command-line app for clients)? What was your experience integrating Rust with your build system like?
Thanks for you're help, this cleared up some info!
Thanks for the suggestions!
Author of `fantoccini` here. I've been using it privately for a while, and it's been working fine there, but you're right that it hasn't seen any kind of production use. I don't have any reason to believe it wouldn't be suitable for that, though only trying it would give us an indication.
You need more than that for C, to match Fortan you need to use the restrict keyword as well. Strict aliasing only forbids alias between pointers of different types, the restrict keyword tells the compiler to ignore potential alias between pointers of the same type. Note the exact semantics of restrict are subtle, but Rust's lifetime system, should provide more than enough information to have the same benefits as restrict, without the programmer having to figure where it is and isn't safe. The bugs from a misapplied restrict can be very frustrating to solve especially since they can hide until a compiler/compiler option change. Technically Rust's alias info exceeds what C can represent, but there's currently no way to pass all that info to LLVM.
This looks really cool. I had never encountered the term "embedded database" -- is querying beyond a key within the scope of this project? 
...not to mention that Rust links statically by default to mitigate the lack of a stable ABI, while C and C++ link dynamically by default. A good blog post exploring what contributes to the difference is this one: https://lifthrasiir.github.io/rustlog/why-is-a-rust-executable-large.html
"rune" in Go is a synonym for "codepoint," so "slicing" by rune means transforming a `String` into a `Vec&lt;char&gt;` and slicing that. But, not only does Go *not* slice by rune, there is nothing in the standard library that will do that for you either. (Presumably on purpose. A `Vec&lt;char&gt;` isn't something you want that often.)
Given that, I'd boil down both points to "Rust makes it much more viable to take advantage of optimizations opportunities that are footgun hazards in C and C++"
Embedded databases are awesome! You may want to check out SQLite or RocksDB if you want to actually use one for production in the next few months, as databases take a long time to get right :) I'm planning on eventually supporting transactions on multiple items in the database, but I need to spend a bit of time getting the simple core rock-solid and high-performance before I feel safe moving to that higher level of abstraction.
Wrong sub, this is for a programming language you're looking for r/playrust
I have a question for /u/shepmaster: could you clean up some space on the machine [the rust playground](https://play.integer32.com/?version=stable&amp;mode=debug) runs on :)? It doesn't compile anymore!
Actually there *is* a way, but it has been deactivated because LLVM tends to miscompile code with it.
[Some madman actually did it.](https://www.npmjs.com/~joshhunt)
It has actually been activated for panic=abort in nightly again :)
I think the distinction you're looking for is Nominal vs Structural. Rust is Nominal.
Approximately the same. In terms of memory usage I assume it's the same as C. &gt;Are there optimizations in Rust that are difficult to apply in C[++] or vice versa? A lot of what Rust does (much like C or C++) is facilitate hand-optimizations. Rust simply has a system to prevent you from shooting yourself in the foot in certain ways. As such, you can do more complicated things than you could in C.
&gt; Build times in Rust are a concern right now, but there is a lot work being undertaken to address that. C++ build times are comparable IIRC. In partial defense of Rust, the time you save by compiling C/C++ is then consumed when you have to run test suites to accomplish what the type checker would already. &gt;the binary sizes being really large. Idk I do fulltime work in Haskell so I might have Stockhölm syndrome, but I've never really cared that much. You could use upx if your use case demands it.
Sqlite's fuzzy typing turned me off the last time I tried it, but I've never used rocksdb. From the docs, sounds like rocksdb is fast. I'm currently using postgresql with jsonb columns. My program only reads on start and only writes in a separate thread. The db is just a storage mechanism that also allows other services to inspect what's happening in the hot path (by querying the db). I must say postgres seems extremely slow (now that I'm newly used to rust speeds) but I can't claim to understand everything that is involved. I'm interested in a flexible and performant way to search collections kind of like you would with sql (except fast). 
Do you mean pv? (http://www.ivarch.com/programs/pv.shtml)
Thanks for the shout out :-D However! `gimli` and associated crates are most definitely a group effort! /u/phoil has probably done more for `gimli` than I have at this point, and /u/jonhoo and /u/tromey have built many of the foundational bits as well! &gt; We considered using cpp_demangle but that is still too early to use but quite promising. I hope to come back to this and get 100% compatibility with `libiberty`'s demangler, but I haven't had the time lately. That said, I always try to make time for PR reviews and help is always welcome :)
SQL can give you world-class performance, and for any database you need to think carefully about your access patterns to get nice performance. Fuzzy search is often really expensive for most databases, but you may do well with something that is built on top of lucene.
Not sure what I think about this. With a let bound iterator + for loop it is really easy to parse quickly since it is obvious which part contains side effects. If for_each is officially ok then it might be more common to do side effects on the middle of an iterator chain. In java I have seen people mutate a counter in the middle of a stream chain before, for instance.
Can you be more precise which tests you're referring to? Also, have you read through the blog posts? They start with a mutex-based implementation and then progress to a lock-free one.
Ok, but how would you approach implementing a container with guarantees like the ones required of `LazyTransform`? Doing things "by hand" is inefficient, but is there a better way?
But that's wrong
This is the defense of every rust fault lol. You can only use his for one thing bro 
This violated pretty much every rule of /r/rust but I thought it was hilarious. :)
Thanks for the extra details and for your work! I know nothing happens automatically yet, which is why I phrased the title with "support for". :}
Wow, totally missed that. Guess I should have tried to actually run one of the shell examples. Thanks for pointing it out!
Hi everyone, I'm the author of the Coco crate and would like to give an overview of where Crossbeam is standing at this point, and then comment a little bit on this blog post series. **What is Crossbeam?** Crossbeam was created in 2015 by /u/aturon and set up inovative foundations in the area of lock-free programming. The most interesting thing about it is the [epoch-based memory reclamation](https://aturon.github.io/blog/2015/08/27/epoch/) system, which offers a novel API that leverages Rust's lifetimes, `Send` and `Sync` traits, and move semantics to provide a nice and (mostly) safe API for building concurrent data structures. This is a big departure from other similar frameworks (liburcu, libcds, liblfds), typically implemented in C or C++, which don't have the luxury of using these powerful language-level features and are considerably more difficult to use correctly. People often ask what is the difference between Rayon and Crossbeam. I like thinking of them the following way. Rayon is all about splitting input data into smaller pieces, solving individual pieces in parallel, and aggregating results - kind of like MapReduce. Examples are parallel sorting and parallel for loops. On the other hand, Crossbeam focuses on working on the same piece of data from multiple threads, without splitting the work. Examples would be concurrent data structures (queues, maps, etc.) and synchronization primitives (monitors, exchangers, phasers, etc.). **Implementation difficulties** If you take a look at Crossbeam's readme, you'll see it promises concurrent sets and maps, but doesn't deliver them. In fact, it offers a rather modest toolbox for concurrent and parallel programming. The reason why it hasn't expanded is twofold. First, Aaron is a very busy guy and had little time to devote to work on Crossbeam. Second, writing concurrent data structures (especially correctly!) is surprisingly difficult and takes a lot of patience and hard work. I've been interested in this area of programming as a hobbyist for quite a while now and experimented with various data structures and memory reclamation techniques, sifted through lots of papers, and so on. The most shocking thing is how little quality material on the subject is around. This is still a very much underresearched and underdeveloped area, where you will find very little practical, working code. **Java is the winner when it comes to lock-freedom. What about Rust?** Java has had the most success here (check out java.util.concurrent) - and the main reason for that is the GC, without a doubt. By using JVM's concurrent GC (a great piece of technology), Java can simply sidestep the issues of memory reclamation in concurrent settings. This makes implementing concurrent data structures much easier. A great example of that would be `ConcurrentSkipListMap` - available in Java's standard library for a decade now. Try finding something similar implemented in other programming languages, and you'll be very disappointed. You might find concurrent skiplists in C++ on GitHub, but they either won't support deletion, will be difficult to use, buggy, or simply be left as abandonware. Taking inspiration from Java, I want to help make Rust the second successful programming language in this area by taking advantage of its unique features. The idea is to build powerful concurrent data structures (and other related tools) and make them easy to use. **What is Coco?** Crossbeam's epoch-based memory reclamation system (I like calling it simply *epoch-based GC*) has multiple problems. For example, the API is unsound, misses some important features, garbage collection might induce long pauses, or too much garbage may accumulate under certain circumstances. Coco is my experimental crate, an attempt at building a better GC and a few concurrent collections. However, keep in mind that lessons learned from Coco will be incorporated into Crossbeam and then Coco will be deprecated. **Revamping Crossbeam** Crossbeam saw little activity for a long time, so early this year Aaron finally decided to transfer mainternership to the community and reach out for help. A lot of people showed interest in helping continue development. In particular, I stepped in by laying out some ideas for improvement and proposing Coco as a new implementation of the epoch GC, and then sort of started leading the project. We decided to adopt the lightweight [RFC process](https://github.com/crossbeam-rs/rfcs), where we make difficult decisions about particular tradeoffs in crate's design. A new repository was created, [`crossbeam-epoch`](https://github.com/crossbeam-rs/crossbeam-epoch), where a new epoch GC is being built. We have several very active members and the project is now going full steam ahead. **Where is Crossbeam heading now?** This month we're hoping to release the first version (0.1.0) of `crossbeam-epoch`, which makes me very excited. There will still be a lot of work after that, but this is the first step. After that we want to adapt the existing data structures to it. The first will probably be Chase-Lev deque and Treiber stack. It's interesting to note that the [Chase-Lev deque](https://docs.rs/crossbeam/0.3.0/crossbeam/sync/chase_lev/fn.deque.html) currently implemented in Crossbeam leaks large amounts of memory, which makes it impractical for Rayon. For that reason, Rayon is using the deque from Coco at the moment. But not for long - we'll fix Crossbeam's deque, of course. After that come queues. If you take a look at [`MsQueue`](https://docs.rs/crossbeam/0.3.0/crossbeam/sync/struct.MsQueue.html) and [`SegQueue`](https://docs.rs/crossbeam/0.3.0/crossbeam/sync/struct.SegQueue.html), you'll notice they offer pretty barebones interfaces. People often use them as MPMC channels, and they clearly lack in this regard. I'm working on a [new channel](https://www.reddit.com/r/rust/comments/6tez0e/designing_a_channel/), which will be based on Crossbeam and be more featureful than the `mpsc` channel in the standard library. Apart from beefed up channels, we also want to implement queues for people that need simplicity and high performance. Then there is also a skiplist. I have a locally working prototype that still needs some ironing, but I believe it can finished by December. A skiplist will be the first map/set Crossbeam will offer, which is pretty cool. By the end of this year, we want to have a solid epoch-based GC, a deque, channels, and a skiplist. **The far future of Crossbeam** After that we'd like to experiment with concurrent hash maps, and other kinds of maps (maybe Bw-Trees and adaptive radix trees). We'd also like to build an extended set of atomic primitives, like `AtomicBox`, `AtomicArc`, `AtomicCell`, etc. There is also flat combining, a general technique that is sort of like a special kind of mutex, but resilient to contention. But more on that next time... Once we finish reworking the foundations of Crossbeam (the epoch GC) and the dust settles down, I hope it will become easier to contribute to the project and we'll be able expand in multiple directions at the same time. By the way, Crossbeam eventually aims to be a wide umbrella project consisting of multiple smaller crates: e.g. we'll have `crossbeam-epoch`, `crossbeam-deque`, `crossbeam-channel`, etc. These crates will build on top of each other. 
Continuing in a reply because of character limit... **What about these blog posts - why does Java end up being faster than Rust?** Finally, let's talk about this. The programs implemented in Java and Rust are as follows: we have consumer threads accessing the global variable in a hot loop, and there is a producer thread that only occasionally changes the variable. The bottleneck here is definitely accessing the variable in consumer loops. We want these accesses to be fast. Consumers implemented in Rust pin the current thread on every access, and pinning executes several atomic instructions already, which is comparatively costly. Then we load the global variable and clone it. Cloning an `Arc&lt;String&gt;` or `String` is not very cheap either. All this adds up. So what does the Java program do differently? Consumers access the volatile global variable in the loop and there's not much else going on. Regarding memory management, the JVM occasionally (comparatively rarely) stops consumer threads, marks objects pointed to by the local variables on thread's stack, and finally traces the object graph. Then it reclaims unreachable objects. In terms of total running time, garbage collection is actually pretty cheap. Here's the interesting thing. When the JVM decides to pause a consumer thread, it sets a boolean flag for the thread that indicates it must be paused. This means consumer threads will have to check whether the flag is set pretty much all the time. These hidden checks are automatically inserted into various places in the code by the just-in-time compiler. Points in code where this happens are called safepoints. Safepoints do incur some cost, but very small - checking a boolean flag is quite efficient. **Can we make the Rust version just as fast?** We could take the same idea from Java and implement it in Rust. The consumer loop was implemented as: for _ in 0..CONSUME_ITERS { if let Some(o) = lt.get_transformed() { if o.0 == "longer" { count += 1; } } } Each call to `lt.get_transformed()` pins the current thread. But now let's pin the thread at the beginning instead and insert safepoints manually (this feature is not yet implemented in Coco and Crossbeam): epoch::pin(|scope| { for _ in 0..CONSUME_ITERS { scope.safepoint(); if let Some(o) = lt.get_transformed(scope) { if o.0 == "longer" { count += 1; } } } }); Every call to `scope.safepoint()` would increment an internal counter and, say, each 50th step unpin and re-pin the current thread. This mechanism would let garbage collection still make progress, but the obvious drawback is that we have to manually declare a safepoint in the hot loop. Note that `lt.get_transformed(scope)` now takes a pinned scope and can use it to directly return a reference with the same lifetime as the scope, thus avoiding expensive clones. The signature of `get_transformed` would be: fn get_transformed&lt;'scope&gt;(&amp;self, scope: &amp;'scope Scope) -&gt; Option&lt;&amp;'scope T&gt;; I don't have any conclusive numbers, but did experiment a little bit with this and feel confident that this way we'd be able beat Java without much difficulty. Of course, is a pity that we have to manually declare safepoints, but such is life. By the way, this safepoints idea is very reminiscent of QSBR (quiescent state based reclamation). Besides safepoints, another missing feature is a primitive that keeps the global variable: let's call it `EpochBox&lt;T&gt;`, which would allow us to safely access and modify it. There was some talk about it [here](https://github.com/crossbeam-rs/rfcs/issues/13#issuecomment-336170546). Anyways... yeah, we're still missing a few features to reach Java's level. But we'll get there eventually. Stay tuned.
I think you're talking about the requirements for `RwLock` to be `Sync`. In that case I think it's correct that the contents have to be both `Sync` (as with all containers other than `Mutex`) and `Send` (I think because the convertibility between `&amp;` and `&amp;mut` lets you emulate `Send` by using `Sync` to move values across threads). But I don't think the same constraints apply to `Send`. I don't think there's a way to emulate `Sync` using `Send` (as there is with `Arc`, which does have the `Send where T: Send + Sync` requirement). For example, if I have a `Cell&lt;T&gt; where T: Send`, I'm perfectly free to move that value across threads. Why should sticking it inside a `RwLock&lt;Cell&lt;T&gt;&gt;` suddenly change that? This stuff is trippy though, so I wouldn't be surprised if I was missing something :)
What's the best option for someone who finds this thread in six months? `blah.chars().take(N).collect()` is one approach, but will still chop multipart chars. Is there a better option? Thanks!
I went ahead and put up a PR: https://github.com/rust-lang/rust/pull/45267 Seemed like the fastest way to find out if we're right :)
As linked in my other comment, use the `unicode-segmentation` crate: https://docs.rs/unicode-segmentation/1.2.0/unicode_segmentation/
Ugh, I suck at reading all around. You're not the only one that mentioned it too... thanks!
All that stuff varies depending on the program, adviser, etc. If you want to chat about it one-on-one, feel free to DM me or find me on other mediums (it's Brad that's a professor, but I can at least offer my perspective).
When you say extremely fast, can you give me an example? That has not been my experience. 
Thanks for the interesting links. Though I don't really understand what `gfx-portability` is. Your link to the Vulkan Portability initiative says that it's about defining a subset of Vulkan that can run on different platforms. I thought gfx itself was already supposed to do that. Also I am not trying to mix `ash` and `gfx-hal`, I was thinking about using only `gfx-hal` instead of `ash`. I am still very much a beginner and looking for advice where to start. So far I have only done a C++ Vulkan tutorial and then compared it with the triangle example of `ash`. I suppose I could just continue using `ash`, but `gfx` is also very attractive to me and with all the gpuweb stuff going on it seems like it will only get better in the future. Sorry for the wall of text, and I appreciate the work you and the gfx-team are doing very much. I think it's super important!
I agree with everything. &gt; as far as I know, it's just a convention I'd say that's basically accurate, but it is an enforceable convention at least. &gt; There is the ability to use #[deny(unsafe_code)] to deny the usage of unsafe code in a module, Yes, and you can also pass a flag to the compiler that effectively does the same thing. I'm the very slow and drawn out process (because I haven't followed up on it much) of trying to propose integrating that somehow into Cargo in a similar way to Safe Haskell. It could also be done in another tool. &gt; what is it that prevents Tock capsules from using unsafe? At the moment it relies on whoever is building the kernel (in our SOSP paper we call that person the "Board Integrator") to verify that crates implementing capsules don't use `unsafe`(e.g. by compiling them the flag or ensuring they have `deny(unsafe)` in their `lib.rs`). The Tock build system currently punts on that problem since we're not yet pulling capsules in from anywhere but ourselves. &gt; you'd also have to assume that Rust has no more soundness bugs Yes, and worse, the core kernel uses `unsafe` a bunch do, e.g., read/write CPU registers, initialize memory, etc. I'm fairly confident in the design of those modules, but I'd be surprised if there aren't bugs that could be exploited. &gt; could they overflow the kernel stack to cause memory corruption and arbitrary behavior, or is there a guard page There's a rough analogue to guard pages (Cortex-Ms don't have pages, but that's just pedantry). It's a bit chip-specific, but we place the kernel stack at the bottom of memory, which happens to be right above unused addresses (at least on the SAM4L and NRF52). If you overflowed the stack by less than 4K (i think) it would fault. In principle this could be _mitigated_ with some static analysis of stack depth, but haven't looked into doing that yet at all. So it _is_ a problem.
Thanks for the answer, though it made me a bit worried. Just to clarify: Are the limitations that you described still present in the current version of `gfx`? Amethyst still seems to be using `gfx` 0.16.
&gt; However, TakeCell and MapCell are very similar, TakeCell could almost be just `type TakeCell&lt;'a, T: 'a + ?Sized&gt; = MapCell&lt;&amp;'a mut T&gt;` That's 99.999% accurate. The only difference is that `MapCell` requires an additional word of memory to store the validity bit, while `TakeCell` takes advantage of Rust's `Option` optimization for non-zero types (like references) and elides that field. Who cares about 4 bytes (or maybe just one byte sometimes) for a relatively unused type? Curmudgeonly embedded systems researchers who want to make sure the numbers will be good in papers.
Oops, thanks! Ever since we added more crates, that instance runs out of disk a lot more quickly. I'm slowly (read: lazily) moving to a larger EC2 instance, but until then I have to clean up some junk every so often.
Oh hmm sorry for not reading more carefully, I'm not sure either.
Thanks :) Yeah, I know the pain of old machines clotting up. You could maybe setup a little cron job to remove the old builds?
&gt; On a more abstract level, Rust makes it more practical to design programs that use memory more efficiently, because the ownership system ensures you can write complex systems using borrowing (passing pointers around) rather than copying data without fear of losing track of who's supposed to free memory when. My usual example for that is string manipulation. C++ programmers, especially in large codebases, tends to gravitate towards just passing around a ton of `std::string`s (or equivalent) to avoid being caught with their pants down when the backing storage for their `char*` gets deallocated. One example of this was [here](https://groups.google.com/a/chromium.org/forum/#!msg/chromium-dev/EUqoIz2iFU4/kPZ5ZK0K3gEJ), where Chrome developers determined that half of all allocations were from `std::string`, often because a function would take a `char*` parameter and construct a `std::string` with it, while the original `char*` was initially derived from one. With Rust, you can safely write functions like `fn split(s: &amp;str) -&gt; Vec&lt;&amp;str&gt;` without needing to worry about the whole thing blowing up in your face when you accidentally pass in a temporary or give the `Vec` to something that stores it in TLS for a while.
[No anymore](https://github.com/rust-lang/rust/pull/45221) :) Sadly you'll have to wait until 1.23 to see the hint in the stable compiler (you'll be able to have it in nightly, naturally).
Oh, doing things by hand isn't inefficient by *nature*. Rather, using higher-level approaches can get you 70-90% there, which is usually more important than optimizing each bit individually. All I meant was that concurrency is hard. Rust's approach is really cool but it's not a panacea. 
We have more rust in python already but recently changed how we work with it. We built this to make it work: https://github.com/getsentry/milksnake Experience rust wise is pretty good. Main obstacles are building it on ancient centos :)
It can't go in the middle of an iterator chain since it returns `()`.
I would personally have preferred if there was no slicing support for strings and if you wanted to slice at the byte level one would have to access it as a `&amp;[u8]` first.
Is it possible to pass a type/name of a struct instead of a value? For example: fn foo(type_name) { let x: type_name = ...; let y = type_name {...}; } foo(String); foo(Bar); //Struct Bar {...} The reason I want to know this is that I want to create an instance of a pre-defined struct that needs a pointer to a struct. main() { fn &lt;T: MyStruct, U&gt;(mystruct: T, anotherstruct: U) let struct_ptr: *const MyStruct = &amp;mystruct; let new_struct = anotherstruct { pointer_to_struct: struct_ptr, } let struct1 = Struct::new(); //creates struct with default values foo(struct1, AnotherStruct); }
These comments of your got me excited to see where your work takes crossbeam. Thank you! :-)
No. It's not possible to turn a type into a value and then back into a type.
Hm, how ancient is ancient? I was under the impression that we deliberately built Rust images on ancient CentOS for maximum libc compatibility.
For your first question, its really up to you. The github page for vscode-rust mentions that it will eventually be superseded by the rust team extension. In regards to your second question, if you use rustup, both compilers can happily live side by side.
&gt; If however needs_drop incorrectly returns false, you’ll leak some items and the resources they might own. Right. That means it's not "purely" an optimization hint.
Got it, thanks.
What does Fluent's message API add on top of ICU?
Sadly, this is *with* a script to perform such cleanups. It seems to be a [bug in Docker](https://github.com/moby/moby/pull/30330), albeit one that was fixed a long time ago. Amazon Linux doesn't really like updating frequently...
&gt; Your link to the Vulkan Portability initiative says that it's about defining a subset of Vulkan that can run on different platforms. I thought gfx itself was already supposed to do that. Yes, there is no contradiction here: gfx-rs is moving towards that goal, but there is still a long way to go: support more Vulkan features, ensure zero overhead, formally specify the minimal deviation from the main Vulkan spec, pass the conformance tests, etc.
Well, MapCell could use an `UnsafeCell&lt;Option&lt;T&gt;&gt;` in its implementation too, instead of basically manually implementing an Option. I don't think that would use any more memory than the current solution, and would have the optimization you mentioned, but correct me if I'm wrong :)
You're not wrong, but then there are _performance_ implications. There's a fairly detailed description of the trade off here: https://github.com/helena-project/tock/issues/178 But an abridged version: It's critical for `map` to (semantically at least) do a `take` so another call can't also get access to the value. With Option, that will literally move (i.e. memcpy) the value into a new allocation on the stack. If it's a reference, no problem that's just in a register. But if it's a large struct that can be very expensive. Instead, MapCell just marks it empy during a map, and yields a reference to the same memory, so there is no memory moving around (even though semantically it's the same).
What I'm thinking of is to put all languages in a match block, and switching on it at runtime. This way formatting is type checked, while we still have some ability to dynamically change the language.
&gt;The easiest way to safely select the first 250 chars is probably input.chars().take(250).collect() You can also use `.char_indices()`. Harder but also faster. 
I mostly use the one by kalitaalexey. I can confirm that it does the thing, though it tends to crash RLS if it tries to be helpful while your code has a particularly egregious syntax error in it. All you need to do is click the RLS status in the bottom bar, but it's a bit annoying. Happens once every hour or so with my settings. You never have to touch the nightly compiler after you add it with rustup to use RLS. Also, there are some other things you may want to do with nightly, like run clippy which is a additional set of lints for Rust. You'll need to `cargo +nightly install clippy` and `cargo +nightly clippy` to run it in your project. The `+nightly` will run the nightly toolchain without changing the default behavior of `cargo`, or interfering at all with your projects. 
I'm curious, if compatibility is a concern, why not just link with musl?
Futures need to run on an event loop to make progress. They are based on polling; an event loop polls futures to ask if their result is ready until the result is ready. You can run your future or stream on an [event loop](https://tokio-rs.github.io/tokio-core/tokio_core/reactor/index.html) or you can call [wait](https://docs.rs/futures/0.1.16/futures/future/trait.Future.html).
I use the Rust (rls) extension in VS Code because it works for my settings (Rust nightly). Haven't had any major issues, and am very satisfied with the intellisense support. I actually switched from the other one you mentioned because I couldn't get it to work right with my environment. Ultimately, select the extension that works best for your environment and use cases.
Just replying here because I've been doing quite a few comparisons between Rust and Fortran on the benchmarksgame nbody task. To match Fortran, you need a compiler that's really good at loop optimizations and as far as I'm aware, Fortran compilers really are a cut above the rest in that department. I personally think loop optimizations are nearly all of the difference between Fortran and Rust in [benchmarksgame's nbody](https://benchmarksgame.alioth.debian.org/u64q/nbody.html). The Rust compiler doesn't unroll those juicy static trip count loops. A Fortran compiler will remove nearly all of them. If you want to get Rust up to snuff, throw `crunchy` at those loops and watch LLVM go wild. I've submitted a Rust solution that uses crunchy, but I have no idea if it'll be accepted (I think it should be but that's rather besides the point).
Hi, consider posting this on a blog (ctrl+c ctrl+v will do), this writeup is too good for being buried on a reddit comment :)
See [`typenum_loops`](https://github.com/millardjn/typenum_loops) for an alternative to `crunchy`.
Why would I use this over crunchy? It looks much harder to use.
The future needs to be returned, because Hyper has an event loop to run it. In general, you should never explicitly wait for futures. This means that in general, if a function runs several futures, they need to be chained. A first correction to your example would look like: ``` Box::new(req.body().for_each(|chunk| { let uppered = chunk.into_iter().collect::&lt;Vec&lt;u8&gt;&gt;(); println!("{}", str::from_utf8(&amp;uppered).unwrap()); futures::future::ok( Response::new().with_body("Try POSTing data to /echo") ) }) ``` But then reading the chunks of a `Body` in Hyper is a stream, you can't simply iterate on it, the concatenation of chunks needs to be chained as well: ``` Box::new(req.body().concat2().map(|uppered| { println!("{}", str::from_utf8(&amp;uppered).unwrap()); Response::new().with_body("Try POSTing data to /echo") }) ``` This allows you to do something else with the chunks than just concatenating them (which allocates a lot of memory). Note that when you want to return `finished` or `done`, you can use `map` instead of `and_then`.
Your first example seems workable, but I am worried about your second one. You seem to be be trying to do something deeply unsafe, and I am not sure why; so I'm going to address that one. // Slightly rewritten first example (not real rust code!) fn foo(type_name) { let x: type_name = expression_that_returns_bar(); let y = type_name { m: 1, n: 2 }; } The key idea is that you cannot do almost anything with a variable that is just a parameter of type T. You may want to define traits that associate more functionality with the type, that lets you do what you want it to do. For example: trait MyTrait { fn fromBaz(baz: Baz) -&gt; Self; fn fromMN(m: i32, n: i32) -&gt; Self; } Then implement the functionality you want for each type you might be passed. [Here is a link to a full executable example](https://play.rust-lang.org/?gist=f24428f0157faf291abc6340e46b6834&amp;version=stable). Obviously this is way more work then it is worse, and again, I am a bit worried about your overall design, so if you could explain more carefully why exactly you are looking to do this, we might be able to help better. 
yeah, I can see that working. I believe it would be fairly limiting (list of locales decided at build time) and not great for memory (if you have 100 languages, you'd have 100 strings per message in memory all in source code), but it may work for some use cases.
Fluent uses a lot of ICU (mainly CLDR, but also is well suited to use many of the ICU formatters). At the same time Fluent can be thought of as a replacement for ICU's MessageFormat, or "MessageFormat 2.0". Here's an wiki page about Fluent and MessageFormat; https://github.com/projectfluent/fluent/wiki/Fluent-and-ICU-MessageFormat and here's one particular about the syntax differences: https://github.com/projectfluent/fluent/wiki/MessageFormat-vs-Fluent-Syntax hope that helps!
As another opinion, I for one am glade that Rust has built-in slicing support. It makes working with unicode in Rust much more pleasant than other languages.
It makes it trivial to unroll according to a type's alignment generically such that each unrolled-iteration gets its own cache line. `partial_unroll` is _exactly_ what I want 90% of the time, plus there's no direct macro involvement.
My test-driven development blog post keeps getting punted because I'm spending time on [Rusoto](https://github.com/rusoto/rusoto). We're down to 17 AWS services left to implement and I'm hoping to get a few more in before reInvent announces a bunch more. There's a new release coming out once we take care of another bug or two. We'll have a complete AWS SDK yet! I'm also penciled in for giving a talk about Rust and AWS for PDXRust in November, so I'll be working on that after my presentation on Behavior Driven Development (BDD) at another meetup next week.
Centos5 for maximum compatibility in our case. 
Because musl python libs are just kinda buggy and means we need to statically link all other common libs too. 
So this `U4` type is indicating the amount of cache each iteration needs?
&gt; Second, it's nice to have an interface that just can't panic. Part of the code review process for capsules in Tock is to search for unwrap and expect. There are a few instances where we still let those fly, but as a rule we bar calls that can panic. It helps to minimize what those calls can be. n.b. this isn't fool-proof: for example, slice indexing can panic, and it doesn't seem practical to avoid that. But fewer ways of panicing is generall better (for us). Hmm… but if you expected it to be there and it's not, is it really better if, instead of panicking, it just silently skips the closure without so much as reporting an error? That sounds like a really bug-prone interface. (n.b. for anyone other than OP reading: `TakeCell::map` returns `Option&lt;R&gt;`, where `R` is the return type of the closure, so it's possible to distinguish between success and failure, but it seems like a lot of callers pass closures that don't return anything, and ignore the resulting `Option&lt;()&gt;` rather than unwrapping it.)
Having O(1) slicing is a bit rough but it's a good foundation for building higher level things -- for example for a regex library to return match ranges as integers that we can turn into substrings in constant time.
There isn't any fuzzy search in tantivy yet. Probably in a few months.
&gt; ++ build times are comparable IIRC Only if you do unity builds. Rust is significantly slower than C++ in many situations sadly.
Needs more meme
I'm too lazy, but somebody should implement a device file - e.g., `/dev/yes` - so that the `yes` program can then just open that file and use `dup2` to replace fd 1 (normally stdout) with it. In theory, it should roughly match the performance of `pv -r &lt;/dev/zero &gt;/dev/null`, which, on my MacBook, is ~15 GB/s.
I'm trying to recreate the directwrite example at: https://msdn.microsoft.com/en-us/library/windows/desktop/dd368152(v=vs.85).aspx. There are many types in this example and creating each pointer (e.g. *IDWriteFactory) produces so much code repetition. My code isn't working yet, so I think finding another approach is probably better because there are so many structs with many fields.
/r/rustjerk, and I say this as a member of the Church of the Rusty Crab.
Most of these changes just look like straight logic errors, in which case I suspect tooling and culture will be a greater aid than any particular language feature. I'd like to know what details you think are irrelevant to the protocol update. I saw [this change](https://github.com/paritytech/parity/commit/301190a16f8ed18fefa3327d2b04d8638a40a1ec) which might look like a confusing low-level detail, but I'd argue it's the opposite. Not only is this a very important distinction, `overflow_add` has nothing to do with Rust. Not only could you make the same mistake in Python, the type and function in question aren't from the language or standard library.
A starting point is /u/llogiq's [Why not to use Rust](https://llogiq.github.io/2017/06/16/no-rust.html) ([discussion here](https://www.reddit.com/r/rust/comments/6hp54n/blog_why_not_to_use_rust/)). Other nice Reddit threads are [Why aren't you using Rust at work?](https://www.reddit.com/r/rust/comments/4kqhqz/why_arent_you_using_rust_at_work/) (from 1 year ago) and [Why did or will STOP using Rust?](https://www.reddit.com/r/rust/comments/31he9f/why_did_or_will_stop_using_rust/). Be warned that a lot of stuff changed since them.
Wow, this sounds great! Can't wait to see it in crossbeam.
Hey all :) Want to write a macro to construct an enum as: ``` pub enum A { A_0, A_1, A_2, ... A_n}; ``` where n can be variable. So basically, create_a!(100); where 100 is the n here. I tried searching for resources online but couldnt find any. Is it possible to do it through macros?
The desire for safe code combined with the type system can lead to hard to understand, hard to use and hard to debug libraries. Prominent examples are Tokio ecosystem and from what I've used (or tried to use): rusqlite, [sample](https://github.com/RustAudio/sample).
unsafe stuff isn't very ergonomic.
Depending on who you ask, this is considered a feature :D
I don't think that's necessarily true but we definitely have some great examples of what not to do with the type system in my opinion.
AFAIK Tokio is confusing because of TLS. (At least for me it was.) Rusqlite because of lack of streaming iterators.
I am using rust at work and have been using it as my personal go-to language since pre-1.0. These are some of the issues I've personally come across: * Some facilities traditionally provided by system languages are not stable or still non-existent in the language: inline assembly, os integration is not as deep and tightly coupled as it is with C, available compiler directives/paragma are not as featureful and flushed out as those available to C programs, integration with C code is not completely seamless yet. A lot of this is worked around with C shims and hacks maintained outside the official rust distribution. * Compilation can take a very long time to complete. This is somewhat mitigated by `cargo check` which is relatively quick. But if execution is a part of your edit-compile loop, you'll have to artificially break apart your crate or stomach the minutes it takes for the crate to compile. * Cargo still needs a lot work: Crate authentication is completely reliant on transport security, and keys are kept online on internet-connected servers unlike linux distribution packages, web extensions or android apks. Build scripts are not sandboxed and have write access to locations outside the crate's root directory with no way to limit them (that I'm aware of). * The crate ecosystem is simply not mature yet. There are high-quality crates on crates.io that put most libraries I've come across to shame. However, the vast majority of the crates I've come across are either "pure rust" crates that are incomplete, poorly tested, and not fully developed; or crates that wrap specific platform libraries that must be compiled and installed externally. Some of these issues are actively being worked on, while others seem to be postponed indefinitely. Unless you're writing low-level libraries or you're willing to commit resources to working around these issues, you might want to check back in a few years. Cheers.
Using it at all in your collection type (as opposed to always looping and calling `drop_in_place` on each item, even if that does nothing) is purely an optimization. Which by the way is mostly useful in debug mode. In release mode, LLVM’s optimizer would likely inline enough stuff to see an empty loop and eliminate it.
This is what I did for crobook-intl. Advantage is you can publish just a binary, plus check at compile time. Disadvantage are indeed memory + compile time. Ultimately I think letting the user (eg app writer) choose at build time would be ideal.
Nice! Perhaps we ought to link to the CfP on [this page](https://fosdem.org/2018/news/2017-10-04-accepted-developer-rooms/) if that's within your control? Curious to see what kind of attendance we'll get.
I've emailed it to the fosdem list as requested. They were going through the moderated messages (as I'm not yet on the list -- getting on the devroom list also took a while :) and I expect the mods to have a significant backlog. Once they catch up, they were going to link the CfP mail on the page you pointed to. Thanks for checking though.
I don't think you can through macro_rules!
I'm curious to hear what issues you ran into with the sample crate? If you get a chance to leave an issue at the repo that'd be greatly appreciated!
Does that "requires nightly" constraint mean it's needed only for _building_ the extension, or is the language server going to be using nightly to build my actual code? If it's the latter then I don't want it.
1. It uses so many types it is hard to understand what is used for what. Especially if audio processing is not something the user is familiar with. Which I am not, I just needed to downsample some audio reasonably efficiently. 2. Type system is incompatible with any native audio driver, so you have to copy everything to get it in and copy again when you get it out. I may be mistaken in this but that is how it looked to me. One might say it's simply a matter of not enough documentation, but that is the same issue as with Tokio. If the system requires a lot of documentation to be understandable, it's not well designed.
Interestingly, I had a "Fastware" workshop with Andrei Alexandrescu in September, and his recommendation was "you probably need to write your own string class". He mentioned that at Facebook, where folly is built, they use a 3-tiered approach to [strings](https://github.com/facebook/folly/blob/master/folly/docs/FBString.md): - short strings are stored inline (up to 23 characters), - medium strings are heap allocated and eagerly copied (up to 255 characters), - longer strings are heap allocated and copied on write. *(The 255 boundary was found by experimentation, from their benchmarks it seemed a good cut-off point)* Personally, I really recommend a `template &lt;std::size_t N&gt; class InlineString {}` which stores N characters (+1 for the NUL character if necessary) contiguously. This way you get guaranteed no-heap allocation for known small strings.
&gt; 1. It uses so many types it is hard to understand what is used for what. Especially if audio processing is not something the user is familiar with. The sample crate is quite low-level and aims to be a set of fundamentals that can be used to build higher-level, nicer, more opinionated abstractions on top - as the README says, it aims to be more like a std for audio (rather than a full batteries-included solution I guess). Perhaps something like `rodio` would be closer to what you're after? &gt; I just needed to downsample some audio reasonably efficiently. Ahh, this is probably the most unstable part of the crate atm - it's been difficult to settle upon a single, nice way of abstracting different kinds of audio rate interpolation. I can imagine this would be tricky to approach with little audio-processing exp, hope it didn't cause you too much trouble! &gt; 2. Type system is incompatible with any native audio driver, so you have to copy everything to get it in and copy again when you get it out. I may be mistaken in this but that is how it looked to me. I'd have to see the specific example you're talking about to help with this - I use sample in multiple projects (with both portaudio and cpal) precisely because it allows me to easily work directly with the audio I/O in a zero-cost manner. That said, it is geared more towards working with frame-interpolated buffers atm - I hope to add some more helpers for working wiith buffer-per-channel setups soon.
I think `restrict` is unfortunately rather complicated to use here, notably because C offers no guarantee that a pointer read from a `__restrict` pointer is also `__restrict` (and indeed it might not). That is in: struct X { char* foo; }; void doit(__restrict X* x, __restrict X* y) { // use x-&gt;foo and y-&gt;foo } There is no guarantee that `x-&gt;foo` and `y-&gt;foo` do not overlap.
Indeed macro_rules can't do it because there's no way to create identifiers. You'd need to write a proc macro (unstable) or a custom derive (which you can do on stable, but it might be a little awkward).
I found that the binary size in Macosx is much smaller than in Linux. In one of my small project using image crate, the binary size is about 2X larger in Linux. #mac ls -lrta target/release -rwxr-xr-x 2 piaoger staff 3085764 Oct 13 13:57 imageops #Linux ls -lrta target/release -rwxrwxr-x 2 piaoger piaogerlinux 6824864 Oct 13 13 17:17 imageops
I'm going to hijack the thread and ask what extensions apart from Tokamak would you guys use for Rust?
Nice app. Is there a code sample for calling Rust function from JavaScript? Any suggestion for type conversion between rust type system and js types?
Thanks. There's still a lot of work I still need to do and figure out around that before I have something working on the web side, but for now you can take a look at https://github.com/JWorthe/rusty_microphone/blob/master/web/main.js, which is the JavaScript side of the proof of concept, and look at the bottom of https://github.com/JWorthe/rusty_microphone/blob/master/src/transforms.rs to see the Rust side of that call. Together, they take a JavaScript array, and eventually get it to a Rust slice. Broadly speaking, you can't just pass JavaScript types into Rust directly, but you can use Module._malloc in JavaScript to allocate some memory which can be read by Rust, write your JavaScript data into that memory, and then pass Rust a raw pointer to it.
A wild language prescriptivist appears
Thanks for the hint, I'll check it out.
Some history on SSO and Rust: https://internals.rust-lang.org/t/small-string-optimization-remove-as-mut-vec/1320
So, to make this clear: it will use nightly to build your code *so that it can learn about it to do its job*. It doesn't require your project to use nightly itself. In other words, I have both stable and nightly Rust installed. I open VS Code. It spins up and says "the RLS is working", while that happens, it's using the nightly compiler on my code. I go over to a terminal and type "cargo run"; that's using stable to build my code. It's pretty close to being entirely on stable though; there's a preview right now.
&gt; There are so many different ways to slice strings Which ways are you thinking of?
The thing that turns the SQL statement into the specific things on disk to look for is not the bottleneck in a well written query. SQL is extremely expressive, which makes it the best choice for a majority of average workloads, but you need to be careful not to use that expressivity to express an expensive operation. SQL databases are used almost everywhere at very high scales because they can be fast and flexible. Facebook and Google use SQL for a majority of their workloads, and build SQL interfaces for all kinds of interesting systems because it is widely understood and extremely expressive.
Any idea why Rust's n-body takes 5 times more memory than C/C++ versions?
[removed]
You mean Rust compile-times or the performance of Rust executables?
The [istring crate](https://crates.io/crates/istring) does this, so if you are jonesing for SSO you can give it a spin. I found that it improved performance substantially (avoiding allocation, avoiding dereferences, avoiding 24 byte data bloat).
Compile times. 
By byte, by code point, by character, by grapheme, by grapheme cluster.
I think we are talking about different things here. I am opposed to the idea that there is should be a `[..250]` operator for the string types. Instead I want there to be a `input.chars().take(250).collect()` and a `input.as_bytes()[..250]`.
Probably like this https://play.rust-lang.org/?gist=2cce03b7990b05662314ca0706ee378a&amp;version=stable
Could you clarify your position? Are you against the convenience of having a built-in operator like `xs[..250]`? Or against the operation entirely? e.g., You would oppose a method on `str` like `slice(byte_start..byte_end)`. I suspect reasonable people can disagree about the former, but the latter is important.
In my experience, at those levels, you're just observing overhead. It's probably jemalloc in this case.
I just started learning Rust 1-2 weeks ago and currently working on a dynamic DNS server which is configurable via a REST API as well as following a tutorial series to write toy browser render engine :P I'm not that far into the language yet but I've kinda fallen in love with it 😂❤️
I need this! What's the crate called? XD 
Sounds interesting. Is this going to be open-sourced?
I am against potentially dangerous operations being too convenient. It should be obvious when I am doing something dangerous. I would prefer if it was called something like `byte_slice(start..end)`.
Thanks for the extensive writeup. The safepoint idea is very interesting, I'd like to see how it pans out in situations where a high-level API treats epoch reclamation as an implementation detail. Consider `LazyTransform` being used as value in a distributed HashMap. New values are provided in a serialized form, and the map's `get` method uses `get_transformed()` to provide lazily constructed and cached values. It is important that the user of the HashMap not be required to invoke `crossbeam::epoch::pin` to use the map, although the map could expose a high-performance interface that did this kind of thing internally. pub get_ref(&amp;'a self, key: K) -&gt; Option&lt;ValueGuard&lt;'a, V&gt;&gt;; pub get(&amp;self, key: K) -&gt; Option&lt;V&gt; { self.get_ref(key).map(V::clone) } `ValueGuard` would contain the appropriate Crossbeam scope/pin and implement `Deref` to the underlying `&amp;V`. In the current Coco design it does not appear to be possible to write `get_ref` with that signature, although a more restricted version should be possible: pub with_ref&lt;F: FnOnce(Option&lt;&amp;V&gt;) -&gt; FRET, FRET&gt;(&amp;self, key: K, fn: F); pub get(&amp;self, key: K) -&gt; Option&lt;V&gt; { self.with_ref(key, |v| v.map(V::clone)) } The question is, can the safepoint design be extended to types that want to hide epoch-based reclamation as an implementation detail. Several unrelated remarks: * I suppose to be sound, `safepoint` should consume the scope and return a new one: `scope = scope.safepoint()`. * With safepoints, could a thread be pinned by default, and only unpinned when necessary? If so, this could allow to revert to the old Crossbeam design where the guard object was obtained from a function rather than received by a function called from inside Crossbeam.
Right, so the only slice that makes sense is bytes; you need allocation to do the rest of those. The representation doesn't work out for anything other than bytes.
I feel like there wouldn't be any runtime cost to being required to access first acknowledge that you're accessing it as `&amp;[u8]`. Its just a type barrier to accidentally doing the wrong thing
Sure, but going back from &amp;[u8] to str has a runtime cost; the check that it is valid utf-8 which is linear in the size of the slice and needs to read all of it.
You should be able to use the unchecked conversion with the example you outlined, which would have no cost.
Thanks mr Awesome I made a better version. Here is the json crate. https://github.com/maciejhirsz/json-rust . And here is my complete version https://pastebin.com/nnYX6JhM . 
How has the wasm target been working for you? I've been running into some issues with the linking step hanging when doing wasm as opposed to asmjs on one of my project.
I use nightly mostly for [clippy](https://github.com/rust-lang-nursery/rust-clippy) development. https://rustup.rs makes it easy to have multiple Rust toolchains side by side. It's the preferred setup method and unless you have a very specific system, it will probably work for you straight away.
Interesting how you can draw opposite conclusions from the same anecdote depending on your previous opinion.
I think I found a checker error having to do with mutable Vecs. No way to tell if it was just my crappy coding. It was causing an index finding function(searches through vec for particular struct and returns the index of it.) To pick the wrong index. No panics nothing just picked the wrong index passed it up and the wrong struct got fed to the user interface and displayed the wrong data. I would love to see someone show us how to go about finding these bugs in the checker so we can go help patch them.
The borrow checking was affecting a search through a vec? Do you think you could provide a playground link to some similar code that reproduces the error?
&gt; Stockhölm My sincere apologies for being pedantic, but it's [spelled with a regular o](https://sv.wikipedia.org/wiki/Stockholm).
The issue is playgrounding it. I thought it was a data race until I realized I was programming in Rust. I can share the link for the code. Some background: there is a Vec of structs that represent parts. These parts get sorted by either part number or part name depending on what the user wants. If the Vec gets sorted by part number and the user chooses to select a part to view details on by using it's part name the indexbyname function gets it wrong and the wrong info gets displayed. It's like it's seeing an old version of the Vec but I don't use any copying. Very odd.
Compile and runtime localization both sound like they have advantages, and should be up to the program's designer. You can kind of sum it up as... on the one hand, it's really nice to make programs (and libraries!) that distribute easily as one file. On the other hand, do you really want to force your translator to install a compiler?
Crunchy doesn't allow abstraction, you have to use literals. In general I see it as a necessary evil more than something that should be used day-to-day. (Author of crunchy here).
This is the thing. C++: "you should write your own string class". Rust: "here's this crate that does what you want, give it a spin". Afaict C's lack of a real module system, and C++'s inheritance of that fact, has probably caused as much suffering and waste as any other misfeature.
You might like /r/emojilang
You might like /r/emojilang
There is some discussion over at https://github.com/rust-lang-nursery/rls/issues/478 for Atom plugins - apart from токамак, there's also `ide-rust` ([repo](https://github.com/mehcode/atom-ide-rust)) that uses the new, recently announced Atom IDE package/functionality.
Have you figured out a way to get goto definitions to work for files that aren't in the crate? Like std? I just get an error like: "Unable to open 'mutex.rs': File not found (file:///c:/Program Files/Microsoft VS Code/src/libstd/sync/mutex.rs)" intellij-rust doesn't seem to have that problem, but I'd like to get it working with rls-vscode because I'm loving it so far!
Sentry is used by Rust, the video game. It's about time Rust started creeping into Rust-game.
&gt; Thanks mr Awesome *Mrs* Awesome
I was actually wanting to do this exact same thing with my NES emulator to learn about Elm. Currently it's using Grpc with a slapped together Ruby+Pry script for debugging.
&gt;is it really better if, instead of panicking, it just silently skips the closure without so much as reporting an error? In many cases, I think yes. First, we often use `TakeCell` in cases where we really _don't_ know if we have something there (e.g. we may or may not have gotten a buffer back from a lower layer). Second, if there is a logic bug in some state machine logic that manifests in a TakeCell being empty when we don't expect, yeah, in the general case, Tock's position is that untrusted (really "semi-trusted") component functionality should fail rather than panic. We have no way of recovering from a panic other than restarting the whole system.
You might want to try `rustup component add rust-src`, if you haven't done that yet - it downloads the sources for std.
"component 'rust-src' is up to date" I think rls-vscode already installs it, so that isn't the problem.
I would say it's less a problem of modules and more a problem of packages; integrating external projects is painful. However, what Alexandrescu meant here is that if you rely a lot on strings, it's likely that the one-size-fits-all existing string implementations are not as optimized as they could be for your specific workloads. It's also unlikely that a single string type is the best for the whole application: different parts, different requirements :)
This isn't an easy question, but I have two closures, one inside the other, and I want the inner one to capture some variable, but the outer one prevents me from doing it. I can't understand why. https://play.rust-lang.org/?gist=31e8b681a4426a4da7231e7beaf4545b&amp;version=stable
one of the things I like about rust is it makes pervasive use of type-parameters more natural.. which would require putting everything in headers to match in C++; as such I'm not so critical of the build times
That would involve unnecessary unsafe code.
Calling `wait()` just cause the function to block and never return though.
Finally it may a bit of both? I would have loved to conclude that it changes/alter the development paradigm compared to other language but it is certainly irrelevant at this stage 
&gt; I'd like to see how it pans out in situations where a high-level API treats epoch reclamation as an implementation detail. Unfortunately, I believe safepoints would always be an intrusive feature by their nature, and not something you can hide as an implementation detail. &gt; In the current Coco design it does not appear to be possible to write `get_ref` with that signature, although a more restricted version should be possible While we presently cannot build something like `ValueGuard&lt;'a, V&gt;`, we could probably tweak pinning a little bit to make that possible. I'll open an issue in the repository to discuss. The `with_ref` function can be implemented. But if this function pins on every call, it still won't be as fast as Java (although this solution will get much closer in terms of performance). &gt; I suppose to be sound, safepoint should consume the scope and return a new one: `scope = scope.safepoint()`. Actually, it'd be enough to make the method take a mutable reference: fn safepoint(&amp;mut self) { ... } And then change the signature of `pin` so that it takes a `FnOnce(&amp;mut Scope) -&gt; R`. &gt; With safepoints, could a thread be pinned by default, and only unpinned when necessary? Not sure what exactly you mean here. If a thread was pinned by default, wouldn't we have to litter the whole codebase with calls to .safepoint()` in order not to block garbage collection? &gt; If so, this could allow to revert to the old Crossbeam design where the guard object was obtained from a function rather than received by a function called from inside Crossbeam. I believe we could switch from the scope model back to the guard model by tweaking the way we pin threads. The reason why went with the scope model is that guards (as previously implemented) were unsound and we didn't have a good solution at the time.
Excuse me.
Hi everyone! I'm trying to read from a file twice (not simultaneously, can't be elegantly concatenated into one loop) and I'm not sure what the best way to do it would be. let path = "/path/to/file.txt" let input = match File::open(&amp;path); let buf_reader = BufReader::new(input); for line in buf_reader.lines() { do_things(line); } for line in buf_reader.lines() { do_other_things(line); } But the value of `buf_reader` was moved the first time I called `lines()` on it, so I can't reuse it. Should I create a new `BufReader`? Or perform another `File::open()`? Or use some borrowing tricks?
The reason is that the `button` closure wants to consume/move `tx1` every time it (the `button` closure) is created, but it can't do that because `tx1` needs to be around for the next time an instance of it is created. You were on the right track by cloning `tx0` inside the `leaf` closure. That would produce a new copy every time.
You can prevent the `BufReader` from being consumed by doing `buf_reader.by_ref().lines()`. However, the file cursor will be at the end so you need to seek back to the beginning if you want to read it again: use std::io::{Seek, SeekFrom}; // After your first loop buf_reader.seek(SeekFrom::Start(0))?;
Thank you!! :)
Which file is it in within the rust project?
This is a really cool idea, and I look forward to being able to try it! It looks like it will eventually handle a lot of issues that are hard to address with simpler localization schemes. When it's reasonably functional, I want to try it with [my subtitles project](https://github.com/emk/subtitles-rs).
This is confusing, and there's sometimes a similar confusion with iterators. The key is this obscure line in the documentation for `BufRead`: impl&lt;'a, B: BufRead + ?Sized&gt; BufRead for &amp;'a mut B That's saying that "for any type `B` that implements `BufRead`, `&amp;mut B` _also_ implements `BufRead`". So if we call `.lines()` on `&amp;mut buf_reader` in your example, it'll work. Here's [your playground with that change](https://play.rust-lang.org/?gist=f8858c200e27c1b8ebbe18560424a7fe&amp;version=stable). Since quite a few methods work this way, `Read` provides the `by_ref` method, which does nothing but return `&amp;mut self`. That makes it slightly less awkward to take the reference when you're chaining methods together. Here's [the same example above, but using `by_ref`](https://play.rust-lang.org/?gist=dc393ccea4d7be58a74d424c334b7086&amp;version=stable). Note that `Iterator` also provides a `by_ref` method, for similar reasons. That might still leave us wondering, *why on earth* did the original authors make things work this way? Why not just make `.lines()` take `&amp;mut self` from the beginning? I think part the answer is that that would make it difficult to return the `Lines` iterator. If `my_buf.lines()` took a reference, and `my_buf` had been created in the current function, it would be a lifetime error to return it. In that case you'd have to manually create a `Lines` struct with the value type inside of it, which would be pretty awkward. The current approach Just Works for most situations, and doesn't have that issue, so maybe that's why they did it. It might also be for consistency with the `Iterator` API, though in that case you have to worry about extra `&amp;`'s getting added to your iterator items, and in this case you don't. I'm not entirely sure.
Thank you for this! Although confusing, running into errors like this (and getting some tips from the community) do help me understand Rust a bit better :)
Thank you very much! I couldn't understand how cloning inside would prevent the error, but now I see it: we need one `tx1` for every instance of the closure.
If you say "an sequel" or "a es-kyu-el" you're either putting more consonants in a row or more vowels in a row unless you consistently switch sequel for es-kyu-el to make the pronunciation right.
No, for the rest you just need to traverse the string `O(n)` and calculate the byte length and then do a slice which is known to be safe and therefore do not need to check for invalid UTF-8.
Why do you need allocations to slice an unicode string by code points ?
 consider ~~posting this on a blog~~ writing up as a master's thesis 
"Race to sleep" is suboptimal with CPUs, never mind GPUs. GPUs will tend to run clocked as low as they can go and still keep up with the workload (i.e. as low as possible while still having a bit of idle time). The last power bands on many devices (both GPU but especially CPU!) can double the power consumption for 10% extra performance!
Trying to wrap my head around Tokio by implementing a simple UDP-based application.
Looks promising. Can it be simplified just to `tr!(...)` like in Qt?
You might have better luck creating a GitHub issue or posting on the [nphysics forums](http://users.nphysics.org/). Not everyone in the Rust community reads all of the different Rust forums.
But that's a knock agains "ref T" – `ref T` is pronounced that way. And `ref T` and `&amp;T` do the opposite of each other in patterns: ``` let x: i32 = 5; let ref y: i32 = x; // type of y is &amp;i32 let &amp;z: &amp;i32 = y; // type of z is i32 ```
No, I'm pretty sure you can't. I'd recommend renaming the method and find/replacing `.for_each` everywhere with `.new_method_name`
Thanks for the link. I posted here because I'm more familiar with reddit, and I've had some earlier success with asking things here.
It does work for `String`, just not `Cell&lt;String&gt;`. `Fn` is just a trait, which is not cloneable. If you use the concrete type `fn` instead of `Fn`, the `Box&lt;(fn() -&gt; ()&gt;` works just fine too, but `fn` is much more restrictive than `Fn`, I believe, since `Fn` can include closures.... I think. I really haven't checked to be sure. Anyways, this is my first time even looking at this library, so I don't have much in the way of explanations, just some observations.
it's definitely fine to post here, I was just letting you know about the other options! I'm not familiar enough with `nphysics` to help.
I haven't been using it for very long, but it's generally been working well. It was a bit confusing at first that cargo puts the actual wasm file in a deps folder. One of the biggest issues I faced so far was that I didn't read the documentation well enough and didn't realize I needed to set noExitRuntime to true from the JavaScript side, and so nothing was working. I did hit one snag where my compile stopped working after an Emscripten update, but I rolled back to the previous version of Emscripten and it started working again.
If the path of is showing is wrong, then that's the problem. I'm not sure where you change it.
The problem is `Iterator` is imported as part of the std prelude, and so all of its methods are automatically in scope. Short of forking Rust, or sticking with the older version, there isn't much you can do. I'm a bit surprised though that you aren't getting an error about there being more than potential method with that name... maybe that's a bug?
yeah I noticed it worked with string and not cell&lt;string&gt; but I feel this is a common enough pattern that I'm just doing something dumb. and I did not know there was a separate fn concept, rust is confusing but interesting
`Unsigned` trait from [`num`](http://rust-num.github.io/num/num/index.html) crate should do the job.
No as far as I know it just in that case picks the "nearest" one automatically. If a trait and a trait that extend it both define a method with the same name it picks the firs tone if the method is directly implemented on the type it picks that one. This also works for deref. Like `Vec::len` exists and `slice::len` and if `Vec::len` didn't it would just Deref to `slice::len` but since it does it picks the first one.
Thank you! I'd really prefer to not add a dependency if possible. Is there a simple way to do this without bringing in a dep?
Well if you do 1/0.016 you get 62.5, which would make me think that that is approximately 60Hz refresh rate, so I think that is tick rate in seconds.
That was my thinking as well. I was just looking for some confirmation. The nphysics ecosystem appears fractally complicated to me.
`Cell` isn't used as much as you would think. Interior mutability is something I haven't encountered all that often, usually you just have to be explicit that the variable is mutable, and then pass it mutably to any functions that want to mutate it.
Well, you can copy paste those traits into your project or implement one yourself. For example: trait ToUsize { fn to_usize(self) -&gt; usize; } And trivial impl for types you need.
`&amp;[T]` requires that each `T` have the same size, and be located contiguously in memory. That's not true of a `String`, as it's a `Vec&lt;u8&gt;`, not a `Vec&lt;char&gt;`, which can take up to 4 times the memory in the worst case.
&gt; The `with_ref` function can be implemented. But if this function pins on every call, it still won't be as fast as Java (although this solution will get much closer in terms of performance). Pinning on every call would be done in the convenience API. The second step would be for the Map to expose a pin-like primitive under a clever name: let map = ConcurrentMap::new(...); { // "lock" the map, i.e. pin the thread let map = map.read(); // assumes get_ref, but could also work with with_ref, only a bit less elegant let mut val = map.get_ref(key1); if val.is_none() { val = read.get_read(key2); } ... do something interesting with value ... } // unpin the thread This is what I previously meant by *although the map could expose a high-performance interface that did this kind of thing internally.* As far as the user is concerned, they have "locked" the map (figuratively speaking), and it is up to the implementation to do the right thing to ensure consistency. A primitive implementation would literally lock it, and a Coco-based one would pin the thread in `read()`. Whatever it does, the user not be made aware of the details, so that the implementation may be changed once a different strategy becomes available. With the availability of safepoints, it would be nice if pinning could somehow be done even earlier, so that `read()` itself doesn't have to pin every time, but instead use `safepoint` to only occasionally unpin/repin the thread. &gt; Actually, it'd be enough to make `safepoint` take a mutable reference: Indeed, I forgot about that! &gt; &gt; With safepoints, could a thread be pinned by default, and only unpinned when necessary? &gt; Not sure what exactly you mean here. If a thread was pinned by default, wouldn't we have to litter the whole codebase with calls to `.safepoint()` in order not to block garbage collection? What I had in mind was, in current Crossbeam you have to pin the thread to use the lock-free primitives provided by Crossbeam. So figuratively you already have to litter the codebase with `epoch::pin` - especially so if you expose pinning to the caller of your API for efficiency as proposed above. Now if safepoints are introduced as an unpin/repin primitive, can we then get rid of the regular pinning to begin with? In other words, could we get away with having `scope.safepoint()` as the *only* (re)pinning primitive? To answer my own question, I don't think it would work. Methods like `Ptr::as_ref` would still need the guard to have a lifetime bound for the reference they return. Come to think of it, having`safepoint` accept `&amp;mut self` itself requires design changes. In old Crossbeam, `pin` returns a guard, but this guard is reentrant, so invoking `safepoint` doesn't prove that the thread can be unpinned/repinned. Coco's `pin` provides the closure with a shared reference. Generally, reentrant pinning seems to contradict having a statically safe `safepoint`.
how would you go about doing what I'm trying to do? I feel it would be more expensive to make the button struct fully immutable cause then one would have to copy every other variable if you wanted to change the text of the button or the position
See my response to your sibling. What you're talking about is what `chars()` does, but it can't be a slice.
In the real world, it needs mutable access. That it doesn't in this example doesn't matter. Happy cake day by the way.
That's interesting. Maybe an emscripten update will help my problem.
Any reason not to just make the user have a mutable button to mutate the text... that's how I'd normally expect it to work.
Simply passing `-Cprefer_dynamic` to rustc turns a 4MiB binary into a 180KiB one for me often enough.
I put together an example here: https://play.rust-lang.org/?gist=7feb06835a9cc499299c9822b2956605&amp;version=stable Basically, in Rust, [mutability is determined by the owner of a variable](https://doc.rust-lang.org/book/first-edition/mutability.html), not by the structure itself. `Cell` allows you to create "interior mutability", which is mutability that the structure creates internally, regardless of what the owner wants. In this case, we just make the button we create be mutable, and then we can mutate the fields without creating copies every time. To make the example more interesting, I even made the on_click_fn take a parameter. I'm really happy to try to explain things more or help you with other parts of Rust, if you want.
dang you just made it all click haha, thanks for your snippet and helping me out. im coming from more abstract languages like Scala, so I'm getting used to a lot of Rust's concepts haha
Glad I could help some!
Corrected it!
Yup. To my knowledge `char` does this as well?
`fn get&lt;T: Into&lt;u64&gt;&gt;(arg: T) { let x: u64 = arg.into() }` should work
&gt; The thing that turns the SQL statement into the specific things on disk to look for is not the bottleneck in a well written query. In general. For SQLite, I think it often is. They compile the SQL statement into a program in their custom VM, and they seem to redo this every time you bind parameters (not just when you prepare a statement). Then they run the VM through an interpreter. SQLite is a hugely impressive piece of software in that it's rock-solid (breathtaking test coverage, and they [argue](http://www.sqlite.org/mostdeployed.html) quite convincingly it's the most widely used database engine in the world, and among the top five pieces of software of any kind). It's also easy to use. Its speed however is not so impressive in my experience.
Cell isn't useful for String. Cell more-or-less requires its contents to be Copy to do anything useful, which is its intended purpose. If you must have interior mutability for a String you need RefCell. Your reduced example doesn't make it clear that interior mutability is needed at all, though.
The whole idea of "str slices" itself is kind of offensive to me. They're not "slices" in the same way that `[T]` is a slice of continuous `T` types each of which are sized. `str` "slices" are in some weird way a slice of a dynamically sized "type" which dynamically encodes its own size into itself by a tag on the first octet except not really when you index again. It's a mess. I don't know what "str slices" are by they aren't analogous to slices at all.
TLS? you mean the protocol, transport layer security? But Tokio is used beyond the web..
A quick stab at it: extern crate rand; use rand::{thread_rng, Rng}; fn main() { let mut rng = thread_rng(); let rand_tup: (i32, i32) = (rng.gen_range(1, 43), rng.gen_range(-42, 0)); println!("{:?}", rand_tup); } Does that do what you want?
It does! Thanks
This reply and the comment chain on that bug explained everything really well for me, thank you! :)
True, it's ambiguous in patterns. I still like that it is derived from the meaning and not the syntax.
Code is in /src/rust/project_one
`usize` isn't the most generic integer by the way, `u64` might be a better fit for your project, as it doesn't have anything to do with pointer-width integers.
i think that maybe doing it recursively would be helpful, might also simplify the code a bit, as you would not need to hold on to an explicit cursor-reference (which is also the source of the borowing problems).
I've tried the recursive approach and the problem persists - to make a proper recursive step I need to hold two mutable references to self, because I need to check whether I should continue or to return the current node... :(
For `Deref`, yeah, it works like that – it picks the first type that has a method with that name. But when the type has more than one method with that name, and they're both from traits, it usually reports the error "multiple applicable items in scope". For example https://play.rust-lang.org/?gist=823940686254d8e314ea195dddce8032&amp;version=stable
Could be worse. You could have an extension method replaced by an inherent method with the *exact same signature*, meaning there's no error at all, and you don't even *realise* the behaviour's changed.
hm, ok, i tried around a bit, and when you borrow &amp;mut self for step_mut you temporarily move self away, so you can't access it until the result of step_mut is out of scope. now you are working on the result of step_mut, and depending on the value of the result you need to either use the result, or the (temporarily moved) self. if you change the return value to indicate if step actually moved (i.e. left or right branch was taken) or it just stayed at self you can instead work with the value and don't have to access self anymore. to do that you can create a new enum with two entries or just Result (or import the either crate). if using Result the return value of step_mut would look like this -&gt; Result&lt;Option&lt;&amp;mut TreeNode&gt;, &amp;mut TreeNode&gt;
I haven't done much cross compilation but there's japaric's [cross](https://github.com/japaric/cross) which wraps cargo, doing all the docker stuff behind the scenes. It supports all those targets, albeit the gnu/mingw one for Windows.
We must be talking past eachother here, because to me it is obvious that you can slice by codepoint in O(n) time without doing any copy. See my gist for a quick hack which slices by code point. https://play.rust-lang.org/?gist=bbd605e23ec5a2db9686061ce823ae24&amp;version=stable
Do unsigned ints implement `Into&lt;usize&gt;` already? Would it make sense to?
Hm, you are right, it looks they indeed do and it's better to use it instead of custom written trait.
Full disclosure: I'm the owner of the crate name on crates.io, having reserved it so that people who take the time to write an actual decoder have a chance over people who just run bindgen and provide some API wrappers. The latter is important as well, but due to less effort involved they are in a more advantageous position in the race for the "good" crate names, and I find crates that are wholly Rust deserve the nicer spots. I've told /u/dgriffen that I'm ready to hand over the crate name once at least some pixel data can be obtained. I'm sure it will be in good hands!
I would love to see encoder too! Even in the form of the binding to the C library for now. I have an application for lossless compression of raw Bayer images on the fly and IIRC flif can do it.
I actually tried implementing a FLIF decoder a couple of months ago as well and managed to get a little further than you did. Unfortunately, I got a bit discouraged over the unfinished specification and having to fill in the missing pieces via reading over the reference implementation. That said, I'll definitely take a look and see if I can help out at all!
Yeah, the encoder would be nice too. The problem is the reference encoder uses the GPL license, and I'm not really willing to encumber my project with that. I've been trying to be careful to only go into their code when the spec is incomplete and use their code to create a rough spec of what needs to be done then implementing it separately
Really the biggest help would be a spec on how the chance tables get updated, the reference code is very complex (probably due to handling encoding as well) and trying to extract the chance table update process from that is a bit daunting.
Thanks for the optimistic view of my abilities! The current plan is to try and get enough working to decode the reference flif image, which is [here](https://i.imgur.com/TwhXRle.png) in PNG form. Even though its a black and white image it is encoded with full RGBA data. 
Yes, you’re saying “a string slice indexed by codepoints” I’m saying &amp;[char], a slice of codepoints.
I finally settled with working implementation, but I'm not very happy about it because I run `step_mut` twice &amp; use recursion: https://gist.github.com/Killavus/e857db454ffeeb25c3b4353aff8873a9 But it works, needs a boolean variable though. If it is possible to make it better (in an iterative way for example), I'd be very glad if someone would point me towards better solution :).
Then this is more than acceptable.
Is there something particularly special you're missing for building mingw Windows binaries? `cargo build --target x86_64-pc-windows-gnu` has worked fine for my programs.
True, but, with the right combination of settings, stripping, and UPX compression, you can get in that ballpark by going in the other direction. Here's my CLI project boilerplate which includes `clap` (including typo suggestions) and `error-chain` and produces either a 185KiB binary (`panic=abort`) or a 205KiB binary (`panic=unwind`) **while** statically linking musl-libc so the only external dependency is the Linux kernel ABI. https://github.com/ssokolow/rust-cli-boilerplate
[removed]
 pub struct MyStruct { value: Option&lt;MyKey&gt;, } fn main() { let mystruct_1 = MyStruct { value: None }; let mystruct_2 = MyStruct { value: None }; let mut items = vec![mystruct_1, mystruct_2]; for element in items.iter_mut() { let cur_value = get_data(element); element.value = Some(cur_value); } let item = items[0].value.unwrap(); } There are a few things you have to watch out: - Rust has to know how big your type will be (the literal size in bytes). Hence the `Option`. You can't dynamically add a field to a struct (like in JS, for example). - Are you sure you can't just add the field when you are creating the struct? Why do you need to pass the whole struct into `get_data`? Wouldn't it be easier to pass the struct fields into the `get_data` function (instead of the whole struct) and have `get_data` be called when `mystruct_1` and `mystruct_2` get created? This way it can also be easier for refactoring. - If you want to use the value afterwards, you have to use `unwrap()`. An `Option` is roughly equivalent to using an uninitialized pointer in C (to fill the field later), only to have it filled later on by some function. I've seen this pattern in C, however the downside is of course that you never know if the value is valid or not. `Option` prevents you accessing a field that may not exist.
You may be wanting to use a HashMap: use std::collections::HashMap; fn main() { let mut items = vec![HashMap::new(), HashMap::new()]; for item in &amp;mut items { item.insert("some_additional_key", "value"); } println!("{}", items[0].get("some_additional_key").unwrap()); } But I think you also might to do want parabol443 is suggesting as well.
Probably worded that badly. Just meant that you obviously only get the gnu target (as opposed to msvc) considering it uses a linux image.
add **additional** key/field to a struct. initially it doesn't exist and I can't modify a struct
you've shown a vector of hashmap. I have a vector of structs! 
Try wrapping the struct with your own struct and adding your own fields.
why? hashmap or btreemap will be better. no?
No, that's not possible. Rust has to know what the type and size of that additional struct field will be at compile time (in this case it will be the type that is returned by `get_data`). The size of that struct field (in bytes) has to be known to the compiler, otherwise, how would you know how much memory you should read if the `value` field gets accessed? You could use a `HashMap`, but then every item in the `HashMap` has to be of the same type. If you could post a bit more surrounding code, maybe we could come up with a better / more idiomatic design.
Presumably your struct will wrap one of those, while adding your own fields...
Switch to Python. No, seriously; that you're trying to monkey patch structures at runtime strongly suggests that Rust is *not* the right tool for whatever you're doing. That, or you need to reconsider your approach entirely. If you go into a Rust project thinking in terms of dynamic typing, you are going to suffer significantly for it.
Yes, use stack. It’s unfortunately named but it’s fantastic.
What you're looking for (modifying the definition of a struct at runtime) is not possible in rust. This is because rust is statically typed: the definitions of types are completely determined statically (at compile time) rather than dynamically (at runtime). What you are looking for would be possible in Python or JavaScript, since those are dynamically typed languages.
`num` is pretty widely used for anything involving generics over numbers, pulling it in is what I would recommend. It'll eventually get merged into the standard library, but it's currently separate (like many things) so that it can mature and evolve with future rust features before it's solidified.
That’s very helpful, thanks for explaining cross’s capabilities! Now if only cross could handle BSD targets as well ;)
Calling `wait` fails because `hyper` depends on events that are delivered by tokio-core. You'll have to run the future on a tokio-core reactor, as /u/roxven suggested.
doesn't `musl` do that?
why? hashmap or btreemap will be better. no? 
why? hashmap or btreemap will be better. no? 
&gt; What you're looking for (modifying the definition of a struct at runtime) is not possible in rust. re-read my question
&gt; What you're looking for (modifying the definition of a struct at runtime) is not possible in rust. re-read my question
 go jerk off. no, seriously.
Like I said- you're asking "how can I implement Y in rust?" when Y is (intentionally) impossible to implement in rust. It would be helpful if you told us *why* you want to add a field to all the elements (i.e. what's the end goal of the program you're writing), and why it's infeasible to modify the struct definition.
 Nay, those are different axes. musl is an alternative to GNU glibc that currently targets the Linux kernel. FreeBSD support for musl is planned. Sad to see FreeBSD as a third tier target for Rust. https://internals.rust-lang.org/t/request-for-adding-free-hardened-bsd-and-linux-musl-to-official-releases/3185 I understand why (relatively small market share), it’s just a pain womp womp.
re-read his answer.
I figured that I needed to return the future in order for the event loop to run it but I had no clue how to do so while returning a string to the client. I ended up tweaking the second example you gave to avoid the concatenation: Box::new(req.body().for_each(|chunk| { println!("{}", str::from_utf8(&amp;chunk).unwrap()); Ok(()) }).map(|_| Response::new().with_body("Ok")))
Yeah this kind of seems broken that valid, idiomatic code can have its semantics changed like that on a minor update. The way I understand it you should essentially never use `foo.bar()` notation unless `bar` is impl'd directly on the type of `foo` because else you always risk the semantics changing on a minor update.
yeah, since the low level API is fully synchronous, it should be easy to add a macro for retrieving a string. But please, notice that Fluent paradigm recommends that you bind a translation to an UI widget, so instead of using `tr!()` it would be better to bind l10n-id to QML widget and write some bindings that react to that.
There's a simple way to make sure it doesn't happen: never, *ever* add new features to existing types or traits. Once a type is defined and published, that's it, end of development forever. ... and that's why this kind of breakage will *always* be possible, because the alternative is to stop adding things. Still sucks when it happens, though.
Not really. I said this before but back in OCaml I already proposed a different way to do modules: 1. Every single binding a module exports must be _explicitly_ imported 2. A module however may involve binding sets to conveniently import in one go and may even define a "prelude" which gets automatically imported when the module is imported 3. Those binding sets however cannot change without updating a major version. You can add a new binding to a module but you have to add it to a new binding set which at that point can't change any more or force users to explicitly import it. The result is that a version update can never introduce a new binding in any scope. Users would have to explicitly import `Iterator::for_each` and if they did so in my case it should immediately signal an error that two conflicting names are in scope. This would also make the specialization question a looot easier. I never liked the idea of "adding new implicit bindings does not break backwards compatibility", yes it does. Especially when your language has a system of priority between bindings which you don't just get an error on compile but the semantics may very well change.
Hi! Just released 0.1.1 which has more complete support for plural rules. It's not a real CLDR based Unicode Plural Rules solution, but a minimized version that should suffice until we get a real ICU crate :)
If you don't want to add dependencies Rust is going to be hard on you. Rust really loves to split off things into crates with the core stdlib being kept small compared to other languages. Why do you prefer not to add a dependency?
So do signed ints however.
`HashMap` is a struct.
This looks like a very OOP way of thinking. " I want my node to have this method ". You probably wan't to expose more advanced method in some BTreeStruct. Try implementing those functions. My intuition is telling me you might need mem::swap ( Just in case you didn't know that existed ). 
For VS Code there is rust lugin with RLS. https://marketplace.visualstudio.com/items?itemName=rust-lang.rust
Wtf is wrong with you? Go leave this subreddit. We don't need people like you here.
I specifically don't want to use VS Code. So it has to be for Atom.
atom has language-rust and ide-rust. Ide-rust only works if you are using nuclide or atom-ide-ui i think. Spacemacs is a great way to use emacs btw.
Ok, then I can't help you. VS Code is, btw., based on Atom, Open Source and hosted on Github, too.
Have you poked jonsneyers about it yet? Maybe it was coincidence, but when I complained about the lack of a spec he quickly added one :)
&gt; VS Code is, btw., based on Atom, That's like saying people evolved from apes.
Probably your best bet is: https://atom.io/packages/atom-ide-ui https://github.com/mehcode/atom-ide-rust 
No, it's not. There are still people out there who actually belive that VS Code = VS and proprietary.
You can use [`as_str`](https://docs.rs/url/1.5.1/url/struct.Url.html#method.as_str) method from Url crate: extern crate iron; use iron::Url; fn main() { let url = Url::parse("http://example.org").unwrap(); println!("{}", url.as_ref().as_str()); } 
VS Code isn't based on Atom, it's just using the same toolkit, Electron
May I ask why you don't want to use VS Code? I've not had a good experience with Rust in Atom.
They meant to write C++ but accidentally overwrote the + with a null byte.
&gt; url.as_ref().as_str() `url` dropped here while still borrowed 
Hi everyone! I'm trying not to spam reddit too much, but we could really use some input on the `mime` crate evaluation :) The checklist is empty at the moment and some more feedback on the state of the docs would be much appreciated. Thanks!
You're right, thx!
You may want to take a break and calm down. Then consider the idea that you failed to describe your actual problem, which prompted the reaction. If you can take a step back and describe the problem instead of a solution that won't work in Rust, we may be able to give you a more satisfying answer. We're here to help, but we also need to hold all of you to the standards set forth by the CoC.
You want to file your complaint at /r/playrust
 let url = something(); let url_str = url.as_str();
error[E0599]: no method named `as_str` found for type `iron::Url` in the current scope 
A, it's to different URL types. This instead, then: let url_str = url.as_ref().as_str(); This returns a `&amp;str`. If you want a `String`, append `.to_string()`.
People have other plugins in their editor aside just language support. 
It can I believe. I've used the [trust](https://github.com/japaric/trust) setup for CI and it gives me FreeBSD and NetBSD binaries for my projects.
As far as I can tell, only u8 does.
I tryed a lot to use Atom Rust plugins. They never worked (failing to find RLS or racer), rustsym found but no context menu option to bring me to symbols, etcetc. Also Atom is slow, it take 2 minutes to startup, sometimes it don't rendere the UI (and removing the config folders is the only soluton, also the ones into project folders). I started using VSCode again. While the old plugins have simlar issues as the Atom ones, there is a new one called Rust(rls) the work wonderfully. ALso it is order of magnitude faster in everything, and way more faster in C/C++ autocompletition. If for some reason you don't want to use VSCode, There is a Rust plugin for JetBrains IDE that worked well last time I tried. However JB products like Intellij are very slow. Otherwise 
But the user has a `str` which is already an unicode str, so: - if I want to slice it by bytes, there exist a non-allocating conversion to `&amp;[u8]` - if I want to slice it by code points, characters, graphemes, grapheme clusters, etc. it is possible to write a non-allocating iterator that gives me a view of a slice of the string by any of those concepts. In other words, it is possible to slice a string by anything without allocation. What it is not possible is to map that slice to a _Rust slice_ (`&amp;[SomeType]`) because Rust slices have some requirements on how `SomeType` is layed out in memory (which is unfortunate, and might be solved by user-defined DSTs).
For me, among other things a big reason to choose Atom over VSC is that the latter simply has a mediocre UI. There simply is no attention to detail in VSC, imho. For an app that I am gonna spend several hours a day in this is kind of a big deal for me. Just like I wouldn't want to live in a poorly furnished apartment either.
I get that - most editors have very similar plugins, though. Without a reply from the OP I won't know for sure, but I suspect it's the UI as /u/regexident mentioned in the other reply to me.
That makes a lot of sense, and could be why - I know that I don't particularly like VSC either (I use IntelliJ).
The same thing happened with `Ord::{min, max}` in the last update. It's hard to tell what gets a response from the Rust devs (me observing that this broken my published code did not, other than "sorry"), but I'd make sure your code is published on crates.io so that crater picks it up. At least that way they can quantify the breakage they introduce.
I don't publish on crates.io because I'm not an ageist piece of shit who thinks it's acceptable to categorically exclude people under 13 like most of the so very self-proclaimed "inclusive" Rust community. Regardless this just seems to be a feature of Rust right now that without updating the major version code can break backwards compatibility which is pretty awkward; what's more it can be done silently.
error[E0597]: url does not live long enough, dropped here while still borrowed 
aaaaaaaaaaaaaaaaaaaaaaaaaaaaaahhhhhhhhhhhh
I know
I also gave you a totally nice advice.
It's not zero-copy and probably not amazing performance, but you could just use the formatter and get back a string on the heap: extern crate iron; fn main() { let url = iron::Url::parse("http://username:password@domain.tld/path?qs=1").unwrap(); let url_string = format!("{}", url); //url_string is a String type /* SNIP */ } 
That error is not due to the line in question. You are still borrowing `url_str` somewhere when it goes out of scope. That's why you might want to use a `String` instead of a `&amp;str`, as the former owns the string text while the latter borrows it.
[removed]
Which is too bad, since having an unsafe block should be enough to warn people that code in that block needs to be given more attention than usual when writing and reviewing it. I don't think you need to place artificial hurdles to dissuade people from writing unsafe code. It should be obvious that they should try to write safe code first and write unsafe code only in places where rust's safety features would be too restricting.
Online services exclude people under the age of 13 because it's required by both US and EU law. It's not ageist, it's just the law. Reddit does it too. Rust is a language that's in heavy development at the moment. There are going to be a lot of breaking changes. Beyond that though, thinking that for each wasn't going to end up on iterator at some point and using that signature is functionally retarded. 
Do you use [bencher](https://crates.io/crates/bencher)? If so, you may copy what I did in [optional](https://github.com/llogiq/optional) – note the section in the `Cargo.toml`, too. Otherwise, you use nightly? Then an `extern crate` should suffice. Perhaps you are seeing a followup error? Without seeing your code, I cannot tell you more.
&gt; Online services exclude people under the age of 13 because it's required by both US and EU law. It's not ageist, it's just the law. Reddit does it too. No it's required by US law, there are plenty of code services that don't that they can use. If Github excluded homosexuals because US law required it they would find it unacceptable and would just say "Okay, then we will not require github, we realize that github can't help it but this isn't acceptable to us" and that's not what they are doing here; they consider it acceptable. As for Reddit I certainly didn't get anything asking for my age or birthday when I signed up but digging super deep all I can find about it in the user agreement which says: &gt; reddit is not directed at people under the age of 13, and reddit does not knowingly collect any personal information from such people. If you know that a user under the age of 13 is accessing reddit, please contact us here. Which seems to be something they purposefully phrased in a highly ambiguous way to maintain friendly with authority while still allowing 13 year olds to use the site. They don't explicitly say that you agree to be older or 13 but just say it isn't "directed at you" then. GNU Savannah or bitbucket make no such requirement—they could've used that, but they don't. &gt; Rust is a language that's in heavy development at the moment. There are going to be a lot of breaking changes. Didn't the stability promises say that that would be over at 1.0? &gt; Beyond that though, thinking that for each wasn't going to end up on iterator at some point and using that signature is functionally retarded. Maybe so but the documentation isn't exactly clear on that it can happen to begin with at any point and very distinctly arouses the impression that code can't break without a semver major update. You really don't know this happens to begin with until you experience it for the first time.
We're talking about children, not homosexuals, not blacks, children. They can't even legally license you to use their code. 
Convert it to a `url::Url` or submit a PR to Iron because that's would be a useful feature.
Is this something that can be turbofished? I don’t remember the exact places turbofish syntax can and can’t be used but iirc it was designed to make choosing which trait/ impl to take a method from more ergonomic, so this could be something it does?
As far as I know it only selects the type paramatres of a function cal if the function has already been identified. This just identifies the wrong one.
Ah yes, the argument that has been repeated throughout history to systemically exclude and discriminate. "We're talking about women, not black _men_!; women can't vote, come oon!"
Uh, that is pretty bad. I honestly wouldn't mind a breaking change that would just throw a compile error. Although it could be pretty annoying to fix those errors manually. Maybe there should be a way to prioritize some traits? use FooTrait; override FooTrait::foo for Bar; 
[removed]
If you cannot change the Struct then you should wrap it with a tuple or a custom struct #[derive(Debug, Copy, Clone)] struct Struct(u32); fn main() { let elem1 = Struct(1); let elem2 = Struct(2); let items = vec![elem1, elem2]; let items_with_data: Vec&lt;(Struct, String)&gt; = items.iter().cloned().map(attach_data).collect(); for x in items_with_data { println!("{:?}", x); } } fn attach_data(elem: Struct) -&gt; (Struct, String) { (elem, get_data(elem)) } fn get_data(elem: Struct) -&gt; String { format!("data_{}", elem.0) }
Note that adding functions to traits is being explicitly seen as allowed breakage by the [policy RFC](https://github.com/nox/rust-rfcs/blob/master/text/1105-api-evolution.md#minor-change-adding-a-defaulted-item) written on this. Both the addition of `min,max`, and of `for_each` fall under that rule.
They were trying to tell you that VS Code and Atom have a common ancestor (Electron), rather than one coming after the other.
Yup, that’s a great way to explain the distinction, this is exactly where the confusion is coming from.
Conversion to `String` can be written a bit more simply as: let url_string = url.to_string(); `x.to_string()` is equivalent to `format!("{}", x)` for `Display` implementors.
Yup. Working code breaks, and it's viewed as ok. I understand the policy, I just wish that more thought would go in to whether things break or not. At this point, I couldn't in good faith recommend Rust to a company that needs to provide their own stability guarantees to customers (short of them asking their customers to pin to an old Rust release).
&gt; ["str slices" are] not "slices" in the same way that [T] is a slice of continuous T types each of which are sized But they are. Think of `str` as `[u8]`. The only difference between `str` and `[u8]` is that `str` is guaranteed to be valid UTF-8.
Yeah, there's very very little reason to use atom over vscode since it has all the same tricks but is just way faster and mostly bug free
But they are not [u8] because they can't be indexed and sliced at any point. That's why they are not really "slices" and why the pretence of indexing them itself is ind of weird.
Dude. Stop pasting an error message as each reply. We're blind here -- we don't have the code. We need more context and more code to be able to help you with error messages. The example /u/onuras provided *works*, so their code isn't the problem. If you don't understand lifetimes and borrowing, then you **need** to read the sections in the Rust Book about them. Now, we can *obviously* eliminate lifetimes and borrowing by just *owning* the String like this: extern crate iron; fn main() { let url = iron::Url::parse("http://example.org").unwrap(); println!("{}", url.as_ref().as_str().to_owned()); } If this code does not work for you, *do not paste an error message here* unless you have other context surrounding that message.
The github url posted goes to a 404
https://github.com/hyperium/mime
&gt; ... is functionally retarded. Seriously, this sort of language is ok here? And upvotes no less...
\**looks at link*\* I think you got the wrong link, there.
It's not that offensive, and the points are accurate.
As mentioned here already, I use the same: https://atom.io/packages/atom-ide-ui https://github.com/mehcode/atom-ide-rust https://atom.io/packages/atom-language-rust https://atom.io/packages/linter-rust 
I copied the wrong example. /facepalm Thanks.
I'm confused; you ask why it's `&amp;E` instead of `E`, but it's neither of those things. It's `Option&lt;RefCell&lt;Weak&lt;E&gt;&gt;&gt;`. I think you're going to have to be more specific about what it is you don't understand and/or what the problem is.
I meant inside the containers. I will improve the question.
Ok, starting again (because I hate nesting): because that's what you asked for: let temp = Rc::new(self); `self` is of type `&amp;E`. Thus, `temp` is of type `Rc&lt;&amp;E&gt;`, and the downgraded weak pointer is `Weak&lt;&amp;E&gt;`. `&amp;self` as the first argument of a method is shorthand for `self: &amp;Self`; it's inconsistent with pattern syntax, but the alternative is a *lot* of extra typing. Also, this approach is just never going to work. The method is not defined on `Rc&lt;E&gt;`, it's defined on `E`. There's no way to get from `&amp;E` back to the `Rc&lt;E&gt;` that it came from. There's also no way to define an inherent method on an `Rc&lt;E&gt;`. What you need to do is move all methods that rely on being accessed via an `Rc&lt;E&gt;` into a trait, then implement that trait on `Rc&lt;E&gt;`. Yes, this is a pain in the backside, but it's the only workaround I'm aware yes. Yes, this *does* make using `Rc` more of a pain than it should be, but there's not much you can do about it.
Microsoft.
I've asked around on their Gitter, no response yet.
Dude
I intuitively would think that instead of having a bunch of different name structs, it would just be an enum. Any reason why it is this way?
Okay won't the trait methods still receive an `&amp;E` as `self`? How would that help to wrap it in an Rc?
No, if you implement the trait on `Rc&lt;E&gt;`, then `self` will be of type `&amp;Rc&lt;E&gt;`, which gives you a pointer to current ref-counted pointer, which you can then clone and downgrade. Something like: trait EExt { fn extend(&amp;self) -&gt; Self; } impl EExt for Rc&lt;E&gt; { fn extend(&amp;self) -&gt; Self { Rc::new(E { vars: vec![], parent: Some(RefCell::new(Rc::downgrade(self))), children: RefCell::new(vec![]), }) } } Also, by the way, the `main` function has to be a free function in the root module; it can't be in an `impl`.
There's also [токамак](https://atom.io/packages/tokamak). For RLS-using extensions, it's worth visiting the [Atom support](https://github.com/rust-lang-nursery/rls/issues/478) issue in the RLS repo to see which extension provides what, what's also left to implement to provide better experience etc.
Wait you can do that? That makes life so much easier, I didn't even know I could do that.
Note that the msvc linker (at least older versions of it) works really great under wine. The [WDK](https://developer.microsoft.com/en-us/windows/hardware/license-terms-enterprise-wdk-1703) contains an older version that should work. I'm also working on getting newer versions of the linker working with wine.
what was the link? just curious... haha
Unrelated code on the playpen.
That might be similar to https://crates.io/crates/inlinable_string.
Are you being sarcastic here? This is probably one of the most ridiculous things I've ever seen someone claim
 ​ ​ ​ ​ ​ ​ ​ ​​ ​ ​ ​ ​ ​
Like I said, welcome to the very self-proclaimed "inclusive" Rust community where people are very tolerant and not bigoted, except in the majority of cases where they are. And Christianity calls itself a religion of peace too and North Korea calls itself democratic—invariably the more people call themselves something the lesser they are it.
It's unambiguously offensive. Substituting "delay" for the word renders it nonsensical, thus, we can conclude the only possible reference would be referring to the group of people that the term has been used to describe the capabilities thereof. An appropriate substitute would be "incompetent", and lose no expressiveness, sans the hurtfulness of that original term.
It's maybe a little long, but not completely out of the picture. It takes me about 25 minutes to do a similar compile, but my machine is a bit beefier.
 Ryzen 1700 @3.70 with 16GB ram I can compile the whole compiler from scratch to a stage2 build in 30min, so your timings seem reasonable. Incremental compiles are usually pretty fast if you can get away with `-i` and `stage 1`. I always need to rebuild to stage 2 because I sadly use custom derive in my rustc fork. An incremental change in librustc usually takes around 15min to stage 2. Also I am not super familiar with the rust build system myself, so take this information with a grain of salt.
Seems about normal to me. Compilers are among the most complex pieces of software out there. 
Jeez, that sucks. Guess I'll just have to deal with it and plan out my changes more carefully than I'm used to. Thanks.
Man, I'm so used to sub 5 minute builds. Going to take some getting used to. Thanks for the info.
Yup; it's the tough part about contributing to the compiler. Someone who does more compiler work may be able to give you some better flags, we'll see. Looks like the best you can do to me, but I don't do much work on the compiler, so I may be missing something.
At least now I know I wasn't doing something blatantly stupid, just being impatient haha.
`-j 4` is unnecessary unless you want to use *less* parallelism than available in hardware. I also use these options in `config.toml` [llvm] ninja = true targets = "X86" [rust] codegen-units = 0 plus [build] rustflags = "-C target-cpu=native" in `.cargo/config`.
Oh cool. I didn't know that. I'll give this a shot. Thanks! 
Doing `build --stage 1`is also not necessary for testing changes in `librustc_lint`. `test src/test/compile-fail --test-args my_test` will actually build much less.
damn, I thought `musl` was cross compatible with *any POSIX system* (which I thought BSD was?)
Well, that's not really a valid point when you are speaking generally. Rust just happens to have really long compile times.
`&lt;TypeImplingInternalIterator as InternalIterator&gt;::for_each(internal_iterator, closure)` Or `InternalIterator::for_each(internal_iterator, closure)`
I hope this isn't hijacking the thread too much; but when you do that line above, it seems to compile the "final piece" of stage 0 and then rebuild the entire stage 1 with the stage 0 compiler. Which takes (here) around 15-20 minutes (compiling for Linux, Ryzen 1700). There should be some way to just recompile stage 1 `librustc_lint` without even touching the existing stage 0 compiler and then everything that depends on stage 1 `librustc_lint` and stop there. There is also `--keep-stage 1 --stage 1`, and that seems to recompile about the right amount of stuff (and is therefore much faster), but the resulting stage 1 compiler is, for reasons I don't know, not incorporating the changes made. Anyone who can shed some light on why this is not working the way I expected?
I'm always annoyed that I have to include `use self::Foo::*;` when actually inside `impl Foo` (I think this would be a good candidate for an RFC, not 100% sure about backwards compatibility, though), but your example is super surprising.
I'm also surprised by this! if you used something like: ``` impl Foo { fn to_string(&amp;self) -&gt; String { match self { self::A =&gt; "a".into(), self::B =&gt; "b".into(), self::C =&gt; "c".into(), } } } ``` You get a better error message: ``` error[E0531]: cannot find unit struct/variant or constant `A` in module `self` --&gt; src/main.rs:10:19 | 10 | self::A =&gt; "a".into(), | ^ not found in `self` | help: possible candidate is found in another module, you can import it into scope | 1 | use Foo::A; | ... ``` I would definitely file a bug on this and have it suggest the same error message as above followed by the bindings error suggestion as this seems more likely the actual error case. 
Hm, you can't use `Self` either.
The `rust` repo has around 3.9 million lines of code once it has fetched everything. 1 million of those are rust, around 1.4 million are C++, nearly 200000 lines of C. There's even some OCaml in there, though I have no idea if it's called during the build. 
have you tried my 2nd answer?
Regarding the `brew` issue, I had a [similar problem](https://github.com/rusoto/rusoto/pull/833) that appears to be fixed by running `brew update` before any `brew install` commands. References on others having and solving the issues are [in this comment.](https://github.com/rusoto/rusoto/pull/833#issuecomment-335649343)
Thank you for `std::mem::swap` - reading 'Learn Rust by Writing Entirely Too Many Linked Lists' showed me these functions. I don't think if they apply in this context, though... Can you elaborate more about 'exposing more advanced method in some BTreeStruct'? Currently my implementation treats Node as private-only detail of the `BinarySearchTree` struct which mostly delegates its methods to nodes, but it handles operations requiring the change of `root` node, too. Is implementing such BinarySearchTree struct is something you wanted to suggest? Anyway, thank you for your comment!
I did not yet. I found the working implementation for now - https://gist.github.com/Killavus/e857db454ffeeb25c3b4353aff8873a9 . It still has problems (I'd like to avoid recursion in non-TCO language and I think using boolean value to `preserve` the decision I did here smells bad...), but I'd love to start implementing proper methods like `add` &amp; `delete` :). Your approach using `Either` type looks promising, though. I got it written down and I'll try to apply this idea on a working example to learn something about it.
yeah you have to explicitly call `&amp;Foo::A` At least the error message for `self` is accurate
Well, AFAIK the largest non-rust part is LLVM, which is c++. When you do a clean build, the LLVM build takes a considerable amount of the total compile-time. However, when changing the compiler you usually only change the rust part, and then LLVM does not have to be recompiled.
i would not worry too much about the tco, for one rust may apply it, its just not guaranteed, and also afaik function calls are not _that_ expensive, its mostly just a jump (rust is more flexible about function call convention then i.e. C) and you have a jump in a loop, too. (check the asm if in doubt) the either type allows you to avoid the boolean by basically encoding it into the type.
If you don't want to deal with traits, you might also consider defining two structs: struct EInner { // ... all the stuff you used to have in E } struct E(Rc&lt;EInner&gt;); impl E { pun fn extend(&amp;self) -&gt; Self { ... } }
Swift has a syntax inside cases where it infers the associated enum (very simple and unambiguous when you're doing a match), and it looks like: switch (some_enum) { case .a: // ... case .b: // ... // etc } There was discussion on adding it to rust but unfortunately it didn't lead anywhere.
&gt; ... or is this a bad idea / wrong way to go about this? That depends on what you want to achieve. Why do you need to do this from another thread?
I want to be able to trigger writes after periods of time, as well as do writes depending on what is read. I'm writing some generic code that I want to be able to reuse for various networking apps I want to write, that is mainly based on passing shorter messages around (as opposed to sending large files). One reason to be able to do this, as stated above, is to be able to read stdin as well, from a separate thread so messages can be written after reading user input.
I had upvoted it because I missed that but at the end; it’s unambiguously not okay. Maybe others did the same? I’ve downvited it now.
WTF does `ninja = true` do?
Nope. For example, library/alpine currently fails to run in Docker on top of FreeBSD hosts.
It makes use of the [Ninja build system](https://ninja-build.org/) instead of using make, which can speed up the LLVM build.
Did you try `&amp;Self::A` or just `Self::A`? I'd expect the former to work.
No, with `Self::` rust is trying to find an associated item with that name.
[removed]
[May be relevant](https://www.reddit.com/r/rust/comments/55ns2m/safe_and_efficient_bidirectional_trees/?st=j8t6kb43&amp;sh=594d572e)
I wouldn't even ask what it does: as soon as I find out some software I use has a ninja option, I just activate it.
My official job title has ninja in it. Best/worst decision I ever made (You get interesting looks at customs when your work permit says "ninja")
What exactly is the type you want to implement `IntoIterator` for? `NewType(Rc&lt;LinkedList&lt;T&gt;&gt;)`? Could you give a short code example of the result you want? Because it's going to be tricky to implement `IntoIterator&lt;Item=T&gt;` for such a type, because `Rc` implies shared ownership, so you can't really get the `T`'s out of the list (unless they're `Copy`).
You may be changing something, but the user guide link is broken both in your post and in the README (as well as the getting started link).
It might have come off as rough, but they're not wrong. You're asking for help to fix your code, but providing nothing to work with. Errors have context around them and we need to see that to help you. People will help you, but you've got to help us to help you. You'll get more favorable responses by saying: Here's what I did, this is what I expected to happen, and this is what actually happened. Being able to ask for what you need, nicely, and effectively is a skill and will benefit long term. If you understand why you're getting this response now and how to fix it then in the future you should be a okay. :)
Iron's only person who can push updates to crates.io is completely off the grid. It hasn't gotten an update in months. Honestly, the community should just get people to use any of the other maintained frameworks, like Rocket, Gotham, Iron, Pencil, Rouille, etc.
Fixed. Thanks!
I've got an edit explaining things better. I got confused, I thought Rc implemented IntoIter, but I was getting it confused with `syntex_syntax::Pointers::P`, which I had been using initially.
It must have something to do with explicitness relating to the hygiene of namespaces. The best I could come up with is impl Foo { fn to_string(&amp;self) -&gt; String { use Foo::*; match *self { A =&gt; "a".into(), B =&gt; "b".into(), C =&gt; "c".into(), } } } 
I like the explicitness of the leading `.` meaning enum in the switch.
One of the CI pipelines supports Windows. Generate windows builds using the binaries produced by the CI pipeline. Everything else can be done with a Docker image.
Vec&lt;T&gt; does not implement Copy since it stores data on the heap, so there is no way to implement copy for the struct you have. But you can clone it just fine using the clone() function.
See, I would do that except, what I really need to be able to do is reference my Node struct in other Node structs. Without Copy, I get errors thrown at me. Error: https://i.imgur.com/4Q2Fhmr.png Code: https://doc.rust-lang.org/error-index.html#E0382
I wonder, if you can't do this in Rust, how does one do binary trees at all?
I really just don't see why pointing to the same variable in two separate instances should be a problem. Error: https://i.imgur.com/BldG1wN.png
I should have looked a bit closer and realized you were making a graph. Making common data structures (trees, graphs, etc.) in Rust can be a bit difficult due to the ownership rules. In this case, it's easiest to use a reference counted pointer. I'd recommend reading the [book](https://doc.rust-lang.org/beta/book/second-edition/ch15-04-rc.html) to get a better understanding of reference counted pointers. Once you understand that, you can do something similar to the following: https://play.rust-lang.org/?gist=62212bcad026e88bbfff0cf86d834fb8&amp;version=stable You can see that it gets a bit more complicated once you have the potential to have multiple nodes pointing to another node. The Rc&lt;RefCell&lt;Node&gt;&gt; in this case allows you to have multiple owners of a mutable node. The RefCell is described in the section of the book after the one I linked.
This will cause a use-after-free, as you'd have two vectors pointing to the same data. I think you might be confused about what `Copy` means; I haven't dug into your exact details, but you may want to check out this part of the book: https://doc.rust-lang.org/book/second-edition/ch04-01-what-is-ownership.html It talks about `String`s, but `Vec`s are the exact same with regards to these semantics, as a `String` is a `Vec&lt;u8&gt;` inside. After that /u/yokatta24's suggestions look good.
In this case, you aren't actually having a variable point to two separate instances. To do that, you'd use &amp;node1 or &amp;mut node1, but just saying node1 means you end up moving the value. Once you've moved a value, you can't use it again. This is why when you move node3 into the node1 structure, you can't move it again into the node2 structure.
Thank you for the information, I will read that chapter. Unfortunately, I haven't had the time to read the entire book.
Okay, so, you *cannot* implement `Term` for types that you don't define yourself, because of the coherence rules. So if you want `Term` to be implemented for [Rc](https://github.com/fitzgen/scrapmetal/blob/957bb08311589c6c1cfc7c46e3817ca6b13488c8/src/term_impls.rs#L390) and [LinkedList](https://github.com/fitzgen/scrapmetal/blob/957bb08311589c6c1cfc7c46e3817ca6b13488c8/src/term_impls.rs#L274) you could create a pull request for the scrapmetal project to make sure support will be included in the next release :)
Not to mention the cost of a weekend in London...
Why ? 
I used `Self::A`, because I didn't copy/paste the code, and just rewrote it: `enum E{A,B,C}impl E{fn f(&amp;self)-&gt;&amp;'static str{match*self{Self::A=&gt;"a",Self::B=&gt;"b",Self::C=&gt;"c"}}}`.
It sounds like you've solved the problem but if you have any future problems. I've written a bst in both a mutable and immutable fashion that you can look at if you want. https://github.com/keeslinp/binary_search_tree. https://github.com/keeslinp/immut_bst
I've changed it.
I would have liked to have something like this: match some_enum { _::A =&gt; // ... _::B =&gt; // ... // etc } This fits the usual inference in Rust - if the type of `some_enum` can be infered without the `match` branches then it should be inferable that the `_`s are the enum's type. And yes, I realize it hampers a bit the soundness of the inference rules in cases like: pub enum Foo { A, B } mod bar { use super::Foo; pub const A: Foo = Foo::B; pub const B: Foo = Foo::A; } let foo: Foo = /*...*/; match foo { _::A =&gt; // ... _::B =&gt; // ... } But I still think an exception should be made for ergonomic reasons... 
Hm, that wasn't intentional. It looks like I broke something with the Travis configuration. Thanks for letting me know.
I haven't made any changes in librustc_lint, but I've found that compile times vary depending on which crates you've modified, and how much of the rest of rustc depends on those crates. When I make a change in librustc_typeck, and recompile with `--keep-stage 1` `--stage 2`, it takes 3-4 minutes, but when I change libsyntax or librustc_errors, it takes 15-30 minutes. Specs: 13" macbook air, 2.2 gHz Intel i7, 8 GB RAM
I think currently `_` in paths isn't valid, so this wouldn't be a breaking change, and I don't think we'd ever want to add something for it to automatically infer a module. Given that, the enum inference feature should be trivial and unambiguous, it just might be something people expect to be capable of more than it is, so we'd have to teach that it only works in `match` and `if let`, and only for enums.
Someone could exhaustively match an enum of MIME types, but MIME types get made all the time, and there are lots of non-standard ones, so I assume it would be quite easy to receive one that you don't handle.
I'm trying to figure out how to then modify structs? Before I could do for example "node1.id_num = 10" to edit struct values, but not I can't figure out how to do that as the previous method no longer works, and the more I read on Rust ownership, the more I wonder why use such a system over more tried and true systems that would allow binary trees and graphs with ease.
underrated post.
It's worth saying that behind the scenes, Python meticulously keeps a count of how many things have a reference to an object, so it knows when the object can be deallocated, and the equivalent in Rust is wrapping the object in an `Arc` (Atomic Reference Count) which only allows immutable access to objects, and then to get mutable access, further putting the object inside a `RefCell` which at runtime tracks who has mutable and immutable access to an object to prevent race conditions, something that Python doesn't prevent by default, but which Rust forces you to deal with at compile-time. This means you end up with `Arc&lt;RefCell&lt;MyObject&gt;&gt;`. This is quite a mouthful, so typically in Rust you try to keep the reference hierarchy much simpler so all this checking isn't necessary in the first place.
I figured it, code: https://i.imgur.com/gNR3QXN.png Gotta use borrow_mut(), more here: https://stackoverflow.com/questions/45053810/how-do-i-modify-the-content-of-a-refcelloptiont
Hi everyone, Has anyone got diesel mysql statically linked in a musl build for alpine linux? I'm using rust-musl-builder with mysql installed and can compile just find, but its still dynamically linking the libmysqlclient.
Why limit it to `match` and `if let`? I'd argue that wherever `_::X`, `_::X()` or `_::X { }` can be resolved to an enum type, `_` should be infered to be that type. So that if you have a function `bar` that accepts a `Foo` you could use `bar(_::A)`. Heck - why even limit it to enums? Why can't we infer `_()` and `_ { }`? And for the sake of completeness(though it'll rarely be useful) - `_` too? I'm thinking about enums that hold structs: struct Foo { a: u32, b: u32, } enum Bar { A, B(Foo), } struct Baz(Bar, Bar); enum Qux { A, B(Baz), } // Full style: Qux::B(Baz(Bar::A, Bar::B(Foo { a: 1, b: 3 }))); // With improved inference: Qux::B(_(_::A, _::B(_ { a: 1, b: 3 }))); 
you listed Iron in the list of not-Iron frameworks?
Wow, I did not know that.
`_::` is not going to help our syntactic reputation.
I'm new to learning rust, and decided that a good learning piece would be to implement Keccak. I'm interested in general feedback of style and performance, as well as maintainability and extendability. I'm also working on a Wasm wrapper around this which can be found here: https://github.com/botagar/keccakrs-wasm Cheers all~!
Yeah, seeing it I thought "yeah I totally get what this syntax is getting at, but then Rust would never get invited to eat lunch at the cool table."
Right, but in the meantime I need to define my own newtype wrappers and add my own instances of Term. So I'm wondering if there's an easy way to make a newtype around LinkedList implement IntoIter like List does, so I don't end up rewriting the List library.
I can imagine. That title is way too badass to be believable.
Slip of the tounge. Their policy has been not too and honestly they should stick to it. If they make an exception now they'll have to do it again later. Just fork and rename it and encourage people not to use Iron.
Not many people seem to know that. Because all the resources everywhere still list it though people use it.
That's a dumb reason to avoid using some software. Especially since it's all open source. 
I'm used to qualify my enum variant name. This happens outside impl too. I find implicit this in C++ confusing when reading coding. I have to check the class definition to see if it's a member or a global.
Well I don't care.
Because /r/rust is about the programming language of the same name, and we don't care about raiding a base.
Are you able to use something like a channel to notify the main thread that it needs to reregister?
&gt; It might have come off as rough yes
And?
You can't use the slice syntax, because a slice in Rust is very specifically a set of continuous equal-size values (and runes / grapheme clusters are not all the same size). But [here](https://play.rust-lang.org/?gist=38e23fc183bb9003bfbf2cdc61da589a&amp;version=stable) is an example of how you would do this sort of thing.
Started playing around with making a game in Rust! After extensively mulling over my options, I've decided to go with glium for graphics, sdl2 for windowing and events, lyon for creating shapes, and specs for the ecs. Pretty new to making games but I took a long time learning the current gamedev environment for rust and I think I have a good idea of how things are working now. I'm hoping to also contribute more to each of these projects individually. I already updated glium-sdl2 to work with the newest versions of each library and sent in a pull request! Also I'm going to rust belt in Colombus in two weeks and I'm very excited! It should be cool to see all of the panels there.
How about you write to a queue and have your main loop (re) register everything in the queue inside it's poll loop... let out_event_queue = std::sync::mpsc::channel; //main polling thread loop { for out_evt in out_evt_queue { // Register the socket poll.register(&amp;out_evt, CLIENT, Ready::writeable(), PollOpt::edge()).unwrap(); } poll.poll(&amp;mut events, None).unwrap(); for event in events.iter() { let wb = get_buffer(event.token); write_to_socket(&amp;event.fd, wb); } } // your example thread thread::spawn(move || { loop { thread::sleep(Duration::from_secs(2)); wb.push(b'x'); evt = create_event_from_write_buffer(wb); out_event_queue.push(wb); } });
Fun fact, Mio used to manage handlers via traits and support a notify() queue which it could poll. Those bits are still present in the `deprecated` part of the crate. see : https://github.com/carllerche/mio/blob/master/test/test_notify.rs
Maybe look into tokio? 
Couldn't this leverage hyper somehow? 
And.
I think the idea behind this crate is to *not* use hyper, in order to achieve diversity.
## Indigo UI Framework This past week I got some more work done on Indigo, my UI framework prototype for Rust. (See last week [here](https://www.reddit.com/r/rust/comments/756kzf/whats_everyone_working_on_this_week_412017/do3xkm8/)) Here's what's new: ### Text Primitive I've implemented text support again. For now only a single style per text component is supported, but I hope to introduce the concept of text runs in order to have them styled independently. Text layout is very basic, with per-character text wrapping supported and that's about it. The property system lacks the concept of inherited styles, so setting a FontSize on a root component does not yet propogate it down to its children. ### Image Primitive I implemented image primitive support which let me build an image component. The loading happens on a dedicated thread which also caches the WebRender ImageKeys, ensuring only one copy of the image is ever stored in the texture cache. It also fallsback to a checkboard image if it cannot find the image specified. Right now it only supports local file sources, but I hope to expand this to include remote sources soon. ### Scroll Primitive The scroll primitive got some fixes,it now properly calculates the viewport (bounds minus scrollbar size) and scrollable extent of its child. Here's an example of the ScrollViewer component built using the Scroll primitive. #[component] impl Component for ScrollViewer { fn primitive_type(&amp;self) -&gt; PrimitiveType { PrimitiveType::Scroll(Cell::new(ScrollPrimitiveInfo::new())) } style! { ScrollViewerDefaultStyle { HorizontalAlignment: Alignment::Stretch; VerticalAlignment: Alignment::Stretch; Scroll: Scroll::Vertical; } } ui! { } } ### Layout Layout is now done on its own dedicated thread ensuring that scrolling remains smooth even when dealing with a really bad layout pass. Padding (which includes border thickness) is now taken into account as part of the content size. Grid layout support is still a work in progress, I need to land some architectural changes in layout before I could move on this anymore. [Here](http://www.videosprout.com/video?id=41e98bd0-720d-4872-a83d-d83a594a6f4f) is a video showing why this is important, I put code in there to purposely choke the Layout Thread. Notice how scrolling remains responsive even while the layout of primitives catches up to viewport changes. [Here](http://www.videosprout.com/video?id=c55e6213-d803-4c7c-8b81-6226973d94bb) is a video without the layout performance issues, just for comparison. ### Style The style macro got some love this week, I removed the requirement for defining property values as brace delimited expressions. They're still supported, but now you can use normal expressions. Before: StyleName { BorderThickness: {Thickness::uniform(10.)}; ... other properties } After: StyleName { BorderThickness: Thickness::uniform(10.); ... other properties } I also reimplemented support for explicit properties passed into the RSX nodes themselves, so you can override properties by explicitly passing them in, for example: &lt;ScrollViewer BackgroundColor={ColorF::new(1., 0., 0., 1.)}&gt; .... &lt;/ScrollViewer&gt; ### Next week I spent a lot of time reorganizing code to make the whole thing less of a hack job, its getting there. Hit Testing broke as a result of all these changes, I'll probably focus on fixing that this week, and making the necessary style macro changes to support defining a style per visual-state. The syntax I'm considering would be something like: StyleName { VisualStyle::Normal =&gt; { BackgroundColor: ColorF::new(1., 0., 0., 1.); }, VisualStyle::Hovered =&gt; { BackgroundColor: ColorF::new(0., 1., 0., 1.); } } 
Wondering: why didn't you use glutin for windowing and events? Is SDL better, and if yes, In which ways is it?
Have you looked at [stdweb](https://github.com/koute/stdweb)? (:
I think what you are looking for is device discovery. [Here](https://stackoverflow.com/questions/16226076/what-is-the-best-way-to-implement-device-discovery-on-a-lan)'s a related SO question.
I tried Tokamak, but I had issues getting it to *actually do the autocompletions*, which is what I was looking for anyways.
Please be nice to the author as usual. I am not the author ;)
Can't use a channel because reading the channel `rx.recv()` is blocking (`poll.poll()` is blocking too)
The second point was fixed in newest stable (1.21.0).
&gt; t_event_queue.push(wb); Sadly I don't think this would work: for out_evt in out_evt_queue { // &lt;-- blocking until it you have an event on the queue so won't run ... } poll.poll(&amp;mut events, None).unwrap(); // &lt;-- blocking until it receives an event, if only a `read` is registered then it's stuck here until a `readable()` event is triggered 
You ought to have linked [to the relevant function](https://github.com/LogoiLab/ProjectOne/blob/b57385feb3f667adf162c3432fa39b045994bcf3/src/rust/project_one/src/part_list.rs#L42). After reading that file I think that way of doing things is error prone. * If you just keep an index in a usize binding somewhere and the vec elsewhere, the compiler can't save you from doing something like modifying the vec and ruining all your indices. If you use references the rustc can help. * You're trying to use a vector [associatively](https://en.wikipedia.org/wiki/Associative_array) (where the key is part number). It would make more sense to use an associative structure like `HashMap&lt;PartNumber, Part&gt;` from the stdlib or some combination of associative structure and something else to provide order if you actually need order.
The [`Rc&lt;RefCell&lt;Box&lt;MyStruct&gt;&gt;&gt;` → `Rc&lt;RefCell&lt;Box&lt;MyTrait&gt;&gt;` thing](https://stackoverflow.com/questions/30861295/how-to-i-pass-rcrefcellboxmystruct-to-a-function-accepting-rcrefcellbox) is straightforward now; `Rc&lt;RefCell&lt;MyStruct&gt;&gt;` will coerce to `Rc&lt;RefCell&lt;MyTrait&gt;&gt;`. Not sure when that landed in stable, but it’s been a while. I’ve updated my answer on that question.
Why is diversity so valued? I mean if there is something great that works for everyone, why do something else?
 // Comparing black and white score, returning the winner. Komi is added to white score to compensate first move advantage match PartialOrd::partial_cmp(&amp;(black_score as f32),&amp;(KOMI + (white_score as f32))) { Some(Ordering::Less) =&gt; Intersection::White, Some(Ordering::Greater) =&gt; Intersection::Black, _ =&gt; unreachable!(), } This is probably not the best way to handle this. For starters, the match expression could be `(black_score as f32).partial_cmp(&amp;(KOMI + white_score as f32))`, but then if you deem `None` and `Ordering::Equal` unreachable (and I’m not convinced without seeing more code that they actually *are* unreachable, though they certainly could be) then you might as well have just used `&lt;`: if black_score as f32 &lt; KOMI + white_score as f32 { Intersection::White } else { Intersection::Black } Still, the claimed failure is that it is too ugly. I don’t find the normal flow in any way ugly, and doing a full partial_cmp is going to be comparatively ugly in *any* language; most lack algebraic data types and so it’s much worse in them than in Rust. I may also add that these core operator traits are more painful to work with than most traits, because of their heavy use of generics and references.
I've been improving my [ALSA](https://crates.io/crates/alsa) crate. I've added a "direct mode" that bypasses the C ALSA library for streaming samples. The reasons for trying this are: * Minimum overhead where it matters most: let alsa-lib do the code heavy setup - then steal its file descriptor and deal with sample streaming from Rust. * RT-safety to the maximum extent possible (no syscalls, no memory allocations etc) * Possibility to allow Send + Sync for structs * It's a fun experiment, and an interesting deep dive into how alsa-lib does things. I also wrote a very basic Hammond organ as [an example](https://github.com/diwic/alsa-rs/blob/master/synth-example/src/main.rs of how to use the new mode. 
The two approaches may learn from one another. Making a better ecosystem on the whole. Also don't forget people might require different things from their libraries and so diversity is good there also. 
&gt; Rust is still “discussing” integer as generic type parameter (since 2015), meaning a matrix type Matrix[M, N, float] will not exist before a long long time. RFC #2000 has now landed, and https://github.com/rust-lang/rust/issues/44580 is tracking its implementation. I think enough people want it that it *won’t* actually be all that long before it’s available, even stable. I’m not very well-informed on the current velocity of such things, but it wouldn’t surprise me if it were stable by the end of next year. (It also wouldn’t surprise me if it weren’t yet stable at the end of next year. It would surprise me if it were unimplemented at the end of next year.) It’s worthwhile pointing out that issues like this take a while because we want to be sure we get the *right* thing, not just something that normally works but is then revealed to be terribly limiting later on. The lack will make you sad now, but in five years time you’ll be glad for the delay, because the end result *is* superior.
&gt; Consequences ? You can’t use Rust arrays to represent a matrix bigger than 4×8, how useful is that? There is a fallacy in this: that using a one-dimensional array is more efficient than using a two-dimensional array. If arrays are stored on the heap, certainly; but consequent to Rust’s representation of arrays, a two-dimensional array like `[[T; 32]; 32]` is a perfectly reasonable way of doing things, with no space overhead (though it will be *ever* so slightly less efficient in some cases, as you need two indexes instead of one—but in practice you were probably using two indexes anyway).
You can do almost anything in Rust, as long as you are very comfortable with writing `unsafe` code. Most stuff can also be done by using types such as `Rc` to work around the strict mutability rules, but comes with a run-time cost. If you are interested in how to implement common datastructures I really recommend checking out how they are implemented in the standard library!
Most of this boils down to language immaturity, with a smattering of unfamiliarity and Rust genuinely being harder because of its greater power/control. I can readily imagine Nim being a more pleasant language for data scientists to work in, though Rust is steadily improving in that regard.
Most libraries don't work on no_std targets, and num is one of them. So I also don't include it as a dependency and just copy paste the code I need into my project.
&gt; the other maintained frameworks, like Rocket, Gotham, **Iron**, Pencil, Rouille, etc. uhhh...
This is the first step, but when people talk about `const` integers, what they actually mean is something like C++ template integer paramters. For example: - be able to give a `struct` a completely different memory layout depending on the values of const integer - be able to provide different inherent methods, trait methods, etc. implementations depending on the value of `const` integers AFAIK achieving the first point is not possible with that RFC (and I don't think it is possible with any of the RFCs that ticki proposed either), and how far the second point can be achieved with the current RFC remains to be seen. For example, C++ `requires` keyword allows specialization via a ["partial ordering of constraints"](http://en.cppreference.com/w/cpp/language/constraints) such that `N == 0 &amp;&amp; N &gt; 0` is more specialized than `N==0`. 
Just for the nitpick : in go scoring system, player scores are integers and the komi is a half integer (currently 5.5 IRC) so yes, the assumption that they cannot be equal is correct (and scores are small enough that float overflow is not an issue). 
If different people require different things, it breaks my assumption of "works for everyone". ;) Learning from one another sounds good, but I have to think about it.
Personally i don't know of a language that has statically typed stack allocated arbitrary sized arrays. Does anybody know a good one ?
Finally got the go ahead to write my final year project (aka dissertation, sort of) with Rust. It's on boosting. So I'll probably be working on that and trying to think of some interesting data/application to apply it to
More [Rust in Action](https://www.manning.com/mcnamara)! This week I am working on the Data chapter, which aims to cover: * how to the same bit pattern represents different values depending on its type * how computers represent numbers (ints, floats, decimal/currency types) * what a CPU instruction is (effectively a `usize` value) * how addresses in memory are also `usize` values * how structs are represented in RAM (and how/why Rust pads fields to align to 32 bit spans) * what arrays look like under the hood * why to build cache-friendly data structures and how Rust makes that possible
Completed the 3 challenges in [here](https://doc.rust-lang.org/book/second-edition/ch03-05-control-flow.html) at the end of page. Here are my [repos!](https://github.com/SpartanIM99) Any tips are welcome.
First impression - since you're aiming for readability, have you considered inlining everything into one file? Rust has good support for namespaces (modules) and splitting into multiple files isn't really necessary for small projects. 7 files for 400 LOC strikes me a little Java-esque.
I spent the last 2 weeks fixing the remaining rendering bugs (I know: "there's always one bug left" ;)) in [media-toc](https://github.com/fengalin/media-toc). I can now resume adding functionalities: chapters management and hopefully the basics for the actual export if everything unfold as expected.
Eww. I didn’t know you could do that in C++. That feels like undesirable language complexity to me. As one who doesn’t see the need for this stuff in anything he writes, I’d rather have distinct types for those sorts of differences than increase the language complexity to such a degree (it works OK in C++ because they use templates, but in a generics system I think it’d be *way* more complex).
C++ 14. See section 8.3.4 of https://isocpp.org/files/papers/N3690.pdf.
They are roughly equivalent and they solve the same problem. Some minor differences: * asyncio is older and more mature * asyncio is one of many async libraries for python, people still use Gevent, Twisted and others. Tokio is the one and preferable way for Rust.
Honest question: are such features desirable in a language like rust? I don't know if it's doable right now, but wouldn't macros be better suited for such behavior?
As a continuation of the book tutorial, it makes sense to remain at the low level to show how things are done. If you use Hyper, it's no longer a basic webserver how to, it's a tutorial on Hyper.
[removed]
It's also likely that only a few different implementations would cover almost all imaginable use-cases, and that re-implementing those yourself when you need it is a bit of a waste.
Let's call this a feature question. Or a Search failure. :P I seem to remember a few months back reading about a method that allows data that is to remain static and immutable to be complied into a Rust executable while sourced from a plain text file. This data is then very efficiently available as the I/O overhead has been taken care of by the complier. Did I just imagine that? Using I/O isn't that big a deal for my project. I'm just looking to play with a new-to-me feature.
On mobile right now, but I believe the macro is called `include_str!`.
What exactly is it your are trying to do? Do you have a code example that isn't working?
Hi, I'm the author of https://github.com/debris/tiny-keccak For me, keccak was also one of the first projects I did in rust. Regarding the style, I totally agree with /u/adwhit86 that putting everything in a single file would significantly increase readability. Regarding the performance, you should avoid heap allocations. Keccak is super simple and there is no need to allocate `Vec`. Your state and output should be just a simple array. After you get rid of heap allocations, write some benchmarks and compare your implementation with other implementations. Consider inlining some code or unrolling the loop. This crate is actually quite good for loop unrolling. https://crates.io/crates/crunchy
&gt; with often nicer ergonomics I'd go so far as to say that using a one-dimensional array to represent a matrix is un-idiomatic, since it needlessly obfuscates the meaning of the index values.
I agree, but then you need a Matrix trait that handles both types of matrices, and all your code must be against this trait. It might be a cleaner way to do it than in C++ but the complexity does not magically disappears, you just shift I somewhere else. We have basically experience with the C++ way of doing thins. Once we are able to solve the same problems in Rust we can start getting experience with the Rust way of solving things. 
It would certainly make handling arrays much nicer. At the moment, if you want to implement a trait for arrays, you need to implement it specifically for each size. This is why the `std` crate only implements traits up to size 32. If we had const integers, we could do something like this: impl&lt;T, N&gt; MyTrait for [T; N] {} And get that trait implemented for *every* sized array in one line.
So this is actually intended as a server-building kit or a microframework rather than what most people would call a web server, right? I did a little work on https://github.com/brson/basic-http-server, which is also quite simple, but actually has a working server (serving files from the filesystem) as a goal.
Julia, via StaticArrays.jl. I think they stop being worthwhile around 15 x 15.
So... is this really that surprising? Rust is a systems programming language. Arithmetic and the like has been optimized for eons, and it's not really a case where you'd want manual memory management most of the time anyway. Anything CPU-bound is going to be blazing-fast, provided you don't write it in Brainfuck. I'm Haskell fangirl #1 but I wouldn't recommend it for general-purpose scientific computing. Matrices can be made easy but fighting the borrow checker... not so much. &gt;I’m not even talking about Rc, RefCell and Box which seems like security through obscurity. (Though it can’t reach Haskell monadic level) Just to nitpick: Haskell offers better *purity* guarantees but it sacrifices memory control. There's a lot that's feasible in Rust that's just not in Haskell. &gt;Rust is still “discussing” integer as generic type parameter (since 2015), meaning a matrix type Matrix[M, N, float] will not exist before a long long time. 100% worth worrying about if you're doing science but this doesn't make the language bad.
&gt; with a smattering of unfamiliarity and Rust genuinely being harder because of its greater power/control. What Rust really does *well* is give you manual control of memory. CPU-bound computations, often pure, can be optimized in nearly any language. Hence the prevalence of FORTRAN in scientific computing. 
I assume the author doesn't know about [ndarray](https://crates.io/crates/ndarray)? Does anyone want to comment if complaints 2 and 3 still hold up once you learn about it?
I’d quite like them for my maths library. In C++, we can have Vector&lt;unsigned int N&gt; but such a thing is not possible in Rust, meaning we have to rewrite implementations for Vector2, Vector3, and Vector4
Did you mean Winit, instead of Glutin? SDL2 is tried and true is probably the main reason. There are still areas that Winit and Glutin fail to compile or function properly.
My own language parser and repl interpreter. I just implemented Rust-style comments (C-style with nesting block comments)
Also that Fortran semantics gives you some guarantees for behaviour (aliasing between arrays for instance) that makes a lot of optimizations possible. 
It's probably better to work with double the score though, to keep integer operations.
I honestly thought rust did this as well.
&gt; At the moment, if you want to implement a trait for arrays, you need to implement it specifically for each size. I know that, but const integers are coming soon enough. I'm talking about the features discussed in the parent comment: changing how a type behaves (and even methods defined) depending on the values of these integers.
Thanks. I was wondering (as I come from a background of heavily OOP languages) what the reasoning is for a single larger file being easier to read than file split along functionality/responsibility? I personally find split up files easier to keep track of functionality, but I'm guessing you're pointing out that I may have overly aggressively segmented the code base? If you could spare more insight, that would be greatly appreciated! ^_^
The author says "slower operations because it is on the heap". That doesn't sound right. Are they referring to slower allocation and the higher probability to cache miss? I thought heap memory was just as fast as stack memory, post allocation (and assuming already in cache). Am I missing a sperate cost?
That's not what I'm talking about. Type-level integers are needed and are coming soon enough. The comment above mine was talking about changing a type behavior and methods defined depending on the value of these integers. This basically could be done by creating different types behaving differently and choosing the right one through a macro. My question is: do we *want* types to be this malleable, or do we want macros to be the intended tool for this?
Shouldn't num-traits work on no_std?
As long as the type’s size and stuff can still be resolved at compile time when changing a property based on any type parameter, I don’t see a problem with this. For example, we could have a member values : [float; N] in my example that would change the size of the struct depending on N. Am I misunderstanding or is this what you mean?
Oh wow, tiny-keccak was an invaluable resource to me when I came across parts of the keccak spec I had trouble understanding! Thanks!! I'll definately look into phasing out the use of Vectors in favour of statically sized arrays. Kinda want to do benchmarking on Vect based code THEN move to arrays. Should yield interesting numbers? I hadn't even considered loop unrolling either. Thanks again!
Binary trees are much simpler: struct Node&lt;T&gt; { left: Option&lt;Node&lt;T&gt;&gt;, right: Option&lt;Node&lt;T&gt;&gt;, data: T }
The only issue I could think of is that there is a pointer redirect involved. You have to pick up the pointer from the stack and then check the heap at that location. I wouldn't be surprised if there was some voodoo in modern processors to make this a negligible cost. 
Yes. We may add a simple binary to do that kind of serving too, but it's primarily for others to build on top of.
It all depends on your caching -- your stack will pretty much always be hot, but data on the heap might get dropped from cache lines if you have enough data.
This might be interesting to you: http://cglab.ca/~abeinges/blah/too-many-lists/book/
It *does* in the sense that `httparse` is Hyper's HTTP parser. Other than that though, hyper doesn't help: we wanted a traditional blocking IO + threadpool server. Hyper today is going full async with Tokio. That's great, but not what we're going for here. If you're doing a project that doesn't need full-on maximum speed, `simple-server` is significantly more straightforward than a full tokio/hyper stack. That's something that might appeal to you if you want to try to read some code to learn how things work, or if you know your needs are small. Super serious production use should probably stick with Hyper/tokio. 
I had to write a lexer in C++ recently for a class and it really made me appreciate some of the finer things in rust, especially the enum type. In order to have tokens with values attached to them you have to either do some funky stuff with structs or you need to have a billion classes. Moments like this are when I love rust.
&gt; Actually you can’t even represent a 8×8 chessboard without coding every properties from scratch (copy, clone, print, indexing with [] …). I’m in luck, go has 9×9, 13×13 and 19×19 board sizes … This is also incorrect, indexing and debugging work fine on slices; it's copy and clone which have issues.
Which seems overly magically implicit to me.
Amazing talk by Alex on futures http://www.video.ethz.ch/events/2017/rust/3fd33e1a-a202-4867-b33a-c901a046100e.html
IIRC it was Rust 1.2 or 1.3? Yeah, 1.2 for stdlib types, 1.3 or 1.4 for other types: https://blog.rust-lang.org/2015/08/06/Rust-1.2.html
&gt; &gt; &gt; Other than that though, hyper doesn't help: we wanted a traditional blocking IO + threadpool server. Hyper today is going full async with Tokio. That's great, but not what we're going for here. Is it possible to wrap a tokio/async library into a blocking API? My theory is that it's possible to change simple-server to use tokio internally without ever making a breaking change on the API.
Typically one would abstract over it with a type like Matrix2D, which hides the strange indexing. Then I think would it still be idiomatic. :)
One cool thing is that Ndarray's view types make much of it's api available for stack allocated data too. Then of course it has huge ergonomic challenges.
Ah. Makes sense thanks.
Sure, in theory. Then you're bringing in *thousands* of lines of code to replace ~250 LOC.
Exactly, hence my remark: maybe macros are better suited for all this.
&gt; I don’t see a problem with this Well, having a type arbitrarily and implicitly change depending on some parameter could be seen as confusing and not something we want in Rust. Plenty of decisions have been made already to forgo such features. &gt; Edit: something else that I think is missing from C++ that I would quite like is methods existing depending on type parameters. For example, a method of Vector z() should only exist if N&gt;2 Now I'm confused, this is exactly what the comment I was responding to was talking about. And consequently, it's what I'm talking about: maybe we're better off using macros for this.
I really like the idea of this. It's small and hopefully will stay that way. I wonder how it compares to other web servers in terms of performance. Not expecting blazing fast, but would be nice to have a rough idea what kind of mileage you can get out of it (are we closer to Python's `SimpleHTTPServer` than NGINX). I would run these tests but I don't have much experience with load / stress testing. Happy to give it a go if anyone can point in me in the right direction. 
Yeah, it's kinda strange that the author looked at Rust and Nim for science. But not [Julia](https://julialang.org), a language explicitly designed for science.
Rust also gives some pretty strong aliasing guarantees: - If it's mutable, it's not aliased elsewhere. - If it's not mutable, it won't change. ...but Rust is compiled using a C++ backend, which doesn't offer as much help here as we might like. Also, Fortran knows a ton about memory access locality and arrays, which can help a lot in array-heavy code.
Yeah, I'd like to do that as well, at some point. Coming up with a representative benchmark is tough, designing it would be the first step, then you'd use something like https://github.com/wg/wrk to actually implement said benchmark.
That was quick! Thanks ETH and the other folks who worked on this!
I'm not 100% sure, but I think it's possible to get something similar as the Eigen3 matrix type in Rust where you can separate between all the concerns (matrix API, inline/indirect storage, statically/dynamically known dimensions): trait MatrixDim { fn new(r: usize, c: usize) -&gt; Self; fn rows(&amp;self) -&gt; usize; fn cols(&amp;self) -&gt; usize; }; trait StaticallyKnownMatrixDim : MatrixDim + Default { const rows: usize; const cols: usize; }; struct DynamicDim { r: usize, c: usize } struct StaticDim&lt;const R: usize, const C: usize&gt;; trait MatrixStorage { type Dim: MatrixDim; type Item; } trait MatrixStorageGetValue: MatrixStorage { // for lazily evaluated things perhaps fn get_value(&amp;self, r: usize, c: usize) -&gt; Self::Item; } trait MatrixStorageGetRef: MatrixStorage { // to allow read-only views fn get_ref(&amp;self, r: usize, c: usize) -&gt; &amp;Self::Item; } trait MatrixStorageGetRefMut: MatrixStorageGetRef { // for read/write access fn get_ref_mut(&amp;mut self, r: usize, c: usize) -&gt; &amp;mut Self::Item; } impl&lt;S&gt; MatrixStorageGetValue for S where S: MatrixStorageGetRef, &lt;S as MatrixStorage&gt;::Item: Clone { ... } struct Inline&lt;Si: StaticallyKnownMatrixSize, T&gt; { // implements MatrixStorage etc size: Si, data: [T; Si::rows * Si::cols], } struct Heap&lt;Si: MatrixSize, T&gt; { // implements MatrixStorage etc size: Si, data: Box&lt;[T]&gt;, } ... struct Matrix&lt;S: MatrixStorage&gt; { storage: S, } impl&lt;S: MatrixStorage&gt; Matrix&lt;S&gt; { fn new(r: usize, c: usize) -&gt; Self { Matrix { storage: S::new(r,c) } } ... } ... // convenience aliases ... type InlineMatrix&lt;T, const R: usize, const C: usize&gt; = Matrix&lt;Inline&lt;StaticSize&lt;R,C&gt;, T&gt;&gt;; type HeapMatrix&lt;T, Si = DynamicSize&gt; = Matrix&lt;Heap&lt;Si, T&gt;&gt;; type MatrixRef&lt;T, Si = DynamicSize&gt; = Matrix&lt;Ref&lt;Si, T&gt;&gt;; type MatrixRefMut&lt;T, Si = DynamicSize&gt; = Matrix&lt;RefMut&lt;Si, T&gt;&gt;; ... let a: InlineMatrix&lt;f32,4,4&gt; = Default::default(); let b: HeapMatrix&lt;f32&gt; = ...; let c: MatrixRef&lt;f32&gt; = a.partial_borrow(.., 0..2); let d: MatrixRefMut&lt;f32&gt; = b.partial_borrow_mut(1.., ..); d[[0,0]] = 3.14 + c[[0,0]]; (untested) I'm not sure how easy it will be to come up with an implementation of `ops::Mul` for a product between a `Matrix&lt;LHS&gt;` and a `Matrix&lt;RHS&gt;`. There is likely a need for some more helper traits which tells us via an associated type what resultung storage type is appropriate. You could even go crazy and implement "expression templates" (matrix expressions where the matrix storage type encodes some expression that is lazily evaluated).
i don't see why code like this: [link](https://gist.github.com/anonymous/cf0a8611140b96b7b8d813231911b965) wouldn't be allowed under the current rfc.
Huh? Python was designed as a "system language" but has become hugely popular due to numpy and scipy support on top of a language that allowed hooks to create optimized numeric libraries. Languages which are general purpose but have roadblocks to major usage patterns should have these roadblocks considered as flaws IMHO.
Someone mentions this in https://github.com/rust-lang/rust/issues/26264
I'm not an expert, but the cost of allocation/deallocation on the heap is definitely non-trivial if you do it a lot. Additionally, the cache can only hold factors of 64 bytes. If your heap object is 70 bytes, that means you waste 58 bytes of cache, if it's in the cache at all. On your stack, everything is going to be in the cache *anyway* ( Unless your stack is huge), so no space wasted. Vectors also need an extra 24 bytes to keep track of their size etc, and it's much harder for the compiler to optimize away bounds checks. 
It is allowed but that `enum` adds an extra field for the discriminant to the struct (memory overhead; null pointer optimization does not apply here), and all code needs to pattern match on that (run-time overhead; introduces a run-time branch on every method).
Oh. Uhm.
Hi, I was working on updating some doc tests over the weekend and I realized the rust-book 2.0 doesn't include a section on documentation like the original had. Is this missed or was there a specific reason for this choice?
wow, so many astrophysicists doing Rust. I'm another one and I am playing around with timely-dataflow for distributed cluster computations. Can we have that party? Where is it going to take place?
Also: include: https://doc.rust-lang.org/std/macro.include.html include_str: https://doc.rust-lang.org/std/macro.include_str.html include_bytes: https://doc.rust-lang.org/std/macro.include_bytes.html
Been waiting for these!
What's the memory layout of multi-dimensional arrays in Rust, and how does indexing on them work? AFAIK `type A = [T; 32]; type B = [A; 32]` just means that the flat memory layout is: &gt; [T; 32] = T_0, ..., T_31 &gt; &gt; [A; 32] = [T;32]_0, ..., [T; 32]_31 &gt; &gt; = T_0_0, ... T_0_32, ... T_31_0, ..., T_31_31 so given `let x: [A; 32] = [[..] ...];`, then: let y = x[3]; // y: [T; 32] and let z = x[1][2]; // z == T_1_2 so it's just the same layout as in C if you were using a single-dimensional array, but with very convenient syntax. 
I tried porting some DSP code I wrote in Julia when I first gave Rust a try. It was a pretty big mess (don't recall why other than trait pollution), and I gave up. I now do embedded work and Rust is much better fit, with the exception of point #3 in this post.
This should be definitely possible with specialization, and probably also without it: ```Rust #![feature(const_generics, specialization)] struct Matrix&lt;const no_rows: usize, const no_cols: usize, const max_rows: Option&lt;usize&gt;, const max_cols: Option&lt;usize&gt;, T&gt; { data: &lt;Self as MatrixImpl&gt;::Data } trait MatrixImpl { type Data; fn add(this: Self::Data, other: Self::Data) -&gt; Self::Data; // ... } impl&lt;const no_rows: usize, const no_cols: usize, const max_rows: Option&lt;usize&gt;, const max_cols: Option&lt;usize&gt;, T&gt; MatrixImpl for MatrixImpl&lt;no_rows, no_cols, max_rows, max_cols, T&gt; { default type Data = HeapMatrix&lt;no_rows, no_cols, T&gt;; // ... } impl&lt;const no_rows: usize, const no_cols: usize, const max_rows: usize, const max_cols: usize, T&gt; MatrixImpl for MatrixImpl&lt;no_rows, no_cols, {Some(max_rows)}, {Some(max_cols)}, T&gt; { type Data = ArrayMatrix&lt;no_rows, no_cols, max_rows, max_cols, T&gt;; // ... } ```
&gt; changing how a type behaves (and even methods defined) depending on the values of these integers &gt; Which seems overly magically implicit to me. /u/crusoe /u/ConspicuousPineapple You are both correct, but this isn't really any different than the specialization feature for traits. This feature allows you to specialize trait impl methods, this allows you to specialize struct layouts. I am not saying that this is necessary (you can just write two structs named differently and be done with it), but it isn't any more magical that what we already have on nightly.
If I understand correctly: You have multiple newtype wrappers, each with a fixed content type, and all of them need to implement `Term` by using `IntoIter::into_iter()`. Question: Can you have just one newtype wrapper? Then you can write: impl&lt;I, T&gt; Term for Wrap&lt;I&gt; where T: Term, I : IntoIter&lt;Item = T&gt; And since `Wrap` is yours you won't run into coherence rule issues.
After spending way too much time playing with my vimrc I'm back to [`uom`](https://github.com/iliekturtles/uom) (type-safe zero-cost dimensional analysis) working through doing unit conversions... or more accurately, propagating a lot of generic type bounds.
That's what the notify queue was for. A simple solution is to try_recv() on the queue, and specify a poll timeout when polling. 
I guess it'd replace a bit more than that. `simple-server` itself is ~250 LOC, scoped-threadpool is 150LOC last I checked, the http parser would be the same, the http crate would be the same. We pulled in the `log` crate for what's technically a single line of logging, I've just filed an issue that lets us get rid of that.
Tokio is async io, not WiFi stuff. People might have built WiFi stuff with tokio but that's not what Tokio solves
Sure, the problem isn't in that part of the API, but more in how you implement functions that take generic Matrix types. I'd belive that you would pretty quickly land in where clause hell where you want to implement one trait for all matrix type but want different implementations for `StaticMatrix` and `DynamicMatrix` types so you write: impl&lt;T: StaticMatrix&gt; SomeTrait for T { ... } impl&lt;T: DymacMatrix&gt; SomeTrait for T { ... } // ERROR which errors because "what about a T that implements both `StaticMatrix` and `DynamicMatrix` ? Well, that cannot exist, but there is no way for you to tell that to the compiler.
That is cool. I love you.
&gt; What's the memory layout of multi-dimensional arrays in Rust, and how does indexing on them work? Rust doesn't have "multi-dimensional arrays" natively, so it works exactly like anything else: each row of the outer array contains an entire inner array. Basically, what you said.
&gt; Rust doesn't have "multi-dimensional arrays" natively, But it does! It is just that the syntax is `[[T; 32]; 32]`. Once you get that then Rust does have multi-dimensional arrays. Sure, the syntax isn't `[T; [32, 32]]` or `[T; 32; 32]` or ..., but that's about it. 
Maybe I'm missing something, but 8.3.4 is: First, the decl-specier-seq determines a type. In a declaration T D the decl-specier-seq T determines the type T. [Example: in the declaration int unsigned i; the type speciers int unsigned determine the type unsigned int
What I mean is, Rust doesn't understand that as a multi-dimensional array. It's just an array whose element happens to also be an array.
I suppose that by "system language" people usually mean [languages for writing kernels, drivers, compilers, and core utilities](https://en.wikipedia.org/wiki/System_programming_language). Python was always meant for stuff which is a level above that, and last time I checked numpy was largely not written in python. :)
Sure, but do languages with multi-dimensional arrays (like C and C++) understand them as anything more than that? AFAIK they don't.
We'll never support the first (which in my opinion is the extremely correct choice) but the second falls out naturally from the RFC. Both of these are the consequence of using a well-understood form of parametric polymorphism.
"Specializing" the struct definition is really problematic. Just look at C++'s optimization of their `Vec&lt;bool&gt;
I certainly agree all the terminology here is confusing. I hear people talk about it as though it's a language-level feature, but this isn't my area of expertise.
Interesting. I'm starting a project in Rust that definitely qualifies as scientific computing, and I do not agree with _anything_ in this post. 1. My background before this was C++ and Python. Plus, if your only complaint about too many symbols is the lovable turbofish... You just don't have to use it. Some people gripe about semicolons and braces but I'm of the opinion that they're necessary to have an expression-oriented language which is something I quite enjoy. 2 and 3. The kind of work I do doesn't give a hoot about stack-allocated arrays. It's all structs and whopping huge vectors usually with actually dynamic size. I agree that it would be really nice to get array dimensions in the type, but I've heard the same response from other scientists on this and the issue of dimensions or units in a type system. These things are not a problem for them. The people I talk to would benefit more from memory safety and scalability, not a type system that understands matrices.
Just like in C they are just nested arrays.
With a timeout it would certainly be one step closer but it would still be stuck on the timeout for however long it is, for each iteration (wouldn't seem smooth). It would feel like a hack. I really appreciate the suggestion though. Maybe I'm going about this entirely the wrong way? Am I using MIO incorrectly? I can't find any projects where someone has used MIO in a multi-threaded client. It a bit like the "Chicken crossing" riddle.
Sadly, my Ukulele intro was cut from the video. Still, having it *is* awesome.
A timeout of 20ms would be perfectly fine. It means you'd be iterating 500x per second, so you won't hose your CPU, but it would be an un-noticeable delay from an end-user point of view. Also keep in mind that sleep() at the OS level, is not guaranteed to be the exact time you specify. It is guaranteed to be *at least* the time you specify. So basically you're never going to hit the exact delay anyways. 
And to answer your other question, you're going about it exactly the right way, and what I'm proposing to you is the way it's been done thousands of times. I've put games, routers and custom databases into production and they all feature polling loops that operate like this. 
IIRC you can just drop the `path`; cargo knows how to search the repo for a `Cargo.toml`for the crate you want.
It would be easier to understand what's happening with a non-`Copy` type like `String`. I have built an example here: https://play.rust-lang.org/?gist=326250cdcb73339be6fe11ca084cea2f&amp;version=stable You get a "use of moved value" error, because your closure is using `move`. If you want to have multiple threads accessing a shared piece of memory, you need to use some kind of synchronization mechanism, you can't easily just hand mutable references out across thread boundaries like in C++. Since `i32` is `Copy`, the `move` just creates a bitwise copy of `x`, which is specific to that new thread.
I left a related comment on a similar topic over on HN https://news.ycombinator.com/item?id=15483267 Also, since you didn't link to the discussion over there: https://www.reddit.com/r/programming/comments/76ke23/high_performance_tensor_library_in_nim/
nalgebra is approaching this. Not there, but approaching.
Other commenters have told you you can't do it. Here is why: When you are writing a code in a dynamically typed language - like Python: class Foo: def __init__(self, a, b): self.a = a self.b = b items = [Foo(1, 2), Foo(3, 4), Foo(5, 6)] The objects are actually pointers to dictionaries: items ----&gt; ptr -----------&gt; "a": 1 ptr ----\ "b": 2 ptr --\ \ \ \ \ \---&gt; "a": 3 \ "b": 4 \ \ \ \-&gt; "a": 5 "b": 6 If you want `items[2].b` this is what Python does: * Go to the location of the list in memory. * Finds the third element(index 2) * Go to the location of the element in memory. * Find the item with `key="b"`. * Return the value of that item. Since all the data(the array's pointer, the array, and the elements) is not really a continuous memory block, it is possible to add new members on the fly: for x in items: x.some_additional_key = get_data(x) Since there is room to put them: items ----&gt; ptr -----------&gt; "a": 1 ptr ----\ "b": 2 ptr --\ \ "some_additional_key": 10 \ \ \ \---&gt; "a": 3 \ "b": 4 \ "some_additional_key": 20 \ \ \-&gt; "a": 5 "b": 6 "some_additional_key": 30 Statically typed languages - like Rust - are different: struct Foo { a: u32, b: u32, } let items = vec![Foo { a: 1, b: 2 }, Foo { a: 3, b: 4 }, Foo { a: 5, b: 6 }]; `Foo` has a known memory layout, so Rust does not create a dictionary for each like Python would - it simply packs them tightly in memory: items ----&gt; 1 2 3 4 5 6 If you want `items[2].b` this is what Rust does: * Each `Foo` is 8B, and you need the one at index 2 - so the offset is 2*8=16. * `b`'s offset inside `Foo` is 1(because it has one 1B field before it) - add it to the offset to get 16+4=20 * The `Vec` struct contains the memory address of where the array starts - add the offset(20) to it and return the `u32` in there. So it'll give you the item at offset 20 - which is 6: offset: 0 4 8 12 16 20 items ----&gt; 1 2 3 4 5 6 So something like this would be impossible: for x in items.iter_mut() { x.some_additional_key = get_data(x); } Even if the syntax would allow it - how would you store it in memory? If you store it like this: items ----&gt; 1 2 10 3 4 20 5 6 30 But, since we did not change the definition of `Foo`, `items[2].b` will still return the value at offset 20: offset: 0 4 8 12 16 20 24 28 32 items ----&gt; 1 2 10 3 4 20 5 6 30 So instead of `items[2].b == 6` we get `items[2].b == 20` - which is obviously wrong! This is one of the general differences between static typing and dynamic typing. There are exceptions - like Python's `__slots__` or C#'s `dynamic` - but the general rule is that with dynamic typing you always pay the price of using a dictionary but get a nice syntax for it, while with static typing you get faster classes/structs that use less memory but if you want the flexibility of a dictionary(which is not really as flexible in static typing, because the values need to be of the same type) you need to pay the price - both in speed&amp;memory(which you would have also payed with dynamic typing) and in syntax(which is less ergonomic than what you get with dynamic typing) Since you can't put the data inside the struct, you are left with two main approaches(each with several variations): * Modify the struct to **create** room to put the data: * add a field to the struct * wrap the struct in another struct with the extra field * for both previous variation - instead of a field use a hashmap for generic data(problem - what will be the type of the values?) * Store the extra data somewhere external: * another vector, same size of `items`. * An hashmap, with the hash of the `Foo`s as the keys. 
Thank you, I really appreciate it. I will go this route. It really helps to know that I'm on the right track too, so much thanks! 
He raises a valid point in that arrays are second-class citizens in Rust. Without some form of const generics (e.g. `Matrix&lt;M, N, f32&gt;`) it is excruciatingly painful to do the matrix/array operations which would otherwise be trivial in C++, Python, or Matlab... and matrices are the bread and butter of scientific computing.
As if things weren't confusing enough. I'm up for it =P.
I have a cron job set up to recompile the internal crate docs and upload them to [GitHub Pages] every night. It usually takes about 40 minutes to do a full recompile, build rustdoc, then generate all internal crate documentation. Normally recompiling the compiler isn't *too* slow if you are only touching one bit and use incremental compilation. That depends on how far down the dependency tree you are though... [GitHub Pages]: https://michael-f-bryan.github.io/rustc-internal-docs/rustc/index.html
For anybody else who is emotionally invested in Rust (like me!) and felt an irrational rage upon seeing somebody describing Rust as "fail[ing] hard": you aren't alone! Take a deep breath...let it all out. I didn't want to at first, but let's give the points an objective look! I was surprised to feel this initial reaction. I honestly believe that behind the post itself is the sincere presentation of real rough edges with using Rust in scientific computing...I just love Rust, is all. :)
Actually, now that I think about it, if all you want is to set messages to go out on a timer, you can use a timer wheel, and the thing that ticks the time forward is the poller itself. e.g. Since your poller ticks forward at a maximum of 20ms, at every tick, you can check a data structure to see if you have any events ready to fire at that time. This is how things like setTimeout are implemented in Nodejs. There is also a timer implementation in the deprecated Mio code. The disadvantage of the queue is that you have to process events serially. That is: You can't set 3 events in a row to emit at 1 minute, 30 seconds and 15 seconds.. If the 1 minute event is in the queue, the others will have to wait for it. The neat thing about the timer wheel is you can manage everything within a single thread. E.g. when the polling loop calls the handlers, that's when you update the Timer Wheel data structure.. The hard part there is managing ownership. The polling loop needs to be able to at least read the timer wheel, meanwhile you need to be able to mutate it within the handlers. 
I believe they're talking about what C exposes via [`alloca(3)`](https://linux.die.net/man/3/alloca), and that's [issue 618](https://github.com/rust-lang/rfcs/issues/618).
This is the heart of the Policy By Design idea: a few common bricks can be combined in an exponential number of "final" products.
The compiler is really telling you what's wrong here; when you peek, you're borrowing some data that's owned by `self.input`. You can only borrow one mutable reference to something at a time.
But then why am I not having the same error for fn read_number?
Yeah, exactly. Here's a 'fixed' gist: https://play.rust-lang.org/?gist=7e87e72ba94c89bd726fdf27ca84c999
Because the first borrow occurs in a nested block.
No, because: * The type checker will not be able to verify the data: struct Foo { a: u32, b: String } let foo: Foo = // ... println!("{:?}", foo.a); // OK - `a` is a field of `Foo` println!("{:?}", foo.x); // not OK - `x` is not a field of `Foo` let bar: HashMap = // ... println!("{:?}", bar["a"]); // Is "a" a key in `bar`? println!("{:?}", bar["x"]); // Is "x" a key in `bar`? * If you do want to check for existence of keys, you'll have to do it at runtime: if let Some(a) = bar.get("a") { // work with `a` } else { // what do you do if there is no a? } * Need to store different types? You're gonna have a bad time! let foo = Foo { a: 1, b: "two".to_string() }; assert!(foo.a == 1); // `a` is known at compile-time to be a number assert!(foo.a == "two"); // `b` is known at compile-time to be a string enum Elem { Number(u32), Text(String) } let mut bar = HashMap::new(); bar.insert("a", Elem::Number(1)); bar.insert("b", Elem::Text("two".to_string())); match bar.get("a") { Some(&amp;Elem::Number(a)) =&gt; { assert!(a == 1); }, Some(_) =&gt; panic!("a is not a number"), None =&gt; panic!("a is not a member of bar"), } match bar.get("b") { Some(&amp;Elem::Text(ref b)) =&gt; { assert!(b == "two"); }, Some(_) =&gt; panic!("b is not a string"), None =&gt; panic!("b is not a member of bar"), } * And of course - getting a value from a hashmap is less efficient than getting it from a staticlaly typed field in a struct. 
Not really an issue, I have tried nested blocking in next.
I am having a lot of trouble with this video player. :( Would it be possible to get downloadable recordings or rehosted elsewhere?
There is [section 14.2](https://doc.rust-lang.org/book/second-edition/ch14-02-publishing-to-crates-io.html) which discusses documentation. It doesn't seem to be as complete as the original, and I found it difficult to find since I don't equate cargo and publishing with documentation in my mind.
Nested blocking in next will not fix your issue. I'm not sure what the *best* solution is, but a solution would be to clone the data you're getting from peek, so that you have an owned copy of the data, and are no longer borrowing from `self.input`.
It's also worth nothing that threads can return values through `join`, so you could do this: https://play.rust-lang.org/?gist=495758ee6b1b0944d45ccb75ab701e7e&amp;version=stable not that it would really help with performance here.
Yeah this is what I came up with. Thanks. let ch = { let temp = self.input.peek(); match temp { Some(ch) =&gt; ch.clone(), None =&gt; return EOF } }; self.input.next(); self.pos += 1; if ch.is_digit(10) { self.read_number() } else if ch == '+' || ch == '-' { self.read_op() } else if ch.is_whitespace() { self.next() } else { self.err(); ERR } 
You may want to look into the `unwrap_or` function for Option, as well.
Yes I saw that, however, like you said, there doesn't seem to be any mention of doc tests or other ways of doing documentation.
&gt; Just look at C++'s optimization of their Vec&lt;bool&gt; type into a bitvector. I agree in that this feature can be used to design poor APIs and `Vec&lt;bool&gt;` is an excellent example of that. But many features can be used poorly, this should not be the only measure we use. &gt; "Specializing" the struct definition is really problematic. What are the problems? How is this more problematic than `specialization` for methods?
&gt; &gt; Rust is still “discussing” integer as generic type parameter (since 2015), meaning a matrix type Matrix[M, N, float] will not exist before a long long time. This is a common complaint versus Rust, especially coming from languages like C++ or D in which it's available. Of course, what most people ignore (willingly or not), is that in C++ or D the templates are unconstrained and all errors signaled at monomorphization. This makes things easier for the compiler writer, I suppose, but it's a huge PITA for the user when a wall of errors appear on the screen. And I mean [wall](https://codegolf.stackexchange.com/a/1957/7125): &gt; template&lt;class T&gt;struct W{T v;W(T v):v(v){}}; &gt; template&lt;class T&gt;int f(T x){f(W&lt;T&gt;(x));} &gt; main(){f(0);} Produces 0.5 MB of output by *default* in g++: &gt; $ g++ golf.cpp 2&gt;&amp;1 | wc -c &gt; 537854 &gt; $ clang golf.cpp 2&gt;&amp;1 | wc -c &gt; 22666 &gt; $ g++ -ftemplate-depth-10000 golf.cpp 2&gt;&amp;1 | wc -c # 268+ MB of RAM and almost 15 minutes &gt; 200750356 
&gt; Sure, but do languages with multi-dimensional arrays (like C and C++) understand them as anything more than that? C# does have a language-level concept of a multi-dimensional array, which allows you to write `[x,y]` as an index. You can read the fact that C/C++/Rust et al do not allow that as an indication that they don't - a conceptually multi-dimensional array is just a bunch of nested flat arrays. 
&gt; I agree in that this feature can be used to design poor APIs and Vec&lt;bool&gt; is an excellent example of that. Why do you consider `Vec&lt;bool&gt;` poor in particular? Is there some surprise cost to the space saving?
If its just a sugar for doing the same as what arielby showed (that is, a kind of parametric behavior) its not going to cause errors, but I'm not convinced its a good idea (`Matrix&lt;0, 0&gt;` means "growable matrix" for example is a really dubious API). If its not sugar of what arielby showed, and instead works like C++, you end up losing safety.
[My post on implementing such a tree in Way Cooler might be helpful](http://way-cooler.org/blog/2016/08/14/designing-a-bi-mutable-directional-tree-safely-in-rust.html). tldr, unless you use unsafe or ugly `RefCell`, you should just use the petgraph crate.
I assume that "debugging" refers to the Debug trait, and this is indeed a problem, along with various other common traits like PartialEq, Ord, Hash... Rust 1.21 fixes the issue with Clone/Copy, however.
Cool, you are right! Thanks!
Slices have a debug impl though, and those work through deref
The 20ms poll timeout seem to be a good choice that will fit a lot of these solutions. It will work for my threaded time loop, and it will work for reading `stdin` and passing data down the line. I thought `poll` with timeout would dispose of the event if it timed out (like a web request timing out), but I am happily mistaken. Do you know of any large piece of software that implements this method of polling with a short timeout where I can poke about in the source code to further educate my self around this? (programming language doesn't matter) Would be good to understand further how this will affect everything from power usage (laptops and mobiles) to whatever side effects it can have etc. Again, thanks a lot for the help! 
I've filed an interesting issue: https://github.com/steveklabnik/simple-server/issues/64
I have returned to my Stardew Valley clone, and have so far added collision detection and made the viewport follow the player. I also published [xnb-rs](https://github.com/jdm/xnb-rs), which I am using for reading XNB game assets.
Okay, after reading the article: his Rust complaints are already being discussed on the front page of this reddit right now: https://www.reddit.com/r/rust/comments/76olo3/why_rust_fails_hard_at_scientific_computing/
It's widely considered poor, in my understanding. The reason is that it doesn't follow the same representation as other vectors, the feature being discussed in this thread. Here's a post from about as official a source as you can get: https://isocpp.org/blog/2012/11/on-vectorbool &gt; There are really two issues here: &gt; &gt; * Is the data structure of an array of bits a good data structure? &gt; * Should the aforementioned data structure be named vector&lt;bool&gt;? &gt; &gt; I have strong opinions on both of these questions. And to get this out of the way up front: &gt; &gt; * Yes. &gt; * No. 
Scientific computing wants to have its cake and eat it too. Copy array's around without actually knowing what is happening behind the scenes allowing the compiler to optimize array/co-array access. The only language that does this is FORTRAN. By the block metrics just about everything outside of FORTRAN and MATLAB are failures.
A Vec is backed by a dynamically allocated Array. Use [small vec](https://github.com/servo/rust-smallvec) for small stack Vectors.
This is not exact and not in std, but close: https://docs.rs/itertools/0.7.1/itertools/trait.Itertools.html#method.tuple_windows. If you really want that `None`s at the ends, something like `let result = once(None).chain(input.map(Some)).once(None).tuple_windows().collect::&lt;Vec&lt;(_, _)&gt;&gt;()`
[removed]
You can make this work pretty easily with the `slice::windows` iterator, but it requires copying the elements so it won't really work if they're not `Copy` or `Clone`. Acknowledging that, it actually comes out to be quite elegant, except for the first and last elements which need to be inserted manually: let mut result = vec![]; // we use `.get()` as it returns `Option` if input.len() &gt; 0 { result.push((None, input.get[0])); } // `windows()` produces a sliding subslice "window" of the given length // but it always starts with the first two elements and ends with the last two // elements, thus why we had to fix up the ends of the vector let iter = input.windows(2).map(|win| (win.get(0), win.get(1))); result.extend(iter); // if`len = 1`, this will push `(elem0, None)` if input.len() &gt; 0 { result.push(input.get(input.len() - 1), None); } By the way, what's supposed to happen if there's only one element? Do you want: * `[(None, elem0)]`, * `[(elem0, None)]`, or * `[(None, elem0), (elem0, None)]`?
I posted an announcement about this already, but I'm working on [flif.rs](https://GitHub.com/dgriffen/flif.rs), a pure rust flif decoder. This week I hope to parse some of the transformation data before moving into MANIAC decoding.
Shameless plug for my own plugin: https://atom.io/packages/racer-plus
I actually tried this but I couldn't get it working properly with nightly. I don't know enough about how Atom plugins are built to properly diagnose the root of the issue but as far as I could tell I had it configured properly.
I'm very curious how other people find the extended API that I wrote, which has an explicit task, and makes the process of notification-delegation to other futures clear.
I might be able to fix it if you can get me a stack trace. I believe Atom automatically includes one if you hit "report issue" on the error. Thanks for downloading it! Sorry it's not quite working for you.
The idea that barring children and gay people is morally equivalent is an extreme false equivalence
I made some changes today and separated out the ui macro into its own syntax plugin. This allows cool things like expressions inside of content in RSX nodes which evaluate to more RSX nodes. &lt;Button Width=200. Height=200. BackgroundColor={ColorF::new(1., 0., 0., 1.)}&gt; {{ if false { ui!(&lt;Button Width=10. Height=10. BackgroundColor={ColorF::new(1., 1., 0., 1.)} /&gt;) } else { ui!(&lt;Button Width=20. Height=20. BackgroundColor={ColorF::new(1., 1., 1., 1.)} /&gt;) } }} &lt;/Button&gt; This was a big change that was necessary to support things like mutating the component tree in response to some internal data change (think adding a new item to a List) I also got the beginnings of event handlers working. My goal is to be able to tie this all together and maybe get a Todo App example working. [Here's a screenshot](https://imgur.com/a/Xk05G)
Me too. Videos are frozen while sound is fine.
And I'm sure you can explain to me what the difference is.
In Python’s numpy library (arrays), essentially a genetic tensor type that can reduce to vector or matrix are heap allocated and single dimensional. A “stride” parameter determines how far one jumps in each direction. This is the standard layout used according to the BLAS API, which — in conjunction with Lapack — is to numerics what OpenGL is to graphics. Hence to operate with custom numerics libraries like the Intel MKL, GotoBLAS or Atlas the appropriate representation is a Vector of float or double. Python’s numpy and Java’s nd4j are both built on the BLAS library. Further the dimensions used in machine learning are often unknown at compile-time. For example most recommenders and topic models boil down to a form of matrix factorisation. The data determine the row-count of the long matrix, and frequently the column count of the wide matrix. The inner column/row count was once a parameter supplied by the practitioner, but in the age of Dirichlet and Pitman-Yor processes, is also data-dependent. Therefore the original author’s presumption that Rust in Science is precluded by the absence of stack allocated matrices with dimensions fixed at compile time is faulty. Rust’s support is precluded by the absence of well-maintained “Rusty” APIs over BLAS and Lapack supporting matrix arithmetic and common décompositions like SVD. 
You stop being a child at a point in your life, for one.
Doing some more work in [rust-chip8](https://github.com/AlexEne/rust-chip8) project and [rust-particles](https://github.com/AlexEne/rust-particles). Nothing too fancy, some cleanup and refactoring. This week I will also start, if time allows my Wolfenstein3D-like project. All of them will be done or mentioned on my twitch/youtube as usual.
I [rewrote my prime checker](https://github.com/xyzatesz/primeutils) utility that I originally wrote in Java. Now improved, but I still want more functionality in the generating part
Save for the first option(empty vector), all of them will require to change the type from `Iterator&lt;Item=(T, T)&gt;` to `Iterator&lt;Item=(Some(T), Some(T))&gt;`.
&gt; Only if you do unity builds. Do you mean "only if you do a complete rebuild of the C++ project each time"? As I understand it, the purpose of unity builds is to *speed up* C++ in situations where you'll be doing that either way by reducing disk seeking. Either way, that's why europa42 said this &gt; but there is a lot work being undertaken to address that. For example, incremental build support is in the "still squashing regressions before we can turn it on by default" stage of development, [as is ThinLTO support](https://internals.rust-lang.org/t/help-test-out-thinlto/6017).
Unity builds in C++ permit you to do optimizations that would otherwise not be possible by the rules of the language. For similar reasons the Rust compilation model is the crate. C++ generally compiles faster than Rust because less stuff recompiles (a single object vs an entire crate). &gt; For example, incremental build support […] […] does not appear anywhere fast enough to make up for the compilation model. At least at the moment it does not look like incremental build support is going to help much.
I just did a crude test and it looks like Rust is not using "linker supplied information" for optimization. Can somebody confirm that, or maybe somebody knows why it works this way?
So, let's break down the first error that shows when attempting to compile your linked code, which seems representative of the rest of the errors too: error[E0499]: cannot borrow `*self` as mutable more than once at a time --&gt; src/main.rs:72:21 | 67 | let ch = self.input.peek(); | ---------- first mutable borrow occurs here ... 72 | self.read_number() | ^^^^ second mutable borrow occurs here ... 85 | } | - first borrow ends here Basically, the compiler is saying "You've already got a borrow (immutable, in this case) on `self` at 67, which you can't share with a mutable reference of `self` at line 72." You can't mix an immutable borrow with a mutable one, so Rust rejects this code based on ownership. Wait, what? What does that mean, exactly? How does `self.input.peek()` borrow `self`? Aren't we done borrowing `self` once `peek` returns? Let's look at the [definition of `peek`](https://doc.rust-lang.org/std/iter/struct.Peekable.html#method.peek) (emphasis mine): &gt; fn peek(&amp;mut self) -&gt; Option&lt;&amp;&lt;I as Iterator&gt;::Item&gt;[src][−] &gt; &gt; ***Returns a reference*** to the next() value without advancing the iterator So, when you use the `peek` method on your code's `self.input`, you're actually still borrowing stuff from `self` -- but the borrow is of the `input` member, since you're borrowing some of its contents. This conflicts with calling `self.read_number`, because you're saying "I want to borrow all of `self`" here. A more visual way to think of this would be to look at a teeny table of our list of borrowing demands when we're using these two snippets: Snippet|Borrowing `self.input`|Borrowing `self.pos` :---|:--:|:--: `self.input.peek()`|**yes**|no `self.read_number()`|**yes**|yes See the conflicts in bold? This is what the compiler is complaining about. The compiler error itself says that you're borrowing `self` at line 67 because you're dot-accessing `self` to get `input` (which, IMHO, isn't the most clear if you're still learning). So...there's two ways you could solve this. ## The easy way: cloning A really simple solution could be to just copy the `char`, instead of hanging on to a reference. Since you're just borrowing a `char`, and your `&amp;char` is going to be a comparable size to a `char` on the stack anyway...There should be no performance implications there. In your case, [here's a diff](https://pastebin.com/iBtCRNnC) that would fix your code (fixed code [here](https://play.rust-lang.org/?gist=423b78d121bafe509e707f31f8920bc5&amp;version=nightly)): --- code.rs 2017-10-16 10:34:52.866213600 -0600 +++ code_fixed_with_clone.rs 2017-10-16 10:34:38.033128000 -0600 @@ -54,7 +54,7 @@ } fn read_op(&amp;mut self) -&gt; Token { - let ch = self.input.next().unwrap(); + let ch = self.input.next().unwrap().clone(); if ch == '+' { Plus(ch) } else { @@ -64,13 +64,13 @@ pub fn next(&amp;mut self) -&gt; Token { - let ch = self.input.peek(); + let ch = self.input.peek().cloned(); match ch { Some(c) =&gt; { self.pos += 1; if c.is_digit(10) { self.read_number() - } else if *c == '+' || *c == '-' { + } else if c == '+' || c == '-' { self.read_op() } else if c.is_whitespace() { self.next() ## The hard way -- when you can't (or shouldn't) just clone In your case, cloning a `char` is just so much easier, so I don't feel that trying to keep the borrows is necessary or even desirable here. That said...have some more tools for your toolbox down the road! :) Sometimes, it might not be desirable to just clone the data you're working with. In this case, it's better to: 1. Use "static" methods for your `struct` definition that use specific members as arguments instead of `self`. When you call another method that takes `self`, you're implicitly asking to use ALL of `self`, even though you may not intend to. This unfortunately means that convenience methods for your `struct` might not be as straightforward to write, but this will allow you to granularize your borrows into what you actually need. 2. Get *values* you need to use with conditionals and other further computation first, then end your borrows of `self` so you can use other methods that need `self`. \#2 is actually a viable solution to the compile errors you're experiencing with `next` -- you can simply store a function pointer, since all of your functions have a matching signature (except for `err` -- see the next section about that). Here's what it might look like applied to the implementation of the `next` method: pub fn next(&amp;mut self) -&gt; Token { let read_fn: &amp;Fn(&amp;mut Interpreter&lt;'a&gt;) -&gt; Token; match self.input.peek() { Some(c) =&gt; { self.pos += 1; if c.is_digit(10) { read_fn = &amp;Interpreter::read_number; } else if *c == '+' || *c == '-' { read_fn = &amp;Interpreter::read_op; } else if c.is_whitespace() { read_fn = &amp;Interpreter::next; } else { Interpreter::err(); } }, None =&gt; return EOF }; read_fn(self) } [Here](https://play.rust-lang.org/?gist=e81fdea95c3f6a22b432dacbd5f59436&amp;version=stable) is the source for the fixed code ([diff from your original code](https://pastebin.com/THaY881a)) -- I also did some other things that I hope with make sense for other review items that I'm going to write about now. :) ## Other code review items I'm not actually going to do a high-level critique of your code here (I don't know that I have enough time to internalize the code that much, unfortunately!), but there were several places I saw that Rust idioms could be applied: * I noticed you're using the `panic!` macro for error handling. I don't know if this was a long-term solution for you, but in case it is...Whoa, there! You should get familiar with the `Result` type and use it -- panics are for when there's literally no good way to recover from something. Almost all idiomatic Rust libraries you find will use `Result` as the way to handle errors (for good reason). The [`error-chain` crate](https://crates.io/crates/error-chain) really makes using `Result` types from different sources a breeze, and I'd definitely recommend learning it. * If you decide that panicking really *is* what you want, then you could actually mark your `err` method as [divergent](https://stackoverflow.com/questions/31082098/why-would-i-use-divergent-functions): pub fn err() -&gt; ! // Note the `!` return type -- Rust actually uses this to mark if a function will ever return or not. * While we're talking about the `err` method...there doesn't seem to be a concrete reason to need a `&amp;mut self`? * There's actually no need for the `box_syntax` feature in this code, whose motivation seems to be the content of the `expr` method -- you can simply use references instead, like so: pub fn expr(&amp;mut self) -&gt; i32 { let add = |a: i32, b: i32| a + b; let subtract = |a: i32, b: i32| a - b; // ... let f: &amp;Fn(i32, i32) -&gt; i32 = match self.next() { Plus(_) =&gt; &amp;add, Minus(_) =&gt; &amp;subtract, _ =&gt; Interpreter::err() }; // ... } * Code that uses `Iterator`s in Rust is generally oriented in the functional programming paradigm -- the snippet for your `while` condition in the `read_number` method can go from this: while { let temp = self.input.peek(); temp.is_some() &amp;&amp; temp.unwrap().is_digit(10) } { ...to this: while self.input.peek().map(|t| t.is_digit(10)).unwrap_or(false) { * There are several places I see that it could be more idiomatic to use a `match` block and/or expressions instead of statements and/or `if`s: * Your `expr` method body could use `match` to simply assign the right value to `f` the first time, going from... let mut f: Box&lt;Fn(i32, i32) -&gt; i32&gt; = box |a: i32, b: i32| a + b; // ... match self.next() { Plus(_) =&gt; f = box |a: i32, b: i32| a + b, Minus(_) =&gt; f = box |a: i32, b: i32| a - b, _ =&gt; self.err() } // ... ...to: let add = |a: i32, b: i32| a + b; let subtract = |a: i32, b: i32| a - b; // ... let f: &amp;Fn(i32, i32) -&gt; i32 = match self.next() { Plus(_) =&gt; &amp;add, Minus(_) =&gt; &amp;subtract, _ =&gt; Interpreter::err() }; // ... } * The `read_op` method body might change from: let ch = self.input.next().unwrap(); if ch == '+' { Plus(ch) } else { Minus(ch) } ...to: match self.input.next().unwrap() { c if c == '+' =&gt; Plus(c), c =&gt; Minus(c) } The most important part here is that you're using an expression where possible -- this is made possible by `match`, which you obviously understand how to use judging from your code elsewhere. :) * `eat`'s body could change from: let tok = self.next(); match tok{ Int(i) =&gt; i, _ =&gt; { self.err(); 0 } } ...to: match self.next() { Int(i) =&gt; i, _ =&gt; Interpreter::err() } 
I would love to see capability-based security pushed way harder than it is in FreeBSD, to the point of making capabilities the default and only way to do anything. Fuchsia, for example, is moving this direction.
Have you read https://github.com/tokio-rs/tokio-rfcs/pull/3 ?
working on [web framework](https://github.com/fafhrd91/actix-web) for [actix](https://github.com/fafhrd91/actix)
That was very enlightening, thank you. After watching this talk, I'm starting to think that the implicit way that the task gets passed around is a big mistake. Most of the questions at the end (the points people are apparently struggling with when trying to understand the future ecosystem) are at least tangentially related to not receiving the task as an argument for the poll function. It would be much more straightforward to understand what you need to do to make this run in a nostd context. It would prevent the Future from operating without a task statically and prevent that ugly panic. It would be more elegant from an API perspective. Is there really no way to make it work? I understand that you still need to do something to implement the `AsyncRead` and `AsyncWrite` traits, so is it so much worse to have to do some more work? Plus, I just have the feeling that this might cause some unexpected problems in the future in a way that we can't even think of right now. The whole thing seems like something that we will look at in 10 years' time and lament that we can't change it anymore due to backwards compatibility.
Continuing work on [tarpaulin](https://github.com/xd009642/tarpaulin) once again this week. Last week I put tarpaulin on the request for contributions and closed some PRs as a result and got the joy of new contributors :). I also closed an issue relating to coverage of different types of templated code and inline functions. Making tarpaulin the only tool that reports coverage for that type of code (to my knowledge). This week I've got another issue I should definitely be closing. Then a couple more I'd like to close. Hoping to stop working on issues and work on some features though!
The C/C++ support for “multi-dimensional” arrays is pretty much the same as Rust’s - as arrays of arrays. There is no language-level concept of an array with multiple indices. ...but I use Eigen so much, it feels like C++ does have multi-dimensional arrays. Once Rust has an equivalent library (which requires integer type parameters / pi types) I’m planning to jump ship.
Thanks for Timely and Friends. I thought about this some more, we'd need a numa aware pool per socket, then carve that into pools per thread. Between threads on a socket should just be passing a ptr. Across sockets should be a memcopy. If all the structs have a `from_raw_parts` then `memcopy` should be good enough. How would one ensure there are no absolute pointers, implement all of your data structures over a `Vec` and use indexes? Sending across machines gets a little trickier. Feels like there are lots of possibilities with Rc, WeakRef, etc for cross process Rusting.
Wait, Redox can be run in docker? How? From my understanding, the container reuses the kernel interface of the host, so the container needs to be a Linux distribution of some sort?
So what, people also often heal a lot of illnesses but the Rust CoC also bans exclusion based on illness. If you wanna use that criterion to say it's okay you've to concede that discriminating based on curable or passing illness is fine.
FYI: I've made an issue in the tracker here: [#33](https://github.com/ErichDonGubler/adhesion-rs/issues/33).
“Scientific Computing” often involves good doses of linear algebra, and that is so much easier with a good Matrix/Vector abstraction.
Not the language itself, but I’ve always understood my beloved Eigen C++ library to do this.
Ticki seems to have gone silent on github, is he okay?
It's released under the LGPL, which means that your project isn't particularly encumbered other than by needing to dynamically link to it so an alternative implementation could be substituted in. The only part that is encumbered by copyleft provisions are changes to the actual LGPL'd library itself, as long as it's possible for the end user to be able to replace the library with a modified version.
Interesting, can you make a comment on the ThinLTO tracking issue? https://github.com/rust-lang/rust/issues/45320
The Rustfest team knows some of these issues and is currently working on getting them fixed, which is also why they were not announced officially. Videos will eventually be available on the [Rust YouTube channel](https://www.youtube.com/rustvideos).
The author seems like he is trying to stick to static languages with static type systems. Julia is a dynamic language with a dynamic type system. I think Julia is a competitor to R but not Fortran. I think the author was looking for something that competes more directly with Fortran.
I'm at work right now but I'll see what I can do when I get back.
You can do much of this with timely communication. At least, the per-process boundary is abomonation serialization, which is "just a memcpy" (though chased if you have nested allocations), and within processes it is just pointers moving. If you set up a process for each socket and pin the cores, you should get something similar to what you want. &gt; How would one ensure there are no absolute pointers, implement all of your data structures over a Vec and use indexes? I'm not sure I understand this question, exactly. The timely computational model ends up looking like dataflow, so if you have a bunch of reads you would like to perform against a distributed `Vec` you end up minting a stream of `usize` indices and get a stream of corresponding `(usize, T)` reports back. Roughly this is what happens with your memory controller under the hood, except we do it in software because .. right. The dataflow model isn't great at ad hoc control flow, and works best when you want to do the same thing over and over. This all works across machines, but the catch is that `Rc` and friends don't have `Serialize` implementations. I think I may have missed the intent of your response; if it was talking out how you might implemented a distributed shared memory Rust, with its ownership and borrowing idioms, then I totally missed it, yes. :)
 #[derive(Debug)] struct Test&lt;T&gt; { inner: [T; 64], } bails on playground because error[E0277]: the trait bound `[T; 64]: std::fmt::Debug` is not satisfied 
One of the goals of the current design (where the task is "magically in the environment somewhere" instead of in an explicit argument) is that it makes it possible to wrap libraries that only understand the standard `Read`/`Write` interface, as long as they're willing to return `io::ErrorKind::WouldBlock` to the caller. There's some discussion of that goal here: https://github.com/alexcrichton/futures-rs/issues/129 It's possible that you could create some kind of shim, to adapt the "explicit argument" approach into a "magically environment" in just the specific cases where that's needed. I think there's some discussion of that option in the github thread too. But I don't know if anyone's tried it.
I believe that's what OP wanted to begin with.
Fantastic work! I'm very excited to see the improved completions, it's something I noticed last time I tried it. I think the main thing I'm really looking forward to now would be [error-chain](https://github.com/rust-lang-nursery/error-chain) support, understanding that supporting macros will be a lot of work but I remember reading a workaround specific to error-chain might be a possibility. I'm a long time Vim user and have always shied away from IDEs, but at the moment the Rust plugin is such a comfort that I'm willing to move out of my editor comfort zone for it.
Can't wait to play with the new release.
That's the reasons we didn't announce the videos yet. Sadly, ETH has released them without consulting with us.
Is there a rough estimate on this?
We don't have access to the material ourselves yet, so, sorry, not estimate.
Sure, you can easily do that, I've created an example [here](https://play.rust-lang.org/?gist=02d2bc0a28aafe9e76422fbd038c3798&amp;version=nightly). Note that this includes parts of the scrapmetal crate to demonstrate how to implement the `Term` trait, which is the important bit.
I'm really eager to try out the preliminary debugger support. It's the one thing keeping me on VSCode.
Oh, you're right. For some reason my eyes skipped right over the "L".
I, too, am working on a Chip8 Interpreter! Still implementing the logic for the opcodes...
We do have a decent Vim plugin: https://github.com/JetBrains/ideavim :) Though, for me personally a combination of multiple cursors (&lt;kbd&gt;Ctr+J&lt;/kbd&gt; or &lt;kbd&gt;⌃G&lt;/kdb&gt;) and extend selection (&lt;kbd&gt;Ctrl+W&lt;/kbd&gt; or &lt;kbd&gt; ⌥↑&lt;/kbd&gt;), which works on AST level in IDEA, works better for quick editing.
That's understandable. Still, many thanks for all the work put into it!
As far as I know, the RFC doesn't intend to make these things explicit.
He is still [tweeting](https://twitter.com/ticki_).
Yeah; my understanding is that the reasons why are laid out in that thread. Maybe OP has better ones to raise there, or something.
A strict* tree is compatible with single (unique) ownership, so it's unproblematic. (* if the binary tree uses parent pointers, it's no longer strictly a tree.)
Just a question, but how would you even get this profile-guided optimization to work? Basically, I'd have to record my program while it's running, right? But what file format does this need / how do I tell the compiler where the PGO file is? I realize this is probably not yet implemented for Rust, but I just wanted to ask.
Is this a reimplementation or just a wrapper? I would love to see this as a proper version of IPFS in a GC-less language with a rich type system.
Just read it and commented! I feel pretty strongly that a global event loop, as proposed in that RFC, is a step in the wrong direction. All my problems with the API had to do with an excess of magic and spooky action at a distance. Namely, that without an explicit task argument, the way that futures are implemented isn't obvious, and the process of delegating notifications to other futures is tricky to get right.
I actually think it's super useful as an exercise to see these "explicit future" traits, regardless of whether they're incorporated into the canonical API. The implicitness might be the best design in the long run, but as a learning aid having them side-by-side with these versions seems great for learning purposes.
I think that issue is a step in the right direction. I think that interoperation with libraries that only understand read/write is a bad idea, since the two kinds of IO are so radically different that they should be clearly separate, in my opinion.
The thing about adapting the standard `Read`/`Write` traits is, as you mention, the code *must* be written with non-blocking IO in mind. This is not the default, idiomatic way of doing things- especially not with default methods like `read_exact` or `write_all`. I would be far more comfortable reusing code like that if it had to explicitly opt-in by implementing an `AsyncRead` trait or something. Otherwise it could work fine in blocking contexts and start invisibly blocking in async contexts.
AFAIK it's a design for in the protocol rather than a programming error.
It is a protocol-level vulnerability, so it could not be prevented by Rust.
From a simplicity and extensibility point of view, how does this compare to [actix-web example webserver](https://github.com/fafhrd91/actix-web#example)?
Yeah a lot of people had that reaction, myself included. The main motivating examples seemed to be TLS crates that folks needed to call into, which are really large not to mention hazardous to rewrite. When I think about it, it also reminds me of one of the things people have said they like the most about Go: that you don't necessarily have to Split The World between sync and async. That would be cool too, if we can make it work.
It would be really cool if we could do that, but I don't think that's possible with the current model. When you make io calls that would block, the go runtime transparently pauses the current goroutine and services other goroutines in the meantime. I think that unfortunately in rust, it would be impossible to do the same thing, since there are no green threads.
&gt; How would one ensure there are no absolute pointers, implement all of your data structures over a Vec and use indexes? I am stream of dis-consciousness right now. That comment was about PID (position independent data) so one could `memcpy` across processes w/o chasing pointers. But really I should just test abomination over rdma and see how it does. 
How does it compare to [actix-web example webserver](https://github.com/fafhrd91/actix-web#example), eg in terms of optimal simplicity and extensibility?
An issue with RDMA may be that Abomonation's deserialization takes a `&amp;mut [u8]` because it needs to do pointer correction internally (to set addresses relative to where the `&amp;mut` ended up in your address space). Having pointer addresses be relative rather than absolute would be great, but I don't think that Rust is heading that way soon. :)
I asked at the very end of the last week's easy question (just wanna double check). Has anyone got diesel mysql statically linked in a musl build for alpine linux? I'm using rust-musl-builder with mysql installed and can compile just find, but its still dynamically linking the libmysqlclient.
My biggest issue with ideavim (which I've been using--sorry--unfortunately for work) is an almost complete lack of good keybinding support. Something as simple as mapping "jk" to "enter normal mode" (very common) is, as far as I can tell, impossible. That doesn't include fuzzy finding support, support for using a leader key (why bother if you can't do multi key maps, though), or any of the main draws of vim. While vim is easily my favorite text editor, I see almost no point in using IdeaVim
It's been awhile since we've had a meetup. I'm super excited.
Will the talk be recorded?
Next time I visit the Infinite Compiler Mandala I will inquire about relative addresses. Maybe the allocator could track absolute vs relative ? Thinking about this, each allocator in a fleet of processes would have a unique memory range, for a hacked up single address space heap. If something is borrowed, it won't be moving until unborrowed, so one could MMAP where it should be and take fault when hit, doing an RDMA transfer. 
What's the relationship between redoxfs and tfs? Is redoxfs something simpler like ext2 or ext3 (with no copy on write etc), while tfs is more complex?
&gt;This data is then very efficiently available as the I/O overhead has been taken care of by the complier. That's not really the benefit. The data becomes part of your executable image. The OS might read the entire executable into memory at startup or it might read it into memory lazily, but either way the I/O still needs to be done. The main benefit is you get a self-contained executable that doesn't need to locate its resource files at runtime.
If you're implying that C++ somehow disallows LTO, it certainly does not.
&gt; Something as simple as mapping "jk" to "enter normal mode" (very common) is, as far as I can tell, impossible. You have been able to do that for years. In fact, ideavim reads your .vimrc, so you don't even have to add them on .ideavimrc.
Fair. You can probably do it with LTO but support for that is weaker and when I still dealt with it that was way slower than unity builds to compile. 
Pretty much
I can donlode them on the rss feed http://www.video.ethz.ch/events/2017/rust.rss.xml?key=27f2e9&amp;quality=Low
Hup, sorry! I didn’t realize that you were deliberately keeping quiet about the ETH server.
I think the idea here is that you have some sort of encoder like, let's say, `struct Base64Writer&lt;T: Writer&gt;`, which doesn't know anything about async IO. When you write bytes into it, it'll convert them to Base64, and maybe buffer them, and then it'll attempt to call `T.write`. Now if the `T` we passed in is secretly an `mio::tcp::TcpStream` or something like that, then one of two things will normally happen: 1. The write succeeds. Bytes go out on the wire (or into the TCP buffer) just like they're supposed to. 2. The write fails with `ErrorKind::WouldBlock`. In that second case, presumably the `Base64Writer` returns that `io::Error` to the caller. Now if the caller is an event loop, it might handle `WouldBlock` gracefully and arrange to call back later. When that time comes, depending on what kind of buffering the `Base64Writer` did internally, it's possible that a subsequent call to `write` will Just Work. There might be a lot of encoders that _don't_ do the appropriate buffering, but for the ones that do, this is a pretty cool equivalence.
We don't usually have access to recording equipment, I'll ask Niko if he's interested and able to find some.
Because it doesn't actually save bools (as in 1 byte per bool), you can't create pointers to its members. Everything that does normally work with pointers for `vector&lt;T&gt;` now returns some struct that represents the bit and that's overloaded to work as close as possible, but not exactly like in other containers. Lots of weird edge cases arise. Some templates break. In addition, it's not a clear win. It's a space-time tradeoff and the standard just makes a choice for you that you have to explicitly opt out of. 
&gt; *Note: in fairness, I seem to recall that Rust still has a few monomorphization errors too, notably when (ab)using associated types.* If I'm not mistaken, several of those abuses are attempts to encode numbers, a feature that this RFC provides officially.
some overlap with my 'gamedev perspective', but I don't think Nim would be viable for me (it's GC based?) his first point , 'too many symbols' - my proposals are aimed at that. (e.g. allowing omitting the types in 'impls' where the 'trait' has already defined it); the last point, 'int type parameters' ... at least we know Rust does want this
Wow. That's so awesome. Thank you. I will improve on the things you have pointed out, again thanks.
Huh. I've been using it for years and don't remember it ever not being able to do those things. You have an `.ideavimrc` file with `source ~/.vimrc` at the top right? I have `ino jk &lt;Esc&gt;` in my vimrc and ideavim reads it just fine. Leader mapping works fine too, I have a bunch of mappings like: `nn &lt;Leader&gt;r :action SearchEverywhere&lt;CR&gt;` You can type `:actionlist` to get the list of all Idea-specific actions (or just look [here](https://gist.github.com/zchee/9c78f91cc5ad771c1f5d)).
&gt; (but `N &gt;=0` is different from `N &gt; 0 &amp;&amp; N == 0`) I would think that's rather obvious. ;-]
We have a few Julia users, and one Julia developer used to work at our facility. Julia is _potentially_ very nice. It may well have a bright future. But at this point it's still version 0.6, with no API stability or indeed any other guarantees. Once they formally nail down the language and the runtime libraries it may be worth using, but until then I'd be wary of using it for anything more serious than kicking its tires as it were.
I don't think so; from [the PR](https://github.com/rust-lang/rust/pull/43690): &gt; `[T; N]` is now `Clone` if `T: Clone` (currently, only if `T: Copy` and for `N &lt;= 32`). The '&lt;= 32' limitation is what the second point was complaining about, and is still there. Or am I misreading the PR?
Thanks a lot for another great release! I'm glad there's Run Cargo support. Previously I was just using the built-in terminal to execute cargo build and that definitely slowed down my workflow. I'm eager to check out the new type handling.
If so then what is /u/aurele referring to? Because no C++ standard has C's VLAs (except as an extension)...
So, I've been wondering if it would make sense to mirror (if not primarily move) the crate ecosystem over to something like IPFS. Seems like it would be a good fit for reproducible builds without needing a single, central point of failure. Also, it would be *really* nice if things like C library source or large data files could be pulled down through IPFS instead of re-bundling them with every single minor crate update over and over again, chewing through large amounts of disk space for no reason whatsoever yes it annoys me how did you know? 
N3690 is **not** C++14, and predates it by a year and a half; in that time, VLAs were retracted (they weren't in C++98-11), so C++14 _doesn't_ have them. The real C++14 text reads: &gt; In a declaration `T D` where `D` has the form &gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;D1 [ _constant-expression^(opt)_] _attribute-specifier-seq^(opt)_ &gt; and the type of the identifier in the declaration `T D1` is "_derived-declarator-type-list_ `T`", then the type of the identifier of `D` is an array type; ... Note that the expression must be constant. Also the wording in N3690's [dcl.array]/4 ("*An array of runtime bound shall only be used ...*") is removed entirely. They weren't added in C++17, and they aren't expected to be in C++2a (to my knowledge). Also, use [N4140](http://www.open-std.org/jtc1/sc22/wg21/prot/14882fdis/n4140.pdf) if you want a C++14 reference. :-]
How could it? The language doesn't permit it. Or is it using compiler extensions?
No problem! Rust really is a fun and rewarding language to code in, but the initial journey to understanding stuff like the ownership rules can be quite a hike. Keep coming back and asking questions when you have them! :)
Of course I will. Actually, Rust is the first language I am learning seriously, and so far it has been quite rewarding.
You may be interested in [primal](https://docs.rs/primal/0.2.3/primal/), which contains fast prime sieves, factorization and primality check.
Yeah I have seen it, and undoubtedly, it is better than my implementations. But my current target was to practice, both rust and git, so that's why I wanted my own implementations.
ideavim is really great. I used to do Java dev and I can't imagine doing that without ideavim.
This program works for me. fn main() { let arr = [7; 100]; let another_arr = arr.clone(); }
Oh. I guess even [the nightly docs](https://doc.rust-lang.org/nightly/std/primitive.array.html) haven't been updated to reflect this then?
Does it read `~/.config/nvim/init.vim` ?
For Python, the obvious comparisons are the [Twisted Python](http://twistedmatrix.com/trac/) third-party library (over a decade old) and the [asyncio](https://docs.python.org/3.5/library/asyncio.html) standard library module. Ruby has [EventMachine](http://rubyeventmachine.com/). For C (and languages that bind to C), there's [url=http://libuv.org/]libuv[/url]. I believe Rust's runtime was based on libuv for a good chunk of it's existence; Node's runtime still is. The "Glib" library that provides the basic runtime for the GTK+ user-interface toolkit also includes [event-loop functionality](https://developer.gnome.org/glib/stable/glib-core.html) For C++, there's [boost::asio](http://www.boost.org/doc/libs/1_65_1/doc/html/boost_asio.html). Be warned: almost every async library wrestles with making async features ergonomic, trying to make async code look and feel as similar to sync code as possible. Exactly how far you can go in that department depends *very* heavily on the language you're working in, so it may be quite difficult to compare documentation across languages, even for the same features written in the same way.
I'd be pretty curious to find sone folks that are successfully compiling a more recent rust for illumos. I've managed to get cargo built at 1.17, and rustc at 1.16 and 1.17. But nothing past that. Despite lots of searching I haven't been able to find anyone who's sharing those binaries or writing about it. There are even a couple threads on the rust discourse that have remained relatively unanswered. 
Ah thanks, so how different is tokio/futures from Nodejs's libuv/promises setup?
In Python, it'd be the core of [Twisted](http://twistedmatrix.com/documents/current/core/howto/tutorial/index.html) that would be most like Tokio... though you could also look at [Tornado](http://www.tornadoweb.org/) in case its docs make something click for you. In-browser JavaScript, Node.js, Go, and Erlang are fundamentally built around async programming in various forms. I never had a need to do async in Perl, PHP, Java, C, or C++, so I can't help you there.
&gt; Preliminary debugger support in CLion. Do want. Just because I know how to use a command-line debugger doesn't mean I like to do so all the time. IntelliJ Rust is rapidly answering YES to all the "Are we IDE yet?" questions, which is super fun to see.
Is there a crate or macro that lets you define a newtype wrapper around a type, and automatically derive all of its method and trait impls? Basically I'm in a situation where I need a newtype to implement a trait for some stdlib types, but want to avoid writing a bunch of boilerplate.
Submitted a patch to fix this: https://github.com/rust-lang/rust/pull/45339. Thanks for bringing this up :).
C# has `Task` which is a built in Future type. The async/await keywords make it very easy to use.
&gt; For C (and languages that bind to C), there's libuv. I believe Rust's runtime was based on libuv for a good chunk of it's existence; Node's runtime still is. The "Glib" library that provides the basic runtime for the GTK+ user-interface toolkit also includes event-loop functionality Note: This was in Rusts Alpha stages, when Rust _had_ a notable runtime. Also, it was only one of the multiple runtimes :).
I'd **really, really** love if there was any plugin to quickly check the Rust std library documentation. It'd be greatly helpful for productivity.
The talk about the 2D simple game was inspiring. I'm checking out the library she mentions and looks quite good.
C libraries shouldn't be bundled with crates, this is the distros job.
And what about when you don't *have* a distro? What about when the crate uses a version of the library the distro might not provide? Like it or not, it's a thing that crates are doing because there is no better alternative that works everywhere. I loathe it, too, but needs must.
One of the biggest security problems of rust is going to be vulnerable C code compiled into the binary from an abandoned crate.
This is great! I've been wanting to do more work with Rust and FreeBSD. The FreeBSD Release Engineering team actively maintains [vagrant boxes](https://app.vagrantup.com/freebsd). It might be worth considering one of those instead of `uchida/freebsd` in your `Vagrantfile`.
For which integer value of `N` do these boolean conditions return a different result?
&gt; There’s also a preliminary debugger support in CLion. What about debugging in IDEA? Is it being worked on right now?
I'll ask, for which value of `N` is it both == 0 _and_ &gt; 0?
Could you elaborate on what exactly do you need? I usually use `Ctrl+Q` or just navigate to the source of standard library and read markdown :) Or do you want some way to open the docs in the browser? I've filed https://github.com/intellij-rust/intellij-rust/issues/1875 for this :) 
&gt; Matrix&lt;0, 0&gt; means "growable matrix" for example is a really dubious API Sure, what people do is `Matrix&lt;-314, -314&gt;` to mean "growable matrix", where the constant `-314` is "hidden", and the API exposes `const Dynamic: isize = -314`, so you end up writing `Matrix&lt;3, Dynamic&gt;` to mean a Matrix with 3 rows but a dynamic number of columns. I am not saying this is the _best_ idea about how to solve this problem, but at least in C++ the alternative would be to offer types for each case, and you end up with many types: - `StaticMatrix` - `MatrixWithFixedColumnsButDynamicRows` - `MatrixWithFixedRowsButDynamicColumns` - `DynamicMatrix` - `DynamicMatrixWithRowUpperBound` - `DynamicMatrixWithColsUpperBound` - `DynamicMatrixWithUpperBounds` - ... Eigen handles all of this with a single interface: - `Matrix&lt;T, noRows, noCols, maxNoRows, maxNoCols, Options&gt;` so `Matrix&lt;f32, Dynamic, Dynamic, 32, 32&gt;` is a matrix with a dynamic number of rows and columns, but that can grow at most to a 32x32 matrix. It is thus allocable on the stack, and growing this matrix requires no memory allocations. I've used types like this a lot when I know the upper bound of the problem, for example one application needed to solve a linear regresion problem ~a million times per iteration of its main tight loop. Each of the linear regresion problems had an upper-bound (9x9 IIRC), so I was able to just use such a dynamic but stack allocated matrix to 1) avoid memory allocations in a tight loop, 2) be able to just push elements on the matrix "dynamically". The code was both nice and efficient. 
Agreed. In a language with such an emphasis on being explicit (like handling every error explicitly), invisible magic threads are just confusing and don't belong.
Duh, I meant `||` thanks, fixed. Gotta. Drink. Coffe :D
&gt; I've filed https://github.com/intellij-rust/intellij-rust/issues/1875 for this :) That's exactly what I mean. Well, I visioned it in a bit different way, it would be a pinnable panel instead of hover.
The page mentions work on capabilities but does not provide a link. I'm interested to see how redox approaches them.
Different thread, but I wanted to ping you and /u/eddyb about it anyways, so just do it here. `where` clauses in the context of const generics where discussed a bit in the merged RFC and the ones from ticki. C++ has "a solution" (not saying it is the best) in "Partial ordering of constraints" of the concepts section of the C++20 standard: &gt; Concept P is said to subsume concept Q if it can be proven that P implies Q without analyzing types and expressions for equivalence (so N &gt;= 0 does not subsume N &gt; 0) 
The main issue is that `vector` is a `Container`, but `vector&lt;bool&gt;` is not. This is because `vector&lt;bool&gt;` has proxy iterators which do not satisfy `Iterator`. This issue was fixed in the STL2.0 already by making proxy iterators, first class. The second issue is that references to `vector&lt;bool&gt;` elements are proxy references. This is bad because in C++ references are not objects, but proxy references are. This leaks in various ways: vector a = {1, 2, 3, 4}; auto i = a[0]; // i copies a[0], i == 1 i = 4; assert(a[0] == 1); // OK vector b = {true, false, true}; auto j = b[0]; // j is not a bool // it is a copy of the proxy reference // it has reference semantics j = false; // changes the value in the vector assert(a[0] == true); // fails, the value was changed through j
fair. you'd have to manually impl it and have it autoderef
&gt; which allows you to write [x,y] as an index. Does this have any advantage over `[x][y]` or is it only syntactic sugar?
I wonder what the benefit of parameterizing the identifier type is? https://github.com/Marwes/haskell-compiler/blob/master/src/core.rs#L81
Also C99 and Ada
Yes, Nim is GC-based (you can choose between different GCs and GC can be disabled, but of course you wouldn't be able to use stdlib without manual deallocation), but it's moving into more GC-free direction - e.g. strings and sequences will probably be GC-free in the future
I recommend learning about JavaScript Promises (equivalent of Rust Futures) and Observables (Rust Streams)
&gt; I usually use `Ctrl+Q` or just navigate to the source of standard library and read markdown :) Can someone please talk some sense into the RLS developers so we can do this in VSCode? ;-[ I don't see what makes the stdlib source so magical or inscrutable that we shouldn't be able to goto-def on it.
Alternative to having it in the IDE you could use Zeal/Dash for quick access to rendered Rust std docs.
The komi is mostly 6.5 or 7.5 nowadays.
If `[x]` is never meaningful without an accompanying `[y]` then no proxy type is needed for the first index, which is nice boilerplate to avoid.
But `[x]` can just return a reference to an inner array, or what am I missing? E.g. if you have a two-dimensional array `[[T; 32]; 32]` and write `[x]` you get a `&amp;[T; 32]`, that is, a reference to an inner array, in this case, the `x-th` row.
`rustc` has more information available than the usual, single-cpp-file compilation process. It does as a lot of symbol interning as possible and with LTO enabled, interning and dead-code elimination are very effective.
C11 made them optional; a compiler is free to simply not implement them and error out, but every major C99 implementation already supported them I think.
Oh, whoa, I did not know you could have an .ideavimrc this might change everything. 
It can in this example, yes, but I'm speaking in general – there are types for which multi-dimensional indexes are useful, but for which there's no meaningful value for one given dimension.
Sure but then the syntax doesn't really matter, e.g., the idiomatic way of implementing multi-dimensional indices in rust is to make the index type a tuple (the Index trait lets you customize the index type), so you end up with: `[(x, y)]` which is suspiciously close to `[x, y]` and still doesn't require proxies. 
Performance issues are no go. 
I get strange errors on compiling: Compiling libc v0.2.31 Compiling rand v0.3.16 invalid expression !1998 = !DIExpression(6, 34, 0, 6) invalid expression !9284 = !DIExpression(6, 34, 0) Compiling durakserver v0.1.0 (file:///home/daniel/Programme/durakserver) invalid expression !11683 = !DIExpression(6, 34, 0, 6) invalid expression !2610 = !DIExpression(6, 34, 0, 6) invalid expression !11656 = !DIExpression(6, 34, 0) invalid expression !11659 = !DIExpression(6, 34, 8) invalid expression !11661 = !DIExpression(6, 34, 40) invalid expression !11688 = !DIExpression(6, 34, 32) invalid expression !13324 = !DIExpression(6, 34, 16) Finished dev [unoptimized + debuginfo] target(s) in 15.15 secs What does this mean? rustc 1.21.0 llvm-libs 5.0.0 cargo 0.22.0
I'm afraid I haven't actually used either one yet. :/
Sure but this is also why to write fast python code you need to be more careful than in Rust. In Rust you can offer the same memory layout that python does, but you can exactly control how the memory is put. For example, want to allocate a 256 element array inside a tight loop? That's free in Rust if you put it on the stack (the stack pointer is going to get bumped anyways). In python that would be a big nono because each loop iterator you would be allocating memory which is slow. What you would then do is move the allocation out of the loop, but guess what, if you do that, then you cannot easily parallelize the loop anymore: before, each iteration had its own internal private copy of the array, but now they all share a copy, so if you want to run iterations in parallel, now you are screwed. 
I haven't found a single vim plugin for any IDE that could scratch my vim itch when editing text. I'm really attached to my workflow and a few plugins. I was hopeful to see some kind of neovim integration inside IntelliJ at some point but any attempt I've seen has been abandoned.
Seems very bare bones and lacking fundamental features at a glance. The readme doesn't even specify what the target instruction set is. But it's small and not too spaghetii, so I'd be interested to take a look around, most compilers are too large to understand nowadays.
&gt; Therefore the original author’s presumption that Rust in Science is precluded by the absence of stack allocated matrices with dimensions fixed at compile time is faulty. Rust’s support is precluded by the absence of well-maintained “Rusty” APIs over BLAS and Lapack supporting matrix arithmetic and common décompositions like SVD. Sorry but science goes beyond machine learning. This _could_ be an argument about what is precluding machine learning in Rust, but that would be wrong too. We had machine learning Rust libraries, but people droped them because it is a highly competitive field, and other languages with better GPGPU support than Rust have better libraries (Caffe2, TensorFlow). They are better because they have better performance, they are more ergonomic, more high level (allow getting job done faster), etc. 
I concede your point. That said, at the likely size of my toy project, the OS is more likely to load the entire executable into memory, meaning that the I/O cost is paid at launch time. Like I said, it's a new-to-me technique.
Many thanks. These links are exactly what I am looking for. And demonstrates the failing of Search: If you don't know the correct term, getting a hit by describing around the issue can be difficult.
Without looking at the code: During parsing it's probably some string type, while during further analysis it's some nameless representation like de Bruijn indices. 
You *could* auto-deref to the wrapped type. This is considered an anti pattern by some, but avoids the boiler-plate. Be sure not to overload existing methods, lest inference errors befall your code.
I haven't thought about the one element case yet, so thanks for the suggestion! The solution you proposed is exactly what I wanted as it uses the standard library and is not as verbose as the one I had in mind.
I'm happy with it but please implement the "g" command there. :) To do something like "g/^$/d" to delete all empty lines of a buffer. Tried it a few days ago and it didn't work. But kudos for implementing "vs", definitely appreciated.
Java has Future, CompletableFuture and reactive streams in version9, which would be (more or less) equivalent.
Thanks!
Hi author there, wow I expect so much comments, I'll go through them and provide my point of view. Don't bury me too deep :).
 if input.len() &gt; 0 { result.push(input.get(input.len() - 1), None); } can be shortened to if let x @ Some(_) = input.last() { result.push(x, None); } Or if that's too complicated, at the least `input.get(input.len() - 1)` can become `input.last()`.
Actually after trying to use static arrays in Nim see (https://github.com/unicredit/linear-algebra/pull/14#issuecomment-290482881)[here], I just abandoned the whole idea of compile-time sized tensors and implemented them backed by dynamic heap arrays (https://github.com/mratsim/Arraymancer/blob/master/src/tensor/data_structure.nim#L29-L40)[like so (MetadataArray is a array[8, int])]. It is idiomatic to represent multidimensional arrays with metadata (shape/dimensions, strides, offset) and data backed by a one-dimensional heap array. It is done by Facebook, Google, Numpy, Julia and even Rust see (Collenchyma)[https://github.com/autumnai/collenchyma/blob/master/src/tensor.rs], Parenchyma, (Coaster)[https://github.com/spearow/coaster/blob/master/src/tensor.rs#L74-L116] deep learning frameworks. At minimum a tensor library should be able to represent 2D, 3D and 4D data if you want to process N images, of C color channels (usually 3, for RGB), of H height and W width. 5D is useful for time to deal with videos or 3D images and 6D everything altogether CHWN Depth and Time. So static arrays or even vec or vec or vec should not be used (whatever your language) as you need to enforce the locality of your data for efficient operations.
&gt; Sorry but science goes beyond machine learning. This could be at most an argument about what is precluding machine learning in Rust Atlas, BLAS or Lapack have nothing to do with machine learning, /u/budgefrankly's comment has literally nothing to do with ML except incidentally.