An alternative would be to mark the constructor as unsafe so clients think twice before calling it.
Fantastic resource! Another good albeit dated resource in the same vein is the [Producing Open Source Software](http://producingoss.com/en/producingoss.html) book by Karl Fogel, an early developer of Subversion and a then-CollabNet employee.
Windows you can open multiple ODBC connections if you create their assets within control panel first. But yeah if one person opens the same asset multiple times nothing you can do. 
Multiple connections is not the problem. It is about Multiple ODBC Environments. The environment handle is used to create the connection handles. Having multiple connections to the same data source should also not be a problem. That is there transaction safety of the underlying DB kicks in. Maybe the user gets an error message, but the safety guarantees of Rust can be satisfied.
I think of it as: Panics are a higher-level analogue to segfaults. They're for when your program's internal state has reached a point where the only safe way to continue is to tear down the thread before the unknown flaw propagates further and corrupts perfectly good data.
[Elegant Library APIs](https://scribbles.pascalhertleif.de/elegant-apis-in-rust.html) is a good read.
Wow! The true anti-aliasing looks very nice!
I think the most secure ones are ring and bindings to OpenSSL. Most of the C and assembly language code in the ring comes from BoringSSL, so if you trust Google you can trust a huge chunk of ring too. And I hear only good things about briansmith in regards of how he handles development of ring. So if you just want to get things done I would recommend to use ring and pwhash-ring. But if you have time and interested in cryptography, look at [RustCrypto](https://github.com/RustCrypto) project too. (I am a bit related to it as you can guess ;) ) Currently it's under active development and far from being production ready, but I hope in the future it will grow into a good ecosystem of cryptographic crates.
I cannot speak to the actual quality of the implementations, but in general order of password hash security, it more or less goes salted hash → bcrypt → scrypt → Argon2. `pwhash` does bcrypt, `ring-pwhash` does scrypt, and `passwors` seems to basically be a wrapper for the `argon2rs` crate. I have a very [basic auth demo](https://github.com/skinkade/rocket-jwt-roles-demo) for Rocket, using the `argon2rs` crate and JSON Web Tokens.
Very happy to see this become public, and it looks very impressive. I'm blushing a bit. Patrick and I indeed had very stimulating conversations, but all the hard work figuring out how to map rendering efficiently to a GPU is Patrick's.
Highlight: &gt;Both the Quantum CSS and Quantum Render projects took important steps last week by merging the relevant project branches into the main Firefox source tree. This means that it’s now possible to run these experimental powered-by-Servo-technology builds by flipping a build-time switch in a local development build, and automated tests are tracking any new regressions that these builds cause.
There is no need for a root node. Your systems (whatever they are, since Froggy doesn't define such concept yet) can operate on some storages or lists of component pointers (like in [ecs_bench](https://github.com/lschmierer/ecs_bench/blob/master/benches/pos_vel_froggy.rs)), or even traverse your BSP tree that stores pointers to components.
Adding a bit of context to the ECS definition. It's an effective model of arranging your data and writing the logic that processes the data. So effective that it's almost universally recognized as a "good" solution for large-scale systems/engines/games in terms of efficiency. However, ECS is quite restrictive. Typically, you'd end up with some logic mapped to ECS and some staying outside. I.e. your drawable components may be fat, have smart pointers to other stuff (like skeleton objects), and even helper methods. This is not exactly in the spirit of ECS. Where CGS (and Froggy!) shines is that it's not nearly as restrictive, while it still has a potential to be as efficient (or better! see benchmarks) as ECS.
I remember the conversation that we had last summer about this very particular subject. I'm thrilled to see the amazing work on Pathfinder and can't wait to delve into the details!
I wouldn't want to give you money for stickers and logos, to be blunt. The paid writer model makes much more sense, though.
&gt; Right now, Pathfinder only supports quadratic Béziers, but extending the algorithm to support cubic Béziers should be straightforward. Will this affect performance much? Or will the accumulation step dominate the timing anyway?
Thanks for that demo link. I'll look into that. I guess I'll try out something along those lines. I'll see if I can find better ways to handle admin or user only routes without duplicated code, but this is a good start.
I also hope to see some interesting discussion here (partly why I opted to post here). I do know that Rocket isn't prod-ready, but I really like its design and wanted to try play around with it.
will look into it :)
Yes, you're right. I'll change it when I get a chance. Darn Google Docs…
There are two long-running soundness bugs in Rust that are a direct consequence of llvm declaring something UB, without giving reasonable tools to manage it: * https://github.com/rust-lang/rust/issues/10184 (out of bounds float -&gt; int casts) * https://github.com/rust-lang/rust/issues/28728 (side-effect-free infinite loops) edit: the latter is also found in Swift (as developed by many of the biggest LLVM developers), which indicates it's not just the Rust devs being lazy: https://bugs.swift.org/browse/SR-3016
Is anyone working to build a gui toolkit based on WebRender? Something with VueJS like API would be really cool.
These days, not finding **rust** in any programming article is f**rust**rating! 
I was annoyed that Cargo build script [input](http://doc.crates.io/environment-variables.html#environment-variables-cargo-sets-for-build-scripts)/[output](http://doc.crates.io/build-script.html#outputs-of-the-build-script) is stringly typed, so I made a library of helper functions: [foreman](https://github.com/durka/foreman). "Don't pick up those environment variables yourself. Just tell the **foreman** what to do with the cargo." TODO: documentation (for now just look at the [test as an example](https://github.com/durka/foreman/blob/master/tests/foo/build.rs)).
This is awesome! So easy to use :D
Very cool! This has been something I've been toying with in my mind for a while, but my paid job has been taking the majority of my time lately. [We have an active branch open to add this to clap proper.](https://github.com/kbknapp/clap-rs/pull/835) I'd love it if we could all combine efforts! I'm on mobile now so I'll post a link soon, but it's one of the open PRs in the mean time. Edit: add link
Yes, we will release some user cases later after RC2. 
Looks awesome! :) Really cool idea, this is a crate to keep in mind for future projects for sure.
This is nice ! Merging it into clap would be even better
Very awesome. I'm not sure if this is even possible, but I was actually just thinking the other day that it would be super useful if clap could deserialize any `ArgMatch` into any type that `: Deserialize`. Is there any reason you chose not to just submit a PR to clap, though?
Oh. Well the OP probably saw pijul linked on Hacker News, I'd guess.
I had imagined that subcommands would be an enum. Using git as an example, enum GitOp { Status { /* status opts */ }, Log { /* log opts */ }, Add { /* add opts */ }, } The parent struct would have any global options and a field for the subcommand.
That is correct, and my initial thought was wrong(C-influenced). Subcommands are derived from an enum in the [PR](https://github.com/kbknapp/clap-rs/pull/835) /r/Kbknapp mentioned.
The floating point issue is way deeper then the LLVM can hope to managed. I chimed in on a [PR](https://github.com/rust-lang/rust/pull/39271#issuecomment-277251582) TL;DR What is/isn't safe in terms of floating point bounds is massively different on different platforms and modes of FPU execution. The C11 and ISO/IEC standard effectively point their fingers are one another to solve this issue. C and C++ have failed for the better part of a decade to solve this problem. I really don't expect any one project to solve this issue. 
It makes sense that an RFC is not the specification. However, it may be useful to maintain a doc of the changes from RFC that emerged during the stabilization process, and perhaps have a reference to that doc from the original RFC.
It's also found in C, because C defines infinite loops (specifically a loop with a constant true condition) to not be undefined behavior.
&gt; if you can convince me that Dialyzer [...] I shouldn't even need to be the one to convince you; [you've clearly already read a very good introduction to Dialyzer](http://learnyousomeerlang.com/dialyzer) that covers exactly the sorts of things for which you're asking: a fault prevention system that's "similar" (your word, not mine) to Rust's. Not *identical*, but similar enough for Erlang and Rust to be on roughly the same level of fault prevention. This is where the note about static typing not being a dependency of fault prevention comes into play. Dialyzer doesn't just check types; it checks input and output values in general. It does so optimistically, sure, but even the optimistic approach allows for finer-grained *stricter* control than what's available in Rust (the static analysis system of which is very dependent on types alone). In other words, Erlang doesn't need static typing to be safe because it has value-based multiple dispatch (a.k.a. "pattern matching") that can be used to not only emulate type checking, but actually *exceed* it. &gt; You have given me no reason to believe the two sets are of comparable sizes. And you've given me no reason to believe they aren't, especially if we're including Dialyzer in this discussion. Both prevent errors around shared mutable state. Both prevent memory from being used after it's freed. Both provide encapsulation of code within discrete modules with clear separation of public and private functions. They both protect against functions receiving or returning invalid values. Those three things make both languages safer than the vast majority of other programming languages out there. The differences in how those three similarities are achieved have trade-offs. Erlang has a safety edge in the first and second similarities due to it disallowing mutability and low-level memory access entirely, but this comes at the cost of performance and flexibility. Rust has a safety edge in the third due to its static checking being integrated into the compiler (with *very* nice error messages), but Erlang (really Dialyzer) in turn has a safety edge there as well due to its ability to check safety beyond types alone (namely: by also checking values and ranges thereof). Without Dialyzer, Rust does indeed have a more tangible lead on that third point, and I've stated as much repeatedly, and have repeatedly conceded that Erlang would do well by incorporating some ideas from Rust (which it could do very well by incorporating Dialyzer as part of the compilation process).
Maybe also try a log scale to see if it makes the left side of the line graph easier to read.
&gt; Then its backends can insert masks/conditionals as needed to emulate the desired behaviour given the hardware's instruction set. This requires really all those corner and edge cases being well defined. Intel's FPU is a 2^160 space problem. Validation of this is non-trivial before the sun goes nova. [speaking of FPU issues](https://arstechnica.com/gadgets/2016/01/intel-skylake-bug-causes-pcs-to-freeze-during-complex-workloads/)
It just astounds me that Rust attracts the kind of people who can start thinking about a problem and within a few months of work can produce world-beating results.
Wow that sounds like a big project to decide to tackle. What was the inspiration?
They are [very recent additions](https://github.com/rust-lang/libc/pull/508) and haven't been released to crates.io (the successful merge automatically updates the documentation AFAIK). You can probably ping the maintainer to push another release.
I opened two PRs this week and I am incredibly impressed with how smooth the process was and how quickly they were merged. I know they are small tasks and don't make much of a dent in the grand scheme of things, but the process was great for me to gain some confidence and become comfortable tackling bigger issues. Thanks to jdm for guiding me so far!
The [Go link](http://agis.io/2014/03/25/contiguous-stacks-in-go.html) outlines their reasons for moving away from split stacks, and it links to a [rust-dev](https://mail.mozilla.org/pipermail/rust-dev/2013-November/006314.html) thread about Rust abandoning split stacks, both for performance reasons. Would those problems affect this new implementation? If not, why not? That rust-dev email interesting. Here's the last paragraph: &gt; Instead of segmented stacks we're going to rely on the OS and MMU to help us map pages lazily. Although the details aren't clear yet, I expect that on 64-bit platforms the number of concurrent tasks will be comparable to using segmented stacks. On 32-bit platforms, with their limited address space, the situation will not be as good, but this is a calculated risk that we can live without the same amount of concurrency there. The OS doesn't map a page in until it's used, but I'm curious how this would work for say thread pools. If a thread was used for something that required a large stack, how would it shrink again afterward? Maybe a `posix_madvise(..., ..., POSIX_MADV_DONTNEED)` call when a thread is returned to the pool? Is that sufficiently cheap? or would it be better to use some scheme to know when the thread grows, so that the `posix_madvise` can be done only when necessary? Anyway, I wonder what happened to that scheme. If it worked out, then I wonder why split stacks are being pursued again now.
For me personally, it's a little bit far stretched to just assume Andromeda is in fact fuchsia without any real explanation as if it is a fact everybody agrees on. And the hole article is based on that wild guess. I would call this "very brave". Ninjaedit: I also have a hard time seeing Rust to be a first class citizen. I know there is no specification about what a first class citizen is. But if you tell me a language is a fist class citizen on a particular system i expect that i can use that language in every aspect. For example i would call C/C++ a fist class citizen on Linux, Windows but not so much on macOS. I would call Objective-C a first class citizen on macOS and Java on Android. Yes i can use Rust on Linux, Windows, macOS, Android but i would not call it a fist class citizen on that platforms. If we agree on that part i have yet to see a reason why this would change on fuchsia? If i have access to the systemcalls, most of the systems libs like GUI and Widget framework etc. i would agree. But just running Rustcode on a plattform ... i don't know. 
As pointed out multiple times from the [HN thread](https://news.ycombinator.com/item?id=13649523), this article has tons of known facts and pure speculations so I wouldn't say much about this. The use of Rust in Fuchsia is indeed a [fact](https://fuchsia.googlesource.com/magenta-rs/), but this does not immediately make Rust a first-class citizen unless you can somehow make an app in Rust, preferably out of the box. Probably /u/raphlinus may be a better source about Rust uses in this project.
I'm very curious about this project although I'm not enough of a programmer to actually contribute anything (and I've recently bought a Windows PC rather then using my really old iMac). Keep it up! :)
This is not quite accurate. In particular, the "scrypt → Argon2" portion is potentially _backwards_, depending on your exact constraints (and fails to account for the distinction between Argon2i, Argon2d, and Argon2id). Alwen and Blocki have been publishing papers formalizing the notion of "memory-hard functions," which fall into two classes: Those with access patterns that depend on their input (dMHF) and those whose access patterns are independent of their input (iMHF). In [one such paper](http://eprint.iacr.org/2016/989), they show that scrypt is an _optimal_ dMHF. In [another such paper](http://eprint.iacr.org/2016/875), they show that Argon2i is _not_ an optimal iMHF, prove an upper bound on _how close it gets_, and describe a family of graphs that _do_ correspond to optimal iMHFs. As a result: If a dMHF is acceptable, scrypt is optimal. If an iMHF is necessary, then it's still a bit up in the air until someone actually implements a memory-hard function based on depth-robust graphs.
It's not that surprising when you look at Rust's general philosophy. The entire community is all about beating the state of the art solutions with better &amp; safer implementations. I'm excited to see how far we can go!
&gt; implementors can also rely on the forgetfulness of an existential type. But how can violating this result in any problems that aren't caught at compile-time anyways?
Normally we don't want memes here, but for this I'll make an exception.
Probably you already know it, but pcwalton is not a recent member of the Rust community. His involvement dates back to the very origins of it. In the rust compiler git repository, he is the [third person ever](https://github.com/rust-lang/rust/commits/c2d4c1116f96463b8af222365d6) recorded to have made a commit.
That looks like a very nice resource. Thanks for sharing!
Thank you so much! The first alternative worked perfectly, it compiles now. I tried to achieve this behavior by using ```.clone()``` on the keys, which didn't work, I didn't know about the ```.cloned()``` function.
I like the lock and snow icons for mutable and immutable borrows, respectively! I think you can also add the following idiom if you are to add the second page to the graphics: let x = some_value; ... let mut x = x; // temporarily mutable, "thawed" ... let x = x; // no longer mutable, "frozen"
Because existentiality is an abstraction in of itself. Sometimes it's useful to create an existential object in which the user is forced to assume that every instance is unique, when in reality you're just reusing the same type over and over. That property can be useful for, e.g. associating values with unnameable, unique types, which then allows you to validate objects and track proofs that you have already validated them. And using the proofs you can, say, unsafely index into a slice without bounds checking, because you already proved that the index is valid earlier. This is basically what bluss's [indexing](https://github.com/bluss/indexing/) library takes advantage of, but it requires faking existential types with lifetimes (which *can* be existential).
Great! I'll look at the PR.
Yes, that was planned: https://github.com/TeXitoi/structopt/issues/1
You almost never want to use `&amp;Vec&lt;T&gt;`, use `&amp;[T]` instead (which `&amp;Vec&lt;T&gt;` will automatically coerce to). In general I think your might benefit from a more functional approach, instead of all the explicit for loops and indexing.
Ace work! I personally thank you, you've made proper argument parsing too easy for me to put off using it! Ergonomic utility libraries like these are just what rust needs!
&gt; which e.g. also simplifies upgrading password hash functions So if I understand this correctly, this is for let's say a web service that used to use hash A, but that was found to be insecure, so now next time a user logs in, the service transparently updates his password hash to hash B? That sounds great :)
Hi, u/TankerReview. I think you're looking for r/playrust.
I'm curious if it can be compared to DirectWrite, which also run on GPU if configured properly as I understand. It could be interesting because it's an "old" technology used in many programs (like Firefox).
Then simply mark it as `unsafe` and explain in comment/doc why it's so.
Cool diagram but the blue fork off of the purple mut would be a compiler error right? You can't mutate something while it is borrowed immutably. 
Updates: binary files working (fixed last week), network windows in Thrussh working (we can now transfer huge patches over the network even with large connection latency). We figured out a way to specify what the "current branch" is. One more command to implement (change current branch), and that should be it. Issues/TODOs you leave for the last minute before releasing are not the most pleasant ones to work on, but we're working on it.
hey, i rewrote your "sum" function to use functional/iterator style. notice how almost no indexes are used outside of the weights[0].len(), making indexing errors more unlikely by iterating along every element of an iteratable (list/map/collection), instead of explicitly pointing to them. this uses a nightly feature, "conservative_impl_trait" allowing things that implement an interface to be returned ergonomically. this is very useful, if you want to chain further computation with iterators. if you do not want to use nightly, you would need to collect into a vec inside the function and return that. #![feature(conservative_impl_trait)] fn main() { let i = vec![1.,2.]; let weights = vec![vec![3.,4.], vec![5.,6.]]; let result: Vec&lt;f32&gt; = sum(&amp;i, &amp;weights).collect(); println!("{:?}", result) } fn sum&lt;'a, 'b&gt;(i: &amp;'a[f32], weights: &amp;'b[Vec&lt;f32&gt;]) -&gt; impl Iterator&lt;Item=f32&gt; { (0..weights[0].len()) .map(move |index| i.iter().zip(weights.iter()) .map(|(i,weight)| i*weight[index]) .sum() ) }
also the "j" parameter in your activate function seems to not be used
Can we make this into our IDE/editor somehow (i.e. choose a variable and its lifetimes/references will show up)? If we can, there will no "lifetime is hard to get" anymore.
They are Stylo and Webrender. Quantum is just the name of the project to bring them into Firefox. https://wiki.mozilla.org/Quantum
Wouldn't this create three different bindings for variable "x"?
Ah sorry took the color coding too literally 
I'm actually working on something to do just that. Check out my progress at https://internals.rust-lang.org/t/borrow-visualizer-for-the-rust-language-service/4187/30?u=nashenas88. It doesn't look like OP's implementation, but it hopefully conveys similar meaning. It's currently in the proof of concept stage, and I'm at the point where I can start making the true changes to the compiler so that I can integrate this into RLS.
have you tried .map( move || {
Are weekly project updates no longer included? Second week Ruma's update hasn't been in there.
The Underhanded Rust Contest submission deadline is listed here as "Mar 31", but the [contest page](https://underhanded.rs/blog/2016/12/15/underhanded-rust.en-US.html) says "Send your submissions to underhanded@rust-lang.org by March 1, 2017." Which is the correct date?
yeah, I have tried that. It still compiles as soap_message, url and headers are cloned in every iteration of map. If I remove: let soap_message = soap_message.clone(); Compilation fails with: let future = pool.spawn_fn(move || { | ^^^^^^^ cannot move out of captured outer variable in an `FnMut` closure Which makes sense as soap_message is moved into the second closure on the first iteration of map.
Yes, the owner of `some_value` is essentially `x`, then `y`, then `z`. It actually might make more sense to newcomers if it used different variables names, to show that an owner gets to decide the mutability of data, and that a transfer of ownership allows the new owner to re-decide the mutability of the data. I think that using the same name `x` implies that there is a reason they need to all be `x`, even though there isn't. But then, the purpose of that trick is to allow temporary mutability of `x`, so it could also be confusing to have three different variable names.
Yeah, I hope that it'll be there one day and done right. And I think there are more important things to do now. (e.g. `impl Trait`), so I hope devs will focus on those important things first. :)
The function for `map` is called once for every iteration, so if it takes ownership then after calling it once, then all times after that it will read from uninitialized memory. I don't know what your `send_and_print_error_messages` function does but if you can change it to take `&amp;str` and `&amp;Headers` rather than `String` and `Headers` then it may work with a little fiddling, depending on how `CpuPool` works. Worst case you'll have to use `Arc`.
Thank you, this helps. But there is a little issue with putting fn get_#ident(&amp;self) -&gt; #ty { &amp;self.#ident } However removing `get_` will work. fn #ident(&amp;self) -&gt; #ty { &amp;self.#ident } but then the function is not preceded with `get_` anymore, but the field name is the function name. 
Yes, it does, see the `OccupiedEntry` [documentation](https://doc.rust-lang.org/std/collections/hash_map/struct.OccupiedEntry.html)
Thanks for the reply. I've changed the send_and_print_error_message to take references for soap_message and url. headers needs to be passed by value as hyper Client will move it as some point (if i pass by reference compiler fails with 'cannot move out of borrowed content'). So this now compiles: `let send_message_futures = (0..message_no) .map(|_| { let soap_message = soap_message.clone(); let url = url.clone(); let headers = headers.clone(); let future = pool.spawn_fn(move || { let res = send_and_print_error_message(&amp;soap_message, &amp;url, headers, timeout); res }); future }) .collect::&lt;Vec&lt;_&gt;&gt;();` However, this fails: `let send_message_futures = (0..message_no) .map(|_| { let future = pool.spawn_fn(move || { let res = send_and_print_error_message(&amp;soap_message, &amp;url, headers, timeout); res }); future }) .collect::&lt;Vec&lt;_&gt;&gt;();` with three: 'cannot move out of captured outer variable in an `FnMut` closure' errors. Interesting.
My PR was just merged for [this benchmark](https://github.com/kostya/benchmarks), serde is *really* fast.
Oops. Sorry and thanks for the correct subreddit. Thanks mate.
That's what this is. It even has lifetime annotations. Hopefully it can be integrated into an IDE or something in the future
Bad link, points to 127.0.0.1
You can't copy the reference. You can deref and Copy the referenced value if the referenced value has the Copy Trait.
You're still passing `headers` by value rather than by reference.
Yeah, I've removed it from send_and_print_error_message definition for now. This now: `let send_message_futures = (0..message_no) .map(|_| { let future = pool.spawn_fn(move || { let res = send_and_print_error_message(&amp;soap_message, &amp;url, timeout); res }); future }) .collect::&lt;Vec&lt;_&gt;&gt;();` fails with two 'let future = pool.spawn_fn(move || { | ^^^^^^^ cannot move out of captured outer variable in an `FnMut` closure' errors. If I remove the move from the second closure I get: 63 | let res = send_and_print_error_message(&amp;soap_message, &amp;url, timeout); | ------------ `**soap_message` is borrowed here | help: to force the closure to take ownership of `**soap_message` (and any other referenced variables), use the `move` keyword, as shown: | let future = pool.spawn_fn(move || { 
FYI, there's another rust project with almost the same name: the [alacritty (with two Ts) terminal emulator](https://github.com/jwilm/alacritty)
This is the stripped down version of send_and_print_error_message: `pub fn send_and_print_error_message(soap_message: &amp;String, url: &amp;String, timeout: u64) -&gt; Result&lt;(), ()&gt; { // Return an empty result for now let res: Result&lt;(), ()&gt; = Ok(()); res }` I guess the question with this version with no moves is: `let send_message_futures = (0..message_no) .map(|_| { let future = pool.spawn_fn(|| { let res = send_and_print_error_message(&amp;soap_message, &amp;url, timeout); res }); future }) .collect::&lt;Vec&lt;_&gt;&gt;();` How do you convince the compiler that &amp;soap_message will not outlive the function it was called from. soap_message is actually dropped when main() finishes after all of the futures have completed running: `for future in send_message_futures { future.wait().expect("Waiting on future to complete failed"); }`
a reference can never outlive the underlying object, not in rust. in other languages that is called a dangling pointer. in rust every reference has a lifetime, in many cases they are just unnamed. by giving them a name you can reference them later, for example in activate i am saying that the returned item depends on 'b, so it needs to be dropped before ix can be dropped. in sum i am just putting them there to satisfy the compiler though (sadly the error message is really confusing, there are like 3 open bugs for that already. right now a reference in rust stays alive until the next time a block ends, so most of the times when the next } shows up. this is going to be improved, keyword non-lexical lifetimes, towards a reference ending right after it was used the last time. the first paragraph sadly was not quite clear to me, i hope i could address most issues. generic floats: something like that would be possible, using a trait. i am not entirely sure if it would be a good idea though, machines these days are fast on f64 and memory is plentiful. map calls a function a -&gt; b on every element of an iterator containing a-types, yielding an iterator containing b-types. |a| {...} is such a function, written inline, called a closure. move is a keyword which signifies to not take references to things, but to move them into the closure, when they are used inside. typed enums (or, more accurately: types) are exactly the way to solve such things. be aware though, that an enum uses as much space as its largest variant. if your content (in this case just an f32) gets too big, you can use Box to put things on the heap instead of storing them inline i feel that the key to understanding rust is: really getting what the stack and heap are, including the realisation that you don't really need the heap in many cases, doing a bit of functional programming (anything except haskell), and adding random life time annotations to please the borrow checker :)
Why do you need to use `rental` anyways? Hacking the type system's reference lifetimes _and_ using a serialization library at the same time just seems outright dangerous, but I don't know how `rental` works internally.
Besides, google press said Andromeda is just a research project - and that Android will not be unified with ChromeOS in a single platform. Android Apps already run on ChromeOS, and they're working to bring ChromeOS "apps" to Android. But none of them will be discontinued. 
Another solution to this is wrapping soap_message and url in Arc and then moving everything into the second closure: `let pool = CpuPool::new(thread_no); let soap_message = Arc::new(soap_message); let url = Arc::new(url); let send_message_futures = (0..message_no) .map(|_| { let soap_message = soap_message.clone(); let url = url.clone(); let future = pool.spawn_fn(move || { let res = send_and_print_error_message(&amp;soap_message, &amp;url, timeout); res }); future }) .collect::&lt;Vec&lt;_&gt;&gt;(); for future in send_message_futures { future.wait().expect("Waiting on future to complete failed"); }`
It's a minor point, but the long `deref` chains like https://github.com/nivekuil/rculock/blob/04efe478a5f249487a820ab9e6b312eab7dafbf9/src/lib.rs#L73 could probably be abbreviated to `Arc::clone(&amp;inner.unwrap())` and `T::clone(&amp;self.inner.unwrap())` (or, alternatively `(**inner.unwrap()).clone()` etc.). Also, I'm fairly sure that [the `unsafe impl`s](https://github.com/nivekuil/rculock/blob/04efe478a5f249487a820ab9e6b312eab7dafbf9/src/lib.rs#L32-L33) are incorrect; for instance, they allow `T = Cell&lt;i32&gt;` to have `RcuLock&lt;Cell&lt;i32&gt;&gt;: Send`. This is bad because one can first call `read` on one thread to get an `Arc&lt;Cell&lt;i32&gt;&gt;`, and then send the whole `RcuLock` to another thread, and then call `read` again to get another `Arc&lt;Cell&lt;i32&gt;&gt;` to the same data. These can be both accessed concurrently, which is a data race due to `Cell`'s lack of any synchronisation. This is undefined behaviour, and, without having thought through it completely, I would be surprised if one could have anything weaker than `T: Sync + Send` (I'd expect the defaulted impls implied by the struct's contents to be the correct ones).
The word `unsafe` appears in the `rental` library around **56 times**. The way that `capnproto` handles memory serialization and deserialization may rely on being able to rewrite pointer addresses freely, which is what a lot of serialization libraries do, which means the `rental` might be left holding an invalid pointer that it later would try to dereference. This is why I say that what you're doing seems outright dangerous. If you can find a way to just use `Rc` or `Arc` instead of `rental`, that might simplify your life. /u/jpernst is the author of `rental`, so maybe he will be able to help you use `rental` better, and he should get notified that he's tagged here, so maybe he'll be along in a few minutes. I don't have any specific knowledge of `rental`, and the documentation is sparse.
Huh, I get false all the time, C or rust, battery or not.
Let me second thetablt's concern for confusion via name overlap with Alacritty, which was widely discussed last month and has around 5,000 stars on GitHub. :P It's not too late to reconsider! I'm very happy to see more projects leveraging tokio (in this case, by way of hyper). Though these days when people hear "reverse proxy" their minds immediately leap to nginx, so I think this would really benefit from a more thorough discussion of how the two compare.
Wow, this is really neat. It reminds me of Feynman diagrams. As other's said, it would be awesome to integrate this into IDEs, and even perhaps the Rust documentation, I think it would be a great visual aid as people go through the examples.
`rental` ensures that the owner is in a `Box` or similar (so that the reference points into fixed memory), and ensures that it cannot be moved or dropped before all references into it are dropped. If I can get this working, it's perfectly safe. Modifying the Cap'n Proto code generator to hold an `Rc` of the `Reader` instead of a reference to it would also work, mind, if that's possible. I'll look into it if I can't make this work.
I didn't realise that he was that early to get on board! What fascinates me is that I believe that the things people can build are (at least partially) shaped by their tools. As tools get better, humans can build bigger and stronger things faster (compare stone axe to power tools). Some of the biggest evidence to me that Rust is heading in the right direction is that it has enabled some extremely impressive software projects. That, to me, implies that it is a better tool than what has come before it.
Thanks for including the disposition along with the FCP announcements, that's very handy. :)
Hello! We're actually extending it to 3/31, we just haven't finished writing the announcement yet.
I believe he wants to refer to data that isn't necessarily the message root. This could be addressed by having a way to convert a POD capnp pointer value to/from an arbitrary struct type given a `message::Builder`.
I'm keen to see your experiments. If you think you have similar goals to mine, perhaps we could work together. My library is somewhat independent of GTK+ : only the main loop and the `connect!` macros are dependent on GTK+. One idea could be to put these in another crate (`relm-gtk`) and having other crates like `relm-cocoa`. I also want to experiment with creating a trait to transform (GTK+) callbacks into a future or stream which could be helpful to support multiple backends.
Rust doesn't have anything like that. I would like to see it added. What I've done is create a type error because at least you can put some text in (even if it has to be camel case). macro_rules! error { ($msg:ident) =&gt; {{ struct $msg; let _: () = $msg; }} } error!(ErrorMessageGoesHere); 
Also, nobody said the speculated Android/ChromeOS merger WAS going to be Fuchsia. 
It's more about trying to find a good practice for constructing APIs using traits in similar cases. I've mentioned that it's possible to explicitly use traits like in your example, but I am interested if such approach is "fine" or "better avoid if possible".
I guess I mean for explaining explicit lifetime semantics. I've had to use them a couple times and it's largely seemed like I was just trying to please the compiler rather than grokking wtf I was engaging in.
Yeah this works but unfortunately newer compilers hide the actual error site when the macro is in another crate: error: macro undefined: 'munch_error!' --&gt; src/lib.rs:6:1 | 6 | munch!(bad argument fn c(arg: Unknown) {}); | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ | = note: this error originates in a macro outside of the current crate (see [#39413](https://github.com/rust-lang/rust/issues/39413))
&gt; scales in O(n) time (across all mutexes) to unlock Do you have any more details about this?
A VS Code extension for this would be fantastic. There's already extensions that basically do this for git history.
&gt; On Unix Mutex uses PThread_Mutex which is a finite system resource, and scales in O(n) time (across all mutexes) to unlock (and lock with contention). This isn't really relevant because it uses `parking_lot` which does not use `pthread_mutex_t`
[glibc's](https://sourceware.org/git/?p=glibc.git;a=blob;f=nptl/pthreadP.h;h=43ca44c8296f17f97baeda18cf50f2367ff19651;hb=HEAD) pthread mutex(es) is a singly linked list in user space. This holds references to a kernel [futex](https://linux.die.net/man/2/futex) which is what manages blocking threads which are waiting. Locking is `O(1)` if there is no contention. You just do an atomic compare/swap. If that fails you transverse the linked list to find the `futex` and do a `syscall` to tell the kernel to put you sleep until the futex is open. When unlocking you always do the `O(n)` transverse to call the kernel to unlock the futex, and wake up what ever is next in the queue (even if nobody is waiting). :.:.: It should be pointed out that `std::sync::Mutex` behaves RADICALLY different on Windows/Unix. Unix will be fair, and threads will blocked/parked while waiting. Under windows you manually go to sleep after ~40,000 lock attempts. But the OS's isn't BLOCKING the thread, a spin lock is. If say 60 threads are attempting to acquire `std::sync::Mutex` this could exhaust a system's CPU on Windows, while Linux avoids the issue. I'll tool up a basic example and submit a bug report. 
It's still a bit confusing, especially with both using Rust, and thus seeming like they could be the same thing. In addition, I googled "alacrity rust", and found mostly alacritty links, though your project is in the results too. With some brief searches, I didn't find any other projects with the name alacrity, in Rust or otherwise.
Moving and copying a `Copy` type cost the same. Both perform the `memcpy` operation, copying the stack-allocated data of the type to the new stackframe for that function call. This is (part of) why `Copy` is restricted to primitives and types built out of primitives - they are all only stack data, no heap data that would need to be copied.
Oh! That makes perfect sense. Thank you. :-)
&gt; Shouldn't `Atomic` alone do what you need? It is designed to reclaim the inner object when the `Guard` returned by `pin` goes out of scope in all threads. `crossbeam` does not run destructors, it just frees memory. By using an `Arc` you can make it so that destructors run. I implemented [something similar](https://gist.github.com/whataloadofwhat/96b6e89db8d3f13eb9ca) (biggest difference is it was optimistic (I'd like to say lock-free?) rather than pessimistic for writing) and rolled my own simplified reference counting for the same reason. The extra indirection involved in having a pointer to an Arc is perhaps a little pointless, but allows leveraging `Arc`'s own reference counting (as well as allowing `Weak` pointers).
I would really like to take a look at the source code. But I can't find a download link or a git(hub) mirror so far... Kind of disappointing.
I implore you to reconsider this decision. There's a reason nobody names their project Firefox or Chrome (beyond legalities), and that's because it is a huge detriment to your own project. Googling for it would become difficult which hurts your project's popularity, and it leads to confusion in both written and spoken discourse. If your own project's branding success and name recognition isn't enough justification, please consider the effect it would have on the other project.
&gt; When unlocking you always do the O(n) transverse to call the kernel to unlock the futex, and wake up what ever is next in the queue (even if nobody is waiting). I have been unable to observe this in practice, e.g. the time for unlocking a mutex when there are a million other locked mutexes is maybe 10x the time when there's only 2 other locked mutexes. I agree there's a linked list, but it doesn't seem to be doing what you say it is doing. &gt; Under windows you manually go to sleep after ~40,000 lock attempts. But the OS's isn't BLOCKING the thread, a spin lock is. Hm, I feel like this is inaccurate. Looking at [the `std` implementation](https://github.com/rust-lang/rust/blob/master/src/libstd/sys/windows/mutex.rs), Rust uses all the normal system synchronisation functions. I can't imagine an operating system that would provide "blocking" primitives that are actually busy-loops, and the documentation for the various functions seems to back this up. Additionally, it seems that CriticialSection (vs. SRWLock) is fair.
I agree, but I think it's unfortunate that so many of Rust's awkwardnesses are visible in common starter-problems. If you have in your head that a linked list is basic/easy, it's going to seem frightening/scary/"suck-y" that it's so hard to implement in Rust.
if people are benchmarking linked lists, they're going to be very disappointed in general with how modern computer architectures favor the performance of contiguous chunks of data rather than disjoint data structures. People can always find something to whine about, and we already have benchmark after benchmark showing that reasonably optimized Rust can perform as well as any other language's well-optimized implementations, oftentimes stomping the best existing implementations in any language, since those languages are too unsafe to do things fearlessly in. I would prefer them complain about performance, which is easily dismissed, rather than ergonomics.
Have you considered using LINT, well "lint," to create some C-like commands that would help you out?
Ah so id just pass the struct to the trait, not declare it 
I use `nginx` currently as a reverse proxy, but it's kind of annoying since it rewrites headers, so I have to throw in specific instructions to preserve certain headers, like the WebSocket upgrade header. I *do* like the TLS handling though. I'd definitely be into using something built for the purpose instead of `nginx`, which has the entire kitchen sink.
Maybe I missed it, but there's no explanation of the benefits of the approach? I was going to explain how `Temperature&lt;Unit&gt;` could give you all the benefits you were mentioning were missing, but then I realised such an implementation is just "Units as Types" in disguise again.
After almost 6 months of work, I am glad to announce the release of **nalgebra v0.11** and its brand new website (with user-guide and rustdoc documentaton): [http://nalgebra.org](http://nalgebra.org). This is a major rewrite of the library, bringing it closer to what I would like it to be when it will reach v1.0. This is the last modification of **nalgebra** of this magnitude. The main modifications are: * Matrices parametrized by type-level integers. This allows statically-allocated vectors and rectangular matrices of arbitrary shapes. Also, if at some point in the future Rust supports integer literals as type parameter, nalgebra will be easily modifiable to use them instead of type-level integers. * More types for transformations, including affine transformations (for non-uniform scaling + rotation + translation) and unit complex numbers (for 2D rotation). * The ability to [easily](http://nalgebra.org/cg_recipes/#transformations-using-matrix4) create 4x4 matrices for 3D translations, rotations, scalings, and projections. This simplifies the use of nalgebra in the context of computer graphics. See the [changelog](http://nalgebra.org/changelog/#version-0110) and the brand new [users guide](http://nalgebra.org) for more details. While the interface might sometimes look like Eigen's (especially the `Dynamic` type parameter that tells that a matrix is dynamically-sized), we do not have expression templates (and using them is not a priority). Also, blas/lapack/matrixmultiply integration is not done yet but is planed (contributions are welcome !) If you encounter any difficulty to update existing code, if some features seem to have disappeared after this rewrite, or to understand compilation error messages (which we tried to keep clear despite the use of type-level integers), do not hesitate to open an issue, or to ask on the [forum](http://users.nphysics.org/c/nalgebra) ! Thanks to the developers of [typenum](https://crates.io/crates/typenum) and [generic-array](https://crates.io/crates/generic-array) as this work strongly depend on theirs !
What confuses me most is that people think they ought to be writing highly optimised basic data structures without using unsafe. My understanding has always been that this is precisely what "unsafe" is for. You can then expose a safe wrapper and continue to use your performance data structure throughout the rest of your code.
An important thing to understand about the borrow checker, is that it does not affect code generation - it is purely an analysis which either succeeds or fails, and so is independent from the logic that determines what will be captured by a closure. In the first example, `basket` is used within the closure, and so it is captured for the entire lifetime of the closure. Now when the borrow checker runs, it sees two overlapping borrows, one of `basket.apple` and one of `basket`. This causes borrow checking to fail. In the second example, only `b` is captured by the closure, which is tied to the lifetime of `basket.banana`. The borrow checker is smart enough to know that `basket.banana` and `basket.apple` can be independently borrowed, which is why the second example succeeds. This happens because the compiler has a simple set of rules for determining what is captured by a closure, and those rules do not allow for capturing "parts" of a struct (like `basket.banana`), only outer variables referenced by name can be captured.
This is cool! I would suggest using fma in a couple of places in the shader, just to make sure you get the single instruction fused multiply-add (the compiler will sometimes do it for you, but might as well make sure - in particular the compiler is probably hesitant to rearrange expressions for you, at least the d3d one is, so unless it looks exactly like a*b+c it won't be able to turn it into a fma). Also - some GPUs (mainly older ones, I think) have very fast swizzling operations. So your manual swapping thing can be done like this: yMinMax.xy = yMinMax.x &gt; yMinMax.y ? yMinMax.yx : yMinMax.xy; Which may be faster on some architectures, but probably not worse on any. The VS also some math using values that seem to be constant and could maybe be partially precomputed. At the very least you could precompute the reciprocal of GLYPH_DESCRIPTOR_UNITS_PER_EM to turn that divide into a mul. You could maybe also divide the IMAGE_DESCRIPTOR_POINT_SIZE by this as a preprocess if these two things are predictable and don't change (didn't look where these come from). Then the atlasPos becomes a single fma with two constants. I haven't tried any of these because I had building problems with cmake. 
Are you referring to the name? That resulted out of my personal general linux experience, here is a short rundown (I tried many more things, but those are the most important steps). I was searching for a window manager I like quite a long time and their name actually represent themselves quite well. Many window managers use a short abbreviation of their defining feature for their name and then you get something like "bspwm", "wtftw", "dwm" or "tinywm". Some are actually somewhat creative but have a mostly meaningless name in terms of their functionality, like "awesome" or "i3". One of the first names I actually really liked was "herbstluftwm". Being german I understood the name immediately and it perfectly describes a comfy but somewhat refreshing feeling. It actually tries to make a statement on it's user experience. What I disliked about most of these window manager is the technical feel of many of my personal favorites. i3 is an awesome window manager, but I was always annoyed I had to setup key bindings for sound/brightness adjustment and a lot of other things. i3 follows the UNIX philosophy perfectly: "Do only one thing, but do it right". But it is not a nice user experience, until you have finally combined everything. And it is always a struggle to keep everything up-to-date and tied together by a lot of scripts, that break with new versions, etc. I started my linux experience with arch linux, so I am used to these kind of things, but I missed the experience of having something just work. So I recently tried out fedora, after trying for a long time to find a complete desktop environment that allowed tiling, the way I was used to by i3. Gnome is a nice environment, but I have no found a single plugin, that satisfied my needs. I quickly ended up using sway (https://github.com/SirCmpwn/sway) with my old i3 config again, losing many benefits of my new distribution. To wrap it up. Fireplace aims to be something that just works (meaning as a sensible default config with a lot of things you expect, like correctly handling volume controls), but to be highly customizable and more importantly hackable/extendable. Most of my frustration resulted out of the fact, that it was neither easy to write a gnome plugin nor to fork i3 and hack my wanted features into it. So yes. I am aiming for the flair - for the user experience I try to build - with the name. Something that makes you feel comfy and welcome.
Would a struct of u32's or whatever primitive automatically have the Copy trait? Why wouldn't it if not? It seems like some things should be by nature Copy-able.
That's a really nice description of how you came up the the name. And it's a great name. However, if you look next to /u/llogiq's name on his comment, you'll see a list of his Rust projects. That's what he was asking you about, if you'd like "fireplace" to show up there by your name.
Wonderful story! Very neat to hear why a certain name was chosen, especially when it seems out of place. I think /u/llogiq was refering to a reddit user flair to show people you work on the project when you post though.
It sounds like you might want [clone_from](https://doc.rust-lang.org/std/clone/trait.Clone.html#method.clone_from), which is already part of Clone?
Thank you.
Yes, that's exactly what I wanted! I guess as a follow up to that, if I derive clone for my struct, (and I assume vector already overrides default clone_from) would it also derive vector's clone_from implementation for cloning the vector part of the struct?
Unfortunately that doesn't seem to be the case today. `clone_from` has a super simple and dumb implementation and it's not built from structural recursion over the pieces. https://github.com/rust-lang/rust/pull/27939 was trying to do that but compile time impact was way too high to justify. With MIR and better recompile times, this might be worth revisiting.
Hmm, in relation to my other comment as well, it seems that deriving `Clone` doesn't derive alternative `clone_from` implementations (such as `Vec`'s). This seems like a big flaw that could be fixed in the `#[derive(Clone)]` implementation by deconstructing self, running clone_from() on each deconstructed element in the struct, and then reconstructing the self from that. I think I'm going to open an issue for it in the rust repo. EDIT: Oh hey, there already an issue [here](https://github.com/rust-lang/rust/issues/13281)
[ripgrep](https://github.com/burntsushi/ripgrep) is one I use daily.
Because you may wish, eg for efficiency, to manipulate memory in a data structure in such a way that is difficult/impossible to prove to the borrow checker as being safe, and thus require using raw pointers. Raw pointer access/manipulation is unsafe. Why would syscalls have to be unsafe?
awesome, thanks. That's intuitive.
Often, structs of arrays do have the advantage of being more memory efficient (due to lack of padding).
Structs automatically becoming `Copy` when all of their members were `Copy` existed in an old, pre-1.0 Rust version but they decided that, overall, manually opting into `Copy` produced better results.
Since I'll probably get something wrong when I go into detail, I'll try to keep it simple: You're creating a heap allocation with `re.replace(input, "1")`in line 5 and associate it with the local variable `rest` by assigning it. That means once `rest`goes out of scope, the heap allocated memory also will go out of scope and hence be droped (that's the whole principle behind the lifetimes). To avoid droping it after the scope of `rest`you do in turn need to associate the lifetime of `rest`to the lifetime of the outer scope (`'t` in this case), so it will only get droped after whatever `'t`is when the function is called is being droped. In your case the scope in which the function is called in line 15 is the scope of the `result`variable, which ends in the last line. I guess there is a reason to declare the function inside the function, in this snipped it's pretty useless and would not require lifetimes if you were to inline the code by yourself (if you need it several times within that function, it's probably the right way to go).
This is related to one of the things that really bugs me about opinions on Haskell too! People want safe code but they don't want to write safe code. 
I second using libweston since it has less XWayland bugs and there's also libweston-desktop for writing compositors with less code.
Looks like the `env!` macro can be abused: [playground](https://play.rust-lang.org/?gist=8c591288a7735fe81026550d5c672da3&amp;version=stable&amp;backtrace=0) :D
In the screenshot it says `1920x1027`. Is that a bug in the system info tool?
Once I can replace XMonad with a tiling wayland compositor that uses libweston(-desktop) or otherwise has only minimal XWayland integration bugs and Alacritty works natively as a Wayland client, I might finally start using Wayland day to day. Until I need a Java GUI application that is.
Thanks! I was just looking at `Entry` itself; didn't think to check out `VacantEntry` and `OccupiedEntry`. Now I have this: /// Adds non-zero `delta` to the day represented by `day` in the map `m`. /// Inserts a map entry if absent; removes the entry if it has 0 entries on exit. fn adjust_day(day: CameraDayKey, delta: CameraDayValue, m: &amp;mut BTreeMap&lt;CameraDayKey, CameraDayValue&gt;) { use ::std::collections::btree_map::Entry; match m.entry(day) { Entry::Vacant(e) =&gt; { e.insert(delta); }, Entry::Occupied(mut e) =&gt; { let remove = { let v = e.get_mut(); v.recordings += delta.recordings; v.duration += delta.duration; v.recordings == 0 }; if remove { e.remove_entry(); } }, } } It'd be nicer with non-lexical lifetimes, but I think it's more readable than before, and presumably a little more efficient also.
Stuff like this is one of the reasons I'm so strongly in favor of [enum variants becoming types](https://github.com/rust-lang/rfcs/pull/1450). Currently, an API designer is forced to choose between three options: 1. Represent their state machine as an enum, and rely on dynamic enforcement of state transitions, with its associated runtime cost. 2. Represent their state machine as a collection of types, and take advantage of statically-enforced transitions, with the cost of increased boilerplate being pushed onto the consumer. Possibly implement a trait to permit dynamic dispatch, but that brings in costs associated with traits being "open", necessitating hacks like private trait prerequisites. Also requires heap allocation for dynamic dispatch. 3. Combine (1) and (2), wrapping the types in the variants of an enum. Type each name twice, and cringe every time you want to pattern-match to extract a field. With enum variants being types in their own right, the story changes drastically. Imagine, defining your enum for the states of your state machine/session type, and then having API consumers be able to choose - for themselves - whether they want a dynamically or statically enforced state machine, with the only boilerplate being a single dispatcher-match function you write? Heck, they could even start off static for some number of stages, and then wrap it up and hand it off to a more general reactor without boxing, or break out to a static fast-path for certain sequences without any need for trait downcasting.
I guess yes if you consider "memory safety without garbage collection" as the choice, but these comments seem to come from the perspective that they feel we should have been able to give them memory safety *and* zero overhead cyclic ownership, which just isn't possible.
Thanks for the kind words! Which part of the docs was it that the examples clarified? Since I know the internal workings, it's hard to know what "tribal knowledge" I'm taking for granted when writing documentation. Because of this, outside comments are always greatly appreciated!
I would be inclined to make `Temperature` a trait implemented by `Fahrenheit` and `Celsius` types. Being a temperature seems like a behavior shared by multiple types (units of measure).
Clever!
How do you go about compiling this? I get this: rustc lib.rs error[E0463]: can't find crate for `itertools` --&gt; lib.rs:1:14 | 1 | #[macro_use] extern crate itertools; | ^^^^^^^^^^^^^^^^^^^^^^^ can't find crate error: aborting due to previous error
For the same reason that calls to C or any other external system are unsafe, which is that syscalls are basically black boxes as far as the rust compiler is concerned.
Perhaps you may want to read the rest.
It's up to you whether you want to use a for loop or chain iterator methods. In my biased opinion, iterator method chaining shows intent better than plain for loops. For struct and trait design, you can sort of treat the combination of them together like a PHP class. You really don't need abstract classes - just use a trait with default implementations. If you need shared fields from abstract classes, just make getters/setters in the trait. In the future, we might have fields in traits to make it more ergonomic. It also helps to see what issues you are having specifically. General advice is, well, general.
Alacritty doesn't work as a native Wayland client? It uses `winit` right? I thought `winit` supported Wayland. 
It is more struct + impl struct = classes. A trait is more similar to an interface ... where you implement one at a time. It defines a contract so you can generalize your code whatever the struct. I hope this makes sense.
Nice! Will there be a single value decomposition in near future?
&gt; performance, which is easily dismissed Not as easily as it might seem. Rust's benchmarksgame entries are still lackluster and I've had numerous discussions where folks questioned Rust's overall performance based on them.
I used it because of the first comment :) I already learned about structs, enums, generics, impls &amp; traits but still don't have much clear idea about code reusing patterns in Rust.
I guess this weird trying "unreal" things might be caused by schooling system. Many people learn (or "learn") in schools and the way current schools teach them is: "Here you have an artificial problem, find a solution!" I didn't learn most stuff this way. I learned it by solving my own real-world problems. That's also why I encountered memory errors and wished to catch them compile-time. I did know about Rust quite long before I tried anything with it simply because I didn't need it. I never-ever tried writing linked lists etc in Rust and have no desire to do so. Why would I write what's already out there?
Maybe some good abstractions would help? Just like `RefCell` is safe abstraction for underlying `UnsafeCell`.
Code reuse in Rust centers around abstractions in the form of traits specified by std and popular crates. For instance, hashable types implement Hash and can then be used in hash tables, while serializable types implement Serialize and Deserialize from serde, a crate which many would consider as standard as std itself, which allows them to be serialized into all of the popular and always growing number of serde compatible formats, most notably JSON via serde_json. To create an abstraction, make a trait. To use an abstraction, implement a trait. All of the generics in Rust are explicit in their trait requirements, and you can go to the page on Hash in std to see all the places it is required in std to see what benefits you get for implementing it. I am not too familiar with php, so I don't know how helpful this was, but I hope it helps!
That's a good idea for Rust. I need to edit my blog post to make it clearer that I'm not writing a Rust specific story though. This post is based how this would work in the most basic of type systems. 
`Rc` is the safe abstraction over multiple ownership.
Thanks for noticing that. I wrote this "announcement" hastily and forget to include the link, what a fail :)
Not exactly what you want, but maybe still interesting: https://scribbles.pascalhertleif.de/elegant-apis-in-rust.html
It really helped. I'll check above implementations you mentioned. Thanks
Like this feature as in ES6. Will we get this in 1.16 or 1.17?
I would say the biggest difference is the way both want to achieve their goals. way-cooler seems to be very focused on the lua/script-language configuration approach, while my goal is to make fireplace usable at first until I explore different configuration formats. And all of them would need to be as easy to use and declarative, so I am not sure, if I will ever make that transition or stick with yaml. Also they already have an IPC interface. They are prototyping and building the whole thing at once, which itself is perfectly fine, but that means it has a few rough edges and a lot of not completely finished features. I try to build a small solid but very modular foundation, that I am slowly extending. Also I try to be as backwards compatible as possible, although some major changes are still happening at this stage of development. I am also a bit more focused on graphical features, like window decorations or my status bar. My current goal is to provide something already every day usable. I am using fireplace productively both at home and at work currently and so far I am already quite happy with the result. My last impression of way-cooler was, that this is not really possible or at least quite an unpolished experience, but that is personal bias, so please add your two cents, if you try both.
Looking at the git mirror Flutter is used for sysui. they "ultimately adopted Dart for the framework and widgets. The underlying graphics framework and the Dart virtual machine are implemented in C/C++" https://flutter.io/faq/#what-language-is-flutter-written-in There is the Xi Editor, Rust is the backend, with a front-end in Flutter for Fuchsia. https://github.com/fuchsia-mirror/xi
&gt; My goal here is not to suggest that one approach is better than the other. IIRC, at one point the Rust developers considered adding regions to the language, alongside the current affine type system. So they're definitely not "better than the other", they're barely even competing. &gt; It seems to me that Cyclone’s approach offers more flexibility in terms of the structures it can represent but, at the same time, is perhaps more “relaxed” about deallocation of memory (i.e. because you must wait for an entire region to be deallocated). Rust is quite strict, on the otherhand, but as a result can be more aggressive in deallocating memory. You can use both smart pointers (which build upon the affine type system) and regions, they're complementary not competing. Witness the old Gc pointer, or the current Rc/Arc ones. In fact, smart pointers are probably the wrong thing to focus on, they are indeed more or less identical to the corresponding C++ smart pointers (Box ~ unique_ptr, Arc ~ shared_ptr)[0], the divergences are that Rust's ownership concepts are enforced by affine types, and supported by the "borrow checker" which enforces the *lifetime of references*, that is that a reference to an object `a` does not outlive the object (with limitations, the borrow checker currently only handles *lexical* lifetimes). [0] to an approximation since the languages don't provide the exact same defaults, tooling or protocols e.g. Rust doesn't have to deal with copy or assignment copy because it has neither
~~once again for the second time,~~ ~~i have a c function in ruby that takes two strings as arguments, how do i do that in rust i can't find anywhere that says how to give a c function a string only how to get a string? this is example use of the function in c, it returns a double.~~ ~~get_tmp("TC0P", "CELSIUS")~~ got it working with https://doc.rust-lang.org/std/ffi/struct.CString.html get_tmp(x: *const c_char, y: *const c_char)-&gt; f64; and then this in the actual rust code let tempKey = CString::new("TC0P").unwrap(); let unit = CString::new("CELSIUS").unwrap(); let x = unsafe { get_tmp(tempKey.as_ptr(), unit.as_ptr()) };
That quote specifically talks about "[when] looping for a *side effect*". Map is a pillar of FP which is all about avoiding side effects, but Rust allows whatever mutation you so choose inside the map block, so using the imperative style for loop makes the reader more aware that mutation might occur. Map is used plenty in Rust when side effects are not necessary.
&gt; Try to write a doubly linked list without getting a segfault at least once. I’ll wait. Try to write about Rust without sounding like a complete touchebag. I'll wait. 
It will be in 1.17 unless someone backports it.
The biggest problem I have when teaching Rust is that a lot of features reall y shine in huge systems. Immutable borrowing of something into a function? Meh, boring, I can see at a glace it's not mutated. Immutable borrowing of something into 100000 line system? Not so much. Proper syncing and locking of a value that I know is going to be used concurrently? Easy, just create that mutex around it. Know which values see potential concurrent use when parallelizing a huge single-threaded program? Now you have problems.
Unless you want to create graph with circle. Also, there's a some cost to it (but it's not that bad).
It's just sugar, and releases happen often enough. There's no reason to backport it to beta.
&gt; One of the main issue I faced is I couldn't figure out how the code should be organized. In my opinion, the best way to organize code is module &gt; functions. Typical PHP code are organized by namespace &gt; classes &gt; functions (class methods). In Rust, you can move the [namespace &gt; classes] part of your PHP codebase into modules. &gt; This is about how to move to Rust's way from OOP. I find it best to learn Rust from scratch without the mental baggage of Java-like OOP. Example, in PHP, public class properties are considered bad since it breaks SOLID. But in rust its perfectly fine to have public struct properties since variables are immutable by default anyways and you also have the "mut" keyword. &gt; So is there any blog posts or any recommended articles about how to move to structs &amp; traits oriented design from OOD ? Instead of learning how to map traditional OOP into Rust, I think its better to learn and master the Rust type system. Its so so much better than classes. The Rust book is a good resource from start to finish to get you going with these concepts. 
Great idea/RFC. I hope your RFC gets approved and incorporated on Rust!
I don't know anything about Wayland (haven't made the switch yet), but am I right in guessing (after a quick look at the source code) that Fireplace *could* be used in a way similar to XMonad, ie as “building blocks” for programming a custom WM (which is more or less what XMonad is, with a few added bells and whistles, like automatic rebuild) rather than a standalone app with a declarative configuration? That would provide the first (IMHO) viable XMonad replacement for Wayland, and thus be extremely cool.
The [XY](http://nalgebra.org/rustdoc/nalgebra/core/coordinates/struct.XY.html) trick is pretty amazing actually. I also created an n-dimensional library and I didn't think it would be possible to get `v.x` field access. I switched away from the n-dimensonal to a low n-dimensional (Vec1 - Vec4) library at compile time. I generate everything with macros 1.1. I had some troubles with the n-dimensional stuff. For example the debug code vs the hand unrolled code with macros 1.1 was ~100 times slower. I don't expect debug code to be fast but this would most likely mean that I would always have to compile in release mode if I want to create a game. But I made heavy use of iterators and I could probably have implemented it much better. I'll do some benchmarks later today to compare nalgebra with my math library. Another thing that I had troubles with was the compile errors. For example: extern crate nalgebra; use nalgebra::*; fn test(v: Vector2&lt;f32&gt;){ println!("{:?}", v.y); } fn main() { //test(Vector2::new(1.0, 2.0)); test(Vector3::new(1.0, 2.0, 3.0)); } results in: error[E0308]: mismatched types --&gt; src/main.rs:10:10 | 10 | test(Vector3::new(1.0, 2.0, 3.0)); | ^^^^^^^^^^^^^^^^^^^^^^^^^^^ expected struct `nalgebra::U2`, found struct `nalgebra::U3` | = note: expected type `nalgebra::Matrix&lt;f32, nalgebra::U2, nalgebra::U1, nalgebra::MatrixArray&lt;f32, nalgebra::U2, nalgebra::U1&gt;&gt;` found type `nalgebra::Matrix&lt;{float}, nalgebra::U3, nalgebra::U1, nalgebra::MatrixArray&lt;{float}, nalgebra::U3, nalgebra::U1&gt;&gt;` = help: here are some functions which might fulfill your needs: - .norm() - .norm_squared() - .normalize_mut() error: aborting due to previous error I am not sure how this could be improved. Maybe wrap it in a new type and deref to the `VectorN`? But this wouldn't solve the errors for the exposed methods like `v1.dot(v2)`. ~~Maybe it would be beneficial to separate matrices from vectors all together? So that the error message could become~~ EDIT: I just saw that the compile error is actually pretty "good" here. expected struct `nalgebra::U2`, found struct `nalgebra::U3`. I also would like to have a real n-dimensional library as I could imagine myself to use high dim matrices for machine learning for example. I also do vector swizzling for Vec2 - Vec4 with macros1.1 would that be something that you would also want? That means a lot of additional methods. For example if I remember correctly that would be 336`methods for a `Vec4`. v.xyzw(); v.zyzw(); ... I also have different perspective matrices for example a matrix for DX, OpenGL and Vulkan. The z values for Dx are from 0 to 1, OpenGL -1 to 1 and in Vulkan from 1 to 0 for the nice reverse z trick to get better precision. I could imagine contributing some features to nalgebra but I haven't made I my mind yet if that would be the best thing for me to do. I definitely will have a closer look at it. Great work by the way. 
Plus it makes it easier to SIMD-ify your code.
This looks very good and I hope to test it in the coming months. One question, is there any possibility of having a `no_std` feature set? I understand that would be a massive undertaking, but I would love to use a library like nalgebra for embedded work where the differences between points, vectors and matrices are important and can't be capture by a raw `[f32; X]` with manual support for addition and so on.
It would lock off the running fireplace instance of that specific user. If you have something like GDM which provides its own lock screen, you should still be able to use that and just disable fireplace's internal lock screen. I have no tried to use it, because I am using my laptop just myself, the lock screen I want to provide is more meant as a replacement of something like `xsecurelock`. In the worst case you have both active and need to unlock both in succession.
Thanks for the clarification!
I would recommend looking into [Cursive](https://github.com/gyscos/Cursive), which supports [pancurses](https://github.com/ihalila/pancurses) through its [alternate backends feature](https://github.com/gyscos/Cursive/wiki/Backends). `pancurses`, as the name would imply, works on both Unix-likes and Windows.
There is [pancurses](https://github.com/ihalila/pancurses), which abstracts over ncurses-rs and pdcurses-sys. It'll use pdcurses-sys for Windows, and ncurses-rs for everything else.
This subreddit is about the rust programming language, not the game.
You may have noticed that your comment wasn't well received, and that's not because of spelling. Basically here on /r/rust, we strive to be show our best selves to each other, so you either don't identify as a Rustacean or you're projecting more than just a bit. In any event, I offer my apology if you found the tone of my blog patronizing. The goal of that little 'exercise' was to show that even seemingly trivial things aren't, though your reaction betrays that you haven't attempted it.
Humans are often silly.
I think it's cool that we're starting to see a bunch of these proc-macro crates now that custom derive has stabilized. Strong support, in the language and the community, for compile-time code generation is a big way of making a strict, compiled language like Rust more user friendly and productive, so I'm excited to see what other useful crates people build.
It did, so I checked it out. I've never used this site, but, the user who posted the job has apparently everything paid out $32k and has good feedback. I'm guessing this isn't the case.
To be fair "doesn't pass borrow checker" and "unsafe" are different sets of things. C++ code isn't inherently unsafe, it just has safety properties that are inherently unproven.
As someone who has paid for contract development before, it makes perfect sense to me. It's ultimately on the client to maintain that application once it's been written, and it's their intellectual property that is being developed. It's the standard thing to do, at least in my part of the industry anyways.
Ooh, this looks really nice! I'm glad you mention this, I like seeing what you've come up with after working on an implementation for a while. Given that there are 10 dev-# branches, are you doing 10 API experiments simultaneously? Do you think your dev-9 approach will come out on top?
&gt; because their totally unsafe linked list is faster than your safe refcounted one... ... and buggy. (You forgot to implement the copy constructor, Joe)
&gt; each a simple `cargo add` away Only if you’ve done `cargo install cargo-edit`.
&gt; What confuses me most is that people think they ought to be writing highly optimised basic data structures without using unsafe. Let's face it, we would all love to be able to :) And Rust is marketed as being *fast* and *safe*, so it's not exactly unreasonable.
The lack of integer literals as type parameter really hurts use cases like linear algebra libraries =(
Depends. If the client has some domain knowledge of the language or an in-house guy who can maintain afterwards (but is currently too busy to build), then it makes sense. When I was contracting back in 2009, a client wanted us to write a big Etsy-style app in Ruby on Rails. We didn't know RoR. We told them it would take 3x as long because we'd have to learn the language and the framework as we build. We asked them why they wanted it in that language and they told us because they heard lots of people are building apps in it and it sounds cool. We ended up firing them before even starting because they had no idea what they were doing (for more reasons than just dictating the language they knew nothing about).
traits in PHP are more like mixins in other languages, it's not a good comparison to Rust's traits. The closest thing OOP has to traits is an interface, one that can have default implementations. &gt; It looks like Rust is not purely OO Rust isn't OO at all, IMO. You solve problems through composition. Rust's traits are closer to Haskell's typeclasses than they are to anything in OO.
As the poster and the client I can say that @jdrlicky is 100% correct, we're thinking ahead and it helps boost the community, not first time around the block either https://github.com/panoptix-za/influxdb-rs (look at contributor list) and (https://www.reddit.com/r/rust/comments/4xcacg/announcing_a_much_improved_nickel_jwt_middleware/)
Not a homework assignment, just would rather not have to resort to electron/js or c++ (but we will if we have to)
It was iterative development and each older dev-# branch mostly represents a dead end or a point where I wanted to follow a slightly different idea. dev-9 is pretty much what I'm going with. I have a more polished version in master that I have been working on (slowly) for the past couple week and haven't pushed yet. Another advantage of using quantities instead of units is that you don't really need to implement or think about the resulting unit of a calculation. You can even have intermediate steps where the quantity/units aren't explicitly defined at all. The calculation just needs to end with a quantity with defined units before you can send anything back to the user.
I have two little issues with the current design: - `build` returns a `Result`, which is annoying - `build` clones First things first: `Option` and `Result`. This is really annoying. One of the common use of builders is to get a "default" object (`Channel` implements `Default` here) and then tweak one or two properties and rely on the "default" value for all others. If I have to specify all properties, I might as well use `Channel { ... }` to have a compile-time guarantee that I didn't forget any. Secondly, why cloning? Rust favors efficiency. It is best to provide efficient by default, and then options on top. Let `build` consume the builder (`fn build(self) -&gt; T`), and have it derive `Clone`. People who use a throw-away builder will not suffer from inefficiency, and those who want to clone it can. Best of both worlds! *Bonus: by moving by default and using `#[derive(Clone)]` you also make your builder compatible with non-clonable types.*
There's no other way to develop an intuition about how to structure code in a new language than to actually write code. I don't believe it's possible for you to 'learn and understand' a new concept without actually doing it. The understanding you think you develop in those situations is transient. You'll think you 'get it', then start to write something and feel completely lost. Don't waste your time reading a bunch of articles before you write some code, just dive in and start writing. While I'm on the topic, I think rocket.rs is probably not the best way to start learning. They use a hell of a lot of codegen that let's you write amazing things, but I don't feel it translates well to other applications because of that. Iron is probably better as a learning tool, it holds your had less. Good luck.
Well, nphysics will be updated to use nalgebra 0.11 very soon. There has not been much activity on nphysics for a while because I was busy with nalgebra and its website. Very unfortunately I can only work on those projets during my free time so it does not go as fast as I would like… nphysics still needs rearchitecting, and it is probably the next big thing I will do now that I am more satisfied with the designs of both ncollide and nalgebra.
I believe the memory layout matches. We use column-major contiguous data storages for owned matrices and matrix slices have compatible strides. All that is left to do is to actually bind to BLAS/LAPACK (behind a feature).
Awesome! Thanks for all your work! Your contributions to the Rust ecosystem have always been super exciting and I'm eager to see what comes next. I'd love to contribute to nphysics myself, but I've been hesitant due to the pending refactor. I'm looking forward to an opportunity to help; having a high-quality native Rust physics engine is highly motivating, and may really help put Rust on the map for certain markets, given how slow-moving and crufty Bullet is these days.
https://doc.rust-lang.org/book/trait-objects.html
The way I see it, you lose something by storing data this way. The user no longer has knowledge or control of what units they're actually using. So, what do you gain? To compare, I am the author of dimensioned, which provides units as sets of systems, of which the user can choose or create their own. It's undergoing a rewrite (that is mostly done), which can be seen on the `rmdim` branch here: https://github.com/paholg/dimensioned/tree/rmdim. In addition to defining unit systems, I define constants for other units (e.g. in `si` there's a `FT` constant) and implement `std::convert::From` to go between systems. For example, see here: https://github.com/paholg/dimensioned/blob/rmdim/examples/conversion.rs Note that everything above `main` would ideally be library code, but I wanted to make a self-contained example showing everything, if one wishes to make their own unit system(s).
Who cares if it works as long as it is fast?
I've done s.th. similar here: https://github.com/willi-kappler/simple_units It's not ready yet, but I hope to publish it soon. The code uses Rust macros to help define new units and combination / conversion between them. It is still hacky and doesn't scale. An alternative solution (that works with stable Rust) is to put the units in comments and use *build.rs* to process and check them.
Yeah, everyone should do that (and I'll update my post once I get to my PC). Someone should RFC cargo-edit &amp; put it into cargo proper.
Thank you very much, I understand now how it works and how I'll have to go about it. Yes, the function is useless in this snippet but will be called multiple times later on.
Arguably `alpha` is just as influential (in the other direction).
&gt;I am afraid the 336 methods could end up enlarging the library size too much… I was more thinking about adding a method like v.swizzle::&lt;Z, Y, Z, W&gt;() (where Z, Y and W are aliases for type-level integers indicating the requested component index) that would be equivalent to your v.zyzw() Yes that is also a concern of mine, though I think the optimizer might throw the unused ones away. I have actually implemented this in D a while ago https://github.com/BreezeEngine/breeze/blob/master/source/breeze/math/vector.d#L74 This was a pretty elegant solution. I think the `v.swizzle::&lt;Z, Y, Z, W&gt;()` approach wouldn't be too bad but then again would it be much nicer than `Vector4::new(v.z, v.y, v.z, v.w);`? Also you can swizzle down a dimension like `v.xyz();` I think swizzling really is more of a gimmick, I just implemented it because I could. The rust code is open source but it is not really polished yet. https://github.com/MaikKlein/breeze/blob/master/src/util/derive/math/vec_derive/src/lib.rs#L414 I unrolled everything but I don't think that there would be a performance difference with --release compared to nalgebra. 
Could you elaborate on the difference between impl Trait and what you can already do?
You create things based on a unit of measure, but afterwards don't have to worry about the unit because only the physical quantity matters. If they want to read a quantity in a certain unit they can extract a plain numeric value again. I think there's merit in that abstraction. I do agree that it might be interesting to speak in terms of a system of units when you for example do normalisation, as you can see in uom, which is mentioned elsewhere in the comments here. Do you have a tutorial to go with the example you linked to? I got lost in the types and the macro calls look nice and simple but I don't know what they get expanded to, so I don't know what they really mean. The code in main looks looks usable, and it looks like you do implicit conversions of units for different physical quantities, which is kind of what I'm arguing for in my post. 
edit: found this "A function can (today) take an argument by value or reference by choosing between generics and trait objects. This leads to either static or dynamic dispatch, the former at the cost of monomorphising the function. However, this doesn't work for return types, since the callee, not the caller decides on the concrete type. Therefore, the function must return a concrete type or a trait object, there is no generic, by value option." which now is clear to me after more reading. http://www.ncameron.org/blog/abstract-return-types-aka-%60impl-trait%60/ It appears Rust already has a pretty good implementation of "abstract classes". the hubub around impl trait made me wrongly think there was no analog for abstract classes in Rust yet. But there is, with trait objects. At the cost of having to pass pointers under the hood.
`impl Trait` does not require a `Box`. This means that you don't need an extra allocation. It also means that it can be statically dispatched, rather than dynamically so. However, it doesn't work in some cases; you can't say "if this, then return this thing, otherwise, return that thing"; you must have a single return type. If you need this kind of thing, you'd use the trait object.
Why can't you do the second part? 
To give a guess, because with impl trait the object is being returned on the stack, and you need to know how big the object you are returning is *before* you call the function, not after it. e.g. you're not allowed to determine the type at runtime since the compiler needs to know how big the object is at compile time, in order to place it on the stack. It seems that you should be able to just allocate enough space on the stack for the largest returnable object. But there are probably some bad tradeoffs with this. edit: perhaps you could return an enumeration? Wouldn't that allow a function to return different types determined at runtime? no?
Out of interest, why would you find this particularly useful? I've mostly programmed in Haskell for some years now, which doesn't have the ability to return "abstract classes", and I can't say I've ever missed the feature.
IIRC cargo features are meant to be mutually compatible (so they can be unioned over the set requested by every dependent crate), so switching between mutually exclusive backends with them might not work too well.
Patient Zero
You can do that, you just are responsible for breaking cycles with `Weak` pointers. RefCell also has overhead over UnsafeCell. An abstraction which breaks the cycles for you would be a garbage collector.
It's hard to say what has the most influence. Consider these scenarios: calc_hidden_layers(1000000, 0, 1, 1) vs. calc_hidden_layers(0, 1, 1000000, 1) Depending on the values, different terms can dominate the result. What is it you're trying to decide? If you're trying to find a term you can drop while minimizing the impact on the result, you may want to empirically evaluate things over a range of realistic inputs and choose the term that has the least impact on average. But maybe you're thinking about something else entirely.
&gt; It seems that you should be able to just allocate enough space on the stack for the largest returnable object. But there are probably some bad tradeoffs with this. That's tricky since you could be receiving a trait object from someone else's code then returning it yourself. And there's no guarantee that the trait object has a statically known size at all. &gt; edit: perhaps you could return an enumeration? That allow a function to return different run-time determined types which implement some trait? no? This can definitely be done manually. You have trait A implemented by many different types. Your function returns X or Y, both of which implement the trait. You can write and Either&lt;X,Y&gt; wrapper which implements the trait by dispatching to X or Y, as available. Probably could write a macro to do this.
One motivation for this feature is returning chains of nested types; this happens with iterators and futures, for example. fn returns_iterator() -&gt; i32 { let v = vec![1, 2, 3]; v.iter().map(|&amp;x| x + 1) .filter(|&amp;&amp;x| x &gt; 1) .map(|&amp;x| x + 2) .filter(|&amp;&amp;x| x &gt; 2) } gives error[E0308]: mismatched types --&gt; hello-world.rs:7:9 &lt;snip&gt; | = note: expected type `i32` = note: found type `std::iter::Filter&lt;std::iter::Map&lt;std::iter::Filter&lt;std::iter::Map&lt;std::slice::Iter&lt;'_, {integer}&gt;, [closure@hello-world.rs:7:22: 7:32]&gt;, [closure@hello-world.rs:8:25: 8:36]&gt;, [closure@hello-world.rs:9:22: 9:32]&gt;, [closure@hello-world.rs:10:25: 10:36]&gt;` consider that fn returns_iterator() -&gt; Filter&lt;Map&lt;Filter&lt;Map&lt;Iter&gt;&gt;&gt;&gt; { (which might not even work due to the closures... see below) vs fn returns_iterator() -&gt; impl Iterator&lt;Item=i32&gt; { In the case of returning a closure, each closure has a unique type, so it is impossible to name the type of a closure and return it unboxed today. You can with `impl Trait`.
I think what we really need is a way to name closure types. 
Hey there! I'm also primarily a PHP dev who started getting into rust about 2 years ago. I initially had basically the same reaction as you mentioned here. I'll echo what other people have already said, which is just try to write some code in Rust and worry about best practices after you get a little more familiar with the tools the language provides. Don't try to force yourself to use every new idiom at once, pick them up as you go. My experience was, trying to make something I expected to be useful in my first few projects just made it impossible to actually get anything done because I was too worried about doing it wrong. So make your first few projects small ones you expect to throw away, and be pleasantly surprised if they do actually turn out to be useful beyond just as a learning experiment. Sending this from my phone, but I'll link a few of my first projects here later. 
"Katas" are often recommended for learning new languages, and I found this very helpful with Rust. Pick a small project you've written before, and know well, and re-write it in Rust. Project Euler and LeetCode are also good if you're a total beginner, just to get comfortable with the syntax.
Yes please! I think "xi" and "fuchsia" are best, as I'm working full time on that. Would be confusing if you listed _all_ the projects I've been playing with :)
I have a couple of projects that are pretty good for beginners! The first is [quantiles](https://github.com/postmates/quantiles). Quantiles is meant to be a repository of summarization algorithms. An implementation of [t-digest](https://github.com/tdunning/t-digest/blob/master/docs/t-digest-paper/histo.pdf) would be nifty. The other is [cernan](https://github.com/postmates/cernan), a telemetry aggregation server. Here's there's a lot of opportunity to write "sinks" – relatively self-contained protocol implementations conforming to internal traits – or cleanup projects. Examples [here](https://github.com/postmates/cernan/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22). With both I'm happy to do mentorship as needed. 
Excellent stuff, and a great example of the power of macros in rust 1.15. So...are you looking to replicate all of shapeless? :D
It's definitely one of those things you need to fail at and keep failing till you get it. I've been using it for close to two years now and rarely run into that issue. Just keep at it!
I don't go quite that far, but I do sometimes find myself banging my head against the wall because I've been using my early Rust projects as an exercise in how far I can push my knowledge and I refuse to give up on using something like `Cow` until I've gotten it to work at least once. (After which, I do a second pass to clean up the code, which may end up stripping it out again.)
Done.
Using their tooling sounds like a great way to ensure compatibility is that an industry standard?
Scala your rust ! I love it!
So, as practice, I wanted to take a C++ program I had made for a CS class and recreate it using Rust. However, I'm running into issues handling multiple .rs files, similar to how C++ uses header files to connect parts of the program together (each class in its own header file, another file for implementing the functions, relevant classes linked into main, etc). Is the module system not capable of doing the same thing C++ does with user-made header files? Or am I going about this all wrong? 
Afaics traits are similar to php interfaces. http://php.net/manual/en/language.oop5.interfaces.php
&gt; jemalloc dropped support for valgrind Why would they do such a thing?
&gt; But everytime I have a hobby project, I typically just say "eff it, I'll just write it in C++ or java" or whatever I think will get the job done the quickest. Sounds like you don't need an idea for a project, just more self discipline. You need to accept the reality of sacrificing immediate productivity for long term gain.
Would it be possible to make your portion of the project available? Is it comparable in capability to Arduino's Wire?
You can use asan with rust, now. Might be a bit buggy. https://github.com/japaric/rust-san/blob/master/README.md
It compiles two versions of Diesel, as r2d2 depends on 0.10. (Will be fixed with the next release of r2d2.)
Will there be any student discounts? 
Damn, this is useful, but I was really hoping this was going to be https://github.com/rust-lang/rfcs/issues/1594. Having to specify every single field in large structs with many optional fields like https://rusoto.github.io/rusoto/rusoto/s3/struct.PutObjectRequest.html is *painful* atm.
Note that we kind of avoid "this years edition" as there might be other RustFests.
Looks cool, very similar to the From derivation of my [derive_more](https://github.com/JelteF/derive_more) crate. Any reason you chose hlists over tuples? It seems like tuples would fit all the use cases as well.
Multiple solutions were proposed in its [RFC](https://github.com/rust-lang/rfcs/pull/1522): * Use generic syntax `fn ret&lt;F: Fn() -&gt; i32&gt;() -&gt; F { move || 42 }` Asked [here](https://github.com/rust-lang/rfcs/pull/1522#issuecomment-229135301) by Digipom, eddyb explained why this was rejected a bit below it. * Use return type inference `fn ret() -&gt; _ { move || 42 }` I'm not sure why this was rejected. This isn't global inference, it would just use local type information to infer the return type. If not enough information is available in the function body it would be an error. ctrl-f "inference" in the issue results in people talking about global inference Specifically the `impl Trait` behaviour can be emulated with new types and return type inference: struct Newtype&lt;I: Iterator&lt;Item = i32&gt;&gt;(I); fn ret&lt;I: Iterator&lt;Item = i32&gt;&gt;(iter: I) -&gt; Newtype&lt;_&gt; { Newtype(iter.map(move |&amp;val| 42)) } A friend told me this is kind of the thing Haskellers use to avoid leaking ridiculous type signatures. It works for them because they have global type inference. But again, the type of _ can be inferred just from `ret`'s function body. * New `impl Trait` syntax The one issue I have with this is that it creates yet another way to constrain types if it is accepted in the argument type position (which sounds like a logical extension): `fn id&lt;F: Fn()&gt;(f: F) -&gt; F` vs `fn id&lt;F&gt;(f: F) -&gt; F where F: Fn()` vs `fn id(f: impl Fn()) -&gt; impl Fn()`. It also appears to solve a different problem then the problem it set out to solve as shown above. This feature can be implemented (with some boilerplate) using return type inference and newtypes.
&gt; I'm not sure why this was rejected. This isn't global inference Yes, it actually is. The types in a function body are often inferred from other function's return types, and if those return types could be inferred from this function's return type, you have global inference. You would have to restrict how much inference is done inside bodies of inferred-return functions, which would add a lot of confusion. &gt; The one issue I have with this [new `impl Trait` syntax] is that it creates yet another way to constrain types. Yes. That's exactly the point. This does something entirely different than what exists in Rust today, and hence deserves new syntax. Generics, like `fn function&lt;T&gt;`, allows the *caller* to choose *any* `T` that matches the constraints. `impl Trait` allows the *function* to choose *one* matching type, and the caller has no idea which. &gt; if it is accepted in the argument type position (which sounds like a logical extension) That's not a logical extension to me. If the function itself chooses one type and you don't know which it has chosen, then you have no way of creating values of that type. Hence this syntax doesn't make sense in argument type position. 
Yes would be good if you can try yet another time. This is really rewarding. At some point you will want to rewrite everything you've done because it magically teaches you lot of things and your program is generally much faster and more stable than what you had.
Hmm something like this? fn foo() -&gt; _ { bar() } fn bar() -&gt; _ { 42 } What I had in mind is that _only_ the body of the function determines the inferred types. The usage site of `foo` and `bar` has no influence on the inferred types. The resulting inferred type is obviously cached and no cycles are allowed. let i: u8 = foo(); The above is an error since `foo` is already defined to return w/e `bar` returns which is fixed on returning an `i32`. It is my understanding that global type inference does not have this restriction and that this more limited inference does not run into the problems global type inference has. Am I missing something? ---- I see your point about the disallowing it in the argument type position and it sounds reasonable.
That would be good! I was rather referring to the fact that I could not attend RustFest in 2016. ;)
&gt; What I had in mind is that only the body of the function determines the inferred types. The usage site of `foo` and `bar` has no influence on the inferred types. Sure, and in these simple examples it could definitely work. What I mean is instead something like fn foo() -&gt; _ { if predicate() { bar() } else { baz() } } Where the functions `bar` and `baz` themselves have inferred returns types, which in turn depend on the functions that they call. And this could recur, of course, so that some function down the chain calls a function whose return type depends on `foo`. The compiler could very well find itself in a cycle where in order to determine the type of `foo` it would need to know the type of `foo`. This makes it global inference; any function type in any module could in theory affect the type of `foo`, and the compiler would sometimes need to bail out and say "I cannot solve this graph in polynomial time". 
And in theory this enum could be auto-generated by the compiler, just like closure types are auto-generated. This would hide the dynamic dispatch, which is a downside, but would add a lot to the ergonomics of returning allocation-free abstract types.
I'm not sure what the proper answer to your question would be, but I've recently come to the delightful opinion that HLists in Rust will be equivalent to path-dependent types in Scala (aside from differences/limitations in the trait/systems), which will lead to some very interesting possibilities. http://stackoverflow.com/questions/2693067/what-is-meant-by-scalas-path-dependent-types http://danielwestheide.com/blog/2013/02/13/the-neophytes-guide-to-scala-part-13-path-dependent-types.html http://lampwww.epfl.ch/~amin/dot/fpdt.pdf
The site's gone a bit slow indeed. Here's the same content in the git repo: https://gitlab.com/vandenoever/sousa/blob/master/blog/a-simple-rust-gui-with-qml.md
You're welcome! Thanks for the kind words—they're a big encouragement to me while I'm making it. :)
I was slightly excited to give this a spin. But apparently Diesel only supports joining 2 tables per query? (My use case is normally ~4, very heavy foreign key usage). 
That's a great way to check!
Can you see if `cargo update` fixes the issue?
CHIP8 is a very approachable project, and rust is a good candidate for it. Would also recommend.
Unless you actually need to join to 4 tables in a single query for filtering purposes, doing that with multiple queries is almost always more efficient.
&gt; Unless you actually need to join to 4 tables in a single query for filtering purposes That is literally it. 
Wouldn't it make more sense to create additional nalgebra-dependent crates for that kind of things?
Servo's update is in there. I guess no one submitted a PR to include Ruma.
Hi. Are you aware of [Traefik](https://traefik.io)? It's not written in Rust, but it sounds like it would be a good fit for the two needs you describe in the blog post. If your primary goal is to implement things yourself, I apologize for butting in, but if you just need to solve the mentioned problems, Traefik would probably fit the bill.
I have a lot of stale projects that I don't see through. I have found best way for me to learn Rust is to find a project that is owned by someone else and start contributing. You will find that pretty much any open source project could use some help. Let a project idea come to you organically. It should benefit you otherwise, you will get bored and not want to get down into the weeds later. 
Singly linked lists are mostly easy. It's just anything doubly linked (lists or trees) that becomes a problem.
Most of the basic SEng principles apply in Rust, but you'll want to apply them to modules instead of classes. You'll still want to use controllers, to separate UI logic from non-UI logic. A controller may, if you can make it stateless, just be a module with a bunch of freestanding functions on it. Abstract factories can implemented by defining a Factory trait, or just taking a closure as an argument. Concrete factories, on the other hand, are just standalone functions. Modules should be single-purpose, focused, and relatively independent. Polymorphism can be done using traits and generics, and/or trait objects. However, most dispatch is actually done using pattern matching in Rust code. This does increase coupling, but it also makes the code easier to follow. This is pure trade-off territory: would you rather have highly generic code that's hard for both you and the computer to execute, or inflexible code that's fast and simple. Information expert is basically the principle used by Rustaceans to determine which function argument gets to be `self`, though the name itself is rarely used. Low coupling is enforced in Rust the same way it's enforced everywhere else. Factor it out into its own module or library. Pure fabrication is kept to a minimum. Protected variations is really easy. You can build them with Algebraic Data Types (enums), or traits, depending on whether you plan to add more variants or more operations.
Ok, this is nice and all, but what I really want for my constructors is more this: fn new() -&gt; Foo { field: value, field: value, } Maybe it's a pipe dream, but it's my pipe dream, darn it! :)
For katas, I've found https://github.com/carols10cents/rustlings to be the best
[citation needed] In what case would doing multiple queries (+network overhead) be more efficient than a single query? 
Slides are located at: https://slides.com/kevinknapp/rusty_utilities Edit: Since the video is long, I like to set the speed to 2x.
I did a Chip-8 emulator but I don't know if I would recommend it for people new to rust. It tends to be a whole lot of math on Copy types, which doesn't give you exposure to what is really unique about rust, which to me is the memory model.
Do you mean class diagrams? Well, you *could* use them by using interfaces as traits and classes as structs, but I don't see the purpose.
[exa](https://github.com/ogham/exa) is a great replacement for ls. [lolcat](https://github.com/ur0/lolcat) has a small codebase. [tealdeer](https://github.com/ur0/lolcat) is a good implementation of tldr. [xsv](https://github.com/BurntSushi/xsv) is a unique utility for exploring tsv/csv files.
This looks quite interesting indeed.
This is pretty common right? If I want the "most recent message from each member of the foo group who's also my friend" or whatever, that's probably 4 or 5 joins.
That's because you can get mentorship for all of them. It's really awesome – I've been mentored by Manish Goregaokar,Alex Crichton and Niko Matsakis so far...
I'd need just a dedicated library specifically for minification as I'm using the Rocket web framework, which doesn't really have a need for a dedicated cargo subcommand. I've been able to implement a content cache with zopfli compression to store compressed versions of my HTML pages after rendering them from Handlebar templates, but I'd like to also be able to minify all my HTML / JS / CSS content before I compress them and put them in a concurrent hashmap.
Here's a TL;DR for those who just want to know the high level details: https://gist.github.com/Gankro/1f79fbf2a9776302a9d4c8c0097cc40e Most interesting to Rust people: * shared XOR mut * implicit RefCell-ish things when static analysis is too strict (class props, globals, escaping closures) * coroutines as the primary mechanism for "returning" borrowed values (important because mutable refs can require a "cleanup" step) * shared access is still pass-by-value * move semantics are very low priority, something users should be able to learn late if ever * no lifetimes (ever?) or concurrency (yet)
And why wouldn't they? Thing is *they can* if they drop the NotInventedHere and use the work that's been done for them already.
the tealdeer link is a duplicate of the lolcat link. thanks though!
&gt; The use of macros for initializing the model seems like a hack here. The model and required FFI could be written like Diesel and Serde derive annotations. The signals and slots are a different matter are harder to simplify. I think the current macros are already pretty simple. &gt; I've always tried to just do layout in QML and keep all program logic in code That is ideal indeed, but it's nice to have the option. &gt; I don't think Qt is the UI library Rust deserves This post is about QML. QML is easy to integrate because the amount of API is quite small. Greating bindings to Qt would be difficult and indeed very much tailored to C++. QML is nice for Rust but Qt is hard to make work nicely. &gt; generate UI cross platform (ideally including Android and iOS) QML Quick Controls 2 work on mobile and desktop. This technology originates from the Nokia Meego times and is heavily optimized for low power and embedded devices. That's why it's popular in e.g. the automotive and home automation industries. 
And you also need the data from all 4 tables you joined to? I'm not saying these queries don't exist, just that they're rare. If it's *only* for filtering purposes, you can just use a subselect instead at no cost.
Yet to begin reading this epic wall of text, but if "no lifetimes ever" is really their sentiment, then AFAIR this would seem to represent critical backtracking from the future that Lattner was publicly envisioning.
Sure, if you are joining only across relationships that are one-to-one, then a single query will always be more efficient. The overwhelming majority of relationships are one-to-many or many-to-many. That's why in my generalization I said "almost always" and not "always".
I assume that by 'custom widget' you mean a QML Item that derives from QQuickPaintedItem. It's possible and would require a few classes to have bindings such as QQuickPaintedItem, QPainter, QLine, QColor, ... http://doc.qt.io/qt-5/qtqml-tutorials-extending-qml-example.html
Not saying that a join is wrong, just trying to present a workaround.
No, I did not see that. Yours is for HTML/JS while my is for GTK+ but that might be useful for me. I also chose in the beginning to receive the state and return the state in the `update` function, but I switched to a struct's field, which is I think more rusty. I'll look at that, thanks.
You're missing the point. `foo joins bar` will duplicate each foo for every bar. It doesn't matter if it's an inner join or an outer join. When I say multiple queries, I'm comparing `select * from foo inner join bar on foo.bar_id = bar.id` to `select * from foo; select * from bar where foo_id = any($1)` where the bind parameter is the ids from the first query. The number of rows returned are the same for both queries. the amount of data that is sent differs substantially, and can very easily result in more round trips at the TCP level than separate queries. I already went into much more detail https://www.reddit.com/r/rust/comments/5uhyjx/diesel_just_released_a_new_version_with_mysql/ddvd6j4/
I don't know about any examples from Rust yet. However, Haskell Typeclasses are similar in a lot of ways to Traits, and the [lens library has a pretty handy inheritance diagram](https://hackage.haskell.org/package/lens). As far as the other diagram types, like component diagrams, activity diagrams, and sequence diagrams, they would be just as applicable to Rust as anything else.
QT is not native anywhere
And nothing is native everywhere, so it's a big ask.
You are still duplicating a.id and a.timestamp, and b.category for each row in c that is returned.
I confirm the layout matches. I wrote nalgebra-lapack a while ago for SVDs, eigensystems, and solving linear equations using lapack: https://github.com/strawlab/nalgebra-lapack I haven't updated it lately, but it used to work fine.
Thanks I'll look to your links. In this example it's easy to find the error, but in a bigger codebase a panic it's much more to find if you don't have some hints
You can use the Fixpoint combinator provided by the odds crate: https://goandylok.github.io/arraydeque/doc/odds/struct.Fix.html
Minor correction: it's spelt "smaht pointer". As in "they's wicked smaht".
Can't get this to build on Windows 10 right now after installing Qt 5.8. Submitted an issue here: https://gitlab.com/vandenoever/sousa/issues/1 Any advice here?
[Termbox](crates.io/crates/termbox) is a high level binding for termbox, which is like ncurses but much simpler
The Qt approach for a long time has been "render a UI that looks as native as possible, using system primitives when feasible, while still allowing full customization and a superset of features natively available on the platform". They've done a remarkably good job overall, but the QWidgets vs. QML split seems to have made it harder for them to keep up. There are now some platforms where QWidgets still produce a better native look and feel than QML, others where QML does much better, and some where both are a little lacking. For Android specifically: QML does a pretty good job with most of the controls, and QWidgets look generally ok but require a fair amount of manual tweaking to be sized correctly. And unfortunately, neither approach gives you anything particularly close to a fully-featured Action Bar, which is a dead giveaway to the user that the app isn't truly native. (I'm mostly done coding a lookalike for a QWidgets-based app I'm currently porting to Android, which is pretty close but not perfect.) They are starting a project to use [truly native controls](http://blog.qt.io/blog/2017/02/06/native-look-feel/) while allowing you to use custom ones only when necessary, but it's still in the pretty early stages. *That's* a project I'd really like to see either wrapped or implemented in Rust. In cases where having a truly native look and feel is very important, you really have no choice but to write all the UI code in the platform's native language and framework (although you can potentially reduce the amount of rework needed for each platform by creating shared libraries of non-UI code). But Qt is a pretty good choice when you want to support multiple platforms with minimal effort and aren't too worried about being crucified for minor deviations from official platform design guidelines.
&gt;Rust sounds great. But everytime I have a hobby project, I typically just say "eff it, I'll just write it in C++ or java" or whatever I think will get the job done the quickest. Rust is often appropriate where C is - i.e. where every bit of performance matters. Don't expect it to replace C++ or Java but do expect it to be faster, and more easily maintainable long-term compared to C. I agree you might need more discipline, but it's also rewarding to find where Rust truly shines. Consider writing it in Java and Rust and them running some sort of benchmarks. 
Project Euler with Rust is only fun if you like seeing the absolute least amount of time. In my opinion it's kind of a bad way to learn Rust because other languages are often genuinely more suited to that. 
I appreciate that you're building on GTK+ instead of rolling the rendering etc yourself. GTK+ has had a lot of work put into it to make it accessible, which is an important part of any serious GUI interface.
One thing I found Rust really great are parsers and state-machines. Today I found that `xar` library is missing. I believe it'd be easy to write one.
I'm just curious would making quickcheck a first class citizen be better? They accomplish almost the same things right? 
No, they're quite different. Quickcheck inserts random input. Fuzzers insert random input, and look at the execution to try and figure out input which will crash the library.
[removed]
It actually turns out that this is the case! The libfuzzer WIP doesn't need any compiler changes.
Why not just make a macro `new!(Foo { field: value, field: value })`? (other than possibly a macro isn't allowed there, although I'm not sure if that's true or not)
Okay that makes a lot more sense! Never needed to reach for a fuzzer but if it was easily available I definitely would. 
First of all, as an author of the library, thanks for the article -- it's absolutely stunning! I was always afraid my library comes hard for people, since I am terrible in explaining things, but it seems you made it out! Do you mind if I use a link to your tutorial, it is amazingly beautiful. Speaking of QML, I absolutely adore its way of building UIs -- elegant and simple. Unfortunately, Qt is built around C++ and its paradigms, so bindings are quite a trouble. One of the biggest problems are macros and I'm looking forward to macros 2.0, that'll introduce procedural macros to ease and to simplify qobject macros, since a current solution is not the best one. I am all open for ideas and everyone's suggestions! There are plenty of problems like [multithreading access to underlying qobjects](https://github.com/White-Oak/qml-rust/issues/32), [some kind of garbage collector to simplify returning of values from slots](https://github.com/White-Oak/qml-rust/issues/33), [windows builds](https://github.com/White-Oak/qml-rust/issues/31) etc, so if anyone feels like helping -- there is still much to do :(
The current situation isn't _too_ bad. I wrote a [post](http://blog.troutwine.us/2017/01/06/american-fuzzy-loping-rust/) describing the approach that's usable today. The work described in the issue will make fuzzing significantly more straightforward, of course, which'll be great! 
maybe its just me, but i have big troubles reading your code. what you are writing seems to not just be a usual stack, as you could just have used a vec with vec.push and vec.pop for that, but the complete lack of comments, and the use of return and break for controll flow make it impossible for me to understand whats happening. i don't even understand what the fields of the Stack struct represent, except heads, which seems to store the content, but why use an option? 
I'm unaware of any interest in the Swift community in integrating with GC. ARC is love. ARC is life.
It looks like you're writing a parser for math expressions. You may want to look at some parser combinator libraries such as nom or peg which use rusty types or macros to let you build up declarative expression parsers and they handle the conversion to common types for you!
Maybe you could give a high level description of what is going on, rather than forcing us to reverse-engineer it? E.g., the name is `Stack` but it seems to be very different to just calling `push`/`pop` on a `Vec&lt;A&gt;`.
Hi, I think as the example you noted is the first one in the "Binding" part of the doc - the authors chose to make it an easy one, so reader could embrace this concept more quickly. Are bindings necessary in this scenario? No. It is more about habits and conventions. Nothing is stopping you from binding. Personally, I prefer to bound every value I use in matching - this is more consistent and easier to read (again - just for me). If someone bind or do not bind such variables - it is competely ok. Just a matter of taste, I suppose.
The only line that requires heads to be an option seems to be this one? &gt; self.heads[start as usize].take() The direct replacement to not using an option would be &gt; std::ptr::read(&amp;self.heads[start as usize]) However, that leaves this Stack in a state such that if it drops, it's going to call the destructor for that element you just made a copy of. Which can lead to undefined behaviour (think use after free, double free, etc). At a glance it seems like the best solution is to simply let the `Vec` keep track of how many elements are in it, by instead of using `.take()`, or `ptr::read`, simply using `heads.pop()`. Otherwise you need to write a custom destructor that makes sure not to drop elements that you've moved out. Alternatively, you could restrict the Stack to only use `Copy` types (which you can safely drop twice). 
And the problem you're trying to solve which requires you to implement a new linked list is...? We're worried you might be on the wrong side of http://xyproblem.info/
[removed]
It's just a simply linked list implementation of a stack but backed by a vector instead of the heap. Instead of storing pointers to tails nodes store indices into a vector.
if you often get told about the xy problem it might be helpful if you would change the way you ask for help, for example by providing all information you can in the op.
Hmm. That does sound like it could be interesting to poke at. As for the XY problem, please look at it from our perspective. We see people wasting our time day-in, day-out because of it and we're not getting paid for this. All you had to say in your initial message was something like "I'm exploring the viability of a more cache-friendly linked list representation". Saying "I'm testing a hypothesis about an algorithm" is a powerful way to rule out the vast majority of xyproblem-related responses.
&gt; This post is about QML. QML is easy to integrate because the amount of API is quite small. Greating bindings to Qt would be difficult and indeed very much tailored to C++. QML is nice for Rust but Qt is hard to make work nicely. Is QML separable from Qt? I've never used QML, but its name ("Qt Markup Language?" Qt's "What is QML" page neglects to actually define the acronym) suggests a pretty tight coupling with Qt.
Yeah. I intend to allow removing from arbitrary spots.
ok, let me ask differently, what advantage to you hope to gain by using a linked list over a simple vec?
&gt; My question is, isn't the binding unnecessary since you can just use the "matched" x as the value? Like this: let x = 1; It's unnecessary in this case. There may be other cases where it's necessary, like when `x` is non-copyable. `@` in match is a rarely used feature anyway.
I see. I think we're all confused by the description as a stack, rather than just calling it a normal linked list. In any case, one approach to completely avoiding the need for `gc` would be to make it doubly linked (i.e. I think `tails` would store something like `struct Pointers { prev: u16, next: 16 }`) and then removing could be done via `swap_remove` plus updating the `next` and `prev` indices of both the `prev`/`next` elements of the element that was removed as well as the element that was swapped into its place. This should mean that `pop` will be guaranteed O(1) (not just amortised), and eliminate some of the book-keeping. This (usually) still saves memory (e.g. the padding in an `Option` is likely to be 4 or 8 bytes), but of course not as much as keeping it singly linked.
You need it if you match an enum, provide a range for some field of it and at the same time want to know which value this field had: match msg { Message::Hello { id: id @ 3...7 } =&gt; println!("{}", id), _ =&gt; (), } [Playground Link](https://is.gd/B8MiXe)
Thanks u/notriddle 
I'll be keeping an eye on that for when it's supported. 99% of my GUI creations need QWidget or GTK+ 2.x so they fit into my KDE+QGtkStyle desktop, but I do have one project that custom QML widgetry would be well-suited for if I'm understanding its features correctly. (An image browser which needs to do GPU compositing and slide/fade animation of a whole bunch of thumbnails for a complex, multi-axis visualization so I can intuitively browse through images by various different relationships.)
Oh apologies. I guess this might be a bit of local slang I lazily allowed into my explanation. I did mean it in the "its directly inspired" sense rather than the "oh it leaked in lazily" sense.
Very cool! Please feel welcome to use my [vobsub](https://crates.io/crates/vobsub) subtitle parser if you need to extract timing information from sub/idx files. These are the ones that most often come with the video, and are therefore the ones most likely to provide an accurate time base.
I think the contrast was meant to be QML vs. the QWidget API.
My main problem with QML itself (rather than the QtQuick Controls) is that it doesn't do enough compile-time verification. (I'm already used to using PyQt5 and we now have rust-cpython, which would basically allow Python to serve as a more flexible alternative to QML in a Rust+QML stack. What I really want is a binding that would let rustc enforce as much correctness in my use of the Qt APIs as possible... especially since frontends are the most annoying parts of an application to test in an automated fashion.)
Rust is as fast as C, so it's not the next fastest. I can't imagine anyone being able to state that Rust is in the same line as garbage collected languages with a straight face. That's just downright false.
This is a "rustified" version of the original that keeps the same method of repeated string replacements: http://play.integer32.com/?gist=7e50bb110f7b6b6566850c0e82c2a54d&amp;version=stable Notable changes: * using `re.replace` with a closure makes the initial `find` unnecessary * a HashMap is not needed, since its keys are ascending integers * made the replacement more compact using format! and capture groups In general, as /u/asp2insp says, you'd probably write a parser that goes through the input char by char, constructs a parse tree, and generates an output string afterwards. That can be fun to do, so for learning more of Rust, I'd recommend trying it without a parser library (which would mostly make you learn the library's idiosyncrasies instead of basic Rust).
For every ordering of languages there is someone to defend it and a benchmark to prove it :) The facts are: * Rust compiles down to LLVM IR, same as clang, which LLVM optimizes and translates to machine code. * Rust puts data on the stack by default. * Rust generics are monomorphized at compile time by default. * Rust does not have a runtime. Therefore, the argument would be that performance can match C/C++ if: * Libraries/benchmarks are written properly. * The Rust compiler does a good job emitting IR that is easy to optimize. * LLVM does a good job optimizing it. These three things are not always true, but especially #2 is always begin worked on, e.g. with MIR optimizations that should reduce the amount of work left to LLVM. #3 is also an area of potential improvement because LLVM does not necessarily (yet) seize opportunities that are not provided by C/C++, such as the strong non-aliasing guarantees of Rust `&amp;mut` references.
Code written in Rust is and will be faster than C or C++. There are already plenty of examples like: * https://github.com/BurntSushi/ripgrep * https://github.com/mmstick/parallel * https://github.com/jwilm/alacritty * https://github.com/pcwalton/pathfinder * https://github.com/ethcore/parity It's not a coincidence that a lot of Rust projects are the fastest implementation of given thing anywhere, ever. Code written in Rust is much easier to work with, reuse existing libraries, write tests, refactor, use multi-threading. In real-life project this is going to be the greatest speedup factor. Plenty of times in C/C++ I ignore small performance waste, just to make the code easier to live with. Especially in C, equipped only with lame macroprocessor I just give up on what could be possible. In C++ it's not much better, since templates combined with all the C++ gotchas, and poorly retrofitted smart-pointers are just barely better. Simple example: Modern C++ codebases are ridden with `shared_ptr` which is like Rust's `Arc`, even if the code is completely single-threaded. Something that in Rust would be done with `Rc` to avoid atomic instructions, completely safety and refactor-proof. As soon as someone will want to pass such a value to another thread `rustc` will make sure it's fixed. C and C++ seeing two pointers, have a hard time telling if they can point to overlapping memory. Lifetimes and ownership and borrowing rules allow much better optimization. Something that in C would require `restricted` qualifier everywhere, which is done practically never. Traits and monomorphization make it possible to have very elaborated abstractions that compile to a super-optimized code. C++ can do it, but it's much easier to do in Rust. Throwing a `trait` here and there is quite easy unlike writing C++ template code.
A couple comments on your code: In gc(), instead of setting every field individually, you could just do something like `*self = Self::new()`. Also, new_heads and new_tails should be initialized to `Vec::with_capacity(self.internal_len)`. Also, my understanding is that all gc() does is takes the live elements of the linked list and puts them in order in a new vector. You may way to consider implementing Iterator and associated functionality. Then gc() could be reduced down to something like `*self = self.drain().collect()` Anyway, on to the broader algorithmic questions: as someone mentioned below, making the list doubly linked would allow you to swap removed elements to the end of the vector and pop them, meaning that you no longer need an Option to represent "holes". That also gets rid of the need for the free list. However, you need to step back and reconsider your goal in the first place. The only advantage of a linked list over a vec is that it supports constant time random access insertions and deletions. However, **that is only true if you already have a reference to the location to be inserted/removed**. That means that in order to be useful, the nodes of a linked list must have a stable address, which in turn means that you can't move them around like you're doing. An implementation like this which doesn't allow you to hold on to references to nodes may as well just be replaced by a vec. You can salvage the idea somewhat by using a TypedArena to allocate your nodes and maintaining a free list of holes. That way your list is mostly contiguous, but you never move nodes. The downside is that the "gc" operation is impossible. Memory can become arbitrarily fragmented and wasted if someone deletes nodes in the wrong pattern. Also, it would have been great if you put an explanation of the code up front instead of burying it in the comments.
They already have a GC, just not a tracing one. :) But it doesn't make any sense to change, Swift has ARC because it needs to interoperate with Objective-C runtime. Having a tracing GC would mean a solution like .NET has for interoperability with COM, where the runtime wraps the COM instances and takes care of translating the semantics between tracing collection and reference counting, leading to a more complex design. [Runtime Callable Wrapper](https://msdn.microsoft.com/en-us/library/8bwh56xe\(v=vs.110\).aspx) [COM Callable Wrapper](https://msdn.microsoft.com/en-us/library/f07c8z1c\(v=vs.110\).aspx)
Are you confused about node being reused? The trick is that disjoint fields of a value can be borrowed independently. That means that one line can move out `node.next` and the next can move out `node.elem` and there's no conflict.
`Vec::with_capacity` won't change your behaviour. It's just a performance optimization. (It allows you to allocate once at the beginning, rather than reallocating as it grows.) As for `push`, I'd need to see your code. **EDIT**: Here's what I meant: https://is.gd/1iZFHy
If I recall correctly there is a misoptimization in LLVM that led to temporarily disabling sending all this aliasing information to LLVM. In addition, I don't think annotating pointers as non-aliasing is currently possible except for function arguments (see [here](http://llvm.org/docs/LangRef.html#noalias)).
Panics are inconvenient, but not unsafe :)
Why do people always say c/c++ when talking about performance when every benchmark shows huge disparities between c and c++?
&gt; There are now some platforms where QWidgets still produce a better native look and feel than QML, others where QML does much better, and some where both are a little lacking. Can also partly confirm this. On Ubuntu 16.04 &amp; Qt 5.5 window menus won't show up with QML unless you set the environment variable `UBUNTU_MENUPROXY` to an empty string (due to a known bug), when resizing windows the contents look quite "wobbly" and sometimes I'm getting graphical glitches when I maximize a window. (These may or may not have been fixed in later Qt versions, but I can't be bothered to check) I really like the concept of QML and the features it brings to the table, but for now I'll stick with Qt Widgets. 
Good point about easier code reuse. The perfect example is the [`bytecount` crate](https://crates.io/crates/bytecount). No doubt there are C++ equivalents, but who is actually going to bother using them when you have to add another dependency manually, which is always a huge pain in C++. With Cargo you just add a line to `Cargo.toml` and then you can use the crate straight away.
Sure; but if the intersection is much smaller that's still a win :)
It is quite capable. It supports multiple layers of tiling and floating windows with easy switching between both of them, either for all or for the focused window, depending on how you configure everything. That aspect is actually highly customizable. You may take a look at the example configuration in the repo. (https://github.com/Drakulix/fireplace/blob/master/fireplace.yaml#L57) I am not exactly sure what you mean by pixmap-themed window decorations. If you want to change the window decorations of every window, you can currently quite easy do that in code, theoretically with the pixmap library, but that one is not used internally and I would call that an implementation detail. You may render window decorations with anything you want in code, but by default it has not many decoration options, because wayland expects the application windows to draw their own decorations and that works just fine, again independently from the library the application uses. If you may explain what exactly you mean with your second question, I can tell you how difficult it would be exactly to setup something like that and maybe add it to the feature requests. :)
I am wondering: can the fuzzer deduce a *next interesting* mutation from observing the program. For example, I am imagining the fuzzer reaching on switch on byte [8, 12) of the input and realize that there are 4 branches possible, so create 4 mutations (one for each branch). Is that a dream?
&gt; It is quite capable. Sounds good. I'll take a look at that after I've slept. &gt; I am not exactly sure what you mean by pixmap-themed window decorations. In the parlance that developed around X11 window managers and themable widget toolkits (eg. GTK+), a "pixmap theme" is a theme which requires no theme-specific code, being composed of a bunch of images which are blitted into place (and, if appropriate, tiled) to create the window decorations. (Similar to how CSS 3's image-based borders support works.) Basically, I want to know that, if I like a theme in one WM, I can port it to my chosen WM with minimal effort. (eg. by taking screenshots with the buttons in various states, then chopping those screenshots up and adding minimal metadata.) Also, speaking of client-side window decorations, how's the input situation with those if the client freezes up? (Basically, I want to make sure that I'm not going to wind up in a WinXP-like situation where, if an application gets lazy in handling events, I won't be able to manipulate the window via its decorations.)
/r/playrust
Prof. Lemire does care, and recently asked on his [blog](http://lemire.me) how bytecount compares to his AVX-enabled C implementation. I haven't yet looked into FFI too much, so if someone wants to build that benchmark, please share the results. Both Prof. Lemire and me are curious.
The requirements you list might be met fine with plain QML. http://doc.qt.io/qt-5/qml-qtquick-shadereffect.html https://www.youtube.com/watch?v=yXf6hhZSnI8 If you want custom positioning and sizing for your images, you can bind to objects on the Rust side and use some thing like ``` Rectangle { x: rustObject.x; y: rustObject.y; width: rustObject.width; height: rustObject.height; /* content */ } ``` with a Repeater. http://doc.qt.io/qt-5/qml-qtquick-repeater.html
I think if we were to go a little further down this rabbit hole of better/optimized code, we would realize we have been solving the problem all wrong. For one we are storing a usize for no reason as vec contains a capacity. And if we look further, if the whole point is to be able to push onto or remove from the ends, than we should have been using a VecDeque. We have been doing insertions/removals (depending on the author) in O(1) for one end but O(n) for the other. We could have been doing O(1) for both sides. We are just looking for a wrapper around VecDeque, where you can't go past a set size. https://is.gd/NMibmP Now we don't need to use any "unsafe"/unchecked indexes for insertion or deletion.
Does this blog text address your question: https://lafintel.wordpress.com/2016/08/15/circumventing-fuzzing-roadblocks-with-compiler-transformations/ LLVM also has support for data-flow-guided fuzzing: http://clang.llvm.org/docs/SanitizerCoverage.html#tracing-data-flow
[removed]
I can totally imagine it, because it happens all the time. Not because of language properties, but because of implementation deficiencies. (Not to say the language itself doesn't have issues that hold back performance, either- it certainly does.) Rust has a good head start by using LLVM, and things have improved over time, but it's just not as mature a compiler and ecosystem C or C++.
Perhaps not pedantic enough- various language properties absolutely encourage and limit both implementation and program performance. This is largely how Fortran beats C, and how C beats Javascript.
`OUT_DIR` is what it is -- a place to store artefacts for you to use in the same build (i.e. foreign libraries your library links to). If you want a different path, just use a different path.
I think the mistake you are making is that of scoping. The variable `n` is not a global variable, it is a local variable. So you actually need to pass it separately to the function like `fibonacci(n)`.
How would I get the directory 3 steps above OUT_DIR?
The quality of implementation has a huge effect whether the code optimises well and runs fast, but it's effectively limited by the design choices baked in the language it implements. Dynamic dispatch, pointer chasing, pointer aliasing and disallowing unsafe memory access are things that are often required by some language features. There's a limit how much the implementation can do.
&gt; Memory management is simple. Nodes get allocated at loading time, and they never get freed or moved around until the RsvgHandle is destroyed. &gt; But Rust doesn't work that way. So there is one object (the SVG file) and hence one lifetime. So at most there needs to be one `Rc`. All objects can go in `Vec&lt;&gt;s` for their type and instead of pointers you can use `usize` internally. Externally, you can use reference with lifetimes that are the same as the lifetime of the whole SVG file. 
&gt; Memory management is simple. Nodes get allocated at loading time, and they never get freed or moved around until the RsvgHandle is destroyed. To free the nodes, the RsvgHandle code just goes through its all_nodes array and calls the node-&gt;free() method on each of them. Any references to the nodes that remain in other places will dangle, but since everything is being freed anyway, things are fine. Before the RsvgHandle is freed, the code can copy pointers around with impunity, as it knows that the all_nodes array basically stores the "master" pointers that will need to be freed in the end. This is possible in Rust, we call it an "arena". This enables safe `&amp;T` reference cycles without reference counting. (There is a small amount of `unsafe` code in the arena library itself, but it’s much more auditable than raw pointers everywhere.) https://github.com/SimonSapin/rust-forest#arena-tree
Nice, something like this then: https://is.gd/EUtysB fn fibonacci(n: i32) -&gt; i32 { if (n == 0) || (n == 1) { } n } fn main() { let n: i32 = 1; println!("{}", fibonacci(n)); } I'm not sure yet if the function will exit if n == 0 or n == 1, I intend to return n if the condition is true
Right, semicolon.. shoulda caught that more easily, thanks
http://doc.crates.io/environment-variables.html#environment-variables-cargo-sets-for-build-scripts
This is the current state of my code, thanks all https://is.gd/1bmySG fn fibonacci(n: i32) -&gt; i32 { // TODO: // Memoize: // - Create an array // - For n not in array // append n to the array // if n already in array // return n from array // Recursion: // - Function should call itself // to calcluate the next n // which should be the last // n plus the current n let mut n_array = []; if n_array.contains(&amp;n) { // TODO: return item from array // return n; is incorrect } else if (n == 0) || (n == 1) { } n // Without Memoize //if (n == 0) || (n == 1) { // return n; //} //return n; } fn main() { let n: i32 = 1; fibonacci(n); } I get a warning about my array being mutable, but I think that I can ignore that because once the function is written all new numbers in the fibonacci sequence should get appended to the array. In that case I believe it should be muttable?: rustc 1.15.1 (021bd294c 2017-02-08) warning: variable does not need to be mutable, #[warn(unused_mut)] on by default --&gt; &lt;anon&gt;:15:13 | 15 | let mut n_array = []; | ^^^^^^^^^^^ 
That's a lot of downvotes for a fairly reasonable statement. It resonates a lot with others who point out that the current C/C++ implementations of the benchmark game are ridiculously far from a real world implementation. However, there are such things as languages that need a runtime, and in general these are slower that their bare-bone competitors.
parallel is a poor example for arguing that Rust is faster than C or C++. First, the original GNU parallel is written in Perl. Second, the two implementations are not comparable: https://www.reddit.com/r/rust/comments/4zf8bk/implementation_of_gnu_parallel_in_rust/d6why1f/
It's not just a matter of using a better back-end. What made it possible for FORTRAN compilers to generate faster code than C compilers was the fact that, unlike C compilers, FORTRAN compilers did not need to consider pointer aliasing possibilities and could optimize much more aggressively. Many C and C++ compilers have a switch which allows the compiler to ignore aliasing considerations and generate code that performs just as well as code produced by a FORTRAN compiler. However, the programmer is then responsible for making sure that their code is safe in the face of these optimizations, which is not an easy task. Turning these optimizations on code which had not been written with this in mind often led to programs that crashed very quickly.
Along with what others have said, one major hurdle with the benchmarks game is that there's a rule that if the language has a default implementation of something, it should be used instead of rewriting. However, if there's no implementation, the submission is free to choose one off the shelf. For example, one of the benchmarks requires a hash, and the C implementation uses khash whereas the Rust implementation uses `std:: collections`, and khash is faster in this test. Rust could probably match C's performance if it were allowed to optimize the hash.
Do you think Rust will get into codegen? It doesn't seem like a priority, but it could be interesting to play with.
`OUT_DIR/../../..`, where `/..` points to the parent folder. E.g. like this: let out = env::var("OUT_DIR").unwrap(); PathBuf::from(out).push("/../../..") Edit: Used `env!` here. Thanks, /u/phaiax!
The docs for `arena-tree` say "The tree can only be accessed from the thread is was created in." Why is that? In the case of rsvg, I think the whole tree is immutable and hence could be shared as long as there's an Arc&lt;'arena, Arena&gt; on every thread. The nodes would then also have lifetime 'arena. 
I'm well aware of the different aliasing rules of C/C++ vs. Fortran vs. Rust. Indeed there are Fortran compilers for both GCC and LLVM but because these are mostly aimed at C/C++ (especially llvm) they simply don't implement all the possible optimizations because that would be extra code that is unneeded for their main purpose. I know that there were past attempts to provide more of rust's aliasing information to LLVM but it didn't work out due to bugs in llvm. My thinking was that a Fortran oriented backend would be much better at utilizing such information from Rust.
Dumb question: why JSON? Are you planning to do any optimizations due to the fact you're working with JSON? Why limit yourself to JSON and not go with plain text?
I'm trying to create an array of `AtomicBools` in a compact way. This doesn't work, since `AtomicBool` is not copy: let my_arr: [AtomicBool; 4] = [ATOMIC_BOOL_INIT; 4]; I can expand that, but it gets unwieldy very quickly: let my_arr: [AtomicBool; 4] = [ATOMIC_BOOL_INIT, ATOMIC_BOOL_INIT, ATOMIC_BOOL_INIT, ATOMIC_BOOL_INIT]; I can hack it with `transmute` (and this is already an unsafe fn, so that's not the worst), but... boy do I hate it, and it's not particularly clean. At least, AFAICT, it is "safe-ish", since `bool` and `AtomicBool` have the same in-memory representation according to the docs. let my_arr: [AtomicBool; 4] = transmute([false; 4]); I think my best bet is to write a macro that will expand to case 2 for me, but I'm hoping somebody out there has already written that (or it's in `core` somewhere and I missed it)
&gt; I bet rust would beat the shit out of that if the benchmark used the same approach. Please [contribute a program that uses the same approach](http://benchmarksgame.alioth.debian.org/play.html#contribute).
The podcast in general is great, but CYSK is really great. I know I'm in for a good commute when Pocket Cast says a new one is available. I'm sure I used something more getopt like in the past but I can't remember what it was called. It's at work so I can't immediately check either. Hate it when that happens.
We've had discussions about it and basically what I got was: feel free to implement it, but there's no guarantee that it'll be accepted even if it's the same as the one the C code uses. It doesn't seem worth spending the time to implement something without something more substantial to go on. Nobody wants to spend time implementing something to have it be rejected.
Those (like other benchmarks) are not indicative (whatever that means) of real-world performance (whatever that means). Unless they are (someone would have to show that they are or aren't).
Are there "fast (reference) implementations" or are there only programs that are fast when built with a specific language implementation and run in a specific environment?
There is however the fact that a completely reasonable implementation using ordermap is still sitting in the queue waiting for your approval, and your answers and requests have little to do with the implementation at hand. I can understand why others shy away from implementing improvements, given the response they get. Seriously, I'm a little disappointed myself.
I'm sure you've met a real-world program in the past. Say, grep. Written in C. Meet [ripgrep](https://github.com/burntsushu/ripgrep), which is faster. Or how about a [terminal](https://crates.io/crates/alacritty)? Find me a faster one written in C. Let's not even start talking about [browser engines] (https://github.com/servo/servo). Real-world applications.
Would *real world implementation* and *mandelbrot* be a contradiction-in-terms? Perhaps you mean something like [written as though performance did not matter ?](http://benchmarksgame.alioth.debian.org/u64q/program.php?test=mandelbrot&amp;lang=gcc&amp;id=2)
I've gotten a small walkthrough for getting examples to run [on this gist](https://gist.github.com/ErichDonGubler/c802e066de7068241f0e6e492e6584ed). I have a working example for Arch Linux, but I've yet to determine how exactly to get Windows to work. Will work on this some more, and try to get make things easier for the Windows audiences. :)
&gt; A custom "hash table" will not be accepted, period. It may be easy to confuse that with &gt; if the language has a default implementation of something, it should be used instead of rewriting Calling it a lie instead of explaining the distinction you are making can leave your audience confused. 
Firstly: please stop spreading that lie. Secondly: When would we know *"it's the same as the one the C code uses"* ? When we've inspected the code and the make test results. &gt; Nobody wants to spend time implementing something to have it be rejected. And I don't want to spend time inspecting experimental libraries for 30 language implementations before there's even a contributed program that requires the library. You see how that doesn't scale?
There's a history. You really don't want to know.
This is a series on writing a toy programming language in Rust. It's not a definitive guide, rather my own explorations and learnings for fun and education. I'd be grateful particularly for any help with idiomatic Rust, if anybody cares to look :-)
Qt has many libraries (41 on my system). Qt5Core is the most basic library that contains QObject and QString. Qt5Qml contains the QML functionality and depends only on Qt5Core and Qt5Network. QML is meant to be light and fast. It started off as a project for mobile phone interfaces. Qt5Gui and Qt5Widgets provide the desktop widgets. This functionality is entirely different, but it's possible to mix QML and Qt5Widgets. Qt5Quick contains UI elements for use with QML. The acronym QML is not well defined. At some point there was Qt Modeling Language and Qt Meta Language. Like with KDE, the acronym is not important. By the way, QML is not limited to only user interfaces. There's a project called silk that uses QML to write web servers.
Having competed in Advent of Code against C programmers to write the most efficient solutions, my Rust solutions always came out as fast as or, more often than not, my Rust solution is faster. The big benefit to writing Rust software is that it's easier to work closer to the metal, safely. So I doubt you'll find a C programmer willing to do the same work as ripgrep.
Thanks! It would be great to also let windows users enjoy this. Personally, I'm on NixOS which has it's own set of hoops to jump through to get the setup going. I've tried to get things working on the latest Ubuntu but also requires some more trial and error. I think providing these instructions in a PR to qml-rust would be a big help. 
Wrong subreddit.
You could have accepted that program a week ago and preempted that discussion once and for all. Just saying.
Cool, thanks
`N` times as long (for some `N &gt;= 1`) now that it's transliteration into C ;-)
The font size of everything is too small, and the colors are very hard to read.
You can look for libQt5Core.so in your system and see of libQt5Qml.so and libQt5Quick.so are there as well. Depending on your OS or distro you might need to install these packages separately. If you installed from the installer from qt.io, the default options will install these libraries for you.
It could be interesting to have a few more benchmarkgame problems that weren't taken from numerical computing. That would make it more general.
Yup. I came here from Haskell/python and it was just plain *weird* how many people from Java etc. corners of things have a moral opposition to using libraries. I saw it in python too but usually people accepted using libraries was a good idea *eventually*. 
Rust is faster than C++ already but their domains of use don't really overlap enough to be a reasonable comparison. &gt;I would hope for it to be more than just a hipster's language to program with. Its performance/design is already there. It's lack of libraries/learning resources/stability etc. that is causing the bottleneck. 
A Rust-to-C compiler shouldn't be *too* hard to construct, assuming you don't care about producing particularly readable C, because the semantics are fairly similar. I imagine it might even be easier than porting ripgrep and all its dependencies to C by hand.
Thanks! I had installed from qt.io, but installed in /opt. Tried reinstalling in home, but had the same problems. Tried copying just the missing .so files to a PATHed lib dir, didn't work, so ended up copying everything under Qt/5.8/gcc_64/lib to the PATHed lib dir. Probably not elegant, but it worked. Also thanks for the article! I'm not familiar with Qt, and not very good at Rust, so plenty of learning to be had :)
This seems very useful! Does it handle cases where one set of subtitles has an extra subtitle or is missing a subtitle? And also cases where something is subtitled with one subtitle in one file, but subtitled with two (or more) subtitles in another file?
I'd say Haskell is an order of magnitude faster than python, it's more in the ballpark of Java/Go
I would like to agree with you, but I wouldn't want to be stuck maintaining qml-rs's build.rs replacement for Qt's build system.
rustbuild has `-i` (incremental) option.
What's the array for, exactly? If all you're going to return is a specific value in a sequence, then you only need a single mutable integer value internally. Additionally, I would avoid writing any software that uses recursion. Recursion is very inefficient on hardware resources and it's often times easier to write your software with a loop instead.
Try again in /r/playrust
Yeah but that doesn't help in this situation, where the only reason `Fix` wants to use function traits is to get the function call syntax which you can't emulate in stable.
The reason its so long the first time in my experience is that its building LLVM. Once that's done its significantly shorter, though still rather long. My advice for hacking on the compiler, as someone who does very little of it, is once you've decided what you want to change, use ripgrep to find the error message that seems most closely related to the thing you're changing, then start looking from there.
&gt; Let's not even start talking about browser engines. Indeed, let's not; just installed Servo and it consistently freezes after a few seconds of clicking links on not-particularly-taxing sites.
As notriddle said `Cell` is not thread-safe, and some kind of mutability is necessary to create cycles. The usual answer to cells + threads is to use locking, but one lock per node could get expensive quickly. You could imagine a shared lock (one per tree?) that protect a number of cells, but that wouldn’t necessarily be convenient to use. Another idea I’ve thought about but not tested is to separate access between phases: * Mutability but on a single thread (such as for initial tree construction, to close cycles) * Thread-safety, but no mutability This seems doable because all node references have the lifetime of the arena, so if you can move the arena or get a `&amp;mut` reference to it that implies that there are no remaining references to a node other than from other nodes in the same arena. At that point it is safe to switch to another phase, which would require transmuting `Arena&lt;MutableNode&gt;` to `Arena&lt;ThreadSafeNode&gt;`. Of course, for *that* to be safe the two node types need to have the same memory representation. That is, being exactly the same except that `Cell&lt;Foo&gt;` becomes `Foo` (or `ReadOnlyCell&lt;Foo&gt;`?). [`#[repr(transparent)]`](https://github.com/rust-lang/rfcs/pull/1758) would need to be added to the language to make remove uncertainty on the safety of this transmute. And [associated type constructors](https://github.com/rust-lang/rfcs/pull/1598) would help make this less error prone, by having one node type generic over a cell contructor instead of two node types defined separately: ```rust struct Node&lt;'arena C: Contructor&gt; { parent: C::Cell&lt;Option&lt;&amp;'arena Node&lt;'arena C&gt;&gt;&gt;; // ... } ``` Even with all this, we end up with a transmuted arena but no way to access any of the nodes inside. So the arena would need to be extended with a node identifier scheme where nodes can be accessed by id after switching phase. Or maybe just a special case for the first node that were inserted in the arena, which is likely the root of the tree. Either way, `Arena&lt;T&gt;::alloc` would need to return `&amp;T` instead of `&amp;mut T` since this new mechanism gives out other references to the same node. ------ This is my grand plan for zero-cost, safe, multi-threaded processing of cyclic graphs. Anyone wants to have a go at implementing it?
I think even on stable, you can just unsafely cast back and forth to `*const Rc&lt;T&gt;` (or would it be `*mut` because the reference count itself is mutable?). The added value of `into_raw` and `from_raw` is that you have a direct pointer to the contents of T. But if you're treating it as an opaque handle, then pointing to the refcount header should be fine.
Yes, Qt in general is pretty good about that (eg. [pytest-qt](https://github.com/pytest-dev/pytest-qt)) but that doesn't change the fact that you're at the top of the integration pyramid, where it's the most annoying to get a test suite that's both fast and has good coverage. (Especially with Rust's very immature mocking ecosystem.) I came to Rust because I kept burning out when aiming to write acceptably thorough Python test suites for want of a type system that would make affine and session types possible, comfortable, and performant.
Hmm. That might actually work. (The core interaction mechanic I want is to have what behaves like an invisible grid where the current row and column are visible and, if you scroll (cardinal directions only), it'll lazy-load a new row/column once you stop, recognizing that accessing row X via column Y will produce different results than accessing column Y via row X, since it's more a visualization of a DAG of similarity metrics than an actual grid.) The #1 concern being a UI that keeps the user waiting as little as possible. (eg. I'll do anticipatory loading of the current row and column in the background.)
Oh, derp, of course. I even do that with `Some(x)` elsewhere in the same code, I just didn't twig that the same thing happens with a *completely* unrestricted pattern. Thanks for the help. (From you and /u/llogiq )
Yeah :( typo, sorry
I tried match. But I don't like to write 10 nested match to check the low nibble for every high nibble :( then I ended up using macro. 
Boxing an empty struct does not allocate: https://is.gd/zIWssh use std::mem; struct Foo; trait Bar {} impl Bar for Foo {} let foo_box = Box::new(Foo); let bar_box: Box&lt;Bar&gt; = Box::new(Foo); println!("Box&lt;Boo&gt;: {:p} Box&lt;Bar&gt;: {:p}", foo_box, bar_box); // Deref is necessary to get the size of the value, else we get // the on-stack size of `Box&lt;Foo&gt;` which is a pointer println!("Alloc size for Box&lt;Foo&gt;: {}", mem::size_of_val(&amp;*foo_box)); println!("Size of Box&lt;Foo&gt; (thin [normal] pointer): {}", mem::size_of_val(&amp;foo_box)); println!("Size of Box&lt;Bar&gt; (fat pointer): {}", mem::size_of_val(&amp;bar_box)) Prints: Box&lt;Foo&gt;: 0x1 Box&lt;Bar&gt;: 0x1 Alloc size for Box&lt;Foo&gt;: 0 Size of Box&lt;Foo&gt; (thin [normal] pointer): 8 Size of Box&lt;Bar&gt; (fat pointer): 16 (`0x1` is used as a pseudo-null value for pointers in the Rust stdlib to enable null (`0x0`) pointer optimizations) The `Box&lt;Foo&gt; as Box&lt;Bar&gt;` trait object must still be two pointers for ABI compatibility, but there's no actual allocation behind it. Note that `Rc` and `Arc` will both allocate because they need someplace to keep their refcounts, but `Vec&lt;Foo&gt;` does not allocate.
As someone who hasn't used Rc yet, am I correct in assuming that construction would be accomplished using the following algorithm? 1. Initialize the child using `Weak::new()` for the parent reference 2. Take an `Rc&lt;Child&gt;` and store it however the parent tracks children. 3. Call `Rc::downgrade` and use the result to replace the empty parent reference from step 1.
Stack size is determined by the operating system, not the application (though new threads can be configured with a custom stack size when spawned, I doubt `rustc` does this). It's likely that your parser implementation is hitting some infinite recursion bug in stable that has since been fixed. Have you tried the latest beta release to see if it crashes or not?
Wait but can't you just match the tuple of the nibbles? ' match (high, low)'?
Because it's still in relatively early development and your system is different from the developers' systems. It's much faster than other browsers when you compare apples to apples and its incompleteness won't be an issue as Mozilla moves the complete pieces of it into Firefox. (See [Oxidization](https://wiki.mozilla.org/Oxidation))
Happy to help :)
&gt;Or how about a terminal? Find me a faster one written in C. In what benchmark is alacritty faster than, say, [st](http://st.suckless.org/)?
I'm not sure I understand the question, which part of repeatedly calling `arg` on a `Command` as [shown here in the documentation](https://doc.rust-lang.org/std/process/struct.Command.html) is giving you trouble?
Do you have the [Visual C++ 2013 libraries](https://www.microsoft.com/en-US/download/details.aspx?id=40784) installed? You also need the [Visual C++ Build Tools](http://landinghub.visualstudio.com/visual-cpp-build-tools) if you want to use the MSVC build of the compiler.
Thanks. :)
I increased the font size a point and I like it a whole lot better myself. Let me know how it works for you.
This sort of gotcha is pretty much the perfect example of why people coming in from I/O-bound programming in managed languages (like me) tend to take one look at high-performance CPU-bound programming and give up because "it's just too difficult if you're not the kind to learn assembly language for fun".
I've often read about rust having better optimisation potential because of richer information about pointer aliasing but that argument was always a bit abstract for me. Would never have expected such a speedup, even if this is a special case.
Well, the regression in the post was solved without any need to go into assembly, only the ability to create a reproducible version and ask for help. I think, as with most bugs, being able to create a reproducible testcase is 90% of the battle.
I think the other takeaway is that Rust is only scratching the surface of the optimization opportunities from the type system, since LLVM mostly only supports what is useful in C/C++. If LLVM ever got good support for Rust, performance would shoot up.
Why does `f64::MAX + 100.0` equal `f64::MAX`?
Yes, but the actual problem is that we have no experience judging what kind of performance we're supposed to expect from things in this environment, which means either assuming whatever we get isn't hiding big oopses like this or throwing an uncertain and possibly quite large amount of time at trying exhaustively to tease variation out of the code to reveal cases like this when they may not even exist. TL;DR: Noticing a regression is by far the easiest case. If it "started out regressed" or we weren't paying attention to runtime when we introduced the regression, then we'd never know. **UPDATE:** Now that I think about it, that *is* what I'm looking for. Some kind of crash course in how fast native languages *should* be when performing various tasks, plus the kinds of ways in which human intuition about performance characteristics diverges from actual performance characteristics due to implementation details in modern CPU architectues. (eg. I've heard of things like cache misses and branch mis-prediction, but, I don't know how to apply knowledge of their existence to the code I actually write.) This failure to optimize is just the scariest example of how capricious low-level details can be. (Heck, as long as I can find a used copy suitably cheaply on AbeBooks.ca, I'd even be willing to pay for a dead-tree version of such an "I've programmed in scripting languages for two decades... just teach me the performance implications of various design choices in native languages" crash course.)
do the book first, its good. if you have some remaining questions, ask.
http://benchmarksgame.alioth.debian.org/ According to an average of these benchmarks C++ is pretty much the same distance from C as it is from Java in terms of performance (not in terms of memory usage). So you could just as easily lump C++ and Java in the same performance category as you could C and C++. I guess if we're just talking about orders of magnitude you can lump all 3 and many other languages including Rust, Swift, Fortran, and more together. From what I've read it seems you should be able to write code in Rust that's on par or faster than code written in C in the future and the only issue now is the immaturity of the Rust compiler. Just a huge number of small compiler tweaks to be done. So Rust may one day become the baseline for performance comparisons. But it seems unlikely it will ever get significantly faster than C as not even hand optimised machine code can get a huge amount faster.
I like this option the best since it's more obvious that a copy is going on. I don't know if this will work, though I assume it will.
Hey, so you apparently know what you're doing. Would appreciate your feedback [here](https://www.reddit.com/r/rust/comments/5uwcm5/first_rust_library_custom_derive_for_super_easy/?st=izc3rhzi&amp;sh=c2f37681).
Well couldn't those optimizations be done at the MIR level? It doesn't necessarily hurt to do it there and we won't have to wait an eternity for LLVM folks to do anything about it.
Probably. I think MIR level optimizations are on the roadmap, they just haven't been implemented yet.
All languages require you to understand more than you wanted to know about how they work to understand how to write somewhat optimal code. Most languages I've encountered do not have up-front discoverable documentation on what is optimal, and importantly, why. Usually this information is scattered throughout blogs, stack-overflow answers and forums. It would be amazing to have a treatise on these "unpredictable" behaviours on the level of the 'nomicon.
This is planned. But not there yet, there are higher priority things to (MIR borrowck, nonlexical lifetimes, etc)
I think they were asking why floating point arithmetic apparently doesn't overflow like integer arithmetic does. I initially assumed overflowing a float would produce `NaN` or `+INF` but now I realize that doesn't make sense because these are reserved for specific mathematical results and not just a number that's outside the range that the implementation can represent. I'd expect this to panic in debug mode, then, to be consistent with integer arithmetic, but I see that floats don't support checked arithmetic, or at least Rust doesn't have an API for it. 
[removed]
There is none. It was a reduced test case. The OP never showed their original code.
Its just the redistributable that they are linking you to not the full Visual Studio. Its basically the c runtime for windows machines (its the runtime that any msvc compiled things are linked against at least)
I'm curious, if you're comfortable sharing, what sort of class is teaching Rust? I would have killed for this sort of exposure in university, but fortunately I was able to find this sort of stuff outside of class. 
Dumping text is not a very useful benchmark. That's not what you use a terminal for. I get the following results with latest alacritty git and st on my system for /usr/bin/time -p dd if=/dev/urandom bs=1 count=10485760 which dumps 10MB of random data to the offending fullscreen terminal. Beware terminal bells, flashing colours and all that. alacritty: real 16.83 user 0.76 sys 16.88 st: real 15.78 user 0.85 sys 16.19 Similarly useful benchmark (not at all), with file metadata cached beforehand: /usr/bin/time -p find /usr alacritty: real 2.26 user 0.73 sys 1.70 st: real 2.24 user 0.80 sys 1.58 Alacritty is **slower** here. &gt; I think alacritty uses the GPU and st doesn't, so I would expect significant performance differences You don't throw GPUs at a problem and expect instant performance boosts for every use case ever. That's not how it works.
Well it's simplified from part of a program I'm working on to generate FNV hash collisions. I'm planning to open source it when it's done, but for now it's a WIP.
I'm a big fan of [rust-from-lang](https://github.com/mgattozzi/rust-from-lang/blob/master/External.md) - a buffet of tutorials based on your programming background. 
Well profiling will suggest which parts of your code you should give extra scrutiny to, but it's not a silver bullet. &gt; You can also solve it by adding more RAM. RAM is cheap, and modern CPUs/motherboards will typically let you get 64G RAM. When I built my computer two years ago, RAM was the most expensive part, $320 for 32GB RAM, and anything more than that would have sharply increased in cost.
Wouldn't having a `size.copy()` be even better? Because then you always know when the copies are being made, instead of having them being implicit. I literally never know when my stuff is being copied and it's confusing as heck.
It would be extremely annoying to have to write things like `x.copy() + y.copy()`. That said, I'm curious why you need to know when implicit copies are occurring in Rust: IME, they're almost always uninteresting (the "almost" being performance edge-cases like the OP).
I'm no Fortran expert but I do know that the latest language version was released in 2008. I'm sure that by 2008 they already added OOP, exceptions, and other such modern features to the language. We're not talking about compilers for fortran77.... 
Some of the solutions are already using custom hash tables, so this is an obvious lie.
I have some keys (i32) as fields in structs that reference other structs. I want to make them type safe so that a function can only take a key that references a specific type of struct. So something like pub type PrimaryKey&lt;T&gt; = i32; pub type ForeignKey&lt;T&gt; = PrimaryKey&lt;T&gt;; but that fails to compile because of the unused type parameters. I realize that I could make an enum and use PhantomData or something along those lines but I would much prefer a type alias. Any ideas?
You still run into the issue where you have a ref that points back to the struct itself eventually. If you're using references, you might run into non-lexical lifetimes. I'd rather `Rc` everything if I go that route. Then you get basically Swift.
I'm aware of all of that... it still tweaks my risk-averse personality enough that I tend to focus on using Rust for doing strict compile-time checking of problems I'd be willing to attempt in Python.
Thanks for pointing out the enum variants bit. I'm pretty used to optimizing Python, so profiling and selecting appropriate algorithms is familiar, but I haven't yet solidified the habits of thought for working in a language where everything isn't indirected through a reference.
Operating Systems and Concurrency! A senior level class at my university
Why pass by value instead of by reference? Doesn't passing by value mean more work?
Cool idea! Might be worth trying to contribute to rust-csv? (Paging /u/BurntSushi.) 1. I think `panic!()` in proc_macro code is not as bad as it is in regular Rust code, since there's not much in the way users can take your `Result` and handle it anyway. 2. Your repository seems to be structured as a crate in a crate. If you instead make all your crates siblings in the repository, you can use [Cargo workspaces](http://doc.crates.io/manifest.html#the-workspace-section), which is more idiomatic and allows you to make better use of Cargo. 3. The reason you're not allowed to export procedural macros and other items from the same crate is an implementation limitations, as far as I understand, and not a design target. I would say defining traits just to use them with a procedural macro is fine. 4. The way I think you could improve it is to realize that what you derive is mostly about converting a CSV row or line into a struct. Now, you attach a method `parse_csv()` to the struct impl, but there's a bit of a mismatch: while the struct is about a single row of data, the parse method handles a whole file. If you instead structure your code to expose an generic Iterator from your main crate that takes a `csv::Reader` and has a type parameter with a trait bound on the derived trait, you can limit the derived code to just the conversion from CSV data/row to struct, and keep the Iterator code generic. Does all that make sense?
You either need the `T` on both sides or neither. Also type aliases is not going to make your code any safer. Consider newtypes.
Currently no integers in generics. This blocks a few things, most notably for my purposes fixed length arrays of lengths not specified by the stdlib. (Last I checked they specify 1-32 and maybe a few higher powers of 2) Sure you can still create them and modify them, but they have nearly no methods. Not exactly usable.
llvm isn't optimized for Rust. Rust could be faster than C/C++, but llvm is more a C/C++ backend.
Before 1.0 rust had garbage collection and it was written as a library, I believe. Perhaps whatever features we need to have that again will manifest at some point. In the mean time, if you need to have values with cycles you'll have to implement that some other way like explicit sharing.
That's very much possible indeed. The async loading can be done on the Rust side. Navigate to X,Y -&gt; show placeholder/throbber or cached data. The load data async and change property of QObject/model which'll update the UI automatically. 
The Rust compiler is written in Rust, and the bootstrap compiler from years ago is written in OCaml. You might be able to use your C compiler to build an OCaml compiler, and then keep bootstrapping progressively newer versions of Rust very slowly, but it's probably easier to just cross-compile rustc itself using an existing rustc. 
It depends. Passing small objects like ints that are smaller than a reference it can be faster just from the size. Even for medium sized objects (1-4 words or so), not having to perform a dereference can be a lot faster (less instructions, less cache misses). For large objects yes, reference is going to be faster.
Should the println macro be amended to wrap whatever you pass into a block, so this sort of tribal knowledge is never needed again?
Thanks for correcting for my laziness, am on mobile.
I'd just transmute to a `*const c_void` tbh. Though there may be some UB I'm not aware of there.
Alacritty is also a bad example, because when it was originally introduced, users pointed out that it's missing several critical features that would decrease performance if added.
Rust generics are a pretty big pain point for me right now. Sometimes I wish I had the "compile time duck typing" that you get from C++ templates. You get to play really fast and loose with the types, so it's a lot faster to write a templated function. This comes at the cost of less-obvious compiler errors. In rust, you have to do a lot more upfront work (Know which traits to use, make sure each type you want to use implements that trait, make newtypes for the ones that don't, etc) to get it to compile. Getting number-crunching to work with generics is absolutely obnoxious, borderline impractical. Mostly because you have to write an eldritch incantation to specify constants;
`@object` was reference counted, so it was the same thing as the current `Rc` except the compiler special-cased it
Anyone got this working in Windows MSVC?
Could you elaborate on this? It seems pretty accurate to me.
Haskell?
You don't need the prefix to your statement. I didn't read it the first time, and I didn't see anything saying "Rust is bad" in it. For cross-compilation development where rustc is already built, you can use xargo (which I personally pronounce cross-cargo) to pull in the necessary stdlib and whatnot to compile for the target architecture. Android is such a target. But if you need to cross-compile to a platform that Rust doesn't support, you're going to have to do a lot of work porting it. You don't need a C or C++ compiler. You can ask in #rust-internals on irc.mozilla.org or on https://internals.rust-lang.org for assistance with that. At no point should you be going to the OCaml version and bootstrapping that way. The nightly versions required to actually build each version since then varies wildly, and even if you did so, it most probably wouldn't produce a version that is "good" for your target. To directly answer your title question, Mutabah is working on a rustc compiler in C++. That compiler is called "mrustc". When it is finished, you'll be able to bootstrap from C++. Again, being able to bootstrap rustc from C++ won't actually help you cross-compile, as you still have stdlib and possibly other things that also need to be platform specific updated.
Haskell is lazy functional. Rust really isn't lazy or functional.
Here's a couple of resources I had posted some time ago in another reddit thread: - https://www.youtube.com/watch?v=Nsf2_Au6KxU - https://www.youtube.com/watch?v=rX0ItVEVjHc - http://www.dataorienteddesign.com/dodmain/ - /r/codeperformance
Anytime you feel the documentation is lacking in some specific way, please file an issue.
Have you enabled incremental compiling? You can easily slap a 200 second compile time down to 10 seconds with it.
Hi `AtomicPtr`! Please explain yourself!
Haven't been keeping super close track of rust recently. What kind of specializations can unstable rust do? - Select a more specific implementation for a trait. Not as problematic as OverlappingInstances in haskell since importing a library can't change existing behavior but still seems like it would be super hairy. Which one is more specific if you have one implementation using Ord and one using Hash? - Select a more specific datatype based on type indices like data families in haskell? As in, Set&lt;Int&gt; might use a sparse bitset and Set&lt;A&gt; a hashset if a is hashable.
&gt; That said enabling decorations like that sounds nice in theory and I am open to implement that. You may open an issue on that subject and then we try to figure out a format for those provided themes. I've been pretty busy these last few days, but I'll try to do a little research and get back to you as soon as possible. (My first impulse is to look into how WMs like Openbox and IceWM handles pixmap themes, since there are a lot of those already available and it'd be nice if they were machine-convertible.) &gt; it will of course depend on the client to propagate these events to fireplace. So if it freezes no events will come through and you will not get any immediate reaction. In other words, exactly what I feared and why my plan so far has been to stay on X11 and see what Kwin does, should nVidia Binary Drivers with non-GNOME Wayland become viable before it's ready. (It's been 85 days since I last logged out and that's not atypical. If there's a way for a hard-to-reproduce bug to crop up on my desktop, it most likely will.) While I don't like GNOME Shell, I actually talked to some GNOME guys and they said it would be possible to implement a system where, unless the client claims a rectangle in the titlebar for a non-standard widget, the WM could capture input before it gets to the window to ensure reliability.
It's one of those cases where LLVM being tuned for C/C++ workloads really show up :(
Unfortunately, it's a bit more complicated than this: only `Copy` types can be freely copied, and you wouldn't want a `String` to be cloned to print it!
Haskell actually does have the ability to return "abstract classes" (with liberal use of GHC extensions): {-# LANGUAGE ExistentialQuantification #-} {-# LANGUAGE KindSignatures #-} {-# LANGUAGE ConstraintKinds #-} data Abstract k = forall a. k a =&gt; Abstract a makeShowable :: Abstract Show makeShowable = Abstract (10 :: Int) showIt :: Abstract Show -&gt; String showIt (Abstract x) = show x 
Great points. One example of "optimal code" becoming non-optimal is in Java, where many tools recommend doing `collection.toArray(new T[collection.size()])` instead of `collection.toArray(new T[0])` for performance, but in fact in recent versions of Hotspot the latter is faster. https://shipilev.net/blog/2016/arrays-wisdom-ancients/
Ahhh thanks. I'll look forward to a second compiler! 
Mozilla is throwing Rust code into Firefox at a pretty high speed right now. Since last week or so, the entirety of Servo is vendored into mozilla-central for some of the Quantum project features. It seems highly unlikely at this point that Mozilla will abandon Rust. And, to be fair, XULRunner was never a "product". Also, have you looked at [Friends of Rust](https://www.rust-lang.org/en-US/friends.html) lately? Lots of companies investing in Rust.
Are there bugs with it? I've enabled it globally in my environment on Linux and haven't run into any issues. Has made developing my web server a lot easier.
There's an issue about [Higher Kinded Types](https://github.com/rust-lang/rfcs/issues/324), and Niko has been talking about [ATC on his blog](http://smallcultfollowing.com/babysteps/blog/2016/11/02/associated-type-constructors-part-1-basic-concepts-and-introduction/) but I don't know if there's a formal RFC or not. /u/desiringmachines (aka @withoutboats) would probably, given he's the "champion" of ATC.
Full procedural macros will give you a TokenStream, and the idea is that a library could convert that into an AST. Exposing rustc's AST directly is a compatibility hazard, and one of the reasons that procedural macros' design took so long.
I personally find the lifetime syntax confusing and it's sometimes impossible to specify explicit lifetime annotations: http://stackoverflow.com/questions/41849854/lifetime-annotation-for-closure-argument
Is there a GitHub issue for this?
rust iterators are lazy, a subset of rust can be considered functional.
I actually made the same discovery a few months in my first job. Until then, it didn't occur to me that returning node identifiers in a graph API could have benefits. The beauty of node identifiers is that you can store them, serialize them (even save them to a database in some cases!) *and* you'll get better performance than with node pointers. After all, they're just integers.
Forgive my ignorance, what is a row type?
&gt; throwing Rust code into Firefox at a pretty high speed And this is really great! It means Mozilla is betting increasingly more money on Rust, that increases trust in that they won't drop it. However, if they are able to put Rust code into Firefox, they are also able to remove it from Firefox and replace it with C++25 or Swift 8.0 or whatever in 8 years from now. Also note, previously Firefox based on xulrunner as well, as well as thunderbird, but that didn't stop them to get rid of it. Regarding the friends of Rust page, I'm aware that companies do use Rust in production, but its one thing to use it, and another thing to actually contribute to the survival of the language. I mean, I'm sure that most if not all companies on that page will say "let's just rewrite rust components in some other language" if Mozilla starts viewing it as legacy thing that needs to be gotten rid of, instead of saying "let's fund the continued development of Rust". Also, its nice to have such a page, but most of the companies on that page are startups, which have a high risk to fail. In fact, if you click some of the links (e.g. Calyptech) you'll see that the website is not reachable any more, meaning the startup died.
Ah, that makes sense. Thanks!
 * Trait inheritance doesn't agree with me much. * Error handling is a bit of a pain when prototyping. * Non-lexical borrow checking. * Macros are too weak (running during type check would be even lovelier) * Lack of variadic functions. * Lack of function overloading. * Operator overloading doesn't work great. * Integers in generics. * Lack of constexpr equivalent. * I would like a slightly more verbose syntax sometimes (not ironic).
well, syn parses the code from a String, which is the only thing you can get out of a TokenStream as of Macros 1.1. This is why I said "full" procedural macros; as the restriction around structs and enums only will be gone, and you won't be parsing from a String; you'll be using tokens directly.
Panics inside proc macros are the way to go according to the [RFC](https://github.com/rust-lang/rfcs/blob/master/text/1681-macros-1.1.md): &gt; &gt; If a macro cannot process the input token stream, it is expected to panic for now, although eventually it will call methods in `rustc_macro` to provide more structured errors. The compiler will wrap up the panic message and display it to the user appropriately. Eventually, however, `librustc_macro` will provide more interesting methods of signaling errors to users. Returning an iterator is a bit of a pain indeed, but there are multiple possible solutions: http://stackoverflow.com/a/27535594/2570866 The creating of trait seems fine, to me.
If you squint a little, `Cell` means `UnsafeCell`. Like inside `AtomicPtr` for example :)
I've never heard of a row type before either, don't feel too ignorant. :P
[Here you go:](https://github.com/vks/bytecounttest) test bench_count ... bench: 29 ns/iter (+/- 1) test bench_naive_count ... bench: 415 ns/iter (+/- 6) test bench_newlinecount_avx ... bench: 55 ns/iter (+/- 0) test bench_newlinecount_avxu ... bench: 40 ns/iter (+/- 0) test bench_newlinecount_avxuu ... bench: 56 ns/iter (+/- 2) test bench_newlinecount_basic ... bench: 1,873 ns/iter (+/- 44) test bench_newlinecount_memchr ... bench: 406 ns/iter (+/- 5) test bench_newlinecount_swar ... bench: 365 ns/iter (+/- 7) 
I assume that means `[T]` on the stack? I’ve heard some opposition to dynamically-sized stack frames, but I don’t remember the reasons.
So how does the performance compare? The main advantage of using pointers is that you can have a pointer to any node of the graph and just dereference it and that's it (if the node is dead you are on your own). [*] Using indices you probably have to traverse the graph from some root until you arrive at the node you want right? How much does that cost? [*] Also, if you do have some structure in your graph (e.g. nodes are clustered in groups of N nodes, think quadtrees, octrees, volume bounded hierarchies, ...) you know that you can just offset the pointer to the first element of a cluster to access the valid node you want. Offsetting an index should work the same, but you still have to then access the node that you want from some root.
1. No stable 'select' call in stdlib to be able to select on multiple channels and timeouts which in my opinion is really essential for CSP ergonomics 2. No default arguments 
I may be misunderstanding what's going on in your screenshot (since no one pointed this out yet), but rust is not an interpreted language like python or javascript; you generally can't run it line-by-line at a command line. So even if you had the necessary libraries on your system, whatever you were trying to do would not have worked. As for solving the missing library issue, I would just go with the gnu variant of the installer. The end result would be more or less the same.
An automatic optimization can hopefully be more general than that! Applying to immutable references to copy types in general, for example.
&gt; there will be an improvement to that, after which references only live until the last time you used them Does this apply to owned types as well? For example, when would the destructor run for this code: { let x = some_type(); let y = 3; x.some_operation(); // is x destructed here y += 1; // or here? } Or is this merely a borrow checker feature, so value types are unchanged? In most code it wouldn't really matter, but it changes the semantics somewhat.
I'll try to answer with how I understand what happens 1. `let mut x = w;` is not a borrow. `x` actually moves to `w`. However, since 42 is a `Copy` type, `w` does not lose ownership and `x` just contains a full copy of `w` (42) 2. Since 42 is `Copy` it actually still retains full ownership of it's value. If `w` contained a non `Copy` type, such as a struct you created yourself and did not explicitly derive `Copy`, then `w` can reclaim ownership by moving it (`w = x`). Note that x will then lose ownership again, since only 1 variable can own data at any time. 3. `let mut z = &amp;42` is just syntactic sugar for declaring a variable, then borrowing it with another variable. Essentially lines 1 and 3 at the same time, but you don't have ownership of that variable. 4. You will learn, it's actually pretty easy once you get used to it.
That's really weird that they worked around it by implementing it for a few common situations instead of making syntax for doing it.
Ruse uses the same standardized floats everyone else does whose behavior is baked into every processor.
I knew that, there was just some finer points that I wasn't aware of.
This is one of the reasons I'm looking for a scripting language to handle new functionality in larger projects. I can prototype really quickly, change types without modifying a lot of code and it's reasonably fast. Once the code settles down, I can reimplement in Rust to make it faster and safer. It would be nice if you could do ad-hoc generics (sort of like Haskell): fn maybe_collect(iterable: I) -&gt; Option&lt;C&gt; ... Do you know if that's something that Rust may support? I like doing this in type-optional languages like Dart (I only make sure I have types in library code), so it would be nice if Rust could do this as well.
&gt; the graphs are long-lived and frequently mutated Do you have problems with the identifiers getting out of sync and pointing to the wrong data? This is what isn't clear to me when suggestions pop up about using indices into `Vec&lt;&gt;` or adjacency lists. I haven't used those methods yet but it seems like it would be too easy to pass around a copyable index/key and later it be invalidated without the users/consumers being updated. 
In my case, there was only one commit, so bisect wouldn't help. Apart from that, I needed to make changes on both sides (such as reducing size to speed up testing), so I actually used two seperate branches. I just left that part out of the post to avoid confusion. bisect is useful to tracking down a regression to a specific commit, but in this case, I already knew which commit the regression occurred on, and the challenge was figuring out what change within that commit caused the problem.
I'm not sure either, but a quick search yielded [this blog post](https://brianmckenna.org/blog/row_polymorphism_isnt_subtyping) and [this research paper](https://arxiv.org/abs/1406.2061). From what I understand, you can have a field represent multiple fields, so when you do an assignment to a field, it expands to several. Something like this: struct A { a: i32, b: i32, c: |_a, _b| a, b = _a, _b, } let a = A{ c(1, 2) }; println!("{:?}", a); // prints: A { a: 1, b: 1 } Though I'm not sure, as I haven't used a language that uses it. It seems this is a feature of TypeScript (MS's optionally typed JavaScript).
My biggest complaint, and one I don't often see mentioned is reference spam. In particular, the design of Rust essentially forces generic functions to take inputs by reference, even if the inputs can be cheaply copied (or cheaply cloned). This is a huge inconvenience when working with simple primitive types like ints, because you still have to track all your references and dereferences and manually insert the appropriate symbols and copies. Apart from that, it is just hard to keep track of how many levels of references there are in general. In normal code, this is hidden by auto-deref, but will suddenly break in the context of a generic paremeter or closure. If you write iterator chaining code, expect it to take several tries to get the refs right, every single time. There's also a small but pervasive boilerplate whenever you use an iterator method. For example, here's some code I wrote recently. let digits = b"0123456789"; assert!(digits.iter().cloned().max().unwrap() &lt; 64); There's so much boilerplate there, especially when you compare it with the equivalent Python. digits = b"0123456789" assert max(digits) &lt; 64 Apart from that, I wrote a [post](https://medium.com/@robertgrosse/my-experience-rewriting-enjarify-in-rust-723089b406ad#.cx4ddq7um) last year detailing my initial experiences with Rust. Some of the complaints are no longer valid, but most of them are.
Thanks for the suggestions. I'll rework the README and will take a look at Quickcheck. 
&gt; (My first impulse is to look into how WMs like Openbox and IceWM handles pixmap themes, since there are a lot of those already available and it'd be nice if they were machine-convertible.) Having existing themes just work with my Implementation is definitely a very good idea. &gt; While I don't like GNOME Shell, I actually talked to some GNOME guys and they said it would be possible to implement a system where, unless the client claims a rec tangle in the titlebar for a non-standard widget, the WM could capture input before it gets to the window to ensure reliability. This problem is exactly why I personally don't like client-side decorations. It would be no problem to intercept the mouse click, but that would be implementation specific, because there is no fixed position for window buttons across frameworks. This is also why I plan to support server-side decorations (title bars) as well, to have an alternative. Wayland also provides a protocol, where the compositor could ping the window and mark it is irresponsible, if it gets not answer in a reasonable amount of time. This functionality is currently not provided by the underlying wayland library `wlc` of fireplace, but it should be very straight forward to implement and use this feature to add an option to kill unresponsive applications. I am always open to further suggestions to improve the current situation and I definitely want to improve it. I have just not got around to do that, because being logged in for 85 days is rather uncommon. But that does not mean, that this should not be possible.
I needed to figure out what the changes were, so I could start removing them, and git diff was the easiest way, especially since I was adding commits on both branches. 
I'm curious, have you heard about the in-progress not-yet-complete [rewrite of the book](http://rust-lang.github.io/book/)? People have mentioned to us that [chapter 4](http://rust-lang.github.io/book/ch04-00-understanding-ownership.html) explains ownership and borrowing way better than the old book does, I'd be interested to hear what you think. &gt; I am trying to really understand mutability, borrowing and ownership, so I can move from the trial-and-error approach to assigning values in rust to 'real programming. I think one issue you're running into is that the code you have here, which *seems* simple, is actually more complicated with respect to ownership and borrowing than "real world" rust code tends to be, in my experience 😂 This is for a few reasons: - Everything is in the same scope, so none of your borrows go out of scope. Usually borrowing comes into play when you call functions or methods, which have a new scope that then ends. - I very rarely take references to integers explicitly like this, since integers are stack-only `Copy` types, so I wouldn't ever write `let mut y = &amp;w` for example, I'd just write `let mut y = w` which would copy w into y. I find it easier to think about borrowing when something can't be copied (because it's of an unknown size that's allocated on the heap), since then it's actually necessary. - Regarding "makes it so complicated to assign values to variables", I very rarely assign references to variables, again, the borrows usually happen when calling functions. 
I agree, it would make sense to have a different crate for lapack operations. It already exists as [nalgebra-lapack](https://github.com/strawlab/nalgebra-lapack) but hat not been updated yet. However, some BLAS operations cannot be on a separate crate like e.g. matrix multiplication (because it requires modifying some operator overloads).
Rust and Servo are the solution to the market share problem. Not something can be solved overnight though. However, Firefox is basically irrelevant in a decision to use Rust though.
Your english is just fine! Also it was extremely helpful that you included a video-- I got a few pieces of information that I think are related to the 2nd bug with not being able to run cargo because the file is being used by another process. I noticed you're using Atom, and that you have a plugin that provides autocomplete. There have been a few issues [like this one](https://github.com/AtomLinter/linter-rust/issues/85) having to do with the Atom plugin, racer, and cargo locking files. I can't tell if the error messages in these questions are the same ones that you're seeing, but these might be some things to try: - http://stackoverflow.com/questions/38640858/is-it-possible-to-deactivate-file-locking-in-cargo/38643019#38643019 - http://stackoverflow.com/questions/39335774/cargo-always-starts-with-blocking-waiting-for-file-lock-on-build-directory The bug you're seeing when running `rustup-init.exe`, I'm not sure what that is. I looked in [rustup's issues](https://github.com/rust-lang-nursery/rustup.rs/issues) but didn't see anything that sounds exactly like what you're seeing. I'd recommend downloading the latest version of `rustup` and seeing if it still happens, and if it does, reporting a new issue.
When I was an undergraduate we read [Computer Systems: A Programmer's Perspective](http://csapp.cs.cmu.edu/), but that might be a bit much. I'm not sure how much it will help you where the rubber meets the road so to speak.
Hey. Nice work! Welcome to the community of crate developers :) I have something to say about `GridIndex` struct: it's way too heavy. You precalculate and fill the struct with so much data, it weighs 176 bytes on the stack + more on the heap. Though what you really only need is just the length and the height of the grid. The user of the struct won't be needing _all_ of these fields, won't they? No reason to push extra work onto the CPU. So why not calculate them on the fly, when the getter methods are called?
I'm a person who loves C and all it's flaws whom would be willing to move away from C if the language is right. So far, Rust seems like the closest thing I've seen to accomplish that. We have some basic stuff for Rust landed: - Cargo support - RustUp installation as you see here - Rust Language Server support - As you type diagnostics (sometimes buggy due to rls, but generally works) - Refactor/rename variable/function/class w/ ctrl+shift+r - Symbol tree on the right - Autocompletion via racer (via language server) Some more background info from a few months ago: https://blogs.gnome.org/chergert/2016/10/
There is no bug running rustup-init :v as i said, running it actually fix the bug and "unlock" cargo Atom doesn't look like the source of the problem as i can build anything without any problem as long as i dont add dependencies in the cargo.toml file thank you for the help even if it doesn't seems like a fix :c 
Let's not move the goalposts. I messed up; there are other UnsafeCell wrappers that would work in Cell's stead, some of which are thread safe.
You mean depending on how the user uses the library, it is possible that some of the fields will never be used? And therefore, why waste all that space and CPU time when the user might not use all of them? If that is what you are implying; is it generally better to defer computation in a library? That way you don't assume how the library would be used .. And thanks for the welcome : ). Enjoying rust so far 
There are libraries that improvise this. The main problem isn't so much that it's impossible but it's rather clunky without compiler support for integers as type-level objects.
&gt; Does this mean that you can pass a struct into a function that has the given fields, and the function only has access to those fields? Yes, if structs could be used as row types, which may or may not be the case in a potential Rust implementation of row types. &gt; So, basically a trait that has certain fields defined, like (obviously won't compile) Sort of, except under that RFC, you'd have to implement the particular trait for each 'record' you want to use the function with. You don't get the compositionality that makes row polymorphism really useful, i.e. being able to do things like this: fn test&lt;R1, R2&gt;(x: { foo: u32, bar: { baz: String | R2 } | R1 }) { ... } or: fn return_row&lt;R&gt;(x: { foo: u32, bar: i32 | R}) -&gt; { foo: u32, baz: String | R } { ... }
For me the most annoying thing is that there's a handful of type system bugs (ICEs) and deficiencies related to trait resolution, and the amount of attention those are getting is surprisingly small. There's also some quality of life issues with trait bounds: e.g. there's no way to have one single trait cover both T: Default and &amp;T: Add.
Did rust end up getting higher kinded types? I remember [seeing this post a while ago](https://www.reddit.com/r/programming/comments/5c0vab/help_rust_make_the_most_informed_decisions_on_hkt/), but not sure if anything came out of it?
True, which is awesome. The first time I ran into it, I was able to figure it out, but it definitely wasn't nice.
If `some_operation` takes x by value and x isn't a copy type, then its destructor probably runs inside- but maybe not, it could also be stored somewhere else or forgotten. X won't have any destructor call instead of this function (except for panic unwinding) since it's moved out of x.
If so, then something has changed and docs will need to be updated.
Short answer: no. Longer answer: it seems likely that we will be pursuing associated type constructors instead, which give you similar power through different means.
The key parts to figuring out where a value goes are: * The ABI- it specifies which arguments (by order, by type, etc) go in which registers and which go in memory * The register allocator- it takes local values in function and decides which ones to put in registers and when * The optimizer- it makes the register allocator's job easier with passes like: * Inlining- gets rid of ABI requirements and lets values stay where they are across function boundaries * Scalar Replacement of Aggregates- separates structs/tuples/etc into individual values so they can fit in registers * Common Subexpression Elimination, Constant Propagation, etc- simplifies calculations so there are fewer values for the register allocator to manage So to answer your question about passing a `(u16, u16)`- it seems x86_64 ABIs on Linux and Windows will pass that in a single register, though that doesn't necessarily apply elsewhere. However, after inlining, it can be passed in a register anywhere.
https://doc.rust-lang.org/std/thread/struct.Builder.html#method.stack_size
Seems kinda weird to suggest a problem will be fixed by a feature you won't expect to see for another 5 years, if ever.
IIRC the XBox 1 was basically an underpowered PC, and it probably has been rooted already, so it's a question of someone writing a target spec and testing/maintaining it.
Another issue is that optimal code sometimes depends on the particular details of the workload and platform, as well as the implementation. Optimization is hard, and you'll never be perfect. You just have to do the best job you can.
true. That makes sense. I'll make the change
That would cause issues with large types with expensive clones Better if the compiler just propagates the alias-free, readonly nature of the reference to the LLVM backend
Even just having `fn maybe_collect&lt;I&gt;(iterable: I) -&gt; ...` would be nicer. The point is that it would infer types that make sense. The problem is that Rust requires you to be specific when you may not be sure what you actually want, and this is where scripting languages really shine. If Rust can infer types for parameters, it'll really improve its ergonomics for prototyping.
You don't really need to match on nibbles when implementing 6502 disassembler.
Often you want to touch multiple edges at once as part of a traversal. Also, unless you go out of your way to arena-allocate the nodes, pointer-based graphs' nodes are farther apart, and you often want to touch multiple nodes at once too. And given a workload like that, using indices means you can make them smaller than pointers and thus fit more in a cache line.
I hope it'll come slightly sooner :) But mostly I am asking whether integer generics would solve the problem, or not.
.arg only takes a single argument at a time, try something more like this https://play.rust-lang.org/?gist=3316a904aa20957b7f8fd63334b617f5&amp;version=stable&amp;backtrace=0 
Oh duh! I didn't make the connection .args() is just another method.. thank you!
Ah I see, definitely not my school then! 
I used `args`, not `arg`. You can also go without the `vec`: let mut iter = line.split_whitespace(); let mut command = Command::new(iter.next().expect("a command")); for arg in iter { command.arg(arg); } command.spawn().expect("failed to execute process");
reddit tip: indent your code by 4 spaces to make it a code block that is easily readable
Awesome thank you
I'm really happy to see this. rustup is supposed to make this type of integration pretty trivial. 
There is even .koeln!
Does the sample code in the documentation here for file io actually work? Copy/pasting it does not even compile. https://doc.rust-lang.org/std/io/trait.Read.html#method.read_to_string pub fn read_file_to_string(filename : &amp;str) -&gt; String { let mut f = try!(File::open(filename)); let mut buffer = String::new(); try!(f.read_to_string(&amp;mut buffer)); return buffer; } 
Well, you may try solving problem 1 using any way you like. Then try to read on iterators. Then try to solve problem 1 without using any loops, only iterators.
No problem! glad to help! 
Do you know if anyone's working on it? I don't recall seeing any movement on it, and it doesn't seem to fit in with any of the core team's goals. Projects like Servo have historically been willing to hack around it to meet their goals. So also no strong pressure to get it done.
welp, then yeah, all of our docs are very wrong. https://github.com/rust-lang/book/issues/455
- I could save a lot of verbosity if I could specify that functions only accepted some particular variant of an enum. - The community loves to throw around fairly academic jargon in ways that make learning the language harder than it should be (Haskell syndrome, although not nearly to the same degree). - Lack of integer generics. - Lack of Ada's extremely nice type subsetting features (stuff like `type Day_type is range 1 .. 31;`) 
Depending on your requirements, you might just want to use the mingw build (which is compiled with windows ports of the same C libraries typically used on GNU/Linux) or setup a linux environment in a VM or on the cloud for development.
There isn't really any "official support" story in GNOME for language bindings — mostly because every time we tried to say something to that effect it backfired spectacularly. :-) So, now, we try to keep an equal footing between all languages that expose the GNOME platform libraries, especially if the consume the introspection data that those libraries provide — like gtk-rs does. There's going to be a [GNOME/Rust hackfest](https://siliconislandblog.wordpress.com/2017/02/11/rust-%F0%9F%96%A4-gnome-hackfest-mexico-city-2017/) in Mexico next month, and one of the goals is to increase the interoperability of the GNOME libraries and data types with Rust. Hopefully that will increase the adoption of Rust when it comes to applications. Another area of interest is solving things like building and distributing Rust code with GNOME libraries as well — things like using `cargo vendor` to avoid downloading crates on sandboxed environments, for instance.
Hi, maybe lack of `y &lt; 512`? https://docs.rs/ameda/0.1.0/src/ameda/lib.rs.html#72
https://github.com/rust-lang/rfcs/pull/1657#issuecomment-268915104
Thanks for the help, everyone. I can see that Rust allows me to pick out members of a struct for borrowing, as in the second code example. In the first code example, it thinks I'm trying to borrow basket twice, so it refuses to compile. Perhaps this is related to the nested method call problem (https://internals.rust-lang.org/t/accepting-nested-method-calls-with-an-mut-self-receiver/4588), another problem I ran into on my project. There are proposals to fix this problem. If so, maybe the first code example could be accepted in a future version of Rust.
Gross oversight. The tests passed though. Regardless, I'm correcting that. Thanks for pointing that out : )
Tokio based hyper is coming soon^TM. I don't know how progress is though
My project suddenly no longer compiles because of an error in a crate. Specifically: /home/me/.cargo/registry/src/github.com-88ac128001ac3a9a/time-0.1.36/Cargo.toml:18:9 expected a key but found an empty string /home/me/.cargo/registry/src/github.com-88ac128001ac3a9a/time-0.1.36/Cargo.toml:18:9-18:10 expected `.`, but found `'` I looked at the file in question, everything looks fine. I deleted the local copy so it would redownload, didn't work. Any ideas? EDIT: Cargo somehow got rolled back to v0.8.0. Reinstalled, all is better now.
&gt;&gt; Am I using sensible algorithms? &gt; And this comes from profiling I have to disagree. Surely profiling is important, but sometimes the profiler can lie, or you can be misreading it. Especially when looking at things like `gprof`'s output, it's very easy to misinterpret things. People tend to dismiss intuition and analysis without profiling when it comes to performance, but having spent a bit of time writing highly optimized algorithms with SIMD/parallelism, there were lots of times when writing on a blackboard gave much better answers than a profiler. &gt; You can also solve it by adding more RAM. RAM is cheap, and modern CPUs/motherboards will typically let you get 64G RAM. Just wanted to add one thing. RAM is cheap, but the more memory your program "actively" uses, the more cache/TLB/page misses you will be getting, and the slower it will run.
&gt; No stable 'select' Coming from Go, this is a pretty big issue for me, but fortunately I haven't needed it yet, as I'm building things a bit differently with `tokio`.
Are you running any anti-virus software other than Windows Defender/MSE?
[Here's the code](https://is.gd/AnNPSe) There are two functions: 1. `get(&amp;self)` 2. `get_iter(&amp;self)` They both return the same thing `Result&lt;slice::Iter&lt;Row&gt;, ()&gt;`. Why does `get_iter()` compile fine whereas `get()` cause an error? What's going on here?
I've tried using `tokio` again recently and the experience wasn't great for me. I found it complex personally. I really feel that core team should think of stabilizing stdlib (which has more exposure and ease of use) `select`. 
The `Source` trait uses `Iterator&lt;Item = Row&gt;` as the bound for `type Iter` meaning that the iterator type is required to return owned values, which is not your apparent intention. Changing it to `Iterator&lt;Item = &amp;'a Row&gt;` fixes it. The error message is noisy and unhelpfully points to the trait impl instead of the associated type, but it does say that it expected a reference and got a struct `HashMap&lt;String, String&gt;` instead, which is a useful hint if nothing else.
I have lots of experience with Node.js and Go, so I'm comfortable writing in either style. I prefer Go's style, but Go works because of green threading in the standard library (e.g. a blocking call can tell the scheduler to call another lib). I was a bit annoyed when they abandoned green threads in Rust, but I understand the desire to keep things simple. That being said, I've found that I often need to mutex things anyway in Go (`select` just isn't quite good enough), so I'll probably end up using `tokio` and a database for the bulk of my network stuff and threads + mutexes for the bits that don't fit (e.g. computation), with occasional use of channels for signaling.
Covering features important for users is an important aspect of maturity for me, and unfortunately I have to deal with mysql. I'm glad I can use it now.
It depends on what the shared fields mean. If it is just some characteristics, then have it in two places. It is okay. If the fields may be aggregated into a struct (with meaningful and making sense name), then do that and use the struct in two places.
I've not tried it this way, but I have a hunch that its possible and that as Rust matures it'll become easier, with improved ergonomics for the core language and crates/tools that target simplicity over speed. I seem to remember there is an ongoing project to support 'true' Gc in rust too.
How would type subsetting work without introducing overhead and the threat of runtime panics for calculated or user-input values? (user inputs "day 75") 
Ada will enforce it statically wherever possible, so if it can rule out any assignment being out of range, it can elide runtime range checks. If it can't, then yes, it has to do runtime bounds checking, but you opt-in with this kind of construct, so obviously you don't pay if you don't want it. I'd also argue it's a "promise" and not a "threat", because Ada lets your trap an OOB exception and handle it gracefully. Rust would need some kind of way to guard the assignment and return a Result on OOB. Edit: theoretically, rust could go the option route and force you to prove it was in range before assigning, or provide an else path. That would make sense for a type that should only ever be in a certain range. 
&gt; Sometimes I wish I had the "compile time duck typing" that you get from C++ templates. You get to play really fast and loose with the types, so it's a lot faster to write a templated function. This comes at the cost of less-obvious compiler errors. In rust, you have to do a lot more upfront work Clearly we need rust++ :)
Don't worry, when Concepts arrive in C++ then templates will be just as awkward to write ;) C++R? I've made peace with this feature; it's much better to read a function signature and _know_ what kind of animal `T` is. Whereas, if you look at the documentation for the C++ stdlib they have to use a lot of English to make type constraints explicit.
&gt; The Rust community is constantly growing, let's hope it will get better. IMO it's more important that we have a good community. Not that we don't want growth, just at a certain point it's too fast and community norms can be eroded. One of my favorite things about Haskell is being able to interact in some capacity with the people that basically made the language. I'm newer to Rust but I definitely see a high level of discourse. 
Be careful thinking like this: just because a logical framework (in this case, affine types) is too small to fit a particular idea into it *does not* mean that that idea isn't sensible and won't fit into some other logical framework. Affine types were chosen not because they're the only sensible way to think about stuff, but because they map really nicely to hardware without needing a garbage collector. The danger in people posting that the borrow checker has "enlightened" them is that they often seem to think their original ideas were wrong or mistaken. In some cases that may indeed be true, but sometimes your idea is perfectly sensible, it's just not *affine*.
I believe this still has to construct n - 1 intermediate sets, and is possibly less efficient than doing it linearly (unless one wants to parallelise, which is probably fairly easy using [`rayon`'s `reduce`](https://docs.rs/rayon/0.6.0/rayon/par_iter/trait.ParallelIterator.html#method.reduce)), because the linear version seems more likely to avoid doing unnecessary work for elements in the intersection of 3 and 4 that aren't in 1 or 2. 
That seems like a pretty good option, I'll try it out
Heheh :D 
And I'm working on [tokio-memcache](https://github.com/svartalf/tokio-memcache/). I'd added a basic support for serde (which is should be feature-gated, I think) and slowly work on a more usable API. Also, there is no existing connection pools for tokio-powered things and creating a new one is too hard for me :(
/r/playrust
Certainly; that said, in this particular case the single ownership limitation led me to a solution to my problem that I think has ended up being simpler and more elegant. I don't feel that the borrow checker has "enlightened" me; if anything, it is a pair of very heavy shackles that really needs to turn into nice lightweight handcuffs before it has any business claiming to be an "enlightening" force ;) This post was as much about wanting to express a positive experience working with graph structures in Rust as about the improved code structure due to single ownership. Most of my other work is in domains where soft real time performance isn't an issue and I'll take a high-quality GC over paying the cognitive penalties Rust makes you pay most days. For lighting control projects, as soon as I get to a level of the stack that doesn't need to push out frames at 100 fps I'll set Rust aside and use a strongly-typed managed functional language instead.
What a coincidence - I have been doing a lot of StatsD stuff for work due to a recent rearchitecting of our telemetry stack and I also used Tokio to write a new UDP StatsD proxy (very simple pass through layer that allows for an API key and such in metrics). I've only taken a brief glance at your code (looks like it has all the basic functionality most users need)? but I'm always glad to see more UDP Tokio work out there - it's definitely the much less commonly seen protocol in Tokio work right now. Do you envision this project becoming a full-fledged StatsD server like statsite, or a more focused version like Github Brubeck that supports only a subset of functionality that you (or your org) might need?
I have never had the need to write a statsD server. A *client* on the other hand, to allow simple code to send metrics to a server, is very useful to me.
 let iter = sets.iter(); let intersection = iter.next().map(|set| iter.fold(set, |set1, set2| set1 &amp; set2)); 
&gt; Also, there is no existing connection pools for tokio-powered things and creating a new one is too hard for me :( I've found building things that are too hard for me to be a great way to learn. :)
Working on [WebRender](https://github.com/servo/webrender) stuff: [BlobImageRenderer](https://github.com/servo/webrender/pull/858) is a sort of "plugin" trait that lets you render arbitrary content to an image (Firefox will use this to draw things that WebRender does not support yet). I am also working on [support for very large images](https://github.com/servo/webrender/pull/897) (larger than the GPU's maximum texture size), which boils down to splitting a large resource into tiles which are transferred to the GPU on demand.
Would something like C#'s Expression type be possible, wher a closure can be passed to a function as a compile time expression of it's symbol tree? maybe in combination with compile-time functions returning a TokenStream.
Can't find the original discussion, but for now these two references: http://stackoverflow.com/questions/32677388/how-can-i-overload-the-as-operator-for-custom-types https://github.com/rust-lang/rust/issues/7080 
I believe I experimented with this approach versus Rust's native when I was looking into building a non-validating json decoder, Rust's native decoding outperformed this every time, though it was on mostly ASCII data so YMMV.
Nope
[Encoding](https://github.com/lifthrasiir/rust-encoding/blob/61e331b/src/codec/utf_8.rs#L94-L139) currently makes use of its variant. Due to the API difference (and yet-to-be-tackled fundamental interface overhead), the exact performance is hard to compare though.
This reminds me of this classic StackOverflow question: Why is it faster to process a sorted array than an unsorted array? https://stackoverflow.com/questions/11227809/why-is-it-faster-to-process-a-sorted-array-than-an-unsorted-array
Put your tests in a separate crate. For my custom derive thingy, I'm using a cargo workspace at the top repository level, with three crates: 1. With my library code (some of these modules include unit tests) 2. The proc_macro code; depends on my library code (contains no tests) 3. `testing` crate, which has a tests dir holding most of my tests This is similar to what [serde](https://github.com/serde-rs/serde) does.
Oh that makes sense. I knew it had something to do with references, I was just looking in the wrong place. Thanks.
BTW you can write `let vec = line.split_whitespace().collect::&lt;Vec&lt;_&gt;&gt;();` instead of the first line. ;)
Agreed. Until Rust provides stable SSE, we shouldn't use it for the solutions. Frankly, I also like the partly self-imposed standard we've had with our Rust solution to not include `unsafe` code either.
Ok... but then it would be easier and more consistent to use the `tests` folder of the parent crate.
I think you're being unfair to Isaac: I agree that his tone is not helpful, but the ad nauseum repetition of "the HashMap has to be std" is tiresome and I understand his frustration when a quick look at some of the other solutions (eg. Java) very quickly proves that that's not the case. AFAICT there hasn't been any actual Rust submission using a crates.io `HashMap`, until this [one](https://alioth.debian.org/tracker/index.php?func=detail&amp;aid=315616&amp;group_id=100815&amp;atid=413122), in which he hasn't even mentioned the 'non-custom-hash-map' rule. "Don't implement your custom hash table" when interpreted generously is perfectly reasonable: use some kind of well known hash map implementation rather than one optimized for this specific benchmark. 
It feels weird to me to put crates in a hierarchical relationship, other than through dependencies. It seems to me that, for example on crates.io, but also in how dynamic libraries have been organized ~forever, that crates mostly exist in a flat namespace. Inside them, there can be a whole hierarchy in the module system. But it seems like an anti-pattern to put crates inside crates in the filesystem/VCS repository. And of course, there are still depends-on relationships between crates.
The reason Macros 1.1 is "1.1" is that it's not done yet; IIRC, some of this "separate crate" shenanigans will go away eventually.
/u/burntsushi roll their own for the [regex](https://github.com/rust-lang/regex/blob/204e40948883606d34cbed9cd98da047b70a718c/src/utf8.rs#L83-L150) crate. I believe they did this to be *faster* then the native Rust validation. 
It wasn't to be faster. (I can probably promise you that it's not faster.) It was to have an API for decoding a single `char` from a `&amp;[u8]` (where that `&amp;[u8]` may not be valid UTF-8). Thinking about it more, if I didn't care that much about speed, it seems like I probably could just determine the length of the next encoded codepoint and use `str::from_utf8`.
What would happen in Python if `digits` were zero-sized? Also `max` function seems to be higher-level of what you wrote. It's not a problem to write this: fn max&lt;T: Ord + Clone, I: IntoIterator&lt;Item=T&gt;&gt;(input: I) -&gt; Option&lt;T&gt; { input.into_iter().cloned().max() }
The hyper master branch is using tokio, and is in a usable state. I think the remaining things to do before release are largely improving api ergonomics, and writing guides.
https://gist.github.com/ErichDonGubler/c802e066de7068241f0e6e492e6584ed has some tips. You could follow issues here https://github.com/White-Oak/qml-rust/search?q=msvc&amp;type=Issues&amp;utf8=%E2%9C%93
This is what I use for [derive_more](https://github.com/JelteF/derive_more) and it works fine.
Large function signatures are certainly a pain when you're prototyping, but they're a blessing when you're reading code. And code tends to be read much more frequently than it's written. I think the correct solution to this is to improve the tooling, so that you could write something like fn maybe_collect&lt;&gt;(iterable) { and have the Rust language server fill out the full signature after you've completed the body. Adding more implicitness to the language itself is the wrong path to take, imho. 
That's why I singled out the libraries and not the build tools. ;-]
I've not looked at your link, but I've been fighting the borrow checker too, mostly in nested loops. I learned that you can do this: for i in &amp;v { //stuff } and it just iterates through the vector like normal, but without borrowing it! It solved the issues I was having. I learned this and some other cool stuff on the rust FAQ. https://www.rust-lang.org/en-US/faq.html
rustc does know that. I believe at one point, it actually provided LLVM with the necessary annotations but it caused very long compile times and, on average, the runtime performance improvements just weren't significant enough to justify the much longer compile time.
Personally I've enjoyed the Lily scripting language, which is statically-typed and Rust-inspired. It is C-based, however, and I don't know what level of pain is involved embedding it in a Rust program. As for Rust interpreters, there's miri, although its focus is more on compile-time metaprogramming a la Nim. But in principle it could be a very cool option.
In that case, I would favor the second approach you proposed. It seems cleaner to me, but it's really a matter of personal preference, I think.
I disagree, because sometimes a clone is expensive. I'd rather it be explicit so that I could, as /u/carols10cents says, go back and remove the clone when I need the code to be faster.
Debugging or lack thereof for Rust on Windows. I've never gotten it working in a remotely acceptable way. Closest I've got is VSCode's gdb plugin but even that seems to skip lines and do odd things. Maybe it's the version of version gdb I use but really gdb should ship with Rust or as a package I can grab with rustup. Stack traces are also broken in stable-gnu on Windows . They work in stable-msvc but the VS2015 link.exe seems to complains of corrupted .pdbs so switching swaps one problem for another. Language wise, Rust is okay although I think macros suck. They may be hygienic etc. but they are a devil to write and have some curious limitations. I wanted to write a macro to refer to make another identifier, e.g this in C "Foo ## SomeOtherType" to make "FooSomeOtherType". Apparently it can't be done. 
I use Rust as a high level programming , I use String waaay more than &amp;str and heavy use of Clone. 
There's inconsistent casing on your atom definitions. Also, the constants would be more useful if they included their units - have you considered implementing a dimensional analysis in the type system, like this one: https://github.com/jesse99/runits
Thank you for these examples
this might be helpful: ~~https://github.com/Phrohdoh/rs-diesel-sqlite~~ (look at the comment below for the new link) it's a port of the diesel Getting Started project to sqlite. I remember seeing some talk that it really _should_ be merged into official Diesel to provide a built-in sqlite example. I don't know why that hasn't happened yet.
&gt; You are comparing programs which is not really valid. Why not? Grep and terminal emulators have existed forever. If a rust implementation is faster that means *something* at least. &gt;but in most cases Rust is not faster than C or even (a little bit) slower. I think the point is less "is Rust or C faster" and more "will you write faster code with Rust or C." And I think the point has always been making fast applications possible via libraries, memory checks at compile time, etc.. 
What's the point of the client signing the cipher text if it's public key isn't preshared?
I'm assuming you're talking about a Rust client? It seems that there are a couple implementations on the crates site with the most popular being [this one](https://crates.io/crates/cadence).
Quick note, the trillion prefix is "tera-", I noticed you used "terra-" in some places.
Thanks for that link. I'll take a look. I googled the words "diesel" and "sqlite" multiple times and with different variations. But I never got that link.
How about inverting the semantics so that `=` means `.clone()` and `&lt;&lt;` or `&lt;-` means move. My main concern is that when operating in the `Gc` language regime, I would like to minimize the amount of compiler complaining. I'd love a Gc language with the same semantics of Rust but without the complexity, some sort of Rust ML. While I would still love to port, or call into Rust code with ease. And just to be clear, I am not advocating a change to Rust but the creation of a lightweight Gc'd language or extension that makes it easy achieve programmer productivity while having good interop with Rust. Maybe that language is Swift or maybe it is ML. Not sure exactly.
&gt; Thanks for sharing. It'd be cool to check out your implementation but it sounds like it's closed source? For the time being, yes, it's closed source - I'm about to make some major improvements to it now that we have it running in production (refactoring the code for cleanliness, packaging for Debian/Ubuntu, performance tweaks now that I understand Tokio a lot more, and the ability to support multiple listeners and proxy endpoints). I'm hoping to open-source it in the coming months after some refactoring and moving some bits around. My company should be very cool with it because there's nothing proprietary in the code and it's not core to the business, and they love to increase their external technical footprint, so to speak. &gt; I don't have any specific plans for the library but am willing to support it if people find value in it. I had looked at other implementations in various languages to see what others did and found a lot of overlap. Time will tell, but I would be happy if it was just a reference too. Yeah, one of my reach goals for this project was perhaps to reimplement a lot of StatsD functionality in Rust once we had the API key proxy working. That's looking less likely now due to time constraints, so I'm just proxying to Statsite which is serving most of our needs (we previously used the original StatsD server and then I moved us to the now-deprecated Mozilla Heka), but I would totally work on your StatsD server as a demonstration for Rust and tokio.
Rust and C/C++ are so similar as far as LLVM optimizations are concerned that this isn't really much of an issue.
I've also been working on a dimensional analysis library, [uom](https://github.com/iliekturtles/uom) (latest work is in the [dev-10](https://github.com/iliekturtles/uom/tree/dev-10) branch), and had similar thoughts about a separate crate or feature set to make conversion factors available. One thing to consider is that this crate could be the un-typed constants with a higher level crate that provides the constants along with their units.
Running in the background is not a property of the process that gets invoked; it's a property of the code that invokes it. Consider how other Rust code can run "in the background"; can that be applied to `Command` as well?
Oooh. Hmm. So like running a process, but not waiting? What happens if you spawn a process but don't wait on it?
http://doc.servo.org/std/process/struct.Child.html#note
`2 * (*e).0` broke my neck, but the rest was correct. I am happy to understand this after 2 weeks of intensive rust practice.
which is what I want, right?
So spawn() returns a child, which I can keep track of using its PID correct?
Minor nit: parent _process_.
Hmm, well how can I run it in the "background" i.e. be able to give other commands while that particular child process is running?
Exactly what I mean
We aren't going to do your homework for you. I highly suggest reading the manual. Here are some links [Multitasking Background/Foreground](https://www.lifewire.com/multitasking-background-foreground-process-2180219) [Background/Foreground job processing](http://unix.stackexchange.com/questions/175741/what-is-background-and-foreground-processes-in-jobs) [Understanding job Background/Foreground](https://linuxconfig.org/understanding-foreground-and-background-linux-processes) None of these mention implementation details. All this has to do is about what *state* a job is in, and how it interacts with your terminal. The hint I'll give you is Background/Foreground have **absolutely nothing** to do with threads/processes. They have to do *jobs*. Jobs *can* be threads/processes, but *a job* is notion *of work to do*. You are managing *jobs*, you have to decide how the notion of *jobs*, *threads/processes*, and *background vs foreground* map together. 
If you look at the E one closely, you'll notice that at no point is anything actually given a non-zero value. So even if you have trouble keeping track of the pointers, you still know that it has to be 0 0 0 0.
Is there any general rule of thumb for how big a struct should be before boxing it? I have a struct about ~136 bytes that gets passed around a lot from function to function. Clippy said boxing wasn't necessary, but I just wanted to check if there's a general idea on how big is too big when it comes to passing/moving a struct around.
I think you're a bit misled, Command by default spawns programs in the background. What blocks (holds up your program) is the wait function. The wait function's purpose is to hold up your program until the child process is done. If you're running a command in the background, try simply not calling wait.
Gotcha
Boxing is an optimization. Like all optimizations, you need to measure to get a good answer. `memcpy` is absurdly fast, but [de]allocation only happens once per object, so it depends on how often you move it.
Don't worry about it, I'm not starving and I get paid to hack on GNOME. Instead become a friend of GNOME or something.
A: fails code review. B: fails code review. C: fails code review. D: fails code review. E: fails code review. 
Mildly fun puzzle, but pretty trivial. Nothing about this was Rust specific. Feels like the sort of thing you throw at CS 101 students to gauge how well they can follow arbitrary rules
&gt; but hopefully that doesn't apply to these numbers. Well, except for purely mathematical constants, physical constants do get change periodically as measurements become more precise. (As long as it’s not one of those “defined” constants like c or μ0.)
&gt; (*e) Was more of the problem for me since it really confused me, so I did not notice :P
Suggestions about code quality or library design are much appreciated, this is my first an currenlty only crate. Pull requests and bug reports are also always welcome.
The only difference in C++ is that you'd have to write f-&gt;clone() instead of f.clone(). I can see how the lack of explicit derefencing might confuse people, but in this case `&amp;mut C` is not cloneable, so there's only one possible interpretation anyway.
Nice write up, thanks. On a side note: If I'd like to write a website* with Rust, which framwork should I for? Iron? Copper? Rocket? Why? Thank you! *The backend, obviously. Frontend is just not my thing, sadly
Could someone tell me why is tokenizing done? Every time I toyed with parsing code I used PEG directly on the input, no tokenizing.
Weird that you can't fold on an iter.
It's not so much about what's "possible". For a program that compiles, there's always only one "possible" interpretation. That doesn't change that Rust leaves fewer hints in the code about what's going on. Note that more hints isn't always better. Readability from that perspective is a balancing act between being concise and not overly redundant, and effectively expressing intent minimizing accidental errors. C++ ends up being a little more verbose, and a little more redundant, but people can work in it effectively without knowing the ins and outs of 1400 pages of the standard. Rust tends to be more concise, at the expense of being ambiguous to readers who don't have a full comprehension of the rules. Java ends up being even more verbose than C++, and Go ends up being terser and less intuitive than Rust. They both end up going too far in their respective directions for my taste.
You can do this today: `struct Foo(Header, [Bar])`. Like `[Bar]` it’s dynamically-sized and can only exist behind a pointer (which is "fat" to include the size). But yeah, it’s easy to construct a value of such a type, especially without unsafe code.
The best way to learn something is to create it ;)
Things like [this](https://is.gd/W10NQW) need to be solved in the new macros system.
Essentially, the macro system prevents macros from interfering with variables declared outside of the macro, amongst other useful things. C's macro system is not hygienic. Rust's and Lisp's (or at least Racket's) are hygienic.
I still have to get around to watching the talk on data-oriented design, but the one you linked is more or less exactly what I was hoping for... a clean, easy-to-understand exploration of what's sitting under the abstractions that has the best potential for "just keep this in mind when you're designing" optimizations. (The cache associativity part was an especially big eye-opener, given that it would bite me hard as a direct result of *trying* to optimize for efficient memory use.) Also, as the [link](https://stackoverflow.com/questions/11227809/why-is-it-faster-to-process-a-sorted-array-than-an-unsorted-array) posted by /u/Basiliskeye pointed out, branch prediction failure is another important "be aware of this" case that a profiler which ignores pipeline stalls wouldn't see. (Though, as one of the commenters pointed out, memory-order traversal is much more important in cases like heap-allocated data structures.)
Anyone know how to write this function signature? #![feature(zero_one,step_trait)] use std::num::Zero; use std::iter::Step; use std::ops::Add; fn main() { let v: Vec&lt;u32&gt; = get_nums(5); println!("{:?}", v); let v: Vec&lt;u64&gt; = get_nums(5); println!("{:?}", v); } fn get_nums&lt;'a, T&gt;(n: usize) -&gt; Vec&lt;T&gt; where T: 'a, T: Zero + Step, &amp;'a T: Add { (T::zero()..).take(n).collect() } The idea is to return a `Vec` of n numbers of any `Zero` type from 0 to n. The error I see is: error: no method named `take` found for type `std::ops::RangeFrom&lt;T&gt;` in the current scope --&gt; &lt;anon&gt;:18:19 | 18 | (T::zero()..).take(n).collect() | ^^^^ | = note: the method `take` exists but the following trait bounds were not satisfied: `&amp;'a T : std::ops::Add`, `std::ops::RangeFrom&lt;T&gt; : std::iter::Iterator` 
Too easy. :D I think it's generally hard in Rust to write puzzle like this that wouldn't be easy to read for any seasoned Rustacean. Maybe some `impl Defer` could make it more ambiguous...
Pre-RFC: Commit to 6-week release schedule of macro system updates, counting up 1.X .
May I suggest elixir? It's a "scripting language" with a really nice application framework (OTP), conservative type checking (dialyzer), a cargo-like project manager (mix), bindings to Rust (rustler), a runtime implementation with decent performance (BEAM is not a competitor with V8, but it beats PHP5), and good support for compile-time metaprogramming.
This works (it was suggested on IRC, not my idea). I do not understand exactly why it works. &lt;edit&gt; Or rather, I don't know why your original code didn't, even after extending it with the Output = T constraint. &lt;/edit&gt; An explanation by someone more experienced would be much appreciated: #![feature(zero_one,step_trait)] use std::num::Zero; use std::iter::Step; use std::ops::Add; fn main() { let v: Vec&lt;u32&gt; = get_nums(5); println!("{:?}", v); let v: Vec&lt;u64&gt; = get_nums(5); println!("{:?}", v); } fn get_nums&lt;'a, T&gt;(n: usize) -&gt; Vec&lt;T&gt; where T: 'a, T: Zero + Step, for&lt;'r&gt; &amp;'r T: Add&lt;Output = T&gt; { (T::zero()..).take(n).collect() } 
You haven't quite satisfied the conditions on [the `Iterator` impl for `RangeFrom&lt;T&gt;`](https://doc.rust-lang.org/beta/src/core/iter/range.rs.html#562-573) (rustdoc horribly mangles them which doesn't help). You want: fn get_nums&lt;T&gt;(n: usize) -&gt; Vec&lt;T&gt; where T: Zero + Step, for&lt;'a&gt; &amp;'a T: Add&lt;Output=T&gt; Or as a shortcut you can basically copy what it says in the error message: fn get_nums&lt;T&gt;(n: usize) -&gt; Vec&lt;T&gt; where T: Zero, RangeFrom&lt;T&gt;: Iterator&lt;Item=T&gt; 
Fixed! The problem was that I copied the apidoc markdown including the relative urls, even though the README needed absolute ones.
It's already mean (sorry...)
That exact type, I think you can’t. But I think you can if: * You’re willing to make the type generic * The size of the slice is known statically at each construction site. I think this is a significant limitation that makes the feature much less useful, but it’s not the same as DSTs not existing at all. It looks like this: ``` struct Foo&lt;T: ?Sized&gt;(u32, T); let foo: Box&lt;Foo&lt;[u8]&gt;&gt; = Box::new(Foo(0, [1, 2, 3])); ``` The expression create a value of type `Box&lt;Foo&lt;[u8; 3]&gt;&gt;` which is a thin pointer to a statically-sized value. Then that value is assigned to the `foo` variable which has a different type. The value is implicitly coerced to a dynamically-sized type with a fat pointer. This coercion is documented at: * https://doc.rust-lang.org/nightly/std/ops/trait.CoerceUnsized.html * https://github.com/rust-lang/rfcs/blob/master/text/0982-dst-coercion.md * https://doc.rust-lang.org/nightly/nomicon/coercions.html The `CoerceUnsized` is unstable so you can’t (yet) extend this coercion to new pointer-like types, but using it with existing pointers like `Box` is stable. ---- If you’re willing to use `unsafe`, then you can lift the statically-known-size-at-construction restriction. There are two cases: * You type is a wrapper for another DST with no other field, like: `struct Foo([u8])`. In that case you can transmute for example `&amp;[u8]` to `&amp;Foo`. There is some [debate](https://github.com/rust-lang/rfcs/pull/1758) about whether the language should guarantee that such a transmute is legal. In the meantime, the standard library [relies on it](https://github.com/rust-lang/rust/blob/3954c70537cc78dc4a8e28c6ffa0a8ae5198387a/src/libstd/sys/unix/os_str.rs#L106). * If you have more data, you have to manually allocate memory, initialize it, then finally transmute a pointer. It’s not pretty, and it’s error-prone. See [`Rc&lt;str&gt;::__from_str`](https://github.com/rust-lang/rust/blob/3954c70537cc78dc4a8e28c6ffa0a8ae5198387a/src/liballoc/rc.rs#L420-L443) for an example. (`str` is dynamically-sized, therefore so is `Rc&lt;str&gt;`. But `Rc&lt;T&gt;` adds a header for holding the reference counts.) To allocate memory on today’s stable Rust you can abuse `Vec::with_capacity` + `Vec::as_mut_ptr` + `std::mem::forget`, and `Vec::from_raw_parts` to deallocate. You have to be careful about the memory alignment requirements of both your header and variable-length data, though. See an example [in Tendril](https://github.com/servo/tendril/blob/85595ee2765f5ed46c36201e7c257f550ace35cd/src/buf32.rs#L26-L57).
&gt; And this is really great! It means Mozilla is betting increasingly more money on Rust, that increases trust in that they won't drop it. However, if they are able to put Rust code into Firefox, they are also able to remove it from Firefox and replace it with C++25 or Swift 8.0 or whatever in 8 years from now. Also note, previously Firefox based on xulrunner as well, as well as thunderbird, but that didn't stop them to get rid of it. If putting lots of Rust code into their flagship product doesn't satisfy you, what would? It's hard for me to imagine a stronger endorsement that Mozilla could make.
[α-conversion](https://en.wikipedia.org/wiki/Lambda_calculus#.CE.B1-conversion) is the theoretical term for what is essentially the ground work for hygienic macros in lambda calculus. Put simply, renaming variables so that the true semantics is unchanged. So if there exist two variables with the same identifier, alpha conversion allows for splitting the name that matches for two distinct variables into two distinct variables.
I wanted to use this for automatically deriving (among other things) Add, but gave up because I need to be able to add by borrowing. Do you envision adding support for this somehow, /u/Jelterminator?
Exactly
I'm working on [an enhancement](https://github.com/rust-lang/regex/issues/341) for the regex crate to support nested character classes and intersections. This will enable regexes like this: [\w&amp;&amp;[^\p{Greek}]] That would match word characters that are not Greek.
Yeah. Everybody knows that c = ħ = k_e = k_B = 1, and that’s not going to change. ;-)
It is not desireable, since I am working with bignums, which can grow arbitrarily large.
Not necessarily. Whether or not they're pure depends on the body of the macro. Hygiene is about how identifiers in the macro interact with identifiers outside it. Here's a [simple example](https://is.gd/GGPT6R). Notice how the two instances of `y` don't clash with each other. If you remove the `let` from the macro (leaving just `y = 1`), it will refuse to compile. Compare and contrast to this roughly equivalent [C version](https://godbolt.org/g/iImQY2), where the macro happily reuses the y variable.
Once you defined `max` with similar semantics to the Python counterpart, the Rust code is almost identical to the Python code: fn max&lt;T: Ord + Copy, I: IntoIterator&lt;Item=T&gt;&gt;(input: I) -&gt; T { input.into_iter().max() .expect("can't find maximum of empty iterator") } fn main() { let digits = b"0123456789"; assert!(*max(digits) &lt; 64); } The boilerplate in the definition of `max` is there for a reason: * Rust has static types and no type inference beyond the boundaries of functions, so `into_iter` and the trait bounds are required. * Rust does give you a choice between panicking and using the error as a value, so `expect` is required. 
A limitation we've hit with serialisation (with `rustc_serialize` actually, but I think it also applies to serde) is that even serialising fixed-size input to binary in a pre-allocated buffer is only possible with an API which returns a `Result`, i.e. advertises that it _could_ fail. (Note: input is some complex types, desired output is a byte sequence.) I don't know that it _would_ ever fail, but my boss is very keen on not using `unwrap` which makes error handling harder than it should be.
U don't have a Linter in VS, also the performance is __currently__ pretty much the same so don't try to spread out __outdated__ information. I use both depending on the language and task. So i use VS for debugging rust stuff via GDB
If you know that something will always be `Ok` and never `Err` (specifically, if it would be a bug if it was `Ok`), then using unwrap is more than okay. Some people (not me) don't like unwrap as it doesn't give nice error messages, they tell you to do expect and include an error message instead.
In my experience Serde's deserializer is very flexible and forgiving. If I write data that will be fed into Serde by hand, it understands pretty well what I'm trying to do. Because of that I don't really care in what "style" things are serialized, as long as the end result is human readable. But I can see that Serde is probably a giant pain in the ass if you're trying to serialize JSON or XML for use in other software.
i am not entirely sure about your usecase, but maybe [void](https://github.com/reem/rust-void/blob/master/src/lib.rs#L90) can help you, it provides you with a void type, which can never ever actually exist, and if you have a result with void as error case you can safely unwrap it, as the void can not possibly exist, and the result has to be the ok type.
Thanks! The last case is what I was hoping for, so the links to `Rc&lt;str&gt;::__from_str` and Tendril's `Buf32::with_capacity` were helpful. It's unsafe and ugly enough that I should do my homework and ensure it's actually worthwhile before using that technique, but it's nice to know it's possible on stable Rust.
You're welcome.
Fold needs an initial value.
This is not the rust game subreddit, this is the rust programming language subreddit. You are probably looking for /r/playrust
This looks very cool! I could imagine using it for Arduinos or AT90CAN128...
Ah, I get it now. This is a way to provide an initial value that isn't the empty set in order to save some cycles.
&gt; It also may not work well with projects that have build scripts due to https://github.com/rust-lang/cargo/issues/3739 Call `cargo` with `--target x86_64-unknown-linux-gnu`. That way it won't build build scripts and dylibs with `-Z sanitizer` (and the other RUSTFLAGS). (This trick to avoid that Cargo "feature" is documented in [rust-san](https://github.com/japaric/rust-san#how-to-use-the-sanitizers) too)
Will do, thanks
Thank you!
I was talking about the game, my bad. Thanks for the quick answer though.
So, did it work?
This is a neat trick, thanks!
I know why the boilerplate exists, but that doesn't mean it isn't still annoying.
The bignums implement addition on references to them, but my struct needs to own them for anything to make sense. (It is a simple newtype.) When I sit down with this project again, it's not unlikely that I will write up a proper issue for you, with examples and everything :)
A PEG handles lexing and parsing as one, given a description of a language. Tokenizing is like splitting a sentence in a string into a list of words and punctuation - it means you can treat the input as a list of objects instead of a list of characters, and write a parser on top of that. It splits up the complexity of parsing when writing a hand-crafted parser. Somebody more knowledgable will likely have a better answer for you!
Awesome news! I think nonlexical lifetimes will really improve general usability.
No problem, it's a regular confusion ;). Have a lot of fun!
This is a self inflicted problem not really a shortcoming.
The biggest one that always bites me is: if &amp;some_variable { let blah = *some_variable; /// etc } Currently this isn't allowed, because `&amp;some_variable` is borrowed for the entire block. This comes up a lot when working with collections, like HashMaps: let mut map = HashMap::new(); // do stuff with `map` if map.has_key("some key") { // do something with map } else { map.insert("some key", "some value"); } ^ This isn't allowed, and was pretty much the whole impetus for the `Entry` api
NLL basically makes the Rust borrow checker smarter. There are currently cases where you the programmer may expect something to compile, but the borrow checker isn't smart enough to recognize what you're doing is safe. This is because right now borrows are tied to lexical scope, which is a slightly more restrictive rule than is necessary, and is not quite what some people expect. NLL relaxes things and thereby allows previously safe but unverifiable programs to pass the borrow checker.
I think the lack of static typing in Python is annoying. ¯\\_(ツ)_/¯
The existing problems not taken from numerical computing have been converted into array/number problems by clever programmers, because computers provide efficient solutions to array/number problems.
I cannot thumbs up this post enough.
Of course! The thing is, there's a little history with this benchmark and Rust, with a lot of people being unhappy with the perceived rules of the problem and a bunch of arguments between some preeminent rustaceans and the maintainer of the benchmarks game. So I thought we should all enjoy this chapter being closed... FOR NOW...
That would be me :P, but other than changing the hashmap dependency and removing some useless allocations and utf-8 validation, I didn't do much.
Well, I'd assume you also evaluated the performance :) Thanks in any case!
And spent a stupid long amount time defending the implementation on the tracker https://alioth.debian.org/tracker/index.php?func=detail&amp;aid=315616&amp;group_id=100815&amp;atid=413122 ... edit: This is not to suggest Isaac was being unreasonable. I think the conversation there was completely within parameters. If I got impatient is just cos I was excited :)
With judicious use of `restrict` and whatnot, I think you should be able to get C code to codegen the same as Rust for such little benchmarks. The benefits of code reuse and reasoning about ownership are visible in larger projects where some of these things are too risky in C.
FWIW, that particular code does work: it's fine if something is borrowed just for the condition of an `if` (in fact, I suspect one *can't* observe the improvements from NLL with just a normal `if`). The problem occurs with something like match map.get("some key") { Some(value) =&gt; { ... } None =&gt; map.insert("some key", "some value") } where there's a reference (`value`) used in one (or more) branches, but not in others. The `if` version is one work-around for the above, as is `Entry` as you say. 
[This earlier blog post](http://smallcultfollowing.com/babysteps/blog/2016/04/27/non-lexical-lifetimes-introduction/) by Niko explains some of the problems with the current system.
I'd love to be wrong about this, but I think you can massage small amounts of C code to generate any LLVM IR you want :P.
Very diplomatically done 👍
You're right, my mistake. For some reason I thought the `if` version sugared down to your `match` version, but I was thinking of this: if let Some(value) = map.get("some key") ... I should have checked the code in the playground first! *facepalm*
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/programming] [Rust is now the fastest language on k-nucleotide! (thanks to \/u\/bluss's OrderMap) • r\/rust](https://np.reddit.com/r/programming/comments/5vddq2/rust_is_now_the_fastest_language_on_knucleotide/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
Kudos to both /u/nwydo and /u/neutralinostar! However, there are still a bunch of slower benchmarks. I think we *should* be able to get much faster n-body by tuning the unrolling cost model. At least in my tests I was able to beat the fastest GCC entry on my machine.
The rules there prohibit explicit unrolling, we need to be smarter :)
In your example, it should be `mut p`. Apart from that I note that `'1` and `'2` overlap in `C0`. Is this a problem? If not, how (and where) do we represent non-overlapping regions as constraints?
Yes I've read it the same way. What I wonder is if this is really faster than the normal emulation.
I am undergoing the traditional, rust beginner dive into the lifetime rabbit hole. If you just want to look at the code which fails due to my logically inconsistent lifetimes, you may find the [playground here](https://is.gd/XLhS2z). The code is trying to build/memoize a cache of [integer partitions](https://en.wikipedia.org/wiki/Partition_\(number_theory\)). As a learning experience, I wanted to build the integer partitions as linked lists, which span multiple vectors in the cache. An example may help clarify this: parititions(4) -&gt; [[4],[3,1],[2,2],[2,1,1],[1,1,1,1]] parititions(5) -&gt; [[5],[4,1],[3,2],[3,1,1],[2,2,1],[2,1,1,1],[1,1,1,1,1]] partitions(5)[4] = partitions(4)[3].push(1) // [2, 1, 1, 1] = [2, 1, 1].push(1) As you can see, a partition may be built by adding one more element to the list of integers in a previous integer's partition. This inspired the linked list solution. I know this is overly complicated, and likely less performant than just using vectors, and copying the small amount of data. But it seemed like an opportunity to grow my understanding of lifetimes.
I was thinking it might be nice to expose the `Sz` type parameter at the `HashMap` level, so that I can get the sweet gains of packing hashes together with indices on 64bit as well if you expect &lt;4B elements.
To clarify first: https://docs.rs/ordermap/0.2.7/src/ordermap/.cargo/registry/src/github.com-1ecc6299db9ec823/ordermap-0.2.7/src/lib.rs.html#87-103 The packing is used by default on 64-bit architectures if the hashmap's capacity fits inside 32 bits. This is a bit silly, it's like having dark code in the library that's never used. We are paying for some indirection in there to pick the 32 or 64 bit case, when the 64-bit case will almost never ever occur (who ever makes a hash map of 4e9 key value pairs? Not bytes, k-v pairs.) I haven't considered this packing as a significant innovation of the ordermap, but maybe I should.
He's talking about the cost model in the compiler for when it decides to unroll. So no explicit unrolling.
No. Fair point.
Oops, my bad :(
I know. I'm thinking of a different strategy.
What's the data like in the benchmark? Rust's regular HashMap should beat OrderMap in lookup benchmarks, at least when the map becomes *big enough* or when cpu cache is no help.
&gt; Stop calling things lies. … this subreddit has rules. Is it OK to post lies in r/rust/ ? (Genuine question).
Ah do you mean tweaking the llvm opt pass parameters? 
Ah, I hadn't realized, that's really cool. That branch is gonna get predicted like hell though, so I imagine that indirection isn't the end of the world!
&gt; ^ This isn't allowed, and was pretty much the whole impetus for the Entry api The Entry API is useful regardless of NLLs, because it avoids having to look up the proper location for a key twice.
Well that's kind of the whole problem. As I understand it the guy that runs the benchmark game *does* try to only accept 'idiomatic' solutions. But then the argument becomes "what is 'idiomatic'?" and where do you draw the line. I think the only true solution would be to get a lot of people to write the same program in different languages, and then benchmark them. You'd need to have some way of adjusting for people's different skill but it could be done. It would be a ton of work though.
The jit compiler (or subroutine threaded interpreter) is on my machine about a factor of 2 faster than the emulator without jit compiler. Here is the output from my machine: &gt; cargo build --release --no-default-features &amp;&amp; time ./target/release/avr-vm ./test/jump/jump-time.bin &gt;/dev/null Finished release [optimized] target(s) in 0.0 secs ./target/release/avr-vm ./test/jump/jump-time.bin &gt; /dev/null 1.53s user 0.02s system 100% cpu 1.542 total &gt; cargo build --release --no-default-features --features jit &amp;&amp; time ./target/release/avr-vm ./test/jump/jump-time.bin &gt;/dev/null Finished release [optimized] target(s) in 0.0 secs ./target/release/avr-vm ./test/jump/jump-time.bin &gt; /dev/null 0.72s user 0.06s system 98% cpu 0.795 total With the jit compilation you don't need to do a match for each instruction, but can run one basic block in one go without doing a switch in between. Also the check for interrupts happens then only after each basic block, not after each instruction, which also helps the performance. But I agree with your point, that the jit compiler doesn't do very much in terms of code generation at the moment and could do a lot more, like removing unneeded calculation of flags,...
&gt; there appears to be bad blood between the two of you Not on my part, I'm only here to show those claims are untruthful and have them corrected (and have the repetition of those untruthful claims corrected).
&gt; there appears to be bad blood between you and Steve Not on my part, I'm only here to show those claims are untruthful and have them corrected (and have the repetition of those untruthful claims corrected).
This isn't constructive. Regardless of your opinions on the matter, he has a hard job. We should remember that. (This is not me trying to excuse anything by the way. I'm just suggesting a touch more empathy.)
As best I can tell, from going through the process myself, the benchmark game web site doesn't update everything at once. It happens piecemeal.
Please be constructive.
I try to be as idiomatic as possible in my submissions. For example in this benchmark, the Iter structure abstract the encoding of the substring, making gen_freq trivial. Not hot-spot code is written to be simple to read. You can also see pidigits abstracting the generation of the digits in an iterator, and providing a safe abstraction over the raw gmp c function. All that at zero cost.
Is there a way to iterate through all possible values of an enum type (assuming they are simple values with no parameters)? Say, I have: enum Direction { RIGHT, DOWN, UP, LEFT } I want to write a brute-force function that exhaustively tries out every possible value (without caring about what it is or what it means). What is the most elegant way to accomplish this, without having to actually hardcode the values into the function (ie something that still works if I decide to add more values to the enum later)? Is it even possible? Ideally, I am looking for a way to iterate through the values with a simple for loop... or does Rust provide no way to do this automatically?
Yeah, I'm saying this is the case if all we're doing is lumping things in orders of magnitude. I agree we should be a bit more precise with our generalisations even though benchmarks are quite flawed. I'm also saying being more precise means acknowledging C++ and C don't have the same performance. I can't actually find the mean on the site anywhere, I just see the rough plots and the individual benchmarks. Where do you see a number given for the mean? Or did you calculate it yourself?
https://github.com/rust-lang/rust/issues/28224 you might be more successful if you turn off jemalloc
Thanks for taking the time to compare the execution time, this satisfy my curiosity ;-)
I think we hit this https://github.com/rust-lang/rust/issues/36481 When I tried to parallelize the creation of the hashtable (simple mapreduce as in the c++ version), the performance was dramatic. The data, as generated by a simple rng, loop about 893 times. Thus the map grow the first time, and then just lookup the 892 next times. I suppose that this bad performance is thus due to this bug. But OrderMap does not have this bug thanks to its iteration order. Thus I suspect that's because of that. Just intuition, nothing verified.
If aliasing rules trigger an unrolling optimization that the C compiler can not usually do, then we can beat C because the benchmark game prohibits explicit unrolling.
[How CPU load is measured](http://benchmarksgame.alioth.debian.org/how-programs-are-measured.html#cpu-load) [How programs are timed](http://benchmarksgame.alioth.debian.org/how-programs-are-measured.html#time) 
So you are guessing visually as well? There are no ticks between 1 and 3. Looks like C++ is about 1.5 and Java is about 2.5 to me when I look more carefully now so you would be correct, wouldn't take long to calculate and make sure.
The static html is generated as infrequently as practicable.
After re-reading http://accidentallyquadratic.tumblr.com/post/153545455987/rust-hash-iteration-reinsertion I'm much less confident that's even related. Maybe because of the poor hash we ended in a similar region.
Unfortunately, it looks like C varargs does not support a dynamic number of arguments. Basically, it starts at the address of the last explicit argument plus its size (at least one is required) and then iterates up, with the number of items and their types being implied elsewhere, usually based on the explicit arguments given (in this case, the `int` argument appears to just be the number of items in the list). You could fool it by changing the stack pointer to the starting pointer of your `Vec` and sticking the first argument at the head of it but that's just a bad idea in general (or reverse the `Vec` and stick the `int` at the tail, I'm not sure because it depends on how the stack works, another reason why this is a bad idea though it might be fun to experiment with). Is [this](https://github.com/orangeduck/mpc) the library that you're using? If so [the definition of `mpc_cleanup()`](https://github.com/orangeduck/mpc/blob/master/mpc.c#L1503) is really stupidly simple. It just calls `mpc_undefine()` and `mpc_delete()` for each item in the list (and amusingly, copies the varargs list to an allocation first for some reason). So instead, you could just iterate over your `Vec` and call these two functions for each item yourself. (It would also save an allocation as mentioned previously.) Addendum: I just realized the allocation is because it looks like it has to do all the `mpc_undefine()`'s *first* before calling `mpc_delete()` on any of the items, so the allocation makes sense in hindsight as a shortcut to avoid iterating the varargs list multiple times, which would be more verbose (and may be detrimental to vectorization, though I can't imagine that really coming into play here--I think the author was just being lazy, which I can't really fault them for, I'd probably do the same).
I'd probably pass the pointer to an intermediate C function that synthesizes the `va_list`. I'm not sure how easy this is. Basically, what would you do in C if you had a vector and needed to splat it into a varargs function? Do that, in C, and pass it the vector from Rust. If the C version can be written in pure Rust (i.e. if it doesn't use the `va_foo` macros) you can try that too.
&gt; I just think it could be clearer, that's all. A fine expression of a perfectly acceptable opinion. If that *was* all, then I would not need to ask you to correct your comment, but that isn't what you've claimed.
&gt; since the whole point of Rust is to eliminate leaks. It isn't; Rust does not guarantee no leaks. It is possible to leak in safe Rust by creating an Rc cycle or `mem::forget`. It's harder to leak by accident, but it's still considered "safe" to leak. valgrind with jemalloc turned off is pretty much what I would use.
&gt; So you are guessing visually as well? No, I have access to the calculated geometric mean values that are shown on the chart. (I admin the website).
On Linux and a recent nightly you can try [LeakSanitizer](https://github.com/rust-lang/rust/pull/38699).
You guys should post them along with the charts.
I want to second flexibility here. Definitely so with Json, as everyone else will mention. I was surprised by how flexible it seemed to be for xml (deserialization only), though I only dabbled with that for a day or so. It was smart enough to let a property correspond to either an attribute or a child element, which was one of my first concerns. However, I did *not* get to see if it could handle lists of multiple types of elements, which would make life pretty hard. 
Thank you! Exactly what I was looking for.
Hihi. Yeah, most physicists currently believe the most important property of all is constant, based on a single experiment. I know of 11 publications that don't support this claim :)
That applies to some domains but not others. I'd say Rust makes certain abstractions possible in large projects but for very small things (e.g. benchmarks, among other things) that isn't necessarily the case. 
I was confused by this as well. If it is intentionally *not* `mut`, then this system significantly changes the semantics of immutability.
People do lie, but it's only against the rules to say so ;-) Shouldn't it be against the rules to lie, because lies are not constructive?
I took the time to rewrite the code you showed me (and is the 'final' example in your article) into one that I would probably write as a first draft: https://gist.github.com/seanmonstar/7f70c703932459452e3f0f0b13ee623a I haven't benchmarked this, nor tested the write and delete methods (I bet the file name isn't chosen correct), and there's a couple other `TODO`s sprinkled around. Here's an overview of some of the differences: - Only 1 reactor thread, which is the main thread. It's run by `Http::new().bind()`. - A `CpuPool` is created, using the number of threads you were using before. - All file IO operations are sent to the `CpuPool`, which allows the reactor thread to keep handling HTTP stuff. - Depending on how much it is blocking on actual file IO, it may make sense to increase the thread pool even bigger, so that the cores can work on other operations when 1 thread is blocked on a lengthy file system operation. Based on what your server is actually doing, it doesn't look like there is need to have multiple threads doing the HTTP stuff. It looks like your server would spend far more time in `crc32` and `std::fs`, and so I tried a setup that would dedicate as much of the machine to that as possible.
Unfortunately some people would make silly claims like A is 5x faster than B.
My (likely limited) understanding is that MIR optimization is mostly targeted at reducing LLVM translation/optimization time, not improving performance (yet).
That doesn't support what OP is trying to do here. And anyway, it seems that calling a variadic function works natively now: https://is.gd/gXtCvx
You have a self-referential data structure. #[derive(Debug)] enum PartitionLink&lt;'a&gt; { End, More(&amp;'a Partition&lt;'a&gt;), } #[derive(Debug)] struct Partition&lt;'a&gt; { value: i64, rest: PartitionLink&lt;'a&gt;, } type PartitionCache&lt;'a&gt; = Vec&lt;Vec&lt;Partition&lt;'a&gt;&gt;&gt;; This is fine in theory, but no durable immutable borrow can coexist with the mutable borrow of the `PartitionCache` object. Try making `PartitionLink::More` into a `(usize,usize)`, it should help substantially. For what it's worth, [here's](https://is.gd/6rfjVp) where I got until I noticed that problem (it compiles, but I had to use `PartitionLink::End` instead to make it work)
Haha OK, fair enough. But you did seem to disagree with me interpreting the results loosely which was just taking from it that C++ and C don't have identical performance. Not sure what insights are acceptable if not that.
Yeah, I've tried this in the past, with good results.
You'll definitely need a platform specific library. If you already know how to do what you want from C++ you could make a wrapper around that API in Rust and use it.
I'd bet it'll also make the compiler's optimizations more reliable because it will depend less on LLVM figuring stuff out.
[removed]
A hashmap has certain guarantees it provides for it to be correct. E.g. inserting an element at a key means that it should always be retrievable with that key until a new element with the same key replaces it. Deleting an element means that you shouldn't be able to get it back later. That kind of stuff. A hashmap implemented in 100% safe Rust could still possibly have memory leaks (unlikely), or correctness bugs, but it can't have memory safety issues like use-after-free or other things which cause segfaults and security issues. (Correctness bugs also can cause security issues, but many memory safety bugs give you carte-blanche to pwn any part of the application, unlike a correctness bug in a hashmap, which would usually be more localized in exploit power)
So are correctness bugs just saying that the implementation isn't 100% correct, or are they bugs from some source of non-deterministic behavior?
I've been reading up on the references provided. Thanks. So, this `TokenStream -&gt; TokenStream` signature is not hygienic because it does just a simple replace?
If rustc starts to dip it's toes in some of the insanely aggressive cache hit optimizations that icc employs I will be over-joyed. Especially where instances of strict-aliasing are provable due to rust's type system. I don't know how much discussion has revolved around these sorts of optimizations. Of course the Intel C++ Compiler is proprietary. But it would be neat to see if rustc can start competing with it, not just the gnu compiler collection.
GCC does usually result in faster code than clang. However, the situation is a little confusing because Apple stopped updating GCC at 4.2.1 (10 years old now), so, before they switched gcc to just be an alias to clang, comparisons on OSX would be between an old GCC and a new clang. However, there are definitely some cases when clang/LLVM happens to do a better job than GCC, maybe this is one.
`GetAsyncKeyState` is part of the Win32 API, so you'll need something which wraps that. If you want to access it directly, the [`winapi`](https://github.com/retep998/winapi-rs) crate provides a direct translation of the API as `user32::GetAsyncKeyState `: pub unsafe extern "system" fn GetAsyncKeyState(vKey: c_int) -&gt; SHORT -- https://retep998.github.io/doc/user32/fn.GetAsyncKeyState.html If you'd like to go a little higher-level (and make your code easier to port to Linux as a side-effect), I'd suggest [pancurses](https://github.com/ihalila/pancurses) or the [Cursive](https://github.com/Gyscos/Cursive) TUI library that supports it as a backend. (If you're not familiar with the term, a TUI is one of those GUI-like things that runs in your terminal. (GUI, but replace "Graphical" with "Textual") The Cursive link has some screenshots like [this one](https://github.com/gyscos/Cursive/blob/master/doc/examples/menubar.png).)
&gt; We had looked around for widely-used, open source library we could adopt and settled on stagefright. A billion Android handsets couldn’t be wrong, we thought. Given that libstagefright had several highly publicized vulnerabilities published not long after, was this deliberate irony?
Well proper blaze support would require convincing Google to adopt Rust, which is not likely in the forseeable future. But you can add custom rules via Skylark, so it should be possible to hack something together in the mean time. 
I meant getting it into the Big 4 (C++, Java, Go, Python). That's a much bigger hurdle. I think Rust has a better shot than anything, since it offers compelling benefits that no other language provides, but I'd guess that it would take several years at least.
It doesn't need to be in the Big 4 to have Blaze support.
(IANAL, and this is not legal advice.) &gt; I think it's ok to change and add to the official logo Read the license (which is linked to on the page you linked to); it allows modifications, but there are terms which apply (and you must obey those terms). &gt; The Rust and Cargo logos (bitmap and vector) are owned by Mozilla and distributed under the terms of the [Creative Commons Attribution license (CC-BY)](https://creativecommons.org/licenses/by/4.0/). If you follow the link, &gt; [You are free to] &gt; Adapt — remix, transform, and build upon the material However, note that the page goes on — and *you should read the entire license*¹, not just what I'm quoting here — &gt; Under the following terms: &gt; &gt; Attribution — You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. Also note (as the page you link to has in bold): &gt; **Note that use of these logos, and the Rust and Cargo names, is also governed by trademark; our trademark policy is described below.** You should also read and understand the trademark policy on that page. ¹also note that I'm quoting the human-readable summary, which is not, as the human-readable summary notes, [the license itself](https://creativecommons.org/licenses/by/4.0/legalcode). It is the license itself that counts, not the human-readable summary. (also, Rust, &gt; This is the most permissive Creative Commons license, is it? I consider CC-0 more permissive than CC-BY.)
This would be just as hard even if we used makefiles to compile Rust. You need to teach the build system about the flags, the configuration, everything. With a complex build system like Firefox's it would still be hard.
I can't offer much help, but your code looks pretty good to me. I actually learned a few tricks from reading it.
Cool, I already reference to your project from my page. Initially I started *simple_units* because I wanted s.th. that works with stable Rust. Now with your rmdim branch it seems to also work with stalbe Rust.
https://doc.rust-lang.org/stable/std/process/struct.Child.html#note (w.r.t lack of Drop). To your question: strictly speaking: not always. It depends if the parent or child dies first. To go back to definitions: a zombie process is one that has been killed or exited, but whose parent process has not `wait()`ed on it yet to clean it up. Technically all processes that exit enter the zombie state, but usually that's only briefly, as the parent process is typically written to clean it up ASAP. The `Child` value going out of scope is purely a concept to Rust - but a "zombie" is a UNIX/kernel concept. However, if the parent process dies (as others have mentioned), then the child process (zombie or not) is re-parented under the init process (pid=1). And one feature of init is that it will `wait()` on child processes that exit/are-killed. IIUC - this feature of init is to clean up after buggy parent processes that don't clean up, or ones that have been killed before they can clean up.
Thanks, I'll have to add a prefix to some constants (other names are also affected).
I tried to re-install Rust, and now cargo generates "spurious error" due to Microsoft HTTP service's internal error. Sigh...
Yes it does. I'm hoping to have that branch published as version 0.6 very soon, but I want to make sure that I've documented and tested things more first.
`TokenStream -&gt; TokenStream` is fine, because tokens inside a token stream can carry hygiene information. The problem is that the full `TokenStream` API is still a work-in-progress, so at the moment it only provides implementations of the `Display` and `FromStr` traits, which serialize tokens to string and back. The problem is that serializing tokens to a string in Rust syntax loses all hygiene information. So if the `TokenStream` returned by a macro is parsed from a string and includes an identifier, there is no way to tell if it’s one introduced by the macro itself or if it refers to part of the input, and there could be name collisions between the two kinds.
Nah, I think it's more than fine as it is.
I tried it again with `--release`, and it does appear to optimize away the whole allocation. That was unexpected.
Rust can pretend to be C code, so any system that can call C code can call Rust code. However, just because it's possible doesn't always mean it will be convenient, or safe, or ergonomic.
There is also a native Rust TUI library in [termion](https://github.com/ticki/termion) (which is a supported backend for Cursive by the way).
I think so, especially given the "full of vulnerabilities" line below.
This will never happen, because modern languages have richer runtimes that are mostly OS agnostic, and OS specific package managers are by definition OS specific. No one has time to write N packages for each library, and even then, only the mainstream OS or distributions would be covered.
Hmm, so developers using this string deserialization API should 'namespace' introduced identifiers to minimize the chance of collisions. `mymacroname_hygiene_value` or something. Ouch. 
&gt; We resolved this by building a Rust super-library that just re-exports all the public interfaces of all the Rust code modules we actually want to have available. Heh, [exactly what I've done](https://github.com/myfreeweb/freepass/blob/fad2a1fd592cbe6a9d8388a5fb7e8b4703ebeccd/capi/src/lib.rs#L7) in a much smaller project…
The original poster was asking about a Win32 API. If Termion gained Windows support since I last checked, they still haven't updated their readme.
That is not what I meant! I saw how they are measured. My point was, if the C variant is slower but uses less CPU cycles/time, how can we compare the results? Is the C code slower because it is waiting on memory?
&gt; in C if you had a vector and needed to splat it into a varargs function I don't think this is easily possible in C (to my knowledge, of course).
At the moment rustc itself does zero optimizations, it merely passes the code unoptimized to LLVM and hopes for the best. On the other hand, LLVM is insanely good. In my opinion it's better than GCC. I've seen chained iterators with closures that were automatically inlined, unrolled, and turned into 128-bit wide SIMD instructions that I didn't even know existed. If you feed the correct flags to rustc, I'm fairly certain that LLVM can emit code that uses the zmm-registers too. So the answer is yes, the compiler is good enough to do these things today. 
Your first suggestion (to 'hack' the stack) sounds fun but also sounds like an implementation dependent bug. I put it on my list of things to try one day, but for now I will do it the way you suggested and simply do the work myself instead of calling that function.
Someone should compile a set of online Rust quizzes as a complementary project to Rust by Example, call it Rust by Quizzing. I think it could be immensely educational.
I _always_ freaking mis-type this, I don't know why. Thanks.
Hi, I have a gameloop where I poll events from a window : for event in self.context.window.poll_events() { // stuff Now I wanted to call a method inside this for-loop that has this signature: fn handle_systemmessage( &amp;mut self, system_message: Option&lt;SystemMessage&gt; ) { and I got this error: error[E0502]: cannot borrow `*self` as mutable because `self.context.window` is also borrowed as immutable --&gt; src/application.rs:77:21 | 65 | for event in self.context.window.poll_events() { | ------------------- immutable borrow occurs here ... 77 | self.handle_systemmessage( system_message ); | ^^^^ mutable borrow occurs here ... 107 | } | - immutable borrow ends here I kinda understand why I get this error, but the question is what to do here. I solved it by caching the system_message in a vector and handle those messages after the poll_events-for-loop: for msg in sys_msgs { self.handle_systemmessage( msg ); } That works, but it feels kinda 'hacky'. Is there a better solution to this? 
You can move `handle_systemmessage()` to a free function and pass the parameters from `self` that it needs, though `context` will still only be accessible by immutable borrow. 
Would it be possible to generalize the combinations using a struct with two boxed trait properties? So instead of MetersPerSecond it might return a Speed or a Ratio type?
On my fedora virtual machine the above function emits an identical (well abi not-with-standing) movl %edi, %eax retq Identical rustc version. Identical build flags 
[Code](https://play.rust-lang.org/?gist=bb1dee0372038ecdfe5d9cb4a7b8d5f5&amp;version=stable&amp;backtrace=0) I am optimistic that this is an easy question. When I attempt to compile this file, I receive a compiler error on line 60. I am under the impression that `values[index]`, where `values` is of type `&amp;Vec&lt;f64&gt;` (through Deref coercion from `std::cell::Ref&lt;'_, Vec&lt;f64&gt;&gt;`) and `index` is of type `usize`, should return a `&amp;f64`. Instead, the compiler seems to think it returns a `f64`. I'm not sure how this can be the case. (As an aside, the intent of this code is that `Tail` should be a lazily-populated array of values which are recursively generated from an initial value. Is there a better way to do this than I have done?)
That's great! Are there any possibility for it to replace libiberty in gdb later on?
Probably not in the GNU project...this is Apache/MIT whereas libiberty is LGPL.
Order of evaluation of function arguments in Rust is, I believe, not specified, but the implementation (rustc) evaluates left to right, and some code does rely on this behavior. I believe the indication has been that said behavior will eventually be standardized to avoid breakage, but no official commitment has been made as of yet.
&gt; libiberty [...] has had tons of classic C bugs. [...] In fact, there were so many of these issues that gdb went so far as to install a signal handler to catch SIGSEGVs during demangling. It “recovered” from the segfaults by longjmping out of the signal handler and printing a warning message before moving along and pretending that nothing happened. Amazing.
For future reference, adding `-C debuginfo=0` to the compiler options overrides the one set by the site, and results in the expected assembly. Yay!
It is not currently guaranteed. [Source](https://www.reddit.com/r/rust/comments/5mhx4f/exploring_the_limits_of_rust_oneliners/dc4mspb/)
Can't you link a MIT library on a GPL program? I thought only the opposite was against the license.
Yes, but I think the problem would be that GNU would not use an MIT library unless they could re-license it under a license they deem acceptable. 
Hey Herman, I recall the evening at the bar where I met you and several others for the first time; that was a lot of fun! I sincerely appreciate what you've done to rename the project. I believe this will be best for both of our projects in the long run. The new name, _weldr_ is fantastic! It ties in quite nicely with the purpose of your application and the web frameworks in the ecosystem. &gt; When I made the blog post today I did not expect such a strong reaction. I would encourage you to consider announcing the project again under the _weldr_ banner. There was definitely an unfair amount of criticism about the name in the original announcement, and I think it prevented a lot of on-topic discussion about what is otherwise a very interesting project. Thanks again, Herman!
First of all, `values[index]` results in an `f64`, but the function is declared to return an `&amp;f64`, so that's the mismatch. `x[i]` actually means `*x.index(i)`. Moving along, unfortunately it's not possible to implement `Index` for that struct. `Index` must return a reference to something that the struct owns. However, you can only hand out references to elements of the vector while you have the `RefCell` borrowed, and there's no way to tie that to the lifetime of the borrow you want to return.
You should be able to use the usual `mod` and `use` with multiple files. For example, this works: https://gist.github.com/killercup/40693f7857741632799627a8fd482b3b
`values[index]` uses the [`Index`](https://doc.rust-lang.org/std/ops/trait.Index.html) trait. From those docs &gt; `container[index]` is actually syntactic sugar for `*container.index(index)`. . .This allows nice things such as `let value = v[index]` if `value` implements `Copy` Since `f64` is `Copy` you get a copy of the value.
I don't want to run someone's assembler code. *"If you're interested in something not shown on the benchmarks game website then [please take the program source code and the measurement scripts and publish your own measurements](http://benchmarksgame.alioth.debian.org/play.html#languagex)."*
And here is the demangled `lets_get_exponential` symbol ;) &gt; `lets_get_exponential(Pair&lt;Pair&lt;Pair&lt;Pair&lt;Pair&lt;Pair&lt;Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt; &gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt; &gt; &gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;Pair&lt;Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt; &gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;Pair&lt;Pair&lt;Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt; &gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt; &gt; &gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;Pair&lt;Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt; &gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt;, Pair&lt;Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt;, Pair&lt;Pair&lt;int, int&gt;, Pair&lt;int, int&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;*)`
I'm curious, I feel like just having a Velocity trait might be sufficient?
Facebook's Buck (very similar to bazel) has preliminary support for Rust, and it's under active development https://buckbuild.com/rule/rust_library.html
Great writeup. The importance of the work Ralph did integrating Rust into Firefox can't be understated. The challenges he's been solving are paving the way for other organizations that need to do this kind of large-scale integration and migration of production systems. It'll be impossible to quantify the impact, but all future Rust production users are going to benefit mightily from this work. 
`include!(...)` macro
Indeed, and I had to try several locations for it to find the one that worked :( It's not the whole solution, but still very helpful.
I actually find the mangled symbol much easier to read ;-P **f** takes a **P**ointer to a **F**unction which returns a **P**ointer to an **A**rray of size **1** and type **i**nt.
Any binary tree like tutorial? I've seen this one: https://xojoc.pw/justcode/rust-binary-search-tree.html But I'm a noob doing research on what tutorials are out there for rust and tree data structure for the summer. I'd like to create a R package written in Rust for my master thesis. edit: Sorry, I forgot to say thanks for reading! 
No problem, love your writings. I was waiting for you to post, but when I saw it did not show at "Last week at Rust" I decided to post :p
Huh. Crazy thing is that crashes rustc, but the intellij-rust plugin works fine with it without any delay and shows the start of the deeply nested tuple on autocomplete.
Do you mean you'd dual-license? GPL in crates is frightening given how important they are to the Rust ecosystem.
I think it's due to the levels, which start at zero. idk.
&gt; Yes, but I think the problem would be that GNU would not use an MIT library unless they could re-license it under a license they deem acceptable. GNU doesn't really work that way. For example, gdb can link against expat, which uses the MIT license. It also links against Python, which isn't GPL or LGPL. The blocker is more likely to be that gdb tends to be conservative about introducing dependencies.
For the curious: OrderMap on [crates.io](https://crates.io/crates/ordermap) and [github](https://github.com/bluss/ordermap).
How does Rust differentiate when to use the copy vs the reference? I'm thinking of right hand vs left hand sides of expression. So when one writes `v[index] = 3.14` the copy isn't being accessed (I guess i'm talking about IndexMut). Edit: Looking at some sparse vec/matrix crates in rust to see how those authors managed it...
&gt; &gt; This is the most permissive Creative Commons license, &gt; &gt; is it? I consider CC-0 more permissive than CC-BY.) When prototyping, I only use CC-0 for temporary assets because I don't want to bother with the attribution in case I want to redistribute it. When I go to production, I use whatever works and try to include attribution for everything I use, even if it's not required by the license.
No problem! Cargo basically only comes into play when you use `extern crate` or want to easily have a `lib.rs` automatically be treated as a library and stuff like that.
It would be a tri-license (it is currently dual MIT/Apache2.0), but yes.
Particularly because it's basically a dependency on LLVM, which would not be a terribly good thing for the project even aside from all the BSD / GNU politics. 
This is defined in the reference, specifically the section on [assignment expressions](https://doc.rust-lang.org/reference.html#assignment-expressions), which are closely tied to [lvalue expressions](https://doc.rust-lang.org/reference.html#lvalues-rvalues-and-temporaries). Here's a summary from across the sections in the reference: &gt; An lvalue is an expression that represents a memory location &gt; When an lvalue is evaluated in an lvalue context, it denotes a memory location; when evaluated in an rvalue context, it denotes the value held in that memory location &gt; The left operand of an assignment or compound-assignment expression is an lvalue context &gt; An assignment expression consists of an lvalue expression followed by an equals sign (=) and an rvalue expression &gt; Evaluating an assignment expression either copies or moves its right-hand operand to its left-hand operand. [Index expressions](https://doc.rust-lang.org/reference.html#index-expressions) (`a[b]`) and dereferences (`*expr`) are two kinds of lvalues so the same rules apply regardless of if they're truly syntactic sugar. When it comes to assignment, the index expression `v[index]` on the left with a value on the right means "copy/move the value to the location `v[index]`". When `v[index]` is on the right, it's the value at `v[index]` that's used, and for `f64` or any other `Copy` type that results in a copy.
After testing: slower. :-/
^(If I can get this working, it will power a testbench in our labs; if I can get it working very very well, it may one day go to space.)
That's unfortunate, but it makes sense. I'll find another way. Thank you for your answer. 
[removed]
Thank you for the kind words.
Thanks very much for your detailed explanation!
&gt; In fact, I personally ran a job that year that used a 90GB heap. I too have used java swing. 
*headdesk* I knew there was a really simple way to say it that I would kick myself for missing.
*headdesk* In retrospect that's stupendously obvious and I'm very irked at myself for not knowing it beforehand. Thanks!
Maybe I'm completely wrong here but wouldn't `&lt;f64 as FromStr&gt;::from_str` work? Doesn't work from bytes, but getting a str from bytes should be [reasonably easy](https://doc.rust-lang.org/std/str/fn.from_utf8.html). 
This is my first serious attempt to build anything in Rust. There are a few work-in-progress Last.fm libraries for Rust out there, but none with working Scrobble support, so I built one. I'm working on a larger scale project which will use this crate to scrobble tracks from Spotify for Spotify Connect clients that don't have built in Last.fm support (such as Amazon Alexa). Any comments on the obvious mistakes in my Rust code would be massively appreciated.
&gt; Are there any possibility for it to replace libiberty in gdb later on? I think it would be interesting to try. It seems to me that it can be done as an experiment easily enough, or maybe as a compile-time option. One wrinkle is that gdb does C++ name canonicalization. This is used to make sure that user input like "break foo&lt;int const&gt;" is canonicalized so it can be matched against whatever is in the symbol table, like "foo&lt;const int&gt;" (made up example, I no longer remember a real one offhand). Canonicalization works by parsing the demangled form and constructing the same sort of AST used by the demangler; then calling the demangler on this AST to get the canonical string form. For this to work, either the AST generation would also have to adapt to the Rust implementation, or gdb would need both demanglers. I don't know whether it's possible for canonicalization to trip over the same demangler bugs that cause crashes elsewhere, I suppose it would depend on whether the bad mangled forms arise from meaningful demangled forms.
`from_str` doesn't work like strtod. strtod returns position where it ended. So you can parse number list. That's what I need.
Lock-free/Wait-free data structures are often mentioned as an example. There was an article by Aaron Turon how you can get by without a GC, but IIRC it boils down to having a custom garbage collector for a subset of the data structures. https://aturon.github.io/blog/2015/08/27/epoch/ Also, reference counting is a form of GC. So if you have pointer-graphs, you need some kind of GC. If you can have cycles, you also need more advanced reference counting to avoid memory leaks (a naïve RC GC cannot reclaim cycles.) 
Yours example doesn't support exponent. Also, numbers like `-.0` is a valid one, so... And no, I can't use default rust parser, because it doesn't return end pos. I need a functions that can extract number from an arbitrary slice. I can only specify a start point, because I don't know the end point before parsing. Here is an example: `10-20 5.5.3-4` it should be parsed as `10`, `-20`, `5.5`, `0.3`, `-4`.
&gt;Are you actually receiving such poorly formatted input data? Welcome to the SVG world =) And yes, stdtod from C works with such input. &gt;-.0 would not be valid with my parser I mean that they should be valid.
People mentioned using this in gdb maybe (but concluded probably not), what about lldb? It looks like they have an incomplete 'fast demangler' [here](https://github.com/llvm-mirror/lldb/blob/master/source/Core/FastDemangle.cpp), and fall back onto llvm's libcxxabi's `__cxa_demangle` [here](https://llvm.org/svn/llvm-project/libcxxabi/trunk/src/cxa_demangle.cpp). Anyone know how good those are considered? I assume that lldb feeling the need to write it's own incomplete faster implementation means 'not very good', but I could be wrong.
I don't want any external(dynamic library like libc) dependency.
Thanks, thats good detail on how the expressions are interpreted, but I guess what I was really driving at is how one implements a user type that might have reason to behave differently on the left side than the right in that case - something in the area that the original comment was looking for. I'm not fully sure what the Tails struct was trying to do, so I find it easier to think of a sparse vector container where all values might be a default except where a non default value has been assigned. In that case, if one wanted to implement the Index trait, would you also implement the return of custom Reference type that could assign space for a new value if it's in a left hand side of the expression, or just supply a copy of the default on the right hand?
It will work if you just [give `f` a return type](http://cdecl.org/?q=void%20f%28int+%28*%28*%29%28%29%29+%5B1%5D%29). The return type isn't included in the mangled name because you can't overload by it.
You could try my crate: [input-stream](https://crates.io/crates/input-stream). It lets you parse numbers (or other data types) from bufreader (which &amp;[u8] is). 
Why are GPL-licensed crates frightening? I currently have an LGPL library and GPL binary on crates.io but I don't see a problem with doing that. *edit: fixed grammar
Heterogeneous list. It's a collection of elements of different types.
I am not sure this works given the underlying implementation. Wanted a nicer solution and essentially ended up reimplementing the cleanup function.
When I attempt to compile it I get an exception from LLVM as seen [Here](http://i.imgur.com/5SgM0vz.png) This might actually be worth opening a bug report issue for.
Would GCC only need to compile MIR in this case, or would this be more than just replacing llvm?
If I'm interpreting your question correctly then you don't have to do anything special. You implement `IndexMut` and you can use it in either side of the expression. You get back an `&amp;mut` reference from `IndexMut` and if the expression `a[b]` is on the LHS then it's in an lvalue context and you get assignment semantics, if it's on the RHS it's in an rvalue context and you get read semantics. I think the worst case is you have to construct a default value in the event you don't have an existing value to overwrite. To get around that you use something like the entry api that the standard maps have, but I don't think that's compatible with indexed assignment (possibly via `DerefMut` but I haven't tried it).
Unfortunately no, since it relies on FromStr (linked above). The actual codebase is pretty small that you can fork my crate and make it try all substrings, and take the last that matched, either that or some other crate that parses partially correct stuff like "5.5.3" into 5.5 and 0.3 Later Edit: You can use a regex to match the longest prefix that looks like a double and then use FromStr to parse it using the rust parser (which nicely handles most cases, like overflow, etc). One such regex is "(+|-)?(\d+)|(\d*\.\d+)(e\d+)?"
What, in your opinion, could they do to improve learning resources? I only ask because so far their documentation seems pretty great.
I find the naming of your builder methods on Options a bit weird (eg. lose the `set_` prefix?). Also `write_built_file_with_opts` should probably be a method on `Options` as well. Otherwise looks good!
I bet you're hitting the same busy-looping that I've seen while waiting for the response stream's body chunks to be produced. You could try this fix: https://github.com/hyperium/hyper/pull/1074
&gt; I think the worst case is you have to construct a default value in the event you don't have an existing value to overwrite. To get around that you use something like the entry api that the standard maps have, but I don't think that's compatible with indexed assignment (possibly via DerefMut but I haven't tried it). Yup, the nominal case makes sense to me, that worst case is what I was asking about because I think it's what one would want to need care of when writing a sparse vector container, or maybe containers which keep the data compressed, etc... Thanks! Looks like some interesting reading ahead for me to see how to accomplish this 
In theory, it should only require adding a MIR -&gt; GCC pass. In practice, I imagine there's currently some "accidental" entanglement with LLVM that might take some effort to tease out.
User-friendliness? I generally have low expectations.
You can still use modules when doing a binary project. It would make sense for your app to be comprised of an "ip" crate and a "gateway" binary, but I agree with /u/YourGamerMom that the current layout is unusual. I wrote [#1](https://github.com/AndyGauge/gateway-guessor/pull/1) to show what I mean.
I'd type that command, but personally I'd take at least fifteen minutes considering a trade, like plumbing or electrician, before pressing enter. 
Autoderef occurs when a dereffable object is passed *by reference* as a function/method parameter or used as a method receiver (in `x.f()`, `x` is the receiver). `println!` secretly takes its parameters by reference (it's a macro, so it can do that without appearing to), so that's why it works.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/redox] [Redox OS - Visual Refresh • r\/rust](https://np.reddit.com/r/Redox/comments/5vo1cy/redox_os_visual_refresh_rrust/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
1) I work for [System76](https://system76.com) as a Kernel Engineer. I spend 10 - 20 hours a week on Redox. 2) I am working on it. All changes for the visual refresh can be seen in the `arc` branch of Redox (https://github.com/redox-os/redox/tree/arc) Thanks for your support!
The new look looks good! I'm currently compiling the `arc` branch to check it out. Here's a question: what are the most pressing needs in your eyes? What needs to be done in the near future?
I've got a question. What's the goal of Redox? Apart from the fact that it's written in Rust, I'm curious what it's actually trying to accomplish. Are you going for an experimental/research operating system? Is it to test the Rust language and improve it? Is it for practical usage?
Yeah, field access triggers autoderef as well.
Are there any Implementations that are not linked-list like?
I had a feeling! Thanks for your great work, on both the os and system 76
And it should also be made clear that Rust can pretend to be C code because there are very clear rules about what compiled C code looks like which all C compilers agree on. C++ doesn't have that yet. (That's why you bind to C++ by writing (or auto-generating) a C glue layer. C++ also knows how to pretend to be C.) The problem being that auto-generation doesn't help much with Qt because the hard part is encoding the mapping between Rust language features and Qt API rules which can't be automatically inferred from the code because they're not enough information. TL;DR: Outside of QML, Qt is a very big, complex API surface and it's inevitably going to take time because humans need to translate stuff that's only in the human-readable API docs into Rust.
Is there a way within the Rust stdlib to get a reference to the Jemalloc malloc functions (for passing as callbacks to C libraries)? Is `alloc::heap::{allocate, deallocate, reallocate}` the typical route?
I think you mean, "can't be overstated".
Do you develop the NVMe support from scratch with Rust?
Looks fantastic! As someone who is very interested in OS development, it's really quite interesting to see the progress being made on Redox. Kudos to you for your time and effort making it! :)
Also, Nagle's algorithm: https://github.com/hyperium/hyper/issues/944 With those two things fixed, my code which delegates work to a CpuPool performs fairly well. Hopefully yours will, too.
At least he wasn't trying to allocate grenades on the stack.
I didn't.
I'm leaving the ethical consequences as an exercise for the reader 😀
Nice update ! What about using Servo (webrender) as a renderer for Redox ? Building an UI with web high performance technologies would be nice. 
Last year I worked on a rust project where a single 1.2TB Vec&lt;u8&gt; would fail to reallocate for growth because the growth factor of Vec would have used up more than the 1.5TB on the machine. 
Great news ! Well done Jackpot51 !
I like the new L&amp;F, quite nice.
Oh great. That pattern looked a lot like the examples I remember from "Learn Rust by Implementing far too Many Linked Lists", so I guess I didn't pay sufficient attention. Nice work
See this blog post for an explanation (you actually cannot add it to the language as it exists currently, because doing so creates a conflict): http://smallcultfollowing.com/babysteps/blog/2016/10/24/supporting-blanket-impls-in-specialization/ It might be possible to support this in the future :)
regex will be times slower for a such case.
I already implemented it. And it's support Path's grammar completely. I'm using it exactly for minification purpose. See [svgcleaner](https://github.com/RazrFalcon/svgcleaner). The problem that I already have a working(mostly) implementation, but I want a more general solution. And I don't want to create "yet another strtod".
Yes. But it will require `unsafe` and relaying on a specific libc behavior (I don't believe that they are completely equal).
Thanks for answer! Thats what I thought, that I'd have to ship my own impl. :) Luckily in my problem the input floats within [0;10000]. So there is no problem of smaller/larger numbers. Thanks for providing the base implementation that I can base on/use :) Btw: I was about to use super simple implementation, which bases that my input number is within range [0;10000] I wanted to simply multiply the number like that let num = input*10000000000000 as i64; let denom = 10000000000000 as i64; let rational = Rational::&lt;i64&gt;new(num,denom); What do you think of this approach? Would it be better for my needs? Thanks!
This works: (playpen link: https://is.gd/X4asGe) use std::rc::Rc; trait Actor {} struct Player {} impl Player { fn new() -&gt; Player { Player {} } } impl Actor for Player {} struct Tile { pub actor: Option&lt;Rc&lt;Box&lt;Actor&gt;&gt;&gt;, } fn main() { let mut tile = Tile { actor: None }; let p: Rc&lt;Box&lt;Actor&gt;&gt; = Rc::new(Box::new(Player::new())); tile.actor = Some(p.clone()); } [edit]: Alternatively, you could also write that line like this: let p = Rc::new(Box::new(Player::new()) as Box&lt;Actor&gt;); the important part is that you have to create a boxed trait object right away afaiui.
YAAAY! I also Started to fiddle around with lastfm... Thank you so much for doing this! I really hope I can do awesome things with this! Thank you so much!
Ah, that works. However, now I can't store *p* in *main* as a *Player*. Is there a way to achieve that? If not, it's not a huge deal; I figure I can cast *p* to *Player* locally when I need to.
No, that doesn't work afaik, you cannot have an Rc&lt;Box&lt;Player&gt;&gt; and Rc&lt;Box&lt;Actor&gt;&gt; refer to the same object
The ambiguity is fixed in C++11, so it wouldn't be necessary to insert them. c++filt most likely does for backwards compatibility.
Like std intrinsics, or the `link_llvm_intrinsic` feature, but we could solve that by adding a `link_gcc_intrinsic` feature and a way to distinguish between backends at compile time. There is no problem that can't be solved by adding new features, in particular when adding new features was what caused the problem in the first case :D
I would prefer D style variadics like [this](https://github.com/dlang/phobos/blob/master/std/meta.d#L118). You can slice them, index them get the length at compile time etc. They basically behave like an array. 
Inspired by your effort, I've been writing my own Vorbis decoder in Python. If you're still following this post, I'd love to pick your brain on a few things. I've finished the page and packet decoding, as well as comment header. I'm currently working on the setup header parsing and, while it's going well, I'm not 100% sure if I'm understanding the specs correctly. I'm wondering if you know of any utilities out there that will spit out decode header details to compare with? Thanks!
Why is that better than [variadic generics (376)](https://github.com/rust-lang/rfcs/issues/376) ?
I tried it with both stable and nightly. I did go ahead and open an issue for it [Here](https://github.com/rust-lang/rust/issues/40041), where retep998 figured out the cause of it.
I don't know much about OS UIs. Do they typically have the hardware acceleration and parallelism found in Servo/webrender? If not, that could be a very advantageous marriage.
Yes - it is Rust in userspace
I [see](https://github.com/bobbo/rustfm-scrobble/blob/master/src/dto.rs) you use serde as it is supposed to be used! Nice! 
Normally I would but I wrote this post on my phone. I did actually use double quotes. 
You can avoid *conflicts* in std. Case in point, `Borrow` has the blanket impls the OP is asking for. No problems there. I always try `Borrow` first for this exact reason.
Already looks better than KDE, and Gnome.
I use the Lalrpop crate for all my parsing needs. If allows you to specify the exact grammar, rather than relying on `strtod` which might possibly have slight differences in different libraries. It generates a highly efficient LR(1) parser at compile time, equally efficient as anything you could ever assemble by hand, and has no run-time overhead. And as a bonus I find the Lalrpop grammar language to be easier to read than just about any other parser combinator I've ever seen. 
Neat! Been following Redox for a while, and happily throwing some bones your way on Patreon. Keep up the good work :) Out of curiosity, is it possible to run Redox headless? I've always been curious to see how server applications would fare on a slim, fresh OS without all the historical baggage. Seems Redox might be a fun test bed to play with high-performance server code. :)
Can you elaborate on that?
Thanks for the support! Yes, Redox can run headless. When running from source, you can use `make qemu vga=no` to run without a display. All features you would expect to have are supported.
Next step: rounded corners! But seriously, nice work on Redox. 
Definitely! Thanks!
Awesome! I'll have to make some time to hack around with it :)
&gt; … it's most people… and that's not because of Steve… Most who've seen these discussions would not have known any details about any of the benchmarks game tasks before they saw what Steve claimed. Why would they? &gt; &gt; So "choosing to call everyone liars" seems hyperbole. &gt; "Everyone who makes this statement", I guess. It's implied. Meh. "It's implied" could be a response to much of your comment.
You're basically asking `web browser Layout Engine built into the DE. ` That requires JIT compiler across the board. Exactly like TempleOS has done. I endorse this idea. [It has been brought up on redox forums from me and others. :) ](https://discourse.redox-os.org/t/i-had-a-semi-radical-ui-idea-that-may-be-hated-with-great-vigor)
&gt; faster n-body by tuning the unrolling cost model Faster than [-C llvm-args='-unroll-threshold=500'](http://benchmarksgame.alioth.debian.org/u64q/program.php?test=nbody&amp;lang=rust&amp;id=2#log) ? 
From Wikipedia: &gt; Constructive criticism is the process of offering valid and well-reasoned opinions about the work of others, usually involving both positive and negative comments, in a friendly manner rather than an oppositional one. The purpose of 'constructive criticism is to improve the outcome. This is in an oppositional, non-friendly manner. The tone serves to antagonize the person being asked, and the comment reads more like a rhetorical question meant to evoke a reaction and/or shame the person. Such comments are disallowed here.