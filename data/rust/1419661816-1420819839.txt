Why not mod foo { pub struct Builder&lt;'a&gt; {...} impl &lt;'a&gt; Builder&lt;'a&gt; { pub fn get_x(&amp;'a self) -&gt; &amp;'a f32 {...} pub fn set_x(&amp;mut self, value: f32) {...} // Since it is only a reference, we can't write to it without `unsafe` pub fn get_blob(&amp;'a self) -&gt; &amp;'a ::capnp::data::Builder&lt;'a&gt; {...} // We can only pass in a `&amp;'a mut Foo` if no `&amp;'a Foo`s exist pub fn set_blob(&amp;mut self, value : ::capnp::data::Reader) {...} pub fn init_blob(self, length: u32) -&gt; ::capnp::data::Builder&lt;'a&gt; {...} ... } ... } ? Also, an idea for making the API nicer: mod foo { pub struct Builder&lt;'a&gt; {...} impl &lt;'a&gt; Builder&lt;'a&gt; { ... // As above or in article pub fn x(&amp;mut self) -&gt; &amp;mut f32 { ... } // now we can do `*foo.x() = 5.6` instead of `foo.set_x(5.6)` pub fn blob(&amp;mut self) -&gt; &amp;mut ::capnp::data::Builder&lt;'a&gt; { ... } // now we can write to blob ... } ... } This would really benefit from [Struct properites](https://github.com/rust-lang/rfcs/issues/419) though.
Wouldn't perl / julia's garbage collectors handle it?
The advantage of `set` is mainly that you provide a generic interface for accessing properties. The alternative would be to use macros to generate the accessor methods for you. That is what ruby kind-of does with `attr_*`. Using a trait for that is a bit more elegant. You could for example more easily replace the standard get/set implementation with a custom one, e.g. to implement a observer pattern.
A lot of the choice has to do with whether you are working with a formally-specified language which is known to be a context-free grammar (CFG), or whether it is a context-dependent grammar. Parser generators do very, very well with CFGs, both in terms of usability for the developer and in performance. The trouble begins when you introduce contextual dependencies -- when your language contains rules that push it out of CFG and into CDG. Even if you exclude the preprocessor, C++ is firmly in the CDG set. The classical example is, how do you parse this? a b(c); This could be (at least!) two things: 1) a declaration of a variable named `b`, having type `a`, with constructor arguments `c`, or 2) a function declaration, having return type `a`, function name `b`, and arg list 'c'. You can't resolve the ambiguity without performing a symbol-table lookup. The symbol-table lookup requires *context*, so boom, you're out of the CFG set. If you're implementing a C++ compiler, then YACC is absolutely not the right tool for the job. Obviously. And for many other languages, it isn't, either. But for CFG grammars, and there are many, YACC (and similar tools) are an excellent choice. They allow you to write your grammar as a crisp, formal specification, rather than combining your grammar specification with your implementation. When you're writing a recursive-descent parser, it is *incredibly* easy to accidentally take a dependency on parsing context, and to create ambiguities that can only be resolved by using context, such as the C++ example. Those dependencies should not be taken on lightly or accidentally; they make it much more difficult to reason about the language, or to write more than one tool that has the same view of the language. Refactoring tools for C++ are very difficult to write, because the language is so incredibly context-dependent. The preprocessor alone makes C++ difficult to reason about. Add in the template system, and things just got exponentially more difficult. I've used YACC to write a lot of small domain-specific languages, and also to parse well-known languages that are in CFG (or very close to it), such as SQL queries. When used for its intended purpose, YACC is a great tool, and being able to easily throw together a parser for recursive input can be a very useful skill to have. Error reporting is an interesting topic. Many people don't realize that you can write error rules in YACC, which are a great help in getting the parser back into some useful state after an error is detected. &gt; The parsers they generate are efficient, but utter crap in the context of a real compiler, which require good error reporting and recovery, This is where I cannot agree. Terms like "real compiler" are really loaded terms, and frankly are more likely to cause arguments than to resolve arguments. I have seen every combination of { good error recovery, bad error recovery } x { hand-written, yacc-generated } compiler, so asserting that one technique always implies one result is simply wrong. Take Rust, for example. Rust is implemented with a recursive-descent parser. In my experience, Rust bails on parsing very easily, and does not do much in the way of error recovery in its earliest parsing phases. It does a fantastic job of error *reporting* in later phases, during name resolution, typeck, borrowck, etc. but all of those phases happen long after parsing has finished. So the example immediately in front of us doesn't show much promise for error *recovery* during parsing. C/C++ compilers do a better job on error recovery, but that's mainly due to their maturity, not the particular approach to implementing the parsing phase. About performance. Recursive descent parsers can waste a lot of time on speculatively interpreting some sequence of tokens. It's really easy to accidentally construct a recursive-descent parser that requires unbound lookahead. More cautious recursive-descent parsers usually have some bounded lookahead N, where N is often some small constant. In general, table-based LALR(1) parsers will run rings around recursive-descent parsers, but this really has more to do with the nature of the grammar that has been specified than with the parsing technique. If you have specified (even accidentally) a highly-context-dependent language, then you may waste a lot of time doing speculative parsing. 
Have you considered storing all the components of the same type in a `Vec&lt;u8&gt;` then casting the component out of it? This gets rid of the Any casting back and forth and is what [ecs.rs](https://github.com/TomBebbington/ecs.rs/blob/master/src/bag.rs) and [ecs-rs] (https://github.com/HeroesGrave/ecs-rs/blob/master/src/buffer.rs) do.
This code is quite generic, and so it may be hard for the compiler to infer everything.
&gt; pub fn x(&amp;mut self) -&gt; &amp;mut f32 { ... } Cap'n Proto uses little endian byte order, so this interface is not possible on big endian systems.
Cool, I was looking at the code and it seems like [this is the fancy line](https://github.com/iron/iron/blob/master/src/middleware.rs#L277): &gt; impl&lt;F: Send + Sync + for&lt;'a&gt; Fn(&amp;'a mut Request) -&gt; IronResult&lt;Response&gt;&gt; Handler for F I'm not familiar with the "for&lt;'a&gt; Fn(&amp;'a mut Request) -&gt; IronResult&lt;Response&gt;" syntax, and specifically surprised by the "for" keyword showing up here. Is there any Rust documentation/explanation about this fancy syntax?
They could very well, but this way, they don't have to re-build the installer with every new rustc+cargo build, and those who don't want to use the installer can install rustc+cargo manually.
To be clear, Iron's modifiers (which is really just a use of there more general [`rust-modifier`](https://github.com/reem/rust-modifier) do not use enums, they use generics - anything implementing `Modifier&lt;Response&gt;` can be passed to `set` or `set_mut`. I recently went about revamping modifiers. Now, you can pass a tuple of modifiers *as* a modifier which just modifies with each modifier in a row (all static dispatch!), and a new constructor for Response, so the above example is much shorter, just: `Response::with((status::Ok, "Hello World!"));` Also, as an aside, the typical way to enforce mandatory arguments when using the builder pattern is to have the initial constructor require all mandatory arguments. Otherwise you can use defaults.
What annoys me is that I don't find that syntax intuitive at all. If I understand correctly the `:` is used to say which kind of closure you want (FnOnce, FnMut etc...) but it's inside the || so it looks like it's a modifier for the parameters or the captured environment. On the other hand the "move" keyword to say how the environment should be captured is outside the ||. It's a bit of a mess IMO. And I don't usually mind sigils too much. Maybe if someone can explain the rationale for this syntax it'll finally click for me.
Author here, if you want to ask anything about it. Looks like you've been working on building an actual network stack, rather than the bits that touch the operating system - these projects could work hand in hand if you're interested. `libpnet` has mostly been in maintenance mode for the past few months, I'll be working on it 9-5 again from mid-January though. I'm really interested to see a full networking stack in Rust, keep up the good work!
Thanks! IIRC you were trying to help me out the other day on IRC when I asked about a similar problem. Since Iron now supports both unboxed closures and trait objects as Handlers, that seems to be pretty flexible, I could perhaps adopt the same solution in my D-Bus bindings.
the C varargs idea is very unsafe, so Rust does not allow that. Here are some options: * array slices: [0i32, 3, 5] * vectors: vec!(0i32, 3, 5), * making your own macro (just like vec! and println! ) In case you need more than one type in your slice or vector, consider wrapping them inside an enum.
In regards to double ended iterating and your comment about branching, I recently read about intrusive lists used in doom 3. http://gpfault.net/posts/intrusive-lists-doom3.txt.html You might be interested
It would be useful to compare it to the old version.
&gt; they need to interact with a C library over which they don't have control. Exactly. &gt; but I have no idea how to define a callback that takes variadic args -- I wouldn't be surprised if it's not currently possible. Thanks anyways! I guess I’ll have to write some boilerplate glue code ``:/`` 
Sheer convenience. You can put it there if you like, though nobody does in practice. The semicolon is only necessary if the `if` is embedded in a larger expression, like an assignment. The same rule effectively applies to loops, FWIW, though they only ever evaluate to `()`.
You've already terminated both branches, so a semicolon would be redundant. You would use one if you're using the if/else as an expression instead of a branch. For example: let x = if true { 5 } else { 4 }; Or if you omit the semicolons from the branches: if true { println!("true") } else { println!("false") }; Same thing with `match` and other expressions.
So I've been playing around with this in my head and it *should* be possible, though fairly ugly. The basic premise is to write a shim function in C that actually gets called, and its entire job is to convert the variable args into a `va_list` and pass it along to your Rust callback as a mutable pointer. It needs to be a pointer because we don't actually know the size of `va_list` and it may vary. The `va_list` should remain valid as long as the shim function is on the stack. The Rust callback can then call a bunch of C functions that take the `va_list *` and basically act as a wrapper around `va_arg`, returning things of the appropriate type. Problems: - `va_arg` in C is defined as a macro and expects to take a C type. One might be able to hijack this machinery to provide a generic API that can pop off an arbitrary number of bytes and transmute them but you might be stuck with supporting a few basic types explicitly. That is, `va_arg_int`, `va_arg_long` etc. - Ideally the shim function is generic so you don't have to explicitly write the shim/rust pair of the functions manually. Since the shim is written in C, macro magic or monomorphization isn't going to help.
Here's a proof of concept for a C API that passes a number of ints back to Rust using variadic arguments: [GitHub link](https://github.com/rraval/rust-fun/tree/master/varargs)
Thanks for the reply! Certainly, I've done something similar in C++ for my last game. The contiguous array of components have some of their own issues... so I have been a bit conflicted with their use. If you have all of the components for each entity aligned together... it makes it really efficient to pull in related components in a single shot. But then if you want all of the components of a single type? And then there is the issue of fragmentation. But then if you have the reverse... well yeah. So then you end up with multiple kinds of data structures for each method you want to access the data. There is an article I linked in another reply that outlines some of these issues. Anyhow, this is something I certainly plan on working on for this library... but as some sort of compromise between implementations. I'm just not sure how exactly I want to go about doing it at this moment. :) I can safely say that I do not have anywhere near enough experience in these types of optimizations.
Data-Oriented Design actually refers to a very specific thing these days. It's a philosophy pushed by Mike Acton, and a few others, which focuses on designing your systems for maximum performance based on how data is actually laid out and accessed in memory. Keeping things in contiguous arrays of data (not of pointers), and iterating over them linearly, is probably the most important single technique espoused by this philosophy, because it works so well with the pre-fetcher to avoid cache misses. So the sense in which you're using "data oriented" makes sense, but it's not the common meaning of the term, and thus likely to cause confusion. It's like saying your game uses "test-driven design" to mean that you used user tests to drive the game design, rather than using unit tests to drive the code design.
*snickers*, I certainly admit that the design as it is right now is not aligned with Mike Acton's philosophy. To be honest, when I posted the link I was more focused on the fact that there are multiple styles for the design of an ECS. Fat components for example is rather common. Aligning the data contiguously, moving away from pointers of data, etc... is an optimization that can happen as the library becomes more refined. While it's something that I've certainly been thinking of, and have made use of in the past... it's also something that I have a lot less experience with in general. I am wanting to make use of this library in my next project. I think it's better to actually *make* a game rather than spending my time building the perfect library. Anyhow, if I could change the title of the reddit post I would. But I also think that focusing on this is kinda missing the whole point of why I posted it in the first place. I do think your example is a touch unfair however :D
It is not possible to get the `&amp;mut self` reference with lifetime `'a` in general, since the value is usually in a more restricted scope.
This isn't really correct. Strictly speaking, the semicolon is required, e.g. it is required after function calls even if they return `()`. The language just has a special case for control flow structures. Also, your second example doesn't actually need the trailing semicolon, since `println!` returns `()`, and so the whole `if` returns `()`.
Yeah I had those without the `::&lt;i32&gt;` part at first, but then it looked weird because all the calls to convert String to different things were the same, which seemed like it could be confusing! How can Rust read my mind?!?! But once you know that it does read your mind through type inference, it makes sense ;) I think I'm going to have to start adding footnotes or something :)
&gt; if you're using the if/else as an expression instead of a branch The semicolon is part of the `let` statement and therefore required. It's not actually part of the `if` expression. `if` itself is always an expression (as someone already pointed out). The following example is probably a better way to demonstrate that: let a = (if true { 2u } else { 3u }) + 2u; 
&gt;my face when looking at that syntax `(|&amp;: _:`
Do you have write permissions in the installation target directory?
&gt; There is also the question of the interface we make available. Using the compiler's AST and giving extensions access to the entire compiler is pretty awful. I actually think the existing AST interface is not too bad. Sure, it could be more convenient, but I'd rather have a single convenient AST interface, instead of having "the real AST" and "the fake AST", usable only for syntax extensions. And I say this with some recent experience; I just wrote a YACC clone, which is invoked as a syntax extension. (See github.com/sivadeilra/racc .) Being able to use the same AST that the compiler uses makes me strangely happy. Also, I could imagine that "syntax" extensions should evolve into "language" extensions, where extension code is invoked at well-defined points in the compilation pipeline, so that it can examine or alter ASTs. Doing this using the same libraries that the compiler itself is using seems like the only sane way to approach it. Can you elaborate on why you think the current AST API is bad, and why? I think more convenience methods / functions for building the most common AST objects would go a long way toward making it approachable. 
There are two reasons the current setup is bad: 1st is the backwards compatibility thing - if we want to not break syntax extensions, then we can't ever change the compiler's AST - that really limits what we can do to the compiler. 2nd, the AST that the compiler wants has a whole bunch of stuff that is irrelevant to syntax extensions (e.g., node ids) and vice versa, there are things libsyntax wants but the compiler doesn't (e.g., ExprParen, which is only used for pretty printing). As far as syntacx is concerned there would be only one AST - the parser would output it, syntax extensions would manipulate it, and the compiler would take it as input. The only change is that the very first step the compiler does is translate to its own AST. The libsyntax AST would be evolved though, to more closely match the source code, so it would have the same backwards compatibility guarantees as Rust syntax itself. As I wrote, I'm hopeful the compiler will stop using any AST for anything other than its early phases, that means the AST will never be a uniform interface for compiler plugins (aka language extensions), In any case, we probably won't want to expose the compiler's internals like that - it precludes either compatibility guarantees or changing the implementation details of the compiler.
I was curious about how close we are to getting incremental codegen. I couldn't find much information, or even the work-in-progress patch. I did find this airmozilla video from 3 months ago: https://air.mozilla.org/faster-rust-builds/
It's actually quite possible to use a single contiguous vector for each component type, using macros to wrap up the boilerplate. I had an early draft of a system that worked that way (I scrapped it due to the complexity of taking multiple mutable borrows into an array, but that was before I knew about split_at_mut). I've put that macro up on this gist, in case it's useful to you: https://gist.github.com/Cifram/7991a3581cc0d37a7095 This was written very early in my time with Rust, so no guarantees it uses the best idioms for everything.
This would be a new addition to conditional compilation attributes. Currently supported ones are listed here: http://doc.rust-lang.org/reference.html#conditional-compilation Please file an issue. Thanks!
Thanks, I did so here: https://github.com/rust-lang/rust/issues/20267 If anyone has any suggestions regarding alternate solutions, I'd love to hear them.
I think the preferred way is the way that feels most natural, and it feels rather unnatural to me personally (my approximate background: C, Python, Haskell) to place a semicolon after a "imperative" piece of control flow.
I guess it is slightly 'impure', making a concession to naturalness and familiarity over being 100% strict about how semicolons are used. In any case, I much prefer for x in y { ... } to for x in y { ... }; The semicolon seems to be awkwardly hanging at the end of an already-complete piece of code. As /u/kibwen pointed out elsewhere, making a `for` loop generate a vector would bake `Vec` and (more significantly) dynamic allocation into the language, which is something we've strongly moved away from recently. An alternative would be creating some sort of fancy iterator that handled `break` and `continue` nicely, but that seems overly complicated. For me, the most natural way of extending `for` into returning non-`()` values would be using `break` as a "`return`" and allowing an `else`, e.g. (completely hypothetical syntax) let x = for a in b { if cond { break a } } else { 10 }; However, I'm not even sure if this is a good idea yet. This has been discussed before, but it's definitely a post 1.0 feature. (This also doesn't require changing our semicolon rules, allowing us to retain the most natural semicolonless form for plain loops.) - http://discuss.rust-lang.org/t/allow-loops-to-return-values-other-than/567 - https://github.com/rust-lang/rfcs/pull/352
If the two types have the same data, you can use an enum with no contents; struct Node { type_: NodeType, data: ... }; enum NodeType { Foo, Bar } and in the functions that need to be different... fn meow(&amp;self) { match self.type_ { //... } } 
I don't see anything wrong with using a `Node` trait, maybe you could use a `Data` struct like in your example to eliminate duplication.
&gt; (This rule only applies when the control-flow structure isn't the last expression in a block.) I understood what you said and assumed that everything is that. But It seems that the last statements of yours (This rule only ...) is not true. What do you mean "the last expression in a block"? I assumed that you means this control-flow will be the last expression in the main(). So I tried with this code &amp; it works fn main() { let x: int = 5; println!("value {}", x); if true { println!("hello"); } else { println!("hello"); } } Can you explain that case?
I agree with you. They should do semicolon more consistent as it's the fundamental concept. Otherwise, It makes more confusing when should add ; when should not
Others said this, but that syntax: (|&amp;: _: &amp;mut Wow. That's kind of silly.
It would be pretty nifty to see syntax extensions on further stages like the CFG, for implementing things like `yield` or `await`.
Im not sure what the best way of handling the translation step of the compiler is though. While a CFG sounds good at first, it makes certain things at codegen harder to do. Like all the destination-passing style stuff we do. Right now we see the destination as we walk the AST and can pass that along. With the CFG, we have to do all sorts of gymnastics to figure that out. Personally, I think that we should take a page out of Haskell's book and work on a minimal language that we translate into. Haskell has Core, and I think that we could do something similar. I think part of the problem we have at the moment is that Rust is quite abstract and high-level, and LLVM's IR is very low. The advantage of a Core-analog would be keeping the power of the Rust type-system (albeit made explicit) with a slightly lower-level language. What it would have is stuff like: * Autoref/autoderef/autoborrowing made explicit. * Method calls turned to function calls. * Overloaded operators turned to function calls. * Explicit casts &amp; coercions. * Fully resolved paths (this is more of a source-representation thing though). Having a simpler representation of the language has a number of advantages. It moves much of the convenience features of the language out of codegen. It makes it easier to write alternative backends for the compiler. It provides a nicer alternative to serializing the main AST into crate metadata, since this would already have been type-checked and verifying that the typing is correct is much easier than re-typing it.
(Totally minor point, `&gt;&gt;=` is already an operator in Rust: assign-shift-right.)
I don't think it's possible to implement the hypothetical `bar::Builder::one_foo()` function that you're using here. The body of the function needs to construct a `foo::Builder`and then return a mutable reference to it, but the constructed `foo:Builder` cannot outlive the scope of the function body.
Fair enough. I agree that using more control-flow information is important. I suppose that a CFG-based system is better for a language like Rust anyway. Haskell, being pure-functional, doesn't have much in the way of control-flow constructs and the purity and laziness aspects mean that they can often get away with not caring about things like evalutation order. I suppose that what we have now: *Parse* -&gt; *Resolve* -&gt; *Type Check* -&gt; *Borrow/Region Check* -&gt; *Codegen*, places too much into the last phase. Instead, having some analysis passes before that which work on a CFG before using said CFG to generate would work best. Thinking about it, I guess that using the AST isn't best way to figure out DPS info anyway. Given the problems it can cause if not handled properly (like cloberring the destination by accident), the AST isn't any better than a CFG, since both need more information than just a node-at-a-time walk can give. I'll admit that I've not thought about this that deeply, however it seems we're on the same page with regards to needing an intermediate representation between AST+side tables and LLVM's IR.
HKT would allow us to define generic functor and monad traits, with a little syntactic sugar it's going to be a big ergonomics win for the error handling and more generally pretty much any code with long chains of match expressions. I really look forward to the day when we can use the haskell `do` notation to chain monadic functions in Rust.
Is there a particular reason you don't support static linking? With LTO, it leads to better code than dynamic linking.
&gt; fix parallel codegen. This is currently broken with what I suspect is a minor bug. There is another bug preventing having it turned on by default. Can you provide any link on what's broken? Can you guide someone in fixing it? &gt; Land incremental codegen. Stuart Pernsteiner came super-close to finishing this. It should be relatively easy to finish it and land it. Again, any link to existing PR or fork?
You *can* always place a semicolon if you like the consistency, there's just a small rule that allows one to have nicer looking code by omitting it on control-flow. (FWIW, after practising Rust a bit, I stopped thinking of semicolon placement consciously. I'll do the natural thing and the compiler will occasionally pipe up and tell me I've made a mistake.)
Typo in the docs on `into_logger` it matches an `IoResult` against `Some(v)` where it should be `Ok(v)`.
No there isn't; I had just gotten started on the project and the first use case for a c api was python via ctypes so I built a dylib. Then I was looking to keep the c api specific to the dylib and not the rust library. Now I'm considering making the c api a seperate project but it would be nice if I could cleanly keep them in the same repository since the c api is just a wrapper. I imagine I'd enable building a static lib as well but first I would like to figure out the cleanest solution to keep the c api separate from the rust api. I suppose marking all of the c methods unsafe might discourage usage but I'd prefer to just not define them in the rlib.
First of all, thank you for writing this! I really appreciate the time you were willing to put in to look at this. I didn't realize that the default log module allowed for custom loggers, I'll look into switching to that for the macros. The one thing I see as a disadvantage to the log library, which is mainly why I made fern, is that log is configurable through environmental variables. Do you think it'd be an ok idea to allow fern to be used with fern_macros or log using feature attributes? I don't want to entirely commit to using log, because of it using env variables for configuration, but I think it'd be a good idea to provide it as an option. In any case, I'll definitely try implementing the logger for log, creating a multiplexer logger, and storing the thread id as part of the log record. I'll try writing some tests as well, I'm not entirely sure how I should create something of a fake stderr/stdout to confirm logging from, but I'll do my best. Thank you again for looking at this, and happy holidays!
Thanks! I'll fix that now.
Are there proposals for generators, like python's `yield`?
Sorry, I misunderstood the original question.
&gt; The loop idea would require making Vec a feature of the language itself rather than just a library type I've thought about that and I'm not sure how I feel about it. I just thought it was a nice side effect of having loops be expressions. The way it is now means that loops aren't really expressions and can't be without additional syntax (e.g. `break &lt;value&gt;`). I understand the sentiment of wanting something nice (and loops are perfect for special cases), but it seems odd that sometimes `;` is required (after assignment) and sometimes it's not, even though any other expression would require it (as in the provided examples).
https://github.com/rust-lang/rfcs/issues/388
Yeah, Stuart (who gave that presentation) did the work. It got 'quite close', I think it was basically working but had a few bugs. I should get in touch to get the branch he was working off. I imagine the rebase will be hell :-(
Although it is possible we'll do that, it is absolutely not a given that we will ever get do notation and other monadic thingies in Rust. I know some people want it, but others don't and prefer using `try!` for error handling. In any case, there is a lot of design work to be done and nothing is guaranteed at this stage.
The blocking using parallel codegen is [#19826](https://github.com/rust-lang/rust/issues/19826). There is also [#20184](https://github.com/rust-lang/rust/issues/20184), which I didn't know about until just now. The bug to turning it on by default is [#18243](https://github.com/rust-lang/rust/issues/18243). I'd be happy to help however I can if you want to take on any of these. Ping me (nrc) or irc. However, I'm no expert, cmr and acrichto know the most about linking (I think) and would probably be more helpful. I don't have a link for the incremental stuff - I should chase that up...
do-notation (at least, desugaring to closures) doesn't play well with control-flow like `break` and `return`, so, without laziness, it might not be as powerful as one would hope.
I really don't want to see Rust adopt that crazy operator soup of a thing Haskell has with all it's 3 and 4 character operators, too hard to read.
What about implicits? They would be useful for things like the execution context of futures, and also toghether with HKT for the kind of idioms used in the scala standard library to reuse big parts of the code, esp. when compared to things like the C++ STL for example.
How to read the &amp;: _: &amp;mut Request? it's puzzling
And I just checked, it's already a reserved keyword.
Interesting, I like this idea a lot. I've personally always liked the relationship between C++ and C (layering the complexity). I was going to post elsewhere , "could a minimal Rust subset be defined.. like a Rust--". the thought had crossed my mind as I'm currently experimenting with a pet language, trying to make it similar to Rust but I can't handle everything with my finite time. If there was a 'Rust--' perhaps my experiment could have been done as an alternate front-end to the same middle or something 
You can install the precompilled nightly with: `$ curl -s https://static.rust-lang.org/rustup.sh | sudo sh`
This isn't true I think. While you can use side tables (and most C code probably will), you can generate a bound in a single instruction (BNDMK) if you have a base pointer and length. Before a loop for example you would call BNDMK to generate the bounds for all relevant array accesses. Then you just call BNDCL (check lower bound) and BNDCU (check upper bound) for every access. I'm not sure why the Rust runtime can't just catch SEGV_BNDERR on every thread, and then panic! the task if it occurs? Is there a good reason other than that it adds overhead?
Thanks -- I recognize that's possible, and I may do that, but I would still like to get the build working for myself :-)
Hi! I had the same issue, and the awesome /u/flaper87 pointed me to the `--disable-docs` flag. I advise you to also install **ccache** (`sudo apt-get install ccache`) to speedup the compilation process, but you should remember to enable it with `--enable-ccache`. The complete command is: ./configure --enable-ccache --disable-docs Hope it helps! :) PS: For the Makefile, make a Pull Request on Github, they will be very happy to merge it! :)
I agree, using the environment variables to control things never really seemed like a great idea to me either... but I don't really have a compelling alternative to 'turn on or off debugging'. It'd be totally plausible to have fern macros that mirror the existing log!, etc implementation but are controlled via some other mechanism (? not sure what that might be though).
For one thing, if you're running as a dynamic library on an operating system with only a single signal handler function per thread/process (i.e. not Windows), your handler will compete with any other code trying to do the same or similar. With Rust not having much of a runtime at all anymore, the constraints required to address this would be a significant departure. Doesn't mean it can't be implemented anyway, though...
Ah, cool, thanks. And yes, I'll try and make that PR. :-)
One other question while I think of it. Obviously I can set the compiler install dir via, ./configure --prefix=/opt/rust ... or whatever I prefer, but I noticed that there is also a parameter, 'local-rust-root', which by default is also /usr/local (hardcoded, not just set equal to the prefix value). Should that also be tweaked in the event of wanting a custom install location? And why/to what purpose?
IntelliJ is written in java and it seems that the derived IDEs (PyCharm, RubyMine, ...) have built their modifications at least partly in their respectives languages. It is probably a very good compromise. Building an IDE from scratch is an enormous task so it makes way more sense to me to use IntelliJ as a starting point, especially if you consider how good an IDE it is. 
I don't know. I fear HKT might really hobble the languages adoption. It's very scary for newbies to hear that Rust is about "monads" or "applicative functors" or other such math-isms. Haskell and Scala both get a lot of FUD thrown at them over their higher kinded type use.
&gt; u could for example more easily replace the standard get/set implementation with a custom one, e.g. to implement a observer pattern. Ok, fair enough. Maybe some macro-based ones will emerge once that part of the language stabilises. Something that metaprogramming offers that pure generics doesn't: **customization**. For example, declaring a property with different, optional behaviours (observer, auto-validation, type conversions). This might seem like overkill for a property, but sometimes it can be appropriate (I'm thinking of data binding with C# and WPF). However that is an entirely different kettle of fish that is probably best tackled separately.
&gt; Also, as an aside, the typical way to enforce mandatory arguments when using the builder pattern is to have the initial constructor require all mandatory arguments. Otherwise you can use defaults. Yup. Out of interest - are status codes not mandatory for a response in Iron? What do they default to if you do not set them? Thanks for clarifying regarding the generic modifiers. Something that I've found with Rust so far is that people love writing very generic, abstracted code and this is no exception :D I'll have to look further into how _rust-modifier_ works. I'm still a bit on the fence as to whether the paradigm is a good idea or not but like many things - once you experience the pain points that they address first hand, the moment of enlightenment comes :)
Windows doesn't really have an analogue for POSIX permissions. The closest is requiring a program to be run as administrator for it to be able to write in certain directories, such as `C:\Program Files`. 
Pretty sure it can be just `|_: &amp;amp;mut Request|` or even simply `|_|` The compiler can (usually) infer the rest.
I never digged enough in the makefile to answer to your question, sorry! :( But, if i may, i suggest you to use the rustup script to install a stable version (it will also install Cargo). Compile it from sources is useful only if you're trying to hack the compiler. If that's the case, please ignore this last part. :)
&gt; Check out the varargs-vsnprintf branch for an example. That’s more or less how I ended up working around the interface. I’m worried that things like this requiring C glue may become serious obstacle to Rust’s world domination at some point in the future: On some platforms one cannot assume that there is a C compiler installed, and portability of a project may suffer. Not to mention that Cargo’s toolchain for integrating GCC is more or less nonexistant: It can’t detect changes to the ``.c`` files so one has to force rebuild them manually upon modification. Which is less convenient than writing a makefile. It’d be great if Rust supported vararg functions, even if only with the C ABI as a constraint and restricted to ``unsafe {}`` blocks. 
If you're ever in doubt, put it there and you won't have a problem. In the meantime, people coming from other semicolon-terminated languages need only apply their usual practice of not bothering to terminate constructs that are used only for their side effects.
I've found it very useful for the framework to be able to tell between the default status code and no status code set, so internally the status is stored as an `Option`.
No compiler-hacking plans at the moment, but I was giving some thought to hacking on the standard library. However, I do find that building &amp; installing a compiler from source can be a good test of how easy it is to get involved in a language community, so call it a stress test if you will ... :-)
We don't need monads to do `IO` or error handling (there is `try!`), but it would be so good if we can get something like parser combinators.
I'd suggest filing a bug if you think there's an oversight here, and if you do I'll be happy to nominate it as a 1.0-blocker.
I fail to see how using pointer-sized integers "aids code consistency and interoperability", when it makes your code behave differently depending on the platform. Does anyone know more about Swift's reasoning here? Maybe it doesn't apply to Rust, but I'd be curious to know.
Unfortunately to get the whole beauty of applicative parser combinators there's also a need for partial function application.
http://doc.rust-lang.org/std/dynamic_lib/struct.DynamicLibrary.html
`int` and `uint` have platform-dependent sizes in Go as well. It's not really an uncommon practice.
Sure, everyone's aping C, but *why*? I *never* use `int` and `unsigned int` in C or C++ code. I always use one of the types from `stdint.h` (edit: or `stddef.h` in the case of `size_t` and `ptrdiff_t`) precisely so that my code will be portable, and if it so happens that I do need a pointer-sized type, then `intptr_t`/`uintptr_t` make that very explicit.
I don't think obfuscating `int` is the way to go. Someone could still use `intx`where they should be using an `i32`. Maybe the compiler can check to see if the `int` is directly/indirectly associated with an unsafe block. My impression of Rust so far is that you shouldn't see "raw pointers" in safe code, I could be wrong.
Sure, but I wanted to know if Swift is just cargo-culting the tradition here, or if they have some other reason for the guideline. From what I've read recently, it seems that promoting pointer-sized integers is actively detrimental, by causing platform-specific bugs and not having any clear benefit. I would be surprised if Swift is doing this just for tradition's sake.
But size_t and ptrdiff_t are just funny names for the uint(x) and int(x) types here. Certainly I use size_t in my C++ code religiously. It will always be faster to use machine-word-size values in many circumstances. Everyone agreeing on a "big enough for most everyday uses" integer type makes interoperability in strongly typed languages easier. Having those two types be the same isn't an outrageous design decision.
Yup thanks, I meant to include sizes/offsets. They also appear to be indirectly associated with an unsafe blocks, no? I'm probably wrong. Vec for example has `len` and `cap` as uint and both are utilized directly/indirectly in unsafe blocks.
&gt; It will always be faster to use machine-word-size values in many circumstances. Untrue. On at least x86-64 and AArch64, if you don't need the 64-bit range, your code will most likely run much faster using 32-bit integers because you can fit twice as many in the same space which will reduce cache traffic. There is no instruction-level speed penalty; the latencies for all operations are the same for both 32-bit and 64-bit (see https://gmplib.org/~tege/x86-timing.pdf ). Using `size_t` unnecessarily on 64-bit systems is typically a performance pessimization.
So, should I stop using `int` and `uint` if I don't care about what size it is? What should I do instead?
Just to follow up: one successful build and install, and I'm working through the tutorial. Thanks again for the help!
See [niko's followup comment](https://github.com/rust-lang/rust/pull/20119#issuecomment-67820619) and sfackler's follow-up comment, for a summary of the RFC this is part of. In short, `Send` will still function as an "automatically derived" thing today, but you can opt-out, or use `unsafe impl` to opt-in for types that don't get it automatically.
Just use `i32`. There's another rfc for making non suffixed integers default to that if there is no other type that can be inferred. It's big enough for most purposes without risking overflow, and efficient on both 32 and 64 bit platforms which is all you are likely to run on unless you're specifically writing embedded code for very simple microcontrollers, but if you're doing that, you likely already know it and know what constraints you're working with.
int and uint are a type that is 'native' and can be handled in a single cycle and is thus fastest, which happens to normally be pointer sized ints. Or at least it was before x64 came along with it's 32 bit ints... It's a bit of a premature and bug prone optimization though. Making int/uint a fixed size 32 bits and using int_fast32 or something would be better imho.
You can't use `derive` for `Send`/`Sync` anymore because they both require an unsafe implementation. In fact, you now get an error when using derive with those 2 built-in traits.
Pls, if a bug is opened, cc the oibit RFC tracker. 
That's the exact issue. People type `5i` or `5u` because it's the shortest thing to type when they don't care about size. Changing the name `int` or `uint` won't solve that. 
It feels unnatural to me to place semicolons sometimes, but not always. Because of this, learning *when* to place one should be consistent. I hate this about the no-semicolon fad in JavaScript lately (they're necessary *sometimes*). Either be consistent within Rust or be consistent with other languages; trying to do both seems like a bad idea to me. Then again, I suppose this particular exception to the rule isn't that confusing and allows a new user to use semicolons like other languages if they always use `return` explicitly and always use control blocks as statements, but I'm not convinced this is a good thing. I mostly use Go, Python and JavaScript right now, and I really like that the language (except JavaScript) forces code to be consistent.
Wonderful! The two resources that you have to use and abuse (IMHO) are [The Guide](http://doc.rust-lang.org/guide.html) and [Rust By Example](http://rustbyexample.com/), if you are serious about learning Rust. Also, the people in the IRC channel #rust on moznet are amazing, every time i asked for something i received response in a short time and exhaustively, even if the question was stupid! Nobody have ever judged me. :)
There is no such thing as a stable version of Rust. Well, for a few weeks anyway.
This is where I'm coming from as well.
If they like try!, they should like do notation...
I would personally like a custom variation of Vec&lt;T&gt; which takes a parameterised index, because for most purposes on most machines, 32bit indices are sufficient. For anything I'm likely to work on a 32bit index would be the best default. with 16byte objects, 32bit signed indices adress 32gb. most programs would never fill the entirity of memory with one vector anyway. (another aside, a parameterised index could give you more type safety too i.e. telling people which indices work with which vectors..) I realise there are areas where full on 64bit indices are genuinely useful too.
Great, thanks for the clarification.
Not again... The previous RFC was discussed extensively for a long time and the core team made their decision. Yes, sometimes owners of the project have to make unpopular decisions, it's their right. I see no reasons why this RFC would be treated differently from the previous one, it adds nothing new.
I agree with this, but sometimes it's true that you actually don't care about what size it is, which means that technically you should be reaching for a bigint (http://en.wikipedia.org/wiki/Arbitrary-precision_arithmetic). One day I hope that Rust incorporates such a type into the stdlib (we'd have had one years ago if GMP's license were compatible with ours).
Exactly, me either. Coming from languages like Python and Go I find myself reaching for `int` and `uint` all the time when I don't care, because that's the easy answer (not that it's the right answer). Personally, changing to some slightly more obscure version of pointer length type identifiers won't change that. What WILL change it is `i`, `u`, and unsuffixed literals being changed to something more sane (such as the above mentioned RFC). So I can only imagine there is a HUGE number of other developers out there who will do the same as me. ESPECIALLY once 1.0 lands and you start attracting a larger number of Python, Ruby, and Go devs. 
Adding x to the end is not clear at all. If I wasn't following rust rfcs, I would have no idea that this is for platform-sized ints. If we are going to change the name at least it should make it clear what its doing.
Whatever YACC takes, which is I think LALR(1).
If you only need it locally, pass `--prefix=whatever` and don't run it as `su`. 
I had this problem a while ago as well. I'm fairly sure what solved it was installing `texlive-xetex`, even though I had the `latex` packages.
It's a *common* practice, but not a *good* practice. C is hell, for this reason. C# is so much nicer to work in, because "int" means the same thing on all platforms.
By the time you are putting 2^32 (4 billion!) items into a Vec&lt;T&gt;, you need to stop using Vec&lt;T&gt;. Think about it -- inserting a single item could mean reallocating and copying. That's a *tremendous* cost in resources, all because you're using a collection that is not appropriate for this size. 
&gt; int and uint are a type that is 'native' and can be handled in a single cycle and is thus fastest This is sort of a myth. It implies that other size ints are somehow slower. On most modern platforms (x64, x86, RISC, ARM), it's just not true. Loads and stores using u8, u16, and u32 all generally take the exact same resources and latencies. On x64, loading u64 takes the same resources as loading u32. So just because some PDP-11 had different latencies 40 years ago, can we please stop distorting the design of new languages?
in C you use size_t and offset_t, and let the stdlib deal with how many bits are required. Could Rust do the same?
I agree. Also, this could be available as an optimization, under the control of the developer. If you control the code running in your process, and you're happy with SEH (on Windows) or SEGV_BNDERR (on Linux), *and* you reduce the cost of your bounds checks, then why not have this as an option? 
In the current version we only do incremental codegen - i.e., we always type check everything, it is only the llvm phase which we skip for already compiled code. This makes the dependency analysis much easier (although it is still very complicated - that is basically why it doesn't work yet!)
If you're learning a low-level language and don't care how big your data is in memory, you're going to have a bad time.
By all platforms you mean one platform?
AFAIU the advantage of strictly using `uint` for `Vec` is that overflow is conceptually impossible, meaning we don't need to bother checking for it. If you use a `Vec` with `u32` indices on a 64-bit platform, any `Vec` containing objects whose size is a single byte can overflow, and would require an overflow check on every push (specifically on this line: http://doc.rust-lang.org/src/collections/vec.rs.html#1122) in order to maintain safety.
I am a systems developer, I've been coding in C (and also Python) for years, and having to think about sizes is annoying and rarely correct. It's a fact of life, but I'm inclined to agree with you that it's not a _good_ fact of life. For instance, let's say you read in a 4-byte value from the network. Then a long while later in the code you realize that you want a disk file that stores an array of 8-byte structures, indexed by this value. You'll probably get the calculation wrong. Take a look at [&lt;elf.h&gt;](http://manpages.debian.org/cgi-bin/man.cgi?query=elf&amp;apropos=0&amp;sektion=5&amp;manpath=Debian+7.0+wheezy&amp;format=html&amp;locale=en) for an example of this being super painful. The _size of each structure_ is also stored in the file! If you accept and parse untrusted ELF files (e.g., you're an app store or a virus scanner or a kernel), you'll probably write an exploitable integer overflow bug on your first try.
In this RFC they propose changing the suffixes to `ix` and `ux`.
No, I mean all platforms. C# runs on Windows, OS X, Linux, and even my son's EV3 Lego microcontroller. The language is an open specification. Mono proved you can implement it anywhere. Are you completely unaware of this, or just trolling? Also, `int` means `i32` in C#, regardless of whether you're targeting x86 or x64.
C# is on multiple platforms.
That sounds like a toolset limitation. Not an inherent reason not to use new hardware capabilities.
Hard to believe how difficult it is to come up with a name for these types which doesn't have *some* serious problem with it. To summarize: * `int`/`uint` carry a strong connotation that these types are safe and useful in most circumstances, which could not be further from the truth. This connotation is strong enough, and the need for a "default integer type" is great enough, that people are likely to use these types liberally under their current names, even if Rust's documentation and the culture both strongly discourage their use. * `iword`/`uword` ~~are the most technically accurate names by far~~ (edit: see strncat's response), but thanks to Win32 and x86, they carry some connotation that the underlying type has a width of 16 bits. (Nevertheless, ~~I personally think these names would be the best choice~~. The term "word", in its proper sense, is still a pretty strong part of the programmers' zeitgeist; I can't imagine many newcomers will genuinely jump to the conclusion that these types are 16-bit, in this day and age; and even if they did, the existence of `i16`/`u16` should quickly clue them in to the fact that something is wrong.) * `iptr`/`uptr` are technically misleading. The types are not necessarily casted pointers, and they do not interact with pointer types in the language (except for being an attractive target for casting raw pointers). * `ips`/`ups` (where "ps" stands for "pointer-sized") do not look like type names, almost objectively. They are also acronyms with catchy, meaningless and distracting pronunciations. * `idiff`/`usize`, `iaddr`/`uaddr` and `index`/`uindex` are technically misleading. The types can be used to represent the size of an object, memory intervals, array indexes, array index intervals, memory locations, casted pointers, and many other things. Selecting one of the types' roles, and making it "canonical", seems inappropriate. * `imem`/`umem` are technically misleading. The types do not interact with memory at all, except in the vague sense that they are best used for "low-level operations" which "directly" interact with program memory. In particular, they don't represent memory locations or blocks of raw memory. * `ivar`/`uvar` (where "var" stands for "variable-width"), by an incredibly unluckly coincidence, clash with one of the most widely-used nouns in the entire discipline of programming. * `ix`/`ux` and `intx`/`uintx` pretty much represent the community throwing our hands up in the air and deciding that these integers can only be described using a wildcard character.
Right, but my point is people are lazy. If `ix` or `ux` are still the shortest thing to type people.are going to use that because they don't have to think. I'm guilty of it too, if I don't have a pre thought out reason for caring about the size, I'll default to whatever is easiest or takes the least thought. Even The Guide uses `int` in places it didn't need to because its easier. There are plenty of times where the size of data in memory doesn't matter enough for me to care. And I'd have a hard time imagining I'm the only person using Rust like that! :)
Is it going to be a part of the standard library when Alpha or version 1.0 comes out?
It's not a toolset limitation. Supporting asynchronous exceptions causes a lot of code bloat and cripples the optimizer a lot more than normal exceptions do. You're asking for the compiler to assume that every single operation can have a side effect (throwing an exception) so code motion is pretty much dead.
&gt; While you can use side tables (and most C code probably will), you can generate a bound in a single instruction (BNDMK) if you have a base pointer and length. &gt; &gt; Before a loop for example you would call BNDMK to generate the bounds for all relevant array accesses. Then you just call BNDCL (check lower bound) and BNDCU (check upper bound) for every access. You're assuming that this is faster than branching... there's a reason the x86 `BOUND` instruction was removed in long mode. I think the fact that compiler optimizations would need to be crippled for this is worse than the alternative. &gt; I'm not sure why the Rust runtime can't just catch SEGV_BNDERR on every thread, and then panic! the task if it occurs? Is there a good reason other than that it adds overhead? As I said, LLVM doesn't support asynchronous exceptions. Even if it did, enabling support would imply *even more* code bloat and missed optimizations from unwinding. Supporting MPX without false negatives and even false positives due to optimizations would require lots of optimization constraints...
Go doesn't define them as pointer-size. They even used to be 32-bit on x86_64.
&gt; the latencies for all operations are the same for both 32-bit and 64-bit Lots of the operations are slower on 64-bit integers. See the thoroughly researched Agner CPU instruction tables.
It depends what you want to happen when a bounds-check failure occurs. I'm assuming that you use one of two policies: 1) fuck it, nuke the whole process from orbit, or 2) reliably kill the current task. I assume that we do not want to keep running code in the current task, if this kind of exception occurs. This works reliably and efficiently in the Windows world. Some compiler runtime checks are emitted as `int 0x2c` calls, and the exception-handling logic knows how to walk the stack and deal with it. Running the dtors is the only hard part. What "code bloat" are you referring to? Are you referring to registering exception handlers every time you enter a throwing scope, and unregistering them on exit? Then don't do that -- that's crazy-expensive. Use table-driven lookups, which are much faster. MSVC uses registration on x86, and table-lookup on x64 (and ARM). The table-lookup approach is much faster, and keeps all of the exception handling goop way off in cold pages, rather than being right up front in your instruction cache. This can be done efficiently; there are plenty of examples of that. And if people gripe about the efficiency cost of bounds-checking, then using hardware bounds-checking to eliminate that cost may be just the thing to drive adoption of Rust.
&gt; It will always be faster to use machine-word-size values in many circumstances. No, in fact on x86 operations on 64-bit integers are often slower. They're also twice as big so they require twice as much memory bandwidth and fill twice as much cache / memory. The vector registers operate on *twice as many* 32-bit integers per instruction so the maximum throughput is 2x higher.
It *is* reasonable to use `Vec` for that many elements. E.g. one can preallocate enough space in one go, or one can rely on the allocator/OS rearranging page tables instead of actually copying. (In any case, it is relatively likely that whatever one is doing that wants such a large number of elements in memory at once is significantly more expensive than the cache- &amp; SIMD-friendly copy.)
&gt; Think about it -- inserting a single item could mean reallocating and copying. That's a tremendous cost in resources, all because you're using a collection that is not appropriate for this size. It doesn't actually imply that if it's implemented well. It's virtual memory so only the page tables need to be updated. It's still `O(n)` but it's really `maximum_address_space / page_size` where `page_size` is now often 2M thanks to transparent huge pages. Rust's memory allocator currently doesn't do this... but it should considering that `mremap` is there for this. Anyway, the entire point of vectors is that they use geometric growth. The throughput doesn't drop as the size increases because the reallocations are far less frequent. It's only a problem if you care about latency... but you have very few guarantees about the latency of dynamic allocations anyway.
At the very least, it runs on both 32 and 64 bit computers. (*edit* ... which is the main relevant platform difference for this int name discussion, downvoter.)
I'm fine with the current int/uint. Although if this lands please consider intz uintz as the z reminds of size and x dont't carry any meaning.
There is meaningful validation: the `unsafe` pops up, and (should!) make you and any code reviewers think twice (or thrice, or more) about what is going on.
&gt; Note that I'm not trying to advocate or defend this practice, just give broader context. The context is that modern C code uses integer types that are well-defined for the use case. It's far easier to write `uint64_t` than using `unsigned long long` and *hoping* it's what you want anyway. &gt; In practice an implementation may define int to be the type that the target is most efficient In practice, that's rarely what's done. The `int` type is almost always 32-bit on architectures with 16-bit and 32-bit registers. &gt; Also, this didn't originate with C. C has integers based on the pointer/object size (uintptr_t, intptr_t, ptrdiff_t, size_t, ssize_t) as does Go (intptr, uintptr) but there's no substantial difference between how they do it.
It is unlikely to be available in the stable branch for a while (the interface makes it very easy to screw up, and it would be good to spend more time trying to improve it).
You got me thinking about operators. Since my thoughts got longer, it evolved into a blog post: http://beza1e1.tuxen.de/articles/operator_soup.html Not really relevant for Rust. Just a thanks for writing inspiration. ;)
* `iwide`/`uwide` have the obvious problem that they imply that the types have an unusually wide bit-width, when they do not. * `iwidth`/`uwidth` and `ipwidth`/`upwidth` clash with the noun "width", which is very widely-used in graphics programming, video-game programming, audio programming, physical simulations, layout of graphical elements, and other disciplines which Rust is targeting. In many popular naming conventions, "iwidth" would also imply "an integer type which is used to represent a width", which is inaccurate. * `ipw`/`upw` are initialisms, which tend not to work well as typenames. They are slow to mentally "scan"; they have no surface meaning to help aid the programmer's memory, unless they deliberately expand them into "int pointer width" or "unsigned pointer width"; and stringing together three letters with no pronunciation will contribute to Rust's already-significant "line noise" effect. * `intw`/`uintw` and `intpw`/`uintpw` are subjectively ugly, in the same way that I find `intx`/`uintx` to be ugly. They take a naming convention which is common in other languages but not used elsewhere in Rust, and they add a suffix to it. This makes them supremely unfamiliar and strange. (I do still prefer `intw`/`uintw` to `intx`/`uintx`, though.) (Sorry to immediately rip into your proposals, but given my last post, you really shouldn't have expected anything else :P)
&gt; On x64, loading u64 takes the same resources as loading u32. That's not true. It takes up twice as much space in memory and cache. Loading an array of 32-bit integers takes half the resources relative to 64-bit integers. Many of the scalar operations are a bit faster on 32-bit integers on most x86 CPUs and vector operations have 2x more throughput. &gt; It implies that other size ints are somehow slower. On x86, scalar 32-bit and 64-bit arithmetic is close in performance but 32-bit has the edge in more cases. Scalar 8-bit and 16-bit arithmetic is *slower* almost across the board. Vector operations turn the table because throughput doubles when you cut the size in half - assuming that the operations you want are supported for the smaller size.
No, it isn't. If you are managing GBs worth of data, you need a fundamentally different data structure than Vec. "Relying" on the VMM to deal with it is 1) a rocket-science answer that hasn't been implemented, and 2) shows that you're trying to make a data structure that does not fit the situation, into one that does, by requiring changes elsewhere in the system. 
&gt; ... but you have very few guarantees about the latency of dynamic allocations anyway. ... when you use the wrong data-structure for the task at hand.
The `int` and `uint` types are used for sizes, indexing and offsets but that's unclear from the naming. It's bad practice to use them for anything else... because it's needlessly hurting performance / memory usage and will cause portability bugs.
&gt; in C you use size_t and offset_t, and let the stdlib deal with how many bits are required. Could Rust do the same? How is that different than what Rust is already doing? The `int` and `uint` types are defined as being large enough for any valid size / index / offset. The maximum valid object size is `int::MAX`.
&gt; It takes up twice as much space in memory and cache. Obviously. By "resources" I was referring to micro-architectural resources, such as reservation stations, branch prediction, space in cache load/store queues, etc. &gt; On x86, scalar 32-bit and 64-bit arithmetic is close in performance but 32-bit has the edge in more cases. Which is a good reason to push people *away* from `sizeof(uint) = sizeof(ptr)`, not toward it. Which I agree with. I really dislike `uint` in Rust because it pushes people toward dependencies on machine architecture, and discourages people to choose the *right* size for an integer in each situation. The right size usually doesn't depend on whether you're running on a 32-bit or 64-bit architecture.
&gt; Yes, sometimes owners of the project have to make unpopular decisions, it's their right. Stop calling it a community project when a corporation makes all of the decisions then. Using the community for free labour while making all of the decisions behind closed doors like this != community project. There wasn't even a public justification for the decision.
&gt; If there were a good popular alternative it might make sense to change. The overwhelming consensus on the last RFC was that they should be renamed. The `uptr`/`iptr` and `umem`/`imem` names were the most popular choices. It was rejected, but the justification for it wasn't provided. No one actually argued the case against renaming them...
If you don't have clear bounds, then you either have to use a big integer or start enforcing clear bounds. Choosing to write incorrect code is your prerogative.
If you don't have clear bounds, then you either have to use a big integer or start enforcing clear bounds. Choosing to write incorrect code is your prerogative. Go only fixed-size integers built-in to the language so it's not different there... Python and Ruby use big integers and if that's what you need then you should be using a big integer in Rust too.
&gt; when they don't care about size How can you avoid caring about the size without using a big integer? If using `i8`, `u8` or even bool (`i1`) isn't enough for your use case then you care about the size. I guess what you really mean is "when people don't care about correctness" which seems to be the norm.
&gt; Hard to believe how difficult it is to come up with a name for these types which doesn't have some serious problem with it. The overwhelming consensus of the community is that either `uptr`/`iptr` or `umem`/`imem` is a major improvement over `uint`/`int`. Your point of view was shown to be a minority one on the previous RFC but you're presenting it as if it's not... &gt; iword/uword are the most technically accurate names by far, but thanks to Win32 and x86, they carry some connotation that the underlying type has a width of 16 bits. (Nevertheless, I personally think these names would be the best choice. The term "word", in its proper sense, is still a pretty strong part of the programmers' zeitgeist; I can't imagine many newcomers will genuinely jump to the conclusion that these types are 16-bit, in this day and age; and even if they did, the existence of i16/u16 should quickly clue them in to the fact that something is wrong.) It has nothing to do with word size. These types will be 64-bit on an architecture with a 64-bit address space and 128-bit integer registers. The x32 ABI is an example of an existing ABI where integer registers are 64-bit but pointers are 32-bit. It's just plain wrong.
Whats wrong with good old size_t and ssize_t? They have more meaning than intx and uintx, and they are well known by most programmers.
exactly, same reason why there won't be any built-in way to `derive` from unsafe traits.
&gt; I see no reasons why this RFC would be treated differently from the previous one, it adds nothing new. This is simply false. The previously suggested names were rejected by the core team because, among other reasons, they were deemed not familiar enough. So this RFC is suggesting different names which are more familiar. That's what's new.
&gt; There are plenty of times where the size of data in memory doesn't matter enough for me to care. This isn't why you need to care about it. You need to care because these are *fixed-size* integers with a lower and upper bound. If you aren't certain of the bounds then you should be using a big integer. &gt; And I'd have a hard time imagining I'm the only person using Rust like that! :) No, you're not the only person choosing to write incorrect code. It's different than ending up with bugs despite putting in a modicum of effort to get it right.
there [was](http://discuss.rust-lang.org/t/a-tale-of-twos-complement/1062) a public justification, but the key element was renaming it would break nearly everything, nearly everything would have to be fixed, and it would delay 1.0. But I think most people would be fine with a delay for that. Because if this work isn't done now, it will never happen, which will just mean lots of confusion down the line. This is going to going to be rightly seen as this pointlessly confusing element of rust. Most outsiders, if they see the variable type "int," it's going to be a big flashing red sign to them "this is the integer type you should normally use." Then they go to the docs and the docs say "most of the time you should not use int," and they'll be like "they shouldn't have called it int."
Ah, I was reading the Go manual and saw that uint is "either 32 or 64 bits" (http://golang.org/ref/spec#Numeric_types) and presumed that it was correlated to platform. Also, it appears that there's no such thing as `intptr` in Go (http://play.golang.org/p/StcpbJ6HXm).
&gt; ... when you use the wrong data-structure for the task at hand. No, as I said you have very few guarantees about the latency of dynamic allocations. Dynamically allocating a linked list node is certainly not `O(1)` in the worst case. The worst-case time complexity is `O(address_space)`. It could be `O(log2(address_space))` if the operating system and memory allocator were designed very carefully but that's not the reality. The cost of copying a large array isn't as bad as the other worst-case costs until it gets far larger than 4G.
&gt;&gt;Also, this didn't originate with C. &gt;C has integers based on the pointer/object size What "this" is referring to is types specifically named `int` and `uint` that are based on pointer/object size. I'm an advocate for renaming these types to `intptr` and `uintptr` in Rust, but as long as we aren't falling back to `int` anymore I don't really care.
I think "int" and "uint" should be the bigints, since the names "int" and "uint" hint that they can represent ANY (non-negative) integer. 
yup but you'd rarely fill the whole of memory with one array. the one time I'd personally have ever got close to needing to do that is in scene clustering, loading every mesh , combining it, dividing it.. the entities are vertices, each going to be typically 16-32+ bytes. (the purpose of clustering is to divide it into smaller chunks within which smaller indices &amp; quantities can be used. anything that actually ships uses data thats' already been divided up somehow). so i'm going to need a 32-64gb machine to reach that limit. in practice any really big data set is going to be images. lets say you're dealing with scanned point clouds or something, you're going to be able to give your situation special treatment. ok, so someone might be datamining the entire text of wikipedia or something, which would indeed be chars. but I suspect anyone doing that would have some specialised data structure eventually too. anyway I don't question the safe default, thats' fair enough. just explaining that for my purposes, 32bit indices are the way to go program wide, and if that overflows I've got bigger problems in my approach. 
&gt; In practice, with reasonably small Vec sizes, the distribution of response times on vector growth is reasonably consistent. That begins to be untrue with extremely large vectors. In practice, dynamic allocations often take a huge amount of time even though the average case is quite fast. It's not any different than the spikes in latency for vector reallocations. Memory allocators rely on amortizing the same kind of huge costs. It's often fine when the latency requirement is a very soft one.
Also, keep in mind that there is no such thing as a "single" cycle instruction on modern machines, for many reasons: 1) multiple execution units, 2) branch prediction, 3) micro-op fusion, 4) cache state, 5) micro-architectural resource availability (reservation stations, register renaming, etc.), 6) data-flow dependencies between instructions; etc. The micro-architecture is actually far more complicated than the macro-architecture (the ISA that we see when doing assembly programming). ARM has a simpler micro-architecture, but even ARM is getting more complex as time goes by. Most of the big RISC processors are also far more complex than the name "RISC" implies.
Look at the distribution of times. Now consider the distribution of times for operations on a vec with total size around 50 KB. Now compare that distribution with a vec with total size around 50 MB, or 50 GB. The relationship between these three cases will not be linear. 
would a check before the *realloc* (capacity change) be a reasonable compromise to get safety and compact indices.. if you're doing some 'push_back()' repeatedly, the capacity would only be changed infrequently I guess. I could definitely accept a check &amp; 'panic!()' there. 
I think you're misunderstanding my meaning. Just because I use `int` instead of `i8` or whatever my code could effectively use isn't incorrect, its unoptimized. You say it only takes a minutia of effort to write correct code, but that's not reality. It takes far more brainpower to write optimized correct code. By your logic everyone who writes in Python is wrong because they could be writing EXACTLY what they need in C++ or Rust. What I'm saying is that devs are lazy. As trivial as deciding on optimized integer width is, there are times when it doesn't matter. And in those cases people pick what's easy and takes the least thought. If I took the time to perfectly optimize all code I wrote I'd be out of a job. Correct to me means no overflows or logic errors...not perfect memory optimization. 
oof, sorry. Didn't know other people had focused in on that part as well. No intention of piling on it. I think it was the positioning of sentences which suggested to me that "breaks everything" was the key reason: &gt; Ultimately, however, we have chosen to leave things as they are. Given that changing the name of the type int would affect literally every Rust program ever written, the bar for making such a change, particularly at this point in the release cycle, is quite high I'm not involved in rust stuff a lot, so obviously take my view with a grain of salt; I just think its confusing to have a type named 'int' that is not the suggested default integer type. In almost every other popular language, when there is an 'int' type it is either _the_ integer type or a sensible default integer type. So any other name, I think, would be an improvement. But: I understand I'm one person, I don't know about how those alternate branches went, maybe intx would be confusing to other people. Also, for people learning rust as a first language or through the official docs it won't be a problem.
Thanks! (sorry, I was away from my laptop and I didn't have the issue number handy :)
The time is proportional to the size of the vector since that's how much it has to copy directly or via page table updates. The memory allocator will find the memory through a red-black tree so it's `O(log2(address_space))` worst-case whether it's small or large. The `O(address_space)` issue kicks in due to design issues in OS memory allocators.
Somebody already suggested `iarch`/`uarch`?
There is a meaningful gain in safety because many people don't think about thread safety when writing FFI bindings. In fact, very often neither did the authors of the library the bindings expose, which means that to figure out how to actually make it thread safe would require a significant investment of time with the original codebase. Thus, I'd say it's a pretty sensible default is to assume it isn't. One line is pretty minimal overhead compared to the amount of time it will take to verify that the library is really thread safe. Also: if you can use `Box&lt;c_void&gt;` you can often also use `&amp;'a mut c_void`. In either case, if the underlying C violates your uniqueness constraint, you're screwed :) (Yes, I realize `&amp;mut T` isn't `Send` currently).
Sure, you're right, but if you were going to default to using `int`, switching to defaulting to `i32` is generally a better choice, as it doesn't have the cross platform compatibility problems and is smaller than a pointer sized integer on 64 bit platforms. If it were possible to use bigints without a lot of other tradeoffs, and they were natively supported in the prelude, then I'd recommend that. But for now, when you need an integer and don't really have a good idea about how big it should be, `i32` is a good default fallback, and the fact that the size is specified means you are more aware of when you need to think about overflow than with simply `int`.
Vector operations aren't all that common unless the code is highly manually optimized. The cache issue is big, though. People always seem to forget that memory accesses aren't random.
In general it's going to be very rare to have to implement these at all, so seeing these in non-FFI code would always be a red flag to me (just like `unsafe`).
I like that suggestion very much! Meaningful (unlike many of the other suggestions); uses an established contraction ("arch"); fits the naming scheme which everybody seems to be naturally gravitating to; only five characters long; can be pronounced with only two syllables; doesn't have any weird semantic clashes, except with "arch-as-in-archway"; and feels just foreign and strange enough to encourage newcomers to treat the types with respect, but not so strange that programmers won't be able to get used to the names with continued exposure. The meaning is a bit unintuitive to people who aren't OS programmers ("I don't get it, why does Rust call `uintptr_t` an 'unsigned arch'?"), but since people are seriously considering `intx`/`uintx`, I expect we can get away with that :P My only niggling worry is that "arch" isn't a noun by itself, and all of the most popular suggestions ("ptr", "diff", "addr", "size", "index") were meaningful nouns. However, a lot of people seemed happy with `imem`/`umem`, so that may not be a problem in practice.
I don't think that's true. We already have to do a bounds check every time before that increment (to make sure we're not going over the capacity) so most of the time that check would already cover it. As /u/doberkatops said, the length check could be done on capacity change, and if it would overflow the program could panic. That might not be appropriate in all program contexts but there are certainly many times where it would be nice to have smaller indices than `uint` (and this comes up in other scenarios as well: since Rust doesn't do implicit widening, lots of indices are stored as `uints` when they really shouldn't be).
All I'm saying is, that's a limitation of the current toolset. Not an inherent in the language, or the machine under the language. "LLVM doesn't support that" doesn't mean it can't be done, and done well.
There are processors that cannot address bytes within a word. On those processors pointers to bytes are larger than pointers to words (and therefore to structs, which are word-aligned), because they contain a word-pointer and the offset of the byte within the word. The offset is used to shift and mask the byte from the word that contains it. 
This isn't really due to type inference per se, rather it's due to the fact that trait resolution provides a limited means of type-level computation and that can (with relatively little effort) be exploited to provide more general computational facilities, like interpreters. This has been around in Haskell for a very long time. As soon as you have multi-parameter type classes and functional dependencies you can do this. There are other combinations of extensions that allow similar things. Since almost nobody sticks Haskell 98 anymore, that ship has already sailed for them… If anything, this is probably more of an indictment of how traits/type-classes work. For instance, Agda implements a feature that is somewhat similar to type-classes but without the prolog-like facilities enabling this hack. As for type inference, lack of global decidable type inference is not a source of unsoundness in general. Dependently typed languages like Agda, Coq, and Idris do not have global type inference but (aside from implementation bugs) are sound and do not allow unbounded evaluation of generally recursive programs.
IOW, I wouldn't say this is a surprising result if the implementors knew what they were buying into with the traits mechanism. From Graydon's comment on twitter, it sounds like he realized this quite awhile ago.
The disagreements seem to be about the *names* of the types, not about the semantics. These arguments are difficult to resolve because the term "int" doesn't express what is being represented. The terms size, offset, ptrdiff do.
Except that like other suggestions it has no numeric connotations. The other problem I see with nearly all of these suggestions is that they make sense only if you've been following the discussion. Look at some of these suggestions as a new Rustacean with no prior knowledge of this discussion (and perhaps systems programming at all) and see if you still feel the same. 
I think the first point is right but not strong enough. `int` is a far to much standard name... It will be hugely misused no matter how much you advertise not to use it as default integer. The second point is irrelevant, none of the suggestion are perfect, but every of them are far better than `int/uint` 
&gt; Except that like other suggestions it has no numeric connotations. I don't know what you mean by this. Could you clarify? &gt; The other problem I see with nearly all of these suggestions is that they make sense only if you've been following the discussion. Look at some of these suggestions as a new Rustacean with no prior knowledge of this discussion (and perhaps systems programming at all) and see if you still feel the same. If your standards for a reasonable type-name is that it must be self-explanatory to a beginner, without any context or explanation, then I'm afraid you're going to be disappointed no matter what name we choose for our machine-dependent integers, even if we stick with `int`/`uint`. If your standards are more lax, and you think that a small amount of exposition is acceptable, then I don't see the problem. Explaining the meaning of the name `uarch` (properly, in a way that makes it into a solid memory aid for the nature of the underlying type) would take perhaps two sentences in the tutorial, and one in the manual.
Note: my main point is there has been little-to-no discussion and they are ignored without acknowledgement whenever someone dissects the justification as incorrect. Maybe they are fundamentally flawed, but everyone's comments seem to focus *only* on the "can't break the world" aspect, if they mention the justification at all. &gt; The second point is irrelevant, none of the suggestion are perfect, but every of them are far better than int/uint I think you're missing the point of this point. The code-appearance/understandability of code that uses `int` is much better than code that uses `imem`. That is, `imem` doesn't feel like an integer type, and it looks fairly ugly (to the point of making Rust unpleasant to write). Maybe `imem` is better than `int` in many other aspects, but this specific point is a pro in favour of `int`, in the core team's opinion. (Of course, the decision isn't justified by a single point: it is the balance of pros and cons, trade-offs and compromises that guides us. In this vein, the first "not strong enough" point is just one piece of the puzzle, it doesn't have to be justification all on its own.)
strncat's point is that regardless of whether you choose `i8` or `int`, if you aren't enforcing the proper bounds for the data contained within then your code is susceptible to overflow bugs. This is a separate concern from optimization.
Maybe it's impossible, but Vec::&lt;u8&gt;::with_capacity(-1u) is still a crash in rust.
For me it is an out-of-memory abort, which seems like expected behaviour? An allocator can't satisfy a request for that much memory and so will fail, which Rust currently handles by exiting.
&gt; iword/uword ... but thanks to Win32 and x86, they carry some connotation that the underlying type has a width of 16 bits. I like 'word', some RISC asm's do use it for a machine word of 32bits/64bits. and of course win32 is just carrying a legacy 16bit machine word. i'd be very happy to have int &amp; word side by side, and to see word not being 16bits. I think it will be clear its' not the win32 meaning, and that could be prefixed if you really were translating some legacy code.. but better to use 'short' if you really want to name a 16bit quantity something other than u16/i16
As an outside observer and programmer who has only been to two Rust talks and vaguely waved my hands in front of something. If the intended default choice for defining the type of an integer is not int, your language is wrong. Period. You can not simply fly in the face of 30 or so years of programming history for the sake of about 2 years worth of existing Rust code and expect documentation to suffice.
Rust's `int/uint` are not only used for container sizes. They are used for offsets, indices, holding casted pointers and the others. So it is deemed that the names should not favour any specific use case. The fact that `intx/uintx` have "less" meaning is an advantage here. 
The problem with `iarch/uarch` is the same with `imem/umem`: they stress the `arch`/`mem` part, not the `i`/`u` part, so some may find them not looking like integer types. (The core team experimented with a variant of Rust that used `imem/umem` and decided against them.) 
&gt; You're assuming that this is faster than branching... there's a reason the x86 BOUND instruction was removed in long mode. I think the fact that compiler optimizations would need to be crippled for this is worse than the alternative. Yes, that's one of my assumptions. Seems kind of pointless to add the instructions and registers if it wasn't any faster? The advantage over BOUND seems to be that it reads the bounds from registers instead of memory? 
So it is going to be a part of the stable branch in the future. Good enough with me. 
It's weird enough that new users will look for explanation, which is good. And "x" for unknown seems quite right.
&gt; You can not simply fly in the face of 30 or so years of programming history Yes you can. :)
I don't think it needs to be hazardous; we could implement `Index` on Slices etc for every integer type (with a `#[cfg]` for i64/u64 only on 64-bit platforms). I think this would remove *most*, if not all, uses of `int`/`uint` in application code, combined with the i32 fallback.
Ah, yes, I forgot multidispatch. Great idea, let's try it :)
"What on earth is `intx`?" is exactly the reaction expected from newcomers. They'll have to actually read the manual and won't make incorrect assumptions. But after reading the manual, I hope that "intx -&gt; int + x -&gt; int with platform dependent sizes" can be a quite clear connotation.
More importantly than Mono, NETMF proved that it really can be *anywhere*.
Ok. If you're talking about YACC itself, I think it's LALR indeed, but if I recall correctly, Bison, for example, can take some LR grammars, hence the question. :)
&gt; Yes, that's one of my assumptions. Seems kind of pointless to add the instructions and registers if it wasn't any faster? The advantage over BOUND seems to be that it reads the bounds from registers instead of memory? Extra register traffic when you almost always need the length in a register for other uses like a loop control variable seems like it's only going to make it slower. If extra registers were helpful they would just add them to the base ABI but x86_64 is already at the point where the diminishing returns are not very compelling. To me it seems like the only advantage is how far it's able to get with code that's mostly uncooperative. It only needs integration into the compiler and places that allocate memory (primarily libc) to provide a faster and more precise implementation of ASAN. It's not like registers in the ABI actually correspond to what the real underlying registers look like. For all we know, they implemented it all in microcode. This is the company that [built a sophisticated rootkit into every CPU](https://en.wikipedia.org/wiki/Intel_Active_Management_Technology) that's running even if the computer is off... They'll put absolutely anything on the CPU die because they have no idea what to do with the ever increasing space now that it isn't giving free performance.
&gt;I know some people want it, but others don't and prefer using try! for error handling. Can you elaborate? At the moment, I can't think of any case where `try!` would be preferable to do notation.
I think he means you shouldn't use the exact name that's been used for 30+ years in a different manner. i.e. if you're going to include the type `int` in your language, but it's *not* going to mean what people think it does, that's a bad idea. If you want to fly in the face of 30+ years, don't include `int` at all and instead use something more properly named. Because as long as `int` is included, and *not* the default integer type, you're setting yourself up for failure.
Rust converts between pointers to different types or arrays of types all the time. It also does many things in terms of bytes like memory allocation, just like C. It has a pervasive need for types able to address every byte in memory but not really anything else. A type would have to be enormous in order to use a 32-bit integer instead of a 64-bit one for indexing an array...
Not very well, the argument I have heard made is that try! is the idiomatic Rust way of doing this, whereas do notation is not. I'm not sure I know the arguments beyond that.
You could also do: struct Data&lt;T&gt; { type_: T, data: ... }; enum NodeType { Foo, Bar } trait Node { ... } impl Node for Data&lt;Foo&gt; { ... } impl Node for Data&lt;Bar&gt; { ... } impl&lt;T&gt; Data&lt;T&gt; { // shared methods }
&gt; Vector operations aren't all that common unless the code is highly manually optimized. There are many common looping patterns that get auto-vectorized by LLVM. Rust encourages people to use iterators pervasively rather than providing a wide range of more specific functions so it's a big deal. The SLP vectorizer is also enabled by default but I haven't seen that kick in much. http://llvm.org/docs/Vectorizers.html
beautiful, so it has been fixed the last months, I just didn't understand that.
&gt; Sure, you're right, but if you were going to default to using int, switching to defaulting to i32 is generally a better choice, as it doesn't have the cross platform compatibility problems and is smaller than a pointer sized integer on 64 bit platforms. I agree, it's better than an `int` default but I would prefer no default at all... because there is no sane default choice. &gt; If it were possible to use bigints without a lot of other tradeoffs, and they were natively supported in the prelude, then I'd recommend that. But for now, when you need an integer and don't really have a good idea about how big it should be, i32 is a good default fallback, and the fact that the size is specified means you are more aware of when you need to think about overflow than with simply int. If you don't know how big it should be, you need to use a big integer. If the language is encouraging you to do anything else then it's a poor quality language. Fixed size integers are an optimization for when you do have clear bounds on the range, which is actually the case for most performance critical code. However, high-level application logic almost always wants big integers...
It was only broken for a very short period of time. The SIGILL instruction is how `abort` is implemented - it's done that way to generate a core dump if they're enabled for debugging.
&gt; No, it isn't. If you are managing GBs worth of data, you need a fundamentally different data structure than Vec. The throughput is still great regardless of the size. The only thing that suffers is latency, which is only important in some cases. If you really care about latency, then dynamic memory allocation already has very worst case times *independent* of the allocation size... asking for 16 more bytes can trigger an O(n) request for more memory from the OS. Both of them are fast *in the common case* only. &gt; a rocket-science answer that hasn't been implemented It isn't a "rocket-science answer". It's already done by glibc but jemalloc doesn't take advantage of it yet because the Linux kernel doesn't expose a way to have `mremap` not unmap the memory. IIRC, the `mremap` API is now available on NetBSD and FreeBSD too, but I'm not sure if the details are exactly the same - it may not even have an equivalent to MREMAP_MAYMOVE at this point.
&gt; Also, AFAICT, Linux's mremap does support reordering the page table. (That said, I'm unsure how many allocators use mremap, I have a feeling jemalloc doesn't yet, due to a missing piece in its API.) It's used by glibc but jemalloc doesn't currently use it because it intentionally never unmaps memory in order to manage VM itself and `mremap` has no way to prevent that. I wrote a patch to implement it but didn't know how to generate interest in it.
It's not going to be the only platform-dependent integer type. There's a use case for the largest hardware supported integer type in places like big integer implementations, where that lets you do more work per instruction - even if they're a bit slower it will be faster.
Your comment is phrased as if it's an unbiased summary of the consensus / situation rather than your opinions on each integer. I can point something like that out without it being an accusation of malice.
&gt; Fundeps aren't actually turing complete, you need undecidable instances for that: Otherwise, GHC will enforce, in effect, that recursive arguments are strictly decreasing. Ah, you are right. That indeed seems to be the case now although I think it may not always have been so. &gt; Also, from my feeling of things, type families seem to be winning out against fundeps when it comes to actually being on a somewhat standards track, and they just made a huge, huge jump when closed type families were introduced: &gt; &gt; Finally, you can do all that nice stuff without a) enabling semantically questionable extensions and b) coding in bloody prolog. It's certainly an improvement. Sort of unfortunate that closed type families weren't the original focus… However, I would be surprised if one could do much interesting using closed type families without eventually requiring UndecidableInstances simply due to the fact that the current checker is not advanced enough to verify totality in many cases. Maybe that is not what you are suggesting though. So I think partly due to that, and partly due to the direction Haskell is heading (the implementation is outpacing the reasoning IMO), the avoidance of semantically questionable extensions is perhaps still a bit much to hope for in general.
&gt; The "_t" is (kind of) idiomatic in C, but not in Rust. Names ending in `_t` are reserved for the standard library, so it's actually bad practice to do it. It lets them add new stuff without worrying at all about conflicts. https://www.gnu.org/software/libc/manual/html_node/Reserved-Names.html
&gt; If the intended default choice for defining the type of an integer is not int, your language is wrong. The only sane default that's appropriate without considering the bounds is a big integer type, and Rust doesn't even have one in the standard library. A pointer size integer is not what you would expect from the `int` / `uint` naming, but that's what it is. It's poorly named, and I don't think `intx` / `uintx` is an improvement... it's syntactic salt in place of good naming practices.
Isn't type _inference_ in Rust already undecidable? There are many cases where the compiler prompts for annotations.
Has anyone considered this further? The advantages of bidirectional type checking are 1) that it's easy to implement since you forgo dealing with global information (unification) in favor of local flow of type information, and 2) that it scales very well to more advanced type systems. IIRC, one of the first uses of it was by Coquand for dependently typed systems, and of course many folks use it for that now.
&gt; If you don't know how big it should be, you need to use a big integer. I don't know. Big integers make a lot of things more complicated; they involve references, allocation, they aren't just simple copyable value types, things that you think of as constant time operations become O(log n), etc. It's less that fixed size integers are an optimization than that they are the simple, easy to reason about atomic types, and bigints are substantially more complicated. In a language where the main idea is to provide mostly low-cost, easy to reason about abstractions with safety, I'm not sure that using bigints by default really achieves that goal. And if overflow is something you're worried about, then why not worry about rounding upon division as well? I've seen of people forget about rounding with integer division, especially if they frequently are dividing values large enough that the rounding is less important, but then hit a bug when rounding causes a value to be 0 when they were expecting it to be non-zero. Should we then go on to make not just bigints, but a full Scheme-style numerical tower the default for arithmetic? That gives you even more safety in the face of likely programming errors, but also a lot more complexity and difficult to reason about performance. I'm really not opposed to your point of view; I'm a huge fan of Scheme, and have spent several years working with it professionally. I'm just wondering if that's the right line of reasoning for a language like Rust.
I'm not saying big integers should be the default or preferred integer type, only that it's wrong to avoid thinking about bounds if you aren't using them. &gt; things that you think of as constant time operations become O(log n) That will only happen if it would have overflowed a fixed-size integer, which would have been wrong. In that case, you have no choice but to enforce bounds or use a big integer. A wrapper can even do a specialized "small integer optimization" to avoid dynamic allocation when the integers are small. That would still have a large performance impact but if you don't have any bounds then you don't have a choice - and it's better than just allocating all of the time. &gt; It's less that fixed size integers are an optimization than that they are the simple, easy to reason about atomic types, and bigints are substantially more complicated. In a language where the main idea is to provide mostly low-cost, easy to reason about abstractions with safety, I'm not sure that using bigints by default really achieves that goal. I don't think there should be a default. The `i32` type is a reasonable default if there has to be one, but that *does not mean you don't have to reason about bounds*. If anything, you have to think about it harder because it's not as easy to see which types are used. &gt; And if overflow is something you're worried about, then why not worry about rounding upon division as well? Integer overflow is one of the most common bug classes, rounding issues are less common - especially since either floating point or big integers / decimals are used in most places where it matters. People already have to worry about division by zero and division of `INT_MIN` by `-1`, so worrying about floor vs. ceil vs. trunc vs. correct rounding isn't the biggest issue. It usually just injects some non-critical bias into an algorithm, making it less good at the job than it should be but not incorrect. &gt; I've seen of people forget about rounding with integer division, especially if they frequently are dividing values large enough that the rounding is less important, but then hit a bug when rounding causes a value to be 0 when they were expecting it to be non-zero. Should we then go on to make not just bigints, but a full Scheme-style numerical tower the default for arithmetic? That gives you even more safety in the face of likely programming errors, but also a lot more complexity and difficult to reason about performance. I wasn't even suggesting that big integers should be the default, only that using fixed-size integers means thinking about overflow. You don't get to use fixed-size integers without thinking about that unless you absolutely don't care about correctness. &gt; I'm really not opposed to your point of view My point of view is that correctness is important, not that big integers should be the default. I don't agree with the idea that we should just be lazy and use `i32` even when it's wrong. A great implementation of big integers does need to be available in the standard library to have any hope of convincing people to use the right type for the job and I don't expect Rust to provide that - which means it's encouraging bad practices.
&gt; you end up having to add "as uint" all over the place (or making your variables "int" to begin with) I encountered this annoying thing in the very first Rust program I tried to write.
To me at least, `arch` has a numerical connotation of being either 64, 32, or what other width your architecture uses. It actually sounds to me like the best suggestion I've heard for a renamed `int` type (since iptr and imem don't sound as much like numbers)
Would it be possible to create something like `do` notation using macros or syntax extensions?
I'm not a big fan of implicits. They're useful in some cases, but I've spent too much time struggling with them in Scala. There's Scala libraries where, instead of passing a parameter to a function directly, you have to set it as an implicit, and the error messages when you forget aren't great. I don't want anything like that in Rust.
What practical problem do HKT solve? Code examples, please :)
Not that I know of. /u/nikomatsakis would be the one to talk to about it. 
Suppose you have a series of database queries, currently you will have to write if let Some(x1) = db_query_1() { if let Some(x2) = db_query_2(x1) { if let Some(x3) = db_query_3(x2) { y = do_sth(x1, x2, x3) ... } } } If we had the `do` notation in Haskell we can write (just a quick example, won't actually work, but you get the idea) do x1 &lt;- db_query_1 x2 &lt;- db_query_2 x1 x3 &lt;- db_query_3 x2 do_sth x1 x2 x3 etc... which does the same thing but much shorter and easier to read. This works because Maybe (Option in Rust) is a monad and with a generic monad trait you can define the `&gt;&gt;=` operator (aka `bind` operator) which allows you to define the `do` notation (which is a syntactic sugar of `&gt;&gt;=`). To define a generic monad trait you need HKT. As someone else pointed out this can be achieved with the `try!` macro in the case of `Result`, but it is specific to `Result` and cannot be generalised (or will be very hard). The good thing about the `do` notation is that it works for all monadic types and is generic. Although as another pointed out above there will be some issues if we were to have the `do` notation in Rust (for instance some believe it's not idiomatic Rust), a lot of design work to do, and no guarantee that it will come to Rust.
Interesting. I'll try to talk to him about it at some point then, although I gather he's pretty busy at the moment… I did some work on bidirectional type systems as part of my research (mostly for type theory, but still relevant). Seems like there may be some strong reasons to consider it for rust, although maybe some of the more recent changes already move in that direction.
the ability to make generic code over smart pointer types might make life easier in a language that revolves around smart pointers. I don't think the addition of HKT implies that the language will become *about* monads etc.. its' just another tool to find uses in its own unique context. C++ has HKT but the syntax is horrible, rust can do better having a fresh syntax to built it into
I don't really get it. What are the possible use cases of this? Where is this useful? Can you give an example? Thanks. 
As far as I can tell, with this library you can't turn a type level number into value level number, nor you can turn a value level number into type level number. So, it can't be used for various useful things where parameterization on integers is needed, for example a `SmallVec&lt;T, N&gt;` class parameterized by stack buffer size. The language already has a tool for the second transformation - fixed arrays, but not for the first. What would be useful here is the implementation of associated constants, which are already accepted as RFC. Assume we can write a trait like this trait Extent { const extent: uint } then we can implement it for lots of fixed arrays (with a macro, of course) impl Extent for [(); 0] { const extent: uint = 0 } impl Extent for [(); 1] { const extent: uint = 1 } impl Extent for [(); 2] { const extent: uint = 2 } ... impl Extent for [(); 100] { const extent: uint = 100 } and have the first transformation available! Then it will be possible to write `SmallVec` as struct SmallVec&lt;T, N: Extent&gt; { // `N::extent` becomes available here as a number } let sv = SmallVec&lt;int, [(); 10]&gt;::new(); even without proper integral generic parameters. Just thinking aloud :)
I don't get what all the "elitism" crying is about. Believe it or not there *are* people in the programming profession that are bad at it. Some that are down right terrible. Some who shouldn't be allowed near a compiler. Traditionally, the industry has tried to deal with this problem by making a language so inexpressive that no one can do too much damage (not giving any names here but the initials are J.A.V.A.) and you can aim for mediocrity. If we could come up with a nice language that is productive and not esoteric while still keeping bad programmers out that makes everything all that much more efficient. Having said all that, I'm not sure if *Rust* is that filter. It's certainly an improvement over C/C++.
You can write data types that are generic over pointer types, e.g. enum BinaryTree&lt;P, T&gt; { Leaf(T), Branch(P&lt;BinaryTree&lt;P, T&gt;&gt;, P&lt;BinaryTree&lt;P, T&gt;&gt;) } can be instantiated with `BinaryTree&lt;Rc, T&gt;` for a thread-local sharable tree of `T`s, or `BinaryTree&lt;Arc, T&gt;` for one that can be passed between threads.
How is it possible to turn a value level number into a type level one? Is there any chance that (after 1.0) some feature will be added and people can do it in Rust? Edit: Oh I think it's "integral generic parameters".
As it happens I read about BiDiTC a bit in the past few weeks, so I'm less uninformed than I was. I have no idea how Rust's current type inference scheme could be formally classified... or what it even is, really. It used to be a HM derivative, then Niko changed it to [something different](http://smallcultfollowing.com/babysteps/blog/2014/07/09/an-experimental-new-type-inference-scheme-for-rust/), and I don't know if it's changed further since then, or only been extended (for higher-rank lifetimes, and so on).
&gt; (with a #[cfg] for i64/u64 only on 64-bit platforms) Why couldn't you index into an array in a 32-bit address space with a 64-bit integer? At worst you'll get an index out of bounds panic, but that's nothing new. Or the conversion will overflow, but we'll be getting checks for that too. Seems less drastic than #[cfg]ing it.
This is starting to remind me some ugly hacks that people do with C++ templates. If it is possible we need proper compiler support for it instead of implementing it like this. The only thing I can think of where this is useful is parametrizing types with values, e.g. vectors with size determined at compile time. Are there any other use cases?
Because of macro hygiene, the "self" and "other" identifiers you pass to the macro aren't the same as the identifiers you introduce inside the macro. You could do this: macro_rules! partial_eq_impl { ($lhs:ty, $rhs:ty, $sign:expr, $cmp:ident, $selfname:ident, $othername:ident, $arg1:expr, $arg2:expr) =&gt; ( impl PartialEq&lt;$rhs&gt; for $lhs { fn eq(&amp;$selfname, $othername: &amp;$rhs) -&gt; bool { let c = unsafe { $sign * $cmp($arg1, $arg2) }; c == 0 } } ) } partial_eq_impl! { Z, Z, 1, A, self, other, self, other } 
And this may help you: https://github.com/rust-lang/rust/issues/15682 
i would be interested in Vec&lt;T,Index=int&gt; , then you could just instantiate your vec's with whatever index you want; you could also get additional type safety/communication of intent by using more specific index types, e.g. make a new type for a Vertex Index, etc.
There is a work-around, you can write this: trait Foo { type AssociatedType; } fn foo&lt;F, T&gt;(f: F) where F: Foo&lt;AssociatedType = T&gt; { } However doing this has exactly the same drawback as *not* using associated types. 
I'm storing components as a box ComponentList&lt;C&gt;, which is a trait that can be implemented for e.g. VecMap, HashMap, BTreeMap, allowing different storage patterns for different sparsity components. This does add the overhead of a vtable lookup, but I think that is a fair trade off. By accessing entities with related components (currently with a macro, to be rewritten as a custom iterator) as an iterator, the AnyMap lookup only has to be done once. This is a big point, since the hashmap lookup is expensive. Fragmentation is solved by reuse, as HeroesGrave. I have versioning too, although I've moved to a single integer + bitshifting, although I need to check if this is actually an improvement over having two fields. Reuse is done in FIFO order, with a minimum free indices check, so fragmentation isn't zero, but this allows a smaller field for version (again, this is something I need to check is worthwhile). I also have a simple System implementation. Note that it's very much work-in-progress, but feel free to take a look: https://github.com/mtsr/entityx-ecs-rs One of my goals, which is something I like about OP's implementation too, is to keep the code simple and clean.
Post 1.0, what I think rust needs to focus on is the rust ecosystem more than the language itself (not that the language isn't important, just that if you want adoption you need a good ecosystem). A slew of blessed libraries probably need to be created. Libraries for web servers, ORMs, Json/XML/Yaml parsers, Database interfaces, testing libraries, the works. Certainly many of these types of libraries already exist, but we need to stabilize, polish, and get more 1.0 versions of those libraries. Hopefully we don't become a neverending swamp of beta versioned libraries ala npm. It may even help if we sat down and wrote some good polished interfaces for doing those actions. From example, java has the JDBC and JAX-RS standards which have several implementers. Those standards have allowed for higher order libraries to be built which do some pretty neat things. The one languagy thing rust could do is to make sure it is fast and a good fit for IDE integration.
something like 80% of all variables are only used ONCE most of Rust variables are not `mut` you simply don't care 90%+ of the time
Actually, this is not dependent types, but more akin to the C++ non-type template parameters. The difference: - a non-type template parameter is for example an array length in Rust: `[int, ..3]` is an array of length 3 - a dependent type however is even more involved, because the exact value might not be known at compile-time, only the relationship. It allows having types like `Stack&lt;T, N&gt;` which is a stack of `N` `T`s and a builder expression like `fn build&lt;T&gt;(N: uint, prod: || -&gt; T) -&gt; Stack&lt;T, N&gt;` where `N` suddenly crosses the boundary between runtime parameter and compile-time type parameter
Even in the case of Erlang, which supports it at language level, I've heard of frequent mistakes in using this feature: if the data persist, then its layout has to be compatible... I imagine in Rust it would lead to scary race conditions...
My thoughts exactly. 
&gt; This is starting to remind me some ugly hacks that people do with C++ templates. If it is possible we need proper compiler support for it instead of implementing it like this. For what it's worth, I agree that this is something of a hack. However, I think "proper compiler support" would entail moving to dependent types. Although I'd personally love to see that, I'd be very surprised to see it happen any time soon for a number of practical reasons. It is _possible_ though: Idris is leading the way here and has even started adopting some ideas from Rust recently. One could support type-level natural numbers for indexing and other things without full spectrum dependent types but I feel it would be a hack of a different nature because it's not a general solution: you can implement several other type-level constructions in the same manner as this library that wouldn't be included in the one-off compiler implementation of type-level nats. &gt; The only thing I can think of where this is useful is parametrizing types with values, e.g. vectors with size determined at compile time. Are there any other use cases? This is pretty much _the_ use case. If anything, it would be useful for stronger static guarantees.
As /u/FunctionPlastic mentioned, the idea is that this could be used for stronger static guarantees. Vectors indexed with their length `Vec&lt;T, N&gt;` is the canonical example. Such a thing would let you pop/index safely without dealing with `Option&lt;T&gt;`, etc.
Which is not what int/uint are at all. int/uint are, for example, 32 bits when using the x32 ABI because that's the pointer size.
I should have been more precise. I consider all of those cases to fall under the intention of a library such as this (eventually). I'm not that familiar with the use cases in C++ but more from the Haskell side (singletons) and dependent types in general. This is meant to approximate some of that functionality as much as possible in Rust. One of the use cases I had in mind for the type-level nats was to couple it with an implementation of type-level lists (hlist) for a coercion-free version of my morphism library. The type parameters would not be erased as usual but carried along in the type. Repetition of parameters would be encoded using these nats. This would allow the user to extract intermediate results from the composition loop which is otherwise not practical.
I see, now I think I've actually misread you message "*The* use case" was about "parametrizing types with values", not about "vectors with size determined at compile time", right?
Swift optimizes `Optional&lt;SomeReferenceType&gt;` and `ImplicitlyUnwrappedOptional&lt;SomeReferenceType&gt;` into a single pointer, exactly the same way Rust does for `Option&lt;NonNullPointerType&gt;` (e.g. `Option&lt;&amp;str&gt;`). The actual rules Swift uses aren't documented, but this is pretty much a given that it does this for `Optional` and `ImplicitlyUnwrappedOptional` on reference types. I don't know if the compiler has special knowledge of those two types, or if it's done using the same type of optimization that Rust does (Rust's optimization works for all 2-variant enums where the first variant is nullary and the second contains a field with a non-nullable pointer type).
That's a pretty complex story for something as mundane as error handling.
It doesn't matter if they're only used once. Integer overflow bugs are pervasive even in a purely functional language where fixed-size integers are the norm. It doesn't make much of a difference. fn main() { let x = 1431655765i32; let y = 3; println!("{}", x * y) }
It's not just for error handling. Error handling is just the simplest example. A more complicated non-trivial example would be parser combinators, for example, Haskell's parsec library.
[Rendered](https://github.com/sinistersnare/rfcs/blob/2ed8d628b74b671783d0c4c6cdf851a6e27f2a0e/text/0000-remove-block-comments.md) Also, not sure why it says "[doc]" in the title. This change covers all types of block comments, not just doc ones.
Trait objects can already be used with any smart pointer type - that was one of the motivations behind DST. The last part in that puzzle is custom coercions which would allow for easy creation of any smart pointer to a trait object. Up and down casting is part of the coercion reform RFC. So both of those things are very much on the roadmap and are going to be one of the first things to get done post-1.0 (possibly even before then).
&gt; One could support type-level natural numbers for indexing and other things without full spectrum dependent types but I feel it would be a hack of a different nature because it's not a general solution: you can implement several other type-level constructions in the same manner as this library that wouldn't be included in the one-off compiler implementation of type-level nats. I do agree but I think type-level natural numbers are probably the single most important feature from a Rust perspective, since bounds checks are so closely intertwined with memory safety. It might be worth supporting them in the core language even if it isn't generally applicable. I definitely understand your perspective though, it's hard to know whether you'll want something if you don't have it.
That is a good point actually. I suppose having type-level nats now wouldn't necessarily preclude moving to real dependent types later either. Idris takes a similar stance on the importance of nats but from a different perspective. They are so crucial to dependently typed programming that they are treated specially from an implementation point of view. Also, unary encodings of the nats like this are not very efficient to work with which might be another argument for special casing them.
Is there no way of definining the successor inductively?
Well, this RFC went very different than I imagined :) I guess if the people want it in, then it stays in.
This would mean that editor/IDE support would be needed for commenting out blocks of code, rather than just dropping `/*` and `*/` on either side. I also quite frequently use the following (and similar): let something = 4 /*some_function()*/; Whilst debugging, which would become impossible (or, rather, annoying) with this change.
I've been doing some work on an RFC in that direction, see https://github.com/rust-lang/rfcs/issues/273
I guess the other point to mention is that in this case I wanted the types to serve as phantom parameters, hence no variants for the enum. Singletons as above would be the way to link them back to inductive values.
Following the popular trend "implement everything in rust", here comes a dependency injection container. At this point this is just an experiment named "what would happen if this was available?". Any comments/feedback/critique is welcome :)
yeah, block comments are good.
Who said that x * y should be `i32`? Since `x` is `i32` then the result of a multiplication should be `i64`.
First of all, there will always be conversations between any group of decision makers; some of them will be public, some of them not. Every open source project (more broadly, every organization of any kind) works this way. I think it's impossible to ask any group of people to place *all* of their discussions in the open. Ultimately I agree that we do need to reduce the division between "the core team" and "the community". But I think that there is no solution as simple as "have more discussions in the open"; indeed with the RFC process and more-broadly-opened-up weekly meetings, we have moved more and more in this direction over time, but that is clearly not the entire solution. Most of the discussion on this very issue *did* actually happen publicly, with the blog post intended as a summary that covered all of the salient justifications that led to the decision. I'm happy to have more discussions in the open, but I don't see how that alone would have led to more mutual understanding above and beyond the blog post—the blog post really did cover all of the reasoning that went into this choice.
I'd like to see this in the standard library.
I tried to preserve mios flexibility and features, but in a much more ergonomic and boiler-plate-reduced way by abstracting the registration and managing of handlers into the library at the cost of a small amount of non-optional allocation. Right now it supports listening on IO objects, timeouts, and "next tick" callbacks, which should cover the vast majority of use cases.
I think the problem is that, at a low enough level, almost everything is unsafe. Pretty much all of the memory management stuff implemented in the Rust libs rely on unsafe, and thus collections and stuff do too. I suppose it might make sense to attempt to say "trust everything in std" and distinguish between that and 3rd-party code that uses unsafe though. In any case, doing it at the type level would be rather difficult with the current setup without severely impacting usability. There might be some sense in being able to get warnings from the compiler if a particular crate or module (aside from std) uses unsafe code in a transitive fashion. Even that is tricky though. Haskell tried to do something similar but it ended up mostly useless in practice AFAIK.
If I can figure out a way to allow for it to be matched against patterns and not just functions, I'd agree, but I think right now it is too limited in functionality to be worth adding to the standard library. I think it would be more interesting to see language level support for using functions in pattern positions, meaning you could match, use them in let bindings, if/while let, and even use them in function parameters.
It makes me think that an alternate syntax macro! param { params .. } Could be useful for some kinds of DSLs and not too problematic to parse (I think).
Yeah, I saw this in one of the recent macro reform RFCs and this macro made me wish that at least that part had made it in. It would have much better ergonomics for macros similar to this one.
Regarding diffing and merging: having a more uniform syntax leads to less noise in diffs. This is true for variation in comment syntax but also amount of indentation (and whether it uses tabs of spaces), positions of braces, etc. One could be even more aggressive and pass files through a pretty-printer before committing. But in any case I think that removing block comments is a bad idea, and like commented there, [this](https://github.com/servo/rust-url/blob/187d71492cdc10b0a8d1a22edb067ee090f802f4/src/lib.rs#L9-L119) wouldn't be very pretty with line comments.
This works only for constant values known at compile type, right?
It's true that the one version propagates but I think it's not in a way that's useful for what the OP is considering. The propagation is only up to the point where some library author uses the function in an `unsafe` block at which point the information is lost for debugging purposes (AFAIK) from the point of view of an application developer using that library.
My glib response is "that's called 'encapsulation'" ;) More seriously, if you were to taint this upwards, almost everything would trasitively have this information. At the lowest levels, it's all unsafe. I'm not sure how useful such a thing would actually be.
`usize`/`isize` `ulen`/`ilen`
Is mio the unofficial official rust io crate? If not, would it be a lot of work to make "event" work against a std io lib?
Yeah, pretty much agree with that too. Perhaps there's a way to provide more useful information about `unsafe` but I don't think more viral propagation would be useful with the current implementation.
Regarding your second example, wouldn't it be easier just to write let something = 4;///some_function(); As you only have to insert / remove text at one point in the code?
It would be a lot of work, because `mio` is an async framework and the IO in std is, or largely is synchronous. 
Maybe you're right. It does seem like indexing would rather just panic .
It's not really possible to make this work on only `std::io` since `std::io` provides a wholly blocking interface. While it now provides file descriptors, you would basically have to duplicate all of mio within event if you wanted `event` to work with only `std::io`.
For this example, this works fine, but your approach kind of breaks down if there's much more than a semicolon after the commented code - you'd need to copy+paste all of it. It's even worse when there's several such comments on one line, which is less common, but sometimes useful.
&gt; Regarding diffing and merging: having a more uniform syntax leads to less noise in diffs What are you talking about? Adding // to the beginning of every line is going to utterly confuse every diff/merge tool I work with. This is a complete waste of developer time, because someone wants to impose their comment favorite style on everyone else. Also, /* ... */ comments are *better* than // comments when I'm writing -- you guessed it! -- long-form comments. I do not have time to waste on adding // to the beginning of line in a block comment. Don't tell me how to format my code, and I won't tell you how to format yours. Use // all you want -- I use it all the time. But don't remove /* ... */ comments. 
The primitive types map to efficient hardware two's complement integer types, not a higher-level software design like that. You are free to implement whatever you want on top of them.
The way I would probably try to do this is using phantom type parameters: enum Foo {} enum Bar {} trait Kind {} impl Kind for Foo {} impl Kind for Bar {} struct Node&lt;K: Kind&gt; { … } impl Node&lt;Foo&gt; { … } impl Node&lt;Bar&gt; { … } The upside to this approach is you can avoid some structural duplication and you don't need to carry around tags. The downside is that you will probably need more annotations (or hints), etc. I wouldn't say it's particularly idiomatic Rust though…
&gt; First of all, there will always be conversations between any group of decision makers; some of them will be public, some of them not. Every open source project (more broadly, every organization of any kind) works this way. I think it's impossible to ask any group of people to place all of their discussions in the open. Sure. By the nature of the thing, of course, it's also impossible to tell from the outside how much discussion is or isn't going on behind closed doors. My impression had been that there's quite a bit, but it *is* possible that that was mistaken. &gt; Ultimately I agree that we do need to reduce the division between "the core team" and "the community". But I think that there is no solution as simple as "have more discussions in the open"; I agree: That's only one part of it. &gt; with the RFC process and more-broadly-opened-up weekly meetings, we have moved more and more in this direction over time Indeed, the direction is good, but I think that yet further moving in it would be helpful. :) &gt; Most of the discussion on this very issue did actually happen publicly Really? Maybe that's true; all I personally saw of it was some oblique references (can't remember where (edit: [just found one](https://github.com/rust-lang/rfcs/pull/464#issuecomment-66936152))) to "consensus building" going on behind the scenes. In that case, it might be an idea to link the preceding discussions (whether IRC logs, or whatever) in the announcement (blog post) so that people can refer to them. Importantly though: There was almost zero participation from members of the core team in [the public discussion thread](https://github.com/rust-lang/rfcs/pull/464). That's what I most think is not right. When anyone else has an opinion on an RFC that they want to express, whether in support or opposition, what they have to do is to lay out their reasoning as a comment in the discussion thread. Then other people can read, be swayed by it, or not, respond to it, and a productive discussion may ensue. Why is it a good idea for members of the core team to be entitled to skip this, to keep their reasoning and discussions to themselves, and only reveal it together with their final decision?
Here is a basic example of how it can be used. It still needs a lot of additional work to make it practical though: pub struct NatVec&lt;A, N: Nat&gt;(Vec&lt;A&gt;); impl&lt;A&gt; NatVec&lt;A, Z&gt; { // create a zero length vector fn new() -&gt; NatVec&lt;A, Z&gt; { NatVec(vec![]) } } impl&lt;A, N: Nat&gt; NatVec&lt;A, S&lt;N&gt;&gt; { // pop decreases size by one fn pop(self) -&gt; (A, NatVec&lt;A, N&gt;) { let NatVec(mut xs) = self; let x = xs.pop().unwrap(); (x, NatVec(xs)) } } impl&lt;A, N: Nat&gt; NatVec&lt;A, N&gt; { // push increases size by one fn push(self, x: A) -&gt; NatVec&lt;A, S&lt;N&gt;&gt; { let NatVec(mut xs) = self; xs.push(x); NatVec(xs) } } // not implemented yet trait Append&lt;A, N1: Nat, N2: Nat, R: Nat&gt; { fn append(self, other: NatVec&lt;A, N2&gt;) -&gt; NatVec&lt;A, R&gt; where Void: Add&lt;N1, N2, Equals = R&gt;; } fn test03() { // OK; explicit type annotations just for demonstration let xs: NatVec&lt;&amp;str, Z&gt; = NatVec::new(); let xs: NatVec&lt;&amp;str, S&lt;Z&gt;&gt; = xs.push("foo"); let (x, xs): (&amp;str, NatVec&lt;&amp;str, Z&gt;) = xs.pop(); // type error let (x, xs): (&amp;str, NatVec&lt;&amp;str, _&gt;) = xs.pop(); } It's possible to implement append, indexing and the other stuff with further infrastructure (needs Fin and some helpers).
I've since rewritten all of the wrapper functions to be totally safe and documented them with their c equivalents; I plan to just ensure the documentation notes that the wrapper functions are not intended to be used from rust and that the impl methods should be preferred. Edit: side note, this was super helpful: http://doc.rust-lang.org/guide-ffi.html
might the OP's suggestion be in the direction of an effects system (separating memory from IO operations for example, and perhaps marking pure code as a sort of 'extra safe', globals are another type of unsafety). is unsafe (and of course const correctness) like an approximate effects system is that too much language complexity (does anything out there actually do this?) could other forms of debug solve the same issue e.g. report what kinds of unsafety are used throughout the callgraph (imagine pretty printing code colour coded with the effects downstream from any point, maybe a nice feature for an IDE)
This is misrepresenting the current state of affairs. db_query_1().and_then(|x1| db_query_2(x1).and_then(|x2| db_query_3(x2).and_then(|x3| y = do_sth(x1, x2, x3) ))); and let x1 = try!(db_query_1()); let x2 = try!(db_query_2(x1)); let x3 = try!(db_query_3(x2)); y = do_sth(x1, x2, x3) are both valid (the latter being the current "idiomatic" way).
You may want to look at https://github.com/huonw/unsafe_ls.
&gt; I suppose it might make sense to attempt to say "trust everything in std" and distinguish between that and 3rd-party code that uses unsafe though. Right. I was imagining the stdlib (or parts thereof) to be whitelisted. This part of the language could be formally verified (ala C programs). From that point on, in userland code, any use of unsafe (or this special variant of unsafe) would be transitive. I'm simply concerned about the long term auditability of large Rust programs or libraries that occasionally use unsafe (which I think will be a significant portion of Rust programs and especially libraries).
Awesome! This is a step in the right direction. I'm imagining this kind of tool will be important to developers debugging large applications.
I'm also curious why this self post was downvoted so much. If you do not think Rust needs more explicit notions of safety, that doesn't mean that this post is irrelevant or off topic. Please see rule #7 on the sidebar. I'm a little disappointed :(
I understand the current unsafe model of encapsulation and how programmers must satisfy invariants. I think it's a pretty good system and lightweight to use. My concern is about when (not if) a programmer makes an error in using "unsafe" and how to debug that. Realistically, we have great tools from C++ (see ASAN and friends) that could probably be modified for use with Rust, but the cool thing about Rust's design goals is that we want to avoid errors statically where possible. If I wanted to fly the a spacecraft with Rust code, I'd want to prove that there is no unsafe code (see my comment about whitelists), or have a very strict understanding of where the unsafe code is. Ideally, I could do this with static checking, whether through the normal type system or some kind of opt-in strict mode that makes "unsafe" transitive.
Maybe they're not idiomatic or encouraged but block comments are super useful when debugging. How can you say they're not used in Rust code when 1.0 isn't even out!? There's like 3 major Rust projects in existence. :P
Yeah you are completely right. I should have come up with a better example than this. But the point I was trying to make was that having HTK allows one to define a more generic syntax that applies to not only `Result` but also to other types. Also currently `try!` is only defined for the `Result` type, but not the `Option` type. So if you are dealing with a series of functions that return `Option`s you cannot use it.
Seems like it'd be neat from the client's perspective if you could say `BinaryTree&lt;Rc&lt;T&gt;&gt;` and have it be able to match-out `P` and `T` internally. (Maybe something like `enum BinaryTree&lt;PT&gt; where P&lt;T&gt; = PT { ... }` )
 let x = 420xXxintxXx;//MLGnoscope 
What's the rationale for the fn typing change? I'm sure it doesn't affect too much code, but it still seems to hurt the ergonomics of functional-style programming slightly, and I can't figure out what the benefits are.
&gt; I feel it is one of the most sought after feature if you are looking to get faster than C performance (that is static runtime C). It's uses the same passes and code generation as when an object file is being generated. The only difference is that it generates the code in-memory and hooks it up via linker hacks. It doesn't do anything more than cutting some work out of the compilation pipeline (replaces generating object files, linking them and loading the library) for runtime code generation, which Rust doesn't use.
As an addendum, a single parameter function defined as such: `fn case&lt;T&gt;(input: T) -&gt; Option&lt;T&gt; { Some(input) }` provides the ability to do essentially a pass-through of the match parameter so that you can then use arbitrary patterns to match.
This could potentially be expanded to look for a trait method to perform the match check.
&gt; Importantly though: There was almost zero participation from members of the core team in the public discussion thread. I think you really hit the nail on the head here, and are right to call the core team out on this. We are a part of the wider Rust community, and as you said, we are not entitled to skip the step of talking with and persuading each other before coming to a final decision in cases like this. This was a breakdown in the RFC process we have been trying to push toward -- in particular, the RFC was not properly [shepherded](https://github.com/rust-lang/rfcs#what-the-process-is). A shepherd is an active community member (usually on the core team) who takes responsibility for moving an issue forward, making sure that active and interested community members (including core team members) participate in the discussion, and try to represent the original RFC author in discussions with the core team (or help arrange real-time discussions if necessary). The shepherd is also responsible for making sure that all of the pros and cons are outlined in the RFC/comments before the core team is ready to make a decision. The final RFC decision shouldn't introduce new arguments; it should be made in terms of arguments that were already made and responded to. In other words, the core team should never make decisions on an RFC on the basis of rationale that was only discussed at the time the decision was made. We made that mistake here, and we will work on steps to avoid it in the future. In this particular case, the issue (which is a longstanding one) was not actively shepherded; it fell through the cracks. However, that does *not* mean that the discussion was ignored! The blog post aimed to -- belatedly -- lay out our evaluation of the arguments people were making, and the pros/cons as we saw them. The broad feedback has led the core team to revisit the decision from scratch; we have closely read the discussion on the latest RFC as well. We plan to respond shortly to the current RFC with a more elaborated analysis of the various tradeoffs and avenues forward as we currently see them. Thanks as always for the care and thought you put into Rust and its community.
How does this work with F#/Scala? Does the compiler have to know to use the unapply function for case expressions? Does is infer this just using the type system?
Maybe to help monomorphize generic functions to get more inlinable calls?
I think this idea has a lot of merit, when combined with the below-mentioned formal verification. Even if we can't put it right in the language, correctness proofs combined with this facility could be powerful. (On the other hand, Rust's semantics can be so complex that these correctness proofs may be difficult to create: is it assumed that all other unsafe code is correct? Can the proof be extended to allow for "certain mistakes" in other unsafe code?) Or, maybe it could turn out to be hellishly complex. I'm imagining that this could be implemented as AST transform that uses `unsafe` blocks with `#[verified]` (or a `#[verified] extern crate std`) to mark all functions as either `#[verified]` or `#[unverified]`, starting with leaf functions. Or something along those lines. It doesn't belong in the language, I think.
That's basically the intent of the current system. If you misuse unsafe code, or your unsafe code is buggy, you'll get an error type that is recognizably caused by unsafe code issues (for example, segfault), so then you've narrowed down the problem to everything marked unsafe (a small fraction of the application, even including dependencies). Tools like unsafe-ls are great because they help optimize this process, and auditing too. But that's all they are - an optimization, a step up from a raw git grep.
Scala uses a kind of Duck Typing here. If an object has an `apply` method then it can be used as a function. If an object has an `unapply` method then it can be used as an extractor. http://www.scala-lang.org/old/node/112 Though this Duck Typing might have been an artifact of the JVM implementation originally (e.g. functions are implemented as singletons with an `apply` method).
I don't really get fixtures, specially this first code fn bar() { let foo = bar(); } Doesn't make sense (it's a stack overflow?). The function name and the called function are supposed to be different, right?
Looks like an error in the docs to me, the tests make the usage pretty obvious: https://github.com/shadowmint/rust-fixture/blob/master/tests/tests.rs edit: there you go, fixed. The bar in the example has something marked with #[register(bar)]; not the function itself. 
I am saying the former yes. I don't get it either. 
the error message friendliness is a matter of the compiler. But would you rather specify things like execution context for every future? or worse yet, not having the possibility to change the default one?
The standard procedure for this sort of thing is to have one thing which is dedicated to just reverse proxying or just SSL termination; this is the most secure way of doing it. [stunnel](https://www.stunnel.org/index.html) is the standard choice if you’re using the SSL termination approach, nginx or apache can both handle the reverse proxying approach.
I actually wish they would work better. They would be great for enum field documenting but you can't use them there ;(
I have been considering and ecs and scene graph, but have come in to snags with rust. I too am interested in where this project goes because this is perhaps the 4th ecs lib for rust :) I do not see the systems/processes portion for this one, which is technically the most difficult I found. I'd also like to see how this works with threading, which is where my main focus has been.
While the safety of code can't be easily evaluated programmatically, one could still easily search for the unsafe blocks when particular sorts of errors normally guaranteed not to happen actually occur. For example, a segfault is normally guaranteed not to happen by the compiler. If it does in fact happen, you can rest assured that the issue is in those blocks marked unsafe. If anything, requiring that unsafe propagates would make finding the source of problems harder.
\#2 is actually a pretty odd design choice. Aren't the vast majority of services that come out of a container stateless, and thus a good candidate for singletons? The only reason you'd want a new object each time you fetch a service out of the container is if it maintains state, which should be rare.
It's also a matter of beefing up module documentation. http://doc.rust-lang.org/nightly/std/iter/index.html should talk about this kind of thing. In general, I've been waiting until things are marked stable, and then trying to fill stuff out. Expect all of this to change quite a bit during the beta periods, as more and more chunks become stable. (The guide does discuss this specific point, but your overall point stands)
I'd very much like to see/implement support for something like this in Rust at some point in the future! It would be very useful for a number of scenarios: Native plugins, JITed code, swapping code at runtime, etc. The issue here is how to make it safe in Rusts definition of the word to unload a dynamic library again after it got loaded, which would require every type that somehow depends on a function pointer or static memory location in that library to have a lifetime bound chained to a RAII handle for that library. So far, me and a few other people on IRC (eddyb and Tobba) have a vague plan about how to support this in the future: - Allowing optional lifetime bounds on raw function pointers (see https://github.com/rust-lang/rfcs/pull/252) - Adding a new compilation mode for crates that compiles them as a plugin, which involves - Adding a new concrete lifetime `'crate`, which would live shorter than `'static` and not be known to be valid for ever, but still outlive any other lifetime. - Making all static items in a plugin crate have the `'crate` lifetime instead, which would still allow using them in a way that is similar to `'static` items, but prevents usage with APIs that explicitly demand the `'static` bound. - Emit sufficient type checking information so that a plugin-loader library can verify that a symbol with a given name and with the right type exists in that library, and can return a handle to the symbol that replaces all uses of `'crate` in the plugin with a regular lifetime bound to the crate handle. However, there still many open questions, so we don't know yet if this approach will actually work. 
indeed, one must use google. substring http://floss.zoomquiet.io/data/20140128093503/index.html#how-do-i-search-for-a-substring http://doc.rust-lang.org/std/str/trait.StrExt.html#tymethod.find_str file updated example because part of test (fn read) https://github.com/rust-lang/rust/blob/master/src/test/bench/sudoku.rs http://floss.zoomquiet.io/data/20140128093503/index.html#file-operations http://rustbyexample.com/fs.html http://doc.rust-lang.org/std/io/fs/ http://rosettacode.org/wiki/File_input/output#Rust iterate over vector (use * ?) https://github.com/rofrol/rust-hackerrank/blob/master/src/bin/find-digits.rs https://stackoverflow.com/questions/27312069/rust-iterate-over-vector-of-functions https://stackoverflow.com/questions/26953280/how-to-create-a-non-consuming-iterator-from-a-vector
Out of curiosity to better understand how F# is doing it, I compiled the following [F# program](https://gist.github.com/Mr-Byte/621c048a0f172e66b45c#file-active_pattern-fs) and then decompiled it into this [C# code](https://gist.github.com/Mr-Byte/621c048a0f172e66b45c#file-active_pattern-cs). It appears to do a few interesting things. First off the active pattern is compiled as a method which returns a generic class that represents a choice. In the match statement, it then appears to first switch, since I'm pattern matching a number and handles that case. Then in the default case it handles the active pattern by checking which choice was selected. I'm tempted to decompile more samples, because this is definitely interesting and there might be a way to desugar such functions in match statements, which could make for a nice RFC to include such functionality in the language.
Have a look at [Slice::split_at_mut](http://doc.rust-lang.org/std/slice/trait.SliceExt.html#tymethod.split_at_mut). This gives you two mutable slices, allowing you to get a mutable reference to one monster from each slice. 
&gt; This sounds good; but I feel a little bit like some of the same impulses are still at work here. Why is it important for the team to speak with a single voice, as "we"? Why can't each member of the team participate in the discussion as an individual, with his or her own opinion? (Maybe there are good reasons for doing it this way; they're just not obvious to me.) Oh, I just meant that one of us will summarize the in-person discussion we had -- not as a decision, but as a summary. In general each of us can and should comment further with our own perspective, and we'll try to step this up as well.
That works... however that doesn't seem to elegant pub fn fight(&amp;mut self) { let (left, right) = self.monsters.split_at_mut(1); let m1 = &amp;mut left[0]; let m2 = &amp;right[0]; m1.hp = 9; }
Then perhaps something like the following will work. First calculate the result of the fight (borrowing multiple monsters immutably) and modify one (or more) of them afterwards: pub fn fight(&amp;mut self) { let (monster_index, new_hp) = { let m1 = &amp;self.monsters[0]; let m2 = &amp;self.monsters[1]; // Do some fighting without modifiying the monsters (1, 9) }; // Now, after m1 &amp; m2 have gone out of scope modify one of them self.monsters[monster_index].hp = new_hp; }
The issue is that the borrow checker is not sophisticated enough. It can't understand that your 0'th and 1'st element are always un-aliased (imagine that you instead had `pub fn fight(&amp;mut self, uint index_a, uint index_b)`). Notice that `Monster` can't be Copy due to `name : String`. What you could do in this case is to extract the relevant information from the attacker (e.g. if you had `dp` for damage points), and do something like: pub fn fight(&amp;mut self) { self.monsters[1].hp -= self.monsters[0].dp; } 
In Rust I find it is best to cast aside preconceptions of what is elegant. By appeasing the borrow checker, you'll get safe, (usually) fast code. In this particular case, the `split_at_mut` solution, though weird, works because it helps Rust preserve its core guarantee about no aliasing `&amp;mut`s (since the left and right side can't alias). Other solutions don't differentiate the structure in the eyes of Rust either require you to change your algorithm (as in /u/TimNN's example) or introduce potential shared mutable aliases (with static checks like `Cell` to ensure it's safe, or runtime ones like `RefCell`to ensure your program will crash if you try). Sometimes one of these will be the better solution, (though I strongly recommend against `RefCell`).
Absolutely. It's hard to do any of this when the language isn't stable. I'm not knocking the work you've done this far (great job, btw). Just wanted to point out a useful path for improvement in the future.
I like that solution better, having to fetch the monster to modify twice makes me a bit tingly, but that's acceptable
I suppose more specific pleasant helper functions could be written ... `.get_pair_mut(i0,i1)`, which could assert that they don't alias, or you could make something to iterate with one mutable reference comparing against each other of the same collection as a mutable reference ..'iterate all the pairs', that sort of thing. (get_mut_pair might actually be easier than the unsafe equiv eventually, because you only reference the collection once.. let (m1,m2)=monsters.get_pair_mut(i0,i1); vs m1:=&amp;mut monsters[i0]; m2:=&amp;mut monsters[i1]; .. if you can get things right out of laziness its' a win-win) Discovery is the issue here. deciding what to name it, how do people find the right name. How would you have known to look for `.split_at_mut()` here in the first place. you start needing lots of new vocabulary to do basic things. I wonder if you could do 'shape analysis' .. you write some raw unsafe code, and a tool compares the shape of what you're doing and suggests the safe equivalents.
As long as you're using a `Vec`, lookups are basically free, especially since after the first lookup you'll have the relevant data in L1 cache, if not in a register.
keeping it simple perhaps: http://is.gd/NtWhlH edit: alternative with enums added: http://is.gd/M3NAH9
With jni bindings available, what stops me from writing an android game in rust?
Doesn't really stop you, but perhaps it would be nicer if Piston [supported Android](https://github.com/PistonDevelopers/piston/issues/116)? Or actually, you might want to ask in /r/rust_gamedev whether the libraries you want to use support Android. (gl-init [does](https://www.reddit.com/r/rust_gamedev/comments/2g3ju1/glinit_is_running_on_android/) so that's a beginning) edit: I think that a simple game doesn't necessarily require the jni; it might use only the ndk to develop a native, pure Rust app.
You could look at the source code of Slice::split_at_mut and write an equivalent function that gives you two references into the vector. The following code is more-or-less a copy of that function: use std::mem; fn get_2&lt;T&gt;(this: &amp;mut [T], idx: uint, idx2: uint) -&gt; (&amp;mut T, &amp;mut T) { assert!(idx != idx2); let this2: &amp;mut [T] = unsafe { mem::transmute_copy(&amp;this) }; (&amp;mut this[idx], &amp;mut this2[idx2]) } Personally I feel always a bit uneasy when I have to use transmute (because you can easily fool yourself about what it is doing exactly). You could rewrite this function by using `split_at_mut` internally: fn get_2&lt;T&gt;(this: &amp;mut [T], idx: uint, idx2: uint) -&gt; (&amp;mut T, &amp;mut T) { assert!(idx != idx2); let (idx, idx2) = if idx &lt; idx2 { (idx, idx2) } else { (idx2, idx) }; let (this, this2) = this.split_at_mut(idx2); (&amp;mut this[idx], &amp;mut this2[0]) }
Please post to /r/playrust. This sub is for the Rust programming language.
Well, you can easily return a Rc value (which I haven't mentioned anywhere, because I haven't had time to experiment with it properly), and it should work as singleton.
You can get a mutable iterator: use std::sync::Future; fn main() { let mut results: Vec&lt;Future&lt;int&gt;&gt; = vec![]; for mut result in results.iter_mut() { let i = result.get(); print!("{} ", i); } } Edit: just saw a bunch of people already left a comment to this effect on the blog itself.
Cool. It's nice to know that this can be added outside the language. Thanks for the ideas.
Also LLVM will probably optimize out most of the code anyway.
Ha, [my thread](http://www.reddit.com/r/rust/comments/2qje69/ffi_dealing_with_va_list/) made the list! I still wonder if it would have evolved differently if I had brought up the topic on the mailing list instead.
Why is this link to a fork that's a commit behind?
The commit behind is just a merge commit. I assume at the time of writing the post the pull request had not been accepted and so the upstream repo would've been out of date by a few months.
This is the correct answer. You don't want an application server to speak directly with the client over SSL. If your backend has to know about SSL details (such as client certificates), you can usually configure your front-end server (e.g. Nginx) to forward relevant details in custom headers.
Thanks for working on those docs. It is greatly appreciated.
Hey everyone, this project is my first time working with Rust so if anyone has some suggestions to improve the code or make it more idiomatic Rust it'd be much appreciated. 
I agree, discoverability is a major issue. In particular, I also wished for something like `get_mut_pair` until I figured out that `split_at_mut` gives you a more general version of the same functionality. I think Rust *desperately* needs a set of "patterns" for appeasing the borrow checker. There are many situations where experienced Rust developers have no issues (because they know exactly how to deal with them, having done so before) but it's really not obvious if you haven't done it before (this is where most instances of `Rc&lt;RefCell&lt;T&gt;&gt;` come from, I suspect). I know several people have tried keeping track of commonly asked questions on IRC, which might be a good start to writing such a document.
I opened up a new discussion on this topic at http://discuss.rust-lang.org/t/restarting-the-int-uint-discussion/1131 with a lot more detailed analysis. Let's hash through this.
In "Design 2: The `int` Type is an Alias for `i32`", you write (in the "Cons" section): &gt; For programs targeting 64-bit deployments, this increases the chance of overflow errors relative to the other defaults we are considering. While some kinds of programs might still overflow even with a 64-bit integer, there are many domains where 32-bits aren’t enough, but 64-bits is far more than enough (e.g. seconds since the epoch, nanoseconds since program start). I would rather work with the predictability of `int = i32` than work with the unpredictability of "`int` means something". If someone makes an unconsidered choice for something like "file offset", and they just use `int`, then the fact that this works on x64 but fails on x86 is really not an advantage of `int = isize` -- it's just an accident. I would *much* rather encourage people to make consistent, well-considered choices, and I think having consistent, universal integer types is the right approach for that. A file offset should be u64 on all platforms -- we shouldn't rely on `int` or `uint` just *accidentally* doing the right thing on a 64-bit platform. C# went down the path of `int = i32`, and I've done a *ton* of systems programming in C#, and I really, really enjoy how consistent the integers are. In C#, `int = i32`, and `long = i64`, and there are `UIntPtr` and `IntPtr` types which have the obvious meaning, with explicit conversions to/from the i32 / i64 / etc. types. It works really well, and I'd love to see Rust take a similar approach. &gt; Rust has an implicit coercion from smaller types to types of bigger or equal size, which means that you can use an int as an index into a vector on 32-bit and 64-bit systems. This would go a *very* long way toward making Rust palatable. Right now, my Rust code is polluted with tons of unnecessary conversions to/from `uint`. For example, from RACC: let mut sp = derives[gram.start_symbol + row] as uint; let mut rule = derives_rules[sp]; while rule &gt; 0 { let symbol = gram.ritem[gram.rrhs[rule as uint] as uint] as uint; if gram.is_var(symbol) { eff.set(row, symbol - gram.start_symbol); } sp += 1; rule = derives_rules[sp]; } That one line has three `as uint` conversions! And they add no value whatsoever to the semantics or correctness of the code. Worse yet, they produce confusion -- which conversions can lose information, and which are simply zero-extending or sign-extending a value to a larger size? Which leads me to the next point. It would be extremely helpful if Rust made a distinction between "conversions which can modify data" (`i32` to `i16`), and "conversions which cannot modify data" (e.g. `u8` to `u32`). Right now, because Rust does not provide implicit conversions, I have to write a ton of repetitive "as xxx" in my code. This is annoying and time-wasting. But the worst part is that, when I read my code, it is impossible (just from reading it) to know whether a particular conversion is dangerous or not. Yes, obviously I can read through the code, but that wastes time, and finding out the exact return type of some function I'm calling may require searching for code in some far-flung crate. This problem doesn't exist in C#, because the conversions that don't modify values are implicit, while the conversions that do are explicit. Please consider adopting this healthy approach. I understand the ramifications for type inference, but C# achieves the same thing with a respectable amount of type inference, including inferring return types, parameter types, generic type parameters, etc. (Especially interesting with respect to closures.) I love Rust. But in every significant project I've done in Rust, the integer conversion mess is one of the most time-wasting, annoying things about the language. It's frustrating because I don't feel like it adds any value to the language. Lifetimes were difficult to master, at first, but now that I have, I see the tremendous value that lifetimes bring to a language. The same is not true for adding "as uint" to every table lookup that my code performs. 
When I see an i32, what I think is "Oh, they have clearly chosen this specific integer size for a reason!" and when I see just an i what it means to me is "Oh, this is just vaguely a number and if we run into any issues about that later we can change that to whatever it needs to be." Also, shouldn't defining an unspecifically sized integer mean that the compiler should be able to optimise it if it can?
Is your point that having implicit coercions would make the `uint` conversions that are necessary (e.g. signed to unsigned, like that one) stand out more?
That's a book I would absolutely buy. Perhaps Rust-by-Example could be expanded with a new section for common, useful patterns or techniques, instead of being limited to just core language concepts.
what would be best is a tool, not a book. write unsafe code, and the tool tells you the safe equivalent. it's a general problem with FP vs imperative- increasing the amount of vocabulary required to do things. Its' much easier to remember a few language primitives that you can string together to do many things
Yes. The program is full of `uint` conversions, mostly for indexing purposes, and made this one hard to spot.
In general, Rust is pushing strongly toward having fewer, more composable concepts... But there are always going to be a few absolutely invaluable but non-obvious things like `split_at_mut`. Unfortunately, it would take an unbelievable sophisticated tool to make good suggestions for the right thing to do in most circumstances, because it's rarely going to be a simple substitution. Frequently, making your code work in an idiomatically rustic way requires restructuring the whole algorithm, in a way that's really not obvious if you haven't been working with Rust for a while, and gotten used to it's patterns. So I think documenting those patterns explicitly would be very useful. It doesn't entirely solve the discoverability problem, because people still have to discover this documentation. But it helps a lot.
&gt; So I think documenting those patterns explicitly would be very useful. fair enough :) &gt; Unfortunately, it would take an unbelievable sophisticated tool to make good suggestions Sure - this is not low hanging fruit - but it would be universally applicable; Rusts syntax should make it easier - because you can parse fragments more easily. ( a big draw to the language) Imagine something cloud based. (Could it be written in a language agnostic way? .. or working for any languages that are similar enough. If done for rust &amp; C++, the 'cost' of collaborating on the tool is divided by a huge C++ community. (of course whilst C++ doesn't have safety as part of the language definition, you can still write 'safer' patterns, and get to a stage where you can satisfy an external static analyser.. its' just lacking the lifetime annotations that handle references escaping better) users submit a code fragment; it consults a data base of substitutions. Users rate the given alternatives... users can also vote on names of helper functions , to arrive at a consensus on what the best are. Unsolved patterns can be accumulated as library requests. A little brute force could be used in that multiple code shapes mapping onto the same substitution could be recalled. I think this would be incredibly useful to the programming world generally.. its not low hanging fruit, but done right would be a great piece of 'collective intelligence' &gt; Frequently, making your code work in an idiomatically rustic way requires restructuring the whole algorithm, its not unique to rust: C++ isn't 'safe' but can be written in a 'more safe' way. references aren't so different to borrowed pointers; if you had no raw pointers outside of unsafe, used lambdas more.. you'd have a decent safe subset of C++ that would translate across to rust. Perhaps one more option for the above would be `monsters.do_with_pair(i0,i1, [](auto m0, auto m1){ ... stuff.. } )` .. which could be shown to be 'safe' in C++ pending the private implementation of 'do_with_pair' dealing with the pointers.. no pointers escape so there's nothing to check
&gt; I’d really prefer to just have an overload of new(x: f32) -&gt; Vector which performed the same construction that broadcast does currently, although this is a relatively minor annoyance. It's not really clear what `new(x: f32)` does. Especially since the parameter is called `x`. It could set `x` to the value passed in and `y` and `z` to `0`. So the Rust version is IMO preferable. Another (minor) point: I'd call `new` from the `broadcast` method in order to have exactly one method which takes care of constructing the value. That's only relevant if you need to enforce some object invariants though.
&gt; shouldn't defining an unspecifically sized integer mean that the compiler should be able to optimise it if it can? "All `int`s are now 8 bits long in the interest of memory usage and cache performance." Of course this is hyperbole, but my feeling is that you should always know how big your integer will need to be. If you don't, use a BigInt. If you can't use BigInts and still don't know your bound, you're in trouble. &gt; "Oh, this is just vaguely a number and if we run into any issues about that later we can change that to whatever it needs to be." So basically you think of `int` as a marker that the size decision hasn't been made, and you need to keep an eye on it if things go wrong? It seems to me like that'd be a great time for a `type`def - `type PossiblyWrongSizedInt = i32` ;) ^Edit: ^Spelling
I haven't thought this through much, but if auto-widening is generally considered undesirable, what about only allowing auto-widening of sized types to isize, rather than all integer types? By far the major source of frustrating 'as uint' conversions for me are when using Vecs and other data structures. I don't mind if I can't auto widen an explicit 32-bit integer to go in an explicitly 64-bit slot.
`rule`, `gram.rrhs[rule]` and `gram.ritem[gram.rrhs[rule]]` in your example - why aren't they `uint`s in the first place? Do you pursue optimization goals using smaller types? Did you measured if it is really needed? I'm surprised you say conversions (and optimizations) like this are often needed, it directly contradicts my experience.
I was a big fan of the [RFC](https://github.com/rust-lang/rfcs/pull/544) to rename intnuint to intx/uintx. One thing that isn't being considered here is his rust would work on 16-bit embedded systems, where not having a pointer-sized integer value could be a problem.
That `Vector::new((1f32, 2f32, 3f32))` is annoying..
There are some parts that can be built in parallel and others that must be built serially. The slowest parts happen to for the most part be ones that must be done serially.
i'd done similar , is there a convention for when you'd want to favour Vec::new(...) vs .to_vec3()? `(x,y,z).to_vec3()` vs `new((x,y,z))` might make the bracket less annoying (less nesting) although the number of brackets is the same
I think you have a point that propagating knowledge an entire library has no 'unsafe' could be useful. Like a form of 'extra safe'. unsafe relegated to a few sources -&gt; unsafe relegated to a few sources in a few libraries
Is the best practice really to use something like OpenSSL in the SSL end? This approach of having a small program just to handle HTTPS makes me think it would be possible to build it around [miTLS](http://www.mitls.org/wsgi/home).
I don't see the point of this. We had an RFC that suggested renaming int/uint, with a lot of discussion and good technical arguments. Then it was rejected, for breaking too much code. Now we 'restart' the discussion, but strangely the 'rename int/uint' option is missing! Well, it's really design 1 in disguise, which suggests removing `int` (and presumably `uint`) and introducing new types for the use cases that are currently reserved for `int` (and presumably `uint`). Which is really the same as simply renaming these types. We know this option was rejected in the past and the reasons for doing so have not changed, so I can't help but feel like this whole discussion 'restart' is made in bad faith, because the author introduces options that he knows will be rejected. In addition, as pointed out by strcat, the author confuses the difference between 'machine sized' and pointer sized, which once again confuses issues that had already been discussed and cleared up in the original renaming RFC, and suggests that despite what the thread name would suggest ('Restarting the discussion') the author hasn't really paid attention to the original discussion. Apart from that, even if any (other) particular design is chosen there is no guarantee that the core team won't simply reject it again, no matter how much support that option has. Of course this time we have 4 different designs as our options, so votes will be spread out more and it will be easier to argue that none of the design really had overwhelming support, and we'll happily forget that there was actual overwhelming support for renaming `int` and `uint`...
&gt; strangely the 'rename int/uint' option is missing! This isn't true, as the post has a strawman `isize`. The important part is to nail down the semantics of 'which types should we have', and then we can bikeshed the name of them. But you have to know what you're talking about before you can give it a good name. &gt; I can't help but feel like this whole discussion 'restart' is made in bad faith, because the author introduces options that he knows will be rejected. The purpose of this post and the resulting discussion is to lay out _every_ possible angle on this. To make sure that all the details are covered, before we finally nail everything down entirely for good.
&gt; While it was very easy to overload the vector * scalar operator, writing the same overload for scalar * vector doesn’t seem to be possible That is sort of embarassing. I think it's a bug [...] Edit: It is! This is [#19035](https://github.com/rust-lang/rust/issues/19035) which might be getting fixed by [19434](https://github.com/rust-lang/rust/pull/19434). Hooray!
&gt; If overflow yields an undefined value then the compiler is free to assume that "a + 1 &gt; a" is always true for any 'a' for instance. If you tell the compiler to assume wrapping arithmetics (as is still currently the case in rust) then the compiler can't make that assumption and has to generate an actual check. Could it really? Because an undefined value should still be in the range of possible values for a type, should it not? So if `a` is the max value for its type, then `a + 1` can't be bigger than `a`. You can optimize based on `a + 1 &gt; a` when you make overflow undefined behaviour, which is undesirable and would go against the goals of Rust. And as others have pointed out, you can use undefined values to create a situation where undefined behaviour is possible. So we get all the downsides and no upsides from the proposed scheme... :(
+1 if you need to be precise about portability there'd want to be a distinction between ALU size and address size.. and maybe something else thats a min or max of both of those.. 
intx or intp, or isize (analogous to size_t) into look fine to me, you could leave 'int' (like float vs f32/f64) to a user typedef to cover ambiguity and differing preferences. aside from that I'd be interested in parameterising the index used in`Vec's` `Vec&lt;T,I=isize&gt;`? -(overflow test overhead only happens on realloc) you could also use that to express more information i.e. what indices are to be used with what vectors in a program... `Mesh{vertices:Vec&lt;Vertex,VertexIndex&gt;, triangles:Vec&lt;([VertexIndex;3],MaterialIndex)&gt;,TriangleIndex&gt;, etc..}`, but we can already do that ourselves in a nonstandard library 
 I've never touched C# or Java ... but reading that they have int=32bits, long=64bits ... that does actually sound like a great idea. still , rust already has specific i32, i64
Came here two minutes too late... ;-) https://twitter.com/zsiciarz/status/550290589737291776
I downloaded the rust nightly from www.rust-lang.org and manually ran the `install.sh`, that did the trick.
Stickied - hopefully more people will see this that way!
&gt; Now we 'restart' the discussion, but strangely the 'rename int/uint' option is missing! No, it's not, as pointed out by Steve below. Option #1 is straightforwardly that, and option #2 effectively includes a renaming of `int` and `uint`. &gt; I can't help but feel like this whole discussion 'restart' is made in bad faith, because the author introduces options that he knows will be rejected. No, the point of the discussion is to in fact have a discussion. Please don't assume malice ("bad faith").
&gt; Then it was rejected, for breaking too much code. The reasons for rejecting it were more complex and nuanced than that. &gt; We know this option was rejected in the past and the reasons for doing so have not changed wycats suggests, towards the start of his post, that those reasons may not have been properly thought out; that there are factors which the core team may have failed to take into consideration. &gt; Apart from that, even if any (other) particular design is chosen there is no guarantee that the core team won't simply reject it again, no matter how much support that option has. Chosen by whom? Rust isn't developed by democratic rule, nor should it be. That sounds like it would be a true clusterfuck, frankly.
A decision was already made.
How often does this happen to you? On OSX, my code compiles fine. On Windows, I am getting the same issue you had.
For the DifferentialGeometry instance member, one possibility might be to create a static DEFAULT_INSTANCE value, take a shared reference to it for the purposes of DifferentialGeometry initialisation, and fix up the reference with the correct value afterwards. Obviously not completely ideal, but allows you to make the value non-optional.
I'm strong +1 on renaming the damn thing. It's misleading and it is a liability. Whatever name chosen I am fine with.
Neat little library! Not a code feature, but you could [publish it to crates.io](http://doc.crates.io/crates-io.html#publishing-crates) to make it even easier for others to discover and use. :) 
By installing the Nightly manually it would mean grabbing the installer itself and running install.sh From the same nightlies page, "curl -s https://static.rust-lang.org/rustup.sh | sudo sh" gave me cargo but no rustc
Can you post this thirty minutes ago? Hmm... can we build a time machine in Rust? 
I hate the old days of Reddit which allowed you to sign up without an email address &gt;:(
passs.co seems to be a company that provides authentication systems. My guess is that you're connected to a public WiFi access point and it's trying to redirect you to the login page, but doing it poorly.
I like this idea. Much of the argument for keeping the types `int` and `uint` seems to be "it will be familiar to beginners coming from other languages". If such a user sees a compiler error "use of undeclared type name `int`", they'll naturally be lead to the guide, which will (ideally) give a succinct explanation as to why you should care about what size of integer you use. That said, it would be nice to allow unsuffixed integers - if only for tooling around with the language. `i32` seems like a reasonable default here.
There is a problem with time - it is immutable.
Cool! I would suggest using [precise_time_ns()](http://doc.rust-lang.org/time/time/fn.precise_time_ns.html) instead of `now_utc()`. `precise_time_ns` is connected to a monotonic time source, so you won't end up with weird timing numbers if you're using a stopwatch while the system time is being adjusted. I believe it'll also give you more precise timing information in some implementations.
Cool! I'll keep an eye on the bug and PR.
It just has "as uint" everywhere. Good luck refactoring.
What are things that are inherently unsafe in Rust? I'm not very experienced with programming in Rust by any measure, but I have yet to come across a problem which required unsafe blocks...
&gt; The buck has to stop somewhere. This "somewhere" can be made very small, as small as a tiny proof-checker which can be less than a 1000 lines of code. You then manually or automatically proove correcteness of all your code and verify it with proof-checker. The only thing you need to trust is the correcteness of proof-checker and correctness of hardware which also can be a subject to automated proof-cheking.
I'm not sure if servo serves as a good example here by just finding all the occurrences of the word unsafe. Servo does a lot of transmutes at lower layers to emulate inheritance in order to make the DOM work. Once rust gains a way to better handle the needs of servo in some form of inheritance, I think there'll be a drastic drop in the usage of unsafe in servo.
You require `unsafe` in three situations: * when calling an `unsafe` function * dereferencing a raw pointer * reading or writing a static mutable variable http://doc.rust-lang.org/reference.html#unsafety That's it.
Cargo uses no unsafe code. I've never written a library that needed to use unsafe, outside of FFI. Skylight is supposed to be `unsafe`-free soon. Furthermore, isolating `unsafe` is still a huge advantage: if something goes wrong, you have a limited place to look.
"Rust in practice isn't a magic unicorn" - bingo!
Damn, commented while I was still writing mine :)
It wraps back to 0, but that only happens once every ~293 years. You should be able to detect that overflow occurred and handle it without too much pain.
I agree, I think it might make sense to allow indexing with any unsigned integer type without explicit cast. Some have proposed more general solutions such as allowing integer widening implicitly but that sounds a bit more dangerous to me but it might do the trick.
&gt; One could support type-level natural numbers for indexing and other things without full spectrum dependent types but I feel it would be a hack of a different nature because it's not a general solution: you can implement several other type-level constructions in the same manner as this library that wouldn't be included in the one-off compiler implementation of type-level nats. Having type-level parameterization over constants (of integer types, and likely as an eventual outer boundary, any type that's `Copy`) which can be reflected down to the term level, but without the reverse direction of allowing types to depend on terms, seems like a reasonable approach to me? (This is essentially what C++ has, and in Glasgow Haskell terms I think it corresponds to `DataKinds` and `TypeLits`.)
Does it mean dependent type?
Something to consider is that C# and Java don't target the hardware directly but rather .Net and the JVM respectively. As such they can afford to ignore the underlying kernel VM and do their stuff the way they like since they control the language *and* the virtual machine. Rust however is meant to run directly on real hardware (on bare metal if need be) so it has to be a little more flexible.
Not necessarily, the proof checker doesn't need to be integrated into the language.
Yea, they manage to hijack the connection, even when you're using https.
true ; i'm just commenting on the choice of names. a lot of people in c++ were just used to assuming 'int'=32bits anyway. And I'm not sure I could tell you what long is supposed to be :) I can see rust has more need of things like size_t like C++ has. anyway it already has i32 etc so its' academic
This is very, very common. I heavily utilize http://foo.com for the purpose of actually seeing a friggin' login page on random Wifi connections.
This is still a possibility! We can add new macro invocation forms any time after 1.0.
Sounds like [view patterns](https://ghc.haskell.org/trac/ghc/wiki/ViewPatterns) in Haskell.
Unsigned numbers are modular, so (900) - (2^64 - 100) = 1000, like you'd expect if it didn't wrap around. Then you only have to worry if the program runs for more than 293 years straight.
That contradiction in start of epoch is not a problem, since you can reliably use that timer only for time deltas anyway. And `899 - (max_u64 - 100)` is defined to be 1000, which is time that you want without any manual handling of an overflow. Also, that `Z_{2^64}` field behaviour makes it easy to do some things, like compute eg. overall time spent in some part of a loop: let (mut start, mut end) = (0, 0); while ... { a(); start += now(); b(); end += now(); } println!("{}", end - start); Edit: Ok, that example is equally simple as "normal" version, so it's a bad example and it's probably only me that likes that one more. Edit2: The version I was really thinking about was: let mut sum = 0; while ... { a(); sum -= now(); b(); sum += now(); } That version saves you one register. Which sometimes can be really important (eg. on GPU).
It looks like there's a paper by Don Syme [here](http://research.microsoft.com/pubs/79947/p29-syme.pdf) which details the design and implementation of Active Patterns. 
Why then does something as simple as URL parsing require `unsafe` blocks?
I am not sure why `Geometry::Instance` has an `inv_transform`, when the transform itself has both the forwards and backwards matrices. 
&gt; simple as URL parsing Someone's never parsed a URL :wink: Well, let's find out: $ git clone https://github.com/servo/rust-url.git $ cd rust-url $ vim `git grep --name-only unsafe` Two files. // Windows drive letter quirk unsafe { path_part.as_mut_vec()[1] = b':' } http://doc.rust-lang.org/nightly/collections/string/struct.String.html#method.as_mut_vec &gt; This is unsafe because it does not check to ensure that the resulting string will be valid UTF-8. let mut output = unsafe { String::from_utf8_unchecked(output_bytes) }; http://doc.rust-lang.org/nightly/collections/string/struct.String.html#method.from_utf8_unchecked &gt; This is unsafe because it assumes that the UTF-8-ness of the vector has already been validated. and finally unsafe { output.as_mut_vec().push(code_point as u8) } Which we've already seen. So, all three of these _could_ be removed, if you wanted. You'd have a very minor performance hit, due to the extra checks for UTF-8 stuff. 
&gt; Cargo uses no unsafe code. False. There are at least 10 `unsafe` blocks in https://github.com/rust-lang/cargo/blob/9d550782bd9aa36341e4a3efcef3dd59507f3c0a/src/cargo/util/sha256.rs 
I +1 to this. It bothers me that some people want int to be static (but implicitly 32 bit) and some want int to represent the word-size pointer type (and thereby require explicit casting everywhere to anything that needs to be serialized). Both incur the punishing overhead of having implicit knowledge, bad for understanding the code, or explicit knowledge, which is bad for making the code understandable. An explicit intptr, uintptr would do a great deal to ease this, as you should basically never be working with those types unless you're also working with unsafe pointer code. Casts to them, I think, should be explicit and possibly even flagged with a warning outside of unsafe code, because interacting with those values is not guaranteed to be portable. Sure, it can be done correctly, but a uintptr in a struct is a code smell to me.
Ahh, this is new :( Upon further reading, this is FFI to OpenSSL, so that doesn't really count, imho. But you're right that it's an asterick.
Try again after emptying your browser cache. It seems a temporary hiccup in the server configuration.
And this is why the author of the linked article finds it daunting to check all uses of `unsafe`. And don't forget that things might change in future versions.
Those are merely FFI bindings. I don't think that really counts.
It is extremely common to build arrays of integers that have specific sizes, in many domains of programming. YACC uses i16 for good reasons, and being forced to use "maybe 32 bits, maybe 64 bits" is simply not acceptable from a performance point of view. Similarly, I've done a lot of graphics work, and lookup tables using u8 and u16 are extremely common. Multiple levels of lookups in indirection tables, i.e. foo[bar[x]], are also extremely common. Rust needs to support these common, healthy, important programming idioms without adding a lot of useless "as uint" cruft. It harms both readability *and* correctness.
This took me approximately five minutes.
Did you spot some bugs in racc? If so, please open an issue on Github, or just send a pull request. Thanks! And yes -- the endless "as uint" conversions definitely *created* lots of bugs during the port. I'm not surprised that there are still some in there.
This is actually somewhat of a misconception about C# / .NET. There is no "virtual machine" in .NET. .NET always translates the bytecode to native machine code. The CIL (the bytecode specification) very precisely specifies the mapping from integer types like i32 to machine behavior. There is no ambiguity. Can you clarify what you mean by "underlying kernel VM"? Not trying to be obnoxious, but that phrase doesn't really mean anything to me.
You're right, there's no need to keep the inverse around since we already have it stored in `transform`. I'll look into adding a set of `as_inverse` functions or such that will treat the Transform as its inverse so we don't need to store two copies of the same thing. Thanks for pointing this out!
When will this be fixed? Should the docs be temporarily changed to reflect this?
Cool! Going to try to go.
Thanks!
Thanks for the link, that's really helpful.
Neat! I like the high-level API. A few code style tips: * The [guidelines](http://aturon.github.io/style/whitespace.html) recommend 4 spaces for indentation. * When returning a value from function, `something` (no semicolon) is usually preferred to `return something;`. For example: fn current_time() -&gt; Timespec { return time::now_utc().to_timespec(); } can be replaced with fn current_time() -&gt; Timespec { time::now_utc().to_timespec() } * This fragment of code: match self.start_time { Some(t1) =&gt; { // ... }, None =&gt; { }, } can be simplified with `if let`: if let Some(t1) = self.start_time { // ... } * The `Option` type has [several useful methods](http://doc.rust-lang.org/core/option/enum.Option.html#method.is_some), so this: pub fn is_running(&amp;self) -&gt; bool { return match self.start_time { Some(_) =&gt; true, None =&gt; false, }; } can be turned into this: pub fn is_running(&amp;self) -&gt; bool { self.start_time.is_some() }
That's good news. I'm thoroughly convinced that macros truly have a lot of power and could be an area where rust shines outside of the usual cool things about rust. I absolutely love metaprogramming. Seeing macros get better and more expressive will be nice.
Interesting read. It even mentions F# active patterns in the related works section. I've found this sort of pattern to be very useful since it can provide for a useful abstraction for matching on many data types. For instance, it can help clean up matching with struct patterns by being able to write views around a struct that let you match on the relevant parts of a struct. If language level support for something like this were added, I could see it as a great abstraction and encapsulation tool. You could have struct fields that aren't directly exposed to the consumer of a module, but they're still able to get their values and match on them via pattern matching.
`unsafe {}` it is!
Mhh, well of course all languages assume a certain model for the underlying hardware architecture. C says that you need a char type that's at least 8 bits and an int type that's at least 16 for instance. It also says that you can always safely point to one byte past the end of any object. It's up to the implementation to conform to the standard and make sure that this is always true. Basically the higher level the language the more arbitrary restrictions they can put on the runtime environment. If you were to write code for a segmented 16bit architecture C would expose that to you. I'm fairly sure a C# or Java runtime would hide that to you because the language doesn't allow for that and would fake a flat memory layout. I might be wrong about that, I haven't consulted the standard, but could you directly address such an architecture with C#? I somehow doubt it. That means that writing portable C code is harder than portable C# code. It also means that you can do all kinds of crazy low level stunts with C that would be very difficult or maybe impossible with a higher level language. Rust is a bit halfway in that regard, I'm fairly sure that you couldn't easily target a segmented architecture with it either for instance.
Pedagogically, it seems like Design 1 would be the most helpful for newcomers like me. I use ruby day-to-day and dabble in other high level languages. The only C code I've written in recent memory is [this puzzle solver](https://gist.github.com/losvedir/661564), and it was just to learn some C. I see now in that gist that I used *int* because I didn't know any better! It just seemed reasonable since "int" is the simplest type name for an integer. But reading through the discussion I see now that that's not the right assumption, and when coding at a low level you should always be aware of your bit widths. In rust, which focuses so highly on safety, I think I'd prefer to not *have* "int", and then have to look up what I'm supposed to do. Having "int" as an alias for "i32" (or something else) is a missed learning opportunity. I'm sympathetic to wycats's community fragmentation concern, but I don't think it would be an issue here. *Good* crates, worth sharing and using, will have thought about their use cases and used appropriate int types, regardless of if there's some "blessed" int default. I guess what I'm saying is that I expect fragmentation is simply inherent in the system, since i32 makes sense in certain domains and i64 in others.
Could you please repeat that stunt with the servo code-base? :wink:
Well... even if we can, you no longer have rustc. 
&gt; In any case, all garbage collected languages I'm aware of have unsafe "escape hatches". Do you think these are used as often as in Rust? I think the central argument was that "in practice" Rust involves the use of `unsafe` quite frequently.
&gt; In My Humble Opinion, Rust is safer than any other language I used before. Which says more about you than about Rust.
I also got an error and I'm on my private network. http://i.imgur.com/4z1VnKK.png EDIT: Seems like doing a Ctrl+R worked.
Despite what I've done with this library, I'm not really a fan of DataKinds/TypeLits/singletons in general. There are just too many usability problems, limitations, and inefficiencies that don't exist with proper dependent types. The argument is usually that it's easier to implement those things than proper dependent types, but it's not true. The reason is because, once you have those features, the usability issues become so apparent that everyone starts pushing for various ways to work around them. This leads to a hodge-podge of new features and extensions that help in some ways but leave all sorts of weird exception cases (and sometimes unsoundness). The language ends up being more complex and unwieldy than if dependent types had been adopted in the first place. This is pretty much exactly what I see happening with GHC right now. I hope Rust doesn't go down that route. Sort of off-topic, but if the only real argument against proper dependent types in something like Rust is because of undecidable type-inference, well, type-inference is already undecidable in Rust so it's irrelevant. Furthermore, Rust already allows arbitrary recursion in the type system (from the brainf•ck example), so the checker can already fail to terminate too. Even the erasure story is better for proper dependent types (see Idris) than for what GHC is doing.
Absolutely! Something like Haskell's [lens library](https://github.com/ekmett/lens#lens-lenses-folds-and-traversals) would also be great for that. I think it's a realistic possibility in Rust once we have HKT.
Assume: Library A provides `struct StructA`. Library B provides `trait TraitB`. Does A implement B? With the situation right now we only need to check in library A and B, and then *only* in the mods that define `StructA` and `TraitB`. If we allow traits to be implemented anywhere, the compiler would have to search all libraries in scope. And since every library could `impl TraitB for StructA` there might even be multiple (possibly conflicting) implementations. If at some point we allow negative trait bounds (e.g. `fn foo&lt;T&gt; where T: !TraitB { ... }`) it becomes very very confusing; if we allow traits to be implemented anywhere then including a new library could caue existing code to fail to compile because a negative trait bound no longer applies.
That (running rustup) is exactly what this thread is about. :)
What about a trait with type parameters though, like `Add`? From a quick test it seems like it's allowed as long as one of the parameter types is defined in your crate, which seems sensible to me. However, something odd came up during my test. If I have #[deriving(Copy)] pub struct Unit; impl Add&lt;i32, i32&gt; for Unit { fn add(self, rhs: i32) -&gt; i32 { rhs + 1 } } impl Add&lt;Unit, i32&gt; for i32 { fn add(self, _rhs: Unit) -&gt; i32 { self + 1 } } Then `Unit + 5` works fine (as expected), `5.add(Unit)` works fine, but `5 + Unit` fails with `error: mismatched types: expected '_', found 'Unit' (expected integral variable, found struct Unit)`. This behaviour seems odd to me - I thought `+` was desugared to `.add(...)`, so I'm not sure why this fails.
Fortunately no application has ever become memory unsafe because it used OpenSSL. :)
I doubt that. You can run the compiler and figure out all the type inference stuff, and the autoderef and operator overloads and all, and then translate to a simpler Rust where everything is explicit. When you translate methods to simple function calls (trivial) you have something that probably looks a lot like C. You can simply change the `&amp;` and `&amp;mut` to C pointers, and then inform the prover that the pointer is noalias in the second case. I imagine an intern could could do this work and end up with a tool that would be able to prove most Rust code (assuming preconditions, postconditions, and possibly loop invariants are supplied).
This should be fixed now. Because of [another issue](https://github.com/rust-lang/cargo/pull/1110) after installing cargo again you may have an unused /usr/local/lib/cargo directory sitting around. It's safe to delete. Sorry for the mess.
&gt; As far as I can tell, with this library you can't turn a type level number into value level number It is now possible to do this: assert_eq!(Add::&lt; N03 , N02 &gt;::to_uint(), 5u); assert_eq!(Mul::&lt; N03 , N02 &gt;::to_uint(), 6u); assert_eq!(Exp::&lt; N03 , N02 &gt;::to_uint(), 8u); assert_eq!(Fac::&lt; N04 &gt;::to_uint(), 24u); assert_eq!(ToSNat::&lt; N04 &gt;::to_snat().to_uint(), 4u);
Andddddd this is why I'm glad I just build / run from source. 
I also had this problem, and in addition had a problem publishing a crate about 30 minutes ago: $ cargo publish Updating registry `...` Uploading ... http error: SSL connect error It seems like this could be related.
I am not a programmer (and usually more of a 'watcher' around here), but thought I could try to further the conversation... The first thought that came to mind is if the pieces are in place to connect to websites. If they're stable/usable. http://arewewebyet.com/ gives me pause, but I hope someone else can chime in to flesh out the topic.
Actually no. The installer doesn't need to be run as root on OS X if Homebrew is installed.
Actually, the Linux kernel's timer is pretty much actively malicious. The epoch is set to slightly before zero (I think about twenty minutes) precisely to make this kind of error common. I haven't looked into what rust's library is using to get the time, but it'd be worth checking the values right after startup.
What I would love to see for learning Rust would be some sort of a mini REPL where you could paste in an expression and the compiler would tell you what type it thinks the expression's result is. 
If I was going to install anything besides Rust, I'd probably segregate them all with GNU Stow. That's a bit much to expect of the standard Rust install procedure, though.
You may as well consider to `impl Index` on a custom type for `gram.rrhs` by yourself so that it takes `i16` as an index.
The compiler itself is essentially a giant `unsafe` block in this context since it's all trusted to be correct. The fact that a feature is hard-wired into the compiler doesn't make it inherently safer than the same thing done in library code. It's not going to be formally verified - it's not realistic at all. At best a subset of the language can be verified... but not the implementation.
One thing to consider is Erlang/OTP has been used in many high volume, high availability production systems for a long time. And Rust is pre-1.0. Also "high performance" in the context of a web service has more to do with persistent storage than application language in my experience. Side note: concurrency isn't a magic sauce to make things faster. It's used in Erlang as a data model, not a performance boost (BEAM was single threaded for a long time IIRC).
Thanks for your answer. I think your code is a good example of where neither `UnsafeCell` nor `RefCell` works - the refcell would panic (both foo and set_name would borrow the refcell and the latter one mutably). And to go from a segfault to a panic is to go from "extremely bad" to "very bad" IMO - I don't want my code to panic either. &gt; Of course, it is possible to write correct unsafe code, but there is a fairly large list of subtle things to think about. So that was actually my question, if there was such a list, and then I could see if it would make sense to use UnsafeCell or not for my use case. So far the list is: * `UnsafeCell` - Never return a reference to the internals * `RefCell` - Compiler prevents you from returning a reference to the internals * `UnsafeCell` - Never hold references while calling externally supplied code * `RefCell` - Never hold refcell while calling externally supplied code * `UnsafeCell` - Manually add NoSync marker * `RefCell` - Adds NoSync marker for you Anything else?
&gt; I think your code is a good example of where neither UnsafeCell nor RefCell works Depending on the desired behaviour, there's no strategy that makes arbitrary internal references "work" in cases like the one I mentioned above. If `name` variable is supposed to be exactly the `name` of `Foo` at the point `name` is used even a garbage collector (GC) would not work: the `name` local variable will point to outdated data. (If `name` doesn't wish to see any updates that occur, then a pervasive GC is one way to handle this, but the whole point of Rust is memory safety without pervasive GC.) A choice of `RefCell` vs. `UnsafeCell` is a matter of how/*if* the incorrectness should be indicated to the programmer. &gt; the refcell would panic (both foo and set_name would borrow the refcell and the latter one mutably). And to go from a segfault to a panic is to go from "extremely bad" to "very bad" IMO - I don't want my code to panic either. This is going from "memory unsafety" to "memory safety". A segfault is essentially the absolute best/luckiest outcome from a corrupt program, memory unsafety can manifest in arbitrarily confusing/bad ways (e.g. reading the private keys of a server straight out of its memory over the internet). `RefCell` has the advantage of stopping the program going rogue by indicating the problem immediately. `UnsafeCell` leaves you completely on your own for making sure every corner case is covered, with absolutely no guarantees about if/when/how mistakes will make themselves known. Sure, your application logic probably stops working after a panic, but it also stops working after a segfault and starts possibly doing really bad things if it doesn't segfault in a memory-unsafe situation (maybe it corrupts your database, maybe it reveals secrets, maybe it lets the user's computer be pwned). It may be the case that the fundamentally broken version (`UnsafeCell`) works in testing (e.g. the old allocated string data was left untouched for long enough during the testsuite run, or the simple allocation patterns that are common in testing mean memory gets reused in a lucky way, so things worked OK), but falls down as soon as people start doing real things with it in production. It may be the case that it usually works in production, until some blackhat works to the right incantation to exploit it. It is playing with fire. &gt; if there was such a list, and then I could see if it would make sense to use UnsafeCell or not for my use case The answer is "no, until you benchmark to see that the overhead of `RefCell` really is a problem". I suppose writing your own RefCell-like data structure that ensures it exposes a safe API is good too, but I'm really dubious of the ability for a single human being to write totally correct unsafe code. It has been my experience that a detailed code review (the sort that usually occur when landing stuff in the main rust repo) will pick up at least one point of incorrectness in large changes to `unsafe` code. `RefCell` panicking at runtime is a little akin to a compiler type error; replacing these with `UnsafeCell` is akin to "fixing" type errors by just force-casting via `std::mem::transmute`: it is *way* worse.
It's a fork less of the original code?
Installing in /usr/local by default and allowing the user to specify a custom prefix if they want to is pretty standard in the un*x world. On linux the package manager typically puts things in /usr. On freeBSD the base system is in /usr and the ports get installed in /usr/local so it might be a little more likely to conflict there. I wouldn't like it if some random installer started putting stuff in my home directory (dotfiles are bad enough). `man 7 hier` on linux gives the guidelines for the filesystem hierarchies.
Good idea! In fact, if it makes sense, one can use a wrapper type for the `i16` used for `rule` and `rrhs`, and `impl Index` to only uses these, to reduce the chance of accidentally e.g. writing `gram.ritem[rule]` (if it makes sense to use different wrappers for `rule` and `rrhs`). For example, the compiler has [a graph representation that has separate types for edge and node indices](https://github.com/rust-lang/rust/blob/c594959cdff07b5545747809bb045bfa2868ebcc/src/librustc/middle/graph.rs#L39-L72), to assist with avoiding mixing them up.
Benchmark: https://gist.github.com/diwic/3a0bd869bac3d5fac810 test test_refcell ... bench: 258 ns/iter (+/- 1) test test_unsafecell ... bench: 231 ns/iter (+/- 2) (This was from just running "cargo bench", I'm not sure if that compiles with optimisations on or off) Whether 11% more CPU consumption is a problem or not depends on your use case, of course. And if you make one-off code running on one computer only, or code dealing with private keys on the Internet. But sure; I get your point - getting UnsafeCell right is difficult (I was just trying to understand *how* difficult), and if you get it wrong, memory unsafety is nasty. I've caught enough memory bugs to like Rust just for the fact that it goes to great lengths to avoid them :-)
One of the big reasons why companies use Erlang for messaging is because there is already a strong open source messaging solution out there to act as a base: [Ejabberd](https://www.ejabberd.im/). If you wanted to rebuild that from scratch I would investigate any language with strong concurrency support(Java, Scala, Go, Erlang to name a few). There's nothing magic about Erlang that lets it do the job but it is battle tested. I would actually shy away from Rust for doing this right now because the language is still in flux and the standard library is being finalized. Also AFAIK there is no stable http [client](http://arewewebyet.com) nor [server](https://www.google.ca/search?q=rust+http+server) library available for Rust, kind of a requirement for a web service.
I've asked here before whether you can [use Servo's CSS selector matching](https://www.reddit.com/r/rust/comments/2k4s0w/is_it_possible_to_use_html5ever_and_servos_css/) to scrape HTML documents, and the answer is "not yet" because the code wasn't extracted from Servo and moved to a library. But you can walk the DOM with html5ever and do it in a lower level way. Also, on a comment to that Github issue, &gt; &gt; Is html5ever’s src/sink intended for production use, or is it closer to a proof of concept? &gt; I won't really know until someone tries to write a production-quality web scraping library with html5ever :) &gt; There are no glaring omissions that I can think of. It's sufficient for running the html5lib tests. All the same, I think a scraping library would want its own tree type, because it's just not that much work, and it could contain consumer-specific fields (caches, etc). So I think html5ever is intended to support your use case. For HTTP you would use [hyper](https://hyperium.github.io/hyper/hyper/index.html).
I agree with this approach, but you need to fix this leak: let mut device = try!(alc::Device::open(None)); let mut context = try!(device.create_context(&amp;[0i32, ..0])); In case "create_context" fails, the device handle will be leaked.
Good point! I will edit the post. If device was RAII as well, this wouldn't happen ;)
Just to add to this, check MongooseIM https://github.com/esl/mongooseim , a fork of ejabberd which is easy to extend and configure
Seems it's not RAII... https://github.com/bjz/openal-rs/blob/master/src/alc.rs Can you edit it using an Option rather than a Result return ?
Sure! It's basically the same thing, only a little more verbose.
I tried... but failed :( With only the top root extern crate openal and some use in the module: error: failed to resolve. Use of undeclared type or module `alc::Device`
ah ah great ! 1. I fixed the String (.to_string() was missing for err) 2. and the context destroy as Context... is RAII :) 3. and rustc told me that the var don't need to be mutable (let mut device = match alc::Device::open(None)) I still have a warning I don't ... : warning: struct field is never used: `context`, #[warn(dead_code)] on by default. By yes it is not read after being set... extern crate openal; use std::i16; use std::f64::consts::PI; use std::num::FloatMath; use std::io::timer::sleep; use std::time::duration::Duration; use openal::al; // A module named `openal_wrapper` mod openalc_wrapper { extern crate openal; use openal::alc; pub struct Openal { device: openal::alc::Device, context: openal::alc::Context, } impl Openal { pub fn init() -&gt; Result&lt;Openal, String&gt; { let device = match alc::Device::open(None) { Some(dev) =&gt; dev, None =&gt; { return Err("Couldn't open device".to_string()); } }; let context = match device.create_context(&amp;[0i32, ..0]) { Some(context) =&gt; context, None =&gt; { // This wouldn't be needed if device followed RAII device.close(); return Err("Couldn't create context".to_string()); } }; context.make_current(); Ok(Openal { device: device, context: context, }) } } impl Drop for Openal { fn drop(&amp;mut self) { self.device.close(); } } } /* * Main */ fn main() { println!("OpenAL test"); // init let wrapper = openalc_wrapper::Openal::init(); match wrapper{ Ok(_) =&gt; println!("Init successful"), Err(e) =&gt; println!("Error: {}", e), }; // play play_sinwave(); } /* * Play */ fn play_sinwave() { let buffer = al::Buffer::gen(); let source = al::Source::gen(); let sample_freq = 44100.0; let tone_freq = 440.0; let duration = 3.0; let num_samples = (sample_freq * duration) as uint; let samples: Vec&lt;i16&gt; = Vec::from_fn(num_samples, |x| { let t = x as f64; ((tone_freq * t * 2.0 * PI / sample_freq).sin() * (i16::MAX - 1) as f64) as i16 }); unsafe { buffer.buffer_data(al::Format::FormatMono16, samples.as_slice(), sample_freq as al::ALsizei) }; source.queue_buffer(&amp;buffer); source.play(); sleep(Duration::milliseconds((duration * 1000.0) as i64)); } 
The code is not my code :) But yes I can do my own version for sure with Result. I'll try !
That being said, depending on the "time to market" and seriousness, Rust could be a good candidate: it will most probably hit 1.0 within the first few months of the year, thus stabilizing, and `hyper` (by jonreem) is being used in Servo for client-side http and has been picked up by server-side middlewares so should provide a reasonable basis. Neither the language nor the library will, of course, be as battle-tested as Erlang's...
Seems like [`context`'s `destroy` is a no-op](https://github.com/bjz/openal-rs/blob/master/src/alc.rs#L131). I believe it was added to be used as a more explicit way of freeing the resources. In this case you can simply remove it. If you don't need to call any method on the context, and just need to hold onto it, you may supress the warning: pub struct Openal { device: openal::alc::Device, #[allow(dead_code)] context: openal::alc::Context, }
 Wrong subreddit. Try /r/playrust .
Erlang at Whatsapp scale to me means requiring an Erlang VM expert be on staff because while Erlang is great most of the time, [it's not great all the time](https://gist.github.com/chewbranca/07d9a6eed3da7b490b47) (shouldn't be anything new for anyone seriously considering Erlang, if you're unfamiliar with these things, there's more you don't know about). At that scale strong systems knowledge is also required. Probably the kind of candidate for the former role can do the later but perhaps not the other way around (in part because there's no bug tracker and spotty docs, so much Erlang knowledge is effectively lore). Depending on your time to market, goals and knowledge, Erlang might be the way to go, but I suspect in the long run Rust will become a better choice.
It's not a connection issue. Apparently the certificate is valid for www.embers.io and embers.io, hence the errors.
That's because `Rc`s are very low cost. They only have an overhead when you call `clone()`.
Glad to hear you like it! I added a Scheduler impl for [TaskPool as well](http://tobbebex.github.io/TaskGraph/task_graph/scheduler/trait.Scheduler.html) (unfortunatly, due to [this](https://github.com/rust-lang/rust/issues/20408) it has to be in a Mutex+Arc for now). TaskPool should be the preferred Scheduler for CPU bound tasks that never block. Thanks for linking to the docs and tests! I hope they can provide some guidance until I get around to write up some more detailed docs.
I don't really buy this argument. You can say the same thing about the compiler for any language, or the compiler for the compiler of a language. I'm talking about making practical solutions to get us a few more 9s of confidence. I'm not entirely satisfied with the way `unsafe` is handled in Rust in that I wouldn't bank on Rust code running on a spacecraft not to fail. I'd like to get closer to that level of reliability statically.
Wow, that easy. Thank you! It works now :)
Types are entirely a compile time thing; if you are referring to trait objects, which represent a value of some unknown type, the trait object type is still a type and it can be `'static` or non-`'static` freely. `'static` refers to types that contain no non-`'static` references, meaning it is not attached to any particular scope, so the owner can be kept alive as long as is desired without the value becoming invalid due to e.g. leaving a certain scope.
A non-`'static` type is a type that contains non-`'static` references, e.g. `&amp;'a T` with `'a` != `'static`. I believe the concern is ensuring that downcasting is safe is much harder if there are lifetimes, and it's likely infinitely less useful. I think the TypeId would have to include some representation of the lifetime that ensures that one can only downcast to references which are valid. (At the moment, I believe the TypeId is a hash that just completely discards lifetime information.)
So basically, 'it's kind of complicated to implement TypeId with lifetimes, so we didn't'? I guess that's a fair call, but it's a bit disappointing, considering it's marked as stable, and means that we're stuck with no real reflection story for a broad class of struct's in the post 1.0 world.
&gt; There's nothing magic about Erlang that lets it do the job but it is battle tested. Erlang's lightweight processes (actors) are preemptively scheduled on the VM. This is a key and useful distinction between Erlang and other runtimes with respect to actor-model concurrency patterns. When attempting to use Akka (Scala/Java) to build Erlang/OTP-like systems you have to be conscious of whether or not your Akka actors are going to block the ExecutorService (thread pool) that the actor system is running on. There's A LOT built into the Erlang VM, abstractions in OTP, and the ecosystem that sets it very, very far apart from alternatives when it comes to building fault-tolerant, highly-available systems/applications. 
Strong systems knowledge is invaluable at significant scale no matter what the language/library choice is. There's certainly a bit of lore aspect to the Erlang/OTP ecosystem that holds it back. Though honestly, having used Rust for the last 9 months, and attempting to onboard a few people recently to use it... I am a bit afraid that Rust is going to be bitten pretty hard by the "Google never forgets" syndrome. A huge portion of what contextual Rust information and code there is out there is often useless at best and misleading at worst because the language has changed so much. I'm hopeful that the effect is short-lived however. However, in terms of ability to debug odd behavior... I've yet to find any system that's as robust as Erlang. You can literally attach to the REPL of any running Erlang VM and interrogate any running application, hot-load code, rollback immediately, etc. In order for Rust (or anything else) to be able to facilitate that it will require the building of a similarly robust runtime platform... which in the case of Rust is sort of antithetical to the point and deployment expectation of the language. 
The type ID hash takes higher-rank lifetimes into account, so for example `fn(&amp;'static int) -&gt; &amp;'static int` and `for&lt;'a&gt; fn(&amp;'a int) -&gt; &amp;'a int` will be given different type IDs. Everything else has been discard by trans by the time the hash is computed.
Erlang has some advantages, sure, but my point was you could implement a messaging system in any language with strong concurrency support either in the standard library or through a 3rd party framework.
Sorry, I was on mobile and could only give half an answer before, not everything I wanted to talk about. You also said something about lifetime elision, lifetime elision is a feature which alleviates the need for lifetimes when writing functions in some cases. In some cases in the past, when using references in function arguments, you need to do the following: fn return_it&lt;'a, T&gt;(thing: &amp;'a T) -&gt; &amp;'a T { thing } but the rules which indicate when a lifetime is needed has changed recently, and therefore the explicit lifetime is no longer needed: fn return_it&lt;T&gt;(thing: &amp;T) -&gt; &amp;T { thing } to read the whole thing, [here is a link to the RFC](https://github.com/rust-lang/rfcs/blob/master/text/0141-lifetime-elision.md)
If the language is up to the task of implementing a feature in a library with `unsafe` moving it into the compiler is a step backwards. It will make the implementation more complex and puts it out of reach of most potential contributors. To audit the standard library you only need to know Rust. To audit the compiler, you need far more knowledge. It's feasible that we could prove small portions of the standard library correct after doing a subset of the language, but validating that `rustc` is correctly implemented is not going to happen. There's nothing magical about a feature in the compiler - it's not inherently more safe, just inherently more complex.
I don't know if there's a deeper reason, I also don't know if it's at all *possible* to take into account lifetimes in a way that is actually useful.
Mine would be a live messaging service, so I believe the number of concurrent connections should have a greater impact on performance than persistent storage. High performance was just an indicator, not the primary reason for the choice of language. But the lightweight process based architecture really fits well with the modelling of a messaging service, where one channel of message hardly has anything to share with the others anyway.
Note, it's 11% if essentially all you are doing is modifying the `RefCell`, I imagine the vast majority of your application is *not* modifying the `RefCell`, so the extra overhead of `RefCell` for that specific operation is not so relevant. &gt; And if you make one-off code running on one computer only, or code dealing with private keys on the Internet. "I want to be able to debug easily" also factors in there.
&gt; So having a single language to bother about might be easier. One way or another, if you're picking Erlang, you're also picking C. (Don't forget port drivers and NIFs.)
Do you not plan to store the messages anywhere? If no, why do you need a server at all? If yes, the impact of concurrent connections is likely to be much higher on your persistent storage (which does not scale well if you're unwilling to accept data loss or corruption) than your application. You can spawn as many servers as you like running some decrepit PHP application behind a load balancer to increase its performance but each one is talking to the same database, and it's much harder to have multiple read/write databases behind a load balancer. When it's all done, I believe you'll be surprised how little time your application spends computing versus waiting on the database or waiting on network traffic. 
Makes sense actually. You'd either 1. Do standard stuff with your things, 2. Do your stuff with standard things, or 3. Do your stuff with your things. Doing standard stuff with standard stuff would come from the language - or there's probably a good reason why it doesn't.
my suggestion is to relax the rule in an *executable*, since an executable wont be included in other projects. (i.e. consider any impl's there as private). There's a counterargument that you might want to move code out into a library, but equally you might already resist dividing up into crates precisely to avoid the restriction IMO.
No, I do not plan to store messages. So you suggest I use P2P? I need a server to manage who can talk to whom. Also, some information about the users has to be stored, so access to this info would be like once per connection, instead of once per message (as in case when storing messages). I do not plan to support sending messages while offline, at least initially. I do not have experience in web services, I am coming from C++. So looking at creating a lean setup with as few components as possible. The number of components a web service needs just overwhelm me!
I hope Rust can outperform coroutine frameworks for Python (eg. http://gevent.org/pygrunn2014.pdf )
Thanks for the thorough replies - seems like this is something that could be added in certain situations
Making things more permissive is generally backwards-compatible. We're taking the conservative approach for the first stable release in order to give us as much future wiggle room as possible.
Maybe we don't need to add equality constraints to where clauses -- this should be the same thing.
Thanks for this. I'm keenly interested in hearing about non-systems programmers coming to rust, and their successes and failures. Trait objects are certainly... a bit unusual. They're also under-documented at the moment, so that may be my fault.
&gt; So if and when we hit scale, we might have to dig deep and customize them. So having a single language to bother about might be easier. I would not use this as a decision criteria. I used to work for a web startup, got acquired, and now work for one of the larger web companies in silicon valley. Our stack was rails, mongo, and rabbitmq. We benchmarked rabbitmq at one point and a single node was doing at least 10k messages/sec. We gave up at that point with a note to revisit rabbitmq performance if our traffic levels ever got close to that high. If you ever hit a point where tuning rabbitmq is a concern, you're probably big enough to hire a rabbitmq expert either permanently or on a contract basis. If you're just starting out you could build your messaging service in *anything*. Go with what you are most comfortable with. What do you think about Erlang's syntax? Do you find it weird, maybe you would be better off developing in a more C++ like language like Java or Go. In the current climate, the speed at which you can release is more important than picking the theoretically perfect tools for the job.
Oh... so even stable APIs are subject to non-breaking changes? I was under the impression that stable -&gt; locked down post 1.0.
It's super easy. #[derive(Copy)] struct A; fn cmp( a1: &amp;A, a2: &amp;A ) -&gt; bool { a1 as *const _ == a2 as *const _ } let (a1,a2) = (A, A); let a3 = a1; let a4 = &amp;a2; println!("{}", cmp(&amp;a1, &amp;a1)); // true println!("{}", cmp(&amp;a1, &amp;a2)); // false println!("{}", cmp(&amp;a1, &amp;a3)); // false println!("{}", cmp(&amp;a2, a4)); // true (For fun, I also made it the same number of lines as the C++ program, but the semantics are the same if you don't do everything on one line and just replace the cmp function in your example). The reason Rust doesn't do pointer comparison by default is that, well, it's not usually what you want :) But it's a systems language and it will never hide operations like address-of from you.
Eq requires that self == self, no? 
Would this mean that every instantiation of a type with different lifetimes had a different TypeID? This increasingly feels like it's generating a lot of rarely-useful work for the compiler (hashing a bunch of stuff on the offchance that someone might want to reflect on it). I really like the idea of adding a `Reflect` bound requirement in order to be able to generate a TypeID for a type. That might help limit the amount of unnecessary work done. `#[deriving(Reflect)]` seems like it would be a worthwhile thing.
Rust can't really force that to be the case, though. Someone could write a version of `Eq` that didn't work that way, though it would be pretty lame to do so, so you can't rely on that for memory safety or anything.
I took the old script from rust-empty (now deprecated) and upgraded it for `cargo build`. If you put it in the directory where you keep clones of repos, then you can start it with `./../watch.sh` from within the project directory or `./../watch.sh -h` for more information.
Yeah, but you're supposed to use `PartialEq` and not `Eq` in that case. So it's not that you can't represent it in the type system, just that you can't enforce it.
That isn't a general solution. There are types with multiple lifetime parameters. You'd have to know the relationship between those lifetimes and other lifetimes in scope, wouldn't you? Like I said, there may be a way around this. Maybe you could do something with variadic generics? I get what you're saying about TypeId, but keep in mind that lifetimes are a bit weird because they're pretty much pervasively parametric, i.e. creating a new TypeId for *every* possible Type-lifetime combination might be really expensive. The same can be true of generic types but at least those aren't *always* generic. (Also, it occurs to me that since lifetimes are really bounds, equality comparisons on them wouldn't really make a whole lot of sense since they actually express subtype relations. You can [currently] talk about a lifetime being longer or shorter than another one, but asking if they're exactly equal feels less useful. If you take `SomeType&lt;'a&gt;` in a function, and `SomeType&lt;'b&gt;` with some additional, shorter bound on `'b` in another function (so `'a: 'b`), do those two have different `TypeId`s? Baking them into the `TypeId` would necessitate that, but that wouldn't capture that it's safe to upcast from `SomeType&lt;'b&gt;` to `SomeType&lt;'a&gt;`. I think something more like 'outlives' as its own, separate concept would be more appropriate here, if you had anything at all. Then you could theoretically cast from `Any&lt;('x,'y,'z)&gt;` to `Foo&lt;'a,'b','c&gt;` [where all three are lifetimes in scope] iff `Foo` has the same `TypeId`, `'a: 'x`, `'b: 'y`, and `'c: 'z` or something. But on that note, how would you handle recursive functions with lifetimes? I assume we can handle them by not actually expanding them [only having to do local reasoning] but that falls apart if we have to assign unique `TypeId`s, doesn't it? Would it be safe to assign a `TypeId` the same lifetime if it shows up in the same scope in the same function, regardless of recursion? Maybe it would. It's really not at all clear to me though--at best, it feels like that would make it generally impossible to compare lifetimes or `TypeId`s across functions, outside of `'static`. That feels like it would be severely limiting the usefulness of calling it for these types).
Got compilation error cmp.rs:8:13: 8:21 error: type `&amp;A` does not implement any method in scope named `as_ptr` cmp.rs:8 a1.as_ptr() as uint == a2.as_ptr() as uint rustc --version rustc 0.13.0-nightly (39d740266 2015-01-01 15:51:08 +0000)
See wrongerontheinternet's reply, it's more complete than mine. Apparently my knowledge of libstd is already out of date :)
Not sure what the relevance here is, given that the `libgit2` port only installs `libgit2.so`. What I'm saying is you can't just `-lgit2` blindly, in general, since it can require linking to other libs. It's strange that cargo builds the bundled libgit2 here, though, since the port *does* install the necessary `.pc` file.
Well… this trick works okay as a stopgap but it has some drawbacks that equality constraints could avoid. One issue with this trick is that it exploits the behavior of `impl` resolution to work and that behavior may change in the future. It's also not so obvious (to someone reading the code) why it works or what `Is` does without documentation. A `where X = Self` might be clearer.
Thanks. I needed it for something unrelated and realized it also could help with the dummy argument thing I've seen in a few libraries so it seemed worth a gist :)
Generating a TypeId for each instantiation of lifetimes is maybe not useful. However it is currently not possible to get any sort of TypeId at all once you deal with a non-static type. I guess if I have a type Foo&lt;'a&gt; I would like to be able to retrieve the TypeId for Foo&lt;'static&gt; at least.
Ah I see, it was the *old* cargo that was incompatible, not the new one.
I'm not sure that it makes sense for main to be considered dead code though during a test, its a bit hard to test the program entry point. It would be nice to at least silence that warning.
I’m not sure how to put this kindly, but I can’t figure out what this reply is about. It looks like you’re repeating from your previous comment, which I don’t feel was necessary.
Haha, sorry. I kind of forgot what I'd written in my previous reply. I was mostly responding to your indication that you weren't sure what sort of subtyping was going on.
No, you're right (although your example isn't a very good one, since in your example there *are* clear nested scopes). That is, I don't think you're correct in that the reason is because of "super" (the outlives relation is more like `instanceof`) but it *is* true that `'a` could have multiple super types (if `'b` and `'c` were not nested, but both directly under `'a`, it would not be possible to choose one as the super type). I'm not convinced that makes it impossible to downcast, though. Doesn't it make it *easier*? That is, you have the reverse relationship: each instantiated type `T` has a unique `sub(T)` (in theory). So in theory (if we worked around the runtime information problem) to downcast would be quite straightforward, right? Just follow the `sub(T)s`. If anything, it seems like what you're trying to say is that it would not be possible to *upcast* to `Any`. But because of the way lifetimes work, `Any&lt;'b&gt;` could only exist in the scope `'b`, so the problem that `'b` and `'c` were both possible supertypes of `'a` would not come up in practice (you couldn't ever try to cast to `'c` from within `'b`).
The example is not a clear demonstration of the lack of `super`, unless you mean it in a different sense from what I do. If you meant that each type has more than one possible upcast, this is also true in Java, but it certainly does not prevent Java from performing downcasting at runtime. The "outlives" relation is more like `instanceof` and the issue is with structures like this: 'a: { 'b: { let i = 0i; } 'c: { let j = 0; } } Is `super('a)` `'b` or `'c`? The answer being neither.
I wanted to demonstrate that 'there is exactly one super lifetime for a given lifetime' is not true, nothing more and nothing else.
As I wrote in the other response to your post, I don't believe this would be a problem for downcasting, since it *is* true that every lifetime has exactly one "direct" subtype, the immediately enclosing scope (so you could follow that chain). And it would not be a problem for upcasting, either, because you cannot name lifetimes that are not in scope (i.e. you would never be able to try to upcast from `'a` to `'c` in scope of `'b`). So I am still not sure I understand the relevance of your point about `super`. (Unless the real issue here is that there is no top type, I guess, but then the "every type has one super" thing feels like a diversion).
In Linux, I use `inotifywait` to launch builds after any change. while true do inotifywait \ -e modify -e create -e delete -e moved_from -e moved_to \ --exclude target --exclude '.*\.sw.' \ -q -r . || break sleep 1 # Wait one second. Multiple files can be writen at once echo -e "\n\n\033[7m# Launch cargo test... @ `date`\033[m" time cargo test done 
Yes, it's super helpful - I end up using it for parsing data coming out of iterators quite a lot.
Old story:) Here's another popular implementation: https://gist.github.com/evgenius/6019316 (it's forked from https://gist.github.com/senko/1154509 )
It would be great if FreeBSD got nightly builds like Windows/Linux/OS X; it's a pain having to include a Makefile with `libpnet` due to the lack of an easy to acquire and/or build cargo on FreeBSD...
You never actually impl'd Foo for Newtype, so even if your impl of Bar for T requires Newtype: Foo, one can't actually be found. Interestingly, I caused an ICE when trying to make your gist compile: http://is.gd/Q0YCUV Edit: I notice OP submitted an [issue](https://github.com/rust-lang/rust/issues/20413) for the very same ICE around the time this was posted.
I also want to preserve the error information, since I'm trying to potentially build a list of errors, e.g. 'expected "abc" or "def", found "ghi"'.
Not related to your question, but does rustc understand both -&gt; and the unicode arrow symbol → (can't test it myself at the moment)? That would be neat, or was it just some text editor you fed the ascii through? (Same for =&gt; and ⇒ as I just noticed.)
I got curious still. Usually borrow checker has no problems proving that mutable reference passed as argument is safe to reuse somewhere else. I suspect you do not need `'b` bound for it. However, I think you added bound to reuse this mutable reference as simple reference here: if self.expect == found { Ok(&amp;*found) } else { Err(self.expect) } One another possible way to make borrow checker happy is to pass two references: one for reuse, and another for mutation: trait Matcher { fn matches&lt;'b&gt;(&amp;self, value: &amp;'b int, mutate: &amp;mut int) -&gt; Result&lt;&amp;'b int, &amp;int&gt;; } Code here: http://is.gd/Yl5nXE
That makes more sense; however, your gist isn't going to compile as-is, because it defines both the struct and the trait in the same crate without impling the trait for the struct. You'd need to actually set up a project with imports, etc..
Thanks for the cross-post! In case the relevance isn't immediately obvious, I developed this new noise function while working on noise-rs, so the first, and currently only, implementation is in Rust.
If you're going to use existing tools, why not use ones that are battle proven and cross platform like grunt or gulp? They're designed with this functionality in mind (unlike make, which is always a hack to get 'watch' functionality working) Shell scripts suck. They're hard to maintain and platform portability is always deeply painful. I wrote https://github.com/shadowmint/rust-watch/blob/master/src/watch.rs to do this in 'pure rust' ('' &lt;-- it's not pure, it uses the fs bindings for libuv) for one of my projects. I'd probably prefer to have a stand alone application that runs as a cargo plugin that way than a shell script. 
One option you can explore is instead of: impl Foo { pub fn add_bar(&amp;mut self, bar:Bar) -&gt; &amp;mut self { } } pass self back instead. Ie. the add_bar() call consumes the instance, and returns a new one, like this: impl Foo { pub fn add_bar(self, bar:Bar) -&gt; Foo { ... return self; } } You'll immediately notice a few downsides however: let foo = Foo::new().add_bar(bar); // Fine let foo = Foo::new(); foo.add_bar(bar); // Destroys foo, because you don't catch the return foo = foo.add_bar(bar); // Fine, but awkward struct HasFoo { pub foo:Foo } has_foo.foo.add_bar(bar); // Error, cannot move out of struct. :( struct HasFoo { pub foo:Option&lt;Foo&gt; } has_foo.foo = has_foo.foo.take().unwrap().add_bar(bar); // awkward In general I've found that if you're 'chaining' like this, you basically need to have two api calls: impl Foo { pub fn add_bar_(self, bar:Bar) -&gt; Foo { ... } // Chained pub fn add_bar(&amp;mut self, bar:Bar) { ... } // Normal } let foo = Foo::new().add_bar_(bar); foo.add_bar(bar2); You can pick whatever calling convention you like; I use a trailing _ for it. It's still a bit awkward, but it does work I guess?
In Rust, `*const T` is just plain old data (it doesn't have reference semantics). This may or may not be true at the LLVM level, of course. Anyway, in Rust it's always perfectly safe to create these values--they have no associated lifetime. What is _unsafe_ is dereferencing them. As long as you never dereference them, you'll never have lifetime issues using them.
Thanks!
Any challenges particular to rust you encountered? Great write up
Yes, `Entries` is renamed to `Iter` in recent nightly. You just need to rustup to fix it.
You might want to consider using a fixed size integer instead of `uint` for portability.
1. No, it's just that values above a certain size (I believe it's anything bigger than a pointer except *probably* doubles) are passed out through pointers, which is how such things are usually implemented anyway. The important point is *actually* that Rust does the allocation *first*, then uses that as the destination for the result, as opposed to writing the result to the stack first and then copying it to the heap. 2. You can use `Cell` or `RefCell` for this; it's called "interior mutability". 3. Because it's easier to reason about and easier to implement. I believe there is an open proposal to move to "non-lexical borrows", but I don't know when that's going to happen. Post-1.0, probably. 4. I dunno... I suspect you could probably put yourself in a really hideous position if it did this and you crashed between deallocating the old `x` and assigning the new one if `x` is shared. For thread-local data, I can't think of where it would cause a problem, though.
Alternatively you can use a refcell to keep track of the borrowing. I don't know how slow this is. struct Game { monsters: Vec&lt;RefCell&lt;Monster&gt;&gt; } pub fn fight0(&amp;self) { let mut attacker = self.monsters[0].borrow_mut(); let mut victim = self.monsters[1].borrow_mut(); let dmg = Game::dmg(&amp;mut *attacker); victim.hp -= dmg; } http://is.gd/hV8LIP
I think if you change and take care of some parts in rust it will be a great alternative for java as a enterprise standard web development language. If rust becomes any easier to learn It will be a great language to learn programming in colleges. I'm learning rust as my first programming language(know basics in many languages but never went too deep) and I find it pretty hard. Maybe its because of lack of tutorials at this stage. But I don't know, I just love it and learning and doing Rosetta code problems everyday. But Steve my suggestion is by the time you finish 1.0 please do something like http://www.theodinproject.com/ its considered as one of the best ways to learn programming. It teaches basics and hard stuff and then real world projects (real world!). I know programming newbies are not your concern now, but please consider it! 
It doesn't seem like you use victim the first time it's indexed? Also you know the index in the body of the function, so it doesn't look like there's any need to return it from that inner block. pub fn fight2(&amp;mut self) { let dmg = Game::dmg(&amp;self.monsters[ATTACKER]); self.monsters[VICTIM].hp -= dmg; } It seems a little strange to be using a Vec in this situation though, as you know both the number of indices and how many monsters can fight at once at compile time (i'm sure this is just a demo though).
How about this: pub fn fight6(&amp;mut self) { if let [ref mut attacker, ref mut victim, ..] = &amp;mut *self.monsters { let dmg = Game::dmg(attacker); victim.hp -= dmg; } }
Re: Non-greedy lifetimes This exact problem is addressed by [RFC 396](https://github.com/rust-lang/rfcs/pull/396). Among 3 examples in RFC 396, your example is equivalent to example 1.
&gt; The important point is actually that Rust does the allocation first, then uses that as the destination for the result, as opposed to writing the result to the stack first and then copying it to the heap. So the caller passes in a pointer to the stack if it's requesting to return by value? &gt; You can use Cell or RefCell for this; it's called "interior mutability". Aha, clever that it gets around the borrow rules that way! And here I thought Rc was just some special case unsafe refcounting. &gt; I suspect you could probably put yourself in a really hideous position if it did this and you crashed between deallocating the old x and assigning the new one if x is shared. Wouldn't that only happen on `cell`s? Shouldn't a `cell` destructor check the reference count first?
Cool, but are you sure there won't be artifacts along the layers? I assume the 4D slice is a XY slice. How would a XZ or XW slice look?
&gt; So the caller passes in a pointer to the stack if it's requesting to return by value? If the result is to be stored in a stack variable, yes, I believe that's what happens. &gt; Aha, clever that it gets around the borrow rules that way! And here I thought Rc was just some special case unsafe refcounting. `Rc` has nothing to do with this. `Rc` doesn't have interior mutability; if you wanted a reference counted, mutable `T`, you'd use `Rc&lt;Cell&lt;T&gt;&gt;` or `Rc&lt;RefCell&lt;T&gt;&gt;` (the latter if `T` contains references). &gt; Wouldn't that only happen on `cell`s? Shouldn't a `cell` destructor check the reference count first? What I was thinking about was this scenario, given you write `x = box foo()`, where `x` is accessible from multiple locations: * The current `x` is destroyed, rendering the `x` variable invalid. * A box for the new `x` is allocated and written to `x`. The pointer is valid, but it points to uninitialised memory. * `foo` is called and panics. So let's say that `x` is a `static mut`; you now have a corrupt global variable. I also can't think of any sane way of actually *preventing* this from happening. On the other hand, by allocating first, the actual safely *observable* effects of the assignment are effectively atomic. But I could be wrong about this.
I wrote a library to solve this problem, [`rust-modifier`](https://github.com/reem/rust-modifier). It allows you to define the modifications done in one of the chaining "steps" once, but use it in both the `self -&gt; Self` and `&amp;mut self -&gt; &amp;mut Self` ownership patterns. `rust-modifier` is currently used in `iron`, where you can find the best examples of interesting modifiers.
Currently the attribute is in the `reflect_mac` crate, which is awkward because it isn't actually a macro, but I'm not sure what the convention is with attribute plugins, so I went for the same pattern as `phf`/`phf_mac` (used internally by the library).
I don't know if there is a fundamental reason that memory deallocation/allocation order is required; in fact, I would think that the equivalent of `free(x); x = malloc(...); *x = foo(...);` is the more natural order. Not sure.
Another fun way to tackle this is to md5 the output from the `find` command, with find printing the last modified timestamp of each file. If the hash changes, just recompile everything! Example: https://github.com/ShaneKilkelly/manuel-contrib-watch/blob/master/manuel-contrib-watch.manuel#L37
Very cool! Spent a lot of time in a previous life building such things as GLSL and Cg shaders, so always fun to see things like this pop up in Rust or elsewhere. However, there's no question those performance numbers are bogus -- 3ns would get you, what, 10 core cycles on a 3-ish GHz CPU. Even assuming a 5x issue rate (which is outlandish), you're not looking at anywhere near enough time to compute a single value. I think another benchmark needs to be made for these numbers to be truly meaningful.
You did the right thing. Syntax extensions like this usually go in their own crate to prevent the final user from having a dependency on libsyntax.
that's used in fight1, it uses unsafe code under the hood, anyway ;)
Sweet, however 0 and 1 as indices are only for demo, in the real version they will differ!
Thanks for the additional information! I'm still confused about Rust's unique features, guess I'll have to look more into it.
Whoah, Rust is a first programming language seems very, very hard, yeah. A lack of tutorials is very much a big part of that. But anyway, thank you for the suggestions :)
Well then, as others have suggested, you can simply `self.monsters[victim].hp -= Game::dmg(&amp;self.monsters[attacker]);` and be done with it. :)
He could have meant µs, but clarification would help. (edit: changed us to µs)
I have been a hardcore software and enterprise geek for more than 5 years I'm 19 now! Even though i don't program That's the reason I can understand rust. I just want to learn rust fast help for servo development. Love servo. I know you are going to make the guide complete by 1.0. Eagerly waiting for it. 
Currently I am also struggling with similar thing - that something usable for iron requires Sync + Send, and they are not usually easy to do. I am wondering maybe the intention was to pass such things over the channel?
And whatever you do, don't use the ~~computer~~ compiler as an example of idiomatic code...
&gt; Also the other reason collections aren't great idiomatic code is just that they're doing lots of low-level stuff (raw ptrs, allocators) most code shouldn't need to worry about. Absolutely, but sometimes one does need to write data structures, and it'd be great to see "best practices" examples for that.
I was wondering how this fits with (de)serialization, if a struct is reflect-enabled, you should be able to (de)serialize it too, right? Maybe you should talk to the person behind this library: https://github.com/erickt/rust-serde to see if you could come up with something that fits you both?
Maybe [Gankro/collect-rs](https://github.com/Gankro/collect-rs)?
I'm biased (several of these are my projects), but here are my recommendations: [Iron](https://github.com/iron/iron) has an almost uniformly good codebase that is well-documented and lives at a very nice level of abstraction. Iron uses a lot of traits and generics, as well as using some cool type hacks in dependencies like [typemap](https://github.com/reem/rust-typemap), [modifier](https://github.com/reem/rust-modifier) and [plugin](https://github.com/reem/rust-plugin), which are small but interesting examples of "crazy" stuff you can do with the type system. [Lazy](https://github.com/reem/rust-lazy) is a pretty short but relatively interesting bit of code that deals with race conditions, unsafe interior mutability, and fast synchronization. It also has some example code for dealing with unboxed closures and `FnOnce` trait objects in particular, which is hard to come by. Parts of [hyper](https://github.com/hyperium/hyper) are really nice, but others get overrun by HTTP being a complex and sometimes ugly protocol. That said, the majority of hyper is clean and modular code and if you gloss over the details of parsing and such you'll get a lot out of reading it. (the header representation is particularly interesting). [mio](https://github.com/carllerche/mio) is a rather different library that deals a lot with FFI, which may be interesting to many potential users of Rust. [Stainless](https://github.com/reem/stainless) is a good example for a simple syntax extension. [Phf](https://github.com/sfackler/rust-phf) is good if you are looking for something more complex. For data structures in particular I echo /u/Gankro with `HashMap` and `BTreeMap` as particularly good examples of collection code. That's all I can think of off the top of my head, but I'll update this or write comments if I think of other cool stuff. 
I've been meaning to write a bunch of examples and new middleware for Iron, I promise, but things have been hectic and new projects are exciting so I get distracted. You likely want to use one of the helpers in [persistent](https://github.com/iron/persistent) to handle the connection. Which one depends on the ownership mode needed and the `Sync`-ness of the data. I wish I could be more helpful. I'll move iron-examples up higher on my todo list, which I have more time for now that I'm on "vacation" :P.
Woo, this is great. Reading the value is a little bit weird (having to manually downcast it from a deref), but looks very useful!
&gt; `and_then(from_str)` Am I reading this wrong? It doesn't seem to work in the latest nightly. **EDIT:** To be specific, I get unresolved name errors, and if I change it to `String::from_str`, it doesn't typecheck.
Sqlite can't be shared amongst threads
It's not a hard requirement, but this library is being designed for broad consumption by Rust developers. Depending on the application, it may be more appropriate to work in f32 or f64, so we wanted to make it work smoothly with either, without requiring any actual casting. (All of the casting it does now is on literals, and thus should be elided.)
The slices pictured are XY slices, yes, but I took XZ and XW slices as well and I couldn't see any obvious artifacts. That's not to say that you might not be able to uncover such artifacts if you animated or processed the noise in the right way.
So, even though I have no experience with the compiler, writing RFCs etc, this syntax has kind of stuck in my head as something useful. So I wrote up this pre-RFC to gather some feedback - to see if you like it too, and if so, how I should proceed with it.
Any actual timeline on when 1.0 alpha will hit? 
[Friday, Jan 9](http://blog.rust-lang.org/2014/12/12/1.0-Timeline.html)
~~Yeah, AUR packages aren't thrilled about it yet... Or maybe~~ they've been fixed. :)
 fn preprocess&lt;'a&gt;(s: &amp;'a String) -&gt; Vec&lt;Line&gt;{ let mut res: Vec&lt;Line&gt; = vec![]; for line in s.as_slice().lines() { match line { "" =&gt; {} // Discard empty lines _ =&gt; res.push(Line(line)) } } return res; } couldnt this be done with a filter? s.as_slice().lines().filter(|line| line != "").collect() that would be cleaner, more idiomatic and faster (i think)
on some unix like os's. ...but hey, who cares if it doesn't work on windoze right?
&gt; `"Hello {}".format!("world")` This syntax would be amazing. &gt; `let f = File::open(&amp;path).try!();` This, however, just feels wrong to me. I admit it looks cleaner than the status quo, but it seems strange for something that looks like a member function to be able to insert `return` statements into the function it's called from. I might be alone in this concern though, and the readability benefit is significant. I'm in favor of something like this, preferably option 2 or 3 (but those are probably only reasonable if this can be done before 1.0). Option 1 would be okay too, but it's not ideal to have two different syntaxes for the same thing.
I think the required `!` makes it clear that "this could do something crazy"; I think it takes about as much getting used to as realizing that `try!(...)` can also insert those same `return` statements.
One problem with this trick is that there's nothing to stop someone from defining another `impl` which is invalid with regard to the intended semantics. If it were possible to keep `Is&lt;_&gt;` private or seal it or something, that would be enough. However, although we can't do either of those in Rust, there is something else we can do to prevent invalid `impl`s: struct Refl&lt;A&gt;; trait Equals&lt;A, B&gt; {} impl&lt;A&gt; Equals&lt;A, A&gt; for Refl&lt;A&gt; {} pub struct Proof&lt;A, B&gt; { _aux: Box&lt;Equals&lt;A, B&gt; + 'static&gt; } pub trait Is&lt;A&gt; { fn consistent() -&gt; Proof&lt;A, Self&gt;; } impl&lt;A&gt; Is&lt;A&gt; for A { fn consistent() -&gt; Proof&lt;A, A&gt; { Proof { _aux: box Refl } } } What this does is require that in order to define an `impl` for `Is&lt;_&gt;`, one must be able to provide a `Proof` object witnessing equality of `A` and `Self` on demand. However, there's only one way to ever obtain such an object: when `A` and `Self` are actually equal.
I think option 3 is a non-starter - it's difficult to reconcile the macro-time concepts of macros with the compile-time concepts of structs - e.g., a procedural macro could rewrite the struct to be completely different. We also don't have proper name resolution when expanding macros, let alone the type information required to know what struct an expression instantiates. I prefer 1 to 2, since it is closer to functions. I'm not sure I like the concept as a whole, but it has been requested a few times, so it certainly has motivation. Note that the priority for macros post-1.0 is stabilisation - deciding on a system we like and are willing to support long term, rather than adding new features. So even if approved, this is unlikely to be added for some time.
Wow, that makes updating on Windows in particular very nice! Thanks! Just wondering: Any thoughts about packaging the source in as well (just like the src.zip in JDK, this "feature" plus Maven-source-packages makes all those fancy tools possible)? Somehow the story for newcomers around auto-completion needs to be sound and [racer](https://github.com/phildawes/racer) needs the source... 
woohoo
You can just rebind the variable. `let mut cout = cout &lt;&lt; "Hello world!" &lt;&lt; endl` A similar thing you can do for the operator implementation itself. You should just return self without cloning it. PS: You should really add a module called `std` to get the real feeling. ;)
See also [Well written Rust code to read and learn from?](https://www.reddit.com/r/rust/comments/2pmaqz/well_written_rust_code_to_read_and_learn_from/)
IIRC you can use `.map(Line)` as well, since data constructors are functions in their own right.
For the problem with unary macros, couldn't the compiler just omit the trailing comma when no other arguments are given Also, for the $self option, shouldn't you specify a fragment specifier, like normal macro arguements or would it only work with expressions
Which version are they running? Edit: Seems they're running 0.13.0 https://www.hackerrank.com/environment
Ah, so that's why people were modding me down... yes that is what I meant :p
&gt; For the problem with unary macros, couldn't the compiler just omit the trailing comma when no other arguments are given I started down the same road and starting wondering if it was a case of too much magic. In any case, if the argument is "you don't have to modify existing macros", what about ones where the logical subject *doesn't* have a comma after it? The `$self` syntax has the huge advantage of being *completely unambiguous*. If you want it to work in method position, you have to define a rule for it, and specify *how* it gets applied. Oh, the other disadvantage of using an automatic rewrite rule: it's impossible to distinguish between method and non-method invocations in the expansion. Might not be useful, but I tend to err on the side of expressiveness. :) &gt; Also, for the $self option, shouldn't you specify a fragment specifier, like normal macro arguements or would it only work with expressions Why bother? Insofar as I know, you can't have anything that *isn't* an expression as the subject of a method call. After all, in order to even *get* to this point, the parser *must* be able to parse everything prior to the macro invocation, which is going to be an expression.
there's a few things I miss from C++ in rust, but repurposing bit shift operators for file IO isn't one of them :) still it is interesting to compare how you can do things in both languages though.
true, i guess im to used to working in Scala, which has tons of implicit stuff everywhere, after that experience, i guess more explicitness and expressiveness is good
Some time ago, I made some experiments about Rust and Emscripten: http://www.reddit.com/r/rust_gamedev/comments/2n0x08/emscripten_experiments/ After some modifications in Cargo (that will hopefully be merged (they aren't yet)), I quickly wrote this `cargo-emscripten` executable that allows you to produce HTML files instead of binaries. For the moment this is just an experiment and is not really usable for three reasons: it is very difficult to compile because of all the breaking changes that rust is currently experiencing, emscripten doesn't use the same version of LLVM as Rust does for the moment, and using any function from libstd doesn't work. 
`cout.by_ref()` should be sufficient (I didn't check)
Unfortunately that's not very helpful, because 0.13.0 (more precisely, 0.13.0-nightly) is the version number of the nightly branch; the last "stable" release was 0.12.0. Listing a date/commit hash would be more specific.
I &lt;3 tomaka17 I saw you on this thread; It looks like 3.5 isn't too far away from being merged :) https://github.com/kripken/emscripten-fastcomp/issues/51
You can implement such traits with references, e.g. `impl&lt;'a&gt; Add&lt;&amp;'a Foo&gt; for &amp;'a Foo { ... }`. In this way it is actually widening (and not missing) the possible operands, as consuming references is a no-op. [Example](https://github.com/rust-lang/num/blob/ce93a10/src/bigint.rs#L190-L229).
In many cases a move/copy makes more sense and we don't have to be concerned about lifetime issues. For the `add` example, it's cheaper to copy simple numbers then it is to create a pointer.
The new Windows installer (http://www.rust-lang.org/install.html) now comes with Cargo [[link]]. [link]:https://www.reddit.com/r/rust/comments/2r92yw/psa_rust_installers_now_come_with_cargo_and_docs/
Forget what I said the latest cargo.toml must be broken, we are using a different one for crates.io since rust brakes so often… ~~Beside that, there must be something wrong with the data. `image` does not show up although it has crates that depend on it and it depends on some other crates.~~ ~~https://crates.io/crates/image/reverse_dependencies~~ ~~https://crates.io/crates/image~~ ~~https://crates.io/crates/num/reverse_dependencies~~ 
(`image` appears at the very bottom in the center of the smaller graph. `num` also appears at the bottom, about 2/3rds of the way across.)
Aha, I didn't realize traits could be implemented for `&amp;T`, thank you! I guess that means trait methods would lean towards `self` except when the desired semantics (i.e. calling a method multiple times) dictate otherwise.
I'm a fan of `kind` or a descriptive prefix (e.g. `list_style_type` in Servo).
I'm talking about naming. `type` is a reserved word, and cannot be used as a name for a variable or a field in a struct.
w0w, that was hidden :c *Edit: it installs Rust 0.9 though :(, anyway to update it?
I favor `try!(foo())` over `foo().try!()`, but `foo().try!().bar()` over `try!(foo()).bar()`. Because the name is `try!`, `try!(foo())` just feels more natural than `foo().try!()`. But the member function syntax makes `foo().try!().bar()` much more readable than the current `try!(foo()).bar()`.
tipe ? 
So, do I need rustupup.sh or should the old rustup work fine? ;P
Geany is love, geany is life
I always use `ty`. I think that's a pretty common abbreviation.
Do you then have to use references explicitly when adding? I.e. `&amp;a + &amp;b` ?
Yeah, `as` in Rust does behave like C casting: `printf("%i\n", (short) 99999);` I think this flavor of casting has it's uses; it's all part of being a systems programming language. Maybe your i32 value was storing four i8 values and you used a cast to extract the values. It would be nice to have people stop shooting themselves in the foot by being lazy.
Slippery slope fallacy. I think they're called context-sensitive keywords, or something to that effect. Just because you have _some_ context-sensitive keywords doesn't mean _all_ keywords must be context-sensitive. As another example, `x` was tossed up as a possibility for array-literal construction -- `['a' x 26]` -- albeit very briefly. (I'm not actually advocating for making `type` a context-sensitive keyword).
I just updated [rust-nightly-bin](https://aur.archlinux.org/packages/rust-nightly-bin), but it seemed to be working on its own. I've also deprecated cargo-nightly-bin, now that Cargo is included :)
I’d stick to the python convention. Just call it `type_`.
Also, OP, when I ran your code, I got this error message: Could not find argument '&lt;vcs&gt;' (from struct field 'arg_vcs'). Which is what tipped me off (because `vcs` is a flag, not an argument).
Thanks. I just wrote down what I did, so idk. Maybe a Postgres expert can explain things here :)
You should update rustup. Old rustup will likely end up reinstalling nightly cargo, which may not be the cargo packaged with rust.
I believe the cargo nightly build has failed for a few days.
This didn't work because overloaded calls weren't integrated with autoderef. But [the fix just landed](https://github.com/rust-lang/rust/pull/20443), and I can confirm both examples work on master. Your code should work on the next nightly.
Thank you for the fast answer. :) 
Geany is okay for writing Rust. Possibly even the best around at the moment.
It seems to me that [`rust-freqdist` depends on xxhash](https://crates.io/crates/xxhash/reverse_dependencies)?
One trouble: what if I have a crate that specifies int as 32bits, and another that specifies it as 64bits, how are they linked together?
Hmm. That might be a case to have it be an alias instead of a unique type. I think I'll change the opening post.
I also get the "no space left on device". I doubt this has anything to do with the computer you're running it from, since Cloud9 is, as the name implies, a cloud service. I'm guessing that the addition of the docs to the installer means it now takes up more than the disk quota on a free Cloud9 instance. Aside from that, I don't think I'm particularly keen on the idea of relying on a zippy connection to an external cloud service to be able to get any work done. If github or bitbucket go down, or my Internet connection dies for a while, I can at least still work on my code locally. I may just need to wait a bit to push it out. If I relied on Cloud9 and lost access to it, that's that. No way to get work done. Interesting idea, but the Internet is going to have to get a LOT more stable before I go that route.
It's not doing the method begin__journey or printing it works
If you put 4 spaces before each line, the code will be formatted correctly. let correct_name = io::stdin().read_line().ok().expect("Failed to read line"); if correct_name == "y".to_string() { println!("it works"); begin_journey(name.as_slice()); } else if correct_name == "n" { ask_name(); }
Start with basic debugging steps. The program isn't entering the condition, so the condition must be false. So `correct_name` must not be equal to `"y"` when you think it should be. So what *is* it equal to?
As singron mentioned, read_line captures everything from the line, including the newline from hitting enter. The solution to your problem is the trim() off the \n. This guide explains it well: http://doc.rust-lang.org/guide.html#guessing-game
it's funny how Graydon says right off the bat that he has no idea and just keeps making up different reasons every time, yet when he comes up with the fungus he manages to convince a lot of people that **that** is the one true origin of the name (just look at the comments in the thread you posted). 
Thanks for the review! &gt; I prefer 1 to 2, since it is closer to functions. It also enables funny stuff like `0i.vec!(1)`, not sure if that's good or bad :-) &gt; Note that the priority for macros post-1.0 is stabilisation Hmm, this implies that macros at 1.0 are unstable, and since 1.0 will not include unstable stuff, this means no println!() in 1.0, which is probably not what we want. What am I missing? &gt; So even if approved, this is unlikely to be added for some time. Ok.
Try [this](http://play.rust-lang.org/?code=fn%20main%28%29%20{%0A%20%20%20%20let%20words%20%3D%20vec![%22first%22.to_string%28%29%2C%20%22second%22.to_string%28%29%2C%20%22third%22.to_string%28%29]%3B%0A%20%20%20%20let%20tail%3A%20Vec%3C%26str%3E%20%3D%20words.tail%28%29%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20.iter%28%29%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20.map%28|x|%20x.as_slice%28%29%29%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20.collect%28%29%3B%0A%20%20%20%20match%20tail.as_slice%28%29%20{%0A%20%20%20%20%20%20%20%20[%22first%22%2C%20..]%20%3D%3E%20println!%28%22first%22%29%2C%0A%20%20%20%20%20%20%20%20[%22second%22%2C%20key]%20%3D%3E%20println!%28%22second%3A%20{}%22%2C%20key%29%2C%0A%20%20%20%20%20%20%20%20_%20%3D%3E%20println!%28%22help%22%29%0A%20%20%20%20}%0A%20%20%20%20%0A}%0A). Basically, use `map` to get a `Vec` of `&amp;str`s instead of `Strings`. I don't know if it's the best way, but it works. --- Also pasted below: fn main() { let words = vec!["first".to_string(), "second".to_string(), "third".to_string()]; let tail: Vec&lt;&amp;str&gt; = words.tail() .iter() .map(|x| x.as_slice()) .collect(); match tail.as_slice() { ["first", ..] =&gt; println!("first"), ["second", key] =&gt; println!("second: {}", key), _ =&gt; println!("help") } }
&gt; avoiding the match =&gt; panic! boilerplate everywhere It does not solve the fundamental issue, just wanted to point out that the `if let` syntax probably has less boilerplate for your use case.
This is possible today in Rust although it requires a little bit of boilerplate and setup. I am working on an experimental macro/compiler plugin that supports the translation of ideas like this into vanilla Rust. You can find a full example here: http://is.gd/YgO7Kl. At its essence encodings like this make it possible to write functions like these: // Accepts any Color fn openminded_function&lt;C: Color&gt;(c: C) {} // Eventually this should just be C: Color&lt;Variant=Blue&gt; fn closeminded_function&lt;C: Color&lt;Variant&gt;(c: C) where C::Variant: TypeEq&lt;Blue&gt; { panic!("Types are for compilers") } let color = Red; openminded_function(color); // This works! closeminded_function(color); // This doesn't! closeminded_function(Blue); // This does! I'm happy to answer any questions about how this works as well, as a note it requires both where clauses, associated types, and poor man's type equality (type equality encoded as a trait).
macro_rules! is stable, however, we want to create a better macro system (possibly, bare macro!). So macro_rules will be deprecated, but remain until 2.0 (or forever) alongside the new and shiny macro system. Syntax extensions will also be stabilised, which are not going to be turned on for 1.0.
&gt; 23 openssl-sys &gt; 8 openssl It is strange that `openssl-sys` is so much more popular than `openssl`; are a lot of people calling into the C bindings directly?
In this case, `time` is popular because it was previously in the stdlib and so is the known/default choice for people looking for any sort of time-related functionality (I personally don't know of anything else for any sort of general time handling). It is *not* popular because it is high-quality. The lack of quality and difficult of handling time properly was the reason for moving it out of the standard library for now. (Similarly with `rustc-serialize`.)
A language that has just the feature you wanted is OCaml, that calls it [polymorphic variants](http://caml.inria.fr/pub/docs/manual-ocaml-400/manual006.html#toc36). OCaml is a Frankenstein in this aspect, and also has regular variants (that work like Rust enums). Those two aren't compatible and most programmers tend to use normal variants, perhaps because they give more useful error messages.
https://csperkins.org/research/misc/2015-01-02-cargo-freebsd.html
This may not work for all cases, but I find it's a surprisingly pleasant workaround sometimes. struct FooData { ... } struct BarData { ... } enum Baz { Foo(FooData), Bar(BarData) }
Yes, this is exactly what I do, too.
I had to put it in the if statement. .as_slice().trim() . It works now!
No problem for me
It's just started working again for me.
Yup, back for me too. Weird.
How about an option to disallow `int` altogether? #![int(disallow)] // or "prevent"
Awesomeness!
Thanks. It's a good start, though 2 days after the publication the script is outdated already. Got to contact the author...
good idea, but i think they’ll mostly use a simple struct like `Rgba { r: u8, g: u8, b: u8 }` and won’t care about whitepoints in the 16-bit-per-channel L\*a\*b\* colorspace :D
I think RGB is the only color format where it makes sense to allow a `u8` representation, since it is the one that is usually mapped to screen pixels. Every other one I can think of (HSL/V, Lab, CMYK) is usually understood as float components.
true. and starting from f32, the lost bits by using a range like 0..1 don’t hurt too much. what do you think of the ranges, especially for the strange L\*a\*b*? use uniformly -1..1 and 0..1 or use the conventional ranges like 0-100 for L and -120..120 for a/b?
[Enum layout optimizations](https://github.com/rust-lang/rust/pull/19765) =&gt; sweet!
So the following attempt at collecting as a slice: let strings = [ "hello".to_string(), "there".to_string(), "steve".to_string(), ]; let slices: &amp;[&amp;str] = strings.iter() .map(|x| x.as_slice()) .collect(); Gives the error: the trait `core::iter::FromIterator&lt;&amp;str&gt;` is not implemented for the type `&amp;[&amp;str]` Which makes sense, looking at [the docs for `FromIterator`](http://doc.rust-lang.org/std/iter/trait.FromIterator.html). Thanks for the explanation Steve. It makes sense to opt in to a certain type of iteration, but I'd personally prefer some sane conventional defaults like an immutable `iter()` :P
`enum Color { Rgb(u8, u8, u8), Lab(f32, f32, f32), Xyz(f32, f32, f32), ...}`?
Hi everyone, I was hoping to contact all of you at the end of the year to see who had a solution and who was willing to review someone else's code. Unfortunately, my machine malfunctioned and I lost a lot of data, so I haven't been able to log into Reddit until today. Sorry about the delay. I'd like to press on with this, so if you have any code and/or want to review someone else's solution, please drop me a message. If you have code, don't forget the link. I'll try to distribute solutions across reviewers as fairly as possible. Also, I'd like to know how you liked the whole idea here. How do you see it working best, where could we find more participants, do you have ideas for suitable future projects, anything.
This works right now: struct Bar&lt;T&gt; where T: Foo { item: &lt;T as Foo&gt;::Item } The "short" notation (`T::Item`) will work in the (near) future. 
Cool, something I did actually made it into TWiR. Although, I wouldn't so much call it a new project. It was kind of more like an experiment/ad-hoc blog post on my part. I really should start a blog for this kind of tinkering with rust.
&gt; I can accept that this is a work in progress though Yeah, Niko is working on improving ergonomics right now. [This PR](https://github.com/rust-lang/rust/pull/20572) let's you use the short `T::Item` notation in most places like traits, impls, bounds, etc (see the tests in the PR). Just not in struct definitions (which is your case) or at least there isn't a test for it so it may or may not work after that lands.
&gt; why no degrees? Because every trigonometric function expects radians. It is non-obvious if degrees pop up in calculations. But to be sure I would look into other libraries what is common. &gt; and are you interested in an unified color library that image depends on? Certainly not now that Rust is so unstable. In general it would depend on how well it could be integrated into the existing model…
I personally thought it was super neat, I couldn't bear to not include it :)
the idea would be to unify the capabilities of the basic model of image and color.rs and extending it with color spaces (like in [colors.rs](https://github.com/retep998/colors-rs))
Amusing: just hours ago I updated Teepee’s header representation to use associated types instead of generics on the market types. Associated types are quite good now.
Of course, you would actually want to deal with `#[deriving(`; had I just replaced the word deriving everywhere I would have been left with some bad grammar in doc comments, for example.
Thanks for the encouragement. I'll have to look into a good platform for writing the blog posts.
Thanks! Glad it was interesting enough to be included in TWiR. My "big" project I'm working on is going slowly for me and I'm waiting on the alpha before I really start digging into it, so I've focused on smaller things and tinkering around with the language and macros to keep myself occupied while I wait for the alpha to be released.
Do you know if crates.io will eventually get mirrors?
I'm a big fan of GitHub Pages + Jekyll, as well as Ghost.
can you show me what a proper Cargo.toml would look like when trying to override a dependencies build file? Because I'm banging my head on a wall as i feel like cargo is just ignoring my config file.
This was the analogy I came to as well...also considering rust is a natural occurrence of leaving something outside and unattended; drawing conclusions from "archaic" and all the "shiny new" languages.
Good idea. I like `disable`.
static.rust-lang.org is served by CloudFront in US and Europe. It should have the full force of Amazon behind it.
I think it's a sensible choice. +1 for me.
-1 for me. I was happy with previous choice where usize was called uint.
Looks good to me.
Unfortunately, this is a situation where it is really hard to satisfy everyone. The core team members who were actively involved in the discussion (wycats and aturon) were initially in favour of retaining `uint`, but were convinced that switching might be a good idea in the course of the discussion. (*edit*: /u/stepancheg's comment doesn't deserve downvoting; it's not wrong or snarky or anything bad.)
I drop sdl2.dll in the project root directory (just above src/ and target/). Is this bad practice? 
Please, just let the bikeshedding end! :)
If I'm understanding correctly, if a type `T` has any non-zero or pointer fields, `sizeOf(Option&lt;T&gt;)` will always be equal to `sizeOf(T)`, with `None` represented by zeroing that field? So `Option` is now safer, more flexible AND more efficient than null pointers? Rad.
I am continually impressed by how the Rust community is able to conduct meaningful discussion and debate to end up with sensible solutions. The elegance and power of this language is a testament to the excellent community on which it is built.
To be exact, pointer types wrapped by `NonZero` will do. (A normal pointer is nullable, which we cannot easily change.)
OFF: do you plan to resubmit the XXHash PR?
Oh, does that mean a lot of user-defined types that are simply tuples of raw pointers will not have optimised `Option` implementations ? Or is raw pointer use rare? Forgive my ignorance, I am an outsider.
The point is, generally speaking, to be able to provide a safe interface for 99% of your code that *doesn't need* unsafe operations to function. That way, if something breaks, you know that it's in all likelyhood the 1% that is unsafe. If you use unsafe blocks for code that could (and should) be safe, you're really only hurting yourself unnecessarily.
C++ can be used safely if you're really careful, but the compiler can't help you with that. It's unsafe by default. Rust provides safety guarantees by default, enforced by the compiler, and only relaxes that in blocks that are specifically marked unsafe.
Raw pointer use is very rare in user code, and even in a most (non-collection) library code. For example, there's not a single use of raw pointers in Cargo code, AFAIK (EDIT: There totally is, my bad. Not in a data structure though).
`Option&lt;T&gt;` was always this way, this brings that to things that are also not `Option&lt;T&gt;`.
Cargo's only unsafe is in FFI bindings to OpenSSL.
`repeat(self).take(n).collect()`
&gt; Technically we could just accept IntoCow and get the desired behaviour right now, but this would technically require a runtime branch in the entry code to determine if we got a ref or a value, and that's lame. I have a feeling that single branch will be much smaller than the cost of hashing, doing look-ups etc.
Right! Just trying to be clear about where our time is spent. I'm a big fan of Amdahl's Law.
I don't think the original proposal was conflating register size and pointer size, just using "machine size" as a synonym for "pointer size". AFAICT, from the discussion-starting post there's no indication of mixing them up other than a slightly unfortunate choice of terminology.
How long does a Rust bootstrap take on your computer? (It can vary quite a lot, so it would be good to have something to compare the 4.5 minutes against.)
I thought that previously `Option&lt;T&gt;` and similar enums only did that optimization if `T` *was* a non-nullable pointer, and not if it *contained* one.
The current inapplicability of anything ccache-like to Rust is a weakness. We'll have to have a completely custom solution. Where nim can use ccache to reuse parts of the build across stages we can not (either in general or across bootstrap stages). The equivalent to us would be caching and reusing LLVM IR, which can be done but is harder because our compilation model is so different from C (nim seemingly has the luxury of literally being able to use ccache). Something like this does need to be done eventually to support compilation on a large scale though.
&gt; (as long as it's performant) It should be, since `repeat` should give its size hint with a high lower bound, and `take` should then give a tight upper bound, so the `vec` doesn't need to allocate more than once. And beyond that, with proper inlining, it's pretty much just a for loop filling in the vector.
However 100% of the input to LLVM comes from rustc :-)
The post was intended to lay out the options for renaming `int` and `uint` in the language. I did not see any confusion among people who actually participated in the discussion as to the meaning of "register-sized" or "pointer-sized". I don't see the practical relevance of this discussion, honestly.
And it would be statically guaranteed to pick one branch for most any type you pass in. However the *real* problem (which aturon kindly reminded me of) is that coherence currently prevents a blanket IntoCow impl.
IIRC, the gates left to remove are globs, macro_rules, aaaand... I'm forgetting one. But they'll all be in the right place by Friday. The lint hasn't landed yet, IIRC.
I wonder if it inlines well enough to see that the bounds check within `take` is the same as the one in `push`. To the playpen! EDIT: Eh, I'm not great at reading LLVM IR, but [here](http://play.rust-lang.org/?code=use%20std%3A%3Aiter%3B%0A%0A%23[inline%28never%29]%0Afn%20get_vec%28size%3A%20uint%2C%20elem%3A%20i32%29%20-%3E%20Vec%3Ci32%3E%20{%0A%20%20%20%20iter%3A%3Arepeat%28elem%29.take%28size%29.collect%28%29%0A}%0A%0Afn%20main%28%29%20{%0A%20%20%20%20let%20_%20%3D%20get_vec%283%2C%204%29%3B%0A}) at -O2 I see a call to `alloc_or_realloc`, which wouldn't be there if it properly elided the second bounds check and did dead code elimination.
&gt; We need an unsafe TrustedLen trait, and a way to static dispatch on it to reclaim the perf. Is it possible to specialize like that with the current type system?
+1. The default integer fallback (along with its implicit type conversion) is a major ergonomic win when coding in Rust. Often you just want to use a (reasonably-small) integer, when the size of the integer is not your major concern. Finally, Rust compiler takes care of that for you... I also agree with other commenters that the outcome of the discussion is constructive. It demonstrates the maturity of the Rust core team and the Rust community when confronting thorny debates. This is not easy at all. Congratulations! 
That was approved a long time ago, the patch just took a while. Fallback is very, very important to me.
&gt; Is there a master list somewhere, of what feature gates will disappear by/in 1.0? There is a list under the "feature stabilization" heading of [#19260](https://github.com/rust-lang/rust/issues/19260). &gt; unsafe_destructor This isn't fixed yet, but [#20539](https://github.com/rust-lang/rust/pull/20539) makes good progress. &gt; Similarly, is there a cargo flag for 'lint for use of unstable api'? I see rustc has --warn unstable, but I'm just been trawling through cargo issues about passing flags to rustc and I can't see any obvious way of doing it? These will start to be activated by default in rustc itself with the alpha and betas. They will be unavoidable hard errors in the 1.0 stable release(making that functionality only available in the nightly branch), but the softer warnings give us a chance to migrate, and also a chance to provide feedback about standard library APIs that people feel are particularly important to be in 1.0 stable.
Not off the top of my head. There's a reason we don't have it yet. The API is unclear.
Right now the #!feature line for an app I'm working on looks like this: `#![feature(link_args, globs, phase, unsafe_destructor, macro_rules, default_type_params, associated_types, old_orphan_check)]` It looks like `globs`, `macro_rules`, `default_type_params`, and `associated_types` should be stabilized for 1.0, and I think parts of `unsafe_destructor` will be as well. I need `old_orphan_check` because of a really weird error I was seeing with associated types ("type parameter `__S` must also appear as a type parameter of some type defined within this crate"). Several other people are seeing similar errors. Hopefully this problem will be fixed before 1.0. I use `phase` for the [rest_easy](https://github.com/cmr/rest_easy) crate, which isn't really needed but tells me when rustc is done with everything that could cause compilation to fail. I use `link_args` to enable a flag that prevents my app from opening a command line on windows, so it only displays the GUI window that I open. (The exact LOC that I use for this is `#[cfg(windows)] #[link_args = "-Wl,--subsystem,windows"] extern {}`) It looks like the situation isn't as bad as I thought. I'd really like to have `link_args` or something similar (or else every Windows application with a GUI must also open a useless command line in the background), but all the others will be stabilized by 1.0 or aren't really needed for this app.
It's kind of silly but it's surprisingly useful when compilation takes a long time (especially with optimizations).
This is wonderful. Fantastic. Thank you for writing!
Basically. I did suggest leaving it in as unstable during alpha, but I guess that wasn't done.
Are you comparing two different executables? What is the code we are comparing? If we are comparing compiling `nim` vs `rustc` then couldn't it just be that `rustc` being a compiler that works directly to llvm instead of C has a few more dependencies and therefore is bigger and takes longer to compile? We need better metrics. Lets make some basic programs in both Rust and Nim, make them do equivalent actions, with equivalent level of programer (that is both code-sets should balance efficiency and readability on a similar level) and llvm optimizations. Then get the metrics of compiling it with the equivalent of `make clean; make` a few times. Then we could measure the size of the executable and speed. Finally some "real world" code is needed. Two projects should be important: a thin wrapper over a C library, most of the weight would come from C, this would allow us to see how much is lost over translation and how much weight is added, I'd expect Nim (becoming raw C) would do better. The second is a large non-trivial project that would be implemented on both languages in the best way possible (a JSON library or something of the sort), functionality should be mostly the same for both libraries, it might be that Rust defeats Nim in this area. Finally with all that data we would be able to reason about the speed issues of the rust compiler, and be able to optimize and fix what we could.
When [#20032](https://github.com/rust-lang/rust/pull/20032) lands, you will be able to use rest_easy via command line, instead of using `phase` feature gate.
Pretty much every language has some sort of escape hatch that lets you poke holes in the type system and do potentially shady things if you really need to, even Haskell or theorem proving languages like Coq or Idris. But just because you _can_ violate safety doesn't mean that it's a common occurrence. The trick is to provide enough tools to the programmer to where it's rare that they need to resort to something the compiler is unable to determine is safe, and Rust accomplishes that pretty well I think. Rust is also somewhat unique in that there are certain low-level things you can do that require some sort of unsafe operations but that aren't even possible in other languages without either modifying the language runtime with new primitives or using an FFI to a C library. Even in those situations, "unsafe in Rust" is usually quite a bit different to "unsafe in C" as far as pitfalls are concerned.
An `unsafe fn` on the trait isn't quite right, unfortunately. It does force anyone using the size hint to write `unsafe { ... }` to be able to call it... but it doesn't require the implementers of the trait to use `unsafe`. So it's still got the burden of proof in not-the-most-useful place: callers of `size_hint_kind`/an `unsafe` `size_hint` have to prove that the size hint is correct if they wish to rely on it, but there's literally no way they can do this. I came up with a similar design to your alternative a few days ago: /// A struct that is `unsafe` to construct, but safe to use pub struct TrustMe&lt;T&gt; { data: T } impl&lt;T&gt; TrustMe&lt;T&gt; { pub unsafe fn new(x: T) -&gt; TrustMe&lt;T&gt; { ... } pub fn into_inner(self) -&gt; T } impl Deref for TrustMe impl DerefMut for TrustMe // ... // users are forced to use `unsafe` to acknowledge that they should be doing correct things fn size_hint(&amp;self) -&gt; TrustMe&lt;(uint, Option&lt;uint&gt;)&gt; But aturon pointed out that as soon as you obtain a `TrustMe&lt;...&gt;` (or `SizeHint`) you can return it, even if it's completely unrelated to the `self` value. E.g. `fn size_hint(&amp;self) -&gt; ... { [1u8].iter().size_hint() }`. A combination of the two approaches, with `unsafe fn size_hint(&amp;self) -&gt; TrustMe&lt;(uint, Option&lt;uint&gt;)&gt;` probably gives more useful semantics (can't obtain a size hint from a different iterator without `unsafe`, can't construct something to return without `unsafe`); but it's a little ridiculous. :( This sort of functionality is a little similar to the `unsafe trait` system that drives `Send` and `Sync`, just on a method level, so there was some suggestion that we could try to use something like that, but aturon tells me he and nmatsakis weren't sure the extra complexity is worth it.
What I've done in hyper is make traits like IntoUrl, which is implemented for &amp;str, String, and Url itself. That way, you don't have to take a Url, serialize, and then let me parse a new Url. 
While fallback is a cool feature, it was in my opinion far less important than int renaming because it could be a backward compatible change. If int/uint were there in 1.0, we would have had to deal with it forever. 
My face right now looks like: `:^T`. &lt;/sarcasm&gt; Seriously, I should mention that a *prefix* `^` is a pointer type syntax in Pascal (while a postfix `^` is used to *dereference* the preceding expression). We are already using a different order from C++ (while using the same punctuation), so I think it is enough even for the purpose of bikeshedding.
Thanks for this feedback! This certainly seems like useful data for making the decision about which ownership model to prefer for now. (My hope, like @Gankro is that we can eventually move to an API that supports passing both owned and borrowed data.)
I imagine that means that while everyone was arguing you secretly snuck in a dastardly patch to rustc without anyone knowing.
How about just using plain english? 
&gt; We are already using a different order from C++ (while using the same punctuation), sure it works in isolation, but if trying to retrofit rust pointers to c++ or c++ references to rust - 'prefix or postfix having a different meaning' would be a really confusing way to go about it especially if you're going to move back &amp; forth between C++ and rust syntax) The next least bad option is to add ^ as a C++ reference in rust. you might ask 'why would anyone want to do this' the world has orders of magnitude more C++ programmers and working, LOC in use, and established tools ; with features like concepts on the way I speculate the languages can exist on a continuum. add &amp; remove features to move to some point between them, instead of facing a total 'bin all your code &amp; rewrite' (known to cause death of an organisation). you could have a dialect of Rust that you can simply transpile your C++ to. Or a dialect of C++ that you could transpile your Rust to. C++ programmers wouldn't have to get so defensive about the idea of a replacement language, you'd just be giving them more tools in C++..
No, generally you declare a default-size integer with: let x:i32 = 42; 
Hm, I'm not sure those are the most useful/relevant numbers; seeing the time it takes to bootstrap just rustc (`make clean &amp;&amp; make -j2`) is more pertinent since that's what doing repeated build will see (one only has to recompile LLVM after it is updated, and this usually happens quite rarely). (That said, it would be nice for us to support using the system LLVM more easily.)
Thanks :-) that is great. Would be nice if the docs had a separate download for people who install buy building source or using homebrew
The ability to log in from most working forms should be there. Logging in to twitter works, not yet sure what other sites. Reddit should be easy enough to add (we don't yet support `&lt;button&gt;` elements for form submission but someone is working on it)
I’ve found myself and others implementing all these methods in too many places, so here is a crate to take care of it. I’ve also included the unchecked downcasting methods I proposed in [RFC 555](https://github.com/rust-lang/rfcs/pull/555) for the fun of it. Question for the Rust developers: is it planned that we’ll ever be able to do `Box&lt;Person + Any&gt;`? Or some way in which methods of an `impl Any` could be accessed from a `Person`? If we could do either of these, that would render this sort of thing rather less necessary. An example of the rather favourable diff this can make on another library: https://github.com/teepee/teepee/commit/3030b7223d0b1ce125b368b10b9c43313d286975
This thread has been linked to from elsewhere on reddit. - [/r/nim] [Nim&amp;#x27;s compile times • /r/rust](http://np.reddit.com/r/nim/comments/2ricy0/nims_compile_times_rrust/) *^If ^you ^follow ^any ^of ^the ^above ^links, ^respect ^the ^rules ^of ^reddit ^and ^don't ^vote ^or ^comment. ^Questions? ^Abuse? [^Message ^me ^here.](http://www.reddit.com/message/compose?to=%2Fr%2Fmeta_bot_mailbag)* 
I fixed a couple of bugs and updated the script. 
It's quite the opposite. There was no more default integer since long time now, but since fallback is being restored to i32, you will be able to declare your default integer : let x = 3; //if the type can't be inferred it will be i32 `int` was not anymore the default integer since long time now. It is now renamed to `isize` so it won't by mistaken for the default type as it seem you did. `isize` is a integer large enough to hold the value of a pointer. It's was not a good default integer since the size may vary, leading to inconstant behavior and performance. 
I was waiting till I had some more lints to share this, but whatever :) If you have any more ideas, feel free to file some issues!
I think I understand what you mean even if it's a bit sweded… :-)
&gt; but it doesn't require the implementers of the trait to use unsafe. I would argue that implementing an `unsafe fn` isn't that much different from `unsafe impl`ing a trait :) &gt; But aturon pointed out that as soon as you obtain a TrustMe&lt;...&gt; (or SizeHint) you can return it, even if it's completely unrelated to the self value. E.g. fn size_hint(&amp;self) -&gt; ... { [1u8].iter().size_hint() }. Cool hack :) Maybe we could prevent this by using the iterator itself as a phantom type on `TrustMe`/`SizeHint`? trait Iterator { fn size_hint(&amp;self) -&gt; SizeHint&lt;Self&gt;; }
The latest change to use references has broke all of my usages of `HashMap::entry`. Now work on my project is stalled until this is fixed. :( See https://github.com/rust-lang/rust/issues/20625
Aside from a few unrelated changes, [this commit to `rust-scan-wavefront`](https://github.com/DanielKeep/rust-scan-wavefront/commit/8579b13d30dcfad3837c082f0d5815466f4fdd3d) shows how I dealt with it. Specifically, you might need the `#![feature(old_orphan_check)]` line which is apparently because `rust-serialize` depends on associated types in such a way that doesn't actually work properly yet. I only reached this point after Huon told me to try that, which was after finding that none of the instructions provided by the compiler, in the breaking change log, and in the `rust-serialize` repository itself actually worked. Well, *anymore*, at any rate. Rust usually deals well with deprecations, but this *particular* one, at this time, feels a bit like an exploded whale carcass. **Edit**: In fairness, there *was* actually a note in the error message about `old_orphan_check` that I initially missed, in part because I couldn't work out what it was talking about.
Not until everyone's considered iminno and uminno. They're clearly the best option.
Higher rank lifetimes, mostly. Also, the compiler wasn't set up to autogenerate implementations of traits that actually had methods.
Thanks, but now I've got the following: src/main.rs:6:27: 6:41 error: wrong number of type arguments: expected 1, found 0 src/main.rs:6 #[derive( RustcEncodable, RustcDecodable)] ^~~~~~~~~~~~~~ note: in expansion of #[derive] With the following code: #![feature(globs)] #![feature(old_orphan_check)] extern crate "rustc-serialize" as rustc_serialize; #[derive( RustcEncodable, RustcDecodable)] struct Oiu { foo: u32, bar: String, } fn main(){ let _oiu = Oiu { foo: 32, bar: "asdf".to_string(),}; } Re: this update, yeah, they kinda messed up on this one. Also RustcDecodable and RustcEncodable is just so damn ugly looking to me... :(
That's an amusing plugin :) You could do something similar with `rustc -Z time-passes | grep ...`
Would love to give it a spin, but broken with latest nightly.
Looks like the error message should be on Contact&lt;C&gt; (it doesn't use C), not the Iterator.
Sorry again for all the trouble!
Afaik its a known bug
I agree. Fortunately there are plans to make unboxed closures easier to use in the future, e.g. fn foo(cb: impl Fn(A) -&gt; B) fn foo() -&gt; impl Fn(A) -&gt; B Many other traits besides `Fn*` could benefit from this sugar.
One instance in which `Vec&lt;Box&lt;T&gt;&gt;` _isn't_ redundant is when you have a trait object as your `T`. But otherwise, yes, it's redundant.
Can't wait till this isn't a thing anymore!
We're talking about the other way around. Vec of pointers is good for unsized or expensive-to-move types. Pointer to Vec is...pointless B-).
LOL. I wrote my first ever set of macros just a few nights ago (new years day to be precise.) I've never really had a use for macros before; but they came in handy while working on a gameboy emulator. The DMG CPU has a lot of instructions which are duplicates operating on different registers, etc. The `macro_rules! foo()` -&gt; `macro_rules! foo {}` is going to make me rewrite over 30 macros. I should've waited a few days lol. --- Time to crank out a regex substitution. Thanks for the guide `^^,`
You could, but not with cargo.
Will that work with Cargo?
composable and flexible are nice properties of those methods but that's not a property desired when looking for particular functionality. I know it can be done "easily" -- I can add my own extension trait to Entry too. But the question is not how to do it, just if you have considered adding something like .setdefault() to the libstd API?
Ah okay, thanks, that makes sense now. It was far, far too late when I tried to read the OP... 
&gt; this compiles I could've sworn that it wouldn't. I guess I should have tried before commenting :p &gt; Unfortunately that would make Iterator non-object safe. Yeah, I'm officially out of ideas now :(
&gt; Yeah, I'm officially out of ideas now :( Yeah, it's a surprisingly hard problem to solve "properly". :(
I hope the `fn(f:|i32|)` syntax makes a comeback once the dust has settled from removing boxed closures.
The idea with Rust is that the compiler, through the typesystem, can guarantee you certain things. If you get a string the compiler makes sure that all the characters are valid UTF-8. Rust can do all sorts of guarantees with this, to the point that it can even know exactly after which line of code it can delete a piece of memory (with lifetimes). The problem is that it's hard to always have the compiler understand that something works, it's not clear. The compiler can't promise you that it can work, it just isn't smart enough to, so by default it won't allow you to it at all. When you declare some code unsafe you are telling the compiler "I promise this works like I am saying and it won't break things" and the compiler believes you. Whenever programmers see a piece of unsafe code they understand that the compiler is not the one that is promising that things are working like they should, the programmer is. This means that if you make a program that uses no unsafe code whatsoever, any unexpected error is probably more a compiler error than a programmer error. Of course it's almost impossible to make a useful programmer without using this unsafe code, so in practice it means that if things break down in unexpected ways, it probably means that one of the unsafe blocks has a bug (so it's not keeping it's promises). What happens when an unsafe block breaks its promises? Well it's considered undefined and can have anything happen, from releasing private information in your computer to attackers, to bringing your whole machine down (though the OS should prevent it, the thing is that the language allows for the program to burn down your laundry). So now unto C++. C++ is a much older language, as in decades older. The mathematics and hardware hadn't matured to the point that compiling like this made sense. So C++ just trusts you know what you are doing. The problem with this is that if your program crashes you must find where it happened. Moreover because it might result from undefined behavior the error might appear to be somewhere else than where it occurred. You can't know with certainty where promises are being broken, since any piece of code depends on the good word of the humans, who are extremely flexible and smart, but fallible.
It will likely be a thing for quite a while with compiler plugins; at least, there's no guarantees even post 1.0. (Although the general language/dependency breakage will definitely be reduced.)
&gt; Rust serializes the post-typechecked AST so that it does not have to typecheck again after expansion Interesting, where can I read more about that? Scala developers are experimenting with using serialized AST (mostly for binary compatibility but there's a bunch of reasons why it'd be useful): https://groups.google.com/d/msg/scala-internals/hshvEUF3JUk/mMu9z2Eg7qkJ
That's not just sugar though, because anonymized return types allow for things that we can't even express today.
This is weird (allocation may be needed for lookup). AFAIU, it is done to fix issue #20625. Wouldn't it be possible for enum Entry to have two lifetime parameters? One to reference the map, and another to reference the key. In that case, result of insert: Entry::Vacant(v) =&gt; v.insert("hello".to_string()) pub struct VacantEntry&lt;'k, 'm, Q : ?Sized + 'k, K : 'm, V : 'm&gt; { key: &amp;'k Q, stack: stack::SearchStack&lt;'m, K, V, node::handle::Edge, node::handle::Leaf&gt;, } impl&lt;'k, 'm, Q: ?Sized + 'k + ToOwned&lt;K&gt;, K : 'v, V : 'v&gt; VacantEntry&lt;'k, 'm, Q, K, V&gt; { pub fn insert(value: V) -&gt; &amp;'m mut V { ... } } won't need the key.
As you say, it's pretty hard to find a balance between brevity, understandability and awesomeness. I'm sure that tweaks (or major rewrites) that are improvements will be readily accepted.
Isn't it possible for `entry` signature to be transformed into something like: fn entry&lt;'a, Q: ?Sized&gt;(&amp;'a mut self, key: Q) -&gt; Entry&lt;'a, Q, K, V&gt; where Q: Eq + Hash&lt;S&gt; + IntoOwned&lt;K&gt; where IntoOwned is trait IntoOwned&lt;Owned&gt; { fn into_owned(self) -&gt; Owned; } `IntoOwned&lt;T&gt;` can be implemented trivially for any `T`, and `IntoOwned&lt;String&gt;` can be implemented for `&amp;str` as `to_string()`.
The header representation scheme uses Any internally, though it's not actually Any but Header which extends Any, as it needs the ability to format the value. That's what this crate is all about.
How do you ensure type safety? Is it checked run time? Can't you use a Trait instead of Any?
As written this would provide no way to compare a Q to a K, without just calling IntoOwned.
It needed a rust upgrade. Fixed now!
Fixed, thanks.
It's just redundant (double allocation); and I've seen it often (used to do it myself) since Box has the connotation of "ownership" while `Vec` doesn't have such a strong connotation (with newbies). So some might use `Box` everywhere that they want ownership.
I would be very happy if Mozilla did something about this. At least contact Google at an organizational level to voice this concern, but part of me really want some nonprofit to take the pain of bringing this practice to courts.
Note: IANAL, and I'm not a Mozilla employee, so I can't help there :)
Mozilla legal is aware of the situation.
&gt; It's probably because bors leaves "fast forwarding to master" comments. Rust didn't get blocked. I suspect it's because this entity has a movie with cargo in the name (ctrl-F "cargo" to see a variety of other taken-down URLs with cargo in the name). (I can't explain the help.github takedown.)
Yay, thanks!
and this is why the DMCA was a really bad idea. this looks like an accident, but what if someone is malicious and uses it as a tool to silence people they don't like? what a mess this all is.
Maybe you can find something on [rosettacode.org](http://rosettacode.org)
That's Crowdsourcing at work. Verifying yourself that something is indeed infringing your intellectual property is costly. Making such automatic requests seems to be cheap. Let the community do the work.
Wait, does that map.entry(key).get().unwrap_or_else(|e| e.insert(0)) thing work right now?
https://github.com/sampsyo/beets/issues/546 (The company is called Wicked Pictures) They probably did a wildcard DMCA for everything that contains their favorite keywords. Is that even legal?
I wouldn't call it a totally bad idea. Here in India the government just directly blocks sites based on copyright infringement. Pastebin and github were recently blocked for some silly reasons. (At the DNS level, so no biggie). I *wish* we had DMCA or something like it here. There are many ways the DMCA can improve though.
When submitting a DMCA takedown request you have to submit a notice where under penalty of perjury you declare: 1. the work whose copyright being violated 2. that the work's copyright is owned by you or someone you are representing. 3. the infringing material (here, a list of URLs) 4. your contact information 5. you have a good faith belief "that use of the material in the manner complained of is not authorized by the copyright owner, its agent, or the law". AFAIK, a lawsuit for perjury for false takedown notices have not been tested in court. (5) seems like the only possible part of the notice that could be contested, but the company's lawyers might try to weasel out by saying that they did not authorize the use of their work on those pages regardless of whether the pages actually infringe. I would love to see a case where this was tested, as at the moment there seems to be no penalty at all for mass takedown notices like this, and often the targeted party doesn't even know that links to their site have been removed from a third party site like google.
Well the requirement for `HashMap::entry()` is that `Q` should implement `ToOwned&lt;K&gt;`. Anything that implements `Clone` has a default impl for `ToOwned`, so either requirement works. `ToOwned` is *maybe* a little more lenient? As then a user could pass an `&amp;str` if their cache is keyed by `String`, or a custom type that works similarly. 
I got it working simply by copying the SDL2.dll from the MinGW distribution of SDL2 into my Rust install: &lt;Rust path&gt;/bin/SDL2.dll Since the Rust installer automatically adds this to your PATH and `rustc` includes it in the linker search directories by default, it should be found without any extra hassle. And fortunately, it appears that re-running the installer to update Rust does not erase the DLL, so you should only have to do this once. However, you will need to include SDL2.dll with your app's exe if you install it on other people's machines. That's probably a given.
&gt; Apparently such abuse of DMCA is something you can sue for but I doubt we want to get into that. Unfortunately, it's not. :(
I think aturon and pcwalton have it about right. The design itself took a while to get nailed down -- the basic ideas were clear, but there are lots of details, and we wanted to think through how the inference could work and so forth to make the user experience as smooth as possible (those designs are still not fully realized, as Aaron said). But beyond that there are some decidedly non-trivial aspects to the unboxed closure design. Higher-ranked trait bounds (and hence object types) is probably the most obvious part; it took a while to work through just what that meant but also how best to implement it (I think the current approach is scaling quite well, but it's the second try), but it also stretches the existing closure code in various ways. I think a big factor has also just been the number of things going on, meaning that no one feature is able to receive anyone's full attention.
If you extend the trait by `as_ref(&amp;self) -&gt; &amp;Borrowed`, then the function could be called with both owned and non-owned keys.
Ah thank you. Git pulled, and was able to build via cargo. Having trouble figuring out how to run the examples though. Sorry for the newb question here, but any tips on getting that going? Not familiar if there's a command with cargo to run them. rustc examples/box_vec.rs examples/box_vec.rs:4:1: 4:21 error: can't find crate for `clippy` examples/box_vec.rs:4 extern crate clippy; ^~~~~~~~~~~~~~~~~~~~ error: aborting due to previous error There an option i need to specify to include the dylib?
You need to link it. `-L target/` should be enough if you've built it. Alternatively, `cargo run --example box_vec`, *however* it will still compile all examples so you will get all the lints at once :p
So it looks like they have a whole bunch of github links*. Looking over the github links it almost looks like they are doing really stupid keyword searches for stuff like cargo, pushover, tasty, lipstick. I really hope that's not the case. You'd think that they would be blocking ~~half the net~~ half of the non-porn net if that were the case. OTOH it looks like all the other domains on that list(or almost all) other then github are porn and/or (likely majority illegal) filesharing sites. Even if they did do extremly simple keyword searches including common words they are likely to hit illegally shared porn(or illegally shared non-porn movies) in 95+% of cases(maybe taking down the odd linux torrent) even if it's not their stuff. I'm guessing someone marked github as a porn/filesharing site for some reason either accidentally or maybe to encourage DMCA hatred. *github links https://github.com/sps/pushover4j https://github.com/sensu/sensu-community-plugins/blob/master/handlers/notification/pushover.rb https://github.com/schneems/wicked/wiki/Testing-Wicked-with-RSpec https://github.com/schneems/wicked https://github.com/satyr/coco https://github.com/satyr https://github.com/sampsyo/beets/issues/546 https://github.com/rust-lang/cargo https://github.com/rniemeyer/knockout-delegatedEvents https://github.com/rniemeyer/knockout-amd-helpers https://github.com/qbit/node-pushover https://github.com/openSUSE/wicked/issues/432 https://github.com/openSUSE/wicked https://github.com/Nuku/Flexible-Survival/blob/master/Stripes/Candy https://github.com/Netflix/Lipstick https://github.com/nemomobile/lipstick https://github.com/mrmrs/colors https://github.com/mirage https://github.com/mileszs/wicked_pdf/issues/78 https://github.com/LubosD/darling https://github.com/laprice/pushover https://github.com/kryap/php-pushover https://github.com/krisselden/broccoli-sane-watcher https://github.com/Knockout-Contrib/Knockout-Validation https://github.com/knockout/knockout https://github.com/knockout https://github.com/kirang20/wgxp-java-rosa https://github.com/jreese/znc-push/blob/master/doc/pushover.md https://github.com/jnwatts/pushover.sh https://github.com/jfinkels/flask-restless/ https://github.com/jasonlewis/resource-watcher https://github.com/huxi/lilith https://github.com/hannorein/rebound https://github.com/gregghz/Watcher https://github.com/feuerbach/tasty https://github.com/facebook/rebound-js https://github.com/facebook/rebound https://github.com/erniebrodeur/pushover https://github.com/entertailion/Fling/blob/master/README.md https://github.com/enkydu/raspi_runner https://github.com/dyaa/Laravel-pushover https://github.com/danesparza/Pushover.NET https://github.com/crazed https://github.com/callmenick/css-loaders-spinners-2/tree/master/js https://github.com/callmenick/css-loaders-spinners-2 https://github.com/allure-framework/allure-core https://github.com/abrt/satyr 
*If*?
If working in l\*a\*b*, you should be using at least 16 bit integers because you will get so much posterization. For one, the color space is larger than what is even visible. Furthermore, the shadows have further reduced color saturation and you will have even more precision loss in practice. I would use absolute values less than one for anything float.
Man, there are a few big names in there. Facebook and OpenSUSE, as well as big community projects like Knockout.
I like being able to immediately see where every name comes from just by scrolling up to the top of the file.
Thank you for considering all the options. isize is not my favorite, but at least it is better (IMO) than other names such as imem or iptr.
Thanks everyone! I figured "trying to do everything at once" must've been a part of it.
It's being ripped out and put back in again, in a slightly different syntax. Projected syntax is `&amp;mut arr[0..2]` but it's not finished just yet. So yes, it might be broken just right now.
Just FYI the `Q: ToOwned&lt;K&gt;` type parameter has been reverted by [PR #20653](https://github.com/rust-lang/rust/pull/20653) which will be included in the next nightly. It is impossible to build collections like TypeMap on AnyMap on top of a by-reference API because they generate the key on behalf of the user, so the reference it needs could never live past the call to `#entry()`. Also there was [a bug](https://github.com/rust-lang/rust/issues/20625) in that the key needed to live as long as the map itself. See some discussion [here](http://www.reddit.com/r/rust/comments/2rgtoi/what_should_the_entry_api_accept_while_we_wait/) --- 
Here's a hack for now: #![feature(slicing_syntax)] fn main() { let mut arr: &amp;mut [u8] = &amp;mut [1u8,2,3,4,5,6]; let foo: &amp;mut [u8] = *(&amp;mut arr[0..2]); foo[0] = 3; } (Explanation: `&amp;mut arr[0..2]` seems to make a `&amp;mut &amp;mut [u8]` for some reason, so we can just dereference that to get the actual slice.)
I meant between -1 and 1 for lab.
AFAIK that is expected behavior, unfortunately. Simply `box foo` won't coerce to a trait object, and different closures have different types, so you cast manually to the trait object.
Great, yes.
I heard Packt Publishing is preparing an introductore Rust book: 'Essential Rust'
This is the problem of unilateral requests with no consequences; the DMCA law could have mandated compensation for successfully countered notices in order to re-establish balance for example, it would probably chill those guys.
As well as Netflix. I wonder if any of these big names would be interested in cracking down on that company so that they refine their process instead of abusing the DMCA process.
Actually, I wonder if it would be possible to randomly select one fragment among N.
That's a shame. I liked being able to do lookup in a `HashMap&lt;String, _&gt;` with `&amp;str`.
I would want it to have pixel operations that can nicely run inside loops and be automatically vectorized. I'd also want a variety of algorithms with different tradeoffs between accuracy and speed, for different parts of the program which run in different precisions, sometimes converting between the precisions. This is what I actually am writing at the moment for my program so it's not an exaggeration. I'm not sure if it's just that my program is weird for switching pixel formats midway through, but that's what I need. Sometimes I want quality (final output) and sometimes I want speed (thumbnails). Basically it would need so many combinations that it would become ridiculous to expect each to be at the maximum performance for each speed/accuracy tradeoff. I have no need for color inversion without control over what exactly the inversion entails, and definitely not in lab where you will end up with so many imaginary colors from trying to do so.
how to ensure vectorization? isn’t this simply a matter of locality (i.e. something that rust already does well). if my image is backed by an array of RGB objects, shouldn’t any operation looping over it be vectorized? the inversion is something i don’t really need. it was just in the library already. ignore it :D does you application have a public repo i could look for what representations you use for what?
I'd be happy to help review it. To be blunt, most Packt books I've seen needed much better review before publication.
I would love to help review. How would you like to coordinate? As well I'm curious how you're going to handle enhancements to the language post-1.0. Rust won't be standing still and I expect there will be several areas that see (backwards-compatible) ergonomic improvement soon after the release. Do you intend to publish errata with each new six-week release?
You can try running that in gdb to get a stack trace and then open an issue. However, on the latest nightly this compiles and runs for me.
I would really appreciate complex, well-explained lifetime examples (and common borrow check error message scenarios). Explanations that show lifetime inference vs explicit equivalents would be super-useful.
Some people... not have words.
Found a precedent [here](https://news.ycombinator.com/item?id=8848907). &gt; In a case brought by EFF in response to an earlier bogus takedown from Diebold, a federal judge in California held that Diebold could be "liable for damages" -- even though Diebold had by that point withdrawn the takedown request and promised not to send another.
Should. http://doc.rust-lang.org/std/collections/hash_map/enum.Entry.html#
Relevant TorrentFreak article: https://torrentfreak.com/google-porn-takedowns-carpet-bomb-github-150107/
How to debug Rust programs. Using GDB, memory profilers, speed profiles, etc..
Although this article seems to be a little bit FUD. Google apparently did not comply…yet. A) Action taken field ist empty B) I can still find cargo on github via the google search 
thanks! i included your usecase into the brainstorming and hopefully find the time to create something adequate!
not had time to get into rust as much as i'd hoped, so if you want a review from a relatively new-to-the-language perspective i'd be glad to.
Well let's [look](http://static.rust-lang.org/doc/master/src/core/str/mod.rs.html#197) at the deprecation message: #[deprecated = "use std::ffi::c_str_to_bytes + str::from_utf8"] May I suggest using `std::ffi::c_str_to_bytes` and `str::from_utf8`? :p Edit: Your own replacement is easily written thus, which should also illustrate how to use the functions that are suggested as replacements: fn from_c_str&lt;'a&gt;(p: &amp;'a *const libc::c_char) -&gt; &amp;'a str { std::str::from_utf8( unsafe { std::ffi::c_str_to_bytes(p) } ).ok().expect("Found invalid utf8") } 
Right now `plugin = true` largely tells Cargo to compile the crate for the host architecture and to not link it transitively upstream. With `macro_rules`-based macros, I do not believe that you want `plugin = true` because you may call into runtime support which needs to be compiled for the target and linked upstream. In short, `plugin = true` should be considered for just plugins, not crates providing macros.
What if we have carousel style examples? Like on http://python.org That way we can showcase multiple strengths of rust, not just one. 
That makes sense. Thanks! Just to be sure I understand, the actual type of the closure is an anonymous struct with a `&amp;` pointer to each captured variables? And one such type is generated each time `rustc` sees a closure expression, so it is different for each `match` arm?
I think these takedown notices aren't even typically real DMCA requests, they're just essentially threats to the content provider that they *will* file one if they have to.
I'd be glad to help review material!
I'd also like to volunteer to review. I've written many many libraries in Rust, and am the author or one of the maintainers of several of the most popular libraries out there, including `Iron` and `Hyper`. I have particular expertise in Rust web technologies, but have also written more general purpose libraries and am generally one of the most active Rust users. &lt;/shameless-plug&gt; EDIT: Forgot to give anyone a way to contact me. I'm on github/IRC as reem and my email is jonathan.reem at the gmail place.
Hello! I am unfamiliar with libopencv, could you give me any more information as to how I could (or why I should) look into this library for this task over say ffmpeg? Cheers, Louis
Thank you. Sadly there doesn't seem to be anything here
Your question is confusing me. Can you include the before and after code snippets.
I see. Thanks for the help.
It doesn't mention servo. Are you sure it's works with that?
I'm just excited here. 
Thanks, nice to see that there's at least some slap on the wrist.
Did you compile with optimization flag?
I just used cargo bench, which according to http://doc.crates.io/manifest.html defaults to: [profile.bench] opt-level = 3 debug = false rpath = false lto = false
What sort of arrangement does Packt usually have with authors? (Compensation, copyright etc.)
Would gladly help to review it.
`.char_at()` is *not* O(n). It receives a byte offset and looks around at that offset to see if it is between two UTF-8 sequences (or the beginning or the end), which can be done in the constant time thanks to the structure of UTF-8. Still it takes time.
Another interesting observation. You can get halfway to LTO performance by annotating the `get_score(...)` function with `#[inline(never)]`. Even when activating LTO #[inline(never)] does not hurt, so I think in this case the compiler is too eager to inline. Maybe a bug? [baseline] test almost_overlapping_matches ... bench: 3899 ns/iter (+/- 186) test matching_broken_up ... bench: 947 ns/iter (+/- 51) test matching_exactly ... bench: 1686 ns/iter (+/- 226) test non_matching ... bench: 208 ns/iter (+/- 22) test overlapping_matches ... bench: 4175 ns/iter (+/- 146) test paths_empty_query ... bench: 7085 ns/iter (+/- 952) test paths_non_matching ... bench: 1005723 ns/iter (+/- 155945) test paths_trivial_query ... bench: 955873 ns/iter (+/- 19859) test progressive_non_sequential_few ... bench: 613735 ns/iter (+/- 16719) test progressive_non_sequential_many ... bench: 2446396 ns/iter (+/- 68348) test progressive_sequential_few ... bench: 741509 ns/iter (+/- 27558) test progressive_sequential_many ... bench: 3301223 ns/iter (+/- 95184) [baseline + LTO] test almost_overlapping_matches ... bench: 2781 ns/iter (+/- 85) test matching_broken_up ... bench: 698 ns/iter (+/- 44) test matching_exactly ... bench: 1208 ns/iter (+/- 26) test non_matching ... bench: 179 ns/iter (+/- 5) test overlapping_matches ... bench: 2991 ns/iter (+/- 97) test paths_empty_query ... bench: 7151 ns/iter (+/- 282) test paths_non_matching ... bench: 830424 ns/iter (+/- 30319) test paths_trivial_query ... bench: 834844 ns/iter (+/- 19193) test progressive_non_sequential_few ... bench: 527996 ns/iter (+/- 12279) test progressive_non_sequential_many ... bench: 1970205 ns/iter (+/- 50075) test progressive_sequential_few ... bench: 619376 ns/iter (+/- 11685) test progressive_sequential_many ... bench: 2644450 ns/iter (+/- 134645) [baseline + inline(never)] test almost_overlapping_matches ... bench: 3346 ns/iter (+/- 77) test matching_broken_up ... bench: 820 ns/iter (+/- 18) test matching_exactly ... bench: 1517 ns/iter (+/- 36) test non_matching ... bench: 186 ns/iter (+/- 5) test overlapping_matches ... bench: 3573 ns/iter (+/- 105) test paths_empty_query ... bench: 26147 ns/iter (+/- 874) test paths_non_matching ... bench: 911298 ns/iter (+/- 29370) test paths_trivial_query ... bench: 907690 ns/iter (+/- 26379) test progressive_non_sequential_few ... bench: 576430 ns/iter (+/- 501401) test progressive_non_sequential_many ... bench: 2100570 ns/iter (+/- 54895) test progressive_sequential_few ... bench: 691770 ns/iter (+/- 12127) test progressive_sequential_many ... bench: 2820707 ns/iter (+/- 57183) [baseline + LTO + inline never] test almost_overlapping_matches ... bench: 2767 ns/iter (+/- 73) test matching_broken_up ... bench: 714 ns/iter (+/- 19) test matching_exactly ... bench: 1275 ns/iter (+/- 40) test non_matching ... bench: 181 ns/iter (+/- 4) test overlapping_matches ... bench: 3058 ns/iter (+/- 109) test paths_empty_query ... bench: 25624 ns/iter (+/- 896) test paths_non_matching ... bench: 845007 ns/iter (+/- 22002) test paths_trivial_query ... bench: 823475 ns/iter (+/- 26543) test progressive_non_sequential_few ... bench: 516231 ns/iter (+/- 15115) test progressive_non_sequential_many ... bench: 1961153 ns/iter (+/- 63912) test progressive_sequential_few ... bench: 628627 ns/iter (+/- 17628) test progressive_sequential_many ... bench: 2629104 ns/iter (+/- 99228) Additionally looking at the output of running the last benchmark variation using perf shows that 9% of the time is spent in the initialization function `get_scores(...)` and 88% in `get_score(...)`. Allocations only account for about 2% of the runtime, surprisingly I also see about 0.52% of `memcpy_sse2_unaligned`. From looking at the code I don't know where this could come from.
[Let's make it official](http://i.imgur.com/vwMin.gifv)
It's a bit annoying that a straight-to-UTF-8 variant (up to the `Result`) is not provided out of the box. I can understand reduction of conversion methods on self which can be easily chained, but this combination is going to be used a lot, and it's unwieldy.
Could you have `Index` implementations instead of just `get`? Though panicking on missing fields wouldn't be that practical, just nicer syntax.
A million times this.
&gt; different closures have different types Even closures that take the same argument type and return the same type?
That said, I _could_ rework the whole ruxt-xlib API, but then my version would deviate from the servo version by far.
Ran into a similar issue when linking rust-crypto.. I wouldn't know how to fix it.
Yup
Just a gentle reminder of rule #3 :)
I also implemented proper unicode handling now. For that I changed get_score to accept `search: &amp;[char]` and `string: &amp;str` stays unchanged. Then the for loop of the string can be replaced by `for (position, string_char) in string.chars().enumerate() ` and the rest can stay the same. Together this gives me the following performance: [+ ... + unicode codepoint comparison ] test almost_overlapping_matches ... bench: 2447 ns/iter (+/- 184) test matching_broken_up ... bench: 672 ns/iter (+/- 29) test matching_exactly ... bench: 1218 ns/iter (+/- 94) test non_matching ... bench: 294 ns/iter (+/- 9) test overlapping_matches ... bench: 2630 ns/iter (+/- 98) test paths_empty_query ... bench: 23821 ns/iter (+/- 1066) test paths_non_matching ... bench: 847292 ns/iter (+/- 18252) test paths_trivial_query ... bench: 871451 ns/iter (+/- 20608) test progressive_non_sequential_few ... bench: 557618 ns/iter (+/- 13538) test progressive_non_sequential_many ... bench: 1296372 ns/iter (+/- 30009) test progressive_sequential_few ... bench: 673103 ns/iter (+/- 17724) test progressive_sequential_many ... bench: 1713415 ns/iter (+/- 27850) For reference here the numbers based on my equivalent u8 implementation: [+ ... + byte comparison instead of unicode (still casting to char for .is_alphanumeric)] test almost_overlapping_matches ... bench: 1981 ns/iter (+/- 90) test matching_broken_up ... bench: 558 ns/iter (+/- 67) test matching_exactly ... bench: 961 ns/iter (+/- 19) test non_matching ... bench: 205 ns/iter (+/- 4) test overlapping_matches ... bench: 2061 ns/iter (+/- 59) test paths_empty_query ... bench: 7134 ns/iter (+/- 201) test paths_non_matching ... bench: 872358 ns/iter (+/- 18203) test paths_trivial_query ... bench: 870279 ns/iter (+/- 22339) test progressive_non_sequential_few ... bench: 562006 ns/iter (+/- 13396) test progressive_non_sequential_many ... bench: 1371625 ns/iter (+/- 34702) test progressive_sequential_few ... bench: 682985 ns/iter (+/- 30355) test progressive_sequential_many ... bench: 1829797 ns/iter (+/- 54104) I guess to be really correct I shouldn't use `.chars()` but one of the normalizing iterators. Or is this enough? Additionally I also tried more optimizations after that, but I will always get mixed results where the small tests get faster/slower and the last six test slower/faster. Ultimatively because of the many indexed accesses this type of code will always be slower than what is possible in an unsafe language. The submitter will either need to refactor the datastructures or add many unsafe accesses to avoid the cost of bound checks. Even bigger performance wins can probably be achieved by pruning strings using a cheap heuristic before this costly algorithm is run.
Have you tried: * `cargo clean` in case there's something left in your build directory which is breaking things * running `rustup.sh --uninstall`, then running `rustup` again to reinstall? It looks like something odd has happened somewhere, and based on the response here it doesn't look like it's happened to many people, so the simplest option is probably starting again rather than trying to diagnose and fix it properly.
Thanks for the suggestions. Tried uninstalling and cleaning, but didn't work :-(
Hard to say then, without knowing how you're building. How are you building/what does your Cargo.toml look like (if you're using cargo)? It looks like parts of libserialize isn't being linked for whatever reason - could be a version mismatch or something, but reinstalling should have fixed that.
It's not just failing on my box either: here's how travis.ci fared: https://travis-ci.org/marcusklaas/backbonzo Edit: link to Cargo.toml: https://github.com/marcusklaas/backbonzo/blob/master/Cargo.toml
Ok, I went on and submitted [an issue](https://github.com/rust-lang/rust/issues/20749). Hopefully this indeed is a (fixable) bug.
I would like to see it working in MSVC enviroinment, but it looks like a very boring and huge effort :)
miniservo-mac definitely does; I demoed it for potential interns yesterday.
Well, as long as it works I'm fine with it.
Perhaps they can publish a wiki/rustdoc with all of the examples from the book, and run them through automated tests at each nightly release, like the rust stdlib and guide docs are. 
Yes, the pattern: impl RemoteTrait&lt;LocalType&gt; for RemoteType {} is being rejected now. If it's blocking any of your libraries, please report it in [this issue](https://github.com/rust-lang/rust/issues/20749)
It definitely worked for me about a week back. The operators didn't, but that was an outstanding issue I believe. Assigning the `f32` to a `x` then using `x.add(y)` did work. This is definitely a new issue.
Correct
Is there a list of tasks that would need to be done to get it to work with LLVM / MSVC in windows?
Cool, I'll hop on tonight (no IRC at work, unfortunately). I love rust and just so happen to be a Windows users (my day job is writing C#, and I love gaming and DirectX programming in my free time), so I'd love to help make rust feel more native to Windows in the long run.
That would be wonderful, thank you :)
Does that count for comments, though?
Good thing this account is scheduled for deletion; should've known posting memes wasn't appropriate, I feel disappointed with myself..
Mozilla is looking to integrate Rust into Gecko which requires being able to build using Visual Studio. So I would imagine Visual Studio/Clang-cl support will be happening. https://github.com/servo/servo/wiki/Mozlandia-Rust-In-Gecko
So `box` is coming back later? Going from ~x to Box::new(x) is a bit overly verbose for my taste.
&gt; If? Most of the time, they are just criminally incompetent. 
Your question is very broad. Are you memory bound? Is the text in ASCII or UTF8? What sort of format are your entries in .. JSON/XML/CSV .. etc? 
does "feature gate" mean this: http://doc.rust-lang.org/syntax/feature_gate/ ?
Is the box syntax going to support custom allocators at some point?
Got bitten by this too while updating `json_macros`. [Here's a failing Travis CI build](https://travis-ci.org/tomjakubowski/json_macros/builds/46335711). Looks like the linker can't find `rustc-serialize`? (meanwhile the rlib exists in `target/deps`).
&gt; Or is there some other objection to ~ that would exclude it from consideration? Part of `box`'s point was support for allocators e.g. `box(RC)`, `box(GC)` or whatever. I don't see how that would be feasible with `~`.
Yes.
No, it still has some compiler magic (DerefMove is the only one off the top of my head).
Since you know scheme, I recommend http://en.m.wikibooks.org/wiki/Write_Yourself_a_Scheme_in_48_Hours Also check out Real World Haskell, which explains algebraic data types quite well.
So the 'box' keyword is being feature gated because the design was never really finished? Is Box::new() just a temporary replacement until box is completed or is it a permanent thing?
~&lt;RC&gt;(x)? ? what's the problem just subsititute ~ for `box`
That's the thing with closures, they have a secret input: the context. Remember that a closure is really more like a secret, ad hoc struct that contains all values it captures. It could promise you that if it doesn't capture anything external or the outsides are the same that it'd be the same type, but this would inform us of two things: * How the compiler chooses what gets captured and what doesn't. * How the internals of the captured environment works (does it always order them the same? Is it based on how they are internally used?). The compiler doesn't really want to let us know this, because then it'd be forced to always act the same. So instead each closure you make is a different type, the reason is that the compiler may (or may not) make it very differently internally to how the outside works. It wants to be able to decide, for any reason, that two closures are different even if they have the same signature. What you do then is you hide the internal differences behind a vtable by boxing the value.
Probably not, though I can't say for sure.
Lack of support for allocators, making `Box&lt;T&gt;` back into a language construct rather than a library one (yes, it's not 100% today) and over-use due to being so easy to type.
Unfortunately their information seems to be a pretty out of date and/or incorrect. Talking about green-threads, tasks, and the runtime. Also claimed unsafe pointers litterally *can't* be done. :/
i'm not really clear on what you're saying. If the tilde replaced the box keyword, why would you have to change anything in the implementation besides the parsing? Also, why do you think that the tilde makes people use pointers more? I've been playing rust for maybe 1-1.5 years and i just never saw that assertion bear out in my code, it seems like one of those "just-so" explanations that sound right.
Unfortunately we ran into a few codegen bugs which mean that current benches are sort of... irrelivant. rustc was just doing some awful things. That said https://github.com/rust-lang/rust/pull/18028 and https://github.com/rust-lang/rust/pull/19782 have a smattering of benchmark numbers.
Don't worry about it! You can't be expected to read all the rules for all the sub-reddits you frequent - and life would be boring if everyone spent all their times abiding to the rules. Looks like you've still got positive imaginary internet points for your comment anyway, so no harm done :)
This is what I want to do. Each of those "featuring" bullet points would be one carousel slide. In fact I started working on a [set of examples](https://gist.github.com/kmcallister/08adcaed91987b74a21e). Some of them are hard to illustrate, though...
Yes it does - also note rule #2, which I would consider a superset of rule #3 - of course this is debatable. We (the mods) have discussed the rules and things quite a bit, and the general consensus is to remain fairly lax with enforcement, unless things are getting out of hand. Ideally everyone would follow the rules, but we don't plan to punish anyone for minor slip ups.
You still have ~2-3 months to find some, make some or move somewhere with more. :P
AFAIU, `box value` exists, because in `Box::new(value)`, value could be too large to be allocated on the stack. So, shouldn't instead signature of `Box::new` be changed to accept closure instead of value: `Box::new(|| value)`? Language won't need `box` expression.
Oh, also: &gt; why would you have to change anything in the implementation besides the parsing? The point is that right now, Rust-the-language doesn't know anything^* about `Box&lt;T&gt;`. Adding `~` as syntax would make the language once again know things about `Box&lt;T&gt;`. *: Okay, so this isn't 100% true, but that kind of fudging is one of the reasons that it's got the feature gate.
The more important use cases of `Box&lt;T&gt;` are the ability to efficiently transfer large data between threads and the ability to use values, whose size is not known at compile time which includes boxed trait objects (`Box&lt;Trait&gt;`). The language does not need `box`, but it helps. Especially because you can also use pattern matching with `box`.
I think I understand what you mean now. Thanks
There isn't really one great resource on how to exploit the type system in Rust (yet), but there are some great ideas out there in the literature for Scala, Haskell, and so on. I would be happy to chat with you about it on IRC, you can find me in #rust on Mozilla's IRC server. Darin Morrison and I hang out in #epsilonz on Freenode and have been playing with some fun ideas from Typelevel programming which allow you to encode powerful static invariants about your programs.
It might work is Rust had "named return value optimization": when compiler calls a function (parameter closure in case of Box::new) it passes a pointer to store resulting value, and called function (parameter closure in case of Box::new) won't need stack space to temporarily store return value.
This does already happen, but I'm not sure we do it everywhere where we could.
Indeed, this runs normally (even without optimizations): fn main() { let a = create_box(|| [0u64; 1_000_000]); println!("{}", a.len()); } fn create_box&lt;T, F&gt;(x: F) -&gt; Box&lt;T&gt; where F: FnOnce() -&gt; T { box x() } while this overflows the stack (even with optimizations): fn main() { let a = create_box([0u64; 1_000_000]); println!("{}", a.len()); } fn create_box&lt;T&gt;(x: T) -&gt; Box&lt;T&gt; { box x }
&gt;&gt; Many people try to compare Rust to Go, but this is flawed. Go is an ancient board game that emphasizes strategy. Rust is more appropriately compared to Chess, a board game focused on low-level tactics. Clojure, with its high-level purview, is a better analogy to the enduring game of stones. Well played.
Wow, this needs far more input from embedded developers. A few things to consider: - instruction memory width is not always data memory width - What happens when rust gets to compile on 48-bit machines? - What happens when code moves from one machine width to the next? - `int` should not exist as name; it's ambiguous. - `word` (or other similar term) should be denoting *memory width* (with caveat that it may be variable in the same program, have fun). - your code should be portable except for the things that rely on memory width, which is obviously unportable. Obviously wrong things should be obvious. - `int` should not be `bignum` - a bigint type should be for that, with appropriate overloads for +, -, etc. Correctness to the nth degree should be the starting point. Rust's design makes it a contender for being on spacecraft. Please make decisions appropriate thereunto.
Though, that borrow checker at times...
I didn't get the impression that they are opposed to cargo so much as that they want complete control over what is going into the build. Letting them use their own crate registry may be enough to satisfy them.
Also they don't want their build tools doing network access to strange remote servers they don't control.
Thanks, callgrind was helpful.
Is rustc a typical Rust program? Perhaps surveying Github / crates.io code would be more fair? (programming language lore cautions people to not create languages merely good for writing compilers).
Yes, that is true.
He checks to see that it `is_some()` so unwrap is safe in that code, I agree though `if let` or `match` would be more idiomatic.
&gt; litterally Hey, I'm _that_ guy. There's only on 't' in literal(ly). I knew I remembered the same typo (litteral) from [somewhere else](http://cglab.ca/~abeinges/blah/rust-lifetimes-and-collections/) (Yes, that particular misspelling irked me a lot :P)
cargo: `box`: 25, `Vec::new()`: 58, `.iter()`: 192, `.clone()`: 240, `to_string()`: 244 servo: `box`: 305, `Vec::new()`: 65, `.iter()`: 342, `.clone()`: 748, `.to_string()`: 116 hyper: `box`: 15, `Vec:new()`: 0, `.iter()`: 18, `.clone()`: 14, `.to_string()`: 77
I love having a language I've been intending to learn explained in terms of another language I've been intending to learn! Just kidding, great work!
It's still preferable to use `match`, e.g.: let t_val = match t { Some(x) =&gt; x, None =&gt; time::now_utc() }; Alternatively, you can use a method on `Option` for this purpose: let t_val = t.unwrap_or_else(|| time::now_utc()); This is short enough that you might even inline it into the println.
&gt; drastic UI changes is simple, because it exists as just an XML file? You mean HTML based, no? Well, IIRC [XUL](https://developer.mozilla.org/en-US/docs/Mozilla/Tech/XUL) interface used for Firefox UI is XML based. It's XML with special ways to define element positioning. So Firefox parses and renders it's UI. If I understand correctly, XUL was made way before HTML5 was conceived. HTML5 has caught up with XUL's possibilities. So the next logical step is to remake XUL in HTML5. Basically your UI is just a page that has iframes with real pages in it (or something). Personally, I think HTML5 is still missing a solid Cassowary Layout engine like [Grid Style Sheets](http://gridstylesheets.org/). If HTML5 actually had that, true separation of visual and data layout could be achieved. 
This is great! I really like your writing style. Be sure to ping me when you write Clojure for Rustaceans, I've been meaning to learn it for ages ;)
Veeeeerrry loosely based. Way HTML handles nesting is very, very complex. Otoh Doctype in HTML is trivial.
&gt; This alludes to the industrial nature of the language and its humble blue collar origins in Mountain View, California. Nitpick: Graydon once said that the name "crates" was inspired by the rusty shipping containers that he sighted daily while passing the shipyards in Vancouver on his way to work. Rust ain't Californian, it's Canadian!
The Gecko release engineers are not the people running crates.io, even if they are under the same corporate umbrella. It's a big company.
Well, it's not like it threatens to eat your laundry or anything.
I'd like some discussion of cross compilation and using Rust for Android and iOS. I'd like to review. I have two years experience using Rust. :) Although it would be more accurate to say I have two years experience using various different languages that later evolved to Rust.
I made a little Rust program to plot complex valued functions a while ago. I have not yet updated it, as I have been waiting for 1.0, so I guess I can go back and do that update now :) Mine has some example pics too. https://github.com/safiire/rust_complex_grapher
This is terribly inconvenient for my joke. I reworded it a bit, but it is still a lie.
Picked a random page and saw this (which I actually ran into yesterday, followed by much pulling of hair): on `http://doc.rust-lang.org/nightly/book/tasks.html`, it talks about a `try` function that as far as I can tell, no longer exists. It talks about how it differs from using `spawn`, with an example that... uses `spawn`... :S
Unfortunately, some of the pages are quite old. That one specifically is essentially entirely wrong (at least at a syntax/naming level). It does say: &gt; NOTE This guide is badly out of date and needs to be rewritten. but that's obviously no substitute for actually being correct! /u/steveklabnik's PR that did this conversion just migrated all existing content into the new format, it did nothing of significance to what the pages actually said. The content is presumably going to continue quickly now that things are laid out more cohesively!
Aye, just thought it was safer to mention it just in case. It's always fun to do a library update rollup *right* after some major change has happened and the docs are all still completely wrong/unupdated.
I was actually just going to search out an ETA for stability today, so this was helpful! Thanks
For the moment nothing works anymore with 3.5 (because of compilation errors), even what used to work with 3.4. I'm currently investigating.
It's also significantly less efficient because it forces a copy. It will be a major regression for large types.
you've got to feel for the guys doing the documentation while the language in such a state. the amount of breaking changes has been something else these last few weeks!
It would not. Please read the rest of this thread about RVO.
This is great! Thanks! Just wondering, is the naming convention a design decision or a implementation compromise? Axpy::axpy sounds a bit redundant.
I hope there will be an attribute to allow overlapping instances. I've found that Rust keeps implementing Haskell extensions :3
Learn two languages for the price of one! Now that's a deal!
AIUI, alpha is just another snapshot as 0.12.0, and the official recommendation is to continue using nightlies (1). The library stabilization will continue in the nightly "channel". I don't know if the team will release more alphas (probably not), or just a single one. In any case, you shouldn't pin your libraries to the alpha version, just continue using nightlies as you have been. My recommendation is to audit the unstable warning you are getting now, address the ones that are under your control (like `box` patterns or using range notation instead of the `range` method, etc), and then stick an `#![allow(unstable)]` in your crate root. (1) Source: [The IRC logs](https://botbot.me/mozilla/rust-internals/2015-01-08/?msg=29072360&amp;page=14): brson: seanmonstar: fwiw the release announcement is going to just say 'keep using nightly' brson: alpha is largely an opportunity to tell the world we're feature complete brson: still not 'use this release'
Is there an easy way to export this guide to a single document/file (I'd like to read the whole thing on my Kindle)?
That's weird. http://blog.rust-lang.org/2014/12/12/1.0-Timeline.html &gt; The alpha release &gt; &gt; Reaching alpha means three things: &gt; &gt; The language is feature-complete. All gates are removed from features we expect to ship with 1.0. &gt; &gt; The standard library is nearly feature-complete. The majority of APIs that will ship in 1.0 stable will already be marked as #[stable]. &gt; &gt; Warnings for #[unstable] features are turned on by default. (Note that the #[experimental] stability level is going away.) &gt; &gt; In other words, 1.0.0-alpha gives a pretty accurate picture of what 1.0 will look like, but doesn’t yet institute release channels. By turning on warnings for unstable APIs but not excluding them altogether, we can get community feedback about which important APIs still need to be stabilized without those APIs simply disappearing over night. That's flatly contradictory to what the blog was talking about before; you're talking about hitting the first and third points, and skipping the second one entirely. Seems weird to me. My understanding was: - alpha hits; feature complete, lints on - minor #[stable] / #[unstable] changes during the alpha - beta hits; stability feature gates are enforced - bugfixes only - 1.0 You're saying: - alpha hits; feature complete, lints on - massive api change and breakage - beta hits; stability gates are enforced - everyones code breaks, people complain - ...? &lt;--- What is this - profit - 1.0
I was eagerly awaiting for more documentation about Trait Objects from core team. Thank you for writing this.