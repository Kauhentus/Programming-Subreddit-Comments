OpenDNS was a very early adopter and one of the first commercial user of Rust. They started with [bloom filter to remove duplicate records from authoritative servers](https://labs.opendns.com/2013/10/04/zeromq-helping-us-block-malicious-domains/) in October 2013. Rust code used ZeroMQ to communicate with the rest of the world. And then [HyperLogLog counter for IP addresses](https://labs.opendns.com/2013/12/05/hyperloglog-and-malware-detection/) in December 2013. Recently, [log processing pipeline using Redis](https://labs.opendns.com/2014/10/01/redesigning-dns-database-low-latency/) is loaded with Rust, October 2014. In the process, they contributed to following Rust crates: [bloomfilter](https://crates.io/crates/bloomfilter), [hyperloglog](https://crates.io/crates/hyperloglog), [redis](https://crates.io/crates/redis), [zmq](https://crates.io/crates/zmq).
(I'm not the original author of the post, though I have written a post detailing the inverse situation)
&gt; `AppendWithUnit` You're thinking of monoids, rather than monads. 
Cool! Maybe the licensing should be dual Apache/MIT though to make it easy to transfer into std in case some macro becomes common and useful enough?
Servo builds for Android, using [android-rs-glue](https://github.com/tomaka/android-rs-glue). The Rust buildbots include a cross compile test for Linux host, ARM Android target, which runs the compiler test suite via `adb`. So at least that combination should not break.
Can you clarify what "logging" means? RUST_LOG and log specs seem to still work.
`cargo bench` does, but `cargo run --example` doesn't, you need to pass `--release`. The code the author said is 30x slower than go is on an example
iOS maintainer here. I can't comment a lot on Android part as I haven't researched it yet, but I can share a bit of my experience with iOS. What kind of functionality do you want to share between platforms? How good it could/should be isolated from the native part? How much data is expected to be flowing between library and native libraries?
Under your "good roadblocks" section: &gt; self.add(self.window[self.wofs], ch); This should not trigger a borrow checker issue if `self.window[self.wofs]` is `Copy` (which I infer based on the fact you used a temp variable to bypass the issue). I run into this problem all the time, I think it's [this](https://github.com/rust-lang/rust/issues/6393) bug. So no, it's not supposed to be like that and maybe it'll be fixed someday. Great article!
&gt; Maybe I'm misunderstanding this library. I'll know more in a bit. Going to skip past implementing this unnecessary bit and try this with a giant vector of bytes. Mind you, byte vectors implement Write (see bottom of http://doc.rust-lang.org/std/io/trait.Write.html), so they are an in-memory output stream.
and what advantage Rust gives you in this use case?
Exactly, this is needed only for particular closures.
It's ok if you want learn and play. But if you want to go to "production" where are a lot of other things to consider... Using Rust has own overhead: - much worse tooling - integration with native code - data transfer, maybe marshaling, async things - instability (I hope that one will be solved soon) Advantages for using Rust should overtake that overhead and so far it might work only when you're doing a platform or a builtin server, just "sharing logic" is not enough. P.S. my point is not to discourage experimenting, it's more about being pragmatic and avoid complaints later because estimation was wrong.
Ah but that's not exhaustive (there's already an issue for it). Adding a new builtin type would break code that does `type f128 = (u64, u64)` (or any other alias with that name).
It isn't copyable (it is an array), but IMHO it still is a compiler bug.
As a matter of taste, I do not like byte separators, because they make things really messy. Try to get the first line of a file when the first line is 2GB long and most libs will explode :) So, to be safe you have to put a limit somewhere, either in the spec or in the data. Now, on a more serious note, I think not_line_ending should return the whole input if it does not see the terminator, and the parser calling it handles applying multiple times if needed. For `Incomplete`, there are two cases, either you know how much data you need, or you don't, so I'll make a sum type `Unknown|Size(usize)`. Making Incomplete(0) meaningful is really ugly. BTW, what protocols are you planning to implement in Rust?
There's been a bunch of suggestions like that. We already output range metadata for discriminants (C like or otherwise). Similarly we could use an invalid unicode value in `char`. The main reasons these haven't been done is that nobody has bothered to implement them, rather than any specific reasons.
The explanation *was* exhaustive, but the situation may have changed since it was written. (To be clear, I was just addressing /u/llogiq's comment that there was no explanation for the closure, not discussing the current situation.) --- In any case, we have control over the language, meaning we have the power to introduce new types in a way that doesn't break old code. The most extreme would be removing the no-shadowing rule from primitive types when new ones are introduced and just let scoping handle but a "softer"/better scheme might be versioning language features for example, the Rust 1.3 compiler might hypothetically have support for `f128`, but can also knows how to do the right thing with Rust 1.0 code where `f128` wasn't reserved. So, there's a variety of ways this can be done, and I'm not too concerned. :) (Just thinking about it now, not reserving might actually be good: someone could write a shim library for `f128` to allow people to write that type in their code, which falls-back to the built-in `f128` if it ever gets implemented.)
What's so great about intellij? Personally I don't really like vm based IDEs, but I'd like to add all great features to [my own IDE](https://github.com/madeso/ride/).
Nope. It's a fully fledged forum. Also very JavaScript heavy, probably why it needs whatever that URL provides.
&gt; What's so great about intellij? Everything? ;) It’s about how the features are combined such that is it nice to use. It is hard to pinpoint it to a single feature. Plus, intellij has a very good foundation to make a language-specific IDE. You don’t have to reinvent the wheel when adding support for a new language. PyCharm for example is just great.
Thanks for that exhaustive reply :-) If I were to write a library for say u128 (and maybe i128), could I use the respective LLVM intrinsics (and if so, how?) or would I need to implement the arithmetic functions in terms of u64 arithmetic and hope that LLVM figures it out anyway?
I was just surprised because the whole page failed to render without it.
Fastly is a CDN, that's where all the static assets are hosted.
/u/graydon2 would be better at explaining, as these were before my time :)
There are plenty of packet types that use segmentation like that, that's why I gave the SLIP example. Unfortunately we don't usually get to choose what we're trying to parse! Normally, you have a maximum size somewhere. If this was C it would be the size of the receiving buffer, and when you've got enough data to hit that limit you barf and handle the error cleanly. The nice thing about the framework you've set up here, is that that 'buffered data limit' can be handled once in the Producer and parsers don't have to worry about it (although obviously they can enforce it if it's in the spec). There aren't any specific protocols that I'm intending to implement. I'm more looking at the sort of protocols that I've had to implement (over and over) in C/C++ in the past. I'm hacking on Rust in my spare time, but also looking at it as a language to use at work. I end up doing all sorts of things - serial protocols, UDP based high speed data passing, network RPC etc.. But I'm also interested in the normal file/stdout parsing that you end up having to do in applications. I want something that's almost as convenient as RE based parsing, but is properly stateful and therefore robust. Yup, that sort of sum type is what I was thinking of. Proper semantic meaning.
&gt; Almost eveyrone wants HKT Funny, when I hanged around Rust, the consensus I perceived was that everyone who wants it can't write it, but everyone that could write it doubt it's useful. &gt; What would it work poorly with? I've heard, that polymorphism plays very weirdly with some advanced functional features. Was it HKT and polymorphism? I'm not sure. I'm not knowledgeable enough on the subject to make a claim either way. It could be OOP and abstract data types play badly, I saw it in a Youtube video, several months ago.
We can potentially do even more. For example, string-cache [supports](https://github.com/servo/string-cache/blob/529d56ea4f6b55eb81a3fb3b54e4cd5034f72a0f/shared/repr.rs#L28-L40) three ways of [interning strings](https://en.wikipedia.org/wiki/String_interning): pub enum UnpackedAtom { /// Pointer to a dynamic table entry. Dynamic(*mut ()), /// Length + bytes of string. Inline(u8, [u8; 7]), /// Index in static interning table. Static(u32), } The desired representation is a single `u64`, using a one-byte tag that also stores the length in the `Inline` case. Currently we implement this with [hand-written packing code](https://github.com/servo/string-cache/blob/529d56ea4f6b55eb81a3fb3b54e4cd5034f72a0f/shared/repr.rs#L60-L79). But in theory, `rustc` could figure it out automatically. (We'd need to use a `u4` type for the `Inline` length.) The packing/unpacking does slow down some operations, so you'd opt in to this analysis with `#[repr(tiny)]` or something. In string-cache, the top priority is efficient comparison and `match` on `Atom`s, so the packed representation is a big win. Tangentially, string-cache's unpacking code works by actually constructing and returning a value of that `UnpackedAtom` enum, which may seem inefficient. Indeed we used to have the bit twiddling mixed in with the interning logic. I converted the code to its present form, and after a day of tuning, it was slightly *faster* than before. When LLVM inlines a function call, it can then use [SROA](http://llvm.org/docs/doxygen/html/SROA_8cpp.html#details) to analyze data structures passed between the functions, the same way it analyzes local variables. `enum UnpackedAtom` *looks* like a data type and you *could* build an instance of it in memory. In our case though, the values don't really exist at runtime. It's just a "protocol" that helps LLVM glue together functions from several modules. Wild stuff. The take-away for me was: * LLVM is fucking amazing, * High-level abstractions in Rust code will often compile away to nothing. Another common example: a chain of iterator methods, each returning `Option&lt;T&gt;`, often compiles into a single loop. Of course you don't necessarily get this from the first / most naive code you write. In general though, the effort I put into helping the optimizer seems quite reasonable for what I get in return. * Sometimes what the Sufficiently Smart Compiler buys you is not absolute performance, but safer / more readable / more maintainable code that compiles the same as what you'd write in C.
Big +1 from me, that kind of thing is a *massive* pain to do after the fact.
Stack iteration: functions that suspend themselves mid-execution and yield results to their caller, while running on the same stack (and permitting the caller to "call around" them, while they're suspended). Like, um, non-escaping, single-frame generators running on the same stack as their caller. Later (once we had lambda) we switched to iteration by passing a lambda and getting callbacks over the range. Finally switched to external iterators at strcat's (correct) pressing. They compose better. Crash diagnostics: we had language-level support for polymorphic logging, and for failure/unwinding. So there was a language feature that essentially marked a value on frame-entry for printing when unwinding in an failing state. Of course all this can be done in libraries now. Binding is a very minimal (also cheaper to implement, less confusing to specify) way of accomplishing some of the things people like about lambda and currying, with less machinery. You have an expression called `bind &lt;call&gt;` that modifies a closure by binding values to some of its arguments. Like for a function `f(x,y,z)` you could have an expression `bind f(1, 2, _)` that produces a new function of the one remaining argument. It saves you having to reason about any interference between the function environment and the binding environment; it's all explicitly written down by passing arguments. The downside is it's more verbose and you have to write more explicit named helper functions. Sather does this, and it was the official C++ answer for closures up until C++11.
Ahh, finally I can get Rust in my favorite barely usable IDE ;)
I left it for a while and I've been busy with other projects. I think I need to rewrite it so that it owns its elements (currently it doesn't compile because closures have changed significantly since 0.11)
Not in macros, no. There's no mechanism in the macro expander to make something happen exactly `n` times, unless that is somehow expressed in the input, like `vec![element1, element2, element3]`. Now, there is `vec![elem; n]` but that doesn't implement the repetition in the macro, it's just a syntactical wrapper around existing APIs. However, this would be trivial to implement as a compiler plugin/syntax extension. You wouldn't even need to perform constant folding, just calculate the value in the plugin and insert it into the AST. The difficulty lies in building the plugin and grokking the sparsely documented and unstable `libsyntax` API. If and/or when we get compile-time function execution (CTFE), this can simply be implemented as a regular Rust function and used to initialize a `const` like so: const FACT_42: u64 = factorial(42); This may or may not have been done already as a compiler plugin itself.
&gt; this would be trivial to implement as a compiler plugin/syntax extension... The difficulty lies in building the plugin and grokking the sparsely documented and unstable `libsyntax` API. Kind of contradicting yourself here... imo the word "trivial" has almost no place in programming, because even the most mathematically trivial concept will usually have some implementation subtleties. What ever happened to things being "easy" or "straightforward" or "possible"? Anyway the [example syntax extension](http://doc.rust-lang.org/book/plugins.html#syntax-extensions) in the Rust book is pretty close to what you'd need.
Variadics: https://github.com/rust-lang/rfcs/issues/376
ADTs and pattern matching for me, and the borrow checker should go without saying...although it's still a love hate relationship with me :)
So you're thinking that a bunch of integers back to back would exhibit low compressibility? I guess that's possible. Maybe I ought to try it out on a text file.
Programmers work so much with discrete values that they begin to conceptualize things as having some discrete value (often with two possible states). Problems in particular end up being "trivial" or "non-trivial"....
That's me! One thing which I think is tangentially related is the use of debuggers in this space. I know I'm supposed to use rust-lldb to debug, at least on OSX, but I haven't used gdb in forever and I could definitely use a tutorial or something on this. Like setting breakpoints in rust modules, tracking the variables, etc. 
Specifically, a bunch of *random* integers back to back have poor compressibility. Random numbers have high [entropy](https://en.wikipedia.org/wiki/Entropy_%28information_theory%29) which basically means that there's a lot of information (or maybe 'potential' information? I don't do a lot of information theory) stored in your data. Therefore, it's difficult to compress since there aren't really any patterns to exploit. As a counterexample, consider HTML - lots of braces, similar tags, etc, that are all common and repeat frequently in patterns. HTML has a relatively low entropy, and is rather compressible.
ever tried compressing an already compressed file? it's essentially the same as what you're doing.
*Random* data is the worst-case for compression, which is all about finding patterns.
Not that build times aren't an issue, but you'll generally find that if you're going to get an error, you'll get it pretty quickly.
Yeah, especially with our gdb support being a bit wonky at times, that'd be helpful. I don't really use debuggers, so I'm also bad with them myself...
Depends how far down the change is in the dependency chain. If it's in Servo itself, it only has to rebuild Servo. If it's in some dependency of a dependency of a dependency... etc. it could take a bit longer.
Most of the time in compilation is spent in the LLVM passes doing optimizations and code generation. Just the type checking portion of an individual module is quite fast!
I'd be curious to hear the results. A text file is mostly just the same couple hundred numbers (representing letters, numbers, symbols, etc) repeated a bunch, while your input is each of a billion different numbers each occurring once. What about if you changed from iterating from 1 to a billion, to just using the _same_ value of `1` a billion times? Lastly, small tip: you can make big numbers a little easier to read by adding underscores: `Range::new(1, 1000000001)` to `Range::new(1, 1_000_000_001)`
&gt; So does rust need to recompile everything even if somebody just changed a single file? Rust's unit of compilation is a crate, so if a file gets touched, its crate needs rebuilt. Servo is split up into ~20 different crates, so only the part the change is in needs to be recompiled.
My input is not actually each of a billion numbers; it's just random numbers in the range 1..1000000001. Having said that, the test runs I was doing involved only about a thousand values, or 10,000, so the odds of any of them repeating would be *terrible.* I posted a repo that does exactly that--compresses whatever you pass into it. I tried it on my Cargo.toml and it made it bigger (as expected per the man page). Tried it on the code itself and it got smaller. So it seems like I didn't do anything *wrong,* per se... Dunno if I'm doing anything the right way or not, but at least it does what it's supposed to.
For reference, this whole idea was brought on by a requirement a buddy of mine actually had: he needed a wire protocol for passing large quantities of numbers around and he wound up rolling his own around an algorithm described here: http://arxiv.org/pdf/1209.2137v6.pdf My original intent was to try something along those lines, but after getting hung up on the streaming thing forever (before someone here helpfully pointed out that byte vectors, et. al all implement Read and Write), I scaled back my evil plan to involve an extant compression library.
Single file in a crate. A crate is under the hood just one big file, even if you've broken it into smaller ones. But if your large project is broken into many crates, you don't need to recompile everything, just everything up the dependency graph. Not usually a big deal. Type errors happen rather early (run with `-Z time-passes`, might need a `-Z unstable-options` as well), it's the llvm stuff that takes long. You should get all your errors early on after typeck/borrowck runs, then wait a while for llvm, and then get all your warnings (which could be errors in case you `#[deny]` a warning). Edit: lints run before llvm, sorry
Coherence checking can take a very long time too e.g. when building `rgtk`.
Same with asserts I believe, because they participated in typestate?
The easiest way is using macros IMHO.
The most prominent concept of Rust is ownership. You code fragments look a bit like as if Language and Localization should actually own their contents instead of borrowing it. E.g.: pub struct Language { id: &amp;'static str, name_english: String, name_native: String, } pub struct Localization { language: Language, strings: HashMap&lt;String, String&gt;, } But this is just a wild speculation since it really depends on what you want to achieve. You’re code is not self-explaining.
&gt; Functional record update syntax Could you give an example please?
Thanks! Thinking about it, the most annoying errors are syntax errors which get probably detected pretty fast. 
Sorry about the ambiguity. I don't want `Localization` to own its `Language` but I do want the other things to be owned. However, I've been told by many sources to prefer `&amp;str` over `String` for speed reasons. Am I right in assuming that this is an exception as the `struct`s are owning the strings?
`&amp;str` is preferred for argument passing into a method. In this case, you need to have String (or some other non-reference type) in Language if you want it to own the string.
&gt; Additionally it should be possible for a single library to support both modes of use. That is *very* good news. The thought of duplicating `regex_macros` makes me dizzy!
I've thought about it but I don't have infinite hours :(
Sadly that code is not public.
I'm not "hung up" on this. I've made my opinion clear and you can do what you want about it. It seems like that's probably "nothing", which is fine by me. It's good to have the discussion in public, but we don't need to agree about everything! Believe me, I understand that any word can upset someone, that it's impossible to be welcoming to everyone. We just try our best. This "trivial" thing is something that bugs me and a lot of other people I know, so I brought it up in what I hoped was a friendly manner. It's not a big deal. &gt; There's a big difference between saying "this task is trivial" and saying "if you can't figure this out, then you should give up because you're a total idiot". Anyone who extrapolates the latter from the former has deeper issues than semantic debates can solve. Words have emotional connotations, and this is a reality of communicating in human language, even if programmers often try to pretend otherwise. You can't put all the blame on the receiving end. We can't *all* be androids programmed for logic ;)
Here are some presentations by the Rust core devs that I personally like: - [Guaranteeing memory safety in Rust](https://air.mozilla.org/guaranteeing-memory-safety-in-rust) by Niko Matsakis (IMO the best explanation of ownership and borrowing) - [Intro to the Rust programming language](https://www.youtube.com/watch?v=agzf6ftEsLU) by Alex Crichton (Explanations with code of major Rust features) - [Concurreny in Rust](https://www.youtube.com/watch?v=oAZ7F7bqT-o) by Alex Crichton (How ownership/borrowing prevents data races, and an overview of the concurrency primitives provided by the stdlib)
Here is how you could have Localization not "own" the Language, and leave everything else as "owned": pub struct Language { id: String, name_english: String, name_native: String, } pub struct Localization&lt;'lang&gt; { language: &amp;'lang Language, strings: HashMap&lt;String,String&gt;, } impl&lt;'lang&gt; Localization&lt;'lang&gt; { pub fn new(language: &amp;'lang Language) -&gt; Localization&lt;'lang&gt; { Localization { language: language, strings: HashMap::new() } } } 
Actually, I think that lifetimes are explained really well in the book. What is not explained properly, however, is how to use them in your code. If I'm implementing methods for my struct, how and where in the declaration do I annotate lifetimes? How do I make my fields share my struct's lifetime? How do I take parameters received and attach them to a struct (often used in `new`) ? I think that if some common use cases were presented, it would really help mitigate this confusion about lifetimes.
I made [a library for this](http://www.reddit.com/r/rust/comments/2xb61l/macros_for_container_initialization_and_iterator/).
I've changed it to dual Apache/MIT in light of this request.
I've also got one in grabbag: [`collect!`](https://github.com/DanielKeep/rust-grabbag/blob/master/grabbag_macros/src/lib.rs#L27). Out of interest, are there any advantages to the way you've done it? I assume it must be using some faster code path given it's *way* more code than mine :D (my bet is on `SizeHint` not doing what I think it's doing).
Had not seen that. Thank you very much.
Thanks for the clarifications! And for all your work on Rust. Eagerly awaiting that retrospective series of posts ;)
It's funny, but "iterator literals" was the thing I was caught up on for a long time. I couldn't find a way to make them work without allocating some kind of temporary storage, which I *really* wanted to avoid. I discarded the idea of a fixed-length array because of no generic value parameterisation, and I don't like *arbitrary* limits. :P I suppose the trade-off here is that a "literator" can be used as an iterable source and does no heap allocation, but is limited in size and (ignoring optimisation) has to be fully constructed on the stack first. `collect!` builds the container element-by-element, so should take less stack space. I suppose the ideal case would be to make it possible to construct a `[T; N]` from an iterator, have `collect!` use `in` syntax, and have an `into_iter` implementation for `[T; N]` (which, I suppose, is more or less what literator does).
I'm not sure what you mean by custom debugger commands...you mean like "enable breakpoint if var_name == 3" on line 726 so the you can control when it stops? Or do you mean something else?
whenever you want to update a sigle record field and the rest you can use something like this syntax: let sphere = Sphere { center: Point(0.0, 10.0), radius: 10.0, material: Material::Glass }; // other has the same radius and material, but different Position let other = Sphere { center: Point(10.0, 10.0), ..sphere}; 
I am interested in having both the html pages and the API being served from Rust
That's a killing feature to me, a Python programmer...
Actually that restriction only applies in Haskell, which allows for orphan instances in the language (the "open world assumption"). Rust doesn't allow orphans, so the example works just fine.
That's pretty cool, thanks! It can kind of replace optional keyword arguments.
On the first, if you use the `MulPeano` trait instead of `*`, there *is* no multiplication to optimize. Something like (warning: untested) pub trait CompileTimeFactorial : Peano { type Output; } impl CompileTimeFactoria for Zero { type Output = One; } impl&lt;LHS&gt; CompileTimeFactorial for Succ&lt;LHS&gt; { type Output = MulPeano&lt;Succ&lt;LHS&gt;, CompileTimeFactorial&lt;LHS&gt;&gt; } On the second, that depends on your type system. I agree that the Rust type system could be regarded a turing tarpit.
Thanks! Currently preparing a Rust intro talk, this is exactly what I needed.
It's a bug, its place is in the bug tracker, not here?
I hadn't realised we were talking about the category of endofunctors ;)
True, while the computation is completely done in types, at the end of the day there's a giant rubble of `+1`s that gets compiled down to whatever by LLVM const folding. There has been some work on rustc in that area, but as far as I know, it's not ready for prime time yet.
Thanks, i hope this comes close after 1.0 :)
posting ads without even looking at the question is not really nice.
Are you using a [`BufferedReader`](http://doc.rust-lang.org/std/old_io/struct.BufferedReader.html)? (Or [`BufReader`](http://doc.rust-lang.org/std/io/struct.BufReader.html) if you're using the new `std::io`.)
Have you activated optimizations while compiling (e.g. `rustc -O3` or `cargo --release`)?
You're welcome. It's a known gotcha. There is even a thread on the Rust internals forum that collects cases like this.
In my experience, Iron is the way to go, but it has a bunch of dependencies that don't always build. The authors are very active and open to pull-requests though. It recently got static file serving with mime types and caching, which will work great for serving HTML without a templating framework.
It's funny, but I was thinking earlier today "you know, I could just feed a string literal into a Lua interpreter and tokenise the output... naah, that'd be stupid. No one would want to add a whole other language as a compilation dependency. It'd be almost as stupid as that idea to make a [compression format with an embedded VM](http://blog.cmpxchg8b.com/2012/09/fun-with-constrained-programming.html)." Clearly, you're a loony. But seriously, the first idea when the "no synexts in stable" thing was announced was "what if I just wrote a macro that piped its input into a program, then tokenised the output". Given how Cargo and build scripts have come out, do you think there'd be support for having a way for Cargo to pass executables to `rustc` so it can use them as filters?
I was also once considering writing an `asm!` like macro that would compile C code to ASM and then include it the regular way. But I didn't think it would be too useful, and I was too lazy. :P
Or at least make it more obvious that the default doesn't have optimizations. Maybe change the message to something like "Building XYZ in debug mode"
Placing debug builds in `target/debug/xyz` and release builds in `target/release/xyz` would perhaps also make it more obvious.
The linux version of the code should be trying `open` before anything else.
I was told one could possibly use gccgo to get better performance, too.
I like video tutorials because I can consume them passively. So often I use videos for stuff that's not high on my agenda or when I'm cooking/tired. Of course, I get less out of that than from reading+coding, but I can't be super concentrated 24/7.
Damnit, you are right ! I get carried away a lot recently ... . What's done is done though, unless I can pull this version and re-release with a lower version. Does cargo allow such trickery ?
Also, depending on how many times you're creating your `a` buffer, the `vec!` macro is quite a bit faster than the iterator approach. extern crate test; use std::iter; use test::Bencher; use test::black_box; const N: usize = 512 * 512; #[bench] fn bench_iter(b: &amp;mut Bencher) { b.iter(|| { black_box(&amp;iter::repeat(0).take(N).collect::&lt;Vec&lt;u8&gt;&gt;()); }); } #[bench] fn bench_vec(b: &amp;mut Bencher) { b.iter(|| { black_box(&amp;vec![0u8; N]); }); } // running 2 tests // test bench_iter ... bench: 263204 ns/iter (+/- 47127) // test bench_vec ... bench: 29998 ns/iter (+/- 11552) // // test result: ok. 0 passed; 0 failed; 0 ignored; 2 measured edit: fixed tests
Enum variants live in their own namespace. So `use grid::Cell::dead`, or just import Cell and then prefix everywhere. 
When we're at the topic of IEEE745r floating point types then we shouldn't forget the decimal ones
&gt; use grid::Cell::dead Getting an unresolved import for this. I created the module of Grid in my main, and use "use" in my grid_builder. Modules and imports kinda confuse me haha.
That's how to import. I don't know your exact structure, so I don't know how to correct. 
It's `use grid::CellValue::*` (to import `dead` and `alive`)
Got it thanks! I truly appreciate all of your guys' time. If you dont mind I got another one. How should I go about returning the grid in the build_from_grid function? I dont really understand how he is using cell_value with the absolute value bars. Edit: I've tried inner: Vec&lt;Vec&lt;cell_value(row,col)&gt;&gt; but that isn't working.
Gccgo has better code generation but no escape analysis. Nothing is allocated on the stack and garbage collection is worse. 
A range literal is in the form of just `from..to`, the `[]`s give it a completely different meaning (statically sized array). So you just want `let v = 1..11;`, which will give you a proper Range struct, which implements `Iterator` directly, so you don't even need the `iter()` call! See [here](http://is.gd/xTOl1F)
&gt; 4 whitespaces for code doesn't seem to work? Just a random guess, but did you have a blank line between code and non-code? It seems to work for me
&gt; Any reason why I can not collect on a fixed size array? You need some way to handle the case where the number of elements collected is different from the fixed array's size. So then collect would have to return a `Result&lt;[f32;3], IncorrectSizeError&gt;` or something...
At a worst case, couldn't it just be panicking?
You buried the lede with `sh_mixin!` - that sounds massively useful and a similar feature is already hailed for its usefulness in nim.
This is the ugliest hack I've seen in a while. I like it. ;)
That's neat. But I didn't delete anything, I just edited my question. Need to take a closer look to #3, I'm not very familiar with mapping yet. Why do you prefer solution #2 over #1? The functions that take iterators seems a bit verbose when it does the same thing as #1? And is there no way to get around v.clone()? This seems a bit odd and switching sum_square(v.clone()) - square_sum(v) to sum_square(v) - square_sum(v.clone()) gives an error? Would be using a reference &amp;v a comparable solution? With v being immutable I don't see why I need to copy here? Something like http://is.gd/P0YUUI?
Does anyone know how far away we are from having this? It's one of my most wanted features right now. I know it's planned, but are we talking like right after 1.0 or like years away? 
 error: could not deduce meaning of question as it relates to rust-lang. suggestion: did you mean to ask a question about rust-the-game?
Right after 1.0 if I have any say in it. (I don't)
I dream of getting dependent types.
that's very neat! i played with D briefly and the option to use D itself to generate code at compile time was a great feature.
&gt;The difference is that Rust cranks this concept and cranks it up to 11 Might want to be &gt;The difference is that Rust **takes** this concept and cranks it up to 11 That was just from a cursory scroll down the article. I dont have much else to say, except that it looks like there is just as much C/C++ code as Rust, which is strange.
&gt; I know it's planned, but are we talking like right after 1.0 or like years away? We don't have an official list of post-1.0 stuff to do and when. With the six week cycle, I would guess that before it starts, we'd take an overview of what to do during that cycle, and try to do it, but right now, we have "start thinking about post 1.0" as a TODO item, and we're going to talk about it post-beta release.
Sometimes, this isn't possible, because you can't write the type. https://github.com/rust-lang/rfcs/issues/518 would fix that, but is post-1.0 because it's backwards compatible. It significantly improves ergonomics, though, so it's high priority.
Already fixed that typo, thanks! &gt; it looks like there is just as much C/C++ code as Rust, which is strange. When talking systems, you have to compare to what systems programmers know. This is sort of a deep-dive rather than a high-level intro, and so I try to show how Rust works with ownership right off the bat. As I said in the intro paragraph, I'm not sure it's better, or that it works. An experiment!
This is good; I like the change of direction taken by starting with the difficult concept and then introducing features as you go. The more conversational style is also nice. This labours the point of ownership a lot more, and this is useful. The completely worked example demystifies the terms neatly. Given how in-depth it is, I agree with your reservations that it's likely not an ideal *first* introduction to Rust. However, it would have an excellent place (alongside other similarly written essays on other typical gotchas) in parallel to the more traditionally written guide. The different approach and style could allow people to get something that they weren't getting from the guide.
I think it could go in an in-depth section of the guide. In Depth - Ownership
Rustless?
Not even if you just return the ... trait object or whatever?
I got it working like this: http://is.gd/pzujPS your original code is http://is.gd/WeUGS6 What you had were general lifetime issues: in line 10, your closure (the anonymous functions) used a value. Anonymous functions borrow the used values by default; in this case this bit you, because the function that starts at line 7 would still think it owns the boolean and destroy it and it's end; thus invalidating any reference. When the iterator would be used at the main function, it would call your closure, which would be *very* confused by the missing value. However, you can make owned closures; these *own* any values referenced within. These are called **moving closures**, because they *move* all the values (thus they own it after moving) and can be created by ``move |args| {code}``. All this is well explained [in the rust book](http://doc.rust-lang.org/book/closures.html) The second issue was in line 8. You create a slice and then an iterator from it. [In the docs](http://doc.rust-lang.org/nightly/std/slice/trait.SliceExt.html) you see that the ``iter`` function of slices actually take a reference to themselves, this is probably the thing that introduced the error. Honestly, I'm not shure how this error works, but it can be fixed by using a vector and ``into_iter`` instead. This does not seem ideal for me, though.
If you read the text of the original RFC, https://github.com/rust-lang/rfcs/blob/62fe495d35f279a24eaef9b61dbe491145c2fb82/0000-abstract-return-types.md#motivation it explains the details
Okay, your code right now does not compile because you are trying to return an iterator that contains two different references to variables on the stack of that function. This is one way to fix it: fn main() { for n in values(true) { println!("{}", n); } } fn values(even: bool) -&gt; Box&lt;Iterator&lt;Item=usize&gt;&gt; { Box::new(vec![3usize, 4, 2, 1].into_iter() .map(|n| n * 2) .filter(move |n| if even { n % 2 == 0 } else { true })) } The difference here are: - `vec![3usize, 4, 2, 1].into_iter()` which creates a owned heap allocated vector and turns it into an iterator vs `[3usize, 4, 2, 1].iter()`, which creates a iterator that yields references to an stack allocated array. - `move |n| if even { ...` which captures variables like `even` by value and stores them inside the closure vs `|n| if even { ...`, which captures variables like `even` by reference to their location on the stack.
Returning a trait object is possible, but especially in the context of iterators discouraged for performance reasons, as you get virtual dispatch.
And no problem! Yeah I'm also always confused about whether I made it better or worse after the compiler errors change...
Content-wise I like this a lot. Some (hopefully) constructive criticism though: I realize this is just a first draft, but I think some of the explanations could be a bit more succinct. The first part especially I think could be slimmed down a lot. The section which explained the C++ program, while very illustrative, had a lot of diagrams which are kind of hard to read. Maybe making these into nicer illustrations would help to make this section more readable? I'm also a bit worried that the first part is kind of conflating scoping and ownership which I don't see as the same thing, since (at least in my mind) scoping is about the validity of a variable, whereas ownership is about the validity of the data that the variable points to. Maybe explaining this juxtaposition could help in explaining how what Rust is doing is actually different from other languages?
Actually I don't think that this does what I want. It just initializes some data structures with a fixed sized array. All those data structures need `FromIterator` which a fixed sized array does not have.
Very constructive, thanks. Yeah, I think the charts can be better, for sure. _lifetimes_ are scopes, not ownership. The owner's lifetime determines when something gets allocated or deallocated, as opposed to scopes formed due to borrowing.
For your first question: I'm not sure I understand what you mean by &gt; But it seems I need to use closures? ... however, if you want to have a function that can be given a range literal, the type would be ``std::ops::Range&lt;_&gt;`` where ``_`` is some integer (for example ``u64``). Note that this function would only take a range, not some random iterator. For this, you would need either static dispatch ([generics](http://doc.rust-lang.org/book/generics.html)) or dynamic dispatch (trait objects, not really well documented yet). Iterators can't be iteratored twice over. Iterators generally throw away every result that was already used, and calculate the next result only when needed. So at line 16, the iterator probably doesn't even remember where to start. The type system saves you from doing something you probably shouldn't do.... This is good! :)
I created https://github.com/rust-lang/cargo/issues/1309 about two weeks ago for that. It's starting to get more attention. I think I'm going to try to start the implementation this weekend.
/edit got it sorted out with the help of the IRC Rust chat: Running code - http://is.gd/QkZapx The closure was something the compiler told me in my first tries. Which code for line 16? There are so many play-rust.org links here and no one has something at line 16? My first idea was just using 2 functions which return the result as an int, using the same immutable array / range from 1-10. Like here: http://imgur.com/xMDVpoC Sounded easy, immutable stuff, nothing to copy, nothing to worry. But the code here is just a big ugly bloat :(. PS: Is there no easy solution to just fill an array with a bunch of numbers from 1-x?? Like in Haskell just let x = [1..10]? Having "Range" is okay but I want a normal, easy array. Or Vector. With the possibility to iterate over it. Multiple times. In different functions.
So it doesn't get lost: &gt; In Rust, we have names for these concepts, which are implicit in other **langauges** Should be `languages`
Ugh, you'd think I'd be good at that word by now, but I still typo it. Thanks.
Good call on both.
&gt; You see, when we introduce our binding, x, Rust knows that x has **ownerhsip** over the memory in the Box `ownership`
This question was cross-posted to [Stack Overflow](http://stackoverflow.com/q/28774496/155423).
&gt; If we’re the only reference, we are free to change what we’re pointing to, **becuase** that won’t cause any problems. `because`
For the lifetime notation, perhaps instead of just `+`, use `+` and `-`: let x = &amp;v[0]; // &lt;+ Add `x` // | v.push("B"); // | // | println!("{}", x); // | } // &lt;- Remove `x` Just an idea...
I think an introduction to Rust should be more to the point so you can start writing programs quickly. This is a good in-depth informational resource, but I think if I was introduced to Rust like this I might be scared away by all the extra information. I like the idea of focusing on Rust's core concepts first though. The Book probably goes into the less interesting syntax stuff a bit too much in the beginning. Also share dyoll1013's concern with explaining pointers and memory management while aiming this at C/C++ programmers. Makes me almost think what we really want is different introductions for different types of people: new programmers, systems (C/C++) programmers, other (Java/Python/everything) programmers. Not sure if that's doable, though.
We might end up doing that, yes.
&gt; Clearly, you're a loony. I try my best! :D &gt; Given how Cargo and build scripts have come out, do you think there'd be support for having a way for Cargo to pass executables to rustc so it can use them as filters? I'm not sure what you mean?
It seems like there's some separation of concerns issue going on here imho. This one chapter is * An introduction to Rust variable bindings * An introduction to Rust ownership and scopes * an introduction to pointers, the stack, and the heap * and a comparison of how Rust and C(++) manage these concepts
I showed this to a friend in the undergraduate computer lab because he was interested in learning rust, but this was his first introduction to it. He was pretty into it, but when he got to "the stack" portion, he closed out of the article because he "already knew what the stack was". I liked the memory layout portion, but I'm willing to bet that a lot of people will jump ship as soon as they see things that remind them of an awful systems-programming class. 
I *believe* it's that &amp;'a [T, 4] is coercing to &amp;'a [T] .
How so?
`sh_mixin!` seem like a great idea for sourcing git revisions, dates, hostname etc. to embedded build information into executable. Neat.
Seems somewhat sensible to me. :)
They're lambdas, not absolute values! `Vec&lt;foo&gt;` is the type of vectors of foo; to actually create one you will need to use a method, e.g. `Vec::new()` to create an empty one or `(some iterator).collect()` if you have an iterator. In that function I think you would be well served by (edit) ranges and `map`.
Historical reasons. AFAIK, 80-bit floats have only ever been widely used in the 80-bit registers of the old x87 floating point units. It's the reason people think that floating point computation isn't reliable or deterministic, since the results would depend on when the compiler decided to evict numbers from the 80-bit registers and put them in a 64-bit or 32-bit slot in main memory, causing completely unpredictable rounding. Now everything new works off SSE registers, which don't have that problem since they're all power of 2 sizes.
Rust for Xers, where X = {Ruby, Python, C, C++, Java, Go, whatever}.
What an elegant and useful post, straight to the point and immediately enlightening.
Have you tried debugging on Windows? I am having a bit of trouble finding the source file and it won't hit my breakpoints that I set :( I'm using minGW64's gdb, it seems to work fine using it from the command line. 
This is no.1 on my Rust wishlist - SO many places that could gain a big performance/ergonomic boost with something along these lines :)
If we do this we'll see an equivalently large number of complaints that everything takes forever to build.
It's always a tension: if you start just diving in and hacking away, you _will_ end up with a nasty borrow checker fight.
&gt; it took me over a month before I realised there was a language reference, Part of this is because the reference is our _worst_ piece of documentation, so I've been downplaying it. Post beta, I'll be updating it, and then I'll be referring to it more heavily from our other docs. Thanks for the thought dump!
Yeah, the distance got to be a problem, for sure. :/
Thanks for getting another set of eyes, I appreciate it :)
Thanks!
Yeah, I thought I mentioned that these were not entirely real, but then you get into virtual memory and ASLR and such...
I think the solution to this is probably just adding more documentation earlier in the book. If people know the implications of debug and release builds, they can use them much more effectively.
Problem is, the `DynamicLibrary` you're opening doesn't live past the `Ok(dll) =&gt; {}` block in that `match`, because you're moving out of it with the by-value `dll` binding. The `DynamicLibrary` dies there, and as a consequence, the function that `libinfo.func`points to is deallocated. Thus, Rust encounters an error trying to call it. (I'm curious why this isn't a segfault, though. Do function pointers follow a different deref path?) Try binding by reference instead: `Ok(ref dll)`. That won't move the `DynamicLibrary` out of `libinfo.lib` and it'll still be alive by the time you call that function. The compiler will probably require you to change the `Err(e)` arm to `Err(ref e)` as well for consistency. 
Maybe it's me that's stupid, but the RFC doesn't mention this syntax, which even compiles: fn test&lt;M: Iterator&lt;Item=i32&gt;&gt;(z: i32) -&gt; M { unimplemented!() } Now I'm not sure exactly how to return something from that function as this did not work: struct X { a: i32 } impl Iterator for X { type Item = i32; fn next(&amp;mut self) -&gt; Option&lt;i32&gt; { None } } fn test&lt;M: Iterator&lt;Item=i32&gt;&gt;(z: i32) -&gt; M { X { a: z } as M // Error: non-scalar cast } 
When I think of ownership in Rust it just comes down to: * copyable types don't care about ownership * clone() if you really need to * you can't write the resource if there is a borrow of the same resource, but read it. * you can't read a resource as long as there is a mutable borrow of the same resource. * ownership tracking is 'deep', thus doesn't stop at the type boundary, unless it's a special type like `Cell` Maybe I didn't get it completely right, but these rules have been interned by my brain so far. Personally, I found that the text takes a little too long to get to the point. In the second part, I only saw tables that I wasn't interested in. Maybe an annotated, color-coded horizontal line would do better there, as it also wouldn't interrupt the normal flow of the text so much.
Sure. They do have the idea of using a method: for (x, y, z) in (a, b, c).zip { // something } Then there are simpler namespace population issues. I think that's why they don't have `container!` macros similar to `vec!` for all the containers. Because of macro scoping rules. Methods would also allow chain and such similar things.
I didn't mind the extra detail. It's been years since I've used C++.
So... I can't create an immutable reference `x` to a mutable.... "binding"(?) `y` because it might change. Can I create a mutable binding to another mutable? What happens then, when we push "b" into our vector, and it reallocates/moves? Is `x` updated to point to the new location of "a"? Is there a way to de-allocate `x` before calling `v.push("B");` that will compile fine? Also, you switched the casing of "A" and "B" half way through the article, which is a little bit confusing.
I think this would be preferable. And it would only happen for large projects, so I think it would be less frequent.
I think this is a great idea, although it does not solve the problem for `cargo run`.
This looks good in its entirety, except... I have been saying this a lot lately, and I'm not sure I'm being extremely pedantic or almost everyone gets it wrong: **Variability and mutability are orthogonal.** A **variable** has a value that *varies* ("over time" can be considered overly restrictive), as opposed to a **constant** which always has the same value (and can thus be computed at compile-time). A **mutable** variable can be *mutated* after creation (this is the default in many languages without some notion of purity), while an **immutable** variable is *frozen* (modulo interior mutability in Rust, Haskell, etc.). A **binding** is a leaf of a pattern tree that produces a variable *bound* to the respective value (of that location in the pattern) when matching a value against that pattern. If we can't agree on this points, I might just forget about the whole thing, but IMHO it would be a shame to not have clear definitions for these concepts.
That can't work: you're asking the *caller* to specify `M`; it has no way of knowing what `M` is supposed to be.
That is very nice and clean. I'm just wondering why more stuff like that isn't already in the STL.
[I mentioned some time before](http://www.reddit.com/r/rust/comments/2vqy81/author_of_unix_in_rust_abandons_rust_in_favor_of/coke4e3) that it would be nice to have some functions that operated on more generic types than currently to allow stuff like `map(my_vec, |x| x*x)`. Any syntactic complaints should go away once we get UFCS. --- Heck, it'd be nice to have an implicit `collect`. This would involve having something like generic returns fn imap&lt;T, TIter, TIterElem, F, FOut, O&gt;(items: T, function: F) -&gt; O where T: IntoIterator&lt;IntoIter=TIter&gt;, TIter: Iterator&lt;Item=TIterElem&gt;, F: FnMut(TIterElem) -&gt; FOut, O: FromIterator&lt;FOut&gt; { items.into_iter().map(function).collect() } where iterables would have something like impl&lt;T: Iterator&lt;Item=TIter&gt;, TIter&gt; FromIterator&lt;TIter&gt; for T { fn from_iter(iter: T) -&gt; T { iter } } I realize that this way lies damnation, but it would be neat.
&gt; The length of a scope is called a 'lifetime,’ taken from that idea of time passing as your program executes. But you can also think of it as a segment of lines of code. I expected that at some point you would correct this with something like "some objects have lifetimes that exceed its lexical scope".
Ah, I was wrong then. I was wondering if there was some sort of "final form" for UFCS that auto resolved in case there was only one possible trait in scope.
Ah, I was wrong then. I was wondering if there was some sort of "final form" for UFCS that auto resolved in case there was only one possible trait in scope.
Overall, I liked it. It prompted me to actually go and generate IR for your examples with -O0 and I was shocked that Rust doesn't do any optimizations. There is an actual stack allocation for the `let x = 5` example. If Rust optimized before generating IR it would actually be FASTER to compile at -O0 and also have better performance in debug builds. That's another topic, but I am glad that this article sparked my curiosity. On the other side, it's impossible to get through. It has obvious tension between explaining C++ code and explaining heap and stack. If I don't know the difference between the heap and the stack, what's the chance that I can understand C++ code? If I know how to program in C++, what's the chance that I need an explanation on the heap and the stack? I think this is really two articles: "How Rust fixes common C++ pitfalls" and "Rust for non-systems programmers" and those two articles have mutually exclusive audiences. Beyond that, I appreciate your efforts in trying to nail down THE BEST presentation of Rust. It's really key that there's a person that is solely focused on the best explanations.
Way too verbose for an introductory guide and sometimes too in-depth. Goes from kindergarten explanations to malloc and memory structures. Like some have suggested, there should be 2 introduction. One for newbies and one for more seasoned programmers and both should be less verbose than this one. At least for me, flowery language and "guider live-commentary" don't help me learn anything. Overall I think you're doing great work. Keep it up!
Author here. Yeah, sorry about this that project was my very first introduction to rust and is using an extremely outdated rustc. Good times but I'm really glad ~is gone!!
I think it's the simple case of avoiding namespaces and importing names. For that reason methods are much simpler and there is only one way to write it as opposed to std::iter::zip, iter::zip, and zip.
I was also glazing over in the stack analysis part, although if I didn't already know it I would have made an effort and concentrated more and compared up/down to see what was going on. If the heap+stack was represented horizontally, you might fit more in, and it might be easier to compare between two different stacks (which on my screen needed paging up/down to compare). The tone kind of reminded me of Illustrating Basic, which if you don't know it was a 70s book of hand-drawn pages covered in annotations and notes explaining Basic in quite a visual way. (It was my introduction to programming.) Here's a page from Illustrating C: https://books.google.com.pe/books?id=Yll8qIrE8zQC&amp;lpg=PP1&amp;dq=illustrating%20c&amp;pg=PA16#v=onepage&amp;q=illustrating%20c&amp;f=false
Since the majority of commonly used languages (Java, Go, Ruby, Python, etc) don't have any user-visible heap/stack distinction, or else abstract it away (C#, Swift value-types) I think drawing heap diagrams and talking about pointer aliasing on the very first chapter of the language guide will drive the majority of programmers away. If you were teaching someone to drive, you wouldn't teach them how to use a hand-crank, as no cars have hand-cranks anymore. Similarly, with these C++ examples, you're educating new users about some complicated concepts, only to tell them they don't need to worry about them, as they have no place in Rust. I think starting with ownership is a great idea, it is _the_ distinguishing factor of Rust. However for a general introduction, I would discuss these features in terms of memory management without reference to C++, except perhaps for a one-liner saying that smart pointers lacking compiler guarantees are less safe than Rust's approach. In terms of borrowing, one could say that in order for Rust to determine when memory can and cannot be freed, you can only have one mutable reference to any form of memory, which gives the compiler freedom to move memory about - in the case of Vec point out that Vec needs to allocate new memory to accommodate the next value, so it needs to deallocate the old memory, but can't while there's still a reference to that memory. By the way, you never said that Vec starts out with a capacity of 1 to begin with - coming from Java I was expecting it to be about 10 or so. Incidentally, Rust's second killer feature is channels and lambdas, it would be useful for these to appear in examples pretty quickly. The C++ examples are still useful though, they'd form a great introduction to a Rust for C++ Programmers guide.
Call it `zip2(x, y)` etc then
&gt; -O0 and I was shocked that Rust doesn't do any optimizations Hm, it's not clear to me why `-O0` not doing any optimisations would be surprising, it *is* explicitly opting out of optimisations. Even clang doesn't do any optimisations, e.g. `int main() { int x = 5; return 0; }` gives this IR: ; Function Attrs: nounwind uwtable define i32 @main() #0 { %1 = alloca i32, align 4 %x = alloca i32, align 4 store i32 0, i32* %1 store i32 5, i32* %x, align 4 ret i32 0 } 
You need to look at all the traits that are implemented for `str`. Rustdoc is fairly useless at giving a summary of methods for a given type. Check out the Trait Implementations section at http://doc.rust-lang.org/std/primitive.str.html (which I think is helpfully not linked from the page you mentioned). I think the important bits are mostly in http://doc.rust-lang.org/std/str/trait.StrExt.html though.
From the original post: &gt; These variables are called by that name because they can change over time, they’re mutable. I agree with eddyb that this is a poor definition of "variable". My favorite resolution of such terminological confusion was proposed by Bob Harper: https://existentialtype.wordpress.com/2012/02/01/words-matter/ https://existentialtype.wordpress.com/2012/02/09/referential-transparency/ A "variable" is given meaning by substitution; an "assignable" is a mutable memory cell. 
There's a hypothetical extension of UFCS (not yet formally proposed) where you can do `use foo::Trait::method` and then just call `method` like a function.
That's interesting. Does git bash comes with a full set of posix tools, like `ls`, `find`, `cat`, etc? I think Cargo uses libgit2, I don't think it comes with a bash implementation.
It will, after beta. Before then, things have changed far too much and it's been impossible to keep correct.
Thanks. The primitive str doc looks like what I need. The StrExt link is also interesting. Am I missing a standard convention perhaps? &lt;thing&gt; + "Ext" contains the methods you can use? 
Here's an issue for this https://github.com/rust-lang/rust/issues/22902 Patches very welcome.
&lt;thing&gt; + "Ext" tends to be called an extension trait that holds methods that for whatever reasons can't be part of an inherent impl block like `impl str { ... }`. You can only do inherent impls for types you define, but `str` is built-in, so libstd has to do an extension trait. Built-in types are a pretty bad case for figuring out what methods are available, but it might also be tricky for other reasons like a type implementing a trait and a separate module adding more traits for all types implementing the first trait.
"Ext" is a convention for extension methods that aren't defined in the same crate as the type. 'str' is a special case because it's a built in type, so all methods on it must be defined in a trait, whereas most types define some methods of their own.
Perhaps the answer should be - on Windows, Cargo should embed the relevant parts of msysgit (specially because quite a few packages require `make`, which sometimes call standard unix tools..)
Is there a list of what those features are anywhere so some of us can help you out? There are some doc issues on github, but those mainly seem to be modifications to existing docs (not that those aren't important, just seemingly separate.)
Closures and associated types are the two big ones. Since they're just two, I didn't bother making issues.
I could see that, although I would disagree with that design decision. In Rust you usually have `use` statements pretty close to the code that uses it, so I don't think there's that much confusion/ugliness in writing use std::iter::zip; let mut it = zip(a.iter(), b.iter()); or let mut it = zip(a.iter(), b.iter()); And as it stands now you have to `use std::iter::IteratorExt` anyways, so typing-wise it's not much different.
Yeah, after reading this very nice write-up (am coming from C++ background, then Java and C#), my first thought was: What will happen when a closure closes over an outer lexicaly scoped variable binding? I thought it was good to nave the RAII comparison for the C++ folks. I suppose that comparison might be a bit lost on others not familiar with C++ destructors and how they are used in smart pointer templates, etc. The Java and C# guys have their resource cleanup mechanism of try/finally, using() (C#), and try(){} (Java 7) - and Go has deferring of closures. And I think Ruby has some sort of resource cleanup deferring mechanism. So all these languages give a variance on something similar - deferred cleanup that happens automatically, usually when some scope lifetime ends. This write-up chose C++ RAII to compare against. So how would one relate that to folks coming from these other languages? Maybe they will easily enough grok C++ RAII and that will be sufficient.
Am new to Rust and second this. Ownership/borrowing is THE Rust big Kahuna. Getting this to sink in is pretty darn essential it looks like for being able to go on and have a good experience with Rust. If this is under appreciated and not well understood, then likely frustration will come of it, and then likely disenchantment and abandonment of the effort to understand Rust. [EDIT] I tend to feel this needs to be called out and highlighted in some manner to new comers when they first approach the Rust intro documentation. That this is what is special about the language, accompanied by a punchy rationale/motivation for why all the bother, then perhaps the option to go read this particular ownership/borrowing intro (or something very similar to it) before proceeding with the broader language tutorial/introduction.
Minor typos: "And the allocation itself was a little easier, becuase Rust used the type system to figure out how many bytes to allocate, you didn’t need to calculate it with sizeof." "Therefore, our data, length, and capacity are updated: 0x03 is the next free location in the heap, so it gets our signle character, A."
That shall be a lesson to me, @deadstone got me there ;). v1.0.3 doens't use open anymore, and is equivalent to v1.0.1 Here are the `open` docs, just for completeness ➜ ~ open -h Usage: open [OPTIONS] -- command This utility help you to start a program on a new virtual terminal (VT). Options: -c, --console=NUM use the given VT number; -e, --exec execute the command, without forking; -f, --force force opening a VT without checking; -l, --login make the command a login shell; -u, --user figure out the owner of the current VT; -s, --switch switch to the new VT; -w, --wait wait for command to complete; -v, --verbose print a message for each action; -V, --version print program version and exit; -h, --help output a brief help message. 
I wonder how far rust programmers will want and see the need for more advanced types. Certainly the type system seems advanced already (I wouldn't know, but that's my impression), and it seems like there is a lot of utility in advanced types when it comes to the domain that Rust is targeting - and not just considering being able to statically show that `unsafe`-blocks are really safe, since I'm guessing that most people are OK with "having to" argue for `unsafe`-block's safety. The question is then if rust bet on the right level of "type expressiveness", or if it might be short of that goal. Based on the opinion of knowledgeable people, it might have been (or will be) simpler for Haskell to go all the way when it comes to dependent types, instead of having a bunch of extensions that each reach a little towards the same goal. Of course, all of that could be said about any attribute of the programming language design space.
shouldn't variadics be able to solve this without magic? there is an RFC for that too :)
Because it's slower to compile (generating more IR makes LLVM run slower) and has bad performance and it's the default behavior of the compiler (!!)
`powershell_mixin!` anyone? Or use the win32 port of busybox: http://intgat.tigress.co.uk/rmy/busybox/index.html It's ash rather than full-fledged bash, but it's 408 kb!
Yes, but obsolete versions thereof. The git-for-windows maintainers are going to be the last people on the planet still using MSYS1 and mingw.org compilers.
The actually really useful functionality would be "reveal in Finder / Explorer". A python implementation for some platforms of that is in Click if you want to steal it: https://github.com/mitsuhiko/click/blob/a6209d156d6d4d8af71b18a6ed3933467d57b746/click/_termui_impl.py#L410-L458
A side note: if I am not mistaken, you are, in fact, "Box"ing it in the heap memory in C# - that happens by default therefore the syntax just isn't as verbose. Here, if not using the `Box`, we have to work with exactly known size of stack memory - literally stored on function call stack. So the thing that the iterator itself refers to has to be some kind of pointer to the real thing - &amp;, Box, etc.
As far as I understand it, collect is already generic upon its return type?
Is there a downside? Or just not yet considered because it's backwards compatible?
Not sure. As I said, I didn't investigate further. It may be because the rust version is using pointer-sized integers, while the go version is using `u8`s. I don't think there'll be too much difference between rust and go in numerics code though. AFAIU go's GC won't show up, since integers aren't boxed.
Right, what I mean is that with a minor tweak your above function compiles just fine: use std::iter::IntoIterator; use std::iter::FromIterator; use std::collections::HashMap; fn imap&lt;T, TIter, TIterElem, F, FOut, O&gt;(items: T, function: F) -&gt; O where T: IntoIterator&lt;IntoIter=TIter, Item=TIterElem&gt;, TIter: Iterator&lt;Item=TIterElem&gt;, F: FnMut(TIterElem) -&gt; FOut, O: FromIterator&lt;FOut&gt; { items.into_iter().map(function).collect() } fn main() { let v = vec![1, 2, 3]; let v2: Vec&lt;i32&gt; = imap(v, |i| i * 2); println!("{:?}", v2); // [2, 4, 6] let identity_map: HashMap&lt;i32, i32&gt; = imap(v2, |i| (i, i)); println!("{:?}", identity_map); // {2: 2, 4: 4, 6: 6} } 
Sure; I'm not saying that the compiler needs anything extra to support this - I'm saying it would need to be done. It's also counterproductive if `Iterator` isn't `FromIter`.
We've submitted this paper to [ICFP 2015](http://icfpconference.org/icfp2015/cfp.html).
 main.rs:8:5: 8:6 error: cannot borrow `v` as mutable because it is also borrowed as immutable main.rs:8 v.push("world"); ^ main.rs:6:14: 6:15 note: previous borrow of `v` occurs here; the immutable borrow prevents subsequent moves or mutable borrows of `v` until the borrow ends main.rs:6 let x = &amp;v[0]; ^ main.rs:11:2: 11:2 note: previous borrow ends here main.rs:1 fn main() { ... main.rs:11 } I'm loving that verbosity of Rust!
Your pizza in 30 minutes or it's free.
Most people host the documentation on the `gh-pages` branch on their github repository. Really nice library design! I like this API much more than getopt. However, the methods `takes_value`, `index`, and `multiple` in your example are pretty hard to understand what they mean without reading the docs. For example, why would I want multiple debug flags?
That makes a lot of sense. Thanks!
&gt; If Rust optimized before generating IR it would actually be FASTER to compile at -O0 That seems likely, but doing it properly would require major changes to the compiler. Right now we don't have a self-contained intermediate representation between the AST and LLVM IR. A rustc IR would also open the door to non-LLVM backends. The most important ones in my mind are a C backend, for maximum portability, and a really fast, non-optimizing direct-to-asm backend for non-opt builds.
Not as major, but default methods are also undocumented.
This is exactly what the `entry` API is designed for: updating or inserting a value without needing two (or more) look-ups. In the briefest form (one can also use `match`) let count = map.entry(key).get().unwrap_or_else(|v| v.insert(0)); *count += 1; Inserting zero (instead of one) allows the insertion and update cases to be handled without `match`, via a uniform `+=`. (I am on my phone, so I apologise for the lack of links to docs and I'm writing the code snippet off the top of my head, it may not be 100% correct.)
Some was asking about this PE problem on IRC and [stackoverflow](http://stackoverflow.com/q/28777933/1256624) yesterday. Maybe you can draw inspiration from there.
I would love to start building some real programs using Rust. I am new to Systems programming (just some C programming back in college), but I love the fact that Rust performs like a Systems language, but feels extremely high level. However, for the moment, I would like to stick to the Windows platform, so I would like to simply develop an app on my Windows machine, and be able to share it with my friends to run on their Windows machines without any other dependencies.
chunks is nonoverlapping
Here's how I solved it: https://github.com/SSheldon/rust-euler/blob/master/p008.rs Note that this was way back on... Rust 0.11? So the code probably doesn't still compile.
libgcc.a sounds like it is from the GCC source tree, in which case it would be under GPL, meaning it can't be included in Rust executables by default. I hope the dll dependency goes away soon if it hasn't already
Yes, it feels more natural with exposure. With practice, the APIs of 'base' types like `Option`, `Result`, `Iterator`s and collections become (closer to) second nature: at least, one more often gets a feeling "I think there's a builtin function for this on this type" and then goes to check the docs to find it. We have tried to get as much consistency as possible across the APIs, e.g. the `get` method for any collection in `std` will return an `Option&lt;...&gt;`, and `entry` exposes the same sort of API for each collection where having `entry` makes sense.
&gt; Is that dependency gone? All rust binaries and dynamic libraries should now statically link without dependencies. If you find that isn't the case in any situation please lodge a bug about it (I've heard people talking about this happen; but never any concrete explanation of what causes it). &gt; Is there a way to statically bind all these dependencies? Only if you're not depending on a brain dead crate that assumes that you can just use system libraries using pkg-config... which unfortunately is basically most of the ffi crates on crates.io. If you're using pure rust, then yes. &gt; What would be the cost implications of using that? Nothing, it's the default. Static binaries are larger, and have various other specific issues (mostly regarding security patches), but there's no runtime performance penalty per se. Fwiw I have msys installed for development; I'm not sure if that's still required... but I suspect that you'll find that many of the basic crates (eg. time) wont compile without a copy of gcc installed. 
I'm not quite sure about this, but if you wanted to link everything statically that probably includes libgcc, which uses the GPL, so your one-binary programs would have use the GPL too. I don't know whether it is possible to use the Rust standard library without libgcc or with a more liberally licensed C standard library.
Have a read of https://www.gnu.org/licenses/gcc-exception-faq.html Long story short; it's a non issue. &gt; What libraries does the GCC Runtime Library Exception cover? &gt; &gt; The GCC Runtime Library Exception covers any file that has a notice in its license headers stating that the exception applies. &gt; This includes libgcc, libstdc++, libfortran, libgomp, libdecnumber, libgcov, and other libraries distributed with GCC.
Because `&amp;str`'s equivalent is `&amp;Path`, not `Path` and it's a slice (doesn't own its data). Just like you need `String` if you want to store a string in a structure that has to outlive any temporaries, you will need `PathBuf` in the same cases.
That's me :). Functional solution should be useable.
Kinda off-topic: I am looking for a CLI arg library that allows the user to specify *subcommands* in the way e.g. `git` has them. Most libraries I've encountered (in Rust and other languages) don't seem to think of this usecase at all, one of the few that really satisfy my needs is [click](http://click.pocoo.org/) from the Python world. Is there something obvious I am missing? Because I definetly have that feeling. I've looked at Cargo, but that one seems to reimplement half of the things that I'd expect such a library to do.
This is fantastic! Are you currently leveraging any remnants of the task isolation stuff to make the browser more resilient against failures?
Yeah, I guess I could do that, but that would require setting up the `fs` segment to have a different base since I don't want to map page number 0 (NULL access should cause a page fault). But that's doable.
Why not just use `map()` to get the product and then find the max out of that set? `max_by()` would return the largest value of the original set, not the value used to determine the maximum, so you would get an `Option&lt;&amp;[i64]&gt;` containing the set of numbers that made the largest product instead of a `Option&lt;i64&gt;` containing the largest product out of your current set up. Either way, rustc has trouble determining the output of the calculation, so you need to annotate it with `: Option&lt;i64&gt;` let largest_product: Option&lt;i64&gt; = input.chunks(4). map(|cs| cs.iter().cloned().product()).max(); EDIT: I just realized this still doesn't solve it since it only checks chunks that are aligned with the input, ie. chunks that start on index 0, 4, 8, etc.. I'll leave figuring out how to fix that to you :D
Awesome thanks! I'll put the docs up there so its easier for people to find and see. I think the traditional multiple flags example is `-v` for verbosity. I didn't use that since `-v` is automatically associated with version (unless you specify your own `-v`, in which case only `--version` gets automatically associated with version). Also, if you or anyone has better ideas for names that are more standard or easier to grasp at first glance, I'm not against considering better ones as this is so new ;)
Can't you just do: let mut word_map: HashMap&lt;_, i64&gt; = HashMap::new(); ... ... ... match word_map.get_mut(&amp;key) { Some(v) =&gt; *v += 1, None =&gt; word_map.insert(key, 1), }
&gt; `let count = map.entry(key).get().unwrap_or_else(|v| v.insert(0)); *count += 1;` How is that different from doing: match word_map.get_mut(&amp;key) { Some(v) =&gt; *v += 1, None =&gt; word_map.insert(key, 1) } It seems we avoid the hashing/traversal process twice, right? 
At least in 64bit mode, fs and gs are special and you just need to set the base address (0 by default). You can do this through the model specific register `0xC0000100`. If you don't set the fs base address you would write to 0. I think it would work (there is nothing valuable at address 0) but it's hacky :).
When I did this, there was no rustc option for no-stack-check, but what I do is: * compile the rest of the kernel to llvm bitcode * extract the llvm bitcode from libcore.rlib and decompress it * merge the kernel and core bitcode with llvm-link * disassemble the linked bitcode with llvm-dis * edit the llvm-assembly with sed to remove the stack-check attributes * reassemble with llvm-as * optimize with opt * compile it to assembly with clang * edit clang's assembly to remove alignment directives (not related to libcore, but it saves a lot of bytes) * assemble that to an object file that can be linked with the assembler parts to produce the final kernel BTW, this is nothing I advocate as a good or sane solution. But it sort of works :)
For one, that code doesn't even compile because the compiler isn't smart enough to handle it. :) But yes, the primary motivation is to avoid a double-search.
This will perform two searches where only one is also necessary. This particular code also won't compile under the current borrow-checker.
I have just posted a new RFC that would make this code work: *map.entry(key).default(0) += 1; https://github.com/rust-lang/rfcs/pull/921
+1 on that. Reminds me of a more powerful version of `setdefault` in Python which I really like :)
You could remove the stack checks from compiled libraries with a simple binary find/replace on the `.text` section.
Adding to what Lars said, we're also [transitioning](https://github.com/servo/servo/pull/4735) to a [multi-process, sandboxed architecture](https://github.com/pcwalton/gaol). That reduces the need for failure isolation within a process, but we're not sure yet if we can do entirely without. Regarding a Servo-specific green threads library, people seemed to favor (at least 4 or 5 months ago) a task queueing system like [Grand Central Dispatch](https://developer.apple.com/library/prerelease/ios/documentation/Performance/Reference/GCD_libdispatch_Ref/index.html).
Thanks for the detailed explanation!
Brilliant! 
I added a citation of a paper published in "Munich, West Germany" one year before I was born. I've been frustrated with how long it takes "proven techniques" to make it into programming languages that people use for real products, particularly in the systems domain. Rust is a big step forward; not just for regions and affine types (which I'd hardly consider "proven"), but for stuff like pattern matching and hygienic macros, which have been well-known as useful features for literally my entire lifetime.
I'm fairly certain docopt can do this.
One question to your solution for #10 - wouldn't it be better to limit n to 2 million before you check for primes? Checking all natural number for i64 for primes, then throwing away 90% of them in the next step seems a bit odd.
As I replied to /u/burntsushi, an advantage of declaring your whole CLI in Click is that it automatically generates help output for you. For the [naval_fate example](http://try.docopt.org/?doc=Naval+Fate.%0D%0A%0D%0AUsage%3A%0D%0A++naval_fate.py+ship+new+%3Cname%3E...%0D%0A++naval_fate.py+ship+%3Cname%3E+move+%3Cx%3E+%3Cy%3E+[--speed%3D%3Ckn%3E]%0D%0A++naval_fate.py+ship+shoot+%3Cx%3E+%3Cy%3E%0D%0A++naval_fate.py+mine+%28set|remove%29+%3Cx%3E+%3Cy%3E+[--moored|--drifting]%0D%0A++naval_fate.py+-h+|+--help%0D%0A++naval_fate.py+--version%0D%0A%0D%0AOptions%3A%0D%0A++-h+--help+++++Show+this+screen.%0D%0A++--version+++++Show+version.%0D%0A++--speed%3D%3Ckn%3E++Speed+in+knots+[default%3A+10].%0D%0A++--moored++++++Moored+%28anchored%29+mine.%0D%0A++--drifting++++Drifting+mine.%0D%0A%0D%0A&amp;argv=ship+--help), implemented in Click: * If you type `naval_ship --help`, you get a listing for global options, and subcommands. * If you type `naval_ship foo --help`, you get the help for the `foo` subcommand. And so on. With docopt, you get the same help string every time, while with Click, the user can more precisely define what they want to learn about.
I suppose that's what I'm going to do for now, but I wouldn't call that docopt taking care of it.
The problem here is that the name of the crate is the name of the file, and spaces are not permitted in crate names. Maybe you could use underscores. EDIT: this is my guess, maybe it is just a bug...
You'll also have to adjust offsets used for internal addressing in the generated code.
My file is "Task10 functional programming.rs". I don't use crates (are they generated in the background? Just made a new file with that filename in the editor, that's it).
If for some reason you can't change the name of your source file to a valid crate name, you can provide one for rustc with the --crate-name option. rustc hello\ world.rs --crate-name="hello" But I wonder: Does a bin crate really need to have a valid crate name? 
[Bees!!](http://i.minus.com/ibnYj76t0JezJQ.gif)
[NOT THE BEES!](http://25.media.tumblr.com/tumblr_lzdjy2aDiu1r2pxgdo1_400.gif)
By analogy with `String`, perhaps we should call the owned version `Pathing` instead ;)
I must kindly ask that you please not go around telling people to disregard the rules of our community. Violations of Rule #6 will absolutely not be tolerated.
Yeah, one core principle of crates.io is that once published, packages cannot be changed or removed. This rule ensures that if your app builds today, it'll continue to build forever (at least in theory).
I know it the other way arond - 1 column PDF is the standard and the reader has the ability to show 2 columns at once.
[THE BEE GEES?](http://i.imgur.com/JCjT1G3.jpg)
I was thinking `NOP` them out which, true, doesn't remove *all* overhead. On the bright side, you have a guaranteed place in each function to add jumps for [Ksplice](http://www.ksplice.com/doc/ksplice.pdf)-style hot-patching!
Yes, a crate is a unit of compilation in Rust, so a single file is just a very basic crate.
Yeah, I realized that's what you meant after I posted. I tried to delete my post, but Reddit apparently didn't follow through.
You should be careful about claiming or implying that Rust eliminates race conditions - it doesn't. It may eliminate torn reads/writes, which may *help* reduce race conditions if exploited carefully, but that's not the same as eliminating race conditions. After all I can easily set up my own app specific invariant that X and Y should always be equal and then update X and Y separately (using messages, or locking or whatever), but if I get pre-empted between these two updates I have a race condition. Even STM wouldn't be enough to *eliminate* race conditions, but at least there you have the option to specify that two things should happen atomically. Rust doesn't even have that so I really think that making any grandiose claims about race conditions is stretching it a bit. Rust is doing ok there, but it's by no means amazing. 
I don't see this claim, all I see is a claim about eliminating data races. Rust _does_ eliminate data races (in safe code). 
Only if you use a very limited and unorthodox definition of "data race" (basically only torn reads/writes but no other forms of data races). You can most definitely get data races in Rust if you use multiple values, each one may be successfully written/read but if they have invariants between them that need to be maintained then you're on your own. EDIT: in fact you can get race conditions with just a single value as well, of course. E.g. the usual "x++" issue, if you forget to lock the value for the duration of the read/update cycle.
The moment we start talking about invariants it's a race condition, not a data race.
Using [`windows`](http://doc.rust-lang.org/1.0.0-alpha.2/std/slice/struct.Windows.html) does the trick.
The definition of `data race` we usually use is a write from one thread occurring concurrently with a read or write from another thread on the same location in memory and at least one of those operations isn't synchronized (with an atomic, for instance). This is memory unsafe, and causes undefined behavior in Rust. Incorrect semantics (but not memory unsafety) as a result of improper logic surrounding the use of concurrency primitives or synchronization is not a data race under that definition, it's some other problem.
Just one random link: http://stackoverflow.com/a/18049303/1198729. And http://docs.oracle.com/cd/E19205-01/820-0619/geojs/index.html (It also mentions that they aren't subsets) For `x=x+1` to be racy, in Rust you'd need to do some strange gymnastics like: let x = { let y = mutex.lock(); y.clone() } // .... let y = mutex.lock(); y = x + 1; The locks here are explicit, this is similar to writing `*NULL` in C++ and complaining about segfaults. This entirely depends on the requirements of the code in place -- in any language you _need_ the ability to lock, read, unlock, and write again from the same thread. The raciness of this depends entirely upon the requirements of the programmer (and anyone writing `x=x+1` this way probably wanted it to work that way).
It's a trivial example that rust allows data races. Of course in reality they would be a lot less obvious than that. The fact remains that Rust doesn't eliminate data races. It eliminates unsynchronized access but that's not enough. EDIT: those links don't seem to support your claims. You can read/write shared data even with locks, the race comes from the fact that you don't keep the lock on the data long enough for the write to be valid when it happens. Rust allows that to happen.
Neither Java nor C++ enforce it I think. Rust has it built into the type system. The idea of "synchronous types" and "threadsafe types" is a novelty. In C++ I am free to send a struct with some pointers in it across a thread. In Rust, I can't, without mutexifying things. As far as most data not being one memory location goes, the RAII mutex works fine for complex structs since the lock is only released when you lose access to it.
I'm probably being terribly naive here, but what is wrong with `rustc libcore/lib.rs -C no-stack-check`?
Agreed, without aliasing you need multiple locks to get something racy, and there's no way a _language_ can prevent you from doing that since there are indistinguishable legitimate use cases for locking, reading, re-locking, and writing.
It _seems_ like D is opt-in thread safety, with `@safe` annotations. Haven't looked into it. There's a major distinction between opt in safety and opt out safety. Most languages can achieve opt-in safety (eg unique pointers in C++), but you still have to worry about programmers forgetting. Opt out safety, not so much, since the opting out is explicit.
Hm, I'm not so sure. They aren't all the same. They look like this: cmp %fs:0x70, %rsp ja &lt;label&gt; where the label is somewhere else in the function. It might be better to just edit the LLVM bitcode, since rlib files contain that too.
A singly-linked-list is a super-easy starter collection. Easy logic, easy ownership. But there's some subtle details (like being able to blow the stack with a default destructor). A splay tree is a great one too. The actual abstract logic is really simple and the ownership is clear, but you need to do some non-trivial work to get the borrow-checker happy. Writing `rotate` in particular was a great learning process for me. Note that you can work your way "up" to a splay tree by implementing an unbalanced binary search tree. Just do `insert`, `remove` and `get`. I *expect* a BinaryHeap to be pretty simple to write, although potentially not very instructive. Also if you want a blog on collections in Rust, well shameless plug: http://cglab.ca/~abeinges/blah/ :)
bees.
I actually didn't use Rust's benchmarking system. Just two source files, timed with bash's time built-in. 
Nickel looks fantastic. I can easily define routes that should be sent as a request to a Nickel server on another port, get the response back and send it through my server. This will work great since it is another Rust project! Thanks for sharing!
It's uncanny how giddy I get when I see a new TWIR post show up. For there not to be new RFCs or massive breaking changes is a bit disappointing but also exciting to think about! :)
Have you ever looked at Azul's vm? Everything I've heard about the C4 GC makes it sound like magic. Of course, I was reading what Azul wrote about it...
I think `vec![0u8; N]` is created at compile time, while the `iter` version isn't, so the comparison is not fair and does not answer the original question (loading a file cannot be done at compile time).
FastCGI is way too complex for its job, IMHO. Most web servers support SCGI now. I'm serving web content with it NP, cf. https://github.com/ArtemGr/rust-scgi (The readme example might be a bit outdated, sorry about that). There is also an alternative SCGI package: https://github.com/kstep/rust-simple-cgi A downside of SCGI is that for simplicity reasons it would close the connection after each request, which makes SCGI server slower than a keep-alive HTTP one. (Arguably a file socket connection should be cheaper to setup and teardown but I haven't tested it with this particular SCGI driver yet). Otherwise I'd be recommending SCGI over HTTP since having two HTTP parsers is an overkill.
I've been working through https://www.coursera.org/course/algs4partI in Rust (instead of Java, as they expect). Not only is the course good as a refresher, but I've found many of the assignments (some samples: randomized queues, union find, solving the 8-puzzle, finding colinear points) to be excellent for trying out many of Rust's concepts (ownership, traits as well as more trivial things like using other crates like getopts and rand) in practice. The great thing about doing them as separate little projects is that you don't have to worry about doing it "right" (code style, best practices, etc) the first time. You can probably find my code on github if you search for it (or pm me), but I won't link to it here to uphold some semblance of the Coursera honour code.
I know that issue, but I found that the issue had been closed, merged 18 days ago.
I can't really say etiquette wise, except that I guess you are free to do it unless the license says you can't, but I think that it can be better from a usability perspective, depending on how the libraries are supposed to be used. Does your library substitute the other library (similar to `std` substitutes `core`)? Then I think you should reexport all the necessary objects to make them accessible from the same crate and appear together in the documentation. I don't think the user should need to include another library just to make your library usable. Is the user supposed to use both libraries together? Then it may be better to not reexport them to make it obvious that they are the same objects. The user would depend on the other library anyway, so there is no need to make it look like they are independent from each other. There are, as usual, always exceptions to these rules, so follow your heart and judge by the situation.
This allows patterns like: use threadpool::ScopedPool; let mut numbers =[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]; // We need an extra scope to shorten the lifetime of the pool { let pool = ScopedPool::new(4); for x in &amp;mut numbers { pool.execute(move|| { *x += 1; }); } } assert_eq!(numbers, [2, 3, 4, 5, 6, 7, 8, 9, 10, 11]); EDIT: make code more rusty
I just went ahead and created some space for a zero qword and set up `FS.base` to point to that minus 0x70. It works well enough.
Total noob here, just real quick, I understand what borrow checking is I think, but which variable represents the borrow that the compiler doesn't allow in that code? 
I've considered using SIMD, but I don't know if Rust's SIMD API is up to it yet. And yeah, that's a good page. One of my issues though is how to parallelize it best, since it uses shared memory heavily. I've got a couple ideas I want to try though, and I'll see which performs best.
I don't know that it's actually best practice, but here are some examples of popular crates using this technique: [iron](https://github.com/iron/iron/blob/master/src/lib.rs#L94), [rust-event](https://github.com/reem/rust-event/blob/master/src/lib.rs#L19), [rustful](https://github.com/Ogeon/rustful/blob/master/src/lib.rs#L19).
Ah great, cheers!
We've run some tests with Azul. The C4 GC really is wonderful, it lives up to the marketing hype. The only problem is the cost...most people aren't willing to shell out the money for it (which is pretty short-sighted, since they end up buying more hardware to fight the GC issue anyway). I think we ran into some bug-related issues, but the license they gave us expired before we could investigate too closely. Last I heard we are still trying to get a new license to start testing again.
Remove the `pub mod oci {}` block, bringing everything in it up to the crate's top level. At present, when you do `extern crate oci` in your main program, you end up with `oci::oci::whatever` - the first part is the crate's top level, not a module inside the crate.
That was going to be my point too - it doesn't appear to have decided what the audience is. I like the idea of explaining the borrow checker up front, but it's too long a section to go in the book. How about this. Skip straight to the last main Rust example: fn main() { let mut v = vec![]; v.push("Hello".to_string()); let x = &amp;v[0]; v.push("world".to_string()); println!("{}", x); } Then explain the hazard here in terms of aliasing, but in a briefer higher level form than the full pointer explanation. Then follow up with the remainder of the guide that explains what Rust does and how the borrow works. That will explain to both newbies and C/C++ veterans what's going on, without taking up a load of time on pointers (and that aspect of it will be understood by the C/C++ types anyway). The only thing I'd say for their benefit is that the implementation of the reference is a bare pointer and all the magic is in the compiler. 
So get_key() borrowed key and required that the lifetime of its borrow on key out lives the lifetime of the result it returned. So we can't refer to key again until the reference we retrieved is out of scope. It seems there ought to be some notion of shareable, read-only borrowing given that immutability is the default... 
What about Timers, by the way? Any alternative for [this](https://doc.rust-lang.org/std/old_io/timer/struct.Timer.html) yet?
&gt; for x in &amp;mut numbers [..] { Why the two dots? I'm assuming pool.execute is spawning a green thread (or whatever you guys call them) and returns control to the for loop immediately when the current number of outstanding threads is &lt;= 4 but otherwise blocks? The idea being that by adding the extra scope allows the compiler to free all the items in the extra scoped region as soon at the pool is done? Why is that important? 
https://github.com/brson/heka-rs
This isn't ready for public consumption yet; the API has issues preventing use on Windows, and I'm going to have to redesign it a bit as a result. Also, it has not had security review, so please do not use in your own projects right now. I'll do a real announcement when it's ready.
It is important because Rust can ensure that the threads have been joined when the pool is dropped. As Steve pointed out, this means that the borrows are released and you are able to use numbers again. A normal thread pool would never release the borrow, because it doesn't know when the worker threads are going to be joined. The thread pool works internally with channels. There are 4 threads waiting for work (they use a `Receiver`) and each time you call `pool.execute` the closure is sent to one of the threads.
thanks!
What would be the syntax if the trait has type parameters (eg `Trait&lt;T&gt;`)? Something like this: `fn&lt;T&gt; Trait&lt;T&gt;::method(&amp;self) { ... }` or `fn Trait&lt;u8&gt;::method(&amp;self) { ... }` for concrete `impl`s? EDIT: What if the trait has associated types?
I'm not saying we should remove the old syntax, which might be needed for more complicated scenarios. And if a trait has a lot of methods, writing `Trait::` in front of every method might be more code than just one `impl Trait for Foo`. But spontaneously, I don't mind e g `fn Iterator&lt;Item=i32&gt;::next(&amp;mut self) -&gt; Option&lt;i32&gt;`.
A few questions: 1. Why is it necessary to do `for x in &amp;mut` when `numbers` is already `&amp;mut`? Could `numbers` just be `[u32; 10]` instead? 2. Why is it necessary to do `numbers[..]`? I was under the impression that `&amp;mut` was already creating a slice.
You might want to read this thread from two days ago: http://www.reddit.com/r/rust/comments/2xearb/returning_an_iterator/
Thanks... good to recall that Rust had already been given a test drive at media decoding. I'm trying to understand, is it (or why, if not) feasible to do the same for next generation of media codec(s). I'm pretty convinced on the possibility part, and have trust on that.
I don't like the idea because for more complex implementations or more than one method at once you still need the old syntax and I don't want more special rules in the already bloated trait system.
You are right in both points. I am going to update the code snippet.
In this case, the extra scope means that the thread pool is destroyed at the end of that scope (so the lifetime of the pool ends there too).
I'm imagining Servo supporting Rust apps instead of just JS ones, how cool that'd be. 
...and also [option::take](http://doc.rust-lang.org/nightly/std/option/enum.Option.html#method.take), which can be easily implemented with those, but allows to write very nice code most of the time!
Unfortunately this is a simplified example, the actual example has the same lifetime problem, but the equivalent to `subset` cannot be represented that easily. I think it should be possible using `Rc` or unsafe code, but I was wondering whether there is a safe, zero-cost way to do this in rust.
&gt; AFAIK, this isn't allowed because you can't move the vector into the struct while the slice is still alive. I was having an issue similar to this, but could you elaborate on why this is? I ended up just moving to a different design but I'm still hazy as to *why* it won't work. Like the OP it seemed like it should work to me, but I'm not sure what you mean by move the set into the struct? You mean when initializing?
&gt; I'm not sure what you mean by move the set into the struct? You mean when initializing? Yes. let set = vec![1, 2, 3]; let my_set = Set { set: set, subset: /* assume this is initialized */; }; `set` gets moved into `my_set`, such that the struct takes ownership of the vector. It gets copied to a different address on the stack. Take the following example: let my_val = 1; let my_ref = &amp;my_val; takes_my_val(my_val); // `my_val` was consumed by the previous function, // `my_ref` now points to garbage and this is a segfault (at best) or undefined behavior (at worst). let my_val = *my_ref; Rust's borrowing framework does not allow you to move a value after taking a reference to it because it will cause that reference to dangle. Simply *moving the value* will do this because it gets copied to a different address on the stack. Since vectors and slicing are implemented on top of this framework, Rust is forced to make the same assumption even though you might be able to prove it is safe. However, as /u/thiez explained to /u/vks_, it can't be proven 100% safe.
Ah ok, I see what you mean now. I was thinking `Set.set` would never change, and therefore "Why wouldn't `Set.subset` be legal?" but now I see what you're saying. Thanks!
Thanks!
&gt; Integer overflow/underflow. It is still an open problem to provide optimized code that checks for overflow or underflow without in- curring significant performance penalties. The current plan for Rust is to have debug-only checking of integer ranges and for Servo to run debug builds against a test suite, but that may miss scenarios that only occur in optimized builds or that are not represented by the test suite. Would it be feasible to also run optimised builds against the test suite?
I think it's a necessity to test both debug and release builds. We could add a third build flavor which enables optimizations but keeps the overflow checks. I'm not sure how valuable that would be, since the presence of the checks will affect optimization dramatically. Like most mistakes you can make, there's no single technique that will catch every overflow problem. We'll perhaps want a combination of [static checks](https://capnproto.org/news/2015-03-02-security-advisory-and-integer-overflow-protection.html#proving-safety-through-template-metaprogramming), debug-build assertions, and instrumentation of unmodified release builds using something like [Pin](https://en.wikipedia.org/wiki/Pin_%28computer_program%29) or Valgrind.
In a recent talk on `std::io` I saw (it was on the front page maybe a week ago?) they said they did not currently have one in part because of the instability of the `Duration` type and (I may be wrong on this second part) the differences between OSs in how timers were handled. If I remember correctly, they needed to spawn another thread to have timeouts on Unix.
Sure, you should be able to do this with a raw pointer and expose it as a slice. But I think you asked for safe and zero cost.
Reminder for `uninitialized()`: in general you should overwrite such a value using unsafe code, `std::ptr::write`, to be safe (if the type has destructors).
I think for end-users the plan was to allow something like `cargo install &lt;app&gt;` but I forgot where the discussion was.
No. Syntax sugar is supposed to make things easier to read, not simply shorter. The 'one way' refers to the easy-to-read part, not some obedience to form.
Given that Rust executables aren't dependent on any preinstalled infrastructure or runtime they can be distributed through existing binary distribution methods such as the OS package manager on Linux, MacPorts or Homebrew on OS X, and Chocolatey on Windows.
I brought this up in the /r/programming thread, great minds think alike :)
For others who probably know the answer to this: here's a playpen link to a program which runs and demonstrates this problem: http://is.gd/WA3tNT Edit: to be clear, I don't really understand what's happening here, I just added use statements and a main function which uses instance().
I'm pretty sure a rust library accessible to the whole system is what is meant by system libraries. I know that pip, at least, does by default install python libraries for the whole system.
I made a simplified test case on the playpen showing the `ONCE` closure only gets called once even for different versions of the function: http://is.gd/rrYRSq I didn't examine your original code closely, but I think this is an issue (it may be a bug in rustc). EDIT: I don't think it's a bug. `static` inside a function in Rust is only a namespacing thing, I think, unlike `static` in functions in C/C++. It's equivalent to if you put the `ONCE` outside the function, which makes it clear this will only be done once for all versions of the function.
A lot of the haskell gsoc projects have been infrastructure projects, you dont have to have a maths phd to contribute meaningfully on those :)
https://github.com/rust-lang/rust/blob/master/src/test/compile-fail/inner-static-type-parameter.rs i don't think it is a bug. maybe it's a feature, just like a bug....
An `Option` value can be turned into a single pointer by the ["nullable pointer optimization"](https://doc.rust-lang.org/book/ffi.html): &gt; a generic enum that contains exactly two variants, one of which contains no data and the other containing a single field, is eligible for the "nullable pointer optimization". When such an enum is instantiated with one of the non-nullable types, it is represented as a single pointer, and the non-data variant is represented as the null pointer. So let t = Option::None; ffi_function(t); is roughly equivalent to let t = std::ptr::null_mut(); ffi_function(t);
I guess it's time to switch over to Yahoo! Summer of Code. But seriously though, Google pulled a cheap move here. Rejected an organization that has been a part for 10 years. It was my last chance at a GSOC before I graduate. This time had everything in place except luck :(
It doesn't. Have a good read of https://docs.python.org/2/distutils/setupscript.html Pip will install dynamically linked *extension* libraries, which link to *system libraries* which must already be present on the system. It's fair to say pip installs global compiled copies of the python library, yes, but the issue here is distributing built applications, including the system libraries they depend on, which is entirely beyond the scope of what either npm or pip do. 
Well, you don't fuck with Google. This is obviously a retour for mozilla switching to other sponsors / advertisers 
Even for the hard bugs, a maths PhD is not going to help you especially much working on GHC. There are parts that involve theoretical CS, but the same is true of rustc or any other interesting compiler. There are other parts that involve fighting with linkers, tracking down mis-optimization, writing concurrent assembly code, and other low-level systems programming skills. Just because some people use Haskell as a syntax to talk *about math* does not mean that using Haskell *as a programming language* requires deep math knowledge. This seems to be one of the falsehoods about the language that is jointly spread by detractors / trolls and by overenthusiastic beginners.
Maybe they ran out of money and/or goodwill.
Yeah, sorry. That wasn't meant as an accusation against you, but I could see how that's not clear.
I like it, especially the fact that it is entirely safe. Although it has an unfortunate limitation: the thread pool has to be created after anything it maps over (as the extra scopes required hints at). In particular, it means one cannot, for example, create a "global" thread pool and then use it (and reuse it) to map across vectors created somewhere down the stack: a new thread pool has to be spawned for each vector even if a previous one could theoretically be reused. I wrote a [thread pool](http://huonw.github.io/simple_parallel/simple_parallel/pool/struct.Pool.html) for my `simple_parallel` library that tries to work with "short-lifetime" jobs, while still ensuring safety. It basically temporarily borrows the pool's resources into some subscope (the executing functions take `&amp;mut` and returning a `JoinGuard`-like object), ensuring that the job must be finished before leaving the scope. However, my approach has a few downsides: it's not possible to implement to be flexible without requiring `unsafe`, there's a million channels all over the place, and its not possible to incrementally submit different jobs to the pool at the same time. The only safe operations it exposes are `for` (run a closure on each element of an iterator) and `map` (parallel version of `Iterator::map`) and both of these occupy the whole pool: one cannot do the `for ... in ... { pool.execute(...) }` possible with `ScopedPool`. Different trade-offs! (Also, my thread pool has a ridiculous amount of synchronisation overhead at the moment.)
i report it to dev team. they said they are working on this problem. https://github.com/rust-lang/rust/issues/22991 thank you very much.
It's surprisingly easy to get into GHC development. There's a lot of grunt work that not enough people have time for. Checkout the Newcomers page: https://ghc.haskell.org/trac/ghc/wiki/Newcomers
yes, mutli-threads modifying one data is not safe. but reading datas is thread safe. i thought about using type name mapping a type instance to solve this problem, but this is not what i want. generic functions sharing same static variable is not right. if the rust-lang won't change this situation, i will go back to write C艹/C井.
`PhantomData` is what you want here. use std::marker::PhantomData; struct Foo&lt;'r&gt; { ptr: *mut Internal, dummy: PhantomData&lt;&amp;'r ()&gt; } 
Then a singleton is overkill, simply have a global static non-mutable reference. &gt; generic functions sharing same static variable is not right. I agree, but the problem is that you are using `unsafe` code. If you want things to work "right" use safe code. It'll prevent you from doing some things, but guess what: they never were safe. &gt; i will go back to write C艹/C井. Understand that Rust is a completely different language. You can't just write C++ and have it also be safe. The whole problem is that there are some things you do on C++ that are unsafe. Rust forces you to realize that you are probably doing something dangerous by forcing you to use unsafe blocks. So lets see what exactly is happening, why you cheated, and why you got bitten in the ass: * A static variable is static. It means that it's a global-level variable and is always the same. The only thing you are doing by declaring it inside the function is hiding it, but all function instances are actually accessing the same static variable. * Rust probably wouldn't let you do something like `static mut ptr: *mut T. So instead you went around the compiler and made it a `*mut u8`. Doing this in C#, C++ or pure C is considered dangerous as hell and you'd better have a real good justification to do it other than to code how you wish, and not how the compiler needs. * Then you implicitly recast the pointer type to what you need and then cast it back. Honestly did you just want to use `void*`? Because that's what you are doing, actually it's worse than using `void*`. * Suddenly we are surprised when you learn why the compiler was telling you that you were using static variables *wrong*: it's always the same address no matter what. Here's the thing about static variables: the compiler sets the space for those at compile time, and they are set by the module. Generics cannot create new statics because statics are expensive and messy. What we need is a static that is actually a pointer to a heap-structure that grows as needed. That's why I proposed a `TypeMap` as something that could be used above. But again I'll ask you: are you doing this the right way? If a Javascript programmer started coding C++ and had callbacks everywhere and coded in a way that is very efficient if an event loop were happening, would you say he's right in saying that C++ is stupid because it block all the time waiting? Or would you berate this person for not understanding the language at all? So here's the thing: generic functions will share the same static variable. This is why a static variable cannot have a generic type. If you don't want this to be so then don't use a static variable. Everything else will be unsafe. I know that in your mind you don't see it, but down the road it will be. That's the whole idea behind Rust, all those hacks that seem like a good idea ultimately *are wrong*. Using statics within generic functions is almost always *wrong* and the few ways on which it technically isn't harmful, it's mostly useless. Trust me you don't want to use a static variable within a C++ template function either (since the body has to be included in the file, this means that every file will have it's own instance of the variable, which is not what you want either). I'm sorry but that's just how Rust works, you need to be able to keep certain things true. If you can't your code won't compile. If you use unsafe to make it compile, then you are understanding the whole thing wrong. Don't obsses over making your solution work, focus on solving the problem in the best way within the language. Give me the reason for these Singletons and I'll give you the solution you should use in Rust.
&gt; This is obviously a retour for mozilla switching to other sponsors / advertisers That's conspiracy theory land. They rejected other orgs. For one reason or another the orgs didn't meet their criteria.
Rust standard library is being cut to the bare minimum in order to reach the API stabilization goals. Most of the more experimental stuff has moved into crates. There is a trie implementation in the crates, I think: https://crates.io/crates/collect
As I understand it, `PhantomData` doesn't actually do anything with its type parameter — so it would essentially make no difference in this case.
Also a nice resource: [Github's trending Rust repositories](https://github.com/trending?l=rust)
It doesn't do nothing, it uses it for variance, but maybe that only matters for type parameters.
`AsciiExt` is also implemented on `[u8]`. Try: let mut sg = String::from_str("abc"); let s: &amp;mut [u8] = unsafe { &amp;mut sg.as_mut_vec() }; s.make_ascii_uppercase();
and instance::&lt;AtomicInt&gt;() return a const AtomicInt reference??
yep.... it's not like cpp. amazing, &amp;AtomicInt CAN modify its value.
I don't quite get what you mean? What's disappointing there? I found those project ideas quite interesting, and for the most part doable.
Would love to see a video with an experienced programmer talking out loud his thoughts while composing a simple rust project. Will check out your URLs! Thank you all.
At least `PhantomData&lt;&amp;'r Internal&gt;` is self-documenting, I'd use it even if it's not strictly necessary.
I also have an implementation of [suffix trees](https://github.com/BurntSushi/suffix) in a linear time (by creating the suffix array first). Although, it has about 6 bytes per character of overhead (for suffix array; suffix tree is a lot more), so running it all on one 50GB dump of text probably isn't going to work unless you have a monster amount of RAM.
How common is this in the wild and how big (in terms of HTML bytes) are pages that use this? It appears to me that parsing HTML and Javascript in full, and backing off on document.write would be a sensible way to go.
As I understand, Y0 is saying that Rust users would be disappointed that Mozilla was rejected for GSoC 2015, because GSoC 2015 projects such as rustfmt are exciting to Rust users.
I have no idea of concrete numbers, but I believe that the idea that many pages don't use `document.write` is what motivates speculative HTML parsing (and why it is "speculative"--the result may have to be discarded-- rather than just " parallel" or some such). :)
For what it's worth, this is a *ton* of work. I have a couple binaries I'd like to distribute. (I'm serious enough about it that I bought a Mac mini just for the sake of compiling. But this solution does not generalize because a Mac mini is expensive! Even if used...) Then packaging it up for all the major Linux distros? And Windows too? It will literally take me days just to setup the infrastructure to compile binaries for each of the three platforms and distribute those on my own. Nevermind trying to get them into all the different package managers... Having a simple `cargo install` command is a *great* stop-gap measure because little guys like me can take advantage of the fact that somebody else will probably have already put in the work to get `rustc` and `cargo` packaged and distributed The Right Way like you've suggested here.
Nope. Well, doc comments, yes. But the comments can be reverse engineered from the spans using the codemap. Or, just add a mode to the parser that generates a different AST, including comments.
Ok. But what if there is a type parameter instead? Then we should use `PhantomData&lt;&amp;'r T&gt;` right?
Correct. I'm a bit disappointed that Mozilla isn't in GSoC 2015, because those projects will probably be postponed for later date. I know people might not choose Rust/Servo, but I assume at least one participant would use Rust.
As were the D-lang folks: http://forum.dlang.org/post/hxcukeixfratfapolxyi@forum.dlang.org
Not sure if `if let` passes that test.
It's the same synth that was used to make all the sounds in [these generated songs](https://dl.dropboxusercontent.com/u/13703122/JenDemosAprilMay.zip). The one in those demos is the old c++ version which is identical feature/sound-wise to this rust one. I'll make a little "synth_editor" soon using conrod and have it so that you can both save/load synths as well as render out the instruments, so I'll share more sounds then when it's easier!
If you use `document.write()`, your page deserves to be shitcanned by the browser. Abort everything, re-do with some utterly unoptimised implementation (try a reference one), sacrifice some kittens to get a curse on your ass.
Put very generally, a type that drops at the end of its scope will only drop every data that it owns. But references explicitly do not imply ownership, so whatever happens with that `&amp;'static str` field will not affect the `str` value it points at. Named lifetimes used inside your struct will never change the semantic of your type, it will always drop at exactly the same points.
It's following a similar pattern to other Eclipse IDEs: JDT (Java Development Tools), DDT (D Development Tools). True, if I followed the pattern strictly it would be called RDT, but there is also an R language (statistical computing and graphics), and also I wanted "Rust" in the name.
&gt; Which one drives people less mad? If we're talking about that feature being added to the already existing `cargo` I'm all for it. I think this should absolutely be an option. But I *also* still believe it's less than optimal for the end user who shouldn't have to care about what language an application is written in to know how to install it. This is how we end up with a package manager for the OS, one for Rust applications, one for Python applications, one for Javascript applications, one for Ruby...etc, etc. And by end user I *only* mean non-developers (the ones who only want a binary and couldn't care less about what it's written in). &gt; The time and resources required to do things The Right Way is so enormous that the choices for end users [...] I agree. My point was simply if things can only be easy for one party (dev or user), the user should win. But I agree, there is a balance to be obtained in the real world. 
&gt; But I also still believe it's less than optimal for the end user who shouldn't have to care about what language an application is written in to know how to install it. Yes, it is absolutely less than optimal! :-) Installing `cargo` to get an application is strictly worse than just getting the application in the first place (from the end user's perspective). That's why I bought a Mac mini: I have plans of at least providing binaries for people to download. Getting them into package managers is probably a line I won't cross though. As you said... the real world gets in the way. :-(
Usually no, since the type parameter will be used elsewhere. Unless it isn't. But putting it there shouldn't affect anything, really.
The pretty-printer already outputs comments and literals in their original form, wherever possible. It should be possible to have a mode where it preserves everything, including whitespace and module structure, but I think you'll need some macro magic to keep it sane. Once you have that, and you've tested it against real-world code, you can start adding transformations to the whitespace, while providing context (e.g. "between an `if` condition and the opening brace). Actually, that example makes me think the only sane way this could be written and customized is *reformatting by example* - where you provide a string which hits all the possible whitespace locations in the AST. /u/pcwalton's experiment used tokens, I believe, I'm not sure how well those work, compared to a full AST. Inside macros, maybe the best option would be to make use of the macro_rules definitions and/or the resulting expansion. And that's not even going into things like splitting long lines so they align neatly - the algorithm we use (in `libsyntax/print`) might be able to handle some of that, but I doubt we're doing it correctly.
That sounds like it could work, thanks!
`pcwalton/rustfmt` uses tokens, but it reconstructs them into a pseudo-AST. It's the same approach as `clang-format`. I firmly believe that it's the right approach, since it handles things like incomplete parse trees and macros. (`clang-format`'s algorithm is so flexible that it can even handle languages that are C-like but aren't C, such as JavaScript.)
Until this `DerefMut` is added, `OwnedAsciiExt::into_ascii_uppercase` should be used instead.
Isn't there a lot of frameworks that end up calling `eval`? You can never be sure what will be evaluated..
How does that invalidate the DOM?
At `eval(a)`, you can't be sure `a` doesn't call `document.write` unless you begin evaluating the Javascript.
True. I'm not sure `if let` is a good idea. Particularly because it reads like the c code: if (a = ...) { ... } Which I feel would be better handled using some kind of matching-assignment operator: if Some(x) matches blah { ... } 
Saw this earlier today and figured I’d give it a swing: http://www.reddit.com/r/programming/comments/2momvr/pcg_a_family_of_better_random_number_generators/. As it happens, it’s slightly faster than `rand`’s `XorShiftRng`.
Yes, but this is already the case even with sequential parsing.
Spoke too soon — I wrote my benchmarks before I had some coffee, so it turns out `PcgRng` is slightly slower than `XorShift`.
That's a good idea, and I'll do that once 1.0 beta drops. Right now I'm enjoying not having any external crate deps.
Just thinking aloud; how much work it would be to get this compile with `#![no_std]` and then port to a microcontroller/zinc.rs...
Incidentally, I keep seeing 'rng' in computer science and my mathematician brain keeps reading [rng](http://en.wikipedia.org/wiki/Rng_(algebra\)).
JFTR: https://github.com/rust-lang/cargo/pull/1318
By bloated i just mean that it is very complex and hard to learn in comparison to other languages. I don't know any simpler alternatives at least none without performance penalty but there should be no more special rules because we could easily end with a complexity like in C++.
Fair enough, thanks for elaborating. If you have any thoughts on making it easier, teaching this stuff is my job, so give me a shout :)
So that's the thing. With speculative parsing you stop and rollback if the script does call `document.write` or something that triggers a DOM manipulation.
I always like to think of references as arrows, you own the arrow, but you don't own what it points at. So you delete what you own: the arrow, the reference, but not what you are pointing at. Then box makes a lot of sense: it's a pointer, but you own what is being pointed too, so it's better to think of it as a box that, somehow, is bigger on the inside than outside.
Not really. The only requirement is that all type arguments are used. Since `T` is already used there's no need to use it, only `'r` needs to be used. You could use `PhantomData&lt;&amp;'r u8&gt;` and it would still work. I think that there'd be value in making an explicit lifetime bound type (much like `MarkerTrait` is a more specific named use of `PhantomFn`). Something like: type BoundLifetime&lt;'a&gt; = PhantomData&lt;&amp;'a ()&gt;; Then you'd only have: use std::marker::BoundLifetime; struct Foo&lt;'r&gt; { ptr: *mut Internal, _bound: BoundLifetime&lt;'r&gt; }
Being a professor is a job/profession and a title, not a degree. What you probably meant to quote was the fact that he is an Honorary Doctor of Science. In any case, that doesn't sound like *Doctor of Philosophy* to me. D.Sc. (h.c.) is a higher doctorate degree.
if he names this shell rush i will be extremely happy.
This could probably be written as a syntax extension. `hold!(MyModel.myfield == 3)`, which expands and evaluates to an object containing the structured form along with references to the data. http://doc.rust-lang.org/nightly/book/plugins.html Actually, a [macro](http://doc.rust-lang.org/nightly/book/macros.html) might be enough.
That's pretty cool! Would it be possible to have a higher period than 2^64?
There is already a Ruby shell named Rush.
OP, I've got a couple questions about your reseeding process. Why are you panicking on all-zero seeds? I'm fairly sure the paper mentions that zero seeds aren't a problem for PCGs like they are for XorShift rngs. And why do you advance the generator twice when you reseed? Personally, I would have handled the reseeding/streams differently. Your way is simple (which is good), but it hides a lot of the power of PCGs. You don't provide a way to select the stream - the user needs to know how your seeding function works to do that. I would've provided `with_stream(s: u64) -&gt; Self` and `set_stream(&amp;mut self, s: u64)` methods, as well as the method based on the rng's memory address suggested in the paper. My seeding functions would only take the state then, rather than the state and the stream. For a user who just wants a plain rng your setup may be preferable, but the whole point of PCGs is the extra stuff you can do with them ;) None of this is criticism by the way, just how I would've done it :)
I also echo /u/Manishearth's thoughts. More specifically, I think what you're after here is a *deeply* embedded domain specific language. Trying to achieve your goal by simple operator overloading is a shallow embedding, and it's going to be much less flexible. A full deep embedding is going to be most convenient compiler plugin. You could also back to regular functions and build a combinator library, but it's hard to do this elegantly in Rust I think. It might be worth exploring. Basically, attack the problem as building up an AST of your DSL and then write an evaluator for it. In Python, the distinction between "deep" and "shallow" is almost non-existent because of the amount of runtime reflection and monkey patching you can do.
I suspect making this change would be useful for "expression templates" where the operators are constructing something that represents the given operation, rather than actually computing it immediately (like your example) and vectorised operations, e.g. Python's NumPy's array `==` does elementwise equality returning a boolean array (similarly languages like R, matlab). &gt;&gt;&gt; import numpy &gt;&gt;&gt; numpy.array([1, 2, 3]) == numpy.array([3, 2, 1]) array([False, True, False], dtype=bool) Unfortunately... I think actually doing this change may be very, very annoying. The other operators support arbitrary output types via associated types, e.g trait Add&lt;Rhs = Self&gt; { type Output; // ... } Making a similar change to `PartialEq` (and `PartialOrd`) would mean that all current users of `PartialEq` generically have to specify `PartialEq&lt;Output = bool&gt;`. I suppose we could do it by splitting out yet another trait, like /// provides overloading for ==/!= trait EqOperators&lt;Rhs = Self&gt; { type Output; fn eq(self, rhs: Rhs) -&gt; Output; } /// types that overload ==/!= to return bool trait PartialEq&lt;Rhs = Self&gt;: EqOperators&lt;Rhs, Output = bool&gt; {} impl&lt;Lhs, Rhs&gt; PartialEq&lt;Rhs&gt; for Lhs where for&lt;'a, 'b&gt; &amp;'a Lhs: EqOperators&lt;&amp;'b Rhs, Output = bool&gt; {} For maximum flexibility, `EqOperators` would have to be change to take its arguments by-value, whereas `PartialEq` will presumably want to work with by-reference values. Hence, the strange `where` clause for the blanket `PartialEq` implementation. I can't really see a way to do this while still ensure it is ergonomic to implement and use these traits. :(
&gt; Panicking matches the rand crate's treatment of seeds; could easily not be necessary. Yeah, it's one of the flaws with the XorShift algorithm. If it starts as all zeroes, it gets stuck there. PCGs don't have this issues. &gt; Advancing the RNG twice is how the provided C implementation functions — I'm testing against its output. I'd looked at the C implementation to check this, but I didn't see it - I'm probably just blind, or wasn't looking in the right place. &gt; Pull requests happily accepted. :) If I get time later today I'll cook something up :)
Is there some lesson to be learned here about Rust's current generic programming capabilities?
Yea I didn't think of that, it does add ambuguity for something like this: struct Foo; impl Foo { fn qux(&amp;self) { // something } } type Baz = Foo; impm Baz { fn bar(&amp;self) { self.qux() } fn qux(&amp;self) { // something else } } What I wanted was to have a clean way of constructing my aliased types, really.
I guess it's more of a namespacing issue. If you call `WeakElement::new` then you get whatever that returns, and if you call `Weak::new`, you get whatever that returns. The formal definition would be that an `impl` block for an aliased type can only contain static methods.
You *probably* could, but I'm not sure there's any point. The result would be slower (you'd have to use virtual rather than static dispatch on the filter predicates) and, as far as I can think, wouldn't really buy you any advantages. If you want to be able to put two different iterators into a common type, you could just box them as trait objects, and then you only have to do virtual dispatch *once*. Just to clarify: in your example, the two `P` predicate types *would not be the same, even if the closure signatures are*. `filter` is (or should) be using unboxed closures, which generate a unique, anonymous type for every closure (even identical ones!).
&gt; Fear not, this is Rust, not some scruffy loosely-typed, garbage-collected, non-blocking language! lol, I'm going to be using this more often 
I have had great fun, just by adding this point `Point::new(0.75 , 0.35 , 0.0),` to the frequency envelope of the example program ! Awesome, I think I will use it to let my commandline tools play something when they are done ... just because ... I CAN !! :D
What makes you say that? It's just a convenient way to get a mut reference to the contents of a box. Boxing stuff that doesn't need to be boxed is the bad "make it compile" trick. 
&gt; That is to say, I don't know what context would ever have MyModel.myfield == 3 as Rust, per se, rather than simply a string you're passing to the database. RethinkDB's interface is not string-oriented. You essentially build up an AST using function calls, which is then serialised and sent off to the database: r.table('users').filter(lambda doc: doc['age'] &gt; 29).run() That lambda is called (on the client) with a special parameter for doc which, as you do things to it, creates part of the AST - it's not run using real data, and all the operators simply return an object encapsulating a part of the AST and allowing you to do further things to that AST. In Rust, I assume you'd do something like: r.table('users').filter(rethinkfn!(|doc| doc['age'] &gt; 20)).run()
Woah, really nice use of associated types! Haven't yet seen anything like it :) It's impressive how readable something so incredibly generic has turned out. Looking forward to seeing how these device implementations go.
Hm. On the other hand, this limitation could become really annoying in the presence of HKT in the future. Consider, for instance, implementing comparison of types that have monadic semantics. Expression templates are one very useful thing, but more generally it is really, really nice to be able to compose arbitrary operations in a way that expresses intent, without having to wrap everything in macros (and it's easier to understand IMO). It seems the choice may come down to more ergonomic APIs or more ergonomic *implementation* of APIs — I personally tend to prefer the former, but maybe a good compromise is somewhere to be found…
Nice summary. Unfortunately one day too late for me. ;) I never got it to work in my application like this. If I don’t box the event loop I always get a stack overflow `thread '&lt;main&gt;' has overflowed its stack`. Edit: problem seems to be solved with the latest version of mio which reduced the size of EventLoop to a few hundred bytes.
Yes.
You _can_ macro it away ala do notation.
I'm just making a tongue-in-cheek comment about the strangeness of that pattern to newbies. I've come across a lot of newbies (and I used to do it myself) trying `(ref foo) =&gt;`, then `(box foo) =&gt;` then `(box ref foo) =&gt;` etc till it works ;P
Yes, one has to be careful when defining traits and types. The things I noticed with Rust so far: - You can’t write a struct which is generic over `&amp;[T]` and `Vec&lt;T&gt;` because `&amp;[T]` doesn’t implement `Index` (actually you could with some `Deref&lt;&amp;[T]&gt;`-magic but then it is not generic over the most obvious `Index` any more which might exclude other containers…). - You can’t write a generic function for numbers. There is no common trait for numbers (of course you could define your own trait, but then nobody could use third party numeric types with your library…). - Boxes could possibly lead to some pain. Some types take `Vec&lt;T&gt;` although they could possibly also take `Box&lt;[T]&gt;`, `Rc&lt;[T]&gt;`, `Rc&lt;Vec&lt;T&gt;&gt;` etc., same goes for `String`. One example is `MemReader` aka. `Cursor`. There’s probably more. To be clear, that’s nothing which is wrong with the language (although some things might require HKTs) it’s just the stdlib. But I am positive that this will settle of somehow. So far the Rust developers have been impressively pragmatic when evolving the language.
I was hoping there would be some actual discussion on potential bugs he found in the original implementation or whatever from coverting it to rust. Granted rust may be better going forward, but if he was able to do a 100% source translation and it just worked, then what was the point.
Rust is also not free of memory leaks as claimed.
I think it's free as long as you don't use ~~GC~~, `ARC`, `RC` and friends. I could be wrong and some of the safe data structures can be abused to cause memory leaks.
For 1.0? :-)
&gt; A Handler can implement some or all of the following functions: Waitaminute, how do you do that? I was in the impression that impl'ing a Trait required implementing the full interface? &gt; Thread safe message channel for cross thread communication Also, I thought std:sync::mpsc already provides this. How do they differ?
&gt; So caveat aside (but not ignored, it is a big issue), I see the advantages as &gt; 1: No crashes because of code misuse (as long as you do not use unsafe {} ) I don't know if "crashes" means something different than normal in C++-lingo, but clearly rust can crash like any *partial* (as opposed to *total*) programming language. And the fact that you can't catch *panics* might rub some people the wrong way. &gt; 8: Very strongly typed (near zero runtime) Being strongly typed can be achieved with both runtime- and compile-time checks. *Runtime* has little to do with it.
&gt; Waitaminute, how do you do that? I was in the impression that impl'ing a Trait required to implement the full interface? That’s exactly the difference between the classic definitions of interfaces and traits. Interfaces may only provides function signatures while traits may also provide implementations. Traits are something in between of interfaces and classes/prototypes. Thus you only have to implement the functions for which no default implementation is provided. In the case of `Handler` all functions have an implementation (basically nop) such that you are free to chose which to override.
What do you mean by partial and total programming languages? I've heard of partial and total functions. Just curious :)
Yeah, even with full GC you can't be free of all memory leaks if you accidentally leave an item in a hash table when there is nothing else that will ever need to look up that key. Like with "no data races" vs. "no race conditions", what Rust provides for memory leaks is "no forgotten `free` calls", a much more narrow but still valuable guarantee.
That sounds odd, did you report the bug?
Its more that you're less likely to find yourself in callback hell, in node almost everything needs to use callbacks right now... Though I think io.js is moving past this.
Well if you like functional programming you're in for a treat. Total functional programming is pretty much programming with total functions - functions that are guaranteed to terminate and return a value of the type that their function signature specifies. It's used more in theorem provers than general purpose programming languages, but Idris is an example of a dependently typed functional programming language which also tries to be general purpose - it lets you annotate your functions as "total" so that you can prove stuff about your programs, which a rich type system like dependent typing sometimes/often demands. [Here](https://uf-ias-2012.wikispaces.com/file/view/turner.pdf) is an article on it. (Granted: "partial" programming language is probably something that I made up on the spot)
It's a constant reminder for us to [make a logo](https://github.com/gfx-rs/gfx-rs/issues/119) ;) 
Yes, this was now removed syntax indeed, at least that is what the compiler keeps telling me. To me this would be more like syntactic sugar, that would only work with marker traits. Those usually have no type parameters. It seems right now, you are forced to break DRY, which becomes even more apparent when type parameters are invovled. Here is an example out of the real world: /// A builder for the *update* method supported by a *playlistItem* resource. /// It is not used directly, but through a `PlaylistItemMethodsBuilder`. /// pub struct PlaylistItemUpdateMethodBuilder&lt;'a, C, NC, A&gt; where NC: 'a, C: 'a, A: 'a, { hub: &amp;'a YouTube&lt;C, NC, A&gt; } impl&lt;'a, C, NC, A&gt; MethodBuilder for PlaylistItemUpdateMethodBuilder&lt;'a, C, NC, A&gt; {} VS /// A builder for the *update* method supported by a *playlistItem* resource. /// It is not used directly, but through a `PlaylistItemMethodsBuilder`. /// pub struct PlaylistItemUpdateMethodBuilder&lt;'a, C, NC, A&gt;: MethodBuilder where NC: 'a, C: 'a, A: 'a, { hub: &amp;'a YouTube&lt;C, NC, A&gt; } I am not even sure where to put the `: MethodBuilder` part in the second example, but in any case, it makes a great visual impact and produces less, more readable code. Of course I am talking about a special case, and there are more things to consider. But in the long run (e.g. Rust 1.5), it would be great if we could make the syntax even nicer to prevent DRY breakage.
Hmm, maybe I ahould change my shell name from rush to yyzed?
DRY is short for "Every piece of knowledge must have a single, unambiguous, authoritative representation within a system." not "never repeat the same words in your code."
Then what would the word be for an application crash due to a well-defined error? Like a panic after calling `unwrap()`. You'd call that "panic" in Rust-lingo, but I don't know if you can use that in a more language-agnostic setting.
Your definition seems like a truism to me, as this is always the case in statically compiled languages where compiler will simply reject duplicate definitions of any kind or ambiguities. Isn't that so ? If I am forced to repeat boilerplate just because I am unable to specify what I want in the portion of the code that has the boilerplate already, I feel DRY is violated. Just because I am obviously repeating myself.
Makes the application more robust to refactoring, at least.
&gt; Also, I thought std:sync::mpsc already provides this. How do they differ? libstd is all about native threads. If you wait to receive on a message channel then you're blocking an OS thread, and can't also wait on IO at the same time. mio messages arrive in the `Handler`. A single thread can wait on both messages and IO events; the message sending integrates with the IO event loop somehow. I haven't used mio but that's my understanding from looking at the API, anyway. Rust used to have functionality in the stdlib that would work with either native OS threads or libuv-based green threads. This was pretty neat, but the performance cost was eventually deemed unacceptable. :/
Yeah, the generics chapter doesn't seem to mention default methods at all. 
Because it’s running there in my case.
I am using Arch, and there already is an issue here: https://github.com/jeremyletang/rust-portaudio/issues/36 I am FreeFull btw.
I’m using the first one. This monster is quite huge! println!("{:?}", ::std::mem::size_of::&lt;EventLoop&lt;(), Event&gt;&gt;()); println!("{:?}", ::std::mem::size_of::&lt;EventLoop&lt;(), ()&gt;&gt;()); println!("{:?}", ::std::mem::size_of::&lt;Event&gt;()); 65744 65744 112 It also doesn’t help if I shrink the enum to 32 bytes…
Does this violate DRY? let x = vec![]; let y = vec![]'; let z = vec![]; Even though we repeat `vec!` here, over and over, this is not a DRY violation. On the actual topic, though, making a change like this would require adding a special case: right now, marker traits are a certain list of traits. We'd have to add a distinction between 'traits' and 'marker traits' in the language, which adds complexity.
I totally understand that, and have similar concerns. Yet, I sometimes like to dream away and delve in 'what-ifs' :).
Totally :)
&gt; I consider it a bug if a program panics. Especially libraries should never panic unless the API explicitly communicates this possibility. Nice, I would agree strongly with this. In terms of panic! is there a mechanism (apart from the obvious) to know a lib will not panic! if you follow all the return paths (match etc). I would like to know the unsafe blocks and panics if possible. Then following return paths etc. and safe code practices would be something to strive for. 
Right, but what if I accidentally continued to use `a` after is was copied? I would prefer to have the compiler tell me that I (possibly) unnecessarily copied a large struct and that if I really intended on doing that, I should do it explicitly with `.clone()`.
Somebody ought to add a terminal renderer to Servo
Then don’t implement `Copy`. This one of the benefits having it to implement explicitly.
Exactly; I think ``Clone`` is what you (and I) want?
If you don't think that you'll be adding a non-`Copy` element to your struct later, than you should implement `Copy`. Remember, a move is just a copy where the compiler disallows using it afterward. That's the only thing this modifies, it's not more or less expensive.
Sure, but that one would be pronounced 'roosh'!
What you want are [bitflags](https://crates.io/crates/bitflags) [Example](http://doc.rust-lang.org/bitflags/bitflags/macro.bitflags!.html#example)
You would need to prove that all divisions are never a division by zero and that all array accesses are always in bound. This is generally not possible in Rust since it requires dependent types.
`EventLoop` contains a `Poll`, and a `Poll` contains `Events`: pub struct Events { len: usize, events: [EpollEvent; 1024] } ...but an `EpollEvent` seems to be 12 (possibly aligned to 16) bytes, so that would not account for the entire 64K. 
Always. If it warns add `Copy` unless you have a really good reason not to do that.
Given I want to have a generic array-like container that is either owned or borrowed. Then Index would be the trait to go.
It kind of depends on how you think about it. I consider a forgotten free as you have no more references to something; you start with a reference to something, the reference goes out of scope, and you forgot to call free on it at that time. And that can happen because you simply forgot, but much more easily because there was some API where the ownership wasn't clear, and you thought that the other end of the API was supposed to be the owner of that pointer and responsible for freeing it. In the case of cyclic references, you do still have a reference to each of the elements; they just aren't rooted anywhere. That's a more complicated problem, and a place where you will be making deliberate design decisions about the structure that has cyclic references, not just forgetting a free or being confused about who owns a particular pointer. With ordinary Rust reference types, you cannot get cyclic references, because there's no coherent way the lifetimes could work out. You have to explicitly opt in to using `Rc`, with the caveat that you now have to deal with cyclic references. Like with any kind of safety guarantees, it's always possible to build a way to violate it in some sense. If you just use one big vector as a virtual heap, and allocate and manage memory in it yourself, of course you can get all of the traditional problems back. If you stick stuff into a big vector or hash map, you can easily forget to remove it even when nothing will be accessing it via those keys again. So any safety guarantees have to be taken with that caveat in mind; you can write yourself a footgun in any language that is turing complete (hey, you could always just write an emulator in that language and then compile unsafe C to it), the question is does the language provide you the tools for avoiding the footgun in the common cases, and helping you be aware of when you are arming a footgun so that you will only use it when you really need it. So, by "no forgotten `free`", I mean you can't do something like the following: int test_something(int a, int b) { int *scratchpad = malloc(b, sizeof(int)); return do_some_processing(a, scratchpad); } Not that it's impossible to leak memory in any way.
I feel like this is a pretty good reason to implement an explicit `move` syntax in Rust.
I think you want to impl [PartialOrd](http://doc.rust-lang.org/std/cmp/trait.PartialOrd.html).
Hm, yeah, it is all related. Rust is safe to use largely because the stdlib provides safe abstractions, e.g. `Iterator`s vs unchecked array indexing. Safe abstractions are also pretty high-level / semantically rich, so we generate a lot of IR for them. You could say that the work LLVM does in optimization is converting the code from a form where it's easy to demonstrate correctness to a form that can be compiled efficiently.
`Eq` and `Ord` are total, so `Eq&lt;Rhs&gt;` would *at least* require that `Rhs: Eq&lt;Self&gt;`. Alternatively, we could stop kidding ourselves and make `Eq` and `Ord` **unsafe** traits before it's too late.
Edit: felling very stupid, I just found about binary_search_by. It's funny because if you type *"binary search"* in the search bar *"binary_search_by"* won't show up. I'll file a bug about that. That's not enough for some cases, like a binary search in my case.
&gt; So from a user perspective there isn't any difference. There is a big difference between `panic!`s and random memory-unsafety-related crashes (like segfaults), namely the fact that a `panic!` is (a) more controlled, and (b) reliable. [Copying from an older comment of mine](http://users.rust-lang.org/t/manually-implementing-a-heap-to-overcome-safety-rules-considered-harmful/398/10?u=huon): There is good failure and bad failure. An application can use a monitor thread to "catch" and handle an unexpected panic as usefully as possible, e.g. - a web server that panics while serving a request would print a "500 internal server error" for that specific request, but continue handling everything else as normal - a text editor might try to save as much state as it can - in general, an application that panics while opening/processing a file could indicate that the specific file could not be handled due to an application bug (providing e.g. info about how to get help/file a bug) but let the user continue using the application, - if an image/video/audio decoder panics in a web browser, it could substitute a placeholder indicating an error, but not take down the whole browser, not even taking down the single web-page being displayed (I believe that servo may (try to) do this now, already) It is harder to achieve this for memory safety problems, even handling/recovering from the best case (a segfault immediately after the memory safety violation) is non-trivial. And that's just the best case, the problems may not manifest as a crash at all, just heap corruption and "heisenbugs"; silently breaking the user's data, or letting their computer be pwned, or displaying their private keys to the whole internet (for example).
No, no rust which isn't, at some level, in some crate, unsafe, should generate one. Which is no rust ever. Everything is built on top of unsafe code; the 'rust can't crash' thing is flat out wrong, and we should call it out when ever it gets said. It just spreads disinformation. Rust gives you, as the programmer, better tools to work with, by providing you guarantees that *certain parts of your code* will not result in *certain types* of failures. It's far far better than what C++ offers, but python or javascript for example, are 'safer' than rust in that there is *no javascript code path* that can result in memory errors; it's a trade off we give to be 'safer' than other low level languages, and faster than other GC based 'safe' languages. ...but: 1) Can a rust application suddenly 'crash'? Yes. An unhandled panic or unsafe code path can cause this. 2) Can a rust application have memory corruption? Yes. An unsafe code path (which every rust program has at some level) can cause this.
This was tried, and the verboseness it adds was too much.
I agree that it would be too verbose if you had to do it on every move, but what if it just forced things to move when they otherwise wouldn't? So if you do let b = a; it will copy if `a` is `Copy`, and move if `a` is not `Copy`, but if you do let b = move a it will move no matter what.
Are you talking about lambda cube? Values and types can depend on other values and types. Values depending on values(VdV) is normal programming. TdT is type-level programming, aka higher kinded type. VdT is generics: you produce new values for each type. TdV is dependent types. "Full" dependent types may mean all four modes, but in a narrow definition, only TdV is dependent types. As I understand, zero checks and bound checks in compile time do require TdV.
My understanding is that checking `arr[x]` would not require dependent types, but `arr[x + y]` would. But again, I could just be wrong. I need to actually write some Idris.
The classic here is [Eliminating Array Bound Checking through Non-dependent types (2004)](http://okmij.org/ftp/Haskell/types.html#branding) by Oleg Kiselyov.
Being able to prove that things like binary search on arrays doesn't go out of bounds sounds pretty hard, dependent types or not.
IMO talking about implementation details is not a good way to understand semantics in rust. As you saw by telling the dude that it's just a memcpy.
I am surprised of reading this: "We’re working on tracking down memory leaks" Can you explain why are you leaking? Is it all about RC cycles? 
It's important to understand all the details when you're dealing with a systems language.
Out of curiosity, if a type is not `Copy`, is there a reason why copy elision is not done by `rustc` and instead we are relying on LLVM?
CSS3 `background-size` was implemented as well. It landed with `image-rendering`.
&gt; If you just use one big vector as a virtual heap, and allocate and manage memory in it yourself, of course you can get all of the traditional problems back. This is how Emscripten provides manual `malloc` / `free` on top of JavaScript. \^_^ I'd like to study at some point the techniques one would use to exploit traditional memory-corruption vulnerabilities inside Emscripten. You couldn't break out of the JS sandbox that way, unless you have another vuln, but you could get XSS-like powers in the page embedding the vulnerable Emscripten app.
Refinement types?
&gt; I would like to know the unsafe blocks and panics if possible. Then following return paths etc. and safe code practices would be something to strive for. Within your project you can use #[forbid(unsafe_code)] on any sub-tree of the module hierarchy. It might be good if libraries could report their use of unsafe code. I am wary though of using such metrics to judge projects I'm not familiar with. It may be that library A has 10 lines of unsafe code and library B has 100 lines because library A is cutting corners and library B put in the effort to uphold a safe interface. We may end up discouraging code patterns that are actually safer because they look worse on some metric. It'd be a cool area to research, though.
&gt;&gt; So from a user perspective there isn't any difference. &gt; &gt; There is a big difference between panic!s and random memory-unsafety-related crashes (like segfaults), namely the fact that a panic! is (a) more controlled, and (b) reliable. &gt; An application can use a monitor thread to "catch" and handle an unexpected panic as usefully as possible. Only if there is such a thread and the program is structured in independent tasks in a way that the lends itself to such recovery. How practical that is remains to be seen. Especially since sharing data between tasks comes with some hurdles I'm not sure how often useful recovery will be possible. Will the webserver recovery task have access to the socket the failed task tried to handle? Will it know in which state that socket is and if it makes sense to write a error response to it? Does it know what the client requested and if the failed task already started to write data? If the task failed that handled a certain document in the text editor, will there be anything left to save? Will it even work to have all the documents neatly separated into different tasks? It sounds like it would make the interaction with the user interface quite a bit more complicated, so it remains to be seen if many applications really will implement such a model. And the point is that it's still a crash. The application saying that the tab has been closed because of an unexpected problem is less severe than the OS saying the whole application got closed for the same reason, but I think it's fair to call both instances a crash. One got handled better than the other, and that's great, but something in the program still went fatally wrong. &gt; And that's just the best case, the problems may not manifest as a crash at all, just [bad things happening] Sure there are good reasons you want the program/task to stop, but in the end that's still a crash. Windows also likely has good reasons when it triggers a blue screen, but nobody is happy when it happens. (No *user* of the program. The developer of the program has different interests.) 
&gt; I always found it strange that rust didn't convert bar to a reference - since the originating fn's reference is destroyed we may as well just use it and save ourselves a copy? It may not actually copy; it depends how LLVM compiles that code. But there's no avoiding a copy in cases where the value is going to outlive the function call. Also it's not going to be an optimization in every case. We've also discussed a `&amp;move` reference type, which I believe would work similar to what you propose, which is also similar to how moves work in C++ (rvalue references).
Remember that we use all sorts of unsafe code to hook into spidermonkey -- which affects both script and layout. Also, what kmc said, there might be an ICE, or that "leak" doesn't mean leak from the POV of Rust's memory safety (unreachable memory), it just means that our program is written in a way that we hog memory.
Not necessarily. You can have documentation that documents the current state of the world without relying on it to be true forever, especially w.r.t. optimization information which necessarily changes as the hardware and compiler changes.
I've created a Pre-RFC thread on discuss: http://internals.rust-lang.org/t/pre-rfc-boolean-operators-that-dont-return-bool/1684
&gt; Its absurd to suggest unsafe code must assert itself to be safe; That is the whole point of `unsafe`. You have and more important you are also able check its uses because its number of uses is countable. In that respect your assertion about the safety of Python etc. is actually wrong. It is less safe than Rust because any part of the code could call at any time into some unsafe lower level API. There is no encapsulation enforced. You have to check and review the whole file. In Rust you only have to review the `unsafe` parts.
No, that is a two hours old change of mio. ;)
Just for information. SSL is active from the beginning of 2015 (https://www.istanco.com/rs-domain-registration.php)
I just added `-Ofast` for the C version, and `-O2 -fllvm` to the Haskell version, but it did not have a big effect. Can you try with your own computer? Do you have other suggestions on how to improve the parsers?
I'm [fighting to revert RFC #0369](https://github.com/rust-lang/rfcs/pull/925) to reintroduce simpler numeric traits as you said - there is need for this.
&gt; You can't write a python statement that does not invoke ffi that crashes with memory corruption. There's no FFI in the following (at least, any FFI that happens is integrated deep into the standard library/interpreter, since `ctypes` is a standard library module): &gt;&gt;&gt; import ctypes as c &gt;&gt;&gt; c.cast(10, c.POINTER(c.c_int)).contents Segmentation fault Equivalent to `unsafe {*(10 as *const i32)}` in Rust.
I rebuilt everything with your suggestions and updated the README. Interestingly, it had a lot more effect on parsing the Big Buck Bunny trailer than the other file. 2 μs less for hammer.
I can't argue with either of your points; they're both right. However. You can audit `unsafe` code, but you cannot assert that it is safe. It's not safe. That's the point! That's why its absurd to say that it is safe!! Certainly you can have more confidence that your explicitly audited unsafe sections are bug free than in say, c (or, as you say, python); you can totally have library authors saying 'my library is safe! I've audited all the unsafe sections' ...but it's joke right. If your library has unsafe sections in it, audited or not, there's a chance bugs are in there, that will cause memory corruption. I'm not interested in arguing the relative merits of safety in different languages: The point I'm making is this: Rust code is not safe. If you write rust, there's some chance you can have hard, memory corrupting errors. Sure, it's not high; in fact, it's much much lower than in some other languages... but it can still happen. Let's not pretend that isn't the case, for some kind of... I don't know, clever marketing slogan about 'Rust can't crash' that you can feed to c++ programmers to get them interested. That's called spin. It's what politicians do. 
The original example mp4 parser was doing a bit more work (completely parsing mvhd atoms in moov atoms), but that was a lot of code to write for the other parsers. Just parsing the ftyp atom was enough for a test. If I remove the moov and mvhd parsing code, I can easily shave off about 250 lines of code from the rust parser.
I don't agree for the most part, but I thought the following is worth highlighting: &gt; The massive disproportionate popularity of Rust for ordinary applications programming if it stays the same will lead to billions of dollars wasted on Rust's deadweight. I am of the opinion that garbage collection is great and everybody should use it when you can afford to. Honestly I am confused by attempts to use Rust for web application programming, and although I don't believe it will lead to "billions of dollars wasted", I still find it pretty baffling.
Web programming in certain ranges can be very latency sensitive and often latency issues come from GC pauses. e.g. I know some shops that switch off GC during working on a request and manually move GC to the back. It's probably not the right fit for your pharmacies website with event calendar, but there are quite some points where I'd like the predictability of Rusts memory management.
Performance aside, the safety guarantees of rust is probably pretty appealing. That alone is a pretty good reason to use rust for web programming, a place with large attack surface
Really? What safety guarantees do Rust provide over Python, for example?
The best time to really learn Rust is when pcwalton is irritated by misconceptions about Rust.
Paper I linked above actually uses binary search as an example.
This is a very good point indeed.
First, nice comparison! Interesting results. I am curious about why the C version is so slow, but it might be that hammer uses type erasure and this has a significant impact. It would be interesting to measure the memory consumption of the three implementations. I expect rust with jemalloc to be really good here. It would also be interesting to compare a C++'s Boost.Spirit based parser with nom, but that is a lot of work.
&gt; Alternatively, we could stop kidding ourselves and make Eq and Ord unsafe traits before it's too late Could you elaborate? (Why these should be unsafe...)
Well, not exactly. Everything that is `Copy` _should_ be `Clone`, ideally implemented as just a memcopy. `Clone` is not a deep clone, its a clone that is as deep as necessary to get a `T` from a `&amp;T` - which for `Copy` types can be literally `return *self;` Ideally Rust would implement Clone automatically for all `T: Copy`, but the lack of negative trait bounds prevents this...
Ruby has segfaulted on me a few times, typically on new versions where they touch the GC, or from some subtle change on their C "api" (which then makes extensions crash). Not that it wouldn't be possible for Rust's stdlib (or some other library) to cause a segfault via `unsafe` code, but the fact that the underlying language is Rust and not C gives me more confidence that this won't happen after stabilization
To anwser my own question, I changed the code to look something like: fn bigbuckbunny_test(b: &amp;mut Bencher) { let data = include_bytes!("../bigbuckbunny.mp4"); b.iter(||{ let foo = full_data_interpreter(data); black_box(foo) }); } and I got the results: test bigbuckbunny_test ... bench: 204 ns/iter (+/- 13) test small_test ... bench: 218 ns/iter (+/- 34)
I had the same impression, and did the same kind of test. Also, the Rust code has the same timing pattern between the two file as the other parsers (quicker on Big Buck Bunny).
Actually, what attracted me to Rust in the first place is that it has a strategy against iterator invalidation and related aliasing problems. That even annoyed me in high-level languages for the longest time.
&gt;and lower server requirements I have a project right now that consists of a chat-server written in Rust, and a companion webapp written in Ruby. The chat-server, which in many regards is more stateful and complex, is currently sitting at 57KiB RES. Whereas my minimalistic Ruby webapp bounces between 100-200_MiB_. &gt; ... for a great many cases, garbage collection is simply not necessary ... I agree, and I haven't found myself missing garbage-collection in the slightest. It's nice to have such an expressive language that doesn't _force you_ into using GC. While the overall development speed may be slower, in my experience I have found that one great advantage to Rust is that it always pushes me towards writing thread-safe code correctly the first time. So while it feels like I spend more time trying to pass type-checking: there's a tradeoff of less time spent debugging. When I write programs in Go: using the data-race detector and resolving deadlocks seems to be a fairly common occurrence for me. Whereas I have had to resolve _one_ deadlock across all the programs I've written in Rust to date. --- Overall I think Rust found a very nice place in the stack of my current webdev project, and I actually struggle to envision another language filling its place.
Yeah; even if the DB is the bottleneck, having a faster app means less latency; http://blog.codinghorror.com/performance-is-a-feature/
They do not all seem like trolls. Only one of them, actually. Anyway, I think that the problem at the root of it is documentation. I know Steve is working hard on it, and I'm grateful, but Rust IMO is currently missing documentation not of advanced features (though there's not enough), nor intro level stuff. But how to structure and compose objects... How to use rc, arc, cell, refcell, etc... I have about four micro projects I'm doing for fun that I'm stuck because i know what i want to do, I know how to do it in java, but not in Rust. I keep running into ownership problems... 
I'm writing a multi-threaded chat server at the moment. I can say that I have spent far less time debugging concurrency issues in Rust than I have in any other language I've used. (I only consider it to be a web service since it exchanges JSON over WebSockets. Plus the client runs in a web browser.) In the case of Ruby: I end up having to reach for C extensions to get performance anyways, which of course have the potential to segfault. By contrast I only reach for FFI in Rust because the ecosystem is immature. In the future I expect there to be many performant libraries that are written mostly in safe contexts. So in Ruby, as my apps grow more complex, I trend towards binding to more C libraries. Thereby increasing my potential for memory safety violations. Whereas in Rust my code will always be trending towards fewer segfaults. 
When it comes to web dev all languages have at least one major disqualifier. 
Not really memory safety, but it's e.g. far easier for me to keep track of mutable state in Rust than it is in Python.
Thanks for the detailed answer (I learned things). However, I think I failed to pose my question clearly. My list of things contained 'truths' (they all must be true). So wrt '&amp;T' I said it is never copied - but you seem to be saying that a reference by definition is copied (&amp;T is Copy). In my mind a &amp;T should never be copied (memcpy() or complex Copy trait impl) when passed as a function argument (unless you explicitly implemented the Copy trait). Should I erase that from my mind and *always* assume that parameters are copied? (that sounds bad from a performance perspective). 
I realize this doesn't contradict your main point (since it can't lead to leaks) but you can certainly have cyclic references without `Rc`. You can just use `&amp;` together with `Cell`, but in those cases there is no question over who owns the memory.
Took me some effort to remember the original characterizations... Too much hpmor lately. 
I guess it is because you could perform some optimizations based on the fact that a type has a total ordering, which are not sound if the type doesn't have it.
But must we choose between garbage collection and Rust? Servo is partly garbage collected, after all. I hope that eventually someone will write a general purpose `Gc` implementation; this way you wouldn't have to switch to an entirely different language and ecosystem just because you don't want to deal with memory management, and could still manually manage sections of code that would otherwise generate a lot of GC pressure. (Though dealing with library types not written with GC in mind might be an issue.) Alternatively, there's always the Apple-like pervasive reference counting approach. 
Perhaps you're thinking about either Haskell or OCaml. OCaml is famed for being a high performance GCed language, while with Haskell (and some complicated extensions) you can write even more of your program at the type level, enabling more compile-time checks. One problem is that managing memory is only one part of the job. Rust's ownership system also helps with RAII for managing other kinds of resources. In Rust, dropping owned objects is deterministic and causes the destructor to be called on spot, and a GC language would defer collecting unreachable data to a later time. This means that in a language like OCaml you would still be opening and closing files manually. And you must guard for the case where an exception is raised between opening and closing the file! I mean, there *is* a way to guarantee the file is closed by pushing the check to the runtime: Common Lisp calls it [unwind-protect](http://www.ai.mit.edu/projects/iiip/doc/CommonLISP/HyperSpec/Body/speope_unwind-protect.html) and OCaml [can implement it](https://stackoverflow.com/questions/11276985/emulating-try-with-finally-in-ocaml) by using "try... finally" like any language with exceptions (people won't generally use it though, and bugs may happen). But last time I checked, the type system of OCaml can't check at compile time whether the file is *always* closed properly.
Or better yet, use std::num::Int; w.write_all(&amp;transmute::&lt;_, [u8; 8]&gt;(i.to_le()))
When I asked that on IRC, I was told to use bit shifts myself (that's what this family of functions did, internally). I suggest you go to rust-lang/rust and github and search for write_le_u64, read the code and port it for your own use. Even better: understand the workings of these bit shifts will be useful if you do low-level stuff.
Nice comparison :) &gt; Without optimizations, and with debug symbols This is misleading because `cargo bench` does a release build, at least unless you've configured otherwise. I expect the un-optimized Rust performance is rather terrible :)
Beyond releasing memory, the linear typing approach allows some patterns that are not available in languages that do not have it (`std::unique_ptr` I am looking at you...). Rust can encode a state-machine easily, for example, because you are assured that once a transition has consumed a state nobody is going to access it. In languages without linear typing, it would be a run-time bug to access it.
I'll take a look. Thanks!
Indeed as mentioned it the other threads it seems really crufty Rust (just looking at `types.rs`, the number of `match xx.clone()` is bugging me); on the other hand I am impressed by the number of languages that the author has accumulated in his repository!
??? Harry killed Voldemort!
I agree with this. I found your "Alternative Introduction to Rust" very helpful--although I realize that that may be too detailed for an introduction to the language. Perhaps at some point, it would be helpful to have appendices that go into more depth about the implementation details about each topic.
Well, Neville killed the final Horcrux.
Thats pretty good market share to say the least. :)
The last sentence makes this seem as if the entire post was set up just to write that.
Probably, but `transmute` + `Int::to_le` is better anyway.
Yup. So much work to do :(
Rust strings are always encoded in UTF-8. Because UTF-8 is variable-width, there's no cheap way in general to replace one character with another. If the new character is a different width, you must resize the string (which might require reallocating and copying the whole string, invalidating references) and move all of the following characters. If you want to work with bytes instead of Unicode code points, you could use `String::into_bytes` to convert to a `Vec&lt;u8&gt;`. You can also use the unsafe `String::as_mut_vec` to mutate the String in-place, as long as you can guarantee that the end result is still valid UTF-8. **Edit:** Or if you do want to work with a mutable array of Unicode code points, use `let mut chars: Vec&lt;char&gt; = str.chars().collect()`.
I've been seeing a lot of comparisons between Rust and Nim. Is it because they are both new languages? And why all the hate? Can't we all just get along?! :)
Amazon's back-end web services and some front-end web applications were also originally written in C++, though first Perl and then Java displaced most of the C++ code years ago. (Source: I worked at Amazon from 2005–2008.) Aside from the speed/latency for users, reduced overhead on the server can also be good for capacity. When it takes hundreds of expensive servers to run your service (because it serves millions of pages per hour), you can save a *lot* of money by making it use just 10% less CPU or memory. The trade-off changes as hardware gets faster and cheaper, of course.
Never saw .xz on Windows anywhere, is it more a Unix thing?
I was sceptical, but actually really enjoyed this. 
This looks great! I'm so happy that there are people who care about tackling these less-than-glamorous issues. :)
Thank you
You added line numbers to backtraces! That's awesome. Finally backtraces are useful, it's so much faster to debug panics now.
I don't think that rust even has a stable ABI. 
&gt; When they had to make their language that was faster for their needs they still decided to make Go gc'ed. Go wasn't really designed to be a *faster* (at runtime) Java. I haven't seen "faster than Java" as one of their design goals, and their compiler is still pretty simple and doesn't do many optimizations.
The difference is that `String` owns its contents, while `&amp;str` is only borrowing them. If you are dynamically creating a string and returning it, you generally need to use a `String`. If you are returning a slice of some previously-existing string, you may be able to return `&amp;str`. `&amp;str` does not own any storage; it's just a view into some other data, usually a static string literal or some `String` variable. That's why code like this will not compile: fn hello(name: &amp;str) -&gt; &amp;str { let s: String = format!("hello, {}", name); let p: &amp;str = &amp;s; return p; } `p` contains a pointer to the contents of `s`. But those contents are owned by `s` and will be deallocated once `s` goes out of scope, making `p` a dangling reference. In Rust this causes a compiler error. In C++ you could end up with undefined behavior (and probably a security hole). But you can do this: fn first_word&lt;'a&gt;(sentence: &amp;'a str) -&gt; &amp;'a str { let first_space = sentence.find(' ').unwrap_or(0); let word = &amp;sentence[..first_space]; return word; } and then `sentence` and `word` will both point to overlapping parts of the same string in memory, and `word` will be valid as long as `sentence` is.
No problem. :) Glad that clears things up. Even from a selfish perspective I think helping people learn Rust is time well spent, because if Rust reaches a certain threshold of viability then I'll never have to do a big project in C or C++ again :)
Use &amp;str as much as possible because it is more general. It does not own the memory, so it can not outlive it's referent. When return a reference you have to write explicit lifetime. fn longest_common_subsequence(a: &amp;'a str, b: &amp;'a str) -&gt; &amp;'a str { ... (I have not tested it, but it is something like this. Instead of creating a new String object just remember the start and end indices and return a subslice with a[start..end] Don't safe the lengths in a vec of vecs because each needs an allocation that is slow, increases memory fragmentation and scatters the data all over the memory which is cache unfriendly. Use a single vec instead and calculate the index with (y*width + x)
Seems like they love unwrap: let port = tessel::port("a").unwrap(); let mut ambient = ambient_attx4::Ambient::new(&amp;port).unwrap(); loop { let sound_level = ambient.sound_level().unwrap(); println!("Sound: {}", sound_level); }
In your case you are returning string which is part of the `a` input parameter, therefore you can return `&amp;str`. You will need to specify this in the function signature using lifetime specifiers: fn longest_common_subsequence&lt;'c&gt;(a: &amp;'c str, b: &amp;str) -&gt; &amp;'c str Here you are declaring a lifetime named `c` and say that returned value is borrowed from `a`. In the body of your function, instead of allocating new string and putting characters into it, you will need to compute beginning and end of the slice and return `&amp;a[begin .. end]`.
&gt; &amp;T is 4 or 8 bytes no matter the size of T. (Or 16, for non-`Sized` `T`.)
Unwrap is all right by me if the program cannot continue without a value or has no sane strategy for dealing with its absence. At least it makes the panicking behavior obvious. 
Ah, very true. We might even one day support structs with multiple DST members; then the fat pointer would include multiple sizes and/or vtable pointers.
Boost.Spirit is the reference framework for building parsers in C++, so from the ones I've heard of, it is the comparison i'm most interested in. It also does the parser generation at compile time, so it is the one closer in "spirit" to nom. I expected rust to consume less memory than C due to jemalloc, but that is really astonishing.
yeh, bzip2 takes a _lot_ more work to compress and a fair bit more work to decompress. Same story for xz, although less so.
Right, but he was talking about replacing C as an FFI language. I'm assuming that using C's FFI and ABI would be cheating on that bit :P
Well Go's faster than python in general (but that's not hard to do). Where Go outshines Java is in easily doing threaded code. This means that it's easier to do programs that take advantage of Google's architecture of many CPUs everywhere (and therefore are faster for Google at least) than with Java. The whole point is that Google uses a lot of GC code. The C++ code is mostly performance critical parts, or at least I'd hope so.
Yeah, that's what I was thinking. I don't know if there's a good way to resolve it, though, and ultimately it is just a marketing thing...
Is that guaranteed to work?
Oh hey, you added support for plucking file and line numbers out of thin air! Do you think you might want to make `panic!` use this as well, and kill the "failbloat" problem for `panic!` for good? Should be an easy change. Might require an RFC though—I'll file one.
I am merely assuming that if Google invests as much as Go as it is, is because it find Go useful. There may be something we are ignoring here.
If programming languages were bad analogies about programming languages
&gt; Does it use zinc.rs? The firmware of the ARM microcontroller and some SoC code is written in C according to their [github repository](https://github.com/tessel/v2-firmware).
`expect` can be better for documentation and diagnostics though
On `Option`, yeah. On `Result`, `unwrap` uses the built-in error message, which is often what you want. I changed a bunch of `.ok().expect("...")` to `.unwrap()` when I realized this.
Good point! Things are much better now with `Error`/`FromError` :)
That's not "better" IMO. The above code is optimized correctly in LLVM and doesn't use unsafe.
Why do you consider it better?
I assume the user does even less like to get his computer hacked because the out-of-bounds write didn't cause a panic, but rather privilege escalation.
There's also the [programming guide](http://www.anz.ru/files/mediatek/MT7620_ProgrammingGuide.pdf), which at least says where all the registers are, though it isn't the best at explaining what they do. Many MTK/Ralink routers use a driver from MediaTek with dubious GPL status, but luckily we don't have to use it. OpenWrt has MT7620 mac80211 WiFi drivers that are fully open source; we wouldn't have chosen the chip otherwise. We don't even need to ship binary firmware blobs for it, though I think it may have an equivalent in ROM. The SAMD21 has the ability to write the SoC flash over USB, and can also do a USB-serial console, so it could be a fun board for rapid OSDev iteration. We managed to squeeze a JTAG footprint for the MIPS core onto the board in the near-final formfactor too -- I can't 100% guarantee that will remain, but now I have a good answer to "Why would anyone ever need that?".
That's great information, thanks :)
I'm so happy we have moderators like you. Seriously, rust has one of the best language communities that I've ever seen.
There's new stuff? I kinda gave up waiting for more a year ago. Oh boy oh boy!
We should send the nim community a cake on their 1.0 birthday.
Theoretically rust should eventually beat nim accross the board? Or maybe not? I know gc isn't always the worst but still.
I think it is a good direction, but that would need a universal support of line numbers in backtrace for every supported platform. Currently it works only with Linux (as libbacktrace only supports ELF/DWARF).
That's mostly a difference between "archiving" formats and "compressing" formats: 7z does both archival and compression, xz only does compression. Both have their pros and cons (compressing-only formats inherently do the solid compression, while archiving formats will allow faster per-file operations), but one particular advantage of xz in this case is that it is a drop-in replacement of gzip, which is commonly used to produce tarballs.
I have some rough ideas for how to enable tracing GCs in Rust. The only major problem I've run into as of yet is dealing with collectors that require write barriers; current working idea is to add an "arena" marker to lifetimes.
With Elmer Fudd on it? :D
It could be argued that such data does not belong in a archive file though, but I guess this is not the place for such a discussion.
We could continue to use the old format on platforms where it isn't supported and evolve it over time.
That construct is really crying out for some syntactic sugar.
It's a little early, but perhaps later this year!
IMO singular vs. plural was ambiguous, in case the reader didn't know that seemingly only the OP of the thread was posting on HN and in that PR, as opposed to it being a group from that thread. He is for that matter a guy.
&gt; You can certainly prove that some kinds of code are valid using formal methods; but it's certainly not a generic solution; otherwise we'd simply use formal verification entirely and not require the borrow checker at all. The borrow checker is a formal method, built-in to the compiler. So yes, Rust is trying to push the boundaries of how much static formal verification happens in industry languages. &gt; The internals of sort are an excellent example; that's not a formal verification, its just a bunch of comments. There could easily be bugs in there. Who knows? You really do actually have to prove correctness. I never said it was formal verification. I said it was a mathematical proof (exactly the sort of thing one would find in a mathematics paper): a human would sit down and verify that the proof is correct and corresponds to the code, and would then be happy that the `unsafe` code is correct. Yes, mistakes can happen, no-one is contesting that, but bugs can happen in an implementation of your favourite "actually memory safe" language too: if you're going to lambast a set of clear comments that prove a piece of `unsafe` code correct, you should also be complaining about the thousands of lines of C inside CPython etc. The benefit of `unsafe` code in Rust is that it is clearly marked, so one knows the few places where the compiler can't automatically complete its proof of "the code is memory safe". Humans can then focus their verification efforts on those precise spots so that your dismissively-labelled "ad hoc" code reviews (they could be, you know, not-ad-hoc, e.g. people could sit down for 2 hours a week and work through piece of `unsafe` code) can maximise the bang-for-buck.
What's interesting about mentioning what Google/Amazon/Twitter... does in cases like this is that they are all famous companies, which are based on consumer-facing products. In turn, they wouldn't be famous if they didn't get a lot of people to use their products. In turn they are going to have a lot of use for server-juice. Which in turn makes efficient use of those a real concern. On the other hand, it's the less famous and widespread user-facing products that might not be concerned so much about how many servers they are using, since they don't have a critical amount of traffic. So they might not really care so much about squeezing out all the juice of their servers, compared to just getting their code running in whatever reasonably fast way. But we hear less about such use-cases since - by necessity - they aren't famous companies. Because if they *were*, they would have a big need for server-juice, and they in turn might have changed to something like C++. I wonder how many programming positions that work directly on under-served servers than on busy server-farms, though?
Oh, I know it was ambiguous; I'm not faulting davebrk for taking it the wrong way :)
Experience and age are probably big factors. When I started programming a long time ago I didnt care about safety I just wanted to make stuff and it probably took me years to evolve into thinking that safety is actually important. The reason Rust gets so much attention is because it is innovative in so many ways, thats what got me into trying Rust. We still have to see how it works out but I have good hopes.
There is [`std::iter::range_inclusive`](http://doc.rust-lang.org/std/iter/fn.range_inclusive.html), but the unstable message says it is likely to be replaced by range notation and adapters?
There is! It is named `map` and I'm really annoyed that almost no one is using it. It would look like (untested): tessel::port("a").map(|port| ambient_attx4::Ambient::new(&amp;port)).and_then(|ambient| loop { println!("Sound: {}", try!(ambient.sound_level())) });
Since CPython runtime is not hardened unlike JavaScript, the answer is yes. As a matter of fact, Python sources that segfault the interpreter is part of CPython repository as a TODO list. https://hg.python.org/cpython/file/tip/Lib/test/crashers
Looks gimmicky as hell.
&gt; Nim looks cute, but the design is rather haphazard, just piling things on. There seems to be no driving philosophy. Exactly! One thing I really love about Rust is that even though I may not always agree with every design decision, they at least have been *very* well thought out with very clear goals in mind. This makes the language feel so much more...*professional* for lack of a better word :)
I believe you mixed up `map` and `and_then`: The function passed to `map` is expected to return a new value, not a `Result`. For chaining `Result`-returning operations you need `and_then` (which can't be used as on your post, because it needs to return a `Result`, not `()`). 
FYI: the braces in your code aren't necessary. for byte in 0u8..256u8 is sufficient.
I depend on `byteorder` and I think it's better than what we had before. The main difference is that the functions in `byteorder` are generic over the endianness, so it's really easy for you to create endian-generic functions as well.
I completely agree, and Azul's exciting late safepoint placement work, now upstream, paves the way here. Stay tuned :)
My guess is that the overhead is caused by the fact that the haskell code will be run in one (or more than one) of the os threads that the haskell runtime maintains and not in the os thread that the C caller runs.
As long as Mozilla doesn't bundle some advertising browser toolbar with Rust I'm not concerned about the future of Mozilla. 
Or use a separate function that you call from `main`, or even a nested function.
If programming languages were reddit comments complaining about other reddit comments complaining about bad analogies about programming languages. Ok, enough off-topic commenting for me for today.
Rust runs 10x faster than nim when compiled with optimizations. If you ever hear anything about rust being slow, make sure they are optimizing their code.
Damn. Well, he stopped being too MarySueish already so his biggest problem (as a character) is solved. He'd see this differently though 😁
It was, that's why I said the *start* was weak
is this just lisp GC mapping to NIM GC, or a size:speed tradeoff (GC can be faster if you let it gobble memory?)
There's a few things you could read, the best thing I've seen is the netmap paper though, which has a fairly good breakdown of where time is spent in the FreeBSD network stack and how it overcomes the overhead - http://info.iet.unipi.it/~luigi/papers/20120503-netmap-atc12.pdf. Most of the other things are either marketing or have less interesting information (depending on what exactly it is you're interested in of course!). There are quite a few current solutions for bypassing the kernel to saturate 10gig+ links which you could take a look at, and might find some interesting information by reading about (let me know if you do): * Intel's DPDK (http://dpdk.org/) * Netmap (http://info.iet.unipi.it/~luigi/netmap/) * MegaPipe (https://www.usenix.org/system/files/conference/osdi12/osdi12-final-40.pdf) * PF_RING ZC (http://www.ntop.org/products/pf_ring/pf_ring-zc-zero-copy/) * Solarflare's OpenOnload (http://www.openonload.org/) * PacketShader (packet processing with the GPU, http://shader.kaist.edu/packetshader/) The general idea is that the overhead of copying memory about, performing lots of system calls, and poorly abstracting the hardware is big enough to make saturating 10gig+ difficult/impossible, and to cut down on this overhead as much as possible. They all take slightly different approaches, with varying levels of portability/open source-ness, but the core ideas are generally the same. Needless to say, I'd happily accept pull requests for [libpnet](https://github.com/libpnet/libpnet) adding support for any of these backends! As I mentioned earlier in the thread, netmap support is already there, and I started on work on DPDK but unfortunately did not have time to finish it.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Nim**](https://en.wikipedia.org/wiki/Nim): [](#sfw) --- &gt; &gt;__Nim__ is a [mathematical](https://en.wikipedia.org/wiki/Mathematical_game) [game of strategy](https://en.wikipedia.org/wiki/Game_of_strategy) in which two players take turns removing objects from distinct heaps. On each turn, a player must remove at least one object, and may remove any number of objects provided they all come from the same heap. &gt;Variants of Nim have been played since ancient times. The game is said to have originated in [China](https://en.wikipedia.org/wiki/China)—it closely resembles the Chinese game of "Tsyan-shizi", or "picking stones" —but the origin is uncertain; the earliest European references to Nim are from the beginning of the 16th century. Its current name was coined by [Charles L. Bouton](https://en.wikipedia.org/wiki/Charles_L._Bouton) of [Harvard University](https://en.wikipedia.org/wiki/Harvard_University), who also developed the complete theory of the game in 1901, but the origins of the name were never fully explained. The name is probably derived from [German](https://en.wikipedia.org/wiki/German_language) *nimm* meaning "take [imperative]", or the obsolete English verb *nim* of the same meaning. &gt;Nim can be played as a *[misère](https://en.wikipedia.org/wiki/Mis%C3%A8re)* game, in which the player to take the last object loses. Nim can also be played as a *normal play* game, which means that the person who makes the last move (i.e., who takes the last object) wins. This is called normal play because most games follow this convention, even though Nim usually does not. &gt;==== &gt;[**Image from article**](https://i.imgur.com/T60Jf12.png) [^(i)](https://commons.wikimedia.org/wiki/File:Subtraction_game_SMIL.svg) --- ^Interesting: [^Nuclear ^Instrumentation ^Module](https://en.wikipedia.org/wiki/Nuclear_Instrumentation_Module) ^| [^Nim ^Farsakhi](https://en.wikipedia.org/wiki/Nim_Farsakhi) ^| [^Dang-e ^Nim](https://en.wikipedia.org/wiki/Dang-e_Nim) ^| [^Nim ^Chimpsky](https://en.wikipedia.org/wiki/Nim_Chimpsky) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cp64s1i) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cp64s1i)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Being treated like you're an extension of the hive-mind of how the Rust community is viewed sure feels great. EDIT: A downvote? That's not chivalrous rust-behaviour, tsk tsk.
What do you base that on? I'd say that we'd need a pretty comprehensive set of benchmarks to say something like that with any generality. For all I know we already do, though.
Why/how would it not work?
I'm more than happy if that works :) I had some vague memory that transmute (except in the stdlib) is not guaranteed to work until there has been made a decision about which transmutes are okay.
I think you mean rule 4. Which is in the sidebar for anyone to see at everytime. Yet the mods feels like pointing it out and reminding us all the time, as if we're children. &gt; If you ever want to learn more about why we like rust, Who do you think I am? I read this thread because I was browsing this subreddit, not because I came from the Nim forum or anything like that. I have no damned affiliation with Nim. 
Cool, thank you!
Is it? Isn’t the `map` inlined away?
&gt; /r/webdev discusses Rust Not really, it's just a few people discussing different languages and Rust is one of them.
Ah so their lisp is probably in old non-optimized rust? I guess that makes sense. (provided it is right anyway)
I guess I take issue that we even have to ask that question :p We don't want to go the Haskell way of performance, where learning to think like the compiler becomes some sort of game.
&gt; This is an attempt to make the fastest IPC on the planet! The fastest IPC in a multicore system would be a busy-waiting one, avoiding any sleep and thus any context switches, wouldn't it? Anyway, cool stuff!
[In other words](http://www.smbc-comics.com/?id=1623) - a newspaper exaggerates things for readership, news at eleven. 
I don’t know, but std::mem::transmute is maked "stable".
I have yet to see a single reliable measure of browser share. All sources that report browser share seem to completely disagree. 
I am okay with this :D
That'll slow down the actual work being done by those communicating processes. But if you want the minimum possible latency, it's a good choice. A common setup in high-frequency trading is 7 threads running on an 8-core box, with one core reserved for kernel interrupts. The threads are each pinned to a core, and they lock + prefault their memory so it's resident in physical RAM. They communicate by busy-waiting on shared memory ring buffers, and they talk to the network card (which may be a custom FPGA dealie) in much the same way. At that point the kernel has almost no involvement in the core operations of the system. (The obvious follow-up question is, why not run your trading strategy in kernel mode, or use some kind of uni-kernel? People probably do this as well; my knowledge of the industry is somewhat out-of-date.)
It's kind of funny how the CPUs use message passing to emulate shared memory so that the software can use shared memory to emulate message passing...
Awesome, thank you very much! I'll swing by #libpnet at some point...have a few questions for you. This low level networking stuff is fascinating, scary and very new to me :)
How does partial application lead to less predictability with regards to optimization?
The reason for choosing eventfd over futex is that one thread can wait for many eventfd's at a time (using epoll), but only one futex. That makes fds more flexible. Whether futexes or eventfds are cheaper is not obvious. [This benchmark](https://github.com/alk/shm_pipe) claims that eventfds are cheaper than futexes, but maybe one can find benchmarks that point the other way too. Maybe it varies depending on the arch too. So; the signal mechanism wraps the ringbuffer (the `fdbuf` wraps `ringbuf`) so it would be easy, hopefully, to write another signalling mechanism (i e futexes) to see if it is faster. Could be interesting!
The downvotes might be because your comment appears to be ambiguous and confrontational. Maybe you could clarify what your issue is?
I find the constant reminders about how-one-should-behave to be tiring and overbearing. The rules are in the sidebar. There is a sticky post debating conduct at the top of the subreddit. Why does the issue have to be brought up again and again? It's related to the concern of certain members for the rust community to be looked at favourably from the outside. Which is seen in comments that basically say "before you go out into that brave world, be sure to represent Rust well! Be a good marketer for us!" And it's not just about being civil and respectful - they want the rust community to be looked at in a good light so that rust-lang gets street-cred by proxy (because who wants to use an up-and-coming language with a shitty community?). That's fine, but sometimes it seems to border on social-engineering territory. I view it, sometimes, as wanting to treat rust-the-community as a (benevolent) hive mind rather than adults[1] who are themselves responsible for their own conduct, and whose *lack* of conduct would reflect badly on *themselves*, not on any community. [1] Granted, there might be kids on this subreddit. Brave kids for investigating such a cutting-edge language, certainly.
I personally don't want to do anything like that, I'd rather work on a higher level, like the Cap'n Proto capabilities or even higher. Spent entirely too much time already on various kinds of IPC back in my day, thank you very much. Was reacting to the "*This is an attempt to make the fastest IPC on the planet!*" phrase in your README, that's all. Don't take it too seriously. Still, if you're doing active streaming then busy-waiting might actually take less power because the CPU won't need to spend its attention (and cache) elsewhere. And some CPUs will put the unused modules to sleep while you're busy-waiting. Depends on the use case, etc.
I'm not certain that it does. But I know that GHC does a lot of work to un-do currying, infer the "real" arity of each function, and generate wrappers for the cases where it is partially applied.
Yeah I see that you typing "true safety" (which is vague) was a typo. I guess maybe both of us dislike diluting terms (like *memory safety*) which are already well-defined.
Maybe it's also just personality types. I'm relatively inexperienced, but I feel like I'm gravitating towards more rigorous languages, like Haskell and Rust. And that's without having footgunned myself even once on some low-level bug that has to do with memory (un)safety.
Well, I would certainly disagree with your argument, mostly because the above comment seemed to me to be specifically about /r/rust. But besides the actual topic, thanks for putting in the time to expand on your comment. 
&gt; Well, I would certainly disagree with your argument, mostly because the above comment seemed to me to be specifically about /r/rust It's about /r/rust and certain members of it. Is that not /r/rust - specific enough? This subreddit is the only *internal* arena in which I personally interact with observe and people from the rust community. (external places would be places like proggit and HN.)
Sorry, either I'm misunderstanding or bad at phrasing :) My point was, the comment by /u/dbaupp was specific about acceptable behavior in this subreddit. There's not much one can do in other places, besides voicing ones disagreement.
My comment was both about about the repeated *reminders* about acceptable behaviour in this subreddit, and about voicing and "dictating" how to behave in other venues. They are two different things, but I think they are related.
&gt; Why does the issue have to be brought up again and again? Because this is the Internet (and on top of that, this is Reddit) Wanting to maintain clean discourse is not just about how the community is viewed. Its about internal community health too. I moderate a couple of Stack Exchange sites. On one particular site, we had a rather smart, but abrasive user. Because of his smartness, he gathered a cultish group of supporters who would get incensed whenever he was told to behave by the moderators (I was not one of them at the time). Quite often the argument that "We're all adults and academicians here; its patronizing to tell us to behave as if we are children" was put forth in some form or the other. His presence helped the community (awesome posts), but it also poisoned it. Some potential new members on seeing his comments were driven off. Some longterm contributors cut down on participation in topics where he lurked. Quite often incensed discussions about his posts were had in chat. Finally, he crossed a line and was banned for a longish period of time; after which he said he was never coming back. This fragmented the community since his supporters were rather angry (many left). Eventually, after the dust settled, we were left with a much better community. Its prospering now, and we don't have unhealthy altercations all the time. But it would have been even better if his behaviour had been met with a firm hand from day one. There would have been no fragmentation, and the community would be a nice one from the start. Being explicit about these things helps. The redundancy in the rules is a good way if achieving this. Nobody reads the code of conduct sticky (its like an EULA) unless they've steady spent time on this sub. Sidebar rules are there, but specific examples - "this is bad" - are much easier as far as retention of rules goes. Threads discussing languages have a tendency to fall to zealotry and personal attacks; having a reminder near the top isn't too bad. ------- Addressing your point about adults; that may be how you assess a community with some bad apples (and it is how I do), but not how the Internet does in general IMO. If a forum allows ad hominem, many people will avoid joining. Again, this is reddit, and people tend to think that trollery, ad hominem, etc are okay. Because to a large part, they are. /r/rust tries to avoid these sorts of comments. It's not about marketing the language. It's about the health of the community of this subreddit (and by extension the Rust community). 
The idea behind `futex` is that you do NOT need to call `FUTEX_NOTIFY` each time writer modifies collection. Writer can check the flag in shared memory, and if flag is already set, do nothing. To make that work, reader must have atomic (check flag, and if flag is zero, wait) operation (and that is a semantic of `FUTEX_WAIT`). So under heavy load, `futex` syscall is rarely used, thus `futex` becomes cheaper. AFAIU, this is true for multiple-producers-multiple-consumers queue, and it does not apply to single-producer-single-consumer queue. Please correct me if I'm wrong.
The ringbuffer implementation has somewhat similar benefits in itself - as long as the buffer is not completely empty or completely full, there are no syscalls, just atomic operations. And this would typically be what happens most of the time under heavy load, i e the buffer would stay half full.
Switching to `ringbuf` in order to busy-wait is a neat trick, you should mention it in the README, IMHO.
The problem is that you want to still access a variable even though you've given it away, all because you wanted to avoid true recursion. In reality this is not the kind of recursive thing that, AFAIK, can be tail-call optimized. In short, first you must append the previous element to the next (do the recursion) *and then you append it*. Here's my [take on it](http://is.gd/yfK9kf). I also took the liberty of cleaning up a bit of the code and fixing some issues. Sorry about the terrible function name `add_all_rec` but I couldn't think of something better. Notice this is very similiar to what /u/HeroesGrave wrote, but this makes why this is happening a bit cleaner. Basically the problem is that you don't realize that borrowing mutably the contents of a Box from a mutably borrowed box would allow for two mutable borrows of the contents. The option hides this away because the lifetime issue takes precedence over you not having access to a mutable borrow, or at least that's how it appears.
**Summary:** - RefCell is hard to use with borrowing and lifetimes. - You can borrow multiple fields of a struct at the same time. - Type erasure (`Box&lt;Trait&gt;`) is not a good solution if you have any sort of data associated with it. - Unless you’re doing FFI, there’s usually a way to avoid unsafe code. You should try and find it. - Abusing escape hatches makes your code easier to write but harder to use. - Abusing macros makes your code harder to write but easier to use. - Rust is a fun language. **ecs-rs** can be found [here](https://github.com/HeroesGrave/ecs-rs). The new, slightly incomplete **tutorial** is [here](https://github.com/HeroesGrave/ecs-rs/blob/master/doc/tutorial.md)
I think it's probably too late to change the default visibility behavior, but I can imagine a `crate pub foo` construction that does what you request. Though I'm not sure how it jives with name resolution.
The nebulous *Internet* where everyone is a seething troll until proven otherwise. Give me a break. 
Don't put words in my mouth. ThenInternet where people are more prone to trollery because of the atmosphere and anonymity.
&gt; as a side note, I would really love to see crate-only visibility in addition to public and private This would be awesome! I've been having similar issues with needing to access fields outside their module, but I hate making them public to the world.
I think "on" Elance is more accurate. "At" Elance sounds like Elance the company is using Rust. Nice to see the first bits of rust work popping up though!
Hah, this might be my cue to go back to freelancing - I wonder if my Elance account is still active.
I wish there was a macro for that. )
Yeah I was thinking the same thing. What I ended up doing was putting all my code in a separate function ``try_wrapper`` and invoking that with a match in main. Honestly it feels like a hack. Is there a more canonical way to do what I'm trying to accomplish?
The result would be identical, so what's the point? At most, it would prevent simultaneous uses of both the original and the copy, but given that those uses could only be in the same block with the `move` operator... the difference for the user is almost precisely **0** (modulo rounding errors). I personally consider opt-in `Copy` to be one of the 1.0 mistakes - I sadly weren't able to implicate myself in the decision process for it, nor did I have a better alternative at the time. Opt-in `Copy` was designed to protect from unsound unsafe code, which largely means pointers. The `Send` model, where `*const T` and `*mut T` **do not** implement the trait, and structures containing thrm have to opt-in, would solve the problem beautifully. Yes, some people have complained about that one, but so far, from them, in defense of more permissive behavior, I've only seen dangerous unsafe code with not enough abstractions - allowing either misuse of a safe API(!!) or making it very hard to reason about - both things which we want to defend against. On the other hand, opt-in `Copy` tries and fails to: * prevent large copies - `[u8; 1 &lt;&lt; 31]` is `Copy` - this is better done with a lint, which could have a custom threshold and even take optimizations into account (one can already `--emit=llvm-ir` and grep for `llvm.memcpy` calls with a constant length) * prevent copies of values that represent "resources" which aren't unsafe to duplicate or forget (which is why they don't implement `Drop`), but doing so is likely unintended - the only example I know of is `Datum` in the compiler, which has to behave like a linear type for rvalues, mimicking the actual language semantics (an rvalue is either used or dropped, but drops are explicit uses during translation) - this really wants linear types and/or being dependently typed on the Rust type of the held value (would that be meta-recursive?) Both of those cases get a warning telling the user to add `#[derive(Copy)]`, which means it's even less effective by default. And we're still missing the ability to (unsafely) implement `Copy` on wrappers around types which are not affine, but don't implement `Copy` themselves - something you can do with `Send`. That said, I'd be happy to find out there's actually a good reason for the sad state of affairs and I welcome /u/pcwalton and /u/nikomatsakis to prove me wrong.
Dunno. I'm used to using a closure http://is.gd/lUbWbU or making a macro. Arguably if you're seeing this then the error bubbled all the way up to your top-level code, which is usually a `main` or an FFI method or a subsystem boundary. That kind of code probably should only do the integration and error-handling part, with all the real work happening elsewhere. Top-level code might end up switching from error propagation to error handing, e.g. not using the `try!` because its use no longer makes any sense: you're on the top, there's nowhere you'd want to bubble the errors to. If you'd want a quick and dirty error handing in `main` you might use the `.ok().expect("message")` combo on the Result: http://is.gd/exzf9E
What I'd like to see is the automatic merging of the given (error) types, allowing one to avoid all those FromError declarations. There was an RFC for that.
It's not too hard to add an Inherited field to your custom error enum. pub enum CustomError { NormalValues, Inherited(Box&lt;InheritableError&gt;), } impl &lt;E: InheritableError + 'static&gt; FromError&lt;E&gt; for CustomError { fn from_error(err: E) -&gt; CustomError { CustomError::Inherited(Box::new(err)) } } pub trait InheritableError: Error + Send { } impl &lt;E&gt; InheritableError for E where E: Error + Send { } This works with #[derive(Display)], so it doesn't make it hard to implement Error for `CustomError` either.
No, it *is* way too hard. Making a new error type should be a one-liner, kind of like making types in this presentation which also explains why it's cool: http://www.reddit.com/r/rust/comments/2nvwu2/functional_programming_patterns_buildstuff_14/ Rust promotes bad software design my making new error types punitively hard to create. Scala: case class MyError (message: String) extends Exception (message) That's all! `try!` simply doesn't grok it. It's sad to see a whole PATTERN emerge around FromError in Rust. Language support here is sorely inferior. &gt; pub trait InheritableError: Error + Send { } &gt; impl &lt;E&gt; InheritableError for E where E: Error + Send { } Oh, cool, now I can't even match on the underlying error. We ended up in a much worse situation than we were in Java or C++. You're kidding me, right? P.S. There are crates that make the error handling usable, like https://github.com/jmesmon/rust-err and https://github.com/reem/rust-error. Someday something like this will make its way into the standard library.
There is a Cargo issue for this: https://github.com/rust-lang/cargo/issues/1359
I always wondered why Go compiles so much faster than Rust.
I wonder if it's because I'm trying to update a project that's ages behind so in the progress of fixing bit-by-bit I've still not had a successful build. Is there some logic in cargo such that even successful dependency builds aren't re-used if the overall build was a failure?
Cargo does seem to rebuild dependencies with `path = "some_dir"` unconditionally.
I saw something about it somewhere... I think it was because of the not-so-optimized LLVM IR from rustc, combined with more time spent on optimizing. Most of the time is spent in LLVM, if I remember it correctly. The Go developers has had some time polishing the output while the Rust developers have been more focused on creating a language. Edit: Basically what /u/-Y0- [said](http://www.reddit.com/r/rust/comments/2y8gue/optimized_dependencies_in_cargo/cp78lcm).
I think there are several reasons, main one Go was made with compile fast mindset from the start. Rust tries to be fast in execution first and in compilation a distant second. Crates also make sure you only need to compile the stuff you changed, not everything (I'm not saying that Go, doesn't or does have smaller compilation units, just why Rust doesn't see this as a major problem). Another major reason is that Rust spends according to stuff I heard a lot of time in LLVM optimization pass, because the IR it outputs isn't really the cleanest possible, so LLVM has to bother more with it. If Rust would clean up the IR and add incremental compilation/codegen, it could gather quite the speed boost. Most likely not as fast as Go, but definitely faster than now.
This was [cross-posted to stack overflow](http://stackoverflow.com/q/28902760/155423). 
Better yet, send everyone in the core dev team some cupcakes :)
I know cargo will rebuild everything after you update rustc, is that what you're talking about? If it's just the speed of compiling one crate, compilation can be sped up more after 1.0, it's just that std stability is the current major focus.
On a side note, if you change the match to an `if let` rustc will panic! For calling `.unwrap()` on a `None` but I can't seem to figure out why?
Perhaps with a private repo. With a public one , someone could then just look at the source of .Travis.yml for the token.
* language designed for fast parsing and analysis (much like Pascal in its time) * no IR? Not sure about that one, but I'd think the compiler can do its codegen more or less in a single pass (like [Turbo Pascal](http://prog21.dadgum.com/47.html)) * almost no optimisations the latter is a pretty important one, and speeds up the compiler more than just asking the compiler for no optimisations (-O0): not having optimisation code in the compiler means the compiler is smaller and has less branching (no need for all the architecture surrounding optimisation passes). Witness [tcc](http://bellard.org/tcc/) which produces relatively slow code compared to e.g. GCC or MSC, but is so fast at compiling said code [you can use it to boot from the Linux kernel source](http://bellard.org/tcc/tccboot.html), said code being compiled by the bootloader.
That'd be nice, but it'd require implementing UAX#29 and adding locales support, plus a conditional API to handle extended and legacy clusters (and the aforementioned locale input for tailored clusters). Not impossible by any means, but I doubt that was high on the list of 1.0 blockers.
Since this is the main function we're talking about, having it return `Result&lt;(), Box&lt;Error&gt;&gt;` and `try!` working with all error types inside its body, seems like a good compromise to me.
One step closer to minimizing the proliferation of naive benchmarks.
There is no performance conequence. The reversal is in the *iterator*, the vec itself isn't reversed.
Ah ok, thanks for thr explanation!
This is a known bug: https://github.com/rust-lang/cargo/issues/1386 It is fixed and has already landed on the latest nightly.
Doesn't the latest nightly come with cargo from 2015-03-04?
One thing I do, which some C and C++ programmers do, is just compile with optimizations turned on always, and debugging turned on. If I remember correctly (no access to a computer), this is how you do that: [profile.dev] opt-level = 2 debug = true It means slower builds, but, at least in my experience, it isn't terrible. Edit: opt-level, not optimization
And I think that this would be a backwards compatible change, because you could allow both no return value, like now, which would be a designated Ok(()), and the new thing.
Thanks. Once again very helpful.
Great work. I came across your project before, but I noticed the Vec&lt;u8&gt; hack and the unsafe code and got scared away. I decided to start in on a new one but it seems your re-factor is just want I wanted to see. Great work, I'll most likely be integrating this into my project A.S.A.P. :)
There was a [discussion about this](http://www.reddit.com/r/rust/comments/2wj6fh/thoughts_of_a_rustacean_learning_go/coregfc) on another thread recently.
That doesn't look better to me. I can't even read the whole thing, it gets cut off on the left of my screen. tessel::port("a").map(|port| ambient_attx4::Ambient::new(&amp;port)).and_then(|ambient| loop { println!("Sound: {}", try!(ambient.sound_level())) } ) ); Wrapping it properly, I also noticed that you missed a trailing paren. This is maybe slightly better than the original, though as /u/TeXitoi points out, monadic do notation would probably be better yet.
Ok(()) and () are different types, so it isn't backwards compatible AFAIK.
If you had some object `&amp;Foo` whose method took an `&amp;Self`, what is the actual type of `&amp;Self`? It can't be another `&amp;Foo`, because the actual types might not be the same. For example, if you wrapped an `usize` in a `&amp;Eq`, and a `String` in `&amp;Eq`, you would be able to compare them, which the language doesn't know how to do. If you don't care what `Foo` is at runtime, pass in a `&amp;Foo`, not `&amp;Self`.
Rust's `fn main()` is **NOT** a FFI function in Rust, among other things, it doesn't return an integer. It's called by the [`start` language item](https://github.com/rust-lang/rust/blob/b8c6eb179e9e792c5470abbe2b6eeadc16f34565/src/libstd/rt/mod.rs#L65) which has no reason whatsoever to take a raw pointer instead of a nice `fn()` pointer (it just was never fixed), and could be made generic over the return type of `main`.
You cannot, because references cannot be `null` in valid Rust. You should define your function as `pub fn uuid_is_null(uu: *const Uuid) -&gt; c_int`. To obtain a `null` pointer, you can call `std::ptr::null()` or `std::ptr::null_mut()`. So you can test your function as `uuid_is_null(std::ptr::null())`.
[This](https://doc.rust-lang.org/std/error/index.html) page has a pretty good example on how to use Error and FromError.
I'm glad you got rid of unsafe code! Your ECS seems to be getting popular, and seeing how it wasn't updated for 3 weeks before now got me a bit suspicious. &gt; it wasn’t too big of a deal because components really should just be POD This is not obvious to me. Why should they be PODs?
According to a recent article (not to hand), Go works with an AST, but they are considering switching to their own IR to enable more optimisations, but still aiming to keep the short compile times. (They've just converted their compiler from C to Go (automatically), and are now cleaning up the resulting source.) As you say, it is designed from the start to be a simpler language anyway.
[RFC 201](https://github.com/rust-lang/rfcs/blob/master/text/0201-error-chaining.md) describes the design and motivation for the current `Error`.
I have no idea, but to help anyone with compiler knowledge [this is what stampede currently looks like](https://github.com/kinghajj/deque/blob/master/tests/lib.rs#L86)
`main` is an interface used to run a Rust program from the OS. The OS starts a program and gets an integer out of it. As such, `main` *IS* an FFI function, by definition: "*A foreign function interface (FFI) is a mechanism by which a program written in one programming language can call routines or make use of services written in another.*" OS, written in one language, calls your program, written in Rust, and expect an integer from it. On Rust side, it's boiled down to the FFI interfaces `main`, `args` and `set_exit_status`. The `lang_start` wrapper is a hidden implementation detail and has nothing to do with this discussion. Someone proposing to make `main` special better tell us his reasons. As I've stated above, if there is a solution for `main` (to which problem, I wonder?) then I don't see why it shouldn't be scaled to every other FFI method out there.
I tend to think that Rust is a good language. But then I see job postings and then I think that Rust is *too* damn good.
If you've followed their developments, they are already reaching twice their goal **in specific scenarios** ;)^([citation needed]). Being that good for the entire internet? They might just give a proper definition to "webscale" then. I can only applaud them from the sidelines and wish them great success on their many journeys to come. ^^^^^^^^^^^^^^^((or I could maybe finish that safe Rust-only on-line PNG decoding library that was competing with libpng. maybe. or upstream my patches to make Facebook/Polymer work in it. when's their birthday?)^)
There was also Samsung OSG which has Rust as one of the open-source projects they're interesting in (likely related to the fact that they did/do some of the work on Servo).
Try to create a variable of type `&amp;Ord`. #[derive(PartialEq,Eq,PartialOrd,Ord)] struct Test { foo: i32, } fn main() { let t: &amp;Ord = &amp;Test { foo: 1 }; } You will get an object-safety error: &lt;anon&gt;:7:19: 7:35 error: cannot convert to a trait object because trait `core::cmp::Ord` is not object-safe [E0038] &lt;anon&gt;:7 let t: &amp;Ord = &amp;Test { foo: 1 }; http://is.gd/HX18z9 This is a _usage_ error, not a _definition_ error. What works, is a variable of `&amp;Test`: #[derive(PartialEq,Eq,PartialOrd,Ord)] struct Test { foo: i32, } fn main() { let t = &amp;Test { foo: 1 }; } Then, when calling `cmp`, the exact type of `&amp;Self` is known - `Test`.
Spamming me with implementation details won't chage the fact that there is an interface between the OS and the Rust program and that to the programmer it's embodied in the `main` function. Error handling on the boundaries between the in-Rust and outside-of-Rust code is a generic problem, `main` being only a single case of it. &gt; If returning a Result is more idiomatic than panicking, in any free function It isn't. Only a function that delegates the error handling to it's user would return an error. A lot of code is wired to handle it's errors. For example, an AJAX handler won't be returning any errors, instead it will log them and report them back to the browser. There is no one on the other side of `main` to handle the Rust errors. (I'm not speaking of `lang_start` here, I'm speaking of semantics!) An error returned from `main` is semantically similar to an uncatched exception, it has no other choice but to `panic!` (or do something indistinguishable similar). When we're implementing a function which returns `Result&lt;..., Error&gt;` we know somebody out there will handle the errors. When we're writing `main`, nobody will handle the errors, the `Result&lt;..., Error&gt;` in that case is just disinformation. It's nowhere near idiomatic. Let's consider the following scenario: `main` starts an embedded web server, then tries to start an embedded FTP server, fails and because Rust allows it, returns an `Error`. You have only three choices in `lang_start`: 1) Ignore the error. Leaving users without FTP with no one the wiser. 2) Print the error to the standard output or standard error, possibly breaking the program which might be using these. Leaving users without FTP with no one the wiser. 3) Panic. None of these is better or more idiomatic than if the programmer explicitly used the `panic!` (or handled the error because compiler told him to!). All it achieves is having the programmer confused and making the program behaviour less transparent.
Does writing a shitty pseudo-CSS engine in Python for a graph renderer count? :P &amp;nbsp; ^^^Probably ^^^not
&gt; we aim to succeed by miles, not inches I would prefer suceeding by using SI and not that US crap ....
But SI is soooo much less romantic... "we aim to succeed by kilometers, not centimeters" is the very antithesis of poetry. :)
'murica
Great tip! I've been doing that in C,C++. As a benefit can debug code in production if needed with gdb. Bencharking showed acceptable performance degradation from doing it this way.
Have you tried the map? Are you worried about performance in a unit test? I am fairly sure that performing a map in Rust is much faster than constructing a scaffolding of objects in Python. 
The latest nightly is this: rustc 1.0.0-nightly (270a677d4 2015-03-07) (built 2015-03-07) cargo 0.0.1-pre-nightly (dd7c7bd 2015-03-04) (built 2015-03-04) The problem that caused cargo to be slow is in rustc, not cargo.
Thank you :)
That link goes to the source in the "master" tree, which will change over time. This link goes to the source at whatever commit happened to be master when I clicked the link: &lt;https://github.com/kinghajj/deque/blob/7bb3af072b6599f5b5cafe8b27ecd2d0ee3bf673/tests/lib.rs#L86&gt; When you link to a specific source file on GitHub, especially when reporting an issue and referencing a specific line number, you should always link to the tree at a specific commit. That way the link won't become outdated. You can get this URL by tapping the `y` key while viewing a source file on the master branch.
Fair point. I'll change the text for the next version.
Very exciting :) On my end though, I'm still waiting for Rust jobs that are fulfillable by non-geniuses ;)
I actually really like C/C++! Does this make me a dark wizard?
I find the whole concept of object safety a bit weird. In haskell, existential types (which can be used the achieve the same thing as trait objects in rust) created for a particular class, are not automatically instances of that class. This way, if you define say an existential type for `Eq` and then try to define an `Eq` instance for that type, you will not be able to do that (at least in a meaningful way). You don't need to add a new concept to the core language to prevent that.
Maybe "by tonnes, not grams"? But that requires switching to weight rather than distance.
"We aim to succeed by parsecs, not angstroms"
Another option would be to define a trait and implement it for `Rows` and your mock type. You can then make all the relevant functions generic over the trait or take a trait object and be able to pass a real `Rows` object or a mock object in without needing to do any translation at runtime.
1852m, about one meridian minute (which is the root of the pre-SI measure). Knots are miles per hour. It's actually not that arbitrary a measure at all. Landratten, allesamt hier.
&gt; Can you provide some examples of these "C++-like" things? Of course, I can provide tons (just read the Rust standard library), but i am only going to provide a few. In a nutshell, every time you need to write generic (e.g. variadics) and efficient (type level values for stack storage, alignment, offsets) code, the chances of your library being completely behind a macro are very high. This happens if you are building a: - formatting library (e.g. see println!), - compile-time parser generator, parsers in general, serialization for HPC file-formats,... - regex engine, - unit checking library, - dimension independent code (1D,2D,3D,...), - linear algebra libraries, - geometry libraries, - meta-programming libraries, - serialization within the language (without relying on compiler plugins), - custom tuple types (e.g. to index a tuple by type instead of by value), and, finally, - entity component systems (just like the one described in this post). Objectively, these are all solvable in modern C++, and very easy to solve in D, without macros. Subjectively, the discrepancy between C++ meta-programming and generic C++ is smaller than that of Rust meta-programming and generic Rust. Others might have a different opinion here. But I think that there is a huge shadow world in Rust between generic library writers and library users. This is bad for the ecosystem since you want to make generic library writers happy so that they can make good libraries, while users will just come if there are better libraries than in other languages. Right now, C++/D just allow you to solve most of these problems in an easier way than Rust. So if you are going to end up writing code behind a Rust macro _all day long_, C++/D might be a better fit to sole your problem _right now_. Having said that, D and in particular C++ meta-programming is far from perfect. Rust hasn't compromised itself with respect to metaprogramming yet (the only tool it has is macros!), this is a good thing for the future because it means it might be able to do better, but it is in my opinion a bad thing for the present because other languages do better there right now. &gt; I don't think this is the correct attitude. The whole point of macros is that they reduce the amount of things that need to be built into the language. Rust macros are a very poor type-unsafe meta-programming facility. They are better than C and C++ macros, but barely so, and far away from type safe C++/D meta-programming. &gt; If there's something that can either be implemented using macros (without significant syntactic pain) or as a language feature, it should probably be done as a macro. I am not arguing about language features, I am arguing that in Rust it is common for _whole_ libraries (like the entity component system in this post) to be written inside a macro. These people are not implementing their favorite looping keyword, they are writing code in "Rust-macros" _all day long_ because they cannot write it in "Rust" (just check out nalgebra). To me this means that for a lot of fields of applications, the language is far from ready.
I like this one.
You'll basically also have this problem in something like C#. If you're coding against a concrete PostgreSQL datatype, testing becomes difficult because a) there may not be a way for you to create a mock value and b) your application unit tests now depend on PostgreSQL. So if it's actually application code that you're writing (infrastructure-agnostic), maybe it would be better to do a data adaption (convert from a rough database `Row` to some application datatype), or code against a trait like /u/Rothon suggested. That having been said, sometimes you do just have to write infrastructural code against something like a database, and test that that code is working (for example - the adaption layer I mentioned above). Testing that becomes not-so-simple if you don't have easy mocks. As far as the type of "magical" mocks you can get in a dynamic language - I wonder if it is at all feasible to do something like that in Rust. I've basically just talked myself in a circle here, and will now proceed to watch this thread for other ideas.
Is there any reason besides lack of compiler maturity that rust will end up slower than C++? I note that the language shootouts where C++ is multiple times faster than rust (fannkuch-redux, n-body), cheats, with vector compiler extensions. I know array indexing is always checked but that shouldn't be that significant - anything else, cases where the use of pattern-matching is forced, I guess? 
Using generics or trait objects and indirection solves the problem, but only at the cost of increased verbosity and complexity through the addition of all these generics. (and a performance cost for trait objects)
&gt; Being a macro gives it more than just variadic arguments. It allows the formatting string and argument types to be checked at compile time, which is pretty awesome You can handle both type-safe variadic arguments and perform compile-time type-checking on a string literal in both C++ and D with CTFE within the language writing normal functions that work not only at compile-time but also at run-time resulting in zero difference between run-time (normal) C++/D code and "meta-programming"/compile-time C++/D code. That is where the bar is. The bar is not on type-checking the arguments, even C's printf arguments are type-checked nowadays... &gt; Do you have any examples of this besides this ECS? I haven't personally come across any, but I'd be interested in seeing them. Maybe you are not a generic high-performance library writer, or maybe rust happens to cover your field of expertise just fine. I put effort compiling a list in the post you are replying to, so I would appreciate if you could try and google for those use cases, there are a couple of Rust libraries for each. Still, repeating myself: - any rust linear algebra library (see nalgebra), - any parsing library (see how servo parses anything...), - any regex engine (there are a couple), - any unit checking library (there was one posted in this subreddit last month)... The common "theme" in these libraries are: Embedded Domain Specific Languages with high performance requirements (no run-time cost allowed, domain specific optimizations should be applied at compile time, ...).
&gt; Being a macro gives it more than just variadic arguments. It allows the formatting string and argument types to be checked at compile time, which is pretty awesome. &gt; &gt; And honestly, I disagree with your premise. I think having a powerful macro system is vastly preferable to a number of other, specific language features. There are other ways to type-check `println` other than making it a macro. 
There was also a [blog post](http://huonw.github.io/blog/2015/01/object-safety/) on the topic not too long ago, it's worth a read.
In Rust? I don't know of any ways besides macros and plugins to do that in Rust (typecheck based on the format string). Unless you mean in other languages, in which case I'd be interested in hearing more details.
&gt; You can do this in C++ and D with CTFE within the language writing normal functions that work both at compile-time and run-time. I don't know a ton about C++, but [this stackoverflow thread](http://stackoverflow.com/questions/3105114/how-to-get-printf-style-compile-time-warnings-or-errors) seems to suggest otherwise. I found [Boost Format](http://www.boost.org/doc/libs/1_43_0/libs/format/doc/format.html) but that looks a lot less clean when compared to Rust's `print`. &gt; every C compiler type-checks the arguments of printf. The stackoverflow thread and Wikipedia tell me that C compilers can only type check the arguments because `printf` is standardized, or by using compiler-specific extensions. That doesn't sound too great to me, it sounds like you're out of luck if you want to implement something similar yourself. &gt; I put effort compiling a list in the post you are replying to, but here come some examples again: Sorry, I meant to ask for specific library examples, I was a bit unclear. It looks like `nalgebra` uses macros to implement traits, which is a fairly common pattern (I've seen it in the standard library). AFAIK there are people working on additions to the type system to remove the need for this, but it seems like a fairly good solution for now to me. As for regex engines, I don't see any macros used in the implementation in the standard `regex` crate, which provides your standard dynamic regexes. There's the `regex!` macro/plugin, but it does static regexes. From a quick search the only C++ library which provides static (compile-time compiled) regexes is `boost::xpressive`, and it needs to use some hacky operator-overloading as well to achieve this.
If you're using a trait to make it so that you can switch out a real for a mock, there's never a time you would need to pass a trait object. But yes, the other points are all pain points of this approach.
I'm also wondering how test doubles could be implemented. Using a trait and then having a "real" and "test" version of a type that implements that trait is the obvious approach, but without reflection capabilities, I'm not sure how you could automatically create a type that implements a trait, and doing it manually would be a non-starter. Maybe a compiler plugin that can do static analysis? Is there really no way to stub a type's methods directly without traits? I'm pretty new to statically typed languages in general (most of my education on test doubles has been in Ruby) but apparently it's possible in others. In Java there is [Mockito](https://github.com/mockito/mockito). I'm not very fluent in Java, but from searching around a bit it seems to do pure class test doubles via reflection and code generation. C++ has [Google Mock](https://code.google.com/p/googlemock/) but I don't know C++ at all and am not sure what the core mechanism it uses to achieve test doubles of static types is.
this was the situation in older versions of Rust, but it was decided to be too confusing.
Where does nalgebra use macros? I only saw it use them internally, to deal with operator overloading uglyness. I really don't think that Rust's type system should be able to handle parser generation (regular expressions, LALR, random binary deserialization, etc.). You want the parsers to come from a shared DSL grammar, and that would require writing a parser for that grammar and a parser generator *in the type system*. This is what code generators are classically useful for, and compiler extensions handle this case just as well. Does C++ really have a template-based LALR generator? Really?
I want a job where they pay me to read papers and blog posts and RFCs all day. :(
&gt;Where does nalgebra use macros? I only saw it use them internally, to deal with operator overloading uglyness. A lot of its implementation is within macros (check the macros folder), they are used to implement the matrix and vector types, for example check mat.rs: https://github.com/sebcrozet/nalgebra/blob/master/src/structs/mat.rs &gt; Does C++ really have a template-based LALR generator? Really? Check Boost.Spirit :) &gt; You want the parsers to come from a shared DSL grammar, and that would require writing a parser for that grammar and a parser generator in the type system. This is what code generators are classically useful for, and compiler extensions handle this case just as well. Is not "just the parser". For a linear algebra library you want to parse a EDSL for linear algebra, then you want to apply domain specific optimizations to ASTs of that EDSL with extra knowledge that the compiler doesn't have (simplify the AST, eliminate no ops, reorder operations to reduce memory bandwidth, reduce the number of temporaries required, optimize the temporary storage for cache locality, expose some parallelism...), and then, finally, you want to generate the code at compile-time (for a single target, for multiple targets, for a threading run-time, vectorization, tiling, tasks, asynchronous data-flow...). The parsing is actually a very small part of what Eigen3, NT2, and Blaze do, one needs to perform the three steps: EDSL-&gt;AST-&gt;OptimizedAST-&gt;Code. It is much nicer if everything happens transparently within the same language, and even nicer if you can implement all this within the language itself (no need to add extra build passes, the compiler actually can tell you what is wrong in the code you write, etc.).
&gt;I don't know a ton about C++, but this stackoverflow thread seems to suggest otherwise. I found Boost Format but that looks a lot less clean when compared to Rust's print. That thread is from 2010, there have been 2 new C++ standards since then. Boost.Format is C++03, check out cppformat for how to do this in C++&gt;=11: https://github.com/cppformat/cppformat &gt; The stackoverflow thread and Wikipedia tell me that C compilers can only type check the arguments because printf is standardized, or by using compiler-specific extensions. That doesn't sound too great to me, it sounds like you're out of luck if you want to implement something similar yourself. It's just like a compiler plugin in Rust. My point is, however, that everyone can do type-safe printing in one way or another (and has been doing it for years in D and C++), even C programmers enjoy this nowadays with compiler plugins in all major compilers. Being able to implement type-safe printing within the language and without macros is what is expected from a modern language, it is only a selling point if one can do it better than the competition. So yes, Rust can do it better than C, but no, it cannot do it better than C++ and D yet (it would probably need some form of CTFE for that). &gt;As for regex engines, I don't see any macros used in the implementation in the standard regex crate, which provides your standard dynamic regexes. There's the regex! macro/plugin, but it does static regexes. From a quick search the only C++ library which provides static (compile-time compiled) regexes is boost::xpressive, and it needs to use some hacky operator-overloading as well to achieve this. Check the D standard library which uses CTFE for creating both static regex engines (note that in D the bang `!(..)` is using for explicitly specifying type arguments, like `::&lt;...&gt;` in Rust): http://dlang.org/phobos/std_regex.html The same could be done in C++11 using old CTFE (the same code can create a regex engine dynamically or at compile time): http://cpptruths.blogspot.de/2011/07/compile-time-regex-matcher-using.html In C++14 writing this is much nicer (CTFE is much more powerful). Boost.Xpressive is pretty old (~2001 or so), but it is able to do the same thing using expression templates (you couldn't parse strings at compile-time in C++03, doing it on C++11 is a pain, but on C++14 it is easy to do). Basically in Rust you have a completely different language for implementing a static regex engine than for implementing a dynamic one, which is not the case in C++ and D, where you just write a normal function, and tell the compiler that you want the result at compile-time.
I think the original draft said something about, "orders of magnitude," but that felt a bit dweeby.
A lot of companies use Haskell knowledge as a hiring signal but don't have much/any important code in Haskell. I don't think that will happen with Rust, because the advantages of using it are pretty immediate, and you can incorporate Rust components into pretty much any codebase. Also I think we are successfully avoiding the "only for geniuses" hype / FUD that's attached to Haskell. Rust is actually making systems programming a lot more accessible to people who thought they could never work on low-level code. 
Why is writing them manually a non-starter?
Maybe this c-tags business is better than using Emacs bookmarks.
Use the `{}` formatter instead of `{:?}`. `{}` is for values that have a [lossless, unambiguous representation as a string](http://doc.rust-lang.org/std/fmt/trait.Display.html)., e.g. strings and integers. `{:?}` is for [debugging](http://doc.rust-lang.org/std/fmt/trait.Debug.html) and other miscellaneous printing, so it displays quotes on strings.
So your format string would become "{subject} {verb} {predicate}"
Why indeed. Why do you think it would be beneficial? What made you think of applying rust or go to that domain? rust and go are also quite different to begin with, so it's not obvious to me why it would be a choice between the two.
Because Golang released 3 year ago and still now I don`t see any CRM system on it and commerce as well. Benefit come from scalability I think and multithreading services I will builded on it more easy. Today modern language can save a lot money on servers.
I working on PHP on my work. It is good fit its purpose as scripting language but every day I fight with systems on it which try solve problem like commerce and at end just not scalable at all. About Go I try it a lot. There easy to write concurrency code and as it is GC lang more easy to produce high-med level code. I know it is wrong compare Go and Rust. Question more about benefit to produce commerce on Rust
It's not even about intelligence. It's about it being a new[1] and well-designed language which attracts many hobbyists (i.e. those not employed by Mozilla, and some others). So then there is a glut of developers who would be willing to work with Rust, and have actual non-work experience on it. Maybe that's not the case. But it's the same way I view Haskell: I don't view normal, non-academic Haskell positions as looking for people with a minimum PhD and successful OSS contributions because you inherently need to be smart to program in Haskell. I view it more as a Buyer's Market, so the buyers can have high standards. Maybe I'm mistaken in that impression. [1] I don't think we can completely rule out the new-and-shiny effect when it comes to programming tools.
I question the use of :? in these examples. See https://github.com/rust-lang/rust-by-example/pull/480 for the pull request that introduced this change, and discussion.
True, Rust has changed a lot. I don't think we were ever that close to Go's concurrency story, though. iirc Go's garbage collection is global, and it doesn't do much to prevent you from sharing mutable objects between threads. Rust has also emphasized threads as the unit of failure isolation for a long time, while Go has [exception handling within a thread](http://blog.golang.org/defer-panic-and-recover). Channels are still a big part of concurrency in Rust. That's not a paradigmatic alignment with Go, just an essential part of any decent threading library. Of course marketing doesn't always reflect reality. I agree with you that we have echoed Go's marketing at various points in the past. For example the "spawning a thread" syntax was the front-page example for a long time, and we held onto the `do` sugar almost entirely for this reason. At this point, Go and Rust are both "languages for concurrency", but in entirely different ways. Go has deep language support for the behavior of a concurrent program *at runtime* — green threads, concurrent IO, and channels. Rust has pushed all that out to (first- or third-party) libraries. What the language has now is a *type system for concurrency*, possibly the best one on the planet. Threads, mutexes, atomics, etc. are thin wrappers over platform-native threading. But they are 100% memory safe due to the type system. This approach is of course anathema to Go's design goals of ultra simplicity + fast compilation, and the general view of types as a nuisance.
It's very labor intensive. Pretty much every language has a solution for this, so it would be quite a souring experience writing them manually.
Any benchmarks for this? For example, how long does it take to generate tags for the rustc source code? One thing that I love about Visual Studio + ReSharper (don't hate me) is the semantic navigation that updates as you type. Re-generating ctags is a bit of a pain and ideally you'd want it to happen _at least_ every time you save - which means that it's gotta be fast... That having been said, kudos for publishing this, it's pretty awesome :d
Also, as I'm working through this site - and I'm sure it will be covered later - it seems fairly arbitrary to specify types between: let after_number = 77u32; let with_colon: u32 = 77; let with_underscore = 77_u32; They all seem to accomplish the same thing? 
Oh wow, thanks for this. If I get work up the courage I'll maybe do a pull request after working through the site with all the little oddities that confused me. 
Deployment = single binary file, copy+paste and run it.
They do cover the same thing, its just showing that underscores do not matter. They could be useful in showing large constants... const SIZE: u64 = 1_000_000_000; The underscores are for readability, and yeah, those 3 have the same uses, basically.
[This conversation](http://www.reddit.com/r/rust/comments/2yckxe/rust_for_ecommerce_crm_analistics_tool/cp89t2r?context=2) made me curious about how Rust marketing has changed over time, so I took some screenshots from the [Git history](https://github.com/rust-lang/rust-www).
Hah, thanks for writing a much better summary then I ever could.
With what rendering engine? :P
Gecko, with `layout.css.devPixelsPerPx = 1.5`, which is why the screenshots are huge (sorry!) I just ran Jekyll and grabbed / cropped a screenshot with ImageMagick. Nothing fancy.
And now Heroku cloud support it via plugin [GitHub](https://github.com/kr/heroku-buildpack-go.git)
Nice! It looks like this is a wrapper around an existing ctags tool, supplying regexes to do the processing - is that correct? Does that mean this is a purely syntactic operation? (I don't use emacs and don't have any idea how ctags work, pretty much). Is this different from the facility to dump ctags directly from the compiler? Could this process be improved by supplying type info some how? Or would you then need to manually implement the process of making the ctags?
I suppose [Iron](https://github.com/iron/iron) will become reasonably popular for large applications, at least if Rust gets the popularity it deserves. By the way: Rust and Go are "hot" languages right now, but you might be interested in seeing some Haskell if you fancy ways to build safer programs by using a better type system. And this includes web apps: see [Yesod](http://www.yesodweb.com/) for example.
&gt; Rust by graydon Sounds like... a whiskey brand? "finely aged" 
I [don't see the problem at all](http://is.gd/bcAVVq) I'd have to see how you are writing the code to understand why the problem is occurring.
How does it look in Servo as of now?
That's exactly what I used. Strange, I'm not getting the error now either. Oh well. 
Try a skip list! Pretty cool data structure 
This post deserves many more points.
Specifically, something [like this?](http://is.gd/hpEAH0)
That would be great to see as well, how Servo rendered it over time.
There is a project called [typo](https://github.com/klutzy/typo) which produces a position-to-type mapping table for [Vim plugin](https://github.com/klutzy/typo.vim). Unfortunately it is broken at the moment.
I always wake up for a second when I see that black diagonal stripe "Fork me on GitHub", because very similar stripe is used on photos of recently deceased people. http://imgur.com/g6LMrDv,R8N3f8A#1 http://imgur.com/g6LMrDv,R8N3f8A#0 It's like "open rust-lang.org or crates.io, notice the stripe" =&gt; "wait, the project is closed??!" =&gt; "oh, it's just "Fork me on GitHub""
 &gt; Nice! It looks like this is a wrapper around an existing ctags tool, supplying regexes to do the processing - is that correct? Yes. &gt; Does that mean this is a purely syntactic operation? (I don't use emacs and don't have any idea how ctags work, pretty much). ctags uses regexes to extract identifiers - in the Rust case: structs, enums, functions, methods or traits - and maps these identifiers to source file positions. ctags is quite simple and you even might get the same identifier twice, in which case you might have to jump twice, but in most cases it works good enough and its simplicity makes it easy integratable into several editors. &gt; &gt; Is this different from the facility to dump ctags directly from the compiler? If you're talking about 'make TAGS.vi', that's only creating the tags for rustc and the standard library. 'rusty-tags' creates tags for a whole cargo project, for all of its dependencies, the direct and the indirect ones. &gt; Could this process be improved by supplying type info some how? Or would you then need to manually implement the process of making the ctags? ctags isn't the right tool for this. To get a really robust type info and something like auto completion, there's a need for rustc/cargo support. 
What you said is true of *nautical miles*. A mile is a bit shorter than a nautical mile, about 1609 meters.
Well then, apparently using the Option&lt;T&gt; field is not the efficient way of storing sparse Data. Thanks for the detailed explanation.
Right. Associative arrays will be more preferable.
We can always pick another color: https://github.com/blog/273-github-ribbons
Not any more. They'll be added back at some point. Their removal is a bit of a sore topic.
There are other languages that can achieve this, and C++ is one of them. What /u/gnzlbg talked about was that they were missing more advanced tools for writing generic code in Rust. They are complaining exactly about &gt;I don't know of any ways besides macros and plugins to do that in Rust 
Do you really need `asm` for this?
I suppose you could attempt to forwardport the removed bits and fill in the rest. 
I'm very enthusiastic about this! For the record, I've managed to run glutin's triangle example on the `emscripten` branch with almost no modification thanks to this crate! (I just had to remove the triangle because it's using immediate mode, which is not available in WebGL) I'm also planning to make glium compatible with OpenGL ES2/WebGL, so that you could compile games for emscripten without any modification required. But long compilation times (around half an hour for a simple triangle) are not encouraging. 
Ok, thanks for giving me yet more example of match coupled to enum. However, that's not quite getting me to the equivalent functionality that I needed as per my OP presented helper functions. What was going on there is that the type of a template argument, which at compile time of my test code will be one of the OCI opaque struct types, is being mapped to some OCI type number (needed when making calls to various OCI APIs). I used the TypeId::of() to get a type representation that could be obtained on the template type argument and then used that to compare against in order to map the type to an OCI type number. I could have used a statically initialized hash table to look up by TypeId but for so few cases my first inclination was to somehow use match (plus I aspire to learn how to use match in a more advanced manner than simple matching on integer literals). I haven't seen a way to couple the opaque OCI struct types into a Rust enum.
For a game I'm developing (behind the scenes), I wanted to embed a scripting language for writing AI. So I picked Tcl, for no reason other than I like it. The bindings are pretty basic at the moment, but they've just reached the most basic level of usefulness.
Well, one day I'll go back to writing my APL interpreter in Rust again. I have a thing for old languages. Especially ones that make people wake up in a cold sweat.
Why isn't Option implemented with null pointers?
It is, when it can be. It's documented a little here - https://doc.rust-lang.org/book/ffi.html#the-%22nullable-pointer-optimization%22 Basically, for Option&lt;T&gt;, if T is not nullable, then size_of::&lt;Option&lt;T&gt;&gt; will equal size_of::&lt;T&gt;. This applies to all T's which are pointers, for example.
It looks [like this today](http://i.imgur.com/BiP7rX0.png). Pretty good aside from the missing logo (SVG) and the code formatting.
No, you can't use match, because you can only match against constants. You could use match with if guards, but that's just the same as if/else. Here's how I'd do it. http://is.gd/opJRff As a side note, #[repr(C)] on a zero-sized struct doesn't mean much, and there may be a warning for that someday. Standard C doesn't actually allow for zero sized structs, although GCC/Clang accepts them.
I don't think there's anything wrong with macros being Turing-complete. Practically any interesting rewriting system is. If people are doing [excessively fancy computation](http://doc.rust-lang.org/book/advanced-macros.html#the-deep-end) in `macro_rules!`, the solution is not to cripple `macro_rules!` but to improve the [tools for procedural metaprogramming](https://github.com/erickt/rust-quasi).
Anything that *contains* a non-nullable value also gets the optimization. e.g. Option&lt;Vec&lt;T&gt;&gt; is the same size as Vec&lt;T&gt;. The algorithm can also apply recursively. So Option&lt;Option&lt;Option&lt;(Box&lt;u8&gt;, Box&lt;u8&gt;, Box&lt;u8&gt;)&gt;&gt;&gt; is the same size as (Box&lt;u8&gt;, Box&lt;u8&gt;, Box&lt;u8&gt;). Edit: Option also isn't magic. This applies to any "Option-like" enum. I *think* enums with many variants can also be packed if there are multiple non-nullables. Not sure if that's been implemented.
Hmm, that's interesting. What is this poisoning mechanics you mention in docs?
So IOW, the TypeId::of() would need to be a compiler intrinsic that makes the TypeId available at compile time yielding it as a compile time constant. Thanks
I actually took it from std::sync. It's just a boolean flag in the lock structure. If the thread holding the lock exclusively panic, we can check that in the drop code for WriteGuard using thread::panicking, and set the poison field in the lock structure to true. It is useful to prevent atomicity of write operations. For instance, you might implement a RedBlack Tree inside the lock. If you panic in the modification of the RedBlack Tree, the structure might not be a correct red/black tree. Hence, releasing the write lock is not enough, and the structure state might be inconsistent. The poison mechanism is here to prevent readers to access maybe-inconsistent data.
Oops, my bad... Edited it a few times before posting, probably should have proof read it. Thanks! [Edit: and fixed :)]
Macros are great for solving a lot of meta-programing problems, but they don't know about types. Some other languages have (easier or harder) facilities for solving meta-programing problems in which you need logic that depends on the type information (or type-level values,...). Rust generics are really nice for the things that they can do, but there is a lot of things that they cannot do. And there is nothing filling the gap between generics and macros in rust. There are RFCs like variadics, HKTs, associated const values, type level integers and booleans... that improve generics in Rust significantly. But even with those accepted, to really fill the gap one needs "full CTFE" (anything else is not going to be enough for somebody). 
At least I want to preserve that cargo test still runs with all debug assertions enabled, even in the dependencies.
Types are like drugs - you smoke one generic joint, and soon you want to inject dependent types heroin and become an unemployable junkie like Coq ;-). Dynamic Langauges manage to skip over this by leaving the types in the programmer's mind. This has the annoying issue of making any kind of (semi-)formal reasoning about code very hard (read: slow and/or complicated runtimes, no good static analysis). C++ somewhat balances by combining an essentially simply-typed value level with a dynamic type level. Rust (and Haskell) are static languages in all levels.
And if you want it and don't have relevant experience? .......... sigh
You can verify this by trying to use `y` after line 6. error: use of moved value: `y` `y` gets moved, even if you can statically verify that branch is never taken. [Playpen](http://is.gd/8P0zFj)
For those of you familiar with windows mutexes, a poisoned lock is the equivlent of WAIT_ABANDONED.
This is just a pipe dream at this point, but I'm **really, really** hoping that emscripten will become a supported target for the official Rust compiler.
[Please God yes...](https://www.destroyallsoftware.com/talks/the-birth-and-death-of-javascript)
This is a good point, and I might want to revisit this-- as I was making this lib 'static seemed to appease lifetime errors that I ran into. I did test this with a custom struct that implemented the drop trait, and after all streams read said data, the drop was performed automatically (since the data is in an arc). I thought this was neat to watch. Additionally, this entire lib could likely be written with channels, and avoid much of the custom code here-- that was never my intention though. Thanks for taking a look so far!
While looking into more of why dynamic drop semantics were chosen, I came across [this comment](https://github.com/rust-lang/rfcs/pull/210#issuecomment-53781591) discussing how moves work in C++. Basically C++ does use dynamic drop flags, but it has to be implemented by the programmer in the move constructor and the destructor. Once the RFC I linked above is implemented, I think the performance hit should be fairly negligible. Drop flags will only be used exactly where they are required, so for the common case there will be no overhead (contrast this with C++, where the move constructor and destructor will be called regardless of the control-flow). And when they are required, it's simple check of a boolean on the stack. So in general this could perform (very slightly) better than the C++ method. Plus, if you ever have a hot spot in your code where you determine that the drop flags are actually causing a performance issue, it is possible to insert manual calls to drop (like in /u/aufdemwegzumhorizont's hypothetical code) in the branches of your code where a value isn't moved. Then the compiler won't have to insert drop flags at all.
I haven't looked much into compiler extensions. Gonna try them out as soon as I get a chance.
Sure, it depends on why you care about space usage. I realize that jemalloc generally requests pages at a time, but you're going to use them up more quickly if you're allocating 16 byte chunks for 3 byte quantities (or whatever :)).
What are you trying to accomplish? An `LD_PRELOAD` hook written in Rust? ...because, if that's the case, you probably want to be looking at retrieving a function pointer via `dlsym(RTLD_NEXT, "write")`. (That's basically the `LD_PRELOAD` equivalent to things like the C++ `super::write()` inheritance construct)
Wrap it in a buffered writer?
If one compiles crates separately, emscripten can generate one Javascript blob for each of them right?
Oh, I forgot we don't get that automatically &gt;_&gt;
It happens because you have moved value to `x` and extended it lifetime. The checker at the runtime will do as follows: { // introduce 'a lifetime let x; { // introduce 'b lifetime let y = Box::new(1); // assume that `y` has lifetime 'b x = if flip_coin() { y } else { Box::new(2) }; // whoops, our previous assumption was wrong, both should has 'a lifetime // extend lifetime, also dissalow usage of `y` as it's value is moved } }
Emscripten recently became [a proper LLVM backend](http://kripken.github.io/emscripten-site/docs/building_from_source/LLVM-Backend.html) and may even move into the LLVM tree one day.
Is this what Servo is going to use for process isolation? 
Was there ever any f80 support? I don't remember it being there.
Yes, I want to update my PR that implements multiprocess mode in Servo to use this. This library is essentially a split-out, stronger version of that PR.
&gt; ... He and his followers see Mozilla as the evil empire ... &gt; ... that way they can play at being the cool rebel hackers ... You make Mozilla look really bad every time you post about this person. You should probably stop doing so until you can curb your childish behavior. 
also on github: https://github.com/rolandshoemaker/theca
&gt; ... have the untrusted process send an IPC message to a broker ... What's the best way to implement this sort of IPC in rust right now?
Possibly via the `urpc` project below :)
There are a set of generated documentations for each supported platform. For example, for x86_64 Linux: https://static.rust-lang.org/dist/rust-docs-nightly-x86_64-unknown-linux-gnu.tar.gz The full list is available [here](https://static.rust-lang.org/dist/) (look at `rust-docs-nightly-*.tar.gz` files).
Please God [`enum {No, Yes, Maybe}`](https://www.destroyallsoftware.com/talks/the-birth-and-death-of-javascript)
Just what I needed, thanks
This is a good question, and I don't have an answer :/ We should do a second book where each chapter describes architecture and code patterns from some well-known Rust project. I don't know if you've seen [AOSA](http://aosabook.org/en/index.html) but it's a fantastic series.
Um, I think you replied to wrong person?
I guess I can confirmd that `None` in `Option` optimizes to `null`. My tree is basically a voxel octree. Voxelizing a solid sphere at 9 levels depth(134,217,728 voxel) grid, would consume around 13 GB of memory. When carved out from the inside, it would only consume 0.3 GB. Now I can voxelize a carved sphere at 11 depth(8,589,934,592) voxel grid and consumes only 5.6 GB. 
Very cool! I was planning on writing my own CLI note tool and you gave me a lot of good ideas. If it doesn't pan out, at least I can fall back on yours!
It's a pity it does not support Windows. Any plans on this (presumably Neovim will have Windows support in 1.x)?
Very nice, but I wish the author provide a straightforward instruction how to make it working. A helpful note on where `sysroot` may have been located for most common rust/cargo installation. 
Been using RustDT for a week now, it was great. Can't wait to try this one!
Feels bad being a windows developer :( Edit: Wanted to clarify I actually like developing on windows and I have no plans moving to linux currently.
Why is everyone thinking this is cool? Compiling anything to JavaScript is very ugly hack.
This looks like it's going to be the best IDE! thank you!
I'm so surprised with how the community as a whole has contributed so little to making tutorials. I know the language is new and any tutorials are likely to be made moot quite quickly but still.
Why? Visual Studio rules, we have F#, compilation to native code via NGEN, .NET Native and deployment to tablets and mobile phones. 
Maybe that's because I've been using an older (2010, I think) version of Visual Studio, but I'm hard pressed to understand what people rave about. Performance is much better than Eclipse, and the memory footprint is probably smaller, but the functionality offered is extremely limited, and I am unimpressed by the test runner. So, what's so great about it (honest question, at the risk of derailing the thread)?
You might want to use an actual license instead of the Public Domain, which is not a legally recognized concept in many nations. CC0 is basically identical to "Public Domain" and is accepted internationally. http://choosealicense.com/licenses/cc0/
Sorry, I am pretty bad at writing these things out properly, it was a bit rushed. The intention was to indicate that you could put it anywhere you like, as long as the `cargo-build --sysroot` flag is pointing to it. I don't suggest placing it in your actual Rust sysroot just because of how much of a hack this is, and because having files on your filesystem that aren't tracked by a package manager isn't great.
Thanks for all the info! I've been meaning to take a look at your fork, it will definitely help make the Rust &lt;-&gt; Emscripten interaction become much better if Rust itself can be compiled with `pnacl-llvm`. I became distracted by my current stopgap approach just because I realised it actually worked, but I'd really like to see this all come together cleanly one day. I will keep an eye on that fork and see if I can get anything up and running with it when I get a chance!
The debugger and profiler are second to none. I'm also unsure what you mean by extremely limited functionality. Are you perhaps running one of the old "Express" editions? If so, you should upgrade to VS2013 Community edition, which is not neutered like the Express versions were.
I think some of the features have been added in newer versions but the reshaper plugin adds a lot of functionality people use and might take for granted and forget that it's part of a plugin(for c# although I think they are slowly adding c++ support as well, note resharper is made by the intellij guys and adds some of the features common in java IDEs. Also it costs a pretty penny)
Ok, thanks for the detailed answer.
I'll have to check, but I think I have real thing. But out of the box, it doesn't seem to do much more than renaming things.
&gt; I know the language is new and any tutorials are likely to be made moot quite quickly but still. That "but still" is kind of loaded. I know I've personally held off from writing things because of language changes. It's just now finally settling down with people using the new `std::io`, but maintaining an up-to-date tutorial is just a *ton of work*. Just ask /u/steveklabnik1 :-)
[There is a license in the repo.](https://github.com/oakes/SolidOak/blob/master/UNLICENSE)
This is likely not the subreddit you’re looking for. Try /r/playrust.
Oh crap, sorry D:
I spend 90% of my windows time in Linux vms via VirtualBox. With the VBox additions installed it runs fine fullscreen on my 1440p middle monitor. Worth trying out, I think. :-) If it weren't for my 1300-game steam catalog I'd probably just be running Linux; at some point I think I'll just get a second PC and leave this one for gaming.
I also game a lot, fortunately the game i mostly play is dota2 which is natively supported in linux. Team fortress 2, CS:GO is also in linux and many more.
Unfortunately choice between Windows and Linux isn't just an aesthetic or usability difference for a lot of people. Some of us have to use windows, either because that's the provided IT, or because we're targeting users who run Windows (and that's still most of them for PCs).
Looks great. Only one question: why are you recommending users `cargo build` rather than `cargo build --release`? Often there is like a tenfold improvement in performance from the latter (and I've seen much larger).
This is the most promising one yet! Good job! Relicense it. Public domain is a bad call. Might I suggest the [WTFPL](http://www.wtfpl.net) ? :-D
Those are development instructions, so I think that `cargo build` is a good place to start. Note that the "Build" button in the IDE itself actually does do a `cargo build --release`, though.
There is no intrinsics to deal with `f128`, also there is no flag (like `#[simd]`) to declare type, so it seems that you need to use assembly to deal with that.
Thanks for the explanation. That's great. But from what I read, the new drop will most likely not make it in 1.0, right?
Rust very different from C++ and overs programming languages. There not case too know Rust only because it is low level language as well. So I think going to be great have same thing like "Machine learning with Rust" or learn "Programming Rust way". Same dos`t know watch is "List" )) And I am too came from PHP))!
The flux has been high such that writing a book is an exercise in following a moving target. A great way to learn a programming language is to undertake a modest, interesting/useful coding project. Am very new to Rust and am writing an Oracle-specific Change-Data-Capture pipeline for populating and syncing external data warehouse databases (that can be non-Oracle or even no-SQL). I'm using the Oracle C driver interface OCI. This project involves FFI and am writing custom Box templates to manage OCI objects, and provide a Rust-safe API to use OCI. It's taking me on a tour of Rust language features as I go. Figure some project idea out and then go at it. No doubt you'll emerge on the other side with a fair mastery of at least intermediate Rust programming.
That is true in a tautological sense. I'm not saying the skepticism is irrational; I'm more-so moping about it because it's a relatively recent change.
Your C version uses `unsigned int`s, the Rust code uses `i64`. After changing the Rust code to `u32`, it takes 1.35s on my machine while the C version takes 1.21s.
Make sure to also use the same integer types. C: #include &lt;stdio.h&gt; #include &lt;stdint.h&gt; typedef int64_t i64; i64 sum_of_divisors(i64 val) { i64 sum = 0; for (i64 i = 1; i &lt; val; ++i) { if (val % i == 0) { sum += i; } } return sum; } int main() { for (i64 i = 1; i &lt; 20000; ++i) { i64 sum1 = sum_of_divisors(i); if (sum1 &lt;= i) { /* * 1) Amicable pairs can not be the same * number. * 2) If sum1 is less than i then we would * have already examined it for amicable pair. */ continue; } i64 sum2 = sum_of_divisors(sum1); if (sum2 == i) { printf("%ld %ld\n", i, sum1); } } } Rust: fn sum_of_divisors(val:i64) -&gt; i64 { let mut sum = 0i64; for i in 1..val { if val % i == 0 { sum += i; } } sum } fn main() { for i in 1..20000i64 { let sum1 = sum_of_divisors(i); if sum1 &lt;= i { /* * 1) Amicable pairs can not be the same * number. * 2) If sum1 is less than i then we would * have already examined it for amicable pair. */ continue; } let sum2 = sum_of_divisors(sum1); if sum2 == i { println!("{} {}", i, sum1); } } } clang -O2: real 0m6.492s user 0m6.493s sys 0m0.000s rustc -C opt-level=2: real 0m6.494s user 0m6.490s sys 0m0.000s 
It's not a recent change - things have fallen into public domain after a certain amount of time (and still do, in theory, so long as the law doesn't change again) - but there's never been a legal framework to voluntarily put things into the public domain before that time in many countries. CC0 provides a license which is equivalent to putting something in the public domain, for the rest of the world.
unsigned int is most likely 32-bit in comparison to u64 in Rust. After changing u64 to u32 I get these times: clang++ -O3 real 0m1.307s user 0m1.305s sys 0m0.000s g++ -O3 real 0m1.344s user 0m1.342s sys 0m0.000s rustc -O (u32) real 0m1.727s user 0m1.726s sys 0m0.000s rustc -O (u64) real 0m2.930s user 0m2.928s sys 0m0.000s
What's morally suspect about them? Just because they are based on a different moral (revolving around the irrevocability of the act of creating something)? And if you take a moral stance, why is the practicability the argument to not care in the end?
Note that rustc -O is equivalent to rustc -C opt-level=2, while you are using -O3 for the C version.
What I meant is, can you use 2 `f64` as a surrogate?
Yes, it was bswap that I ment. Anyway, byteorder is an equally good alternative to use :) . I just extended the ad-hoc code I had already written to compensate for the loss of `read_be_i64` to a full crate.
You could just call the syscall directly. I don't know if there's a rust wrapper, but if you want to write an assembly wrapper, you can check out [io-yasm][repo]. [repo]: https://github.com/GBGamer/io-yasm/blob/master/src/libasmio.s
I totally agree; gofmt is one of my favorite tools that Go has, and I can't wait until rustfmt lands. No more arguments about inconsistent style/tabs—just write it how you want and let rustfmt take care of the rest.
It's a thing of the past, though, and it won't come back by wishful licensing.
I actually authored the PR to neovim that added support for building it as a static library. Then I made [neovim-rs](https://github.com/oakes/neovim-rs) which lets you embed it in a Rust project. It does not use `.nvim`; it uses its own `.soak` and `.soakrc`.
Using the same integer type closed the final gap. I converted both C and Rust code to use 64 bit unsigned integer. [C code](https://gist.github.com/anonymous/4635c9c2a3c863fde3bf) and [Rust code](https://gist.github.com/anonymous/b1104c46e2946eee13ad). They are taking nearly identical time to run. C code: real 0m2.000s user 0m1.994s sys 0m0.004s Rust code: real 0m2.061s user 0m2.054s sys 0m0.005s 
Also `clang -O` or `gcc -O` do the same as `rustc -O`, they're equivalent to `-O2`, and `-O3` may or may not improve runtime.
Almost the same code and the same result : for loop : 0m2.28s real 0m2.27s user 0m0.00s system iter : 0m2.28s real 0m2.28s user 0m0.00s system
Ok, that's cool; I'll add some symlinks. What I meant though was, how are you doing the actual embedding? Does nVim expose a GUI toolkit? Are you faking a terminal and running it in that? Are you drawing characters manually?
Word of warning, things may break if you symlink `.soak` to `.nvim`. There are several things SolidOak relies on that it puts there, including for the rename button, autocomplete, and syntax highlighting. It should be safe to symlink `.soakrc` to `.nvimrc` though. I am just running Neovim in a separate process and piping its stdin/stdout to a terminal emulator widget in the window. It is possible to use Neovim to control a GUI text editor, and eventually I'd like to do that, because terminal emulators are a bit clunky.
Perhaps you should dual license it — pick some license like Apache, MIT, BSD, etc. and also public domain. 
Then get into politics and change the law. Seriously, I don't even mean that as snark, we need people with those views there. A LICENSE file isn't changing anything.
Might not make it in for the release, yeah. But its definitely planned to be added backwards-compatible at a later point.
&gt; contrast this with C++, where the move constructor and destructor will be called regardless of the control-flow I would point out that should the move-constructor and destructor be inlined, then the optimizer should be able to eliminate trivial calls easily. Similarly for the dynamic drop flag, the optimizer should be able to eliminate the runtime check in a number of cases. 
What's IMO missing in the whitespace guidelines is that there shouldn't be multiple consecutive empty lines.
See https://github.com/rust-lang/rust/issues/23192.
Thanks for the suggestions! In this case (and ones like mine), I don't think just picking a project is the issue. For example, several of the small pet projects I've ported to Rust seem to work regardless of if I use boxes or not in various parts. I've seen people say use boxes when your data is "big" but that's not very descriptive as big to some people may be 1kb, wheras other it may be 1mb, etc. I'm kind of picking on boxes here, but the same can be said of several Rust "tools." Rust was my first dip into a deeper level that I've always wanted to try I just feel like by never using things like box I'm potentially shooting myself in the foot and not knowing it. Just cause it works doesn't mean it works well :P
Basically, CC0 can be summarized in plain English as "If your laws allow me to put things into the public domain earlier than usual, then this is in the public domain. If not, then I hereby grant you a license to do whatever the hell you want without asking me." (The verbosity of the CC0 comes from covering all of the "are you sure you *really* meant **anything**?" and "No, not even *implied* warranties or promises that it's suitable for purpose X" cases)
I've been using the `UNLICENSE` for years for the same reasons as you. My license choice doesn't come up too often, but whenever someone on reddit points it out, I end up with the same response you got. People tend to pile it on. I just try to keep my responses simple. Something like, "The `UNLICENSE` most closely reflects my philosophical views; however, I want to make sure my code is as widely usable as possible. If you're in a situation where the `UNLICENSE` makes it impossible to use my code, please file an issue and we will work something out." (Nobody has taken me up on my offer yet.)
Coming from Python, this is a completely alien workflow that strikes me as inefficient. Ideally your texteditor should be configured for the correct formatting?
I'm a little curious about a couple things in the `find()` function, if you don't mind explaining them: * It looks like `iterate(0, |i| i+1)` is just an iterator over the the integers `0..std::u32::MAX`, which works by taking `0` and applying the function `|i| i+1`, then applying the function to the result, and so on (similar to Haskell's `fold`s). Is that correct? The documentation for iterate is a little confusing to me. * What's up with the `.next()` at the end? Why do you need it?
You wrote something not starting with `night-`! Glad to see we like the same languages, oakes :p
What are the controls supposed to be? Doesn't seem to accept any input at all - mouse, cursor keys, WSAD, nothing. Firefox 36.0.1. EDIT: the "Fullscreen" button doesn't work either.
It should be the arrow keys. Got it working by checking and unchecking the Lock/Hide pointer check box and then clicking on the canvas.
Hah yeah, I guess I'm limiting that prefix to my Clojure stuff. My naming schemes are mysterious.
Gotcha. Works now, albeit very sluggish. Are you supposed to be dodging the snake things? I just ignore them and nothing bad seems to happen; they crawl all over you and Blood never goes down.
They bite if they get within a radius of the head, but when missing they wait some seconds before trying again. Don't let their heads get close to your face and you'll be fine (except for running out of air). In the sequel https://github.com/bvssvni/ld31 , you have reached the surface and now have to follow the stream while avoiding hungry sea birds, swimming for the beach while bleeding from the sea snake wounds. This game is currently broken, and rust-sdl2_mixer isn't working yet with latest Rust.
Not at all! &gt;... (similar to Haskell's folds). Is that correct? Your description sounds right, but I'm not a Haskell programmer, so I can't say for sure. &gt; What's up with the .next() at the end? The call to `.next()` pulls the first value out of the iterator. Because the iterator is lazy, this will do all the computation up to the point where it finds a result.
I think Classic Snake would have been a better example to sell...this doesn't run on my computer; it loads, but it's unusably slow. Good job though. It's neat to see what's possible, even if it's not really optimal. 
I needed this as well, thanks :)
So dual-boot? I run windows for Visual Studio and for the occasional situation where something is broken and I don't have n hours at that moment to get it working, and for everything else Arch Linux.
This is pretty cool, but the game itself is pretty terrible.
all it does is printing "trap!" to the console. 
[Riposte from a previous thread](https://www.reddit.com/r/rust/comments/2uqlek/noob_question_poor_style/coask0x).
What's a rollup PR?
hah :). I think more an issue with how this was setup, or how emscripten compiles out those APIs. Main thing to do with keyboard events in games is prevent propegation and defaults, so the keyboard doesnt move the screen in anyway.
This should perhaps be made more obvious, either with an easy command for accessing/opening them, or a note during installation saying where the docs are installed.
These days I just rollup all PRs, small or big (excluding OS-specific ones or really large ones), test locally, and send through the CI.
Seriously, stop the reaction of pointing Windows people to using Linux. It doesn't help the people, it doesn't help the community. Windows support of Rust is meh because we don't have enough people using Windows. The proper reaction is: "great, a windows user, can we get somehow involved with you?"
Can you give a brief explaination as to why the interger type matter? I'm a Python programmer trying to learn Rust, so there's a lot of low level stuff that I've never had to think about. Thanks.
Please obey rule 2 and 4. We have a huge problem with mediocre Windows support and making people feel bad about using it is not making them feel welcome.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**CPU cache**](https://en.wikipedia.org/wiki/CPU%20cache): [](#sfw) --- &gt; &gt;A __CPU cache__ is a [cache](https://en.wikipedia.org/wiki/Cache_(computing\)) used by the [central processing unit](https://en.wikipedia.org/wiki/Central_processing_unit) (CPU) of a [computer](https://en.wikipedia.org/wiki/Computer) to reduce the average time to access data from the [main memory](https://en.wikipedia.org/wiki/Main_memory). The cache is a smaller, faster memory which stores copies of the data from frequently used [main memory](https://en.wikipedia.org/wiki/Main_memory) locations. Most CPUs have different independent caches, including instruction and data caches, where the data cache is usually organized as a hierarchy of more cache levels (L1, L2 etc.) &gt;==== &gt;[**Image**](https://i.imgur.com/MOJCJgx.png) [^(i)](https://commons.wikimedia.org/wiki/File:Cache,missrate.png) --- ^Interesting: [^Back-side ^bus](https://en.wikipedia.org/wiki/Back-side_bus) ^| [^Dinero ^\(cache ^simulator)](https://en.wikipedia.org/wiki/Dinero_\(cache_simulator\)) ^| [^Macintosh ^IIx](https://en.wikipedia.org/wiki/Macintosh_IIx) ^| [^Cache ^\(computing)](https://en.wikipedia.org/wiki/Cache_\(computing\)) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cpb9s5f) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cpb9s5f)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Letting the person know that there is a much greener grass on the other side will make him much better. I agree with what you say, and it is good for the best interest of the rust community, but IMO it's just a bit selfish.
Do that when the ask about a better environment. I refuse the assertion that Linux is in any way the greener side. You are answering a question the person hasn't asked.
Feel free to open a PR! :)
Isn't homu used today?
I'd like to make clear that you picked up the discussion, I was talking to /u/oakes. I'm not flaming.
Any type that implements `IntoIterator` ([found here](https://doc.rust-lang.org/std/iter/trait.IntoIterator.html)) can be used in a for loop. The type itself should work, [this works for me](http://is.gd/N1isXf). This does, however, move the Vec, which might be what you are running into. A plain `Vec&lt;T&gt;` has a `T` for its iterations, while iterating over a `&amp;Vec&lt;T&gt;` gets you a `&amp;T`. This isn't that well documented yet, [it was a fairly recent change](https://github.com/rust-lang/rust/pull/20790). 
The `for` loop implicitly calls the [`IntoIterator` trait](http://doc.rust-lang.org/std/iter/trait.IntoIterator.html). The instance for `&amp;Vec&lt;T&gt;` produces an iterator over immutable references to the elements. There *is* an instance for `Vec&lt;T&gt;` as well: fn main() { let list = vec![10i64, 20, 30]; for p in list { println!("{}", p); } } Here, the `for` loop consumes the vector, turning it into an iterator which moves the elements out one by one. If you try to use `list` after this loop, you will get a "use of moved value" error from the compiler.
I'm not waxing about the public domain (I like using CC0 for quite some things), I'm waxing specifically about the UNLICENSE, which actively does make these projects unusable for me. Also, now, you are condesceding, implying that I'm just talking and doing no work.
The Vec itself gives you `into_iter()`, instead of `iter()`, basically.
I've already said that I refuse to accommodate perverse legal systems. If you live under the jurisdiction of one, I suggest that you ignore the law and keep on coding.
 https://github.com/rust-lang/rust/pull/23279
This article isn't done yet! There are three topics I'd love to cover that I detail but I haven't found the way to approach these problems! **Please** feel encouraged to contribute, I'll credit you!
/r/playrust
Wrong subreddit, dude. This is /r/rust for the Rust programming language. I think you're looking for /r/playrust
Hijacking this question for another quick vector iterator question: What is the difference between vector.drain() and vector.into_iter()? They both convert the vector into an iterator but when I was having lifetime issues with one I switched to the other and it worked. Does vector.into_iter() move and vector.drain() borrow? Do they have different performance implications?
&gt; I feel stupid. Don't. This is a current weakness of the Rustdoc output, not a fault with you. We have an issue open for it: https://github.com/rust-lang/rust/issues/19190 Basically, `Vec&lt;T&gt;` implements `Deref`: http://doc.rust-lang.org/nightly/std/vec/struct.Vec.html#method.deref This says "If you have a `Vec&lt;T&gt;`, and you use `*`, then you get a `&amp;[T]`. EDIT: Ugh, this is not true, it gives a `[T]` directly. Look at `Target`, not at the return type of `Deref`! Thanks /u/kimundi for the correction here. Furthermore, the `iter` method comes from `SliceExt`: http://doc.rust-lang.org/std/slice/trait.SliceExt.html#tymethod.iter and waaaaay down at the bottom of the page, you can see `impl&lt;T&gt; SliceExt for [T]`. Combine this with the fact that method calls will do a deref as many times as they need to to find a method. So: `vec.iter()` derefs the `Vec&lt;T&gt;`, and gets a `[T]`, it finds `iter()` there. Make sense? If it makes you feel any better, I got lost around the `SliceExt` bit and had to ask myself on IRC... 
`drain` moves the elements out, but doesn't change the `Vec`'s capacity. `into_iter()` moves the `Vec`, deallocating it. `drain` lets you avoid reallocating, `into_iter` avoids keeping unused memory around.
Thank you. That is a very good answer.
Had to use nbsp to get `^^^^^^foo bar baz` to work, that was not fun :(.
I wonder if the rust devs have read about research on multi-stage evaluation, particularly MetaML and MetaOcaml. Experiences from languages that try to make multi-stage programming a first-class feature might be somewhat instructive if you want to add so-called CTFE to a language.
Mar 11, 2016? O.o
I've also been working on a `poll()` type call [here](https://github.com/Hoverbear/capnp-rpc-rust/commit/5c846758cf20448d008497b011f9774797913d80) but I'm not sure it's the right way to go about things.
I'm not super familiar with the problem space but it is interesting. I just read something very similar to what you're saying here: http://nanomsg.org/documentation-zeromq.html &gt; Threading Model &gt; One of the big architectural blunders I've done in ZeroMQ is its threading model. Each individual object is managed exclusively by a single thread.
I was surprised when I read this post. I originally thought that `&amp;`-ing a `Vec&lt;T&gt;` would give a `&amp;Vec&lt;T&gt;`. But I did the experiment, and you're right: it can also give a `&amp;[T]`. I'm guessing this is part of the Hindley-Milner type expansion performed by the compiler?
&amp; is creating a reference, * is de referencing. And we don't actually use HM, just something like it.
As for a message a year into the future, I would have expected it to be "Servo is now in final preparations to replace Gecko in FireFox"
This is very literal by the way: fn main() { for _ in 0..2 { } } Directly expands to (without the comments): $ rustc -Z unstable-options --pretty=expanded file.rs #![feature(no_std)] #![no_std] #[prelude_import] use std::prelude::v1::*; #[macro_use] extern crate "std" as std; fn main() { { let result = // `into_iter` is called directly with `0..2` from the // expansion of the `for` loop. match ::std::iter::IntoIterator::into_iter(0..2) { mut iter =&gt; loop { match ::std::iter::Iterator::next(&amp;mut iter) { ::std::option::Option::Some(_) =&gt; { } ::std::option::Option::None =&gt; break , } }, }; result } } 
Yes please!
&gt; Unfortunately, I wasn't able to figure out a painless way of extracting a full set of values from the list without doing a manual walk like this. It looks like [lists like these](http://docs.capnproto-rust.org/capnp/data_list/struct.Reader.html) would benefit from implementing `Iterator` (or `IntoIterator`?), in which case you could potentially do: let entries: Vec&lt;String&gt; = params.get_entries().map(|x| x.to_string()).collect();
Ah, I think that bullet point was overly shortened :) It should read Fix *some* RTL-related layout bugs, not Fix everything and make our RTL support complete. For example we don’t have any bidi support yet. I’m not aware of such a test suite, and unfortunately nobody on the team (as far as I know) reads Hebrew or Arabic. We’ll ask the community’s help to tell us if things look right when we have more support for this, but we’re not there yet.
What the hell: https://doc.rust-lang.org/std/iter/trait.IntoIterator.html impl&lt;'a, T&gt; IntoIterator for &amp;'a [T; 0] impl&lt;'a, T&gt; IntoIterator for &amp;'a mut [T; 0] impl&lt;'a, T&gt; IntoIterator for &amp;'a [T; 1] impl&lt;'a, T&gt; IntoIterator for &amp;'a mut [T; 1] ... impl&lt;'a, T&gt; IntoIterator for &amp;'a [T; 32] impl&lt;'a, T&gt; IntoIterator for &amp;'a mut [T; 32] So a [T, 33] doesn't implement IntoIterator?
http://pcwalton.github.io/blog/2012/12/26/typestate-is-dead/
This blog post is incredibly old, but surprisingly still applicable for the most part. 
As `io` is still unstable the I wouldn’t conclude too much out of that yet. 
 #![cfg_attr(test, feature(test))] This means: if `cfg(test)` (which is the case when testing), then there is `#![feature(test)]`.
I too am a big fan of verification and contract-based programming (the first language I learned at university was Eiffel, btw.). That said for practical programs, contracts are quite a heavyweight solution, both in terms of programmer time and compiler complexity. Many benefits of contracts can already be had with type-based invariant specification and possibly specialized lints (as pcwalton's article hints). I suspect that the type system will gain in power in the months after 1.0 finalizes (e.g. Linear types are a much wanted feature), giving us 80% of the bang for a fraction of the buck.
You're spawning a thread for every pixel, which is very very expensive. One approach would be to instead spawn a thread for each row and run across all the columns in that row sequentially in a single thread. This could even be done using `std::thread::scoped` (and maybe the `.chunks_mut` iterator) to avoid the overhead of channels (and arcs) and the need for the second pair of loops by writing directly into the pixel buffer. Those approaches would be augmented by using a threadpool, e.g. http://doc.rust-lang.org/threadpool/ (available on crates.io). (I suppose I should mention my [`simple_parallel`](http://huonw.github.io/simple_parallel/simple_parallel) library (also available on crates.io); in particular the `for_` and `Pool::for_` functions, which do this sort of parallel for loop automagically, leveraging iterators to ensure maximum usability and safety. They're basically the `scoped` variant mentioned above, with the freestanding one identical and the `Pool` version limiting the number of threads spawned. The latter likely has a lot of synchronisation overhead atm.)
It says "around March 9" and https://github.com/rust-lang/rust/issues/22500 has "the Week of March 9", so there are a few days of things still landing yet.
 &gt; You're spawning a thread for every pixel, which is very very expensive. I did a quick change to spawn only threads for each line of the image and it is far more faster now. Here is the new code: for y in 0..screen.height{ let new_percentage = (y as f64 * 100.0 / screen.height as f64).round() as u64; if new_percentage &gt; percentage{ println!("{}%",new_percentage); } percentage = new_percentage; let tx = tx.clone(); let arc_model_clone = arc_model.clone(); let arc_camera_clone = arc_camera.clone(); let arc_screen_clone = arc_screen.clone(); Thread::spawn(move || { let mut line = Vec::new(); for x in 0..width{ let color = trace_pixel(lod, view_lod, &amp;arc_model_clone, &amp;arc_screen_clone, &amp;arc_camera_clone, x, y, max_distance); line.push(color); } tx.send((y, line)); }); } for j in 0..screen.height{ let (y, line) = rx.recv().ok().expect("Could not recieve answer"); for i in 0..screen.width{ let index = y * screen.width + i; pixels[index as usize] = line[i as usize].clone(); } } pixels Thanks you very much.
Spawn threads only when necessary. Keep threads around while there are jobs in queue. The easiest way to synchronize is to send work for threads and receive the job results over queues/channels. For CPU intensive tasks, have the same number of threads as CPU cores (usual "thread poll" idea). For I/O bound tasks, higher number of threads might be worth it when a thread is waiting on the I/O resource. But then the blocking solution might not be the best one. These are some guidelines I learned from other languages over the years.
&gt; For CPU intensive tasks, have the same number of threads as CPU cores Out of curiosity, are there any cross-platform libs that can tell you how many cores are available on a system at runtime?
I guess there is no way to push for type-level integers in 1.0 right?
On Rust? What about `std::os::num_cpus()`? As far as I know it's dynamically executed.
This series has some good advice for how to design threaded systems - http://www.1024cores.net/home/scalable-architecture/general-recipe
You may be right, and I think that was a thread of thought in my mind (though perhaps it stems from a lack of understand about exactly how XUL works) but every time I've spoken with someone knowledgeable of the topic (developers), the response is that Servo will not be be able to replace Gecko at all in Firefox. If it truly is impractical to parse, my hope would be that it would be possible to at least port large parts of the Firefox codebase as there is no need to waste years of man-hours on the basics. All that said, it is important, I think, that addons have a easy path forward as decimating the addon ecosystem would be bad idea.
[Resharper](https://www.jetbrains.com/resharper/) is the goto plugin for .net [Visual Assist X](http://www.wholetomato.com/) is the goto plugin for C++ The CLion initiative might be better once it's released and boost the C++ intellisense for resharper but currently vax is the best. However, the C++ intellisense will always lack the features/performance C# has as it's a more complicated language.
I think you should split by threads as high up as possible. The number of worker threads should both be A) tunable and B) in a low range of 1-32 threads, far fewer than the number of rows.
You could spawn a number of threads equal to the number of cores you have and give each a subset of roughly W x H / C pixels.
As much of your code is dealing with CLI arguments, you might want to look into using a crate like [getopts](https://crates.io/crates/getopts) or [docopt](https://crates.io/crates/docopt).
I did optimize the code to spawn threads equal to the number of cores, and it's much faster than the initial code.
Yeah I really like what you're doing. Windows support is a long way off for me, and yours looks like it will be a great option for it.
IIRC, they are disabled by default in release builds
Yeah, this was just the next tiny step in a long journey to make RTL and other writing modes work. The CSS Working Group has a test suite that includes [tests for bidirectional text and other related layout features](http://test.csswg.org/suites/css-writing-modes-3_dev/nightly-unstable/html/toc.htm).
Racer/autocompletion is still way off for me. Currently I got 8 bulletpoints before I can make a public release that might be useful :)
That looks really good! I'll try to read your code later today. Do you think adding encryption support will be hard? Or can you reuse many already-existing implementations (from HTTPS/1.1)? (Github tip: Always link to specific revisions of files and line numbers. You can easily change any link to one that contains the current commit hash by pressing `y`.)
Just checking - you're familiar with the [WrappingOps](http://doc.rust-lang.org/std/num/wrapping/trait.WrappingOps.html), right? If it's just a few operations that are expected to sometimes wrap.
*crickets*
Isn't HTTP/2 always encrypted?
One of the major roadblocks to this is that cargo doesn't build on the latest nightly, but rather on the version that is marked in `src/rustversion.txt`. Currently that is `2015-03-11`, though you may have cloned the repository before that update, and using the wrong nightly won't work. For the `src\rust-installer` error, you just need to use `git submodule update --init`. That will clone all submodules in the git repository. After doing that, it should build successfully assuming you have the right nightly version.
Nope. They removed that requirement. You can now either upgrade an HTTP/1.1 connection to HTTP/2 (using the usual HTTP/1.1 upgrade mechanism) or start a cleartext TCP connection directly by sending the HTTP/2 preface to a server as the first bytes. Check [this](http://http2.github.io/http2-spec/#discover-http) part of the spec. Although, it's worth noting that most major browsers are planning to only support the TLS-protected and negotiated parts.
Here's an example: #![feature(core)] fn main() { let a = Wrapping(140u8); let b = a + a; println!("{}", b.0); // prints 24 }
It shouldn't be too hard to support the TLS-backed connections... All I'd need is a `io::Read` and an `io::Write` trait implementation that writes the given bytes by first encrypting them. I didn't go into full detail yet how that would look in Rust, but given what I've seen it shouldn't be difficult or any different from how it's done for https over HTTP/1.1 right now. The biggest difference is probably that HTTP/2 requires that TLS v1.2 (or higher) be used, but practically this should only mean that the OpenSSL binding needs to allow us to specify this. Cheers for the GitHub tip!
By default rust executables use static linking, which includes all the code from the .rlib dependencies directly in your executable, so none of the .so or .rlib files need to be distributed with it. Executables created by rustc basically have two runtime dependencies: libc, and (possibly) an unwinding library. Because the libc interfaces are relatively stable, if you build a Rust application on a relatively old revision of Linux or Mac, that binary will be highly compatible with other revisions of the OS. On windows the situation is complicated because we depend on MinGW, the implementation of the GNU toolchain for Windows.
Thanks for the clarification. Static linking definitely simplifies things a lot.
I think this is the kind of project best done outside the mainstream compiler. I'd be really interested to see it happen though. I would be interested to see how far it could be done in programmable macros/syntax extensions - I imagine loop invariants should be possible, but object invariants might be more difficult (since the checks have to expand non-locally). This is exactly the kind of task we want compiler plugins to be used for, so if we can make the compiler's APIs more flexible to accommodate this kind of stuff, we'd be happy to do so. On the libhoare front, I've added contracts on methods (as opposed to just on functions), which was the main thing stopping me using it elsewhere. I need to find time to finish that off though. I don't think we can do much about the syntax, but let me know if you have suggestions for improvement. What would you want to check inside a method which isn't achievable with `assert!`? Obviously, static checking would be nice, but how an invariant is checked is orthogonal to what is checked. Issues and patches for libhoare are very welcome :-)
Iron docs are [here](http://ironframework.io/doc/iron/). The status codes are [here](http://ironframework.io/doc/iron/status/enum.Status.html) and are re-exported to `iron::status`
It's not in your hot loop, but it also looks like you're doing a lot of work to compute and print `new_percentage` with every outer iteration, whereas that's something you don't do in the non-threaded version. I'd remove all that code for a more even comparison.
Um. Okidoke.
I don't really see how any of these relate to safety at all? What do you even mean by "clean"?
That use of `Arc`/`Mutex` looks perfectly fine, assuming that that is semantically what you are trying to achieve. :) However... you probably don't need to use `OsRng` as your main source of randomness. Using it like that will call into the operating system for every piece of random data that you need, which is much slower than doing it in "user-space", e.g. using `ChaChaRng`. You can randomly seed a `ChaChaRng` from the OS using the implementation of `Rand` for it: let prng = match OsRng::new() { Ok(mut os_rng) =&gt; Arc::new(Mutex::new(PrngHandler { map: HashMap::new(), rng: os_rng.gen::&lt;rand::ChaChaRng&gt;(), })), Err(e) =&gt; { println!("{}", e.description()); return; } };
Don't be so quick to judge! Rust itself was much different in its early one-person-hacking-on-a-compiler days. Languages need time to mature.
looks cool! :)
As someone who does a lot of web-related stuff, please, please, please, please always pick option #2. &gt; I have no idea where to find the error other status codes at all. http://httpstatus.es/ is a nice website that explains them pretty simply.
I was discussing this with the front end guy at work right before I left, actually, and he made a strong case for option one--and I actually preferred option two before I talked to him. :| I probably don't care enough to pick beyond flipping a coin still, mind you. :P
&gt; Although, it's worth noting that most major browsers are planning to only support the TLS-protected and negotiated parts. Which hopefully means that they're going to accept self-signed certificates, and handle fake MITM certificates by checking against a DNS record, which is hopefully protected by DNSSEC. Meaning that you can read in the dns record what cert authority the key should be signed by. And if that doesn't work *still* use encryption but don't display lock symbols or such to the user, as it may be encrypted but not authenticated. This "let's not encrypt stuff but send plain text because the key might be the wrong one" situation is completely absurd. You know, [like here](https://ccc.de/) (which isn't even self-signed, it's CACert) vs [here](http://ccc.de/).
I don't s̲̭͕̞͕̗ͅee͍ a͇̻̖̹͍͖n̡yt̰̻͍͙̹ḩ̟̯̜i̝̜̪͉͖͍̥n̡͎̣g̱ o̭̫̖͟u̘̜̪͓̟̻̻̟ͅt̶̸̻̤̹̗ͅ ̸̼͖̼̬̭̜o̧͔̱̪̥͘f̛̪̟͖ ̲̣t̗͍̹͉͡ẖ̷̪̗͙̲̹͖͡ͅe̸͙̺̯͚ oͣ͒̋͞͏̟̩̥̩ŕ̵͍̥̱̙̜̭̼̥ͥ̓̍͒ͧͪͣͮ͞d̵̹̯͎̝͇̦̐͆ͫͥ͜i̷͉̳̮̬ͬ̊̄̆̔̇n̺̲̳͎̅̀̾̋̌ͫͭ̔ͅạ̡̨̠̮̱̩̾͛ͭr̲̹̬͔̺̼̱ͫ̓ͨ͑ͦ̿̀̂ͫ̀y̛̟͚̻͋̀́.
*&gt;Grabs Hatchet*
I always knew this day would come.
I hope we had a lower-cased variant of digits (and *some* symbols, too) so that texts like "libarena-4e7c5e5c.so" does not look weird.